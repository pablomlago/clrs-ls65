Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-02 18:56:31.952481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-02 18:56:31.952794: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-02 18:56:31.979720: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-02 18:56:36.425402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0302 18:56:47.198305 22699590365312 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0302 18:56:47.208408 22699590365312 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0302 18:56:47.599468 22699590365312 run.py:307] Creating samplers for algo bellman_ford
W0302 18:56:47.599904 22699590365312 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:47.600199 22699590365312 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:47.808264 22699590365312 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:47.808502 22699590365312 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.057016 22699590365312 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.057282 22699590365312 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.401758 22699590365312 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.402016 22699590365312 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.829373 22699590365312 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.829629 22699590365312 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:49.366869 22699590365312 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0302 18:56:49.367133 22699590365312 samplers.py:112] Creating a dataset with 64 samples.
I0302 18:56:49.405546 22699590365312 run.py:166] Dataset not found in ./datasets_1/24/CLRS30_v1.0.0. Downloading...
I0302 18:57:05.331628 22699590365312 dataset_info.py:482] Load dataset info from ./datasets_1/24/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:05.334151 22699590365312 dataset_info.py:482] Load dataset info from ./datasets_1/24/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:05.334936 22699590365312 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/24/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0302 18:57:05.335016 22699590365312 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/24/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:21.545282 22699590365312 run.py:483] Algo bellman_ford step 0 current loss 3.546275, current_train_items 32.
I0302 18:57:24.406448 22699590365312 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.3525390625, 'score': 0.3525390625, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0302 18:57:24.406722 22699590365312 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.353, val scores are: bellman_ford: 0.353
I0302 18:57:34.551728 22699590365312 run.py:483] Algo bellman_ford step 1 current loss 210.560974, current_train_items 64.
I0302 18:57:45.673593 22699590365312 run.py:483] Algo bellman_ford step 2 current loss 116473664.000000, current_train_items 96.
I0302 18:57:56.802876 22699590365312 run.py:483] Algo bellman_ford step 3 current loss 5764884480.000000, current_train_items 128.
I0302 18:58:06.916321 22699590365312 run.py:483] Algo bellman_ford step 4 current loss 369227040.000000, current_train_items 160.
I0302 18:58:06.934693 22699590365312 run.py:483] Algo bellman_ford step 5 current loss 3.433601, current_train_items 192.
I0302 18:58:06.951556 22699590365312 run.py:483] Algo bellman_ford step 6 current loss 122.353447, current_train_items 224.
I0302 18:58:06.973788 22699590365312 run.py:483] Algo bellman_ford step 7 current loss 286023.437500, current_train_items 256.
I0302 18:58:07.002725 22699590365312 run.py:483] Algo bellman_ford step 8 current loss 33257326.000000, current_train_items 288.
I0302 18:58:07.034234 22699590365312 run.py:483] Algo bellman_ford step 9 current loss 19665736.000000, current_train_items 320.
I0302 18:58:07.051550 22699590365312 run.py:483] Algo bellman_ford step 10 current loss 4.704422, current_train_items 352.
I0302 18:58:07.068073 22699590365312 run.py:483] Algo bellman_ford step 11 current loss 12.153404, current_train_items 384.
I0302 18:58:07.089644 22699590365312 run.py:483] Algo bellman_ford step 12 current loss 52126.726562, current_train_items 416.
I0302 18:58:07.119942 22699590365312 run.py:483] Algo bellman_ford step 13 current loss 1754121.500000, current_train_items 448.
I0302 18:58:07.147860 22699590365312 run.py:483] Algo bellman_ford step 14 current loss 603583.562500, current_train_items 480.
I0302 18:58:07.165612 22699590365312 run.py:483] Algo bellman_ford step 15 current loss 1.492238, current_train_items 512.
I0302 18:58:07.181687 22699590365312 run.py:483] Algo bellman_ford step 16 current loss 5.710592, current_train_items 544.
I0302 18:58:07.205959 22699590365312 run.py:483] Algo bellman_ford step 17 current loss 2703.508301, current_train_items 576.
I0302 18:58:07.234388 22699590365312 run.py:483] Algo bellman_ford step 18 current loss 36857.589844, current_train_items 608.
I0302 18:58:07.266414 22699590365312 run.py:483] Algo bellman_ford step 19 current loss 43240.906250, current_train_items 640.
I0302 18:58:07.283856 22699590365312 run.py:483] Algo bellman_ford step 20 current loss 1.365895, current_train_items 672.
I0302 18:58:07.299384 22699590365312 run.py:483] Algo bellman_ford step 21 current loss 3.223949, current_train_items 704.
I0302 18:58:07.322688 22699590365312 run.py:483] Algo bellman_ford step 22 current loss 265.616577, current_train_items 736.
I0302 18:58:07.351311 22699590365312 run.py:483] Algo bellman_ford step 23 current loss 5879.010742, current_train_items 768.
I0302 18:58:07.381936 22699590365312 run.py:483] Algo bellman_ford step 24 current loss 10282.359375, current_train_items 800.
I0302 18:58:07.399285 22699590365312 run.py:483] Algo bellman_ford step 25 current loss 1.333871, current_train_items 832.
I0302 18:58:07.415409 22699590365312 run.py:483] Algo bellman_ford step 26 current loss 2.211044, current_train_items 864.
I0302 18:58:07.438511 22699590365312 run.py:483] Algo bellman_ford step 27 current loss 23.964787, current_train_items 896.
I0302 18:58:07.467926 22699590365312 run.py:483] Algo bellman_ford step 28 current loss 407.195953, current_train_items 928.
I0302 18:58:07.500219 22699590365312 run.py:483] Algo bellman_ford step 29 current loss 1154.078125, current_train_items 960.
I0302 18:58:07.517645 22699590365312 run.py:483] Algo bellman_ford step 30 current loss 1.179518, current_train_items 992.
I0302 18:58:07.532857 22699590365312 run.py:483] Algo bellman_ford step 31 current loss 1.726849, current_train_items 1024.
I0302 18:58:07.555110 22699590365312 run.py:483] Algo bellman_ford step 32 current loss 17.523361, current_train_items 1056.
I0302 18:58:07.584949 22699590365312 run.py:483] Algo bellman_ford step 33 current loss 217.136749, current_train_items 1088.
I0302 18:58:07.615737 22699590365312 run.py:483] Algo bellman_ford step 34 current loss 792.043945, current_train_items 1120.
I0302 18:58:07.633375 22699590365312 run.py:483] Algo bellman_ford step 35 current loss 1.226556, current_train_items 1152.
I0302 18:58:07.648895 22699590365312 run.py:483] Algo bellman_ford step 36 current loss 1.659601, current_train_items 1184.
I0302 18:58:07.671806 22699590365312 run.py:483] Algo bellman_ford step 37 current loss 9.304388, current_train_items 1216.
I0302 18:58:07.701211 22699590365312 run.py:483] Algo bellman_ford step 38 current loss 16.264179, current_train_items 1248.
W0302 18:58:07.723799 22699590365312 samplers.py:155] Increasing hint lengh from 9 to 11
I0302 18:58:14.376014 22699590365312 run.py:483] Algo bellman_ford step 39 current loss 41308.792969, current_train_items 1280.
I0302 18:58:14.395889 22699590365312 run.py:483] Algo bellman_ford step 40 current loss 1.131601, current_train_items 1312.
I0302 18:58:14.412293 22699590365312 run.py:483] Algo bellman_ford step 41 current loss 1.634354, current_train_items 1344.
I0302 18:58:14.434876 22699590365312 run.py:483] Algo bellman_ford step 42 current loss 2.681914, current_train_items 1376.
I0302 18:58:14.465571 22699590365312 run.py:483] Algo bellman_ford step 43 current loss 4.124504, current_train_items 1408.
I0302 18:58:14.498174 22699590365312 run.py:483] Algo bellman_ford step 44 current loss 36.824394, current_train_items 1440.
I0302 18:58:14.517238 22699590365312 run.py:483] Algo bellman_ford step 45 current loss 1.047320, current_train_items 1472.
I0302 18:58:14.533527 22699590365312 run.py:483] Algo bellman_ford step 46 current loss 1.665968, current_train_items 1504.
I0302 18:58:14.555723 22699590365312 run.py:483] Algo bellman_ford step 47 current loss 2.342397, current_train_items 1536.
I0302 18:58:14.583082 22699590365312 run.py:483] Algo bellman_ford step 48 current loss 3.572228, current_train_items 1568.
I0302 18:58:14.612215 22699590365312 run.py:483] Algo bellman_ford step 49 current loss 46.497540, current_train_items 1600.
I0302 18:58:14.630816 22699590365312 run.py:483] Algo bellman_ford step 50 current loss 0.946686, current_train_items 1632.
I0302 18:58:14.640295 22699590365312 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.6435546875, 'score': 0.6435546875, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0302 18:58:14.640407 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.353, current avg val score is 0.644, val scores are: bellman_ford: 0.644
I0302 18:58:14.669362 22699590365312 run.py:483] Algo bellman_ford step 51 current loss 1.686058, current_train_items 1664.
I0302 18:58:14.692475 22699590365312 run.py:483] Algo bellman_ford step 52 current loss 2.280454, current_train_items 1696.
I0302 18:58:14.721535 22699590365312 run.py:483] Algo bellman_ford step 53 current loss 2.316198, current_train_items 1728.
I0302 18:58:14.754374 22699590365312 run.py:483] Algo bellman_ford step 54 current loss 4.541225, current_train_items 1760.
I0302 18:58:14.773455 22699590365312 run.py:483] Algo bellman_ford step 55 current loss 0.919078, current_train_items 1792.
I0302 18:58:14.789505 22699590365312 run.py:483] Algo bellman_ford step 56 current loss 1.306435, current_train_items 1824.
I0302 18:58:14.811983 22699590365312 run.py:483] Algo bellman_ford step 57 current loss 2.031691, current_train_items 1856.
I0302 18:58:14.839649 22699590365312 run.py:483] Algo bellman_ford step 58 current loss 1.960546, current_train_items 1888.
I0302 18:58:14.872447 22699590365312 run.py:483] Algo bellman_ford step 59 current loss 2.531487, current_train_items 1920.
I0302 18:58:14.891222 22699590365312 run.py:483] Algo bellman_ford step 60 current loss 0.901324, current_train_items 1952.
W0302 18:58:14.900455 22699590365312 samplers.py:155] Increasing hint lengh from 6 to 7
I0302 18:58:21.279662 22699590365312 run.py:483] Algo bellman_ford step 61 current loss 1.229578, current_train_items 1984.
I0302 18:58:21.304038 22699590365312 run.py:483] Algo bellman_ford step 62 current loss 1.684071, current_train_items 2016.
I0302 18:58:21.333499 22699590365312 run.py:483] Algo bellman_ford step 63 current loss 2.105275, current_train_items 2048.
I0302 18:58:21.367917 22699590365312 run.py:483] Algo bellman_ford step 64 current loss 3.147929, current_train_items 2080.
I0302 18:58:21.387623 22699590365312 run.py:483] Algo bellman_ford step 65 current loss 0.800274, current_train_items 2112.
I0302 18:58:21.403716 22699590365312 run.py:483] Algo bellman_ford step 66 current loss 1.121736, current_train_items 2144.
I0302 18:58:21.428282 22699590365312 run.py:483] Algo bellman_ford step 67 current loss 2.062977, current_train_items 2176.
I0302 18:58:21.456380 22699590365312 run.py:483] Algo bellman_ford step 68 current loss 2.030727, current_train_items 2208.
I0302 18:58:21.488228 22699590365312 run.py:483] Algo bellman_ford step 69 current loss 2.667486, current_train_items 2240.
I0302 18:58:21.506791 22699590365312 run.py:483] Algo bellman_ford step 70 current loss 0.746774, current_train_items 2272.
I0302 18:58:21.522898 22699590365312 run.py:483] Algo bellman_ford step 71 current loss 1.282185, current_train_items 2304.
I0302 18:58:21.546046 22699590365312 run.py:483] Algo bellman_ford step 72 current loss 1.646436, current_train_items 2336.
I0302 18:58:21.575709 22699590365312 run.py:483] Algo bellman_ford step 73 current loss 1.755543, current_train_items 2368.
I0302 18:58:21.607889 22699590365312 run.py:483] Algo bellman_ford step 74 current loss 2.124496, current_train_items 2400.
I0302 18:58:21.626357 22699590365312 run.py:483] Algo bellman_ford step 75 current loss 0.582051, current_train_items 2432.
I0302 18:58:21.642885 22699590365312 run.py:483] Algo bellman_ford step 76 current loss 1.230583, current_train_items 2464.
I0302 18:58:21.665416 22699590365312 run.py:483] Algo bellman_ford step 77 current loss 1.692176, current_train_items 2496.
I0302 18:58:21.693778 22699590365312 run.py:483] Algo bellman_ford step 78 current loss 1.593042, current_train_items 2528.
I0302 18:58:21.723092 22699590365312 run.py:483] Algo bellman_ford step 79 current loss 1.868960, current_train_items 2560.
I0302 18:58:21.742200 22699590365312 run.py:483] Algo bellman_ford step 80 current loss 0.649794, current_train_items 2592.
I0302 18:58:21.758007 22699590365312 run.py:483] Algo bellman_ford step 81 current loss 1.057534, current_train_items 2624.
I0302 18:58:21.780853 22699590365312 run.py:483] Algo bellman_ford step 82 current loss 1.658227, current_train_items 2656.
I0302 18:58:21.809388 22699590365312 run.py:483] Algo bellman_ford step 83 current loss 1.790410, current_train_items 2688.
I0302 18:58:21.839467 22699590365312 run.py:483] Algo bellman_ford step 84 current loss 2.035706, current_train_items 2720.
I0302 18:58:21.857719 22699590365312 run.py:483] Algo bellman_ford step 85 current loss 0.565893, current_train_items 2752.
I0302 18:58:21.873905 22699590365312 run.py:483] Algo bellman_ford step 86 current loss 1.150569, current_train_items 2784.
I0302 18:58:21.898200 22699590365312 run.py:483] Algo bellman_ford step 87 current loss 1.982311, current_train_items 2816.
I0302 18:58:21.927351 22699590365312 run.py:483] Algo bellman_ford step 88 current loss 1.991226, current_train_items 2848.
I0302 18:58:21.959035 22699590365312 run.py:483] Algo bellman_ford step 89 current loss 2.117722, current_train_items 2880.
I0302 18:58:21.977807 22699590365312 run.py:483] Algo bellman_ford step 90 current loss 0.585141, current_train_items 2912.
I0302 18:58:21.994160 22699590365312 run.py:483] Algo bellman_ford step 91 current loss 1.071512, current_train_items 2944.
I0302 18:58:22.016807 22699590365312 run.py:483] Algo bellman_ford step 92 current loss 1.440681, current_train_items 2976.
I0302 18:58:22.047177 22699590365312 run.py:483] Algo bellman_ford step 93 current loss 1.855213, current_train_items 3008.
I0302 18:58:22.078482 22699590365312 run.py:483] Algo bellman_ford step 94 current loss 1.967391, current_train_items 3040.
I0302 18:58:22.097440 22699590365312 run.py:483] Algo bellman_ford step 95 current loss 0.564288, current_train_items 3072.
I0302 18:58:22.113255 22699590365312 run.py:483] Algo bellman_ford step 96 current loss 0.965197, current_train_items 3104.
I0302 18:58:22.136360 22699590365312 run.py:483] Algo bellman_ford step 97 current loss 1.588029, current_train_items 3136.
I0302 18:58:22.165525 22699590365312 run.py:483] Algo bellman_ford step 98 current loss 1.619799, current_train_items 3168.
I0302 18:58:22.197240 22699590365312 run.py:483] Algo bellman_ford step 99 current loss 2.044533, current_train_items 3200.
I0302 18:58:22.215452 22699590365312 run.py:483] Algo bellman_ford step 100 current loss 0.648592, current_train_items 3232.
I0302 18:58:22.225064 22699590365312 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.7578125, 'score': 0.7578125, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0302 18:58:22.225182 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.644, current avg val score is 0.758, val scores are: bellman_ford: 0.758
I0302 18:58:22.254308 22699590365312 run.py:483] Algo bellman_ford step 101 current loss 1.060943, current_train_items 3264.
I0302 18:58:22.277455 22699590365312 run.py:483] Algo bellman_ford step 102 current loss 1.299988, current_train_items 3296.
I0302 18:58:22.306102 22699590365312 run.py:483] Algo bellman_ford step 103 current loss 1.384206, current_train_items 3328.
I0302 18:58:22.340283 22699590365312 run.py:483] Algo bellman_ford step 104 current loss 2.013919, current_train_items 3360.
I0302 18:58:22.359349 22699590365312 run.py:483] Algo bellman_ford step 105 current loss 0.477070, current_train_items 3392.
I0302 18:58:22.375475 22699590365312 run.py:483] Algo bellman_ford step 106 current loss 0.935193, current_train_items 3424.
I0302 18:58:22.398788 22699590365312 run.py:483] Algo bellman_ford step 107 current loss 1.221484, current_train_items 3456.
I0302 18:58:22.427042 22699590365312 run.py:483] Algo bellman_ford step 108 current loss 1.600678, current_train_items 3488.
I0302 18:58:22.457306 22699590365312 run.py:483] Algo bellman_ford step 109 current loss 2.522945, current_train_items 3520.
I0302 18:58:22.475663 22699590365312 run.py:483] Algo bellman_ford step 110 current loss 0.549996, current_train_items 3552.
I0302 18:58:22.491744 22699590365312 run.py:483] Algo bellman_ford step 111 current loss 0.876510, current_train_items 3584.
I0302 18:58:22.514181 22699590365312 run.py:483] Algo bellman_ford step 112 current loss 1.127458, current_train_items 3616.
I0302 18:58:22.543339 22699590365312 run.py:483] Algo bellman_ford step 113 current loss 1.582496, current_train_items 3648.
I0302 18:58:22.573912 22699590365312 run.py:483] Algo bellman_ford step 114 current loss 1.674430, current_train_items 3680.
I0302 18:58:22.592232 22699590365312 run.py:483] Algo bellman_ford step 115 current loss 0.494570, current_train_items 3712.
I0302 18:58:22.608428 22699590365312 run.py:483] Algo bellman_ford step 116 current loss 1.083297, current_train_items 3744.
I0302 18:58:22.631982 22699590365312 run.py:483] Algo bellman_ford step 117 current loss 1.484266, current_train_items 3776.
I0302 18:58:22.660481 22699590365312 run.py:483] Algo bellman_ford step 118 current loss 1.344604, current_train_items 3808.
I0302 18:58:22.689479 22699590365312 run.py:483] Algo bellman_ford step 119 current loss 1.931577, current_train_items 3840.
I0302 18:58:22.707803 22699590365312 run.py:483] Algo bellman_ford step 120 current loss 0.509325, current_train_items 3872.
I0302 18:58:22.724103 22699590365312 run.py:483] Algo bellman_ford step 121 current loss 0.921259, current_train_items 3904.
I0302 18:58:22.747683 22699590365312 run.py:483] Algo bellman_ford step 122 current loss 1.341980, current_train_items 3936.
I0302 18:58:22.777181 22699590365312 run.py:483] Algo bellman_ford step 123 current loss 1.418474, current_train_items 3968.
I0302 18:58:22.812324 22699590365312 run.py:483] Algo bellman_ford step 124 current loss 2.117611, current_train_items 4000.
I0302 18:58:22.830993 22699590365312 run.py:483] Algo bellman_ford step 125 current loss 0.624633, current_train_items 4032.
I0302 18:58:22.847335 22699590365312 run.py:483] Algo bellman_ford step 126 current loss 0.994668, current_train_items 4064.
I0302 18:58:22.870612 22699590365312 run.py:483] Algo bellman_ford step 127 current loss 1.321791, current_train_items 4096.
I0302 18:58:22.899549 22699590365312 run.py:483] Algo bellman_ford step 128 current loss 1.450786, current_train_items 4128.
I0302 18:58:22.931228 22699590365312 run.py:483] Algo bellman_ford step 129 current loss 1.555615, current_train_items 4160.
I0302 18:58:22.949501 22699590365312 run.py:483] Algo bellman_ford step 130 current loss 0.481115, current_train_items 4192.
I0302 18:58:22.965329 22699590365312 run.py:483] Algo bellman_ford step 131 current loss 0.672201, current_train_items 4224.
I0302 18:58:22.989350 22699590365312 run.py:483] Algo bellman_ford step 132 current loss 1.383714, current_train_items 4256.
I0302 18:58:23.018439 22699590365312 run.py:483] Algo bellman_ford step 133 current loss 1.505332, current_train_items 4288.
I0302 18:58:23.049715 22699590365312 run.py:483] Algo bellman_ford step 134 current loss 1.817497, current_train_items 4320.
I0302 18:58:23.067896 22699590365312 run.py:483] Algo bellman_ford step 135 current loss 0.474191, current_train_items 4352.
I0302 18:58:23.084006 22699590365312 run.py:483] Algo bellman_ford step 136 current loss 0.842238, current_train_items 4384.
I0302 18:58:23.107193 22699590365312 run.py:483] Algo bellman_ford step 137 current loss 1.385350, current_train_items 4416.
I0302 18:58:23.135832 22699590365312 run.py:483] Algo bellman_ford step 138 current loss 1.381930, current_train_items 4448.
I0302 18:58:23.168493 22699590365312 run.py:483] Algo bellman_ford step 139 current loss 1.786762, current_train_items 4480.
I0302 18:58:23.186615 22699590365312 run.py:483] Algo bellman_ford step 140 current loss 0.411294, current_train_items 4512.
I0302 18:58:23.202878 22699590365312 run.py:483] Algo bellman_ford step 141 current loss 1.027536, current_train_items 4544.
I0302 18:58:23.225850 22699590365312 run.py:483] Algo bellman_ford step 142 current loss 1.295496, current_train_items 4576.
I0302 18:58:23.255500 22699590365312 run.py:483] Algo bellman_ford step 143 current loss 1.555551, current_train_items 4608.
I0302 18:58:23.286526 22699590365312 run.py:483] Algo bellman_ford step 144 current loss 1.515413, current_train_items 4640.
I0302 18:58:23.304611 22699590365312 run.py:483] Algo bellman_ford step 145 current loss 0.541922, current_train_items 4672.
I0302 18:58:23.320565 22699590365312 run.py:483] Algo bellman_ford step 146 current loss 0.857897, current_train_items 4704.
I0302 18:58:23.342948 22699590365312 run.py:483] Algo bellman_ford step 147 current loss 1.388196, current_train_items 4736.
I0302 18:58:23.372061 22699590365312 run.py:483] Algo bellman_ford step 148 current loss 1.577440, current_train_items 4768.
I0302 18:58:23.401938 22699590365312 run.py:483] Algo bellman_ford step 149 current loss 1.661129, current_train_items 4800.
I0302 18:58:23.420315 22699590365312 run.py:483] Algo bellman_ford step 150 current loss 0.545054, current_train_items 4832.
I0302 18:58:23.428360 22699590365312 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.8349609375, 'score': 0.8349609375, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0302 18:58:23.428471 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.758, current avg val score is 0.835, val scores are: bellman_ford: 0.835
I0302 18:58:23.457001 22699590365312 run.py:483] Algo bellman_ford step 151 current loss 0.698575, current_train_items 4864.
I0302 18:58:23.480312 22699590365312 run.py:483] Algo bellman_ford step 152 current loss 1.104155, current_train_items 4896.
I0302 18:58:23.509311 22699590365312 run.py:483] Algo bellman_ford step 153 current loss 1.311486, current_train_items 4928.
I0302 18:58:23.544650 22699590365312 run.py:483] Algo bellman_ford step 154 current loss 1.933102, current_train_items 4960.
I0302 18:58:23.563797 22699590365312 run.py:483] Algo bellman_ford step 155 current loss 0.511520, current_train_items 4992.
I0302 18:58:23.579975 22699590365312 run.py:483] Algo bellman_ford step 156 current loss 0.943222, current_train_items 5024.
I0302 18:58:23.603038 22699590365312 run.py:483] Algo bellman_ford step 157 current loss 1.179235, current_train_items 5056.
I0302 18:58:23.630698 22699590365312 run.py:483] Algo bellman_ford step 158 current loss 1.430203, current_train_items 5088.
I0302 18:58:23.662590 22699590365312 run.py:483] Algo bellman_ford step 159 current loss 1.695565, current_train_items 5120.
I0302 18:58:23.680882 22699590365312 run.py:483] Algo bellman_ford step 160 current loss 0.480861, current_train_items 5152.
I0302 18:58:23.696945 22699590365312 run.py:483] Algo bellman_ford step 161 current loss 0.801791, current_train_items 5184.
I0302 18:58:23.719274 22699590365312 run.py:483] Algo bellman_ford step 162 current loss 1.169179, current_train_items 5216.
I0302 18:58:23.748399 22699590365312 run.py:483] Algo bellman_ford step 163 current loss 1.574957, current_train_items 5248.
I0302 18:58:23.779549 22699590365312 run.py:483] Algo bellman_ford step 164 current loss 1.647142, current_train_items 5280.
I0302 18:58:23.798213 22699590365312 run.py:483] Algo bellman_ford step 165 current loss 0.522533, current_train_items 5312.
I0302 18:58:23.814558 22699590365312 run.py:483] Algo bellman_ford step 166 current loss 0.956972, current_train_items 5344.
I0302 18:58:23.836621 22699590365312 run.py:483] Algo bellman_ford step 167 current loss 1.106691, current_train_items 5376.
I0302 18:58:23.866168 22699590365312 run.py:483] Algo bellman_ford step 168 current loss 1.358120, current_train_items 5408.
I0302 18:58:23.897508 22699590365312 run.py:483] Algo bellman_ford step 169 current loss 1.523678, current_train_items 5440.
I0302 18:58:23.916077 22699590365312 run.py:483] Algo bellman_ford step 170 current loss 0.643093, current_train_items 5472.
I0302 18:58:23.932265 22699590365312 run.py:483] Algo bellman_ford step 171 current loss 0.839453, current_train_items 5504.
I0302 18:58:23.954075 22699590365312 run.py:483] Algo bellman_ford step 172 current loss 1.101014, current_train_items 5536.
I0302 18:58:23.983643 22699590365312 run.py:483] Algo bellman_ford step 173 current loss 1.425706, current_train_items 5568.
I0302 18:58:24.018881 22699590365312 run.py:483] Algo bellman_ford step 174 current loss 1.789699, current_train_items 5600.
I0302 18:58:24.037360 22699590365312 run.py:483] Algo bellman_ford step 175 current loss 0.497755, current_train_items 5632.
I0302 18:58:24.053478 22699590365312 run.py:483] Algo bellman_ford step 176 current loss 0.754712, current_train_items 5664.
I0302 18:58:24.076877 22699590365312 run.py:483] Algo bellman_ford step 177 current loss 1.156272, current_train_items 5696.
I0302 18:58:24.104575 22699590365312 run.py:483] Algo bellman_ford step 178 current loss 1.071170, current_train_items 5728.
I0302 18:58:24.135001 22699590365312 run.py:483] Algo bellman_ford step 179 current loss 1.491380, current_train_items 5760.
I0302 18:58:24.154063 22699590365312 run.py:483] Algo bellman_ford step 180 current loss 0.547092, current_train_items 5792.
I0302 18:58:24.170260 22699590365312 run.py:483] Algo bellman_ford step 181 current loss 0.779168, current_train_items 5824.
I0302 18:58:24.193960 22699590365312 run.py:483] Algo bellman_ford step 182 current loss 1.266391, current_train_items 5856.
I0302 18:58:24.222930 22699590365312 run.py:483] Algo bellman_ford step 183 current loss 1.216129, current_train_items 5888.
I0302 18:58:24.252439 22699590365312 run.py:483] Algo bellman_ford step 184 current loss 1.492167, current_train_items 5920.
I0302 18:58:24.270963 22699590365312 run.py:483] Algo bellman_ford step 185 current loss 0.403324, current_train_items 5952.
I0302 18:58:24.287561 22699590365312 run.py:483] Algo bellman_ford step 186 current loss 0.862960, current_train_items 5984.
I0302 18:58:24.309709 22699590365312 run.py:483] Algo bellman_ford step 187 current loss 1.272980, current_train_items 6016.
I0302 18:58:24.338078 22699590365312 run.py:483] Algo bellman_ford step 188 current loss 1.566502, current_train_items 6048.
I0302 18:58:24.371515 22699590365312 run.py:483] Algo bellman_ford step 189 current loss 1.765848, current_train_items 6080.
I0302 18:58:24.390138 22699590365312 run.py:483] Algo bellman_ford step 190 current loss 0.438175, current_train_items 6112.
I0302 18:58:24.406408 22699590365312 run.py:483] Algo bellman_ford step 191 current loss 0.880991, current_train_items 6144.
I0302 18:58:24.430279 22699590365312 run.py:483] Algo bellman_ford step 192 current loss 1.475325, current_train_items 6176.
I0302 18:58:24.458920 22699590365312 run.py:483] Algo bellman_ford step 193 current loss 1.514505, current_train_items 6208.
I0302 18:58:24.490070 22699590365312 run.py:483] Algo bellman_ford step 194 current loss 1.603659, current_train_items 6240.
I0302 18:58:24.508853 22699590365312 run.py:483] Algo bellman_ford step 195 current loss 0.432770, current_train_items 6272.
I0302 18:58:24.524912 22699590365312 run.py:483] Algo bellman_ford step 196 current loss 0.806933, current_train_items 6304.
I0302 18:58:24.547443 22699590365312 run.py:483] Algo bellman_ford step 197 current loss 1.208520, current_train_items 6336.
I0302 18:58:24.576961 22699590365312 run.py:483] Algo bellman_ford step 198 current loss 1.576612, current_train_items 6368.
I0302 18:58:24.609424 22699590365312 run.py:483] Algo bellman_ford step 199 current loss 1.528214, current_train_items 6400.
I0302 18:58:24.627930 22699590365312 run.py:483] Algo bellman_ford step 200 current loss 0.484475, current_train_items 6432.
I0302 18:58:24.635672 22699590365312 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.8154296875, 'score': 0.8154296875, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0302 18:58:24.635777 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.835, current avg val score is 0.815, val scores are: bellman_ford: 0.815
I0302 18:58:24.652281 22699590365312 run.py:483] Algo bellman_ford step 201 current loss 0.882862, current_train_items 6464.
I0302 18:58:24.675590 22699590365312 run.py:483] Algo bellman_ford step 202 current loss 1.234501, current_train_items 6496.
I0302 18:58:24.706378 22699590365312 run.py:483] Algo bellman_ford step 203 current loss 1.520167, current_train_items 6528.
I0302 18:58:24.741305 22699590365312 run.py:483] Algo bellman_ford step 204 current loss 1.728342, current_train_items 6560.
I0302 18:58:24.759927 22699590365312 run.py:483] Algo bellman_ford step 205 current loss 0.438510, current_train_items 6592.
I0302 18:58:24.775069 22699590365312 run.py:483] Algo bellman_ford step 206 current loss 0.688766, current_train_items 6624.
I0302 18:58:24.798453 22699590365312 run.py:483] Algo bellman_ford step 207 current loss 1.276963, current_train_items 6656.
I0302 18:58:24.827635 22699590365312 run.py:483] Algo bellman_ford step 208 current loss 1.286493, current_train_items 6688.
I0302 18:58:24.857923 22699590365312 run.py:483] Algo bellman_ford step 209 current loss 1.416050, current_train_items 6720.
I0302 18:58:24.876000 22699590365312 run.py:483] Algo bellman_ford step 210 current loss 0.570510, current_train_items 6752.
I0302 18:58:24.892057 22699590365312 run.py:483] Algo bellman_ford step 211 current loss 0.756734, current_train_items 6784.
I0302 18:58:24.915205 22699590365312 run.py:483] Algo bellman_ford step 212 current loss 1.784561, current_train_items 6816.
I0302 18:58:24.944527 22699590365312 run.py:483] Algo bellman_ford step 213 current loss 1.710280, current_train_items 6848.
I0302 18:58:24.972177 22699590365312 run.py:483] Algo bellman_ford step 214 current loss 1.558335, current_train_items 6880.
I0302 18:58:24.990325 22699590365312 run.py:483] Algo bellman_ford step 215 current loss 0.397683, current_train_items 6912.
I0302 18:58:25.006065 22699590365312 run.py:483] Algo bellman_ford step 216 current loss 0.707586, current_train_items 6944.
I0302 18:58:25.028425 22699590365312 run.py:483] Algo bellman_ford step 217 current loss 1.100621, current_train_items 6976.
I0302 18:58:25.056715 22699590365312 run.py:483] Algo bellman_ford step 218 current loss 1.452352, current_train_items 7008.
I0302 18:58:25.087011 22699590365312 run.py:483] Algo bellman_ford step 219 current loss 1.720109, current_train_items 7040.
I0302 18:58:25.105084 22699590365312 run.py:483] Algo bellman_ford step 220 current loss 0.421080, current_train_items 7072.
I0302 18:58:25.121187 22699590365312 run.py:483] Algo bellman_ford step 221 current loss 0.839513, current_train_items 7104.
I0302 18:58:25.144494 22699590365312 run.py:483] Algo bellman_ford step 222 current loss 1.264795, current_train_items 7136.
I0302 18:58:25.172549 22699590365312 run.py:483] Algo bellman_ford step 223 current loss 1.339018, current_train_items 7168.
I0302 18:58:25.204080 22699590365312 run.py:483] Algo bellman_ford step 224 current loss 1.731349, current_train_items 7200.
I0302 18:58:25.222281 22699590365312 run.py:483] Algo bellman_ford step 225 current loss 0.497465, current_train_items 7232.
I0302 18:58:25.238435 22699590365312 run.py:483] Algo bellman_ford step 226 current loss 0.997870, current_train_items 7264.
I0302 18:58:25.261038 22699590365312 run.py:483] Algo bellman_ford step 227 current loss 1.368795, current_train_items 7296.
I0302 18:58:25.290096 22699590365312 run.py:483] Algo bellman_ford step 228 current loss 1.390335, current_train_items 7328.
I0302 18:58:25.321970 22699590365312 run.py:483] Algo bellman_ford step 229 current loss 1.871867, current_train_items 7360.
I0302 18:58:25.339988 22699590365312 run.py:483] Algo bellman_ford step 230 current loss 0.420007, current_train_items 7392.
I0302 18:58:25.356013 22699590365312 run.py:483] Algo bellman_ford step 231 current loss 0.782644, current_train_items 7424.
I0302 18:58:25.379128 22699590365312 run.py:483] Algo bellman_ford step 232 current loss 1.176381, current_train_items 7456.
I0302 18:58:25.408604 22699590365312 run.py:483] Algo bellman_ford step 233 current loss 1.265554, current_train_items 7488.
I0302 18:58:25.441291 22699590365312 run.py:483] Algo bellman_ford step 234 current loss 1.562616, current_train_items 7520.
I0302 18:58:25.459717 22699590365312 run.py:483] Algo bellman_ford step 235 current loss 0.510271, current_train_items 7552.
I0302 18:58:25.475354 22699590365312 run.py:483] Algo bellman_ford step 236 current loss 0.788694, current_train_items 7584.
I0302 18:58:25.497742 22699590365312 run.py:483] Algo bellman_ford step 237 current loss 1.353080, current_train_items 7616.
I0302 18:58:25.526283 22699590365312 run.py:483] Algo bellman_ford step 238 current loss 1.542469, current_train_items 7648.
I0302 18:58:25.557482 22699590365312 run.py:483] Algo bellman_ford step 239 current loss 1.756476, current_train_items 7680.
I0302 18:58:25.575499 22699590365312 run.py:483] Algo bellman_ford step 240 current loss 0.418402, current_train_items 7712.
I0302 18:58:25.591570 22699590365312 run.py:483] Algo bellman_ford step 241 current loss 0.973251, current_train_items 7744.
I0302 18:58:25.614209 22699590365312 run.py:483] Algo bellman_ford step 242 current loss 1.137323, current_train_items 7776.
I0302 18:58:25.642665 22699590365312 run.py:483] Algo bellman_ford step 243 current loss 1.264407, current_train_items 7808.
I0302 18:58:25.671210 22699590365312 run.py:483] Algo bellman_ford step 244 current loss 1.369978, current_train_items 7840.
I0302 18:58:25.689436 22699590365312 run.py:483] Algo bellman_ford step 245 current loss 0.449008, current_train_items 7872.
I0302 18:58:25.705300 22699590365312 run.py:483] Algo bellman_ford step 246 current loss 0.735466, current_train_items 7904.
I0302 18:58:25.727054 22699590365312 run.py:483] Algo bellman_ford step 247 current loss 1.085028, current_train_items 7936.
I0302 18:58:25.756547 22699590365312 run.py:483] Algo bellman_ford step 248 current loss 1.449935, current_train_items 7968.
I0302 18:58:25.787744 22699590365312 run.py:483] Algo bellman_ford step 249 current loss 1.299582, current_train_items 8000.
I0302 18:58:25.805820 22699590365312 run.py:483] Algo bellman_ford step 250 current loss 0.454584, current_train_items 8032.
I0302 18:58:25.813999 22699590365312 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.8017578125, 'score': 0.8017578125, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0302 18:58:25.814104 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.835, current avg val score is 0.802, val scores are: bellman_ford: 0.802
I0302 18:58:25.831166 22699590365312 run.py:483] Algo bellman_ford step 251 current loss 0.916730, current_train_items 8064.
I0302 18:58:25.854941 22699590365312 run.py:483] Algo bellman_ford step 252 current loss 1.226393, current_train_items 8096.
I0302 18:58:25.884925 22699590365312 run.py:483] Algo bellman_ford step 253 current loss 1.298815, current_train_items 8128.
I0302 18:58:25.918101 22699590365312 run.py:483] Algo bellman_ford step 254 current loss 1.512069, current_train_items 8160.
I0302 18:58:25.937275 22699590365312 run.py:483] Algo bellman_ford step 255 current loss 0.530156, current_train_items 8192.
I0302 18:58:25.952833 22699590365312 run.py:483] Algo bellman_ford step 256 current loss 0.821292, current_train_items 8224.
I0302 18:58:25.974809 22699590365312 run.py:483] Algo bellman_ford step 257 current loss 1.033810, current_train_items 8256.
I0302 18:58:26.003552 22699590365312 run.py:483] Algo bellman_ford step 258 current loss 1.301177, current_train_items 8288.
I0302 18:58:26.033826 22699590365312 run.py:483] Algo bellman_ford step 259 current loss 1.488313, current_train_items 8320.
I0302 18:58:26.052619 22699590365312 run.py:483] Algo bellman_ford step 260 current loss 0.421425, current_train_items 8352.
I0302 18:58:26.068497 22699590365312 run.py:483] Algo bellman_ford step 261 current loss 0.732804, current_train_items 8384.
I0302 18:58:26.090928 22699590365312 run.py:483] Algo bellman_ford step 262 current loss 1.098635, current_train_items 8416.
I0302 18:58:26.119162 22699590365312 run.py:483] Algo bellman_ford step 263 current loss 1.181072, current_train_items 8448.
I0302 18:58:26.149533 22699590365312 run.py:483] Algo bellman_ford step 264 current loss 1.440029, current_train_items 8480.
I0302 18:58:26.168112 22699590365312 run.py:483] Algo bellman_ford step 265 current loss 0.360546, current_train_items 8512.
I0302 18:58:26.184309 22699590365312 run.py:483] Algo bellman_ford step 266 current loss 0.776680, current_train_items 8544.
I0302 18:58:26.207864 22699590365312 run.py:483] Algo bellman_ford step 267 current loss 1.107716, current_train_items 8576.
I0302 18:58:26.237834 22699590365312 run.py:483] Algo bellman_ford step 268 current loss 1.218581, current_train_items 8608.
I0302 18:58:26.267950 22699590365312 run.py:483] Algo bellman_ford step 269 current loss 1.321223, current_train_items 8640.
I0302 18:58:26.286859 22699590365312 run.py:483] Algo bellman_ford step 270 current loss 0.407900, current_train_items 8672.
I0302 18:58:26.302426 22699590365312 run.py:483] Algo bellman_ford step 271 current loss 0.629957, current_train_items 8704.
I0302 18:58:26.325875 22699590365312 run.py:483] Algo bellman_ford step 272 current loss 0.987763, current_train_items 8736.
I0302 18:58:26.354797 22699590365312 run.py:483] Algo bellman_ford step 273 current loss 1.210515, current_train_items 8768.
I0302 18:58:26.384786 22699590365312 run.py:483] Algo bellman_ford step 274 current loss 1.380937, current_train_items 8800.
I0302 18:58:26.403472 22699590365312 run.py:483] Algo bellman_ford step 275 current loss 0.456687, current_train_items 8832.
I0302 18:58:26.419537 22699590365312 run.py:483] Algo bellman_ford step 276 current loss 0.603201, current_train_items 8864.
I0302 18:58:26.443280 22699590365312 run.py:483] Algo bellman_ford step 277 current loss 1.203928, current_train_items 8896.
I0302 18:58:26.473143 22699590365312 run.py:483] Algo bellman_ford step 278 current loss 1.412862, current_train_items 8928.
I0302 18:58:26.504485 22699590365312 run.py:483] Algo bellman_ford step 279 current loss 1.261453, current_train_items 8960.
I0302 18:58:26.522904 22699590365312 run.py:483] Algo bellman_ford step 280 current loss 0.352517, current_train_items 8992.
I0302 18:58:26.539150 22699590365312 run.py:483] Algo bellman_ford step 281 current loss 0.826291, current_train_items 9024.
I0302 18:58:26.562688 22699590365312 run.py:483] Algo bellman_ford step 282 current loss 1.089629, current_train_items 9056.
I0302 18:58:26.591251 22699590365312 run.py:483] Algo bellman_ford step 283 current loss 1.378475, current_train_items 9088.
I0302 18:58:26.624266 22699590365312 run.py:483] Algo bellman_ford step 284 current loss 1.997828, current_train_items 9120.
I0302 18:58:26.643252 22699590365312 run.py:483] Algo bellman_ford step 285 current loss 0.406449, current_train_items 9152.
I0302 18:58:26.659382 22699590365312 run.py:483] Algo bellman_ford step 286 current loss 0.732303, current_train_items 9184.
I0302 18:58:26.681661 22699590365312 run.py:483] Algo bellman_ford step 287 current loss 1.051391, current_train_items 9216.
I0302 18:58:26.711292 22699590365312 run.py:483] Algo bellman_ford step 288 current loss 1.357321, current_train_items 9248.
I0302 18:58:26.743708 22699590365312 run.py:483] Algo bellman_ford step 289 current loss 1.688178, current_train_items 9280.
I0302 18:58:26.762509 22699590365312 run.py:483] Algo bellman_ford step 290 current loss 0.337979, current_train_items 9312.
I0302 18:58:26.778570 22699590365312 run.py:483] Algo bellman_ford step 291 current loss 0.783217, current_train_items 9344.
I0302 18:58:26.800995 22699590365312 run.py:483] Algo bellman_ford step 292 current loss 0.939144, current_train_items 9376.
I0302 18:58:26.829697 22699590365312 run.py:483] Algo bellman_ford step 293 current loss 1.231922, current_train_items 9408.
I0302 18:58:26.861195 22699590365312 run.py:483] Algo bellman_ford step 294 current loss 1.386975, current_train_items 9440.
I0302 18:58:26.879480 22699590365312 run.py:483] Algo bellman_ford step 295 current loss 0.385096, current_train_items 9472.
I0302 18:58:26.895178 22699590365312 run.py:483] Algo bellman_ford step 296 current loss 0.709509, current_train_items 9504.
I0302 18:58:26.918692 22699590365312 run.py:483] Algo bellman_ford step 297 current loss 1.028920, current_train_items 9536.
I0302 18:58:26.948514 22699590365312 run.py:483] Algo bellman_ford step 298 current loss 1.284117, current_train_items 9568.
I0302 18:58:26.979438 22699590365312 run.py:483] Algo bellman_ford step 299 current loss 1.438035, current_train_items 9600.
I0302 18:58:26.998188 22699590365312 run.py:483] Algo bellman_ford step 300 current loss 0.413844, current_train_items 9632.
I0302 18:58:27.005727 22699590365312 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.8505859375, 'score': 0.8505859375, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0302 18:58:27.005834 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.835, current avg val score is 0.851, val scores are: bellman_ford: 0.851
I0302 18:58:27.035743 22699590365312 run.py:483] Algo bellman_ford step 301 current loss 0.743067, current_train_items 9664.
I0302 18:58:27.058352 22699590365312 run.py:483] Algo bellman_ford step 302 current loss 0.999556, current_train_items 9696.
I0302 18:58:27.087093 22699590365312 run.py:483] Algo bellman_ford step 303 current loss 0.998216, current_train_items 9728.
I0302 18:58:27.119936 22699590365312 run.py:483] Algo bellman_ford step 304 current loss 1.398453, current_train_items 9760.
I0302 18:58:27.138935 22699590365312 run.py:483] Algo bellman_ford step 305 current loss 0.452569, current_train_items 9792.
I0302 18:58:27.155398 22699590365312 run.py:483] Algo bellman_ford step 306 current loss 0.878467, current_train_items 9824.
I0302 18:58:27.179713 22699590365312 run.py:483] Algo bellman_ford step 307 current loss 1.073049, current_train_items 9856.
I0302 18:58:27.209069 22699590365312 run.py:483] Algo bellman_ford step 308 current loss 1.138809, current_train_items 9888.
I0302 18:58:27.240613 22699590365312 run.py:483] Algo bellman_ford step 309 current loss 1.328968, current_train_items 9920.
I0302 18:58:27.259384 22699590365312 run.py:483] Algo bellman_ford step 310 current loss 0.446863, current_train_items 9952.
I0302 18:58:27.275246 22699590365312 run.py:483] Algo bellman_ford step 311 current loss 0.752290, current_train_items 9984.
I0302 18:58:27.297981 22699590365312 run.py:483] Algo bellman_ford step 312 current loss 1.082418, current_train_items 10016.
I0302 18:58:27.327887 22699590365312 run.py:483] Algo bellman_ford step 313 current loss 1.324660, current_train_items 10048.
I0302 18:58:27.359427 22699590365312 run.py:483] Algo bellman_ford step 314 current loss 1.499239, current_train_items 10080.
I0302 18:58:27.377715 22699590365312 run.py:483] Algo bellman_ford step 315 current loss 0.449565, current_train_items 10112.
I0302 18:58:27.393999 22699590365312 run.py:483] Algo bellman_ford step 316 current loss 0.757017, current_train_items 10144.
I0302 18:58:27.416542 22699590365312 run.py:483] Algo bellman_ford step 317 current loss 0.939426, current_train_items 10176.
I0302 18:58:27.444457 22699590365312 run.py:483] Algo bellman_ford step 318 current loss 1.100370, current_train_items 10208.
I0302 18:58:27.475198 22699590365312 run.py:483] Algo bellman_ford step 319 current loss 1.516011, current_train_items 10240.
I0302 18:58:27.493424 22699590365312 run.py:483] Algo bellman_ford step 320 current loss 0.365095, current_train_items 10272.
I0302 18:58:27.509528 22699590365312 run.py:483] Algo bellman_ford step 321 current loss 0.863080, current_train_items 10304.
I0302 18:58:27.532220 22699590365312 run.py:483] Algo bellman_ford step 322 current loss 1.025305, current_train_items 10336.
I0302 18:58:27.560382 22699590365312 run.py:483] Algo bellman_ford step 323 current loss 1.189081, current_train_items 10368.
I0302 18:58:27.592020 22699590365312 run.py:483] Algo bellman_ford step 324 current loss 1.851965, current_train_items 10400.
I0302 18:58:27.610440 22699590365312 run.py:483] Algo bellman_ford step 325 current loss 0.493095, current_train_items 10432.
I0302 18:58:27.626128 22699590365312 run.py:483] Algo bellman_ford step 326 current loss 0.736728, current_train_items 10464.
I0302 18:58:27.649288 22699590365312 run.py:483] Algo bellman_ford step 327 current loss 1.156551, current_train_items 10496.
I0302 18:58:27.679183 22699590365312 run.py:483] Algo bellman_ford step 328 current loss 1.425671, current_train_items 10528.
I0302 18:58:27.711292 22699590365312 run.py:483] Algo bellman_ford step 329 current loss 1.406232, current_train_items 10560.
I0302 18:58:27.729708 22699590365312 run.py:483] Algo bellman_ford step 330 current loss 0.409124, current_train_items 10592.
I0302 18:58:27.745678 22699590365312 run.py:483] Algo bellman_ford step 331 current loss 0.777390, current_train_items 10624.
I0302 18:58:27.768678 22699590365312 run.py:483] Algo bellman_ford step 332 current loss 1.038084, current_train_items 10656.
I0302 18:58:27.798368 22699590365312 run.py:483] Algo bellman_ford step 333 current loss 1.280242, current_train_items 10688.
I0302 18:58:27.831308 22699590365312 run.py:483] Algo bellman_ford step 334 current loss 1.583925, current_train_items 10720.
I0302 18:58:27.849826 22699590365312 run.py:483] Algo bellman_ford step 335 current loss 0.448983, current_train_items 10752.
I0302 18:58:27.866468 22699590365312 run.py:483] Algo bellman_ford step 336 current loss 0.923255, current_train_items 10784.
I0302 18:58:27.890613 22699590365312 run.py:483] Algo bellman_ford step 337 current loss 1.185564, current_train_items 10816.
I0302 18:58:27.920277 22699590365312 run.py:483] Algo bellman_ford step 338 current loss 1.158459, current_train_items 10848.
I0302 18:58:27.952680 22699590365312 run.py:483] Algo bellman_ford step 339 current loss 1.339193, current_train_items 10880.
I0302 18:58:27.970949 22699590365312 run.py:483] Algo bellman_ford step 340 current loss 0.375372, current_train_items 10912.
I0302 18:58:27.986699 22699590365312 run.py:483] Algo bellman_ford step 341 current loss 0.762968, current_train_items 10944.
I0302 18:58:28.009233 22699590365312 run.py:483] Algo bellman_ford step 342 current loss 1.232087, current_train_items 10976.
I0302 18:58:28.037355 22699590365312 run.py:483] Algo bellman_ford step 343 current loss 1.068905, current_train_items 11008.
I0302 18:58:28.069539 22699590365312 run.py:483] Algo bellman_ford step 344 current loss 1.319931, current_train_items 11040.
I0302 18:58:28.088270 22699590365312 run.py:483] Algo bellman_ford step 345 current loss 0.437279, current_train_items 11072.
I0302 18:58:28.104332 22699590365312 run.py:483] Algo bellman_ford step 346 current loss 0.741905, current_train_items 11104.
I0302 18:58:28.128504 22699590365312 run.py:483] Algo bellman_ford step 347 current loss 1.172981, current_train_items 11136.
I0302 18:58:28.157451 22699590365312 run.py:483] Algo bellman_ford step 348 current loss 1.192557, current_train_items 11168.
I0302 18:58:28.190068 22699590365312 run.py:483] Algo bellman_ford step 349 current loss 1.626334, current_train_items 11200.
I0302 18:58:28.208595 22699590365312 run.py:483] Algo bellman_ford step 350 current loss 0.523377, current_train_items 11232.
I0302 18:58:28.216911 22699590365312 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.8232421875, 'score': 0.8232421875, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0302 18:58:28.217017 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.851, current avg val score is 0.823, val scores are: bellman_ford: 0.823
I0302 18:58:28.233381 22699590365312 run.py:483] Algo bellman_ford step 351 current loss 0.718842, current_train_items 11264.
I0302 18:58:28.257211 22699590365312 run.py:483] Algo bellman_ford step 352 current loss 1.062953, current_train_items 11296.
I0302 18:58:28.286615 22699590365312 run.py:483] Algo bellman_ford step 353 current loss 1.358635, current_train_items 11328.
I0302 18:58:28.320279 22699590365312 run.py:483] Algo bellman_ford step 354 current loss 1.503311, current_train_items 11360.
I0302 18:58:28.339457 22699590365312 run.py:483] Algo bellman_ford step 355 current loss 0.402375, current_train_items 11392.
I0302 18:58:28.355060 22699590365312 run.py:483] Algo bellman_ford step 356 current loss 0.740502, current_train_items 11424.
I0302 18:58:28.377599 22699590365312 run.py:483] Algo bellman_ford step 357 current loss 0.949040, current_train_items 11456.
I0302 18:58:28.408416 22699590365312 run.py:483] Algo bellman_ford step 358 current loss 1.373706, current_train_items 11488.
I0302 18:58:28.441136 22699590365312 run.py:483] Algo bellman_ford step 359 current loss 1.538303, current_train_items 11520.
I0302 18:58:28.460058 22699590365312 run.py:483] Algo bellman_ford step 360 current loss 0.375551, current_train_items 11552.
I0302 18:58:28.476418 22699590365312 run.py:483] Algo bellman_ford step 361 current loss 0.730378, current_train_items 11584.
I0302 18:58:28.498284 22699590365312 run.py:483] Algo bellman_ford step 362 current loss 0.993707, current_train_items 11616.
I0302 18:58:28.527117 22699590365312 run.py:483] Algo bellman_ford step 363 current loss 1.272299, current_train_items 11648.
I0302 18:58:28.557308 22699590365312 run.py:483] Algo bellman_ford step 364 current loss 1.169563, current_train_items 11680.
I0302 18:58:28.575814 22699590365312 run.py:483] Algo bellman_ford step 365 current loss 0.368251, current_train_items 11712.
I0302 18:58:28.591705 22699590365312 run.py:483] Algo bellman_ford step 366 current loss 0.887195, current_train_items 11744.
I0302 18:58:28.613813 22699590365312 run.py:483] Algo bellman_ford step 367 current loss 0.918831, current_train_items 11776.
I0302 18:58:28.643629 22699590365312 run.py:483] Algo bellman_ford step 368 current loss 1.280025, current_train_items 11808.
I0302 18:58:28.673355 22699590365312 run.py:483] Algo bellman_ford step 369 current loss 1.347325, current_train_items 11840.
I0302 18:58:28.692138 22699590365312 run.py:483] Algo bellman_ford step 370 current loss 0.363411, current_train_items 11872.
I0302 18:58:28.707895 22699590365312 run.py:483] Algo bellman_ford step 371 current loss 0.643030, current_train_items 11904.
I0302 18:58:28.730228 22699590365312 run.py:483] Algo bellman_ford step 372 current loss 1.051054, current_train_items 11936.
I0302 18:58:28.760437 22699590365312 run.py:483] Algo bellman_ford step 373 current loss 1.161217, current_train_items 11968.
I0302 18:58:28.793878 22699590365312 run.py:483] Algo bellman_ford step 374 current loss 1.508396, current_train_items 12000.
I0302 18:58:28.812641 22699590365312 run.py:483] Algo bellman_ford step 375 current loss 0.430467, current_train_items 12032.
I0302 18:58:28.828210 22699590365312 run.py:483] Algo bellman_ford step 376 current loss 0.644722, current_train_items 12064.
I0302 18:58:28.851548 22699590365312 run.py:483] Algo bellman_ford step 377 current loss 1.083184, current_train_items 12096.
I0302 18:58:28.880697 22699590365312 run.py:483] Algo bellman_ford step 378 current loss 1.008396, current_train_items 12128.
I0302 18:58:28.913621 22699590365312 run.py:483] Algo bellman_ford step 379 current loss 1.458816, current_train_items 12160.
I0302 18:58:28.931874 22699590365312 run.py:483] Algo bellman_ford step 380 current loss 0.374670, current_train_items 12192.
I0302 18:58:28.948093 22699590365312 run.py:483] Algo bellman_ford step 381 current loss 0.766599, current_train_items 12224.
I0302 18:58:28.971077 22699590365312 run.py:483] Algo bellman_ford step 382 current loss 1.001654, current_train_items 12256.
I0302 18:58:29.000191 22699590365312 run.py:483] Algo bellman_ford step 383 current loss 1.227937, current_train_items 12288.
I0302 18:58:29.032730 22699590365312 run.py:483] Algo bellman_ford step 384 current loss 1.755115, current_train_items 12320.
I0302 18:58:29.051421 22699590365312 run.py:483] Algo bellman_ford step 385 current loss 0.448850, current_train_items 12352.
I0302 18:58:29.067661 22699590365312 run.py:483] Algo bellman_ford step 386 current loss 0.741192, current_train_items 12384.
I0302 18:58:29.090095 22699590365312 run.py:483] Algo bellman_ford step 387 current loss 1.048617, current_train_items 12416.
I0302 18:58:29.118884 22699590365312 run.py:483] Algo bellman_ford step 388 current loss 1.092280, current_train_items 12448.
I0302 18:58:29.150972 22699590365312 run.py:483] Algo bellman_ford step 389 current loss 1.550881, current_train_items 12480.
I0302 18:58:29.169522 22699590365312 run.py:483] Algo bellman_ford step 390 current loss 0.378280, current_train_items 12512.
I0302 18:58:29.185315 22699590365312 run.py:483] Algo bellman_ford step 391 current loss 0.741308, current_train_items 12544.
I0302 18:58:29.207920 22699590365312 run.py:483] Algo bellman_ford step 392 current loss 1.052945, current_train_items 12576.
I0302 18:58:29.238337 22699590365312 run.py:483] Algo bellman_ford step 393 current loss 1.325466, current_train_items 12608.
I0302 18:58:29.269533 22699590365312 run.py:483] Algo bellman_ford step 394 current loss 1.497680, current_train_items 12640.
I0302 18:58:29.287955 22699590365312 run.py:483] Algo bellman_ford step 395 current loss 0.350789, current_train_items 12672.
I0302 18:58:29.304006 22699590365312 run.py:483] Algo bellman_ford step 396 current loss 0.773851, current_train_items 12704.
I0302 18:58:29.327313 22699590365312 run.py:483] Algo bellman_ford step 397 current loss 1.180598, current_train_items 12736.
I0302 18:58:29.357617 22699590365312 run.py:483] Algo bellman_ford step 398 current loss 1.171868, current_train_items 12768.
I0302 18:58:29.387920 22699590365312 run.py:483] Algo bellman_ford step 399 current loss 1.309237, current_train_items 12800.
I0302 18:58:29.406641 22699590365312 run.py:483] Algo bellman_ford step 400 current loss 0.321490, current_train_items 12832.
I0302 18:58:29.414527 22699590365312 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.8388671875, 'score': 0.8388671875, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0302 18:58:29.414631 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.851, current avg val score is 0.839, val scores are: bellman_ford: 0.839
I0302 18:58:29.431686 22699590365312 run.py:483] Algo bellman_ford step 401 current loss 0.691060, current_train_items 12864.
I0302 18:58:29.455221 22699590365312 run.py:483] Algo bellman_ford step 402 current loss 1.238721, current_train_items 12896.
I0302 18:58:29.484793 22699590365312 run.py:483] Algo bellman_ford step 403 current loss 1.418404, current_train_items 12928.
I0302 18:58:29.515574 22699590365312 run.py:483] Algo bellman_ford step 404 current loss 2.245072, current_train_items 12960.
I0302 18:58:29.534341 22699590365312 run.py:483] Algo bellman_ford step 405 current loss 0.477079, current_train_items 12992.
I0302 18:58:29.549450 22699590365312 run.py:483] Algo bellman_ford step 406 current loss 0.681647, current_train_items 13024.
I0302 18:58:29.572044 22699590365312 run.py:483] Algo bellman_ford step 407 current loss 1.141645, current_train_items 13056.
I0302 18:58:29.602246 22699590365312 run.py:483] Algo bellman_ford step 408 current loss 1.220407, current_train_items 13088.
I0302 18:58:29.634405 22699590365312 run.py:483] Algo bellman_ford step 409 current loss 1.553449, current_train_items 13120.
I0302 18:58:29.652608 22699590365312 run.py:483] Algo bellman_ford step 410 current loss 0.423029, current_train_items 13152.
I0302 18:58:29.668388 22699590365312 run.py:483] Algo bellman_ford step 411 current loss 0.618209, current_train_items 13184.
I0302 18:58:29.691036 22699590365312 run.py:483] Algo bellman_ford step 412 current loss 1.002591, current_train_items 13216.
I0302 18:58:29.720450 22699590365312 run.py:483] Algo bellman_ford step 413 current loss 1.099985, current_train_items 13248.
I0302 18:58:29.752844 22699590365312 run.py:483] Algo bellman_ford step 414 current loss 1.311709, current_train_items 13280.
I0302 18:58:29.771117 22699590365312 run.py:483] Algo bellman_ford step 415 current loss 0.497216, current_train_items 13312.
I0302 18:58:29.786806 22699590365312 run.py:483] Algo bellman_ford step 416 current loss 0.627349, current_train_items 13344.
I0302 18:58:29.810017 22699590365312 run.py:483] Algo bellman_ford step 417 current loss 0.948862, current_train_items 13376.
I0302 18:58:29.840785 22699590365312 run.py:483] Algo bellman_ford step 418 current loss 1.377199, current_train_items 13408.
I0302 18:58:29.872117 22699590365312 run.py:483] Algo bellman_ford step 419 current loss 1.620365, current_train_items 13440.
I0302 18:58:29.890135 22699590365312 run.py:483] Algo bellman_ford step 420 current loss 0.407166, current_train_items 13472.
I0302 18:58:29.906001 22699590365312 run.py:483] Algo bellman_ford step 421 current loss 0.586453, current_train_items 13504.
I0302 18:58:29.929366 22699590365312 run.py:483] Algo bellman_ford step 422 current loss 1.211819, current_train_items 13536.
I0302 18:58:29.957688 22699590365312 run.py:483] Algo bellman_ford step 423 current loss 1.202297, current_train_items 13568.
I0302 18:58:29.989043 22699590365312 run.py:483] Algo bellman_ford step 424 current loss 3.188807, current_train_items 13600.
I0302 18:58:30.007177 22699590365312 run.py:483] Algo bellman_ford step 425 current loss 0.438067, current_train_items 13632.
I0302 18:58:30.022772 22699590365312 run.py:483] Algo bellman_ford step 426 current loss 0.508503, current_train_items 13664.
I0302 18:58:30.044993 22699590365312 run.py:483] Algo bellman_ford step 427 current loss 0.994440, current_train_items 13696.
I0302 18:58:30.073479 22699590365312 run.py:483] Algo bellman_ford step 428 current loss 1.271282, current_train_items 13728.
I0302 18:58:30.104855 22699590365312 run.py:483] Algo bellman_ford step 429 current loss 1.407059, current_train_items 13760.
I0302 18:58:30.123102 22699590365312 run.py:483] Algo bellman_ford step 430 current loss 0.489854, current_train_items 13792.
I0302 18:58:30.138876 22699590365312 run.py:483] Algo bellman_ford step 431 current loss 0.700020, current_train_items 13824.
I0302 18:58:30.162050 22699590365312 run.py:483] Algo bellman_ford step 432 current loss 1.123395, current_train_items 13856.
I0302 18:58:30.190207 22699590365312 run.py:483] Algo bellman_ford step 433 current loss 1.097616, current_train_items 13888.
I0302 18:58:30.221373 22699590365312 run.py:483] Algo bellman_ford step 434 current loss 1.425826, current_train_items 13920.
I0302 18:58:30.239715 22699590365312 run.py:483] Algo bellman_ford step 435 current loss 0.367581, current_train_items 13952.
I0302 18:58:30.255908 22699590365312 run.py:483] Algo bellman_ford step 436 current loss 0.683227, current_train_items 13984.
I0302 18:58:30.278064 22699590365312 run.py:483] Algo bellman_ford step 437 current loss 1.023590, current_train_items 14016.
I0302 18:58:30.306730 22699590365312 run.py:483] Algo bellman_ford step 438 current loss 1.034271, current_train_items 14048.
I0302 18:58:30.338176 22699590365312 run.py:483] Algo bellman_ford step 439 current loss 1.240187, current_train_items 14080.
I0302 18:58:30.356049 22699590365312 run.py:483] Algo bellman_ford step 440 current loss 0.384972, current_train_items 14112.
I0302 18:58:30.371753 22699590365312 run.py:483] Algo bellman_ford step 441 current loss 0.689669, current_train_items 14144.
I0302 18:58:30.393796 22699590365312 run.py:483] Algo bellman_ford step 442 current loss 1.339399, current_train_items 14176.
I0302 18:58:30.422564 22699590365312 run.py:483] Algo bellman_ford step 443 current loss 1.249380, current_train_items 14208.
I0302 18:58:30.453109 22699590365312 run.py:483] Algo bellman_ford step 444 current loss 1.247504, current_train_items 14240.
I0302 18:58:30.471177 22699590365312 run.py:483] Algo bellman_ford step 445 current loss 0.380187, current_train_items 14272.
I0302 18:58:30.487937 22699590365312 run.py:483] Algo bellman_ford step 446 current loss 1.043172, current_train_items 14304.
I0302 18:58:30.511128 22699590365312 run.py:483] Algo bellman_ford step 447 current loss 1.071324, current_train_items 14336.
I0302 18:58:30.538388 22699590365312 run.py:483] Algo bellman_ford step 448 current loss 1.338595, current_train_items 14368.
I0302 18:58:30.567920 22699590365312 run.py:483] Algo bellman_ford step 449 current loss 1.318215, current_train_items 14400.
I0302 18:58:30.586583 22699590365312 run.py:483] Algo bellman_ford step 450 current loss 0.577715, current_train_items 14432.
I0302 18:58:30.594717 22699590365312 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.8408203125, 'score': 0.8408203125, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0302 18:58:30.594822 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.851, current avg val score is 0.841, val scores are: bellman_ford: 0.841
I0302 18:58:30.611502 22699590365312 run.py:483] Algo bellman_ford step 451 current loss 0.799459, current_train_items 14464.
I0302 18:58:30.634607 22699590365312 run.py:483] Algo bellman_ford step 452 current loss 0.997768, current_train_items 14496.
I0302 18:58:30.665130 22699590365312 run.py:483] Algo bellman_ford step 453 current loss 1.250431, current_train_items 14528.
I0302 18:58:30.697119 22699590365312 run.py:483] Algo bellman_ford step 454 current loss 1.321858, current_train_items 14560.
I0302 18:58:30.715926 22699590365312 run.py:483] Algo bellman_ford step 455 current loss 0.485325, current_train_items 14592.
I0302 18:58:30.731878 22699590365312 run.py:483] Algo bellman_ford step 456 current loss 0.659523, current_train_items 14624.
I0302 18:58:30.754280 22699590365312 run.py:483] Algo bellman_ford step 457 current loss 1.026552, current_train_items 14656.
I0302 18:58:30.783287 22699590365312 run.py:483] Algo bellman_ford step 458 current loss 1.086222, current_train_items 14688.
I0302 18:58:30.813843 22699590365312 run.py:483] Algo bellman_ford step 459 current loss 1.325376, current_train_items 14720.
I0302 18:58:30.832403 22699590365312 run.py:483] Algo bellman_ford step 460 current loss 0.351290, current_train_items 14752.
I0302 18:58:30.848638 22699590365312 run.py:483] Algo bellman_ford step 461 current loss 0.839950, current_train_items 14784.
I0302 18:58:30.871821 22699590365312 run.py:483] Algo bellman_ford step 462 current loss 1.022405, current_train_items 14816.
I0302 18:58:30.901149 22699590365312 run.py:483] Algo bellman_ford step 463 current loss 1.261895, current_train_items 14848.
I0302 18:58:30.934089 22699590365312 run.py:483] Algo bellman_ford step 464 current loss 1.604220, current_train_items 14880.
I0302 18:58:30.952595 22699590365312 run.py:483] Algo bellman_ford step 465 current loss 0.382403, current_train_items 14912.
I0302 18:58:30.968142 22699590365312 run.py:483] Algo bellman_ford step 466 current loss 0.598611, current_train_items 14944.
I0302 18:58:30.991394 22699590365312 run.py:483] Algo bellman_ford step 467 current loss 1.069135, current_train_items 14976.
I0302 18:58:31.019036 22699590365312 run.py:483] Algo bellman_ford step 468 current loss 1.208943, current_train_items 15008.
I0302 18:58:31.051705 22699590365312 run.py:483] Algo bellman_ford step 469 current loss 1.318509, current_train_items 15040.
I0302 18:58:31.070147 22699590365312 run.py:483] Algo bellman_ford step 470 current loss 0.376127, current_train_items 15072.
I0302 18:58:31.086090 22699590365312 run.py:483] Algo bellman_ford step 471 current loss 0.755656, current_train_items 15104.
I0302 18:58:31.109413 22699590365312 run.py:483] Algo bellman_ford step 472 current loss 1.156493, current_train_items 15136.
I0302 18:58:31.138650 22699590365312 run.py:483] Algo bellman_ford step 473 current loss 1.213544, current_train_items 15168.
I0302 18:58:31.171088 22699590365312 run.py:483] Algo bellman_ford step 474 current loss 1.476260, current_train_items 15200.
I0302 18:58:31.190194 22699590365312 run.py:483] Algo bellman_ford step 475 current loss 0.452662, current_train_items 15232.
I0302 18:58:31.205893 22699590365312 run.py:483] Algo bellman_ford step 476 current loss 0.673601, current_train_items 15264.
I0302 18:58:31.228949 22699590365312 run.py:483] Algo bellman_ford step 477 current loss 0.913729, current_train_items 15296.
I0302 18:58:31.256906 22699590365312 run.py:483] Algo bellman_ford step 478 current loss 1.154561, current_train_items 15328.
I0302 18:58:31.289249 22699590365312 run.py:483] Algo bellman_ford step 479 current loss 1.686310, current_train_items 15360.
I0302 18:58:31.307727 22699590365312 run.py:483] Algo bellman_ford step 480 current loss 0.419429, current_train_items 15392.
I0302 18:58:31.323611 22699590365312 run.py:483] Algo bellman_ford step 481 current loss 0.702549, current_train_items 15424.
I0302 18:58:31.345956 22699590365312 run.py:483] Algo bellman_ford step 482 current loss 0.834129, current_train_items 15456.
I0302 18:58:31.375141 22699590365312 run.py:483] Algo bellman_ford step 483 current loss 1.026377, current_train_items 15488.
I0302 18:58:31.406978 22699590365312 run.py:483] Algo bellman_ford step 484 current loss 1.495932, current_train_items 15520.
I0302 18:58:31.425552 22699590365312 run.py:483] Algo bellman_ford step 485 current loss 0.396229, current_train_items 15552.
I0302 18:58:31.441628 22699590365312 run.py:483] Algo bellman_ford step 486 current loss 0.762549, current_train_items 15584.
I0302 18:58:31.464797 22699590365312 run.py:483] Algo bellman_ford step 487 current loss 0.984482, current_train_items 15616.
I0302 18:58:31.493664 22699590365312 run.py:483] Algo bellman_ford step 488 current loss 1.066643, current_train_items 15648.
I0302 18:58:31.524684 22699590365312 run.py:483] Algo bellman_ford step 489 current loss 1.329850, current_train_items 15680.
I0302 18:58:31.543368 22699590365312 run.py:483] Algo bellman_ford step 490 current loss 0.381980, current_train_items 15712.
I0302 18:58:31.559484 22699590365312 run.py:483] Algo bellman_ford step 491 current loss 0.673176, current_train_items 15744.
I0302 18:58:31.581959 22699590365312 run.py:483] Algo bellman_ford step 492 current loss 0.968974, current_train_items 15776.
I0302 18:58:31.610605 22699590365312 run.py:483] Algo bellman_ford step 493 current loss 1.126239, current_train_items 15808.
I0302 18:58:31.643170 22699590365312 run.py:483] Algo bellman_ford step 494 current loss 1.524289, current_train_items 15840.
I0302 18:58:31.661622 22699590365312 run.py:483] Algo bellman_ford step 495 current loss 0.386266, current_train_items 15872.
I0302 18:58:31.677355 22699590365312 run.py:483] Algo bellman_ford step 496 current loss 0.618478, current_train_items 15904.
I0302 18:58:31.701452 22699590365312 run.py:483] Algo bellman_ford step 497 current loss 1.113838, current_train_items 15936.
I0302 18:58:31.731342 22699590365312 run.py:483] Algo bellman_ford step 498 current loss 1.156370, current_train_items 15968.
I0302 18:58:31.762761 22699590365312 run.py:483] Algo bellman_ford step 499 current loss 1.203836, current_train_items 16000.
I0302 18:58:31.781313 22699590365312 run.py:483] Algo bellman_ford step 500 current loss 0.411634, current_train_items 16032.
I0302 18:58:31.789112 22699590365312 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.8408203125, 'score': 0.8408203125, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0302 18:58:31.789223 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.851, current avg val score is 0.841, val scores are: bellman_ford: 0.841
I0302 18:58:31.805282 22699590365312 run.py:483] Algo bellman_ford step 501 current loss 0.557277, current_train_items 16064.
I0302 18:58:31.828729 22699590365312 run.py:483] Algo bellman_ford step 502 current loss 1.068832, current_train_items 16096.
I0302 18:58:31.859594 22699590365312 run.py:483] Algo bellman_ford step 503 current loss 1.231321, current_train_items 16128.
I0302 18:58:31.893941 22699590365312 run.py:483] Algo bellman_ford step 504 current loss 1.466119, current_train_items 16160.
I0302 18:58:31.912685 22699590365312 run.py:483] Algo bellman_ford step 505 current loss 0.352546, current_train_items 16192.
I0302 18:58:31.927791 22699590365312 run.py:483] Algo bellman_ford step 506 current loss 0.612935, current_train_items 16224.
I0302 18:58:31.949912 22699590365312 run.py:483] Algo bellman_ford step 507 current loss 0.976752, current_train_items 16256.
I0302 18:58:31.979730 22699590365312 run.py:483] Algo bellman_ford step 508 current loss 1.342867, current_train_items 16288.
I0302 18:58:32.012309 22699590365312 run.py:483] Algo bellman_ford step 509 current loss 1.448617, current_train_items 16320.
I0302 18:58:32.030823 22699590365312 run.py:483] Algo bellman_ford step 510 current loss 0.408149, current_train_items 16352.
I0302 18:58:32.047007 22699590365312 run.py:483] Algo bellman_ford step 511 current loss 0.694661, current_train_items 16384.
I0302 18:58:32.070005 22699590365312 run.py:483] Algo bellman_ford step 512 current loss 0.934274, current_train_items 16416.
I0302 18:58:32.099689 22699590365312 run.py:483] Algo bellman_ford step 513 current loss 1.160962, current_train_items 16448.
I0302 18:58:32.131983 22699590365312 run.py:483] Algo bellman_ford step 514 current loss 1.380546, current_train_items 16480.
I0302 18:58:32.150501 22699590365312 run.py:483] Algo bellman_ford step 515 current loss 0.444334, current_train_items 16512.
I0302 18:58:32.166124 22699590365312 run.py:483] Algo bellman_ford step 516 current loss 0.628080, current_train_items 16544.
I0302 18:58:32.189714 22699590365312 run.py:483] Algo bellman_ford step 517 current loss 0.872340, current_train_items 16576.
I0302 18:58:32.220587 22699590365312 run.py:483] Algo bellman_ford step 518 current loss 1.082136, current_train_items 16608.
I0302 18:58:32.252323 22699590365312 run.py:483] Algo bellman_ford step 519 current loss 1.347319, current_train_items 16640.
I0302 18:58:32.270793 22699590365312 run.py:483] Algo bellman_ford step 520 current loss 0.407160, current_train_items 16672.
I0302 18:58:32.287000 22699590365312 run.py:483] Algo bellman_ford step 521 current loss 0.635196, current_train_items 16704.
I0302 18:58:32.309899 22699590365312 run.py:483] Algo bellman_ford step 522 current loss 0.821998, current_train_items 16736.
I0302 18:58:32.339796 22699590365312 run.py:483] Algo bellman_ford step 523 current loss 1.087709, current_train_items 16768.
I0302 18:58:32.370821 22699590365312 run.py:483] Algo bellman_ford step 524 current loss 1.362521, current_train_items 16800.
I0302 18:58:32.389030 22699590365312 run.py:483] Algo bellman_ford step 525 current loss 0.341493, current_train_items 16832.
I0302 18:58:32.405318 22699590365312 run.py:483] Algo bellman_ford step 526 current loss 0.744925, current_train_items 16864.
I0302 18:58:32.428470 22699590365312 run.py:483] Algo bellman_ford step 527 current loss 0.955950, current_train_items 16896.
I0302 18:58:32.456775 22699590365312 run.py:483] Algo bellman_ford step 528 current loss 0.956952, current_train_items 16928.
I0302 18:58:32.489013 22699590365312 run.py:483] Algo bellman_ford step 529 current loss 1.446603, current_train_items 16960.
I0302 18:58:32.507624 22699590365312 run.py:483] Algo bellman_ford step 530 current loss 0.397171, current_train_items 16992.
I0302 18:58:32.523589 22699590365312 run.py:483] Algo bellman_ford step 531 current loss 0.725627, current_train_items 17024.
I0302 18:58:32.546127 22699590365312 run.py:483] Algo bellman_ford step 532 current loss 0.926507, current_train_items 17056.
I0302 18:58:32.575647 22699590365312 run.py:483] Algo bellman_ford step 533 current loss 1.188227, current_train_items 17088.
I0302 18:58:32.607671 22699590365312 run.py:483] Algo bellman_ford step 534 current loss 1.479026, current_train_items 17120.
I0302 18:58:32.625846 22699590365312 run.py:483] Algo bellman_ford step 535 current loss 0.393853, current_train_items 17152.
I0302 18:58:32.641371 22699590365312 run.py:483] Algo bellman_ford step 536 current loss 0.585250, current_train_items 17184.
I0302 18:58:32.663357 22699590365312 run.py:483] Algo bellman_ford step 537 current loss 0.902213, current_train_items 17216.
I0302 18:58:32.691651 22699590365312 run.py:483] Algo bellman_ford step 538 current loss 0.938485, current_train_items 17248.
I0302 18:58:32.722162 22699590365312 run.py:483] Algo bellman_ford step 539 current loss 1.203582, current_train_items 17280.
I0302 18:58:32.740476 22699590365312 run.py:483] Algo bellman_ford step 540 current loss 0.365760, current_train_items 17312.
I0302 18:58:32.755792 22699590365312 run.py:483] Algo bellman_ford step 541 current loss 0.539667, current_train_items 17344.
I0302 18:58:32.779363 22699590365312 run.py:483] Algo bellman_ford step 542 current loss 1.081092, current_train_items 17376.
I0302 18:58:32.809072 22699590365312 run.py:483] Algo bellman_ford step 543 current loss 1.133229, current_train_items 17408.
I0302 18:58:32.841178 22699590365312 run.py:483] Algo bellman_ford step 544 current loss 1.309594, current_train_items 17440.
I0302 18:58:32.859825 22699590365312 run.py:483] Algo bellman_ford step 545 current loss 0.454074, current_train_items 17472.
I0302 18:58:32.875774 22699590365312 run.py:483] Algo bellman_ford step 546 current loss 0.665976, current_train_items 17504.
I0302 18:58:32.898528 22699590365312 run.py:483] Algo bellman_ford step 547 current loss 0.992575, current_train_items 17536.
I0302 18:58:32.928563 22699590365312 run.py:483] Algo bellman_ford step 548 current loss 1.199128, current_train_items 17568.
I0302 18:58:32.958107 22699590365312 run.py:483] Algo bellman_ford step 549 current loss 1.110458, current_train_items 17600.
I0302 18:58:32.976472 22699590365312 run.py:483] Algo bellman_ford step 550 current loss 0.383433, current_train_items 17632.
I0302 18:58:32.984423 22699590365312 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.85546875, 'score': 0.85546875, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0302 18:58:32.984527 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.851, current avg val score is 0.855, val scores are: bellman_ford: 0.855
I0302 18:58:33.015065 22699590365312 run.py:483] Algo bellman_ford step 551 current loss 0.601111, current_train_items 17664.
I0302 18:58:33.038145 22699590365312 run.py:483] Algo bellman_ford step 552 current loss 0.886061, current_train_items 17696.
I0302 18:58:33.069789 22699590365312 run.py:483] Algo bellman_ford step 553 current loss 1.175720, current_train_items 17728.
I0302 18:58:33.101761 22699590365312 run.py:483] Algo bellman_ford step 554 current loss 1.328965, current_train_items 17760.
I0302 18:58:33.121097 22699590365312 run.py:483] Algo bellman_ford step 555 current loss 0.362059, current_train_items 17792.
I0302 18:58:33.136918 22699590365312 run.py:483] Algo bellman_ford step 556 current loss 0.602231, current_train_items 17824.
I0302 18:58:33.160628 22699590365312 run.py:483] Algo bellman_ford step 557 current loss 1.012578, current_train_items 17856.
I0302 18:58:33.189707 22699590365312 run.py:483] Algo bellman_ford step 558 current loss 1.184488, current_train_items 17888.
I0302 18:58:33.223240 22699590365312 run.py:483] Algo bellman_ford step 559 current loss 1.382065, current_train_items 17920.
I0302 18:58:33.242434 22699590365312 run.py:483] Algo bellman_ford step 560 current loss 0.469556, current_train_items 17952.
I0302 18:58:33.258653 22699590365312 run.py:483] Algo bellman_ford step 561 current loss 0.556017, current_train_items 17984.
I0302 18:58:33.281902 22699590365312 run.py:483] Algo bellman_ford step 562 current loss 0.875644, current_train_items 18016.
I0302 18:58:33.310803 22699590365312 run.py:483] Algo bellman_ford step 563 current loss 0.966350, current_train_items 18048.
I0302 18:58:33.340461 22699590365312 run.py:483] Algo bellman_ford step 564 current loss 1.128299, current_train_items 18080.
I0302 18:58:33.358922 22699590365312 run.py:483] Algo bellman_ford step 565 current loss 0.356856, current_train_items 18112.
I0302 18:58:33.375078 22699590365312 run.py:483] Algo bellman_ford step 566 current loss 0.680192, current_train_items 18144.
I0302 18:58:33.399308 22699590365312 run.py:483] Algo bellman_ford step 567 current loss 1.083456, current_train_items 18176.
I0302 18:58:33.428641 22699590365312 run.py:483] Algo bellman_ford step 568 current loss 1.004047, current_train_items 18208.
I0302 18:58:33.459684 22699590365312 run.py:483] Algo bellman_ford step 569 current loss 1.202695, current_train_items 18240.
I0302 18:58:33.478499 22699590365312 run.py:483] Algo bellman_ford step 570 current loss 0.358226, current_train_items 18272.
I0302 18:58:33.494232 22699590365312 run.py:483] Algo bellman_ford step 571 current loss 0.675556, current_train_items 18304.
I0302 18:58:33.517598 22699590365312 run.py:483] Algo bellman_ford step 572 current loss 1.036590, current_train_items 18336.
I0302 18:58:33.548015 22699590365312 run.py:483] Algo bellman_ford step 573 current loss 1.205311, current_train_items 18368.
I0302 18:58:33.578336 22699590365312 run.py:483] Algo bellman_ford step 574 current loss 1.216196, current_train_items 18400.
I0302 18:58:33.597138 22699590365312 run.py:483] Algo bellman_ford step 575 current loss 0.407499, current_train_items 18432.
I0302 18:58:33.613001 22699590365312 run.py:483] Algo bellman_ford step 576 current loss 0.736672, current_train_items 18464.
I0302 18:58:33.636102 22699590365312 run.py:483] Algo bellman_ford step 577 current loss 0.930620, current_train_items 18496.
I0302 18:58:33.665714 22699590365312 run.py:483] Algo bellman_ford step 578 current loss 1.041082, current_train_items 18528.
I0302 18:58:33.698163 22699590365312 run.py:483] Algo bellman_ford step 579 current loss 1.355252, current_train_items 18560.
I0302 18:58:33.716730 22699590365312 run.py:483] Algo bellman_ford step 580 current loss 0.387664, current_train_items 18592.
I0302 18:58:33.732692 22699590365312 run.py:483] Algo bellman_ford step 581 current loss 0.626687, current_train_items 18624.
I0302 18:58:33.754451 22699590365312 run.py:483] Algo bellman_ford step 582 current loss 0.835259, current_train_items 18656.
I0302 18:58:33.782515 22699590365312 run.py:483] Algo bellman_ford step 583 current loss 0.797481, current_train_items 18688.
I0302 18:58:33.813745 22699590365312 run.py:483] Algo bellman_ford step 584 current loss 1.265533, current_train_items 18720.
I0302 18:58:33.831955 22699590365312 run.py:483] Algo bellman_ford step 585 current loss 0.346821, current_train_items 18752.
I0302 18:58:33.847998 22699590365312 run.py:483] Algo bellman_ford step 586 current loss 0.687955, current_train_items 18784.
I0302 18:58:33.869631 22699590365312 run.py:483] Algo bellman_ford step 587 current loss 0.705288, current_train_items 18816.
I0302 18:58:33.898369 22699590365312 run.py:483] Algo bellman_ford step 588 current loss 0.974804, current_train_items 18848.
I0302 18:58:33.931478 22699590365312 run.py:483] Algo bellman_ford step 589 current loss 1.355608, current_train_items 18880.
I0302 18:58:33.950338 22699590365312 run.py:483] Algo bellman_ford step 590 current loss 0.388526, current_train_items 18912.
I0302 18:58:33.966360 22699590365312 run.py:483] Algo bellman_ford step 591 current loss 0.695102, current_train_items 18944.
I0302 18:58:33.989443 22699590365312 run.py:483] Algo bellman_ford step 592 current loss 0.989148, current_train_items 18976.
I0302 18:58:34.018680 22699590365312 run.py:483] Algo bellman_ford step 593 current loss 1.105559, current_train_items 19008.
I0302 18:58:34.050085 22699590365312 run.py:483] Algo bellman_ford step 594 current loss 1.152685, current_train_items 19040.
I0302 18:58:34.068440 22699590365312 run.py:483] Algo bellman_ford step 595 current loss 0.349683, current_train_items 19072.
I0302 18:58:34.084476 22699590365312 run.py:483] Algo bellman_ford step 596 current loss 0.652091, current_train_items 19104.
I0302 18:58:34.108378 22699590365312 run.py:483] Algo bellman_ford step 597 current loss 0.914265, current_train_items 19136.
I0302 18:58:34.137762 22699590365312 run.py:483] Algo bellman_ford step 598 current loss 0.981518, current_train_items 19168.
I0302 18:58:34.169392 22699590365312 run.py:483] Algo bellman_ford step 599 current loss 1.137791, current_train_items 19200.
I0302 18:58:34.187924 22699590365312 run.py:483] Algo bellman_ford step 600 current loss 0.409915, current_train_items 19232.
I0302 18:58:34.195993 22699590365312 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.876953125, 'score': 0.876953125, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0302 18:58:34.196099 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.855, current avg val score is 0.877, val scores are: bellman_ford: 0.877
I0302 18:58:34.224344 22699590365312 run.py:483] Algo bellman_ford step 601 current loss 0.517318, current_train_items 19264.
I0302 18:58:34.247900 22699590365312 run.py:483] Algo bellman_ford step 602 current loss 0.821729, current_train_items 19296.
I0302 18:58:34.277185 22699590365312 run.py:483] Algo bellman_ford step 603 current loss 1.013642, current_train_items 19328.
I0302 18:58:34.308705 22699590365312 run.py:483] Algo bellman_ford step 604 current loss 1.210296, current_train_items 19360.
I0302 18:58:34.328182 22699590365312 run.py:483] Algo bellman_ford step 605 current loss 0.377969, current_train_items 19392.
I0302 18:58:34.343823 22699590365312 run.py:483] Algo bellman_ford step 606 current loss 0.638001, current_train_items 19424.
I0302 18:58:34.367498 22699590365312 run.py:483] Algo bellman_ford step 607 current loss 0.966197, current_train_items 19456.
I0302 18:58:34.395950 22699590365312 run.py:483] Algo bellman_ford step 608 current loss 0.989600, current_train_items 19488.
I0302 18:58:34.427804 22699590365312 run.py:483] Algo bellman_ford step 609 current loss 1.168690, current_train_items 19520.
I0302 18:58:34.446472 22699590365312 run.py:483] Algo bellman_ford step 610 current loss 0.320439, current_train_items 19552.
I0302 18:58:34.462680 22699590365312 run.py:483] Algo bellman_ford step 611 current loss 0.584477, current_train_items 19584.
I0302 18:58:34.485972 22699590365312 run.py:483] Algo bellman_ford step 612 current loss 0.827738, current_train_items 19616.
I0302 18:58:34.513242 22699590365312 run.py:483] Algo bellman_ford step 613 current loss 0.895743, current_train_items 19648.
I0302 18:58:34.544803 22699590365312 run.py:483] Algo bellman_ford step 614 current loss 1.423568, current_train_items 19680.
I0302 18:58:34.562881 22699590365312 run.py:483] Algo bellman_ford step 615 current loss 0.271486, current_train_items 19712.
I0302 18:58:34.579228 22699590365312 run.py:483] Algo bellman_ford step 616 current loss 0.607545, current_train_items 19744.
I0302 18:58:34.601539 22699590365312 run.py:483] Algo bellman_ford step 617 current loss 0.894003, current_train_items 19776.
I0302 18:58:34.630152 22699590365312 run.py:483] Algo bellman_ford step 618 current loss 0.958333, current_train_items 19808.
I0302 18:58:34.661834 22699590365312 run.py:483] Algo bellman_ford step 619 current loss 1.105628, current_train_items 19840.
I0302 18:58:34.680514 22699590365312 run.py:483] Algo bellman_ford step 620 current loss 0.398715, current_train_items 19872.
I0302 18:58:34.696899 22699590365312 run.py:483] Algo bellman_ford step 621 current loss 0.749601, current_train_items 19904.
I0302 18:58:34.720247 22699590365312 run.py:483] Algo bellman_ford step 622 current loss 1.052441, current_train_items 19936.
I0302 18:58:34.748897 22699590365312 run.py:483] Algo bellman_ford step 623 current loss 1.240216, current_train_items 19968.
I0302 18:58:34.780390 22699590365312 run.py:483] Algo bellman_ford step 624 current loss 1.321715, current_train_items 20000.
I0302 18:58:34.798723 22699590365312 run.py:483] Algo bellman_ford step 625 current loss 0.430447, current_train_items 20032.
I0302 18:58:34.814483 22699590365312 run.py:483] Algo bellman_ford step 626 current loss 0.567619, current_train_items 20064.
I0302 18:58:34.838051 22699590365312 run.py:483] Algo bellman_ford step 627 current loss 1.007408, current_train_items 20096.
I0302 18:58:34.868207 22699590365312 run.py:483] Algo bellman_ford step 628 current loss 1.298584, current_train_items 20128.
I0302 18:58:34.900241 22699590365312 run.py:483] Algo bellman_ford step 629 current loss 1.278379, current_train_items 20160.
I0302 18:58:34.918994 22699590365312 run.py:483] Algo bellman_ford step 630 current loss 0.346967, current_train_items 20192.
I0302 18:58:34.934756 22699590365312 run.py:483] Algo bellman_ford step 631 current loss 0.637149, current_train_items 20224.
I0302 18:58:34.957327 22699590365312 run.py:483] Algo bellman_ford step 632 current loss 0.969418, current_train_items 20256.
I0302 18:58:34.986255 22699590365312 run.py:483] Algo bellman_ford step 633 current loss 1.095274, current_train_items 20288.
I0302 18:58:35.017570 22699590365312 run.py:483] Algo bellman_ford step 634 current loss 1.236026, current_train_items 20320.
I0302 18:58:35.036035 22699590365312 run.py:483] Algo bellman_ford step 635 current loss 0.400549, current_train_items 20352.
I0302 18:58:35.051941 22699590365312 run.py:483] Algo bellman_ford step 636 current loss 0.634407, current_train_items 20384.
I0302 18:58:35.074579 22699590365312 run.py:483] Algo bellman_ford step 637 current loss 0.978158, current_train_items 20416.
I0302 18:58:35.104767 22699590365312 run.py:483] Algo bellman_ford step 638 current loss 1.056983, current_train_items 20448.
I0302 18:58:35.133589 22699590365312 run.py:483] Algo bellman_ford step 639 current loss 1.130495, current_train_items 20480.
I0302 18:58:35.152187 22699590365312 run.py:483] Algo bellman_ford step 640 current loss 0.442781, current_train_items 20512.
I0302 18:58:35.167950 22699590365312 run.py:483] Algo bellman_ford step 641 current loss 0.673410, current_train_items 20544.
I0302 18:58:35.189943 22699590365312 run.py:483] Algo bellman_ford step 642 current loss 1.002895, current_train_items 20576.
I0302 18:58:35.218693 22699590365312 run.py:483] Algo bellman_ford step 643 current loss 1.053254, current_train_items 20608.
I0302 18:58:35.250428 22699590365312 run.py:483] Algo bellman_ford step 644 current loss 1.276588, current_train_items 20640.
I0302 18:58:35.268428 22699590365312 run.py:483] Algo bellman_ford step 645 current loss 0.405222, current_train_items 20672.
I0302 18:58:35.285371 22699590365312 run.py:483] Algo bellman_ford step 646 current loss 0.851792, current_train_items 20704.
I0302 18:58:35.307635 22699590365312 run.py:483] Algo bellman_ford step 647 current loss 0.828417, current_train_items 20736.
I0302 18:58:35.337727 22699590365312 run.py:483] Algo bellman_ford step 648 current loss 1.174346, current_train_items 20768.
I0302 18:58:35.369810 22699590365312 run.py:483] Algo bellman_ford step 649 current loss 1.221012, current_train_items 20800.
I0302 18:58:35.388122 22699590365312 run.py:483] Algo bellman_ford step 650 current loss 0.478751, current_train_items 20832.
I0302 18:58:35.396524 22699590365312 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.8681640625, 'score': 0.8681640625, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0302 18:58:35.396630 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.877, current avg val score is 0.868, val scores are: bellman_ford: 0.868
I0302 18:58:35.413506 22699590365312 run.py:483] Algo bellman_ford step 651 current loss 0.698734, current_train_items 20864.
I0302 18:58:35.436135 22699590365312 run.py:483] Algo bellman_ford step 652 current loss 0.844546, current_train_items 20896.
I0302 18:58:35.464277 22699590365312 run.py:483] Algo bellman_ford step 653 current loss 0.919417, current_train_items 20928.
I0302 18:58:35.495557 22699590365312 run.py:483] Algo bellman_ford step 654 current loss 1.057520, current_train_items 20960.
I0302 18:58:35.514096 22699590365312 run.py:483] Algo bellman_ford step 655 current loss 0.377464, current_train_items 20992.
I0302 18:58:35.529661 22699590365312 run.py:483] Algo bellman_ford step 656 current loss 0.735322, current_train_items 21024.
I0302 18:58:35.552858 22699590365312 run.py:483] Algo bellman_ford step 657 current loss 0.985843, current_train_items 21056.
I0302 18:58:35.582169 22699590365312 run.py:483] Algo bellman_ford step 658 current loss 1.040513, current_train_items 21088.
I0302 18:58:35.612500 22699590365312 run.py:483] Algo bellman_ford step 659 current loss 1.197387, current_train_items 21120.
I0302 18:58:35.631059 22699590365312 run.py:483] Algo bellman_ford step 660 current loss 0.425361, current_train_items 21152.
I0302 18:58:35.646887 22699590365312 run.py:483] Algo bellman_ford step 661 current loss 0.599253, current_train_items 21184.
I0302 18:58:35.668953 22699590365312 run.py:483] Algo bellman_ford step 662 current loss 0.801815, current_train_items 21216.
I0302 18:58:35.697644 22699590365312 run.py:483] Algo bellman_ford step 663 current loss 1.096867, current_train_items 21248.
I0302 18:58:35.731277 22699590365312 run.py:483] Algo bellman_ford step 664 current loss 1.274664, current_train_items 21280.
I0302 18:58:35.749664 22699590365312 run.py:483] Algo bellman_ford step 665 current loss 0.391455, current_train_items 21312.
I0302 18:58:35.765603 22699590365312 run.py:483] Algo bellman_ford step 666 current loss 0.613271, current_train_items 21344.
I0302 18:58:35.789100 22699590365312 run.py:483] Algo bellman_ford step 667 current loss 1.036400, current_train_items 21376.
I0302 18:58:35.817518 22699590365312 run.py:483] Algo bellman_ford step 668 current loss 1.228363, current_train_items 21408.
I0302 18:58:35.848118 22699590365312 run.py:483] Algo bellman_ford step 669 current loss 1.267103, current_train_items 21440.
I0302 18:58:35.866618 22699590365312 run.py:483] Algo bellman_ford step 670 current loss 0.311170, current_train_items 21472.
I0302 18:58:35.882319 22699590365312 run.py:483] Algo bellman_ford step 671 current loss 0.552395, current_train_items 21504.
I0302 18:58:35.905670 22699590365312 run.py:483] Algo bellman_ford step 672 current loss 0.868460, current_train_items 21536.
I0302 18:58:35.934116 22699590365312 run.py:483] Algo bellman_ford step 673 current loss 1.092104, current_train_items 21568.
I0302 18:58:35.966437 22699590365312 run.py:483] Algo bellman_ford step 674 current loss 1.121941, current_train_items 21600.
I0302 18:58:35.985268 22699590365312 run.py:483] Algo bellman_ford step 675 current loss 0.362817, current_train_items 21632.
I0302 18:58:36.000759 22699590365312 run.py:483] Algo bellman_ford step 676 current loss 0.525077, current_train_items 21664.
I0302 18:58:36.022922 22699590365312 run.py:483] Algo bellman_ford step 677 current loss 0.914619, current_train_items 21696.
I0302 18:58:36.052231 22699590365312 run.py:483] Algo bellman_ford step 678 current loss 0.957755, current_train_items 21728.
I0302 18:58:36.084431 22699590365312 run.py:483] Algo bellman_ford step 679 current loss 1.107030, current_train_items 21760.
I0302 18:58:36.102757 22699590365312 run.py:483] Algo bellman_ford step 680 current loss 0.398734, current_train_items 21792.
I0302 18:58:36.118690 22699590365312 run.py:483] Algo bellman_ford step 681 current loss 0.657973, current_train_items 21824.
I0302 18:58:36.141514 22699590365312 run.py:483] Algo bellman_ford step 682 current loss 0.937837, current_train_items 21856.
I0302 18:58:36.169610 22699590365312 run.py:483] Algo bellman_ford step 683 current loss 1.010436, current_train_items 21888.
I0302 18:58:36.201798 22699590365312 run.py:483] Algo bellman_ford step 684 current loss 1.306773, current_train_items 21920.
I0302 18:58:36.220599 22699590365312 run.py:483] Algo bellman_ford step 685 current loss 0.417200, current_train_items 21952.
I0302 18:58:36.236430 22699590365312 run.py:483] Algo bellman_ford step 686 current loss 0.623280, current_train_items 21984.
I0302 18:58:36.259196 22699590365312 run.py:483] Algo bellman_ford step 687 current loss 0.951496, current_train_items 22016.
I0302 18:58:36.286526 22699590365312 run.py:483] Algo bellman_ford step 688 current loss 0.933095, current_train_items 22048.
I0302 18:58:36.319560 22699590365312 run.py:483] Algo bellman_ford step 689 current loss 1.407727, current_train_items 22080.
I0302 18:58:36.337898 22699590365312 run.py:483] Algo bellman_ford step 690 current loss 0.421265, current_train_items 22112.
I0302 18:58:36.354277 22699590365312 run.py:483] Algo bellman_ford step 691 current loss 0.748354, current_train_items 22144.
I0302 18:58:36.377504 22699590365312 run.py:483] Algo bellman_ford step 692 current loss 0.954389, current_train_items 22176.
I0302 18:58:36.405510 22699590365312 run.py:483] Algo bellman_ford step 693 current loss 1.324600, current_train_items 22208.
I0302 18:58:36.438289 22699590365312 run.py:483] Algo bellman_ford step 694 current loss 1.289559, current_train_items 22240.
I0302 18:58:36.456649 22699590365312 run.py:483] Algo bellman_ford step 695 current loss 0.416334, current_train_items 22272.
I0302 18:58:36.472468 22699590365312 run.py:483] Algo bellman_ford step 696 current loss 0.629531, current_train_items 22304.
I0302 18:58:36.493825 22699590365312 run.py:483] Algo bellman_ford step 697 current loss 1.118195, current_train_items 22336.
I0302 18:58:36.522603 22699590365312 run.py:483] Algo bellman_ford step 698 current loss 1.100738, current_train_items 22368.
I0302 18:58:36.555714 22699590365312 run.py:483] Algo bellman_ford step 699 current loss 1.471003, current_train_items 22400.
I0302 18:58:36.574219 22699590365312 run.py:483] Algo bellman_ford step 700 current loss 0.366031, current_train_items 22432.
I0302 18:58:36.581722 22699590365312 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.8701171875, 'score': 0.8701171875, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0302 18:58:36.581829 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.877, current avg val score is 0.870, val scores are: bellman_ford: 0.870
I0302 18:58:36.597982 22699590365312 run.py:483] Algo bellman_ford step 701 current loss 0.603862, current_train_items 22464.
I0302 18:58:36.621175 22699590365312 run.py:483] Algo bellman_ford step 702 current loss 1.055446, current_train_items 22496.
I0302 18:58:36.649195 22699590365312 run.py:483] Algo bellman_ford step 703 current loss 1.054236, current_train_items 22528.
I0302 18:58:36.679972 22699590365312 run.py:483] Algo bellman_ford step 704 current loss 1.286098, current_train_items 22560.
I0302 18:58:36.698643 22699590365312 run.py:483] Algo bellman_ford step 705 current loss 0.386681, current_train_items 22592.
I0302 18:58:36.714142 22699590365312 run.py:483] Algo bellman_ford step 706 current loss 0.662430, current_train_items 22624.
I0302 18:58:36.736812 22699590365312 run.py:483] Algo bellman_ford step 707 current loss 0.800634, current_train_items 22656.
I0302 18:58:36.765512 22699590365312 run.py:483] Algo bellman_ford step 708 current loss 0.930497, current_train_items 22688.
I0302 18:58:36.797881 22699590365312 run.py:483] Algo bellman_ford step 709 current loss 1.188843, current_train_items 22720.
I0302 18:58:36.816128 22699590365312 run.py:483] Algo bellman_ford step 710 current loss 0.388874, current_train_items 22752.
I0302 18:58:36.832121 22699590365312 run.py:483] Algo bellman_ford step 711 current loss 0.595255, current_train_items 22784.
I0302 18:58:36.855674 22699590365312 run.py:483] Algo bellman_ford step 712 current loss 0.987353, current_train_items 22816.
I0302 18:58:36.885503 22699590365312 run.py:483] Algo bellman_ford step 713 current loss 1.136953, current_train_items 22848.
I0302 18:58:36.914921 22699590365312 run.py:483] Algo bellman_ford step 714 current loss 1.254997, current_train_items 22880.
I0302 18:58:36.933090 22699590365312 run.py:483] Algo bellman_ford step 715 current loss 0.320007, current_train_items 22912.
I0302 18:58:36.949379 22699590365312 run.py:483] Algo bellman_ford step 716 current loss 0.817870, current_train_items 22944.
I0302 18:58:36.973470 22699590365312 run.py:483] Algo bellman_ford step 717 current loss 0.950892, current_train_items 22976.
I0302 18:58:37.001465 22699590365312 run.py:483] Algo bellman_ford step 718 current loss 0.864680, current_train_items 23008.
I0302 18:58:37.033125 22699590365312 run.py:483] Algo bellman_ford step 719 current loss 1.120356, current_train_items 23040.
I0302 18:58:37.051781 22699590365312 run.py:483] Algo bellman_ford step 720 current loss 0.477119, current_train_items 23072.
I0302 18:58:37.068092 22699590365312 run.py:483] Algo bellman_ford step 721 current loss 0.646406, current_train_items 23104.
I0302 18:58:37.090605 22699590365312 run.py:483] Algo bellman_ford step 722 current loss 0.752848, current_train_items 23136.
I0302 18:58:37.118674 22699590365312 run.py:483] Algo bellman_ford step 723 current loss 0.938740, current_train_items 23168.
I0302 18:58:37.149446 22699590365312 run.py:483] Algo bellman_ford step 724 current loss 1.117382, current_train_items 23200.
I0302 18:58:37.167572 22699590365312 run.py:483] Algo bellman_ford step 725 current loss 0.328924, current_train_items 23232.
I0302 18:58:37.184325 22699590365312 run.py:483] Algo bellman_ford step 726 current loss 0.619895, current_train_items 23264.
I0302 18:58:37.208017 22699590365312 run.py:483] Algo bellman_ford step 727 current loss 1.009317, current_train_items 23296.
I0302 18:58:37.236300 22699590365312 run.py:483] Algo bellman_ford step 728 current loss 0.939191, current_train_items 23328.
I0302 18:58:37.267355 22699590365312 run.py:483] Algo bellman_ford step 729 current loss 1.196024, current_train_items 23360.
I0302 18:58:37.285632 22699590365312 run.py:483] Algo bellman_ford step 730 current loss 0.311395, current_train_items 23392.
I0302 18:58:37.301602 22699590365312 run.py:483] Algo bellman_ford step 731 current loss 0.676209, current_train_items 23424.
I0302 18:58:37.324421 22699590365312 run.py:483] Algo bellman_ford step 732 current loss 0.842718, current_train_items 23456.
I0302 18:58:37.353010 22699590365312 run.py:483] Algo bellman_ford step 733 current loss 0.946918, current_train_items 23488.
I0302 18:58:37.385337 22699590365312 run.py:483] Algo bellman_ford step 734 current loss 1.108528, current_train_items 23520.
I0302 18:58:37.403546 22699590365312 run.py:483] Algo bellman_ford step 735 current loss 0.369037, current_train_items 23552.
I0302 18:58:37.419756 22699590365312 run.py:483] Algo bellman_ford step 736 current loss 0.554007, current_train_items 23584.
I0302 18:58:37.442609 22699590365312 run.py:483] Algo bellman_ford step 737 current loss 0.871382, current_train_items 23616.
I0302 18:58:37.471371 22699590365312 run.py:483] Algo bellman_ford step 738 current loss 1.010915, current_train_items 23648.
I0302 18:58:37.501008 22699590365312 run.py:483] Algo bellman_ford step 739 current loss 0.995226, current_train_items 23680.
I0302 18:58:37.519262 22699590365312 run.py:483] Algo bellman_ford step 740 current loss 0.495381, current_train_items 23712.
I0302 18:58:37.535325 22699590365312 run.py:483] Algo bellman_ford step 741 current loss 0.593140, current_train_items 23744.
I0302 18:58:37.557580 22699590365312 run.py:483] Algo bellman_ford step 742 current loss 0.770842, current_train_items 23776.
I0302 18:58:37.585413 22699590365312 run.py:483] Algo bellman_ford step 743 current loss 0.841938, current_train_items 23808.
I0302 18:58:37.618507 22699590365312 run.py:483] Algo bellman_ford step 744 current loss 1.220637, current_train_items 23840.
I0302 18:58:37.636772 22699590365312 run.py:483] Algo bellman_ford step 745 current loss 0.351180, current_train_items 23872.
I0302 18:58:37.652648 22699590365312 run.py:483] Algo bellman_ford step 746 current loss 0.659135, current_train_items 23904.
I0302 18:58:37.675048 22699590365312 run.py:483] Algo bellman_ford step 747 current loss 0.908728, current_train_items 23936.
I0302 18:58:37.704411 22699590365312 run.py:483] Algo bellman_ford step 748 current loss 1.209045, current_train_items 23968.
I0302 18:58:37.735421 22699590365312 run.py:483] Algo bellman_ford step 749 current loss 1.170032, current_train_items 24000.
I0302 18:58:37.753226 22699590365312 run.py:483] Algo bellman_ford step 750 current loss 0.398036, current_train_items 24032.
I0302 18:58:37.761381 22699590365312 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.859375, 'score': 0.859375, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0302 18:58:37.761485 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.877, current avg val score is 0.859, val scores are: bellman_ford: 0.859
I0302 18:58:37.778570 22699590365312 run.py:483] Algo bellman_ford step 751 current loss 0.657429, current_train_items 24064.
I0302 18:58:37.801395 22699590365312 run.py:483] Algo bellman_ford step 752 current loss 0.874000, current_train_items 24096.
I0302 18:58:37.830383 22699590365312 run.py:483] Algo bellman_ford step 753 current loss 1.087363, current_train_items 24128.
I0302 18:58:37.863431 22699590365312 run.py:483] Algo bellman_ford step 754 current loss 1.212963, current_train_items 24160.
I0302 18:58:37.882556 22699590365312 run.py:483] Algo bellman_ford step 755 current loss 0.374586, current_train_items 24192.
I0302 18:58:37.897895 22699590365312 run.py:483] Algo bellman_ford step 756 current loss 0.571694, current_train_items 24224.
I0302 18:58:37.921953 22699590365312 run.py:483] Algo bellman_ford step 757 current loss 0.850663, current_train_items 24256.
I0302 18:58:37.950861 22699590365312 run.py:483] Algo bellman_ford step 758 current loss 0.920216, current_train_items 24288.
I0302 18:58:37.982113 22699590365312 run.py:483] Algo bellman_ford step 759 current loss 1.098842, current_train_items 24320.
I0302 18:58:38.000909 22699590365312 run.py:483] Algo bellman_ford step 760 current loss 0.320681, current_train_items 24352.
I0302 18:58:38.016756 22699590365312 run.py:483] Algo bellman_ford step 761 current loss 0.709965, current_train_items 24384.
I0302 18:58:38.039713 22699590365312 run.py:483] Algo bellman_ford step 762 current loss 0.932327, current_train_items 24416.
I0302 18:58:38.068086 22699590365312 run.py:483] Algo bellman_ford step 763 current loss 0.870675, current_train_items 24448.
I0302 18:58:38.100466 22699590365312 run.py:483] Algo bellman_ford step 764 current loss 1.090462, current_train_items 24480.
I0302 18:58:38.119051 22699590365312 run.py:483] Algo bellman_ford step 765 current loss 0.391782, current_train_items 24512.
I0302 18:58:38.135456 22699590365312 run.py:483] Algo bellman_ford step 766 current loss 0.668673, current_train_items 24544.
I0302 18:58:38.158643 22699590365312 run.py:483] Algo bellman_ford step 767 current loss 0.904748, current_train_items 24576.
I0302 18:58:38.187981 22699590365312 run.py:483] Algo bellman_ford step 768 current loss 1.049303, current_train_items 24608.
I0302 18:58:38.220395 22699590365312 run.py:483] Algo bellman_ford step 769 current loss 0.990213, current_train_items 24640.
I0302 18:58:38.239413 22699590365312 run.py:483] Algo bellman_ford step 770 current loss 0.328662, current_train_items 24672.
I0302 18:58:38.255514 22699590365312 run.py:483] Algo bellman_ford step 771 current loss 0.718431, current_train_items 24704.
I0302 18:58:38.277924 22699590365312 run.py:483] Algo bellman_ford step 772 current loss 0.964179, current_train_items 24736.
I0302 18:58:38.306553 22699590365312 run.py:483] Algo bellman_ford step 773 current loss 0.876447, current_train_items 24768.
I0302 18:58:38.339083 22699590365312 run.py:483] Algo bellman_ford step 774 current loss 1.222491, current_train_items 24800.
I0302 18:58:38.358002 22699590365312 run.py:483] Algo bellman_ford step 775 current loss 0.312716, current_train_items 24832.
I0302 18:58:38.374611 22699590365312 run.py:483] Algo bellman_ford step 776 current loss 0.645695, current_train_items 24864.
I0302 18:58:38.397748 22699590365312 run.py:483] Algo bellman_ford step 777 current loss 0.932802, current_train_items 24896.
I0302 18:58:38.426098 22699590365312 run.py:483] Algo bellman_ford step 778 current loss 0.895164, current_train_items 24928.
I0302 18:58:38.457163 22699590365312 run.py:483] Algo bellman_ford step 779 current loss 0.981507, current_train_items 24960.
I0302 18:58:38.475765 22699590365312 run.py:483] Algo bellman_ford step 780 current loss 0.426921, current_train_items 24992.
I0302 18:58:38.491880 22699590365312 run.py:483] Algo bellman_ford step 781 current loss 0.665959, current_train_items 25024.
I0302 18:58:38.514508 22699590365312 run.py:483] Algo bellman_ford step 782 current loss 0.748290, current_train_items 25056.
I0302 18:58:38.543330 22699590365312 run.py:483] Algo bellman_ford step 783 current loss 0.913414, current_train_items 25088.
I0302 18:58:38.575887 22699590365312 run.py:483] Algo bellman_ford step 784 current loss 1.117120, current_train_items 25120.
I0302 18:58:38.594212 22699590365312 run.py:483] Algo bellman_ford step 785 current loss 0.366498, current_train_items 25152.
I0302 18:58:38.610517 22699590365312 run.py:483] Algo bellman_ford step 786 current loss 0.762241, current_train_items 25184.
I0302 18:58:38.632968 22699590365312 run.py:483] Algo bellman_ford step 787 current loss 0.835575, current_train_items 25216.
I0302 18:58:38.662637 22699590365312 run.py:483] Algo bellman_ford step 788 current loss 0.988995, current_train_items 25248.
I0302 18:58:38.692283 22699590365312 run.py:483] Algo bellman_ford step 789 current loss 1.115513, current_train_items 25280.
I0302 18:58:38.711308 22699590365312 run.py:483] Algo bellman_ford step 790 current loss 0.339353, current_train_items 25312.
I0302 18:58:38.727733 22699590365312 run.py:483] Algo bellman_ford step 791 current loss 0.723828, current_train_items 25344.
I0302 18:58:38.749146 22699590365312 run.py:483] Algo bellman_ford step 792 current loss 0.770438, current_train_items 25376.
I0302 18:58:38.778318 22699590365312 run.py:483] Algo bellman_ford step 793 current loss 1.003017, current_train_items 25408.
I0302 18:58:38.808505 22699590365312 run.py:483] Algo bellman_ford step 794 current loss 1.003349, current_train_items 25440.
I0302 18:58:38.826775 22699590365312 run.py:483] Algo bellman_ford step 795 current loss 0.364525, current_train_items 25472.
I0302 18:58:38.842354 22699590365312 run.py:483] Algo bellman_ford step 796 current loss 0.536447, current_train_items 25504.
I0302 18:58:38.865620 22699590365312 run.py:483] Algo bellman_ford step 797 current loss 0.910872, current_train_items 25536.
I0302 18:58:38.893892 22699590365312 run.py:483] Algo bellman_ford step 798 current loss 1.058918, current_train_items 25568.
I0302 18:58:38.924726 22699590365312 run.py:483] Algo bellman_ford step 799 current loss 1.470720, current_train_items 25600.
I0302 18:58:38.943115 22699590365312 run.py:483] Algo bellman_ford step 800 current loss 0.245750, current_train_items 25632.
I0302 18:58:38.951076 22699590365312 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.822265625, 'score': 0.822265625, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0302 18:58:38.951191 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.877, current avg val score is 0.822, val scores are: bellman_ford: 0.822
I0302 18:58:38.967760 22699590365312 run.py:483] Algo bellman_ford step 801 current loss 0.795139, current_train_items 25664.
I0302 18:58:38.991933 22699590365312 run.py:483] Algo bellman_ford step 802 current loss 1.180927, current_train_items 25696.
I0302 18:58:39.021522 22699590365312 run.py:483] Algo bellman_ford step 803 current loss 1.417327, current_train_items 25728.
I0302 18:58:39.053395 22699590365312 run.py:483] Algo bellman_ford step 804 current loss 1.302428, current_train_items 25760.
I0302 18:58:39.072507 22699590365312 run.py:483] Algo bellman_ford step 805 current loss 0.328105, current_train_items 25792.
I0302 18:58:39.088161 22699590365312 run.py:483] Algo bellman_ford step 806 current loss 0.717446, current_train_items 25824.
I0302 18:58:39.111907 22699590365312 run.py:483] Algo bellman_ford step 807 current loss 0.924961, current_train_items 25856.
I0302 18:58:39.141013 22699590365312 run.py:483] Algo bellman_ford step 808 current loss 0.945056, current_train_items 25888.
I0302 18:58:39.171385 22699590365312 run.py:483] Algo bellman_ford step 809 current loss 1.143684, current_train_items 25920.
I0302 18:58:39.190140 22699590365312 run.py:483] Algo bellman_ford step 810 current loss 0.487013, current_train_items 25952.
I0302 18:58:39.206672 22699590365312 run.py:483] Algo bellman_ford step 811 current loss 0.842567, current_train_items 25984.
I0302 18:58:39.229145 22699590365312 run.py:483] Algo bellman_ford step 812 current loss 0.797812, current_train_items 26016.
I0302 18:58:39.257834 22699590365312 run.py:483] Algo bellman_ford step 813 current loss 1.033886, current_train_items 26048.
I0302 18:58:39.289345 22699590365312 run.py:483] Algo bellman_ford step 814 current loss 1.318433, current_train_items 26080.
I0302 18:58:39.307774 22699590365312 run.py:483] Algo bellman_ford step 815 current loss 0.403416, current_train_items 26112.
I0302 18:58:39.324074 22699590365312 run.py:483] Algo bellman_ford step 816 current loss 0.742009, current_train_items 26144.
I0302 18:58:39.347149 22699590365312 run.py:483] Algo bellman_ford step 817 current loss 0.886105, current_train_items 26176.
I0302 18:58:39.377262 22699590365312 run.py:483] Algo bellman_ford step 818 current loss 1.098273, current_train_items 26208.
I0302 18:58:39.410565 22699590365312 run.py:483] Algo bellman_ford step 819 current loss 1.280888, current_train_items 26240.
I0302 18:58:39.429000 22699590365312 run.py:483] Algo bellman_ford step 820 current loss 0.539007, current_train_items 26272.
I0302 18:58:39.444688 22699590365312 run.py:483] Algo bellman_ford step 821 current loss 0.692820, current_train_items 26304.
I0302 18:58:39.467482 22699590365312 run.py:483] Algo bellman_ford step 822 current loss 0.764129, current_train_items 26336.
I0302 18:58:39.496665 22699590365312 run.py:483] Algo bellman_ford step 823 current loss 1.005386, current_train_items 26368.
I0302 18:58:39.527230 22699590365312 run.py:483] Algo bellman_ford step 824 current loss 1.280061, current_train_items 26400.
I0302 18:58:39.545824 22699590365312 run.py:483] Algo bellman_ford step 825 current loss 0.387090, current_train_items 26432.
I0302 18:58:39.562187 22699590365312 run.py:483] Algo bellman_ford step 826 current loss 0.754101, current_train_items 26464.
I0302 18:58:39.585860 22699590365312 run.py:483] Algo bellman_ford step 827 current loss 0.889605, current_train_items 26496.
I0302 18:58:39.615004 22699590365312 run.py:483] Algo bellman_ford step 828 current loss 1.158276, current_train_items 26528.
I0302 18:58:39.646628 22699590365312 run.py:483] Algo bellman_ford step 829 current loss 1.093446, current_train_items 26560.
I0302 18:58:39.665009 22699590365312 run.py:483] Algo bellman_ford step 830 current loss 0.334158, current_train_items 26592.
I0302 18:58:39.681025 22699590365312 run.py:483] Algo bellman_ford step 831 current loss 0.662406, current_train_items 26624.
I0302 18:58:39.704456 22699590365312 run.py:483] Algo bellman_ford step 832 current loss 0.888366, current_train_items 26656.
I0302 18:58:39.733497 22699590365312 run.py:483] Algo bellman_ford step 833 current loss 1.141106, current_train_items 26688.
I0302 18:58:39.764120 22699590365312 run.py:483] Algo bellman_ford step 834 current loss 1.243299, current_train_items 26720.
I0302 18:58:39.782537 22699590365312 run.py:483] Algo bellman_ford step 835 current loss 0.430551, current_train_items 26752.
I0302 18:58:39.798403 22699590365312 run.py:483] Algo bellman_ford step 836 current loss 0.639410, current_train_items 26784.
I0302 18:58:39.821771 22699590365312 run.py:483] Algo bellman_ford step 837 current loss 0.951363, current_train_items 26816.
I0302 18:58:39.850795 22699590365312 run.py:483] Algo bellman_ford step 838 current loss 0.858292, current_train_items 26848.
I0302 18:58:39.883483 22699590365312 run.py:483] Algo bellman_ford step 839 current loss 1.374200, current_train_items 26880.
I0302 18:58:39.901440 22699590365312 run.py:483] Algo bellman_ford step 840 current loss 0.469657, current_train_items 26912.
I0302 18:58:39.917447 22699590365312 run.py:483] Algo bellman_ford step 841 current loss 0.726110, current_train_items 26944.
I0302 18:58:39.940648 22699590365312 run.py:483] Algo bellman_ford step 842 current loss 0.982267, current_train_items 26976.
I0302 18:58:39.968645 22699590365312 run.py:483] Algo bellman_ford step 843 current loss 0.940082, current_train_items 27008.
I0302 18:58:39.999417 22699590365312 run.py:483] Algo bellman_ford step 844 current loss 1.051003, current_train_items 27040.
I0302 18:58:40.017684 22699590365312 run.py:483] Algo bellman_ford step 845 current loss 0.390447, current_train_items 27072.
I0302 18:58:40.033875 22699590365312 run.py:483] Algo bellman_ford step 846 current loss 0.619709, current_train_items 27104.
I0302 18:58:40.058306 22699590365312 run.py:483] Algo bellman_ford step 847 current loss 0.966660, current_train_items 27136.
I0302 18:58:40.086747 22699590365312 run.py:483] Algo bellman_ford step 848 current loss 0.953264, current_train_items 27168.
I0302 18:58:40.118733 22699590365312 run.py:483] Algo bellman_ford step 849 current loss 1.272681, current_train_items 27200.
I0302 18:58:40.137034 22699590365312 run.py:483] Algo bellman_ford step 850 current loss 0.328778, current_train_items 27232.
I0302 18:58:40.145237 22699590365312 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.884765625, 'score': 0.884765625, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0302 18:58:40.145342 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.877, current avg val score is 0.885, val scores are: bellman_ford: 0.885
I0302 18:58:40.174049 22699590365312 run.py:483] Algo bellman_ford step 851 current loss 0.578317, current_train_items 27264.
I0302 18:58:40.197709 22699590365312 run.py:483] Algo bellman_ford step 852 current loss 0.851979, current_train_items 27296.
I0302 18:58:40.228812 22699590365312 run.py:483] Algo bellman_ford step 853 current loss 1.066010, current_train_items 27328.
I0302 18:58:40.260416 22699590365312 run.py:483] Algo bellman_ford step 854 current loss 1.116364, current_train_items 27360.
I0302 18:58:40.279743 22699590365312 run.py:483] Algo bellman_ford step 855 current loss 0.413508, current_train_items 27392.
I0302 18:58:40.296003 22699590365312 run.py:483] Algo bellman_ford step 856 current loss 0.474355, current_train_items 27424.
I0302 18:58:40.318527 22699590365312 run.py:483] Algo bellman_ford step 857 current loss 0.747540, current_train_items 27456.
I0302 18:58:40.348995 22699590365312 run.py:483] Algo bellman_ford step 858 current loss 0.883574, current_train_items 27488.
I0302 18:58:40.380142 22699590365312 run.py:483] Algo bellman_ford step 859 current loss 0.941565, current_train_items 27520.
I0302 18:58:40.399194 22699590365312 run.py:483] Algo bellman_ford step 860 current loss 0.390077, current_train_items 27552.
I0302 18:58:40.415585 22699590365312 run.py:483] Algo bellman_ford step 861 current loss 0.705017, current_train_items 27584.
I0302 18:58:40.438420 22699590365312 run.py:483] Algo bellman_ford step 862 current loss 0.815888, current_train_items 27616.
I0302 18:58:40.467842 22699590365312 run.py:483] Algo bellman_ford step 863 current loss 0.987590, current_train_items 27648.
I0302 18:58:40.498907 22699590365312 run.py:483] Algo bellman_ford step 864 current loss 1.164820, current_train_items 27680.
I0302 18:58:40.517285 22699590365312 run.py:483] Algo bellman_ford step 865 current loss 0.343960, current_train_items 27712.
I0302 18:58:40.533054 22699590365312 run.py:483] Algo bellman_ford step 866 current loss 0.579356, current_train_items 27744.
I0302 18:58:40.556983 22699590365312 run.py:483] Algo bellman_ford step 867 current loss 0.912381, current_train_items 27776.
I0302 18:58:40.585866 22699590365312 run.py:483] Algo bellman_ford step 868 current loss 0.877553, current_train_items 27808.
I0302 18:58:40.618806 22699590365312 run.py:483] Algo bellman_ford step 869 current loss 1.111011, current_train_items 27840.
I0302 18:58:40.637289 22699590365312 run.py:483] Algo bellman_ford step 870 current loss 0.304613, current_train_items 27872.
I0302 18:58:40.653676 22699590365312 run.py:483] Algo bellman_ford step 871 current loss 0.645045, current_train_items 27904.
I0302 18:58:40.675729 22699590365312 run.py:483] Algo bellman_ford step 872 current loss 0.654242, current_train_items 27936.
I0302 18:58:40.705178 22699590365312 run.py:483] Algo bellman_ford step 873 current loss 0.989024, current_train_items 27968.
I0302 18:58:40.736397 22699590365312 run.py:483] Algo bellman_ford step 874 current loss 1.045588, current_train_items 28000.
I0302 18:58:40.755292 22699590365312 run.py:483] Algo bellman_ford step 875 current loss 0.366074, current_train_items 28032.
I0302 18:58:40.771686 22699590365312 run.py:483] Algo bellman_ford step 876 current loss 0.626666, current_train_items 28064.
I0302 18:58:40.796631 22699590365312 run.py:483] Algo bellman_ford step 877 current loss 0.967647, current_train_items 28096.
I0302 18:58:40.824827 22699590365312 run.py:483] Algo bellman_ford step 878 current loss 0.834903, current_train_items 28128.
I0302 18:58:40.857381 22699590365312 run.py:483] Algo bellman_ford step 879 current loss 1.079926, current_train_items 28160.
I0302 18:58:40.875619 22699590365312 run.py:483] Algo bellman_ford step 880 current loss 0.327327, current_train_items 28192.
I0302 18:58:40.891504 22699590365312 run.py:483] Algo bellman_ford step 881 current loss 0.590896, current_train_items 28224.
I0302 18:58:40.914014 22699590365312 run.py:483] Algo bellman_ford step 882 current loss 0.715926, current_train_items 28256.
I0302 18:58:40.943346 22699590365312 run.py:483] Algo bellman_ford step 883 current loss 0.897368, current_train_items 28288.
I0302 18:58:40.976009 22699590365312 run.py:483] Algo bellman_ford step 884 current loss 1.130069, current_train_items 28320.
I0302 18:58:40.994751 22699590365312 run.py:483] Algo bellman_ford step 885 current loss 0.331129, current_train_items 28352.
I0302 18:58:41.010697 22699590365312 run.py:483] Algo bellman_ford step 886 current loss 0.711502, current_train_items 28384.
I0302 18:58:41.033723 22699590365312 run.py:483] Algo bellman_ford step 887 current loss 0.871427, current_train_items 28416.
I0302 18:58:41.062369 22699590365312 run.py:483] Algo bellman_ford step 888 current loss 0.931653, current_train_items 28448.
I0302 18:58:41.095143 22699590365312 run.py:483] Algo bellman_ford step 889 current loss 1.102172, current_train_items 28480.
I0302 18:58:41.114077 22699590365312 run.py:483] Algo bellman_ford step 890 current loss 0.330376, current_train_items 28512.
I0302 18:58:41.130184 22699590365312 run.py:483] Algo bellman_ford step 891 current loss 0.596827, current_train_items 28544.
I0302 18:58:41.153566 22699590365312 run.py:483] Algo bellman_ford step 892 current loss 0.961427, current_train_items 28576.
I0302 18:58:41.183768 22699590365312 run.py:483] Algo bellman_ford step 893 current loss 0.976280, current_train_items 28608.
I0302 18:58:41.214931 22699590365312 run.py:483] Algo bellman_ford step 894 current loss 1.067033, current_train_items 28640.
I0302 18:58:41.233175 22699590365312 run.py:483] Algo bellman_ford step 895 current loss 0.338056, current_train_items 28672.
I0302 18:58:41.249004 22699590365312 run.py:483] Algo bellman_ford step 896 current loss 0.486429, current_train_items 28704.
I0302 18:58:41.272861 22699590365312 run.py:483] Algo bellman_ford step 897 current loss 0.920758, current_train_items 28736.
I0302 18:58:41.302479 22699590365312 run.py:483] Algo bellman_ford step 898 current loss 1.037632, current_train_items 28768.
I0302 18:58:41.336123 22699590365312 run.py:483] Algo bellman_ford step 899 current loss 1.420988, current_train_items 28800.
I0302 18:58:41.354514 22699590365312 run.py:483] Algo bellman_ford step 900 current loss 0.451240, current_train_items 28832.
I0302 18:58:41.362491 22699590365312 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.8837890625, 'score': 0.8837890625, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0302 18:58:41.362597 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.885, current avg val score is 0.884, val scores are: bellman_ford: 0.884
I0302 18:58:41.379786 22699590365312 run.py:483] Algo bellman_ford step 901 current loss 0.598575, current_train_items 28864.
I0302 18:58:41.403952 22699590365312 run.py:483] Algo bellman_ford step 902 current loss 0.819243, current_train_items 28896.
I0302 18:58:41.434520 22699590365312 run.py:483] Algo bellman_ford step 903 current loss 0.832023, current_train_items 28928.
I0302 18:58:41.466671 22699590365312 run.py:483] Algo bellman_ford step 904 current loss 1.066591, current_train_items 28960.
I0302 18:58:41.485552 22699590365312 run.py:483] Algo bellman_ford step 905 current loss 0.312904, current_train_items 28992.
I0302 18:58:41.501759 22699590365312 run.py:483] Algo bellman_ford step 906 current loss 0.656945, current_train_items 29024.
I0302 18:58:41.525075 22699590365312 run.py:483] Algo bellman_ford step 907 current loss 0.749021, current_train_items 29056.
I0302 18:58:41.555319 22699590365312 run.py:483] Algo bellman_ford step 908 current loss 0.984554, current_train_items 29088.
I0302 18:58:41.587520 22699590365312 run.py:483] Algo bellman_ford step 909 current loss 1.114524, current_train_items 29120.
I0302 18:58:41.605872 22699590365312 run.py:483] Algo bellman_ford step 910 current loss 0.347621, current_train_items 29152.
I0302 18:58:41.622652 22699590365312 run.py:483] Algo bellman_ford step 911 current loss 0.645096, current_train_items 29184.
I0302 18:58:41.646547 22699590365312 run.py:483] Algo bellman_ford step 912 current loss 0.825842, current_train_items 29216.
I0302 18:58:41.674064 22699590365312 run.py:483] Algo bellman_ford step 913 current loss 0.824480, current_train_items 29248.
I0302 18:58:41.706965 22699590365312 run.py:483] Algo bellman_ford step 914 current loss 1.113593, current_train_items 29280.
I0302 18:58:41.725146 22699590365312 run.py:483] Algo bellman_ford step 915 current loss 0.299463, current_train_items 29312.
I0302 18:58:41.741399 22699590365312 run.py:483] Algo bellman_ford step 916 current loss 0.510289, current_train_items 29344.
I0302 18:58:41.764856 22699590365312 run.py:483] Algo bellman_ford step 917 current loss 0.843231, current_train_items 29376.
I0302 18:58:41.794825 22699590365312 run.py:483] Algo bellman_ford step 918 current loss 1.061650, current_train_items 29408.
I0302 18:58:41.827698 22699590365312 run.py:483] Algo bellman_ford step 919 current loss 1.207025, current_train_items 29440.
I0302 18:58:41.846399 22699590365312 run.py:483] Algo bellman_ford step 920 current loss 0.333878, current_train_items 29472.
I0302 18:58:41.862835 22699590365312 run.py:483] Algo bellman_ford step 921 current loss 0.591387, current_train_items 29504.
I0302 18:58:41.886486 22699590365312 run.py:483] Algo bellman_ford step 922 current loss 0.793508, current_train_items 29536.
I0302 18:58:41.915558 22699590365312 run.py:483] Algo bellman_ford step 923 current loss 0.825318, current_train_items 29568.
I0302 18:58:41.947097 22699590365312 run.py:483] Algo bellman_ford step 924 current loss 1.161716, current_train_items 29600.
I0302 18:58:41.965496 22699590365312 run.py:483] Algo bellman_ford step 925 current loss 0.413824, current_train_items 29632.
I0302 18:58:41.981302 22699590365312 run.py:483] Algo bellman_ford step 926 current loss 0.569228, current_train_items 29664.
I0302 18:58:42.004953 22699590365312 run.py:483] Algo bellman_ford step 927 current loss 0.758446, current_train_items 29696.
I0302 18:58:42.035207 22699590365312 run.py:483] Algo bellman_ford step 928 current loss 0.907159, current_train_items 29728.
I0302 18:58:42.065638 22699590365312 run.py:483] Algo bellman_ford step 929 current loss 1.039621, current_train_items 29760.
I0302 18:58:42.084100 22699590365312 run.py:483] Algo bellman_ford step 930 current loss 0.370135, current_train_items 29792.
I0302 18:58:42.100270 22699590365312 run.py:483] Algo bellman_ford step 931 current loss 0.691706, current_train_items 29824.
I0302 18:58:42.123459 22699590365312 run.py:483] Algo bellman_ford step 932 current loss 0.857867, current_train_items 29856.
I0302 18:58:42.151784 22699590365312 run.py:483] Algo bellman_ford step 933 current loss 0.911073, current_train_items 29888.
I0302 18:58:42.184372 22699590365312 run.py:483] Algo bellman_ford step 934 current loss 1.190606, current_train_items 29920.
I0302 18:58:42.202605 22699590365312 run.py:483] Algo bellman_ford step 935 current loss 0.363301, current_train_items 29952.
I0302 18:58:42.218709 22699590365312 run.py:483] Algo bellman_ford step 936 current loss 0.659053, current_train_items 29984.
I0302 18:58:42.241054 22699590365312 run.py:483] Algo bellman_ford step 937 current loss 0.681224, current_train_items 30016.
I0302 18:58:42.269973 22699590365312 run.py:483] Algo bellman_ford step 938 current loss 0.869819, current_train_items 30048.
I0302 18:58:42.301787 22699590365312 run.py:483] Algo bellman_ford step 939 current loss 1.258048, current_train_items 30080.
I0302 18:58:42.320047 22699590365312 run.py:483] Algo bellman_ford step 940 current loss 0.389952, current_train_items 30112.
I0302 18:58:42.336175 22699590365312 run.py:483] Algo bellman_ford step 941 current loss 0.619424, current_train_items 30144.
I0302 18:58:42.359944 22699590365312 run.py:483] Algo bellman_ford step 942 current loss 0.826300, current_train_items 30176.
I0302 18:58:42.389121 22699590365312 run.py:483] Algo bellman_ford step 943 current loss 0.768354, current_train_items 30208.
I0302 18:58:42.421463 22699590365312 run.py:483] Algo bellman_ford step 944 current loss 1.211456, current_train_items 30240.
I0302 18:58:42.440086 22699590365312 run.py:483] Algo bellman_ford step 945 current loss 0.312360, current_train_items 30272.
I0302 18:58:42.455768 22699590365312 run.py:483] Algo bellman_ford step 946 current loss 0.625755, current_train_items 30304.
I0302 18:58:42.479827 22699590365312 run.py:483] Algo bellman_ford step 947 current loss 0.838596, current_train_items 30336.
I0302 18:58:42.508653 22699590365312 run.py:483] Algo bellman_ford step 948 current loss 0.830244, current_train_items 30368.
I0302 18:58:42.540292 22699590365312 run.py:483] Algo bellman_ford step 949 current loss 1.181903, current_train_items 30400.
I0302 18:58:42.558743 22699590365312 run.py:483] Algo bellman_ford step 950 current loss 0.424446, current_train_items 30432.
I0302 18:58:42.566990 22699590365312 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.859375, 'score': 0.859375, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0302 18:58:42.567092 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.885, current avg val score is 0.859, val scores are: bellman_ford: 0.859
I0302 18:58:42.583740 22699590365312 run.py:483] Algo bellman_ford step 951 current loss 0.617962, current_train_items 30464.
I0302 18:58:42.606889 22699590365312 run.py:483] Algo bellman_ford step 952 current loss 0.830499, current_train_items 30496.
I0302 18:58:42.636181 22699590365312 run.py:483] Algo bellman_ford step 953 current loss 0.974410, current_train_items 30528.
I0302 18:58:42.665670 22699590365312 run.py:483] Algo bellman_ford step 954 current loss 0.854428, current_train_items 30560.
I0302 18:58:42.684553 22699590365312 run.py:483] Algo bellman_ford step 955 current loss 0.334261, current_train_items 30592.
I0302 18:58:42.700390 22699590365312 run.py:483] Algo bellman_ford step 956 current loss 0.619955, current_train_items 30624.
I0302 18:58:42.723604 22699590365312 run.py:483] Algo bellman_ford step 957 current loss 0.916547, current_train_items 30656.
I0302 18:58:42.751184 22699590365312 run.py:483] Algo bellman_ford step 958 current loss 1.005442, current_train_items 30688.
I0302 18:58:42.782900 22699590365312 run.py:483] Algo bellman_ford step 959 current loss 1.272963, current_train_items 30720.
I0302 18:58:42.801319 22699590365312 run.py:483] Algo bellman_ford step 960 current loss 0.293042, current_train_items 30752.
I0302 18:58:42.817958 22699590365312 run.py:483] Algo bellman_ford step 961 current loss 0.742490, current_train_items 30784.
I0302 18:58:42.839292 22699590365312 run.py:483] Algo bellman_ford step 962 current loss 0.711519, current_train_items 30816.
I0302 18:58:42.866616 22699590365312 run.py:483] Algo bellman_ford step 963 current loss 0.791638, current_train_items 30848.
I0302 18:58:42.898583 22699590365312 run.py:483] Algo bellman_ford step 964 current loss 1.244296, current_train_items 30880.
I0302 18:58:42.916884 22699590365312 run.py:483] Algo bellman_ford step 965 current loss 0.456648, current_train_items 30912.
I0302 18:58:42.932759 22699590365312 run.py:483] Algo bellman_ford step 966 current loss 0.523367, current_train_items 30944.
I0302 18:58:42.956013 22699590365312 run.py:483] Algo bellman_ford step 967 current loss 0.748366, current_train_items 30976.
I0302 18:58:42.985398 22699590365312 run.py:483] Algo bellman_ford step 968 current loss 0.860071, current_train_items 31008.
I0302 18:58:43.016462 22699590365312 run.py:483] Algo bellman_ford step 969 current loss 1.077437, current_train_items 31040.
I0302 18:58:43.034775 22699590365312 run.py:483] Algo bellman_ford step 970 current loss 0.377215, current_train_items 31072.
I0302 18:58:43.050995 22699590365312 run.py:483] Algo bellman_ford step 971 current loss 0.587018, current_train_items 31104.
I0302 18:58:43.073963 22699590365312 run.py:483] Algo bellman_ford step 972 current loss 0.843394, current_train_items 31136.
I0302 18:58:43.103782 22699590365312 run.py:483] Algo bellman_ford step 973 current loss 0.931740, current_train_items 31168.
I0302 18:58:43.136827 22699590365312 run.py:483] Algo bellman_ford step 974 current loss 1.070345, current_train_items 31200.
I0302 18:58:43.155442 22699590365312 run.py:483] Algo bellman_ford step 975 current loss 0.392777, current_train_items 31232.
I0302 18:58:43.171252 22699590365312 run.py:483] Algo bellman_ford step 976 current loss 0.428725, current_train_items 31264.
I0302 18:58:43.193732 22699590365312 run.py:483] Algo bellman_ford step 977 current loss 0.735990, current_train_items 31296.
I0302 18:58:43.222306 22699590365312 run.py:483] Algo bellman_ford step 978 current loss 0.816755, current_train_items 31328.
I0302 18:58:43.253500 22699590365312 run.py:483] Algo bellman_ford step 979 current loss 0.999495, current_train_items 31360.
I0302 18:58:43.271472 22699590365312 run.py:483] Algo bellman_ford step 980 current loss 0.305628, current_train_items 31392.
I0302 18:58:43.287671 22699590365312 run.py:483] Algo bellman_ford step 981 current loss 0.599934, current_train_items 31424.
I0302 18:58:43.310163 22699590365312 run.py:483] Algo bellman_ford step 982 current loss 0.750378, current_train_items 31456.
I0302 18:58:43.338265 22699590365312 run.py:483] Algo bellman_ford step 983 current loss 0.787571, current_train_items 31488.
I0302 18:58:43.370163 22699590365312 run.py:483] Algo bellman_ford step 984 current loss 1.006117, current_train_items 31520.
I0302 18:58:43.388834 22699590365312 run.py:483] Algo bellman_ford step 985 current loss 0.393115, current_train_items 31552.
I0302 18:58:43.405343 22699590365312 run.py:483] Algo bellman_ford step 986 current loss 0.664063, current_train_items 31584.
I0302 18:58:43.428004 22699590365312 run.py:483] Algo bellman_ford step 987 current loss 0.745702, current_train_items 31616.
I0302 18:58:43.457366 22699590365312 run.py:483] Algo bellman_ford step 988 current loss 0.768949, current_train_items 31648.
I0302 18:58:43.488078 22699590365312 run.py:483] Algo bellman_ford step 989 current loss 0.885374, current_train_items 31680.
I0302 18:58:43.506382 22699590365312 run.py:483] Algo bellman_ford step 990 current loss 0.291791, current_train_items 31712.
I0302 18:58:43.522071 22699590365312 run.py:483] Algo bellman_ford step 991 current loss 0.567647, current_train_items 31744.
I0302 18:58:43.545524 22699590365312 run.py:483] Algo bellman_ford step 992 current loss 0.834877, current_train_items 31776.
I0302 18:58:43.572969 22699590365312 run.py:483] Algo bellman_ford step 993 current loss 0.888974, current_train_items 31808.
I0302 18:58:43.607066 22699590365312 run.py:483] Algo bellman_ford step 994 current loss 1.280079, current_train_items 31840.
I0302 18:58:43.625563 22699590365312 run.py:483] Algo bellman_ford step 995 current loss 0.411503, current_train_items 31872.
I0302 18:58:43.641318 22699590365312 run.py:483] Algo bellman_ford step 996 current loss 0.631796, current_train_items 31904.
I0302 18:58:43.663845 22699590365312 run.py:483] Algo bellman_ford step 997 current loss 0.737339, current_train_items 31936.
I0302 18:58:43.693614 22699590365312 run.py:483] Algo bellman_ford step 998 current loss 0.891454, current_train_items 31968.
I0302 18:58:43.724436 22699590365312 run.py:483] Algo bellman_ford step 999 current loss 1.018242, current_train_items 32000.
I0302 18:58:43.742893 22699590365312 run.py:483] Algo bellman_ford step 1000 current loss 0.330933, current_train_items 32032.
I0302 18:58:43.750880 22699590365312 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.8701171875, 'score': 0.8701171875, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0302 18:58:43.750997 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.885, current avg val score is 0.870, val scores are: bellman_ford: 0.870
I0302 18:58:43.768181 22699590365312 run.py:483] Algo bellman_ford step 1001 current loss 0.590200, current_train_items 32064.
I0302 18:58:43.791901 22699590365312 run.py:483] Algo bellman_ford step 1002 current loss 0.658894, current_train_items 32096.
I0302 18:58:43.820164 22699590365312 run.py:483] Algo bellman_ford step 1003 current loss 0.880152, current_train_items 32128.
I0302 18:58:43.855026 22699590365312 run.py:483] Algo bellman_ford step 1004 current loss 1.109435, current_train_items 32160.
I0302 18:58:43.873877 22699590365312 run.py:483] Algo bellman_ford step 1005 current loss 0.335029, current_train_items 32192.
I0302 18:58:43.889433 22699590365312 run.py:483] Algo bellman_ford step 1006 current loss 0.624151, current_train_items 32224.
I0302 18:58:43.913649 22699590365312 run.py:483] Algo bellman_ford step 1007 current loss 0.738038, current_train_items 32256.
I0302 18:58:43.941714 22699590365312 run.py:483] Algo bellman_ford step 1008 current loss 0.751259, current_train_items 32288.
I0302 18:58:43.974298 22699590365312 run.py:483] Algo bellman_ford step 1009 current loss 0.954039, current_train_items 32320.
I0302 18:58:43.993152 22699590365312 run.py:483] Algo bellman_ford step 1010 current loss 0.281360, current_train_items 32352.
I0302 18:58:44.009442 22699590365312 run.py:483] Algo bellman_ford step 1011 current loss 0.561485, current_train_items 32384.
I0302 18:58:44.033460 22699590365312 run.py:483] Algo bellman_ford step 1012 current loss 0.768526, current_train_items 32416.
I0302 18:58:44.062993 22699590365312 run.py:483] Algo bellman_ford step 1013 current loss 0.898926, current_train_items 32448.
I0302 18:58:44.095276 22699590365312 run.py:483] Algo bellman_ford step 1014 current loss 0.975348, current_train_items 32480.
I0302 18:58:44.113688 22699590365312 run.py:483] Algo bellman_ford step 1015 current loss 0.380665, current_train_items 32512.
I0302 18:58:44.130027 22699590365312 run.py:483] Algo bellman_ford step 1016 current loss 0.585906, current_train_items 32544.
I0302 18:58:44.153224 22699590365312 run.py:483] Algo bellman_ford step 1017 current loss 0.792813, current_train_items 32576.
I0302 18:58:44.182669 22699590365312 run.py:483] Algo bellman_ford step 1018 current loss 0.694364, current_train_items 32608.
I0302 18:58:44.217076 22699590365312 run.py:483] Algo bellman_ford step 1019 current loss 1.168975, current_train_items 32640.
I0302 18:58:44.235501 22699590365312 run.py:483] Algo bellman_ford step 1020 current loss 0.370257, current_train_items 32672.
I0302 18:58:44.251388 22699590365312 run.py:483] Algo bellman_ford step 1021 current loss 0.561673, current_train_items 32704.
I0302 18:58:44.274043 22699590365312 run.py:483] Algo bellman_ford step 1022 current loss 0.661441, current_train_items 32736.
I0302 18:58:44.302537 22699590365312 run.py:483] Algo bellman_ford step 1023 current loss 0.772169, current_train_items 32768.
I0302 18:58:44.332340 22699590365312 run.py:483] Algo bellman_ford step 1024 current loss 1.094536, current_train_items 32800.
I0302 18:58:44.350729 22699590365312 run.py:483] Algo bellman_ford step 1025 current loss 0.334280, current_train_items 32832.
I0302 18:58:44.367380 22699590365312 run.py:483] Algo bellman_ford step 1026 current loss 0.622427, current_train_items 32864.
I0302 18:58:44.390374 22699590365312 run.py:483] Algo bellman_ford step 1027 current loss 0.759755, current_train_items 32896.
I0302 18:58:44.420257 22699590365312 run.py:483] Algo bellman_ford step 1028 current loss 0.935253, current_train_items 32928.
I0302 18:58:44.451190 22699590365312 run.py:483] Algo bellman_ford step 1029 current loss 1.027851, current_train_items 32960.
I0302 18:58:44.469830 22699590365312 run.py:483] Algo bellman_ford step 1030 current loss 0.446451, current_train_items 32992.
I0302 18:58:44.485519 22699590365312 run.py:483] Algo bellman_ford step 1031 current loss 0.515501, current_train_items 33024.
I0302 18:58:44.507926 22699590365312 run.py:483] Algo bellman_ford step 1032 current loss 0.772309, current_train_items 33056.
I0302 18:58:44.536004 22699590365312 run.py:483] Algo bellman_ford step 1033 current loss 0.831588, current_train_items 33088.
I0302 18:58:44.568406 22699590365312 run.py:483] Algo bellman_ford step 1034 current loss 1.140132, current_train_items 33120.
I0302 18:58:44.586724 22699590365312 run.py:483] Algo bellman_ford step 1035 current loss 0.454953, current_train_items 33152.
I0302 18:58:44.602921 22699590365312 run.py:483] Algo bellman_ford step 1036 current loss 0.564357, current_train_items 33184.
I0302 18:58:44.626682 22699590365312 run.py:483] Algo bellman_ford step 1037 current loss 0.756379, current_train_items 33216.
I0302 18:58:44.656349 22699590365312 run.py:483] Algo bellman_ford step 1038 current loss 0.915812, current_train_items 33248.
I0302 18:58:44.689694 22699590365312 run.py:483] Algo bellman_ford step 1039 current loss 1.214147, current_train_items 33280.
I0302 18:58:44.708076 22699590365312 run.py:483] Algo bellman_ford step 1040 current loss 0.290814, current_train_items 33312.
I0302 18:58:44.724886 22699590365312 run.py:483] Algo bellman_ford step 1041 current loss 0.707973, current_train_items 33344.
I0302 18:58:44.748230 22699590365312 run.py:483] Algo bellman_ford step 1042 current loss 0.899357, current_train_items 33376.
I0302 18:58:44.777054 22699590365312 run.py:483] Algo bellman_ford step 1043 current loss 0.865124, current_train_items 33408.
I0302 18:58:44.808343 22699590365312 run.py:483] Algo bellman_ford step 1044 current loss 0.987095, current_train_items 33440.
I0302 18:58:44.826790 22699590365312 run.py:483] Algo bellman_ford step 1045 current loss 0.247742, current_train_items 33472.
I0302 18:58:44.842476 22699590365312 run.py:483] Algo bellman_ford step 1046 current loss 0.502117, current_train_items 33504.
I0302 18:58:44.865783 22699590365312 run.py:483] Algo bellman_ford step 1047 current loss 0.835615, current_train_items 33536.
I0302 18:58:44.895850 22699590365312 run.py:483] Algo bellman_ford step 1048 current loss 0.985309, current_train_items 33568.
I0302 18:58:44.928087 22699590365312 run.py:483] Algo bellman_ford step 1049 current loss 0.951281, current_train_items 33600.
I0302 18:58:44.946494 22699590365312 run.py:483] Algo bellman_ford step 1050 current loss 0.335751, current_train_items 33632.
I0302 18:58:44.954566 22699590365312 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.8876953125, 'score': 0.8876953125, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0302 18:58:44.954672 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.885, current avg val score is 0.888, val scores are: bellman_ford: 0.888
I0302 18:58:44.984231 22699590365312 run.py:483] Algo bellman_ford step 1051 current loss 0.523923, current_train_items 33664.
I0302 18:58:45.008111 22699590365312 run.py:483] Algo bellman_ford step 1052 current loss 0.870087, current_train_items 33696.
I0302 18:58:45.037638 22699590365312 run.py:483] Algo bellman_ford step 1053 current loss 0.842570, current_train_items 33728.
I0302 18:58:45.070059 22699590365312 run.py:483] Algo bellman_ford step 1054 current loss 1.089511, current_train_items 33760.
I0302 18:58:45.089410 22699590365312 run.py:483] Algo bellman_ford step 1055 current loss 0.381840, current_train_items 33792.
I0302 18:58:45.105714 22699590365312 run.py:483] Algo bellman_ford step 1056 current loss 0.548859, current_train_items 33824.
I0302 18:58:45.129355 22699590365312 run.py:483] Algo bellman_ford step 1057 current loss 0.832007, current_train_items 33856.
I0302 18:58:45.159900 22699590365312 run.py:483] Algo bellman_ford step 1058 current loss 0.917198, current_train_items 33888.
I0302 18:58:45.189690 22699590365312 run.py:483] Algo bellman_ford step 1059 current loss 0.945464, current_train_items 33920.
I0302 18:58:45.208715 22699590365312 run.py:483] Algo bellman_ford step 1060 current loss 0.319643, current_train_items 33952.
I0302 18:58:45.224787 22699590365312 run.py:483] Algo bellman_ford step 1061 current loss 0.567392, current_train_items 33984.
I0302 18:58:45.246288 22699590365312 run.py:483] Algo bellman_ford step 1062 current loss 0.628497, current_train_items 34016.
I0302 18:58:45.274665 22699590365312 run.py:483] Algo bellman_ford step 1063 current loss 0.898036, current_train_items 34048.
I0302 18:58:45.306982 22699590365312 run.py:483] Algo bellman_ford step 1064 current loss 1.240446, current_train_items 34080.
I0302 18:58:45.325610 22699590365312 run.py:483] Algo bellman_ford step 1065 current loss 0.327874, current_train_items 34112.
I0302 18:58:45.341644 22699590365312 run.py:483] Algo bellman_ford step 1066 current loss 0.585199, current_train_items 34144.
I0302 18:58:45.364865 22699590365312 run.py:483] Algo bellman_ford step 1067 current loss 0.758775, current_train_items 34176.
I0302 18:58:45.394495 22699590365312 run.py:483] Algo bellman_ford step 1068 current loss 0.827567, current_train_items 34208.
I0302 18:58:45.427989 22699590365312 run.py:483] Algo bellman_ford step 1069 current loss 1.084251, current_train_items 34240.
I0302 18:58:45.446516 22699590365312 run.py:483] Algo bellman_ford step 1070 current loss 0.319246, current_train_items 34272.
I0302 18:58:45.462955 22699590365312 run.py:483] Algo bellman_ford step 1071 current loss 0.657750, current_train_items 34304.
I0302 18:58:45.485852 22699590365312 run.py:483] Algo bellman_ford step 1072 current loss 0.748718, current_train_items 34336.
I0302 18:58:45.514974 22699590365312 run.py:483] Algo bellman_ford step 1073 current loss 0.766106, current_train_items 34368.
I0302 18:58:45.545671 22699590365312 run.py:483] Algo bellman_ford step 1074 current loss 0.863921, current_train_items 34400.
I0302 18:58:45.564354 22699590365312 run.py:483] Algo bellman_ford step 1075 current loss 0.282544, current_train_items 34432.
I0302 18:58:45.580678 22699590365312 run.py:483] Algo bellman_ford step 1076 current loss 0.722151, current_train_items 34464.
I0302 18:58:45.604239 22699590365312 run.py:483] Algo bellman_ford step 1077 current loss 0.791869, current_train_items 34496.
I0302 18:58:45.633998 22699590365312 run.py:483] Algo bellman_ford step 1078 current loss 0.836153, current_train_items 34528.
I0302 18:58:45.665293 22699590365312 run.py:483] Algo bellman_ford step 1079 current loss 1.008211, current_train_items 34560.
I0302 18:58:45.683629 22699590365312 run.py:483] Algo bellman_ford step 1080 current loss 0.320234, current_train_items 34592.
I0302 18:58:45.700107 22699590365312 run.py:483] Algo bellman_ford step 1081 current loss 0.530739, current_train_items 34624.
I0302 18:58:45.722586 22699590365312 run.py:483] Algo bellman_ford step 1082 current loss 0.860150, current_train_items 34656.
I0302 18:58:45.751397 22699590365312 run.py:483] Algo bellman_ford step 1083 current loss 0.776875, current_train_items 34688.
I0302 18:58:45.783977 22699590365312 run.py:483] Algo bellman_ford step 1084 current loss 1.096404, current_train_items 34720.
I0302 18:58:45.802796 22699590365312 run.py:483] Algo bellman_ford step 1085 current loss 0.345007, current_train_items 34752.
I0302 18:58:45.819055 22699590365312 run.py:483] Algo bellman_ford step 1086 current loss 0.560974, current_train_items 34784.
I0302 18:58:45.842433 22699590365312 run.py:483] Algo bellman_ford step 1087 current loss 0.756716, current_train_items 34816.
I0302 18:58:45.870182 22699590365312 run.py:483] Algo bellman_ford step 1088 current loss 0.756833, current_train_items 34848.
I0302 18:58:45.900376 22699590365312 run.py:483] Algo bellman_ford step 1089 current loss 1.034635, current_train_items 34880.
I0302 18:58:45.918950 22699590365312 run.py:483] Algo bellman_ford step 1090 current loss 0.391578, current_train_items 34912.
I0302 18:58:45.935436 22699590365312 run.py:483] Algo bellman_ford step 1091 current loss 0.625161, current_train_items 34944.
I0302 18:58:45.957798 22699590365312 run.py:483] Algo bellman_ford step 1092 current loss 0.828006, current_train_items 34976.
I0302 18:58:45.987643 22699590365312 run.py:483] Algo bellman_ford step 1093 current loss 0.874932, current_train_items 35008.
I0302 18:58:46.020305 22699590365312 run.py:483] Algo bellman_ford step 1094 current loss 0.965547, current_train_items 35040.
I0302 18:58:46.038761 22699590365312 run.py:483] Algo bellman_ford step 1095 current loss 0.346691, current_train_items 35072.
I0302 18:58:46.054802 22699590365312 run.py:483] Algo bellman_ford step 1096 current loss 0.641000, current_train_items 35104.
I0302 18:58:46.077619 22699590365312 run.py:483] Algo bellman_ford step 1097 current loss 0.800681, current_train_items 35136.
I0302 18:58:46.107337 22699590365312 run.py:483] Algo bellman_ford step 1098 current loss 0.844379, current_train_items 35168.
I0302 18:58:46.138665 22699590365312 run.py:483] Algo bellman_ford step 1099 current loss 0.975550, current_train_items 35200.
I0302 18:58:46.157212 22699590365312 run.py:483] Algo bellman_ford step 1100 current loss 0.376447, current_train_items 35232.
I0302 18:58:46.165045 22699590365312 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0302 18:58:46.165151 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.888, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:58:46.194554 22699590365312 run.py:483] Algo bellman_ford step 1101 current loss 0.719994, current_train_items 35264.
I0302 18:58:46.218424 22699590365312 run.py:483] Algo bellman_ford step 1102 current loss 0.821150, current_train_items 35296.
I0302 18:58:46.249969 22699590365312 run.py:483] Algo bellman_ford step 1103 current loss 0.990377, current_train_items 35328.
I0302 18:58:46.284039 22699590365312 run.py:483] Algo bellman_ford step 1104 current loss 1.262983, current_train_items 35360.
I0302 18:58:46.303046 22699590365312 run.py:483] Algo bellman_ford step 1105 current loss 0.342105, current_train_items 35392.
I0302 18:58:46.318871 22699590365312 run.py:483] Algo bellman_ford step 1106 current loss 0.578781, current_train_items 35424.
I0302 18:58:46.342415 22699590365312 run.py:483] Algo bellman_ford step 1107 current loss 0.818546, current_train_items 35456.
I0302 18:58:46.370751 22699590365312 run.py:483] Algo bellman_ford step 1108 current loss 1.156580, current_train_items 35488.
I0302 18:58:46.400934 22699590365312 run.py:483] Algo bellman_ford step 1109 current loss 1.122690, current_train_items 35520.
I0302 18:58:46.419483 22699590365312 run.py:483] Algo bellman_ford step 1110 current loss 0.415418, current_train_items 35552.
I0302 18:58:46.435770 22699590365312 run.py:483] Algo bellman_ford step 1111 current loss 0.620726, current_train_items 35584.
I0302 18:58:46.458844 22699590365312 run.py:483] Algo bellman_ford step 1112 current loss 0.748170, current_train_items 35616.
I0302 18:58:46.486871 22699590365312 run.py:483] Algo bellman_ford step 1113 current loss 0.865081, current_train_items 35648.
I0302 18:58:46.519764 22699590365312 run.py:483] Algo bellman_ford step 1114 current loss 1.231598, current_train_items 35680.
I0302 18:58:46.538070 22699590365312 run.py:483] Algo bellman_ford step 1115 current loss 0.333361, current_train_items 35712.
I0302 18:58:46.553887 22699590365312 run.py:483] Algo bellman_ford step 1116 current loss 0.525947, current_train_items 35744.
I0302 18:58:46.577476 22699590365312 run.py:483] Algo bellman_ford step 1117 current loss 0.775190, current_train_items 35776.
I0302 18:58:46.607065 22699590365312 run.py:483] Algo bellman_ford step 1118 current loss 0.769198, current_train_items 35808.
I0302 18:58:46.639246 22699590365312 run.py:483] Algo bellman_ford step 1119 current loss 1.125086, current_train_items 35840.
I0302 18:58:46.657789 22699590365312 run.py:483] Algo bellman_ford step 1120 current loss 0.351941, current_train_items 35872.
I0302 18:58:46.673355 22699590365312 run.py:483] Algo bellman_ford step 1121 current loss 0.516155, current_train_items 35904.
I0302 18:58:46.696856 22699590365312 run.py:483] Algo bellman_ford step 1122 current loss 0.710373, current_train_items 35936.
I0302 18:58:46.724965 22699590365312 run.py:483] Algo bellman_ford step 1123 current loss 0.812910, current_train_items 35968.
I0302 18:58:46.758610 22699590365312 run.py:483] Algo bellman_ford step 1124 current loss 1.173961, current_train_items 36000.
I0302 18:58:46.777182 22699590365312 run.py:483] Algo bellman_ford step 1125 current loss 0.373974, current_train_items 36032.
I0302 18:58:46.792909 22699590365312 run.py:483] Algo bellman_ford step 1126 current loss 0.602690, current_train_items 36064.
I0302 18:58:46.815966 22699590365312 run.py:483] Algo bellman_ford step 1127 current loss 0.820843, current_train_items 36096.
I0302 18:58:46.845283 22699590365312 run.py:483] Algo bellman_ford step 1128 current loss 0.956972, current_train_items 36128.
I0302 18:58:46.875125 22699590365312 run.py:483] Algo bellman_ford step 1129 current loss 1.246618, current_train_items 36160.
I0302 18:58:46.893449 22699590365312 run.py:483] Algo bellman_ford step 1130 current loss 0.362619, current_train_items 36192.
I0302 18:58:46.909497 22699590365312 run.py:483] Algo bellman_ford step 1131 current loss 0.476916, current_train_items 36224.
I0302 18:58:46.932596 22699590365312 run.py:483] Algo bellman_ford step 1132 current loss 0.791810, current_train_items 36256.
I0302 18:58:46.961680 22699590365312 run.py:483] Algo bellman_ford step 1133 current loss 0.995183, current_train_items 36288.
I0302 18:58:46.992428 22699590365312 run.py:483] Algo bellman_ford step 1134 current loss 1.072011, current_train_items 36320.
I0302 18:58:47.010804 22699590365312 run.py:483] Algo bellman_ford step 1135 current loss 0.357274, current_train_items 36352.
I0302 18:58:47.026996 22699590365312 run.py:483] Algo bellman_ford step 1136 current loss 0.614690, current_train_items 36384.
I0302 18:58:47.049629 22699590365312 run.py:483] Algo bellman_ford step 1137 current loss 0.677388, current_train_items 36416.
I0302 18:58:47.079672 22699590365312 run.py:483] Algo bellman_ford step 1138 current loss 0.893705, current_train_items 36448.
I0302 18:58:47.111069 22699590365312 run.py:483] Algo bellman_ford step 1139 current loss 0.947609, current_train_items 36480.
I0302 18:58:47.129518 22699590365312 run.py:483] Algo bellman_ford step 1140 current loss 0.352888, current_train_items 36512.
I0302 18:58:47.145989 22699590365312 run.py:483] Algo bellman_ford step 1141 current loss 0.570359, current_train_items 36544.
I0302 18:58:47.168252 22699590365312 run.py:483] Algo bellman_ford step 1142 current loss 0.677377, current_train_items 36576.
I0302 18:58:47.196908 22699590365312 run.py:483] Algo bellman_ford step 1143 current loss 0.957610, current_train_items 36608.
I0302 18:58:47.228227 22699590365312 run.py:483] Algo bellman_ford step 1144 current loss 1.026581, current_train_items 36640.
I0302 18:58:47.246885 22699590365312 run.py:483] Algo bellman_ford step 1145 current loss 0.435523, current_train_items 36672.
I0302 18:58:47.263072 22699590365312 run.py:483] Algo bellman_ford step 1146 current loss 0.628612, current_train_items 36704.
I0302 18:58:47.286236 22699590365312 run.py:483] Algo bellman_ford step 1147 current loss 0.795223, current_train_items 36736.
I0302 18:58:47.315607 22699590365312 run.py:483] Algo bellman_ford step 1148 current loss 0.925807, current_train_items 36768.
I0302 18:58:47.345414 22699590365312 run.py:483] Algo bellman_ford step 1149 current loss 0.991548, current_train_items 36800.
I0302 18:58:47.363907 22699590365312 run.py:483] Algo bellman_ford step 1150 current loss 0.349665, current_train_items 36832.
I0302 18:58:47.372335 22699590365312 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.875, 'score': 0.875, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0302 18:58:47.372442 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.913, current avg val score is 0.875, val scores are: bellman_ford: 0.875
I0302 18:58:47.389016 22699590365312 run.py:483] Algo bellman_ford step 1151 current loss 0.552036, current_train_items 36864.
I0302 18:58:47.412959 22699590365312 run.py:483] Algo bellman_ford step 1152 current loss 0.809025, current_train_items 36896.
I0302 18:58:47.443799 22699590365312 run.py:483] Algo bellman_ford step 1153 current loss 0.883931, current_train_items 36928.
W0302 18:58:47.464937 22699590365312 samplers.py:155] Increasing hint lengh from 11 to 12
I0302 18:58:54.178552 22699590365312 run.py:483] Algo bellman_ford step 1154 current loss 1.032779, current_train_items 36960.
I0302 18:58:54.199169 22699590365312 run.py:483] Algo bellman_ford step 1155 current loss 0.355085, current_train_items 36992.
I0302 18:58:54.215403 22699590365312 run.py:483] Algo bellman_ford step 1156 current loss 0.498542, current_train_items 37024.
I0302 18:58:54.238617 22699590365312 run.py:483] Algo bellman_ford step 1157 current loss 0.729241, current_train_items 37056.
I0302 18:58:54.267756 22699590365312 run.py:483] Algo bellman_ford step 1158 current loss 0.886270, current_train_items 37088.
I0302 18:58:54.298059 22699590365312 run.py:483] Algo bellman_ford step 1159 current loss 0.919251, current_train_items 37120.
I0302 18:58:54.317786 22699590365312 run.py:483] Algo bellman_ford step 1160 current loss 0.389921, current_train_items 37152.
I0302 18:58:54.334236 22699590365312 run.py:483] Algo bellman_ford step 1161 current loss 0.550397, current_train_items 37184.
I0302 18:58:54.356648 22699590365312 run.py:483] Algo bellman_ford step 1162 current loss 0.770095, current_train_items 37216.
I0302 18:58:54.385563 22699590365312 run.py:483] Algo bellman_ford step 1163 current loss 0.781302, current_train_items 37248.
I0302 18:58:54.418182 22699590365312 run.py:483] Algo bellman_ford step 1164 current loss 1.108005, current_train_items 37280.
I0302 18:58:54.437102 22699590365312 run.py:483] Algo bellman_ford step 1165 current loss 0.309734, current_train_items 37312.
I0302 18:58:54.453447 22699590365312 run.py:483] Algo bellman_ford step 1166 current loss 0.577935, current_train_items 37344.
I0302 18:58:54.476477 22699590365312 run.py:483] Algo bellman_ford step 1167 current loss 0.879815, current_train_items 37376.
I0302 18:58:54.506081 22699590365312 run.py:483] Algo bellman_ford step 1168 current loss 0.905765, current_train_items 37408.
I0302 18:58:54.540319 22699590365312 run.py:483] Algo bellman_ford step 1169 current loss 1.033666, current_train_items 37440.
I0302 18:58:54.559874 22699590365312 run.py:483] Algo bellman_ford step 1170 current loss 0.344186, current_train_items 37472.
I0302 18:58:54.576653 22699590365312 run.py:483] Algo bellman_ford step 1171 current loss 0.671459, current_train_items 37504.
I0302 18:58:54.599920 22699590365312 run.py:483] Algo bellman_ford step 1172 current loss 0.858507, current_train_items 37536.
I0302 18:58:54.630066 22699590365312 run.py:483] Algo bellman_ford step 1173 current loss 0.934716, current_train_items 37568.
I0302 18:58:54.662470 22699590365312 run.py:483] Algo bellman_ford step 1174 current loss 1.128343, current_train_items 37600.
I0302 18:58:54.681877 22699590365312 run.py:483] Algo bellman_ford step 1175 current loss 0.346104, current_train_items 37632.
I0302 18:58:54.698163 22699590365312 run.py:483] Algo bellman_ford step 1176 current loss 0.563297, current_train_items 37664.
I0302 18:58:54.720566 22699590365312 run.py:483] Algo bellman_ford step 1177 current loss 0.696593, current_train_items 37696.
I0302 18:58:54.749684 22699590365312 run.py:483] Algo bellman_ford step 1178 current loss 0.782929, current_train_items 37728.
I0302 18:58:54.784024 22699590365312 run.py:483] Algo bellman_ford step 1179 current loss 1.100884, current_train_items 37760.
I0302 18:58:54.802940 22699590365312 run.py:483] Algo bellman_ford step 1180 current loss 0.297673, current_train_items 37792.
I0302 18:58:54.819543 22699590365312 run.py:483] Algo bellman_ford step 1181 current loss 0.730630, current_train_items 37824.
I0302 18:58:54.842586 22699590365312 run.py:483] Algo bellman_ford step 1182 current loss 0.782564, current_train_items 37856.
I0302 18:58:54.870822 22699590365312 run.py:483] Algo bellman_ford step 1183 current loss 0.912261, current_train_items 37888.
I0302 18:58:54.902748 22699590365312 run.py:483] Algo bellman_ford step 1184 current loss 0.981599, current_train_items 37920.
I0302 18:58:54.922319 22699590365312 run.py:483] Algo bellman_ford step 1185 current loss 0.390456, current_train_items 37952.
I0302 18:58:54.938330 22699590365312 run.py:483] Algo bellman_ford step 1186 current loss 0.578505, current_train_items 37984.
I0302 18:58:54.961467 22699590365312 run.py:483] Algo bellman_ford step 1187 current loss 0.815325, current_train_items 38016.
I0302 18:58:54.990521 22699590365312 run.py:483] Algo bellman_ford step 1188 current loss 0.922409, current_train_items 38048.
I0302 18:58:55.021562 22699590365312 run.py:483] Algo bellman_ford step 1189 current loss 1.039932, current_train_items 38080.
I0302 18:58:55.040741 22699590365312 run.py:483] Algo bellman_ford step 1190 current loss 0.332631, current_train_items 38112.
I0302 18:58:55.057107 22699590365312 run.py:483] Algo bellman_ford step 1191 current loss 0.501708, current_train_items 38144.
I0302 18:58:55.081005 22699590365312 run.py:483] Algo bellman_ford step 1192 current loss 0.863199, current_train_items 38176.
I0302 18:58:55.110427 22699590365312 run.py:483] Algo bellman_ford step 1193 current loss 1.029096, current_train_items 38208.
I0302 18:58:55.142849 22699590365312 run.py:483] Algo bellman_ford step 1194 current loss 0.999514, current_train_items 38240.
I0302 18:58:55.161744 22699590365312 run.py:483] Algo bellman_ford step 1195 current loss 0.391682, current_train_items 38272.
I0302 18:58:55.178005 22699590365312 run.py:483] Algo bellman_ford step 1196 current loss 0.602799, current_train_items 38304.
I0302 18:58:55.200814 22699590365312 run.py:483] Algo bellman_ford step 1197 current loss 0.833515, current_train_items 38336.
I0302 18:58:55.231643 22699590365312 run.py:483] Algo bellman_ford step 1198 current loss 0.874334, current_train_items 38368.
I0302 18:58:55.265088 22699590365312 run.py:483] Algo bellman_ford step 1199 current loss 1.151774, current_train_items 38400.
I0302 18:58:55.284412 22699590365312 run.py:483] Algo bellman_ford step 1200 current loss 0.459286, current_train_items 38432.
I0302 18:58:55.293956 22699590365312 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.8876953125, 'score': 0.8876953125, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0302 18:58:55.294066 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.913, current avg val score is 0.888, val scores are: bellman_ford: 0.888
I0302 18:58:55.311087 22699590365312 run.py:483] Algo bellman_ford step 1201 current loss 0.579125, current_train_items 38464.
I0302 18:58:55.334284 22699590365312 run.py:483] Algo bellman_ford step 1202 current loss 0.660733, current_train_items 38496.
I0302 18:58:55.364873 22699590365312 run.py:483] Algo bellman_ford step 1203 current loss 0.829758, current_train_items 38528.
I0302 18:58:55.398466 22699590365312 run.py:483] Algo bellman_ford step 1204 current loss 1.090135, current_train_items 38560.
I0302 18:58:55.417767 22699590365312 run.py:483] Algo bellman_ford step 1205 current loss 0.324869, current_train_items 38592.
I0302 18:58:55.433752 22699590365312 run.py:483] Algo bellman_ford step 1206 current loss 0.492655, current_train_items 38624.
I0302 18:58:55.456178 22699590365312 run.py:483] Algo bellman_ford step 1207 current loss 0.778788, current_train_items 38656.
I0302 18:58:55.485721 22699590365312 run.py:483] Algo bellman_ford step 1208 current loss 0.816096, current_train_items 38688.
I0302 18:58:55.519090 22699590365312 run.py:483] Algo bellman_ford step 1209 current loss 1.037623, current_train_items 38720.
I0302 18:58:55.537881 22699590365312 run.py:483] Algo bellman_ford step 1210 current loss 0.324247, current_train_items 38752.
I0302 18:58:55.554058 22699590365312 run.py:483] Algo bellman_ford step 1211 current loss 0.637851, current_train_items 38784.
I0302 18:58:55.576761 22699590365312 run.py:483] Algo bellman_ford step 1212 current loss 0.817325, current_train_items 38816.
I0302 18:58:55.604938 22699590365312 run.py:483] Algo bellman_ford step 1213 current loss 0.736155, current_train_items 38848.
I0302 18:58:55.636779 22699590365312 run.py:483] Algo bellman_ford step 1214 current loss 0.900707, current_train_items 38880.
I0302 18:58:55.655459 22699590365312 run.py:483] Algo bellman_ford step 1215 current loss 0.324767, current_train_items 38912.
I0302 18:58:55.671239 22699590365312 run.py:483] Algo bellman_ford step 1216 current loss 0.515316, current_train_items 38944.
I0302 18:58:55.694104 22699590365312 run.py:483] Algo bellman_ford step 1217 current loss 0.744232, current_train_items 38976.
I0302 18:58:55.724065 22699590365312 run.py:483] Algo bellman_ford step 1218 current loss 1.033479, current_train_items 39008.
I0302 18:58:55.756389 22699590365312 run.py:483] Algo bellman_ford step 1219 current loss 1.080555, current_train_items 39040.
I0302 18:58:55.775360 22699590365312 run.py:483] Algo bellman_ford step 1220 current loss 0.285904, current_train_items 39072.
I0302 18:58:55.791329 22699590365312 run.py:483] Algo bellman_ford step 1221 current loss 0.543562, current_train_items 39104.
I0302 18:58:55.814953 22699590365312 run.py:483] Algo bellman_ford step 1222 current loss 0.798268, current_train_items 39136.
I0302 18:58:55.844922 22699590365312 run.py:483] Algo bellman_ford step 1223 current loss 0.843866, current_train_items 39168.
I0302 18:58:55.877085 22699590365312 run.py:483] Algo bellman_ford step 1224 current loss 0.940381, current_train_items 39200.
I0302 18:58:55.895845 22699590365312 run.py:483] Algo bellman_ford step 1225 current loss 0.315477, current_train_items 39232.
I0302 18:58:55.912084 22699590365312 run.py:483] Algo bellman_ford step 1226 current loss 0.543500, current_train_items 39264.
I0302 18:58:55.935961 22699590365312 run.py:483] Algo bellman_ford step 1227 current loss 0.710321, current_train_items 39296.
I0302 18:58:55.964730 22699590365312 run.py:483] Algo bellman_ford step 1228 current loss 0.804859, current_train_items 39328.
I0302 18:58:55.997841 22699590365312 run.py:483] Algo bellman_ford step 1229 current loss 1.101477, current_train_items 39360.
I0302 18:58:56.016750 22699590365312 run.py:483] Algo bellman_ford step 1230 current loss 0.296732, current_train_items 39392.
I0302 18:58:56.032993 22699590365312 run.py:483] Algo bellman_ford step 1231 current loss 0.490961, current_train_items 39424.
I0302 18:58:56.056636 22699590365312 run.py:483] Algo bellman_ford step 1232 current loss 0.882005, current_train_items 39456.
I0302 18:58:56.085685 22699590365312 run.py:483] Algo bellman_ford step 1233 current loss 0.895931, current_train_items 39488.
I0302 18:58:56.116456 22699590365312 run.py:483] Algo bellman_ford step 1234 current loss 0.956075, current_train_items 39520.
I0302 18:58:56.135179 22699590365312 run.py:483] Algo bellman_ford step 1235 current loss 0.299149, current_train_items 39552.
I0302 18:58:56.150889 22699590365312 run.py:483] Algo bellman_ford step 1236 current loss 0.512287, current_train_items 39584.
I0302 18:58:56.173463 22699590365312 run.py:483] Algo bellman_ford step 1237 current loss 0.736791, current_train_items 39616.
I0302 18:58:56.202699 22699590365312 run.py:483] Algo bellman_ford step 1238 current loss 0.885904, current_train_items 39648.
I0302 18:58:56.236036 22699590365312 run.py:483] Algo bellman_ford step 1239 current loss 0.989238, current_train_items 39680.
I0302 18:58:56.255129 22699590365312 run.py:483] Algo bellman_ford step 1240 current loss 0.315565, current_train_items 39712.
I0302 18:58:56.271364 22699590365312 run.py:483] Algo bellman_ford step 1241 current loss 0.744335, current_train_items 39744.
I0302 18:58:56.294803 22699590365312 run.py:483] Algo bellman_ford step 1242 current loss 0.838444, current_train_items 39776.
I0302 18:58:56.323899 22699590365312 run.py:483] Algo bellman_ford step 1243 current loss 1.096690, current_train_items 39808.
I0302 18:58:56.356199 22699590365312 run.py:483] Algo bellman_ford step 1244 current loss 1.138875, current_train_items 39840.
I0302 18:58:56.374779 22699590365312 run.py:483] Algo bellman_ford step 1245 current loss 0.311278, current_train_items 39872.
I0302 18:58:56.390803 22699590365312 run.py:483] Algo bellman_ford step 1246 current loss 0.451344, current_train_items 39904.
I0302 18:58:56.413091 22699590365312 run.py:483] Algo bellman_ford step 1247 current loss 0.739153, current_train_items 39936.
I0302 18:58:56.442688 22699590365312 run.py:483] Algo bellman_ford step 1248 current loss 0.898517, current_train_items 39968.
I0302 18:58:56.474056 22699590365312 run.py:483] Algo bellman_ford step 1249 current loss 1.171866, current_train_items 40000.
I0302 18:58:56.492815 22699590365312 run.py:483] Algo bellman_ford step 1250 current loss 0.346693, current_train_items 40032.
I0302 18:58:56.501054 22699590365312 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.8388671875, 'score': 0.8388671875, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0302 18:58:56.501176 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.913, current avg val score is 0.839, val scores are: bellman_ford: 0.839
I0302 18:58:56.517995 22699590365312 run.py:483] Algo bellman_ford step 1251 current loss 0.508468, current_train_items 40064.
I0302 18:58:56.541458 22699590365312 run.py:483] Algo bellman_ford step 1252 current loss 0.774988, current_train_items 40096.
I0302 18:58:56.569503 22699590365312 run.py:483] Algo bellman_ford step 1253 current loss 0.713207, current_train_items 40128.
I0302 18:58:56.599168 22699590365312 run.py:483] Algo bellman_ford step 1254 current loss 0.816827, current_train_items 40160.
I0302 18:58:56.618352 22699590365312 run.py:483] Algo bellman_ford step 1255 current loss 0.330109, current_train_items 40192.
I0302 18:58:56.634559 22699590365312 run.py:483] Algo bellman_ford step 1256 current loss 0.560154, current_train_items 40224.
I0302 18:58:56.657318 22699590365312 run.py:483] Algo bellman_ford step 1257 current loss 0.820896, current_train_items 40256.
I0302 18:58:56.686177 22699590365312 run.py:483] Algo bellman_ford step 1258 current loss 0.790948, current_train_items 40288.
I0302 18:58:56.717411 22699590365312 run.py:483] Algo bellman_ford step 1259 current loss 0.985694, current_train_items 40320.
I0302 18:58:56.736613 22699590365312 run.py:483] Algo bellman_ford step 1260 current loss 0.371100, current_train_items 40352.
I0302 18:58:56.753466 22699590365312 run.py:483] Algo bellman_ford step 1261 current loss 0.567783, current_train_items 40384.
I0302 18:58:56.776397 22699590365312 run.py:483] Algo bellman_ford step 1262 current loss 0.868207, current_train_items 40416.
I0302 18:58:56.803386 22699590365312 run.py:483] Algo bellman_ford step 1263 current loss 0.700289, current_train_items 40448.
I0302 18:58:56.837634 22699590365312 run.py:483] Algo bellman_ford step 1264 current loss 1.154722, current_train_items 40480.
I0302 18:58:56.856468 22699590365312 run.py:483] Algo bellman_ford step 1265 current loss 0.276901, current_train_items 40512.
I0302 18:58:56.872757 22699590365312 run.py:483] Algo bellman_ford step 1266 current loss 0.559169, current_train_items 40544.
I0302 18:58:56.895148 22699590365312 run.py:483] Algo bellman_ford step 1267 current loss 0.883657, current_train_items 40576.
I0302 18:58:56.923560 22699590365312 run.py:483] Algo bellman_ford step 1268 current loss 0.835007, current_train_items 40608.
I0302 18:58:56.956005 22699590365312 run.py:483] Algo bellman_ford step 1269 current loss 0.939463, current_train_items 40640.
I0302 18:58:56.975257 22699590365312 run.py:483] Algo bellman_ford step 1270 current loss 0.286988, current_train_items 40672.
I0302 18:58:56.991242 22699590365312 run.py:483] Algo bellman_ford step 1271 current loss 0.535801, current_train_items 40704.
I0302 18:58:57.012913 22699590365312 run.py:483] Algo bellman_ford step 1272 current loss 0.779966, current_train_items 40736.
I0302 18:58:57.040711 22699590365312 run.py:483] Algo bellman_ford step 1273 current loss 0.718814, current_train_items 40768.
I0302 18:58:57.073198 22699590365312 run.py:483] Algo bellman_ford step 1274 current loss 0.906632, current_train_items 40800.
I0302 18:58:57.092339 22699590365312 run.py:483] Algo bellman_ford step 1275 current loss 0.331610, current_train_items 40832.
I0302 18:58:57.108595 22699590365312 run.py:483] Algo bellman_ford step 1276 current loss 0.600947, current_train_items 40864.
I0302 18:58:57.130289 22699590365312 run.py:483] Algo bellman_ford step 1277 current loss 0.752409, current_train_items 40896.
I0302 18:58:57.159173 22699590365312 run.py:483] Algo bellman_ford step 1278 current loss 0.838536, current_train_items 40928.
I0302 18:58:57.190592 22699590365312 run.py:483] Algo bellman_ford step 1279 current loss 1.094038, current_train_items 40960.
I0302 18:58:57.209432 22699590365312 run.py:483] Algo bellman_ford step 1280 current loss 0.390976, current_train_items 40992.
I0302 18:58:57.225459 22699590365312 run.py:483] Algo bellman_ford step 1281 current loss 0.537347, current_train_items 41024.
I0302 18:58:57.248736 22699590365312 run.py:483] Algo bellman_ford step 1282 current loss 0.748834, current_train_items 41056.
I0302 18:58:57.278633 22699590365312 run.py:483] Algo bellman_ford step 1283 current loss 0.839901, current_train_items 41088.
I0302 18:58:57.309806 22699590365312 run.py:483] Algo bellman_ford step 1284 current loss 0.987113, current_train_items 41120.
I0302 18:58:57.329049 22699590365312 run.py:483] Algo bellman_ford step 1285 current loss 0.456583, current_train_items 41152.
I0302 18:58:57.345287 22699590365312 run.py:483] Algo bellman_ford step 1286 current loss 0.601680, current_train_items 41184.
I0302 18:58:57.369262 22699590365312 run.py:483] Algo bellman_ford step 1287 current loss 0.915872, current_train_items 41216.
I0302 18:58:57.398577 22699590365312 run.py:483] Algo bellman_ford step 1288 current loss 0.932126, current_train_items 41248.
I0302 18:58:57.430647 22699590365312 run.py:483] Algo bellman_ford step 1289 current loss 0.951406, current_train_items 41280.
I0302 18:58:57.450001 22699590365312 run.py:483] Algo bellman_ford step 1290 current loss 0.374850, current_train_items 41312.
I0302 18:58:57.466373 22699590365312 run.py:483] Algo bellman_ford step 1291 current loss 0.723652, current_train_items 41344.
I0302 18:58:57.488574 22699590365312 run.py:483] Algo bellman_ford step 1292 current loss 0.754355, current_train_items 41376.
I0302 18:58:57.517480 22699590365312 run.py:483] Algo bellman_ford step 1293 current loss 0.979604, current_train_items 41408.
I0302 18:58:57.550554 22699590365312 run.py:483] Algo bellman_ford step 1294 current loss 1.068416, current_train_items 41440.
I0302 18:58:57.569317 22699590365312 run.py:483] Algo bellman_ford step 1295 current loss 0.325830, current_train_items 41472.
I0302 18:58:57.585473 22699590365312 run.py:483] Algo bellman_ford step 1296 current loss 0.475500, current_train_items 41504.
I0302 18:58:57.608799 22699590365312 run.py:483] Algo bellman_ford step 1297 current loss 0.887820, current_train_items 41536.
I0302 18:58:57.636305 22699590365312 run.py:483] Algo bellman_ford step 1298 current loss 0.752003, current_train_items 41568.
I0302 18:58:57.666241 22699590365312 run.py:483] Algo bellman_ford step 1299 current loss 0.933653, current_train_items 41600.
I0302 18:58:57.685259 22699590365312 run.py:483] Algo bellman_ford step 1300 current loss 0.433317, current_train_items 41632.
I0302 18:58:57.692891 22699590365312 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0302 18:58:57.692995 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.913, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 18:58:57.709646 22699590365312 run.py:483] Algo bellman_ford step 1301 current loss 0.541455, current_train_items 41664.
I0302 18:58:57.733361 22699590365312 run.py:483] Algo bellman_ford step 1302 current loss 0.815311, current_train_items 41696.
I0302 18:58:57.763492 22699590365312 run.py:483] Algo bellman_ford step 1303 current loss 0.936568, current_train_items 41728.
I0302 18:58:57.794393 22699590365312 run.py:483] Algo bellman_ford step 1304 current loss 1.058768, current_train_items 41760.
I0302 18:58:57.813565 22699590365312 run.py:483] Algo bellman_ford step 1305 current loss 0.258614, current_train_items 41792.
I0302 18:58:57.829469 22699590365312 run.py:483] Algo bellman_ford step 1306 current loss 0.630112, current_train_items 41824.
I0302 18:58:57.851899 22699590365312 run.py:483] Algo bellman_ford step 1307 current loss 0.769991, current_train_items 41856.
I0302 18:58:57.880574 22699590365312 run.py:483] Algo bellman_ford step 1308 current loss 0.910433, current_train_items 41888.
I0302 18:58:57.911977 22699590365312 run.py:483] Algo bellman_ford step 1309 current loss 1.007455, current_train_items 41920.
I0302 18:58:57.930910 22699590365312 run.py:483] Algo bellman_ford step 1310 current loss 0.268010, current_train_items 41952.
I0302 18:58:57.947067 22699590365312 run.py:483] Algo bellman_ford step 1311 current loss 0.568082, current_train_items 41984.
I0302 18:58:57.969336 22699590365312 run.py:483] Algo bellman_ford step 1312 current loss 0.749549, current_train_items 42016.
I0302 18:58:57.998914 22699590365312 run.py:483] Algo bellman_ford step 1313 current loss 0.915830, current_train_items 42048.
I0302 18:58:58.031040 22699590365312 run.py:483] Algo bellman_ford step 1314 current loss 0.896220, current_train_items 42080.
I0302 18:58:58.049653 22699590365312 run.py:483] Algo bellman_ford step 1315 current loss 0.275154, current_train_items 42112.
I0302 18:58:58.065537 22699590365312 run.py:483] Algo bellman_ford step 1316 current loss 0.495801, current_train_items 42144.
I0302 18:58:58.088361 22699590365312 run.py:483] Algo bellman_ford step 1317 current loss 0.762310, current_train_items 42176.
I0302 18:58:58.117443 22699590365312 run.py:483] Algo bellman_ford step 1318 current loss 0.728345, current_train_items 42208.
I0302 18:58:58.148991 22699590365312 run.py:483] Algo bellman_ford step 1319 current loss 0.995980, current_train_items 42240.
I0302 18:58:58.167714 22699590365312 run.py:483] Algo bellman_ford step 1320 current loss 0.302776, current_train_items 42272.
I0302 18:58:58.183885 22699590365312 run.py:483] Algo bellman_ford step 1321 current loss 0.478148, current_train_items 42304.
I0302 18:58:58.207120 22699590365312 run.py:483] Algo bellman_ford step 1322 current loss 1.019966, current_train_items 42336.
I0302 18:58:58.236834 22699590365312 run.py:483] Algo bellman_ford step 1323 current loss 0.913674, current_train_items 42368.
I0302 18:58:58.268038 22699590365312 run.py:483] Algo bellman_ford step 1324 current loss 1.044444, current_train_items 42400.
I0302 18:58:58.287084 22699590365312 run.py:483] Algo bellman_ford step 1325 current loss 0.368893, current_train_items 42432.
I0302 18:58:58.303029 22699590365312 run.py:483] Algo bellman_ford step 1326 current loss 0.464221, current_train_items 42464.
I0302 18:58:58.325219 22699590365312 run.py:483] Algo bellman_ford step 1327 current loss 0.589090, current_train_items 42496.
I0302 18:58:58.354045 22699590365312 run.py:483] Algo bellman_ford step 1328 current loss 0.818544, current_train_items 42528.
I0302 18:58:58.386120 22699590365312 run.py:483] Algo bellman_ford step 1329 current loss 1.209271, current_train_items 42560.
I0302 18:58:58.404705 22699590365312 run.py:483] Algo bellman_ford step 1330 current loss 0.379738, current_train_items 42592.
I0302 18:58:58.420677 22699590365312 run.py:483] Algo bellman_ford step 1331 current loss 0.541570, current_train_items 42624.
I0302 18:58:58.442880 22699590365312 run.py:483] Algo bellman_ford step 1332 current loss 0.738167, current_train_items 42656.
I0302 18:58:58.471513 22699590365312 run.py:483] Algo bellman_ford step 1333 current loss 0.996952, current_train_items 42688.
I0302 18:58:58.502167 22699590365312 run.py:483] Algo bellman_ford step 1334 current loss 0.983224, current_train_items 42720.
I0302 18:58:58.521121 22699590365312 run.py:483] Algo bellman_ford step 1335 current loss 0.352898, current_train_items 42752.
I0302 18:58:58.536979 22699590365312 run.py:483] Algo bellman_ford step 1336 current loss 0.522108, current_train_items 42784.
I0302 18:58:58.560864 22699590365312 run.py:483] Algo bellman_ford step 1337 current loss 0.800196, current_train_items 42816.
I0302 18:58:58.589694 22699590365312 run.py:483] Algo bellman_ford step 1338 current loss 1.136635, current_train_items 42848.
I0302 18:58:58.623815 22699590365312 run.py:483] Algo bellman_ford step 1339 current loss 1.226934, current_train_items 42880.
I0302 18:58:58.642770 22699590365312 run.py:483] Algo bellman_ford step 1340 current loss 0.396978, current_train_items 42912.
I0302 18:58:58.658896 22699590365312 run.py:483] Algo bellman_ford step 1341 current loss 0.607814, current_train_items 42944.
I0302 18:58:58.682199 22699590365312 run.py:483] Algo bellman_ford step 1342 current loss 0.801580, current_train_items 42976.
I0302 18:58:58.711245 22699590365312 run.py:483] Algo bellman_ford step 1343 current loss 0.770884, current_train_items 43008.
I0302 18:58:58.743115 22699590365312 run.py:483] Algo bellman_ford step 1344 current loss 1.005209, current_train_items 43040.
I0302 18:58:58.762287 22699590365312 run.py:483] Algo bellman_ford step 1345 current loss 0.352697, current_train_items 43072.
I0302 18:58:58.778640 22699590365312 run.py:483] Algo bellman_ford step 1346 current loss 0.579603, current_train_items 43104.
I0302 18:58:58.802978 22699590365312 run.py:483] Algo bellman_ford step 1347 current loss 0.868397, current_train_items 43136.
I0302 18:58:58.830902 22699590365312 run.py:483] Algo bellman_ford step 1348 current loss 0.866621, current_train_items 43168.
I0302 18:58:58.861883 22699590365312 run.py:483] Algo bellman_ford step 1349 current loss 1.012183, current_train_items 43200.
I0302 18:58:58.880964 22699590365312 run.py:483] Algo bellman_ford step 1350 current loss 0.330085, current_train_items 43232.
I0302 18:58:58.889489 22699590365312 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.8740234375, 'score': 0.8740234375, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0302 18:58:58.889597 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.913, current avg val score is 0.874, val scores are: bellman_ford: 0.874
I0302 18:58:58.906602 22699590365312 run.py:483] Algo bellman_ford step 1351 current loss 0.597978, current_train_items 43264.
I0302 18:58:58.930893 22699590365312 run.py:483] Algo bellman_ford step 1352 current loss 0.633625, current_train_items 43296.
I0302 18:58:58.959120 22699590365312 run.py:483] Algo bellman_ford step 1353 current loss 0.857731, current_train_items 43328.
I0302 18:58:58.993574 22699590365312 run.py:483] Algo bellman_ford step 1354 current loss 0.988015, current_train_items 43360.
I0302 18:58:59.013078 22699590365312 run.py:483] Algo bellman_ford step 1355 current loss 0.353738, current_train_items 43392.
I0302 18:58:59.029001 22699590365312 run.py:483] Algo bellman_ford step 1356 current loss 0.552215, current_train_items 43424.
I0302 18:58:59.051244 22699590365312 run.py:483] Algo bellman_ford step 1357 current loss 0.756065, current_train_items 43456.
I0302 18:58:59.080044 22699590365312 run.py:483] Algo bellman_ford step 1358 current loss 0.766730, current_train_items 43488.
I0302 18:58:59.113759 22699590365312 run.py:483] Algo bellman_ford step 1359 current loss 1.012080, current_train_items 43520.
I0302 18:58:59.133062 22699590365312 run.py:483] Algo bellman_ford step 1360 current loss 0.367378, current_train_items 43552.
I0302 18:58:59.149917 22699590365312 run.py:483] Algo bellman_ford step 1361 current loss 0.616058, current_train_items 43584.
I0302 18:58:59.173429 22699590365312 run.py:483] Algo bellman_ford step 1362 current loss 0.724731, current_train_items 43616.
I0302 18:58:59.202476 22699590365312 run.py:483] Algo bellman_ford step 1363 current loss 0.901316, current_train_items 43648.
I0302 18:58:59.235846 22699590365312 run.py:483] Algo bellman_ford step 1364 current loss 0.950816, current_train_items 43680.
I0302 18:58:59.254958 22699590365312 run.py:483] Algo bellman_ford step 1365 current loss 0.349341, current_train_items 43712.
I0302 18:58:59.270860 22699590365312 run.py:483] Algo bellman_ford step 1366 current loss 0.521534, current_train_items 43744.
I0302 18:58:59.293058 22699590365312 run.py:483] Algo bellman_ford step 1367 current loss 0.658027, current_train_items 43776.
I0302 18:58:59.320446 22699590365312 run.py:483] Algo bellman_ford step 1368 current loss 0.786112, current_train_items 43808.
I0302 18:58:59.353054 22699590365312 run.py:483] Algo bellman_ford step 1369 current loss 0.968024, current_train_items 43840.
I0302 18:58:59.372590 22699590365312 run.py:483] Algo bellman_ford step 1370 current loss 0.334434, current_train_items 43872.
I0302 18:58:59.389177 22699590365312 run.py:483] Algo bellman_ford step 1371 current loss 0.578037, current_train_items 43904.
I0302 18:58:59.412325 22699590365312 run.py:483] Algo bellman_ford step 1372 current loss 0.699775, current_train_items 43936.
I0302 18:58:59.442497 22699590365312 run.py:483] Algo bellman_ford step 1373 current loss 0.839117, current_train_items 43968.
I0302 18:58:59.475236 22699590365312 run.py:483] Algo bellman_ford step 1374 current loss 0.977986, current_train_items 44000.
I0302 18:58:59.494575 22699590365312 run.py:483] Algo bellman_ford step 1375 current loss 0.323635, current_train_items 44032.
I0302 18:58:59.510385 22699590365312 run.py:483] Algo bellman_ford step 1376 current loss 0.512230, current_train_items 44064.
I0302 18:58:59.533243 22699590365312 run.py:483] Algo bellman_ford step 1377 current loss 0.722841, current_train_items 44096.
I0302 18:58:59.562813 22699590365312 run.py:483] Algo bellman_ford step 1378 current loss 0.795366, current_train_items 44128.
I0302 18:58:59.595011 22699590365312 run.py:483] Algo bellman_ford step 1379 current loss 0.962067, current_train_items 44160.
I0302 18:58:59.614303 22699590365312 run.py:483] Algo bellman_ford step 1380 current loss 0.387052, current_train_items 44192.
I0302 18:58:59.630788 22699590365312 run.py:483] Algo bellman_ford step 1381 current loss 0.585220, current_train_items 44224.
I0302 18:58:59.653413 22699590365312 run.py:483] Algo bellman_ford step 1382 current loss 0.704965, current_train_items 44256.
I0302 18:58:59.682817 22699590365312 run.py:483] Algo bellman_ford step 1383 current loss 0.790493, current_train_items 44288.
I0302 18:58:59.712392 22699590365312 run.py:483] Algo bellman_ford step 1384 current loss 0.855601, current_train_items 44320.
I0302 18:58:59.731456 22699590365312 run.py:483] Algo bellman_ford step 1385 current loss 0.263404, current_train_items 44352.
I0302 18:58:59.747210 22699590365312 run.py:483] Algo bellman_ford step 1386 current loss 0.601121, current_train_items 44384.
I0302 18:58:59.768826 22699590365312 run.py:483] Algo bellman_ford step 1387 current loss 0.571632, current_train_items 44416.
I0302 18:58:59.798427 22699590365312 run.py:483] Algo bellman_ford step 1388 current loss 0.838259, current_train_items 44448.
I0302 18:58:59.830427 22699590365312 run.py:483] Algo bellman_ford step 1389 current loss 1.007037, current_train_items 44480.
I0302 18:58:59.849552 22699590365312 run.py:483] Algo bellman_ford step 1390 current loss 0.313474, current_train_items 44512.
I0302 18:58:59.865437 22699590365312 run.py:483] Algo bellman_ford step 1391 current loss 0.481324, current_train_items 44544.
I0302 18:58:59.888849 22699590365312 run.py:483] Algo bellman_ford step 1392 current loss 0.745784, current_train_items 44576.
I0302 18:58:59.918398 22699590365312 run.py:483] Algo bellman_ford step 1393 current loss 0.804997, current_train_items 44608.
I0302 18:58:59.950296 22699590365312 run.py:483] Algo bellman_ford step 1394 current loss 1.025109, current_train_items 44640.
I0302 18:58:59.968923 22699590365312 run.py:483] Algo bellman_ford step 1395 current loss 0.278862, current_train_items 44672.
I0302 18:58:59.985063 22699590365312 run.py:483] Algo bellman_ford step 1396 current loss 0.499059, current_train_items 44704.
I0302 18:59:00.008030 22699590365312 run.py:483] Algo bellman_ford step 1397 current loss 0.822509, current_train_items 44736.
I0302 18:59:00.038343 22699590365312 run.py:483] Algo bellman_ford step 1398 current loss 0.960323, current_train_items 44768.
I0302 18:59:00.068992 22699590365312 run.py:483] Algo bellman_ford step 1399 current loss 0.975446, current_train_items 44800.
I0302 18:59:00.088428 22699590365312 run.py:483] Algo bellman_ford step 1400 current loss 0.391331, current_train_items 44832.
I0302 18:59:00.096393 22699590365312 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0302 18:59:00.096497 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.913, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 18:59:00.112921 22699590365312 run.py:483] Algo bellman_ford step 1401 current loss 0.476909, current_train_items 44864.
I0302 18:59:00.136675 22699590365312 run.py:483] Algo bellman_ford step 1402 current loss 0.779115, current_train_items 44896.
I0302 18:59:00.167191 22699590365312 run.py:483] Algo bellman_ford step 1403 current loss 1.101016, current_train_items 44928.
I0302 18:59:00.200600 22699590365312 run.py:483] Algo bellman_ford step 1404 current loss 1.118215, current_train_items 44960.
I0302 18:59:00.219896 22699590365312 run.py:483] Algo bellman_ford step 1405 current loss 0.389182, current_train_items 44992.
I0302 18:59:00.236152 22699590365312 run.py:483] Algo bellman_ford step 1406 current loss 0.567717, current_train_items 45024.
I0302 18:59:00.259181 22699590365312 run.py:483] Algo bellman_ford step 1407 current loss 0.868879, current_train_items 45056.
I0302 18:59:00.289189 22699590365312 run.py:483] Algo bellman_ford step 1408 current loss 1.064437, current_train_items 45088.
I0302 18:59:00.321192 22699590365312 run.py:483] Algo bellman_ford step 1409 current loss 1.003687, current_train_items 45120.
I0302 18:59:00.340006 22699590365312 run.py:483] Algo bellman_ford step 1410 current loss 0.360976, current_train_items 45152.
I0302 18:59:00.356569 22699590365312 run.py:483] Algo bellman_ford step 1411 current loss 0.532424, current_train_items 45184.
I0302 18:59:00.379555 22699590365312 run.py:483] Algo bellman_ford step 1412 current loss 0.925635, current_train_items 45216.
I0302 18:59:00.406842 22699590365312 run.py:483] Algo bellman_ford step 1413 current loss 0.796657, current_train_items 45248.
I0302 18:59:00.437947 22699590365312 run.py:483] Algo bellman_ford step 1414 current loss 1.162051, current_train_items 45280.
I0302 18:59:00.456769 22699590365312 run.py:483] Algo bellman_ford step 1415 current loss 0.289188, current_train_items 45312.
I0302 18:59:00.473331 22699590365312 run.py:483] Algo bellman_ford step 1416 current loss 0.697460, current_train_items 45344.
I0302 18:59:00.495509 22699590365312 run.py:483] Algo bellman_ford step 1417 current loss 0.785257, current_train_items 45376.
I0302 18:59:00.523723 22699590365312 run.py:483] Algo bellman_ford step 1418 current loss 1.177129, current_train_items 45408.
I0302 18:59:00.556304 22699590365312 run.py:483] Algo bellman_ford step 1419 current loss 1.235662, current_train_items 45440.
I0302 18:59:00.575544 22699590365312 run.py:483] Algo bellman_ford step 1420 current loss 0.411102, current_train_items 45472.
I0302 18:59:00.591755 22699590365312 run.py:483] Algo bellman_ford step 1421 current loss 0.629602, current_train_items 45504.
I0302 18:59:00.615447 22699590365312 run.py:483] Algo bellman_ford step 1422 current loss 0.757596, current_train_items 45536.
I0302 18:59:00.645468 22699590365312 run.py:483] Algo bellman_ford step 1423 current loss 0.921440, current_train_items 45568.
I0302 18:59:00.676613 22699590365312 run.py:483] Algo bellman_ford step 1424 current loss 0.972400, current_train_items 45600.
I0302 18:59:00.695550 22699590365312 run.py:483] Algo bellman_ford step 1425 current loss 0.331541, current_train_items 45632.
I0302 18:59:00.712212 22699590365312 run.py:483] Algo bellman_ford step 1426 current loss 0.582087, current_train_items 45664.
I0302 18:59:00.735926 22699590365312 run.py:483] Algo bellman_ford step 1427 current loss 0.816032, current_train_items 45696.
I0302 18:59:00.765242 22699590365312 run.py:483] Algo bellman_ford step 1428 current loss 0.874744, current_train_items 45728.
I0302 18:59:00.795844 22699590365312 run.py:483] Algo bellman_ford step 1429 current loss 0.845139, current_train_items 45760.
I0302 18:59:00.814628 22699590365312 run.py:483] Algo bellman_ford step 1430 current loss 0.330042, current_train_items 45792.
I0302 18:59:00.830508 22699590365312 run.py:483] Algo bellman_ford step 1431 current loss 0.566567, current_train_items 45824.
I0302 18:59:00.854765 22699590365312 run.py:483] Algo bellman_ford step 1432 current loss 0.820316, current_train_items 45856.
I0302 18:59:00.882323 22699590365312 run.py:483] Algo bellman_ford step 1433 current loss 0.748103, current_train_items 45888.
I0302 18:59:00.912794 22699590365312 run.py:483] Algo bellman_ford step 1434 current loss 0.854752, current_train_items 45920.
I0302 18:59:00.931307 22699590365312 run.py:483] Algo bellman_ford step 1435 current loss 0.343024, current_train_items 45952.
I0302 18:59:00.947371 22699590365312 run.py:483] Algo bellman_ford step 1436 current loss 0.661038, current_train_items 45984.
I0302 18:59:00.969865 22699590365312 run.py:483] Algo bellman_ford step 1437 current loss 0.798162, current_train_items 46016.
I0302 18:59:00.998169 22699590365312 run.py:483] Algo bellman_ford step 1438 current loss 0.757274, current_train_items 46048.
I0302 18:59:01.030961 22699590365312 run.py:483] Algo bellman_ford step 1439 current loss 1.018888, current_train_items 46080.
I0302 18:59:01.049790 22699590365312 run.py:483] Algo bellman_ford step 1440 current loss 0.368297, current_train_items 46112.
I0302 18:59:01.065962 22699590365312 run.py:483] Algo bellman_ford step 1441 current loss 0.702182, current_train_items 46144.
I0302 18:59:01.088766 22699590365312 run.py:483] Algo bellman_ford step 1442 current loss 0.753110, current_train_items 46176.
I0302 18:59:01.118234 22699590365312 run.py:483] Algo bellman_ford step 1443 current loss 0.786762, current_train_items 46208.
I0302 18:59:01.154063 22699590365312 run.py:483] Algo bellman_ford step 1444 current loss 1.181187, current_train_items 46240.
I0302 18:59:01.173055 22699590365312 run.py:483] Algo bellman_ford step 1445 current loss 0.354323, current_train_items 46272.
I0302 18:59:01.188678 22699590365312 run.py:483] Algo bellman_ford step 1446 current loss 0.538776, current_train_items 46304.
I0302 18:59:01.211880 22699590365312 run.py:483] Algo bellman_ford step 1447 current loss 0.795247, current_train_items 46336.
I0302 18:59:01.240978 22699590365312 run.py:483] Algo bellman_ford step 1448 current loss 0.898052, current_train_items 46368.
I0302 18:59:01.271922 22699590365312 run.py:483] Algo bellman_ford step 1449 current loss 1.030232, current_train_items 46400.
I0302 18:59:01.290764 22699590365312 run.py:483] Algo bellman_ford step 1450 current loss 0.348679, current_train_items 46432.
I0302 18:59:01.298997 22699590365312 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.8681640625, 'score': 0.8681640625, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0302 18:59:01.299105 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.913, current avg val score is 0.868, val scores are: bellman_ford: 0.868
I0302 18:59:01.315509 22699590365312 run.py:483] Algo bellman_ford step 1451 current loss 0.565215, current_train_items 46464.
I0302 18:59:01.338783 22699590365312 run.py:483] Algo bellman_ford step 1452 current loss 0.606138, current_train_items 46496.
I0302 18:59:01.369323 22699590365312 run.py:483] Algo bellman_ford step 1453 current loss 0.868424, current_train_items 46528.
I0302 18:59:01.403310 22699590365312 run.py:483] Algo bellman_ford step 1454 current loss 1.059571, current_train_items 46560.
I0302 18:59:01.422611 22699590365312 run.py:483] Algo bellman_ford step 1455 current loss 0.383295, current_train_items 46592.
I0302 18:59:01.438560 22699590365312 run.py:483] Algo bellman_ford step 1456 current loss 0.535586, current_train_items 46624.
I0302 18:59:01.460800 22699590365312 run.py:483] Algo bellman_ford step 1457 current loss 0.558919, current_train_items 46656.
I0302 18:59:01.490681 22699590365312 run.py:483] Algo bellman_ford step 1458 current loss 0.885469, current_train_items 46688.
I0302 18:59:01.522624 22699590365312 run.py:483] Algo bellman_ford step 1459 current loss 0.898800, current_train_items 46720.
I0302 18:59:01.541759 22699590365312 run.py:483] Algo bellman_ford step 1460 current loss 0.347860, current_train_items 46752.
I0302 18:59:01.557517 22699590365312 run.py:483] Algo bellman_ford step 1461 current loss 0.622598, current_train_items 46784.
I0302 18:59:01.579616 22699590365312 run.py:483] Algo bellman_ford step 1462 current loss 0.697901, current_train_items 46816.
I0302 18:59:01.608483 22699590365312 run.py:483] Algo bellman_ford step 1463 current loss 0.830767, current_train_items 46848.
I0302 18:59:01.639503 22699590365312 run.py:483] Algo bellman_ford step 1464 current loss 0.939452, current_train_items 46880.
I0302 18:59:01.658552 22699590365312 run.py:483] Algo bellman_ford step 1465 current loss 0.375549, current_train_items 46912.
I0302 18:59:01.674911 22699590365312 run.py:483] Algo bellman_ford step 1466 current loss 0.492395, current_train_items 46944.
I0302 18:59:01.697722 22699590365312 run.py:483] Algo bellman_ford step 1467 current loss 0.604956, current_train_items 46976.
I0302 18:59:01.725094 22699590365312 run.py:483] Algo bellman_ford step 1468 current loss 0.754418, current_train_items 47008.
I0302 18:59:01.756028 22699590365312 run.py:483] Algo bellman_ford step 1469 current loss 0.892171, current_train_items 47040.
I0302 18:59:01.775374 22699590365312 run.py:483] Algo bellman_ford step 1470 current loss 0.300995, current_train_items 47072.
I0302 18:59:01.791981 22699590365312 run.py:483] Algo bellman_ford step 1471 current loss 0.592290, current_train_items 47104.
I0302 18:59:01.814400 22699590365312 run.py:483] Algo bellman_ford step 1472 current loss 0.703454, current_train_items 47136.
I0302 18:59:01.843507 22699590365312 run.py:483] Algo bellman_ford step 1473 current loss 0.891833, current_train_items 47168.
I0302 18:59:01.876099 22699590365312 run.py:483] Algo bellman_ford step 1474 current loss 0.971318, current_train_items 47200.
I0302 18:59:01.895460 22699590365312 run.py:483] Algo bellman_ford step 1475 current loss 0.413594, current_train_items 47232.
I0302 18:59:01.911480 22699590365312 run.py:483] Algo bellman_ford step 1476 current loss 0.482333, current_train_items 47264.
I0302 18:59:01.934172 22699590365312 run.py:483] Algo bellman_ford step 1477 current loss 0.650370, current_train_items 47296.
I0302 18:59:01.962840 22699590365312 run.py:483] Algo bellman_ford step 1478 current loss 0.892748, current_train_items 47328.
I0302 18:59:01.996254 22699590365312 run.py:483] Algo bellman_ford step 1479 current loss 0.984508, current_train_items 47360.
I0302 18:59:02.014960 22699590365312 run.py:483] Algo bellman_ford step 1480 current loss 0.328325, current_train_items 47392.
I0302 18:59:02.031088 22699590365312 run.py:483] Algo bellman_ford step 1481 current loss 0.532151, current_train_items 47424.
I0302 18:59:02.053145 22699590365312 run.py:483] Algo bellman_ford step 1482 current loss 0.666627, current_train_items 47456.
I0302 18:59:02.082329 22699590365312 run.py:483] Algo bellman_ford step 1483 current loss 0.754019, current_train_items 47488.
I0302 18:59:02.112578 22699590365312 run.py:483] Algo bellman_ford step 1484 current loss 0.816211, current_train_items 47520.
I0302 18:59:02.132102 22699590365312 run.py:483] Algo bellman_ford step 1485 current loss 0.376234, current_train_items 47552.
I0302 18:59:02.147799 22699590365312 run.py:483] Algo bellman_ford step 1486 current loss 0.559253, current_train_items 47584.
I0302 18:59:02.171188 22699590365312 run.py:483] Algo bellman_ford step 1487 current loss 0.733831, current_train_items 47616.
I0302 18:59:02.198614 22699590365312 run.py:483] Algo bellman_ford step 1488 current loss 0.724540, current_train_items 47648.
I0302 18:59:02.231130 22699590365312 run.py:483] Algo bellman_ford step 1489 current loss 0.966034, current_train_items 47680.
I0302 18:59:02.250064 22699590365312 run.py:483] Algo bellman_ford step 1490 current loss 0.376652, current_train_items 47712.
I0302 18:59:02.266280 22699590365312 run.py:483] Algo bellman_ford step 1491 current loss 0.551238, current_train_items 47744.
I0302 18:59:02.289063 22699590365312 run.py:483] Algo bellman_ford step 1492 current loss 0.662566, current_train_items 47776.
I0302 18:59:02.317985 22699590365312 run.py:483] Algo bellman_ford step 1493 current loss 0.741471, current_train_items 47808.
I0302 18:59:02.350980 22699590365312 run.py:483] Algo bellman_ford step 1494 current loss 0.976462, current_train_items 47840.
I0302 18:59:02.369930 22699590365312 run.py:483] Algo bellman_ford step 1495 current loss 0.342587, current_train_items 47872.
I0302 18:59:02.386385 22699590365312 run.py:483] Algo bellman_ford step 1496 current loss 0.660265, current_train_items 47904.
I0302 18:59:02.409711 22699590365312 run.py:483] Algo bellman_ford step 1497 current loss 0.825857, current_train_items 47936.
I0302 18:59:02.439666 22699590365312 run.py:483] Algo bellman_ford step 1498 current loss 0.871993, current_train_items 47968.
I0302 18:59:02.473327 22699590365312 run.py:483] Algo bellman_ford step 1499 current loss 0.994045, current_train_items 48000.
I0302 18:59:02.492430 22699590365312 run.py:483] Algo bellman_ford step 1500 current loss 0.299076, current_train_items 48032.
I0302 18:59:02.500582 22699590365312 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0302 18:59:02.500721 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.913, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:59:02.517630 22699590365312 run.py:483] Algo bellman_ford step 1501 current loss 0.592701, current_train_items 48064.
I0302 18:59:02.541576 22699590365312 run.py:483] Algo bellman_ford step 1502 current loss 0.985130, current_train_items 48096.
I0302 18:59:02.570786 22699590365312 run.py:483] Algo bellman_ford step 1503 current loss 0.975876, current_train_items 48128.
I0302 18:59:02.602983 22699590365312 run.py:483] Algo bellman_ford step 1504 current loss 0.978816, current_train_items 48160.
I0302 18:59:02.622648 22699590365312 run.py:483] Algo bellman_ford step 1505 current loss 0.296348, current_train_items 48192.
I0302 18:59:02.638664 22699590365312 run.py:483] Algo bellman_ford step 1506 current loss 0.509117, current_train_items 48224.
I0302 18:59:02.661372 22699590365312 run.py:483] Algo bellman_ford step 1507 current loss 0.749388, current_train_items 48256.
I0302 18:59:02.690142 22699590365312 run.py:483] Algo bellman_ford step 1508 current loss 0.875470, current_train_items 48288.
I0302 18:59:02.722264 22699590365312 run.py:483] Algo bellman_ford step 1509 current loss 0.980466, current_train_items 48320.
I0302 18:59:02.741731 22699590365312 run.py:483] Algo bellman_ford step 1510 current loss 0.342244, current_train_items 48352.
I0302 18:59:02.758067 22699590365312 run.py:483] Algo bellman_ford step 1511 current loss 0.624976, current_train_items 48384.
I0302 18:59:02.781647 22699590365312 run.py:483] Algo bellman_ford step 1512 current loss 0.733257, current_train_items 48416.
I0302 18:59:02.809494 22699590365312 run.py:483] Algo bellman_ford step 1513 current loss 0.735298, current_train_items 48448.
I0302 18:59:02.843498 22699590365312 run.py:483] Algo bellman_ford step 1514 current loss 1.051363, current_train_items 48480.
I0302 18:59:02.862673 22699590365312 run.py:483] Algo bellman_ford step 1515 current loss 0.288334, current_train_items 48512.
I0302 18:59:02.878546 22699590365312 run.py:483] Algo bellman_ford step 1516 current loss 0.593033, current_train_items 48544.
I0302 18:59:02.901457 22699590365312 run.py:483] Algo bellman_ford step 1517 current loss 0.855564, current_train_items 48576.
I0302 18:59:02.931250 22699590365312 run.py:483] Algo bellman_ford step 1518 current loss 0.905178, current_train_items 48608.
I0302 18:59:02.963291 22699590365312 run.py:483] Algo bellman_ford step 1519 current loss 1.012635, current_train_items 48640.
I0302 18:59:02.982446 22699590365312 run.py:483] Algo bellman_ford step 1520 current loss 0.339014, current_train_items 48672.
I0302 18:59:02.998327 22699590365312 run.py:483] Algo bellman_ford step 1521 current loss 0.651385, current_train_items 48704.
I0302 18:59:03.022307 22699590365312 run.py:483] Algo bellman_ford step 1522 current loss 0.723033, current_train_items 48736.
I0302 18:59:03.051817 22699590365312 run.py:483] Algo bellman_ford step 1523 current loss 0.916701, current_train_items 48768.
I0302 18:59:03.084013 22699590365312 run.py:483] Algo bellman_ford step 1524 current loss 1.128286, current_train_items 48800.
I0302 18:59:03.102971 22699590365312 run.py:483] Algo bellman_ford step 1525 current loss 0.462358, current_train_items 48832.
I0302 18:59:03.118708 22699590365312 run.py:483] Algo bellman_ford step 1526 current loss 0.556504, current_train_items 48864.
I0302 18:59:03.142848 22699590365312 run.py:483] Algo bellman_ford step 1527 current loss 0.939907, current_train_items 48896.
I0302 18:59:03.171971 22699590365312 run.py:483] Algo bellman_ford step 1528 current loss 0.986802, current_train_items 48928.
I0302 18:59:03.204784 22699590365312 run.py:483] Algo bellman_ford step 1529 current loss 1.145041, current_train_items 48960.
I0302 18:59:03.223826 22699590365312 run.py:483] Algo bellman_ford step 1530 current loss 0.355417, current_train_items 48992.
I0302 18:59:03.239693 22699590365312 run.py:483] Algo bellman_ford step 1531 current loss 0.573501, current_train_items 49024.
I0302 18:59:03.262895 22699590365312 run.py:483] Algo bellman_ford step 1532 current loss 0.727518, current_train_items 49056.
I0302 18:59:03.292809 22699590365312 run.py:483] Algo bellman_ford step 1533 current loss 0.840741, current_train_items 49088.
I0302 18:59:03.325769 22699590365312 run.py:483] Algo bellman_ford step 1534 current loss 1.062074, current_train_items 49120.
I0302 18:59:03.345178 22699590365312 run.py:483] Algo bellman_ford step 1535 current loss 0.438823, current_train_items 49152.
I0302 18:59:03.361712 22699590365312 run.py:483] Algo bellman_ford step 1536 current loss 0.563494, current_train_items 49184.
I0302 18:59:03.384926 22699590365312 run.py:483] Algo bellman_ford step 1537 current loss 0.791126, current_train_items 49216.
I0302 18:59:03.414851 22699590365312 run.py:483] Algo bellman_ford step 1538 current loss 0.915460, current_train_items 49248.
I0302 18:59:03.449855 22699590365312 run.py:483] Algo bellman_ford step 1539 current loss 1.198376, current_train_items 49280.
I0302 18:59:03.468974 22699590365312 run.py:483] Algo bellman_ford step 1540 current loss 0.442944, current_train_items 49312.
I0302 18:59:03.484834 22699590365312 run.py:483] Algo bellman_ford step 1541 current loss 0.562341, current_train_items 49344.
I0302 18:59:03.507811 22699590365312 run.py:483] Algo bellman_ford step 1542 current loss 0.700541, current_train_items 49376.
I0302 18:59:03.536514 22699590365312 run.py:483] Algo bellman_ford step 1543 current loss 0.860839, current_train_items 49408.
I0302 18:59:03.568826 22699590365312 run.py:483] Algo bellman_ford step 1544 current loss 1.067589, current_train_items 49440.
I0302 18:59:03.587954 22699590365312 run.py:483] Algo bellman_ford step 1545 current loss 0.341528, current_train_items 49472.
I0302 18:59:03.603965 22699590365312 run.py:483] Algo bellman_ford step 1546 current loss 0.625285, current_train_items 49504.
I0302 18:59:03.627249 22699590365312 run.py:483] Algo bellman_ford step 1547 current loss 0.733102, current_train_items 49536.
I0302 18:59:03.657412 22699590365312 run.py:483] Algo bellman_ford step 1548 current loss 0.919653, current_train_items 49568.
I0302 18:59:03.691130 22699590365312 run.py:483] Algo bellman_ford step 1549 current loss 1.113579, current_train_items 49600.
I0302 18:59:03.710016 22699590365312 run.py:483] Algo bellman_ford step 1550 current loss 0.364384, current_train_items 49632.
I0302 18:59:03.718380 22699590365312 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.9404296875, 'score': 0.9404296875, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0302 18:59:03.718486 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.913, current avg val score is 0.940, val scores are: bellman_ford: 0.940
I0302 18:59:03.748522 22699590365312 run.py:483] Algo bellman_ford step 1551 current loss 0.541850, current_train_items 49664.
I0302 18:59:03.771412 22699590365312 run.py:483] Algo bellman_ford step 1552 current loss 0.677899, current_train_items 49696.
I0302 18:59:03.801648 22699590365312 run.py:483] Algo bellman_ford step 1553 current loss 0.731011, current_train_items 49728.
I0302 18:59:03.833402 22699590365312 run.py:483] Algo bellman_ford step 1554 current loss 0.883124, current_train_items 49760.
I0302 18:59:03.853273 22699590365312 run.py:483] Algo bellman_ford step 1555 current loss 0.293635, current_train_items 49792.
I0302 18:59:03.869379 22699590365312 run.py:483] Algo bellman_ford step 1556 current loss 0.589327, current_train_items 49824.
I0302 18:59:03.893202 22699590365312 run.py:483] Algo bellman_ford step 1557 current loss 0.903088, current_train_items 49856.
I0302 18:59:03.923275 22699590365312 run.py:483] Algo bellman_ford step 1558 current loss 0.819616, current_train_items 49888.
I0302 18:59:03.956572 22699590365312 run.py:483] Algo bellman_ford step 1559 current loss 1.018320, current_train_items 49920.
I0302 18:59:03.976225 22699590365312 run.py:483] Algo bellman_ford step 1560 current loss 0.320089, current_train_items 49952.
I0302 18:59:03.992682 22699590365312 run.py:483] Algo bellman_ford step 1561 current loss 0.548097, current_train_items 49984.
I0302 18:59:04.015853 22699590365312 run.py:483] Algo bellman_ford step 1562 current loss 0.639312, current_train_items 50016.
I0302 18:59:04.045876 22699590365312 run.py:483] Algo bellman_ford step 1563 current loss 0.800649, current_train_items 50048.
I0302 18:59:04.079336 22699590365312 run.py:483] Algo bellman_ford step 1564 current loss 0.891788, current_train_items 50080.
I0302 18:59:04.098176 22699590365312 run.py:483] Algo bellman_ford step 1565 current loss 0.300741, current_train_items 50112.
I0302 18:59:04.114514 22699590365312 run.py:483] Algo bellman_ford step 1566 current loss 0.592460, current_train_items 50144.
I0302 18:59:04.138037 22699590365312 run.py:483] Algo bellman_ford step 1567 current loss 0.683519, current_train_items 50176.
I0302 18:59:04.166769 22699590365312 run.py:483] Algo bellman_ford step 1568 current loss 0.842304, current_train_items 50208.
I0302 18:59:04.200295 22699590365312 run.py:483] Algo bellman_ford step 1569 current loss 0.968762, current_train_items 50240.
I0302 18:59:04.219729 22699590365312 run.py:483] Algo bellman_ford step 1570 current loss 0.337107, current_train_items 50272.
I0302 18:59:04.236138 22699590365312 run.py:483] Algo bellman_ford step 1571 current loss 0.495557, current_train_items 50304.
I0302 18:59:04.260049 22699590365312 run.py:483] Algo bellman_ford step 1572 current loss 0.722021, current_train_items 50336.
I0302 18:59:04.289304 22699590365312 run.py:483] Algo bellman_ford step 1573 current loss 0.770458, current_train_items 50368.
I0302 18:59:04.319266 22699590365312 run.py:483] Algo bellman_ford step 1574 current loss 0.949708, current_train_items 50400.
I0302 18:59:04.338703 22699590365312 run.py:483] Algo bellman_ford step 1575 current loss 0.351420, current_train_items 50432.
I0302 18:59:04.354904 22699590365312 run.py:483] Algo bellman_ford step 1576 current loss 0.536141, current_train_items 50464.
I0302 18:59:04.376703 22699590365312 run.py:483] Algo bellman_ford step 1577 current loss 0.678428, current_train_items 50496.
I0302 18:59:04.406976 22699590365312 run.py:483] Algo bellman_ford step 1578 current loss 1.222858, current_train_items 50528.
I0302 18:59:04.439060 22699590365312 run.py:483] Algo bellman_ford step 1579 current loss 1.008776, current_train_items 50560.
I0302 18:59:04.458234 22699590365312 run.py:483] Algo bellman_ford step 1580 current loss 0.447332, current_train_items 50592.
I0302 18:59:04.474132 22699590365312 run.py:483] Algo bellman_ford step 1581 current loss 0.589936, current_train_items 50624.
I0302 18:59:04.497179 22699590365312 run.py:483] Algo bellman_ford step 1582 current loss 0.580019, current_train_items 50656.
I0302 18:59:04.526615 22699590365312 run.py:483] Algo bellman_ford step 1583 current loss 0.882703, current_train_items 50688.
I0302 18:59:04.559656 22699590365312 run.py:483] Algo bellman_ford step 1584 current loss 1.066887, current_train_items 50720.
I0302 18:59:04.578836 22699590365312 run.py:483] Algo bellman_ford step 1585 current loss 0.330044, current_train_items 50752.
I0302 18:59:04.594564 22699590365312 run.py:483] Algo bellman_ford step 1586 current loss 0.576450, current_train_items 50784.
I0302 18:59:04.617908 22699590365312 run.py:483] Algo bellman_ford step 1587 current loss 0.661122, current_train_items 50816.
I0302 18:59:04.647036 22699590365312 run.py:483] Algo bellman_ford step 1588 current loss 0.826307, current_train_items 50848.
I0302 18:59:04.678785 22699590365312 run.py:483] Algo bellman_ford step 1589 current loss 0.944546, current_train_items 50880.
I0302 18:59:04.698204 22699590365312 run.py:483] Algo bellman_ford step 1590 current loss 0.310606, current_train_items 50912.
I0302 18:59:04.714465 22699590365312 run.py:483] Algo bellman_ford step 1591 current loss 0.475872, current_train_items 50944.
I0302 18:59:04.736629 22699590365312 run.py:483] Algo bellman_ford step 1592 current loss 0.606456, current_train_items 50976.
I0302 18:59:04.767099 22699590365312 run.py:483] Algo bellman_ford step 1593 current loss 0.844682, current_train_items 51008.
I0302 18:59:04.800832 22699590365312 run.py:483] Algo bellman_ford step 1594 current loss 0.968149, current_train_items 51040.
I0302 18:59:04.820048 22699590365312 run.py:483] Algo bellman_ford step 1595 current loss 0.363636, current_train_items 51072.
I0302 18:59:04.835722 22699590365312 run.py:483] Algo bellman_ford step 1596 current loss 0.612689, current_train_items 51104.
I0302 18:59:04.859772 22699590365312 run.py:483] Algo bellman_ford step 1597 current loss 0.823547, current_train_items 51136.
I0302 18:59:04.888322 22699590365312 run.py:483] Algo bellman_ford step 1598 current loss 0.716828, current_train_items 51168.
I0302 18:59:04.921274 22699590365312 run.py:483] Algo bellman_ford step 1599 current loss 0.926592, current_train_items 51200.
I0302 18:59:04.940518 22699590365312 run.py:483] Algo bellman_ford step 1600 current loss 0.310752, current_train_items 51232.
I0302 18:59:04.948432 22699590365312 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0302 18:59:04.948541 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 18:59:04.965065 22699590365312 run.py:483] Algo bellman_ford step 1601 current loss 0.501502, current_train_items 51264.
I0302 18:59:04.989451 22699590365312 run.py:483] Algo bellman_ford step 1602 current loss 0.749971, current_train_items 51296.
I0302 18:59:05.019186 22699590365312 run.py:483] Algo bellman_ford step 1603 current loss 0.848243, current_train_items 51328.
I0302 18:59:05.052374 22699590365312 run.py:483] Algo bellman_ford step 1604 current loss 0.841908, current_train_items 51360.
I0302 18:59:05.071801 22699590365312 run.py:483] Algo bellman_ford step 1605 current loss 0.315040, current_train_items 51392.
I0302 18:59:05.087220 22699590365312 run.py:483] Algo bellman_ford step 1606 current loss 0.491796, current_train_items 51424.
I0302 18:59:05.110257 22699590365312 run.py:483] Algo bellman_ford step 1607 current loss 0.738409, current_train_items 51456.
I0302 18:59:05.140578 22699590365312 run.py:483] Algo bellman_ford step 1608 current loss 0.803325, current_train_items 51488.
I0302 18:59:05.174091 22699590365312 run.py:483] Algo bellman_ford step 1609 current loss 1.072928, current_train_items 51520.
I0302 18:59:05.193453 22699590365312 run.py:483] Algo bellman_ford step 1610 current loss 0.378696, current_train_items 51552.
I0302 18:59:05.209881 22699590365312 run.py:483] Algo bellman_ford step 1611 current loss 0.630475, current_train_items 51584.
I0302 18:59:05.233077 22699590365312 run.py:483] Algo bellman_ford step 1612 current loss 0.756542, current_train_items 51616.
I0302 18:59:05.262927 22699590365312 run.py:483] Algo bellman_ford step 1613 current loss 0.921914, current_train_items 51648.
I0302 18:59:05.294730 22699590365312 run.py:483] Algo bellman_ford step 1614 current loss 1.227814, current_train_items 51680.
I0302 18:59:05.313936 22699590365312 run.py:483] Algo bellman_ford step 1615 current loss 0.355915, current_train_items 51712.
I0302 18:59:05.330206 22699590365312 run.py:483] Algo bellman_ford step 1616 current loss 0.507476, current_train_items 51744.
I0302 18:59:05.353393 22699590365312 run.py:483] Algo bellman_ford step 1617 current loss 0.829279, current_train_items 51776.
I0302 18:59:05.381955 22699590365312 run.py:483] Algo bellman_ford step 1618 current loss 0.829099, current_train_items 51808.
I0302 18:59:05.416231 22699590365312 run.py:483] Algo bellman_ford step 1619 current loss 1.083585, current_train_items 51840.
I0302 18:59:05.435234 22699590365312 run.py:483] Algo bellman_ford step 1620 current loss 0.343697, current_train_items 51872.
I0302 18:59:05.451222 22699590365312 run.py:483] Algo bellman_ford step 1621 current loss 0.515618, current_train_items 51904.
I0302 18:59:05.474716 22699590365312 run.py:483] Algo bellman_ford step 1622 current loss 0.810576, current_train_items 51936.
I0302 18:59:05.504327 22699590365312 run.py:483] Algo bellman_ford step 1623 current loss 0.912659, current_train_items 51968.
I0302 18:59:05.536893 22699590365312 run.py:483] Algo bellman_ford step 1624 current loss 1.041909, current_train_items 52000.
I0302 18:59:05.556286 22699590365312 run.py:483] Algo bellman_ford step 1625 current loss 0.371254, current_train_items 52032.
I0302 18:59:05.572655 22699590365312 run.py:483] Algo bellman_ford step 1626 current loss 0.466337, current_train_items 52064.
I0302 18:59:05.595796 22699590365312 run.py:483] Algo bellman_ford step 1627 current loss 0.735050, current_train_items 52096.
I0302 18:59:05.626573 22699590365312 run.py:483] Algo bellman_ford step 1628 current loss 0.812594, current_train_items 52128.
I0302 18:59:05.658311 22699590365312 run.py:483] Algo bellman_ford step 1629 current loss 0.869650, current_train_items 52160.
I0302 18:59:05.677524 22699590365312 run.py:483] Algo bellman_ford step 1630 current loss 0.351209, current_train_items 52192.
I0302 18:59:05.694213 22699590365312 run.py:483] Algo bellman_ford step 1631 current loss 0.653295, current_train_items 52224.
I0302 18:59:05.717837 22699590365312 run.py:483] Algo bellman_ford step 1632 current loss 0.821903, current_train_items 52256.
I0302 18:59:05.746862 22699590365312 run.py:483] Algo bellman_ford step 1633 current loss 0.742007, current_train_items 52288.
I0302 18:59:05.779482 22699590365312 run.py:483] Algo bellman_ford step 1634 current loss 0.867885, current_train_items 52320.
I0302 18:59:05.798623 22699590365312 run.py:483] Algo bellman_ford step 1635 current loss 0.302390, current_train_items 52352.
I0302 18:59:05.814792 22699590365312 run.py:483] Algo bellman_ford step 1636 current loss 0.565147, current_train_items 52384.
I0302 18:59:05.838462 22699590365312 run.py:483] Algo bellman_ford step 1637 current loss 0.735880, current_train_items 52416.
I0302 18:59:05.868368 22699590365312 run.py:483] Algo bellman_ford step 1638 current loss 0.753612, current_train_items 52448.
I0302 18:59:05.903079 22699590365312 run.py:483] Algo bellman_ford step 1639 current loss 1.016794, current_train_items 52480.
I0302 18:59:05.921810 22699590365312 run.py:483] Algo bellman_ford step 1640 current loss 0.336014, current_train_items 52512.
I0302 18:59:05.937659 22699590365312 run.py:483] Algo bellman_ford step 1641 current loss 0.492045, current_train_items 52544.
I0302 18:59:05.960774 22699590365312 run.py:483] Algo bellman_ford step 1642 current loss 0.784203, current_train_items 52576.
I0302 18:59:05.989906 22699590365312 run.py:483] Algo bellman_ford step 1643 current loss 0.908939, current_train_items 52608.
I0302 18:59:06.021210 22699590365312 run.py:483] Algo bellman_ford step 1644 current loss 0.972750, current_train_items 52640.
I0302 18:59:06.040293 22699590365312 run.py:483] Algo bellman_ford step 1645 current loss 0.460044, current_train_items 52672.
I0302 18:59:06.056382 22699590365312 run.py:483] Algo bellman_ford step 1646 current loss 0.549190, current_train_items 52704.
I0302 18:59:06.080561 22699590365312 run.py:483] Algo bellman_ford step 1647 current loss 0.752020, current_train_items 52736.
I0302 18:59:06.108528 22699590365312 run.py:483] Algo bellman_ford step 1648 current loss 0.697626, current_train_items 52768.
I0302 18:59:06.142115 22699590365312 run.py:483] Algo bellman_ford step 1649 current loss 0.996159, current_train_items 52800.
I0302 18:59:06.161019 22699590365312 run.py:483] Algo bellman_ford step 1650 current loss 0.568757, current_train_items 52832.
I0302 18:59:06.169214 22699590365312 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.8837890625, 'score': 0.8837890625, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0302 18:59:06.169322 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.884, val scores are: bellman_ford: 0.884
I0302 18:59:06.186624 22699590365312 run.py:483] Algo bellman_ford step 1651 current loss 0.574937, current_train_items 52864.
I0302 18:59:06.211020 22699590365312 run.py:483] Algo bellman_ford step 1652 current loss 0.882442, current_train_items 52896.
I0302 18:59:06.240733 22699590365312 run.py:483] Algo bellman_ford step 1653 current loss 0.822387, current_train_items 52928.
I0302 18:59:06.272672 22699590365312 run.py:483] Algo bellman_ford step 1654 current loss 0.987936, current_train_items 52960.
I0302 18:59:06.292167 22699590365312 run.py:483] Algo bellman_ford step 1655 current loss 0.357915, current_train_items 52992.
I0302 18:59:06.308216 22699590365312 run.py:483] Algo bellman_ford step 1656 current loss 0.767567, current_train_items 53024.
I0302 18:59:06.331185 22699590365312 run.py:483] Algo bellman_ford step 1657 current loss 1.088570, current_train_items 53056.
I0302 18:59:06.359299 22699590365312 run.py:483] Algo bellman_ford step 1658 current loss 1.207558, current_train_items 53088.
I0302 18:59:06.390007 22699590365312 run.py:483] Algo bellman_ford step 1659 current loss 1.181069, current_train_items 53120.
I0302 18:59:06.409284 22699590365312 run.py:483] Algo bellman_ford step 1660 current loss 0.343427, current_train_items 53152.
I0302 18:59:06.425541 22699590365312 run.py:483] Algo bellman_ford step 1661 current loss 0.613198, current_train_items 53184.
I0302 18:59:06.448008 22699590365312 run.py:483] Algo bellman_ford step 1662 current loss 0.850311, current_train_items 53216.
I0302 18:59:06.478897 22699590365312 run.py:483] Algo bellman_ford step 1663 current loss 1.101845, current_train_items 53248.
I0302 18:59:06.511963 22699590365312 run.py:483] Algo bellman_ford step 1664 current loss 1.453874, current_train_items 53280.
I0302 18:59:06.531274 22699590365312 run.py:483] Algo bellman_ford step 1665 current loss 0.309046, current_train_items 53312.
I0302 18:59:06.547360 22699590365312 run.py:483] Algo bellman_ford step 1666 current loss 0.576688, current_train_items 53344.
I0302 18:59:06.569858 22699590365312 run.py:483] Algo bellman_ford step 1667 current loss 0.819750, current_train_items 53376.
I0302 18:59:06.598145 22699590365312 run.py:483] Algo bellman_ford step 1668 current loss 0.818742, current_train_items 53408.
I0302 18:59:06.629029 22699590365312 run.py:483] Algo bellman_ford step 1669 current loss 1.041529, current_train_items 53440.
I0302 18:59:06.648477 22699590365312 run.py:483] Algo bellman_ford step 1670 current loss 0.398671, current_train_items 53472.
I0302 18:59:06.664304 22699590365312 run.py:483] Algo bellman_ford step 1671 current loss 0.566594, current_train_items 53504.
I0302 18:59:06.686674 22699590365312 run.py:483] Algo bellman_ford step 1672 current loss 0.662565, current_train_items 53536.
I0302 18:59:06.716982 22699590365312 run.py:483] Algo bellman_ford step 1673 current loss 0.946464, current_train_items 53568.
I0302 18:59:06.750011 22699590365312 run.py:483] Algo bellman_ford step 1674 current loss 1.209177, current_train_items 53600.
I0302 18:59:06.769038 22699590365312 run.py:483] Algo bellman_ford step 1675 current loss 0.302146, current_train_items 53632.
I0302 18:59:06.785375 22699590365312 run.py:483] Algo bellman_ford step 1676 current loss 0.578713, current_train_items 53664.
I0302 18:59:06.807787 22699590365312 run.py:483] Algo bellman_ford step 1677 current loss 0.670169, current_train_items 53696.
I0302 18:59:06.836968 22699590365312 run.py:483] Algo bellman_ford step 1678 current loss 1.003425, current_train_items 53728.
I0302 18:59:06.870304 22699590365312 run.py:483] Algo bellman_ford step 1679 current loss 1.075377, current_train_items 53760.
I0302 18:59:06.889685 22699590365312 run.py:483] Algo bellman_ford step 1680 current loss 0.321254, current_train_items 53792.
I0302 18:59:06.905547 22699590365312 run.py:483] Algo bellman_ford step 1681 current loss 0.456157, current_train_items 53824.
I0302 18:59:06.927659 22699590365312 run.py:483] Algo bellman_ford step 1682 current loss 0.707784, current_train_items 53856.
I0302 18:59:06.955669 22699590365312 run.py:483] Algo bellman_ford step 1683 current loss 0.753450, current_train_items 53888.
I0302 18:59:06.987841 22699590365312 run.py:483] Algo bellman_ford step 1684 current loss 0.949700, current_train_items 53920.
I0302 18:59:07.007371 22699590365312 run.py:483] Algo bellman_ford step 1685 current loss 0.324541, current_train_items 53952.
I0302 18:59:07.023452 22699590365312 run.py:483] Algo bellman_ford step 1686 current loss 0.616515, current_train_items 53984.
I0302 18:59:07.046458 22699590365312 run.py:483] Algo bellman_ford step 1687 current loss 0.723019, current_train_items 54016.
I0302 18:59:07.075368 22699590365312 run.py:483] Algo bellman_ford step 1688 current loss 0.784728, current_train_items 54048.
I0302 18:59:07.109662 22699590365312 run.py:483] Algo bellman_ford step 1689 current loss 1.207253, current_train_items 54080.
I0302 18:59:07.128580 22699590365312 run.py:483] Algo bellman_ford step 1690 current loss 0.290344, current_train_items 54112.
I0302 18:59:07.145120 22699590365312 run.py:483] Algo bellman_ford step 1691 current loss 0.600659, current_train_items 54144.
I0302 18:59:07.168250 22699590365312 run.py:483] Algo bellman_ford step 1692 current loss 0.764601, current_train_items 54176.
I0302 18:59:07.198778 22699590365312 run.py:483] Algo bellman_ford step 1693 current loss 0.766547, current_train_items 54208.
I0302 18:59:07.231639 22699590365312 run.py:483] Algo bellman_ford step 1694 current loss 1.042190, current_train_items 54240.
I0302 18:59:07.250643 22699590365312 run.py:483] Algo bellman_ford step 1695 current loss 0.354778, current_train_items 54272.
I0302 18:59:07.267109 22699590365312 run.py:483] Algo bellman_ford step 1696 current loss 0.468281, current_train_items 54304.
I0302 18:59:07.288475 22699590365312 run.py:483] Algo bellman_ford step 1697 current loss 0.692194, current_train_items 54336.
I0302 18:59:07.316836 22699590365312 run.py:483] Algo bellman_ford step 1698 current loss 0.726703, current_train_items 54368.
I0302 18:59:07.350024 22699590365312 run.py:483] Algo bellman_ford step 1699 current loss 0.958506, current_train_items 54400.
I0302 18:59:07.369301 22699590365312 run.py:483] Algo bellman_ford step 1700 current loss 0.397738, current_train_items 54432.
I0302 18:59:07.377245 22699590365312 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0302 18:59:07.377351 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 18:59:07.394232 22699590365312 run.py:483] Algo bellman_ford step 1701 current loss 0.518921, current_train_items 54464.
I0302 18:59:07.417495 22699590365312 run.py:483] Algo bellman_ford step 1702 current loss 0.679841, current_train_items 54496.
I0302 18:59:07.447184 22699590365312 run.py:483] Algo bellman_ford step 1703 current loss 0.750936, current_train_items 54528.
I0302 18:59:07.479398 22699590365312 run.py:483] Algo bellman_ford step 1704 current loss 0.943877, current_train_items 54560.
I0302 18:59:07.499350 22699590365312 run.py:483] Algo bellman_ford step 1705 current loss 0.337543, current_train_items 54592.
I0302 18:59:07.514993 22699590365312 run.py:483] Algo bellman_ford step 1706 current loss 0.457731, current_train_items 54624.
I0302 18:59:07.538283 22699590365312 run.py:483] Algo bellman_ford step 1707 current loss 0.682809, current_train_items 54656.
I0302 18:59:07.567669 22699590365312 run.py:483] Algo bellman_ford step 1708 current loss 0.818174, current_train_items 54688.
I0302 18:59:07.601377 22699590365312 run.py:483] Algo bellman_ford step 1709 current loss 0.982215, current_train_items 54720.
I0302 18:59:07.620353 22699590365312 run.py:483] Algo bellman_ford step 1710 current loss 0.360896, current_train_items 54752.
I0302 18:59:07.636633 22699590365312 run.py:483] Algo bellman_ford step 1711 current loss 0.630847, current_train_items 54784.
I0302 18:59:07.659947 22699590365312 run.py:483] Algo bellman_ford step 1712 current loss 0.791033, current_train_items 54816.
I0302 18:59:07.688533 22699590365312 run.py:483] Algo bellman_ford step 1713 current loss 0.730903, current_train_items 54848.
I0302 18:59:07.720691 22699590365312 run.py:483] Algo bellman_ford step 1714 current loss 0.762443, current_train_items 54880.
I0302 18:59:07.739588 22699590365312 run.py:483] Algo bellman_ford step 1715 current loss 0.366672, current_train_items 54912.
I0302 18:59:07.755356 22699590365312 run.py:483] Algo bellman_ford step 1716 current loss 0.468683, current_train_items 54944.
I0302 18:59:07.779013 22699590365312 run.py:483] Algo bellman_ford step 1717 current loss 0.665992, current_train_items 54976.
I0302 18:59:07.806696 22699590365312 run.py:483] Algo bellman_ford step 1718 current loss 0.784899, current_train_items 55008.
I0302 18:59:07.839130 22699590365312 run.py:483] Algo bellman_ford step 1719 current loss 0.943662, current_train_items 55040.
I0302 18:59:07.858530 22699590365312 run.py:483] Algo bellman_ford step 1720 current loss 0.319524, current_train_items 55072.
I0302 18:59:07.874444 22699590365312 run.py:483] Algo bellman_ford step 1721 current loss 0.579799, current_train_items 55104.
I0302 18:59:07.896934 22699590365312 run.py:483] Algo bellman_ford step 1722 current loss 0.659550, current_train_items 55136.
I0302 18:59:07.925828 22699590365312 run.py:483] Algo bellman_ford step 1723 current loss 0.795599, current_train_items 55168.
I0302 18:59:07.957342 22699590365312 run.py:483] Algo bellman_ford step 1724 current loss 0.900072, current_train_items 55200.
I0302 18:59:07.976437 22699590365312 run.py:483] Algo bellman_ford step 1725 current loss 0.330809, current_train_items 55232.
I0302 18:59:07.992649 22699590365312 run.py:483] Algo bellman_ford step 1726 current loss 0.645497, current_train_items 55264.
I0302 18:59:08.017026 22699590365312 run.py:483] Algo bellman_ford step 1727 current loss 0.726032, current_train_items 55296.
I0302 18:59:08.046440 22699590365312 run.py:483] Algo bellman_ford step 1728 current loss 0.744749, current_train_items 55328.
I0302 18:59:08.078589 22699590365312 run.py:483] Algo bellman_ford step 1729 current loss 0.994504, current_train_items 55360.
I0302 18:59:08.097622 22699590365312 run.py:483] Algo bellman_ford step 1730 current loss 0.280548, current_train_items 55392.
I0302 18:59:08.113763 22699590365312 run.py:483] Algo bellman_ford step 1731 current loss 0.490609, current_train_items 55424.
I0302 18:59:08.136706 22699590365312 run.py:483] Algo bellman_ford step 1732 current loss 0.743499, current_train_items 55456.
I0302 18:59:08.165972 22699590365312 run.py:483] Algo bellman_ford step 1733 current loss 0.807279, current_train_items 55488.
I0302 18:59:08.196013 22699590365312 run.py:483] Algo bellman_ford step 1734 current loss 1.037216, current_train_items 55520.
I0302 18:59:08.215402 22699590365312 run.py:483] Algo bellman_ford step 1735 current loss 0.348645, current_train_items 55552.
I0302 18:59:08.231650 22699590365312 run.py:483] Algo bellman_ford step 1736 current loss 0.542971, current_train_items 55584.
I0302 18:59:08.255000 22699590365312 run.py:483] Algo bellman_ford step 1737 current loss 0.697113, current_train_items 55616.
I0302 18:59:08.284246 22699590365312 run.py:483] Algo bellman_ford step 1738 current loss 0.854565, current_train_items 55648.
I0302 18:59:08.316676 22699590365312 run.py:483] Algo bellman_ford step 1739 current loss 0.929636, current_train_items 55680.
I0302 18:59:08.335530 22699590365312 run.py:483] Algo bellman_ford step 1740 current loss 0.311455, current_train_items 55712.
I0302 18:59:08.351763 22699590365312 run.py:483] Algo bellman_ford step 1741 current loss 0.568192, current_train_items 55744.
I0302 18:59:08.374904 22699590365312 run.py:483] Algo bellman_ford step 1742 current loss 0.571199, current_train_items 55776.
I0302 18:59:08.404868 22699590365312 run.py:483] Algo bellman_ford step 1743 current loss 0.701617, current_train_items 55808.
I0302 18:59:08.436305 22699590365312 run.py:483] Algo bellman_ford step 1744 current loss 0.889667, current_train_items 55840.
I0302 18:59:08.455541 22699590365312 run.py:483] Algo bellman_ford step 1745 current loss 0.327012, current_train_items 55872.
I0302 18:59:08.471955 22699590365312 run.py:483] Algo bellman_ford step 1746 current loss 0.651888, current_train_items 55904.
I0302 18:59:08.494935 22699590365312 run.py:483] Algo bellman_ford step 1747 current loss 0.853087, current_train_items 55936.
I0302 18:59:08.524494 22699590365312 run.py:483] Algo bellman_ford step 1748 current loss 0.880296, current_train_items 55968.
I0302 18:59:08.555163 22699590365312 run.py:483] Algo bellman_ford step 1749 current loss 1.013450, current_train_items 56000.
I0302 18:59:08.574312 22699590365312 run.py:483] Algo bellman_ford step 1750 current loss 0.277926, current_train_items 56032.
I0302 18:59:08.582643 22699590365312 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0302 18:59:08.582749 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 18:59:08.599732 22699590365312 run.py:483] Algo bellman_ford step 1751 current loss 0.500031, current_train_items 56064.
I0302 18:59:08.623385 22699590365312 run.py:483] Algo bellman_ford step 1752 current loss 0.652214, current_train_items 56096.
I0302 18:59:08.654465 22699590365312 run.py:483] Algo bellman_ford step 1753 current loss 0.792600, current_train_items 56128.
I0302 18:59:08.688711 22699590365312 run.py:483] Algo bellman_ford step 1754 current loss 0.909221, current_train_items 56160.
I0302 18:59:08.708509 22699590365312 run.py:483] Algo bellman_ford step 1755 current loss 0.308307, current_train_items 56192.
I0302 18:59:08.724635 22699590365312 run.py:483] Algo bellman_ford step 1756 current loss 0.516515, current_train_items 56224.
I0302 18:59:08.748116 22699590365312 run.py:483] Algo bellman_ford step 1757 current loss 0.741016, current_train_items 56256.
I0302 18:59:08.778049 22699590365312 run.py:483] Algo bellman_ford step 1758 current loss 0.911597, current_train_items 56288.
I0302 18:59:08.808370 22699590365312 run.py:483] Algo bellman_ford step 1759 current loss 0.874342, current_train_items 56320.
I0302 18:59:08.827657 22699590365312 run.py:483] Algo bellman_ford step 1760 current loss 0.320999, current_train_items 56352.
I0302 18:59:08.843854 22699590365312 run.py:483] Algo bellman_ford step 1761 current loss 0.490042, current_train_items 56384.
I0302 18:59:08.867722 22699590365312 run.py:483] Algo bellman_ford step 1762 current loss 0.732084, current_train_items 56416.
I0302 18:59:08.899088 22699590365312 run.py:483] Algo bellman_ford step 1763 current loss 0.796962, current_train_items 56448.
I0302 18:59:08.933363 22699590365312 run.py:483] Algo bellman_ford step 1764 current loss 1.185792, current_train_items 56480.
I0302 18:59:08.952949 22699590365312 run.py:483] Algo bellman_ford step 1765 current loss 0.345966, current_train_items 56512.
I0302 18:59:08.968887 22699590365312 run.py:483] Algo bellman_ford step 1766 current loss 0.505405, current_train_items 56544.
I0302 18:59:08.991758 22699590365312 run.py:483] Algo bellman_ford step 1767 current loss 0.616482, current_train_items 56576.
I0302 18:59:09.021138 22699590365312 run.py:483] Algo bellman_ford step 1768 current loss 0.760053, current_train_items 56608.
I0302 18:59:09.052357 22699590365312 run.py:483] Algo bellman_ford step 1769 current loss 0.951022, current_train_items 56640.
I0302 18:59:09.071748 22699590365312 run.py:483] Algo bellman_ford step 1770 current loss 0.319710, current_train_items 56672.
I0302 18:59:09.088087 22699590365312 run.py:483] Algo bellman_ford step 1771 current loss 0.530285, current_train_items 56704.
I0302 18:59:09.112017 22699590365312 run.py:483] Algo bellman_ford step 1772 current loss 0.751290, current_train_items 56736.
I0302 18:59:09.142448 22699590365312 run.py:483] Algo bellman_ford step 1773 current loss 0.830064, current_train_items 56768.
I0302 18:59:09.173790 22699590365312 run.py:483] Algo bellman_ford step 1774 current loss 0.896395, current_train_items 56800.
I0302 18:59:09.193268 22699590365312 run.py:483] Algo bellman_ford step 1775 current loss 0.328945, current_train_items 56832.
I0302 18:59:09.209617 22699590365312 run.py:483] Algo bellman_ford step 1776 current loss 0.573530, current_train_items 56864.
I0302 18:59:09.233001 22699590365312 run.py:483] Algo bellman_ford step 1777 current loss 0.776476, current_train_items 56896.
I0302 18:59:09.262198 22699590365312 run.py:483] Algo bellman_ford step 1778 current loss 0.900301, current_train_items 56928.
I0302 18:59:09.292374 22699590365312 run.py:483] Algo bellman_ford step 1779 current loss 0.828168, current_train_items 56960.
I0302 18:59:09.311438 22699590365312 run.py:483] Algo bellman_ford step 1780 current loss 0.276266, current_train_items 56992.
I0302 18:59:09.327531 22699590365312 run.py:483] Algo bellman_ford step 1781 current loss 0.490904, current_train_items 57024.
I0302 18:59:09.350571 22699590365312 run.py:483] Algo bellman_ford step 1782 current loss 0.707686, current_train_items 57056.
I0302 18:59:09.380421 22699590365312 run.py:483] Algo bellman_ford step 1783 current loss 0.886113, current_train_items 57088.
I0302 18:59:09.411760 22699590365312 run.py:483] Algo bellman_ford step 1784 current loss 0.916422, current_train_items 57120.
I0302 18:59:09.431028 22699590365312 run.py:483] Algo bellman_ford step 1785 current loss 0.343381, current_train_items 57152.
I0302 18:59:09.447419 22699590365312 run.py:483] Algo bellman_ford step 1786 current loss 0.565750, current_train_items 57184.
I0302 18:59:09.469710 22699590365312 run.py:483] Algo bellman_ford step 1787 current loss 0.740680, current_train_items 57216.
I0302 18:59:09.498431 22699590365312 run.py:483] Algo bellman_ford step 1788 current loss 0.798539, current_train_items 57248.
I0302 18:59:09.530597 22699590365312 run.py:483] Algo bellman_ford step 1789 current loss 0.823122, current_train_items 57280.
I0302 18:59:09.549969 22699590365312 run.py:483] Algo bellman_ford step 1790 current loss 0.248102, current_train_items 57312.
I0302 18:59:09.566792 22699590365312 run.py:483] Algo bellman_ford step 1791 current loss 0.638337, current_train_items 57344.
I0302 18:59:09.589888 22699590365312 run.py:483] Algo bellman_ford step 1792 current loss 0.725398, current_train_items 57376.
I0302 18:59:09.618312 22699590365312 run.py:483] Algo bellman_ford step 1793 current loss 0.932546, current_train_items 57408.
I0302 18:59:09.650517 22699590365312 run.py:483] Algo bellman_ford step 1794 current loss 0.925394, current_train_items 57440.
I0302 18:59:09.669357 22699590365312 run.py:483] Algo bellman_ford step 1795 current loss 0.395768, current_train_items 57472.
I0302 18:59:09.685407 22699590365312 run.py:483] Algo bellman_ford step 1796 current loss 0.551554, current_train_items 57504.
I0302 18:59:09.709217 22699590365312 run.py:483] Algo bellman_ford step 1797 current loss 0.732887, current_train_items 57536.
I0302 18:59:09.739169 22699590365312 run.py:483] Algo bellman_ford step 1798 current loss 0.911198, current_train_items 57568.
I0302 18:59:09.772541 22699590365312 run.py:483] Algo bellman_ford step 1799 current loss 0.960889, current_train_items 57600.
I0302 18:59:09.791957 22699590365312 run.py:483] Algo bellman_ford step 1800 current loss 0.356799, current_train_items 57632.
I0302 18:59:09.799794 22699590365312 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.87890625, 'score': 0.87890625, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0302 18:59:09.799901 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.879, val scores are: bellman_ford: 0.879
I0302 18:59:09.816415 22699590365312 run.py:483] Algo bellman_ford step 1801 current loss 0.422734, current_train_items 57664.
I0302 18:59:09.840277 22699590365312 run.py:483] Algo bellman_ford step 1802 current loss 0.701354, current_train_items 57696.
I0302 18:59:09.869284 22699590365312 run.py:483] Algo bellman_ford step 1803 current loss 0.800668, current_train_items 57728.
I0302 18:59:09.900793 22699590365312 run.py:483] Algo bellman_ford step 1804 current loss 1.177877, current_train_items 57760.
I0302 18:59:09.920218 22699590365312 run.py:483] Algo bellman_ford step 1805 current loss 0.345921, current_train_items 57792.
I0302 18:59:09.935923 22699590365312 run.py:483] Algo bellman_ford step 1806 current loss 0.475419, current_train_items 57824.
I0302 18:59:09.958341 22699590365312 run.py:483] Algo bellman_ford step 1807 current loss 0.677774, current_train_items 57856.
I0302 18:59:09.986105 22699590365312 run.py:483] Algo bellman_ford step 1808 current loss 0.780610, current_train_items 57888.
I0302 18:59:10.019029 22699590365312 run.py:483] Algo bellman_ford step 1809 current loss 0.975756, current_train_items 57920.
I0302 18:59:10.037635 22699590365312 run.py:483] Algo bellman_ford step 1810 current loss 0.499456, current_train_items 57952.
I0302 18:59:10.053601 22699590365312 run.py:483] Algo bellman_ford step 1811 current loss 0.629270, current_train_items 57984.
I0302 18:59:10.076974 22699590365312 run.py:483] Algo bellman_ford step 1812 current loss 0.782080, current_train_items 58016.
I0302 18:59:10.104221 22699590365312 run.py:483] Algo bellman_ford step 1813 current loss 0.731551, current_train_items 58048.
I0302 18:59:10.135929 22699590365312 run.py:483] Algo bellman_ford step 1814 current loss 1.081338, current_train_items 58080.
I0302 18:59:10.154698 22699590365312 run.py:483] Algo bellman_ford step 1815 current loss 0.302498, current_train_items 58112.
I0302 18:59:10.171303 22699590365312 run.py:483] Algo bellman_ford step 1816 current loss 0.697995, current_train_items 58144.
I0302 18:59:10.193840 22699590365312 run.py:483] Algo bellman_ford step 1817 current loss 0.654757, current_train_items 58176.
I0302 18:59:10.223206 22699590365312 run.py:483] Algo bellman_ford step 1818 current loss 0.903394, current_train_items 58208.
I0302 18:59:10.254940 22699590365312 run.py:483] Algo bellman_ford step 1819 current loss 0.913464, current_train_items 58240.
I0302 18:59:10.273725 22699590365312 run.py:483] Algo bellman_ford step 1820 current loss 0.418156, current_train_items 58272.
I0302 18:59:10.290277 22699590365312 run.py:483] Algo bellman_ford step 1821 current loss 0.547323, current_train_items 58304.
I0302 18:59:10.315214 22699590365312 run.py:483] Algo bellman_ford step 1822 current loss 0.770885, current_train_items 58336.
I0302 18:59:10.345117 22699590365312 run.py:483] Algo bellman_ford step 1823 current loss 0.805867, current_train_items 58368.
I0302 18:59:10.375865 22699590365312 run.py:483] Algo bellman_ford step 1824 current loss 0.848758, current_train_items 58400.
I0302 18:59:10.394447 22699590365312 run.py:483] Algo bellman_ford step 1825 current loss 0.327351, current_train_items 58432.
I0302 18:59:10.410772 22699590365312 run.py:483] Algo bellman_ford step 1826 current loss 0.684818, current_train_items 58464.
I0302 18:59:10.433666 22699590365312 run.py:483] Algo bellman_ford step 1827 current loss 0.719982, current_train_items 58496.
I0302 18:59:10.463907 22699590365312 run.py:483] Algo bellman_ford step 1828 current loss 1.254010, current_train_items 58528.
I0302 18:59:10.498526 22699590365312 run.py:483] Algo bellman_ford step 1829 current loss 1.327111, current_train_items 58560.
I0302 18:59:10.517459 22699590365312 run.py:483] Algo bellman_ford step 1830 current loss 0.339614, current_train_items 58592.
I0302 18:59:10.533736 22699590365312 run.py:483] Algo bellman_ford step 1831 current loss 0.621320, current_train_items 58624.
I0302 18:59:10.558259 22699590365312 run.py:483] Algo bellman_ford step 1832 current loss 0.777709, current_train_items 58656.
I0302 18:59:10.587660 22699590365312 run.py:483] Algo bellman_ford step 1833 current loss 0.883300, current_train_items 58688.
I0302 18:59:10.618781 22699590365312 run.py:483] Algo bellman_ford step 1834 current loss 0.847080, current_train_items 58720.
I0302 18:59:10.637509 22699590365312 run.py:483] Algo bellman_ford step 1835 current loss 0.303739, current_train_items 58752.
I0302 18:59:10.653912 22699590365312 run.py:483] Algo bellman_ford step 1836 current loss 0.568254, current_train_items 58784.
I0302 18:59:10.678140 22699590365312 run.py:483] Algo bellman_ford step 1837 current loss 0.925535, current_train_items 58816.
I0302 18:59:10.706967 22699590365312 run.py:483] Algo bellman_ford step 1838 current loss 0.804859, current_train_items 58848.
I0302 18:59:10.739282 22699590365312 run.py:483] Algo bellman_ford step 1839 current loss 0.936331, current_train_items 58880.
I0302 18:59:10.757972 22699590365312 run.py:483] Algo bellman_ford step 1840 current loss 0.297944, current_train_items 58912.
I0302 18:59:10.774263 22699590365312 run.py:483] Algo bellman_ford step 1841 current loss 0.555974, current_train_items 58944.
I0302 18:59:10.797788 22699590365312 run.py:483] Algo bellman_ford step 1842 current loss 0.729713, current_train_items 58976.
I0302 18:59:10.827769 22699590365312 run.py:483] Algo bellman_ford step 1843 current loss 0.862151, current_train_items 59008.
I0302 18:59:10.859641 22699590365312 run.py:483] Algo bellman_ford step 1844 current loss 1.157672, current_train_items 59040.
I0302 18:59:10.878436 22699590365312 run.py:483] Algo bellman_ford step 1845 current loss 0.360253, current_train_items 59072.
I0302 18:59:10.894553 22699590365312 run.py:483] Algo bellman_ford step 1846 current loss 0.495132, current_train_items 59104.
I0302 18:59:10.917813 22699590365312 run.py:483] Algo bellman_ford step 1847 current loss 0.673965, current_train_items 59136.
I0302 18:59:10.947402 22699590365312 run.py:483] Algo bellman_ford step 1848 current loss 0.746670, current_train_items 59168.
I0302 18:59:10.980760 22699590365312 run.py:483] Algo bellman_ford step 1849 current loss 0.997393, current_train_items 59200.
I0302 18:59:10.999655 22699590365312 run.py:483] Algo bellman_ford step 1850 current loss 0.377307, current_train_items 59232.
I0302 18:59:11.007797 22699590365312 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0302 18:59:11.007907 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 18:59:11.024813 22699590365312 run.py:483] Algo bellman_ford step 1851 current loss 0.592382, current_train_items 59264.
I0302 18:59:11.048637 22699590365312 run.py:483] Algo bellman_ford step 1852 current loss 0.705561, current_train_items 59296.
I0302 18:59:11.078625 22699590365312 run.py:483] Algo bellman_ford step 1853 current loss 0.850411, current_train_items 59328.
I0302 18:59:11.111733 22699590365312 run.py:483] Algo bellman_ford step 1854 current loss 0.867330, current_train_items 59360.
I0302 18:59:11.131344 22699590365312 run.py:483] Algo bellman_ford step 1855 current loss 0.341125, current_train_items 59392.
I0302 18:59:11.146819 22699590365312 run.py:483] Algo bellman_ford step 1856 current loss 0.592695, current_train_items 59424.
I0302 18:59:11.170679 22699590365312 run.py:483] Algo bellman_ford step 1857 current loss 0.755669, current_train_items 59456.
I0302 18:59:11.199064 22699590365312 run.py:483] Algo bellman_ford step 1858 current loss 0.778337, current_train_items 59488.
I0302 18:59:11.231907 22699590365312 run.py:483] Algo bellman_ford step 1859 current loss 0.914362, current_train_items 59520.
I0302 18:59:11.251451 22699590365312 run.py:483] Algo bellman_ford step 1860 current loss 0.266690, current_train_items 59552.
I0302 18:59:11.268136 22699590365312 run.py:483] Algo bellman_ford step 1861 current loss 0.452514, current_train_items 59584.
I0302 18:59:11.290884 22699590365312 run.py:483] Algo bellman_ford step 1862 current loss 0.699222, current_train_items 59616.
I0302 18:59:11.319956 22699590365312 run.py:483] Algo bellman_ford step 1863 current loss 0.782790, current_train_items 59648.
I0302 18:59:11.352904 22699590365312 run.py:483] Algo bellman_ford step 1864 current loss 0.956617, current_train_items 59680.
I0302 18:59:11.372333 22699590365312 run.py:483] Algo bellman_ford step 1865 current loss 0.303618, current_train_items 59712.
I0302 18:59:11.388623 22699590365312 run.py:483] Algo bellman_ford step 1866 current loss 0.548129, current_train_items 59744.
I0302 18:59:11.412571 22699590365312 run.py:483] Algo bellman_ford step 1867 current loss 0.788644, current_train_items 59776.
I0302 18:59:11.442360 22699590365312 run.py:483] Algo bellman_ford step 1868 current loss 0.799986, current_train_items 59808.
I0302 18:59:11.474799 22699590365312 run.py:483] Algo bellman_ford step 1869 current loss 0.799068, current_train_items 59840.
I0302 18:59:11.494141 22699590365312 run.py:483] Algo bellman_ford step 1870 current loss 0.349433, current_train_items 59872.
I0302 18:59:11.510308 22699590365312 run.py:483] Algo bellman_ford step 1871 current loss 0.469081, current_train_items 59904.
I0302 18:59:11.532869 22699590365312 run.py:483] Algo bellman_ford step 1872 current loss 0.640380, current_train_items 59936.
I0302 18:59:11.562418 22699590365312 run.py:483] Algo bellman_ford step 1873 current loss 0.771727, current_train_items 59968.
I0302 18:59:11.593982 22699590365312 run.py:483] Algo bellman_ford step 1874 current loss 0.854658, current_train_items 60000.
I0302 18:59:11.613260 22699590365312 run.py:483] Algo bellman_ford step 1875 current loss 0.366297, current_train_items 60032.
I0302 18:59:11.629346 22699590365312 run.py:483] Algo bellman_ford step 1876 current loss 0.444530, current_train_items 60064.
I0302 18:59:11.652493 22699590365312 run.py:483] Algo bellman_ford step 1877 current loss 0.919728, current_train_items 60096.
I0302 18:59:11.680571 22699590365312 run.py:483] Algo bellman_ford step 1878 current loss 0.889959, current_train_items 60128.
I0302 18:59:11.715415 22699590365312 run.py:483] Algo bellman_ford step 1879 current loss 1.136885, current_train_items 60160.
I0302 18:59:11.734265 22699590365312 run.py:483] Algo bellman_ford step 1880 current loss 0.318172, current_train_items 60192.
I0302 18:59:11.750726 22699590365312 run.py:483] Algo bellman_ford step 1881 current loss 0.631415, current_train_items 60224.
I0302 18:59:11.773295 22699590365312 run.py:483] Algo bellman_ford step 1882 current loss 0.706405, current_train_items 60256.
I0302 18:59:11.802864 22699590365312 run.py:483] Algo bellman_ford step 1883 current loss 0.925296, current_train_items 60288.
I0302 18:59:11.835199 22699590365312 run.py:483] Algo bellman_ford step 1884 current loss 1.177155, current_train_items 60320.
I0302 18:59:11.854441 22699590365312 run.py:483] Algo bellman_ford step 1885 current loss 0.270992, current_train_items 60352.
I0302 18:59:11.870851 22699590365312 run.py:483] Algo bellman_ford step 1886 current loss 0.575521, current_train_items 60384.
I0302 18:59:11.893877 22699590365312 run.py:483] Algo bellman_ford step 1887 current loss 0.789655, current_train_items 60416.
I0302 18:59:11.921973 22699590365312 run.py:483] Algo bellman_ford step 1888 current loss 0.702274, current_train_items 60448.
I0302 18:59:11.955783 22699590365312 run.py:483] Algo bellman_ford step 1889 current loss 0.990735, current_train_items 60480.
I0302 18:59:11.974699 22699590365312 run.py:483] Algo bellman_ford step 1890 current loss 0.341668, current_train_items 60512.
I0302 18:59:11.990924 22699590365312 run.py:483] Algo bellman_ford step 1891 current loss 0.558661, current_train_items 60544.
I0302 18:59:12.013478 22699590365312 run.py:483] Algo bellman_ford step 1892 current loss 0.676606, current_train_items 60576.
I0302 18:59:12.043474 22699590365312 run.py:483] Algo bellman_ford step 1893 current loss 0.821340, current_train_items 60608.
I0302 18:59:12.078454 22699590365312 run.py:483] Algo bellman_ford step 1894 current loss 1.078446, current_train_items 60640.
I0302 18:59:12.097584 22699590365312 run.py:483] Algo bellman_ford step 1895 current loss 0.401684, current_train_items 60672.
I0302 18:59:12.113743 22699590365312 run.py:483] Algo bellman_ford step 1896 current loss 0.531634, current_train_items 60704.
I0302 18:59:12.137185 22699590365312 run.py:483] Algo bellman_ford step 1897 current loss 0.821769, current_train_items 60736.
I0302 18:59:12.166898 22699590365312 run.py:483] Algo bellman_ford step 1898 current loss 0.797163, current_train_items 60768.
I0302 18:59:12.199761 22699590365312 run.py:483] Algo bellman_ford step 1899 current loss 0.950303, current_train_items 60800.
I0302 18:59:12.219300 22699590365312 run.py:483] Algo bellman_ford step 1900 current loss 0.353682, current_train_items 60832.
I0302 18:59:12.227188 22699590365312 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0302 18:59:12.227297 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 18:59:12.244013 22699590365312 run.py:483] Algo bellman_ford step 1901 current loss 0.597153, current_train_items 60864.
I0302 18:59:12.267145 22699590365312 run.py:483] Algo bellman_ford step 1902 current loss 0.604139, current_train_items 60896.
I0302 18:59:12.295100 22699590365312 run.py:483] Algo bellman_ford step 1903 current loss 0.702992, current_train_items 60928.
I0302 18:59:12.327957 22699590365312 run.py:483] Algo bellman_ford step 1904 current loss 0.857947, current_train_items 60960.
I0302 18:59:12.347449 22699590365312 run.py:483] Algo bellman_ford step 1905 current loss 0.398613, current_train_items 60992.
I0302 18:59:12.363253 22699590365312 run.py:483] Algo bellman_ford step 1906 current loss 0.530474, current_train_items 61024.
I0302 18:59:12.385631 22699590365312 run.py:483] Algo bellman_ford step 1907 current loss 0.692393, current_train_items 61056.
I0302 18:59:12.414601 22699590365312 run.py:483] Algo bellman_ford step 1908 current loss 0.845158, current_train_items 61088.
I0302 18:59:12.446308 22699590365312 run.py:483] Algo bellman_ford step 1909 current loss 0.900472, current_train_items 61120.
I0302 18:59:12.465388 22699590365312 run.py:483] Algo bellman_ford step 1910 current loss 0.334337, current_train_items 61152.
I0302 18:59:12.481773 22699590365312 run.py:483] Algo bellman_ford step 1911 current loss 0.499611, current_train_items 61184.
I0302 18:59:12.505704 22699590365312 run.py:483] Algo bellman_ford step 1912 current loss 0.741521, current_train_items 61216.
I0302 18:59:12.534770 22699590365312 run.py:483] Algo bellman_ford step 1913 current loss 0.766192, current_train_items 61248.
I0302 18:59:12.567542 22699590365312 run.py:483] Algo bellman_ford step 1914 current loss 0.908793, current_train_items 61280.
I0302 18:59:12.586500 22699590365312 run.py:483] Algo bellman_ford step 1915 current loss 0.266412, current_train_items 61312.
I0302 18:59:12.602850 22699590365312 run.py:483] Algo bellman_ford step 1916 current loss 0.494560, current_train_items 61344.
I0302 18:59:12.626372 22699590365312 run.py:483] Algo bellman_ford step 1917 current loss 0.648741, current_train_items 61376.
I0302 18:59:12.656288 22699590365312 run.py:483] Algo bellman_ford step 1918 current loss 0.733811, current_train_items 61408.
I0302 18:59:12.690177 22699590365312 run.py:483] Algo bellman_ford step 1919 current loss 0.997478, current_train_items 61440.
I0302 18:59:12.709116 22699590365312 run.py:483] Algo bellman_ford step 1920 current loss 0.344221, current_train_items 61472.
I0302 18:59:12.725465 22699590365312 run.py:483] Algo bellman_ford step 1921 current loss 0.589575, current_train_items 61504.
I0302 18:59:12.748236 22699590365312 run.py:483] Algo bellman_ford step 1922 current loss 0.758112, current_train_items 61536.
I0302 18:59:12.776799 22699590365312 run.py:483] Algo bellman_ford step 1923 current loss 0.684929, current_train_items 61568.
I0302 18:59:12.809031 22699590365312 run.py:483] Algo bellman_ford step 1924 current loss 0.959159, current_train_items 61600.
I0302 18:59:12.828015 22699590365312 run.py:483] Algo bellman_ford step 1925 current loss 0.331009, current_train_items 61632.
I0302 18:59:12.844220 22699590365312 run.py:483] Algo bellman_ford step 1926 current loss 0.618705, current_train_items 61664.
I0302 18:59:12.866454 22699590365312 run.py:483] Algo bellman_ford step 1927 current loss 0.647354, current_train_items 61696.
I0302 18:59:12.895015 22699590365312 run.py:483] Algo bellman_ford step 1928 current loss 0.753537, current_train_items 61728.
I0302 18:59:12.925473 22699590365312 run.py:483] Algo bellman_ford step 1929 current loss 0.897929, current_train_items 61760.
I0302 18:59:12.944566 22699590365312 run.py:483] Algo bellman_ford step 1930 current loss 0.418349, current_train_items 61792.
I0302 18:59:12.960283 22699590365312 run.py:483] Algo bellman_ford step 1931 current loss 0.389529, current_train_items 61824.
I0302 18:59:12.984686 22699590365312 run.py:483] Algo bellman_ford step 1932 current loss 0.702833, current_train_items 61856.
I0302 18:59:13.014160 22699590365312 run.py:483] Algo bellman_ford step 1933 current loss 0.838970, current_train_items 61888.
I0302 18:59:13.044722 22699590365312 run.py:483] Algo bellman_ford step 1934 current loss 0.959502, current_train_items 61920.
I0302 18:59:13.063445 22699590365312 run.py:483] Algo bellman_ford step 1935 current loss 0.366372, current_train_items 61952.
I0302 18:59:13.079751 22699590365312 run.py:483] Algo bellman_ford step 1936 current loss 0.440941, current_train_items 61984.
I0302 18:59:13.103761 22699590365312 run.py:483] Algo bellman_ford step 1937 current loss 0.619565, current_train_items 62016.
I0302 18:59:13.132282 22699590365312 run.py:483] Algo bellman_ford step 1938 current loss 0.722136, current_train_items 62048.
I0302 18:59:13.164996 22699590365312 run.py:483] Algo bellman_ford step 1939 current loss 0.993658, current_train_items 62080.
I0302 18:59:13.183835 22699590365312 run.py:483] Algo bellman_ford step 1940 current loss 0.297601, current_train_items 62112.
I0302 18:59:13.200063 22699590365312 run.py:483] Algo bellman_ford step 1941 current loss 0.465154, current_train_items 62144.
I0302 18:59:13.223019 22699590365312 run.py:483] Algo bellman_ford step 1942 current loss 0.689915, current_train_items 62176.
I0302 18:59:13.251290 22699590365312 run.py:483] Algo bellman_ford step 1943 current loss 0.722527, current_train_items 62208.
I0302 18:59:13.283635 22699590365312 run.py:483] Algo bellman_ford step 1944 current loss 0.853297, current_train_items 62240.
I0302 18:59:13.302919 22699590365312 run.py:483] Algo bellman_ford step 1945 current loss 0.340294, current_train_items 62272.
I0302 18:59:13.319097 22699590365312 run.py:483] Algo bellman_ford step 1946 current loss 0.549079, current_train_items 62304.
I0302 18:59:13.342480 22699590365312 run.py:483] Algo bellman_ford step 1947 current loss 0.647833, current_train_items 62336.
I0302 18:59:13.372815 22699590365312 run.py:483] Algo bellman_ford step 1948 current loss 0.877644, current_train_items 62368.
I0302 18:59:13.404117 22699590365312 run.py:483] Algo bellman_ford step 1949 current loss 0.845809, current_train_items 62400.
I0302 18:59:13.423137 22699590365312 run.py:483] Algo bellman_ford step 1950 current loss 0.302056, current_train_items 62432.
I0302 18:59:13.431384 22699590365312 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0302 18:59:13.431493 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 18:59:13.448968 22699590365312 run.py:483] Algo bellman_ford step 1951 current loss 0.613178, current_train_items 62464.
I0302 18:59:13.472792 22699590365312 run.py:483] Algo bellman_ford step 1952 current loss 0.671710, current_train_items 62496.
I0302 18:59:13.504573 22699590365312 run.py:483] Algo bellman_ford step 1953 current loss 0.775023, current_train_items 62528.
I0302 18:59:13.537471 22699590365312 run.py:483] Algo bellman_ford step 1954 current loss 0.780180, current_train_items 62560.
I0302 18:59:13.557261 22699590365312 run.py:483] Algo bellman_ford step 1955 current loss 0.359395, current_train_items 62592.
I0302 18:59:13.572638 22699590365312 run.py:483] Algo bellman_ford step 1956 current loss 0.449250, current_train_items 62624.
I0302 18:59:13.595944 22699590365312 run.py:483] Algo bellman_ford step 1957 current loss 0.895710, current_train_items 62656.
I0302 18:59:13.625916 22699590365312 run.py:483] Algo bellman_ford step 1958 current loss 0.870926, current_train_items 62688.
I0302 18:59:13.659568 22699590365312 run.py:483] Algo bellman_ford step 1959 current loss 0.874239, current_train_items 62720.
I0302 18:59:13.679271 22699590365312 run.py:483] Algo bellman_ford step 1960 current loss 0.380134, current_train_items 62752.
I0302 18:59:13.695792 22699590365312 run.py:483] Algo bellman_ford step 1961 current loss 0.675735, current_train_items 62784.
I0302 18:59:13.718722 22699590365312 run.py:483] Algo bellman_ford step 1962 current loss 0.812046, current_train_items 62816.
I0302 18:59:13.747310 22699590365312 run.py:483] Algo bellman_ford step 1963 current loss 0.776800, current_train_items 62848.
I0302 18:59:13.782069 22699590365312 run.py:483] Algo bellman_ford step 1964 current loss 1.071950, current_train_items 62880.
I0302 18:59:13.801180 22699590365312 run.py:483] Algo bellman_ford step 1965 current loss 0.356130, current_train_items 62912.
I0302 18:59:13.817345 22699590365312 run.py:483] Algo bellman_ford step 1966 current loss 0.462339, current_train_items 62944.
I0302 18:59:13.840415 22699590365312 run.py:483] Algo bellman_ford step 1967 current loss 0.712744, current_train_items 62976.
I0302 18:59:13.869324 22699590365312 run.py:483] Algo bellman_ford step 1968 current loss 0.755396, current_train_items 63008.
I0302 18:59:13.901121 22699590365312 run.py:483] Algo bellman_ford step 1969 current loss 0.913220, current_train_items 63040.
I0302 18:59:13.920349 22699590365312 run.py:483] Algo bellman_ford step 1970 current loss 0.290344, current_train_items 63072.
I0302 18:59:13.936390 22699590365312 run.py:483] Algo bellman_ford step 1971 current loss 0.492024, current_train_items 63104.
I0302 18:59:13.958862 22699590365312 run.py:483] Algo bellman_ford step 1972 current loss 0.644711, current_train_items 63136.
I0302 18:59:13.988377 22699590365312 run.py:483] Algo bellman_ford step 1973 current loss 0.836372, current_train_items 63168.
I0302 18:59:14.024018 22699590365312 run.py:483] Algo bellman_ford step 1974 current loss 1.093830, current_train_items 63200.
I0302 18:59:14.043548 22699590365312 run.py:483] Algo bellman_ford step 1975 current loss 0.287414, current_train_items 63232.
I0302 18:59:14.059485 22699590365312 run.py:483] Algo bellman_ford step 1976 current loss 0.453769, current_train_items 63264.
I0302 18:59:14.081387 22699590365312 run.py:483] Algo bellman_ford step 1977 current loss 0.572802, current_train_items 63296.
I0302 18:59:14.110169 22699590365312 run.py:483] Algo bellman_ford step 1978 current loss 0.796169, current_train_items 63328.
I0302 18:59:14.142917 22699590365312 run.py:483] Algo bellman_ford step 1979 current loss 1.095163, current_train_items 63360.
I0302 18:59:14.161744 22699590365312 run.py:483] Algo bellman_ford step 1980 current loss 0.359023, current_train_items 63392.
I0302 18:59:14.178025 22699590365312 run.py:483] Algo bellman_ford step 1981 current loss 0.555763, current_train_items 63424.
I0302 18:59:14.201032 22699590365312 run.py:483] Algo bellman_ford step 1982 current loss 0.615845, current_train_items 63456.
I0302 18:59:14.229294 22699590365312 run.py:483] Algo bellman_ford step 1983 current loss 0.654375, current_train_items 63488.
I0302 18:59:14.264381 22699590365312 run.py:483] Algo bellman_ford step 1984 current loss 0.921787, current_train_items 63520.
I0302 18:59:14.284039 22699590365312 run.py:483] Algo bellman_ford step 1985 current loss 0.255373, current_train_items 63552.
I0302 18:59:14.300243 22699590365312 run.py:483] Algo bellman_ford step 1986 current loss 0.517193, current_train_items 63584.
I0302 18:59:14.322225 22699590365312 run.py:483] Algo bellman_ford step 1987 current loss 0.622124, current_train_items 63616.
I0302 18:59:14.350646 22699590365312 run.py:483] Algo bellman_ford step 1988 current loss 0.727805, current_train_items 63648.
I0302 18:59:14.383736 22699590365312 run.py:483] Algo bellman_ford step 1989 current loss 0.993297, current_train_items 63680.
I0302 18:59:14.403082 22699590365312 run.py:483] Algo bellman_ford step 1990 current loss 0.395114, current_train_items 63712.
I0302 18:59:14.419325 22699590365312 run.py:483] Algo bellman_ford step 1991 current loss 0.532741, current_train_items 63744.
I0302 18:59:14.442597 22699590365312 run.py:483] Algo bellman_ford step 1992 current loss 0.669025, current_train_items 63776.
I0302 18:59:14.471995 22699590365312 run.py:483] Algo bellman_ford step 1993 current loss 0.731125, current_train_items 63808.
I0302 18:59:14.500357 22699590365312 run.py:483] Algo bellman_ford step 1994 current loss 0.734774, current_train_items 63840.
I0302 18:59:14.519519 22699590365312 run.py:483] Algo bellman_ford step 1995 current loss 0.367150, current_train_items 63872.
I0302 18:59:14.536226 22699590365312 run.py:483] Algo bellman_ford step 1996 current loss 0.683008, current_train_items 63904.
I0302 18:59:14.559025 22699590365312 run.py:483] Algo bellman_ford step 1997 current loss 0.819544, current_train_items 63936.
I0302 18:59:14.589715 22699590365312 run.py:483] Algo bellman_ford step 1998 current loss 0.852913, current_train_items 63968.
I0302 18:59:14.621271 22699590365312 run.py:483] Algo bellman_ford step 1999 current loss 0.915997, current_train_items 64000.
I0302 18:59:14.640610 22699590365312 run.py:483] Algo bellman_ford step 2000 current loss 0.286857, current_train_items 64032.
I0302 18:59:14.648626 22699590365312 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.84765625, 'score': 0.84765625, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0302 18:59:14.648732 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.848, val scores are: bellman_ford: 0.848
I0302 18:59:14.665388 22699590365312 run.py:483] Algo bellman_ford step 2001 current loss 0.467097, current_train_items 64064.
I0302 18:59:14.689710 22699590365312 run.py:483] Algo bellman_ford step 2002 current loss 0.927255, current_train_items 64096.
I0302 18:59:14.719507 22699590365312 run.py:483] Algo bellman_ford step 2003 current loss 0.778306, current_train_items 64128.
I0302 18:59:14.752093 22699590365312 run.py:483] Algo bellman_ford step 2004 current loss 0.935722, current_train_items 64160.
I0302 18:59:14.771726 22699590365312 run.py:483] Algo bellman_ford step 2005 current loss 0.347837, current_train_items 64192.
I0302 18:59:14.787808 22699590365312 run.py:483] Algo bellman_ford step 2006 current loss 0.501298, current_train_items 64224.
I0302 18:59:14.811088 22699590365312 run.py:483] Algo bellman_ford step 2007 current loss 0.796426, current_train_items 64256.
I0302 18:59:14.841268 22699590365312 run.py:483] Algo bellman_ford step 2008 current loss 0.929668, current_train_items 64288.
I0302 18:59:14.873000 22699590365312 run.py:483] Algo bellman_ford step 2009 current loss 1.043362, current_train_items 64320.
I0302 18:59:14.892137 22699590365312 run.py:483] Algo bellman_ford step 2010 current loss 0.367906, current_train_items 64352.
I0302 18:59:14.908412 22699590365312 run.py:483] Algo bellman_ford step 2011 current loss 0.470971, current_train_items 64384.
I0302 18:59:14.931614 22699590365312 run.py:483] Algo bellman_ford step 2012 current loss 0.705225, current_train_items 64416.
I0302 18:59:14.960084 22699590365312 run.py:483] Algo bellman_ford step 2013 current loss 0.840910, current_train_items 64448.
I0302 18:59:14.992362 22699590365312 run.py:483] Algo bellman_ford step 2014 current loss 1.043461, current_train_items 64480.
I0302 18:59:15.011073 22699590365312 run.py:483] Algo bellman_ford step 2015 current loss 0.348665, current_train_items 64512.
I0302 18:59:15.027771 22699590365312 run.py:483] Algo bellman_ford step 2016 current loss 0.579836, current_train_items 64544.
I0302 18:59:15.050995 22699590365312 run.py:483] Algo bellman_ford step 2017 current loss 0.834229, current_train_items 64576.
I0302 18:59:15.078375 22699590365312 run.py:483] Algo bellman_ford step 2018 current loss 0.608570, current_train_items 64608.
I0302 18:59:15.111879 22699590365312 run.py:483] Algo bellman_ford step 2019 current loss 1.040201, current_train_items 64640.
I0302 18:59:15.130589 22699590365312 run.py:483] Algo bellman_ford step 2020 current loss 0.332396, current_train_items 64672.
I0302 18:59:15.146719 22699590365312 run.py:483] Algo bellman_ford step 2021 current loss 0.498257, current_train_items 64704.
I0302 18:59:15.170287 22699590365312 run.py:483] Algo bellman_ford step 2022 current loss 0.858195, current_train_items 64736.
I0302 18:59:15.199661 22699590365312 run.py:483] Algo bellman_ford step 2023 current loss 0.709866, current_train_items 64768.
I0302 18:59:15.235109 22699590365312 run.py:483] Algo bellman_ford step 2024 current loss 0.900702, current_train_items 64800.
I0302 18:59:15.254220 22699590365312 run.py:483] Algo bellman_ford step 2025 current loss 0.387123, current_train_items 64832.
I0302 18:59:15.270194 22699590365312 run.py:483] Algo bellman_ford step 2026 current loss 0.502058, current_train_items 64864.
I0302 18:59:15.293544 22699590365312 run.py:483] Algo bellman_ford step 2027 current loss 0.685791, current_train_items 64896.
I0302 18:59:15.322802 22699590365312 run.py:483] Algo bellman_ford step 2028 current loss 0.741881, current_train_items 64928.
I0302 18:59:15.354518 22699590365312 run.py:483] Algo bellman_ford step 2029 current loss 0.844964, current_train_items 64960.
I0302 18:59:15.373945 22699590365312 run.py:483] Algo bellman_ford step 2030 current loss 0.341414, current_train_items 64992.
I0302 18:59:15.390479 22699590365312 run.py:483] Algo bellman_ford step 2031 current loss 0.590746, current_train_items 65024.
I0302 18:59:15.412856 22699590365312 run.py:483] Algo bellman_ford step 2032 current loss 0.598616, current_train_items 65056.
I0302 18:59:15.443325 22699590365312 run.py:483] Algo bellman_ford step 2033 current loss 0.775926, current_train_items 65088.
I0302 18:59:15.474838 22699590365312 run.py:483] Algo bellman_ford step 2034 current loss 0.812318, current_train_items 65120.
I0302 18:59:15.494068 22699590365312 run.py:483] Algo bellman_ford step 2035 current loss 0.352359, current_train_items 65152.
I0302 18:59:15.509986 22699590365312 run.py:483] Algo bellman_ford step 2036 current loss 0.495404, current_train_items 65184.
I0302 18:59:15.532736 22699590365312 run.py:483] Algo bellman_ford step 2037 current loss 0.768102, current_train_items 65216.
I0302 18:59:15.561979 22699590365312 run.py:483] Algo bellman_ford step 2038 current loss 0.806256, current_train_items 65248.
I0302 18:59:15.594311 22699590365312 run.py:483] Algo bellman_ford step 2039 current loss 0.872660, current_train_items 65280.
I0302 18:59:15.613468 22699590365312 run.py:483] Algo bellman_ford step 2040 current loss 0.346730, current_train_items 65312.
I0302 18:59:15.629381 22699590365312 run.py:483] Algo bellman_ford step 2041 current loss 0.534815, current_train_items 65344.
I0302 18:59:15.653488 22699590365312 run.py:483] Algo bellman_ford step 2042 current loss 0.749568, current_train_items 65376.
I0302 18:59:15.683376 22699590365312 run.py:483] Algo bellman_ford step 2043 current loss 0.729592, current_train_items 65408.
I0302 18:59:15.710346 22699590365312 run.py:483] Algo bellman_ford step 2044 current loss 0.843421, current_train_items 65440.
I0302 18:59:15.729568 22699590365312 run.py:483] Algo bellman_ford step 2045 current loss 0.330562, current_train_items 65472.
I0302 18:59:15.746268 22699590365312 run.py:483] Algo bellman_ford step 2046 current loss 0.558186, current_train_items 65504.
I0302 18:59:15.769344 22699590365312 run.py:483] Algo bellman_ford step 2047 current loss 0.746650, current_train_items 65536.
I0302 18:59:15.798286 22699590365312 run.py:483] Algo bellman_ford step 2048 current loss 0.700881, current_train_items 65568.
I0302 18:59:15.828315 22699590365312 run.py:483] Algo bellman_ford step 2049 current loss 0.978737, current_train_items 65600.
I0302 18:59:15.847494 22699590365312 run.py:483] Algo bellman_ford step 2050 current loss 0.377030, current_train_items 65632.
I0302 18:59:15.855870 22699590365312 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.8662109375, 'score': 0.8662109375, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0302 18:59:15.855977 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.866, val scores are: bellman_ford: 0.866
I0302 18:59:15.873476 22699590365312 run.py:483] Algo bellman_ford step 2051 current loss 0.644766, current_train_items 65664.
I0302 18:59:15.896899 22699590365312 run.py:483] Algo bellman_ford step 2052 current loss 0.647748, current_train_items 65696.
I0302 18:59:15.926165 22699590365312 run.py:483] Algo bellman_ford step 2053 current loss 0.946987, current_train_items 65728.
I0302 18:59:15.961706 22699590365312 run.py:483] Algo bellman_ford step 2054 current loss 1.301959, current_train_items 65760.
I0302 18:59:15.981245 22699590365312 run.py:483] Algo bellman_ford step 2055 current loss 0.346192, current_train_items 65792.
I0302 18:59:15.996997 22699590365312 run.py:483] Algo bellman_ford step 2056 current loss 0.469244, current_train_items 65824.
I0302 18:59:16.019774 22699590365312 run.py:483] Algo bellman_ford step 2057 current loss 0.687030, current_train_items 65856.
I0302 18:59:16.049022 22699590365312 run.py:483] Algo bellman_ford step 2058 current loss 0.646018, current_train_items 65888.
I0302 18:59:16.081675 22699590365312 run.py:483] Algo bellman_ford step 2059 current loss 0.913084, current_train_items 65920.
I0302 18:59:16.100969 22699590365312 run.py:483] Algo bellman_ford step 2060 current loss 0.365379, current_train_items 65952.
I0302 18:59:16.117341 22699590365312 run.py:483] Algo bellman_ford step 2061 current loss 0.470110, current_train_items 65984.
I0302 18:59:16.140314 22699590365312 run.py:483] Algo bellman_ford step 2062 current loss 0.671955, current_train_items 66016.
I0302 18:59:16.170199 22699590365312 run.py:483] Algo bellman_ford step 2063 current loss 0.781176, current_train_items 66048.
I0302 18:59:16.204256 22699590365312 run.py:483] Algo bellman_ford step 2064 current loss 0.878124, current_train_items 66080.
I0302 18:59:16.223353 22699590365312 run.py:483] Algo bellman_ford step 2065 current loss 0.306323, current_train_items 66112.
I0302 18:59:16.239689 22699590365312 run.py:483] Algo bellman_ford step 2066 current loss 0.497669, current_train_items 66144.
I0302 18:59:16.262878 22699590365312 run.py:483] Algo bellman_ford step 2067 current loss 0.678189, current_train_items 66176.
I0302 18:59:16.292865 22699590365312 run.py:483] Algo bellman_ford step 2068 current loss 0.815105, current_train_items 66208.
I0302 18:59:16.325200 22699590365312 run.py:483] Algo bellman_ford step 2069 current loss 0.878694, current_train_items 66240.
I0302 18:59:16.344511 22699590365312 run.py:483] Algo bellman_ford step 2070 current loss 0.343076, current_train_items 66272.
I0302 18:59:16.360923 22699590365312 run.py:483] Algo bellman_ford step 2071 current loss 0.540600, current_train_items 66304.
I0302 18:59:16.384363 22699590365312 run.py:483] Algo bellman_ford step 2072 current loss 0.628508, current_train_items 66336.
I0302 18:59:16.414053 22699590365312 run.py:483] Algo bellman_ford step 2073 current loss 0.958758, current_train_items 66368.
I0302 18:59:16.446784 22699590365312 run.py:483] Algo bellman_ford step 2074 current loss 0.897333, current_train_items 66400.
I0302 18:59:16.466163 22699590365312 run.py:483] Algo bellman_ford step 2075 current loss 0.368213, current_train_items 66432.
I0302 18:59:16.482812 22699590365312 run.py:483] Algo bellman_ford step 2076 current loss 0.565853, current_train_items 66464.
I0302 18:59:16.505980 22699590365312 run.py:483] Algo bellman_ford step 2077 current loss 0.694664, current_train_items 66496.
I0302 18:59:16.533408 22699590365312 run.py:483] Algo bellman_ford step 2078 current loss 0.710983, current_train_items 66528.
I0302 18:59:16.565250 22699590365312 run.py:483] Algo bellman_ford step 2079 current loss 0.819252, current_train_items 66560.
I0302 18:59:16.584302 22699590365312 run.py:483] Algo bellman_ford step 2080 current loss 0.273223, current_train_items 66592.
I0302 18:59:16.600519 22699590365312 run.py:483] Algo bellman_ford step 2081 current loss 0.471453, current_train_items 66624.
I0302 18:59:16.623632 22699590365312 run.py:483] Algo bellman_ford step 2082 current loss 0.718835, current_train_items 66656.
I0302 18:59:16.654398 22699590365312 run.py:483] Algo bellman_ford step 2083 current loss 0.852150, current_train_items 66688.
I0302 18:59:16.687689 22699590365312 run.py:483] Algo bellman_ford step 2084 current loss 0.921662, current_train_items 66720.
I0302 18:59:16.707225 22699590365312 run.py:483] Algo bellman_ford step 2085 current loss 0.327776, current_train_items 66752.
I0302 18:59:16.723455 22699590365312 run.py:483] Algo bellman_ford step 2086 current loss 0.646119, current_train_items 66784.
I0302 18:59:16.746479 22699590365312 run.py:483] Algo bellman_ford step 2087 current loss 0.821580, current_train_items 66816.
I0302 18:59:16.774015 22699590365312 run.py:483] Algo bellman_ford step 2088 current loss 0.616678, current_train_items 66848.
I0302 18:59:16.806580 22699590365312 run.py:483] Algo bellman_ford step 2089 current loss 0.841619, current_train_items 66880.
I0302 18:59:16.825986 22699590365312 run.py:483] Algo bellman_ford step 2090 current loss 0.298021, current_train_items 66912.
I0302 18:59:16.842231 22699590365312 run.py:483] Algo bellman_ford step 2091 current loss 0.562203, current_train_items 66944.
I0302 18:59:16.864926 22699590365312 run.py:483] Algo bellman_ford step 2092 current loss 0.875286, current_train_items 66976.
I0302 18:59:16.894219 22699590365312 run.py:483] Algo bellman_ford step 2093 current loss 0.849323, current_train_items 67008.
I0302 18:59:16.926882 22699590365312 run.py:483] Algo bellman_ford step 2094 current loss 0.972040, current_train_items 67040.
I0302 18:59:16.946110 22699590365312 run.py:483] Algo bellman_ford step 2095 current loss 0.300980, current_train_items 67072.
I0302 18:59:16.962471 22699590365312 run.py:483] Algo bellman_ford step 2096 current loss 0.450515, current_train_items 67104.
I0302 18:59:16.985308 22699590365312 run.py:483] Algo bellman_ford step 2097 current loss 0.730970, current_train_items 67136.
I0302 18:59:17.013804 22699590365312 run.py:483] Algo bellman_ford step 2098 current loss 0.841255, current_train_items 67168.
I0302 18:59:17.045668 22699590365312 run.py:483] Algo bellman_ford step 2099 current loss 1.090598, current_train_items 67200.
I0302 18:59:17.065483 22699590365312 run.py:483] Algo bellman_ford step 2100 current loss 0.315031, current_train_items 67232.
I0302 18:59:17.073415 22699590365312 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0302 18:59:17.073524 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 18:59:17.090092 22699590365312 run.py:483] Algo bellman_ford step 2101 current loss 0.446249, current_train_items 67264.
I0302 18:59:17.113124 22699590365312 run.py:483] Algo bellman_ford step 2102 current loss 0.615610, current_train_items 67296.
I0302 18:59:17.142845 22699590365312 run.py:483] Algo bellman_ford step 2103 current loss 0.839767, current_train_items 67328.
I0302 18:59:17.175301 22699590365312 run.py:483] Algo bellman_ford step 2104 current loss 0.871487, current_train_items 67360.
I0302 18:59:17.194848 22699590365312 run.py:483] Algo bellman_ford step 2105 current loss 0.382103, current_train_items 67392.
I0302 18:59:17.209933 22699590365312 run.py:483] Algo bellman_ford step 2106 current loss 0.470863, current_train_items 67424.
I0302 18:59:17.233578 22699590365312 run.py:483] Algo bellman_ford step 2107 current loss 0.679427, current_train_items 67456.
I0302 18:59:17.264288 22699590365312 run.py:483] Algo bellman_ford step 2108 current loss 0.873372, current_train_items 67488.
I0302 18:59:17.295017 22699590365312 run.py:483] Algo bellman_ford step 2109 current loss 0.798320, current_train_items 67520.
I0302 18:59:17.314028 22699590365312 run.py:483] Algo bellman_ford step 2110 current loss 0.365174, current_train_items 67552.
I0302 18:59:17.330193 22699590365312 run.py:483] Algo bellman_ford step 2111 current loss 0.594583, current_train_items 67584.
I0302 18:59:17.352699 22699590365312 run.py:483] Algo bellman_ford step 2112 current loss 0.793189, current_train_items 67616.
I0302 18:59:17.381218 22699590365312 run.py:483] Algo bellman_ford step 2113 current loss 0.736786, current_train_items 67648.
I0302 18:59:17.412299 22699590365312 run.py:483] Algo bellman_ford step 2114 current loss 0.843879, current_train_items 67680.
I0302 18:59:17.431548 22699590365312 run.py:483] Algo bellman_ford step 2115 current loss 0.341754, current_train_items 67712.
I0302 18:59:17.447666 22699590365312 run.py:483] Algo bellman_ford step 2116 current loss 0.466563, current_train_items 67744.
I0302 18:59:17.471485 22699590365312 run.py:483] Algo bellman_ford step 2117 current loss 0.754404, current_train_items 67776.
I0302 18:59:17.501037 22699590365312 run.py:483] Algo bellman_ford step 2118 current loss 0.857645, current_train_items 67808.
I0302 18:59:17.532579 22699590365312 run.py:483] Algo bellman_ford step 2119 current loss 0.964525, current_train_items 67840.
I0302 18:59:17.551983 22699590365312 run.py:483] Algo bellman_ford step 2120 current loss 0.317897, current_train_items 67872.
I0302 18:59:17.567801 22699590365312 run.py:483] Algo bellman_ford step 2121 current loss 0.518102, current_train_items 67904.
I0302 18:59:17.590508 22699590365312 run.py:483] Algo bellman_ford step 2122 current loss 0.573682, current_train_items 67936.
I0302 18:59:17.620066 22699590365312 run.py:483] Algo bellman_ford step 2123 current loss 0.681885, current_train_items 67968.
I0302 18:59:17.648385 22699590365312 run.py:483] Algo bellman_ford step 2124 current loss 0.905662, current_train_items 68000.
I0302 18:59:17.667292 22699590365312 run.py:483] Algo bellman_ford step 2125 current loss 0.280830, current_train_items 68032.
I0302 18:59:17.683754 22699590365312 run.py:483] Algo bellman_ford step 2126 current loss 0.542952, current_train_items 68064.
I0302 18:59:17.708227 22699590365312 run.py:483] Algo bellman_ford step 2127 current loss 0.792014, current_train_items 68096.
I0302 18:59:17.737308 22699590365312 run.py:483] Algo bellman_ford step 2128 current loss 0.810520, current_train_items 68128.
I0302 18:59:17.769349 22699590365312 run.py:483] Algo bellman_ford step 2129 current loss 0.896818, current_train_items 68160.
I0302 18:59:17.788505 22699590365312 run.py:483] Algo bellman_ford step 2130 current loss 0.324491, current_train_items 68192.
I0302 18:59:17.805193 22699590365312 run.py:483] Algo bellman_ford step 2131 current loss 0.501907, current_train_items 68224.
I0302 18:59:17.828708 22699590365312 run.py:483] Algo bellman_ford step 2132 current loss 0.701972, current_train_items 68256.
I0302 18:59:17.857419 22699590365312 run.py:483] Algo bellman_ford step 2133 current loss 0.829480, current_train_items 68288.
I0302 18:59:17.890795 22699590365312 run.py:483] Algo bellman_ford step 2134 current loss 0.911526, current_train_items 68320.
I0302 18:59:17.909783 22699590365312 run.py:483] Algo bellman_ford step 2135 current loss 0.321837, current_train_items 68352.
I0302 18:59:17.926076 22699590365312 run.py:483] Algo bellman_ford step 2136 current loss 0.486373, current_train_items 68384.
I0302 18:59:17.948536 22699590365312 run.py:483] Algo bellman_ford step 2137 current loss 0.732935, current_train_items 68416.
I0302 18:59:17.977837 22699590365312 run.py:483] Algo bellman_ford step 2138 current loss 0.863949, current_train_items 68448.
I0302 18:59:18.013060 22699590365312 run.py:483] Algo bellman_ford step 2139 current loss 1.134069, current_train_items 68480.
I0302 18:59:18.031870 22699590365312 run.py:483] Algo bellman_ford step 2140 current loss 0.389444, current_train_items 68512.
I0302 18:59:18.047854 22699590365312 run.py:483] Algo bellman_ford step 2141 current loss 0.478895, current_train_items 68544.
I0302 18:59:18.069509 22699590365312 run.py:483] Algo bellman_ford step 2142 current loss 0.531748, current_train_items 68576.
I0302 18:59:18.098472 22699590365312 run.py:483] Algo bellman_ford step 2143 current loss 0.687924, current_train_items 68608.
I0302 18:59:18.130686 22699590365312 run.py:483] Algo bellman_ford step 2144 current loss 0.959542, current_train_items 68640.
I0302 18:59:18.149803 22699590365312 run.py:483] Algo bellman_ford step 2145 current loss 0.263433, current_train_items 68672.
I0302 18:59:18.166411 22699590365312 run.py:483] Algo bellman_ford step 2146 current loss 0.485932, current_train_items 68704.
I0302 18:59:18.190186 22699590365312 run.py:483] Algo bellman_ford step 2147 current loss 0.897541, current_train_items 68736.
I0302 18:59:18.220131 22699590365312 run.py:483] Algo bellman_ford step 2148 current loss 0.749663, current_train_items 68768.
I0302 18:59:18.251136 22699590365312 run.py:483] Algo bellman_ford step 2149 current loss 0.791266, current_train_items 68800.
I0302 18:59:18.270307 22699590365312 run.py:483] Algo bellman_ford step 2150 current loss 0.382075, current_train_items 68832.
I0302 18:59:18.278518 22699590365312 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.8876953125, 'score': 0.8876953125, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0302 18:59:18.278625 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.888, val scores are: bellman_ford: 0.888
I0302 18:59:18.295420 22699590365312 run.py:483] Algo bellman_ford step 2151 current loss 0.539926, current_train_items 68864.
I0302 18:59:18.318762 22699590365312 run.py:483] Algo bellman_ford step 2152 current loss 0.702955, current_train_items 68896.
I0302 18:59:18.348075 22699590365312 run.py:483] Algo bellman_ford step 2153 current loss 0.769125, current_train_items 68928.
I0302 18:59:18.380315 22699590365312 run.py:483] Algo bellman_ford step 2154 current loss 0.832655, current_train_items 68960.
I0302 18:59:18.399553 22699590365312 run.py:483] Algo bellman_ford step 2155 current loss 0.381319, current_train_items 68992.
I0302 18:59:18.415404 22699590365312 run.py:483] Algo bellman_ford step 2156 current loss 0.462580, current_train_items 69024.
I0302 18:59:18.438471 22699590365312 run.py:483] Algo bellman_ford step 2157 current loss 0.641259, current_train_items 69056.
I0302 18:59:18.468018 22699590365312 run.py:483] Algo bellman_ford step 2158 current loss 0.750106, current_train_items 69088.
I0302 18:59:18.500318 22699590365312 run.py:483] Algo bellman_ford step 2159 current loss 1.004701, current_train_items 69120.
I0302 18:59:18.519557 22699590365312 run.py:483] Algo bellman_ford step 2160 current loss 0.344371, current_train_items 69152.
I0302 18:59:18.535575 22699590365312 run.py:483] Algo bellman_ford step 2161 current loss 0.442661, current_train_items 69184.
I0302 18:59:18.558559 22699590365312 run.py:483] Algo bellman_ford step 2162 current loss 0.702831, current_train_items 69216.
I0302 18:59:18.586302 22699590365312 run.py:483] Algo bellman_ford step 2163 current loss 0.707880, current_train_items 69248.
I0302 18:59:18.619632 22699590365312 run.py:483] Algo bellman_ford step 2164 current loss 0.946067, current_train_items 69280.
I0302 18:59:18.638519 22699590365312 run.py:483] Algo bellman_ford step 2165 current loss 0.339898, current_train_items 69312.
I0302 18:59:18.655109 22699590365312 run.py:483] Algo bellman_ford step 2166 current loss 0.583133, current_train_items 69344.
I0302 18:59:18.677590 22699590365312 run.py:483] Algo bellman_ford step 2167 current loss 0.611300, current_train_items 69376.
I0302 18:59:18.705686 22699590365312 run.py:483] Algo bellman_ford step 2168 current loss 0.710482, current_train_items 69408.
I0302 18:59:18.737879 22699590365312 run.py:483] Algo bellman_ford step 2169 current loss 1.017538, current_train_items 69440.
I0302 18:59:18.756970 22699590365312 run.py:483] Algo bellman_ford step 2170 current loss 0.297040, current_train_items 69472.
I0302 18:59:18.772968 22699590365312 run.py:483] Algo bellman_ford step 2171 current loss 0.516076, current_train_items 69504.
I0302 18:59:18.795382 22699590365312 run.py:483] Algo bellman_ford step 2172 current loss 0.688120, current_train_items 69536.
I0302 18:59:18.825716 22699590365312 run.py:483] Algo bellman_ford step 2173 current loss 0.763364, current_train_items 69568.
I0302 18:59:18.856090 22699590365312 run.py:483] Algo bellman_ford step 2174 current loss 0.802434, current_train_items 69600.
I0302 18:59:18.875232 22699590365312 run.py:483] Algo bellman_ford step 2175 current loss 0.305444, current_train_items 69632.
I0302 18:59:18.890995 22699590365312 run.py:483] Algo bellman_ford step 2176 current loss 0.498396, current_train_items 69664.
I0302 18:59:18.914412 22699590365312 run.py:483] Algo bellman_ford step 2177 current loss 0.844532, current_train_items 69696.
I0302 18:59:18.942520 22699590365312 run.py:483] Algo bellman_ford step 2178 current loss 0.773228, current_train_items 69728.
I0302 18:59:18.975217 22699590365312 run.py:483] Algo bellman_ford step 2179 current loss 0.792494, current_train_items 69760.
I0302 18:59:18.993729 22699590365312 run.py:483] Algo bellman_ford step 2180 current loss 0.295995, current_train_items 69792.
I0302 18:59:19.009949 22699590365312 run.py:483] Algo bellman_ford step 2181 current loss 0.511471, current_train_items 69824.
I0302 18:59:19.032817 22699590365312 run.py:483] Algo bellman_ford step 2182 current loss 0.783348, current_train_items 69856.
I0302 18:59:19.061328 22699590365312 run.py:483] Algo bellman_ford step 2183 current loss 0.747232, current_train_items 69888.
I0302 18:59:19.094710 22699590365312 run.py:483] Algo bellman_ford step 2184 current loss 0.938892, current_train_items 69920.
I0302 18:59:19.114123 22699590365312 run.py:483] Algo bellman_ford step 2185 current loss 0.335760, current_train_items 69952.
I0302 18:59:19.129756 22699590365312 run.py:483] Algo bellman_ford step 2186 current loss 0.443090, current_train_items 69984.
I0302 18:59:19.152225 22699590365312 run.py:483] Algo bellman_ford step 2187 current loss 0.747938, current_train_items 70016.
I0302 18:59:19.181519 22699590365312 run.py:483] Algo bellman_ford step 2188 current loss 0.830072, current_train_items 70048.
W0302 18:59:19.205851 22699590365312 samplers.py:155] Increasing hint lengh from 12 to 13
I0302 18:59:25.937337 22699590365312 run.py:483] Algo bellman_ford step 2189 current loss 1.134254, current_train_items 70080.
I0302 18:59:25.957736 22699590365312 run.py:483] Algo bellman_ford step 2190 current loss 0.306908, current_train_items 70112.
I0302 18:59:25.974496 22699590365312 run.py:483] Algo bellman_ford step 2191 current loss 0.537877, current_train_items 70144.
I0302 18:59:25.997830 22699590365312 run.py:483] Algo bellman_ford step 2192 current loss 0.760827, current_train_items 70176.
W0302 18:59:26.018856 22699590365312 samplers.py:155] Increasing hint lengh from 10 to 12
I0302 18:59:33.046895 22699590365312 run.py:483] Algo bellman_ford step 2193 current loss 0.969857, current_train_items 70208.
I0302 18:59:33.078971 22699590365312 run.py:483] Algo bellman_ford step 2194 current loss 0.870190, current_train_items 70240.
I0302 18:59:33.098869 22699590365312 run.py:483] Algo bellman_ford step 2195 current loss 0.316052, current_train_items 70272.
I0302 18:59:33.115352 22699590365312 run.py:483] Algo bellman_ford step 2196 current loss 0.506649, current_train_items 70304.
I0302 18:59:33.138330 22699590365312 run.py:483] Algo bellman_ford step 2197 current loss 0.600130, current_train_items 70336.
I0302 18:59:33.166643 22699590365312 run.py:483] Algo bellman_ford step 2198 current loss 0.782393, current_train_items 70368.
I0302 18:59:33.199026 22699590365312 run.py:483] Algo bellman_ford step 2199 current loss 0.789491, current_train_items 70400.
I0302 18:59:33.218656 22699590365312 run.py:483] Algo bellman_ford step 2200 current loss 0.362361, current_train_items 70432.
I0302 18:59:33.227937 22699590365312 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0302 18:59:33.228071 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 18:59:33.245148 22699590365312 run.py:483] Algo bellman_ford step 2201 current loss 0.534402, current_train_items 70464.
I0302 18:59:33.267932 22699590365312 run.py:483] Algo bellman_ford step 2202 current loss 0.557518, current_train_items 70496.
I0302 18:59:33.297616 22699590365312 run.py:483] Algo bellman_ford step 2203 current loss 0.695568, current_train_items 70528.
I0302 18:59:33.331001 22699590365312 run.py:483] Algo bellman_ford step 2204 current loss 0.838002, current_train_items 70560.
I0302 18:59:33.351046 22699590365312 run.py:483] Algo bellman_ford step 2205 current loss 0.259890, current_train_items 70592.
I0302 18:59:33.367313 22699590365312 run.py:483] Algo bellman_ford step 2206 current loss 0.518512, current_train_items 70624.
I0302 18:59:33.389965 22699590365312 run.py:483] Algo bellman_ford step 2207 current loss 0.725744, current_train_items 70656.
I0302 18:59:33.419740 22699590365312 run.py:483] Algo bellman_ford step 2208 current loss 0.780683, current_train_items 70688.
I0302 18:59:33.453717 22699590365312 run.py:483] Algo bellman_ford step 2209 current loss 0.947712, current_train_items 70720.
I0302 18:59:33.473294 22699590365312 run.py:483] Algo bellman_ford step 2210 current loss 0.352590, current_train_items 70752.
I0302 18:59:33.489482 22699590365312 run.py:483] Algo bellman_ford step 2211 current loss 0.503876, current_train_items 70784.
I0302 18:59:33.512965 22699590365312 run.py:483] Algo bellman_ford step 2212 current loss 0.805260, current_train_items 70816.
I0302 18:59:33.543069 22699590365312 run.py:483] Algo bellman_ford step 2213 current loss 0.873073, current_train_items 70848.
I0302 18:59:33.576916 22699590365312 run.py:483] Algo bellman_ford step 2214 current loss 0.873319, current_train_items 70880.
I0302 18:59:33.596510 22699590365312 run.py:483] Algo bellman_ford step 2215 current loss 0.379098, current_train_items 70912.
I0302 18:59:33.612610 22699590365312 run.py:483] Algo bellman_ford step 2216 current loss 0.482456, current_train_items 70944.
I0302 18:59:33.634461 22699590365312 run.py:483] Algo bellman_ford step 2217 current loss 0.718986, current_train_items 70976.
I0302 18:59:33.665091 22699590365312 run.py:483] Algo bellman_ford step 2218 current loss 0.965879, current_train_items 71008.
I0302 18:59:33.696834 22699590365312 run.py:483] Algo bellman_ford step 2219 current loss 1.119961, current_train_items 71040.
I0302 18:59:33.716267 22699590365312 run.py:483] Algo bellman_ford step 2220 current loss 0.320063, current_train_items 71072.
I0302 18:59:33.732395 22699590365312 run.py:483] Algo bellman_ford step 2221 current loss 0.462447, current_train_items 71104.
I0302 18:59:33.754813 22699590365312 run.py:483] Algo bellman_ford step 2222 current loss 0.756255, current_train_items 71136.
I0302 18:59:33.785119 22699590365312 run.py:483] Algo bellman_ford step 2223 current loss 0.892317, current_train_items 71168.
I0302 18:59:33.819324 22699590365312 run.py:483] Algo bellman_ford step 2224 current loss 0.927614, current_train_items 71200.
I0302 18:59:33.838701 22699590365312 run.py:483] Algo bellman_ford step 2225 current loss 0.335817, current_train_items 71232.
I0302 18:59:33.854916 22699590365312 run.py:483] Algo bellman_ford step 2226 current loss 0.499553, current_train_items 71264.
I0302 18:59:33.878141 22699590365312 run.py:483] Algo bellman_ford step 2227 current loss 0.649451, current_train_items 71296.
I0302 18:59:33.908299 22699590365312 run.py:483] Algo bellman_ford step 2228 current loss 0.791238, current_train_items 71328.
I0302 18:59:33.938706 22699590365312 run.py:483] Algo bellman_ford step 2229 current loss 0.923496, current_train_items 71360.
I0302 18:59:33.958246 22699590365312 run.py:483] Algo bellman_ford step 2230 current loss 0.357557, current_train_items 71392.
I0302 18:59:33.974674 22699590365312 run.py:483] Algo bellman_ford step 2231 current loss 0.661050, current_train_items 71424.
I0302 18:59:33.997367 22699590365312 run.py:483] Algo bellman_ford step 2232 current loss 0.705156, current_train_items 71456.
I0302 18:59:34.026561 22699590365312 run.py:483] Algo bellman_ford step 2233 current loss 0.695792, current_train_items 71488.
I0302 18:59:34.059192 22699590365312 run.py:483] Algo bellman_ford step 2234 current loss 1.012982, current_train_items 71520.
I0302 18:59:34.078539 22699590365312 run.py:483] Algo bellman_ford step 2235 current loss 0.304575, current_train_items 71552.
I0302 18:59:34.094901 22699590365312 run.py:483] Algo bellman_ford step 2236 current loss 0.525646, current_train_items 71584.
I0302 18:59:34.118838 22699590365312 run.py:483] Algo bellman_ford step 2237 current loss 0.799011, current_train_items 71616.
I0302 18:59:34.148850 22699590365312 run.py:483] Algo bellman_ford step 2238 current loss 0.793461, current_train_items 71648.
I0302 18:59:34.180746 22699590365312 run.py:483] Algo bellman_ford step 2239 current loss 0.766171, current_train_items 71680.
I0302 18:59:34.200055 22699590365312 run.py:483] Algo bellman_ford step 2240 current loss 0.377200, current_train_items 71712.
I0302 18:59:34.216648 22699590365312 run.py:483] Algo bellman_ford step 2241 current loss 0.560147, current_train_items 71744.
I0302 18:59:34.240942 22699590365312 run.py:483] Algo bellman_ford step 2242 current loss 0.757721, current_train_items 71776.
I0302 18:59:34.270900 22699590365312 run.py:483] Algo bellman_ford step 2243 current loss 0.869009, current_train_items 71808.
I0302 18:59:34.301999 22699590365312 run.py:483] Algo bellman_ford step 2244 current loss 0.793666, current_train_items 71840.
I0302 18:59:34.321752 22699590365312 run.py:483] Algo bellman_ford step 2245 current loss 0.282044, current_train_items 71872.
I0302 18:59:34.338147 22699590365312 run.py:483] Algo bellman_ford step 2246 current loss 0.524835, current_train_items 71904.
I0302 18:59:34.360433 22699590365312 run.py:483] Algo bellman_ford step 2247 current loss 0.797137, current_train_items 71936.
I0302 18:59:34.391047 22699590365312 run.py:483] Algo bellman_ford step 2248 current loss 0.767079, current_train_items 71968.
I0302 18:59:34.424098 22699590365312 run.py:483] Algo bellman_ford step 2249 current loss 0.979872, current_train_items 72000.
I0302 18:59:34.443593 22699590365312 run.py:483] Algo bellman_ford step 2250 current loss 0.335245, current_train_items 72032.
I0302 18:59:34.452023 22699590365312 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.8935546875, 'score': 0.8935546875, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0302 18:59:34.452127 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.894, val scores are: bellman_ford: 0.894
I0302 18:59:34.468802 22699590365312 run.py:483] Algo bellman_ford step 2251 current loss 0.547477, current_train_items 72064.
I0302 18:59:34.491316 22699590365312 run.py:483] Algo bellman_ford step 2252 current loss 0.606875, current_train_items 72096.
I0302 18:59:34.522787 22699590365312 run.py:483] Algo bellman_ford step 2253 current loss 0.862923, current_train_items 72128.
I0302 18:59:34.556762 22699590365312 run.py:483] Algo bellman_ford step 2254 current loss 0.949157, current_train_items 72160.
I0302 18:59:34.576580 22699590365312 run.py:483] Algo bellman_ford step 2255 current loss 0.326390, current_train_items 72192.
I0302 18:59:34.592666 22699590365312 run.py:483] Algo bellman_ford step 2256 current loss 0.504320, current_train_items 72224.
I0302 18:59:34.615540 22699590365312 run.py:483] Algo bellman_ford step 2257 current loss 0.661269, current_train_items 72256.
I0302 18:59:34.644752 22699590365312 run.py:483] Algo bellman_ford step 2258 current loss 0.730257, current_train_items 72288.
I0302 18:59:34.678780 22699590365312 run.py:483] Algo bellman_ford step 2259 current loss 0.918495, current_train_items 72320.
I0302 18:59:34.698541 22699590365312 run.py:483] Algo bellman_ford step 2260 current loss 0.309783, current_train_items 72352.
I0302 18:59:34.715196 22699590365312 run.py:483] Algo bellman_ford step 2261 current loss 0.579370, current_train_items 72384.
I0302 18:59:34.739334 22699590365312 run.py:483] Algo bellman_ford step 2262 current loss 0.753478, current_train_items 72416.
I0302 18:59:34.768311 22699590365312 run.py:483] Algo bellman_ford step 2263 current loss 0.772303, current_train_items 72448.
I0302 18:59:34.801246 22699590365312 run.py:483] Algo bellman_ford step 2264 current loss 0.855690, current_train_items 72480.
I0302 18:59:34.820698 22699590365312 run.py:483] Algo bellman_ford step 2265 current loss 0.359789, current_train_items 72512.
I0302 18:59:34.836775 22699590365312 run.py:483] Algo bellman_ford step 2266 current loss 0.484540, current_train_items 72544.
I0302 18:59:34.860690 22699590365312 run.py:483] Algo bellman_ford step 2267 current loss 0.687667, current_train_items 72576.
I0302 18:59:34.890596 22699590365312 run.py:483] Algo bellman_ford step 2268 current loss 0.803235, current_train_items 72608.
I0302 18:59:34.923170 22699590365312 run.py:483] Algo bellman_ford step 2269 current loss 0.868238, current_train_items 72640.
I0302 18:59:34.942933 22699590365312 run.py:483] Algo bellman_ford step 2270 current loss 0.273767, current_train_items 72672.
I0302 18:59:34.958937 22699590365312 run.py:483] Algo bellman_ford step 2271 current loss 0.485693, current_train_items 72704.
I0302 18:59:34.980677 22699590365312 run.py:483] Algo bellman_ford step 2272 current loss 0.679284, current_train_items 72736.
I0302 18:59:35.011245 22699590365312 run.py:483] Algo bellman_ford step 2273 current loss 0.834270, current_train_items 72768.
I0302 18:59:35.046524 22699590365312 run.py:483] Algo bellman_ford step 2274 current loss 1.023287, current_train_items 72800.
I0302 18:59:35.066656 22699590365312 run.py:483] Algo bellman_ford step 2275 current loss 0.386576, current_train_items 72832.
I0302 18:59:35.082749 22699590365312 run.py:483] Algo bellman_ford step 2276 current loss 0.455697, current_train_items 72864.
I0302 18:59:35.103970 22699590365312 run.py:483] Algo bellman_ford step 2277 current loss 0.594115, current_train_items 72896.
I0302 18:59:35.133126 22699590365312 run.py:483] Algo bellman_ford step 2278 current loss 0.754345, current_train_items 72928.
I0302 18:59:35.166968 22699590365312 run.py:483] Algo bellman_ford step 2279 current loss 0.909374, current_train_items 72960.
I0302 18:59:35.186267 22699590365312 run.py:483] Algo bellman_ford step 2280 current loss 0.320932, current_train_items 72992.
I0302 18:59:35.202124 22699590365312 run.py:483] Algo bellman_ford step 2281 current loss 0.571447, current_train_items 73024.
I0302 18:59:35.225643 22699590365312 run.py:483] Algo bellman_ford step 2282 current loss 0.613247, current_train_items 73056.
I0302 18:59:35.258257 22699590365312 run.py:483] Algo bellman_ford step 2283 current loss 0.872679, current_train_items 73088.
I0302 18:59:35.292285 22699590365312 run.py:483] Algo bellman_ford step 2284 current loss 0.973238, current_train_items 73120.
I0302 18:59:35.311940 22699590365312 run.py:483] Algo bellman_ford step 2285 current loss 0.329584, current_train_items 73152.
I0302 18:59:35.328125 22699590365312 run.py:483] Algo bellman_ford step 2286 current loss 0.476963, current_train_items 73184.
I0302 18:59:35.350804 22699590365312 run.py:483] Algo bellman_ford step 2287 current loss 0.667321, current_train_items 73216.
I0302 18:59:35.380471 22699590365312 run.py:483] Algo bellman_ford step 2288 current loss 0.740025, current_train_items 73248.
I0302 18:59:35.415744 22699590365312 run.py:483] Algo bellman_ford step 2289 current loss 0.843044, current_train_items 73280.
I0302 18:59:35.435235 22699590365312 run.py:483] Algo bellman_ford step 2290 current loss 0.326510, current_train_items 73312.
I0302 18:59:35.450844 22699590365312 run.py:483] Algo bellman_ford step 2291 current loss 0.449492, current_train_items 73344.
I0302 18:59:35.473091 22699590365312 run.py:483] Algo bellman_ford step 2292 current loss 0.649307, current_train_items 73376.
I0302 18:59:35.503900 22699590365312 run.py:483] Algo bellman_ford step 2293 current loss 0.832285, current_train_items 73408.
I0302 18:59:35.537256 22699590365312 run.py:483] Algo bellman_ford step 2294 current loss 1.064151, current_train_items 73440.
I0302 18:59:35.556536 22699590365312 run.py:483] Algo bellman_ford step 2295 current loss 0.291679, current_train_items 73472.
I0302 18:59:35.572811 22699590365312 run.py:483] Algo bellman_ford step 2296 current loss 0.567687, current_train_items 73504.
I0302 18:59:35.594855 22699590365312 run.py:483] Algo bellman_ford step 2297 current loss 0.560630, current_train_items 73536.
I0302 18:59:35.623834 22699590365312 run.py:483] Algo bellman_ford step 2298 current loss 0.729266, current_train_items 73568.
I0302 18:59:35.657122 22699590365312 run.py:483] Algo bellman_ford step 2299 current loss 0.945670, current_train_items 73600.
I0302 18:59:35.676872 22699590365312 run.py:483] Algo bellman_ford step 2300 current loss 0.311772, current_train_items 73632.
I0302 18:59:35.684925 22699590365312 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0302 18:59:35.685030 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 18:59:35.701854 22699590365312 run.py:483] Algo bellman_ford step 2301 current loss 0.610005, current_train_items 73664.
I0302 18:59:35.725445 22699590365312 run.py:483] Algo bellman_ford step 2302 current loss 0.709192, current_train_items 73696.
I0302 18:59:35.757010 22699590365312 run.py:483] Algo bellman_ford step 2303 current loss 0.769254, current_train_items 73728.
I0302 18:59:35.790895 22699590365312 run.py:483] Algo bellman_ford step 2304 current loss 0.992268, current_train_items 73760.
I0302 18:59:35.810989 22699590365312 run.py:483] Algo bellman_ford step 2305 current loss 0.332335, current_train_items 73792.
I0302 18:59:35.827385 22699590365312 run.py:483] Algo bellman_ford step 2306 current loss 0.661400, current_train_items 73824.
I0302 18:59:35.850129 22699590365312 run.py:483] Algo bellman_ford step 2307 current loss 0.708573, current_train_items 73856.
I0302 18:59:35.879425 22699590365312 run.py:483] Algo bellman_ford step 2308 current loss 0.711074, current_train_items 73888.
I0302 18:59:35.913534 22699590365312 run.py:483] Algo bellman_ford step 2309 current loss 0.816836, current_train_items 73920.
I0302 18:59:35.933099 22699590365312 run.py:483] Algo bellman_ford step 2310 current loss 0.403597, current_train_items 73952.
I0302 18:59:35.949337 22699590365312 run.py:483] Algo bellman_ford step 2311 current loss 0.526085, current_train_items 73984.
I0302 18:59:35.972352 22699590365312 run.py:483] Algo bellman_ford step 2312 current loss 0.762719, current_train_items 74016.
I0302 18:59:36.002618 22699590365312 run.py:483] Algo bellman_ford step 2313 current loss 0.726177, current_train_items 74048.
I0302 18:59:36.036286 22699590365312 run.py:483] Algo bellman_ford step 2314 current loss 0.994478, current_train_items 74080.
I0302 18:59:36.056052 22699590365312 run.py:483] Algo bellman_ford step 2315 current loss 0.402053, current_train_items 74112.
I0302 18:59:36.072081 22699590365312 run.py:483] Algo bellman_ford step 2316 current loss 0.647256, current_train_items 74144.
I0302 18:59:36.094997 22699590365312 run.py:483] Algo bellman_ford step 2317 current loss 0.672112, current_train_items 74176.
I0302 18:59:36.125649 22699590365312 run.py:483] Algo bellman_ford step 2318 current loss 0.779989, current_train_items 74208.
I0302 18:59:36.159866 22699590365312 run.py:483] Algo bellman_ford step 2319 current loss 0.851126, current_train_items 74240.
I0302 18:59:36.179579 22699590365312 run.py:483] Algo bellman_ford step 2320 current loss 0.358184, current_train_items 74272.
I0302 18:59:36.195538 22699590365312 run.py:483] Algo bellman_ford step 2321 current loss 0.546810, current_train_items 74304.
I0302 18:59:36.219357 22699590365312 run.py:483] Algo bellman_ford step 2322 current loss 0.777818, current_train_items 74336.
I0302 18:59:36.249149 22699590365312 run.py:483] Algo bellman_ford step 2323 current loss 0.785037, current_train_items 74368.
I0302 18:59:36.282788 22699590365312 run.py:483] Algo bellman_ford step 2324 current loss 0.939895, current_train_items 74400.
I0302 18:59:36.302740 22699590365312 run.py:483] Algo bellman_ford step 2325 current loss 0.302821, current_train_items 74432.
I0302 18:59:36.318764 22699590365312 run.py:483] Algo bellman_ford step 2326 current loss 0.493616, current_train_items 74464.
I0302 18:59:36.341318 22699590365312 run.py:483] Algo bellman_ford step 2327 current loss 0.672307, current_train_items 74496.
I0302 18:59:36.370008 22699590365312 run.py:483] Algo bellman_ford step 2328 current loss 0.653601, current_train_items 74528.
I0302 18:59:36.402289 22699590365312 run.py:483] Algo bellman_ford step 2329 current loss 0.729189, current_train_items 74560.
I0302 18:59:36.421958 22699590365312 run.py:483] Algo bellman_ford step 2330 current loss 0.365238, current_train_items 74592.
I0302 18:59:36.437731 22699590365312 run.py:483] Algo bellman_ford step 2331 current loss 0.530341, current_train_items 74624.
I0302 18:59:36.461465 22699590365312 run.py:483] Algo bellman_ford step 2332 current loss 0.769689, current_train_items 74656.
I0302 18:59:36.492271 22699590365312 run.py:483] Algo bellman_ford step 2333 current loss 0.792332, current_train_items 74688.
I0302 18:59:36.526577 22699590365312 run.py:483] Algo bellman_ford step 2334 current loss 0.823017, current_train_items 74720.
I0302 18:59:36.546072 22699590365312 run.py:483] Algo bellman_ford step 2335 current loss 0.335815, current_train_items 74752.
I0302 18:59:36.562558 22699590365312 run.py:483] Algo bellman_ford step 2336 current loss 0.492248, current_train_items 74784.
I0302 18:59:36.585805 22699590365312 run.py:483] Algo bellman_ford step 2337 current loss 0.714257, current_train_items 74816.
I0302 18:59:36.615501 22699590365312 run.py:483] Algo bellman_ford step 2338 current loss 0.772668, current_train_items 74848.
I0302 18:59:36.648293 22699590365312 run.py:483] Algo bellman_ford step 2339 current loss 0.874305, current_train_items 74880.
I0302 18:59:36.668319 22699590365312 run.py:483] Algo bellman_ford step 2340 current loss 0.369876, current_train_items 74912.
I0302 18:59:36.684700 22699590365312 run.py:483] Algo bellman_ford step 2341 current loss 0.469198, current_train_items 74944.
I0302 18:59:36.706838 22699590365312 run.py:483] Algo bellman_ford step 2342 current loss 0.638886, current_train_items 74976.
I0302 18:59:36.736086 22699590365312 run.py:483] Algo bellman_ford step 2343 current loss 0.768818, current_train_items 75008.
I0302 18:59:36.768001 22699590365312 run.py:483] Algo bellman_ford step 2344 current loss 0.787739, current_train_items 75040.
I0302 18:59:36.787797 22699590365312 run.py:483] Algo bellman_ford step 2345 current loss 0.336569, current_train_items 75072.
I0302 18:59:36.803702 22699590365312 run.py:483] Algo bellman_ford step 2346 current loss 0.441894, current_train_items 75104.
I0302 18:59:36.826764 22699590365312 run.py:483] Algo bellman_ford step 2347 current loss 0.630347, current_train_items 75136.
I0302 18:59:36.855585 22699590365312 run.py:483] Algo bellman_ford step 2348 current loss 0.632293, current_train_items 75168.
I0302 18:59:36.887482 22699590365312 run.py:483] Algo bellman_ford step 2349 current loss 0.773732, current_train_items 75200.
I0302 18:59:36.906991 22699590365312 run.py:483] Algo bellman_ford step 2350 current loss 0.257050, current_train_items 75232.
I0302 18:59:36.915101 22699590365312 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.8740234375, 'score': 0.8740234375, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0302 18:59:36.915220 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.874, val scores are: bellman_ford: 0.874
I0302 18:59:36.932017 22699590365312 run.py:483] Algo bellman_ford step 2351 current loss 0.411045, current_train_items 75264.
I0302 18:59:36.955172 22699590365312 run.py:483] Algo bellman_ford step 2352 current loss 0.656167, current_train_items 75296.
I0302 18:59:36.983848 22699590365312 run.py:483] Algo bellman_ford step 2353 current loss 0.684373, current_train_items 75328.
I0302 18:59:37.018891 22699590365312 run.py:483] Algo bellman_ford step 2354 current loss 0.943536, current_train_items 75360.
I0302 18:59:37.038993 22699590365312 run.py:483] Algo bellman_ford step 2355 current loss 0.375702, current_train_items 75392.
I0302 18:59:37.054488 22699590365312 run.py:483] Algo bellman_ford step 2356 current loss 0.585265, current_train_items 75424.
I0302 18:59:37.077548 22699590365312 run.py:483] Algo bellman_ford step 2357 current loss 0.771895, current_train_items 75456.
I0302 18:59:37.106275 22699590365312 run.py:483] Algo bellman_ford step 2358 current loss 0.722326, current_train_items 75488.
I0302 18:59:37.140905 22699590365312 run.py:483] Algo bellman_ford step 2359 current loss 1.081184, current_train_items 75520.
I0302 18:59:37.160443 22699590365312 run.py:483] Algo bellman_ford step 2360 current loss 0.301703, current_train_items 75552.
I0302 18:59:37.177368 22699590365312 run.py:483] Algo bellman_ford step 2361 current loss 0.613250, current_train_items 75584.
I0302 18:59:37.201089 22699590365312 run.py:483] Algo bellman_ford step 2362 current loss 0.668225, current_train_items 75616.
I0302 18:59:37.230056 22699590365312 run.py:483] Algo bellman_ford step 2363 current loss 0.734445, current_train_items 75648.
I0302 18:59:37.262961 22699590365312 run.py:483] Algo bellman_ford step 2364 current loss 0.906814, current_train_items 75680.
I0302 18:59:37.282221 22699590365312 run.py:483] Algo bellman_ford step 2365 current loss 0.288669, current_train_items 75712.
I0302 18:59:37.298595 22699590365312 run.py:483] Algo bellman_ford step 2366 current loss 0.474344, current_train_items 75744.
I0302 18:59:37.321856 22699590365312 run.py:483] Algo bellman_ford step 2367 current loss 0.737874, current_train_items 75776.
I0302 18:59:37.352589 22699590365312 run.py:483] Algo bellman_ford step 2368 current loss 0.796828, current_train_items 75808.
I0302 18:59:37.386723 22699590365312 run.py:483] Algo bellman_ford step 2369 current loss 0.877428, current_train_items 75840.
I0302 18:59:37.406787 22699590365312 run.py:483] Algo bellman_ford step 2370 current loss 0.306932, current_train_items 75872.
I0302 18:59:37.422818 22699590365312 run.py:483] Algo bellman_ford step 2371 current loss 0.560389, current_train_items 75904.
I0302 18:59:37.445390 22699590365312 run.py:483] Algo bellman_ford step 2372 current loss 0.705376, current_train_items 75936.
I0302 18:59:37.474323 22699590365312 run.py:483] Algo bellman_ford step 2373 current loss 0.813828, current_train_items 75968.
I0302 18:59:37.506414 22699590365312 run.py:483] Algo bellman_ford step 2374 current loss 0.816890, current_train_items 76000.
I0302 18:59:37.525871 22699590365312 run.py:483] Algo bellman_ford step 2375 current loss 0.289767, current_train_items 76032.
I0302 18:59:37.542200 22699590365312 run.py:483] Algo bellman_ford step 2376 current loss 0.568346, current_train_items 76064.
I0302 18:59:37.565039 22699590365312 run.py:483] Algo bellman_ford step 2377 current loss 0.740014, current_train_items 76096.
I0302 18:59:37.596314 22699590365312 run.py:483] Algo bellman_ford step 2378 current loss 0.857069, current_train_items 76128.
I0302 18:59:37.630753 22699590365312 run.py:483] Algo bellman_ford step 2379 current loss 0.998589, current_train_items 76160.
I0302 18:59:37.650420 22699590365312 run.py:483] Algo bellman_ford step 2380 current loss 0.293167, current_train_items 76192.
I0302 18:59:37.666985 22699590365312 run.py:483] Algo bellman_ford step 2381 current loss 0.535499, current_train_items 76224.
I0302 18:59:37.690288 22699590365312 run.py:483] Algo bellman_ford step 2382 current loss 0.769880, current_train_items 76256.
I0302 18:59:37.719439 22699590365312 run.py:483] Algo bellman_ford step 2383 current loss 0.720368, current_train_items 76288.
I0302 18:59:37.752809 22699590365312 run.py:483] Algo bellman_ford step 2384 current loss 1.077243, current_train_items 76320.
I0302 18:59:37.772571 22699590365312 run.py:483] Algo bellman_ford step 2385 current loss 0.297394, current_train_items 76352.
I0302 18:59:37.788472 22699590365312 run.py:483] Algo bellman_ford step 2386 current loss 0.489768, current_train_items 76384.
I0302 18:59:37.810440 22699590365312 run.py:483] Algo bellman_ford step 2387 current loss 0.720438, current_train_items 76416.
I0302 18:59:37.840394 22699590365312 run.py:483] Algo bellman_ford step 2388 current loss 0.851481, current_train_items 76448.
I0302 18:59:37.874989 22699590365312 run.py:483] Algo bellman_ford step 2389 current loss 0.884908, current_train_items 76480.
I0302 18:59:37.895020 22699590365312 run.py:483] Algo bellman_ford step 2390 current loss 0.307220, current_train_items 76512.
I0302 18:59:37.910843 22699590365312 run.py:483] Algo bellman_ford step 2391 current loss 0.483179, current_train_items 76544.
I0302 18:59:37.932143 22699590365312 run.py:483] Algo bellman_ford step 2392 current loss 0.724839, current_train_items 76576.
I0302 18:59:37.961533 22699590365312 run.py:483] Algo bellman_ford step 2393 current loss 0.838564, current_train_items 76608.
I0302 18:59:37.997449 22699590365312 run.py:483] Algo bellman_ford step 2394 current loss 1.052892, current_train_items 76640.
I0302 18:59:38.016880 22699590365312 run.py:483] Algo bellman_ford step 2395 current loss 0.380604, current_train_items 76672.
I0302 18:59:38.032831 22699590365312 run.py:483] Algo bellman_ford step 2396 current loss 0.556348, current_train_items 76704.
I0302 18:59:38.055010 22699590365312 run.py:483] Algo bellman_ford step 2397 current loss 0.624362, current_train_items 76736.
I0302 18:59:38.084180 22699590365312 run.py:483] Algo bellman_ford step 2398 current loss 0.889058, current_train_items 76768.
I0302 18:59:38.118443 22699590365312 run.py:483] Algo bellman_ford step 2399 current loss 1.047117, current_train_items 76800.
I0302 18:59:38.138289 22699590365312 run.py:483] Algo bellman_ford step 2400 current loss 0.265301, current_train_items 76832.
I0302 18:59:38.145918 22699590365312 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0302 18:59:38.146027 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 18:59:38.162534 22699590365312 run.py:483] Algo bellman_ford step 2401 current loss 0.577655, current_train_items 76864.
I0302 18:59:38.185968 22699590365312 run.py:483] Algo bellman_ford step 2402 current loss 0.685985, current_train_items 76896.
I0302 18:59:38.217454 22699590365312 run.py:483] Algo bellman_ford step 2403 current loss 0.913437, current_train_items 76928.
I0302 18:59:38.254132 22699590365312 run.py:483] Algo bellman_ford step 2404 current loss 1.055467, current_train_items 76960.
I0302 18:59:38.274443 22699590365312 run.py:483] Algo bellman_ford step 2405 current loss 0.239151, current_train_items 76992.
I0302 18:59:38.290434 22699590365312 run.py:483] Algo bellman_ford step 2406 current loss 0.526495, current_train_items 77024.
I0302 18:59:38.313996 22699590365312 run.py:483] Algo bellman_ford step 2407 current loss 0.664236, current_train_items 77056.
I0302 18:59:38.342992 22699590365312 run.py:483] Algo bellman_ford step 2408 current loss 0.729344, current_train_items 77088.
I0302 18:59:38.375850 22699590365312 run.py:483] Algo bellman_ford step 2409 current loss 0.946541, current_train_items 77120.
I0302 18:59:38.395290 22699590365312 run.py:483] Algo bellman_ford step 2410 current loss 0.293903, current_train_items 77152.
I0302 18:59:38.411324 22699590365312 run.py:483] Algo bellman_ford step 2411 current loss 0.446450, current_train_items 77184.
I0302 18:59:38.435596 22699590365312 run.py:483] Algo bellman_ford step 2412 current loss 0.936580, current_train_items 77216.
I0302 18:59:38.466711 22699590365312 run.py:483] Algo bellman_ford step 2413 current loss 0.770085, current_train_items 77248.
I0302 18:59:38.500964 22699590365312 run.py:483] Algo bellman_ford step 2414 current loss 1.167373, current_train_items 77280.
I0302 18:59:38.520797 22699590365312 run.py:483] Algo bellman_ford step 2415 current loss 0.340280, current_train_items 77312.
I0302 18:59:38.536602 22699590365312 run.py:483] Algo bellman_ford step 2416 current loss 0.493788, current_train_items 77344.
I0302 18:59:38.559701 22699590365312 run.py:483] Algo bellman_ford step 2417 current loss 0.620815, current_train_items 77376.
I0302 18:59:38.590710 22699590365312 run.py:483] Algo bellman_ford step 2418 current loss 0.781820, current_train_items 77408.
I0302 18:59:38.622149 22699590365312 run.py:483] Algo bellman_ford step 2419 current loss 0.843907, current_train_items 77440.
I0302 18:59:38.641896 22699590365312 run.py:483] Algo bellman_ford step 2420 current loss 0.307009, current_train_items 77472.
I0302 18:59:38.657823 22699590365312 run.py:483] Algo bellman_ford step 2421 current loss 0.513879, current_train_items 77504.
I0302 18:59:38.681598 22699590365312 run.py:483] Algo bellman_ford step 2422 current loss 0.710469, current_train_items 77536.
I0302 18:59:38.711887 22699590365312 run.py:483] Algo bellman_ford step 2423 current loss 0.792136, current_train_items 77568.
I0302 18:59:38.746130 22699590365312 run.py:483] Algo bellman_ford step 2424 current loss 0.923425, current_train_items 77600.
I0302 18:59:38.765649 22699590365312 run.py:483] Algo bellman_ford step 2425 current loss 0.263517, current_train_items 77632.
I0302 18:59:38.781669 22699590365312 run.py:483] Algo bellman_ford step 2426 current loss 0.477920, current_train_items 77664.
I0302 18:59:38.806074 22699590365312 run.py:483] Algo bellman_ford step 2427 current loss 0.767021, current_train_items 77696.
I0302 18:59:38.835072 22699590365312 run.py:483] Algo bellman_ford step 2428 current loss 0.801814, current_train_items 77728.
I0302 18:59:38.867940 22699590365312 run.py:483] Algo bellman_ford step 2429 current loss 0.785064, current_train_items 77760.
I0302 18:59:38.887739 22699590365312 run.py:483] Algo bellman_ford step 2430 current loss 0.384998, current_train_items 77792.
I0302 18:59:38.904122 22699590365312 run.py:483] Algo bellman_ford step 2431 current loss 0.532156, current_train_items 77824.
I0302 18:59:38.927922 22699590365312 run.py:483] Algo bellman_ford step 2432 current loss 0.674451, current_train_items 77856.
I0302 18:59:38.957817 22699590365312 run.py:483] Algo bellman_ford step 2433 current loss 0.678721, current_train_items 77888.
I0302 18:59:38.991727 22699590365312 run.py:483] Algo bellman_ford step 2434 current loss 0.783415, current_train_items 77920.
I0302 18:59:39.011426 22699590365312 run.py:483] Algo bellman_ford step 2435 current loss 0.314301, current_train_items 77952.
I0302 18:59:39.027810 22699590365312 run.py:483] Algo bellman_ford step 2436 current loss 0.558117, current_train_items 77984.
I0302 18:59:39.051206 22699590365312 run.py:483] Algo bellman_ford step 2437 current loss 0.695960, current_train_items 78016.
I0302 18:59:39.079860 22699590365312 run.py:483] Algo bellman_ford step 2438 current loss 0.759762, current_train_items 78048.
I0302 18:59:39.112144 22699590365312 run.py:483] Algo bellman_ford step 2439 current loss 0.755980, current_train_items 78080.
I0302 18:59:39.131930 22699590365312 run.py:483] Algo bellman_ford step 2440 current loss 0.397757, current_train_items 78112.
I0302 18:59:39.148050 22699590365312 run.py:483] Algo bellman_ford step 2441 current loss 0.460366, current_train_items 78144.
I0302 18:59:39.172383 22699590365312 run.py:483] Algo bellman_ford step 2442 current loss 0.873233, current_train_items 78176.
I0302 18:59:39.202497 22699590365312 run.py:483] Algo bellman_ford step 2443 current loss 0.814582, current_train_items 78208.
I0302 18:59:39.237503 22699590365312 run.py:483] Algo bellman_ford step 2444 current loss 0.849176, current_train_items 78240.
I0302 18:59:39.257278 22699590365312 run.py:483] Algo bellman_ford step 2445 current loss 0.360766, current_train_items 78272.
I0302 18:59:39.273440 22699590365312 run.py:483] Algo bellman_ford step 2446 current loss 0.600127, current_train_items 78304.
I0302 18:59:39.297289 22699590365312 run.py:483] Algo bellman_ford step 2447 current loss 0.749407, current_train_items 78336.
I0302 18:59:39.326821 22699590365312 run.py:483] Algo bellman_ford step 2448 current loss 0.786699, current_train_items 78368.
I0302 18:59:39.359723 22699590365312 run.py:483] Algo bellman_ford step 2449 current loss 0.887489, current_train_items 78400.
I0302 18:59:39.379163 22699590365312 run.py:483] Algo bellman_ford step 2450 current loss 0.397447, current_train_items 78432.
I0302 18:59:39.387341 22699590365312 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0302 18:59:39.387446 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 18:59:39.404257 22699590365312 run.py:483] Algo bellman_ford step 2451 current loss 0.445459, current_train_items 78464.
I0302 18:59:39.428224 22699590365312 run.py:483] Algo bellman_ford step 2452 current loss 0.761557, current_train_items 78496.
I0302 18:59:39.458318 22699590365312 run.py:483] Algo bellman_ford step 2453 current loss 0.697018, current_train_items 78528.
I0302 18:59:39.493120 22699590365312 run.py:483] Algo bellman_ford step 2454 current loss 0.861759, current_train_items 78560.
I0302 18:59:39.513220 22699590365312 run.py:483] Algo bellman_ford step 2455 current loss 0.277334, current_train_items 78592.
I0302 18:59:39.529205 22699590365312 run.py:483] Algo bellman_ford step 2456 current loss 0.544807, current_train_items 78624.
I0302 18:59:39.551441 22699590365312 run.py:483] Algo bellman_ford step 2457 current loss 0.641374, current_train_items 78656.
I0302 18:59:39.581567 22699590365312 run.py:483] Algo bellman_ford step 2458 current loss 0.727108, current_train_items 78688.
I0302 18:59:39.616474 22699590365312 run.py:483] Algo bellman_ford step 2459 current loss 0.922067, current_train_items 78720.
I0302 18:59:39.636457 22699590365312 run.py:483] Algo bellman_ford step 2460 current loss 0.289164, current_train_items 78752.
I0302 18:59:39.653130 22699590365312 run.py:483] Algo bellman_ford step 2461 current loss 0.580877, current_train_items 78784.
I0302 18:59:39.676070 22699590365312 run.py:483] Algo bellman_ford step 2462 current loss 0.621905, current_train_items 78816.
I0302 18:59:39.707066 22699590365312 run.py:483] Algo bellman_ford step 2463 current loss 0.817502, current_train_items 78848.
I0302 18:59:39.740435 22699590365312 run.py:483] Algo bellman_ford step 2464 current loss 0.804372, current_train_items 78880.
I0302 18:59:39.759982 22699590365312 run.py:483] Algo bellman_ford step 2465 current loss 0.281399, current_train_items 78912.
I0302 18:59:39.776198 22699590365312 run.py:483] Algo bellman_ford step 2466 current loss 0.536296, current_train_items 78944.
I0302 18:59:39.799818 22699590365312 run.py:483] Algo bellman_ford step 2467 current loss 0.687211, current_train_items 78976.
I0302 18:59:39.830415 22699590365312 run.py:483] Algo bellman_ford step 2468 current loss 0.826650, current_train_items 79008.
I0302 18:59:39.865651 22699590365312 run.py:483] Algo bellman_ford step 2469 current loss 0.914733, current_train_items 79040.
I0302 18:59:39.885787 22699590365312 run.py:483] Algo bellman_ford step 2470 current loss 0.373623, current_train_items 79072.
I0302 18:59:39.902009 22699590365312 run.py:483] Algo bellman_ford step 2471 current loss 0.436645, current_train_items 79104.
I0302 18:59:39.925691 22699590365312 run.py:483] Algo bellman_ford step 2472 current loss 0.794226, current_train_items 79136.
I0302 18:59:39.956903 22699590365312 run.py:483] Algo bellman_ford step 2473 current loss 0.887000, current_train_items 79168.
I0302 18:59:39.989828 22699590365312 run.py:483] Algo bellman_ford step 2474 current loss 0.790599, current_train_items 79200.
I0302 18:59:40.009860 22699590365312 run.py:483] Algo bellman_ford step 2475 current loss 0.281315, current_train_items 79232.
I0302 18:59:40.026027 22699590365312 run.py:483] Algo bellman_ford step 2476 current loss 0.570284, current_train_items 79264.
I0302 18:59:40.048027 22699590365312 run.py:483] Algo bellman_ford step 2477 current loss 0.528633, current_train_items 79296.
I0302 18:59:40.077433 22699590365312 run.py:483] Algo bellman_ford step 2478 current loss 0.910735, current_train_items 79328.
I0302 18:59:40.112266 22699590365312 run.py:483] Algo bellman_ford step 2479 current loss 1.017203, current_train_items 79360.
I0302 18:59:40.131830 22699590365312 run.py:483] Algo bellman_ford step 2480 current loss 0.345338, current_train_items 79392.
I0302 18:59:40.147633 22699590365312 run.py:483] Algo bellman_ford step 2481 current loss 0.519882, current_train_items 79424.
I0302 18:59:40.171482 22699590365312 run.py:483] Algo bellman_ford step 2482 current loss 0.828618, current_train_items 79456.
I0302 18:59:40.201430 22699590365312 run.py:483] Algo bellman_ford step 2483 current loss 0.869061, current_train_items 79488.
I0302 18:59:40.236305 22699590365312 run.py:483] Algo bellman_ford step 2484 current loss 0.984643, current_train_items 79520.
I0302 18:59:40.256637 22699590365312 run.py:483] Algo bellman_ford step 2485 current loss 0.345760, current_train_items 79552.
I0302 18:59:40.272942 22699590365312 run.py:483] Algo bellman_ford step 2486 current loss 0.543555, current_train_items 79584.
I0302 18:59:40.296839 22699590365312 run.py:483] Algo bellman_ford step 2487 current loss 0.712863, current_train_items 79616.
I0302 18:59:40.324242 22699590365312 run.py:483] Algo bellman_ford step 2488 current loss 0.678983, current_train_items 79648.
I0302 18:59:40.358355 22699590365312 run.py:483] Algo bellman_ford step 2489 current loss 0.803071, current_train_items 79680.
I0302 18:59:40.378490 22699590365312 run.py:483] Algo bellman_ford step 2490 current loss 0.296219, current_train_items 79712.
I0302 18:59:40.394632 22699590365312 run.py:483] Algo bellman_ford step 2491 current loss 0.531393, current_train_items 79744.
I0302 18:59:40.417142 22699590365312 run.py:483] Algo bellman_ford step 2492 current loss 0.648671, current_train_items 79776.
I0302 18:59:40.446991 22699590365312 run.py:483] Algo bellman_ford step 2493 current loss 0.746284, current_train_items 79808.
I0302 18:59:40.479584 22699590365312 run.py:483] Algo bellman_ford step 2494 current loss 0.770100, current_train_items 79840.
I0302 18:59:40.499364 22699590365312 run.py:483] Algo bellman_ford step 2495 current loss 0.301011, current_train_items 79872.
I0302 18:59:40.515954 22699590365312 run.py:483] Algo bellman_ford step 2496 current loss 0.450419, current_train_items 79904.
I0302 18:59:40.538784 22699590365312 run.py:483] Algo bellman_ford step 2497 current loss 0.630779, current_train_items 79936.
I0302 18:59:40.567693 22699590365312 run.py:483] Algo bellman_ford step 2498 current loss 0.759530, current_train_items 79968.
I0302 18:59:40.600945 22699590365312 run.py:483] Algo bellman_ford step 2499 current loss 0.944693, current_train_items 80000.
I0302 18:59:40.621117 22699590365312 run.py:483] Algo bellman_ford step 2500 current loss 0.397768, current_train_items 80032.
I0302 18:59:40.628981 22699590365312 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0302 18:59:40.629089 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 18:59:40.645976 22699590365312 run.py:483] Algo bellman_ford step 2501 current loss 0.459686, current_train_items 80064.
I0302 18:59:40.670094 22699590365312 run.py:483] Algo bellman_ford step 2502 current loss 0.698877, current_train_items 80096.
I0302 18:59:40.699779 22699590365312 run.py:483] Algo bellman_ford step 2503 current loss 0.695566, current_train_items 80128.
I0302 18:59:40.735006 22699590365312 run.py:483] Algo bellman_ford step 2504 current loss 0.811806, current_train_items 80160.
I0302 18:59:40.755250 22699590365312 run.py:483] Algo bellman_ford step 2505 current loss 0.296914, current_train_items 80192.
I0302 18:59:40.771222 22699590365312 run.py:483] Algo bellman_ford step 2506 current loss 0.560835, current_train_items 80224.
I0302 18:59:40.795010 22699590365312 run.py:483] Algo bellman_ford step 2507 current loss 0.745376, current_train_items 80256.
I0302 18:59:40.824472 22699590365312 run.py:483] Algo bellman_ford step 2508 current loss 0.759119, current_train_items 80288.
I0302 18:59:40.859169 22699590365312 run.py:483] Algo bellman_ford step 2509 current loss 0.989925, current_train_items 80320.
I0302 18:59:40.878597 22699590365312 run.py:483] Algo bellman_ford step 2510 current loss 0.317852, current_train_items 80352.
I0302 18:59:40.895035 22699590365312 run.py:483] Algo bellman_ford step 2511 current loss 0.619201, current_train_items 80384.
I0302 18:59:40.917957 22699590365312 run.py:483] Algo bellman_ford step 2512 current loss 0.703947, current_train_items 80416.
I0302 18:59:40.947196 22699590365312 run.py:483] Algo bellman_ford step 2513 current loss 0.674410, current_train_items 80448.
I0302 18:59:40.981339 22699590365312 run.py:483] Algo bellman_ford step 2514 current loss 0.864713, current_train_items 80480.
I0302 18:59:41.000748 22699590365312 run.py:483] Algo bellman_ford step 2515 current loss 0.294143, current_train_items 80512.
I0302 18:59:41.017017 22699590365312 run.py:483] Algo bellman_ford step 2516 current loss 0.491196, current_train_items 80544.
I0302 18:59:41.040460 22699590365312 run.py:483] Algo bellman_ford step 2517 current loss 0.806676, current_train_items 80576.
I0302 18:59:41.070034 22699590365312 run.py:483] Algo bellman_ford step 2518 current loss 0.734177, current_train_items 80608.
I0302 18:59:41.104271 22699590365312 run.py:483] Algo bellman_ford step 2519 current loss 0.878996, current_train_items 80640.
I0302 18:59:41.123753 22699590365312 run.py:483] Algo bellman_ford step 2520 current loss 0.334304, current_train_items 80672.
I0302 18:59:41.139960 22699590365312 run.py:483] Algo bellman_ford step 2521 current loss 0.501171, current_train_items 80704.
I0302 18:59:41.161680 22699590365312 run.py:483] Algo bellman_ford step 2522 current loss 0.569018, current_train_items 80736.
I0302 18:59:41.191294 22699590365312 run.py:483] Algo bellman_ford step 2523 current loss 0.696220, current_train_items 80768.
I0302 18:59:41.224117 22699590365312 run.py:483] Algo bellman_ford step 2524 current loss 0.864212, current_train_items 80800.
I0302 18:59:41.244075 22699590365312 run.py:483] Algo bellman_ford step 2525 current loss 0.335817, current_train_items 80832.
I0302 18:59:41.260232 22699590365312 run.py:483] Algo bellman_ford step 2526 current loss 0.472936, current_train_items 80864.
I0302 18:59:41.283259 22699590365312 run.py:483] Algo bellman_ford step 2527 current loss 0.674885, current_train_items 80896.
I0302 18:59:41.312539 22699590365312 run.py:483] Algo bellman_ford step 2528 current loss 0.778390, current_train_items 80928.
I0302 18:59:41.346951 22699590365312 run.py:483] Algo bellman_ford step 2529 current loss 0.895589, current_train_items 80960.
I0302 18:59:41.366371 22699590365312 run.py:483] Algo bellman_ford step 2530 current loss 0.332884, current_train_items 80992.
I0302 18:59:41.382095 22699590365312 run.py:483] Algo bellman_ford step 2531 current loss 0.489999, current_train_items 81024.
I0302 18:59:41.405743 22699590365312 run.py:483] Algo bellman_ford step 2532 current loss 0.708044, current_train_items 81056.
I0302 18:59:41.435612 22699590365312 run.py:483] Algo bellman_ford step 2533 current loss 0.694683, current_train_items 81088.
I0302 18:59:41.468645 22699590365312 run.py:483] Algo bellman_ford step 2534 current loss 0.794298, current_train_items 81120.
I0302 18:59:41.488375 22699590365312 run.py:483] Algo bellman_ford step 2535 current loss 0.321803, current_train_items 81152.
I0302 18:59:41.504242 22699590365312 run.py:483] Algo bellman_ford step 2536 current loss 0.449358, current_train_items 81184.
W0302 18:59:41.520472 22699590365312 samplers.py:155] Increasing hint lengh from 9 to 10
I0302 18:59:48.101591 22699590365312 run.py:483] Algo bellman_ford step 2537 current loss 0.798196, current_train_items 81216.
I0302 18:59:48.133949 22699590365312 run.py:483] Algo bellman_ford step 2538 current loss 0.814152, current_train_items 81248.
I0302 18:59:48.169039 22699590365312 run.py:483] Algo bellman_ford step 2539 current loss 0.951537, current_train_items 81280.
I0302 18:59:48.189251 22699590365312 run.py:483] Algo bellman_ford step 2540 current loss 0.298733, current_train_items 81312.
I0302 18:59:48.205336 22699590365312 run.py:483] Algo bellman_ford step 2541 current loss 0.405875, current_train_items 81344.
I0302 18:59:48.229239 22699590365312 run.py:483] Algo bellman_ford step 2542 current loss 0.676912, current_train_items 81376.
I0302 18:59:48.258582 22699590365312 run.py:483] Algo bellman_ford step 2543 current loss 0.692575, current_train_items 81408.
I0302 18:59:48.292518 22699590365312 run.py:483] Algo bellman_ford step 2544 current loss 0.841859, current_train_items 81440.
I0302 18:59:48.312625 22699590365312 run.py:483] Algo bellman_ford step 2545 current loss 0.322003, current_train_items 81472.
I0302 18:59:48.328775 22699590365312 run.py:483] Algo bellman_ford step 2546 current loss 0.507783, current_train_items 81504.
I0302 18:59:48.352650 22699590365312 run.py:483] Algo bellman_ford step 2547 current loss 0.639470, current_train_items 81536.
I0302 18:59:48.384393 22699590365312 run.py:483] Algo bellman_ford step 2548 current loss 0.766976, current_train_items 81568.
I0302 18:59:48.416979 22699590365312 run.py:483] Algo bellman_ford step 2549 current loss 0.905512, current_train_items 81600.
I0302 18:59:48.436918 22699590365312 run.py:483] Algo bellman_ford step 2550 current loss 0.271215, current_train_items 81632.
I0302 18:59:48.446538 22699590365312 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0302 18:59:48.446643 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 18:59:48.463686 22699590365312 run.py:483] Algo bellman_ford step 2551 current loss 0.529250, current_train_items 81664.
I0302 18:59:48.487867 22699590365312 run.py:483] Algo bellman_ford step 2552 current loss 0.754731, current_train_items 81696.
I0302 18:59:48.519783 22699590365312 run.py:483] Algo bellman_ford step 2553 current loss 0.811262, current_train_items 81728.
I0302 18:59:48.553910 22699590365312 run.py:483] Algo bellman_ford step 2554 current loss 0.822035, current_train_items 81760.
I0302 18:59:48.574290 22699590365312 run.py:483] Algo bellman_ford step 2555 current loss 0.313363, current_train_items 81792.
I0302 18:59:48.590650 22699590365312 run.py:483] Algo bellman_ford step 2556 current loss 0.560093, current_train_items 81824.
I0302 18:59:48.614332 22699590365312 run.py:483] Algo bellman_ford step 2557 current loss 0.601929, current_train_items 81856.
I0302 18:59:48.645541 22699590365312 run.py:483] Algo bellman_ford step 2558 current loss 0.734558, current_train_items 81888.
I0302 18:59:48.680651 22699590365312 run.py:483] Algo bellman_ford step 2559 current loss 0.943909, current_train_items 81920.
I0302 18:59:48.700618 22699590365312 run.py:483] Algo bellman_ford step 2560 current loss 0.390501, current_train_items 81952.
I0302 18:59:48.717216 22699590365312 run.py:483] Algo bellman_ford step 2561 current loss 0.478574, current_train_items 81984.
I0302 18:59:48.740225 22699590365312 run.py:483] Algo bellman_ford step 2562 current loss 0.607233, current_train_items 82016.
I0302 18:59:48.771932 22699590365312 run.py:483] Algo bellman_ford step 2563 current loss 0.800620, current_train_items 82048.
I0302 18:59:48.805649 22699590365312 run.py:483] Algo bellman_ford step 2564 current loss 0.868725, current_train_items 82080.
I0302 18:59:48.825336 22699590365312 run.py:483] Algo bellman_ford step 2565 current loss 0.318048, current_train_items 82112.
I0302 18:59:48.841660 22699590365312 run.py:483] Algo bellman_ford step 2566 current loss 0.476116, current_train_items 82144.
I0302 18:59:48.864939 22699590365312 run.py:483] Algo bellman_ford step 2567 current loss 0.622324, current_train_items 82176.
I0302 18:59:48.896412 22699590365312 run.py:483] Algo bellman_ford step 2568 current loss 0.753480, current_train_items 82208.
I0302 18:59:48.929509 22699590365312 run.py:483] Algo bellman_ford step 2569 current loss 0.765458, current_train_items 82240.
I0302 18:59:48.949295 22699590365312 run.py:483] Algo bellman_ford step 2570 current loss 0.326361, current_train_items 82272.
I0302 18:59:48.965481 22699590365312 run.py:483] Algo bellman_ford step 2571 current loss 0.574871, current_train_items 82304.
I0302 18:59:48.987529 22699590365312 run.py:483] Algo bellman_ford step 2572 current loss 0.574968, current_train_items 82336.
I0302 18:59:49.017982 22699590365312 run.py:483] Algo bellman_ford step 2573 current loss 0.596754, current_train_items 82368.
I0302 18:59:49.052556 22699590365312 run.py:483] Algo bellman_ford step 2574 current loss 0.844605, current_train_items 82400.
I0302 18:59:49.072504 22699590365312 run.py:483] Algo bellman_ford step 2575 current loss 0.338271, current_train_items 82432.
I0302 18:59:49.088505 22699590365312 run.py:483] Algo bellman_ford step 2576 current loss 0.489800, current_train_items 82464.
I0302 18:59:49.111316 22699590365312 run.py:483] Algo bellman_ford step 2577 current loss 0.646402, current_train_items 82496.
I0302 18:59:49.142920 22699590365312 run.py:483] Algo bellman_ford step 2578 current loss 0.760284, current_train_items 82528.
I0302 18:59:49.175361 22699590365312 run.py:483] Algo bellman_ford step 2579 current loss 1.169182, current_train_items 82560.
I0302 18:59:49.194988 22699590365312 run.py:483] Algo bellman_ford step 2580 current loss 0.395654, current_train_items 82592.
I0302 18:59:49.211336 22699590365312 run.py:483] Algo bellman_ford step 2581 current loss 0.456430, current_train_items 82624.
I0302 18:59:49.234347 22699590365312 run.py:483] Algo bellman_ford step 2582 current loss 0.677190, current_train_items 82656.
I0302 18:59:49.264607 22699590365312 run.py:483] Algo bellman_ford step 2583 current loss 0.626720, current_train_items 82688.
I0302 18:59:49.299566 22699590365312 run.py:483] Algo bellman_ford step 2584 current loss 1.016945, current_train_items 82720.
I0302 18:59:49.319284 22699590365312 run.py:483] Algo bellman_ford step 2585 current loss 0.330772, current_train_items 82752.
I0302 18:59:49.335623 22699590365312 run.py:483] Algo bellman_ford step 2586 current loss 0.546718, current_train_items 82784.
I0302 18:59:49.358578 22699590365312 run.py:483] Algo bellman_ford step 2587 current loss 0.549054, current_train_items 82816.
I0302 18:59:49.389105 22699590365312 run.py:483] Algo bellman_ford step 2588 current loss 0.657726, current_train_items 82848.
I0302 18:59:49.423067 22699590365312 run.py:483] Algo bellman_ford step 2589 current loss 0.887107, current_train_items 82880.
I0302 18:59:49.442968 22699590365312 run.py:483] Algo bellman_ford step 2590 current loss 0.286471, current_train_items 82912.
I0302 18:59:49.459469 22699590365312 run.py:483] Algo bellman_ford step 2591 current loss 0.465542, current_train_items 82944.
I0302 18:59:49.483260 22699590365312 run.py:483] Algo bellman_ford step 2592 current loss 0.712916, current_train_items 82976.
I0302 18:59:49.514643 22699590365312 run.py:483] Algo bellman_ford step 2593 current loss 0.773397, current_train_items 83008.
I0302 18:59:49.546252 22699590365312 run.py:483] Algo bellman_ford step 2594 current loss 0.950982, current_train_items 83040.
I0302 18:59:49.565738 22699590365312 run.py:483] Algo bellman_ford step 2595 current loss 0.381547, current_train_items 83072.
I0302 18:59:49.582242 22699590365312 run.py:483] Algo bellman_ford step 2596 current loss 0.501603, current_train_items 83104.
I0302 18:59:49.606692 22699590365312 run.py:483] Algo bellman_ford step 2597 current loss 0.745426, current_train_items 83136.
I0302 18:59:49.637298 22699590365312 run.py:483] Algo bellman_ford step 2598 current loss 0.702644, current_train_items 83168.
I0302 18:59:49.670706 22699590365312 run.py:483] Algo bellman_ford step 2599 current loss 0.835450, current_train_items 83200.
I0302 18:59:49.690771 22699590365312 run.py:483] Algo bellman_ford step 2600 current loss 0.345484, current_train_items 83232.
I0302 18:59:49.698788 22699590365312 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0302 18:59:49.698895 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 18:59:49.715696 22699590365312 run.py:483] Algo bellman_ford step 2601 current loss 0.504582, current_train_items 83264.
I0302 18:59:49.739337 22699590365312 run.py:483] Algo bellman_ford step 2602 current loss 0.682896, current_train_items 83296.
I0302 18:59:49.768707 22699590365312 run.py:483] Algo bellman_ford step 2603 current loss 0.734791, current_train_items 83328.
I0302 18:59:49.802715 22699590365312 run.py:483] Algo bellman_ford step 2604 current loss 0.953112, current_train_items 83360.
I0302 18:59:49.822543 22699590365312 run.py:483] Algo bellman_ford step 2605 current loss 0.321151, current_train_items 83392.
I0302 18:59:49.838382 22699590365312 run.py:483] Algo bellman_ford step 2606 current loss 0.472456, current_train_items 83424.
I0302 18:59:49.862245 22699590365312 run.py:483] Algo bellman_ford step 2607 current loss 0.622069, current_train_items 83456.
I0302 18:59:49.893442 22699590365312 run.py:483] Algo bellman_ford step 2608 current loss 0.731222, current_train_items 83488.
I0302 18:59:49.927189 22699590365312 run.py:483] Algo bellman_ford step 2609 current loss 1.062434, current_train_items 83520.
I0302 18:59:49.946732 22699590365312 run.py:483] Algo bellman_ford step 2610 current loss 0.307235, current_train_items 83552.
I0302 18:59:49.962763 22699590365312 run.py:483] Algo bellman_ford step 2611 current loss 0.477978, current_train_items 83584.
I0302 18:59:49.985848 22699590365312 run.py:483] Algo bellman_ford step 2612 current loss 0.563115, current_train_items 83616.
I0302 18:59:50.016369 22699590365312 run.py:483] Algo bellman_ford step 2613 current loss 0.687124, current_train_items 83648.
I0302 18:59:50.048647 22699590365312 run.py:483] Algo bellman_ford step 2614 current loss 0.798422, current_train_items 83680.
I0302 18:59:50.068296 22699590365312 run.py:483] Algo bellman_ford step 2615 current loss 0.354205, current_train_items 83712.
I0302 18:59:50.084144 22699590365312 run.py:483] Algo bellman_ford step 2616 current loss 0.395484, current_train_items 83744.
I0302 18:59:50.108606 22699590365312 run.py:483] Algo bellman_ford step 2617 current loss 0.645211, current_train_items 83776.
I0302 18:59:50.139976 22699590365312 run.py:483] Algo bellman_ford step 2618 current loss 0.733008, current_train_items 83808.
I0302 18:59:50.172529 22699590365312 run.py:483] Algo bellman_ford step 2619 current loss 0.777871, current_train_items 83840.
I0302 18:59:50.192208 22699590365312 run.py:483] Algo bellman_ford step 2620 current loss 0.323025, current_train_items 83872.
I0302 18:59:50.208446 22699590365312 run.py:483] Algo bellman_ford step 2621 current loss 0.460855, current_train_items 83904.
I0302 18:59:50.232107 22699590365312 run.py:483] Algo bellman_ford step 2622 current loss 0.627612, current_train_items 83936.
I0302 18:59:50.262765 22699590365312 run.py:483] Algo bellman_ford step 2623 current loss 0.664606, current_train_items 83968.
I0302 18:59:50.295135 22699590365312 run.py:483] Algo bellman_ford step 2624 current loss 0.811306, current_train_items 84000.
I0302 18:59:50.314553 22699590365312 run.py:483] Algo bellman_ford step 2625 current loss 0.291290, current_train_items 84032.
I0302 18:59:50.330638 22699590365312 run.py:483] Algo bellman_ford step 2626 current loss 0.430196, current_train_items 84064.
I0302 18:59:50.354196 22699590365312 run.py:483] Algo bellman_ford step 2627 current loss 0.576743, current_train_items 84096.
I0302 18:59:50.384994 22699590365312 run.py:483] Algo bellman_ford step 2628 current loss 0.729752, current_train_items 84128.
I0302 18:59:50.418300 22699590365312 run.py:483] Algo bellman_ford step 2629 current loss 0.738996, current_train_items 84160.
I0302 18:59:50.437807 22699590365312 run.py:483] Algo bellman_ford step 2630 current loss 0.266362, current_train_items 84192.
I0302 18:59:50.453938 22699590365312 run.py:483] Algo bellman_ford step 2631 current loss 0.532595, current_train_items 84224.
I0302 18:59:50.476814 22699590365312 run.py:483] Algo bellman_ford step 2632 current loss 0.625767, current_train_items 84256.
I0302 18:59:50.506831 22699590365312 run.py:483] Algo bellman_ford step 2633 current loss 0.698363, current_train_items 84288.
I0302 18:59:50.542405 22699590365312 run.py:483] Algo bellman_ford step 2634 current loss 0.926217, current_train_items 84320.
I0302 18:59:50.562090 22699590365312 run.py:483] Algo bellman_ford step 2635 current loss 0.214756, current_train_items 84352.
I0302 18:59:50.578104 22699590365312 run.py:483] Algo bellman_ford step 2636 current loss 0.569535, current_train_items 84384.
I0302 18:59:50.602119 22699590365312 run.py:483] Algo bellman_ford step 2637 current loss 0.773950, current_train_items 84416.
I0302 18:59:50.633535 22699590365312 run.py:483] Algo bellman_ford step 2638 current loss 0.772913, current_train_items 84448.
I0302 18:59:50.667575 22699590365312 run.py:483] Algo bellman_ford step 2639 current loss 0.835863, current_train_items 84480.
I0302 18:59:50.687089 22699590365312 run.py:483] Algo bellman_ford step 2640 current loss 0.364380, current_train_items 84512.
I0302 18:59:50.703184 22699590365312 run.py:483] Algo bellman_ford step 2641 current loss 0.503972, current_train_items 84544.
I0302 18:59:50.725816 22699590365312 run.py:483] Algo bellman_ford step 2642 current loss 0.680454, current_train_items 84576.
I0302 18:59:50.756101 22699590365312 run.py:483] Algo bellman_ford step 2643 current loss 0.868398, current_train_items 84608.
I0302 18:59:50.790369 22699590365312 run.py:483] Algo bellman_ford step 2644 current loss 1.002279, current_train_items 84640.
I0302 18:59:50.810084 22699590365312 run.py:483] Algo bellman_ford step 2645 current loss 0.336500, current_train_items 84672.
I0302 18:59:50.826144 22699590365312 run.py:483] Algo bellman_ford step 2646 current loss 0.536859, current_train_items 84704.
I0302 18:59:50.849730 22699590365312 run.py:483] Algo bellman_ford step 2647 current loss 0.708119, current_train_items 84736.
I0302 18:59:50.880967 22699590365312 run.py:483] Algo bellman_ford step 2648 current loss 0.671981, current_train_items 84768.
I0302 18:59:50.916498 22699590365312 run.py:483] Algo bellman_ford step 2649 current loss 0.826711, current_train_items 84800.
I0302 18:59:50.936202 22699590365312 run.py:483] Algo bellman_ford step 2650 current loss 0.339099, current_train_items 84832.
I0302 18:59:50.944541 22699590365312 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0302 18:59:50.944648 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 18:59:50.961260 22699590365312 run.py:483] Algo bellman_ford step 2651 current loss 0.432461, current_train_items 84864.
I0302 18:59:50.984683 22699590365312 run.py:483] Algo bellman_ford step 2652 current loss 0.601691, current_train_items 84896.
I0302 18:59:51.016552 22699590365312 run.py:483] Algo bellman_ford step 2653 current loss 0.707228, current_train_items 84928.
I0302 18:59:51.050070 22699590365312 run.py:483] Algo bellman_ford step 2654 current loss 0.789296, current_train_items 84960.
I0302 18:59:51.069949 22699590365312 run.py:483] Algo bellman_ford step 2655 current loss 0.266087, current_train_items 84992.
I0302 18:59:51.086036 22699590365312 run.py:483] Algo bellman_ford step 2656 current loss 0.486478, current_train_items 85024.
I0302 18:59:51.109297 22699590365312 run.py:483] Algo bellman_ford step 2657 current loss 0.609103, current_train_items 85056.
I0302 18:59:51.138947 22699590365312 run.py:483] Algo bellman_ford step 2658 current loss 0.755059, current_train_items 85088.
I0302 18:59:51.174460 22699590365312 run.py:483] Algo bellman_ford step 2659 current loss 0.971464, current_train_items 85120.
I0302 18:59:51.194236 22699590365312 run.py:483] Algo bellman_ford step 2660 current loss 0.359814, current_train_items 85152.
I0302 18:59:51.211040 22699590365312 run.py:483] Algo bellman_ford step 2661 current loss 0.491660, current_train_items 85184.
I0302 18:59:51.233847 22699590365312 run.py:483] Algo bellman_ford step 2662 current loss 0.656647, current_train_items 85216.
I0302 18:59:51.264491 22699590365312 run.py:483] Algo bellman_ford step 2663 current loss 0.741990, current_train_items 85248.
I0302 18:59:51.300374 22699590365312 run.py:483] Algo bellman_ford step 2664 current loss 0.887890, current_train_items 85280.
I0302 18:59:51.319919 22699590365312 run.py:483] Algo bellman_ford step 2665 current loss 0.299947, current_train_items 85312.
I0302 18:59:51.336167 22699590365312 run.py:483] Algo bellman_ford step 2666 current loss 0.514208, current_train_items 85344.
I0302 18:59:51.360234 22699590365312 run.py:483] Algo bellman_ford step 2667 current loss 0.586233, current_train_items 85376.
I0302 18:59:51.391694 22699590365312 run.py:483] Algo bellman_ford step 2668 current loss 0.712623, current_train_items 85408.
I0302 18:59:51.424412 22699590365312 run.py:483] Algo bellman_ford step 2669 current loss 0.940847, current_train_items 85440.
I0302 18:59:51.444054 22699590365312 run.py:483] Algo bellman_ford step 2670 current loss 0.349050, current_train_items 85472.
I0302 18:59:51.460512 22699590365312 run.py:483] Algo bellman_ford step 2671 current loss 0.463234, current_train_items 85504.
I0302 18:59:51.483656 22699590365312 run.py:483] Algo bellman_ford step 2672 current loss 0.615966, current_train_items 85536.
I0302 18:59:51.515233 22699590365312 run.py:483] Algo bellman_ford step 2673 current loss 0.699914, current_train_items 85568.
I0302 18:59:51.550191 22699590365312 run.py:483] Algo bellman_ford step 2674 current loss 0.816275, current_train_items 85600.
I0302 18:59:51.569880 22699590365312 run.py:483] Algo bellman_ford step 2675 current loss 0.338559, current_train_items 85632.
I0302 18:59:51.586066 22699590365312 run.py:483] Algo bellman_ford step 2676 current loss 0.475598, current_train_items 85664.
I0302 18:59:51.610059 22699590365312 run.py:483] Algo bellman_ford step 2677 current loss 0.665458, current_train_items 85696.
I0302 18:59:51.639521 22699590365312 run.py:483] Algo bellman_ford step 2678 current loss 0.639452, current_train_items 85728.
I0302 18:59:51.671590 22699590365312 run.py:483] Algo bellman_ford step 2679 current loss 0.712964, current_train_items 85760.
I0302 18:59:51.690862 22699590365312 run.py:483] Algo bellman_ford step 2680 current loss 0.306486, current_train_items 85792.
I0302 18:59:51.707068 22699590365312 run.py:483] Algo bellman_ford step 2681 current loss 0.522079, current_train_items 85824.
I0302 18:59:51.730512 22699590365312 run.py:483] Algo bellman_ford step 2682 current loss 0.771346, current_train_items 85856.
I0302 18:59:51.762472 22699590365312 run.py:483] Algo bellman_ford step 2683 current loss 0.800328, current_train_items 85888.
I0302 18:59:51.796305 22699590365312 run.py:483] Algo bellman_ford step 2684 current loss 0.933745, current_train_items 85920.
I0302 18:59:51.816054 22699590365312 run.py:483] Algo bellman_ford step 2685 current loss 0.272352, current_train_items 85952.
I0302 18:59:51.832214 22699590365312 run.py:483] Algo bellman_ford step 2686 current loss 0.537132, current_train_items 85984.
I0302 18:59:51.855623 22699590365312 run.py:483] Algo bellman_ford step 2687 current loss 0.674845, current_train_items 86016.
I0302 18:59:51.886146 22699590365312 run.py:483] Algo bellman_ford step 2688 current loss 0.754270, current_train_items 86048.
I0302 18:59:51.918352 22699590365312 run.py:483] Algo bellman_ford step 2689 current loss 0.717106, current_train_items 86080.
I0302 18:59:51.938267 22699590365312 run.py:483] Algo bellman_ford step 2690 current loss 0.280499, current_train_items 86112.
I0302 18:59:51.954560 22699590365312 run.py:483] Algo bellman_ford step 2691 current loss 0.504524, current_train_items 86144.
I0302 18:59:51.978940 22699590365312 run.py:483] Algo bellman_ford step 2692 current loss 0.719523, current_train_items 86176.
I0302 18:59:52.010807 22699590365312 run.py:483] Algo bellman_ford step 2693 current loss 0.706255, current_train_items 86208.
I0302 18:59:52.044321 22699590365312 run.py:483] Algo bellman_ford step 2694 current loss 0.773162, current_train_items 86240.
I0302 18:59:52.063846 22699590365312 run.py:483] Algo bellman_ford step 2695 current loss 0.308034, current_train_items 86272.
I0302 18:59:52.079808 22699590365312 run.py:483] Algo bellman_ford step 2696 current loss 0.419386, current_train_items 86304.
I0302 18:59:52.103464 22699590365312 run.py:483] Algo bellman_ford step 2697 current loss 0.691403, current_train_items 86336.
I0302 18:59:52.133804 22699590365312 run.py:483] Algo bellman_ford step 2698 current loss 0.716502, current_train_items 86368.
I0302 18:59:52.165483 22699590365312 run.py:483] Algo bellman_ford step 2699 current loss 1.040466, current_train_items 86400.
I0302 18:59:52.185088 22699590365312 run.py:483] Algo bellman_ford step 2700 current loss 0.342468, current_train_items 86432.
I0302 18:59:52.192998 22699590365312 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0302 18:59:52.193103 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 18:59:52.209956 22699590365312 run.py:483] Algo bellman_ford step 2701 current loss 0.520947, current_train_items 86464.
I0302 18:59:52.233626 22699590365312 run.py:483] Algo bellman_ford step 2702 current loss 0.568487, current_train_items 86496.
I0302 18:59:52.266839 22699590365312 run.py:483] Algo bellman_ford step 2703 current loss 0.777203, current_train_items 86528.
I0302 18:59:52.302934 22699590365312 run.py:483] Algo bellman_ford step 2704 current loss 0.889891, current_train_items 86560.
I0302 18:59:52.322751 22699590365312 run.py:483] Algo bellman_ford step 2705 current loss 0.332688, current_train_items 86592.
I0302 18:59:52.338488 22699590365312 run.py:483] Algo bellman_ford step 2706 current loss 0.537597, current_train_items 86624.
I0302 18:59:52.362211 22699590365312 run.py:483] Algo bellman_ford step 2707 current loss 0.732086, current_train_items 86656.
I0302 18:59:52.391666 22699590365312 run.py:483] Algo bellman_ford step 2708 current loss 0.674517, current_train_items 86688.
I0302 18:59:52.423623 22699590365312 run.py:483] Algo bellman_ford step 2709 current loss 0.702673, current_train_items 86720.
I0302 18:59:52.443463 22699590365312 run.py:483] Algo bellman_ford step 2710 current loss 0.319657, current_train_items 86752.
I0302 18:59:52.459852 22699590365312 run.py:483] Algo bellman_ford step 2711 current loss 0.449301, current_train_items 86784.
I0302 18:59:52.483799 22699590365312 run.py:483] Algo bellman_ford step 2712 current loss 0.665788, current_train_items 86816.
I0302 18:59:52.515305 22699590365312 run.py:483] Algo bellman_ford step 2713 current loss 0.708546, current_train_items 86848.
I0302 18:59:52.550886 22699590365312 run.py:483] Algo bellman_ford step 2714 current loss 0.940419, current_train_items 86880.
I0302 18:59:52.570615 22699590365312 run.py:483] Algo bellman_ford step 2715 current loss 0.344279, current_train_items 86912.
I0302 18:59:52.586376 22699590365312 run.py:483] Algo bellman_ford step 2716 current loss 0.377372, current_train_items 86944.
I0302 18:59:52.610750 22699590365312 run.py:483] Algo bellman_ford step 2717 current loss 0.707371, current_train_items 86976.
I0302 18:59:52.640259 22699590365312 run.py:483] Algo bellman_ford step 2718 current loss 0.728776, current_train_items 87008.
I0302 18:59:52.673755 22699590365312 run.py:483] Algo bellman_ford step 2719 current loss 0.794852, current_train_items 87040.
I0302 18:59:52.693173 22699590365312 run.py:483] Algo bellman_ford step 2720 current loss 0.344210, current_train_items 87072.
I0302 18:59:52.709329 22699590365312 run.py:483] Algo bellman_ford step 2721 current loss 0.669238, current_train_items 87104.
I0302 18:59:52.732603 22699590365312 run.py:483] Algo bellman_ford step 2722 current loss 0.690763, current_train_items 87136.
I0302 18:59:52.763904 22699590365312 run.py:483] Algo bellman_ford step 2723 current loss 0.788402, current_train_items 87168.
I0302 18:59:52.797025 22699590365312 run.py:483] Algo bellman_ford step 2724 current loss 0.716076, current_train_items 87200.
I0302 18:59:52.816383 22699590365312 run.py:483] Algo bellman_ford step 2725 current loss 0.312299, current_train_items 87232.
I0302 18:59:52.832403 22699590365312 run.py:483] Algo bellman_ford step 2726 current loss 0.604978, current_train_items 87264.
I0302 18:59:52.856285 22699590365312 run.py:483] Algo bellman_ford step 2727 current loss 0.700563, current_train_items 87296.
I0302 18:59:52.887046 22699590365312 run.py:483] Algo bellman_ford step 2728 current loss 0.711834, current_train_items 87328.
I0302 18:59:52.919882 22699590365312 run.py:483] Algo bellman_ford step 2729 current loss 0.835286, current_train_items 87360.
I0302 18:59:52.939587 22699590365312 run.py:483] Algo bellman_ford step 2730 current loss 0.325148, current_train_items 87392.
I0302 18:59:52.955622 22699590365312 run.py:483] Algo bellman_ford step 2731 current loss 0.424496, current_train_items 87424.
I0302 18:59:52.977785 22699590365312 run.py:483] Algo bellman_ford step 2732 current loss 0.684901, current_train_items 87456.
I0302 18:59:53.007176 22699590365312 run.py:483] Algo bellman_ford step 2733 current loss 0.666848, current_train_items 87488.
I0302 18:59:53.040641 22699590365312 run.py:483] Algo bellman_ford step 2734 current loss 0.795921, current_train_items 87520.
I0302 18:59:53.060262 22699590365312 run.py:483] Algo bellman_ford step 2735 current loss 0.322812, current_train_items 87552.
I0302 18:59:53.075922 22699590365312 run.py:483] Algo bellman_ford step 2736 current loss 0.485787, current_train_items 87584.
I0302 18:59:53.098828 22699590365312 run.py:483] Algo bellman_ford step 2737 current loss 0.631660, current_train_items 87616.
I0302 18:59:53.128534 22699590365312 run.py:483] Algo bellman_ford step 2738 current loss 0.645255, current_train_items 87648.
I0302 18:59:53.160670 22699590365312 run.py:483] Algo bellman_ford step 2739 current loss 0.810123, current_train_items 87680.
I0302 18:59:53.180249 22699590365312 run.py:483] Algo bellman_ford step 2740 current loss 0.409811, current_train_items 87712.
I0302 18:59:53.196179 22699590365312 run.py:483] Algo bellman_ford step 2741 current loss 0.523906, current_train_items 87744.
I0302 18:59:53.219515 22699590365312 run.py:483] Algo bellman_ford step 2742 current loss 0.653422, current_train_items 87776.
I0302 18:59:53.251724 22699590365312 run.py:483] Algo bellman_ford step 2743 current loss 0.740765, current_train_items 87808.
I0302 18:59:53.284465 22699590365312 run.py:483] Algo bellman_ford step 2744 current loss 0.808381, current_train_items 87840.
I0302 18:59:53.303668 22699590365312 run.py:483] Algo bellman_ford step 2745 current loss 0.295144, current_train_items 87872.
I0302 18:59:53.319558 22699590365312 run.py:483] Algo bellman_ford step 2746 current loss 0.514005, current_train_items 87904.
I0302 18:59:53.342781 22699590365312 run.py:483] Algo bellman_ford step 2747 current loss 0.657104, current_train_items 87936.
I0302 18:59:53.374195 22699590365312 run.py:483] Algo bellman_ford step 2748 current loss 0.750482, current_train_items 87968.
I0302 18:59:53.406846 22699590365312 run.py:483] Algo bellman_ford step 2749 current loss 0.840495, current_train_items 88000.
I0302 18:59:53.426166 22699590365312 run.py:483] Algo bellman_ford step 2750 current loss 0.340107, current_train_items 88032.
I0302 18:59:53.434353 22699590365312 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0302 18:59:53.434458 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 18:59:53.451495 22699590365312 run.py:483] Algo bellman_ford step 2751 current loss 0.545247, current_train_items 88064.
I0302 18:59:53.475540 22699590365312 run.py:483] Algo bellman_ford step 2752 current loss 0.673804, current_train_items 88096.
I0302 18:59:53.507258 22699590365312 run.py:483] Algo bellman_ford step 2753 current loss 0.771806, current_train_items 88128.
I0302 18:59:53.540220 22699590365312 run.py:483] Algo bellman_ford step 2754 current loss 0.827358, current_train_items 88160.
I0302 18:59:53.560371 22699590365312 run.py:483] Algo bellman_ford step 2755 current loss 0.307934, current_train_items 88192.
I0302 18:59:53.576552 22699590365312 run.py:483] Algo bellman_ford step 2756 current loss 0.448414, current_train_items 88224.
I0302 18:59:53.599618 22699590365312 run.py:483] Algo bellman_ford step 2757 current loss 0.589638, current_train_items 88256.
I0302 18:59:53.631175 22699590365312 run.py:483] Algo bellman_ford step 2758 current loss 0.760163, current_train_items 88288.
I0302 18:59:53.666907 22699590365312 run.py:483] Algo bellman_ford step 2759 current loss 0.951599, current_train_items 88320.
I0302 18:59:53.686857 22699590365312 run.py:483] Algo bellman_ford step 2760 current loss 0.319064, current_train_items 88352.
I0302 18:59:53.703440 22699590365312 run.py:483] Algo bellman_ford step 2761 current loss 0.514051, current_train_items 88384.
I0302 18:59:53.726951 22699590365312 run.py:483] Algo bellman_ford step 2762 current loss 0.730245, current_train_items 88416.
I0302 18:59:53.758213 22699590365312 run.py:483] Algo bellman_ford step 2763 current loss 0.698321, current_train_items 88448.
I0302 18:59:53.791616 22699590365312 run.py:483] Algo bellman_ford step 2764 current loss 0.883762, current_train_items 88480.
I0302 18:59:53.810980 22699590365312 run.py:483] Algo bellman_ford step 2765 current loss 0.379984, current_train_items 88512.
I0302 18:59:53.827504 22699590365312 run.py:483] Algo bellman_ford step 2766 current loss 0.506775, current_train_items 88544.
I0302 18:59:53.851058 22699590365312 run.py:483] Algo bellman_ford step 2767 current loss 0.686680, current_train_items 88576.
I0302 18:59:53.883275 22699590365312 run.py:483] Algo bellman_ford step 2768 current loss 0.707923, current_train_items 88608.
I0302 18:59:53.916834 22699590365312 run.py:483] Algo bellman_ford step 2769 current loss 0.866678, current_train_items 88640.
I0302 18:59:53.936451 22699590365312 run.py:483] Algo bellman_ford step 2770 current loss 0.303686, current_train_items 88672.
I0302 18:59:53.952796 22699590365312 run.py:483] Algo bellman_ford step 2771 current loss 0.510505, current_train_items 88704.
I0302 18:59:53.976552 22699590365312 run.py:483] Algo bellman_ford step 2772 current loss 0.680190, current_train_items 88736.
I0302 18:59:54.006884 22699590365312 run.py:483] Algo bellman_ford step 2773 current loss 0.632676, current_train_items 88768.
I0302 18:59:54.040259 22699590365312 run.py:483] Algo bellman_ford step 2774 current loss 0.806657, current_train_items 88800.
I0302 18:59:54.059864 22699590365312 run.py:483] Algo bellman_ford step 2775 current loss 0.269917, current_train_items 88832.
I0302 18:59:54.076599 22699590365312 run.py:483] Algo bellman_ford step 2776 current loss 0.626458, current_train_items 88864.
I0302 18:59:54.100062 22699590365312 run.py:483] Algo bellman_ford step 2777 current loss 0.724925, current_train_items 88896.
I0302 18:59:54.130523 22699590365312 run.py:483] Algo bellman_ford step 2778 current loss 0.716427, current_train_items 88928.
I0302 18:59:54.164638 22699590365312 run.py:483] Algo bellman_ford step 2779 current loss 1.011179, current_train_items 88960.
I0302 18:59:54.184605 22699590365312 run.py:483] Algo bellman_ford step 2780 current loss 0.336336, current_train_items 88992.
I0302 18:59:54.200729 22699590365312 run.py:483] Algo bellman_ford step 2781 current loss 0.591191, current_train_items 89024.
I0302 18:59:54.224790 22699590365312 run.py:483] Algo bellman_ford step 2782 current loss 0.676267, current_train_items 89056.
I0302 18:59:54.255473 22699590365312 run.py:483] Algo bellman_ford step 2783 current loss 0.700418, current_train_items 89088.
I0302 18:59:54.289617 22699590365312 run.py:483] Algo bellman_ford step 2784 current loss 0.833973, current_train_items 89120.
I0302 18:59:54.309454 22699590365312 run.py:483] Algo bellman_ford step 2785 current loss 0.386976, current_train_items 89152.
I0302 18:59:54.325588 22699590365312 run.py:483] Algo bellman_ford step 2786 current loss 0.490770, current_train_items 89184.
I0302 18:59:54.347819 22699590365312 run.py:483] Algo bellman_ford step 2787 current loss 0.532539, current_train_items 89216.
I0302 18:59:54.377877 22699590365312 run.py:483] Algo bellman_ford step 2788 current loss 0.714060, current_train_items 89248.
I0302 18:59:54.408417 22699590365312 run.py:483] Algo bellman_ford step 2789 current loss 0.746594, current_train_items 89280.
I0302 18:59:54.428214 22699590365312 run.py:483] Algo bellman_ford step 2790 current loss 0.386300, current_train_items 89312.
I0302 18:59:54.444308 22699590365312 run.py:483] Algo bellman_ford step 2791 current loss 0.489564, current_train_items 89344.
I0302 18:59:54.467802 22699590365312 run.py:483] Algo bellman_ford step 2792 current loss 0.556559, current_train_items 89376.
I0302 18:59:54.497838 22699590365312 run.py:483] Algo bellman_ford step 2793 current loss 0.629054, current_train_items 89408.
I0302 18:59:54.533364 22699590365312 run.py:483] Algo bellman_ford step 2794 current loss 0.852747, current_train_items 89440.
I0302 18:59:54.553235 22699590365312 run.py:483] Algo bellman_ford step 2795 current loss 0.308189, current_train_items 89472.
I0302 18:59:54.569895 22699590365312 run.py:483] Algo bellman_ford step 2796 current loss 0.549782, current_train_items 89504.
I0302 18:59:54.592936 22699590365312 run.py:483] Algo bellman_ford step 2797 current loss 0.720428, current_train_items 89536.
I0302 18:59:54.624179 22699590365312 run.py:483] Algo bellman_ford step 2798 current loss 0.743629, current_train_items 89568.
I0302 18:59:54.657492 22699590365312 run.py:483] Algo bellman_ford step 2799 current loss 0.773645, current_train_items 89600.
I0302 18:59:54.677677 22699590365312 run.py:483] Algo bellman_ford step 2800 current loss 0.313682, current_train_items 89632.
I0302 18:59:54.685584 22699590365312 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0302 18:59:54.685691 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 18:59:54.702500 22699590365312 run.py:483] Algo bellman_ford step 2801 current loss 0.456352, current_train_items 89664.
I0302 18:59:54.727211 22699590365312 run.py:483] Algo bellman_ford step 2802 current loss 0.713963, current_train_items 89696.
I0302 18:59:54.759270 22699590365312 run.py:483] Algo bellman_ford step 2803 current loss 0.760789, current_train_items 89728.
I0302 18:59:54.793094 22699590365312 run.py:483] Algo bellman_ford step 2804 current loss 0.774736, current_train_items 89760.
I0302 18:59:54.813240 22699590365312 run.py:483] Algo bellman_ford step 2805 current loss 0.344848, current_train_items 89792.
I0302 18:59:54.829535 22699590365312 run.py:483] Algo bellman_ford step 2806 current loss 0.569327, current_train_items 89824.
I0302 18:59:54.852265 22699590365312 run.py:483] Algo bellman_ford step 2807 current loss 0.710171, current_train_items 89856.
I0302 18:59:54.883842 22699590365312 run.py:483] Algo bellman_ford step 2808 current loss 0.818011, current_train_items 89888.
I0302 18:59:54.917993 22699590365312 run.py:483] Algo bellman_ford step 2809 current loss 0.838670, current_train_items 89920.
I0302 18:59:54.937585 22699590365312 run.py:483] Algo bellman_ford step 2810 current loss 0.266842, current_train_items 89952.
I0302 18:59:54.954147 22699590365312 run.py:483] Algo bellman_ford step 2811 current loss 0.587639, current_train_items 89984.
I0302 18:59:54.976831 22699590365312 run.py:483] Algo bellman_ford step 2812 current loss 0.602966, current_train_items 90016.
I0302 18:59:55.008441 22699590365312 run.py:483] Algo bellman_ford step 2813 current loss 0.899888, current_train_items 90048.
I0302 18:59:55.041620 22699590365312 run.py:483] Algo bellman_ford step 2814 current loss 0.818165, current_train_items 90080.
I0302 18:59:55.061062 22699590365312 run.py:483] Algo bellman_ford step 2815 current loss 0.329761, current_train_items 90112.
I0302 18:59:55.077649 22699590365312 run.py:483] Algo bellman_ford step 2816 current loss 0.476072, current_train_items 90144.
I0302 18:59:55.101257 22699590365312 run.py:483] Algo bellman_ford step 2817 current loss 0.835735, current_train_items 90176.
I0302 18:59:55.128691 22699590365312 run.py:483] Algo bellman_ford step 2818 current loss 0.628156, current_train_items 90208.
I0302 18:59:55.162170 22699590365312 run.py:483] Algo bellman_ford step 2819 current loss 0.833341, current_train_items 90240.
I0302 18:59:55.181655 22699590365312 run.py:483] Algo bellman_ford step 2820 current loss 0.356001, current_train_items 90272.
I0302 18:59:55.197771 22699590365312 run.py:483] Algo bellman_ford step 2821 current loss 0.416383, current_train_items 90304.
I0302 18:59:55.221080 22699590365312 run.py:483] Algo bellman_ford step 2822 current loss 0.692902, current_train_items 90336.
I0302 18:59:55.251711 22699590365312 run.py:483] Algo bellman_ford step 2823 current loss 0.966768, current_train_items 90368.
I0302 18:59:55.287760 22699590365312 run.py:483] Algo bellman_ford step 2824 current loss 0.927976, current_train_items 90400.
I0302 18:59:55.307639 22699590365312 run.py:483] Algo bellman_ford step 2825 current loss 0.322762, current_train_items 90432.
I0302 18:59:55.323509 22699590365312 run.py:483] Algo bellman_ford step 2826 current loss 0.586887, current_train_items 90464.
I0302 18:59:55.347012 22699590365312 run.py:483] Algo bellman_ford step 2827 current loss 0.760863, current_train_items 90496.
I0302 18:59:55.377964 22699590365312 run.py:483] Algo bellman_ford step 2828 current loss 0.863819, current_train_items 90528.
I0302 18:59:55.410190 22699590365312 run.py:483] Algo bellman_ford step 2829 current loss 0.943615, current_train_items 90560.
I0302 18:59:55.429787 22699590365312 run.py:483] Algo bellman_ford step 2830 current loss 0.262366, current_train_items 90592.
I0302 18:59:55.446366 22699590365312 run.py:483] Algo bellman_ford step 2831 current loss 0.593940, current_train_items 90624.
I0302 18:59:55.469588 22699590365312 run.py:483] Algo bellman_ford step 2832 current loss 0.831817, current_train_items 90656.
I0302 18:59:55.499452 22699590365312 run.py:483] Algo bellman_ford step 2833 current loss 0.869710, current_train_items 90688.
I0302 18:59:55.533001 22699590365312 run.py:483] Algo bellman_ford step 2834 current loss 0.829495, current_train_items 90720.
I0302 18:59:55.552449 22699590365312 run.py:483] Algo bellman_ford step 2835 current loss 0.307400, current_train_items 90752.
I0302 18:59:55.568175 22699590365312 run.py:483] Algo bellman_ford step 2836 current loss 0.480223, current_train_items 90784.
I0302 18:59:55.591252 22699590365312 run.py:483] Algo bellman_ford step 2837 current loss 0.559439, current_train_items 90816.
I0302 18:59:55.622132 22699590365312 run.py:483] Algo bellman_ford step 2838 current loss 0.766156, current_train_items 90848.
I0302 18:59:55.657891 22699590365312 run.py:483] Algo bellman_ford step 2839 current loss 0.917067, current_train_items 90880.
I0302 18:59:55.677614 22699590365312 run.py:483] Algo bellman_ford step 2840 current loss 0.296745, current_train_items 90912.
I0302 18:59:55.693516 22699590365312 run.py:483] Algo bellman_ford step 2841 current loss 0.468269, current_train_items 90944.
I0302 18:59:55.717117 22699590365312 run.py:483] Algo bellman_ford step 2842 current loss 0.656203, current_train_items 90976.
I0302 18:59:55.748664 22699590365312 run.py:483] Algo bellman_ford step 2843 current loss 0.737352, current_train_items 91008.
I0302 18:59:55.782876 22699590365312 run.py:483] Algo bellman_ford step 2844 current loss 0.869057, current_train_items 91040.
I0302 18:59:55.802722 22699590365312 run.py:483] Algo bellman_ford step 2845 current loss 0.308450, current_train_items 91072.
I0302 18:59:55.819334 22699590365312 run.py:483] Algo bellman_ford step 2846 current loss 0.541419, current_train_items 91104.
I0302 18:59:55.842409 22699590365312 run.py:483] Algo bellman_ford step 2847 current loss 0.657374, current_train_items 91136.
I0302 18:59:55.872909 22699590365312 run.py:483] Algo bellman_ford step 2848 current loss 0.635664, current_train_items 91168.
I0302 18:59:55.907979 22699590365312 run.py:483] Algo bellman_ford step 2849 current loss 0.835611, current_train_items 91200.
I0302 18:59:55.927748 22699590365312 run.py:483] Algo bellman_ford step 2850 current loss 0.334296, current_train_items 91232.
I0302 18:59:55.935975 22699590365312 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0302 18:59:55.936081 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:59:55.952460 22699590365312 run.py:483] Algo bellman_ford step 2851 current loss 0.439378, current_train_items 91264.
I0302 18:59:55.976304 22699590365312 run.py:483] Algo bellman_ford step 2852 current loss 0.611709, current_train_items 91296.
I0302 18:59:56.008120 22699590365312 run.py:483] Algo bellman_ford step 2853 current loss 0.797731, current_train_items 91328.
I0302 18:59:56.041960 22699590365312 run.py:483] Algo bellman_ford step 2854 current loss 0.811554, current_train_items 91360.
I0302 18:59:56.061935 22699590365312 run.py:483] Algo bellman_ford step 2855 current loss 0.326695, current_train_items 91392.
I0302 18:59:56.077932 22699590365312 run.py:483] Algo bellman_ford step 2856 current loss 0.515620, current_train_items 91424.
I0302 18:59:56.101554 22699590365312 run.py:483] Algo bellman_ford step 2857 current loss 0.632594, current_train_items 91456.
I0302 18:59:56.130698 22699590365312 run.py:483] Algo bellman_ford step 2858 current loss 0.568287, current_train_items 91488.
I0302 18:59:56.162779 22699590365312 run.py:483] Algo bellman_ford step 2859 current loss 0.707266, current_train_items 91520.
I0302 18:59:56.182843 22699590365312 run.py:483] Algo bellman_ford step 2860 current loss 0.339294, current_train_items 91552.
I0302 18:59:56.199130 22699590365312 run.py:483] Algo bellman_ford step 2861 current loss 0.442443, current_train_items 91584.
I0302 18:59:56.222367 22699590365312 run.py:483] Algo bellman_ford step 2862 current loss 0.603038, current_train_items 91616.
I0302 18:59:56.253799 22699590365312 run.py:483] Algo bellman_ford step 2863 current loss 0.796647, current_train_items 91648.
I0302 18:59:56.284811 22699590365312 run.py:483] Algo bellman_ford step 2864 current loss 0.731614, current_train_items 91680.
I0302 18:59:56.304764 22699590365312 run.py:483] Algo bellman_ford step 2865 current loss 0.332919, current_train_items 91712.
I0302 18:59:56.320922 22699590365312 run.py:483] Algo bellman_ford step 2866 current loss 0.468987, current_train_items 91744.
I0302 18:59:56.344356 22699590365312 run.py:483] Algo bellman_ford step 2867 current loss 0.628311, current_train_items 91776.
I0302 18:59:56.374525 22699590365312 run.py:483] Algo bellman_ford step 2868 current loss 0.682451, current_train_items 91808.
I0302 18:59:56.408342 22699590365312 run.py:483] Algo bellman_ford step 2869 current loss 0.845559, current_train_items 91840.
I0302 18:59:56.428219 22699590365312 run.py:483] Algo bellman_ford step 2870 current loss 0.263898, current_train_items 91872.
I0302 18:59:56.444653 22699590365312 run.py:483] Algo bellman_ford step 2871 current loss 0.440457, current_train_items 91904.
I0302 18:59:56.468401 22699590365312 run.py:483] Algo bellman_ford step 2872 current loss 0.631681, current_train_items 91936.
I0302 18:59:56.500097 22699590365312 run.py:483] Algo bellman_ford step 2873 current loss 0.741560, current_train_items 91968.
I0302 18:59:56.532944 22699590365312 run.py:483] Algo bellman_ford step 2874 current loss 0.811925, current_train_items 92000.
I0302 18:59:56.552767 22699590365312 run.py:483] Algo bellman_ford step 2875 current loss 0.320844, current_train_items 92032.
I0302 18:59:56.569041 22699590365312 run.py:483] Algo bellman_ford step 2876 current loss 0.586498, current_train_items 92064.
I0302 18:59:56.591911 22699590365312 run.py:483] Algo bellman_ford step 2877 current loss 0.604803, current_train_items 92096.
I0302 18:59:56.623860 22699590365312 run.py:483] Algo bellman_ford step 2878 current loss 0.726828, current_train_items 92128.
I0302 18:59:56.656369 22699590365312 run.py:483] Algo bellman_ford step 2879 current loss 0.801521, current_train_items 92160.
I0302 18:59:56.676417 22699590365312 run.py:483] Algo bellman_ford step 2880 current loss 0.344736, current_train_items 92192.
I0302 18:59:56.692534 22699590365312 run.py:483] Algo bellman_ford step 2881 current loss 0.491271, current_train_items 92224.
I0302 18:59:56.716088 22699590365312 run.py:483] Algo bellman_ford step 2882 current loss 0.619270, current_train_items 92256.
I0302 18:59:56.747214 22699590365312 run.py:483] Algo bellman_ford step 2883 current loss 0.702898, current_train_items 92288.
I0302 18:59:56.778710 22699590365312 run.py:483] Algo bellman_ford step 2884 current loss 0.883181, current_train_items 92320.
I0302 18:59:56.798702 22699590365312 run.py:483] Algo bellman_ford step 2885 current loss 0.293921, current_train_items 92352.
I0302 18:59:56.814820 22699590365312 run.py:483] Algo bellman_ford step 2886 current loss 0.459104, current_train_items 92384.
I0302 18:59:56.838889 22699590365312 run.py:483] Algo bellman_ford step 2887 current loss 0.699546, current_train_items 92416.
I0302 18:59:56.868894 22699590365312 run.py:483] Algo bellman_ford step 2888 current loss 0.687639, current_train_items 92448.
I0302 18:59:56.902969 22699590365312 run.py:483] Algo bellman_ford step 2889 current loss 0.830704, current_train_items 92480.
I0302 18:59:56.923314 22699590365312 run.py:483] Algo bellman_ford step 2890 current loss 0.379547, current_train_items 92512.
I0302 18:59:56.939444 22699590365312 run.py:483] Algo bellman_ford step 2891 current loss 0.480167, current_train_items 92544.
I0302 18:59:56.963992 22699590365312 run.py:483] Algo bellman_ford step 2892 current loss 0.623867, current_train_items 92576.
I0302 18:59:56.993997 22699590365312 run.py:483] Algo bellman_ford step 2893 current loss 0.763893, current_train_items 92608.
I0302 18:59:57.027922 22699590365312 run.py:483] Algo bellman_ford step 2894 current loss 0.869844, current_train_items 92640.
I0302 18:59:57.047261 22699590365312 run.py:483] Algo bellman_ford step 2895 current loss 0.328591, current_train_items 92672.
I0302 18:59:57.064064 22699590365312 run.py:483] Algo bellman_ford step 2896 current loss 0.562008, current_train_items 92704.
I0302 18:59:57.086397 22699590365312 run.py:483] Algo bellman_ford step 2897 current loss 0.631421, current_train_items 92736.
I0302 18:59:57.116860 22699590365312 run.py:483] Algo bellman_ford step 2898 current loss 0.624026, current_train_items 92768.
I0302 18:59:57.147941 22699590365312 run.py:483] Algo bellman_ford step 2899 current loss 0.627256, current_train_items 92800.
I0302 18:59:57.167397 22699590365312 run.py:483] Algo bellman_ford step 2900 current loss 0.273293, current_train_items 92832.
I0302 18:59:57.175447 22699590365312 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0302 18:59:57.175554 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 18:59:57.191923 22699590365312 run.py:483] Algo bellman_ford step 2901 current loss 0.411018, current_train_items 92864.
I0302 18:59:57.215676 22699590365312 run.py:483] Algo bellman_ford step 2902 current loss 0.648041, current_train_items 92896.
I0302 18:59:57.247045 22699590365312 run.py:483] Algo bellman_ford step 2903 current loss 0.651802, current_train_items 92928.
I0302 18:59:57.281381 22699590365312 run.py:483] Algo bellman_ford step 2904 current loss 0.768277, current_train_items 92960.
I0302 18:59:57.301671 22699590365312 run.py:483] Algo bellman_ford step 2905 current loss 0.392034, current_train_items 92992.
I0302 18:59:57.317744 22699590365312 run.py:483] Algo bellman_ford step 2906 current loss 0.466573, current_train_items 93024.
I0302 18:59:57.341823 22699590365312 run.py:483] Algo bellman_ford step 2907 current loss 0.668002, current_train_items 93056.
I0302 18:59:57.374266 22699590365312 run.py:483] Algo bellman_ford step 2908 current loss 0.827592, current_train_items 93088.
I0302 18:59:57.408471 22699590365312 run.py:483] Algo bellman_ford step 2909 current loss 0.919614, current_train_items 93120.
I0302 18:59:57.427999 22699590365312 run.py:483] Algo bellman_ford step 2910 current loss 0.311265, current_train_items 93152.
I0302 18:59:57.444615 22699590365312 run.py:483] Algo bellman_ford step 2911 current loss 0.563689, current_train_items 93184.
I0302 18:59:57.468193 22699590365312 run.py:483] Algo bellman_ford step 2912 current loss 0.644333, current_train_items 93216.
I0302 18:59:57.498809 22699590365312 run.py:483] Algo bellman_ford step 2913 current loss 0.753908, current_train_items 93248.
I0302 18:59:57.533773 22699590365312 run.py:483] Algo bellman_ford step 2914 current loss 1.015057, current_train_items 93280.
I0302 18:59:57.553243 22699590365312 run.py:483] Algo bellman_ford step 2915 current loss 0.277408, current_train_items 93312.
I0302 18:59:57.569356 22699590365312 run.py:483] Algo bellman_ford step 2916 current loss 0.515085, current_train_items 93344.
I0302 18:59:57.592317 22699590365312 run.py:483] Algo bellman_ford step 2917 current loss 0.630598, current_train_items 93376.
I0302 18:59:57.624531 22699590365312 run.py:483] Algo bellman_ford step 2918 current loss 0.853159, current_train_items 93408.
I0302 18:59:57.659871 22699590365312 run.py:483] Algo bellman_ford step 2919 current loss 0.919479, current_train_items 93440.
I0302 18:59:57.679642 22699590365312 run.py:483] Algo bellman_ford step 2920 current loss 0.353716, current_train_items 93472.
I0302 18:59:57.695876 22699590365312 run.py:483] Algo bellman_ford step 2921 current loss 0.602402, current_train_items 93504.
I0302 18:59:57.719801 22699590365312 run.py:483] Algo bellman_ford step 2922 current loss 0.740757, current_train_items 93536.
I0302 18:59:57.749944 22699590365312 run.py:483] Algo bellman_ford step 2923 current loss 0.780405, current_train_items 93568.
I0302 18:59:57.785520 22699590365312 run.py:483] Algo bellman_ford step 2924 current loss 1.093997, current_train_items 93600.
I0302 18:59:57.805473 22699590365312 run.py:483] Algo bellman_ford step 2925 current loss 0.304339, current_train_items 93632.
I0302 18:59:57.821567 22699590365312 run.py:483] Algo bellman_ford step 2926 current loss 0.498549, current_train_items 93664.
I0302 18:59:57.846127 22699590365312 run.py:483] Algo bellman_ford step 2927 current loss 0.778558, current_train_items 93696.
I0302 18:59:57.877384 22699590365312 run.py:483] Algo bellman_ford step 2928 current loss 0.714309, current_train_items 93728.
I0302 18:59:57.911456 22699590365312 run.py:483] Algo bellman_ford step 2929 current loss 0.828999, current_train_items 93760.
I0302 18:59:57.931015 22699590365312 run.py:483] Algo bellman_ford step 2930 current loss 0.328178, current_train_items 93792.
I0302 18:59:57.946992 22699590365312 run.py:483] Algo bellman_ford step 2931 current loss 0.473623, current_train_items 93824.
I0302 18:59:57.971929 22699590365312 run.py:483] Algo bellman_ford step 2932 current loss 0.711438, current_train_items 93856.
I0302 18:59:58.002146 22699590365312 run.py:483] Algo bellman_ford step 2933 current loss 0.755418, current_train_items 93888.
I0302 18:59:58.035386 22699590365312 run.py:483] Algo bellman_ford step 2934 current loss 0.727915, current_train_items 93920.
I0302 18:59:58.055284 22699590365312 run.py:483] Algo bellman_ford step 2935 current loss 0.322787, current_train_items 93952.
I0302 18:59:58.071330 22699590365312 run.py:483] Algo bellman_ford step 2936 current loss 0.415116, current_train_items 93984.
I0302 18:59:58.095137 22699590365312 run.py:483] Algo bellman_ford step 2937 current loss 0.685802, current_train_items 94016.
I0302 18:59:58.124349 22699590365312 run.py:483] Algo bellman_ford step 2938 current loss 0.601316, current_train_items 94048.
I0302 18:59:58.157971 22699590365312 run.py:483] Algo bellman_ford step 2939 current loss 0.853066, current_train_items 94080.
I0302 18:59:58.177386 22699590365312 run.py:483] Algo bellman_ford step 2940 current loss 0.410010, current_train_items 94112.
I0302 18:59:58.193848 22699590365312 run.py:483] Algo bellman_ford step 2941 current loss 0.549160, current_train_items 94144.
I0302 18:59:58.217909 22699590365312 run.py:483] Algo bellman_ford step 2942 current loss 0.696538, current_train_items 94176.
I0302 18:59:58.249696 22699590365312 run.py:483] Algo bellman_ford step 2943 current loss 0.808555, current_train_items 94208.
I0302 18:59:58.285309 22699590365312 run.py:483] Algo bellman_ford step 2944 current loss 0.804101, current_train_items 94240.
I0302 18:59:58.304703 22699590365312 run.py:483] Algo bellman_ford step 2945 current loss 0.430587, current_train_items 94272.
I0302 18:59:58.321166 22699590365312 run.py:483] Algo bellman_ford step 2946 current loss 0.530058, current_train_items 94304.
I0302 18:59:58.344770 22699590365312 run.py:483] Algo bellman_ford step 2947 current loss 0.630235, current_train_items 94336.
I0302 18:59:58.375472 22699590365312 run.py:483] Algo bellman_ford step 2948 current loss 0.682043, current_train_items 94368.
I0302 18:59:58.408666 22699590365312 run.py:483] Algo bellman_ford step 2949 current loss 0.698730, current_train_items 94400.
I0302 18:59:58.428423 22699590365312 run.py:483] Algo bellman_ford step 2950 current loss 0.363066, current_train_items 94432.
I0302 18:59:58.436695 22699590365312 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0302 18:59:58.436800 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 18:59:58.453665 22699590365312 run.py:483] Algo bellman_ford step 2951 current loss 0.569016, current_train_items 94464.
I0302 18:59:58.475705 22699590365312 run.py:483] Algo bellman_ford step 2952 current loss 0.473760, current_train_items 94496.
I0302 18:59:58.506020 22699590365312 run.py:483] Algo bellman_ford step 2953 current loss 0.658008, current_train_items 94528.
I0302 18:59:58.540247 22699590365312 run.py:483] Algo bellman_ford step 2954 current loss 0.843690, current_train_items 94560.
I0302 18:59:58.559944 22699590365312 run.py:483] Algo bellman_ford step 2955 current loss 0.315701, current_train_items 94592.
I0302 18:59:58.575533 22699590365312 run.py:483] Algo bellman_ford step 2956 current loss 0.465925, current_train_items 94624.
I0302 18:59:58.599438 22699590365312 run.py:483] Algo bellman_ford step 2957 current loss 0.838801, current_train_items 94656.
I0302 18:59:58.631215 22699590365312 run.py:483] Algo bellman_ford step 2958 current loss 0.766088, current_train_items 94688.
I0302 18:59:58.664894 22699590365312 run.py:483] Algo bellman_ford step 2959 current loss 0.823873, current_train_items 94720.
I0302 18:59:58.684465 22699590365312 run.py:483] Algo bellman_ford step 2960 current loss 0.344917, current_train_items 94752.
I0302 18:59:58.700915 22699590365312 run.py:483] Algo bellman_ford step 2961 current loss 0.439414, current_train_items 94784.
I0302 18:59:58.723688 22699590365312 run.py:483] Algo bellman_ford step 2962 current loss 0.859262, current_train_items 94816.
I0302 18:59:58.753257 22699590365312 run.py:483] Algo bellman_ford step 2963 current loss 0.884476, current_train_items 94848.
I0302 18:59:58.787850 22699590365312 run.py:483] Algo bellman_ford step 2964 current loss 0.884299, current_train_items 94880.
I0302 18:59:58.806963 22699590365312 run.py:483] Algo bellman_ford step 2965 current loss 0.293137, current_train_items 94912.
I0302 18:59:58.823080 22699590365312 run.py:483] Algo bellman_ford step 2966 current loss 0.531295, current_train_items 94944.
I0302 18:59:58.846426 22699590365312 run.py:483] Algo bellman_ford step 2967 current loss 0.775366, current_train_items 94976.
I0302 18:59:58.876624 22699590365312 run.py:483] Algo bellman_ford step 2968 current loss 0.737307, current_train_items 95008.
I0302 18:59:58.910710 22699590365312 run.py:483] Algo bellman_ford step 2969 current loss 0.884696, current_train_items 95040.
I0302 18:59:58.930700 22699590365312 run.py:483] Algo bellman_ford step 2970 current loss 0.319541, current_train_items 95072.
I0302 18:59:58.946629 22699590365312 run.py:483] Algo bellman_ford step 2971 current loss 0.498526, current_train_items 95104.
I0302 18:59:58.969624 22699590365312 run.py:483] Algo bellman_ford step 2972 current loss 0.689774, current_train_items 95136.
I0302 18:59:58.999207 22699590365312 run.py:483] Algo bellman_ford step 2973 current loss 0.670534, current_train_items 95168.
I0302 18:59:59.035247 22699590365312 run.py:483] Algo bellman_ford step 2974 current loss 0.975195, current_train_items 95200.
I0302 18:59:59.054929 22699590365312 run.py:483] Algo bellman_ford step 2975 current loss 0.373281, current_train_items 95232.
I0302 18:59:59.071862 22699590365312 run.py:483] Algo bellman_ford step 2976 current loss 0.514443, current_train_items 95264.
I0302 18:59:59.094762 22699590365312 run.py:483] Algo bellman_ford step 2977 current loss 0.595549, current_train_items 95296.
I0302 18:59:59.124609 22699590365312 run.py:483] Algo bellman_ford step 2978 current loss 0.749184, current_train_items 95328.
I0302 18:59:59.159177 22699590365312 run.py:483] Algo bellman_ford step 2979 current loss 0.929689, current_train_items 95360.
I0302 18:59:59.178547 22699590365312 run.py:483] Algo bellman_ford step 2980 current loss 0.280344, current_train_items 95392.
I0302 18:59:59.194474 22699590365312 run.py:483] Algo bellman_ford step 2981 current loss 0.508943, current_train_items 95424.
I0302 18:59:59.218338 22699590365312 run.py:483] Algo bellman_ford step 2982 current loss 0.692768, current_train_items 95456.
I0302 18:59:59.250837 22699590365312 run.py:483] Algo bellman_ford step 2983 current loss 0.699540, current_train_items 95488.
I0302 18:59:59.284363 22699590365312 run.py:483] Algo bellman_ford step 2984 current loss 0.853403, current_train_items 95520.
I0302 18:59:59.303962 22699590365312 run.py:483] Algo bellman_ford step 2985 current loss 0.325360, current_train_items 95552.
I0302 18:59:59.320570 22699590365312 run.py:483] Algo bellman_ford step 2986 current loss 0.527290, current_train_items 95584.
I0302 18:59:59.344766 22699590365312 run.py:483] Algo bellman_ford step 2987 current loss 0.599489, current_train_items 95616.
I0302 18:59:59.375200 22699590365312 run.py:483] Algo bellman_ford step 2988 current loss 0.685356, current_train_items 95648.
I0302 18:59:59.408944 22699590365312 run.py:483] Algo bellman_ford step 2989 current loss 0.818825, current_train_items 95680.
I0302 18:59:59.428540 22699590365312 run.py:483] Algo bellman_ford step 2990 current loss 0.339692, current_train_items 95712.
I0302 18:59:59.445385 22699590365312 run.py:483] Algo bellman_ford step 2991 current loss 0.513643, current_train_items 95744.
I0302 18:59:59.468983 22699590365312 run.py:483] Algo bellman_ford step 2992 current loss 0.669355, current_train_items 95776.
I0302 18:59:59.500365 22699590365312 run.py:483] Algo bellman_ford step 2993 current loss 0.818338, current_train_items 95808.
I0302 18:59:59.535036 22699590365312 run.py:483] Algo bellman_ford step 2994 current loss 0.982581, current_train_items 95840.
I0302 18:59:59.554275 22699590365312 run.py:483] Algo bellman_ford step 2995 current loss 0.246620, current_train_items 95872.
I0302 18:59:59.570527 22699590365312 run.py:483] Algo bellman_ford step 2996 current loss 0.492539, current_train_items 95904.
I0302 18:59:59.594925 22699590365312 run.py:483] Algo bellman_ford step 2997 current loss 0.702304, current_train_items 95936.
I0302 18:59:59.626287 22699590365312 run.py:483] Algo bellman_ford step 2998 current loss 0.705837, current_train_items 95968.
I0302 18:59:59.658751 22699590365312 run.py:483] Algo bellman_ford step 2999 current loss 0.865881, current_train_items 96000.
I0302 18:59:59.678275 22699590365312 run.py:483] Algo bellman_ford step 3000 current loss 0.355141, current_train_items 96032.
I0302 18:59:59.686135 22699590365312 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0302 18:59:59.686249 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 18:59:59.702812 22699590365312 run.py:483] Algo bellman_ford step 3001 current loss 0.568577, current_train_items 96064.
I0302 18:59:59.726373 22699590365312 run.py:483] Algo bellman_ford step 3002 current loss 0.539226, current_train_items 96096.
I0302 18:59:59.757518 22699590365312 run.py:483] Algo bellman_ford step 3003 current loss 0.765401, current_train_items 96128.
I0302 18:59:59.790635 22699590365312 run.py:483] Algo bellman_ford step 3004 current loss 0.774748, current_train_items 96160.
I0302 18:59:59.810532 22699590365312 run.py:483] Algo bellman_ford step 3005 current loss 0.283335, current_train_items 96192.
I0302 18:59:59.826299 22699590365312 run.py:483] Algo bellman_ford step 3006 current loss 0.472433, current_train_items 96224.
I0302 18:59:59.850952 22699590365312 run.py:483] Algo bellman_ford step 3007 current loss 0.630352, current_train_items 96256.
I0302 18:59:59.882367 22699590365312 run.py:483] Algo bellman_ford step 3008 current loss 0.745592, current_train_items 96288.
I0302 18:59:59.918547 22699590365312 run.py:483] Algo bellman_ford step 3009 current loss 0.900891, current_train_items 96320.
I0302 18:59:59.938326 22699590365312 run.py:483] Algo bellman_ford step 3010 current loss 0.356265, current_train_items 96352.
I0302 18:59:59.954577 22699590365312 run.py:483] Algo bellman_ford step 3011 current loss 0.498933, current_train_items 96384.
I0302 18:59:59.978324 22699590365312 run.py:483] Algo bellman_ford step 3012 current loss 0.608159, current_train_items 96416.
I0302 19:00:00.008525 22699590365312 run.py:483] Algo bellman_ford step 3013 current loss 0.642172, current_train_items 96448.
I0302 19:00:00.042993 22699590365312 run.py:483] Algo bellman_ford step 3014 current loss 0.951265, current_train_items 96480.
I0302 19:00:00.062695 22699590365312 run.py:483] Algo bellman_ford step 3015 current loss 0.309557, current_train_items 96512.
I0302 19:00:00.079246 22699590365312 run.py:483] Algo bellman_ford step 3016 current loss 0.510080, current_train_items 96544.
I0302 19:00:00.102346 22699590365312 run.py:483] Algo bellman_ford step 3017 current loss 0.656416, current_train_items 96576.
I0302 19:00:00.132506 22699590365312 run.py:483] Algo bellman_ford step 3018 current loss 0.674722, current_train_items 96608.
I0302 19:00:00.164865 22699590365312 run.py:483] Algo bellman_ford step 3019 current loss 0.757686, current_train_items 96640.
I0302 19:00:00.184080 22699590365312 run.py:483] Algo bellman_ford step 3020 current loss 0.234402, current_train_items 96672.
I0302 19:00:00.199823 22699590365312 run.py:483] Algo bellman_ford step 3021 current loss 0.516650, current_train_items 96704.
I0302 19:00:00.224311 22699590365312 run.py:483] Algo bellman_ford step 3022 current loss 0.622505, current_train_items 96736.
I0302 19:00:00.255455 22699590365312 run.py:483] Algo bellman_ford step 3023 current loss 0.778387, current_train_items 96768.
I0302 19:00:00.288958 22699590365312 run.py:483] Algo bellman_ford step 3024 current loss 0.856134, current_train_items 96800.
I0302 19:00:00.308426 22699590365312 run.py:483] Algo bellman_ford step 3025 current loss 0.254747, current_train_items 96832.
I0302 19:00:00.324491 22699590365312 run.py:483] Algo bellman_ford step 3026 current loss 0.553137, current_train_items 96864.
I0302 19:00:00.346952 22699590365312 run.py:483] Algo bellman_ford step 3027 current loss 0.541615, current_train_items 96896.
I0302 19:00:00.377408 22699590365312 run.py:483] Algo bellman_ford step 3028 current loss 0.675771, current_train_items 96928.
I0302 19:00:00.409642 22699590365312 run.py:483] Algo bellman_ford step 3029 current loss 0.757007, current_train_items 96960.
I0302 19:00:00.429421 22699590365312 run.py:483] Algo bellman_ford step 3030 current loss 0.315680, current_train_items 96992.
I0302 19:00:00.445401 22699590365312 run.py:483] Algo bellman_ford step 3031 current loss 0.409309, current_train_items 97024.
I0302 19:00:00.469789 22699590365312 run.py:483] Algo bellman_ford step 3032 current loss 0.615563, current_train_items 97056.
I0302 19:00:00.501405 22699590365312 run.py:483] Algo bellman_ford step 3033 current loss 0.744465, current_train_items 97088.
I0302 19:00:00.535628 22699590365312 run.py:483] Algo bellman_ford step 3034 current loss 0.747862, current_train_items 97120.
I0302 19:00:00.555238 22699590365312 run.py:483] Algo bellman_ford step 3035 current loss 0.291064, current_train_items 97152.
I0302 19:00:00.571657 22699590365312 run.py:483] Algo bellman_ford step 3036 current loss 0.501442, current_train_items 97184.
I0302 19:00:00.595322 22699590365312 run.py:483] Algo bellman_ford step 3037 current loss 0.712302, current_train_items 97216.
I0302 19:00:00.626532 22699590365312 run.py:483] Algo bellman_ford step 3038 current loss 0.835844, current_train_items 97248.
I0302 19:00:00.658602 22699590365312 run.py:483] Algo bellman_ford step 3039 current loss 0.929813, current_train_items 97280.
I0302 19:00:00.678030 22699590365312 run.py:483] Algo bellman_ford step 3040 current loss 0.274459, current_train_items 97312.
I0302 19:00:00.694115 22699590365312 run.py:483] Algo bellman_ford step 3041 current loss 0.442771, current_train_items 97344.
I0302 19:00:00.717633 22699590365312 run.py:483] Algo bellman_ford step 3042 current loss 0.582548, current_train_items 97376.
I0302 19:00:00.748339 22699590365312 run.py:483] Algo bellman_ford step 3043 current loss 0.724621, current_train_items 97408.
I0302 19:00:00.780905 22699590365312 run.py:483] Algo bellman_ford step 3044 current loss 0.848543, current_train_items 97440.
I0302 19:00:00.800371 22699590365312 run.py:483] Algo bellman_ford step 3045 current loss 0.340783, current_train_items 97472.
I0302 19:00:00.816589 22699590365312 run.py:483] Algo bellman_ford step 3046 current loss 0.612054, current_train_items 97504.
I0302 19:00:00.839945 22699590365312 run.py:483] Algo bellman_ford step 3047 current loss 0.717366, current_train_items 97536.
I0302 19:00:00.871190 22699590365312 run.py:483] Algo bellman_ford step 3048 current loss 0.752302, current_train_items 97568.
I0302 19:00:00.904234 22699590365312 run.py:483] Algo bellman_ford step 3049 current loss 0.844882, current_train_items 97600.
I0302 19:00:00.923600 22699590365312 run.py:483] Algo bellman_ford step 3050 current loss 0.394505, current_train_items 97632.
I0302 19:00:00.931918 22699590365312 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.88671875, 'score': 0.88671875, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0302 19:00:00.932025 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.887, val scores are: bellman_ford: 0.887
I0302 19:00:00.948658 22699590365312 run.py:483] Algo bellman_ford step 3051 current loss 0.522992, current_train_items 97664.
I0302 19:00:00.972327 22699590365312 run.py:483] Algo bellman_ford step 3052 current loss 0.849060, current_train_items 97696.
I0302 19:00:01.003826 22699590365312 run.py:483] Algo bellman_ford step 3053 current loss 0.799815, current_train_items 97728.
I0302 19:00:01.035604 22699590365312 run.py:483] Algo bellman_ford step 3054 current loss 0.796947, current_train_items 97760.
I0302 19:00:01.055531 22699590365312 run.py:483] Algo bellman_ford step 3055 current loss 0.402056, current_train_items 97792.
I0302 19:00:01.071311 22699590365312 run.py:483] Algo bellman_ford step 3056 current loss 0.513184, current_train_items 97824.
I0302 19:00:01.094471 22699590365312 run.py:483] Algo bellman_ford step 3057 current loss 0.752165, current_train_items 97856.
I0302 19:00:01.125128 22699590365312 run.py:483] Algo bellman_ford step 3058 current loss 0.781615, current_train_items 97888.
I0302 19:00:01.157292 22699590365312 run.py:483] Algo bellman_ford step 3059 current loss 0.912160, current_train_items 97920.
I0302 19:00:01.176711 22699590365312 run.py:483] Algo bellman_ford step 3060 current loss 0.356387, current_train_items 97952.
I0302 19:00:01.193140 22699590365312 run.py:483] Algo bellman_ford step 3061 current loss 0.576875, current_train_items 97984.
I0302 19:00:01.215712 22699590365312 run.py:483] Algo bellman_ford step 3062 current loss 0.639671, current_train_items 98016.
I0302 19:00:01.247241 22699590365312 run.py:483] Algo bellman_ford step 3063 current loss 0.994913, current_train_items 98048.
I0302 19:00:01.281359 22699590365312 run.py:483] Algo bellman_ford step 3064 current loss 0.927291, current_train_items 98080.
I0302 19:00:01.300771 22699590365312 run.py:483] Algo bellman_ford step 3065 current loss 0.373867, current_train_items 98112.
I0302 19:00:01.316787 22699590365312 run.py:483] Algo bellman_ford step 3066 current loss 0.513377, current_train_items 98144.
I0302 19:00:01.340934 22699590365312 run.py:483] Algo bellman_ford step 3067 current loss 0.665973, current_train_items 98176.
I0302 19:00:01.371079 22699590365312 run.py:483] Algo bellman_ford step 3068 current loss 0.674713, current_train_items 98208.
I0302 19:00:01.406605 22699590365312 run.py:483] Algo bellman_ford step 3069 current loss 0.893624, current_train_items 98240.
I0302 19:00:01.426568 22699590365312 run.py:483] Algo bellman_ford step 3070 current loss 0.305962, current_train_items 98272.
I0302 19:00:01.442902 22699590365312 run.py:483] Algo bellman_ford step 3071 current loss 0.518513, current_train_items 98304.
I0302 19:00:01.465578 22699590365312 run.py:483] Algo bellman_ford step 3072 current loss 0.608653, current_train_items 98336.
I0302 19:00:01.495346 22699590365312 run.py:483] Algo bellman_ford step 3073 current loss 0.684020, current_train_items 98368.
I0302 19:00:01.530247 22699590365312 run.py:483] Algo bellman_ford step 3074 current loss 0.880229, current_train_items 98400.
I0302 19:00:01.549844 22699590365312 run.py:483] Algo bellman_ford step 3075 current loss 0.287377, current_train_items 98432.
I0302 19:00:01.566395 22699590365312 run.py:483] Algo bellman_ford step 3076 current loss 0.680176, current_train_items 98464.
I0302 19:00:01.589594 22699590365312 run.py:483] Algo bellman_ford step 3077 current loss 0.622655, current_train_items 98496.
I0302 19:00:01.620319 22699590365312 run.py:483] Algo bellman_ford step 3078 current loss 0.815947, current_train_items 98528.
I0302 19:00:01.652000 22699590365312 run.py:483] Algo bellman_ford step 3079 current loss 0.758844, current_train_items 98560.
I0302 19:00:01.671449 22699590365312 run.py:483] Algo bellman_ford step 3080 current loss 0.308284, current_train_items 98592.
I0302 19:00:01.687757 22699590365312 run.py:483] Algo bellman_ford step 3081 current loss 0.550394, current_train_items 98624.
I0302 19:00:01.710834 22699590365312 run.py:483] Algo bellman_ford step 3082 current loss 0.627788, current_train_items 98656.
I0302 19:00:01.741408 22699590365312 run.py:483] Algo bellman_ford step 3083 current loss 0.792179, current_train_items 98688.
I0302 19:00:01.774859 22699590365312 run.py:483] Algo bellman_ford step 3084 current loss 1.002378, current_train_items 98720.
I0302 19:00:01.794590 22699590365312 run.py:483] Algo bellman_ford step 3085 current loss 0.331667, current_train_items 98752.
I0302 19:00:01.811061 22699590365312 run.py:483] Algo bellman_ford step 3086 current loss 0.606944, current_train_items 98784.
I0302 19:00:01.833099 22699590365312 run.py:483] Algo bellman_ford step 3087 current loss 0.602541, current_train_items 98816.
I0302 19:00:01.863722 22699590365312 run.py:483] Algo bellman_ford step 3088 current loss 0.822715, current_train_items 98848.
I0302 19:00:01.898404 22699590365312 run.py:483] Algo bellman_ford step 3089 current loss 0.835647, current_train_items 98880.
I0302 19:00:01.918257 22699590365312 run.py:483] Algo bellman_ford step 3090 current loss 0.277248, current_train_items 98912.
I0302 19:00:01.934441 22699590365312 run.py:483] Algo bellman_ford step 3091 current loss 0.533245, current_train_items 98944.
I0302 19:00:01.956604 22699590365312 run.py:483] Algo bellman_ford step 3092 current loss 0.721430, current_train_items 98976.
I0302 19:00:01.987357 22699590365312 run.py:483] Algo bellman_ford step 3093 current loss 0.828234, current_train_items 99008.
I0302 19:00:02.020500 22699590365312 run.py:483] Algo bellman_ford step 3094 current loss 1.070556, current_train_items 99040.
I0302 19:00:02.040328 22699590365312 run.py:483] Algo bellman_ford step 3095 current loss 0.350217, current_train_items 99072.
I0302 19:00:02.056944 22699590365312 run.py:483] Algo bellman_ford step 3096 current loss 0.514050, current_train_items 99104.
I0302 19:00:02.081342 22699590365312 run.py:483] Algo bellman_ford step 3097 current loss 0.778756, current_train_items 99136.
I0302 19:00:02.111463 22699590365312 run.py:483] Algo bellman_ford step 3098 current loss 0.817438, current_train_items 99168.
I0302 19:00:02.144293 22699590365312 run.py:483] Algo bellman_ford step 3099 current loss 0.828043, current_train_items 99200.
I0302 19:00:02.163971 22699590365312 run.py:483] Algo bellman_ford step 3100 current loss 0.375379, current_train_items 99232.
I0302 19:00:02.172011 22699590365312 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0302 19:00:02.172118 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:00:02.189989 22699590365312 run.py:483] Algo bellman_ford step 3101 current loss 0.492473, current_train_items 99264.
I0302 19:00:02.214068 22699590365312 run.py:483] Algo bellman_ford step 3102 current loss 0.731747, current_train_items 99296.
I0302 19:00:02.245542 22699590365312 run.py:483] Algo bellman_ford step 3103 current loss 0.773005, current_train_items 99328.
I0302 19:00:02.280309 22699590365312 run.py:483] Algo bellman_ford step 3104 current loss 0.833539, current_train_items 99360.
I0302 19:00:02.300474 22699590365312 run.py:483] Algo bellman_ford step 3105 current loss 0.290796, current_train_items 99392.
I0302 19:00:02.316005 22699590365312 run.py:483] Algo bellman_ford step 3106 current loss 0.529280, current_train_items 99424.
I0302 19:00:02.339582 22699590365312 run.py:483] Algo bellman_ford step 3107 current loss 0.655038, current_train_items 99456.
I0302 19:00:02.371337 22699590365312 run.py:483] Algo bellman_ford step 3108 current loss 0.777609, current_train_items 99488.
I0302 19:00:02.403802 22699590365312 run.py:483] Algo bellman_ford step 3109 current loss 0.845350, current_train_items 99520.
I0302 19:00:02.423449 22699590365312 run.py:483] Algo bellman_ford step 3110 current loss 0.271298, current_train_items 99552.
I0302 19:00:02.439617 22699590365312 run.py:483] Algo bellman_ford step 3111 current loss 0.487172, current_train_items 99584.
I0302 19:00:02.462930 22699590365312 run.py:483] Algo bellman_ford step 3112 current loss 0.748657, current_train_items 99616.
I0302 19:00:02.493441 22699590365312 run.py:483] Algo bellman_ford step 3113 current loss 0.775386, current_train_items 99648.
I0302 19:00:02.528105 22699590365312 run.py:483] Algo bellman_ford step 3114 current loss 0.915111, current_train_items 99680.
I0302 19:00:02.547796 22699590365312 run.py:483] Algo bellman_ford step 3115 current loss 0.309133, current_train_items 99712.
I0302 19:00:02.564201 22699590365312 run.py:483] Algo bellman_ford step 3116 current loss 0.509685, current_train_items 99744.
I0302 19:00:02.587675 22699590365312 run.py:483] Algo bellman_ford step 3117 current loss 0.688765, current_train_items 99776.
I0302 19:00:02.619269 22699590365312 run.py:483] Algo bellman_ford step 3118 current loss 0.837704, current_train_items 99808.
I0302 19:00:02.652020 22699590365312 run.py:483] Algo bellman_ford step 3119 current loss 0.874077, current_train_items 99840.
I0302 19:00:02.671499 22699590365312 run.py:483] Algo bellman_ford step 3120 current loss 0.197786, current_train_items 99872.
I0302 19:00:02.687582 22699590365312 run.py:483] Algo bellman_ford step 3121 current loss 0.591109, current_train_items 99904.
I0302 19:00:02.711432 22699590365312 run.py:483] Algo bellman_ford step 3122 current loss 0.666063, current_train_items 99936.
I0302 19:00:02.742649 22699590365312 run.py:483] Algo bellman_ford step 3123 current loss 0.741686, current_train_items 99968.
I0302 19:00:02.775913 22699590365312 run.py:483] Algo bellman_ford step 3124 current loss 0.880853, current_train_items 100000.
I0302 19:00:02.795424 22699590365312 run.py:483] Algo bellman_ford step 3125 current loss 0.311006, current_train_items 100032.
I0302 19:00:02.811652 22699590365312 run.py:483] Algo bellman_ford step 3126 current loss 0.526067, current_train_items 100064.
I0302 19:00:02.834222 22699590365312 run.py:483] Algo bellman_ford step 3127 current loss 0.601347, current_train_items 100096.
I0302 19:00:02.866189 22699590365312 run.py:483] Algo bellman_ford step 3128 current loss 0.758456, current_train_items 100128.
I0302 19:00:02.898461 22699590365312 run.py:483] Algo bellman_ford step 3129 current loss 0.842361, current_train_items 100160.
I0302 19:00:02.918310 22699590365312 run.py:483] Algo bellman_ford step 3130 current loss 0.270747, current_train_items 100192.
I0302 19:00:02.934506 22699590365312 run.py:483] Algo bellman_ford step 3131 current loss 0.474318, current_train_items 100224.
I0302 19:00:02.958257 22699590365312 run.py:483] Algo bellman_ford step 3132 current loss 0.581736, current_train_items 100256.
I0302 19:00:02.988666 22699590365312 run.py:483] Algo bellman_ford step 3133 current loss 0.741681, current_train_items 100288.
I0302 19:00:03.023764 22699590365312 run.py:483] Algo bellman_ford step 3134 current loss 0.957629, current_train_items 100320.
I0302 19:00:03.043321 22699590365312 run.py:483] Algo bellman_ford step 3135 current loss 0.384697, current_train_items 100352.
I0302 19:00:03.059433 22699590365312 run.py:483] Algo bellman_ford step 3136 current loss 0.472805, current_train_items 100384.
I0302 19:00:03.083594 22699590365312 run.py:483] Algo bellman_ford step 3137 current loss 0.642022, current_train_items 100416.
I0302 19:00:03.113731 22699590365312 run.py:483] Algo bellman_ford step 3138 current loss 0.690117, current_train_items 100448.
I0302 19:00:03.147784 22699590365312 run.py:483] Algo bellman_ford step 3139 current loss 0.769843, current_train_items 100480.
I0302 19:00:03.167345 22699590365312 run.py:483] Algo bellman_ford step 3140 current loss 0.348440, current_train_items 100512.
I0302 19:00:03.184101 22699590365312 run.py:483] Algo bellman_ford step 3141 current loss 0.533351, current_train_items 100544.
I0302 19:00:03.206462 22699590365312 run.py:483] Algo bellman_ford step 3142 current loss 0.603170, current_train_items 100576.
I0302 19:00:03.238410 22699590365312 run.py:483] Algo bellman_ford step 3143 current loss 0.681732, current_train_items 100608.
I0302 19:00:03.271509 22699590365312 run.py:483] Algo bellman_ford step 3144 current loss 0.764532, current_train_items 100640.
I0302 19:00:03.291085 22699590365312 run.py:483] Algo bellman_ford step 3145 current loss 0.269902, current_train_items 100672.
I0302 19:00:03.307214 22699590365312 run.py:483] Algo bellman_ford step 3146 current loss 0.391904, current_train_items 100704.
I0302 19:00:03.330049 22699590365312 run.py:483] Algo bellman_ford step 3147 current loss 0.572251, current_train_items 100736.
I0302 19:00:03.359030 22699590365312 run.py:483] Algo bellman_ford step 3148 current loss 0.677642, current_train_items 100768.
I0302 19:00:03.394468 22699590365312 run.py:483] Algo bellman_ford step 3149 current loss 0.896337, current_train_items 100800.
I0302 19:00:03.413681 22699590365312 run.py:483] Algo bellman_ford step 3150 current loss 0.286141, current_train_items 100832.
I0302 19:00:03.422039 22699590365312 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0302 19:00:03.422146 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:03.439065 22699590365312 run.py:483] Algo bellman_ford step 3151 current loss 0.486264, current_train_items 100864.
I0302 19:00:03.464132 22699590365312 run.py:483] Algo bellman_ford step 3152 current loss 0.808606, current_train_items 100896.
I0302 19:00:03.495668 22699590365312 run.py:483] Algo bellman_ford step 3153 current loss 0.893141, current_train_items 100928.
I0302 19:00:03.530468 22699590365312 run.py:483] Algo bellman_ford step 3154 current loss 0.984763, current_train_items 100960.
I0302 19:00:03.550602 22699590365312 run.py:483] Algo bellman_ford step 3155 current loss 0.268321, current_train_items 100992.
I0302 19:00:03.566456 22699590365312 run.py:483] Algo bellman_ford step 3156 current loss 0.510630, current_train_items 101024.
I0302 19:00:03.590682 22699590365312 run.py:483] Algo bellman_ford step 3157 current loss 0.653124, current_train_items 101056.
I0302 19:00:03.620879 22699590365312 run.py:483] Algo bellman_ford step 3158 current loss 0.724593, current_train_items 101088.
I0302 19:00:03.653738 22699590365312 run.py:483] Algo bellman_ford step 3159 current loss 0.897887, current_train_items 101120.
I0302 19:00:03.673989 22699590365312 run.py:483] Algo bellman_ford step 3160 current loss 0.342276, current_train_items 101152.
I0302 19:00:03.690044 22699590365312 run.py:483] Algo bellman_ford step 3161 current loss 0.533110, current_train_items 101184.
I0302 19:00:03.713444 22699590365312 run.py:483] Algo bellman_ford step 3162 current loss 0.683426, current_train_items 101216.
I0302 19:00:03.743351 22699590365312 run.py:483] Algo bellman_ford step 3163 current loss 0.642905, current_train_items 101248.
I0302 19:00:03.778151 22699590365312 run.py:483] Algo bellman_ford step 3164 current loss 0.891022, current_train_items 101280.
I0302 19:00:03.797940 22699590365312 run.py:483] Algo bellman_ford step 3165 current loss 0.349204, current_train_items 101312.
I0302 19:00:03.814774 22699590365312 run.py:483] Algo bellman_ford step 3166 current loss 0.553539, current_train_items 101344.
I0302 19:00:03.839010 22699590365312 run.py:483] Algo bellman_ford step 3167 current loss 0.735428, current_train_items 101376.
I0302 19:00:03.870081 22699590365312 run.py:483] Algo bellman_ford step 3168 current loss 0.849065, current_train_items 101408.
I0302 19:00:03.903174 22699590365312 run.py:483] Algo bellman_ford step 3169 current loss 0.937901, current_train_items 101440.
I0302 19:00:03.923299 22699590365312 run.py:483] Algo bellman_ford step 3170 current loss 0.414766, current_train_items 101472.
I0302 19:00:03.939938 22699590365312 run.py:483] Algo bellman_ford step 3171 current loss 0.555640, current_train_items 101504.
I0302 19:00:03.963490 22699590365312 run.py:483] Algo bellman_ford step 3172 current loss 0.608551, current_train_items 101536.
I0302 19:00:03.994637 22699590365312 run.py:483] Algo bellman_ford step 3173 current loss 0.728304, current_train_items 101568.
I0302 19:00:04.026127 22699590365312 run.py:483] Algo bellman_ford step 3174 current loss 0.797363, current_train_items 101600.
I0302 19:00:04.045884 22699590365312 run.py:483] Algo bellman_ford step 3175 current loss 0.269356, current_train_items 101632.
I0302 19:00:04.062213 22699590365312 run.py:483] Algo bellman_ford step 3176 current loss 0.471479, current_train_items 101664.
I0302 19:00:04.084942 22699590365312 run.py:483] Algo bellman_ford step 3177 current loss 0.572998, current_train_items 101696.
I0302 19:00:04.113730 22699590365312 run.py:483] Algo bellman_ford step 3178 current loss 0.639742, current_train_items 101728.
I0302 19:00:04.148279 22699590365312 run.py:483] Algo bellman_ford step 3179 current loss 1.038597, current_train_items 101760.
I0302 19:00:04.167593 22699590365312 run.py:483] Algo bellman_ford step 3180 current loss 0.373000, current_train_items 101792.
I0302 19:00:04.183891 22699590365312 run.py:483] Algo bellman_ford step 3181 current loss 0.432077, current_train_items 101824.
I0302 19:00:04.207550 22699590365312 run.py:483] Algo bellman_ford step 3182 current loss 0.669371, current_train_items 101856.
I0302 19:00:04.238590 22699590365312 run.py:483] Algo bellman_ford step 3183 current loss 0.739867, current_train_items 101888.
I0302 19:00:04.274770 22699590365312 run.py:483] Algo bellman_ford step 3184 current loss 0.862494, current_train_items 101920.
I0302 19:00:04.295102 22699590365312 run.py:483] Algo bellman_ford step 3185 current loss 0.359120, current_train_items 101952.
I0302 19:00:04.311326 22699590365312 run.py:483] Algo bellman_ford step 3186 current loss 0.566550, current_train_items 101984.
I0302 19:00:04.335418 22699590365312 run.py:483] Algo bellman_ford step 3187 current loss 0.728725, current_train_items 102016.
I0302 19:00:04.365965 22699590365312 run.py:483] Algo bellman_ford step 3188 current loss 0.733711, current_train_items 102048.
I0302 19:00:04.399341 22699590365312 run.py:483] Algo bellman_ford step 3189 current loss 0.786423, current_train_items 102080.
I0302 19:00:04.419434 22699590365312 run.py:483] Algo bellman_ford step 3190 current loss 0.338980, current_train_items 102112.
I0302 19:00:04.435847 22699590365312 run.py:483] Algo bellman_ford step 3191 current loss 0.424587, current_train_items 102144.
I0302 19:00:04.459403 22699590365312 run.py:483] Algo bellman_ford step 3192 current loss 0.763695, current_train_items 102176.
I0302 19:00:04.489849 22699590365312 run.py:483] Algo bellman_ford step 3193 current loss 0.615105, current_train_items 102208.
I0302 19:00:04.523592 22699590365312 run.py:483] Algo bellman_ford step 3194 current loss 0.787000, current_train_items 102240.
I0302 19:00:04.543372 22699590365312 run.py:483] Algo bellman_ford step 3195 current loss 0.351022, current_train_items 102272.
I0302 19:00:04.559175 22699590365312 run.py:483] Algo bellman_ford step 3196 current loss 0.403768, current_train_items 102304.
I0302 19:00:04.582622 22699590365312 run.py:483] Algo bellman_ford step 3197 current loss 0.710883, current_train_items 102336.
I0302 19:00:04.613710 22699590365312 run.py:483] Algo bellman_ford step 3198 current loss 0.775790, current_train_items 102368.
I0302 19:00:04.647515 22699590365312 run.py:483] Algo bellman_ford step 3199 current loss 0.858550, current_train_items 102400.
I0302 19:00:04.667699 22699590365312 run.py:483] Algo bellman_ford step 3200 current loss 0.320648, current_train_items 102432.
I0302 19:00:04.675723 22699590365312 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0302 19:00:04.675830 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:00:04.692258 22699590365312 run.py:483] Algo bellman_ford step 3201 current loss 0.392979, current_train_items 102464.
I0302 19:00:04.715751 22699590365312 run.py:483] Algo bellman_ford step 3202 current loss 0.635750, current_train_items 102496.
I0302 19:00:04.748195 22699590365312 run.py:483] Algo bellman_ford step 3203 current loss 0.941535, current_train_items 102528.
I0302 19:00:04.782344 22699590365312 run.py:483] Algo bellman_ford step 3204 current loss 0.884306, current_train_items 102560.
I0302 19:00:04.802832 22699590365312 run.py:483] Algo bellman_ford step 3205 current loss 0.333604, current_train_items 102592.
I0302 19:00:04.818403 22699590365312 run.py:483] Algo bellman_ford step 3206 current loss 0.532573, current_train_items 102624.
I0302 19:00:04.840628 22699590365312 run.py:483] Algo bellman_ford step 3207 current loss 0.522151, current_train_items 102656.
I0302 19:00:04.872258 22699590365312 run.py:483] Algo bellman_ford step 3208 current loss 0.678700, current_train_items 102688.
I0302 19:00:04.906308 22699590365312 run.py:483] Algo bellman_ford step 3209 current loss 0.879178, current_train_items 102720.
I0302 19:00:04.925971 22699590365312 run.py:483] Algo bellman_ford step 3210 current loss 0.338434, current_train_items 102752.
I0302 19:00:04.941851 22699590365312 run.py:483] Algo bellman_ford step 3211 current loss 0.381715, current_train_items 102784.
I0302 19:00:04.965241 22699590365312 run.py:483] Algo bellman_ford step 3212 current loss 0.622656, current_train_items 102816.
I0302 19:00:04.996219 22699590365312 run.py:483] Algo bellman_ford step 3213 current loss 0.787652, current_train_items 102848.
I0302 19:00:05.029933 22699590365312 run.py:483] Algo bellman_ford step 3214 current loss 0.741755, current_train_items 102880.
I0302 19:00:05.049633 22699590365312 run.py:483] Algo bellman_ford step 3215 current loss 0.299229, current_train_items 102912.
I0302 19:00:05.065665 22699590365312 run.py:483] Algo bellman_ford step 3216 current loss 0.504316, current_train_items 102944.
I0302 19:00:05.089707 22699590365312 run.py:483] Algo bellman_ford step 3217 current loss 0.613370, current_train_items 102976.
I0302 19:00:05.119376 22699590365312 run.py:483] Algo bellman_ford step 3218 current loss 0.768159, current_train_items 103008.
I0302 19:00:05.152160 22699590365312 run.py:483] Algo bellman_ford step 3219 current loss 0.763372, current_train_items 103040.
I0302 19:00:05.171760 22699590365312 run.py:483] Algo bellman_ford step 3220 current loss 0.262935, current_train_items 103072.
I0302 19:00:05.187946 22699590365312 run.py:483] Algo bellman_ford step 3221 current loss 0.524293, current_train_items 103104.
I0302 19:00:05.212057 22699590365312 run.py:483] Algo bellman_ford step 3222 current loss 0.723357, current_train_items 103136.
I0302 19:00:05.243196 22699590365312 run.py:483] Algo bellman_ford step 3223 current loss 0.800878, current_train_items 103168.
I0302 19:00:05.275014 22699590365312 run.py:483] Algo bellman_ford step 3224 current loss 0.707108, current_train_items 103200.
I0302 19:00:05.294598 22699590365312 run.py:483] Algo bellman_ford step 3225 current loss 0.319433, current_train_items 103232.
I0302 19:00:05.310087 22699590365312 run.py:483] Algo bellman_ford step 3226 current loss 0.383063, current_train_items 103264.
I0302 19:00:05.333677 22699590365312 run.py:483] Algo bellman_ford step 3227 current loss 0.730238, current_train_items 103296.
I0302 19:00:05.365015 22699590365312 run.py:483] Algo bellman_ford step 3228 current loss 1.089260, current_train_items 103328.
I0302 19:00:05.402310 22699590365312 run.py:483] Algo bellman_ford step 3229 current loss 1.622702, current_train_items 103360.
I0302 19:00:05.421801 22699590365312 run.py:483] Algo bellman_ford step 3230 current loss 0.296560, current_train_items 103392.
I0302 19:00:05.438070 22699590365312 run.py:483] Algo bellman_ford step 3231 current loss 0.490622, current_train_items 103424.
I0302 19:00:05.461182 22699590365312 run.py:483] Algo bellman_ford step 3232 current loss 0.624738, current_train_items 103456.
I0302 19:00:05.492085 22699590365312 run.py:483] Algo bellman_ford step 3233 current loss 0.736495, current_train_items 103488.
I0302 19:00:05.526956 22699590365312 run.py:483] Algo bellman_ford step 3234 current loss 0.825839, current_train_items 103520.
I0302 19:00:05.546368 22699590365312 run.py:483] Algo bellman_ford step 3235 current loss 0.351026, current_train_items 103552.
I0302 19:00:05.562803 22699590365312 run.py:483] Algo bellman_ford step 3236 current loss 0.532031, current_train_items 103584.
I0302 19:00:05.585573 22699590365312 run.py:483] Algo bellman_ford step 3237 current loss 0.672433, current_train_items 103616.
I0302 19:00:05.617478 22699590365312 run.py:483] Algo bellman_ford step 3238 current loss 0.779848, current_train_items 103648.
I0302 19:00:05.650899 22699590365312 run.py:483] Algo bellman_ford step 3239 current loss 0.820856, current_train_items 103680.
I0302 19:00:05.670379 22699590365312 run.py:483] Algo bellman_ford step 3240 current loss 0.307959, current_train_items 103712.
I0302 19:00:05.686637 22699590365312 run.py:483] Algo bellman_ford step 3241 current loss 0.456653, current_train_items 103744.
I0302 19:00:05.709790 22699590365312 run.py:483] Algo bellman_ford step 3242 current loss 0.684994, current_train_items 103776.
I0302 19:00:05.739742 22699590365312 run.py:483] Algo bellman_ford step 3243 current loss 0.554278, current_train_items 103808.
I0302 19:00:05.774262 22699590365312 run.py:483] Algo bellman_ford step 3244 current loss 0.867648, current_train_items 103840.
I0302 19:00:05.793602 22699590365312 run.py:483] Algo bellman_ford step 3245 current loss 0.251318, current_train_items 103872.
I0302 19:00:05.809903 22699590365312 run.py:483] Algo bellman_ford step 3246 current loss 0.477609, current_train_items 103904.
I0302 19:00:05.833173 22699590365312 run.py:483] Algo bellman_ford step 3247 current loss 0.619508, current_train_items 103936.
I0302 19:00:05.864724 22699590365312 run.py:483] Algo bellman_ford step 3248 current loss 0.650837, current_train_items 103968.
I0302 19:00:05.896987 22699590365312 run.py:483] Algo bellman_ford step 3249 current loss 0.814030, current_train_items 104000.
I0302 19:00:05.916886 22699590365312 run.py:483] Algo bellman_ford step 3250 current loss 0.305565, current_train_items 104032.
I0302 19:00:05.925309 22699590365312 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0302 19:00:05.925412 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:00:05.942093 22699590365312 run.py:483] Algo bellman_ford step 3251 current loss 0.499640, current_train_items 104064.
I0302 19:00:05.964223 22699590365312 run.py:483] Algo bellman_ford step 3252 current loss 0.614181, current_train_items 104096.
I0302 19:00:05.996516 22699590365312 run.py:483] Algo bellman_ford step 3253 current loss 0.830330, current_train_items 104128.
I0302 19:00:06.030467 22699590365312 run.py:483] Algo bellman_ford step 3254 current loss 0.806029, current_train_items 104160.
I0302 19:00:06.050497 22699590365312 run.py:483] Algo bellman_ford step 3255 current loss 0.385011, current_train_items 104192.
I0302 19:00:06.066178 22699590365312 run.py:483] Algo bellman_ford step 3256 current loss 0.452539, current_train_items 104224.
I0302 19:00:06.088821 22699590365312 run.py:483] Algo bellman_ford step 3257 current loss 0.643305, current_train_items 104256.
I0302 19:00:06.120698 22699590365312 run.py:483] Algo bellman_ford step 3258 current loss 0.745766, current_train_items 104288.
I0302 19:00:06.155540 22699590365312 run.py:483] Algo bellman_ford step 3259 current loss 1.045235, current_train_items 104320.
I0302 19:00:06.175119 22699590365312 run.py:483] Algo bellman_ford step 3260 current loss 0.392372, current_train_items 104352.
I0302 19:00:06.191267 22699590365312 run.py:483] Algo bellman_ford step 3261 current loss 0.476315, current_train_items 104384.
I0302 19:00:06.213787 22699590365312 run.py:483] Algo bellman_ford step 3262 current loss 0.588929, current_train_items 104416.
I0302 19:00:06.244613 22699590365312 run.py:483] Algo bellman_ford step 3263 current loss 0.783722, current_train_items 104448.
I0302 19:00:06.278627 22699590365312 run.py:483] Algo bellman_ford step 3264 current loss 0.833522, current_train_items 104480.
I0302 19:00:06.298101 22699590365312 run.py:483] Algo bellman_ford step 3265 current loss 0.414674, current_train_items 104512.
I0302 19:00:06.314432 22699590365312 run.py:483] Algo bellman_ford step 3266 current loss 0.506127, current_train_items 104544.
I0302 19:00:06.337433 22699590365312 run.py:483] Algo bellman_ford step 3267 current loss 0.598505, current_train_items 104576.
I0302 19:00:06.366312 22699590365312 run.py:483] Algo bellman_ford step 3268 current loss 0.680192, current_train_items 104608.
I0302 19:00:06.402151 22699590365312 run.py:483] Algo bellman_ford step 3269 current loss 1.114451, current_train_items 104640.
I0302 19:00:06.422051 22699590365312 run.py:483] Algo bellman_ford step 3270 current loss 0.321315, current_train_items 104672.
I0302 19:00:06.438410 22699590365312 run.py:483] Algo bellman_ford step 3271 current loss 0.497120, current_train_items 104704.
I0302 19:00:06.461390 22699590365312 run.py:483] Algo bellman_ford step 3272 current loss 0.665066, current_train_items 104736.
I0302 19:00:06.490828 22699590365312 run.py:483] Algo bellman_ford step 3273 current loss 0.684777, current_train_items 104768.
I0302 19:00:06.522257 22699590365312 run.py:483] Algo bellman_ford step 3274 current loss 0.685812, current_train_items 104800.
I0302 19:00:06.542127 22699590365312 run.py:483] Algo bellman_ford step 3275 current loss 0.323305, current_train_items 104832.
I0302 19:00:06.558172 22699590365312 run.py:483] Algo bellman_ford step 3276 current loss 0.426049, current_train_items 104864.
I0302 19:00:06.580916 22699590365312 run.py:483] Algo bellman_ford step 3277 current loss 0.695201, current_train_items 104896.
I0302 19:00:06.612339 22699590365312 run.py:483] Algo bellman_ford step 3278 current loss 0.774037, current_train_items 104928.
I0302 19:00:06.646473 22699590365312 run.py:483] Algo bellman_ford step 3279 current loss 0.824166, current_train_items 104960.
I0302 19:00:06.666113 22699590365312 run.py:483] Algo bellman_ford step 3280 current loss 0.336408, current_train_items 104992.
I0302 19:00:06.682077 22699590365312 run.py:483] Algo bellman_ford step 3281 current loss 0.562211, current_train_items 105024.
I0302 19:00:06.704728 22699590365312 run.py:483] Algo bellman_ford step 3282 current loss 0.819519, current_train_items 105056.
I0302 19:00:06.735760 22699590365312 run.py:483] Algo bellman_ford step 3283 current loss 0.948174, current_train_items 105088.
I0302 19:00:06.769640 22699590365312 run.py:483] Algo bellman_ford step 3284 current loss 0.908733, current_train_items 105120.
I0302 19:00:06.789371 22699590365312 run.py:483] Algo bellman_ford step 3285 current loss 0.321723, current_train_items 105152.
I0302 19:00:06.805869 22699590365312 run.py:483] Algo bellman_ford step 3286 current loss 0.525975, current_train_items 105184.
I0302 19:00:06.829211 22699590365312 run.py:483] Algo bellman_ford step 3287 current loss 0.689997, current_train_items 105216.
I0302 19:00:06.858688 22699590365312 run.py:483] Algo bellman_ford step 3288 current loss 0.632559, current_train_items 105248.
I0302 19:00:06.890309 22699590365312 run.py:483] Algo bellman_ford step 3289 current loss 0.781847, current_train_items 105280.
I0302 19:00:06.909999 22699590365312 run.py:483] Algo bellman_ford step 3290 current loss 0.376430, current_train_items 105312.
I0302 19:00:06.926673 22699590365312 run.py:483] Algo bellman_ford step 3291 current loss 0.464333, current_train_items 105344.
I0302 19:00:06.949635 22699590365312 run.py:483] Algo bellman_ford step 3292 current loss 0.685953, current_train_items 105376.
I0302 19:00:06.980726 22699590365312 run.py:483] Algo bellman_ford step 3293 current loss 0.803000, current_train_items 105408.
I0302 19:00:07.014044 22699590365312 run.py:483] Algo bellman_ford step 3294 current loss 0.839771, current_train_items 105440.
I0302 19:00:07.033558 22699590365312 run.py:483] Algo bellman_ford step 3295 current loss 0.340571, current_train_items 105472.
I0302 19:00:07.049299 22699590365312 run.py:483] Algo bellman_ford step 3296 current loss 0.494902, current_train_items 105504.
I0302 19:00:07.073227 22699590365312 run.py:483] Algo bellman_ford step 3297 current loss 0.857813, current_train_items 105536.
I0302 19:00:07.105244 22699590365312 run.py:483] Algo bellman_ford step 3298 current loss 0.842761, current_train_items 105568.
I0302 19:00:07.139878 22699590365312 run.py:483] Algo bellman_ford step 3299 current loss 0.964239, current_train_items 105600.
I0302 19:00:07.159460 22699590365312 run.py:483] Algo bellman_ford step 3300 current loss 0.360368, current_train_items 105632.
I0302 19:00:07.167388 22699590365312 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.869140625, 'score': 0.869140625, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0302 19:00:07.167494 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.869, val scores are: bellman_ford: 0.869
I0302 19:00:07.184213 22699590365312 run.py:483] Algo bellman_ford step 3301 current loss 0.543013, current_train_items 105664.
I0302 19:00:07.207756 22699590365312 run.py:483] Algo bellman_ford step 3302 current loss 0.742548, current_train_items 105696.
I0302 19:00:07.237581 22699590365312 run.py:483] Algo bellman_ford step 3303 current loss 0.666958, current_train_items 105728.
I0302 19:00:07.269695 22699590365312 run.py:483] Algo bellman_ford step 3304 current loss 0.767057, current_train_items 105760.
I0302 19:00:07.289650 22699590365312 run.py:483] Algo bellman_ford step 3305 current loss 0.232801, current_train_items 105792.
I0302 19:00:07.305769 22699590365312 run.py:483] Algo bellman_ford step 3306 current loss 0.592651, current_train_items 105824.
I0302 19:00:07.329790 22699590365312 run.py:483] Algo bellman_ford step 3307 current loss 0.726792, current_train_items 105856.
I0302 19:00:07.360790 22699590365312 run.py:483] Algo bellman_ford step 3308 current loss 0.726955, current_train_items 105888.
I0302 19:00:07.396256 22699590365312 run.py:483] Algo bellman_ford step 3309 current loss 0.929085, current_train_items 105920.
I0302 19:00:07.415826 22699590365312 run.py:483] Algo bellman_ford step 3310 current loss 0.319949, current_train_items 105952.
I0302 19:00:07.432223 22699590365312 run.py:483] Algo bellman_ford step 3311 current loss 0.480185, current_train_items 105984.
I0302 19:00:07.455705 22699590365312 run.py:483] Algo bellman_ford step 3312 current loss 0.657548, current_train_items 106016.
I0302 19:00:07.486968 22699590365312 run.py:483] Algo bellman_ford step 3313 current loss 0.730364, current_train_items 106048.
I0302 19:00:07.519509 22699590365312 run.py:483] Algo bellman_ford step 3314 current loss 0.848703, current_train_items 106080.
I0302 19:00:07.538951 22699590365312 run.py:483] Algo bellman_ford step 3315 current loss 0.338704, current_train_items 106112.
I0302 19:00:07.555517 22699590365312 run.py:483] Algo bellman_ford step 3316 current loss 0.455449, current_train_items 106144.
I0302 19:00:07.578437 22699590365312 run.py:483] Algo bellman_ford step 3317 current loss 0.645050, current_train_items 106176.
I0302 19:00:07.608838 22699590365312 run.py:483] Algo bellman_ford step 3318 current loss 0.741045, current_train_items 106208.
I0302 19:00:07.642863 22699590365312 run.py:483] Algo bellman_ford step 3319 current loss 0.918517, current_train_items 106240.
I0302 19:00:07.662469 22699590365312 run.py:483] Algo bellman_ford step 3320 current loss 0.302975, current_train_items 106272.
I0302 19:00:07.678628 22699590365312 run.py:483] Algo bellman_ford step 3321 current loss 0.580625, current_train_items 106304.
I0302 19:00:07.702909 22699590365312 run.py:483] Algo bellman_ford step 3322 current loss 0.699702, current_train_items 106336.
I0302 19:00:07.734066 22699590365312 run.py:483] Algo bellman_ford step 3323 current loss 0.684860, current_train_items 106368.
I0302 19:00:07.766820 22699590365312 run.py:483] Algo bellman_ford step 3324 current loss 0.904720, current_train_items 106400.
I0302 19:00:07.786252 22699590365312 run.py:483] Algo bellman_ford step 3325 current loss 0.343456, current_train_items 106432.
I0302 19:00:07.802533 22699590365312 run.py:483] Algo bellman_ford step 3326 current loss 0.467613, current_train_items 106464.
I0302 19:00:07.825689 22699590365312 run.py:483] Algo bellman_ford step 3327 current loss 0.613892, current_train_items 106496.
I0302 19:00:07.856089 22699590365312 run.py:483] Algo bellman_ford step 3328 current loss 0.684347, current_train_items 106528.
I0302 19:00:07.889112 22699590365312 run.py:483] Algo bellman_ford step 3329 current loss 0.755990, current_train_items 106560.
I0302 19:00:07.908839 22699590365312 run.py:483] Algo bellman_ford step 3330 current loss 0.408337, current_train_items 106592.
I0302 19:00:07.924878 22699590365312 run.py:483] Algo bellman_ford step 3331 current loss 0.451574, current_train_items 106624.
I0302 19:00:07.948028 22699590365312 run.py:483] Algo bellman_ford step 3332 current loss 0.613648, current_train_items 106656.
I0302 19:00:07.978218 22699590365312 run.py:483] Algo bellman_ford step 3333 current loss 0.622484, current_train_items 106688.
I0302 19:00:08.010641 22699590365312 run.py:483] Algo bellman_ford step 3334 current loss 0.681579, current_train_items 106720.
I0302 19:00:08.029859 22699590365312 run.py:483] Algo bellman_ford step 3335 current loss 0.326087, current_train_items 106752.
I0302 19:00:08.045624 22699590365312 run.py:483] Algo bellman_ford step 3336 current loss 0.568159, current_train_items 106784.
I0302 19:00:08.069423 22699590365312 run.py:483] Algo bellman_ford step 3337 current loss 0.672802, current_train_items 106816.
I0302 19:00:08.101654 22699590365312 run.py:483] Algo bellman_ford step 3338 current loss 0.646304, current_train_items 106848.
I0302 19:00:08.136662 22699590365312 run.py:483] Algo bellman_ford step 3339 current loss 0.768933, current_train_items 106880.
I0302 19:00:08.155951 22699590365312 run.py:483] Algo bellman_ford step 3340 current loss 0.288092, current_train_items 106912.
I0302 19:00:08.172549 22699590365312 run.py:483] Algo bellman_ford step 3341 current loss 0.463442, current_train_items 106944.
I0302 19:00:08.196268 22699590365312 run.py:483] Algo bellman_ford step 3342 current loss 0.695553, current_train_items 106976.
I0302 19:00:08.228261 22699590365312 run.py:483] Algo bellman_ford step 3343 current loss 0.812039, current_train_items 107008.
I0302 19:00:08.259753 22699590365312 run.py:483] Algo bellman_ford step 3344 current loss 0.756867, current_train_items 107040.
I0302 19:00:08.279559 22699590365312 run.py:483] Algo bellman_ford step 3345 current loss 0.303041, current_train_items 107072.
I0302 19:00:08.295454 22699590365312 run.py:483] Algo bellman_ford step 3346 current loss 0.454023, current_train_items 107104.
I0302 19:00:08.319016 22699590365312 run.py:483] Algo bellman_ford step 3347 current loss 0.590160, current_train_items 107136.
I0302 19:00:08.349384 22699590365312 run.py:483] Algo bellman_ford step 3348 current loss 0.628007, current_train_items 107168.
I0302 19:00:08.380806 22699590365312 run.py:483] Algo bellman_ford step 3349 current loss 0.720497, current_train_items 107200.
I0302 19:00:08.400815 22699590365312 run.py:483] Algo bellman_ford step 3350 current loss 0.368783, current_train_items 107232.
I0302 19:00:08.409087 22699590365312 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0302 19:00:08.409202 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 19:00:08.426454 22699590365312 run.py:483] Algo bellman_ford step 3351 current loss 0.517109, current_train_items 107264.
I0302 19:00:08.450984 22699590365312 run.py:483] Algo bellman_ford step 3352 current loss 0.780269, current_train_items 107296.
I0302 19:00:08.482888 22699590365312 run.py:483] Algo bellman_ford step 3353 current loss 0.655893, current_train_items 107328.
I0302 19:00:08.517346 22699590365312 run.py:483] Algo bellman_ford step 3354 current loss 0.902179, current_train_items 107360.
I0302 19:00:08.537177 22699590365312 run.py:483] Algo bellman_ford step 3355 current loss 0.294655, current_train_items 107392.
I0302 19:00:08.552995 22699590365312 run.py:483] Algo bellman_ford step 3356 current loss 0.482941, current_train_items 107424.
I0302 19:00:08.576594 22699590365312 run.py:483] Algo bellman_ford step 3357 current loss 0.648136, current_train_items 107456.
I0302 19:00:08.607195 22699590365312 run.py:483] Algo bellman_ford step 3358 current loss 0.826769, current_train_items 107488.
I0302 19:00:08.642312 22699590365312 run.py:483] Algo bellman_ford step 3359 current loss 0.855314, current_train_items 107520.
I0302 19:00:08.662262 22699590365312 run.py:483] Algo bellman_ford step 3360 current loss 0.280706, current_train_items 107552.
I0302 19:00:08.678283 22699590365312 run.py:483] Algo bellman_ford step 3361 current loss 0.503409, current_train_items 107584.
I0302 19:00:08.702287 22699590365312 run.py:483] Algo bellman_ford step 3362 current loss 0.637476, current_train_items 107616.
I0302 19:00:08.732977 22699590365312 run.py:483] Algo bellman_ford step 3363 current loss 0.608867, current_train_items 107648.
I0302 19:00:08.765487 22699590365312 run.py:483] Algo bellman_ford step 3364 current loss 0.859390, current_train_items 107680.
I0302 19:00:08.785189 22699590365312 run.py:483] Algo bellman_ford step 3365 current loss 0.371121, current_train_items 107712.
I0302 19:00:08.801022 22699590365312 run.py:483] Algo bellman_ford step 3366 current loss 0.477894, current_train_items 107744.
I0302 19:00:08.825055 22699590365312 run.py:483] Algo bellman_ford step 3367 current loss 0.697893, current_train_items 107776.
I0302 19:00:08.854279 22699590365312 run.py:483] Algo bellman_ford step 3368 current loss 0.728550, current_train_items 107808.
I0302 19:00:08.886660 22699590365312 run.py:483] Algo bellman_ford step 3369 current loss 0.725669, current_train_items 107840.
I0302 19:00:08.906795 22699590365312 run.py:483] Algo bellman_ford step 3370 current loss 0.359072, current_train_items 107872.
I0302 19:00:08.923126 22699590365312 run.py:483] Algo bellman_ford step 3371 current loss 0.465272, current_train_items 107904.
I0302 19:00:08.946498 22699590365312 run.py:483] Algo bellman_ford step 3372 current loss 0.647867, current_train_items 107936.
I0302 19:00:08.978511 22699590365312 run.py:483] Algo bellman_ford step 3373 current loss 0.836544, current_train_items 107968.
I0302 19:00:09.011207 22699590365312 run.py:483] Algo bellman_ford step 3374 current loss 0.832126, current_train_items 108000.
I0302 19:00:09.031091 22699590365312 run.py:483] Algo bellman_ford step 3375 current loss 0.314687, current_train_items 108032.
I0302 19:00:09.046946 22699590365312 run.py:483] Algo bellman_ford step 3376 current loss 0.470007, current_train_items 108064.
I0302 19:00:09.069801 22699590365312 run.py:483] Algo bellman_ford step 3377 current loss 0.677833, current_train_items 108096.
I0302 19:00:09.099518 22699590365312 run.py:483] Algo bellman_ford step 3378 current loss 0.659262, current_train_items 108128.
I0302 19:00:09.133989 22699590365312 run.py:483] Algo bellman_ford step 3379 current loss 0.830868, current_train_items 108160.
I0302 19:00:09.153796 22699590365312 run.py:483] Algo bellman_ford step 3380 current loss 0.298154, current_train_items 108192.
I0302 19:00:09.170273 22699590365312 run.py:483] Algo bellman_ford step 3381 current loss 0.558778, current_train_items 108224.
I0302 19:00:09.194896 22699590365312 run.py:483] Algo bellman_ford step 3382 current loss 0.639651, current_train_items 108256.
I0302 19:00:09.224699 22699590365312 run.py:483] Algo bellman_ford step 3383 current loss 0.730800, current_train_items 108288.
I0302 19:00:09.259548 22699590365312 run.py:483] Algo bellman_ford step 3384 current loss 1.065805, current_train_items 108320.
I0302 19:00:09.279659 22699590365312 run.py:483] Algo bellman_ford step 3385 current loss 0.342498, current_train_items 108352.
I0302 19:00:09.296293 22699590365312 run.py:483] Algo bellman_ford step 3386 current loss 0.579987, current_train_items 108384.
I0302 19:00:09.319305 22699590365312 run.py:483] Algo bellman_ford step 3387 current loss 0.648165, current_train_items 108416.
I0302 19:00:09.349442 22699590365312 run.py:483] Algo bellman_ford step 3388 current loss 0.911359, current_train_items 108448.
I0302 19:00:09.381231 22699590365312 run.py:483] Algo bellman_ford step 3389 current loss 0.941494, current_train_items 108480.
I0302 19:00:09.400944 22699590365312 run.py:483] Algo bellman_ford step 3390 current loss 0.344124, current_train_items 108512.
I0302 19:00:09.417110 22699590365312 run.py:483] Algo bellman_ford step 3391 current loss 0.499164, current_train_items 108544.
I0302 19:00:09.440397 22699590365312 run.py:483] Algo bellman_ford step 3392 current loss 0.658258, current_train_items 108576.
I0302 19:00:09.470370 22699590365312 run.py:483] Algo bellman_ford step 3393 current loss 0.622381, current_train_items 108608.
I0302 19:00:09.502075 22699590365312 run.py:483] Algo bellman_ford step 3394 current loss 0.774929, current_train_items 108640.
I0302 19:00:09.521778 22699590365312 run.py:483] Algo bellman_ford step 3395 current loss 0.365386, current_train_items 108672.
I0302 19:00:09.538050 22699590365312 run.py:483] Algo bellman_ford step 3396 current loss 0.461981, current_train_items 108704.
I0302 19:00:09.561510 22699590365312 run.py:483] Algo bellman_ford step 3397 current loss 0.607163, current_train_items 108736.
I0302 19:00:09.592520 22699590365312 run.py:483] Algo bellman_ford step 3398 current loss 0.795894, current_train_items 108768.
I0302 19:00:09.624940 22699590365312 run.py:483] Algo bellman_ford step 3399 current loss 0.703468, current_train_items 108800.
I0302 19:00:09.645068 22699590365312 run.py:483] Algo bellman_ford step 3400 current loss 0.330634, current_train_items 108832.
I0302 19:00:09.652911 22699590365312 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0302 19:00:09.653019 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:00:09.670208 22699590365312 run.py:483] Algo bellman_ford step 3401 current loss 0.560021, current_train_items 108864.
I0302 19:00:09.693710 22699590365312 run.py:483] Algo bellman_ford step 3402 current loss 0.576787, current_train_items 108896.
I0302 19:00:09.724843 22699590365312 run.py:483] Algo bellman_ford step 3403 current loss 0.727781, current_train_items 108928.
I0302 19:00:09.759732 22699590365312 run.py:483] Algo bellman_ford step 3404 current loss 0.921818, current_train_items 108960.
I0302 19:00:09.779529 22699590365312 run.py:483] Algo bellman_ford step 3405 current loss 0.279957, current_train_items 108992.
I0302 19:00:09.794983 22699590365312 run.py:483] Algo bellman_ford step 3406 current loss 0.530970, current_train_items 109024.
I0302 19:00:09.818463 22699590365312 run.py:483] Algo bellman_ford step 3407 current loss 0.653576, current_train_items 109056.
I0302 19:00:09.850633 22699590365312 run.py:483] Algo bellman_ford step 3408 current loss 0.679081, current_train_items 109088.
I0302 19:00:09.883087 22699590365312 run.py:483] Algo bellman_ford step 3409 current loss 0.718093, current_train_items 109120.
I0302 19:00:09.902209 22699590365312 run.py:483] Algo bellman_ford step 3410 current loss 0.304427, current_train_items 109152.
I0302 19:00:09.918565 22699590365312 run.py:483] Algo bellman_ford step 3411 current loss 0.551604, current_train_items 109184.
I0302 19:00:09.942633 22699590365312 run.py:483] Algo bellman_ford step 3412 current loss 0.576732, current_train_items 109216.
I0302 19:00:09.974896 22699590365312 run.py:483] Algo bellman_ford step 3413 current loss 0.786815, current_train_items 109248.
I0302 19:00:10.011150 22699590365312 run.py:483] Algo bellman_ford step 3414 current loss 0.845329, current_train_items 109280.
I0302 19:00:10.030446 22699590365312 run.py:483] Algo bellman_ford step 3415 current loss 0.313513, current_train_items 109312.
I0302 19:00:10.046688 22699590365312 run.py:483] Algo bellman_ford step 3416 current loss 0.522729, current_train_items 109344.
I0302 19:00:10.070370 22699590365312 run.py:483] Algo bellman_ford step 3417 current loss 0.672284, current_train_items 109376.
I0302 19:00:10.101113 22699590365312 run.py:483] Algo bellman_ford step 3418 current loss 0.868251, current_train_items 109408.
I0302 19:00:10.133751 22699590365312 run.py:483] Algo bellman_ford step 3419 current loss 0.734841, current_train_items 109440.
I0302 19:00:10.153246 22699590365312 run.py:483] Algo bellman_ford step 3420 current loss 0.334949, current_train_items 109472.
I0302 19:00:10.169140 22699590365312 run.py:483] Algo bellman_ford step 3421 current loss 0.493417, current_train_items 109504.
I0302 19:00:10.192561 22699590365312 run.py:483] Algo bellman_ford step 3422 current loss 0.724709, current_train_items 109536.
I0302 19:00:10.223396 22699590365312 run.py:483] Algo bellman_ford step 3423 current loss 0.691565, current_train_items 109568.
I0302 19:00:10.256233 22699590365312 run.py:483] Algo bellman_ford step 3424 current loss 0.854173, current_train_items 109600.
I0302 19:00:10.275535 22699590365312 run.py:483] Algo bellman_ford step 3425 current loss 0.314971, current_train_items 109632.
I0302 19:00:10.292105 22699590365312 run.py:483] Algo bellman_ford step 3426 current loss 0.508666, current_train_items 109664.
I0302 19:00:10.315040 22699590365312 run.py:483] Algo bellman_ford step 3427 current loss 0.602723, current_train_items 109696.
I0302 19:00:10.345761 22699590365312 run.py:483] Algo bellman_ford step 3428 current loss 0.720087, current_train_items 109728.
I0302 19:00:10.376329 22699590365312 run.py:483] Algo bellman_ford step 3429 current loss 0.724624, current_train_items 109760.
I0302 19:00:10.395822 22699590365312 run.py:483] Algo bellman_ford step 3430 current loss 0.303653, current_train_items 109792.
I0302 19:00:10.412332 22699590365312 run.py:483] Algo bellman_ford step 3431 current loss 0.501825, current_train_items 109824.
I0302 19:00:10.435100 22699590365312 run.py:483] Algo bellman_ford step 3432 current loss 0.565388, current_train_items 109856.
I0302 19:00:10.467597 22699590365312 run.py:483] Algo bellman_ford step 3433 current loss 0.877580, current_train_items 109888.
I0302 19:00:10.499717 22699590365312 run.py:483] Algo bellman_ford step 3434 current loss 0.757127, current_train_items 109920.
I0302 19:00:10.519127 22699590365312 run.py:483] Algo bellman_ford step 3435 current loss 0.344973, current_train_items 109952.
I0302 19:00:10.535230 22699590365312 run.py:483] Algo bellman_ford step 3436 current loss 0.462791, current_train_items 109984.
I0302 19:00:10.558609 22699590365312 run.py:483] Algo bellman_ford step 3437 current loss 0.776978, current_train_items 110016.
I0302 19:00:10.588208 22699590365312 run.py:483] Algo bellman_ford step 3438 current loss 0.834133, current_train_items 110048.
I0302 19:00:10.619527 22699590365312 run.py:483] Algo bellman_ford step 3439 current loss 0.859740, current_train_items 110080.
I0302 19:00:10.638751 22699590365312 run.py:483] Algo bellman_ford step 3440 current loss 0.341548, current_train_items 110112.
I0302 19:00:10.655244 22699590365312 run.py:483] Algo bellman_ford step 3441 current loss 0.526684, current_train_items 110144.
I0302 19:00:10.678004 22699590365312 run.py:483] Algo bellman_ford step 3442 current loss 0.653637, current_train_items 110176.
I0302 19:00:10.709353 22699590365312 run.py:483] Algo bellman_ford step 3443 current loss 0.874940, current_train_items 110208.
I0302 19:00:10.743444 22699590365312 run.py:483] Algo bellman_ford step 3444 current loss 0.825743, current_train_items 110240.
I0302 19:00:10.762828 22699590365312 run.py:483] Algo bellman_ford step 3445 current loss 0.318305, current_train_items 110272.
I0302 19:00:10.779334 22699590365312 run.py:483] Algo bellman_ford step 3446 current loss 0.537940, current_train_items 110304.
I0302 19:00:10.802793 22699590365312 run.py:483] Algo bellman_ford step 3447 current loss 0.614771, current_train_items 110336.
I0302 19:00:10.833160 22699590365312 run.py:483] Algo bellman_ford step 3448 current loss 0.710270, current_train_items 110368.
I0302 19:00:10.866611 22699590365312 run.py:483] Algo bellman_ford step 3449 current loss 1.127799, current_train_items 110400.
I0302 19:00:10.885949 22699590365312 run.py:483] Algo bellman_ford step 3450 current loss 0.347137, current_train_items 110432.
I0302 19:00:10.894043 22699590365312 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0302 19:00:10.894148 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 19:00:10.910922 22699590365312 run.py:483] Algo bellman_ford step 3451 current loss 0.613660, current_train_items 110464.
I0302 19:00:10.935256 22699590365312 run.py:483] Algo bellman_ford step 3452 current loss 0.707545, current_train_items 110496.
I0302 19:00:10.965291 22699590365312 run.py:483] Algo bellman_ford step 3453 current loss 0.785128, current_train_items 110528.
I0302 19:00:11.001262 22699590365312 run.py:483] Algo bellman_ford step 3454 current loss 0.917889, current_train_items 110560.
I0302 19:00:11.021508 22699590365312 run.py:483] Algo bellman_ford step 3455 current loss 0.277050, current_train_items 110592.
I0302 19:00:11.037116 22699590365312 run.py:483] Algo bellman_ford step 3456 current loss 0.450846, current_train_items 110624.
I0302 19:00:11.060517 22699590365312 run.py:483] Algo bellman_ford step 3457 current loss 0.643508, current_train_items 110656.
I0302 19:00:11.092374 22699590365312 run.py:483] Algo bellman_ford step 3458 current loss 0.711729, current_train_items 110688.
I0302 19:00:11.124890 22699590365312 run.py:483] Algo bellman_ford step 3459 current loss 0.790299, current_train_items 110720.
I0302 19:00:11.144883 22699590365312 run.py:483] Algo bellman_ford step 3460 current loss 0.287652, current_train_items 110752.
I0302 19:00:11.161365 22699590365312 run.py:483] Algo bellman_ford step 3461 current loss 0.549046, current_train_items 110784.
I0302 19:00:11.184592 22699590365312 run.py:483] Algo bellman_ford step 3462 current loss 0.652510, current_train_items 110816.
I0302 19:00:11.214903 22699590365312 run.py:483] Algo bellman_ford step 3463 current loss 0.732342, current_train_items 110848.
I0302 19:00:11.247376 22699590365312 run.py:483] Algo bellman_ford step 3464 current loss 0.865768, current_train_items 110880.
I0302 19:00:11.267142 22699590365312 run.py:483] Algo bellman_ford step 3465 current loss 0.304351, current_train_items 110912.
I0302 19:00:11.283629 22699590365312 run.py:483] Algo bellman_ford step 3466 current loss 0.569995, current_train_items 110944.
I0302 19:00:11.307715 22699590365312 run.py:483] Algo bellman_ford step 3467 current loss 0.753398, current_train_items 110976.
I0302 19:00:11.339715 22699590365312 run.py:483] Algo bellman_ford step 3468 current loss 0.869213, current_train_items 111008.
I0302 19:00:11.375178 22699590365312 run.py:483] Algo bellman_ford step 3469 current loss 0.868924, current_train_items 111040.
I0302 19:00:11.395088 22699590365312 run.py:483] Algo bellman_ford step 3470 current loss 0.306216, current_train_items 111072.
I0302 19:00:11.411837 22699590365312 run.py:483] Algo bellman_ford step 3471 current loss 0.485632, current_train_items 111104.
I0302 19:00:11.434960 22699590365312 run.py:483] Algo bellman_ford step 3472 current loss 0.607910, current_train_items 111136.
I0302 19:00:11.466865 22699590365312 run.py:483] Algo bellman_ford step 3473 current loss 0.678925, current_train_items 111168.
I0302 19:00:11.500190 22699590365312 run.py:483] Algo bellman_ford step 3474 current loss 0.814576, current_train_items 111200.
I0302 19:00:11.519968 22699590365312 run.py:483] Algo bellman_ford step 3475 current loss 0.291643, current_train_items 111232.
I0302 19:00:11.536051 22699590365312 run.py:483] Algo bellman_ford step 3476 current loss 0.404332, current_train_items 111264.
I0302 19:00:11.558762 22699590365312 run.py:483] Algo bellman_ford step 3477 current loss 0.554037, current_train_items 111296.
I0302 19:00:11.588970 22699590365312 run.py:483] Algo bellman_ford step 3478 current loss 0.537413, current_train_items 111328.
I0302 19:00:11.623555 22699590365312 run.py:483] Algo bellman_ford step 3479 current loss 0.981017, current_train_items 111360.
I0302 19:00:11.643010 22699590365312 run.py:483] Algo bellman_ford step 3480 current loss 0.288654, current_train_items 111392.
I0302 19:00:11.659019 22699590365312 run.py:483] Algo bellman_ford step 3481 current loss 0.533693, current_train_items 111424.
I0302 19:00:11.682264 22699590365312 run.py:483] Algo bellman_ford step 3482 current loss 0.599353, current_train_items 111456.
I0302 19:00:11.712285 22699590365312 run.py:483] Algo bellman_ford step 3483 current loss 0.634107, current_train_items 111488.
I0302 19:00:11.745161 22699590365312 run.py:483] Algo bellman_ford step 3484 current loss 0.817613, current_train_items 111520.
I0302 19:00:11.765112 22699590365312 run.py:483] Algo bellman_ford step 3485 current loss 0.301582, current_train_items 111552.
I0302 19:00:11.781400 22699590365312 run.py:483] Algo bellman_ford step 3486 current loss 0.463327, current_train_items 111584.
I0302 19:00:11.805266 22699590365312 run.py:483] Algo bellman_ford step 3487 current loss 0.603179, current_train_items 111616.
I0302 19:00:11.835345 22699590365312 run.py:483] Algo bellman_ford step 3488 current loss 0.628417, current_train_items 111648.
I0302 19:00:11.867182 22699590365312 run.py:483] Algo bellman_ford step 3489 current loss 0.728187, current_train_items 111680.
I0302 19:00:11.887371 22699590365312 run.py:483] Algo bellman_ford step 3490 current loss 0.315690, current_train_items 111712.
I0302 19:00:11.903227 22699590365312 run.py:483] Algo bellman_ford step 3491 current loss 0.419205, current_train_items 111744.
I0302 19:00:11.926177 22699590365312 run.py:483] Algo bellman_ford step 3492 current loss 0.615466, current_train_items 111776.
I0302 19:00:11.957681 22699590365312 run.py:483] Algo bellman_ford step 3493 current loss 0.818241, current_train_items 111808.
I0302 19:00:11.990560 22699590365312 run.py:483] Algo bellman_ford step 3494 current loss 0.678281, current_train_items 111840.
I0302 19:00:12.010170 22699590365312 run.py:483] Algo bellman_ford step 3495 current loss 0.298593, current_train_items 111872.
I0302 19:00:12.026341 22699590365312 run.py:483] Algo bellman_ford step 3496 current loss 0.502507, current_train_items 111904.
I0302 19:00:12.049585 22699590365312 run.py:483] Algo bellman_ford step 3497 current loss 0.721691, current_train_items 111936.
I0302 19:00:12.080783 22699590365312 run.py:483] Algo bellman_ford step 3498 current loss 0.718894, current_train_items 111968.
I0302 19:00:12.115039 22699590365312 run.py:483] Algo bellman_ford step 3499 current loss 1.015301, current_train_items 112000.
I0302 19:00:12.134973 22699590365312 run.py:483] Algo bellman_ford step 3500 current loss 0.285689, current_train_items 112032.
I0302 19:00:12.142902 22699590365312 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0302 19:00:12.143006 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 19:00:12.159372 22699590365312 run.py:483] Algo bellman_ford step 3501 current loss 0.402582, current_train_items 112064.
I0302 19:00:12.183014 22699590365312 run.py:483] Algo bellman_ford step 3502 current loss 0.607960, current_train_items 112096.
I0302 19:00:12.214219 22699590365312 run.py:483] Algo bellman_ford step 3503 current loss 0.824381, current_train_items 112128.
I0302 19:00:12.249855 22699590365312 run.py:483] Algo bellman_ford step 3504 current loss 0.865081, current_train_items 112160.
I0302 19:00:12.270093 22699590365312 run.py:483] Algo bellman_ford step 3505 current loss 0.370315, current_train_items 112192.
I0302 19:00:12.285343 22699590365312 run.py:483] Algo bellman_ford step 3506 current loss 0.440058, current_train_items 112224.
I0302 19:00:12.309130 22699590365312 run.py:483] Algo bellman_ford step 3507 current loss 0.599463, current_train_items 112256.
I0302 19:00:12.338880 22699590365312 run.py:483] Algo bellman_ford step 3508 current loss 0.710007, current_train_items 112288.
I0302 19:00:12.369422 22699590365312 run.py:483] Algo bellman_ford step 3509 current loss 0.690289, current_train_items 112320.
I0302 19:00:12.388765 22699590365312 run.py:483] Algo bellman_ford step 3510 current loss 0.411017, current_train_items 112352.
I0302 19:00:12.404836 22699590365312 run.py:483] Algo bellman_ford step 3511 current loss 0.389816, current_train_items 112384.
I0302 19:00:12.427661 22699590365312 run.py:483] Algo bellman_ford step 3512 current loss 0.632219, current_train_items 112416.
I0302 19:00:12.457498 22699590365312 run.py:483] Algo bellman_ford step 3513 current loss 0.679525, current_train_items 112448.
I0302 19:00:12.489481 22699590365312 run.py:483] Algo bellman_ford step 3514 current loss 0.716750, current_train_items 112480.
I0302 19:00:12.508625 22699590365312 run.py:483] Algo bellman_ford step 3515 current loss 0.287708, current_train_items 112512.
I0302 19:00:12.524285 22699590365312 run.py:483] Algo bellman_ford step 3516 current loss 0.485507, current_train_items 112544.
I0302 19:00:12.547473 22699590365312 run.py:483] Algo bellman_ford step 3517 current loss 0.609802, current_train_items 112576.
I0302 19:00:12.578646 22699590365312 run.py:483] Algo bellman_ford step 3518 current loss 0.648976, current_train_items 112608.
I0302 19:00:12.614104 22699590365312 run.py:483] Algo bellman_ford step 3519 current loss 0.882661, current_train_items 112640.
I0302 19:00:12.633491 22699590365312 run.py:483] Algo bellman_ford step 3520 current loss 0.288830, current_train_items 112672.
I0302 19:00:12.649411 22699590365312 run.py:483] Algo bellman_ford step 3521 current loss 0.433814, current_train_items 112704.
I0302 19:00:12.671617 22699590365312 run.py:483] Algo bellman_ford step 3522 current loss 0.531401, current_train_items 112736.
I0302 19:00:12.702388 22699590365312 run.py:483] Algo bellman_ford step 3523 current loss 0.713558, current_train_items 112768.
I0302 19:00:12.735275 22699590365312 run.py:483] Algo bellman_ford step 3524 current loss 0.851994, current_train_items 112800.
I0302 19:00:12.754504 22699590365312 run.py:483] Algo bellman_ford step 3525 current loss 0.356314, current_train_items 112832.
I0302 19:00:12.770413 22699590365312 run.py:483] Algo bellman_ford step 3526 current loss 0.481335, current_train_items 112864.
I0302 19:00:12.793797 22699590365312 run.py:483] Algo bellman_ford step 3527 current loss 0.625611, current_train_items 112896.
I0302 19:00:12.825864 22699590365312 run.py:483] Algo bellman_ford step 3528 current loss 0.697658, current_train_items 112928.
I0302 19:00:12.857716 22699590365312 run.py:483] Algo bellman_ford step 3529 current loss 0.756489, current_train_items 112960.
I0302 19:00:12.876916 22699590365312 run.py:483] Algo bellman_ford step 3530 current loss 0.292041, current_train_items 112992.
I0302 19:00:12.893282 22699590365312 run.py:483] Algo bellman_ford step 3531 current loss 0.443020, current_train_items 113024.
I0302 19:00:12.918091 22699590365312 run.py:483] Algo bellman_ford step 3532 current loss 0.621109, current_train_items 113056.
I0302 19:00:12.951647 22699590365312 run.py:483] Algo bellman_ford step 3533 current loss 0.864000, current_train_items 113088.
I0302 19:00:12.985310 22699590365312 run.py:483] Algo bellman_ford step 3534 current loss 0.942281, current_train_items 113120.
I0302 19:00:13.004803 22699590365312 run.py:483] Algo bellman_ford step 3535 current loss 0.289403, current_train_items 113152.
I0302 19:00:13.021054 22699590365312 run.py:483] Algo bellman_ford step 3536 current loss 0.492563, current_train_items 113184.
I0302 19:00:13.044301 22699590365312 run.py:483] Algo bellman_ford step 3537 current loss 0.586490, current_train_items 113216.
I0302 19:00:13.075320 22699590365312 run.py:483] Algo bellman_ford step 3538 current loss 0.726083, current_train_items 113248.
I0302 19:00:13.110594 22699590365312 run.py:483] Algo bellman_ford step 3539 current loss 0.800015, current_train_items 113280.
I0302 19:00:13.130035 22699590365312 run.py:483] Algo bellman_ford step 3540 current loss 0.362306, current_train_items 113312.
I0302 19:00:13.146013 22699590365312 run.py:483] Algo bellman_ford step 3541 current loss 0.653937, current_train_items 113344.
I0302 19:00:13.169898 22699590365312 run.py:483] Algo bellman_ford step 3542 current loss 0.682164, current_train_items 113376.
I0302 19:00:13.200406 22699590365312 run.py:483] Algo bellman_ford step 3543 current loss 0.648674, current_train_items 113408.
I0302 19:00:13.232794 22699590365312 run.py:483] Algo bellman_ford step 3544 current loss 0.867927, current_train_items 113440.
I0302 19:00:13.252488 22699590365312 run.py:483] Algo bellman_ford step 3545 current loss 0.288363, current_train_items 113472.
I0302 19:00:13.268403 22699590365312 run.py:483] Algo bellman_ford step 3546 current loss 0.488388, current_train_items 113504.
I0302 19:00:13.291841 22699590365312 run.py:483] Algo bellman_ford step 3547 current loss 0.753098, current_train_items 113536.
I0302 19:00:13.321437 22699590365312 run.py:483] Algo bellman_ford step 3548 current loss 0.647683, current_train_items 113568.
I0302 19:00:13.354698 22699590365312 run.py:483] Algo bellman_ford step 3549 current loss 0.809586, current_train_items 113600.
I0302 19:00:13.374091 22699590365312 run.py:483] Algo bellman_ford step 3550 current loss 0.310468, current_train_items 113632.
I0302 19:00:13.382354 22699590365312 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0302 19:00:13.382460 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 19:00:13.399705 22699590365312 run.py:483] Algo bellman_ford step 3551 current loss 0.479883, current_train_items 113664.
I0302 19:00:13.425377 22699590365312 run.py:483] Algo bellman_ford step 3552 current loss 0.625874, current_train_items 113696.
I0302 19:00:13.456836 22699590365312 run.py:483] Algo bellman_ford step 3553 current loss 0.638230, current_train_items 113728.
I0302 19:00:13.490700 22699590365312 run.py:483] Algo bellman_ford step 3554 current loss 0.805489, current_train_items 113760.
I0302 19:00:13.510568 22699590365312 run.py:483] Algo bellman_ford step 3555 current loss 0.342054, current_train_items 113792.
I0302 19:00:13.526567 22699590365312 run.py:483] Algo bellman_ford step 3556 current loss 0.498883, current_train_items 113824.
I0302 19:00:13.550751 22699590365312 run.py:483] Algo bellman_ford step 3557 current loss 0.578983, current_train_items 113856.
I0302 19:00:13.582501 22699590365312 run.py:483] Algo bellman_ford step 3558 current loss 0.715150, current_train_items 113888.
I0302 19:00:13.615599 22699590365312 run.py:483] Algo bellman_ford step 3559 current loss 0.712364, current_train_items 113920.
I0302 19:00:13.635614 22699590365312 run.py:483] Algo bellman_ford step 3560 current loss 0.327521, current_train_items 113952.
I0302 19:00:13.651896 22699590365312 run.py:483] Algo bellman_ford step 3561 current loss 0.494628, current_train_items 113984.
I0302 19:00:13.676418 22699590365312 run.py:483] Algo bellman_ford step 3562 current loss 0.653991, current_train_items 114016.
I0302 19:00:13.706138 22699590365312 run.py:483] Algo bellman_ford step 3563 current loss 0.686519, current_train_items 114048.
I0302 19:00:13.743136 22699590365312 run.py:483] Algo bellman_ford step 3564 current loss 0.907412, current_train_items 114080.
I0302 19:00:13.762995 22699590365312 run.py:483] Algo bellman_ford step 3565 current loss 0.263224, current_train_items 114112.
I0302 19:00:13.779567 22699590365312 run.py:483] Algo bellman_ford step 3566 current loss 0.528717, current_train_items 114144.
I0302 19:00:13.803305 22699590365312 run.py:483] Algo bellman_ford step 3567 current loss 0.574740, current_train_items 114176.
I0302 19:00:13.833804 22699590365312 run.py:483] Algo bellman_ford step 3568 current loss 0.756323, current_train_items 114208.
I0302 19:00:13.868740 22699590365312 run.py:483] Algo bellman_ford step 3569 current loss 0.903818, current_train_items 114240.
I0302 19:00:13.888360 22699590365312 run.py:483] Algo bellman_ford step 3570 current loss 0.233206, current_train_items 114272.
I0302 19:00:13.904515 22699590365312 run.py:483] Algo bellman_ford step 3571 current loss 0.434251, current_train_items 114304.
I0302 19:00:13.927939 22699590365312 run.py:483] Algo bellman_ford step 3572 current loss 0.651000, current_train_items 114336.
I0302 19:00:13.958399 22699590365312 run.py:483] Algo bellman_ford step 3573 current loss 0.688491, current_train_items 114368.
I0302 19:00:13.993928 22699590365312 run.py:483] Algo bellman_ford step 3574 current loss 0.895785, current_train_items 114400.
I0302 19:00:14.013523 22699590365312 run.py:483] Algo bellman_ford step 3575 current loss 0.284432, current_train_items 114432.
I0302 19:00:14.029886 22699590365312 run.py:483] Algo bellman_ford step 3576 current loss 0.474280, current_train_items 114464.
I0302 19:00:14.053063 22699590365312 run.py:483] Algo bellman_ford step 3577 current loss 0.645874, current_train_items 114496.
I0302 19:00:14.083706 22699590365312 run.py:483] Algo bellman_ford step 3578 current loss 0.658684, current_train_items 114528.
I0302 19:00:14.116820 22699590365312 run.py:483] Algo bellman_ford step 3579 current loss 0.712075, current_train_items 114560.
I0302 19:00:14.136815 22699590365312 run.py:483] Algo bellman_ford step 3580 current loss 0.320900, current_train_items 114592.
I0302 19:00:14.153008 22699590365312 run.py:483] Algo bellman_ford step 3581 current loss 0.499604, current_train_items 114624.
I0302 19:00:14.177605 22699590365312 run.py:483] Algo bellman_ford step 3582 current loss 0.598344, current_train_items 114656.
I0302 19:00:14.209270 22699590365312 run.py:483] Algo bellman_ford step 3583 current loss 0.632838, current_train_items 114688.
I0302 19:00:14.243383 22699590365312 run.py:483] Algo bellman_ford step 3584 current loss 0.723309, current_train_items 114720.
I0302 19:00:14.263128 22699590365312 run.py:483] Algo bellman_ford step 3585 current loss 0.314616, current_train_items 114752.
I0302 19:00:14.279346 22699590365312 run.py:483] Algo bellman_ford step 3586 current loss 0.567754, current_train_items 114784.
I0302 19:00:14.302263 22699590365312 run.py:483] Algo bellman_ford step 3587 current loss 0.687089, current_train_items 114816.
I0302 19:00:14.333935 22699590365312 run.py:483] Algo bellman_ford step 3588 current loss 0.741339, current_train_items 114848.
I0302 19:00:14.368733 22699590365312 run.py:483] Algo bellman_ford step 3589 current loss 0.760621, current_train_items 114880.
I0302 19:00:14.388411 22699590365312 run.py:483] Algo bellman_ford step 3590 current loss 0.336745, current_train_items 114912.
I0302 19:00:14.404593 22699590365312 run.py:483] Algo bellman_ford step 3591 current loss 0.434665, current_train_items 114944.
I0302 19:00:14.429551 22699590365312 run.py:483] Algo bellman_ford step 3592 current loss 0.673872, current_train_items 114976.
I0302 19:00:14.461884 22699590365312 run.py:483] Algo bellman_ford step 3593 current loss 0.699965, current_train_items 115008.
I0302 19:00:14.496024 22699590365312 run.py:483] Algo bellman_ford step 3594 current loss 0.929002, current_train_items 115040.
I0302 19:00:14.515527 22699590365312 run.py:483] Algo bellman_ford step 3595 current loss 0.282884, current_train_items 115072.
I0302 19:00:14.531974 22699590365312 run.py:483] Algo bellman_ford step 3596 current loss 0.477688, current_train_items 115104.
I0302 19:00:14.555044 22699590365312 run.py:483] Algo bellman_ford step 3597 current loss 0.602582, current_train_items 115136.
I0302 19:00:14.584601 22699590365312 run.py:483] Algo bellman_ford step 3598 current loss 0.622131, current_train_items 115168.
I0302 19:00:14.618907 22699590365312 run.py:483] Algo bellman_ford step 3599 current loss 0.836557, current_train_items 115200.
I0302 19:00:14.638904 22699590365312 run.py:483] Algo bellman_ford step 3600 current loss 0.305698, current_train_items 115232.
I0302 19:00:14.647615 22699590365312 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0302 19:00:14.647724 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:00:14.665250 22699590365312 run.py:483] Algo bellman_ford step 3601 current loss 0.506330, current_train_items 115264.
I0302 19:00:14.689523 22699590365312 run.py:483] Algo bellman_ford step 3602 current loss 0.645093, current_train_items 115296.
I0302 19:00:14.720605 22699590365312 run.py:483] Algo bellman_ford step 3603 current loss 0.645893, current_train_items 115328.
I0302 19:00:14.757606 22699590365312 run.py:483] Algo bellman_ford step 3604 current loss 0.802111, current_train_items 115360.
I0302 19:00:14.777693 22699590365312 run.py:483] Algo bellman_ford step 3605 current loss 0.327402, current_train_items 115392.
I0302 19:00:14.794013 22699590365312 run.py:483] Algo bellman_ford step 3606 current loss 0.542474, current_train_items 115424.
I0302 19:00:14.816740 22699590365312 run.py:483] Algo bellman_ford step 3607 current loss 0.575670, current_train_items 115456.
I0302 19:00:14.846436 22699590365312 run.py:483] Algo bellman_ford step 3608 current loss 0.735022, current_train_items 115488.
I0302 19:00:14.878325 22699590365312 run.py:483] Algo bellman_ford step 3609 current loss 0.816647, current_train_items 115520.
I0302 19:00:14.898071 22699590365312 run.py:483] Algo bellman_ford step 3610 current loss 0.317598, current_train_items 115552.
I0302 19:00:14.914566 22699590365312 run.py:483] Algo bellman_ford step 3611 current loss 0.520095, current_train_items 115584.
I0302 19:00:14.938961 22699590365312 run.py:483] Algo bellman_ford step 3612 current loss 0.686583, current_train_items 115616.
I0302 19:00:14.969027 22699590365312 run.py:483] Algo bellman_ford step 3613 current loss 0.603727, current_train_items 115648.
I0302 19:00:15.000958 22699590365312 run.py:483] Algo bellman_ford step 3614 current loss 0.766031, current_train_items 115680.
I0302 19:00:15.020372 22699590365312 run.py:483] Algo bellman_ford step 3615 current loss 0.310132, current_train_items 115712.
I0302 19:00:15.036911 22699590365312 run.py:483] Algo bellman_ford step 3616 current loss 0.553793, current_train_items 115744.
I0302 19:00:15.059946 22699590365312 run.py:483] Algo bellman_ford step 3617 current loss 0.608141, current_train_items 115776.
I0302 19:00:15.091327 22699590365312 run.py:483] Algo bellman_ford step 3618 current loss 0.844159, current_train_items 115808.
I0302 19:00:15.121612 22699590365312 run.py:483] Algo bellman_ford step 3619 current loss 0.713028, current_train_items 115840.
I0302 19:00:15.141361 22699590365312 run.py:483] Algo bellman_ford step 3620 current loss 0.299956, current_train_items 115872.
I0302 19:00:15.157570 22699590365312 run.py:483] Algo bellman_ford step 3621 current loss 0.471129, current_train_items 115904.
I0302 19:00:15.181069 22699590365312 run.py:483] Algo bellman_ford step 3622 current loss 0.662374, current_train_items 115936.
I0302 19:00:15.212789 22699590365312 run.py:483] Algo bellman_ford step 3623 current loss 0.787402, current_train_items 115968.
I0302 19:00:15.245738 22699590365312 run.py:483] Algo bellman_ford step 3624 current loss 0.724404, current_train_items 116000.
I0302 19:00:15.265298 22699590365312 run.py:483] Algo bellman_ford step 3625 current loss 0.348000, current_train_items 116032.
I0302 19:00:15.281647 22699590365312 run.py:483] Algo bellman_ford step 3626 current loss 0.559548, current_train_items 116064.
I0302 19:00:15.305677 22699590365312 run.py:483] Algo bellman_ford step 3627 current loss 0.645473, current_train_items 116096.
I0302 19:00:15.337543 22699590365312 run.py:483] Algo bellman_ford step 3628 current loss 0.776966, current_train_items 116128.
I0302 19:00:15.370042 22699590365312 run.py:483] Algo bellman_ford step 3629 current loss 0.751373, current_train_items 116160.
I0302 19:00:15.389927 22699590365312 run.py:483] Algo bellman_ford step 3630 current loss 0.271311, current_train_items 116192.
I0302 19:00:15.406009 22699590365312 run.py:483] Algo bellman_ford step 3631 current loss 0.391331, current_train_items 116224.
I0302 19:00:15.429810 22699590365312 run.py:483] Algo bellman_ford step 3632 current loss 0.565910, current_train_items 116256.
I0302 19:00:15.460490 22699590365312 run.py:483] Algo bellman_ford step 3633 current loss 0.769857, current_train_items 116288.
I0302 19:00:15.493316 22699590365312 run.py:483] Algo bellman_ford step 3634 current loss 0.719478, current_train_items 116320.
I0302 19:00:15.512866 22699590365312 run.py:483] Algo bellman_ford step 3635 current loss 0.253887, current_train_items 116352.
I0302 19:00:15.529100 22699590365312 run.py:483] Algo bellman_ford step 3636 current loss 0.547498, current_train_items 116384.
I0302 19:00:15.551949 22699590365312 run.py:483] Algo bellman_ford step 3637 current loss 0.551501, current_train_items 116416.
I0302 19:00:15.582937 22699590365312 run.py:483] Algo bellman_ford step 3638 current loss 0.780324, current_train_items 116448.
I0302 19:00:15.615289 22699590365312 run.py:483] Algo bellman_ford step 3639 current loss 0.843764, current_train_items 116480.
I0302 19:00:15.634992 22699590365312 run.py:483] Algo bellman_ford step 3640 current loss 0.267456, current_train_items 116512.
I0302 19:00:15.650796 22699590365312 run.py:483] Algo bellman_ford step 3641 current loss 0.460362, current_train_items 116544.
I0302 19:00:15.674776 22699590365312 run.py:483] Algo bellman_ford step 3642 current loss 0.666943, current_train_items 116576.
I0302 19:00:15.708747 22699590365312 run.py:483] Algo bellman_ford step 3643 current loss 0.739946, current_train_items 116608.
I0302 19:00:15.743079 22699590365312 run.py:483] Algo bellman_ford step 3644 current loss 0.895630, current_train_items 116640.
I0302 19:00:15.762592 22699590365312 run.py:483] Algo bellman_ford step 3645 current loss 0.275754, current_train_items 116672.
I0302 19:00:15.778806 22699590365312 run.py:483] Algo bellman_ford step 3646 current loss 0.433765, current_train_items 116704.
I0302 19:00:15.804102 22699590365312 run.py:483] Algo bellman_ford step 3647 current loss 0.613525, current_train_items 116736.
I0302 19:00:15.834391 22699590365312 run.py:483] Algo bellman_ford step 3648 current loss 0.655589, current_train_items 116768.
I0302 19:00:15.868181 22699590365312 run.py:483] Algo bellman_ford step 3649 current loss 0.831146, current_train_items 116800.
I0302 19:00:15.887594 22699590365312 run.py:483] Algo bellman_ford step 3650 current loss 0.325807, current_train_items 116832.
I0302 19:00:15.895877 22699590365312 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0302 19:00:15.895981 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:00:15.913001 22699590365312 run.py:483] Algo bellman_ford step 3651 current loss 0.503988, current_train_items 116864.
I0302 19:00:15.935524 22699590365312 run.py:483] Algo bellman_ford step 3652 current loss 0.587344, current_train_items 116896.
I0302 19:00:15.965835 22699590365312 run.py:483] Algo bellman_ford step 3653 current loss 0.568827, current_train_items 116928.
I0302 19:00:15.999019 22699590365312 run.py:483] Algo bellman_ford step 3654 current loss 0.743320, current_train_items 116960.
I0302 19:00:16.018700 22699590365312 run.py:483] Algo bellman_ford step 3655 current loss 0.308109, current_train_items 116992.
I0302 19:00:16.034267 22699590365312 run.py:483] Algo bellman_ford step 3656 current loss 0.510192, current_train_items 117024.
I0302 19:00:16.057990 22699590365312 run.py:483] Algo bellman_ford step 3657 current loss 0.571438, current_train_items 117056.
I0302 19:00:16.088118 22699590365312 run.py:483] Algo bellman_ford step 3658 current loss 0.558579, current_train_items 117088.
I0302 19:00:16.121464 22699590365312 run.py:483] Algo bellman_ford step 3659 current loss 0.779198, current_train_items 117120.
I0302 19:00:16.141303 22699590365312 run.py:483] Algo bellman_ford step 3660 current loss 0.267935, current_train_items 117152.
I0302 19:00:16.157734 22699590365312 run.py:483] Algo bellman_ford step 3661 current loss 0.442813, current_train_items 117184.
I0302 19:00:16.180459 22699590365312 run.py:483] Algo bellman_ford step 3662 current loss 0.550752, current_train_items 117216.
I0302 19:00:16.211975 22699590365312 run.py:483] Algo bellman_ford step 3663 current loss 0.747826, current_train_items 117248.
I0302 19:00:16.246369 22699590365312 run.py:483] Algo bellman_ford step 3664 current loss 0.808389, current_train_items 117280.
I0302 19:00:16.265519 22699590365312 run.py:483] Algo bellman_ford step 3665 current loss 0.465575, current_train_items 117312.
I0302 19:00:16.281412 22699590365312 run.py:483] Algo bellman_ford step 3666 current loss 0.493290, current_train_items 117344.
I0302 19:00:16.305372 22699590365312 run.py:483] Algo bellman_ford step 3667 current loss 0.752387, current_train_items 117376.
I0302 19:00:16.336239 22699590365312 run.py:483] Algo bellman_ford step 3668 current loss 0.664191, current_train_items 117408.
I0302 19:00:16.371014 22699590365312 run.py:483] Algo bellman_ford step 3669 current loss 0.801695, current_train_items 117440.
I0302 19:00:16.390644 22699590365312 run.py:483] Algo bellman_ford step 3670 current loss 0.328197, current_train_items 117472.
I0302 19:00:16.407027 22699590365312 run.py:483] Algo bellman_ford step 3671 current loss 0.524824, current_train_items 117504.
I0302 19:00:16.430051 22699590365312 run.py:483] Algo bellman_ford step 3672 current loss 0.633274, current_train_items 117536.
I0302 19:00:16.460590 22699590365312 run.py:483] Algo bellman_ford step 3673 current loss 0.764004, current_train_items 117568.
I0302 19:00:16.494702 22699590365312 run.py:483] Algo bellman_ford step 3674 current loss 1.100544, current_train_items 117600.
I0302 19:00:16.514169 22699590365312 run.py:483] Algo bellman_ford step 3675 current loss 0.238142, current_train_items 117632.
I0302 19:00:16.529993 22699590365312 run.py:483] Algo bellman_ford step 3676 current loss 0.461501, current_train_items 117664.
I0302 19:00:16.552744 22699590365312 run.py:483] Algo bellman_ford step 3677 current loss 0.586232, current_train_items 117696.
I0302 19:00:16.583379 22699590365312 run.py:483] Algo bellman_ford step 3678 current loss 0.718571, current_train_items 117728.
I0302 19:00:16.616896 22699590365312 run.py:483] Algo bellman_ford step 3679 current loss 0.814169, current_train_items 117760.
I0302 19:00:16.636291 22699590365312 run.py:483] Algo bellman_ford step 3680 current loss 0.288057, current_train_items 117792.
I0302 19:00:16.652698 22699590365312 run.py:483] Algo bellman_ford step 3681 current loss 0.608383, current_train_items 117824.
I0302 19:00:16.676319 22699590365312 run.py:483] Algo bellman_ford step 3682 current loss 0.619997, current_train_items 117856.
I0302 19:00:16.706494 22699590365312 run.py:483] Algo bellman_ford step 3683 current loss 0.683967, current_train_items 117888.
I0302 19:00:16.739946 22699590365312 run.py:483] Algo bellman_ford step 3684 current loss 0.776860, current_train_items 117920.
I0302 19:00:16.759469 22699590365312 run.py:483] Algo bellman_ford step 3685 current loss 0.230617, current_train_items 117952.
I0302 19:00:16.775591 22699590365312 run.py:483] Algo bellman_ford step 3686 current loss 0.487429, current_train_items 117984.
I0302 19:00:16.798266 22699590365312 run.py:483] Algo bellman_ford step 3687 current loss 0.516892, current_train_items 118016.
I0302 19:00:16.830312 22699590365312 run.py:483] Algo bellman_ford step 3688 current loss 0.703121, current_train_items 118048.
I0302 19:00:16.865199 22699590365312 run.py:483] Algo bellman_ford step 3689 current loss 0.799280, current_train_items 118080.
I0302 19:00:16.884848 22699590365312 run.py:483] Algo bellman_ford step 3690 current loss 0.287867, current_train_items 118112.
I0302 19:00:16.901023 22699590365312 run.py:483] Algo bellman_ford step 3691 current loss 0.452939, current_train_items 118144.
I0302 19:00:16.922787 22699590365312 run.py:483] Algo bellman_ford step 3692 current loss 0.617782, current_train_items 118176.
I0302 19:00:16.955187 22699590365312 run.py:483] Algo bellman_ford step 3693 current loss 0.704776, current_train_items 118208.
I0302 19:00:16.987787 22699590365312 run.py:483] Algo bellman_ford step 3694 current loss 0.692298, current_train_items 118240.
I0302 19:00:17.007197 22699590365312 run.py:483] Algo bellman_ford step 3695 current loss 0.330418, current_train_items 118272.
I0302 19:00:17.023971 22699590365312 run.py:483] Algo bellman_ford step 3696 current loss 0.556009, current_train_items 118304.
I0302 19:00:17.047476 22699590365312 run.py:483] Algo bellman_ford step 3697 current loss 0.634071, current_train_items 118336.
I0302 19:00:17.079021 22699590365312 run.py:483] Algo bellman_ford step 3698 current loss 0.707767, current_train_items 118368.
I0302 19:00:17.110628 22699590365312 run.py:483] Algo bellman_ford step 3699 current loss 0.787313, current_train_items 118400.
I0302 19:00:17.130679 22699590365312 run.py:483] Algo bellman_ford step 3700 current loss 0.358796, current_train_items 118432.
I0302 19:00:17.138517 22699590365312 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0302 19:00:17.138622 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:00:17.155130 22699590365312 run.py:483] Algo bellman_ford step 3701 current loss 0.415330, current_train_items 118464.
I0302 19:00:17.178991 22699590365312 run.py:483] Algo bellman_ford step 3702 current loss 0.653388, current_train_items 118496.
I0302 19:00:17.211609 22699590365312 run.py:483] Algo bellman_ford step 3703 current loss 0.850905, current_train_items 118528.
I0302 19:00:17.244810 22699590365312 run.py:483] Algo bellman_ford step 3704 current loss 0.707920, current_train_items 118560.
I0302 19:00:17.264422 22699590365312 run.py:483] Algo bellman_ford step 3705 current loss 0.287919, current_train_items 118592.
I0302 19:00:17.280510 22699590365312 run.py:483] Algo bellman_ford step 3706 current loss 0.601957, current_train_items 118624.
I0302 19:00:17.304061 22699590365312 run.py:483] Algo bellman_ford step 3707 current loss 0.682864, current_train_items 118656.
I0302 19:00:17.333664 22699590365312 run.py:483] Algo bellman_ford step 3708 current loss 0.677908, current_train_items 118688.
I0302 19:00:17.367493 22699590365312 run.py:483] Algo bellman_ford step 3709 current loss 0.915109, current_train_items 118720.
I0302 19:00:17.387002 22699590365312 run.py:483] Algo bellman_ford step 3710 current loss 0.348672, current_train_items 118752.
I0302 19:00:17.403354 22699590365312 run.py:483] Algo bellman_ford step 3711 current loss 0.508452, current_train_items 118784.
I0302 19:00:17.426608 22699590365312 run.py:483] Algo bellman_ford step 3712 current loss 0.649040, current_train_items 118816.
I0302 19:00:17.458637 22699590365312 run.py:483] Algo bellman_ford step 3713 current loss 0.762601, current_train_items 118848.
I0302 19:00:17.492531 22699590365312 run.py:483] Algo bellman_ford step 3714 current loss 0.856575, current_train_items 118880.
I0302 19:00:17.511681 22699590365312 run.py:483] Algo bellman_ford step 3715 current loss 0.240972, current_train_items 118912.
I0302 19:00:17.527730 22699590365312 run.py:483] Algo bellman_ford step 3716 current loss 0.456829, current_train_items 118944.
I0302 19:00:17.550969 22699590365312 run.py:483] Algo bellman_ford step 3717 current loss 0.621845, current_train_items 118976.
I0302 19:00:17.582226 22699590365312 run.py:483] Algo bellman_ford step 3718 current loss 0.727052, current_train_items 119008.
I0302 19:00:17.614501 22699590365312 run.py:483] Algo bellman_ford step 3719 current loss 0.791780, current_train_items 119040.
I0302 19:00:17.634106 22699590365312 run.py:483] Algo bellman_ford step 3720 current loss 0.325867, current_train_items 119072.
I0302 19:00:17.650357 22699590365312 run.py:483] Algo bellman_ford step 3721 current loss 0.566334, current_train_items 119104.
I0302 19:00:17.673643 22699590365312 run.py:483] Algo bellman_ford step 3722 current loss 0.534267, current_train_items 119136.
I0302 19:00:17.703972 22699590365312 run.py:483] Algo bellman_ford step 3723 current loss 0.661624, current_train_items 119168.
I0302 19:00:17.736660 22699590365312 run.py:483] Algo bellman_ford step 3724 current loss 0.742801, current_train_items 119200.
I0302 19:00:17.755864 22699590365312 run.py:483] Algo bellman_ford step 3725 current loss 0.361681, current_train_items 119232.
I0302 19:00:17.772354 22699590365312 run.py:483] Algo bellman_ford step 3726 current loss 0.381381, current_train_items 119264.
I0302 19:00:17.796677 22699590365312 run.py:483] Algo bellman_ford step 3727 current loss 0.682420, current_train_items 119296.
I0302 19:00:17.827677 22699590365312 run.py:483] Algo bellman_ford step 3728 current loss 0.799130, current_train_items 119328.
I0302 19:00:17.860770 22699590365312 run.py:483] Algo bellman_ford step 3729 current loss 0.783645, current_train_items 119360.
I0302 19:00:17.880061 22699590365312 run.py:483] Algo bellman_ford step 3730 current loss 0.258466, current_train_items 119392.
I0302 19:00:17.896002 22699590365312 run.py:483] Algo bellman_ford step 3731 current loss 0.483836, current_train_items 119424.
I0302 19:00:17.919554 22699590365312 run.py:483] Algo bellman_ford step 3732 current loss 0.646509, current_train_items 119456.
I0302 19:00:17.948336 22699590365312 run.py:483] Algo bellman_ford step 3733 current loss 0.632024, current_train_items 119488.
I0302 19:00:17.982421 22699590365312 run.py:483] Algo bellman_ford step 3734 current loss 0.821962, current_train_items 119520.
I0302 19:00:18.001724 22699590365312 run.py:483] Algo bellman_ford step 3735 current loss 0.294006, current_train_items 119552.
I0302 19:00:18.017656 22699590365312 run.py:483] Algo bellman_ford step 3736 current loss 0.473140, current_train_items 119584.
I0302 19:00:18.040412 22699590365312 run.py:483] Algo bellman_ford step 3737 current loss 0.551836, current_train_items 119616.
I0302 19:00:18.071302 22699590365312 run.py:483] Algo bellman_ford step 3738 current loss 0.765564, current_train_items 119648.
I0302 19:00:18.103956 22699590365312 run.py:483] Algo bellman_ford step 3739 current loss 0.756873, current_train_items 119680.
I0302 19:00:18.123572 22699590365312 run.py:483] Algo bellman_ford step 3740 current loss 0.321140, current_train_items 119712.
I0302 19:00:18.139528 22699590365312 run.py:483] Algo bellman_ford step 3741 current loss 0.445196, current_train_items 119744.
I0302 19:00:18.162218 22699590365312 run.py:483] Algo bellman_ford step 3742 current loss 0.617385, current_train_items 119776.
I0302 19:00:18.192686 22699590365312 run.py:483] Algo bellman_ford step 3743 current loss 0.859860, current_train_items 119808.
I0302 19:00:18.223615 22699590365312 run.py:483] Algo bellman_ford step 3744 current loss 0.804654, current_train_items 119840.
I0302 19:00:18.242876 22699590365312 run.py:483] Algo bellman_ford step 3745 current loss 0.229745, current_train_items 119872.
I0302 19:00:18.258756 22699590365312 run.py:483] Algo bellman_ford step 3746 current loss 0.434368, current_train_items 119904.
I0302 19:00:18.282362 22699590365312 run.py:483] Algo bellman_ford step 3747 current loss 0.526221, current_train_items 119936.
I0302 19:00:18.313638 22699590365312 run.py:483] Algo bellman_ford step 3748 current loss 0.723023, current_train_items 119968.
I0302 19:00:18.348722 22699590365312 run.py:483] Algo bellman_ford step 3749 current loss 0.789112, current_train_items 120000.
I0302 19:00:18.368211 22699590365312 run.py:483] Algo bellman_ford step 3750 current loss 0.305568, current_train_items 120032.
I0302 19:00:18.376574 22699590365312 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0302 19:00:18.376707 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:00:18.393738 22699590365312 run.py:483] Algo bellman_ford step 3751 current loss 0.559922, current_train_items 120064.
I0302 19:00:18.417395 22699590365312 run.py:483] Algo bellman_ford step 3752 current loss 0.618699, current_train_items 120096.
I0302 19:00:18.448350 22699590365312 run.py:483] Algo bellman_ford step 3753 current loss 0.684088, current_train_items 120128.
I0302 19:00:18.484294 22699590365312 run.py:483] Algo bellman_ford step 3754 current loss 0.795333, current_train_items 120160.
I0302 19:00:18.503790 22699590365312 run.py:483] Algo bellman_ford step 3755 current loss 0.317199, current_train_items 120192.
I0302 19:00:18.519701 22699590365312 run.py:483] Algo bellman_ford step 3756 current loss 0.539098, current_train_items 120224.
I0302 19:00:18.543683 22699590365312 run.py:483] Algo bellman_ford step 3757 current loss 0.677967, current_train_items 120256.
I0302 19:00:18.574197 22699590365312 run.py:483] Algo bellman_ford step 3758 current loss 0.701987, current_train_items 120288.
I0302 19:00:18.605705 22699590365312 run.py:483] Algo bellman_ford step 3759 current loss 0.666346, current_train_items 120320.
I0302 19:00:18.625437 22699590365312 run.py:483] Algo bellman_ford step 3760 current loss 0.292250, current_train_items 120352.
I0302 19:00:18.641705 22699590365312 run.py:483] Algo bellman_ford step 3761 current loss 0.567604, current_train_items 120384.
I0302 19:00:18.664055 22699590365312 run.py:483] Algo bellman_ford step 3762 current loss 0.535261, current_train_items 120416.
I0302 19:00:18.694379 22699590365312 run.py:483] Algo bellman_ford step 3763 current loss 0.703022, current_train_items 120448.
I0302 19:00:18.727771 22699590365312 run.py:483] Algo bellman_ford step 3764 current loss 1.113176, current_train_items 120480.
I0302 19:00:18.747006 22699590365312 run.py:483] Algo bellman_ford step 3765 current loss 0.402645, current_train_items 120512.
I0302 19:00:18.763642 22699590365312 run.py:483] Algo bellman_ford step 3766 current loss 0.396725, current_train_items 120544.
I0302 19:00:18.787185 22699590365312 run.py:483] Algo bellman_ford step 3767 current loss 0.718605, current_train_items 120576.
I0302 19:00:18.818652 22699590365312 run.py:483] Algo bellman_ford step 3768 current loss 0.778724, current_train_items 120608.
I0302 19:00:18.850908 22699590365312 run.py:483] Algo bellman_ford step 3769 current loss 0.791860, current_train_items 120640.
I0302 19:00:18.870672 22699590365312 run.py:483] Algo bellman_ford step 3770 current loss 0.300380, current_train_items 120672.
I0302 19:00:18.886979 22699590365312 run.py:483] Algo bellman_ford step 3771 current loss 0.499459, current_train_items 120704.
I0302 19:00:18.909367 22699590365312 run.py:483] Algo bellman_ford step 3772 current loss 0.580040, current_train_items 120736.
I0302 19:00:18.939364 22699590365312 run.py:483] Algo bellman_ford step 3773 current loss 0.618302, current_train_items 120768.
I0302 19:00:18.969920 22699590365312 run.py:483] Algo bellman_ford step 3774 current loss 0.645495, current_train_items 120800.
I0302 19:00:18.989650 22699590365312 run.py:483] Algo bellman_ford step 3775 current loss 0.324465, current_train_items 120832.
I0302 19:00:19.006260 22699590365312 run.py:483] Algo bellman_ford step 3776 current loss 0.491519, current_train_items 120864.
I0302 19:00:19.028794 22699590365312 run.py:483] Algo bellman_ford step 3777 current loss 0.621585, current_train_items 120896.
I0302 19:00:19.060074 22699590365312 run.py:483] Algo bellman_ford step 3778 current loss 0.747668, current_train_items 120928.
I0302 19:00:19.093640 22699590365312 run.py:483] Algo bellman_ford step 3779 current loss 0.840371, current_train_items 120960.
I0302 19:00:19.112931 22699590365312 run.py:483] Algo bellman_ford step 3780 current loss 0.295523, current_train_items 120992.
I0302 19:00:19.129257 22699590365312 run.py:483] Algo bellman_ford step 3781 current loss 0.534683, current_train_items 121024.
I0302 19:00:19.152195 22699590365312 run.py:483] Algo bellman_ford step 3782 current loss 0.620575, current_train_items 121056.
I0302 19:00:19.181826 22699590365312 run.py:483] Algo bellman_ford step 3783 current loss 0.652793, current_train_items 121088.
I0302 19:00:19.217740 22699590365312 run.py:483] Algo bellman_ford step 3784 current loss 0.820329, current_train_items 121120.
I0302 19:00:19.237134 22699590365312 run.py:483] Algo bellman_ford step 3785 current loss 0.278452, current_train_items 121152.
I0302 19:00:19.253203 22699590365312 run.py:483] Algo bellman_ford step 3786 current loss 0.362963, current_train_items 121184.
I0302 19:00:19.276614 22699590365312 run.py:483] Algo bellman_ford step 3787 current loss 0.760139, current_train_items 121216.
I0302 19:00:19.307197 22699590365312 run.py:483] Algo bellman_ford step 3788 current loss 0.934429, current_train_items 121248.
I0302 19:00:19.340465 22699590365312 run.py:483] Algo bellman_ford step 3789 current loss 0.781477, current_train_items 121280.
I0302 19:00:19.360265 22699590365312 run.py:483] Algo bellman_ford step 3790 current loss 0.242971, current_train_items 121312.
I0302 19:00:19.376534 22699590365312 run.py:483] Algo bellman_ford step 3791 current loss 0.458159, current_train_items 121344.
I0302 19:00:19.399314 22699590365312 run.py:483] Algo bellman_ford step 3792 current loss 0.565752, current_train_items 121376.
I0302 19:00:19.430799 22699590365312 run.py:483] Algo bellman_ford step 3793 current loss 0.874479, current_train_items 121408.
I0302 19:00:19.463544 22699590365312 run.py:483] Algo bellman_ford step 3794 current loss 0.805902, current_train_items 121440.
I0302 19:00:19.482846 22699590365312 run.py:483] Algo bellman_ford step 3795 current loss 0.389828, current_train_items 121472.
I0302 19:00:19.499289 22699590365312 run.py:483] Algo bellman_ford step 3796 current loss 0.535944, current_train_items 121504.
I0302 19:00:19.522880 22699590365312 run.py:483] Algo bellman_ford step 3797 current loss 0.610481, current_train_items 121536.
I0302 19:00:19.554292 22699590365312 run.py:483] Algo bellman_ford step 3798 current loss 0.831987, current_train_items 121568.
I0302 19:00:19.586361 22699590365312 run.py:483] Algo bellman_ford step 3799 current loss 0.707751, current_train_items 121600.
I0302 19:00:19.606086 22699590365312 run.py:483] Algo bellman_ford step 3800 current loss 0.293800, current_train_items 121632.
I0302 19:00:19.613847 22699590365312 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0302 19:00:19.613954 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:19.630843 22699590365312 run.py:483] Algo bellman_ford step 3801 current loss 0.470721, current_train_items 121664.
I0302 19:00:19.655654 22699590365312 run.py:483] Algo bellman_ford step 3802 current loss 0.548984, current_train_items 121696.
I0302 19:00:19.686992 22699590365312 run.py:483] Algo bellman_ford step 3803 current loss 0.788211, current_train_items 121728.
I0302 19:00:19.718811 22699590365312 run.py:483] Algo bellman_ford step 3804 current loss 0.765716, current_train_items 121760.
I0302 19:00:19.738998 22699590365312 run.py:483] Algo bellman_ford step 3805 current loss 0.293615, current_train_items 121792.
I0302 19:00:19.755015 22699590365312 run.py:483] Algo bellman_ford step 3806 current loss 0.518394, current_train_items 121824.
I0302 19:00:19.778815 22699590365312 run.py:483] Algo bellman_ford step 3807 current loss 0.542981, current_train_items 121856.
I0302 19:00:19.809456 22699590365312 run.py:483] Algo bellman_ford step 3808 current loss 0.692169, current_train_items 121888.
I0302 19:00:19.844509 22699590365312 run.py:483] Algo bellman_ford step 3809 current loss 0.703167, current_train_items 121920.
I0302 19:00:19.864073 22699590365312 run.py:483] Algo bellman_ford step 3810 current loss 0.388231, current_train_items 121952.
I0302 19:00:19.880567 22699590365312 run.py:483] Algo bellman_ford step 3811 current loss 0.421824, current_train_items 121984.
I0302 19:00:19.904313 22699590365312 run.py:483] Algo bellman_ford step 3812 current loss 0.538960, current_train_items 122016.
I0302 19:00:19.937053 22699590365312 run.py:483] Algo bellman_ford step 3813 current loss 0.743434, current_train_items 122048.
I0302 19:00:19.972374 22699590365312 run.py:483] Algo bellman_ford step 3814 current loss 0.957664, current_train_items 122080.
I0302 19:00:19.992098 22699590365312 run.py:483] Algo bellman_ford step 3815 current loss 0.340641, current_train_items 122112.
I0302 19:00:20.008677 22699590365312 run.py:483] Algo bellman_ford step 3816 current loss 0.443466, current_train_items 122144.
I0302 19:00:20.032504 22699590365312 run.py:483] Algo bellman_ford step 3817 current loss 0.593151, current_train_items 122176.
I0302 19:00:20.063412 22699590365312 run.py:483] Algo bellman_ford step 3818 current loss 0.595392, current_train_items 122208.
I0302 19:00:20.096884 22699590365312 run.py:483] Algo bellman_ford step 3819 current loss 0.812225, current_train_items 122240.
I0302 19:00:20.116581 22699590365312 run.py:483] Algo bellman_ford step 3820 current loss 0.299237, current_train_items 122272.
I0302 19:00:20.132355 22699590365312 run.py:483] Algo bellman_ford step 3821 current loss 0.428318, current_train_items 122304.
I0302 19:00:20.155758 22699590365312 run.py:483] Algo bellman_ford step 3822 current loss 0.723000, current_train_items 122336.
I0302 19:00:20.186855 22699590365312 run.py:483] Algo bellman_ford step 3823 current loss 0.717537, current_train_items 122368.
I0302 19:00:20.221040 22699590365312 run.py:483] Algo bellman_ford step 3824 current loss 0.820611, current_train_items 122400.
I0302 19:00:20.241058 22699590365312 run.py:483] Algo bellman_ford step 3825 current loss 0.343432, current_train_items 122432.
I0302 19:00:20.256909 22699590365312 run.py:483] Algo bellman_ford step 3826 current loss 0.437783, current_train_items 122464.
I0302 19:00:20.280732 22699590365312 run.py:483] Algo bellman_ford step 3827 current loss 0.620656, current_train_items 122496.
I0302 19:00:20.312200 22699590365312 run.py:483] Algo bellman_ford step 3828 current loss 0.666213, current_train_items 122528.
I0302 19:00:20.344051 22699590365312 run.py:483] Algo bellman_ford step 3829 current loss 0.722003, current_train_items 122560.
I0302 19:00:20.363529 22699590365312 run.py:483] Algo bellman_ford step 3830 current loss 0.292826, current_train_items 122592.
I0302 19:00:20.379742 22699590365312 run.py:483] Algo bellman_ford step 3831 current loss 0.533114, current_train_items 122624.
I0302 19:00:20.402339 22699590365312 run.py:483] Algo bellman_ford step 3832 current loss 0.546873, current_train_items 122656.
I0302 19:00:20.434226 22699590365312 run.py:483] Algo bellman_ford step 3833 current loss 0.832009, current_train_items 122688.
I0302 19:00:20.467018 22699590365312 run.py:483] Algo bellman_ford step 3834 current loss 0.801739, current_train_items 122720.
I0302 19:00:20.486528 22699590365312 run.py:483] Algo bellman_ford step 3835 current loss 0.315039, current_train_items 122752.
I0302 19:00:20.502167 22699590365312 run.py:483] Algo bellman_ford step 3836 current loss 0.388272, current_train_items 122784.
I0302 19:00:20.526763 22699590365312 run.py:483] Algo bellman_ford step 3837 current loss 0.720616, current_train_items 122816.
I0302 19:00:20.558160 22699590365312 run.py:483] Algo bellman_ford step 3838 current loss 0.682009, current_train_items 122848.
I0302 19:00:20.591518 22699590365312 run.py:483] Algo bellman_ford step 3839 current loss 0.688288, current_train_items 122880.
I0302 19:00:20.610921 22699590365312 run.py:483] Algo bellman_ford step 3840 current loss 0.322337, current_train_items 122912.
I0302 19:00:20.626777 22699590365312 run.py:483] Algo bellman_ford step 3841 current loss 0.533161, current_train_items 122944.
I0302 19:00:20.649669 22699590365312 run.py:483] Algo bellman_ford step 3842 current loss 0.536146, current_train_items 122976.
I0302 19:00:20.680579 22699590365312 run.py:483] Algo bellman_ford step 3843 current loss 0.778017, current_train_items 123008.
I0302 19:00:20.715095 22699590365312 run.py:483] Algo bellman_ford step 3844 current loss 0.916277, current_train_items 123040.
I0302 19:00:20.734837 22699590365312 run.py:483] Algo bellman_ford step 3845 current loss 0.297990, current_train_items 123072.
I0302 19:00:20.750953 22699590365312 run.py:483] Algo bellman_ford step 3846 current loss 0.447637, current_train_items 123104.
I0302 19:00:20.776088 22699590365312 run.py:483] Algo bellman_ford step 3847 current loss 0.717532, current_train_items 123136.
I0302 19:00:20.806578 22699590365312 run.py:483] Algo bellman_ford step 3848 current loss 0.635016, current_train_items 123168.
I0302 19:00:20.840001 22699590365312 run.py:483] Algo bellman_ford step 3849 current loss 0.810014, current_train_items 123200.
I0302 19:00:20.859579 22699590365312 run.py:483] Algo bellman_ford step 3850 current loss 0.318053, current_train_items 123232.
I0302 19:00:20.867633 22699590365312 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0302 19:00:20.867740 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 19:00:20.884193 22699590365312 run.py:483] Algo bellman_ford step 3851 current loss 0.387722, current_train_items 123264.
I0302 19:00:20.907270 22699590365312 run.py:483] Algo bellman_ford step 3852 current loss 0.598905, current_train_items 123296.
I0302 19:00:20.939389 22699590365312 run.py:483] Algo bellman_ford step 3853 current loss 0.727293, current_train_items 123328.
I0302 19:00:20.975246 22699590365312 run.py:483] Algo bellman_ford step 3854 current loss 0.735498, current_train_items 123360.
I0302 19:00:20.995553 22699590365312 run.py:483] Algo bellman_ford step 3855 current loss 0.234568, current_train_items 123392.
I0302 19:00:21.011277 22699590365312 run.py:483] Algo bellman_ford step 3856 current loss 0.436731, current_train_items 123424.
I0302 19:00:21.033610 22699590365312 run.py:483] Algo bellman_ford step 3857 current loss 0.623645, current_train_items 123456.
I0302 19:00:21.065017 22699590365312 run.py:483] Algo bellman_ford step 3858 current loss 0.717405, current_train_items 123488.
I0302 19:00:21.100877 22699590365312 run.py:483] Algo bellman_ford step 3859 current loss 0.966455, current_train_items 123520.
I0302 19:00:21.120765 22699590365312 run.py:483] Algo bellman_ford step 3860 current loss 0.347531, current_train_items 123552.
I0302 19:00:21.137196 22699590365312 run.py:483] Algo bellman_ford step 3861 current loss 0.457769, current_train_items 123584.
I0302 19:00:21.160377 22699590365312 run.py:483] Algo bellman_ford step 3862 current loss 0.599922, current_train_items 123616.
I0302 19:00:21.191249 22699590365312 run.py:483] Algo bellman_ford step 3863 current loss 0.768942, current_train_items 123648.
I0302 19:00:21.227094 22699590365312 run.py:483] Algo bellman_ford step 3864 current loss 0.830432, current_train_items 123680.
I0302 19:00:21.246658 22699590365312 run.py:483] Algo bellman_ford step 3865 current loss 0.436536, current_train_items 123712.
I0302 19:00:21.262528 22699590365312 run.py:483] Algo bellman_ford step 3866 current loss 0.495184, current_train_items 123744.
I0302 19:00:21.285733 22699590365312 run.py:483] Algo bellman_ford step 3867 current loss 0.579364, current_train_items 123776.
I0302 19:00:21.316075 22699590365312 run.py:483] Algo bellman_ford step 3868 current loss 0.629140, current_train_items 123808.
I0302 19:00:21.348952 22699590365312 run.py:483] Algo bellman_ford step 3869 current loss 0.746954, current_train_items 123840.
I0302 19:00:21.368859 22699590365312 run.py:483] Algo bellman_ford step 3870 current loss 0.307891, current_train_items 123872.
I0302 19:00:21.384606 22699590365312 run.py:483] Algo bellman_ford step 3871 current loss 0.455950, current_train_items 123904.
I0302 19:00:21.408168 22699590365312 run.py:483] Algo bellman_ford step 3872 current loss 0.645157, current_train_items 123936.
I0302 19:00:21.437433 22699590365312 run.py:483] Algo bellman_ford step 3873 current loss 0.563521, current_train_items 123968.
I0302 19:00:21.470108 22699590365312 run.py:483] Algo bellman_ford step 3874 current loss 0.807350, current_train_items 124000.
I0302 19:00:21.489890 22699590365312 run.py:483] Algo bellman_ford step 3875 current loss 0.351476, current_train_items 124032.
I0302 19:00:21.506368 22699590365312 run.py:483] Algo bellman_ford step 3876 current loss 0.498439, current_train_items 124064.
I0302 19:00:21.529967 22699590365312 run.py:483] Algo bellman_ford step 3877 current loss 0.688760, current_train_items 124096.
I0302 19:00:21.561238 22699590365312 run.py:483] Algo bellman_ford step 3878 current loss 0.660652, current_train_items 124128.
I0302 19:00:21.594427 22699590365312 run.py:483] Algo bellman_ford step 3879 current loss 0.857414, current_train_items 124160.
I0302 19:00:21.613855 22699590365312 run.py:483] Algo bellman_ford step 3880 current loss 0.348674, current_train_items 124192.
I0302 19:00:21.629904 22699590365312 run.py:483] Algo bellman_ford step 3881 current loss 0.521410, current_train_items 124224.
I0302 19:00:21.653636 22699590365312 run.py:483] Algo bellman_ford step 3882 current loss 1.012728, current_train_items 124256.
I0302 19:00:21.684086 22699590365312 run.py:483] Algo bellman_ford step 3883 current loss 0.916888, current_train_items 124288.
I0302 19:00:21.716253 22699590365312 run.py:483] Algo bellman_ford step 3884 current loss 0.953367, current_train_items 124320.
I0302 19:00:21.736230 22699590365312 run.py:483] Algo bellman_ford step 3885 current loss 0.302663, current_train_items 124352.
I0302 19:00:21.752842 22699590365312 run.py:483] Algo bellman_ford step 3886 current loss 0.488032, current_train_items 124384.
I0302 19:00:21.775964 22699590365312 run.py:483] Algo bellman_ford step 3887 current loss 0.772392, current_train_items 124416.
I0302 19:00:21.806605 22699590365312 run.py:483] Algo bellman_ford step 3888 current loss 0.673909, current_train_items 124448.
I0302 19:00:21.841043 22699590365312 run.py:483] Algo bellman_ford step 3889 current loss 0.787860, current_train_items 124480.
I0302 19:00:21.860774 22699590365312 run.py:483] Algo bellman_ford step 3890 current loss 0.293479, current_train_items 124512.
I0302 19:00:21.877022 22699590365312 run.py:483] Algo bellman_ford step 3891 current loss 0.624490, current_train_items 124544.
I0302 19:00:21.900878 22699590365312 run.py:483] Algo bellman_ford step 3892 current loss 0.628625, current_train_items 124576.
I0302 19:00:21.929706 22699590365312 run.py:483] Algo bellman_ford step 3893 current loss 0.567321, current_train_items 124608.
I0302 19:00:21.964045 22699590365312 run.py:483] Algo bellman_ford step 3894 current loss 0.732467, current_train_items 124640.
I0302 19:00:21.983418 22699590365312 run.py:483] Algo bellman_ford step 3895 current loss 0.263138, current_train_items 124672.
I0302 19:00:21.999831 22699590365312 run.py:483] Algo bellman_ford step 3896 current loss 0.496272, current_train_items 124704.
I0302 19:00:22.024292 22699590365312 run.py:483] Algo bellman_ford step 3897 current loss 0.633110, current_train_items 124736.
I0302 19:00:22.055486 22699590365312 run.py:483] Algo bellman_ford step 3898 current loss 0.659153, current_train_items 124768.
I0302 19:00:22.088465 22699590365312 run.py:483] Algo bellman_ford step 3899 current loss 0.851383, current_train_items 124800.
I0302 19:00:22.108384 22699590365312 run.py:483] Algo bellman_ford step 3900 current loss 0.311274, current_train_items 124832.
I0302 19:00:22.116131 22699590365312 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0302 19:00:22.116248 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:00:22.133194 22699590365312 run.py:483] Algo bellman_ford step 3901 current loss 0.532265, current_train_items 124864.
I0302 19:00:22.157218 22699590365312 run.py:483] Algo bellman_ford step 3902 current loss 0.621726, current_train_items 124896.
I0302 19:00:22.188435 22699590365312 run.py:483] Algo bellman_ford step 3903 current loss 0.706620, current_train_items 124928.
I0302 19:00:22.222086 22699590365312 run.py:483] Algo bellman_ford step 3904 current loss 0.785667, current_train_items 124960.
I0302 19:00:22.241976 22699590365312 run.py:483] Algo bellman_ford step 3905 current loss 0.248177, current_train_items 124992.
I0302 19:00:22.257712 22699590365312 run.py:483] Algo bellman_ford step 3906 current loss 0.446088, current_train_items 125024.
I0302 19:00:22.279953 22699590365312 run.py:483] Algo bellman_ford step 3907 current loss 0.554307, current_train_items 125056.
I0302 19:00:22.311239 22699590365312 run.py:483] Algo bellman_ford step 3908 current loss 0.718445, current_train_items 125088.
I0302 19:00:22.344949 22699590365312 run.py:483] Algo bellman_ford step 3909 current loss 0.785811, current_train_items 125120.
I0302 19:00:22.364077 22699590365312 run.py:483] Algo bellman_ford step 3910 current loss 0.284260, current_train_items 125152.
I0302 19:00:22.379868 22699590365312 run.py:483] Algo bellman_ford step 3911 current loss 0.434469, current_train_items 125184.
I0302 19:00:22.403226 22699590365312 run.py:483] Algo bellman_ford step 3912 current loss 0.662380, current_train_items 125216.
I0302 19:00:22.433397 22699590365312 run.py:483] Algo bellman_ford step 3913 current loss 0.680162, current_train_items 125248.
I0302 19:00:22.463666 22699590365312 run.py:483] Algo bellman_ford step 3914 current loss 0.787227, current_train_items 125280.
I0302 19:00:22.482824 22699590365312 run.py:483] Algo bellman_ford step 3915 current loss 0.407636, current_train_items 125312.
I0302 19:00:22.498709 22699590365312 run.py:483] Algo bellman_ford step 3916 current loss 0.440014, current_train_items 125344.
I0302 19:00:22.523185 22699590365312 run.py:483] Algo bellman_ford step 3917 current loss 0.801592, current_train_items 125376.
I0302 19:00:22.552635 22699590365312 run.py:483] Algo bellman_ford step 3918 current loss 0.806943, current_train_items 125408.
I0302 19:00:22.586133 22699590365312 run.py:483] Algo bellman_ford step 3919 current loss 0.763121, current_train_items 125440.
I0302 19:00:22.605643 22699590365312 run.py:483] Algo bellman_ford step 3920 current loss 0.312155, current_train_items 125472.
I0302 19:00:22.621955 22699590365312 run.py:483] Algo bellman_ford step 3921 current loss 0.570006, current_train_items 125504.
I0302 19:00:22.646224 22699590365312 run.py:483] Algo bellman_ford step 3922 current loss 0.700368, current_train_items 125536.
I0302 19:00:22.677526 22699590365312 run.py:483] Algo bellman_ford step 3923 current loss 0.954963, current_train_items 125568.
I0302 19:00:22.713257 22699590365312 run.py:483] Algo bellman_ford step 3924 current loss 0.945036, current_train_items 125600.
I0302 19:00:22.732825 22699590365312 run.py:483] Algo bellman_ford step 3925 current loss 0.330353, current_train_items 125632.
I0302 19:00:22.748989 22699590365312 run.py:483] Algo bellman_ford step 3926 current loss 0.408821, current_train_items 125664.
I0302 19:00:22.772514 22699590365312 run.py:483] Algo bellman_ford step 3927 current loss 0.639003, current_train_items 125696.
I0302 19:00:22.802971 22699590365312 run.py:483] Algo bellman_ford step 3928 current loss 0.789735, current_train_items 125728.
I0302 19:00:22.835743 22699590365312 run.py:483] Algo bellman_ford step 3929 current loss 0.852381, current_train_items 125760.
I0302 19:00:22.854882 22699590365312 run.py:483] Algo bellman_ford step 3930 current loss 0.290360, current_train_items 125792.
I0302 19:00:22.870750 22699590365312 run.py:483] Algo bellman_ford step 3931 current loss 0.592822, current_train_items 125824.
I0302 19:00:22.893871 22699590365312 run.py:483] Algo bellman_ford step 3932 current loss 0.660189, current_train_items 125856.
I0302 19:00:22.925931 22699590365312 run.py:483] Algo bellman_ford step 3933 current loss 0.705714, current_train_items 125888.
I0302 19:00:22.959163 22699590365312 run.py:483] Algo bellman_ford step 3934 current loss 0.883371, current_train_items 125920.
I0302 19:00:22.978649 22699590365312 run.py:483] Algo bellman_ford step 3935 current loss 0.325941, current_train_items 125952.
I0302 19:00:22.994959 22699590365312 run.py:483] Algo bellman_ford step 3936 current loss 0.509742, current_train_items 125984.
I0302 19:00:23.018023 22699590365312 run.py:483] Algo bellman_ford step 3937 current loss 0.573265, current_train_items 126016.
I0302 19:00:23.049070 22699590365312 run.py:483] Algo bellman_ford step 3938 current loss 0.777176, current_train_items 126048.
I0302 19:00:23.083273 22699590365312 run.py:483] Algo bellman_ford step 3939 current loss 0.758101, current_train_items 126080.
I0302 19:00:23.102426 22699590365312 run.py:483] Algo bellman_ford step 3940 current loss 0.340323, current_train_items 126112.
I0302 19:00:23.118481 22699590365312 run.py:483] Algo bellman_ford step 3941 current loss 0.455279, current_train_items 126144.
I0302 19:00:23.141781 22699590365312 run.py:483] Algo bellman_ford step 3942 current loss 0.620383, current_train_items 126176.
I0302 19:00:23.171250 22699590365312 run.py:483] Algo bellman_ford step 3943 current loss 0.667228, current_train_items 126208.
I0302 19:00:23.205375 22699590365312 run.py:483] Algo bellman_ford step 3944 current loss 0.751634, current_train_items 126240.
I0302 19:00:23.224553 22699590365312 run.py:483] Algo bellman_ford step 3945 current loss 0.325789, current_train_items 126272.
I0302 19:00:23.240969 22699590365312 run.py:483] Algo bellman_ford step 3946 current loss 0.439380, current_train_items 126304.
I0302 19:00:23.264003 22699590365312 run.py:483] Algo bellman_ford step 3947 current loss 0.559406, current_train_items 126336.
I0302 19:00:23.292985 22699590365312 run.py:483] Algo bellman_ford step 3948 current loss 0.761327, current_train_items 126368.
I0302 19:00:23.326728 22699590365312 run.py:483] Algo bellman_ford step 3949 current loss 0.691564, current_train_items 126400.
I0302 19:00:23.346015 22699590365312 run.py:483] Algo bellman_ford step 3950 current loss 0.256425, current_train_items 126432.
I0302 19:00:23.354213 22699590365312 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0302 19:00:23.354319 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0302 19:00:23.371477 22699590365312 run.py:483] Algo bellman_ford step 3951 current loss 0.452856, current_train_items 126464.
I0302 19:00:23.395909 22699590365312 run.py:483] Algo bellman_ford step 3952 current loss 0.630843, current_train_items 126496.
I0302 19:00:23.426451 22699590365312 run.py:483] Algo bellman_ford step 3953 current loss 0.680536, current_train_items 126528.
I0302 19:00:23.458595 22699590365312 run.py:483] Algo bellman_ford step 3954 current loss 0.720177, current_train_items 126560.
I0302 19:00:23.478405 22699590365312 run.py:483] Algo bellman_ford step 3955 current loss 0.316481, current_train_items 126592.
I0302 19:00:23.494175 22699590365312 run.py:483] Algo bellman_ford step 3956 current loss 0.393146, current_train_items 126624.
I0302 19:00:23.517957 22699590365312 run.py:483] Algo bellman_ford step 3957 current loss 0.598868, current_train_items 126656.
I0302 19:00:23.549807 22699590365312 run.py:483] Algo bellman_ford step 3958 current loss 0.788513, current_train_items 126688.
I0302 19:00:23.585587 22699590365312 run.py:483] Algo bellman_ford step 3959 current loss 0.857752, current_train_items 126720.
I0302 19:00:23.605415 22699590365312 run.py:483] Algo bellman_ford step 3960 current loss 0.305251, current_train_items 126752.
I0302 19:00:23.621412 22699590365312 run.py:483] Algo bellman_ford step 3961 current loss 0.457012, current_train_items 126784.
I0302 19:00:23.643459 22699590365312 run.py:483] Algo bellman_ford step 3962 current loss 0.496721, current_train_items 126816.
I0302 19:00:23.674622 22699590365312 run.py:483] Algo bellman_ford step 3963 current loss 0.678426, current_train_items 126848.
I0302 19:00:23.708437 22699590365312 run.py:483] Algo bellman_ford step 3964 current loss 0.987144, current_train_items 126880.
I0302 19:00:23.727874 22699590365312 run.py:483] Algo bellman_ford step 3965 current loss 0.394234, current_train_items 126912.
I0302 19:00:23.744019 22699590365312 run.py:483] Algo bellman_ford step 3966 current loss 0.466693, current_train_items 126944.
I0302 19:00:23.768999 22699590365312 run.py:483] Algo bellman_ford step 3967 current loss 0.722065, current_train_items 126976.
I0302 19:00:23.800012 22699590365312 run.py:483] Algo bellman_ford step 3968 current loss 0.690789, current_train_items 127008.
I0302 19:00:23.832791 22699590365312 run.py:483] Algo bellman_ford step 3969 current loss 0.742411, current_train_items 127040.
I0302 19:00:23.852751 22699590365312 run.py:483] Algo bellman_ford step 3970 current loss 0.329451, current_train_items 127072.
I0302 19:00:23.868901 22699590365312 run.py:483] Algo bellman_ford step 3971 current loss 0.451293, current_train_items 127104.
I0302 19:00:23.892680 22699590365312 run.py:483] Algo bellman_ford step 3972 current loss 0.623200, current_train_items 127136.
I0302 19:00:23.924180 22699590365312 run.py:483] Algo bellman_ford step 3973 current loss 0.859832, current_train_items 127168.
I0302 19:00:23.956306 22699590365312 run.py:483] Algo bellman_ford step 3974 current loss 0.830773, current_train_items 127200.
I0302 19:00:23.976390 22699590365312 run.py:483] Algo bellman_ford step 3975 current loss 0.262651, current_train_items 127232.
I0302 19:00:23.992832 22699590365312 run.py:483] Algo bellman_ford step 3976 current loss 0.497189, current_train_items 127264.
I0302 19:00:24.015872 22699590365312 run.py:483] Algo bellman_ford step 3977 current loss 0.563600, current_train_items 127296.
I0302 19:00:24.047405 22699590365312 run.py:483] Algo bellman_ford step 3978 current loss 0.788533, current_train_items 127328.
I0302 19:00:24.080719 22699590365312 run.py:483] Algo bellman_ford step 3979 current loss 0.911211, current_train_items 127360.
I0302 19:00:24.100243 22699590365312 run.py:483] Algo bellman_ford step 3980 current loss 0.317712, current_train_items 127392.
I0302 19:00:24.116257 22699590365312 run.py:483] Algo bellman_ford step 3981 current loss 0.497956, current_train_items 127424.
I0302 19:00:24.139389 22699590365312 run.py:483] Algo bellman_ford step 3982 current loss 0.691771, current_train_items 127456.
I0302 19:00:24.170668 22699590365312 run.py:483] Algo bellman_ford step 3983 current loss 0.797499, current_train_items 127488.
I0302 19:00:24.203013 22699590365312 run.py:483] Algo bellman_ford step 3984 current loss 0.890076, current_train_items 127520.
I0302 19:00:24.223051 22699590365312 run.py:483] Algo bellman_ford step 3985 current loss 0.301343, current_train_items 127552.
I0302 19:00:24.239475 22699590365312 run.py:483] Algo bellman_ford step 3986 current loss 0.662544, current_train_items 127584.
I0302 19:00:24.262522 22699590365312 run.py:483] Algo bellman_ford step 3987 current loss 0.619146, current_train_items 127616.
I0302 19:00:24.293696 22699590365312 run.py:483] Algo bellman_ford step 3988 current loss 0.736774, current_train_items 127648.
I0302 19:00:24.327460 22699590365312 run.py:483] Algo bellman_ford step 3989 current loss 0.812072, current_train_items 127680.
I0302 19:00:24.347282 22699590365312 run.py:483] Algo bellman_ford step 3990 current loss 0.279745, current_train_items 127712.
I0302 19:00:24.363695 22699590365312 run.py:483] Algo bellman_ford step 3991 current loss 0.477871, current_train_items 127744.
I0302 19:00:24.386115 22699590365312 run.py:483] Algo bellman_ford step 3992 current loss 0.571556, current_train_items 127776.
I0302 19:00:24.416679 22699590365312 run.py:483] Algo bellman_ford step 3993 current loss 0.769764, current_train_items 127808.
I0302 19:00:24.450465 22699590365312 run.py:483] Algo bellman_ford step 3994 current loss 1.117325, current_train_items 127840.
I0302 19:00:24.470175 22699590365312 run.py:483] Algo bellman_ford step 3995 current loss 0.333749, current_train_items 127872.
I0302 19:00:24.486529 22699590365312 run.py:483] Algo bellman_ford step 3996 current loss 0.508605, current_train_items 127904.
I0302 19:00:24.510130 22699590365312 run.py:483] Algo bellman_ford step 3997 current loss 0.583820, current_train_items 127936.
I0302 19:00:24.540346 22699590365312 run.py:483] Algo bellman_ford step 3998 current loss 0.667570, current_train_items 127968.
I0302 19:00:24.575213 22699590365312 run.py:483] Algo bellman_ford step 3999 current loss 0.809735, current_train_items 128000.
I0302 19:00:24.595180 22699590365312 run.py:483] Algo bellman_ford step 4000 current loss 0.361740, current_train_items 128032.
I0302 19:00:24.602977 22699590365312 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0302 19:00:24.603080 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:00:24.619889 22699590365312 run.py:483] Algo bellman_ford step 4001 current loss 0.492231, current_train_items 128064.
I0302 19:00:24.644085 22699590365312 run.py:483] Algo bellman_ford step 4002 current loss 0.641896, current_train_items 128096.
I0302 19:00:24.675113 22699590365312 run.py:483] Algo bellman_ford step 4003 current loss 0.635278, current_train_items 128128.
I0302 19:00:24.707973 22699590365312 run.py:483] Algo bellman_ford step 4004 current loss 0.784239, current_train_items 128160.
I0302 19:00:24.727978 22699590365312 run.py:483] Algo bellman_ford step 4005 current loss 0.262625, current_train_items 128192.
I0302 19:00:24.743970 22699590365312 run.py:483] Algo bellman_ford step 4006 current loss 0.471784, current_train_items 128224.
I0302 19:00:24.767542 22699590365312 run.py:483] Algo bellman_ford step 4007 current loss 0.681055, current_train_items 128256.
I0302 19:00:24.799055 22699590365312 run.py:483] Algo bellman_ford step 4008 current loss 0.700192, current_train_items 128288.
I0302 19:00:24.831290 22699590365312 run.py:483] Algo bellman_ford step 4009 current loss 0.745517, current_train_items 128320.
I0302 19:00:24.851267 22699590365312 run.py:483] Algo bellman_ford step 4010 current loss 0.302160, current_train_items 128352.
I0302 19:00:24.867374 22699590365312 run.py:483] Algo bellman_ford step 4011 current loss 0.416220, current_train_items 128384.
I0302 19:00:24.889874 22699590365312 run.py:483] Algo bellman_ford step 4012 current loss 0.614177, current_train_items 128416.
I0302 19:00:24.920884 22699590365312 run.py:483] Algo bellman_ford step 4013 current loss 0.693414, current_train_items 128448.
I0302 19:00:24.954281 22699590365312 run.py:483] Algo bellman_ford step 4014 current loss 0.874631, current_train_items 128480.
I0302 19:00:24.974030 22699590365312 run.py:483] Algo bellman_ford step 4015 current loss 0.246688, current_train_items 128512.
I0302 19:00:24.990188 22699590365312 run.py:483] Algo bellman_ford step 4016 current loss 0.473302, current_train_items 128544.
I0302 19:00:25.013901 22699590365312 run.py:483] Algo bellman_ford step 4017 current loss 0.568040, current_train_items 128576.
I0302 19:00:25.044661 22699590365312 run.py:483] Algo bellman_ford step 4018 current loss 0.694300, current_train_items 128608.
I0302 19:00:25.077434 22699590365312 run.py:483] Algo bellman_ford step 4019 current loss 0.747918, current_train_items 128640.
I0302 19:00:25.097134 22699590365312 run.py:483] Algo bellman_ford step 4020 current loss 0.294301, current_train_items 128672.
I0302 19:00:25.113276 22699590365312 run.py:483] Algo bellman_ford step 4021 current loss 0.494660, current_train_items 128704.
I0302 19:00:25.137844 22699590365312 run.py:483] Algo bellman_ford step 4022 current loss 0.670655, current_train_items 128736.
I0302 19:00:25.167865 22699590365312 run.py:483] Algo bellman_ford step 4023 current loss 0.588657, current_train_items 128768.
I0302 19:00:25.204087 22699590365312 run.py:483] Algo bellman_ford step 4024 current loss 0.997387, current_train_items 128800.
I0302 19:00:25.223648 22699590365312 run.py:483] Algo bellman_ford step 4025 current loss 0.305018, current_train_items 128832.
I0302 19:00:25.240085 22699590365312 run.py:483] Algo bellman_ford step 4026 current loss 0.359612, current_train_items 128864.
I0302 19:00:25.263132 22699590365312 run.py:483] Algo bellman_ford step 4027 current loss 0.548381, current_train_items 128896.
I0302 19:00:25.293285 22699590365312 run.py:483] Algo bellman_ford step 4028 current loss 0.660758, current_train_items 128928.
I0302 19:00:25.325923 22699590365312 run.py:483] Algo bellman_ford step 4029 current loss 0.696475, current_train_items 128960.
I0302 19:00:25.345549 22699590365312 run.py:483] Algo bellman_ford step 4030 current loss 0.348746, current_train_items 128992.
I0302 19:00:25.361396 22699590365312 run.py:483] Algo bellman_ford step 4031 current loss 0.444827, current_train_items 129024.
I0302 19:00:25.385000 22699590365312 run.py:483] Algo bellman_ford step 4032 current loss 0.610031, current_train_items 129056.
I0302 19:00:25.417147 22699590365312 run.py:483] Algo bellman_ford step 4033 current loss 0.709705, current_train_items 129088.
I0302 19:00:25.450169 22699590365312 run.py:483] Algo bellman_ford step 4034 current loss 0.750522, current_train_items 129120.
I0302 19:00:25.469942 22699590365312 run.py:483] Algo bellman_ford step 4035 current loss 0.275470, current_train_items 129152.
I0302 19:00:25.485871 22699590365312 run.py:483] Algo bellman_ford step 4036 current loss 0.541372, current_train_items 129184.
I0302 19:00:25.509591 22699590365312 run.py:483] Algo bellman_ford step 4037 current loss 0.547952, current_train_items 129216.
I0302 19:00:25.541054 22699590365312 run.py:483] Algo bellman_ford step 4038 current loss 0.708122, current_train_items 129248.
I0302 19:00:25.574743 22699590365312 run.py:483] Algo bellman_ford step 4039 current loss 0.925779, current_train_items 129280.
I0302 19:00:25.594385 22699590365312 run.py:483] Algo bellman_ford step 4040 current loss 0.261912, current_train_items 129312.
I0302 19:00:25.610693 22699590365312 run.py:483] Algo bellman_ford step 4041 current loss 0.411711, current_train_items 129344.
I0302 19:00:25.633810 22699590365312 run.py:483] Algo bellman_ford step 4042 current loss 0.642360, current_train_items 129376.
I0302 19:00:25.665961 22699590365312 run.py:483] Algo bellman_ford step 4043 current loss 0.753977, current_train_items 129408.
I0302 19:00:25.698373 22699590365312 run.py:483] Algo bellman_ford step 4044 current loss 0.724503, current_train_items 129440.
I0302 19:00:25.717717 22699590365312 run.py:483] Algo bellman_ford step 4045 current loss 0.339344, current_train_items 129472.
I0302 19:00:25.734194 22699590365312 run.py:483] Algo bellman_ford step 4046 current loss 0.547645, current_train_items 129504.
I0302 19:00:25.757458 22699590365312 run.py:483] Algo bellman_ford step 4047 current loss 0.633654, current_train_items 129536.
I0302 19:00:25.787303 22699590365312 run.py:483] Algo bellman_ford step 4048 current loss 0.639061, current_train_items 129568.
I0302 19:00:25.821718 22699590365312 run.py:483] Algo bellman_ford step 4049 current loss 0.761896, current_train_items 129600.
I0302 19:00:25.841464 22699590365312 run.py:483] Algo bellman_ford step 4050 current loss 0.291498, current_train_items 129632.
I0302 19:00:25.849390 22699590365312 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0302 19:00:25.849495 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 19:00:25.866685 22699590365312 run.py:483] Algo bellman_ford step 4051 current loss 0.526722, current_train_items 129664.
I0302 19:00:25.890014 22699590365312 run.py:483] Algo bellman_ford step 4052 current loss 0.613464, current_train_items 129696.
I0302 19:00:25.921114 22699590365312 run.py:483] Algo bellman_ford step 4053 current loss 0.720299, current_train_items 129728.
I0302 19:00:25.956671 22699590365312 run.py:483] Algo bellman_ford step 4054 current loss 0.828852, current_train_items 129760.
I0302 19:00:25.977015 22699590365312 run.py:483] Algo bellman_ford step 4055 current loss 0.373725, current_train_items 129792.
I0302 19:00:25.992816 22699590365312 run.py:483] Algo bellman_ford step 4056 current loss 0.411593, current_train_items 129824.
I0302 19:00:26.016177 22699590365312 run.py:483] Algo bellman_ford step 4057 current loss 0.556048, current_train_items 129856.
I0302 19:00:26.046400 22699590365312 run.py:483] Algo bellman_ford step 4058 current loss 0.648775, current_train_items 129888.
I0302 19:00:26.078628 22699590365312 run.py:483] Algo bellman_ford step 4059 current loss 0.748053, current_train_items 129920.
I0302 19:00:26.098560 22699590365312 run.py:483] Algo bellman_ford step 4060 current loss 0.327823, current_train_items 129952.
I0302 19:00:26.115245 22699590365312 run.py:483] Algo bellman_ford step 4061 current loss 0.395328, current_train_items 129984.
I0302 19:00:26.137504 22699590365312 run.py:483] Algo bellman_ford step 4062 current loss 0.585263, current_train_items 130016.
I0302 19:00:26.167372 22699590365312 run.py:483] Algo bellman_ford step 4063 current loss 0.585693, current_train_items 130048.
I0302 19:00:26.202301 22699590365312 run.py:483] Algo bellman_ford step 4064 current loss 0.851642, current_train_items 130080.
I0302 19:00:26.221579 22699590365312 run.py:483] Algo bellman_ford step 4065 current loss 0.276267, current_train_items 130112.
I0302 19:00:26.237289 22699590365312 run.py:483] Algo bellman_ford step 4066 current loss 0.583298, current_train_items 130144.
I0302 19:00:26.262273 22699590365312 run.py:483] Algo bellman_ford step 4067 current loss 0.713821, current_train_items 130176.
I0302 19:00:26.292685 22699590365312 run.py:483] Algo bellman_ford step 4068 current loss 0.590029, current_train_items 130208.
I0302 19:00:26.327578 22699590365312 run.py:483] Algo bellman_ford step 4069 current loss 0.790900, current_train_items 130240.
I0302 19:00:26.347452 22699590365312 run.py:483] Algo bellman_ford step 4070 current loss 0.362088, current_train_items 130272.
I0302 19:00:26.364005 22699590365312 run.py:483] Algo bellman_ford step 4071 current loss 0.466693, current_train_items 130304.
I0302 19:00:26.387273 22699590365312 run.py:483] Algo bellman_ford step 4072 current loss 0.661246, current_train_items 130336.
I0302 19:00:26.416499 22699590365312 run.py:483] Algo bellman_ford step 4073 current loss 0.730941, current_train_items 130368.
I0302 19:00:26.449075 22699590365312 run.py:483] Algo bellman_ford step 4074 current loss 0.717961, current_train_items 130400.
I0302 19:00:26.468879 22699590365312 run.py:483] Algo bellman_ford step 4075 current loss 0.271841, current_train_items 130432.
I0302 19:00:26.485536 22699590365312 run.py:483] Algo bellman_ford step 4076 current loss 0.502040, current_train_items 130464.
I0302 19:00:26.509637 22699590365312 run.py:483] Algo bellman_ford step 4077 current loss 0.698520, current_train_items 130496.
I0302 19:00:26.539783 22699590365312 run.py:483] Algo bellman_ford step 4078 current loss 0.695833, current_train_items 130528.
I0302 19:00:26.572495 22699590365312 run.py:483] Algo bellman_ford step 4079 current loss 0.682091, current_train_items 130560.
I0302 19:00:26.591967 22699590365312 run.py:483] Algo bellman_ford step 4080 current loss 0.294713, current_train_items 130592.
I0302 19:00:26.608463 22699590365312 run.py:483] Algo bellman_ford step 4081 current loss 0.375934, current_train_items 130624.
I0302 19:00:26.632546 22699590365312 run.py:483] Algo bellman_ford step 4082 current loss 0.681797, current_train_items 130656.
I0302 19:00:26.664806 22699590365312 run.py:483] Algo bellman_ford step 4083 current loss 0.662677, current_train_items 130688.
I0302 19:00:26.695945 22699590365312 run.py:483] Algo bellman_ford step 4084 current loss 0.746320, current_train_items 130720.
I0302 19:00:26.715969 22699590365312 run.py:483] Algo bellman_ford step 4085 current loss 0.303594, current_train_items 130752.
I0302 19:00:26.732261 22699590365312 run.py:483] Algo bellman_ford step 4086 current loss 0.459050, current_train_items 130784.
I0302 19:00:26.755044 22699590365312 run.py:483] Algo bellman_ford step 4087 current loss 0.537495, current_train_items 130816.
I0302 19:00:26.785266 22699590365312 run.py:483] Algo bellman_ford step 4088 current loss 0.667022, current_train_items 130848.
I0302 19:00:26.817177 22699590365312 run.py:483] Algo bellman_ford step 4089 current loss 0.758386, current_train_items 130880.
I0302 19:00:26.836908 22699590365312 run.py:483] Algo bellman_ford step 4090 current loss 0.279447, current_train_items 130912.
I0302 19:00:26.853026 22699590365312 run.py:483] Algo bellman_ford step 4091 current loss 0.391809, current_train_items 130944.
I0302 19:00:26.876990 22699590365312 run.py:483] Algo bellman_ford step 4092 current loss 0.612091, current_train_items 130976.
I0302 19:00:26.908492 22699590365312 run.py:483] Algo bellman_ford step 4093 current loss 0.755121, current_train_items 131008.
I0302 19:00:26.942308 22699590365312 run.py:483] Algo bellman_ford step 4094 current loss 0.895183, current_train_items 131040.
I0302 19:00:26.961818 22699590365312 run.py:483] Algo bellman_ford step 4095 current loss 0.310196, current_train_items 131072.
I0302 19:00:26.978281 22699590365312 run.py:483] Algo bellman_ford step 4096 current loss 0.442138, current_train_items 131104.
I0302 19:00:27.002326 22699590365312 run.py:483] Algo bellman_ford step 4097 current loss 0.555518, current_train_items 131136.
I0302 19:00:27.033228 22699590365312 run.py:483] Algo bellman_ford step 4098 current loss 0.654507, current_train_items 131168.
I0302 19:00:27.067574 22699590365312 run.py:483] Algo bellman_ford step 4099 current loss 0.856554, current_train_items 131200.
I0302 19:00:27.087456 22699590365312 run.py:483] Algo bellman_ford step 4100 current loss 0.311060, current_train_items 131232.
I0302 19:00:27.095352 22699590365312 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0302 19:00:27.095459 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:00:27.111722 22699590365312 run.py:483] Algo bellman_ford step 4101 current loss 0.419508, current_train_items 131264.
I0302 19:00:27.134068 22699590365312 run.py:483] Algo bellman_ford step 4102 current loss 0.655473, current_train_items 131296.
I0302 19:00:27.166292 22699590365312 run.py:483] Algo bellman_ford step 4103 current loss 0.782905, current_train_items 131328.
I0302 19:00:27.201735 22699590365312 run.py:483] Algo bellman_ford step 4104 current loss 0.784773, current_train_items 131360.
I0302 19:00:27.221455 22699590365312 run.py:483] Algo bellman_ford step 4105 current loss 0.269994, current_train_items 131392.
I0302 19:00:27.237574 22699590365312 run.py:483] Algo bellman_ford step 4106 current loss 0.433236, current_train_items 131424.
I0302 19:00:27.261516 22699590365312 run.py:483] Algo bellman_ford step 4107 current loss 0.677871, current_train_items 131456.
I0302 19:00:27.292623 22699590365312 run.py:483] Algo bellman_ford step 4108 current loss 0.675330, current_train_items 131488.
I0302 19:00:27.328524 22699590365312 run.py:483] Algo bellman_ford step 4109 current loss 0.855217, current_train_items 131520.
I0302 19:00:27.348278 22699590365312 run.py:483] Algo bellman_ford step 4110 current loss 0.349102, current_train_items 131552.
I0302 19:00:27.364876 22699590365312 run.py:483] Algo bellman_ford step 4111 current loss 0.522015, current_train_items 131584.
I0302 19:00:27.387587 22699590365312 run.py:483] Algo bellman_ford step 4112 current loss 0.625436, current_train_items 131616.
I0302 19:00:27.419105 22699590365312 run.py:483] Algo bellman_ford step 4113 current loss 0.630266, current_train_items 131648.
I0302 19:00:27.453444 22699590365312 run.py:483] Algo bellman_ford step 4114 current loss 0.788602, current_train_items 131680.
I0302 19:00:27.472779 22699590365312 run.py:483] Algo bellman_ford step 4115 current loss 0.244988, current_train_items 131712.
I0302 19:00:27.489284 22699590365312 run.py:483] Algo bellman_ford step 4116 current loss 0.556057, current_train_items 131744.
I0302 19:00:27.513917 22699590365312 run.py:483] Algo bellman_ford step 4117 current loss 0.663802, current_train_items 131776.
I0302 19:00:27.544129 22699590365312 run.py:483] Algo bellman_ford step 4118 current loss 0.641712, current_train_items 131808.
I0302 19:00:27.578135 22699590365312 run.py:483] Algo bellman_ford step 4119 current loss 0.911156, current_train_items 131840.
I0302 19:00:27.597786 22699590365312 run.py:483] Algo bellman_ford step 4120 current loss 0.278812, current_train_items 131872.
I0302 19:00:27.614050 22699590365312 run.py:483] Algo bellman_ford step 4121 current loss 0.439897, current_train_items 131904.
I0302 19:00:27.638622 22699590365312 run.py:483] Algo bellman_ford step 4122 current loss 0.658083, current_train_items 131936.
I0302 19:00:27.670347 22699590365312 run.py:483] Algo bellman_ford step 4123 current loss 0.783877, current_train_items 131968.
I0302 19:00:27.704198 22699590365312 run.py:483] Algo bellman_ford step 4124 current loss 0.822267, current_train_items 132000.
I0302 19:00:27.723569 22699590365312 run.py:483] Algo bellman_ford step 4125 current loss 0.230927, current_train_items 132032.
I0302 19:00:27.739836 22699590365312 run.py:483] Algo bellman_ford step 4126 current loss 0.528980, current_train_items 132064.
I0302 19:00:27.764299 22699590365312 run.py:483] Algo bellman_ford step 4127 current loss 0.912186, current_train_items 132096.
I0302 19:00:27.794566 22699590365312 run.py:483] Algo bellman_ford step 4128 current loss 0.845224, current_train_items 132128.
I0302 19:00:27.827173 22699590365312 run.py:483] Algo bellman_ford step 4129 current loss 0.757284, current_train_items 132160.
I0302 19:00:27.846853 22699590365312 run.py:483] Algo bellman_ford step 4130 current loss 0.336296, current_train_items 132192.
I0302 19:00:27.863279 22699590365312 run.py:483] Algo bellman_ford step 4131 current loss 0.431461, current_train_items 132224.
I0302 19:00:27.886669 22699590365312 run.py:483] Algo bellman_ford step 4132 current loss 0.690427, current_train_items 132256.
I0302 19:00:27.918490 22699590365312 run.py:483] Algo bellman_ford step 4133 current loss 0.792138, current_train_items 132288.
I0302 19:00:27.953288 22699590365312 run.py:483] Algo bellman_ford step 4134 current loss 0.695309, current_train_items 132320.
I0302 19:00:27.972988 22699590365312 run.py:483] Algo bellman_ford step 4135 current loss 0.261398, current_train_items 132352.
I0302 19:00:27.989062 22699590365312 run.py:483] Algo bellman_ford step 4136 current loss 0.432906, current_train_items 132384.
I0302 19:00:28.012148 22699590365312 run.py:483] Algo bellman_ford step 4137 current loss 0.664965, current_train_items 132416.
I0302 19:00:28.042710 22699590365312 run.py:483] Algo bellman_ford step 4138 current loss 0.607120, current_train_items 132448.
I0302 19:00:28.076886 22699590365312 run.py:483] Algo bellman_ford step 4139 current loss 0.771929, current_train_items 132480.
I0302 19:00:28.096588 22699590365312 run.py:483] Algo bellman_ford step 4140 current loss 0.309199, current_train_items 132512.
I0302 19:00:28.112407 22699590365312 run.py:483] Algo bellman_ford step 4141 current loss 0.427126, current_train_items 132544.
I0302 19:00:28.136821 22699590365312 run.py:483] Algo bellman_ford step 4142 current loss 0.702958, current_train_items 132576.
I0302 19:00:28.166853 22699590365312 run.py:483] Algo bellman_ford step 4143 current loss 0.735242, current_train_items 132608.
I0302 19:00:28.199956 22699590365312 run.py:483] Algo bellman_ford step 4144 current loss 0.827153, current_train_items 132640.
I0302 19:00:28.219388 22699590365312 run.py:483] Algo bellman_ford step 4145 current loss 0.278080, current_train_items 132672.
I0302 19:00:28.235218 22699590365312 run.py:483] Algo bellman_ford step 4146 current loss 0.379790, current_train_items 132704.
I0302 19:00:28.258686 22699590365312 run.py:483] Algo bellman_ford step 4147 current loss 0.634584, current_train_items 132736.
I0302 19:00:28.288136 22699590365312 run.py:483] Algo bellman_ford step 4148 current loss 0.607751, current_train_items 132768.
I0302 19:00:28.320742 22699590365312 run.py:483] Algo bellman_ford step 4149 current loss 0.755853, current_train_items 132800.
I0302 19:00:28.340538 22699590365312 run.py:483] Algo bellman_ford step 4150 current loss 0.318639, current_train_items 132832.
I0302 19:00:28.348577 22699590365312 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0302 19:00:28.348683 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 19:00:28.365464 22699590365312 run.py:483] Algo bellman_ford step 4151 current loss 0.486895, current_train_items 132864.
I0302 19:00:28.389525 22699590365312 run.py:483] Algo bellman_ford step 4152 current loss 0.607123, current_train_items 132896.
I0302 19:00:28.420358 22699590365312 run.py:483] Algo bellman_ford step 4153 current loss 0.616523, current_train_items 132928.
I0302 19:00:28.455680 22699590365312 run.py:483] Algo bellman_ford step 4154 current loss 0.830083, current_train_items 132960.
I0302 19:00:28.475645 22699590365312 run.py:483] Algo bellman_ford step 4155 current loss 0.318872, current_train_items 132992.
I0302 19:00:28.491910 22699590365312 run.py:483] Algo bellman_ford step 4156 current loss 0.428404, current_train_items 133024.
I0302 19:00:28.514646 22699590365312 run.py:483] Algo bellman_ford step 4157 current loss 0.531219, current_train_items 133056.
I0302 19:00:28.544557 22699590365312 run.py:483] Algo bellman_ford step 4158 current loss 0.689046, current_train_items 133088.
I0302 19:00:28.578112 22699590365312 run.py:483] Algo bellman_ford step 4159 current loss 0.920861, current_train_items 133120.
I0302 19:00:28.597906 22699590365312 run.py:483] Algo bellman_ford step 4160 current loss 0.265039, current_train_items 133152.
I0302 19:00:28.614250 22699590365312 run.py:483] Algo bellman_ford step 4161 current loss 0.425985, current_train_items 133184.
I0302 19:00:28.636218 22699590365312 run.py:483] Algo bellman_ford step 4162 current loss 0.577345, current_train_items 133216.
I0302 19:00:28.667143 22699590365312 run.py:483] Algo bellman_ford step 4163 current loss 0.675025, current_train_items 133248.
I0302 19:00:28.701425 22699590365312 run.py:483] Algo bellman_ford step 4164 current loss 0.742790, current_train_items 133280.
I0302 19:00:28.720945 22699590365312 run.py:483] Algo bellman_ford step 4165 current loss 0.364692, current_train_items 133312.
I0302 19:00:28.736967 22699590365312 run.py:483] Algo bellman_ford step 4166 current loss 0.410540, current_train_items 133344.
I0302 19:00:28.761205 22699590365312 run.py:483] Algo bellman_ford step 4167 current loss 0.749498, current_train_items 133376.
I0302 19:00:28.791646 22699590365312 run.py:483] Algo bellman_ford step 4168 current loss 0.671015, current_train_items 133408.
I0302 19:00:28.824428 22699590365312 run.py:483] Algo bellman_ford step 4169 current loss 0.777910, current_train_items 133440.
I0302 19:00:28.844130 22699590365312 run.py:483] Algo bellman_ford step 4170 current loss 0.277839, current_train_items 133472.
I0302 19:00:28.859734 22699590365312 run.py:483] Algo bellman_ford step 4171 current loss 0.422620, current_train_items 133504.
I0302 19:00:28.882650 22699590365312 run.py:483] Algo bellman_ford step 4172 current loss 0.607686, current_train_items 133536.
I0302 19:00:28.912943 22699590365312 run.py:483] Algo bellman_ford step 4173 current loss 0.585150, current_train_items 133568.
I0302 19:00:28.948008 22699590365312 run.py:483] Algo bellman_ford step 4174 current loss 0.838629, current_train_items 133600.
I0302 19:00:28.967673 22699590365312 run.py:483] Algo bellman_ford step 4175 current loss 0.265265, current_train_items 133632.
I0302 19:00:28.984448 22699590365312 run.py:483] Algo bellman_ford step 4176 current loss 0.479317, current_train_items 133664.
I0302 19:00:29.007624 22699590365312 run.py:483] Algo bellman_ford step 4177 current loss 0.600709, current_train_items 133696.
I0302 19:00:29.038465 22699590365312 run.py:483] Algo bellman_ford step 4178 current loss 0.708878, current_train_items 133728.
I0302 19:00:29.072141 22699590365312 run.py:483] Algo bellman_ford step 4179 current loss 0.828723, current_train_items 133760.
I0302 19:00:29.091591 22699590365312 run.py:483] Algo bellman_ford step 4180 current loss 0.292024, current_train_items 133792.
I0302 19:00:29.107754 22699590365312 run.py:483] Algo bellman_ford step 4181 current loss 0.430691, current_train_items 133824.
I0302 19:00:29.132519 22699590365312 run.py:483] Algo bellman_ford step 4182 current loss 0.552201, current_train_items 133856.
I0302 19:00:29.161537 22699590365312 run.py:483] Algo bellman_ford step 4183 current loss 0.643783, current_train_items 133888.
I0302 19:00:29.195331 22699590365312 run.py:483] Algo bellman_ford step 4184 current loss 0.797923, current_train_items 133920.
I0302 19:00:29.215024 22699590365312 run.py:483] Algo bellman_ford step 4185 current loss 0.294996, current_train_items 133952.
I0302 19:00:29.231049 22699590365312 run.py:483] Algo bellman_ford step 4186 current loss 0.503649, current_train_items 133984.
I0302 19:00:29.254062 22699590365312 run.py:483] Algo bellman_ford step 4187 current loss 0.675583, current_train_items 134016.
I0302 19:00:29.284229 22699590365312 run.py:483] Algo bellman_ford step 4188 current loss 0.764518, current_train_items 134048.
I0302 19:00:29.316479 22699590365312 run.py:483] Algo bellman_ford step 4189 current loss 0.776303, current_train_items 134080.
I0302 19:00:29.336495 22699590365312 run.py:483] Algo bellman_ford step 4190 current loss 0.328056, current_train_items 134112.
I0302 19:00:29.353028 22699590365312 run.py:483] Algo bellman_ford step 4191 current loss 0.448625, current_train_items 134144.
I0302 19:00:29.373848 22699590365312 run.py:483] Algo bellman_ford step 4192 current loss 0.507773, current_train_items 134176.
I0302 19:00:29.405231 22699590365312 run.py:483] Algo bellman_ford step 4193 current loss 0.678031, current_train_items 134208.
I0302 19:00:29.440769 22699590365312 run.py:483] Algo bellman_ford step 4194 current loss 0.859630, current_train_items 134240.
I0302 19:00:29.460016 22699590365312 run.py:483] Algo bellman_ford step 4195 current loss 0.309883, current_train_items 134272.
I0302 19:00:29.476078 22699590365312 run.py:483] Algo bellman_ford step 4196 current loss 0.533700, current_train_items 134304.
I0302 19:00:29.499655 22699590365312 run.py:483] Algo bellman_ford step 4197 current loss 0.704625, current_train_items 134336.
I0302 19:00:29.530108 22699590365312 run.py:483] Algo bellman_ford step 4198 current loss 0.741846, current_train_items 134368.
I0302 19:00:29.563026 22699590365312 run.py:483] Algo bellman_ford step 4199 current loss 0.783573, current_train_items 134400.
I0302 19:00:29.582432 22699590365312 run.py:483] Algo bellman_ford step 4200 current loss 0.279671, current_train_items 134432.
I0302 19:00:29.590379 22699590365312 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0302 19:00:29.590486 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:00:29.607595 22699590365312 run.py:483] Algo bellman_ford step 4201 current loss 0.587982, current_train_items 134464.
I0302 19:00:29.630777 22699590365312 run.py:483] Algo bellman_ford step 4202 current loss 0.633550, current_train_items 134496.
I0302 19:00:29.661711 22699590365312 run.py:483] Algo bellman_ford step 4203 current loss 0.795903, current_train_items 134528.
I0302 19:00:29.695826 22699590365312 run.py:483] Algo bellman_ford step 4204 current loss 0.981573, current_train_items 134560.
I0302 19:00:29.715958 22699590365312 run.py:483] Algo bellman_ford step 4205 current loss 0.331159, current_train_items 134592.
I0302 19:00:29.732121 22699590365312 run.py:483] Algo bellman_ford step 4206 current loss 0.555298, current_train_items 134624.
I0302 19:00:29.756069 22699590365312 run.py:483] Algo bellman_ford step 4207 current loss 0.584284, current_train_items 134656.
I0302 19:00:29.787628 22699590365312 run.py:483] Algo bellman_ford step 4208 current loss 0.690391, current_train_items 134688.
I0302 19:00:29.822275 22699590365312 run.py:483] Algo bellman_ford step 4209 current loss 0.913421, current_train_items 134720.
I0302 19:00:29.842124 22699590365312 run.py:483] Algo bellman_ford step 4210 current loss 0.270290, current_train_items 134752.
I0302 19:00:29.858292 22699590365312 run.py:483] Algo bellman_ford step 4211 current loss 0.444311, current_train_items 134784.
I0302 19:00:29.882083 22699590365312 run.py:483] Algo bellman_ford step 4212 current loss 0.639398, current_train_items 134816.
I0302 19:00:29.913272 22699590365312 run.py:483] Algo bellman_ford step 4213 current loss 0.642922, current_train_items 134848.
I0302 19:00:29.946472 22699590365312 run.py:483] Algo bellman_ford step 4214 current loss 0.771756, current_train_items 134880.
I0302 19:00:29.966116 22699590365312 run.py:483] Algo bellman_ford step 4215 current loss 0.358465, current_train_items 134912.
I0302 19:00:29.982350 22699590365312 run.py:483] Algo bellman_ford step 4216 current loss 0.455018, current_train_items 134944.
I0302 19:00:30.005170 22699590365312 run.py:483] Algo bellman_ford step 4217 current loss 0.565023, current_train_items 134976.
I0302 19:00:30.035620 22699590365312 run.py:483] Algo bellman_ford step 4218 current loss 0.660663, current_train_items 135008.
I0302 19:00:30.068758 22699590365312 run.py:483] Algo bellman_ford step 4219 current loss 0.738181, current_train_items 135040.
I0302 19:00:30.088583 22699590365312 run.py:483] Algo bellman_ford step 4220 current loss 0.278202, current_train_items 135072.
I0302 19:00:30.104875 22699590365312 run.py:483] Algo bellman_ford step 4221 current loss 0.469125, current_train_items 135104.
I0302 19:00:30.128890 22699590365312 run.py:483] Algo bellman_ford step 4222 current loss 0.579114, current_train_items 135136.
I0302 19:00:30.160876 22699590365312 run.py:483] Algo bellman_ford step 4223 current loss 0.692156, current_train_items 135168.
I0302 19:00:30.195934 22699590365312 run.py:483] Algo bellman_ford step 4224 current loss 0.795078, current_train_items 135200.
I0302 19:00:30.215830 22699590365312 run.py:483] Algo bellman_ford step 4225 current loss 0.283081, current_train_items 135232.
I0302 19:00:30.231754 22699590365312 run.py:483] Algo bellman_ford step 4226 current loss 0.432588, current_train_items 135264.
I0302 19:00:30.254660 22699590365312 run.py:483] Algo bellman_ford step 4227 current loss 0.588847, current_train_items 135296.
I0302 19:00:30.284367 22699590365312 run.py:483] Algo bellman_ford step 4228 current loss 0.708710, current_train_items 135328.
I0302 19:00:30.316043 22699590365312 run.py:483] Algo bellman_ford step 4229 current loss 0.662974, current_train_items 135360.
I0302 19:00:30.335321 22699590365312 run.py:483] Algo bellman_ford step 4230 current loss 0.342817, current_train_items 135392.
I0302 19:00:30.351304 22699590365312 run.py:483] Algo bellman_ford step 4231 current loss 0.436822, current_train_items 135424.
I0302 19:00:30.375017 22699590365312 run.py:483] Algo bellman_ford step 4232 current loss 0.595839, current_train_items 135456.
I0302 19:00:30.405785 22699590365312 run.py:483] Algo bellman_ford step 4233 current loss 0.650696, current_train_items 135488.
I0302 19:00:30.438651 22699590365312 run.py:483] Algo bellman_ford step 4234 current loss 0.809826, current_train_items 135520.
I0302 19:00:30.458477 22699590365312 run.py:483] Algo bellman_ford step 4235 current loss 0.275372, current_train_items 135552.
I0302 19:00:30.475216 22699590365312 run.py:483] Algo bellman_ford step 4236 current loss 0.540702, current_train_items 135584.
I0302 19:00:30.498512 22699590365312 run.py:483] Algo bellman_ford step 4237 current loss 0.620655, current_train_items 135616.
I0302 19:00:30.529561 22699590365312 run.py:483] Algo bellman_ford step 4238 current loss 0.637987, current_train_items 135648.
I0302 19:00:30.564701 22699590365312 run.py:483] Algo bellman_ford step 4239 current loss 0.877410, current_train_items 135680.
I0302 19:00:30.584515 22699590365312 run.py:483] Algo bellman_ford step 4240 current loss 0.264242, current_train_items 135712.
I0302 19:00:30.600480 22699590365312 run.py:483] Algo bellman_ford step 4241 current loss 0.412609, current_train_items 135744.
I0302 19:00:30.623291 22699590365312 run.py:483] Algo bellman_ford step 4242 current loss 0.536281, current_train_items 135776.
I0302 19:00:30.654834 22699590365312 run.py:483] Algo bellman_ford step 4243 current loss 0.757286, current_train_items 135808.
I0302 19:00:30.687277 22699590365312 run.py:483] Algo bellman_ford step 4244 current loss 0.869397, current_train_items 135840.
I0302 19:00:30.706597 22699590365312 run.py:483] Algo bellman_ford step 4245 current loss 0.287890, current_train_items 135872.
I0302 19:00:30.722419 22699590365312 run.py:483] Algo bellman_ford step 4246 current loss 0.423146, current_train_items 135904.
I0302 19:00:30.747172 22699590365312 run.py:483] Algo bellman_ford step 4247 current loss 0.602757, current_train_items 135936.
I0302 19:00:30.778665 22699590365312 run.py:483] Algo bellman_ford step 4248 current loss 0.605123, current_train_items 135968.
I0302 19:00:30.813961 22699590365312 run.py:483] Algo bellman_ford step 4249 current loss 0.774072, current_train_items 136000.
I0302 19:00:30.833456 22699590365312 run.py:483] Algo bellman_ford step 4250 current loss 0.370998, current_train_items 136032.
I0302 19:00:30.841577 22699590365312 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0302 19:00:30.841683 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:00:30.858453 22699590365312 run.py:483] Algo bellman_ford step 4251 current loss 0.419852, current_train_items 136064.
I0302 19:00:30.882308 22699590365312 run.py:483] Algo bellman_ford step 4252 current loss 0.631353, current_train_items 136096.
I0302 19:00:30.913822 22699590365312 run.py:483] Algo bellman_ford step 4253 current loss 0.703404, current_train_items 136128.
I0302 19:00:30.947509 22699590365312 run.py:483] Algo bellman_ford step 4254 current loss 0.772696, current_train_items 136160.
I0302 19:00:30.967447 22699590365312 run.py:483] Algo bellman_ford step 4255 current loss 0.353452, current_train_items 136192.
I0302 19:00:30.983232 22699590365312 run.py:483] Algo bellman_ford step 4256 current loss 0.462879, current_train_items 136224.
I0302 19:00:31.005883 22699590365312 run.py:483] Algo bellman_ford step 4257 current loss 0.505381, current_train_items 136256.
I0302 19:00:31.036926 22699590365312 run.py:483] Algo bellman_ford step 4258 current loss 0.727804, current_train_items 136288.
I0302 19:00:31.071280 22699590365312 run.py:483] Algo bellman_ford step 4259 current loss 0.734334, current_train_items 136320.
I0302 19:00:31.091214 22699590365312 run.py:483] Algo bellman_ford step 4260 current loss 0.279155, current_train_items 136352.
I0302 19:00:31.107867 22699590365312 run.py:483] Algo bellman_ford step 4261 current loss 0.560972, current_train_items 136384.
I0302 19:00:31.131084 22699590365312 run.py:483] Algo bellman_ford step 4262 current loss 0.589610, current_train_items 136416.
I0302 19:00:31.162078 22699590365312 run.py:483] Algo bellman_ford step 4263 current loss 0.669201, current_train_items 136448.
I0302 19:00:31.196078 22699590365312 run.py:483] Algo bellman_ford step 4264 current loss 0.714520, current_train_items 136480.
I0302 19:00:31.215892 22699590365312 run.py:483] Algo bellman_ford step 4265 current loss 0.385244, current_train_items 136512.
I0302 19:00:31.232170 22699590365312 run.py:483] Algo bellman_ford step 4266 current loss 0.560220, current_train_items 136544.
I0302 19:00:31.255586 22699590365312 run.py:483] Algo bellman_ford step 4267 current loss 0.627541, current_train_items 136576.
I0302 19:00:31.285740 22699590365312 run.py:483] Algo bellman_ford step 4268 current loss 0.634625, current_train_items 136608.
I0302 19:00:31.318601 22699590365312 run.py:483] Algo bellman_ford step 4269 current loss 0.813743, current_train_items 136640.
I0302 19:00:31.338547 22699590365312 run.py:483] Algo bellman_ford step 4270 current loss 0.363902, current_train_items 136672.
I0302 19:00:31.354565 22699590365312 run.py:483] Algo bellman_ford step 4271 current loss 0.466598, current_train_items 136704.
I0302 19:00:31.376866 22699590365312 run.py:483] Algo bellman_ford step 4272 current loss 0.546058, current_train_items 136736.
I0302 19:00:31.407259 22699590365312 run.py:483] Algo bellman_ford step 4273 current loss 0.689834, current_train_items 136768.
I0302 19:00:31.439734 22699590365312 run.py:483] Algo bellman_ford step 4274 current loss 0.770693, current_train_items 136800.
I0302 19:00:31.459590 22699590365312 run.py:483] Algo bellman_ford step 4275 current loss 0.275381, current_train_items 136832.
I0302 19:00:31.475768 22699590365312 run.py:483] Algo bellman_ford step 4276 current loss 0.384118, current_train_items 136864.
I0302 19:00:31.498726 22699590365312 run.py:483] Algo bellman_ford step 4277 current loss 0.633893, current_train_items 136896.
I0302 19:00:31.531456 22699590365312 run.py:483] Algo bellman_ford step 4278 current loss 0.708323, current_train_items 136928.
I0302 19:00:31.565560 22699590365312 run.py:483] Algo bellman_ford step 4279 current loss 0.766657, current_train_items 136960.
I0302 19:00:31.585525 22699590365312 run.py:483] Algo bellman_ford step 4280 current loss 0.265947, current_train_items 136992.
I0302 19:00:31.601630 22699590365312 run.py:483] Algo bellman_ford step 4281 current loss 0.416736, current_train_items 137024.
I0302 19:00:31.625205 22699590365312 run.py:483] Algo bellman_ford step 4282 current loss 0.528985, current_train_items 137056.
I0302 19:00:31.655087 22699590365312 run.py:483] Algo bellman_ford step 4283 current loss 0.674762, current_train_items 137088.
I0302 19:00:31.690638 22699590365312 run.py:483] Algo bellman_ford step 4284 current loss 0.824557, current_train_items 137120.
I0302 19:00:31.710400 22699590365312 run.py:483] Algo bellman_ford step 4285 current loss 0.291062, current_train_items 137152.
I0302 19:00:31.726405 22699590365312 run.py:483] Algo bellman_ford step 4286 current loss 0.434167, current_train_items 137184.
I0302 19:00:31.749626 22699590365312 run.py:483] Algo bellman_ford step 4287 current loss 0.682208, current_train_items 137216.
I0302 19:00:31.781818 22699590365312 run.py:483] Algo bellman_ford step 4288 current loss 0.816717, current_train_items 137248.
I0302 19:00:31.815632 22699590365312 run.py:483] Algo bellman_ford step 4289 current loss 1.116222, current_train_items 137280.
I0302 19:00:31.835190 22699590365312 run.py:483] Algo bellman_ford step 4290 current loss 0.343477, current_train_items 137312.
I0302 19:00:31.851799 22699590365312 run.py:483] Algo bellman_ford step 4291 current loss 0.480554, current_train_items 137344.
I0302 19:00:31.876170 22699590365312 run.py:483] Algo bellman_ford step 4292 current loss 0.633112, current_train_items 137376.
I0302 19:00:31.908926 22699590365312 run.py:483] Algo bellman_ford step 4293 current loss 0.836752, current_train_items 137408.
I0302 19:00:31.944357 22699590365312 run.py:483] Algo bellman_ford step 4294 current loss 0.859764, current_train_items 137440.
I0302 19:00:31.964149 22699590365312 run.py:483] Algo bellman_ford step 4295 current loss 0.280373, current_train_items 137472.
I0302 19:00:31.980094 22699590365312 run.py:483] Algo bellman_ford step 4296 current loss 0.520908, current_train_items 137504.
I0302 19:00:32.002889 22699590365312 run.py:483] Algo bellman_ford step 4297 current loss 0.844804, current_train_items 137536.
I0302 19:00:32.033836 22699590365312 run.py:483] Algo bellman_ford step 4298 current loss 0.666841, current_train_items 137568.
I0302 19:00:32.064147 22699590365312 run.py:483] Algo bellman_ford step 4299 current loss 0.638947, current_train_items 137600.
I0302 19:00:32.083955 22699590365312 run.py:483] Algo bellman_ford step 4300 current loss 0.337814, current_train_items 137632.
I0302 19:00:32.091979 22699590365312 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0302 19:00:32.092084 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:00:32.108795 22699590365312 run.py:483] Algo bellman_ford step 4301 current loss 0.626953, current_train_items 137664.
I0302 19:00:32.132507 22699590365312 run.py:483] Algo bellman_ford step 4302 current loss 0.656260, current_train_items 137696.
I0302 19:00:32.165074 22699590365312 run.py:483] Algo bellman_ford step 4303 current loss 0.939083, current_train_items 137728.
I0302 19:00:32.201094 22699590365312 run.py:483] Algo bellman_ford step 4304 current loss 0.965478, current_train_items 137760.
I0302 19:00:32.221140 22699590365312 run.py:483] Algo bellman_ford step 4305 current loss 0.326649, current_train_items 137792.
I0302 19:00:32.237004 22699590365312 run.py:483] Algo bellman_ford step 4306 current loss 0.415228, current_train_items 137824.
I0302 19:00:32.262060 22699590365312 run.py:483] Algo bellman_ford step 4307 current loss 0.686363, current_train_items 137856.
I0302 19:00:32.294027 22699590365312 run.py:483] Algo bellman_ford step 4308 current loss 0.842366, current_train_items 137888.
I0302 19:00:32.330149 22699590365312 run.py:483] Algo bellman_ford step 4309 current loss 0.810365, current_train_items 137920.
I0302 19:00:32.350025 22699590365312 run.py:483] Algo bellman_ford step 4310 current loss 0.314025, current_train_items 137952.
I0302 19:00:32.366181 22699590365312 run.py:483] Algo bellman_ford step 4311 current loss 0.466727, current_train_items 137984.
I0302 19:00:32.388564 22699590365312 run.py:483] Algo bellman_ford step 4312 current loss 0.562482, current_train_items 138016.
I0302 19:00:32.419925 22699590365312 run.py:483] Algo bellman_ford step 4313 current loss 0.699004, current_train_items 138048.
I0302 19:00:32.453891 22699590365312 run.py:483] Algo bellman_ford step 4314 current loss 0.813555, current_train_items 138080.
I0302 19:00:32.473394 22699590365312 run.py:483] Algo bellman_ford step 4315 current loss 0.336808, current_train_items 138112.
I0302 19:00:32.489322 22699590365312 run.py:483] Algo bellman_ford step 4316 current loss 0.415637, current_train_items 138144.
I0302 19:00:32.513325 22699590365312 run.py:483] Algo bellman_ford step 4317 current loss 0.588240, current_train_items 138176.
I0302 19:00:32.543440 22699590365312 run.py:483] Algo bellman_ford step 4318 current loss 0.689252, current_train_items 138208.
I0302 19:00:32.576639 22699590365312 run.py:483] Algo bellman_ford step 4319 current loss 0.809462, current_train_items 138240.
I0302 19:00:32.596214 22699590365312 run.py:483] Algo bellman_ford step 4320 current loss 0.414451, current_train_items 138272.
I0302 19:00:32.612322 22699590365312 run.py:483] Algo bellman_ford step 4321 current loss 0.505717, current_train_items 138304.
I0302 19:00:32.635563 22699590365312 run.py:483] Algo bellman_ford step 4322 current loss 0.565251, current_train_items 138336.
I0302 19:00:32.665138 22699590365312 run.py:483] Algo bellman_ford step 4323 current loss 0.711675, current_train_items 138368.
I0302 19:00:32.699385 22699590365312 run.py:483] Algo bellman_ford step 4324 current loss 1.046213, current_train_items 138400.
I0302 19:00:32.718864 22699590365312 run.py:483] Algo bellman_ford step 4325 current loss 0.301401, current_train_items 138432.
I0302 19:00:32.735204 22699590365312 run.py:483] Algo bellman_ford step 4326 current loss 0.441484, current_train_items 138464.
I0302 19:00:32.758444 22699590365312 run.py:483] Algo bellman_ford step 4327 current loss 0.615464, current_train_items 138496.
I0302 19:00:32.790709 22699590365312 run.py:483] Algo bellman_ford step 4328 current loss 0.792456, current_train_items 138528.
I0302 19:00:32.822952 22699590365312 run.py:483] Algo bellman_ford step 4329 current loss 0.828805, current_train_items 138560.
I0302 19:00:32.842877 22699590365312 run.py:483] Algo bellman_ford step 4330 current loss 0.308935, current_train_items 138592.
I0302 19:00:32.859336 22699590365312 run.py:483] Algo bellman_ford step 4331 current loss 0.519096, current_train_items 138624.
I0302 19:00:32.883485 22699590365312 run.py:483] Algo bellman_ford step 4332 current loss 0.653796, current_train_items 138656.
I0302 19:00:32.913744 22699590365312 run.py:483] Algo bellman_ford step 4333 current loss 0.693992, current_train_items 138688.
I0302 19:00:32.945427 22699590365312 run.py:483] Algo bellman_ford step 4334 current loss 0.656389, current_train_items 138720.
I0302 19:00:32.965043 22699590365312 run.py:483] Algo bellman_ford step 4335 current loss 0.318269, current_train_items 138752.
I0302 19:00:32.981192 22699590365312 run.py:483] Algo bellman_ford step 4336 current loss 0.526396, current_train_items 138784.
I0302 19:00:33.005177 22699590365312 run.py:483] Algo bellman_ford step 4337 current loss 0.688899, current_train_items 138816.
I0302 19:00:33.037119 22699590365312 run.py:483] Algo bellman_ford step 4338 current loss 0.755664, current_train_items 138848.
I0302 19:00:33.069724 22699590365312 run.py:483] Algo bellman_ford step 4339 current loss 0.761121, current_train_items 138880.
I0302 19:00:33.089444 22699590365312 run.py:483] Algo bellman_ford step 4340 current loss 0.304438, current_train_items 138912.
I0302 19:00:33.105376 22699590365312 run.py:483] Algo bellman_ford step 4341 current loss 0.393791, current_train_items 138944.
I0302 19:00:33.128983 22699590365312 run.py:483] Algo bellman_ford step 4342 current loss 0.563741, current_train_items 138976.
I0302 19:00:33.159905 22699590365312 run.py:483] Algo bellman_ford step 4343 current loss 0.794677, current_train_items 139008.
I0302 19:00:33.194365 22699590365312 run.py:483] Algo bellman_ford step 4344 current loss 0.761598, current_train_items 139040.
I0302 19:00:33.213793 22699590365312 run.py:483] Algo bellman_ford step 4345 current loss 0.309694, current_train_items 139072.
I0302 19:00:33.230285 22699590365312 run.py:483] Algo bellman_ford step 4346 current loss 0.464610, current_train_items 139104.
I0302 19:00:33.253919 22699590365312 run.py:483] Algo bellman_ford step 4347 current loss 0.614356, current_train_items 139136.
I0302 19:00:33.285314 22699590365312 run.py:483] Algo bellman_ford step 4348 current loss 0.611051, current_train_items 139168.
I0302 19:00:33.318059 22699590365312 run.py:483] Algo bellman_ford step 4349 current loss 0.824362, current_train_items 139200.
I0302 19:00:33.337897 22699590365312 run.py:483] Algo bellman_ford step 4350 current loss 0.327325, current_train_items 139232.
I0302 19:00:33.346072 22699590365312 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0302 19:00:33.346187 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:33.363087 22699590365312 run.py:483] Algo bellman_ford step 4351 current loss 0.479100, current_train_items 139264.
I0302 19:00:33.387791 22699590365312 run.py:483] Algo bellman_ford step 4352 current loss 0.713048, current_train_items 139296.
I0302 19:00:33.418850 22699590365312 run.py:483] Algo bellman_ford step 4353 current loss 0.687764, current_train_items 139328.
I0302 19:00:33.453810 22699590365312 run.py:483] Algo bellman_ford step 4354 current loss 0.797692, current_train_items 139360.
I0302 19:00:33.474152 22699590365312 run.py:483] Algo bellman_ford step 4355 current loss 0.301678, current_train_items 139392.
I0302 19:00:33.490123 22699590365312 run.py:483] Algo bellman_ford step 4356 current loss 0.403888, current_train_items 139424.
I0302 19:00:33.514167 22699590365312 run.py:483] Algo bellman_ford step 4357 current loss 0.636987, current_train_items 139456.
I0302 19:00:33.543187 22699590365312 run.py:483] Algo bellman_ford step 4358 current loss 0.630273, current_train_items 139488.
I0302 19:00:33.575351 22699590365312 run.py:483] Algo bellman_ford step 4359 current loss 0.710861, current_train_items 139520.
I0302 19:00:33.595207 22699590365312 run.py:483] Algo bellman_ford step 4360 current loss 0.297634, current_train_items 139552.
I0302 19:00:33.611923 22699590365312 run.py:483] Algo bellman_ford step 4361 current loss 0.605392, current_train_items 139584.
I0302 19:00:33.634457 22699590365312 run.py:483] Algo bellman_ford step 4362 current loss 0.686984, current_train_items 139616.
I0302 19:00:33.666296 22699590365312 run.py:483] Algo bellman_ford step 4363 current loss 0.848353, current_train_items 139648.
I0302 19:00:33.701379 22699590365312 run.py:483] Algo bellman_ford step 4364 current loss 0.962207, current_train_items 139680.
I0302 19:00:33.720657 22699590365312 run.py:483] Algo bellman_ford step 4365 current loss 0.296294, current_train_items 139712.
I0302 19:00:33.736920 22699590365312 run.py:483] Algo bellman_ford step 4366 current loss 0.615212, current_train_items 139744.
I0302 19:00:33.760283 22699590365312 run.py:483] Algo bellman_ford step 4367 current loss 0.629842, current_train_items 139776.
I0302 19:00:33.790316 22699590365312 run.py:483] Algo bellman_ford step 4368 current loss 0.653666, current_train_items 139808.
I0302 19:00:33.824543 22699590365312 run.py:483] Algo bellman_ford step 4369 current loss 0.838007, current_train_items 139840.
I0302 19:00:33.844097 22699590365312 run.py:483] Algo bellman_ford step 4370 current loss 0.338384, current_train_items 139872.
I0302 19:00:33.860220 22699590365312 run.py:483] Algo bellman_ford step 4371 current loss 0.538918, current_train_items 139904.
I0302 19:00:33.883474 22699590365312 run.py:483] Algo bellman_ford step 4372 current loss 0.551326, current_train_items 139936.
I0302 19:00:33.913851 22699590365312 run.py:483] Algo bellman_ford step 4373 current loss 0.688355, current_train_items 139968.
I0302 19:00:33.946866 22699590365312 run.py:483] Algo bellman_ford step 4374 current loss 0.779114, current_train_items 140000.
I0302 19:00:33.966789 22699590365312 run.py:483] Algo bellman_ford step 4375 current loss 0.333641, current_train_items 140032.
I0302 19:00:33.982986 22699590365312 run.py:483] Algo bellman_ford step 4376 current loss 0.392682, current_train_items 140064.
I0302 19:00:34.005614 22699590365312 run.py:483] Algo bellman_ford step 4377 current loss 0.591748, current_train_items 140096.
I0302 19:00:34.035801 22699590365312 run.py:483] Algo bellman_ford step 4378 current loss 0.601147, current_train_items 140128.
I0302 19:00:34.069358 22699590365312 run.py:483] Algo bellman_ford step 4379 current loss 0.764134, current_train_items 140160.
I0302 19:00:34.088993 22699590365312 run.py:483] Algo bellman_ford step 4380 current loss 0.327963, current_train_items 140192.
I0302 19:00:34.105048 22699590365312 run.py:483] Algo bellman_ford step 4381 current loss 0.444423, current_train_items 140224.
I0302 19:00:34.128577 22699590365312 run.py:483] Algo bellman_ford step 4382 current loss 0.657927, current_train_items 140256.
I0302 19:00:34.159535 22699590365312 run.py:483] Algo bellman_ford step 4383 current loss 0.734017, current_train_items 140288.
I0302 19:00:34.193288 22699590365312 run.py:483] Algo bellman_ford step 4384 current loss 0.776645, current_train_items 140320.
I0302 19:00:34.213123 22699590365312 run.py:483] Algo bellman_ford step 4385 current loss 0.314949, current_train_items 140352.
I0302 19:00:34.229034 22699590365312 run.py:483] Algo bellman_ford step 4386 current loss 0.437138, current_train_items 140384.
I0302 19:00:34.252325 22699590365312 run.py:483] Algo bellman_ford step 4387 current loss 0.736479, current_train_items 140416.
I0302 19:00:34.283079 22699590365312 run.py:483] Algo bellman_ford step 4388 current loss 0.684790, current_train_items 140448.
I0302 19:00:34.315076 22699590365312 run.py:483] Algo bellman_ford step 4389 current loss 0.714468, current_train_items 140480.
I0302 19:00:34.335132 22699590365312 run.py:483] Algo bellman_ford step 4390 current loss 0.227202, current_train_items 140512.
I0302 19:00:34.351074 22699590365312 run.py:483] Algo bellman_ford step 4391 current loss 0.465808, current_train_items 140544.
I0302 19:00:34.373975 22699590365312 run.py:483] Algo bellman_ford step 4392 current loss 0.663793, current_train_items 140576.
I0302 19:00:34.404761 22699590365312 run.py:483] Algo bellman_ford step 4393 current loss 0.722205, current_train_items 140608.
I0302 19:00:34.438655 22699590365312 run.py:483] Algo bellman_ford step 4394 current loss 0.805923, current_train_items 140640.
I0302 19:00:34.458223 22699590365312 run.py:483] Algo bellman_ford step 4395 current loss 0.299177, current_train_items 140672.
I0302 19:00:34.474673 22699590365312 run.py:483] Algo bellman_ford step 4396 current loss 0.500810, current_train_items 140704.
I0302 19:00:34.498277 22699590365312 run.py:483] Algo bellman_ford step 4397 current loss 0.610635, current_train_items 140736.
I0302 19:00:34.529283 22699590365312 run.py:483] Algo bellman_ford step 4398 current loss 0.688730, current_train_items 140768.
I0302 19:00:34.562408 22699590365312 run.py:483] Algo bellman_ford step 4399 current loss 0.707332, current_train_items 140800.
I0302 19:00:34.582469 22699590365312 run.py:483] Algo bellman_ford step 4400 current loss 0.367810, current_train_items 140832.
I0302 19:00:34.590401 22699590365312 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0302 19:00:34.590507 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.940, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:00:34.607477 22699590365312 run.py:483] Algo bellman_ford step 4401 current loss 0.400555, current_train_items 140864.
I0302 19:00:34.630915 22699590365312 run.py:483] Algo bellman_ford step 4402 current loss 0.607632, current_train_items 140896.
I0302 19:00:34.661535 22699590365312 run.py:483] Algo bellman_ford step 4403 current loss 0.636181, current_train_items 140928.
I0302 19:00:34.699775 22699590365312 run.py:483] Algo bellman_ford step 4404 current loss 1.015313, current_train_items 140960.
I0302 19:00:34.719961 22699590365312 run.py:483] Algo bellman_ford step 4405 current loss 0.278498, current_train_items 140992.
I0302 19:00:34.736022 22699590365312 run.py:483] Algo bellman_ford step 4406 current loss 0.455073, current_train_items 141024.
I0302 19:00:34.759551 22699590365312 run.py:483] Algo bellman_ford step 4407 current loss 0.690748, current_train_items 141056.
I0302 19:00:34.790218 22699590365312 run.py:483] Algo bellman_ford step 4408 current loss 0.665471, current_train_items 141088.
I0302 19:00:34.825500 22699590365312 run.py:483] Algo bellman_ford step 4409 current loss 0.951041, current_train_items 141120.
I0302 19:00:34.845082 22699590365312 run.py:483] Algo bellman_ford step 4410 current loss 0.321718, current_train_items 141152.
I0302 19:00:34.861288 22699590365312 run.py:483] Algo bellman_ford step 4411 current loss 0.508095, current_train_items 141184.
I0302 19:00:34.884965 22699590365312 run.py:483] Algo bellman_ford step 4412 current loss 0.702870, current_train_items 141216.
I0302 19:00:34.916958 22699590365312 run.py:483] Algo bellman_ford step 4413 current loss 0.836698, current_train_items 141248.
I0302 19:00:34.949378 22699590365312 run.py:483] Algo bellman_ford step 4414 current loss 0.880012, current_train_items 141280.
I0302 19:00:34.968801 22699590365312 run.py:483] Algo bellman_ford step 4415 current loss 0.302151, current_train_items 141312.
I0302 19:00:34.985394 22699590365312 run.py:483] Algo bellman_ford step 4416 current loss 0.482033, current_train_items 141344.
I0302 19:00:35.008692 22699590365312 run.py:483] Algo bellman_ford step 4417 current loss 0.636137, current_train_items 141376.
I0302 19:00:35.039295 22699590365312 run.py:483] Algo bellman_ford step 4418 current loss 0.716063, current_train_items 141408.
I0302 19:00:35.073073 22699590365312 run.py:483] Algo bellman_ford step 4419 current loss 0.754326, current_train_items 141440.
I0302 19:00:35.092252 22699590365312 run.py:483] Algo bellman_ford step 4420 current loss 0.367922, current_train_items 141472.
I0302 19:00:35.108314 22699590365312 run.py:483] Algo bellman_ford step 4421 current loss 0.533533, current_train_items 141504.
I0302 19:00:35.131540 22699590365312 run.py:483] Algo bellman_ford step 4422 current loss 0.605639, current_train_items 141536.
I0302 19:00:35.162429 22699590365312 run.py:483] Algo bellman_ford step 4423 current loss 0.639666, current_train_items 141568.
I0302 19:00:35.196770 22699590365312 run.py:483] Algo bellman_ford step 4424 current loss 0.889815, current_train_items 141600.
I0302 19:00:35.216749 22699590365312 run.py:483] Algo bellman_ford step 4425 current loss 0.338096, current_train_items 141632.
I0302 19:00:35.232135 22699590365312 run.py:483] Algo bellman_ford step 4426 current loss 0.453253, current_train_items 141664.
I0302 19:00:35.256007 22699590365312 run.py:483] Algo bellman_ford step 4427 current loss 0.650228, current_train_items 141696.
I0302 19:00:35.287283 22699590365312 run.py:483] Algo bellman_ford step 4428 current loss 0.674368, current_train_items 141728.
I0302 19:00:35.320685 22699590365312 run.py:483] Algo bellman_ford step 4429 current loss 0.850855, current_train_items 141760.
I0302 19:00:35.340390 22699590365312 run.py:483] Algo bellman_ford step 4430 current loss 0.268077, current_train_items 141792.
I0302 19:00:35.356090 22699590365312 run.py:483] Algo bellman_ford step 4431 current loss 0.423003, current_train_items 141824.
I0302 19:00:35.379757 22699590365312 run.py:483] Algo bellman_ford step 4432 current loss 0.570522, current_train_items 141856.
I0302 19:00:35.409905 22699590365312 run.py:483] Algo bellman_ford step 4433 current loss 0.655375, current_train_items 141888.
I0302 19:00:35.443363 22699590365312 run.py:483] Algo bellman_ford step 4434 current loss 0.714900, current_train_items 141920.
I0302 19:00:35.463065 22699590365312 run.py:483] Algo bellman_ford step 4435 current loss 0.324576, current_train_items 141952.
I0302 19:00:35.479289 22699590365312 run.py:483] Algo bellman_ford step 4436 current loss 0.478124, current_train_items 141984.
I0302 19:00:35.503385 22699590365312 run.py:483] Algo bellman_ford step 4437 current loss 0.641236, current_train_items 142016.
I0302 19:00:35.533390 22699590365312 run.py:483] Algo bellman_ford step 4438 current loss 0.524622, current_train_items 142048.
I0302 19:00:35.567978 22699590365312 run.py:483] Algo bellman_ford step 4439 current loss 0.751787, current_train_items 142080.
I0302 19:00:35.587323 22699590365312 run.py:483] Algo bellman_ford step 4440 current loss 0.380079, current_train_items 142112.
I0302 19:00:35.603890 22699590365312 run.py:483] Algo bellman_ford step 4441 current loss 0.594814, current_train_items 142144.
I0302 19:00:35.626949 22699590365312 run.py:483] Algo bellman_ford step 4442 current loss 0.601033, current_train_items 142176.
I0302 19:00:35.657345 22699590365312 run.py:483] Algo bellman_ford step 4443 current loss 0.679977, current_train_items 142208.
I0302 19:00:35.692166 22699590365312 run.py:483] Algo bellman_ford step 4444 current loss 0.713631, current_train_items 142240.
I0302 19:00:35.711701 22699590365312 run.py:483] Algo bellman_ford step 4445 current loss 0.332421, current_train_items 142272.
I0302 19:00:35.727708 22699590365312 run.py:483] Algo bellman_ford step 4446 current loss 0.429825, current_train_items 142304.
I0302 19:00:35.751667 22699590365312 run.py:483] Algo bellman_ford step 4447 current loss 0.517547, current_train_items 142336.
I0302 19:00:35.783374 22699590365312 run.py:483] Algo bellman_ford step 4448 current loss 0.769439, current_train_items 142368.
I0302 19:00:35.816721 22699590365312 run.py:483] Algo bellman_ford step 4449 current loss 0.721005, current_train_items 142400.
I0302 19:00:35.836607 22699590365312 run.py:483] Algo bellman_ford step 4450 current loss 0.331292, current_train_items 142432.
I0302 19:00:35.844754 22699590365312 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0302 19:00:35.844865 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.940, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 19:00:35.873444 22699590365312 run.py:483] Algo bellman_ford step 4451 current loss 0.424862, current_train_items 142464.
I0302 19:00:35.897253 22699590365312 run.py:483] Algo bellman_ford step 4452 current loss 0.510100, current_train_items 142496.
I0302 19:00:35.927389 22699590365312 run.py:483] Algo bellman_ford step 4453 current loss 0.623566, current_train_items 142528.
I0302 19:00:35.960125 22699590365312 run.py:483] Algo bellman_ford step 4454 current loss 0.680440, current_train_items 142560.
I0302 19:00:35.980410 22699590365312 run.py:483] Algo bellman_ford step 4455 current loss 0.272483, current_train_items 142592.
I0302 19:00:35.996485 22699590365312 run.py:483] Algo bellman_ford step 4456 current loss 0.433396, current_train_items 142624.
I0302 19:00:36.020188 22699590365312 run.py:483] Algo bellman_ford step 4457 current loss 0.540886, current_train_items 142656.
I0302 19:00:36.049774 22699590365312 run.py:483] Algo bellman_ford step 4458 current loss 0.620647, current_train_items 142688.
I0302 19:00:36.086478 22699590365312 run.py:483] Algo bellman_ford step 4459 current loss 0.801400, current_train_items 142720.
I0302 19:00:36.106903 22699590365312 run.py:483] Algo bellman_ford step 4460 current loss 0.349737, current_train_items 142752.
I0302 19:00:36.123326 22699590365312 run.py:483] Algo bellman_ford step 4461 current loss 0.471783, current_train_items 142784.
I0302 19:00:36.147306 22699590365312 run.py:483] Algo bellman_ford step 4462 current loss 0.737398, current_train_items 142816.
I0302 19:00:36.176965 22699590365312 run.py:483] Algo bellman_ford step 4463 current loss 0.529520, current_train_items 142848.
I0302 19:00:36.210596 22699590365312 run.py:483] Algo bellman_ford step 4464 current loss 0.779686, current_train_items 142880.
I0302 19:00:36.230356 22699590365312 run.py:483] Algo bellman_ford step 4465 current loss 0.268439, current_train_items 142912.
I0302 19:00:36.246616 22699590365312 run.py:483] Algo bellman_ford step 4466 current loss 0.481503, current_train_items 142944.
I0302 19:00:36.270013 22699590365312 run.py:483] Algo bellman_ford step 4467 current loss 0.691869, current_train_items 142976.
I0302 19:00:36.302352 22699590365312 run.py:483] Algo bellman_ford step 4468 current loss 0.757324, current_train_items 143008.
I0302 19:00:36.336539 22699590365312 run.py:483] Algo bellman_ford step 4469 current loss 0.860655, current_train_items 143040.
I0302 19:00:36.356327 22699590365312 run.py:483] Algo bellman_ford step 4470 current loss 0.290267, current_train_items 143072.
I0302 19:00:36.372418 22699590365312 run.py:483] Algo bellman_ford step 4471 current loss 0.512313, current_train_items 143104.
I0302 19:00:36.395403 22699590365312 run.py:483] Algo bellman_ford step 4472 current loss 0.632127, current_train_items 143136.
I0302 19:00:36.426078 22699590365312 run.py:483] Algo bellman_ford step 4473 current loss 0.775669, current_train_items 143168.
I0302 19:00:36.462132 22699590365312 run.py:483] Algo bellman_ford step 4474 current loss 0.962458, current_train_items 143200.
I0302 19:00:36.482383 22699590365312 run.py:483] Algo bellman_ford step 4475 current loss 0.328638, current_train_items 143232.
I0302 19:00:36.498421 22699590365312 run.py:483] Algo bellman_ford step 4476 current loss 0.514526, current_train_items 143264.
I0302 19:00:36.521402 22699590365312 run.py:483] Algo bellman_ford step 4477 current loss 0.628782, current_train_items 143296.
I0302 19:00:36.551937 22699590365312 run.py:483] Algo bellman_ford step 4478 current loss 0.691332, current_train_items 143328.
I0302 19:00:36.586961 22699590365312 run.py:483] Algo bellman_ford step 4479 current loss 0.897640, current_train_items 143360.
I0302 19:00:36.606108 22699590365312 run.py:483] Algo bellman_ford step 4480 current loss 0.260798, current_train_items 143392.
I0302 19:00:36.622236 22699590365312 run.py:483] Algo bellman_ford step 4481 current loss 0.511002, current_train_items 143424.
I0302 19:00:36.645993 22699590365312 run.py:483] Algo bellman_ford step 4482 current loss 0.684976, current_train_items 143456.
I0302 19:00:36.677095 22699590365312 run.py:483] Algo bellman_ford step 4483 current loss 0.731943, current_train_items 143488.
I0302 19:00:36.708273 22699590365312 run.py:483] Algo bellman_ford step 4484 current loss 0.721475, current_train_items 143520.
I0302 19:00:36.728312 22699590365312 run.py:483] Algo bellman_ford step 4485 current loss 0.346672, current_train_items 143552.
I0302 19:00:36.744714 22699590365312 run.py:483] Algo bellman_ford step 4486 current loss 0.426954, current_train_items 143584.
I0302 19:00:36.769372 22699590365312 run.py:483] Algo bellman_ford step 4487 current loss 0.742795, current_train_items 143616.
I0302 19:00:36.800549 22699590365312 run.py:483] Algo bellman_ford step 4488 current loss 0.741796, current_train_items 143648.
I0302 19:00:36.833549 22699590365312 run.py:483] Algo bellman_ford step 4489 current loss 0.888223, current_train_items 143680.
I0302 19:00:36.853579 22699590365312 run.py:483] Algo bellman_ford step 4490 current loss 0.303212, current_train_items 143712.
I0302 19:00:36.869403 22699590365312 run.py:483] Algo bellman_ford step 4491 current loss 0.464028, current_train_items 143744.
I0302 19:00:36.892958 22699590365312 run.py:483] Algo bellman_ford step 4492 current loss 0.637161, current_train_items 143776.
I0302 19:00:36.924221 22699590365312 run.py:483] Algo bellman_ford step 4493 current loss 0.668358, current_train_items 143808.
I0302 19:00:36.958217 22699590365312 run.py:483] Algo bellman_ford step 4494 current loss 0.910370, current_train_items 143840.
I0302 19:00:36.977809 22699590365312 run.py:483] Algo bellman_ford step 4495 current loss 0.393186, current_train_items 143872.
I0302 19:00:36.994292 22699590365312 run.py:483] Algo bellman_ford step 4496 current loss 0.492521, current_train_items 143904.
I0302 19:00:37.017933 22699590365312 run.py:483] Algo bellman_ford step 4497 current loss 0.678161, current_train_items 143936.
I0302 19:00:37.047625 22699590365312 run.py:483] Algo bellman_ford step 4498 current loss 0.693003, current_train_items 143968.
I0302 19:00:37.079071 22699590365312 run.py:483] Algo bellman_ford step 4499 current loss 0.732644, current_train_items 144000.
I0302 19:00:37.099116 22699590365312 run.py:483] Algo bellman_ford step 4500 current loss 0.319757, current_train_items 144032.
I0302 19:00:37.106857 22699590365312 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0302 19:00:37.106963 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 19:00:37.123903 22699590365312 run.py:483] Algo bellman_ford step 4501 current loss 0.464992, current_train_items 144064.
I0302 19:00:37.148202 22699590365312 run.py:483] Algo bellman_ford step 4502 current loss 0.751252, current_train_items 144096.
I0302 19:00:37.179545 22699590365312 run.py:483] Algo bellman_ford step 4503 current loss 0.770219, current_train_items 144128.
I0302 19:00:37.213541 22699590365312 run.py:483] Algo bellman_ford step 4504 current loss 0.759544, current_train_items 144160.
I0302 19:00:37.233439 22699590365312 run.py:483] Algo bellman_ford step 4505 current loss 0.268620, current_train_items 144192.
I0302 19:00:37.249500 22699590365312 run.py:483] Algo bellman_ford step 4506 current loss 0.507853, current_train_items 144224.
I0302 19:00:37.272181 22699590365312 run.py:483] Algo bellman_ford step 4507 current loss 0.591509, current_train_items 144256.
I0302 19:00:37.301949 22699590365312 run.py:483] Algo bellman_ford step 4508 current loss 0.630660, current_train_items 144288.
I0302 19:00:37.334314 22699590365312 run.py:483] Algo bellman_ford step 4509 current loss 0.728051, current_train_items 144320.
I0302 19:00:37.353667 22699590365312 run.py:483] Algo bellman_ford step 4510 current loss 0.285277, current_train_items 144352.
I0302 19:00:37.369945 22699590365312 run.py:483] Algo bellman_ford step 4511 current loss 0.461010, current_train_items 144384.
I0302 19:00:37.393482 22699590365312 run.py:483] Algo bellman_ford step 4512 current loss 0.637055, current_train_items 144416.
I0302 19:00:37.424065 22699590365312 run.py:483] Algo bellman_ford step 4513 current loss 0.681852, current_train_items 144448.
I0302 19:00:37.456702 22699590365312 run.py:483] Algo bellman_ford step 4514 current loss 0.794912, current_train_items 144480.
I0302 19:00:37.476089 22699590365312 run.py:483] Algo bellman_ford step 4515 current loss 0.364361, current_train_items 144512.
I0302 19:00:37.492488 22699590365312 run.py:483] Algo bellman_ford step 4516 current loss 0.471396, current_train_items 144544.
I0302 19:00:37.514754 22699590365312 run.py:483] Algo bellman_ford step 4517 current loss 0.533330, current_train_items 144576.
I0302 19:00:37.545949 22699590365312 run.py:483] Algo bellman_ford step 4518 current loss 0.701296, current_train_items 144608.
I0302 19:00:37.582512 22699590365312 run.py:483] Algo bellman_ford step 4519 current loss 0.980999, current_train_items 144640.
I0302 19:00:37.601868 22699590365312 run.py:483] Algo bellman_ford step 4520 current loss 0.301150, current_train_items 144672.
I0302 19:00:37.618121 22699590365312 run.py:483] Algo bellman_ford step 4521 current loss 0.473598, current_train_items 144704.
I0302 19:00:37.641170 22699590365312 run.py:483] Algo bellman_ford step 4522 current loss 0.549763, current_train_items 144736.
I0302 19:00:37.673879 22699590365312 run.py:483] Algo bellman_ford step 4523 current loss 0.733178, current_train_items 144768.
I0302 19:00:37.706559 22699590365312 run.py:483] Algo bellman_ford step 4524 current loss 0.768312, current_train_items 144800.
I0302 19:00:37.725749 22699590365312 run.py:483] Algo bellman_ford step 4525 current loss 0.304616, current_train_items 144832.
I0302 19:00:37.741854 22699590365312 run.py:483] Algo bellman_ford step 4526 current loss 0.424082, current_train_items 144864.
I0302 19:00:37.765960 22699590365312 run.py:483] Algo bellman_ford step 4527 current loss 0.611937, current_train_items 144896.
I0302 19:00:37.796738 22699590365312 run.py:483] Algo bellman_ford step 4528 current loss 0.582310, current_train_items 144928.
I0302 19:00:37.830316 22699590365312 run.py:483] Algo bellman_ford step 4529 current loss 0.836406, current_train_items 144960.
I0302 19:00:37.849623 22699590365312 run.py:483] Algo bellman_ford step 4530 current loss 0.350571, current_train_items 144992.
I0302 19:00:37.865563 22699590365312 run.py:483] Algo bellman_ford step 4531 current loss 0.419749, current_train_items 145024.
I0302 19:00:37.887935 22699590365312 run.py:483] Algo bellman_ford step 4532 current loss 0.543769, current_train_items 145056.
I0302 19:00:37.916990 22699590365312 run.py:483] Algo bellman_ford step 4533 current loss 0.553243, current_train_items 145088.
I0302 19:00:37.951794 22699590365312 run.py:483] Algo bellman_ford step 4534 current loss 0.738408, current_train_items 145120.
I0302 19:00:37.971178 22699590365312 run.py:483] Algo bellman_ford step 4535 current loss 0.242613, current_train_items 145152.
I0302 19:00:37.987236 22699590365312 run.py:483] Algo bellman_ford step 4536 current loss 0.435734, current_train_items 145184.
I0302 19:00:38.010893 22699590365312 run.py:483] Algo bellman_ford step 4537 current loss 0.619955, current_train_items 145216.
I0302 19:00:38.041218 22699590365312 run.py:483] Algo bellman_ford step 4538 current loss 0.512297, current_train_items 145248.
I0302 19:00:38.076409 22699590365312 run.py:483] Algo bellman_ford step 4539 current loss 0.898523, current_train_items 145280.
I0302 19:00:38.095821 22699590365312 run.py:483] Algo bellman_ford step 4540 current loss 0.270856, current_train_items 145312.
I0302 19:00:38.111892 22699590365312 run.py:483] Algo bellman_ford step 4541 current loss 0.468832, current_train_items 145344.
I0302 19:00:38.134785 22699590365312 run.py:483] Algo bellman_ford step 4542 current loss 0.575014, current_train_items 145376.
I0302 19:00:38.164739 22699590365312 run.py:483] Algo bellman_ford step 4543 current loss 0.575038, current_train_items 145408.
I0302 19:00:38.198737 22699590365312 run.py:483] Algo bellman_ford step 4544 current loss 0.955093, current_train_items 145440.
I0302 19:00:38.217984 22699590365312 run.py:483] Algo bellman_ford step 4545 current loss 0.341249, current_train_items 145472.
I0302 19:00:38.233939 22699590365312 run.py:483] Algo bellman_ford step 4546 current loss 0.489827, current_train_items 145504.
I0302 19:00:38.256582 22699590365312 run.py:483] Algo bellman_ford step 4547 current loss 0.602883, current_train_items 145536.
I0302 19:00:38.285802 22699590365312 run.py:483] Algo bellman_ford step 4548 current loss 0.689443, current_train_items 145568.
I0302 19:00:38.320030 22699590365312 run.py:483] Algo bellman_ford step 4549 current loss 0.869873, current_train_items 145600.
I0302 19:00:38.339307 22699590365312 run.py:483] Algo bellman_ford step 4550 current loss 0.264966, current_train_items 145632.
I0302 19:00:38.347532 22699590365312 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0302 19:00:38.347637 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:00:38.364697 22699590365312 run.py:483] Algo bellman_ford step 4551 current loss 0.483639, current_train_items 145664.
I0302 19:00:38.388252 22699590365312 run.py:483] Algo bellman_ford step 4552 current loss 0.573971, current_train_items 145696.
I0302 19:00:38.419841 22699590365312 run.py:483] Algo bellman_ford step 4553 current loss 0.723616, current_train_items 145728.
I0302 19:00:38.453718 22699590365312 run.py:483] Algo bellman_ford step 4554 current loss 0.867543, current_train_items 145760.
I0302 19:00:38.473695 22699590365312 run.py:483] Algo bellman_ford step 4555 current loss 0.307393, current_train_items 145792.
I0302 19:00:38.489134 22699590365312 run.py:483] Algo bellman_ford step 4556 current loss 0.510225, current_train_items 145824.
I0302 19:00:38.511672 22699590365312 run.py:483] Algo bellman_ford step 4557 current loss 0.614652, current_train_items 145856.
I0302 19:00:38.543331 22699590365312 run.py:483] Algo bellman_ford step 4558 current loss 0.653832, current_train_items 145888.
I0302 19:00:38.578206 22699590365312 run.py:483] Algo bellman_ford step 4559 current loss 0.900234, current_train_items 145920.
I0302 19:00:38.597814 22699590365312 run.py:483] Algo bellman_ford step 4560 current loss 0.263803, current_train_items 145952.
I0302 19:00:38.614241 22699590365312 run.py:483] Algo bellman_ford step 4561 current loss 0.465878, current_train_items 145984.
I0302 19:00:38.636807 22699590365312 run.py:483] Algo bellman_ford step 4562 current loss 0.565297, current_train_items 146016.
I0302 19:00:38.666730 22699590365312 run.py:483] Algo bellman_ford step 4563 current loss 0.623442, current_train_items 146048.
I0302 19:00:38.701311 22699590365312 run.py:483] Algo bellman_ford step 4564 current loss 0.843206, current_train_items 146080.
I0302 19:00:38.720884 22699590365312 run.py:483] Algo bellman_ford step 4565 current loss 0.307535, current_train_items 146112.
I0302 19:00:38.736969 22699590365312 run.py:483] Algo bellman_ford step 4566 current loss 0.520400, current_train_items 146144.
I0302 19:00:38.760864 22699590365312 run.py:483] Algo bellman_ford step 4567 current loss 0.563659, current_train_items 146176.
I0302 19:00:38.792556 22699590365312 run.py:483] Algo bellman_ford step 4568 current loss 0.727579, current_train_items 146208.
I0302 19:00:38.825290 22699590365312 run.py:483] Algo bellman_ford step 4569 current loss 0.710231, current_train_items 146240.
I0302 19:00:38.844601 22699590365312 run.py:483] Algo bellman_ford step 4570 current loss 0.261107, current_train_items 146272.
I0302 19:00:38.860588 22699590365312 run.py:483] Algo bellman_ford step 4571 current loss 0.426216, current_train_items 146304.
I0302 19:00:38.884064 22699590365312 run.py:483] Algo bellman_ford step 4572 current loss 0.618416, current_train_items 146336.
I0302 19:00:38.914077 22699590365312 run.py:483] Algo bellman_ford step 4573 current loss 0.667400, current_train_items 146368.
I0302 19:00:38.946648 22699590365312 run.py:483] Algo bellman_ford step 4574 current loss 0.804369, current_train_items 146400.
I0302 19:00:38.966236 22699590365312 run.py:483] Algo bellman_ford step 4575 current loss 0.305159, current_train_items 146432.
I0302 19:00:38.982343 22699590365312 run.py:483] Algo bellman_ford step 4576 current loss 0.500036, current_train_items 146464.
I0302 19:00:39.005370 22699590365312 run.py:483] Algo bellman_ford step 4577 current loss 0.675649, current_train_items 146496.
I0302 19:00:39.036507 22699590365312 run.py:483] Algo bellman_ford step 4578 current loss 0.795824, current_train_items 146528.
I0302 19:00:39.070094 22699590365312 run.py:483] Algo bellman_ford step 4579 current loss 0.781530, current_train_items 146560.
I0302 19:00:39.089540 22699590365312 run.py:483] Algo bellman_ford step 4580 current loss 0.298120, current_train_items 146592.
I0302 19:00:39.106102 22699590365312 run.py:483] Algo bellman_ford step 4581 current loss 0.483710, current_train_items 146624.
I0302 19:00:39.128999 22699590365312 run.py:483] Algo bellman_ford step 4582 current loss 0.559870, current_train_items 146656.
I0302 19:00:39.159885 22699590365312 run.py:483] Algo bellman_ford step 4583 current loss 0.665440, current_train_items 146688.
I0302 19:00:39.193491 22699590365312 run.py:483] Algo bellman_ford step 4584 current loss 0.906356, current_train_items 146720.
I0302 19:00:39.213310 22699590365312 run.py:483] Algo bellman_ford step 4585 current loss 0.275169, current_train_items 146752.
I0302 19:00:39.229736 22699590365312 run.py:483] Algo bellman_ford step 4586 current loss 0.470962, current_train_items 146784.
I0302 19:00:39.252045 22699590365312 run.py:483] Algo bellman_ford step 4587 current loss 0.552063, current_train_items 146816.
I0302 19:00:39.282597 22699590365312 run.py:483] Algo bellman_ford step 4588 current loss 0.671597, current_train_items 146848.
I0302 19:00:39.315206 22699590365312 run.py:483] Algo bellman_ford step 4589 current loss 0.735452, current_train_items 146880.
I0302 19:00:39.334867 22699590365312 run.py:483] Algo bellman_ford step 4590 current loss 0.310721, current_train_items 146912.
I0302 19:00:39.350930 22699590365312 run.py:483] Algo bellman_ford step 4591 current loss 0.564356, current_train_items 146944.
I0302 19:00:39.373459 22699590365312 run.py:483] Algo bellman_ford step 4592 current loss 0.604430, current_train_items 146976.
I0302 19:00:39.405297 22699590365312 run.py:483] Algo bellman_ford step 4593 current loss 0.737611, current_train_items 147008.
I0302 19:00:39.439586 22699590365312 run.py:483] Algo bellman_ford step 4594 current loss 0.801729, current_train_items 147040.
I0302 19:00:39.459091 22699590365312 run.py:483] Algo bellman_ford step 4595 current loss 0.305519, current_train_items 147072.
I0302 19:00:39.475186 22699590365312 run.py:483] Algo bellman_ford step 4596 current loss 0.425781, current_train_items 147104.
I0302 19:00:39.500277 22699590365312 run.py:483] Algo bellman_ford step 4597 current loss 0.789838, current_train_items 147136.
I0302 19:00:39.530900 22699590365312 run.py:483] Algo bellman_ford step 4598 current loss 0.653972, current_train_items 147168.
I0302 19:00:39.562607 22699590365312 run.py:483] Algo bellman_ford step 4599 current loss 0.650229, current_train_items 147200.
I0302 19:00:39.582495 22699590365312 run.py:483] Algo bellman_ford step 4600 current loss 0.288897, current_train_items 147232.
I0302 19:00:39.590427 22699590365312 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.8916015625, 'score': 0.8916015625, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0302 19:00:39.590534 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.892, val scores are: bellman_ford: 0.892
I0302 19:00:39.607743 22699590365312 run.py:483] Algo bellman_ford step 4601 current loss 0.471084, current_train_items 147264.
I0302 19:00:39.632536 22699590365312 run.py:483] Algo bellman_ford step 4602 current loss 0.748895, current_train_items 147296.
I0302 19:00:39.663837 22699590365312 run.py:483] Algo bellman_ford step 4603 current loss 0.817519, current_train_items 147328.
I0302 19:00:39.698564 22699590365312 run.py:483] Algo bellman_ford step 4604 current loss 0.959072, current_train_items 147360.
I0302 19:00:39.718265 22699590365312 run.py:483] Algo bellman_ford step 4605 current loss 0.265504, current_train_items 147392.
I0302 19:00:39.734441 22699590365312 run.py:483] Algo bellman_ford step 4606 current loss 0.497258, current_train_items 147424.
I0302 19:00:39.757880 22699590365312 run.py:483] Algo bellman_ford step 4607 current loss 0.575949, current_train_items 147456.
I0302 19:00:39.789515 22699590365312 run.py:483] Algo bellman_ford step 4608 current loss 0.618078, current_train_items 147488.
I0302 19:00:39.821372 22699590365312 run.py:483] Algo bellman_ford step 4609 current loss 0.642207, current_train_items 147520.
I0302 19:00:39.840951 22699590365312 run.py:483] Algo bellman_ford step 4610 current loss 0.284219, current_train_items 147552.
I0302 19:00:39.856980 22699590365312 run.py:483] Algo bellman_ford step 4611 current loss 0.505752, current_train_items 147584.
I0302 19:00:39.880728 22699590365312 run.py:483] Algo bellman_ford step 4612 current loss 0.569298, current_train_items 147616.
I0302 19:00:39.912022 22699590365312 run.py:483] Algo bellman_ford step 4613 current loss 0.686613, current_train_items 147648.
I0302 19:00:39.946450 22699590365312 run.py:483] Algo bellman_ford step 4614 current loss 0.842940, current_train_items 147680.
I0302 19:00:39.966031 22699590365312 run.py:483] Algo bellman_ford step 4615 current loss 0.332465, current_train_items 147712.
I0302 19:00:39.982254 22699590365312 run.py:483] Algo bellman_ford step 4616 current loss 0.515634, current_train_items 147744.
I0302 19:00:40.005884 22699590365312 run.py:483] Algo bellman_ford step 4617 current loss 0.551993, current_train_items 147776.
I0302 19:00:40.036149 22699590365312 run.py:483] Algo bellman_ford step 4618 current loss 0.568578, current_train_items 147808.
I0302 19:00:40.069837 22699590365312 run.py:483] Algo bellman_ford step 4619 current loss 0.724270, current_train_items 147840.
I0302 19:00:40.089418 22699590365312 run.py:483] Algo bellman_ford step 4620 current loss 0.273436, current_train_items 147872.
I0302 19:00:40.105283 22699590365312 run.py:483] Algo bellman_ford step 4621 current loss 0.381306, current_train_items 147904.
I0302 19:00:40.128711 22699590365312 run.py:483] Algo bellman_ford step 4622 current loss 0.568339, current_train_items 147936.
I0302 19:00:40.159511 22699590365312 run.py:483] Algo bellman_ford step 4623 current loss 0.607110, current_train_items 147968.
I0302 19:00:40.192357 22699590365312 run.py:483] Algo bellman_ford step 4624 current loss 1.097906, current_train_items 148000.
I0302 19:00:40.211869 22699590365312 run.py:483] Algo bellman_ford step 4625 current loss 0.255703, current_train_items 148032.
I0302 19:00:40.228241 22699590365312 run.py:483] Algo bellman_ford step 4626 current loss 0.503358, current_train_items 148064.
I0302 19:00:40.252357 22699590365312 run.py:483] Algo bellman_ford step 4627 current loss 0.622155, current_train_items 148096.
I0302 19:00:40.283988 22699590365312 run.py:483] Algo bellman_ford step 4628 current loss 0.694455, current_train_items 148128.
I0302 19:00:40.318948 22699590365312 run.py:483] Algo bellman_ford step 4629 current loss 0.819209, current_train_items 148160.
I0302 19:00:40.338305 22699590365312 run.py:483] Algo bellman_ford step 4630 current loss 0.265771, current_train_items 148192.
I0302 19:00:40.354629 22699590365312 run.py:483] Algo bellman_ford step 4631 current loss 0.426136, current_train_items 148224.
I0302 19:00:40.378965 22699590365312 run.py:483] Algo bellman_ford step 4632 current loss 0.663274, current_train_items 148256.
I0302 19:00:40.408280 22699590365312 run.py:483] Algo bellman_ford step 4633 current loss 0.647113, current_train_items 148288.
I0302 19:00:40.442407 22699590365312 run.py:483] Algo bellman_ford step 4634 current loss 0.711399, current_train_items 148320.
I0302 19:00:40.462291 22699590365312 run.py:483] Algo bellman_ford step 4635 current loss 0.299013, current_train_items 148352.
I0302 19:00:40.478986 22699590365312 run.py:483] Algo bellman_ford step 4636 current loss 0.469296, current_train_items 148384.
I0302 19:00:40.502900 22699590365312 run.py:483] Algo bellman_ford step 4637 current loss 0.598724, current_train_items 148416.
I0302 19:00:40.535521 22699590365312 run.py:483] Algo bellman_ford step 4638 current loss 0.674074, current_train_items 148448.
I0302 19:00:40.569482 22699590365312 run.py:483] Algo bellman_ford step 4639 current loss 0.765809, current_train_items 148480.
I0302 19:00:40.588879 22699590365312 run.py:483] Algo bellman_ford step 4640 current loss 0.302732, current_train_items 148512.
I0302 19:00:40.605050 22699590365312 run.py:483] Algo bellman_ford step 4641 current loss 0.459879, current_train_items 148544.
I0302 19:00:40.628818 22699590365312 run.py:483] Algo bellman_ford step 4642 current loss 0.608739, current_train_items 148576.
I0302 19:00:40.660274 22699590365312 run.py:483] Algo bellman_ford step 4643 current loss 0.603968, current_train_items 148608.
I0302 19:00:40.694726 22699590365312 run.py:483] Algo bellman_ford step 4644 current loss 0.759887, current_train_items 148640.
I0302 19:00:40.714088 22699590365312 run.py:483] Algo bellman_ford step 4645 current loss 0.312868, current_train_items 148672.
I0302 19:00:40.730325 22699590365312 run.py:483] Algo bellman_ford step 4646 current loss 0.404149, current_train_items 148704.
I0302 19:00:40.752847 22699590365312 run.py:483] Algo bellman_ford step 4647 current loss 0.514329, current_train_items 148736.
I0302 19:00:40.781821 22699590365312 run.py:483] Algo bellman_ford step 4648 current loss 0.627360, current_train_items 148768.
I0302 19:00:40.813297 22699590365312 run.py:483] Algo bellman_ford step 4649 current loss 0.709910, current_train_items 148800.
I0302 19:00:40.833059 22699590365312 run.py:483] Algo bellman_ford step 4650 current loss 0.271329, current_train_items 148832.
I0302 19:00:40.841243 22699590365312 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0302 19:00:40.841351 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:00:40.858537 22699590365312 run.py:483] Algo bellman_ford step 4651 current loss 0.486490, current_train_items 148864.
I0302 19:00:40.882550 22699590365312 run.py:483] Algo bellman_ford step 4652 current loss 0.607126, current_train_items 148896.
I0302 19:00:40.914970 22699590365312 run.py:483] Algo bellman_ford step 4653 current loss 0.653923, current_train_items 148928.
I0302 19:00:40.949015 22699590365312 run.py:483] Algo bellman_ford step 4654 current loss 0.854271, current_train_items 148960.
I0302 19:00:40.969309 22699590365312 run.py:483] Algo bellman_ford step 4655 current loss 0.332562, current_train_items 148992.
I0302 19:00:40.985266 22699590365312 run.py:483] Algo bellman_ford step 4656 current loss 0.540206, current_train_items 149024.
I0302 19:00:41.009226 22699590365312 run.py:483] Algo bellman_ford step 4657 current loss 0.590752, current_train_items 149056.
I0302 19:00:41.039855 22699590365312 run.py:483] Algo bellman_ford step 4658 current loss 0.670877, current_train_items 149088.
I0302 19:00:41.072481 22699590365312 run.py:483] Algo bellman_ford step 4659 current loss 0.671207, current_train_items 149120.
I0302 19:00:41.092109 22699590365312 run.py:483] Algo bellman_ford step 4660 current loss 0.334460, current_train_items 149152.
I0302 19:00:41.108475 22699590365312 run.py:483] Algo bellman_ford step 4661 current loss 0.495556, current_train_items 149184.
I0302 19:00:41.132558 22699590365312 run.py:483] Algo bellman_ford step 4662 current loss 0.539247, current_train_items 149216.
I0302 19:00:41.162873 22699590365312 run.py:483] Algo bellman_ford step 4663 current loss 0.635426, current_train_items 149248.
I0302 19:00:41.200079 22699590365312 run.py:483] Algo bellman_ford step 4664 current loss 0.908986, current_train_items 149280.
I0302 19:00:41.219905 22699590365312 run.py:483] Algo bellman_ford step 4665 current loss 0.308980, current_train_items 149312.
I0302 19:00:41.235870 22699590365312 run.py:483] Algo bellman_ford step 4666 current loss 0.423019, current_train_items 149344.
I0302 19:00:41.259003 22699590365312 run.py:483] Algo bellman_ford step 4667 current loss 0.562976, current_train_items 149376.
I0302 19:00:41.290987 22699590365312 run.py:483] Algo bellman_ford step 4668 current loss 0.677995, current_train_items 149408.
I0302 19:00:41.325316 22699590365312 run.py:483] Algo bellman_ford step 4669 current loss 0.708677, current_train_items 149440.
I0302 19:00:41.345377 22699590365312 run.py:483] Algo bellman_ford step 4670 current loss 0.324696, current_train_items 149472.
I0302 19:00:41.361212 22699590365312 run.py:483] Algo bellman_ford step 4671 current loss 0.405035, current_train_items 149504.
I0302 19:00:41.383580 22699590365312 run.py:483] Algo bellman_ford step 4672 current loss 0.610084, current_train_items 149536.
I0302 19:00:41.413354 22699590365312 run.py:483] Algo bellman_ford step 4673 current loss 0.668009, current_train_items 149568.
I0302 19:00:41.446969 22699590365312 run.py:483] Algo bellman_ford step 4674 current loss 0.635972, current_train_items 149600.
I0302 19:00:41.466704 22699590365312 run.py:483] Algo bellman_ford step 4675 current loss 0.270548, current_train_items 149632.
I0302 19:00:41.483440 22699590365312 run.py:483] Algo bellman_ford step 4676 current loss 0.534530, current_train_items 149664.
I0302 19:00:41.507088 22699590365312 run.py:483] Algo bellman_ford step 4677 current loss 0.632144, current_train_items 149696.
I0302 19:00:41.539487 22699590365312 run.py:483] Algo bellman_ford step 4678 current loss 0.747804, current_train_items 149728.
I0302 19:00:41.573148 22699590365312 run.py:483] Algo bellman_ford step 4679 current loss 0.748473, current_train_items 149760.
I0302 19:00:41.592782 22699590365312 run.py:483] Algo bellman_ford step 4680 current loss 0.301369, current_train_items 149792.
I0302 19:00:41.608570 22699590365312 run.py:483] Algo bellman_ford step 4681 current loss 0.422518, current_train_items 149824.
I0302 19:00:41.631965 22699590365312 run.py:483] Algo bellman_ford step 4682 current loss 0.590546, current_train_items 149856.
I0302 19:00:41.662683 22699590365312 run.py:483] Algo bellman_ford step 4683 current loss 0.676637, current_train_items 149888.
I0302 19:00:41.697457 22699590365312 run.py:483] Algo bellman_ford step 4684 current loss 0.878804, current_train_items 149920.
I0302 19:00:41.717581 22699590365312 run.py:483] Algo bellman_ford step 4685 current loss 0.292303, current_train_items 149952.
I0302 19:00:41.733431 22699590365312 run.py:483] Algo bellman_ford step 4686 current loss 0.409665, current_train_items 149984.
I0302 19:00:41.757000 22699590365312 run.py:483] Algo bellman_ford step 4687 current loss 0.726143, current_train_items 150016.
I0302 19:00:41.788723 22699590365312 run.py:483] Algo bellman_ford step 4688 current loss 0.760071, current_train_items 150048.
I0302 19:00:41.823668 22699590365312 run.py:483] Algo bellman_ford step 4689 current loss 0.879106, current_train_items 150080.
I0302 19:00:41.843657 22699590365312 run.py:483] Algo bellman_ford step 4690 current loss 0.285394, current_train_items 150112.
I0302 19:00:41.859777 22699590365312 run.py:483] Algo bellman_ford step 4691 current loss 0.459708, current_train_items 150144.
I0302 19:00:41.882680 22699590365312 run.py:483] Algo bellman_ford step 4692 current loss 0.629671, current_train_items 150176.
I0302 19:00:41.914046 22699590365312 run.py:483] Algo bellman_ford step 4693 current loss 0.815910, current_train_items 150208.
I0302 19:00:41.946740 22699590365312 run.py:483] Algo bellman_ford step 4694 current loss 1.109264, current_train_items 150240.
I0302 19:00:41.966501 22699590365312 run.py:483] Algo bellman_ford step 4695 current loss 0.280320, current_train_items 150272.
I0302 19:00:41.982678 22699590365312 run.py:483] Algo bellman_ford step 4696 current loss 0.507755, current_train_items 150304.
I0302 19:00:42.006945 22699590365312 run.py:483] Algo bellman_ford step 4697 current loss 0.796372, current_train_items 150336.
I0302 19:00:42.037520 22699590365312 run.py:483] Algo bellman_ford step 4698 current loss 0.669214, current_train_items 150368.
I0302 19:00:42.069548 22699590365312 run.py:483] Algo bellman_ford step 4699 current loss 0.796291, current_train_items 150400.
I0302 19:00:42.089739 22699590365312 run.py:483] Algo bellman_ford step 4700 current loss 0.430383, current_train_items 150432.
I0302 19:00:42.097558 22699590365312 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0302 19:00:42.097665 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 19:00:42.114100 22699590365312 run.py:483] Algo bellman_ford step 4701 current loss 0.455221, current_train_items 150464.
I0302 19:00:42.138206 22699590365312 run.py:483] Algo bellman_ford step 4702 current loss 0.656269, current_train_items 150496.
I0302 19:00:42.170616 22699590365312 run.py:483] Algo bellman_ford step 4703 current loss 0.750840, current_train_items 150528.
I0302 19:00:42.205857 22699590365312 run.py:483] Algo bellman_ford step 4704 current loss 0.818547, current_train_items 150560.
I0302 19:00:42.225688 22699590365312 run.py:483] Algo bellman_ford step 4705 current loss 0.405487, current_train_items 150592.
I0302 19:00:42.241302 22699590365312 run.py:483] Algo bellman_ford step 4706 current loss 0.387894, current_train_items 150624.
I0302 19:00:42.264973 22699590365312 run.py:483] Algo bellman_ford step 4707 current loss 0.633518, current_train_items 150656.
I0302 19:00:42.297034 22699590365312 run.py:483] Algo bellman_ford step 4708 current loss 0.820496, current_train_items 150688.
I0302 19:00:42.330512 22699590365312 run.py:483] Algo bellman_ford step 4709 current loss 0.778208, current_train_items 150720.
I0302 19:00:42.349764 22699590365312 run.py:483] Algo bellman_ford step 4710 current loss 0.363819, current_train_items 150752.
I0302 19:00:42.365917 22699590365312 run.py:483] Algo bellman_ford step 4711 current loss 0.455588, current_train_items 150784.
I0302 19:00:42.389867 22699590365312 run.py:483] Algo bellman_ford step 4712 current loss 0.605289, current_train_items 150816.
I0302 19:00:42.420789 22699590365312 run.py:483] Algo bellman_ford step 4713 current loss 0.598680, current_train_items 150848.
I0302 19:00:42.452803 22699590365312 run.py:483] Algo bellman_ford step 4714 current loss 0.753819, current_train_items 150880.
I0302 19:00:42.472258 22699590365312 run.py:483] Algo bellman_ford step 4715 current loss 0.282363, current_train_items 150912.
I0302 19:00:42.488492 22699590365312 run.py:483] Algo bellman_ford step 4716 current loss 0.636728, current_train_items 150944.
I0302 19:00:42.510379 22699590365312 run.py:483] Algo bellman_ford step 4717 current loss 0.546528, current_train_items 150976.
I0302 19:00:42.541449 22699590365312 run.py:483] Algo bellman_ford step 4718 current loss 0.755251, current_train_items 151008.
I0302 19:00:42.576881 22699590365312 run.py:483] Algo bellman_ford step 4719 current loss 0.823274, current_train_items 151040.
I0302 19:00:42.596122 22699590365312 run.py:483] Algo bellman_ford step 4720 current loss 0.322767, current_train_items 151072.
I0302 19:00:42.611935 22699590365312 run.py:483] Algo bellman_ford step 4721 current loss 0.524743, current_train_items 151104.
I0302 19:00:42.636081 22699590365312 run.py:483] Algo bellman_ford step 4722 current loss 0.640856, current_train_items 151136.
I0302 19:00:42.665390 22699590365312 run.py:483] Algo bellman_ford step 4723 current loss 0.654882, current_train_items 151168.
I0302 19:00:42.699171 22699590365312 run.py:483] Algo bellman_ford step 4724 current loss 0.758770, current_train_items 151200.
I0302 19:00:42.718535 22699590365312 run.py:483] Algo bellman_ford step 4725 current loss 0.341267, current_train_items 151232.
I0302 19:00:42.734949 22699590365312 run.py:483] Algo bellman_ford step 4726 current loss 0.500258, current_train_items 151264.
I0302 19:00:42.759452 22699590365312 run.py:483] Algo bellman_ford step 4727 current loss 0.714753, current_train_items 151296.
I0302 19:00:42.788523 22699590365312 run.py:483] Algo bellman_ford step 4728 current loss 0.633391, current_train_items 151328.
I0302 19:00:42.818239 22699590365312 run.py:483] Algo bellman_ford step 4729 current loss 0.627127, current_train_items 151360.
I0302 19:00:42.837824 22699590365312 run.py:483] Algo bellman_ford step 4730 current loss 0.317993, current_train_items 151392.
I0302 19:00:42.853695 22699590365312 run.py:483] Algo bellman_ford step 4731 current loss 0.395025, current_train_items 151424.
I0302 19:00:42.876278 22699590365312 run.py:483] Algo bellman_ford step 4732 current loss 0.553012, current_train_items 151456.
I0302 19:00:42.906786 22699590365312 run.py:483] Algo bellman_ford step 4733 current loss 0.765980, current_train_items 151488.
I0302 19:00:42.940428 22699590365312 run.py:483] Algo bellman_ford step 4734 current loss 0.785874, current_train_items 151520.
I0302 19:00:42.959580 22699590365312 run.py:483] Algo bellman_ford step 4735 current loss 0.286664, current_train_items 151552.
I0302 19:00:42.975960 22699590365312 run.py:483] Algo bellman_ford step 4736 current loss 0.480323, current_train_items 151584.
I0302 19:00:42.998778 22699590365312 run.py:483] Algo bellman_ford step 4737 current loss 0.707174, current_train_items 151616.
I0302 19:00:43.028754 22699590365312 run.py:483] Algo bellman_ford step 4738 current loss 0.708056, current_train_items 151648.
I0302 19:00:43.061182 22699590365312 run.py:483] Algo bellman_ford step 4739 current loss 0.815177, current_train_items 151680.
I0302 19:00:43.080501 22699590365312 run.py:483] Algo bellman_ford step 4740 current loss 0.327597, current_train_items 151712.
I0302 19:00:43.096368 22699590365312 run.py:483] Algo bellman_ford step 4741 current loss 0.558510, current_train_items 151744.
I0302 19:00:43.119265 22699590365312 run.py:483] Algo bellman_ford step 4742 current loss 0.550672, current_train_items 151776.
I0302 19:00:43.150419 22699590365312 run.py:483] Algo bellman_ford step 4743 current loss 0.733592, current_train_items 151808.
I0302 19:00:43.182207 22699590365312 run.py:483] Algo bellman_ford step 4744 current loss 0.844421, current_train_items 151840.
I0302 19:00:43.201601 22699590365312 run.py:483] Algo bellman_ford step 4745 current loss 0.337653, current_train_items 151872.
I0302 19:00:43.218182 22699590365312 run.py:483] Algo bellman_ford step 4746 current loss 0.432304, current_train_items 151904.
I0302 19:00:43.242649 22699590365312 run.py:483] Algo bellman_ford step 4747 current loss 0.587617, current_train_items 151936.
I0302 19:00:43.273049 22699590365312 run.py:483] Algo bellman_ford step 4748 current loss 0.638484, current_train_items 151968.
I0302 19:00:43.306136 22699590365312 run.py:483] Algo bellman_ford step 4749 current loss 0.722529, current_train_items 152000.
I0302 19:00:43.325434 22699590365312 run.py:483] Algo bellman_ford step 4750 current loss 0.287665, current_train_items 152032.
I0302 19:00:43.333628 22699590365312 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0302 19:00:43.333731 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:00:43.350716 22699590365312 run.py:483] Algo bellman_ford step 4751 current loss 0.483680, current_train_items 152064.
I0302 19:00:43.375234 22699590365312 run.py:483] Algo bellman_ford step 4752 current loss 0.642163, current_train_items 152096.
I0302 19:00:43.408278 22699590365312 run.py:483] Algo bellman_ford step 4753 current loss 0.723012, current_train_items 152128.
I0302 19:00:43.442067 22699590365312 run.py:483] Algo bellman_ford step 4754 current loss 0.762836, current_train_items 152160.
I0302 19:00:43.461923 22699590365312 run.py:483] Algo bellman_ford step 4755 current loss 0.325179, current_train_items 152192.
I0302 19:00:43.477778 22699590365312 run.py:483] Algo bellman_ford step 4756 current loss 0.480445, current_train_items 152224.
I0302 19:00:43.501838 22699590365312 run.py:483] Algo bellman_ford step 4757 current loss 0.595952, current_train_items 152256.
I0302 19:00:43.532742 22699590365312 run.py:483] Algo bellman_ford step 4758 current loss 0.635096, current_train_items 152288.
I0302 19:00:43.567480 22699590365312 run.py:483] Algo bellman_ford step 4759 current loss 0.950517, current_train_items 152320.
I0302 19:00:43.587416 22699590365312 run.py:483] Algo bellman_ford step 4760 current loss 0.332040, current_train_items 152352.
I0302 19:00:43.603698 22699590365312 run.py:483] Algo bellman_ford step 4761 current loss 0.450517, current_train_items 152384.
I0302 19:00:43.627017 22699590365312 run.py:483] Algo bellman_ford step 4762 current loss 0.606154, current_train_items 152416.
I0302 19:00:43.657379 22699590365312 run.py:483] Algo bellman_ford step 4763 current loss 0.665744, current_train_items 152448.
I0302 19:00:43.691301 22699590365312 run.py:483] Algo bellman_ford step 4764 current loss 0.864002, current_train_items 152480.
I0302 19:00:43.710739 22699590365312 run.py:483] Algo bellman_ford step 4765 current loss 0.452939, current_train_items 152512.
I0302 19:00:43.726355 22699590365312 run.py:483] Algo bellman_ford step 4766 current loss 0.386802, current_train_items 152544.
I0302 19:00:43.750583 22699590365312 run.py:483] Algo bellman_ford step 4767 current loss 0.575774, current_train_items 152576.
I0302 19:00:43.780288 22699590365312 run.py:483] Algo bellman_ford step 4768 current loss 0.648012, current_train_items 152608.
I0302 19:00:43.813052 22699590365312 run.py:483] Algo bellman_ford step 4769 current loss 0.808762, current_train_items 152640.
I0302 19:00:43.832931 22699590365312 run.py:483] Algo bellman_ford step 4770 current loss 0.450314, current_train_items 152672.
I0302 19:00:43.848824 22699590365312 run.py:483] Algo bellman_ford step 4771 current loss 0.348555, current_train_items 152704.
I0302 19:00:43.870988 22699590365312 run.py:483] Algo bellman_ford step 4772 current loss 0.528921, current_train_items 152736.
I0302 19:00:43.901967 22699590365312 run.py:483] Algo bellman_ford step 4773 current loss 0.781541, current_train_items 152768.
I0302 19:00:43.936118 22699590365312 run.py:483] Algo bellman_ford step 4774 current loss 0.790758, current_train_items 152800.
I0302 19:00:43.956221 22699590365312 run.py:483] Algo bellman_ford step 4775 current loss 0.295566, current_train_items 152832.
I0302 19:00:43.972546 22699590365312 run.py:483] Algo bellman_ford step 4776 current loss 0.426659, current_train_items 152864.
I0302 19:00:43.995918 22699590365312 run.py:483] Algo bellman_ford step 4777 current loss 0.546503, current_train_items 152896.
I0302 19:00:44.027185 22699590365312 run.py:483] Algo bellman_ford step 4778 current loss 0.690322, current_train_items 152928.
I0302 19:00:44.059417 22699590365312 run.py:483] Algo bellman_ford step 4779 current loss 0.676739, current_train_items 152960.
I0302 19:00:44.079226 22699590365312 run.py:483] Algo bellman_ford step 4780 current loss 0.272080, current_train_items 152992.
I0302 19:00:44.095384 22699590365312 run.py:483] Algo bellman_ford step 4781 current loss 0.528444, current_train_items 153024.
I0302 19:00:44.119386 22699590365312 run.py:483] Algo bellman_ford step 4782 current loss 0.592309, current_train_items 153056.
I0302 19:00:44.151294 22699590365312 run.py:483] Algo bellman_ford step 4783 current loss 0.685876, current_train_items 153088.
I0302 19:00:44.185763 22699590365312 run.py:483] Algo bellman_ford step 4784 current loss 0.732076, current_train_items 153120.
I0302 19:00:44.205795 22699590365312 run.py:483] Algo bellman_ford step 4785 current loss 0.545955, current_train_items 153152.
I0302 19:00:44.222319 22699590365312 run.py:483] Algo bellman_ford step 4786 current loss 0.457055, current_train_items 153184.
I0302 19:00:44.246221 22699590365312 run.py:483] Algo bellman_ford step 4787 current loss 0.570718, current_train_items 153216.
I0302 19:00:44.276392 22699590365312 run.py:483] Algo bellman_ford step 4788 current loss 0.620701, current_train_items 153248.
I0302 19:00:44.308919 22699590365312 run.py:483] Algo bellman_ford step 4789 current loss 0.775590, current_train_items 153280.
I0302 19:00:44.328521 22699590365312 run.py:483] Algo bellman_ford step 4790 current loss 0.380259, current_train_items 153312.
I0302 19:00:44.344650 22699590365312 run.py:483] Algo bellman_ford step 4791 current loss 0.469986, current_train_items 153344.
I0302 19:00:44.367947 22699590365312 run.py:483] Algo bellman_ford step 4792 current loss 0.501887, current_train_items 153376.
I0302 19:00:44.398765 22699590365312 run.py:483] Algo bellman_ford step 4793 current loss 0.728857, current_train_items 153408.
I0302 19:00:44.434408 22699590365312 run.py:483] Algo bellman_ford step 4794 current loss 0.797166, current_train_items 153440.
I0302 19:00:44.453801 22699590365312 run.py:483] Algo bellman_ford step 4795 current loss 0.235115, current_train_items 153472.
I0302 19:00:44.470072 22699590365312 run.py:483] Algo bellman_ford step 4796 current loss 0.400207, current_train_items 153504.
I0302 19:00:44.494130 22699590365312 run.py:483] Algo bellman_ford step 4797 current loss 0.572775, current_train_items 153536.
I0302 19:00:44.524787 22699590365312 run.py:483] Algo bellman_ford step 4798 current loss 0.592388, current_train_items 153568.
I0302 19:00:44.557696 22699590365312 run.py:483] Algo bellman_ford step 4799 current loss 0.754489, current_train_items 153600.
I0302 19:00:44.577638 22699590365312 run.py:483] Algo bellman_ford step 4800 current loss 0.280322, current_train_items 153632.
I0302 19:00:44.585725 22699590365312 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0302 19:00:44.585831 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:00:44.602643 22699590365312 run.py:483] Algo bellman_ford step 4801 current loss 0.582146, current_train_items 153664.
I0302 19:00:44.626737 22699590365312 run.py:483] Algo bellman_ford step 4802 current loss 0.633085, current_train_items 153696.
I0302 19:00:44.657200 22699590365312 run.py:483] Algo bellman_ford step 4803 current loss 0.607229, current_train_items 153728.
I0302 19:00:44.694000 22699590365312 run.py:483] Algo bellman_ford step 4804 current loss 0.780227, current_train_items 153760.
I0302 19:00:44.713840 22699590365312 run.py:483] Algo bellman_ford step 4805 current loss 0.340697, current_train_items 153792.
I0302 19:00:44.730287 22699590365312 run.py:483] Algo bellman_ford step 4806 current loss 0.496428, current_train_items 153824.
I0302 19:00:44.753881 22699590365312 run.py:483] Algo bellman_ford step 4807 current loss 0.677549, current_train_items 153856.
I0302 19:00:44.785125 22699590365312 run.py:483] Algo bellman_ford step 4808 current loss 0.754556, current_train_items 153888.
I0302 19:00:44.818822 22699590365312 run.py:483] Algo bellman_ford step 4809 current loss 0.825954, current_train_items 153920.
I0302 19:00:44.838836 22699590365312 run.py:483] Algo bellman_ford step 4810 current loss 0.325267, current_train_items 153952.
I0302 19:00:44.854961 22699590365312 run.py:483] Algo bellman_ford step 4811 current loss 0.477527, current_train_items 153984.
I0302 19:00:44.878134 22699590365312 run.py:483] Algo bellman_ford step 4812 current loss 0.634140, current_train_items 154016.
I0302 19:00:44.908573 22699590365312 run.py:483] Algo bellman_ford step 4813 current loss 0.644868, current_train_items 154048.
I0302 19:00:44.943193 22699590365312 run.py:483] Algo bellman_ford step 4814 current loss 0.818170, current_train_items 154080.
I0302 19:00:44.962932 22699590365312 run.py:483] Algo bellman_ford step 4815 current loss 0.364315, current_train_items 154112.
I0302 19:00:44.979131 22699590365312 run.py:483] Algo bellman_ford step 4816 current loss 0.427244, current_train_items 154144.
I0302 19:00:45.004053 22699590365312 run.py:483] Algo bellman_ford step 4817 current loss 0.679160, current_train_items 154176.
I0302 19:00:45.035249 22699590365312 run.py:483] Algo bellman_ford step 4818 current loss 0.756182, current_train_items 154208.
I0302 19:00:45.065169 22699590365312 run.py:483] Algo bellman_ford step 4819 current loss 0.660119, current_train_items 154240.
I0302 19:00:45.085167 22699590365312 run.py:483] Algo bellman_ford step 4820 current loss 0.292236, current_train_items 154272.
I0302 19:00:45.101258 22699590365312 run.py:483] Algo bellman_ford step 4821 current loss 0.503485, current_train_items 154304.
I0302 19:00:45.125240 22699590365312 run.py:483] Algo bellman_ford step 4822 current loss 0.626701, current_train_items 154336.
I0302 19:00:45.155598 22699590365312 run.py:483] Algo bellman_ford step 4823 current loss 0.786620, current_train_items 154368.
I0302 19:00:45.189987 22699590365312 run.py:483] Algo bellman_ford step 4824 current loss 0.842509, current_train_items 154400.
I0302 19:00:45.209723 22699590365312 run.py:483] Algo bellman_ford step 4825 current loss 0.300882, current_train_items 154432.
I0302 19:00:45.226377 22699590365312 run.py:483] Algo bellman_ford step 4826 current loss 0.383000, current_train_items 154464.
I0302 19:00:45.249580 22699590365312 run.py:483] Algo bellman_ford step 4827 current loss 0.648700, current_train_items 154496.
I0302 19:00:45.281167 22699590365312 run.py:483] Algo bellman_ford step 4828 current loss 0.629481, current_train_items 154528.
I0302 19:00:45.315510 22699590365312 run.py:483] Algo bellman_ford step 4829 current loss 0.806396, current_train_items 154560.
I0302 19:00:45.334930 22699590365312 run.py:483] Algo bellman_ford step 4830 current loss 0.242581, current_train_items 154592.
I0302 19:00:45.351174 22699590365312 run.py:483] Algo bellman_ford step 4831 current loss 0.476434, current_train_items 154624.
I0302 19:00:45.375205 22699590365312 run.py:483] Algo bellman_ford step 4832 current loss 0.612599, current_train_items 154656.
I0302 19:00:45.406829 22699590365312 run.py:483] Algo bellman_ford step 4833 current loss 0.777103, current_train_items 154688.
I0302 19:00:45.439777 22699590365312 run.py:483] Algo bellman_ford step 4834 current loss 0.699649, current_train_items 154720.
I0302 19:00:45.458900 22699590365312 run.py:483] Algo bellman_ford step 4835 current loss 0.214101, current_train_items 154752.
I0302 19:00:45.474939 22699590365312 run.py:483] Algo bellman_ford step 4836 current loss 0.402878, current_train_items 154784.
I0302 19:00:45.497626 22699590365312 run.py:483] Algo bellman_ford step 4837 current loss 0.591766, current_train_items 154816.
I0302 19:00:45.528402 22699590365312 run.py:483] Algo bellman_ford step 4838 current loss 0.663922, current_train_items 154848.
I0302 19:00:45.559922 22699590365312 run.py:483] Algo bellman_ford step 4839 current loss 0.736031, current_train_items 154880.
I0302 19:00:45.579384 22699590365312 run.py:483] Algo bellman_ford step 4840 current loss 0.295920, current_train_items 154912.
I0302 19:00:45.595293 22699590365312 run.py:483] Algo bellman_ford step 4841 current loss 0.423139, current_train_items 154944.
I0302 19:00:45.618806 22699590365312 run.py:483] Algo bellman_ford step 4842 current loss 0.699733, current_train_items 154976.
I0302 19:00:45.649041 22699590365312 run.py:483] Algo bellman_ford step 4843 current loss 0.676737, current_train_items 155008.
I0302 19:00:45.683711 22699590365312 run.py:483] Algo bellman_ford step 4844 current loss 0.971873, current_train_items 155040.
I0302 19:00:45.703456 22699590365312 run.py:483] Algo bellman_ford step 4845 current loss 0.384285, current_train_items 155072.
I0302 19:00:45.719902 22699590365312 run.py:483] Algo bellman_ford step 4846 current loss 0.495451, current_train_items 155104.
I0302 19:00:45.742610 22699590365312 run.py:483] Algo bellman_ford step 4847 current loss 0.590734, current_train_items 155136.
I0302 19:00:45.772971 22699590365312 run.py:483] Algo bellman_ford step 4848 current loss 0.762950, current_train_items 155168.
I0302 19:00:45.806499 22699590365312 run.py:483] Algo bellman_ford step 4849 current loss 0.773410, current_train_items 155200.
I0302 19:00:45.825951 22699590365312 run.py:483] Algo bellman_ford step 4850 current loss 0.332108, current_train_items 155232.
I0302 19:00:45.834223 22699590365312 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.939453125, 'score': 0.939453125, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0302 19:00:45.834329 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.939, val scores are: bellman_ford: 0.939
I0302 19:00:45.851448 22699590365312 run.py:483] Algo bellman_ford step 4851 current loss 0.458595, current_train_items 155264.
I0302 19:00:45.874177 22699590365312 run.py:483] Algo bellman_ford step 4852 current loss 0.597568, current_train_items 155296.
I0302 19:00:45.904716 22699590365312 run.py:483] Algo bellman_ford step 4853 current loss 0.641002, current_train_items 155328.
I0302 19:00:45.938626 22699590365312 run.py:483] Algo bellman_ford step 4854 current loss 0.699742, current_train_items 155360.
I0302 19:00:45.958867 22699590365312 run.py:483] Algo bellman_ford step 4855 current loss 0.341484, current_train_items 155392.
I0302 19:00:45.975253 22699590365312 run.py:483] Algo bellman_ford step 4856 current loss 0.508104, current_train_items 155424.
I0302 19:00:45.999535 22699590365312 run.py:483] Algo bellman_ford step 4857 current loss 0.592769, current_train_items 155456.
I0302 19:00:46.030955 22699590365312 run.py:483] Algo bellman_ford step 4858 current loss 0.663002, current_train_items 155488.
I0302 19:00:46.066244 22699590365312 run.py:483] Algo bellman_ford step 4859 current loss 0.795960, current_train_items 155520.
I0302 19:00:46.085867 22699590365312 run.py:483] Algo bellman_ford step 4860 current loss 0.294032, current_train_items 155552.
I0302 19:00:46.102972 22699590365312 run.py:483] Algo bellman_ford step 4861 current loss 0.525147, current_train_items 155584.
I0302 19:00:46.126102 22699590365312 run.py:483] Algo bellman_ford step 4862 current loss 0.643635, current_train_items 155616.
I0302 19:00:46.157137 22699590365312 run.py:483] Algo bellman_ford step 4863 current loss 0.608351, current_train_items 155648.
I0302 19:00:46.189428 22699590365312 run.py:483] Algo bellman_ford step 4864 current loss 0.647201, current_train_items 155680.
I0302 19:00:46.208701 22699590365312 run.py:483] Algo bellman_ford step 4865 current loss 0.295366, current_train_items 155712.
I0302 19:00:46.224667 22699590365312 run.py:483] Algo bellman_ford step 4866 current loss 0.486203, current_train_items 155744.
I0302 19:00:46.248447 22699590365312 run.py:483] Algo bellman_ford step 4867 current loss 0.693722, current_train_items 155776.
I0302 19:00:46.280253 22699590365312 run.py:483] Algo bellman_ford step 4868 current loss 0.741989, current_train_items 155808.
I0302 19:00:46.312505 22699590365312 run.py:483] Algo bellman_ford step 4869 current loss 0.714545, current_train_items 155840.
I0302 19:00:46.332252 22699590365312 run.py:483] Algo bellman_ford step 4870 current loss 0.295168, current_train_items 155872.
I0302 19:00:46.348411 22699590365312 run.py:483] Algo bellman_ford step 4871 current loss 0.503044, current_train_items 155904.
I0302 19:00:46.371302 22699590365312 run.py:483] Algo bellman_ford step 4872 current loss 0.528207, current_train_items 155936.
I0302 19:00:46.400350 22699590365312 run.py:483] Algo bellman_ford step 4873 current loss 0.540147, current_train_items 155968.
I0302 19:00:46.432743 22699590365312 run.py:483] Algo bellman_ford step 4874 current loss 0.781975, current_train_items 156000.
I0302 19:00:46.452388 22699590365312 run.py:483] Algo bellman_ford step 4875 current loss 0.235533, current_train_items 156032.
I0302 19:00:46.468370 22699590365312 run.py:483] Algo bellman_ford step 4876 current loss 0.460232, current_train_items 156064.
I0302 19:00:46.490895 22699590365312 run.py:483] Algo bellman_ford step 4877 current loss 0.530165, current_train_items 156096.
I0302 19:00:46.520557 22699590365312 run.py:483] Algo bellman_ford step 4878 current loss 0.639282, current_train_items 156128.
I0302 19:00:46.554045 22699590365312 run.py:483] Algo bellman_ford step 4879 current loss 0.761718, current_train_items 156160.
I0302 19:00:46.573628 22699590365312 run.py:483] Algo bellman_ford step 4880 current loss 0.305430, current_train_items 156192.
I0302 19:00:46.589643 22699590365312 run.py:483] Algo bellman_ford step 4881 current loss 0.370396, current_train_items 156224.
I0302 19:00:46.613224 22699590365312 run.py:483] Algo bellman_ford step 4882 current loss 0.591740, current_train_items 156256.
I0302 19:00:46.644376 22699590365312 run.py:483] Algo bellman_ford step 4883 current loss 0.721756, current_train_items 156288.
I0302 19:00:46.678703 22699590365312 run.py:483] Algo bellman_ford step 4884 current loss 0.783804, current_train_items 156320.
I0302 19:00:46.698242 22699590365312 run.py:483] Algo bellman_ford step 4885 current loss 0.324162, current_train_items 156352.
I0302 19:00:46.714715 22699590365312 run.py:483] Algo bellman_ford step 4886 current loss 0.517549, current_train_items 156384.
I0302 19:00:46.737997 22699590365312 run.py:483] Algo bellman_ford step 4887 current loss 0.603367, current_train_items 156416.
I0302 19:00:46.768241 22699590365312 run.py:483] Algo bellman_ford step 4888 current loss 0.594953, current_train_items 156448.
I0302 19:00:46.800866 22699590365312 run.py:483] Algo bellman_ford step 4889 current loss 0.702165, current_train_items 156480.
I0302 19:00:46.820671 22699590365312 run.py:483] Algo bellman_ford step 4890 current loss 0.262681, current_train_items 156512.
I0302 19:00:46.836707 22699590365312 run.py:483] Algo bellman_ford step 4891 current loss 0.448517, current_train_items 156544.
I0302 19:00:46.859868 22699590365312 run.py:483] Algo bellman_ford step 4892 current loss 0.642205, current_train_items 156576.
I0302 19:00:46.889211 22699590365312 run.py:483] Algo bellman_ford step 4893 current loss 0.731540, current_train_items 156608.
I0302 19:00:46.921439 22699590365312 run.py:483] Algo bellman_ford step 4894 current loss 0.869176, current_train_items 156640.
I0302 19:00:46.940635 22699590365312 run.py:483] Algo bellman_ford step 4895 current loss 0.227268, current_train_items 156672.
I0302 19:00:46.956829 22699590365312 run.py:483] Algo bellman_ford step 4896 current loss 0.421752, current_train_items 156704.
I0302 19:00:46.979574 22699590365312 run.py:483] Algo bellman_ford step 4897 current loss 0.554820, current_train_items 156736.
I0302 19:00:47.009216 22699590365312 run.py:483] Algo bellman_ford step 4898 current loss 0.718895, current_train_items 156768.
I0302 19:00:47.043194 22699590365312 run.py:483] Algo bellman_ford step 4899 current loss 0.855562, current_train_items 156800.
I0302 19:00:47.062937 22699590365312 run.py:483] Algo bellman_ford step 4900 current loss 0.360083, current_train_items 156832.
I0302 19:00:47.070803 22699590365312 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.935546875, 'score': 0.935546875, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0302 19:00:47.070935 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.936, val scores are: bellman_ford: 0.936
I0302 19:00:47.087385 22699590365312 run.py:483] Algo bellman_ford step 4901 current loss 0.428398, current_train_items 156864.
I0302 19:00:47.111212 22699590365312 run.py:483] Algo bellman_ford step 4902 current loss 0.618516, current_train_items 156896.
I0302 19:00:47.144110 22699590365312 run.py:483] Algo bellman_ford step 4903 current loss 0.712467, current_train_items 156928.
I0302 19:00:47.177171 22699590365312 run.py:483] Algo bellman_ford step 4904 current loss 0.792980, current_train_items 156960.
I0302 19:00:47.197000 22699590365312 run.py:483] Algo bellman_ford step 4905 current loss 0.425762, current_train_items 156992.
I0302 19:00:47.212727 22699590365312 run.py:483] Algo bellman_ford step 4906 current loss 0.477242, current_train_items 157024.
I0302 19:00:47.236387 22699590365312 run.py:483] Algo bellman_ford step 4907 current loss 0.543376, current_train_items 157056.
I0302 19:00:47.267228 22699590365312 run.py:483] Algo bellman_ford step 4908 current loss 0.637048, current_train_items 157088.
I0302 19:00:47.299773 22699590365312 run.py:483] Algo bellman_ford step 4909 current loss 0.740878, current_train_items 157120.
I0302 19:00:47.319231 22699590365312 run.py:483] Algo bellman_ford step 4910 current loss 0.358130, current_train_items 157152.
I0302 19:00:47.335568 22699590365312 run.py:483] Algo bellman_ford step 4911 current loss 0.485048, current_train_items 157184.
I0302 19:00:47.359208 22699590365312 run.py:483] Algo bellman_ford step 4912 current loss 0.546490, current_train_items 157216.
I0302 19:00:47.390786 22699590365312 run.py:483] Algo bellman_ford step 4913 current loss 0.747376, current_train_items 157248.
I0302 19:00:47.423583 22699590365312 run.py:483] Algo bellman_ford step 4914 current loss 0.767313, current_train_items 157280.
I0302 19:00:47.442792 22699590365312 run.py:483] Algo bellman_ford step 4915 current loss 0.272572, current_train_items 157312.
I0302 19:00:47.458681 22699590365312 run.py:483] Algo bellman_ford step 4916 current loss 0.475632, current_train_items 157344.
I0302 19:00:47.482035 22699590365312 run.py:483] Algo bellman_ford step 4917 current loss 0.625386, current_train_items 157376.
I0302 19:00:47.511791 22699590365312 run.py:483] Algo bellman_ford step 4918 current loss 0.671236, current_train_items 157408.
I0302 19:00:47.544815 22699590365312 run.py:483] Algo bellman_ford step 4919 current loss 0.718159, current_train_items 157440.
I0302 19:00:47.564132 22699590365312 run.py:483] Algo bellman_ford step 4920 current loss 0.317949, current_train_items 157472.
I0302 19:00:47.579706 22699590365312 run.py:483] Algo bellman_ford step 4921 current loss 0.342451, current_train_items 157504.
I0302 19:00:47.603121 22699590365312 run.py:483] Algo bellman_ford step 4922 current loss 0.550362, current_train_items 157536.
I0302 19:00:47.633647 22699590365312 run.py:483] Algo bellman_ford step 4923 current loss 0.680597, current_train_items 157568.
I0302 19:00:47.666975 22699590365312 run.py:483] Algo bellman_ford step 4924 current loss 0.755525, current_train_items 157600.
I0302 19:00:47.686353 22699590365312 run.py:483] Algo bellman_ford step 4925 current loss 0.248298, current_train_items 157632.
I0302 19:00:47.702527 22699590365312 run.py:483] Algo bellman_ford step 4926 current loss 0.474324, current_train_items 157664.
I0302 19:00:47.726464 22699590365312 run.py:483] Algo bellman_ford step 4927 current loss 0.610049, current_train_items 157696.
I0302 19:00:47.756244 22699590365312 run.py:483] Algo bellman_ford step 4928 current loss 0.613157, current_train_items 157728.
I0302 19:00:47.789418 22699590365312 run.py:483] Algo bellman_ford step 4929 current loss 0.805240, current_train_items 157760.
I0302 19:00:47.808821 22699590365312 run.py:483] Algo bellman_ford step 4930 current loss 0.272664, current_train_items 157792.
I0302 19:00:47.824675 22699590365312 run.py:483] Algo bellman_ford step 4931 current loss 0.522498, current_train_items 157824.
I0302 19:00:47.847856 22699590365312 run.py:483] Algo bellman_ford step 4932 current loss 0.605742, current_train_items 157856.
I0302 19:00:47.877806 22699590365312 run.py:483] Algo bellman_ford step 4933 current loss 0.635072, current_train_items 157888.
I0302 19:00:47.911243 22699590365312 run.py:483] Algo bellman_ford step 4934 current loss 0.858950, current_train_items 157920.
I0302 19:00:47.930423 22699590365312 run.py:483] Algo bellman_ford step 4935 current loss 0.358253, current_train_items 157952.
I0302 19:00:47.946604 22699590365312 run.py:483] Algo bellman_ford step 4936 current loss 0.430519, current_train_items 157984.
I0302 19:00:47.971269 22699590365312 run.py:483] Algo bellman_ford step 4937 current loss 0.613163, current_train_items 158016.
I0302 19:00:48.001676 22699590365312 run.py:483] Algo bellman_ford step 4938 current loss 0.637528, current_train_items 158048.
I0302 19:00:48.036635 22699590365312 run.py:483] Algo bellman_ford step 4939 current loss 0.853178, current_train_items 158080.
I0302 19:00:48.055860 22699590365312 run.py:483] Algo bellman_ford step 4940 current loss 0.317807, current_train_items 158112.
I0302 19:00:48.071643 22699590365312 run.py:483] Algo bellman_ford step 4941 current loss 0.400022, current_train_items 158144.
I0302 19:00:48.094674 22699590365312 run.py:483] Algo bellman_ford step 4942 current loss 0.554450, current_train_items 158176.
I0302 19:00:48.126206 22699590365312 run.py:483] Algo bellman_ford step 4943 current loss 0.661898, current_train_items 158208.
I0302 19:00:48.157846 22699590365312 run.py:483] Algo bellman_ford step 4944 current loss 0.846434, current_train_items 158240.
I0302 19:00:48.176963 22699590365312 run.py:483] Algo bellman_ford step 4945 current loss 0.389510, current_train_items 158272.
I0302 19:00:48.193046 22699590365312 run.py:483] Algo bellman_ford step 4946 current loss 0.465017, current_train_items 158304.
I0302 19:00:48.216435 22699590365312 run.py:483] Algo bellman_ford step 4947 current loss 0.545884, current_train_items 158336.
I0302 19:00:48.247354 22699590365312 run.py:483] Algo bellman_ford step 4948 current loss 0.737341, current_train_items 158368.
I0302 19:00:48.281856 22699590365312 run.py:483] Algo bellman_ford step 4949 current loss 0.707214, current_train_items 158400.
I0302 19:00:48.301073 22699590365312 run.py:483] Algo bellman_ford step 4950 current loss 0.286988, current_train_items 158432.
I0302 19:00:48.309228 22699590365312 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0302 19:00:48.309333 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 19:00:48.325792 22699590365312 run.py:483] Algo bellman_ford step 4951 current loss 0.419223, current_train_items 158464.
I0302 19:00:48.349124 22699590365312 run.py:483] Algo bellman_ford step 4952 current loss 0.546592, current_train_items 158496.
I0302 19:00:48.380321 22699590365312 run.py:483] Algo bellman_ford step 4953 current loss 0.559233, current_train_items 158528.
I0302 19:00:48.415971 22699590365312 run.py:483] Algo bellman_ford step 4954 current loss 0.638887, current_train_items 158560.
I0302 19:00:48.435948 22699590365312 run.py:483] Algo bellman_ford step 4955 current loss 0.296558, current_train_items 158592.
I0302 19:00:48.452048 22699590365312 run.py:483] Algo bellman_ford step 4956 current loss 0.452728, current_train_items 158624.
I0302 19:00:48.476399 22699590365312 run.py:483] Algo bellman_ford step 4957 current loss 0.624682, current_train_items 158656.
I0302 19:00:48.507591 22699590365312 run.py:483] Algo bellman_ford step 4958 current loss 0.719192, current_train_items 158688.
I0302 19:00:48.538611 22699590365312 run.py:483] Algo bellman_ford step 4959 current loss 0.855276, current_train_items 158720.
I0302 19:00:48.558151 22699590365312 run.py:483] Algo bellman_ford step 4960 current loss 0.288999, current_train_items 158752.
I0302 19:00:48.574486 22699590365312 run.py:483] Algo bellman_ford step 4961 current loss 0.413691, current_train_items 158784.
I0302 19:00:48.597397 22699590365312 run.py:483] Algo bellman_ford step 4962 current loss 0.571064, current_train_items 158816.
I0302 19:00:48.626841 22699590365312 run.py:483] Algo bellman_ford step 4963 current loss 0.603310, current_train_items 158848.
I0302 19:00:48.659870 22699590365312 run.py:483] Algo bellman_ford step 4964 current loss 0.723510, current_train_items 158880.
I0302 19:00:48.679137 22699590365312 run.py:483] Algo bellman_ford step 4965 current loss 0.256085, current_train_items 158912.
I0302 19:00:48.695266 22699590365312 run.py:483] Algo bellman_ford step 4966 current loss 0.452281, current_train_items 158944.
I0302 19:00:48.719285 22699590365312 run.py:483] Algo bellman_ford step 4967 current loss 0.731660, current_train_items 158976.
I0302 19:00:48.751210 22699590365312 run.py:483] Algo bellman_ford step 4968 current loss 0.690343, current_train_items 159008.
I0302 19:00:48.782631 22699590365312 run.py:483] Algo bellman_ford step 4969 current loss 0.791229, current_train_items 159040.
I0302 19:00:48.802321 22699590365312 run.py:483] Algo bellman_ford step 4970 current loss 0.305571, current_train_items 159072.
I0302 19:00:48.818417 22699590365312 run.py:483] Algo bellman_ford step 4971 current loss 0.404965, current_train_items 159104.
I0302 19:00:48.841788 22699590365312 run.py:483] Algo bellman_ford step 4972 current loss 0.549108, current_train_items 159136.
I0302 19:00:48.873533 22699590365312 run.py:483] Algo bellman_ford step 4973 current loss 0.667961, current_train_items 159168.
I0302 19:00:48.906810 22699590365312 run.py:483] Algo bellman_ford step 4974 current loss 0.758326, current_train_items 159200.
I0302 19:00:48.926443 22699590365312 run.py:483] Algo bellman_ford step 4975 current loss 0.357037, current_train_items 159232.
I0302 19:00:48.942973 22699590365312 run.py:483] Algo bellman_ford step 4976 current loss 0.411938, current_train_items 159264.
I0302 19:00:48.966597 22699590365312 run.py:483] Algo bellman_ford step 4977 current loss 0.615879, current_train_items 159296.
I0302 19:00:48.998433 22699590365312 run.py:483] Algo bellman_ford step 4978 current loss 0.669372, current_train_items 159328.
I0302 19:00:49.032749 22699590365312 run.py:483] Algo bellman_ford step 4979 current loss 0.702127, current_train_items 159360.
I0302 19:00:49.052260 22699590365312 run.py:483] Algo bellman_ford step 4980 current loss 0.300401, current_train_items 159392.
I0302 19:00:49.068507 22699590365312 run.py:483] Algo bellman_ford step 4981 current loss 0.448100, current_train_items 159424.
I0302 19:00:49.092671 22699590365312 run.py:483] Algo bellman_ford step 4982 current loss 0.592579, current_train_items 159456.
I0302 19:00:49.121410 22699590365312 run.py:483] Algo bellman_ford step 4983 current loss 0.657393, current_train_items 159488.
I0302 19:00:49.154298 22699590365312 run.py:483] Algo bellman_ford step 4984 current loss 0.770717, current_train_items 159520.
I0302 19:00:49.174011 22699590365312 run.py:483] Algo bellman_ford step 4985 current loss 0.305302, current_train_items 159552.
I0302 19:00:49.190265 22699590365312 run.py:483] Algo bellman_ford step 4986 current loss 0.434654, current_train_items 159584.
I0302 19:00:49.212158 22699590365312 run.py:483] Algo bellman_ford step 4987 current loss 0.547164, current_train_items 159616.
I0302 19:00:49.241985 22699590365312 run.py:483] Algo bellman_ford step 4988 current loss 0.682200, current_train_items 159648.
I0302 19:00:49.277804 22699590365312 run.py:483] Algo bellman_ford step 4989 current loss 0.898331, current_train_items 159680.
I0302 19:00:49.297316 22699590365312 run.py:483] Algo bellman_ford step 4990 current loss 0.327345, current_train_items 159712.
I0302 19:00:49.313190 22699590365312 run.py:483] Algo bellman_ford step 4991 current loss 0.397318, current_train_items 159744.
I0302 19:00:49.336508 22699590365312 run.py:483] Algo bellman_ford step 4992 current loss 0.591720, current_train_items 159776.
I0302 19:00:49.367145 22699590365312 run.py:483] Algo bellman_ford step 4993 current loss 0.611534, current_train_items 159808.
I0302 19:00:49.400673 22699590365312 run.py:483] Algo bellman_ford step 4994 current loss 0.766518, current_train_items 159840.
I0302 19:00:49.420278 22699590365312 run.py:483] Algo bellman_ford step 4995 current loss 0.259446, current_train_items 159872.
I0302 19:00:49.436665 22699590365312 run.py:483] Algo bellman_ford step 4996 current loss 0.429810, current_train_items 159904.
I0302 19:00:49.459762 22699590365312 run.py:483] Algo bellman_ford step 4997 current loss 0.671458, current_train_items 159936.
I0302 19:00:49.490772 22699590365312 run.py:483] Algo bellman_ford step 4998 current loss 0.733406, current_train_items 159968.
I0302 19:00:49.524855 22699590365312 run.py:483] Algo bellman_ford step 4999 current loss 0.884650, current_train_items 160000.
I0302 19:00:49.544633 22699590365312 run.py:483] Algo bellman_ford step 5000 current loss 0.375233, current_train_items 160032.
I0302 19:00:49.552448 22699590365312 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0302 19:00:49.552551 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:00:49.569653 22699590365312 run.py:483] Algo bellman_ford step 5001 current loss 0.496332, current_train_items 160064.
I0302 19:00:49.593351 22699590365312 run.py:483] Algo bellman_ford step 5002 current loss 0.641694, current_train_items 160096.
I0302 19:00:49.623290 22699590365312 run.py:483] Algo bellman_ford step 5003 current loss 0.547253, current_train_items 160128.
I0302 19:00:49.656281 22699590365312 run.py:483] Algo bellman_ford step 5004 current loss 0.729276, current_train_items 160160.
I0302 19:00:49.676229 22699590365312 run.py:483] Algo bellman_ford step 5005 current loss 0.289972, current_train_items 160192.
I0302 19:00:49.692296 22699590365312 run.py:483] Algo bellman_ford step 5006 current loss 0.447793, current_train_items 160224.
I0302 19:00:49.716481 22699590365312 run.py:483] Algo bellman_ford step 5007 current loss 0.709658, current_train_items 160256.
I0302 19:00:49.749471 22699590365312 run.py:483] Algo bellman_ford step 5008 current loss 0.741913, current_train_items 160288.
I0302 19:00:49.781423 22699590365312 run.py:483] Algo bellman_ford step 5009 current loss 0.800138, current_train_items 160320.
I0302 19:00:49.801028 22699590365312 run.py:483] Algo bellman_ford step 5010 current loss 0.328686, current_train_items 160352.
I0302 19:00:49.816945 22699590365312 run.py:483] Algo bellman_ford step 5011 current loss 0.426606, current_train_items 160384.
I0302 19:00:49.840468 22699590365312 run.py:483] Algo bellman_ford step 5012 current loss 0.568604, current_train_items 160416.
I0302 19:00:49.870615 22699590365312 run.py:483] Algo bellman_ford step 5013 current loss 0.667408, current_train_items 160448.
I0302 19:00:49.905381 22699590365312 run.py:483] Algo bellman_ford step 5014 current loss 0.750759, current_train_items 160480.
I0302 19:00:49.925307 22699590365312 run.py:483] Algo bellman_ford step 5015 current loss 0.388874, current_train_items 160512.
I0302 19:00:49.942080 22699590365312 run.py:483] Algo bellman_ford step 5016 current loss 0.575871, current_train_items 160544.
I0302 19:00:49.965108 22699590365312 run.py:483] Algo bellman_ford step 5017 current loss 0.681574, current_train_items 160576.
I0302 19:00:49.996248 22699590365312 run.py:483] Algo bellman_ford step 5018 current loss 0.675487, current_train_items 160608.
I0302 19:00:50.031443 22699590365312 run.py:483] Algo bellman_ford step 5019 current loss 0.848058, current_train_items 160640.
I0302 19:00:50.050923 22699590365312 run.py:483] Algo bellman_ford step 5020 current loss 0.268855, current_train_items 160672.
I0302 19:00:50.066843 22699590365312 run.py:483] Algo bellman_ford step 5021 current loss 0.436118, current_train_items 160704.
I0302 19:00:50.091161 22699590365312 run.py:483] Algo bellman_ford step 5022 current loss 0.685654, current_train_items 160736.
I0302 19:00:50.122050 22699590365312 run.py:483] Algo bellman_ford step 5023 current loss 0.757644, current_train_items 160768.
I0302 19:00:50.155845 22699590365312 run.py:483] Algo bellman_ford step 5024 current loss 0.752855, current_train_items 160800.
I0302 19:00:50.175559 22699590365312 run.py:483] Algo bellman_ford step 5025 current loss 0.291351, current_train_items 160832.
I0302 19:00:50.191209 22699590365312 run.py:483] Algo bellman_ford step 5026 current loss 0.428378, current_train_items 160864.
I0302 19:00:50.213885 22699590365312 run.py:483] Algo bellman_ford step 5027 current loss 0.516901, current_train_items 160896.
I0302 19:00:50.243140 22699590365312 run.py:483] Algo bellman_ford step 5028 current loss 0.541708, current_train_items 160928.
I0302 19:00:50.276353 22699590365312 run.py:483] Algo bellman_ford step 5029 current loss 0.726231, current_train_items 160960.
I0302 19:00:50.296285 22699590365312 run.py:483] Algo bellman_ford step 5030 current loss 0.245047, current_train_items 160992.
I0302 19:00:50.312466 22699590365312 run.py:483] Algo bellman_ford step 5031 current loss 0.434326, current_train_items 161024.
I0302 19:00:50.336338 22699590365312 run.py:483] Algo bellman_ford step 5032 current loss 0.582886, current_train_items 161056.
I0302 19:00:50.366261 22699590365312 run.py:483] Algo bellman_ford step 5033 current loss 0.592476, current_train_items 161088.
I0302 19:00:50.400447 22699590365312 run.py:483] Algo bellman_ford step 5034 current loss 0.833164, current_train_items 161120.
I0302 19:00:50.420421 22699590365312 run.py:483] Algo bellman_ford step 5035 current loss 0.343169, current_train_items 161152.
I0302 19:00:50.436685 22699590365312 run.py:483] Algo bellman_ford step 5036 current loss 0.454380, current_train_items 161184.
I0302 19:00:50.460226 22699590365312 run.py:483] Algo bellman_ford step 5037 current loss 0.601569, current_train_items 161216.
I0302 19:00:50.492221 22699590365312 run.py:483] Algo bellman_ford step 5038 current loss 0.772125, current_train_items 161248.
I0302 19:00:50.525687 22699590365312 run.py:483] Algo bellman_ford step 5039 current loss 0.746288, current_train_items 161280.
I0302 19:00:50.545440 22699590365312 run.py:483] Algo bellman_ford step 5040 current loss 0.314117, current_train_items 161312.
I0302 19:00:50.561003 22699590365312 run.py:483] Algo bellman_ford step 5041 current loss 0.402124, current_train_items 161344.
I0302 19:00:50.585233 22699590365312 run.py:483] Algo bellman_ford step 5042 current loss 0.635040, current_train_items 161376.
I0302 19:00:50.614958 22699590365312 run.py:483] Algo bellman_ford step 5043 current loss 0.575550, current_train_items 161408.
I0302 19:00:50.648490 22699590365312 run.py:483] Algo bellman_ford step 5044 current loss 0.730182, current_train_items 161440.
I0302 19:00:50.667664 22699590365312 run.py:483] Algo bellman_ford step 5045 current loss 0.257709, current_train_items 161472.
I0302 19:00:50.683813 22699590365312 run.py:483] Algo bellman_ford step 5046 current loss 0.480895, current_train_items 161504.
I0302 19:00:50.706526 22699590365312 run.py:483] Algo bellman_ford step 5047 current loss 0.559669, current_train_items 161536.
I0302 19:00:50.737238 22699590365312 run.py:483] Algo bellman_ford step 5048 current loss 0.696980, current_train_items 161568.
I0302 19:00:50.771134 22699590365312 run.py:483] Algo bellman_ford step 5049 current loss 0.715275, current_train_items 161600.
I0302 19:00:50.790779 22699590365312 run.py:483] Algo bellman_ford step 5050 current loss 0.347743, current_train_items 161632.
I0302 19:00:50.798880 22699590365312 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0302 19:00:50.798987 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:00:50.815837 22699590365312 run.py:483] Algo bellman_ford step 5051 current loss 0.478078, current_train_items 161664.
I0302 19:00:50.840414 22699590365312 run.py:483] Algo bellman_ford step 5052 current loss 0.600920, current_train_items 161696.
I0302 19:00:50.871567 22699590365312 run.py:483] Algo bellman_ford step 5053 current loss 0.654611, current_train_items 161728.
I0302 19:00:50.903941 22699590365312 run.py:483] Algo bellman_ford step 5054 current loss 0.730525, current_train_items 161760.
I0302 19:00:50.924116 22699590365312 run.py:483] Algo bellman_ford step 5055 current loss 0.340969, current_train_items 161792.
I0302 19:00:50.940932 22699590365312 run.py:483] Algo bellman_ford step 5056 current loss 0.483945, current_train_items 161824.
I0302 19:00:50.965494 22699590365312 run.py:483] Algo bellman_ford step 5057 current loss 0.612913, current_train_items 161856.
I0302 19:00:50.995747 22699590365312 run.py:483] Algo bellman_ford step 5058 current loss 0.672898, current_train_items 161888.
I0302 19:00:51.029505 22699590365312 run.py:483] Algo bellman_ford step 5059 current loss 0.744521, current_train_items 161920.
I0302 19:00:51.049189 22699590365312 run.py:483] Algo bellman_ford step 5060 current loss 0.285158, current_train_items 161952.
I0302 19:00:51.065249 22699590365312 run.py:483] Algo bellman_ford step 5061 current loss 0.403140, current_train_items 161984.
I0302 19:00:51.087207 22699590365312 run.py:483] Algo bellman_ford step 5062 current loss 0.569309, current_train_items 162016.
I0302 19:00:51.117650 22699590365312 run.py:483] Algo bellman_ford step 5063 current loss 0.708240, current_train_items 162048.
I0302 19:00:51.148702 22699590365312 run.py:483] Algo bellman_ford step 5064 current loss 0.718585, current_train_items 162080.
I0302 19:00:51.168167 22699590365312 run.py:483] Algo bellman_ford step 5065 current loss 0.309830, current_train_items 162112.
I0302 19:00:51.184261 22699590365312 run.py:483] Algo bellman_ford step 5066 current loss 0.446573, current_train_items 162144.
I0302 19:00:51.208501 22699590365312 run.py:483] Algo bellman_ford step 5067 current loss 0.588708, current_train_items 162176.
I0302 19:00:51.239147 22699590365312 run.py:483] Algo bellman_ford step 5068 current loss 0.581425, current_train_items 162208.
I0302 19:00:51.273614 22699590365312 run.py:483] Algo bellman_ford step 5069 current loss 0.758986, current_train_items 162240.
I0302 19:00:51.293253 22699590365312 run.py:483] Algo bellman_ford step 5070 current loss 0.273477, current_train_items 162272.
I0302 19:00:51.309808 22699590365312 run.py:483] Algo bellman_ford step 5071 current loss 0.415966, current_train_items 162304.
I0302 19:00:51.331579 22699590365312 run.py:483] Algo bellman_ford step 5072 current loss 0.620564, current_train_items 162336.
I0302 19:00:51.362972 22699590365312 run.py:483] Algo bellman_ford step 5073 current loss 0.660714, current_train_items 162368.
I0302 19:00:51.393651 22699590365312 run.py:483] Algo bellman_ford step 5074 current loss 0.618535, current_train_items 162400.
I0302 19:00:51.413304 22699590365312 run.py:483] Algo bellman_ford step 5075 current loss 0.367579, current_train_items 162432.
I0302 19:00:51.429395 22699590365312 run.py:483] Algo bellman_ford step 5076 current loss 0.510950, current_train_items 162464.
I0302 19:00:51.452294 22699590365312 run.py:483] Algo bellman_ford step 5077 current loss 0.521292, current_train_items 162496.
I0302 19:00:51.482909 22699590365312 run.py:483] Algo bellman_ford step 5078 current loss 0.808562, current_train_items 162528.
I0302 19:00:51.515250 22699590365312 run.py:483] Algo bellman_ford step 5079 current loss 0.932371, current_train_items 162560.
I0302 19:00:51.534575 22699590365312 run.py:483] Algo bellman_ford step 5080 current loss 0.286419, current_train_items 162592.
I0302 19:00:51.550356 22699590365312 run.py:483] Algo bellman_ford step 5081 current loss 0.370927, current_train_items 162624.
I0302 19:00:51.573528 22699590365312 run.py:483] Algo bellman_ford step 5082 current loss 0.545484, current_train_items 162656.
I0302 19:00:51.604511 22699590365312 run.py:483] Algo bellman_ford step 5083 current loss 0.654274, current_train_items 162688.
I0302 19:00:51.637759 22699590365312 run.py:483] Algo bellman_ford step 5084 current loss 0.696868, current_train_items 162720.
I0302 19:00:51.657670 22699590365312 run.py:483] Algo bellman_ford step 5085 current loss 0.309194, current_train_items 162752.
I0302 19:00:51.673668 22699590365312 run.py:483] Algo bellman_ford step 5086 current loss 0.486851, current_train_items 162784.
I0302 19:00:51.696862 22699590365312 run.py:483] Algo bellman_ford step 5087 current loss 0.640435, current_train_items 162816.
I0302 19:00:51.727791 22699590365312 run.py:483] Algo bellman_ford step 5088 current loss 0.555375, current_train_items 162848.
I0302 19:00:51.761973 22699590365312 run.py:483] Algo bellman_ford step 5089 current loss 0.775459, current_train_items 162880.
I0302 19:00:51.781892 22699590365312 run.py:483] Algo bellman_ford step 5090 current loss 0.317031, current_train_items 162912.
I0302 19:00:51.798210 22699590365312 run.py:483] Algo bellman_ford step 5091 current loss 0.462851, current_train_items 162944.
I0302 19:00:51.820228 22699590365312 run.py:483] Algo bellman_ford step 5092 current loss 0.562806, current_train_items 162976.
I0302 19:00:51.849847 22699590365312 run.py:483] Algo bellman_ford step 5093 current loss 0.622570, current_train_items 163008.
I0302 19:00:51.881974 22699590365312 run.py:483] Algo bellman_ford step 5094 current loss 0.721554, current_train_items 163040.
I0302 19:00:51.901190 22699590365312 run.py:483] Algo bellman_ford step 5095 current loss 0.308119, current_train_items 163072.
I0302 19:00:51.917339 22699590365312 run.py:483] Algo bellman_ford step 5096 current loss 0.538666, current_train_items 163104.
I0302 19:00:51.941726 22699590365312 run.py:483] Algo bellman_ford step 5097 current loss 0.653856, current_train_items 163136.
I0302 19:00:51.973318 22699590365312 run.py:483] Algo bellman_ford step 5098 current loss 0.874026, current_train_items 163168.
I0302 19:00:52.007664 22699590365312 run.py:483] Algo bellman_ford step 5099 current loss 0.908475, current_train_items 163200.
I0302 19:00:52.027410 22699590365312 run.py:483] Algo bellman_ford step 5100 current loss 0.261456, current_train_items 163232.
I0302 19:00:52.035457 22699590365312 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0302 19:00:52.035564 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:00:52.052092 22699590365312 run.py:483] Algo bellman_ford step 5101 current loss 0.528348, current_train_items 163264.
I0302 19:00:52.074247 22699590365312 run.py:483] Algo bellman_ford step 5102 current loss 0.470837, current_train_items 163296.
I0302 19:00:52.105905 22699590365312 run.py:483] Algo bellman_ford step 5103 current loss 0.664463, current_train_items 163328.
I0302 19:00:52.140882 22699590365312 run.py:483] Algo bellman_ford step 5104 current loss 0.922139, current_train_items 163360.
I0302 19:00:52.160952 22699590365312 run.py:483] Algo bellman_ford step 5105 current loss 0.304490, current_train_items 163392.
I0302 19:00:52.177216 22699590365312 run.py:483] Algo bellman_ford step 5106 current loss 0.549173, current_train_items 163424.
I0302 19:00:52.200663 22699590365312 run.py:483] Algo bellman_ford step 5107 current loss 0.593869, current_train_items 163456.
I0302 19:00:52.230011 22699590365312 run.py:483] Algo bellman_ford step 5108 current loss 0.651717, current_train_items 163488.
I0302 19:00:52.264289 22699590365312 run.py:483] Algo bellman_ford step 5109 current loss 0.809847, current_train_items 163520.
I0302 19:00:52.284038 22699590365312 run.py:483] Algo bellman_ford step 5110 current loss 0.295851, current_train_items 163552.
I0302 19:00:52.300307 22699590365312 run.py:483] Algo bellman_ford step 5111 current loss 0.438917, current_train_items 163584.
I0302 19:00:52.323369 22699590365312 run.py:483] Algo bellman_ford step 5112 current loss 0.525737, current_train_items 163616.
I0302 19:00:52.353456 22699590365312 run.py:483] Algo bellman_ford step 5113 current loss 0.663717, current_train_items 163648.
I0302 19:00:52.386119 22699590365312 run.py:483] Algo bellman_ford step 5114 current loss 0.717015, current_train_items 163680.
I0302 19:00:52.405724 22699590365312 run.py:483] Algo bellman_ford step 5115 current loss 0.296138, current_train_items 163712.
I0302 19:00:52.422057 22699590365312 run.py:483] Algo bellman_ford step 5116 current loss 0.452330, current_train_items 163744.
I0302 19:00:52.446299 22699590365312 run.py:483] Algo bellman_ford step 5117 current loss 0.634754, current_train_items 163776.
I0302 19:00:52.476396 22699590365312 run.py:483] Algo bellman_ford step 5118 current loss 0.642836, current_train_items 163808.
I0302 19:00:52.510696 22699590365312 run.py:483] Algo bellman_ford step 5119 current loss 0.662997, current_train_items 163840.
I0302 19:00:52.530481 22699590365312 run.py:483] Algo bellman_ford step 5120 current loss 0.314214, current_train_items 163872.
I0302 19:00:52.546222 22699590365312 run.py:483] Algo bellman_ford step 5121 current loss 0.459286, current_train_items 163904.
I0302 19:00:52.569317 22699590365312 run.py:483] Algo bellman_ford step 5122 current loss 0.584431, current_train_items 163936.
I0302 19:00:52.599066 22699590365312 run.py:483] Algo bellman_ford step 5123 current loss 0.600767, current_train_items 163968.
I0302 19:00:52.632421 22699590365312 run.py:483] Algo bellman_ford step 5124 current loss 0.719904, current_train_items 164000.
I0302 19:00:52.651674 22699590365312 run.py:483] Algo bellman_ford step 5125 current loss 0.340334, current_train_items 164032.
I0302 19:00:52.667611 22699590365312 run.py:483] Algo bellman_ford step 5126 current loss 0.431791, current_train_items 164064.
I0302 19:00:52.691348 22699590365312 run.py:483] Algo bellman_ford step 5127 current loss 0.734102, current_train_items 164096.
I0302 19:00:52.722135 22699590365312 run.py:483] Algo bellman_ford step 5128 current loss 0.701565, current_train_items 164128.
I0302 19:00:52.756737 22699590365312 run.py:483] Algo bellman_ford step 5129 current loss 0.783532, current_train_items 164160.
I0302 19:00:52.776100 22699590365312 run.py:483] Algo bellman_ford step 5130 current loss 0.338407, current_train_items 164192.
I0302 19:00:52.792425 22699590365312 run.py:483] Algo bellman_ford step 5131 current loss 0.416722, current_train_items 164224.
I0302 19:00:52.814829 22699590365312 run.py:483] Algo bellman_ford step 5132 current loss 0.712576, current_train_items 164256.
I0302 19:00:52.845882 22699590365312 run.py:483] Algo bellman_ford step 5133 current loss 0.641196, current_train_items 164288.
I0302 19:00:52.878534 22699590365312 run.py:483] Algo bellman_ford step 5134 current loss 0.699212, current_train_items 164320.
I0302 19:00:52.898130 22699590365312 run.py:483] Algo bellman_ford step 5135 current loss 0.313041, current_train_items 164352.
I0302 19:00:52.914223 22699590365312 run.py:483] Algo bellman_ford step 5136 current loss 0.449626, current_train_items 164384.
I0302 19:00:52.937690 22699590365312 run.py:483] Algo bellman_ford step 5137 current loss 0.645470, current_train_items 164416.
I0302 19:00:52.968002 22699590365312 run.py:483] Algo bellman_ford step 5138 current loss 0.638152, current_train_items 164448.
I0302 19:00:53.001703 22699590365312 run.py:483] Algo bellman_ford step 5139 current loss 0.798234, current_train_items 164480.
I0302 19:00:53.021347 22699590365312 run.py:483] Algo bellman_ford step 5140 current loss 0.320502, current_train_items 164512.
I0302 19:00:53.037798 22699590365312 run.py:483] Algo bellman_ford step 5141 current loss 0.470120, current_train_items 164544.
I0302 19:00:53.061914 22699590365312 run.py:483] Algo bellman_ford step 5142 current loss 0.628038, current_train_items 164576.
I0302 19:00:53.092576 22699590365312 run.py:483] Algo bellman_ford step 5143 current loss 0.676904, current_train_items 164608.
I0302 19:00:53.126166 22699590365312 run.py:483] Algo bellman_ford step 5144 current loss 0.762444, current_train_items 164640.
I0302 19:00:53.145580 22699590365312 run.py:483] Algo bellman_ford step 5145 current loss 0.265651, current_train_items 164672.
I0302 19:00:53.161510 22699590365312 run.py:483] Algo bellman_ford step 5146 current loss 0.456935, current_train_items 164704.
I0302 19:00:53.183754 22699590365312 run.py:483] Algo bellman_ford step 5147 current loss 0.596202, current_train_items 164736.
I0302 19:00:53.213556 22699590365312 run.py:483] Algo bellman_ford step 5148 current loss 0.701861, current_train_items 164768.
I0302 19:00:53.246057 22699590365312 run.py:483] Algo bellman_ford step 5149 current loss 0.781526, current_train_items 164800.
I0302 19:00:53.265634 22699590365312 run.py:483] Algo bellman_ford step 5150 current loss 0.252830, current_train_items 164832.
I0302 19:00:53.273677 22699590365312 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0302 19:00:53.273782 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:00:53.290632 22699590365312 run.py:483] Algo bellman_ford step 5151 current loss 0.485073, current_train_items 164864.
I0302 19:00:53.314088 22699590365312 run.py:483] Algo bellman_ford step 5152 current loss 0.572634, current_train_items 164896.
I0302 19:00:53.343834 22699590365312 run.py:483] Algo bellman_ford step 5153 current loss 0.698073, current_train_items 164928.
I0302 19:00:53.376849 22699590365312 run.py:483] Algo bellman_ford step 5154 current loss 0.771357, current_train_items 164960.
I0302 19:00:53.396752 22699590365312 run.py:483] Algo bellman_ford step 5155 current loss 0.328685, current_train_items 164992.
I0302 19:00:53.412673 22699590365312 run.py:483] Algo bellman_ford step 5156 current loss 0.403183, current_train_items 165024.
I0302 19:00:53.436836 22699590365312 run.py:483] Algo bellman_ford step 5157 current loss 0.558754, current_train_items 165056.
I0302 19:00:53.468849 22699590365312 run.py:483] Algo bellman_ford step 5158 current loss 0.754359, current_train_items 165088.
I0302 19:00:53.501050 22699590365312 run.py:483] Algo bellman_ford step 5159 current loss 0.796280, current_train_items 165120.
I0302 19:00:53.520626 22699590365312 run.py:483] Algo bellman_ford step 5160 current loss 0.254896, current_train_items 165152.
I0302 19:00:53.536670 22699590365312 run.py:483] Algo bellman_ford step 5161 current loss 0.421758, current_train_items 165184.
I0302 19:00:53.560827 22699590365312 run.py:483] Algo bellman_ford step 5162 current loss 0.634138, current_train_items 165216.
I0302 19:00:53.592502 22699590365312 run.py:483] Algo bellman_ford step 5163 current loss 0.653664, current_train_items 165248.
I0302 19:00:53.624476 22699590365312 run.py:483] Algo bellman_ford step 5164 current loss 0.689000, current_train_items 165280.
I0302 19:00:53.643682 22699590365312 run.py:483] Algo bellman_ford step 5165 current loss 0.273320, current_train_items 165312.
I0302 19:00:53.659628 22699590365312 run.py:483] Algo bellman_ford step 5166 current loss 0.413223, current_train_items 165344.
I0302 19:00:53.681881 22699590365312 run.py:483] Algo bellman_ford step 5167 current loss 0.566898, current_train_items 165376.
I0302 19:00:53.712879 22699590365312 run.py:483] Algo bellman_ford step 5168 current loss 0.648279, current_train_items 165408.
I0302 19:00:53.747165 22699590365312 run.py:483] Algo bellman_ford step 5169 current loss 0.715019, current_train_items 165440.
I0302 19:00:53.766778 22699590365312 run.py:483] Algo bellman_ford step 5170 current loss 0.321716, current_train_items 165472.
I0302 19:00:53.783043 22699590365312 run.py:483] Algo bellman_ford step 5171 current loss 0.436833, current_train_items 165504.
I0302 19:00:53.806425 22699590365312 run.py:483] Algo bellman_ford step 5172 current loss 0.564185, current_train_items 165536.
I0302 19:00:53.837849 22699590365312 run.py:483] Algo bellman_ford step 5173 current loss 0.697972, current_train_items 165568.
I0302 19:00:53.870188 22699590365312 run.py:483] Algo bellman_ford step 5174 current loss 0.756294, current_train_items 165600.
I0302 19:00:53.889647 22699590365312 run.py:483] Algo bellman_ford step 5175 current loss 0.275080, current_train_items 165632.
I0302 19:00:53.905972 22699590365312 run.py:483] Algo bellman_ford step 5176 current loss 0.377158, current_train_items 165664.
I0302 19:00:53.929183 22699590365312 run.py:483] Algo bellman_ford step 5177 current loss 0.531954, current_train_items 165696.
I0302 19:00:53.959461 22699590365312 run.py:483] Algo bellman_ford step 5178 current loss 0.581131, current_train_items 165728.
I0302 19:00:53.991646 22699590365312 run.py:483] Algo bellman_ford step 5179 current loss 0.799357, current_train_items 165760.
I0302 19:00:54.010814 22699590365312 run.py:483] Algo bellman_ford step 5180 current loss 0.334315, current_train_items 165792.
I0302 19:00:54.026880 22699590365312 run.py:483] Algo bellman_ford step 5181 current loss 0.434830, current_train_items 165824.
I0302 19:00:54.049987 22699590365312 run.py:483] Algo bellman_ford step 5182 current loss 0.566091, current_train_items 165856.
I0302 19:00:54.080397 22699590365312 run.py:483] Algo bellman_ford step 5183 current loss 0.651271, current_train_items 165888.
I0302 19:00:54.112246 22699590365312 run.py:483] Algo bellman_ford step 5184 current loss 0.634420, current_train_items 165920.
I0302 19:00:54.131807 22699590365312 run.py:483] Algo bellman_ford step 5185 current loss 0.310053, current_train_items 165952.
I0302 19:00:54.148380 22699590365312 run.py:483] Algo bellman_ford step 5186 current loss 0.445040, current_train_items 165984.
I0302 19:00:54.170304 22699590365312 run.py:483] Algo bellman_ford step 5187 current loss 0.571144, current_train_items 166016.
I0302 19:00:54.201378 22699590365312 run.py:483] Algo bellman_ford step 5188 current loss 0.702296, current_train_items 166048.
I0302 19:00:54.233500 22699590365312 run.py:483] Algo bellman_ford step 5189 current loss 0.747977, current_train_items 166080.
I0302 19:00:54.253103 22699590365312 run.py:483] Algo bellman_ford step 5190 current loss 0.298998, current_train_items 166112.
I0302 19:00:54.269186 22699590365312 run.py:483] Algo bellman_ford step 5191 current loss 0.534922, current_train_items 166144.
I0302 19:00:54.292766 22699590365312 run.py:483] Algo bellman_ford step 5192 current loss 0.774289, current_train_items 166176.
I0302 19:00:54.322568 22699590365312 run.py:483] Algo bellman_ford step 5193 current loss 0.914384, current_train_items 166208.
I0302 19:00:54.355582 22699590365312 run.py:483] Algo bellman_ford step 5194 current loss 0.919712, current_train_items 166240.
I0302 19:00:54.374894 22699590365312 run.py:483] Algo bellman_ford step 5195 current loss 0.290737, current_train_items 166272.
I0302 19:00:54.391028 22699590365312 run.py:483] Algo bellman_ford step 5196 current loss 0.504847, current_train_items 166304.
I0302 19:00:54.414685 22699590365312 run.py:483] Algo bellman_ford step 5197 current loss 0.637795, current_train_items 166336.
I0302 19:00:54.446349 22699590365312 run.py:483] Algo bellman_ford step 5198 current loss 0.700827, current_train_items 166368.
I0302 19:00:54.478824 22699590365312 run.py:483] Algo bellman_ford step 5199 current loss 0.782042, current_train_items 166400.
I0302 19:00:54.498398 22699590365312 run.py:483] Algo bellman_ford step 5200 current loss 0.306845, current_train_items 166432.
I0302 19:00:54.506331 22699590365312 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0302 19:00:54.506437 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:00:54.523346 22699590365312 run.py:483] Algo bellman_ford step 5201 current loss 0.458286, current_train_items 166464.
I0302 19:00:54.547768 22699590365312 run.py:483] Algo bellman_ford step 5202 current loss 0.614069, current_train_items 166496.
I0302 19:00:54.580256 22699590365312 run.py:483] Algo bellman_ford step 5203 current loss 0.690592, current_train_items 166528.
I0302 19:00:54.615363 22699590365312 run.py:483] Algo bellman_ford step 5204 current loss 0.659788, current_train_items 166560.
I0302 19:00:54.635550 22699590365312 run.py:483] Algo bellman_ford step 5205 current loss 0.266831, current_train_items 166592.
I0302 19:00:54.651295 22699590365312 run.py:483] Algo bellman_ford step 5206 current loss 0.573252, current_train_items 166624.
I0302 19:00:54.675100 22699590365312 run.py:483] Algo bellman_ford step 5207 current loss 0.653459, current_train_items 166656.
I0302 19:00:54.704781 22699590365312 run.py:483] Algo bellman_ford step 5208 current loss 0.575116, current_train_items 166688.
I0302 19:00:54.740700 22699590365312 run.py:483] Algo bellman_ford step 5209 current loss 0.902454, current_train_items 166720.
I0302 19:00:54.760462 22699590365312 run.py:483] Algo bellman_ford step 5210 current loss 0.333812, current_train_items 166752.
I0302 19:00:54.776112 22699590365312 run.py:483] Algo bellman_ford step 5211 current loss 0.486293, current_train_items 166784.
I0302 19:00:54.799760 22699590365312 run.py:483] Algo bellman_ford step 5212 current loss 0.573615, current_train_items 166816.
I0302 19:00:54.832506 22699590365312 run.py:483] Algo bellman_ford step 5213 current loss 0.744987, current_train_items 166848.
I0302 19:00:54.865372 22699590365312 run.py:483] Algo bellman_ford step 5214 current loss 0.751647, current_train_items 166880.
I0302 19:00:54.884701 22699590365312 run.py:483] Algo bellman_ford step 5215 current loss 0.284082, current_train_items 166912.
I0302 19:00:54.901108 22699590365312 run.py:483] Algo bellman_ford step 5216 current loss 0.416024, current_train_items 166944.
I0302 19:00:54.924656 22699590365312 run.py:483] Algo bellman_ford step 5217 current loss 0.650195, current_train_items 166976.
I0302 19:00:54.954886 22699590365312 run.py:483] Algo bellman_ford step 5218 current loss 0.688824, current_train_items 167008.
I0302 19:00:54.988078 22699590365312 run.py:483] Algo bellman_ford step 5219 current loss 0.783721, current_train_items 167040.
I0302 19:00:55.007914 22699590365312 run.py:483] Algo bellman_ford step 5220 current loss 0.310553, current_train_items 167072.
I0302 19:00:55.023937 22699590365312 run.py:483] Algo bellman_ford step 5221 current loss 0.465973, current_train_items 167104.
I0302 19:00:55.046584 22699590365312 run.py:483] Algo bellman_ford step 5222 current loss 0.567521, current_train_items 167136.
I0302 19:00:55.078350 22699590365312 run.py:483] Algo bellman_ford step 5223 current loss 0.754513, current_train_items 167168.
I0302 19:00:55.110932 22699590365312 run.py:483] Algo bellman_ford step 5224 current loss 0.700086, current_train_items 167200.
I0302 19:00:55.130563 22699590365312 run.py:483] Algo bellman_ford step 5225 current loss 0.330411, current_train_items 167232.
I0302 19:00:55.146642 22699590365312 run.py:483] Algo bellman_ford step 5226 current loss 0.416795, current_train_items 167264.
I0302 19:00:55.170622 22699590365312 run.py:483] Algo bellman_ford step 5227 current loss 0.589437, current_train_items 167296.
I0302 19:00:55.202766 22699590365312 run.py:483] Algo bellman_ford step 5228 current loss 0.640974, current_train_items 167328.
I0302 19:00:55.237542 22699590365312 run.py:483] Algo bellman_ford step 5229 current loss 0.776837, current_train_items 167360.
I0302 19:00:55.257391 22699590365312 run.py:483] Algo bellman_ford step 5230 current loss 0.278491, current_train_items 167392.
I0302 19:00:55.273604 22699590365312 run.py:483] Algo bellman_ford step 5231 current loss 0.507484, current_train_items 167424.
I0302 19:00:55.297729 22699590365312 run.py:483] Algo bellman_ford step 5232 current loss 0.574608, current_train_items 167456.
I0302 19:00:55.328510 22699590365312 run.py:483] Algo bellman_ford step 5233 current loss 0.602513, current_train_items 167488.
I0302 19:00:55.363392 22699590365312 run.py:483] Algo bellman_ford step 5234 current loss 0.765213, current_train_items 167520.
I0302 19:00:55.382872 22699590365312 run.py:483] Algo bellman_ford step 5235 current loss 0.281873, current_train_items 167552.
I0302 19:00:55.398729 22699590365312 run.py:483] Algo bellman_ford step 5236 current loss 0.438048, current_train_items 167584.
I0302 19:00:55.422529 22699590365312 run.py:483] Algo bellman_ford step 5237 current loss 0.566840, current_train_items 167616.
I0302 19:00:55.455133 22699590365312 run.py:483] Algo bellman_ford step 5238 current loss 0.643922, current_train_items 167648.
I0302 19:00:55.489603 22699590365312 run.py:483] Algo bellman_ford step 5239 current loss 0.759198, current_train_items 167680.
I0302 19:00:55.509233 22699590365312 run.py:483] Algo bellman_ford step 5240 current loss 0.364869, current_train_items 167712.
I0302 19:00:55.525414 22699590365312 run.py:483] Algo bellman_ford step 5241 current loss 0.436865, current_train_items 167744.
I0302 19:00:55.549212 22699590365312 run.py:483] Algo bellman_ford step 5242 current loss 0.630189, current_train_items 167776.
I0302 19:00:55.580357 22699590365312 run.py:483] Algo bellman_ford step 5243 current loss 0.626060, current_train_items 167808.
I0302 19:00:55.611592 22699590365312 run.py:483] Algo bellman_ford step 5244 current loss 0.613184, current_train_items 167840.
I0302 19:00:55.630912 22699590365312 run.py:483] Algo bellman_ford step 5245 current loss 0.346714, current_train_items 167872.
I0302 19:00:55.647051 22699590365312 run.py:483] Algo bellman_ford step 5246 current loss 0.455878, current_train_items 167904.
I0302 19:00:55.671081 22699590365312 run.py:483] Algo bellman_ford step 5247 current loss 0.626881, current_train_items 167936.
I0302 19:00:55.702654 22699590365312 run.py:483] Algo bellman_ford step 5248 current loss 0.688410, current_train_items 167968.
I0302 19:00:55.736718 22699590365312 run.py:483] Algo bellman_ford step 5249 current loss 0.791834, current_train_items 168000.
I0302 19:00:55.756445 22699590365312 run.py:483] Algo bellman_ford step 5250 current loss 0.269452, current_train_items 168032.
I0302 19:00:55.764587 22699590365312 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0302 19:00:55.764693 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:00:55.781408 22699590365312 run.py:483] Algo bellman_ford step 5251 current loss 0.425060, current_train_items 168064.
I0302 19:00:55.805976 22699590365312 run.py:483] Algo bellman_ford step 5252 current loss 0.659391, current_train_items 168096.
I0302 19:00:55.837862 22699590365312 run.py:483] Algo bellman_ford step 5253 current loss 0.677032, current_train_items 168128.
I0302 19:00:55.869642 22699590365312 run.py:483] Algo bellman_ford step 5254 current loss 0.585605, current_train_items 168160.
I0302 19:00:55.889906 22699590365312 run.py:483] Algo bellman_ford step 5255 current loss 0.300531, current_train_items 168192.
I0302 19:00:55.905830 22699590365312 run.py:483] Algo bellman_ford step 5256 current loss 0.448430, current_train_items 168224.
I0302 19:00:55.928746 22699590365312 run.py:483] Algo bellman_ford step 5257 current loss 0.651610, current_train_items 168256.
I0302 19:00:55.960084 22699590365312 run.py:483] Algo bellman_ford step 5258 current loss 0.728087, current_train_items 168288.
I0302 19:00:55.991824 22699590365312 run.py:483] Algo bellman_ford step 5259 current loss 0.595537, current_train_items 168320.
I0302 19:00:56.011734 22699590365312 run.py:483] Algo bellman_ford step 5260 current loss 0.291570, current_train_items 168352.
I0302 19:00:56.028103 22699590365312 run.py:483] Algo bellman_ford step 5261 current loss 0.445566, current_train_items 168384.
I0302 19:00:56.050873 22699590365312 run.py:483] Algo bellman_ford step 5262 current loss 0.604596, current_train_items 168416.
I0302 19:00:56.082666 22699590365312 run.py:483] Algo bellman_ford step 5263 current loss 0.653627, current_train_items 168448.
I0302 19:00:56.114920 22699590365312 run.py:483] Algo bellman_ford step 5264 current loss 0.697254, current_train_items 168480.
I0302 19:00:56.134569 22699590365312 run.py:483] Algo bellman_ford step 5265 current loss 0.229800, current_train_items 168512.
I0302 19:00:56.150700 22699590365312 run.py:483] Algo bellman_ford step 5266 current loss 0.495422, current_train_items 168544.
I0302 19:00:56.175029 22699590365312 run.py:483] Algo bellman_ford step 5267 current loss 0.604280, current_train_items 168576.
I0302 19:00:56.205228 22699590365312 run.py:483] Algo bellman_ford step 5268 current loss 0.636995, current_train_items 168608.
I0302 19:00:56.236948 22699590365312 run.py:483] Algo bellman_ford step 5269 current loss 0.679841, current_train_items 168640.
I0302 19:00:56.256683 22699590365312 run.py:483] Algo bellman_ford step 5270 current loss 0.340627, current_train_items 168672.
I0302 19:00:56.272706 22699590365312 run.py:483] Algo bellman_ford step 5271 current loss 0.510186, current_train_items 168704.
I0302 19:00:56.295433 22699590365312 run.py:483] Algo bellman_ford step 5272 current loss 0.568740, current_train_items 168736.
I0302 19:00:56.325945 22699590365312 run.py:483] Algo bellman_ford step 5273 current loss 0.617573, current_train_items 168768.
I0302 19:00:56.358708 22699590365312 run.py:483] Algo bellman_ford step 5274 current loss 0.678214, current_train_items 168800.
I0302 19:00:56.378668 22699590365312 run.py:483] Algo bellman_ford step 5275 current loss 0.335791, current_train_items 168832.
I0302 19:00:56.394737 22699590365312 run.py:483] Algo bellman_ford step 5276 current loss 0.495548, current_train_items 168864.
I0302 19:00:56.417534 22699590365312 run.py:483] Algo bellman_ford step 5277 current loss 0.603070, current_train_items 168896.
I0302 19:00:56.448221 22699590365312 run.py:483] Algo bellman_ford step 5278 current loss 0.610642, current_train_items 168928.
I0302 19:00:56.482314 22699590365312 run.py:483] Algo bellman_ford step 5279 current loss 0.831373, current_train_items 168960.
I0302 19:00:56.501950 22699590365312 run.py:483] Algo bellman_ford step 5280 current loss 0.325903, current_train_items 168992.
I0302 19:00:56.518291 22699590365312 run.py:483] Algo bellman_ford step 5281 current loss 0.521160, current_train_items 169024.
I0302 19:00:56.542957 22699590365312 run.py:483] Algo bellman_ford step 5282 current loss 0.666460, current_train_items 169056.
I0302 19:00:56.574149 22699590365312 run.py:483] Algo bellman_ford step 5283 current loss 0.650969, current_train_items 169088.
I0302 19:00:56.607348 22699590365312 run.py:483] Algo bellman_ford step 5284 current loss 0.768380, current_train_items 169120.
I0302 19:00:56.627494 22699590365312 run.py:483] Algo bellman_ford step 5285 current loss 0.329410, current_train_items 169152.
I0302 19:00:56.643763 22699590365312 run.py:483] Algo bellman_ford step 5286 current loss 0.449184, current_train_items 169184.
I0302 19:00:56.668229 22699590365312 run.py:483] Algo bellman_ford step 5287 current loss 0.650224, current_train_items 169216.
I0302 19:00:56.698846 22699590365312 run.py:483] Algo bellman_ford step 5288 current loss 0.670190, current_train_items 169248.
I0302 19:00:56.733160 22699590365312 run.py:483] Algo bellman_ford step 5289 current loss 0.834508, current_train_items 169280.
I0302 19:00:56.752818 22699590365312 run.py:483] Algo bellman_ford step 5290 current loss 0.332592, current_train_items 169312.
I0302 19:00:56.769410 22699590365312 run.py:483] Algo bellman_ford step 5291 current loss 0.439850, current_train_items 169344.
I0302 19:00:56.792362 22699590365312 run.py:483] Algo bellman_ford step 5292 current loss 0.546313, current_train_items 169376.
I0302 19:00:56.823429 22699590365312 run.py:483] Algo bellman_ford step 5293 current loss 0.718169, current_train_items 169408.
I0302 19:00:56.855746 22699590365312 run.py:483] Algo bellman_ford step 5294 current loss 0.912250, current_train_items 169440.
I0302 19:00:56.875394 22699590365312 run.py:483] Algo bellman_ford step 5295 current loss 0.269415, current_train_items 169472.
I0302 19:00:56.891332 22699590365312 run.py:483] Algo bellman_ford step 5296 current loss 0.418147, current_train_items 169504.
I0302 19:00:56.913905 22699590365312 run.py:483] Algo bellman_ford step 5297 current loss 0.603083, current_train_items 169536.
I0302 19:00:56.945820 22699590365312 run.py:483] Algo bellman_ford step 5298 current loss 0.724683, current_train_items 169568.
I0302 19:00:56.981703 22699590365312 run.py:483] Algo bellman_ford step 5299 current loss 0.925982, current_train_items 169600.
I0302 19:00:57.001462 22699590365312 run.py:483] Algo bellman_ford step 5300 current loss 0.350946, current_train_items 169632.
I0302 19:00:57.009469 22699590365312 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0302 19:00:57.009576 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 19:00:57.025992 22699590365312 run.py:483] Algo bellman_ford step 5301 current loss 0.413247, current_train_items 169664.
I0302 19:00:57.050231 22699590365312 run.py:483] Algo bellman_ford step 5302 current loss 0.640487, current_train_items 169696.
I0302 19:00:57.082451 22699590365312 run.py:483] Algo bellman_ford step 5303 current loss 0.641065, current_train_items 169728.
I0302 19:00:57.117169 22699590365312 run.py:483] Algo bellman_ford step 5304 current loss 0.766806, current_train_items 169760.
I0302 19:00:57.137291 22699590365312 run.py:483] Algo bellman_ford step 5305 current loss 0.282140, current_train_items 169792.
I0302 19:00:57.153515 22699590365312 run.py:483] Algo bellman_ford step 5306 current loss 0.521208, current_train_items 169824.
I0302 19:00:57.178180 22699590365312 run.py:483] Algo bellman_ford step 5307 current loss 0.656126, current_train_items 169856.
I0302 19:00:57.210181 22699590365312 run.py:483] Algo bellman_ford step 5308 current loss 0.694070, current_train_items 169888.
I0302 19:00:57.242198 22699590365312 run.py:483] Algo bellman_ford step 5309 current loss 0.723132, current_train_items 169920.
I0302 19:00:57.261675 22699590365312 run.py:483] Algo bellman_ford step 5310 current loss 0.290009, current_train_items 169952.
I0302 19:00:57.277351 22699590365312 run.py:483] Algo bellman_ford step 5311 current loss 0.380967, current_train_items 169984.
I0302 19:00:57.300959 22699590365312 run.py:483] Algo bellman_ford step 5312 current loss 0.547291, current_train_items 170016.
I0302 19:00:57.331846 22699590365312 run.py:483] Algo bellman_ford step 5313 current loss 0.706226, current_train_items 170048.
I0302 19:00:57.366351 22699590365312 run.py:483] Algo bellman_ford step 5314 current loss 0.782210, current_train_items 170080.
I0302 19:00:57.386044 22699590365312 run.py:483] Algo bellman_ford step 5315 current loss 0.339468, current_train_items 170112.
I0302 19:00:57.402174 22699590365312 run.py:483] Algo bellman_ford step 5316 current loss 0.426419, current_train_items 170144.
I0302 19:00:57.425667 22699590365312 run.py:483] Algo bellman_ford step 5317 current loss 0.613377, current_train_items 170176.
I0302 19:00:57.455608 22699590365312 run.py:483] Algo bellman_ford step 5318 current loss 0.554927, current_train_items 170208.
I0302 19:00:57.489460 22699590365312 run.py:483] Algo bellman_ford step 5319 current loss 0.831947, current_train_items 170240.
I0302 19:00:57.508923 22699590365312 run.py:483] Algo bellman_ford step 5320 current loss 0.296102, current_train_items 170272.
I0302 19:00:57.525212 22699590365312 run.py:483] Algo bellman_ford step 5321 current loss 0.442970, current_train_items 170304.
I0302 19:00:57.549592 22699590365312 run.py:483] Algo bellman_ford step 5322 current loss 0.625444, current_train_items 170336.
I0302 19:00:57.578902 22699590365312 run.py:483] Algo bellman_ford step 5323 current loss 0.634330, current_train_items 170368.
I0302 19:00:57.613274 22699590365312 run.py:483] Algo bellman_ford step 5324 current loss 0.678499, current_train_items 170400.
I0302 19:00:57.633127 22699590365312 run.py:483] Algo bellman_ford step 5325 current loss 0.341540, current_train_items 170432.
I0302 19:00:57.649305 22699590365312 run.py:483] Algo bellman_ford step 5326 current loss 0.452298, current_train_items 170464.
I0302 19:00:57.671737 22699590365312 run.py:483] Algo bellman_ford step 5327 current loss 0.504625, current_train_items 170496.
I0302 19:00:57.703615 22699590365312 run.py:483] Algo bellman_ford step 5328 current loss 0.668377, current_train_items 170528.
I0302 19:00:57.736535 22699590365312 run.py:483] Algo bellman_ford step 5329 current loss 0.769726, current_train_items 170560.
I0302 19:00:57.756212 22699590365312 run.py:483] Algo bellman_ford step 5330 current loss 0.354342, current_train_items 170592.
I0302 19:00:57.772305 22699590365312 run.py:483] Algo bellman_ford step 5331 current loss 0.414662, current_train_items 170624.
I0302 19:00:57.795805 22699590365312 run.py:483] Algo bellman_ford step 5332 current loss 0.559583, current_train_items 170656.
I0302 19:00:57.825442 22699590365312 run.py:483] Algo bellman_ford step 5333 current loss 0.552675, current_train_items 170688.
I0302 19:00:57.857775 22699590365312 run.py:483] Algo bellman_ford step 5334 current loss 0.655857, current_train_items 170720.
I0302 19:00:57.877624 22699590365312 run.py:483] Algo bellman_ford step 5335 current loss 0.364599, current_train_items 170752.
I0302 19:00:57.893944 22699590365312 run.py:483] Algo bellman_ford step 5336 current loss 0.460856, current_train_items 170784.
I0302 19:00:57.917042 22699590365312 run.py:483] Algo bellman_ford step 5337 current loss 0.598414, current_train_items 170816.
I0302 19:00:57.946843 22699590365312 run.py:483] Algo bellman_ford step 5338 current loss 0.629115, current_train_items 170848.
I0302 19:00:57.980945 22699590365312 run.py:483] Algo bellman_ford step 5339 current loss 0.731153, current_train_items 170880.
I0302 19:00:58.000473 22699590365312 run.py:483] Algo bellman_ford step 5340 current loss 0.252728, current_train_items 170912.
I0302 19:00:58.016852 22699590365312 run.py:483] Algo bellman_ford step 5341 current loss 0.489573, current_train_items 170944.
I0302 19:00:58.042081 22699590365312 run.py:483] Algo bellman_ford step 5342 current loss 0.568456, current_train_items 170976.
I0302 19:00:58.073436 22699590365312 run.py:483] Algo bellman_ford step 5343 current loss 0.663750, current_train_items 171008.
I0302 19:00:58.106552 22699590365312 run.py:483] Algo bellman_ford step 5344 current loss 0.835482, current_train_items 171040.
I0302 19:00:58.125819 22699590365312 run.py:483] Algo bellman_ford step 5345 current loss 0.213841, current_train_items 171072.
I0302 19:00:58.141385 22699590365312 run.py:483] Algo bellman_ford step 5346 current loss 0.428410, current_train_items 171104.
I0302 19:00:58.164541 22699590365312 run.py:483] Algo bellman_ford step 5347 current loss 0.549441, current_train_items 171136.
I0302 19:00:58.196691 22699590365312 run.py:483] Algo bellman_ford step 5348 current loss 0.661373, current_train_items 171168.
I0302 19:00:58.230097 22699590365312 run.py:483] Algo bellman_ford step 5349 current loss 0.730471, current_train_items 171200.
I0302 19:00:58.249511 22699590365312 run.py:483] Algo bellman_ford step 5350 current loss 0.427213, current_train_items 171232.
I0302 19:00:58.257802 22699590365312 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0302 19:00:58.257906 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0302 19:00:58.274208 22699590365312 run.py:483] Algo bellman_ford step 5351 current loss 0.469454, current_train_items 171264.
I0302 19:00:58.298771 22699590365312 run.py:483] Algo bellman_ford step 5352 current loss 0.616951, current_train_items 171296.
I0302 19:00:58.328968 22699590365312 run.py:483] Algo bellman_ford step 5353 current loss 0.767428, current_train_items 171328.
I0302 19:00:58.362545 22699590365312 run.py:483] Algo bellman_ford step 5354 current loss 0.717320, current_train_items 171360.
I0302 19:00:58.382594 22699590365312 run.py:483] Algo bellman_ford step 5355 current loss 0.286783, current_train_items 171392.
I0302 19:00:58.398600 22699590365312 run.py:483] Algo bellman_ford step 5356 current loss 0.537412, current_train_items 171424.
I0302 19:00:58.422299 22699590365312 run.py:483] Algo bellman_ford step 5357 current loss 0.659344, current_train_items 171456.
I0302 19:00:58.451944 22699590365312 run.py:483] Algo bellman_ford step 5358 current loss 0.562178, current_train_items 171488.
I0302 19:00:58.484830 22699590365312 run.py:483] Algo bellman_ford step 5359 current loss 0.714984, current_train_items 171520.
I0302 19:00:58.504757 22699590365312 run.py:483] Algo bellman_ford step 5360 current loss 0.347211, current_train_items 171552.
I0302 19:00:58.520917 22699590365312 run.py:483] Algo bellman_ford step 5361 current loss 0.538300, current_train_items 171584.
I0302 19:00:58.544891 22699590365312 run.py:483] Algo bellman_ford step 5362 current loss 0.631484, current_train_items 171616.
I0302 19:00:58.576121 22699590365312 run.py:483] Algo bellman_ford step 5363 current loss 0.679698, current_train_items 171648.
I0302 19:00:58.609650 22699590365312 run.py:483] Algo bellman_ford step 5364 current loss 0.760869, current_train_items 171680.
I0302 19:00:58.629357 22699590365312 run.py:483] Algo bellman_ford step 5365 current loss 0.303357, current_train_items 171712.
I0302 19:00:58.645531 22699590365312 run.py:483] Algo bellman_ford step 5366 current loss 0.483564, current_train_items 171744.
I0302 19:00:58.668659 22699590365312 run.py:483] Algo bellman_ford step 5367 current loss 0.631486, current_train_items 171776.
I0302 19:00:58.699378 22699590365312 run.py:483] Algo bellman_ford step 5368 current loss 0.679884, current_train_items 171808.
I0302 19:00:58.731971 22699590365312 run.py:483] Algo bellman_ford step 5369 current loss 0.759810, current_train_items 171840.
I0302 19:00:58.751750 22699590365312 run.py:483] Algo bellman_ford step 5370 current loss 0.310094, current_train_items 171872.
I0302 19:00:58.767995 22699590365312 run.py:483] Algo bellman_ford step 5371 current loss 0.441577, current_train_items 171904.
I0302 19:00:58.790979 22699590365312 run.py:483] Algo bellman_ford step 5372 current loss 0.626443, current_train_items 171936.
I0302 19:00:58.821495 22699590365312 run.py:483] Algo bellman_ford step 5373 current loss 0.751526, current_train_items 171968.
I0302 19:00:58.852463 22699590365312 run.py:483] Algo bellman_ford step 5374 current loss 0.777446, current_train_items 172000.
I0302 19:00:58.872369 22699590365312 run.py:483] Algo bellman_ford step 5375 current loss 0.277356, current_train_items 172032.
I0302 19:00:58.888547 22699590365312 run.py:483] Algo bellman_ford step 5376 current loss 0.504906, current_train_items 172064.
I0302 19:00:58.911560 22699590365312 run.py:483] Algo bellman_ford step 5377 current loss 0.690705, current_train_items 172096.
I0302 19:00:58.941627 22699590365312 run.py:483] Algo bellman_ford step 5378 current loss 0.675933, current_train_items 172128.
I0302 19:00:58.972048 22699590365312 run.py:483] Algo bellman_ford step 5379 current loss 0.673588, current_train_items 172160.
I0302 19:00:58.991692 22699590365312 run.py:483] Algo bellman_ford step 5380 current loss 0.375462, current_train_items 172192.
I0302 19:00:59.007999 22699590365312 run.py:483] Algo bellman_ford step 5381 current loss 0.475951, current_train_items 172224.
I0302 19:00:59.032347 22699590365312 run.py:483] Algo bellman_ford step 5382 current loss 0.743356, current_train_items 172256.
I0302 19:00:59.063161 22699590365312 run.py:483] Algo bellman_ford step 5383 current loss 0.777922, current_train_items 172288.
I0302 19:00:59.095376 22699590365312 run.py:483] Algo bellman_ford step 5384 current loss 0.846100, current_train_items 172320.
I0302 19:00:59.115336 22699590365312 run.py:483] Algo bellman_ford step 5385 current loss 0.297850, current_train_items 172352.
I0302 19:00:59.131065 22699590365312 run.py:483] Algo bellman_ford step 5386 current loss 0.512515, current_train_items 172384.
I0302 19:00:59.154075 22699590365312 run.py:483] Algo bellman_ford step 5387 current loss 0.506795, current_train_items 172416.
I0302 19:00:59.183980 22699590365312 run.py:483] Algo bellman_ford step 5388 current loss 0.577377, current_train_items 172448.
I0302 19:00:59.217437 22699590365312 run.py:483] Algo bellman_ford step 5389 current loss 0.802603, current_train_items 172480.
I0302 19:00:59.237568 22699590365312 run.py:483] Algo bellman_ford step 5390 current loss 0.297499, current_train_items 172512.
I0302 19:00:59.254143 22699590365312 run.py:483] Algo bellman_ford step 5391 current loss 0.496639, current_train_items 172544.
I0302 19:00:59.276690 22699590365312 run.py:483] Algo bellman_ford step 5392 current loss 0.635016, current_train_items 172576.
I0302 19:00:59.308618 22699590365312 run.py:483] Algo bellman_ford step 5393 current loss 0.716099, current_train_items 172608.
I0302 19:00:59.342976 22699590365312 run.py:483] Algo bellman_ford step 5394 current loss 0.796975, current_train_items 172640.
I0302 19:00:59.362561 22699590365312 run.py:483] Algo bellman_ford step 5395 current loss 0.332519, current_train_items 172672.
I0302 19:00:59.379057 22699590365312 run.py:483] Algo bellman_ford step 5396 current loss 0.474354, current_train_items 172704.
I0302 19:00:59.402470 22699590365312 run.py:483] Algo bellman_ford step 5397 current loss 0.552954, current_train_items 172736.
I0302 19:00:59.433212 22699590365312 run.py:483] Algo bellman_ford step 5398 current loss 0.663966, current_train_items 172768.
I0302 19:00:59.467609 22699590365312 run.py:483] Algo bellman_ford step 5399 current loss 0.914354, current_train_items 172800.
I0302 19:00:59.487671 22699590365312 run.py:483] Algo bellman_ford step 5400 current loss 0.235353, current_train_items 172832.
I0302 19:00:59.495680 22699590365312 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0302 19:00:59.495786 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:00:59.513119 22699590365312 run.py:483] Algo bellman_ford step 5401 current loss 0.520753, current_train_items 172864.
I0302 19:00:59.537703 22699590365312 run.py:483] Algo bellman_ford step 5402 current loss 0.694713, current_train_items 172896.
I0302 19:00:59.570431 22699590365312 run.py:483] Algo bellman_ford step 5403 current loss 0.881852, current_train_items 172928.
I0302 19:00:59.605642 22699590365312 run.py:483] Algo bellman_ford step 5404 current loss 0.761952, current_train_items 172960.
I0302 19:00:59.625811 22699590365312 run.py:483] Algo bellman_ford step 5405 current loss 0.238701, current_train_items 172992.
I0302 19:00:59.641482 22699590365312 run.py:483] Algo bellman_ford step 5406 current loss 0.407799, current_train_items 173024.
I0302 19:00:59.664280 22699590365312 run.py:483] Algo bellman_ford step 5407 current loss 0.516088, current_train_items 173056.
I0302 19:00:59.693545 22699590365312 run.py:483] Algo bellman_ford step 5408 current loss 0.614993, current_train_items 173088.
I0302 19:00:59.726201 22699590365312 run.py:483] Algo bellman_ford step 5409 current loss 0.598026, current_train_items 173120.
I0302 19:00:59.746067 22699590365312 run.py:483] Algo bellman_ford step 5410 current loss 0.278380, current_train_items 173152.
I0302 19:00:59.762610 22699590365312 run.py:483] Algo bellman_ford step 5411 current loss 0.485222, current_train_items 173184.
I0302 19:00:59.786169 22699590365312 run.py:483] Algo bellman_ford step 5412 current loss 0.621286, current_train_items 173216.
I0302 19:00:59.817407 22699590365312 run.py:483] Algo bellman_ford step 5413 current loss 0.590186, current_train_items 173248.
I0302 19:00:59.851519 22699590365312 run.py:483] Algo bellman_ford step 5414 current loss 0.782927, current_train_items 173280.
I0302 19:00:59.871358 22699590365312 run.py:483] Algo bellman_ford step 5415 current loss 0.225338, current_train_items 173312.
I0302 19:00:59.887771 22699590365312 run.py:483] Algo bellman_ford step 5416 current loss 0.447451, current_train_items 173344.
I0302 19:00:59.911680 22699590365312 run.py:483] Algo bellman_ford step 5417 current loss 0.598163, current_train_items 173376.
I0302 19:00:59.943955 22699590365312 run.py:483] Algo bellman_ford step 5418 current loss 0.676915, current_train_items 173408.
I0302 19:00:59.979328 22699590365312 run.py:483] Algo bellman_ford step 5419 current loss 0.796894, current_train_items 173440.
I0302 19:00:59.998933 22699590365312 run.py:483] Algo bellman_ford step 5420 current loss 0.328459, current_train_items 173472.
I0302 19:01:00.015249 22699590365312 run.py:483] Algo bellman_ford step 5421 current loss 0.463016, current_train_items 173504.
I0302 19:01:00.038331 22699590365312 run.py:483] Algo bellman_ford step 5422 current loss 0.514071, current_train_items 173536.
I0302 19:01:00.068385 22699590365312 run.py:483] Algo bellman_ford step 5423 current loss 0.675563, current_train_items 173568.
I0302 19:01:00.099736 22699590365312 run.py:483] Algo bellman_ford step 5424 current loss 0.802999, current_train_items 173600.
I0302 19:01:00.119822 22699590365312 run.py:483] Algo bellman_ford step 5425 current loss 0.310474, current_train_items 173632.
I0302 19:01:00.135766 22699590365312 run.py:483] Algo bellman_ford step 5426 current loss 0.402608, current_train_items 173664.
I0302 19:01:00.159536 22699590365312 run.py:483] Algo bellman_ford step 5427 current loss 0.687564, current_train_items 173696.
I0302 19:01:00.190133 22699590365312 run.py:483] Algo bellman_ford step 5428 current loss 0.755584, current_train_items 173728.
I0302 19:01:00.224321 22699590365312 run.py:483] Algo bellman_ford step 5429 current loss 0.807079, current_train_items 173760.
I0302 19:01:00.243789 22699590365312 run.py:483] Algo bellman_ford step 5430 current loss 0.281709, current_train_items 173792.
I0302 19:01:00.260254 22699590365312 run.py:483] Algo bellman_ford step 5431 current loss 0.427425, current_train_items 173824.
I0302 19:01:00.284127 22699590365312 run.py:483] Algo bellman_ford step 5432 current loss 0.551551, current_train_items 173856.
I0302 19:01:00.317420 22699590365312 run.py:483] Algo bellman_ford step 5433 current loss 0.770919, current_train_items 173888.
I0302 19:01:00.353173 22699590365312 run.py:483] Algo bellman_ford step 5434 current loss 0.895271, current_train_items 173920.
I0302 19:01:00.372627 22699590365312 run.py:483] Algo bellman_ford step 5435 current loss 0.259911, current_train_items 173952.
I0302 19:01:00.388723 22699590365312 run.py:483] Algo bellman_ford step 5436 current loss 0.485172, current_train_items 173984.
I0302 19:01:00.412311 22699590365312 run.py:483] Algo bellman_ford step 5437 current loss 0.603506, current_train_items 174016.
I0302 19:01:00.442416 22699590365312 run.py:483] Algo bellman_ford step 5438 current loss 0.755314, current_train_items 174048.
I0302 19:01:00.479387 22699590365312 run.py:483] Algo bellman_ford step 5439 current loss 0.814588, current_train_items 174080.
I0302 19:01:00.498777 22699590365312 run.py:483] Algo bellman_ford step 5440 current loss 0.352855, current_train_items 174112.
I0302 19:01:00.514779 22699590365312 run.py:483] Algo bellman_ford step 5441 current loss 0.405560, current_train_items 174144.
I0302 19:01:00.537863 22699590365312 run.py:483] Algo bellman_ford step 5442 current loss 0.572294, current_train_items 174176.
I0302 19:01:00.568162 22699590365312 run.py:483] Algo bellman_ford step 5443 current loss 0.663317, current_train_items 174208.
I0302 19:01:00.601861 22699590365312 run.py:483] Algo bellman_ford step 5444 current loss 0.704735, current_train_items 174240.
I0302 19:01:00.621504 22699590365312 run.py:483] Algo bellman_ford step 5445 current loss 0.271804, current_train_items 174272.
I0302 19:01:00.637545 22699590365312 run.py:483] Algo bellman_ford step 5446 current loss 0.481702, current_train_items 174304.
I0302 19:01:00.661248 22699590365312 run.py:483] Algo bellman_ford step 5447 current loss 0.565218, current_train_items 174336.
I0302 19:01:00.693217 22699590365312 run.py:483] Algo bellman_ford step 5448 current loss 0.733200, current_train_items 174368.
I0302 19:01:00.726265 22699590365312 run.py:483] Algo bellman_ford step 5449 current loss 0.656855, current_train_items 174400.
I0302 19:01:00.745822 22699590365312 run.py:483] Algo bellman_ford step 5450 current loss 0.265954, current_train_items 174432.
I0302 19:01:00.753913 22699590365312 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0302 19:01:00.754018 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:01:00.770565 22699590365312 run.py:483] Algo bellman_ford step 5451 current loss 0.487195, current_train_items 174464.
I0302 19:01:00.794487 22699590365312 run.py:483] Algo bellman_ford step 5452 current loss 0.596056, current_train_items 174496.
I0302 19:01:00.824728 22699590365312 run.py:483] Algo bellman_ford step 5453 current loss 0.626210, current_train_items 174528.
I0302 19:01:00.858081 22699590365312 run.py:483] Algo bellman_ford step 5454 current loss 0.807926, current_train_items 174560.
I0302 19:01:00.878288 22699590365312 run.py:483] Algo bellman_ford step 5455 current loss 0.304928, current_train_items 174592.
I0302 19:01:00.894474 22699590365312 run.py:483] Algo bellman_ford step 5456 current loss 0.470740, current_train_items 174624.
I0302 19:01:00.918597 22699590365312 run.py:483] Algo bellman_ford step 5457 current loss 0.584274, current_train_items 174656.
I0302 19:01:00.950173 22699590365312 run.py:483] Algo bellman_ford step 5458 current loss 0.671994, current_train_items 174688.
I0302 19:01:00.981027 22699590365312 run.py:483] Algo bellman_ford step 5459 current loss 0.767359, current_train_items 174720.
I0302 19:01:01.000829 22699590365312 run.py:483] Algo bellman_ford step 5460 current loss 0.288386, current_train_items 174752.
I0302 19:01:01.017351 22699590365312 run.py:483] Algo bellman_ford step 5461 current loss 0.454735, current_train_items 174784.
I0302 19:01:01.041095 22699590365312 run.py:483] Algo bellman_ford step 5462 current loss 0.701157, current_train_items 174816.
I0302 19:01:01.071254 22699590365312 run.py:483] Algo bellman_ford step 5463 current loss 0.777502, current_train_items 174848.
I0302 19:01:01.105231 22699590365312 run.py:483] Algo bellman_ford step 5464 current loss 0.855943, current_train_items 174880.
I0302 19:01:01.124830 22699590365312 run.py:483] Algo bellman_ford step 5465 current loss 0.307775, current_train_items 174912.
I0302 19:01:01.140728 22699590365312 run.py:483] Algo bellman_ford step 5466 current loss 0.418858, current_train_items 174944.
I0302 19:01:01.163935 22699590365312 run.py:483] Algo bellman_ford step 5467 current loss 0.582667, current_train_items 174976.
I0302 19:01:01.193990 22699590365312 run.py:483] Algo bellman_ford step 5468 current loss 0.702076, current_train_items 175008.
I0302 19:01:01.227486 22699590365312 run.py:483] Algo bellman_ford step 5469 current loss 0.811274, current_train_items 175040.
I0302 19:01:01.247533 22699590365312 run.py:483] Algo bellman_ford step 5470 current loss 0.401095, current_train_items 175072.
I0302 19:01:01.263809 22699590365312 run.py:483] Algo bellman_ford step 5471 current loss 0.663535, current_train_items 175104.
I0302 19:01:01.286567 22699590365312 run.py:483] Algo bellman_ford step 5472 current loss 0.497639, current_train_items 175136.
I0302 19:01:01.317389 22699590365312 run.py:483] Algo bellman_ford step 5473 current loss 0.623682, current_train_items 175168.
I0302 19:01:01.351964 22699590365312 run.py:483] Algo bellman_ford step 5474 current loss 0.812008, current_train_items 175200.
I0302 19:01:01.371644 22699590365312 run.py:483] Algo bellman_ford step 5475 current loss 0.319604, current_train_items 175232.
I0302 19:01:01.387844 22699590365312 run.py:483] Algo bellman_ford step 5476 current loss 0.460249, current_train_items 175264.
I0302 19:01:01.410807 22699590365312 run.py:483] Algo bellman_ford step 5477 current loss 0.549782, current_train_items 175296.
I0302 19:01:01.441869 22699590365312 run.py:483] Algo bellman_ford step 5478 current loss 0.581360, current_train_items 175328.
I0302 19:01:01.474960 22699590365312 run.py:483] Algo bellman_ford step 5479 current loss 0.794252, current_train_items 175360.
I0302 19:01:01.494796 22699590365312 run.py:483] Algo bellman_ford step 5480 current loss 0.331596, current_train_items 175392.
I0302 19:01:01.511539 22699590365312 run.py:483] Algo bellman_ford step 5481 current loss 0.477614, current_train_items 175424.
I0302 19:01:01.536133 22699590365312 run.py:483] Algo bellman_ford step 5482 current loss 0.732569, current_train_items 175456.
I0302 19:01:01.567746 22699590365312 run.py:483] Algo bellman_ford step 5483 current loss 0.740061, current_train_items 175488.
I0302 19:01:01.603322 22699590365312 run.py:483] Algo bellman_ford step 5484 current loss 0.721513, current_train_items 175520.
I0302 19:01:01.623012 22699590365312 run.py:483] Algo bellman_ford step 5485 current loss 0.314909, current_train_items 175552.
I0302 19:01:01.638984 22699590365312 run.py:483] Algo bellman_ford step 5486 current loss 0.404956, current_train_items 175584.
I0302 19:01:01.661698 22699590365312 run.py:483] Algo bellman_ford step 5487 current loss 0.565850, current_train_items 175616.
I0302 19:01:01.694101 22699590365312 run.py:483] Algo bellman_ford step 5488 current loss 0.708318, current_train_items 175648.
I0302 19:01:01.726478 22699590365312 run.py:483] Algo bellman_ford step 5489 current loss 0.771859, current_train_items 175680.
I0302 19:01:01.746300 22699590365312 run.py:483] Algo bellman_ford step 5490 current loss 0.299810, current_train_items 175712.
I0302 19:01:01.762492 22699590365312 run.py:483] Algo bellman_ford step 5491 current loss 0.450466, current_train_items 175744.
I0302 19:01:01.786135 22699590365312 run.py:483] Algo bellman_ford step 5492 current loss 0.586600, current_train_items 175776.
I0302 19:01:01.816813 22699590365312 run.py:483] Algo bellman_ford step 5493 current loss 0.609748, current_train_items 175808.
I0302 19:01:01.849728 22699590365312 run.py:483] Algo bellman_ford step 5494 current loss 0.675332, current_train_items 175840.
I0302 19:01:01.869256 22699590365312 run.py:483] Algo bellman_ford step 5495 current loss 0.306317, current_train_items 175872.
I0302 19:01:01.885574 22699590365312 run.py:483] Algo bellman_ford step 5496 current loss 0.409276, current_train_items 175904.
I0302 19:01:01.909298 22699590365312 run.py:483] Algo bellman_ford step 5497 current loss 0.666845, current_train_items 175936.
I0302 19:01:01.940302 22699590365312 run.py:483] Algo bellman_ford step 5498 current loss 0.607207, current_train_items 175968.
I0302 19:01:01.974577 22699590365312 run.py:483] Algo bellman_ford step 5499 current loss 0.869391, current_train_items 176000.
I0302 19:01:01.994451 22699590365312 run.py:483] Algo bellman_ford step 5500 current loss 0.329548, current_train_items 176032.
I0302 19:01:02.002395 22699590365312 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.9365234375, 'score': 0.9365234375, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0302 19:01:02.002502 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.937, val scores are: bellman_ford: 0.937
I0302 19:01:02.019725 22699590365312 run.py:483] Algo bellman_ford step 5501 current loss 0.497469, current_train_items 176064.
I0302 19:01:02.043908 22699590365312 run.py:483] Algo bellman_ford step 5502 current loss 0.608369, current_train_items 176096.
I0302 19:01:02.074734 22699590365312 run.py:483] Algo bellman_ford step 5503 current loss 0.639910, current_train_items 176128.
I0302 19:01:02.108775 22699590365312 run.py:483] Algo bellman_ford step 5504 current loss 0.811279, current_train_items 176160.
I0302 19:01:02.128552 22699590365312 run.py:483] Algo bellman_ford step 5505 current loss 0.277145, current_train_items 176192.
I0302 19:01:02.144601 22699590365312 run.py:483] Algo bellman_ford step 5506 current loss 0.403325, current_train_items 176224.
I0302 19:01:02.167510 22699590365312 run.py:483] Algo bellman_ford step 5507 current loss 0.518563, current_train_items 176256.
I0302 19:01:02.197071 22699590365312 run.py:483] Algo bellman_ford step 5508 current loss 0.650295, current_train_items 176288.
I0302 19:01:02.229789 22699590365312 run.py:483] Algo bellman_ford step 5509 current loss 0.679418, current_train_items 176320.
I0302 19:01:02.249148 22699590365312 run.py:483] Algo bellman_ford step 5510 current loss 0.316785, current_train_items 176352.
I0302 19:01:02.265595 22699590365312 run.py:483] Algo bellman_ford step 5511 current loss 0.444363, current_train_items 176384.
I0302 19:01:02.289971 22699590365312 run.py:483] Algo bellman_ford step 5512 current loss 0.703517, current_train_items 176416.
I0302 19:01:02.320501 22699590365312 run.py:483] Algo bellman_ford step 5513 current loss 0.652138, current_train_items 176448.
I0302 19:01:02.352003 22699590365312 run.py:483] Algo bellman_ford step 5514 current loss 0.932740, current_train_items 176480.
I0302 19:01:02.371415 22699590365312 run.py:483] Algo bellman_ford step 5515 current loss 0.307610, current_train_items 176512.
I0302 19:01:02.387200 22699590365312 run.py:483] Algo bellman_ford step 5516 current loss 0.454861, current_train_items 176544.
I0302 19:01:02.410880 22699590365312 run.py:483] Algo bellman_ford step 5517 current loss 0.662749, current_train_items 176576.
I0302 19:01:02.441091 22699590365312 run.py:483] Algo bellman_ford step 5518 current loss 0.657947, current_train_items 176608.
I0302 19:01:02.475991 22699590365312 run.py:483] Algo bellman_ford step 5519 current loss 0.782573, current_train_items 176640.
I0302 19:01:02.495353 22699590365312 run.py:483] Algo bellman_ford step 5520 current loss 0.324884, current_train_items 176672.
I0302 19:01:02.511837 22699590365312 run.py:483] Algo bellman_ford step 5521 current loss 0.507920, current_train_items 176704.
I0302 19:01:02.534566 22699590365312 run.py:483] Algo bellman_ford step 5522 current loss 0.583906, current_train_items 176736.
I0302 19:01:02.565908 22699590365312 run.py:483] Algo bellman_ford step 5523 current loss 0.742065, current_train_items 176768.
I0302 19:01:02.600220 22699590365312 run.py:483] Algo bellman_ford step 5524 current loss 0.939206, current_train_items 176800.
I0302 19:01:02.619499 22699590365312 run.py:483] Algo bellman_ford step 5525 current loss 0.250614, current_train_items 176832.
I0302 19:01:02.635268 22699590365312 run.py:483] Algo bellman_ford step 5526 current loss 0.429579, current_train_items 176864.
I0302 19:01:02.659399 22699590365312 run.py:483] Algo bellman_ford step 5527 current loss 0.593897, current_train_items 176896.
I0302 19:01:02.689089 22699590365312 run.py:483] Algo bellman_ford step 5528 current loss 0.636241, current_train_items 176928.
I0302 19:01:02.721009 22699590365312 run.py:483] Algo bellman_ford step 5529 current loss 0.712722, current_train_items 176960.
I0302 19:01:02.740603 22699590365312 run.py:483] Algo bellman_ford step 5530 current loss 0.366589, current_train_items 176992.
I0302 19:01:02.756833 22699590365312 run.py:483] Algo bellman_ford step 5531 current loss 0.419758, current_train_items 177024.
I0302 19:01:02.781049 22699590365312 run.py:483] Algo bellman_ford step 5532 current loss 0.599899, current_train_items 177056.
I0302 19:01:02.811192 22699590365312 run.py:483] Algo bellman_ford step 5533 current loss 0.692635, current_train_items 177088.
I0302 19:01:02.846415 22699590365312 run.py:483] Algo bellman_ford step 5534 current loss 0.854445, current_train_items 177120.
I0302 19:01:02.865739 22699590365312 run.py:483] Algo bellman_ford step 5535 current loss 0.261075, current_train_items 177152.
I0302 19:01:02.882103 22699590365312 run.py:483] Algo bellman_ford step 5536 current loss 0.525151, current_train_items 177184.
I0302 19:01:02.904709 22699590365312 run.py:483] Algo bellman_ford step 5537 current loss 0.613768, current_train_items 177216.
I0302 19:01:02.934672 22699590365312 run.py:483] Algo bellman_ford step 5538 current loss 0.662318, current_train_items 177248.
I0302 19:01:02.969630 22699590365312 run.py:483] Algo bellman_ford step 5539 current loss 0.762519, current_train_items 177280.
I0302 19:01:02.988988 22699590365312 run.py:483] Algo bellman_ford step 5540 current loss 0.280192, current_train_items 177312.
I0302 19:01:03.005524 22699590365312 run.py:483] Algo bellman_ford step 5541 current loss 0.404377, current_train_items 177344.
I0302 19:01:03.029574 22699590365312 run.py:483] Algo bellman_ford step 5542 current loss 0.570883, current_train_items 177376.
I0302 19:01:03.060136 22699590365312 run.py:483] Algo bellman_ford step 5543 current loss 0.548363, current_train_items 177408.
I0302 19:01:03.092688 22699590365312 run.py:483] Algo bellman_ford step 5544 current loss 0.771900, current_train_items 177440.
I0302 19:01:03.112015 22699590365312 run.py:483] Algo bellman_ford step 5545 current loss 0.294916, current_train_items 177472.
I0302 19:01:03.128629 22699590365312 run.py:483] Algo bellman_ford step 5546 current loss 0.487609, current_train_items 177504.
I0302 19:01:03.151937 22699590365312 run.py:483] Algo bellman_ford step 5547 current loss 0.586293, current_train_items 177536.
I0302 19:01:03.181991 22699590365312 run.py:483] Algo bellman_ford step 5548 current loss 0.627360, current_train_items 177568.
I0302 19:01:03.214337 22699590365312 run.py:483] Algo bellman_ford step 5549 current loss 0.791496, current_train_items 177600.
I0302 19:01:03.233506 22699590365312 run.py:483] Algo bellman_ford step 5550 current loss 0.250622, current_train_items 177632.
I0302 19:01:03.241436 22699590365312 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.9443359375, 'score': 0.9443359375, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0302 19:01:03.241544 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.942, current avg val score is 0.944, val scores are: bellman_ford: 0.944
I0302 19:01:03.270417 22699590365312 run.py:483] Algo bellman_ford step 5551 current loss 0.406610, current_train_items 177664.
I0302 19:01:03.295309 22699590365312 run.py:483] Algo bellman_ford step 5552 current loss 0.663657, current_train_items 177696.
I0302 19:01:03.326244 22699590365312 run.py:483] Algo bellman_ford step 5553 current loss 0.767653, current_train_items 177728.
I0302 19:01:03.363375 22699590365312 run.py:483] Algo bellman_ford step 5554 current loss 0.922540, current_train_items 177760.
I0302 19:01:03.383911 22699590365312 run.py:483] Algo bellman_ford step 5555 current loss 0.263931, current_train_items 177792.
I0302 19:01:03.399859 22699590365312 run.py:483] Algo bellman_ford step 5556 current loss 0.390534, current_train_items 177824.
I0302 19:01:03.423743 22699590365312 run.py:483] Algo bellman_ford step 5557 current loss 0.630000, current_train_items 177856.
I0302 19:01:03.454064 22699590365312 run.py:483] Algo bellman_ford step 5558 current loss 0.614346, current_train_items 177888.
I0302 19:01:03.488110 22699590365312 run.py:483] Algo bellman_ford step 5559 current loss 0.694927, current_train_items 177920.
I0302 19:01:03.508047 22699590365312 run.py:483] Algo bellman_ford step 5560 current loss 0.275755, current_train_items 177952.
I0302 19:01:03.524125 22699590365312 run.py:483] Algo bellman_ford step 5561 current loss 0.490375, current_train_items 177984.
I0302 19:01:03.547269 22699590365312 run.py:483] Algo bellman_ford step 5562 current loss 0.607040, current_train_items 178016.
I0302 19:01:03.578624 22699590365312 run.py:483] Algo bellman_ford step 5563 current loss 0.685525, current_train_items 178048.
I0302 19:01:03.613433 22699590365312 run.py:483] Algo bellman_ford step 5564 current loss 0.780237, current_train_items 178080.
I0302 19:01:03.633344 22699590365312 run.py:483] Algo bellman_ford step 5565 current loss 0.362880, current_train_items 178112.
I0302 19:01:03.649424 22699590365312 run.py:483] Algo bellman_ford step 5566 current loss 0.416095, current_train_items 178144.
I0302 19:01:03.673268 22699590365312 run.py:483] Algo bellman_ford step 5567 current loss 0.651230, current_train_items 178176.
I0302 19:01:03.705005 22699590365312 run.py:483] Algo bellman_ford step 5568 current loss 0.959947, current_train_items 178208.
I0302 19:01:03.739132 22699590365312 run.py:483] Algo bellman_ford step 5569 current loss 1.010759, current_train_items 178240.
I0302 19:01:03.759243 22699590365312 run.py:483] Algo bellman_ford step 5570 current loss 0.378863, current_train_items 178272.
I0302 19:01:03.775186 22699590365312 run.py:483] Algo bellman_ford step 5571 current loss 0.442417, current_train_items 178304.
I0302 19:01:03.798318 22699590365312 run.py:483] Algo bellman_ford step 5572 current loss 0.528661, current_train_items 178336.
I0302 19:01:03.829030 22699590365312 run.py:483] Algo bellman_ford step 5573 current loss 0.634700, current_train_items 178368.
I0302 19:01:03.863019 22699590365312 run.py:483] Algo bellman_ford step 5574 current loss 0.925419, current_train_items 178400.
I0302 19:01:03.883109 22699590365312 run.py:483] Algo bellman_ford step 5575 current loss 0.336034, current_train_items 178432.
I0302 19:01:03.899164 22699590365312 run.py:483] Algo bellman_ford step 5576 current loss 0.593136, current_train_items 178464.
I0302 19:01:03.922096 22699590365312 run.py:483] Algo bellman_ford step 5577 current loss 0.624458, current_train_items 178496.
I0302 19:01:03.952893 22699590365312 run.py:483] Algo bellman_ford step 5578 current loss 0.687947, current_train_items 178528.
I0302 19:01:03.988013 22699590365312 run.py:483] Algo bellman_ford step 5579 current loss 0.767550, current_train_items 178560.
I0302 19:01:04.007460 22699590365312 run.py:483] Algo bellman_ford step 5580 current loss 0.276288, current_train_items 178592.
I0302 19:01:04.023232 22699590365312 run.py:483] Algo bellman_ford step 5581 current loss 0.394995, current_train_items 178624.
I0302 19:01:04.047805 22699590365312 run.py:483] Algo bellman_ford step 5582 current loss 0.569766, current_train_items 178656.
I0302 19:01:04.078482 22699590365312 run.py:483] Algo bellman_ford step 5583 current loss 0.622116, current_train_items 178688.
I0302 19:01:04.112614 22699590365312 run.py:483] Algo bellman_ford step 5584 current loss 0.777145, current_train_items 178720.
I0302 19:01:04.132665 22699590365312 run.py:483] Algo bellman_ford step 5585 current loss 0.368956, current_train_items 178752.
I0302 19:01:04.148884 22699590365312 run.py:483] Algo bellman_ford step 5586 current loss 0.440856, current_train_items 178784.
I0302 19:01:04.173548 22699590365312 run.py:483] Algo bellman_ford step 5587 current loss 0.653740, current_train_items 178816.
I0302 19:01:04.205358 22699590365312 run.py:483] Algo bellman_ford step 5588 current loss 0.694322, current_train_items 178848.
I0302 19:01:04.236581 22699590365312 run.py:483] Algo bellman_ford step 5589 current loss 0.690186, current_train_items 178880.
I0302 19:01:04.256733 22699590365312 run.py:483] Algo bellman_ford step 5590 current loss 0.305027, current_train_items 178912.
I0302 19:01:04.272892 22699590365312 run.py:483] Algo bellman_ford step 5591 current loss 0.417827, current_train_items 178944.
I0302 19:01:04.295202 22699590365312 run.py:483] Algo bellman_ford step 5592 current loss 0.564039, current_train_items 178976.
I0302 19:01:04.326117 22699590365312 run.py:483] Algo bellman_ford step 5593 current loss 0.660353, current_train_items 179008.
I0302 19:01:04.361164 22699590365312 run.py:483] Algo bellman_ford step 5594 current loss 0.669440, current_train_items 179040.
I0302 19:01:04.380817 22699590365312 run.py:483] Algo bellman_ford step 5595 current loss 0.418405, current_train_items 179072.
I0302 19:01:04.396807 22699590365312 run.py:483] Algo bellman_ford step 5596 current loss 0.476773, current_train_items 179104.
I0302 19:01:04.420266 22699590365312 run.py:483] Algo bellman_ford step 5597 current loss 0.606127, current_train_items 179136.
I0302 19:01:04.451005 22699590365312 run.py:483] Algo bellman_ford step 5598 current loss 0.618683, current_train_items 179168.
I0302 19:01:04.480653 22699590365312 run.py:483] Algo bellman_ford step 5599 current loss 0.643538, current_train_items 179200.
I0302 19:01:04.500521 22699590365312 run.py:483] Algo bellman_ford step 5600 current loss 0.292488, current_train_items 179232.
I0302 19:01:04.508310 22699590365312 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0302 19:01:04.508415 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.944, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:01:04.525189 22699590365312 run.py:483] Algo bellman_ford step 5601 current loss 0.488552, current_train_items 179264.
I0302 19:01:04.549162 22699590365312 run.py:483] Algo bellman_ford step 5602 current loss 0.672298, current_train_items 179296.
I0302 19:01:04.580336 22699590365312 run.py:483] Algo bellman_ford step 5603 current loss 0.738764, current_train_items 179328.
I0302 19:01:04.612662 22699590365312 run.py:483] Algo bellman_ford step 5604 current loss 0.698628, current_train_items 179360.
I0302 19:01:04.632667 22699590365312 run.py:483] Algo bellman_ford step 5605 current loss 0.263150, current_train_items 179392.
I0302 19:01:04.648332 22699590365312 run.py:483] Algo bellman_ford step 5606 current loss 0.457475, current_train_items 179424.
I0302 19:01:04.672133 22699590365312 run.py:483] Algo bellman_ford step 5607 current loss 0.530306, current_train_items 179456.
I0302 19:01:04.701698 22699590365312 run.py:483] Algo bellman_ford step 5608 current loss 0.608898, current_train_items 179488.
I0302 19:01:04.735887 22699590365312 run.py:483] Algo bellman_ford step 5609 current loss 0.845824, current_train_items 179520.
I0302 19:01:04.755714 22699590365312 run.py:483] Algo bellman_ford step 5610 current loss 0.247604, current_train_items 179552.
I0302 19:01:04.771996 22699590365312 run.py:483] Algo bellman_ford step 5611 current loss 0.447842, current_train_items 179584.
I0302 19:01:04.795105 22699590365312 run.py:483] Algo bellman_ford step 5612 current loss 0.537086, current_train_items 179616.
I0302 19:01:04.826694 22699590365312 run.py:483] Algo bellman_ford step 5613 current loss 0.648710, current_train_items 179648.
I0302 19:01:04.858848 22699590365312 run.py:483] Algo bellman_ford step 5614 current loss 0.710506, current_train_items 179680.
I0302 19:01:04.878583 22699590365312 run.py:483] Algo bellman_ford step 5615 current loss 0.259442, current_train_items 179712.
I0302 19:01:04.894910 22699590365312 run.py:483] Algo bellman_ford step 5616 current loss 0.463273, current_train_items 179744.
I0302 19:01:04.918735 22699590365312 run.py:483] Algo bellman_ford step 5617 current loss 0.528742, current_train_items 179776.
I0302 19:01:04.948872 22699590365312 run.py:483] Algo bellman_ford step 5618 current loss 0.652407, current_train_items 179808.
I0302 19:01:04.983474 22699590365312 run.py:483] Algo bellman_ford step 5619 current loss 0.832527, current_train_items 179840.
I0302 19:01:05.003025 22699590365312 run.py:483] Algo bellman_ford step 5620 current loss 0.288148, current_train_items 179872.
I0302 19:01:05.019115 22699590365312 run.py:483] Algo bellman_ford step 5621 current loss 0.490924, current_train_items 179904.
I0302 19:01:05.042488 22699590365312 run.py:483] Algo bellman_ford step 5622 current loss 0.600872, current_train_items 179936.
I0302 19:01:05.073002 22699590365312 run.py:483] Algo bellman_ford step 5623 current loss 0.644161, current_train_items 179968.
I0302 19:01:05.106893 22699590365312 run.py:483] Algo bellman_ford step 5624 current loss 0.689298, current_train_items 180000.
I0302 19:01:05.126300 22699590365312 run.py:483] Algo bellman_ford step 5625 current loss 0.268360, current_train_items 180032.
I0302 19:01:05.142747 22699590365312 run.py:483] Algo bellman_ford step 5626 current loss 0.534505, current_train_items 180064.
I0302 19:01:05.166548 22699590365312 run.py:483] Algo bellman_ford step 5627 current loss 0.633216, current_train_items 180096.
I0302 19:01:05.197337 22699590365312 run.py:483] Algo bellman_ford step 5628 current loss 0.676292, current_train_items 180128.
I0302 19:01:05.231342 22699590365312 run.py:483] Algo bellman_ford step 5629 current loss 0.706579, current_train_items 180160.
I0302 19:01:05.250707 22699590365312 run.py:483] Algo bellman_ford step 5630 current loss 0.376668, current_train_items 180192.
I0302 19:01:05.266708 22699590365312 run.py:483] Algo bellman_ford step 5631 current loss 0.371201, current_train_items 180224.
I0302 19:01:05.291138 22699590365312 run.py:483] Algo bellman_ford step 5632 current loss 0.597435, current_train_items 180256.
I0302 19:01:05.321764 22699590365312 run.py:483] Algo bellman_ford step 5633 current loss 0.596435, current_train_items 180288.
I0302 19:01:05.355876 22699590365312 run.py:483] Algo bellman_ford step 5634 current loss 0.772910, current_train_items 180320.
I0302 19:01:05.375663 22699590365312 run.py:483] Algo bellman_ford step 5635 current loss 0.285832, current_train_items 180352.
I0302 19:01:05.391887 22699590365312 run.py:483] Algo bellman_ford step 5636 current loss 0.455816, current_train_items 180384.
I0302 19:01:05.415402 22699590365312 run.py:483] Algo bellman_ford step 5637 current loss 0.610958, current_train_items 180416.
I0302 19:01:05.445196 22699590365312 run.py:483] Algo bellman_ford step 5638 current loss 0.623658, current_train_items 180448.
I0302 19:01:05.477474 22699590365312 run.py:483] Algo bellman_ford step 5639 current loss 0.816006, current_train_items 180480.
I0302 19:01:05.497037 22699590365312 run.py:483] Algo bellman_ford step 5640 current loss 0.274638, current_train_items 180512.
I0302 19:01:05.512924 22699590365312 run.py:483] Algo bellman_ford step 5641 current loss 0.398824, current_train_items 180544.
I0302 19:01:05.536447 22699590365312 run.py:483] Algo bellman_ford step 5642 current loss 0.528471, current_train_items 180576.
I0302 19:01:05.567275 22699590365312 run.py:483] Algo bellman_ford step 5643 current loss 0.715489, current_train_items 180608.
I0302 19:01:05.600279 22699590365312 run.py:483] Algo bellman_ford step 5644 current loss 0.717040, current_train_items 180640.
I0302 19:01:05.620037 22699590365312 run.py:483] Algo bellman_ford step 5645 current loss 0.282733, current_train_items 180672.
I0302 19:01:05.636689 22699590365312 run.py:483] Algo bellman_ford step 5646 current loss 0.464274, current_train_items 180704.
I0302 19:01:05.659814 22699590365312 run.py:483] Algo bellman_ford step 5647 current loss 0.597503, current_train_items 180736.
I0302 19:01:05.690426 22699590365312 run.py:483] Algo bellman_ford step 5648 current loss 0.616562, current_train_items 180768.
I0302 19:01:05.725503 22699590365312 run.py:483] Algo bellman_ford step 5649 current loss 0.780597, current_train_items 180800.
I0302 19:01:05.745145 22699590365312 run.py:483] Algo bellman_ford step 5650 current loss 0.252649, current_train_items 180832.
I0302 19:01:05.753369 22699590365312 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0302 19:01:05.753476 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.944, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:01:05.770425 22699590365312 run.py:483] Algo bellman_ford step 5651 current loss 0.531852, current_train_items 180864.
I0302 19:01:05.793909 22699590365312 run.py:483] Algo bellman_ford step 5652 current loss 0.632302, current_train_items 180896.
I0302 19:01:05.826663 22699590365312 run.py:483] Algo bellman_ford step 5653 current loss 0.814612, current_train_items 180928.
I0302 19:01:05.862107 22699590365312 run.py:483] Algo bellman_ford step 5654 current loss 0.967165, current_train_items 180960.
I0302 19:01:05.882114 22699590365312 run.py:483] Algo bellman_ford step 5655 current loss 0.293475, current_train_items 180992.
I0302 19:01:05.898244 22699590365312 run.py:483] Algo bellman_ford step 5656 current loss 0.433040, current_train_items 181024.
I0302 19:01:05.920725 22699590365312 run.py:483] Algo bellman_ford step 5657 current loss 0.545274, current_train_items 181056.
I0302 19:01:05.952285 22699590365312 run.py:483] Algo bellman_ford step 5658 current loss 0.674983, current_train_items 181088.
I0302 19:01:05.986118 22699590365312 run.py:483] Algo bellman_ford step 5659 current loss 0.733269, current_train_items 181120.
I0302 19:01:06.006286 22699590365312 run.py:483] Algo bellman_ford step 5660 current loss 0.334477, current_train_items 181152.
I0302 19:01:06.022257 22699590365312 run.py:483] Algo bellman_ford step 5661 current loss 0.383808, current_train_items 181184.
I0302 19:01:06.045227 22699590365312 run.py:483] Algo bellman_ford step 5662 current loss 0.556541, current_train_items 181216.
I0302 19:01:06.078356 22699590365312 run.py:483] Algo bellman_ford step 5663 current loss 0.696024, current_train_items 181248.
I0302 19:01:06.112375 22699590365312 run.py:483] Algo bellman_ford step 5664 current loss 0.756794, current_train_items 181280.
I0302 19:01:06.132065 22699590365312 run.py:483] Algo bellman_ford step 5665 current loss 0.292722, current_train_items 181312.
I0302 19:01:06.148567 22699590365312 run.py:483] Algo bellman_ford step 5666 current loss 0.482238, current_train_items 181344.
I0302 19:01:06.171713 22699590365312 run.py:483] Algo bellman_ford step 5667 current loss 0.518440, current_train_items 181376.
I0302 19:01:06.203492 22699590365312 run.py:483] Algo bellman_ford step 5668 current loss 0.678197, current_train_items 181408.
I0302 19:01:06.236481 22699590365312 run.py:483] Algo bellman_ford step 5669 current loss 0.652942, current_train_items 181440.
I0302 19:01:06.256438 22699590365312 run.py:483] Algo bellman_ford step 5670 current loss 0.235139, current_train_items 181472.
I0302 19:01:06.272429 22699590365312 run.py:483] Algo bellman_ford step 5671 current loss 0.386850, current_train_items 181504.
I0302 19:01:06.295048 22699590365312 run.py:483] Algo bellman_ford step 5672 current loss 0.563163, current_train_items 181536.
I0302 19:01:06.325713 22699590365312 run.py:483] Algo bellman_ford step 5673 current loss 0.628993, current_train_items 181568.
I0302 19:01:06.358873 22699590365312 run.py:483] Algo bellman_ford step 5674 current loss 0.616920, current_train_items 181600.
I0302 19:01:06.378620 22699590365312 run.py:483] Algo bellman_ford step 5675 current loss 0.260793, current_train_items 181632.
I0302 19:01:06.394686 22699590365312 run.py:483] Algo bellman_ford step 5676 current loss 0.464998, current_train_items 181664.
I0302 19:01:06.418062 22699590365312 run.py:483] Algo bellman_ford step 5677 current loss 0.683771, current_train_items 181696.
I0302 19:01:06.450394 22699590365312 run.py:483] Algo bellman_ford step 5678 current loss 0.860630, current_train_items 181728.
I0302 19:01:06.483513 22699590365312 run.py:483] Algo bellman_ford step 5679 current loss 0.851287, current_train_items 181760.
I0302 19:01:06.503224 22699590365312 run.py:483] Algo bellman_ford step 5680 current loss 0.302213, current_train_items 181792.
I0302 19:01:06.519672 22699590365312 run.py:483] Algo bellman_ford step 5681 current loss 0.435350, current_train_items 181824.
I0302 19:01:06.542864 22699590365312 run.py:483] Algo bellman_ford step 5682 current loss 0.576947, current_train_items 181856.
I0302 19:01:06.573322 22699590365312 run.py:483] Algo bellman_ford step 5683 current loss 0.594782, current_train_items 181888.
I0302 19:01:06.605919 22699590365312 run.py:483] Algo bellman_ford step 5684 current loss 0.605311, current_train_items 181920.
I0302 19:01:06.625655 22699590365312 run.py:483] Algo bellman_ford step 5685 current loss 0.261501, current_train_items 181952.
I0302 19:01:06.641832 22699590365312 run.py:483] Algo bellman_ford step 5686 current loss 0.537627, current_train_items 181984.
I0302 19:01:06.665369 22699590365312 run.py:483] Algo bellman_ford step 5687 current loss 0.610990, current_train_items 182016.
I0302 19:01:06.696830 22699590365312 run.py:483] Algo bellman_ford step 5688 current loss 0.667430, current_train_items 182048.
I0302 19:01:06.729431 22699590365312 run.py:483] Algo bellman_ford step 5689 current loss 0.755100, current_train_items 182080.
I0302 19:01:06.749372 22699590365312 run.py:483] Algo bellman_ford step 5690 current loss 0.283047, current_train_items 182112.
I0302 19:01:06.766111 22699590365312 run.py:483] Algo bellman_ford step 5691 current loss 0.442988, current_train_items 182144.
I0302 19:01:06.789215 22699590365312 run.py:483] Algo bellman_ford step 5692 current loss 0.550707, current_train_items 182176.
I0302 19:01:06.819772 22699590365312 run.py:483] Algo bellman_ford step 5693 current loss 0.672210, current_train_items 182208.
I0302 19:01:06.852387 22699590365312 run.py:483] Algo bellman_ford step 5694 current loss 0.873274, current_train_items 182240.
I0302 19:01:06.871930 22699590365312 run.py:483] Algo bellman_ford step 5695 current loss 0.260946, current_train_items 182272.
I0302 19:01:06.888391 22699590365312 run.py:483] Algo bellman_ford step 5696 current loss 0.542109, current_train_items 182304.
I0302 19:01:06.911610 22699590365312 run.py:483] Algo bellman_ford step 5697 current loss 0.746498, current_train_items 182336.
I0302 19:01:06.943109 22699590365312 run.py:483] Algo bellman_ford step 5698 current loss 0.957914, current_train_items 182368.
I0302 19:01:06.977238 22699590365312 run.py:483] Algo bellman_ford step 5699 current loss 0.907657, current_train_items 182400.
I0302 19:01:06.997075 22699590365312 run.py:483] Algo bellman_ford step 5700 current loss 0.334780, current_train_items 182432.
I0302 19:01:07.004859 22699590365312 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0302 19:01:07.004963 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.944, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 19:01:07.021291 22699590365312 run.py:483] Algo bellman_ford step 5701 current loss 0.438239, current_train_items 182464.
I0302 19:01:07.044514 22699590365312 run.py:483] Algo bellman_ford step 5702 current loss 0.594888, current_train_items 182496.
I0302 19:01:07.076129 22699590365312 run.py:483] Algo bellman_ford step 5703 current loss 0.777266, current_train_items 182528.
I0302 19:01:07.111253 22699590365312 run.py:483] Algo bellman_ford step 5704 current loss 0.863059, current_train_items 182560.
I0302 19:01:07.131409 22699590365312 run.py:483] Algo bellman_ford step 5705 current loss 0.327967, current_train_items 182592.
I0302 19:01:07.147124 22699590365312 run.py:483] Algo bellman_ford step 5706 current loss 0.407663, current_train_items 182624.
I0302 19:01:07.171280 22699590365312 run.py:483] Algo bellman_ford step 5707 current loss 0.597934, current_train_items 182656.
I0302 19:01:07.201898 22699590365312 run.py:483] Algo bellman_ford step 5708 current loss 0.657816, current_train_items 182688.
I0302 19:01:07.235564 22699590365312 run.py:483] Algo bellman_ford step 5709 current loss 0.691875, current_train_items 182720.
I0302 19:01:07.255222 22699590365312 run.py:483] Algo bellman_ford step 5710 current loss 0.286558, current_train_items 182752.
I0302 19:01:07.271131 22699590365312 run.py:483] Algo bellman_ford step 5711 current loss 0.639540, current_train_items 182784.
I0302 19:01:07.293942 22699590365312 run.py:483] Algo bellman_ford step 5712 current loss 0.602760, current_train_items 182816.
I0302 19:01:07.324150 22699590365312 run.py:483] Algo bellman_ford step 5713 current loss 0.643536, current_train_items 182848.
I0302 19:01:07.357224 22699590365312 run.py:483] Algo bellman_ford step 5714 current loss 0.673727, current_train_items 182880.
I0302 19:01:07.376779 22699590365312 run.py:483] Algo bellman_ford step 5715 current loss 0.344449, current_train_items 182912.
I0302 19:01:07.393077 22699590365312 run.py:483] Algo bellman_ford step 5716 current loss 0.443869, current_train_items 182944.
I0302 19:01:07.416971 22699590365312 run.py:483] Algo bellman_ford step 5717 current loss 0.612191, current_train_items 182976.
I0302 19:01:07.447493 22699590365312 run.py:483] Algo bellman_ford step 5718 current loss 0.694453, current_train_items 183008.
I0302 19:01:07.481313 22699590365312 run.py:483] Algo bellman_ford step 5719 current loss 0.845569, current_train_items 183040.
I0302 19:01:07.500973 22699590365312 run.py:483] Algo bellman_ford step 5720 current loss 0.245252, current_train_items 183072.
I0302 19:01:07.517035 22699590365312 run.py:483] Algo bellman_ford step 5721 current loss 0.508750, current_train_items 183104.
I0302 19:01:07.539972 22699590365312 run.py:483] Algo bellman_ford step 5722 current loss 0.662686, current_train_items 183136.
I0302 19:01:07.570570 22699590365312 run.py:483] Algo bellman_ford step 5723 current loss 0.690900, current_train_items 183168.
I0302 19:01:07.603254 22699590365312 run.py:483] Algo bellman_ford step 5724 current loss 0.780912, current_train_items 183200.
I0302 19:01:07.622965 22699590365312 run.py:483] Algo bellman_ford step 5725 current loss 0.326086, current_train_items 183232.
I0302 19:01:07.639101 22699590365312 run.py:483] Algo bellman_ford step 5726 current loss 0.434907, current_train_items 183264.
I0302 19:01:07.663736 22699590365312 run.py:483] Algo bellman_ford step 5727 current loss 0.688241, current_train_items 183296.
I0302 19:01:07.693610 22699590365312 run.py:483] Algo bellman_ford step 5728 current loss 0.738941, current_train_items 183328.
I0302 19:01:07.726771 22699590365312 run.py:483] Algo bellman_ford step 5729 current loss 0.804885, current_train_items 183360.
I0302 19:01:07.746145 22699590365312 run.py:483] Algo bellman_ford step 5730 current loss 0.311443, current_train_items 183392.
I0302 19:01:07.762655 22699590365312 run.py:483] Algo bellman_ford step 5731 current loss 0.541215, current_train_items 183424.
I0302 19:01:07.786836 22699590365312 run.py:483] Algo bellman_ford step 5732 current loss 0.769009, current_train_items 183456.
I0302 19:01:07.817122 22699590365312 run.py:483] Algo bellman_ford step 5733 current loss 0.723171, current_train_items 183488.
I0302 19:01:07.851330 22699590365312 run.py:483] Algo bellman_ford step 5734 current loss 0.881998, current_train_items 183520.
I0302 19:01:07.870815 22699590365312 run.py:483] Algo bellman_ford step 5735 current loss 0.291090, current_train_items 183552.
I0302 19:01:07.887213 22699590365312 run.py:483] Algo bellman_ford step 5736 current loss 0.481400, current_train_items 183584.
I0302 19:01:07.910719 22699590365312 run.py:483] Algo bellman_ford step 5737 current loss 0.621844, current_train_items 183616.
I0302 19:01:07.941145 22699590365312 run.py:483] Algo bellman_ford step 5738 current loss 0.610008, current_train_items 183648.
I0302 19:01:07.974954 22699590365312 run.py:483] Algo bellman_ford step 5739 current loss 0.671132, current_train_items 183680.
I0302 19:01:07.994469 22699590365312 run.py:483] Algo bellman_ford step 5740 current loss 0.335866, current_train_items 183712.
I0302 19:01:08.010919 22699590365312 run.py:483] Algo bellman_ford step 5741 current loss 0.437866, current_train_items 183744.
I0302 19:01:08.035274 22699590365312 run.py:483] Algo bellman_ford step 5742 current loss 0.540199, current_train_items 183776.
I0302 19:01:08.067083 22699590365312 run.py:483] Algo bellman_ford step 5743 current loss 0.613246, current_train_items 183808.
I0302 19:01:08.101363 22699590365312 run.py:483] Algo bellman_ford step 5744 current loss 0.774408, current_train_items 183840.
I0302 19:01:08.121023 22699590365312 run.py:483] Algo bellman_ford step 5745 current loss 0.362894, current_train_items 183872.
I0302 19:01:08.137585 22699590365312 run.py:483] Algo bellman_ford step 5746 current loss 0.484374, current_train_items 183904.
I0302 19:01:08.161310 22699590365312 run.py:483] Algo bellman_ford step 5747 current loss 0.590443, current_train_items 183936.
I0302 19:01:08.192046 22699590365312 run.py:483] Algo bellman_ford step 5748 current loss 0.732330, current_train_items 183968.
I0302 19:01:08.224696 22699590365312 run.py:483] Algo bellman_ford step 5749 current loss 0.717046, current_train_items 184000.
I0302 19:01:08.244299 22699590365312 run.py:483] Algo bellman_ford step 5750 current loss 0.307441, current_train_items 184032.
I0302 19:01:08.252480 22699590365312 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0302 19:01:08.252588 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.944, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 19:01:08.269993 22699590365312 run.py:483] Algo bellman_ford step 5751 current loss 0.416527, current_train_items 184064.
I0302 19:01:08.293606 22699590365312 run.py:483] Algo bellman_ford step 5752 current loss 0.614171, current_train_items 184096.
I0302 19:01:08.324084 22699590365312 run.py:483] Algo bellman_ford step 5753 current loss 0.574457, current_train_items 184128.
I0302 19:01:08.359484 22699590365312 run.py:483] Algo bellman_ford step 5754 current loss 0.802951, current_train_items 184160.
I0302 19:01:08.379803 22699590365312 run.py:483] Algo bellman_ford step 5755 current loss 0.277595, current_train_items 184192.
I0302 19:01:08.395400 22699590365312 run.py:483] Algo bellman_ford step 5756 current loss 0.458513, current_train_items 184224.
I0302 19:01:08.419576 22699590365312 run.py:483] Algo bellman_ford step 5757 current loss 0.683919, current_train_items 184256.
I0302 19:01:08.451297 22699590365312 run.py:483] Algo bellman_ford step 5758 current loss 0.809731, current_train_items 184288.
I0302 19:01:08.484852 22699590365312 run.py:483] Algo bellman_ford step 5759 current loss 0.786866, current_train_items 184320.
I0302 19:01:08.504634 22699590365312 run.py:483] Algo bellman_ford step 5760 current loss 0.387538, current_train_items 184352.
I0302 19:01:08.521027 22699590365312 run.py:483] Algo bellman_ford step 5761 current loss 0.461595, current_train_items 184384.
I0302 19:01:08.544669 22699590365312 run.py:483] Algo bellman_ford step 5762 current loss 0.536662, current_train_items 184416.
I0302 19:01:08.574393 22699590365312 run.py:483] Algo bellman_ford step 5763 current loss 0.596234, current_train_items 184448.
I0302 19:01:08.608708 22699590365312 run.py:483] Algo bellman_ford step 5764 current loss 0.746635, current_train_items 184480.
I0302 19:01:08.628503 22699590365312 run.py:483] Algo bellman_ford step 5765 current loss 0.333646, current_train_items 184512.
I0302 19:01:08.644710 22699590365312 run.py:483] Algo bellman_ford step 5766 current loss 0.484916, current_train_items 184544.
I0302 19:01:08.667785 22699590365312 run.py:483] Algo bellman_ford step 5767 current loss 0.597049, current_train_items 184576.
I0302 19:01:08.699421 22699590365312 run.py:483] Algo bellman_ford step 5768 current loss 0.733187, current_train_items 184608.
I0302 19:01:08.732656 22699590365312 run.py:483] Algo bellman_ford step 5769 current loss 0.710484, current_train_items 184640.
I0302 19:01:08.752608 22699590365312 run.py:483] Algo bellman_ford step 5770 current loss 0.268874, current_train_items 184672.
I0302 19:01:08.768709 22699590365312 run.py:483] Algo bellman_ford step 5771 current loss 0.435869, current_train_items 184704.
I0302 19:01:08.790778 22699590365312 run.py:483] Algo bellman_ford step 5772 current loss 0.587000, current_train_items 184736.
I0302 19:01:08.820770 22699590365312 run.py:483] Algo bellman_ford step 5773 current loss 0.686777, current_train_items 184768.
I0302 19:01:08.853357 22699590365312 run.py:483] Algo bellman_ford step 5774 current loss 0.647293, current_train_items 184800.
I0302 19:01:08.873368 22699590365312 run.py:483] Algo bellman_ford step 5775 current loss 0.324611, current_train_items 184832.
I0302 19:01:08.889866 22699590365312 run.py:483] Algo bellman_ford step 5776 current loss 0.402450, current_train_items 184864.
I0302 19:01:08.912284 22699590365312 run.py:483] Algo bellman_ford step 5777 current loss 0.553864, current_train_items 184896.
I0302 19:01:08.943291 22699590365312 run.py:483] Algo bellman_ford step 5778 current loss 0.663373, current_train_items 184928.
I0302 19:01:08.977023 22699590365312 run.py:483] Algo bellman_ford step 5779 current loss 0.791394, current_train_items 184960.
I0302 19:01:08.996565 22699590365312 run.py:483] Algo bellman_ford step 5780 current loss 0.252508, current_train_items 184992.
I0302 19:01:09.012668 22699590365312 run.py:483] Algo bellman_ford step 5781 current loss 0.420096, current_train_items 185024.
I0302 19:01:09.036983 22699590365312 run.py:483] Algo bellman_ford step 5782 current loss 0.577211, current_train_items 185056.
I0302 19:01:09.066848 22699590365312 run.py:483] Algo bellman_ford step 5783 current loss 0.612478, current_train_items 185088.
I0302 19:01:09.098585 22699590365312 run.py:483] Algo bellman_ford step 5784 current loss 0.778490, current_train_items 185120.
I0302 19:01:09.118314 22699590365312 run.py:483] Algo bellman_ford step 5785 current loss 0.318351, current_train_items 185152.
I0302 19:01:09.134273 22699590365312 run.py:483] Algo bellman_ford step 5786 current loss 0.452172, current_train_items 185184.
I0302 19:01:09.156567 22699590365312 run.py:483] Algo bellman_ford step 5787 current loss 0.530641, current_train_items 185216.
I0302 19:01:09.188138 22699590365312 run.py:483] Algo bellman_ford step 5788 current loss 0.655878, current_train_items 185248.
I0302 19:01:09.220463 22699590365312 run.py:483] Algo bellman_ford step 5789 current loss 0.717830, current_train_items 185280.
I0302 19:01:09.240266 22699590365312 run.py:483] Algo bellman_ford step 5790 current loss 0.253978, current_train_items 185312.
I0302 19:01:09.256789 22699590365312 run.py:483] Algo bellman_ford step 5791 current loss 0.475117, current_train_items 185344.
I0302 19:01:09.278984 22699590365312 run.py:483] Algo bellman_ford step 5792 current loss 0.568039, current_train_items 185376.
I0302 19:01:09.309690 22699590365312 run.py:483] Algo bellman_ford step 5793 current loss 0.668226, current_train_items 185408.
I0302 19:01:09.343211 22699590365312 run.py:483] Algo bellman_ford step 5794 current loss 0.789469, current_train_items 185440.
I0302 19:01:09.362677 22699590365312 run.py:483] Algo bellman_ford step 5795 current loss 0.271969, current_train_items 185472.
I0302 19:01:09.378657 22699590365312 run.py:483] Algo bellman_ford step 5796 current loss 0.404614, current_train_items 185504.
I0302 19:01:09.402675 22699590365312 run.py:483] Algo bellman_ford step 5797 current loss 0.640386, current_train_items 185536.
I0302 19:01:09.433648 22699590365312 run.py:483] Algo bellman_ford step 5798 current loss 0.591647, current_train_items 185568.
I0302 19:01:09.467885 22699590365312 run.py:483] Algo bellman_ford step 5799 current loss 0.811867, current_train_items 185600.
I0302 19:01:09.487640 22699590365312 run.py:483] Algo bellman_ford step 5800 current loss 0.309099, current_train_items 185632.
I0302 19:01:09.495644 22699590365312 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.884765625, 'score': 0.884765625, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0302 19:01:09.495750 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.944, current avg val score is 0.885, val scores are: bellman_ford: 0.885
I0302 19:01:09.512728 22699590365312 run.py:483] Algo bellman_ford step 5801 current loss 0.550519, current_train_items 185664.
I0302 19:01:09.535505 22699590365312 run.py:483] Algo bellman_ford step 5802 current loss 0.581892, current_train_items 185696.
I0302 19:01:09.566627 22699590365312 run.py:483] Algo bellman_ford step 5803 current loss 0.738857, current_train_items 185728.
I0302 19:01:09.602085 22699590365312 run.py:483] Algo bellman_ford step 5804 current loss 0.830395, current_train_items 185760.
I0302 19:01:09.622053 22699590365312 run.py:483] Algo bellman_ford step 5805 current loss 0.300168, current_train_items 185792.
I0302 19:01:09.638069 22699590365312 run.py:483] Algo bellman_ford step 5806 current loss 0.396155, current_train_items 185824.
I0302 19:01:09.662369 22699590365312 run.py:483] Algo bellman_ford step 5807 current loss 0.578954, current_train_items 185856.
I0302 19:01:09.693578 22699590365312 run.py:483] Algo bellman_ford step 5808 current loss 0.678110, current_train_items 185888.
I0302 19:01:09.726879 22699590365312 run.py:483] Algo bellman_ford step 5809 current loss 0.661080, current_train_items 185920.
I0302 19:01:09.746325 22699590365312 run.py:483] Algo bellman_ford step 5810 current loss 0.281993, current_train_items 185952.
I0302 19:01:09.762265 22699590365312 run.py:483] Algo bellman_ford step 5811 current loss 0.412450, current_train_items 185984.
I0302 19:01:09.786088 22699590365312 run.py:483] Algo bellman_ford step 5812 current loss 0.585145, current_train_items 186016.
I0302 19:01:09.817483 22699590365312 run.py:483] Algo bellman_ford step 5813 current loss 0.714822, current_train_items 186048.
I0302 19:01:09.851964 22699590365312 run.py:483] Algo bellman_ford step 5814 current loss 0.827104, current_train_items 186080.
I0302 19:01:09.871573 22699590365312 run.py:483] Algo bellman_ford step 5815 current loss 0.294215, current_train_items 186112.
I0302 19:01:09.887926 22699590365312 run.py:483] Algo bellman_ford step 5816 current loss 0.437843, current_train_items 186144.
I0302 19:01:09.911332 22699590365312 run.py:483] Algo bellman_ford step 5817 current loss 0.599860, current_train_items 186176.
I0302 19:01:09.941263 22699590365312 run.py:483] Algo bellman_ford step 5818 current loss 0.582960, current_train_items 186208.
I0302 19:01:09.975336 22699590365312 run.py:483] Algo bellman_ford step 5819 current loss 0.839903, current_train_items 186240.
I0302 19:01:09.994683 22699590365312 run.py:483] Algo bellman_ford step 5820 current loss 0.303671, current_train_items 186272.
I0302 19:01:10.010693 22699590365312 run.py:483] Algo bellman_ford step 5821 current loss 0.410987, current_train_items 186304.
I0302 19:01:10.034409 22699590365312 run.py:483] Algo bellman_ford step 5822 current loss 0.618677, current_train_items 186336.
I0302 19:01:10.065262 22699590365312 run.py:483] Algo bellman_ford step 5823 current loss 0.639286, current_train_items 186368.
I0302 19:01:10.098244 22699590365312 run.py:483] Algo bellman_ford step 5824 current loss 0.783368, current_train_items 186400.
I0302 19:01:10.117783 22699590365312 run.py:483] Algo bellman_ford step 5825 current loss 0.305065, current_train_items 186432.
I0302 19:01:10.134076 22699590365312 run.py:483] Algo bellman_ford step 5826 current loss 0.412108, current_train_items 186464.
I0302 19:01:10.158263 22699590365312 run.py:483] Algo bellman_ford step 5827 current loss 0.604878, current_train_items 186496.
I0302 19:01:10.189812 22699590365312 run.py:483] Algo bellman_ford step 5828 current loss 0.644739, current_train_items 186528.
I0302 19:01:10.224231 22699590365312 run.py:483] Algo bellman_ford step 5829 current loss 0.905258, current_train_items 186560.
I0302 19:01:10.244109 22699590365312 run.py:483] Algo bellman_ford step 5830 current loss 0.275342, current_train_items 186592.
I0302 19:01:10.260477 22699590365312 run.py:483] Algo bellman_ford step 5831 current loss 0.508432, current_train_items 186624.
I0302 19:01:10.283492 22699590365312 run.py:483] Algo bellman_ford step 5832 current loss 0.522592, current_train_items 186656.
I0302 19:01:10.314284 22699590365312 run.py:483] Algo bellman_ford step 5833 current loss 0.596465, current_train_items 186688.
I0302 19:01:10.344677 22699590365312 run.py:483] Algo bellman_ford step 5834 current loss 0.589073, current_train_items 186720.
I0302 19:01:10.364452 22699590365312 run.py:483] Algo bellman_ford step 5835 current loss 0.275916, current_train_items 186752.
I0302 19:01:10.379932 22699590365312 run.py:483] Algo bellman_ford step 5836 current loss 0.438090, current_train_items 186784.
I0302 19:01:10.404758 22699590365312 run.py:483] Algo bellman_ford step 5837 current loss 0.644423, current_train_items 186816.
I0302 19:01:10.434049 22699590365312 run.py:483] Algo bellman_ford step 5838 current loss 0.499526, current_train_items 186848.
I0302 19:01:10.468927 22699590365312 run.py:483] Algo bellman_ford step 5839 current loss 0.824359, current_train_items 186880.
I0302 19:01:10.488625 22699590365312 run.py:483] Algo bellman_ford step 5840 current loss 0.306589, current_train_items 186912.
I0302 19:01:10.505040 22699590365312 run.py:483] Algo bellman_ford step 5841 current loss 0.464876, current_train_items 186944.
I0302 19:01:10.527858 22699590365312 run.py:483] Algo bellman_ford step 5842 current loss 0.614287, current_train_items 186976.
I0302 19:01:10.557437 22699590365312 run.py:483] Algo bellman_ford step 5843 current loss 0.584058, current_train_items 187008.
I0302 19:01:10.590319 22699590365312 run.py:483] Algo bellman_ford step 5844 current loss 0.690906, current_train_items 187040.
I0302 19:01:10.610125 22699590365312 run.py:483] Algo bellman_ford step 5845 current loss 0.285970, current_train_items 187072.
I0302 19:01:10.626444 22699590365312 run.py:483] Algo bellman_ford step 5846 current loss 0.434373, current_train_items 187104.
I0302 19:01:10.650933 22699590365312 run.py:483] Algo bellman_ford step 5847 current loss 0.620719, current_train_items 187136.
I0302 19:01:10.680435 22699590365312 run.py:483] Algo bellman_ford step 5848 current loss 0.629935, current_train_items 187168.
I0302 19:01:10.714140 22699590365312 run.py:483] Algo bellman_ford step 5849 current loss 0.705932, current_train_items 187200.
I0302 19:01:10.733677 22699590365312 run.py:483] Algo bellman_ford step 5850 current loss 0.269487, current_train_items 187232.
I0302 19:01:10.741889 22699590365312 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0302 19:01:10.741993 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.944, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:01:10.758966 22699590365312 run.py:483] Algo bellman_ford step 5851 current loss 0.508918, current_train_items 187264.
I0302 19:01:10.782379 22699590365312 run.py:483] Algo bellman_ford step 5852 current loss 0.756960, current_train_items 187296.
I0302 19:01:10.812631 22699590365312 run.py:483] Algo bellman_ford step 5853 current loss 0.612878, current_train_items 187328.
I0302 19:01:10.846623 22699590365312 run.py:483] Algo bellman_ford step 5854 current loss 0.796057, current_train_items 187360.
I0302 19:01:10.866425 22699590365312 run.py:483] Algo bellman_ford step 5855 current loss 0.339807, current_train_items 187392.
I0302 19:01:10.882387 22699590365312 run.py:483] Algo bellman_ford step 5856 current loss 0.530506, current_train_items 187424.
I0302 19:01:10.906374 22699590365312 run.py:483] Algo bellman_ford step 5857 current loss 0.574161, current_train_items 187456.
I0302 19:01:10.936406 22699590365312 run.py:483] Algo bellman_ford step 5858 current loss 0.615555, current_train_items 187488.
I0302 19:01:10.969925 22699590365312 run.py:483] Algo bellman_ford step 5859 current loss 0.709217, current_train_items 187520.
I0302 19:01:10.989952 22699590365312 run.py:483] Algo bellman_ford step 5860 current loss 0.300039, current_train_items 187552.
I0302 19:01:11.006323 22699590365312 run.py:483] Algo bellman_ford step 5861 current loss 0.416152, current_train_items 187584.
I0302 19:01:11.028568 22699590365312 run.py:483] Algo bellman_ford step 5862 current loss 0.669138, current_train_items 187616.
I0302 19:01:11.059963 22699590365312 run.py:483] Algo bellman_ford step 5863 current loss 0.706943, current_train_items 187648.
I0302 19:01:11.094128 22699590365312 run.py:483] Algo bellman_ford step 5864 current loss 0.888603, current_train_items 187680.
I0302 19:01:11.113693 22699590365312 run.py:483] Algo bellman_ford step 5865 current loss 0.325011, current_train_items 187712.
I0302 19:01:11.130374 22699590365312 run.py:483] Algo bellman_ford step 5866 current loss 0.472078, current_train_items 187744.
I0302 19:01:11.153832 22699590365312 run.py:483] Algo bellman_ford step 5867 current loss 0.563144, current_train_items 187776.
I0302 19:01:11.184052 22699590365312 run.py:483] Algo bellman_ford step 5868 current loss 0.723524, current_train_items 187808.
I0302 19:01:11.219121 22699590365312 run.py:483] Algo bellman_ford step 5869 current loss 0.769340, current_train_items 187840.
I0302 19:01:11.239291 22699590365312 run.py:483] Algo bellman_ford step 5870 current loss 0.274122, current_train_items 187872.
I0302 19:01:11.255622 22699590365312 run.py:483] Algo bellman_ford step 5871 current loss 0.358932, current_train_items 187904.
I0302 19:01:11.277807 22699590365312 run.py:483] Algo bellman_ford step 5872 current loss 0.559013, current_train_items 187936.
I0302 19:01:11.308089 22699590365312 run.py:483] Algo bellman_ford step 5873 current loss 0.588046, current_train_items 187968.
I0302 19:01:11.341677 22699590365312 run.py:483] Algo bellman_ford step 5874 current loss 0.696735, current_train_items 188000.
I0302 19:01:11.361493 22699590365312 run.py:483] Algo bellman_ford step 5875 current loss 0.317200, current_train_items 188032.
I0302 19:01:11.377584 22699590365312 run.py:483] Algo bellman_ford step 5876 current loss 0.459968, current_train_items 188064.
I0302 19:01:11.400104 22699590365312 run.py:483] Algo bellman_ford step 5877 current loss 0.562664, current_train_items 188096.
I0302 19:01:11.430349 22699590365312 run.py:483] Algo bellman_ford step 5878 current loss 0.658390, current_train_items 188128.
I0302 19:01:11.464771 22699590365312 run.py:483] Algo bellman_ford step 5879 current loss 0.769012, current_train_items 188160.
I0302 19:01:11.484558 22699590365312 run.py:483] Algo bellman_ford step 5880 current loss 0.272669, current_train_items 188192.
I0302 19:01:11.500467 22699590365312 run.py:483] Algo bellman_ford step 5881 current loss 0.502772, current_train_items 188224.
I0302 19:01:11.524412 22699590365312 run.py:483] Algo bellman_ford step 5882 current loss 0.551300, current_train_items 188256.
I0302 19:01:11.555127 22699590365312 run.py:483] Algo bellman_ford step 5883 current loss 0.571645, current_train_items 188288.
I0302 19:01:11.589726 22699590365312 run.py:483] Algo bellman_ford step 5884 current loss 0.714516, current_train_items 188320.
I0302 19:01:11.610065 22699590365312 run.py:483] Algo bellman_ford step 5885 current loss 0.315389, current_train_items 188352.
I0302 19:01:11.626080 22699590365312 run.py:483] Algo bellman_ford step 5886 current loss 0.446297, current_train_items 188384.
I0302 19:01:11.649188 22699590365312 run.py:483] Algo bellman_ford step 5887 current loss 0.557191, current_train_items 188416.
I0302 19:01:11.679347 22699590365312 run.py:483] Algo bellman_ford step 5888 current loss 0.634190, current_train_items 188448.
I0302 19:01:11.711482 22699590365312 run.py:483] Algo bellman_ford step 5889 current loss 0.782600, current_train_items 188480.
I0302 19:01:11.731416 22699590365312 run.py:483] Algo bellman_ford step 5890 current loss 0.326911, current_train_items 188512.
I0302 19:01:11.747607 22699590365312 run.py:483] Algo bellman_ford step 5891 current loss 0.492621, current_train_items 188544.
I0302 19:01:11.770697 22699590365312 run.py:483] Algo bellman_ford step 5892 current loss 0.554304, current_train_items 188576.
I0302 19:01:11.800324 22699590365312 run.py:483] Algo bellman_ford step 5893 current loss 0.544300, current_train_items 188608.
I0302 19:01:11.831722 22699590365312 run.py:483] Algo bellman_ford step 5894 current loss 0.661268, current_train_items 188640.
I0302 19:01:11.851322 22699590365312 run.py:483] Algo bellman_ford step 5895 current loss 0.261479, current_train_items 188672.
I0302 19:01:11.867597 22699590365312 run.py:483] Algo bellman_ford step 5896 current loss 0.469820, current_train_items 188704.
I0302 19:01:11.891469 22699590365312 run.py:483] Algo bellman_ford step 5897 current loss 0.593832, current_train_items 188736.
I0302 19:01:11.922826 22699590365312 run.py:483] Algo bellman_ford step 5898 current loss 0.658657, current_train_items 188768.
I0302 19:01:11.955891 22699590365312 run.py:483] Algo bellman_ford step 5899 current loss 0.687528, current_train_items 188800.
I0302 19:01:11.975794 22699590365312 run.py:483] Algo bellman_ford step 5900 current loss 0.269934, current_train_items 188832.
I0302 19:01:11.983723 22699590365312 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0302 19:01:11.983830 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.944, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:01:12.000395 22699590365312 run.py:483] Algo bellman_ford step 5901 current loss 0.416427, current_train_items 188864.
I0302 19:01:12.024627 22699590365312 run.py:483] Algo bellman_ford step 5902 current loss 0.561473, current_train_items 188896.
I0302 19:01:12.056029 22699590365312 run.py:483] Algo bellman_ford step 5903 current loss 0.659327, current_train_items 188928.
I0302 19:01:12.087931 22699590365312 run.py:483] Algo bellman_ford step 5904 current loss 0.745387, current_train_items 188960.
I0302 19:01:12.107827 22699590365312 run.py:483] Algo bellman_ford step 5905 current loss 0.315880, current_train_items 188992.
I0302 19:01:12.124078 22699590365312 run.py:483] Algo bellman_ford step 5906 current loss 0.483466, current_train_items 189024.
I0302 19:01:12.147099 22699590365312 run.py:483] Algo bellman_ford step 5907 current loss 0.617795, current_train_items 189056.
I0302 19:01:12.177782 22699590365312 run.py:483] Algo bellman_ford step 5908 current loss 0.566583, current_train_items 189088.
I0302 19:01:12.210119 22699590365312 run.py:483] Algo bellman_ford step 5909 current loss 0.793596, current_train_items 189120.
I0302 19:01:12.229791 22699590365312 run.py:483] Algo bellman_ford step 5910 current loss 0.275203, current_train_items 189152.
I0302 19:01:12.246140 22699590365312 run.py:483] Algo bellman_ford step 5911 current loss 0.491605, current_train_items 189184.
I0302 19:01:12.269792 22699590365312 run.py:483] Algo bellman_ford step 5912 current loss 0.680206, current_train_items 189216.
I0302 19:01:12.299666 22699590365312 run.py:483] Algo bellman_ford step 5913 current loss 0.682226, current_train_items 189248.
I0302 19:01:12.332058 22699590365312 run.py:483] Algo bellman_ford step 5914 current loss 0.792354, current_train_items 189280.
I0302 19:01:12.351778 22699590365312 run.py:483] Algo bellman_ford step 5915 current loss 0.276963, current_train_items 189312.
I0302 19:01:12.368067 22699590365312 run.py:483] Algo bellman_ford step 5916 current loss 0.442042, current_train_items 189344.
I0302 19:01:12.391479 22699590365312 run.py:483] Algo bellman_ford step 5917 current loss 0.606451, current_train_items 189376.
I0302 19:01:12.421670 22699590365312 run.py:483] Algo bellman_ford step 5918 current loss 0.690049, current_train_items 189408.
I0302 19:01:12.455740 22699590365312 run.py:483] Algo bellman_ford step 5919 current loss 0.734992, current_train_items 189440.
I0302 19:01:12.475403 22699590365312 run.py:483] Algo bellman_ford step 5920 current loss 0.334149, current_train_items 189472.
I0302 19:01:12.491513 22699590365312 run.py:483] Algo bellman_ford step 5921 current loss 0.472579, current_train_items 189504.
I0302 19:01:12.514275 22699590365312 run.py:483] Algo bellman_ford step 5922 current loss 0.595157, current_train_items 189536.
I0302 19:01:12.543617 22699590365312 run.py:483] Algo bellman_ford step 5923 current loss 0.607714, current_train_items 189568.
I0302 19:01:12.576598 22699590365312 run.py:483] Algo bellman_ford step 5924 current loss 0.762708, current_train_items 189600.
I0302 19:01:12.596107 22699590365312 run.py:483] Algo bellman_ford step 5925 current loss 0.260521, current_train_items 189632.
I0302 19:01:12.611932 22699590365312 run.py:483] Algo bellman_ford step 5926 current loss 0.529671, current_train_items 189664.
I0302 19:01:12.634650 22699590365312 run.py:483] Algo bellman_ford step 5927 current loss 0.569363, current_train_items 189696.
I0302 19:01:12.665813 22699590365312 run.py:483] Algo bellman_ford step 5928 current loss 0.659708, current_train_items 189728.
I0302 19:01:12.698012 22699590365312 run.py:483] Algo bellman_ford step 5929 current loss 0.680996, current_train_items 189760.
I0302 19:01:12.717622 22699590365312 run.py:483] Algo bellman_ford step 5930 current loss 0.327819, current_train_items 189792.
I0302 19:01:12.733777 22699590365312 run.py:483] Algo bellman_ford step 5931 current loss 0.443227, current_train_items 189824.
I0302 19:01:12.756919 22699590365312 run.py:483] Algo bellman_ford step 5932 current loss 0.562208, current_train_items 189856.
I0302 19:01:12.786658 22699590365312 run.py:483] Algo bellman_ford step 5933 current loss 0.578843, current_train_items 189888.
I0302 19:01:12.821008 22699590365312 run.py:483] Algo bellman_ford step 5934 current loss 0.810704, current_train_items 189920.
I0302 19:01:12.840832 22699590365312 run.py:483] Algo bellman_ford step 5935 current loss 0.317243, current_train_items 189952.
I0302 19:01:12.856595 22699590365312 run.py:483] Algo bellman_ford step 5936 current loss 0.479360, current_train_items 189984.
I0302 19:01:12.879816 22699590365312 run.py:483] Algo bellman_ford step 5937 current loss 0.599241, current_train_items 190016.
I0302 19:01:12.911204 22699590365312 run.py:483] Algo bellman_ford step 5938 current loss 0.628154, current_train_items 190048.
I0302 19:01:12.943179 22699590365312 run.py:483] Algo bellman_ford step 5939 current loss 0.722268, current_train_items 190080.
I0302 19:01:12.963136 22699590365312 run.py:483] Algo bellman_ford step 5940 current loss 0.289026, current_train_items 190112.
I0302 19:01:12.978968 22699590365312 run.py:483] Algo bellman_ford step 5941 current loss 0.426603, current_train_items 190144.
I0302 19:01:13.002452 22699590365312 run.py:483] Algo bellman_ford step 5942 current loss 0.587431, current_train_items 190176.
I0302 19:01:13.033580 22699590365312 run.py:483] Algo bellman_ford step 5943 current loss 0.610263, current_train_items 190208.
I0302 19:01:13.067528 22699590365312 run.py:483] Algo bellman_ford step 5944 current loss 0.703686, current_train_items 190240.
I0302 19:01:13.087253 22699590365312 run.py:483] Algo bellman_ford step 5945 current loss 0.223486, current_train_items 190272.
I0302 19:01:13.103639 22699590365312 run.py:483] Algo bellman_ford step 5946 current loss 0.491036, current_train_items 190304.
I0302 19:01:13.127227 22699590365312 run.py:483] Algo bellman_ford step 5947 current loss 0.628714, current_train_items 190336.
I0302 19:01:13.156696 22699590365312 run.py:483] Algo bellman_ford step 5948 current loss 0.591399, current_train_items 190368.
I0302 19:01:13.190220 22699590365312 run.py:483] Algo bellman_ford step 5949 current loss 0.726766, current_train_items 190400.
I0302 19:01:13.209665 22699590365312 run.py:483] Algo bellman_ford step 5950 current loss 0.268182, current_train_items 190432.
I0302 19:01:13.217787 22699590365312 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0302 19:01:13.217894 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.944, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:01:13.234671 22699590365312 run.py:483] Algo bellman_ford step 5951 current loss 0.412015, current_train_items 190464.
I0302 19:01:13.258519 22699590365312 run.py:483] Algo bellman_ford step 5952 current loss 0.599292, current_train_items 190496.
I0302 19:01:13.289669 22699590365312 run.py:483] Algo bellman_ford step 5953 current loss 0.629659, current_train_items 190528.
I0302 19:01:13.323527 22699590365312 run.py:483] Algo bellman_ford step 5954 current loss 0.667787, current_train_items 190560.
I0302 19:01:13.344077 22699590365312 run.py:483] Algo bellman_ford step 5955 current loss 0.281078, current_train_items 190592.
I0302 19:01:13.360279 22699590365312 run.py:483] Algo bellman_ford step 5956 current loss 0.478209, current_train_items 190624.
I0302 19:01:13.382917 22699590365312 run.py:483] Algo bellman_ford step 5957 current loss 0.542935, current_train_items 190656.
I0302 19:01:13.413014 22699590365312 run.py:483] Algo bellman_ford step 5958 current loss 0.570707, current_train_items 190688.
I0302 19:01:13.446365 22699590365312 run.py:483] Algo bellman_ford step 5959 current loss 0.645404, current_train_items 190720.
I0302 19:01:13.466405 22699590365312 run.py:483] Algo bellman_ford step 5960 current loss 0.220818, current_train_items 190752.
I0302 19:01:13.482517 22699590365312 run.py:483] Algo bellman_ford step 5961 current loss 0.484679, current_train_items 190784.
I0302 19:01:13.506266 22699590365312 run.py:483] Algo bellman_ford step 5962 current loss 0.531891, current_train_items 190816.
I0302 19:01:13.535441 22699590365312 run.py:483] Algo bellman_ford step 5963 current loss 0.571754, current_train_items 190848.
I0302 19:01:13.568890 22699590365312 run.py:483] Algo bellman_ford step 5964 current loss 0.746954, current_train_items 190880.
I0302 19:01:13.588489 22699590365312 run.py:483] Algo bellman_ford step 5965 current loss 0.326726, current_train_items 190912.
I0302 19:01:13.604380 22699590365312 run.py:483] Algo bellman_ford step 5966 current loss 0.482970, current_train_items 190944.
I0302 19:01:13.628284 22699590365312 run.py:483] Algo bellman_ford step 5967 current loss 0.555925, current_train_items 190976.
I0302 19:01:13.658472 22699590365312 run.py:483] Algo bellman_ford step 5968 current loss 0.588601, current_train_items 191008.
I0302 19:01:13.693031 22699590365312 run.py:483] Algo bellman_ford step 5969 current loss 0.712122, current_train_items 191040.
I0302 19:01:13.713026 22699590365312 run.py:483] Algo bellman_ford step 5970 current loss 0.312695, current_train_items 191072.
I0302 19:01:13.729323 22699590365312 run.py:483] Algo bellman_ford step 5971 current loss 0.473568, current_train_items 191104.
I0302 19:01:13.752562 22699590365312 run.py:483] Algo bellman_ford step 5972 current loss 0.492680, current_train_items 191136.
I0302 19:01:13.784540 22699590365312 run.py:483] Algo bellman_ford step 5973 current loss 0.625055, current_train_items 191168.
I0302 19:01:13.817938 22699590365312 run.py:483] Algo bellman_ford step 5974 current loss 0.689031, current_train_items 191200.
I0302 19:01:13.838225 22699590365312 run.py:483] Algo bellman_ford step 5975 current loss 0.331813, current_train_items 191232.
I0302 19:01:13.854325 22699590365312 run.py:483] Algo bellman_ford step 5976 current loss 0.420345, current_train_items 191264.
I0302 19:01:13.877232 22699590365312 run.py:483] Algo bellman_ford step 5977 current loss 0.561200, current_train_items 191296.
I0302 19:01:13.908182 22699590365312 run.py:483] Algo bellman_ford step 5978 current loss 0.660236, current_train_items 191328.
I0302 19:01:13.940261 22699590365312 run.py:483] Algo bellman_ford step 5979 current loss 0.619362, current_train_items 191360.
I0302 19:01:13.959634 22699590365312 run.py:483] Algo bellman_ford step 5980 current loss 0.266204, current_train_items 191392.
I0302 19:01:13.976654 22699590365312 run.py:483] Algo bellman_ford step 5981 current loss 0.500757, current_train_items 191424.
I0302 19:01:13.999357 22699590365312 run.py:483] Algo bellman_ford step 5982 current loss 0.544397, current_train_items 191456.
I0302 19:01:14.029503 22699590365312 run.py:483] Algo bellman_ford step 5983 current loss 0.590344, current_train_items 191488.
I0302 19:01:14.062060 22699590365312 run.py:483] Algo bellman_ford step 5984 current loss 0.797508, current_train_items 191520.
I0302 19:01:14.081683 22699590365312 run.py:483] Algo bellman_ford step 5985 current loss 0.296997, current_train_items 191552.
I0302 19:01:14.098679 22699590365312 run.py:483] Algo bellman_ford step 5986 current loss 0.405457, current_train_items 191584.
I0302 19:01:14.122021 22699590365312 run.py:483] Algo bellman_ford step 5987 current loss 0.594069, current_train_items 191616.
I0302 19:01:14.151545 22699590365312 run.py:483] Algo bellman_ford step 5988 current loss 0.736582, current_train_items 191648.
I0302 19:01:14.184513 22699590365312 run.py:483] Algo bellman_ford step 5989 current loss 0.808116, current_train_items 191680.
I0302 19:01:14.204516 22699590365312 run.py:483] Algo bellman_ford step 5990 current loss 0.264598, current_train_items 191712.
I0302 19:01:14.220201 22699590365312 run.py:483] Algo bellman_ford step 5991 current loss 0.423713, current_train_items 191744.
I0302 19:01:14.243587 22699590365312 run.py:483] Algo bellman_ford step 5992 current loss 0.659612, current_train_items 191776.
I0302 19:01:14.275985 22699590365312 run.py:483] Algo bellman_ford step 5993 current loss 0.681809, current_train_items 191808.
I0302 19:01:14.309289 22699590365312 run.py:483] Algo bellman_ford step 5994 current loss 0.735355, current_train_items 191840.
I0302 19:01:14.328760 22699590365312 run.py:483] Algo bellman_ford step 5995 current loss 0.305099, current_train_items 191872.
I0302 19:01:14.344954 22699590365312 run.py:483] Algo bellman_ford step 5996 current loss 0.430334, current_train_items 191904.
I0302 19:01:14.368611 22699590365312 run.py:483] Algo bellman_ford step 5997 current loss 0.631928, current_train_items 191936.
I0302 19:01:14.398746 22699590365312 run.py:483] Algo bellman_ford step 5998 current loss 0.675455, current_train_items 191968.
I0302 19:01:14.430922 22699590365312 run.py:483] Algo bellman_ford step 5999 current loss 0.656495, current_train_items 192000.
I0302 19:01:14.451085 22699590365312 run.py:483] Algo bellman_ford step 6000 current loss 0.250515, current_train_items 192032.
I0302 19:01:14.459134 22699590365312 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0302 19:01:14.459249 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.944, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:01:14.476185 22699590365312 run.py:483] Algo bellman_ford step 6001 current loss 0.448639, current_train_items 192064.
I0302 19:01:14.500030 22699590365312 run.py:483] Algo bellman_ford step 6002 current loss 0.651352, current_train_items 192096.
I0302 19:01:14.529770 22699590365312 run.py:483] Algo bellman_ford step 6003 current loss 0.600986, current_train_items 192128.
I0302 19:01:14.562617 22699590365312 run.py:483] Algo bellman_ford step 6004 current loss 0.755139, current_train_items 192160.
I0302 19:01:14.582859 22699590365312 run.py:483] Algo bellman_ford step 6005 current loss 0.244705, current_train_items 192192.
I0302 19:01:14.598620 22699590365312 run.py:483] Algo bellman_ford step 6006 current loss 0.421274, current_train_items 192224.
I0302 19:01:14.622863 22699590365312 run.py:483] Algo bellman_ford step 6007 current loss 0.618887, current_train_items 192256.
I0302 19:01:14.655447 22699590365312 run.py:483] Algo bellman_ford step 6008 current loss 0.716997, current_train_items 192288.
I0302 19:01:14.690467 22699590365312 run.py:483] Algo bellman_ford step 6009 current loss 0.744424, current_train_items 192320.
I0302 19:01:14.710125 22699590365312 run.py:483] Algo bellman_ford step 6010 current loss 0.268843, current_train_items 192352.
I0302 19:01:14.726249 22699590365312 run.py:483] Algo bellman_ford step 6011 current loss 0.443648, current_train_items 192384.
I0302 19:01:14.749605 22699590365312 run.py:483] Algo bellman_ford step 6012 current loss 0.572204, current_train_items 192416.
I0302 19:01:14.779885 22699590365312 run.py:483] Algo bellman_ford step 6013 current loss 0.581822, current_train_items 192448.
I0302 19:01:14.815289 22699590365312 run.py:483] Algo bellman_ford step 6014 current loss 0.704024, current_train_items 192480.
I0302 19:01:14.834809 22699590365312 run.py:483] Algo bellman_ford step 6015 current loss 0.289021, current_train_items 192512.
I0302 19:01:14.851042 22699590365312 run.py:483] Algo bellman_ford step 6016 current loss 0.452288, current_train_items 192544.
I0302 19:01:14.874197 22699590365312 run.py:483] Algo bellman_ford step 6017 current loss 0.546567, current_train_items 192576.
I0302 19:01:14.905366 22699590365312 run.py:483] Algo bellman_ford step 6018 current loss 0.685334, current_train_items 192608.
I0302 19:01:14.938567 22699590365312 run.py:483] Algo bellman_ford step 6019 current loss 0.765981, current_train_items 192640.
I0302 19:01:14.958078 22699590365312 run.py:483] Algo bellman_ford step 6020 current loss 0.303799, current_train_items 192672.
I0302 19:01:14.974509 22699590365312 run.py:483] Algo bellman_ford step 6021 current loss 0.403697, current_train_items 192704.
I0302 19:01:14.998112 22699590365312 run.py:483] Algo bellman_ford step 6022 current loss 0.531374, current_train_items 192736.
I0302 19:01:15.030824 22699590365312 run.py:483] Algo bellman_ford step 6023 current loss 0.722904, current_train_items 192768.
I0302 19:01:15.064491 22699590365312 run.py:483] Algo bellman_ford step 6024 current loss 0.712412, current_train_items 192800.
I0302 19:01:15.083995 22699590365312 run.py:483] Algo bellman_ford step 6025 current loss 0.282361, current_train_items 192832.
I0302 19:01:15.100389 22699590365312 run.py:483] Algo bellman_ford step 6026 current loss 0.581190, current_train_items 192864.
I0302 19:01:15.123935 22699590365312 run.py:483] Algo bellman_ford step 6027 current loss 0.567063, current_train_items 192896.
I0302 19:01:15.154643 22699590365312 run.py:483] Algo bellman_ford step 6028 current loss 0.697753, current_train_items 192928.
I0302 19:01:15.188020 22699590365312 run.py:483] Algo bellman_ford step 6029 current loss 0.844385, current_train_items 192960.
I0302 19:01:15.207277 22699590365312 run.py:483] Algo bellman_ford step 6030 current loss 0.286500, current_train_items 192992.
I0302 19:01:15.223122 22699590365312 run.py:483] Algo bellman_ford step 6031 current loss 0.421003, current_train_items 193024.
I0302 19:01:15.246313 22699590365312 run.py:483] Algo bellman_ford step 6032 current loss 0.493628, current_train_items 193056.
I0302 19:01:15.278679 22699590365312 run.py:483] Algo bellman_ford step 6033 current loss 0.712248, current_train_items 193088.
I0302 19:01:15.311955 22699590365312 run.py:483] Algo bellman_ford step 6034 current loss 0.682526, current_train_items 193120.
I0302 19:01:15.331465 22699590365312 run.py:483] Algo bellman_ford step 6035 current loss 0.252920, current_train_items 193152.
I0302 19:01:15.347458 22699590365312 run.py:483] Algo bellman_ford step 6036 current loss 0.444182, current_train_items 193184.
I0302 19:01:15.370856 22699590365312 run.py:483] Algo bellman_ford step 6037 current loss 0.618305, current_train_items 193216.
I0302 19:01:15.402369 22699590365312 run.py:483] Algo bellman_ford step 6038 current loss 0.840709, current_train_items 193248.
I0302 19:01:15.434579 22699590365312 run.py:483] Algo bellman_ford step 6039 current loss 0.775951, current_train_items 193280.
I0302 19:01:15.453770 22699590365312 run.py:483] Algo bellman_ford step 6040 current loss 0.360954, current_train_items 193312.
I0302 19:01:15.469980 22699590365312 run.py:483] Algo bellman_ford step 6041 current loss 0.448628, current_train_items 193344.
I0302 19:01:15.494125 22699590365312 run.py:483] Algo bellman_ford step 6042 current loss 0.634038, current_train_items 193376.
I0302 19:01:15.524876 22699590365312 run.py:483] Algo bellman_ford step 6043 current loss 0.691308, current_train_items 193408.
I0302 19:01:15.560599 22699590365312 run.py:483] Algo bellman_ford step 6044 current loss 0.887080, current_train_items 193440.
I0302 19:01:15.580200 22699590365312 run.py:483] Algo bellman_ford step 6045 current loss 0.318152, current_train_items 193472.
I0302 19:01:15.596625 22699590365312 run.py:483] Algo bellman_ford step 6046 current loss 0.485532, current_train_items 193504.
I0302 19:01:15.619715 22699590365312 run.py:483] Algo bellman_ford step 6047 current loss 0.651465, current_train_items 193536.
I0302 19:01:15.651652 22699590365312 run.py:483] Algo bellman_ford step 6048 current loss 0.747767, current_train_items 193568.
I0302 19:01:15.686006 22699590365312 run.py:483] Algo bellman_ford step 6049 current loss 0.764163, current_train_items 193600.
I0302 19:01:15.705402 22699590365312 run.py:483] Algo bellman_ford step 6050 current loss 0.258125, current_train_items 193632.
I0302 19:01:15.713571 22699590365312 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.9453125, 'score': 0.9453125, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0302 19:01:15.713703 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.944, current avg val score is 0.945, val scores are: bellman_ford: 0.945
I0302 19:01:15.743017 22699590365312 run.py:483] Algo bellman_ford step 6051 current loss 0.427220, current_train_items 193664.
I0302 19:01:15.767497 22699590365312 run.py:483] Algo bellman_ford step 6052 current loss 0.576171, current_train_items 193696.
I0302 19:01:15.798786 22699590365312 run.py:483] Algo bellman_ford step 6053 current loss 0.643800, current_train_items 193728.
I0302 19:01:15.833889 22699590365312 run.py:483] Algo bellman_ford step 6054 current loss 0.734394, current_train_items 193760.
I0302 19:01:15.854314 22699590365312 run.py:483] Algo bellman_ford step 6055 current loss 0.350726, current_train_items 193792.
I0302 19:01:15.870024 22699590365312 run.py:483] Algo bellman_ford step 6056 current loss 0.520763, current_train_items 193824.
I0302 19:01:15.892924 22699590365312 run.py:483] Algo bellman_ford step 6057 current loss 0.725734, current_train_items 193856.
I0302 19:01:15.923531 22699590365312 run.py:483] Algo bellman_ford step 6058 current loss 0.674859, current_train_items 193888.
I0302 19:01:15.954533 22699590365312 run.py:483] Algo bellman_ford step 6059 current loss 0.658950, current_train_items 193920.
I0302 19:01:15.974795 22699590365312 run.py:483] Algo bellman_ford step 6060 current loss 0.276545, current_train_items 193952.
I0302 19:01:15.991213 22699590365312 run.py:483] Algo bellman_ford step 6061 current loss 0.432020, current_train_items 193984.
I0302 19:01:16.013178 22699590365312 run.py:483] Algo bellman_ford step 6062 current loss 0.500906, current_train_items 194016.
I0302 19:01:16.042855 22699590365312 run.py:483] Algo bellman_ford step 6063 current loss 0.650211, current_train_items 194048.
I0302 19:01:16.076691 22699590365312 run.py:483] Algo bellman_ford step 6064 current loss 0.768509, current_train_items 194080.
I0302 19:01:16.096364 22699590365312 run.py:483] Algo bellman_ford step 6065 current loss 0.270659, current_train_items 194112.
I0302 19:01:16.112454 22699590365312 run.py:483] Algo bellman_ford step 6066 current loss 0.412368, current_train_items 194144.
I0302 19:01:16.136423 22699590365312 run.py:483] Algo bellman_ford step 6067 current loss 0.591992, current_train_items 194176.
I0302 19:01:16.167809 22699590365312 run.py:483] Algo bellman_ford step 6068 current loss 0.668395, current_train_items 194208.
I0302 19:01:16.200552 22699590365312 run.py:483] Algo bellman_ford step 6069 current loss 0.685420, current_train_items 194240.
I0302 19:01:16.220649 22699590365312 run.py:483] Algo bellman_ford step 6070 current loss 0.323501, current_train_items 194272.
I0302 19:01:16.236680 22699590365312 run.py:483] Algo bellman_ford step 6071 current loss 0.503267, current_train_items 194304.
I0302 19:01:16.260283 22699590365312 run.py:483] Algo bellman_ford step 6072 current loss 0.620805, current_train_items 194336.
I0302 19:01:16.290167 22699590365312 run.py:483] Algo bellman_ford step 6073 current loss 0.661776, current_train_items 194368.
I0302 19:01:16.323828 22699590365312 run.py:483] Algo bellman_ford step 6074 current loss 0.887282, current_train_items 194400.
I0302 19:01:16.343818 22699590365312 run.py:483] Algo bellman_ford step 6075 current loss 0.305560, current_train_items 194432.
I0302 19:01:16.360147 22699590365312 run.py:483] Algo bellman_ford step 6076 current loss 0.445918, current_train_items 194464.
I0302 19:01:16.382400 22699590365312 run.py:483] Algo bellman_ford step 6077 current loss 0.571214, current_train_items 194496.
I0302 19:01:16.412678 22699590365312 run.py:483] Algo bellman_ford step 6078 current loss 0.606005, current_train_items 194528.
I0302 19:01:16.446705 22699590365312 run.py:483] Algo bellman_ford step 6079 current loss 0.772655, current_train_items 194560.
I0302 19:01:16.466057 22699590365312 run.py:483] Algo bellman_ford step 6080 current loss 0.266952, current_train_items 194592.
I0302 19:01:16.482358 22699590365312 run.py:483] Algo bellman_ford step 6081 current loss 0.368860, current_train_items 194624.
I0302 19:01:16.505424 22699590365312 run.py:483] Algo bellman_ford step 6082 current loss 0.530716, current_train_items 194656.
I0302 19:01:16.537755 22699590365312 run.py:483] Algo bellman_ford step 6083 current loss 0.685466, current_train_items 194688.
I0302 19:01:16.569976 22699590365312 run.py:483] Algo bellman_ford step 6084 current loss 0.626508, current_train_items 194720.
I0302 19:01:16.589807 22699590365312 run.py:483] Algo bellman_ford step 6085 current loss 0.297785, current_train_items 194752.
I0302 19:01:16.606130 22699590365312 run.py:483] Algo bellman_ford step 6086 current loss 0.541317, current_train_items 194784.
I0302 19:01:16.628872 22699590365312 run.py:483] Algo bellman_ford step 6087 current loss 0.596437, current_train_items 194816.
I0302 19:01:16.658829 22699590365312 run.py:483] Algo bellman_ford step 6088 current loss 0.657063, current_train_items 194848.
I0302 19:01:16.692947 22699590365312 run.py:483] Algo bellman_ford step 6089 current loss 0.780913, current_train_items 194880.
I0302 19:01:16.712883 22699590365312 run.py:483] Algo bellman_ford step 6090 current loss 0.316064, current_train_items 194912.
I0302 19:01:16.729216 22699590365312 run.py:483] Algo bellman_ford step 6091 current loss 0.463923, current_train_items 194944.
I0302 19:01:16.752255 22699590365312 run.py:483] Algo bellman_ford step 6092 current loss 0.529587, current_train_items 194976.
I0302 19:01:16.782538 22699590365312 run.py:483] Algo bellman_ford step 6093 current loss 0.605004, current_train_items 195008.
I0302 19:01:16.815124 22699590365312 run.py:483] Algo bellman_ford step 6094 current loss 0.859185, current_train_items 195040.
I0302 19:01:16.834702 22699590365312 run.py:483] Algo bellman_ford step 6095 current loss 0.264486, current_train_items 195072.
I0302 19:01:16.850846 22699590365312 run.py:483] Algo bellman_ford step 6096 current loss 0.412027, current_train_items 195104.
I0302 19:01:16.874536 22699590365312 run.py:483] Algo bellman_ford step 6097 current loss 0.597501, current_train_items 195136.
I0302 19:01:16.904145 22699590365312 run.py:483] Algo bellman_ford step 6098 current loss 0.583812, current_train_items 195168.
I0302 19:01:16.937784 22699590365312 run.py:483] Algo bellman_ford step 6099 current loss 0.819083, current_train_items 195200.
I0302 19:01:16.957683 22699590365312 run.py:483] Algo bellman_ford step 6100 current loss 0.190165, current_train_items 195232.
I0302 19:01:16.965627 22699590365312 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0302 19:01:16.965735 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:01:16.981963 22699590365312 run.py:483] Algo bellman_ford step 6101 current loss 0.422073, current_train_items 195264.
I0302 19:01:17.006634 22699590365312 run.py:483] Algo bellman_ford step 6102 current loss 0.571706, current_train_items 195296.
I0302 19:01:17.039177 22699590365312 run.py:483] Algo bellman_ford step 6103 current loss 0.773579, current_train_items 195328.
I0302 19:01:17.072316 22699590365312 run.py:483] Algo bellman_ford step 6104 current loss 0.681092, current_train_items 195360.
I0302 19:01:17.092278 22699590365312 run.py:483] Algo bellman_ford step 6105 current loss 0.300583, current_train_items 195392.
I0302 19:01:17.107674 22699590365312 run.py:483] Algo bellman_ford step 6106 current loss 0.481091, current_train_items 195424.
I0302 19:01:17.131080 22699590365312 run.py:483] Algo bellman_ford step 6107 current loss 0.596741, current_train_items 195456.
I0302 19:01:17.162016 22699590365312 run.py:483] Algo bellman_ford step 6108 current loss 0.578663, current_train_items 195488.
I0302 19:01:17.194875 22699590365312 run.py:483] Algo bellman_ford step 6109 current loss 0.758927, current_train_items 195520.
I0302 19:01:17.214424 22699590365312 run.py:483] Algo bellman_ford step 6110 current loss 0.295832, current_train_items 195552.
I0302 19:01:17.230321 22699590365312 run.py:483] Algo bellman_ford step 6111 current loss 0.383443, current_train_items 195584.
I0302 19:01:17.253808 22699590365312 run.py:483] Algo bellman_ford step 6112 current loss 0.571814, current_train_items 195616.
I0302 19:01:17.284026 22699590365312 run.py:483] Algo bellman_ford step 6113 current loss 0.587163, current_train_items 195648.
I0302 19:01:17.317347 22699590365312 run.py:483] Algo bellman_ford step 6114 current loss 0.653980, current_train_items 195680.
I0302 19:01:17.337215 22699590365312 run.py:483] Algo bellman_ford step 6115 current loss 0.243601, current_train_items 195712.
I0302 19:01:17.353461 22699590365312 run.py:483] Algo bellman_ford step 6116 current loss 0.424887, current_train_items 195744.
I0302 19:01:17.377207 22699590365312 run.py:483] Algo bellman_ford step 6117 current loss 0.475532, current_train_items 195776.
I0302 19:01:17.408933 22699590365312 run.py:483] Algo bellman_ford step 6118 current loss 0.696279, current_train_items 195808.
I0302 19:01:17.441824 22699590365312 run.py:483] Algo bellman_ford step 6119 current loss 0.767684, current_train_items 195840.
I0302 19:01:17.461601 22699590365312 run.py:483] Algo bellman_ford step 6120 current loss 0.391731, current_train_items 195872.
I0302 19:01:17.477907 22699590365312 run.py:483] Algo bellman_ford step 6121 current loss 0.443264, current_train_items 195904.
I0302 19:01:17.501085 22699590365312 run.py:483] Algo bellman_ford step 6122 current loss 0.517905, current_train_items 195936.
I0302 19:01:17.532963 22699590365312 run.py:483] Algo bellman_ford step 6123 current loss 0.616730, current_train_items 195968.
I0302 19:01:17.566420 22699590365312 run.py:483] Algo bellman_ford step 6124 current loss 0.713533, current_train_items 196000.
I0302 19:01:17.585985 22699590365312 run.py:483] Algo bellman_ford step 6125 current loss 0.247609, current_train_items 196032.
I0302 19:01:17.601715 22699590365312 run.py:483] Algo bellman_ford step 6126 current loss 0.483720, current_train_items 196064.
I0302 19:01:17.626788 22699590365312 run.py:483] Algo bellman_ford step 6127 current loss 0.668553, current_train_items 196096.
I0302 19:01:17.657733 22699590365312 run.py:483] Algo bellman_ford step 6128 current loss 0.556494, current_train_items 196128.
I0302 19:01:17.690521 22699590365312 run.py:483] Algo bellman_ford step 6129 current loss 0.652897, current_train_items 196160.
I0302 19:01:17.710181 22699590365312 run.py:483] Algo bellman_ford step 6130 current loss 0.324144, current_train_items 196192.
I0302 19:01:17.726435 22699590365312 run.py:483] Algo bellman_ford step 6131 current loss 0.400320, current_train_items 196224.
I0302 19:01:17.750760 22699590365312 run.py:483] Algo bellman_ford step 6132 current loss 0.663371, current_train_items 196256.
I0302 19:01:17.780510 22699590365312 run.py:483] Algo bellman_ford step 6133 current loss 0.621653, current_train_items 196288.
I0302 19:01:17.813242 22699590365312 run.py:483] Algo bellman_ford step 6134 current loss 0.650550, current_train_items 196320.
I0302 19:01:17.833033 22699590365312 run.py:483] Algo bellman_ford step 6135 current loss 0.265327, current_train_items 196352.
I0302 19:01:17.848649 22699590365312 run.py:483] Algo bellman_ford step 6136 current loss 0.440886, current_train_items 196384.
I0302 19:01:17.872351 22699590365312 run.py:483] Algo bellman_ford step 6137 current loss 0.652141, current_train_items 196416.
I0302 19:01:17.903979 22699590365312 run.py:483] Algo bellman_ford step 6138 current loss 0.771775, current_train_items 196448.
I0302 19:01:17.937616 22699590365312 run.py:483] Algo bellman_ford step 6139 current loss 0.832851, current_train_items 196480.
I0302 19:01:17.957504 22699590365312 run.py:483] Algo bellman_ford step 6140 current loss 0.284199, current_train_items 196512.
I0302 19:01:17.973580 22699590365312 run.py:483] Algo bellman_ford step 6141 current loss 0.357793, current_train_items 196544.
I0302 19:01:17.997894 22699590365312 run.py:483] Algo bellman_ford step 6142 current loss 0.610290, current_train_items 196576.
I0302 19:01:18.029124 22699590365312 run.py:483] Algo bellman_ford step 6143 current loss 0.694309, current_train_items 196608.
I0302 19:01:18.062222 22699590365312 run.py:483] Algo bellman_ford step 6144 current loss 0.811674, current_train_items 196640.
I0302 19:01:18.081804 22699590365312 run.py:483] Algo bellman_ford step 6145 current loss 0.246557, current_train_items 196672.
I0302 19:01:18.098085 22699590365312 run.py:483] Algo bellman_ford step 6146 current loss 0.508298, current_train_items 196704.
I0302 19:01:18.121186 22699590365312 run.py:483] Algo bellman_ford step 6147 current loss 0.486055, current_train_items 196736.
I0302 19:01:18.151986 22699590365312 run.py:483] Algo bellman_ford step 6148 current loss 0.627040, current_train_items 196768.
I0302 19:01:18.186435 22699590365312 run.py:483] Algo bellman_ford step 6149 current loss 0.769089, current_train_items 196800.
I0302 19:01:18.206236 22699590365312 run.py:483] Algo bellman_ford step 6150 current loss 0.296211, current_train_items 196832.
I0302 19:01:18.214407 22699590365312 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.94140625, 'score': 0.94140625, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0302 19:01:18.214514 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.941, val scores are: bellman_ford: 0.941
I0302 19:01:18.231356 22699590365312 run.py:483] Algo bellman_ford step 6151 current loss 0.450790, current_train_items 196864.
I0302 19:01:18.253999 22699590365312 run.py:483] Algo bellman_ford step 6152 current loss 0.507903, current_train_items 196896.
I0302 19:01:18.283977 22699590365312 run.py:483] Algo bellman_ford step 6153 current loss 0.608593, current_train_items 196928.
I0302 19:01:18.317970 22699590365312 run.py:483] Algo bellman_ford step 6154 current loss 0.710961, current_train_items 196960.
I0302 19:01:18.338241 22699590365312 run.py:483] Algo bellman_ford step 6155 current loss 0.253406, current_train_items 196992.
I0302 19:01:18.354373 22699590365312 run.py:483] Algo bellman_ford step 6156 current loss 0.360743, current_train_items 197024.
I0302 19:01:18.377831 22699590365312 run.py:483] Algo bellman_ford step 6157 current loss 0.443616, current_train_items 197056.
I0302 19:01:18.407672 22699590365312 run.py:483] Algo bellman_ford step 6158 current loss 0.548886, current_train_items 197088.
I0302 19:01:18.441069 22699590365312 run.py:483] Algo bellman_ford step 6159 current loss 0.651336, current_train_items 197120.
I0302 19:01:18.460819 22699590365312 run.py:483] Algo bellman_ford step 6160 current loss 0.326127, current_train_items 197152.
I0302 19:01:18.477428 22699590365312 run.py:483] Algo bellman_ford step 6161 current loss 0.457456, current_train_items 197184.
I0302 19:01:18.500506 22699590365312 run.py:483] Algo bellman_ford step 6162 current loss 0.597618, current_train_items 197216.
I0302 19:01:18.532598 22699590365312 run.py:483] Algo bellman_ford step 6163 current loss 0.680403, current_train_items 197248.
I0302 19:01:18.566533 22699590365312 run.py:483] Algo bellman_ford step 6164 current loss 0.728043, current_train_items 197280.
I0302 19:01:18.585952 22699590365312 run.py:483] Algo bellman_ford step 6165 current loss 0.218349, current_train_items 197312.
I0302 19:01:18.602188 22699590365312 run.py:483] Algo bellman_ford step 6166 current loss 0.415152, current_train_items 197344.
I0302 19:01:18.625555 22699590365312 run.py:483] Algo bellman_ford step 6167 current loss 0.511694, current_train_items 197376.
I0302 19:01:18.656754 22699590365312 run.py:483] Algo bellman_ford step 6168 current loss 0.578326, current_train_items 197408.
I0302 19:01:18.690476 22699590365312 run.py:483] Algo bellman_ford step 6169 current loss 0.707722, current_train_items 197440.
I0302 19:01:18.710612 22699590365312 run.py:483] Algo bellman_ford step 6170 current loss 0.294250, current_train_items 197472.
I0302 19:01:18.726578 22699590365312 run.py:483] Algo bellman_ford step 6171 current loss 0.408520, current_train_items 197504.
I0302 19:01:18.749871 22699590365312 run.py:483] Algo bellman_ford step 6172 current loss 0.484522, current_train_items 197536.
I0302 19:01:18.781763 22699590365312 run.py:483] Algo bellman_ford step 6173 current loss 0.688657, current_train_items 197568.
I0302 19:01:18.815281 22699590365312 run.py:483] Algo bellman_ford step 6174 current loss 0.707214, current_train_items 197600.
I0302 19:01:18.835476 22699590365312 run.py:483] Algo bellman_ford step 6175 current loss 0.263855, current_train_items 197632.
I0302 19:01:18.851872 22699590365312 run.py:483] Algo bellman_ford step 6176 current loss 0.604359, current_train_items 197664.
I0302 19:01:18.874581 22699590365312 run.py:483] Algo bellman_ford step 6177 current loss 0.580191, current_train_items 197696.
I0302 19:01:18.905770 22699590365312 run.py:483] Algo bellman_ford step 6178 current loss 0.630716, current_train_items 197728.
I0302 19:01:18.939198 22699590365312 run.py:483] Algo bellman_ford step 6179 current loss 0.870371, current_train_items 197760.
I0302 19:01:18.958615 22699590365312 run.py:483] Algo bellman_ford step 6180 current loss 0.300679, current_train_items 197792.
I0302 19:01:18.974782 22699590365312 run.py:483] Algo bellman_ford step 6181 current loss 0.447731, current_train_items 197824.
I0302 19:01:18.997961 22699590365312 run.py:483] Algo bellman_ford step 6182 current loss 0.606441, current_train_items 197856.
I0302 19:01:19.028873 22699590365312 run.py:483] Algo bellman_ford step 6183 current loss 0.657644, current_train_items 197888.
I0302 19:01:19.064470 22699590365312 run.py:483] Algo bellman_ford step 6184 current loss 0.782773, current_train_items 197920.
I0302 19:01:19.084297 22699590365312 run.py:483] Algo bellman_ford step 6185 current loss 0.336707, current_train_items 197952.
I0302 19:01:19.100527 22699590365312 run.py:483] Algo bellman_ford step 6186 current loss 0.431358, current_train_items 197984.
I0302 19:01:19.122920 22699590365312 run.py:483] Algo bellman_ford step 6187 current loss 0.590632, current_train_items 198016.
I0302 19:01:19.153287 22699590365312 run.py:483] Algo bellman_ford step 6188 current loss 0.658590, current_train_items 198048.
I0302 19:01:19.187046 22699590365312 run.py:483] Algo bellman_ford step 6189 current loss 0.801489, current_train_items 198080.
I0302 19:01:19.207229 22699590365312 run.py:483] Algo bellman_ford step 6190 current loss 0.278442, current_train_items 198112.
I0302 19:01:19.223663 22699590365312 run.py:483] Algo bellman_ford step 6191 current loss 0.441645, current_train_items 198144.
I0302 19:01:19.247347 22699590365312 run.py:483] Algo bellman_ford step 6192 current loss 0.606689, current_train_items 198176.
I0302 19:01:19.279098 22699590365312 run.py:483] Algo bellman_ford step 6193 current loss 0.703748, current_train_items 198208.
I0302 19:01:19.311377 22699590365312 run.py:483] Algo bellman_ford step 6194 current loss 0.692226, current_train_items 198240.
I0302 19:01:19.331007 22699590365312 run.py:483] Algo bellman_ford step 6195 current loss 0.436127, current_train_items 198272.
I0302 19:01:19.346652 22699590365312 run.py:483] Algo bellman_ford step 6196 current loss 0.379443, current_train_items 198304.
I0302 19:01:19.370920 22699590365312 run.py:483] Algo bellman_ford step 6197 current loss 0.668778, current_train_items 198336.
I0302 19:01:19.400503 22699590365312 run.py:483] Algo bellman_ford step 6198 current loss 0.630784, current_train_items 198368.
I0302 19:01:19.435053 22699590365312 run.py:483] Algo bellman_ford step 6199 current loss 0.700951, current_train_items 198400.
I0302 19:01:19.455111 22699590365312 run.py:483] Algo bellman_ford step 6200 current loss 0.307699, current_train_items 198432.
I0302 19:01:19.463089 22699590365312 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0302 19:01:19.463205 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:01:19.480118 22699590365312 run.py:483] Algo bellman_ford step 6201 current loss 0.383116, current_train_items 198464.
I0302 19:01:19.504979 22699590365312 run.py:483] Algo bellman_ford step 6202 current loss 0.567388, current_train_items 198496.
I0302 19:01:19.534921 22699590365312 run.py:483] Algo bellman_ford step 6203 current loss 0.607387, current_train_items 198528.
I0302 19:01:19.567898 22699590365312 run.py:483] Algo bellman_ford step 6204 current loss 0.679742, current_train_items 198560.
I0302 19:01:19.588072 22699590365312 run.py:483] Algo bellman_ford step 6205 current loss 0.387048, current_train_items 198592.
I0302 19:01:19.603338 22699590365312 run.py:483] Algo bellman_ford step 6206 current loss 0.436466, current_train_items 198624.
I0302 19:01:19.626821 22699590365312 run.py:483] Algo bellman_ford step 6207 current loss 0.601282, current_train_items 198656.
I0302 19:01:19.657960 22699590365312 run.py:483] Algo bellman_ford step 6208 current loss 0.717718, current_train_items 198688.
I0302 19:01:19.692666 22699590365312 run.py:483] Algo bellman_ford step 6209 current loss 0.760669, current_train_items 198720.
I0302 19:01:19.712124 22699590365312 run.py:483] Algo bellman_ford step 6210 current loss 0.233943, current_train_items 198752.
I0302 19:01:19.728354 22699590365312 run.py:483] Algo bellman_ford step 6211 current loss 0.434823, current_train_items 198784.
I0302 19:01:19.751300 22699590365312 run.py:483] Algo bellman_ford step 6212 current loss 0.561038, current_train_items 198816.
I0302 19:01:19.780840 22699590365312 run.py:483] Algo bellman_ford step 6213 current loss 0.647197, current_train_items 198848.
I0302 19:01:19.815354 22699590365312 run.py:483] Algo bellman_ford step 6214 current loss 0.680888, current_train_items 198880.
I0302 19:01:19.834938 22699590365312 run.py:483] Algo bellman_ford step 6215 current loss 0.262190, current_train_items 198912.
I0302 19:01:19.851015 22699590365312 run.py:483] Algo bellman_ford step 6216 current loss 0.403531, current_train_items 198944.
I0302 19:01:19.873561 22699590365312 run.py:483] Algo bellman_ford step 6217 current loss 0.547384, current_train_items 198976.
I0302 19:01:19.904047 22699590365312 run.py:483] Algo bellman_ford step 6218 current loss 0.614725, current_train_items 199008.
I0302 19:01:19.936313 22699590365312 run.py:483] Algo bellman_ford step 6219 current loss 0.735244, current_train_items 199040.
I0302 19:01:19.955774 22699590365312 run.py:483] Algo bellman_ford step 6220 current loss 0.252709, current_train_items 199072.
I0302 19:01:19.971396 22699590365312 run.py:483] Algo bellman_ford step 6221 current loss 0.419833, current_train_items 199104.
I0302 19:01:19.995595 22699590365312 run.py:483] Algo bellman_ford step 6222 current loss 0.592779, current_train_items 199136.
I0302 19:01:20.025462 22699590365312 run.py:483] Algo bellman_ford step 6223 current loss 0.561018, current_train_items 199168.
I0302 19:01:20.059659 22699590365312 run.py:483] Algo bellman_ford step 6224 current loss 0.797380, current_train_items 199200.
I0302 19:01:20.079298 22699590365312 run.py:483] Algo bellman_ford step 6225 current loss 0.343069, current_train_items 199232.
I0302 19:01:20.095308 22699590365312 run.py:483] Algo bellman_ford step 6226 current loss 0.513074, current_train_items 199264.
I0302 19:01:20.117765 22699590365312 run.py:483] Algo bellman_ford step 6227 current loss 0.569836, current_train_items 199296.
I0302 19:01:20.147714 22699590365312 run.py:483] Algo bellman_ford step 6228 current loss 0.674505, current_train_items 199328.
I0302 19:01:20.179437 22699590365312 run.py:483] Algo bellman_ford step 6229 current loss 0.701135, current_train_items 199360.
I0302 19:01:20.199064 22699590365312 run.py:483] Algo bellman_ford step 6230 current loss 0.283541, current_train_items 199392.
I0302 19:01:20.215277 22699590365312 run.py:483] Algo bellman_ford step 6231 current loss 0.558380, current_train_items 199424.
I0302 19:01:20.238965 22699590365312 run.py:483] Algo bellman_ford step 6232 current loss 0.588005, current_train_items 199456.
I0302 19:01:20.268868 22699590365312 run.py:483] Algo bellman_ford step 6233 current loss 0.772554, current_train_items 199488.
I0302 19:01:20.302010 22699590365312 run.py:483] Algo bellman_ford step 6234 current loss 0.798838, current_train_items 199520.
I0302 19:01:20.321122 22699590365312 run.py:483] Algo bellman_ford step 6235 current loss 0.185862, current_train_items 199552.
I0302 19:01:20.337340 22699590365312 run.py:483] Algo bellman_ford step 6236 current loss 0.400757, current_train_items 199584.
I0302 19:01:20.359922 22699590365312 run.py:483] Algo bellman_ford step 6237 current loss 0.604866, current_train_items 199616.
I0302 19:01:20.391276 22699590365312 run.py:483] Algo bellman_ford step 6238 current loss 0.575756, current_train_items 199648.
I0302 19:01:20.421979 22699590365312 run.py:483] Algo bellman_ford step 6239 current loss 0.720233, current_train_items 199680.
I0302 19:01:20.441376 22699590365312 run.py:483] Algo bellman_ford step 6240 current loss 0.297908, current_train_items 199712.
I0302 19:01:20.457482 22699590365312 run.py:483] Algo bellman_ford step 6241 current loss 0.479427, current_train_items 199744.
I0302 19:01:20.481245 22699590365312 run.py:483] Algo bellman_ford step 6242 current loss 0.667023, current_train_items 199776.
I0302 19:01:20.512386 22699590365312 run.py:483] Algo bellman_ford step 6243 current loss 0.698100, current_train_items 199808.
I0302 19:01:20.544283 22699590365312 run.py:483] Algo bellman_ford step 6244 current loss 0.738741, current_train_items 199840.
I0302 19:01:20.563945 22699590365312 run.py:483] Algo bellman_ford step 6245 current loss 0.349164, current_train_items 199872.
I0302 19:01:20.580039 22699590365312 run.py:483] Algo bellman_ford step 6246 current loss 0.504243, current_train_items 199904.
I0302 19:01:20.602799 22699590365312 run.py:483] Algo bellman_ford step 6247 current loss 0.575518, current_train_items 199936.
I0302 19:01:20.634263 22699590365312 run.py:483] Algo bellman_ford step 6248 current loss 0.762344, current_train_items 199968.
I0302 19:01:20.666094 22699590365312 run.py:483] Algo bellman_ford step 6249 current loss 0.696250, current_train_items 200000.
I0302 19:01:20.685615 22699590365312 run.py:483] Algo bellman_ford step 6250 current loss 0.276613, current_train_items 200032.
I0302 19:01:20.693722 22699590365312 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0302 19:01:20.693826 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:01:20.710824 22699590365312 run.py:483] Algo bellman_ford step 6251 current loss 0.487811, current_train_items 200064.
I0302 19:01:20.734495 22699590365312 run.py:483] Algo bellman_ford step 6252 current loss 0.570348, current_train_items 200096.
I0302 19:01:20.766763 22699590365312 run.py:483] Algo bellman_ford step 6253 current loss 0.716111, current_train_items 200128.
I0302 19:01:20.799430 22699590365312 run.py:483] Algo bellman_ford step 6254 current loss 0.732252, current_train_items 200160.
I0302 19:01:20.819358 22699590365312 run.py:483] Algo bellman_ford step 6255 current loss 0.302885, current_train_items 200192.
I0302 19:01:20.835123 22699590365312 run.py:483] Algo bellman_ford step 6256 current loss 0.419219, current_train_items 200224.
I0302 19:01:20.859199 22699590365312 run.py:483] Algo bellman_ford step 6257 current loss 0.572281, current_train_items 200256.
I0302 19:01:20.889454 22699590365312 run.py:483] Algo bellman_ford step 6258 current loss 0.677698, current_train_items 200288.
I0302 19:01:20.922027 22699590365312 run.py:483] Algo bellman_ford step 6259 current loss 0.889526, current_train_items 200320.
I0302 19:01:20.941855 22699590365312 run.py:483] Algo bellman_ford step 6260 current loss 0.470120, current_train_items 200352.
I0302 19:01:20.958296 22699590365312 run.py:483] Algo bellman_ford step 6261 current loss 0.422523, current_train_items 200384.
I0302 19:01:20.981165 22699590365312 run.py:483] Algo bellman_ford step 6262 current loss 0.570731, current_train_items 200416.
I0302 19:01:21.011551 22699590365312 run.py:483] Algo bellman_ford step 6263 current loss 0.660167, current_train_items 200448.
I0302 19:01:21.044978 22699590365312 run.py:483] Algo bellman_ford step 6264 current loss 0.766315, current_train_items 200480.
I0302 19:01:21.064658 22699590365312 run.py:483] Algo bellman_ford step 6265 current loss 0.279626, current_train_items 200512.
I0302 19:01:21.081429 22699590365312 run.py:483] Algo bellman_ford step 6266 current loss 0.525439, current_train_items 200544.
I0302 19:01:21.106072 22699590365312 run.py:483] Algo bellman_ford step 6267 current loss 0.601496, current_train_items 200576.
I0302 19:01:21.136343 22699590365312 run.py:483] Algo bellman_ford step 6268 current loss 0.605960, current_train_items 200608.
I0302 19:01:21.166866 22699590365312 run.py:483] Algo bellman_ford step 6269 current loss 0.691137, current_train_items 200640.
I0302 19:01:21.186836 22699590365312 run.py:483] Algo bellman_ford step 6270 current loss 0.256885, current_train_items 200672.
I0302 19:01:21.203184 22699590365312 run.py:483] Algo bellman_ford step 6271 current loss 0.437522, current_train_items 200704.
I0302 19:01:21.226302 22699590365312 run.py:483] Algo bellman_ford step 6272 current loss 0.539841, current_train_items 200736.
I0302 19:01:21.256131 22699590365312 run.py:483] Algo bellman_ford step 6273 current loss 0.630982, current_train_items 200768.
I0302 19:01:21.290980 22699590365312 run.py:483] Algo bellman_ford step 6274 current loss 0.809076, current_train_items 200800.
I0302 19:01:21.311131 22699590365312 run.py:483] Algo bellman_ford step 6275 current loss 0.313714, current_train_items 200832.
I0302 19:01:21.327521 22699590365312 run.py:483] Algo bellman_ford step 6276 current loss 0.439986, current_train_items 200864.
I0302 19:01:21.350772 22699590365312 run.py:483] Algo bellman_ford step 6277 current loss 0.590708, current_train_items 200896.
I0302 19:01:21.380750 22699590365312 run.py:483] Algo bellman_ford step 6278 current loss 0.618204, current_train_items 200928.
I0302 19:01:21.415296 22699590365312 run.py:483] Algo bellman_ford step 6279 current loss 0.782902, current_train_items 200960.
I0302 19:01:21.435168 22699590365312 run.py:483] Algo bellman_ford step 6280 current loss 0.259511, current_train_items 200992.
I0302 19:01:21.451113 22699590365312 run.py:483] Algo bellman_ford step 6281 current loss 0.368888, current_train_items 201024.
I0302 19:01:21.474568 22699590365312 run.py:483] Algo bellman_ford step 6282 current loss 0.642183, current_train_items 201056.
I0302 19:01:21.505594 22699590365312 run.py:483] Algo bellman_ford step 6283 current loss 0.664536, current_train_items 201088.
I0302 19:01:21.539972 22699590365312 run.py:483] Algo bellman_ford step 6284 current loss 0.804053, current_train_items 201120.
I0302 19:01:21.559810 22699590365312 run.py:483] Algo bellman_ford step 6285 current loss 0.287902, current_train_items 201152.
I0302 19:01:21.576120 22699590365312 run.py:483] Algo bellman_ford step 6286 current loss 0.458274, current_train_items 201184.
I0302 19:01:21.599496 22699590365312 run.py:483] Algo bellman_ford step 6287 current loss 0.528111, current_train_items 201216.
I0302 19:01:21.630411 22699590365312 run.py:483] Algo bellman_ford step 6288 current loss 0.629854, current_train_items 201248.
I0302 19:01:21.663895 22699590365312 run.py:483] Algo bellman_ford step 6289 current loss 0.750516, current_train_items 201280.
I0302 19:01:21.683915 22699590365312 run.py:483] Algo bellman_ford step 6290 current loss 0.279259, current_train_items 201312.
I0302 19:01:21.700347 22699590365312 run.py:483] Algo bellman_ford step 6291 current loss 0.471007, current_train_items 201344.
I0302 19:01:21.723552 22699590365312 run.py:483] Algo bellman_ford step 6292 current loss 0.548463, current_train_items 201376.
I0302 19:01:21.755164 22699590365312 run.py:483] Algo bellman_ford step 6293 current loss 0.758732, current_train_items 201408.
I0302 19:01:21.790266 22699590365312 run.py:483] Algo bellman_ford step 6294 current loss 0.700135, current_train_items 201440.
I0302 19:01:21.809971 22699590365312 run.py:483] Algo bellman_ford step 6295 current loss 0.246754, current_train_items 201472.
I0302 19:01:21.826856 22699590365312 run.py:483] Algo bellman_ford step 6296 current loss 0.443853, current_train_items 201504.
I0302 19:01:21.850084 22699590365312 run.py:483] Algo bellman_ford step 6297 current loss 0.517519, current_train_items 201536.
I0302 19:01:21.881297 22699590365312 run.py:483] Algo bellman_ford step 6298 current loss 0.636372, current_train_items 201568.
I0302 19:01:21.914013 22699590365312 run.py:483] Algo bellman_ford step 6299 current loss 0.641771, current_train_items 201600.
I0302 19:01:21.933549 22699590365312 run.py:483] Algo bellman_ford step 6300 current loss 0.271737, current_train_items 201632.
I0302 19:01:21.941556 22699590365312 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0302 19:01:21.941662 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:01:21.958644 22699590365312 run.py:483] Algo bellman_ford step 6301 current loss 0.445707, current_train_items 201664.
I0302 19:01:21.982143 22699590365312 run.py:483] Algo bellman_ford step 6302 current loss 0.633958, current_train_items 201696.
I0302 19:01:22.012932 22699590365312 run.py:483] Algo bellman_ford step 6303 current loss 0.578635, current_train_items 201728.
I0302 19:01:22.044254 22699590365312 run.py:483] Algo bellman_ford step 6304 current loss 0.613288, current_train_items 201760.
I0302 19:01:22.064417 22699590365312 run.py:483] Algo bellman_ford step 6305 current loss 0.289529, current_train_items 201792.
I0302 19:01:22.080035 22699590365312 run.py:483] Algo bellman_ford step 6306 current loss 0.418841, current_train_items 201824.
I0302 19:01:22.103325 22699590365312 run.py:483] Algo bellman_ford step 6307 current loss 0.530677, current_train_items 201856.
I0302 19:01:22.134877 22699590365312 run.py:483] Algo bellman_ford step 6308 current loss 0.630721, current_train_items 201888.
I0302 19:01:22.167968 22699590365312 run.py:483] Algo bellman_ford step 6309 current loss 0.809430, current_train_items 201920.
I0302 19:01:22.187648 22699590365312 run.py:483] Algo bellman_ford step 6310 current loss 0.309613, current_train_items 201952.
I0302 19:01:22.203625 22699590365312 run.py:483] Algo bellman_ford step 6311 current loss 0.468740, current_train_items 201984.
I0302 19:01:22.225978 22699590365312 run.py:483] Algo bellman_ford step 6312 current loss 0.523275, current_train_items 202016.
I0302 19:01:22.257924 22699590365312 run.py:483] Algo bellman_ford step 6313 current loss 0.644602, current_train_items 202048.
I0302 19:01:22.292074 22699590365312 run.py:483] Algo bellman_ford step 6314 current loss 0.804124, current_train_items 202080.
I0302 19:01:22.311889 22699590365312 run.py:483] Algo bellman_ford step 6315 current loss 0.247767, current_train_items 202112.
I0302 19:01:22.327789 22699590365312 run.py:483] Algo bellman_ford step 6316 current loss 0.421471, current_train_items 202144.
I0302 19:01:22.351112 22699590365312 run.py:483] Algo bellman_ford step 6317 current loss 0.542558, current_train_items 202176.
I0302 19:01:22.381177 22699590365312 run.py:483] Algo bellman_ford step 6318 current loss 0.519439, current_train_items 202208.
I0302 19:01:22.414093 22699590365312 run.py:483] Algo bellman_ford step 6319 current loss 0.747588, current_train_items 202240.
I0302 19:01:22.433830 22699590365312 run.py:483] Algo bellman_ford step 6320 current loss 0.296463, current_train_items 202272.
I0302 19:01:22.450169 22699590365312 run.py:483] Algo bellman_ford step 6321 current loss 0.423613, current_train_items 202304.
I0302 19:01:22.474403 22699590365312 run.py:483] Algo bellman_ford step 6322 current loss 0.585481, current_train_items 202336.
I0302 19:01:22.506215 22699590365312 run.py:483] Algo bellman_ford step 6323 current loss 0.684410, current_train_items 202368.
I0302 19:01:22.539773 22699590365312 run.py:483] Algo bellman_ford step 6324 current loss 0.744414, current_train_items 202400.
I0302 19:01:22.559422 22699590365312 run.py:483] Algo bellman_ford step 6325 current loss 0.319026, current_train_items 202432.
I0302 19:01:22.575228 22699590365312 run.py:483] Algo bellman_ford step 6326 current loss 0.419845, current_train_items 202464.
I0302 19:01:22.599606 22699590365312 run.py:483] Algo bellman_ford step 6327 current loss 0.540220, current_train_items 202496.
I0302 19:01:22.630233 22699590365312 run.py:483] Algo bellman_ford step 6328 current loss 0.621584, current_train_items 202528.
I0302 19:01:22.664179 22699590365312 run.py:483] Algo bellman_ford step 6329 current loss 0.646447, current_train_items 202560.
I0302 19:01:22.683627 22699590365312 run.py:483] Algo bellman_ford step 6330 current loss 0.321113, current_train_items 202592.
I0302 19:01:22.699775 22699590365312 run.py:483] Algo bellman_ford step 6331 current loss 0.509152, current_train_items 202624.
I0302 19:01:22.722687 22699590365312 run.py:483] Algo bellman_ford step 6332 current loss 0.585818, current_train_items 202656.
I0302 19:01:22.753252 22699590365312 run.py:483] Algo bellman_ford step 6333 current loss 0.642591, current_train_items 202688.
I0302 19:01:22.785777 22699590365312 run.py:483] Algo bellman_ford step 6334 current loss 0.728104, current_train_items 202720.
I0302 19:01:22.805315 22699590365312 run.py:483] Algo bellman_ford step 6335 current loss 0.274803, current_train_items 202752.
I0302 19:01:22.821543 22699590365312 run.py:483] Algo bellman_ford step 6336 current loss 0.433005, current_train_items 202784.
I0302 19:01:22.845303 22699590365312 run.py:483] Algo bellman_ford step 6337 current loss 0.548898, current_train_items 202816.
I0302 19:01:22.877055 22699590365312 run.py:483] Algo bellman_ford step 6338 current loss 0.746128, current_train_items 202848.
I0302 19:01:22.910119 22699590365312 run.py:483] Algo bellman_ford step 6339 current loss 0.652734, current_train_items 202880.
I0302 19:01:22.929621 22699590365312 run.py:483] Algo bellman_ford step 6340 current loss 0.347351, current_train_items 202912.
I0302 19:01:22.945388 22699590365312 run.py:483] Algo bellman_ford step 6341 current loss 0.480276, current_train_items 202944.
I0302 19:01:22.969610 22699590365312 run.py:483] Algo bellman_ford step 6342 current loss 0.639821, current_train_items 202976.
I0302 19:01:22.999688 22699590365312 run.py:483] Algo bellman_ford step 6343 current loss 0.661099, current_train_items 203008.
I0302 19:01:23.030491 22699590365312 run.py:483] Algo bellman_ford step 6344 current loss 0.638279, current_train_items 203040.
I0302 19:01:23.050161 22699590365312 run.py:483] Algo bellman_ford step 6345 current loss 0.235461, current_train_items 203072.
I0302 19:01:23.066511 22699590365312 run.py:483] Algo bellman_ford step 6346 current loss 0.515797, current_train_items 203104.
I0302 19:01:23.089841 22699590365312 run.py:483] Algo bellman_ford step 6347 current loss 0.571772, current_train_items 203136.
I0302 19:01:23.119978 22699590365312 run.py:483] Algo bellman_ford step 6348 current loss 0.708471, current_train_items 203168.
I0302 19:01:23.155222 22699590365312 run.py:483] Algo bellman_ford step 6349 current loss 0.684841, current_train_items 203200.
I0302 19:01:23.174580 22699590365312 run.py:483] Algo bellman_ford step 6350 current loss 0.360889, current_train_items 203232.
I0302 19:01:23.182654 22699590365312 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0302 19:01:23.182758 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:01:23.199824 22699590365312 run.py:483] Algo bellman_ford step 6351 current loss 0.459799, current_train_items 203264.
I0302 19:01:23.222944 22699590365312 run.py:483] Algo bellman_ford step 6352 current loss 0.599833, current_train_items 203296.
I0302 19:01:23.253929 22699590365312 run.py:483] Algo bellman_ford step 6353 current loss 0.596570, current_train_items 203328.
I0302 19:01:23.287170 22699590365312 run.py:483] Algo bellman_ford step 6354 current loss 0.810132, current_train_items 203360.
I0302 19:01:23.307095 22699590365312 run.py:483] Algo bellman_ford step 6355 current loss 0.316332, current_train_items 203392.
I0302 19:01:23.323001 22699590365312 run.py:483] Algo bellman_ford step 6356 current loss 0.460363, current_train_items 203424.
I0302 19:01:23.347054 22699590365312 run.py:483] Algo bellman_ford step 6357 current loss 0.578103, current_train_items 203456.
I0302 19:01:23.377703 22699590365312 run.py:483] Algo bellman_ford step 6358 current loss 0.660034, current_train_items 203488.
I0302 19:01:23.411408 22699590365312 run.py:483] Algo bellman_ford step 6359 current loss 0.659355, current_train_items 203520.
I0302 19:01:23.431288 22699590365312 run.py:483] Algo bellman_ford step 6360 current loss 0.217638, current_train_items 203552.
I0302 19:01:23.447670 22699590365312 run.py:483] Algo bellman_ford step 6361 current loss 0.446949, current_train_items 203584.
I0302 19:01:23.470163 22699590365312 run.py:483] Algo bellman_ford step 6362 current loss 0.687154, current_train_items 203616.
I0302 19:01:23.500952 22699590365312 run.py:483] Algo bellman_ford step 6363 current loss 0.547220, current_train_items 203648.
I0302 19:01:23.533346 22699590365312 run.py:483] Algo bellman_ford step 6364 current loss 0.699807, current_train_items 203680.
I0302 19:01:23.552941 22699590365312 run.py:483] Algo bellman_ford step 6365 current loss 0.265726, current_train_items 203712.
I0302 19:01:23.569200 22699590365312 run.py:483] Algo bellman_ford step 6366 current loss 0.410200, current_train_items 203744.
I0302 19:01:23.592388 22699590365312 run.py:483] Algo bellman_ford step 6367 current loss 0.591267, current_train_items 203776.
I0302 19:01:23.624484 22699590365312 run.py:483] Algo bellman_ford step 6368 current loss 0.736403, current_train_items 203808.
I0302 19:01:23.656529 22699590365312 run.py:483] Algo bellman_ford step 6369 current loss 0.629940, current_train_items 203840.
I0302 19:01:23.676428 22699590365312 run.py:483] Algo bellman_ford step 6370 current loss 0.308167, current_train_items 203872.
I0302 19:01:23.692715 22699590365312 run.py:483] Algo bellman_ford step 6371 current loss 0.466008, current_train_items 203904.
I0302 19:01:23.716382 22699590365312 run.py:483] Algo bellman_ford step 6372 current loss 0.579393, current_train_items 203936.
I0302 19:01:23.746404 22699590365312 run.py:483] Algo bellman_ford step 6373 current loss 0.624497, current_train_items 203968.
I0302 19:01:23.779922 22699590365312 run.py:483] Algo bellman_ford step 6374 current loss 0.672719, current_train_items 204000.
I0302 19:01:23.799888 22699590365312 run.py:483] Algo bellman_ford step 6375 current loss 0.338865, current_train_items 204032.
I0302 19:01:23.815754 22699590365312 run.py:483] Algo bellman_ford step 6376 current loss 0.448793, current_train_items 204064.
I0302 19:01:23.839272 22699590365312 run.py:483] Algo bellman_ford step 6377 current loss 0.607477, current_train_items 204096.
I0302 19:01:23.869886 22699590365312 run.py:483] Algo bellman_ford step 6378 current loss 0.507333, current_train_items 204128.
I0302 19:01:23.905587 22699590365312 run.py:483] Algo bellman_ford step 6379 current loss 0.782603, current_train_items 204160.
I0302 19:01:23.924724 22699590365312 run.py:483] Algo bellman_ford step 6380 current loss 0.304086, current_train_items 204192.
I0302 19:01:23.940678 22699590365312 run.py:483] Algo bellman_ford step 6381 current loss 0.443879, current_train_items 204224.
I0302 19:01:23.964037 22699590365312 run.py:483] Algo bellman_ford step 6382 current loss 0.526844, current_train_items 204256.
I0302 19:01:23.994178 22699590365312 run.py:483] Algo bellman_ford step 6383 current loss 0.646017, current_train_items 204288.
I0302 19:01:24.027543 22699590365312 run.py:483] Algo bellman_ford step 6384 current loss 0.698475, current_train_items 204320.
I0302 19:01:24.047443 22699590365312 run.py:483] Algo bellman_ford step 6385 current loss 0.350921, current_train_items 204352.
I0302 19:01:24.063010 22699590365312 run.py:483] Algo bellman_ford step 6386 current loss 0.460521, current_train_items 204384.
I0302 19:01:24.086663 22699590365312 run.py:483] Algo bellman_ford step 6387 current loss 0.546786, current_train_items 204416.
I0302 19:01:24.116844 22699590365312 run.py:483] Algo bellman_ford step 6388 current loss 0.592061, current_train_items 204448.
I0302 19:01:24.149753 22699590365312 run.py:483] Algo bellman_ford step 6389 current loss 0.690542, current_train_items 204480.
I0302 19:01:24.169564 22699590365312 run.py:483] Algo bellman_ford step 6390 current loss 0.292205, current_train_items 204512.
I0302 19:01:24.185926 22699590365312 run.py:483] Algo bellman_ford step 6391 current loss 0.456760, current_train_items 204544.
I0302 19:01:24.208946 22699590365312 run.py:483] Algo bellman_ford step 6392 current loss 0.529577, current_train_items 204576.
I0302 19:01:24.238101 22699590365312 run.py:483] Algo bellman_ford step 6393 current loss 0.528075, current_train_items 204608.
I0302 19:01:24.271768 22699590365312 run.py:483] Algo bellman_ford step 6394 current loss 0.718282, current_train_items 204640.
I0302 19:01:24.291777 22699590365312 run.py:483] Algo bellman_ford step 6395 current loss 0.307708, current_train_items 204672.
I0302 19:01:24.307781 22699590365312 run.py:483] Algo bellman_ford step 6396 current loss 0.405447, current_train_items 204704.
I0302 19:01:24.330739 22699590365312 run.py:483] Algo bellman_ford step 6397 current loss 0.637942, current_train_items 204736.
I0302 19:01:24.361050 22699590365312 run.py:483] Algo bellman_ford step 6398 current loss 0.691759, current_train_items 204768.
I0302 19:01:24.392627 22699590365312 run.py:483] Algo bellman_ford step 6399 current loss 0.659904, current_train_items 204800.
I0302 19:01:24.412345 22699590365312 run.py:483] Algo bellman_ford step 6400 current loss 0.293784, current_train_items 204832.
I0302 19:01:24.420145 22699590365312 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0302 19:01:24.420261 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:01:24.436385 22699590365312 run.py:483] Algo bellman_ford step 6401 current loss 0.415465, current_train_items 204864.
I0302 19:01:24.459520 22699590365312 run.py:483] Algo bellman_ford step 6402 current loss 0.561464, current_train_items 204896.
I0302 19:01:24.490432 22699590365312 run.py:483] Algo bellman_ford step 6403 current loss 0.657629, current_train_items 204928.
I0302 19:01:24.523796 22699590365312 run.py:483] Algo bellman_ford step 6404 current loss 0.837520, current_train_items 204960.
I0302 19:01:24.543556 22699590365312 run.py:483] Algo bellman_ford step 6405 current loss 0.280388, current_train_items 204992.
I0302 19:01:24.559121 22699590365312 run.py:483] Algo bellman_ford step 6406 current loss 0.465186, current_train_items 205024.
I0302 19:01:24.581942 22699590365312 run.py:483] Algo bellman_ford step 6407 current loss 0.515019, current_train_items 205056.
I0302 19:01:24.612344 22699590365312 run.py:483] Algo bellman_ford step 6408 current loss 0.595189, current_train_items 205088.
I0302 19:01:24.647077 22699590365312 run.py:483] Algo bellman_ford step 6409 current loss 0.739729, current_train_items 205120.
I0302 19:01:24.666317 22699590365312 run.py:483] Algo bellman_ford step 6410 current loss 0.265197, current_train_items 205152.
I0302 19:01:24.682031 22699590365312 run.py:483] Algo bellman_ford step 6411 current loss 0.417533, current_train_items 205184.
I0302 19:01:24.704332 22699590365312 run.py:483] Algo bellman_ford step 6412 current loss 0.584784, current_train_items 205216.
I0302 19:01:24.734966 22699590365312 run.py:483] Algo bellman_ford step 6413 current loss 0.708067, current_train_items 205248.
I0302 19:01:24.768267 22699590365312 run.py:483] Algo bellman_ford step 6414 current loss 0.609277, current_train_items 205280.
I0302 19:01:24.787738 22699590365312 run.py:483] Algo bellman_ford step 6415 current loss 0.212245, current_train_items 205312.
I0302 19:01:24.803659 22699590365312 run.py:483] Algo bellman_ford step 6416 current loss 0.392458, current_train_items 205344.
I0302 19:01:24.827653 22699590365312 run.py:483] Algo bellman_ford step 6417 current loss 0.623231, current_train_items 205376.
I0302 19:01:24.859287 22699590365312 run.py:483] Algo bellman_ford step 6418 current loss 0.634309, current_train_items 205408.
I0302 19:01:24.892860 22699590365312 run.py:483] Algo bellman_ford step 6419 current loss 0.817883, current_train_items 205440.
I0302 19:01:24.912738 22699590365312 run.py:483] Algo bellman_ford step 6420 current loss 0.308435, current_train_items 205472.
I0302 19:01:24.928791 22699590365312 run.py:483] Algo bellman_ford step 6421 current loss 0.416737, current_train_items 205504.
I0302 19:01:24.951741 22699590365312 run.py:483] Algo bellman_ford step 6422 current loss 0.580508, current_train_items 205536.
I0302 19:01:24.981860 22699590365312 run.py:483] Algo bellman_ford step 6423 current loss 0.622455, current_train_items 205568.
I0302 19:01:25.014537 22699590365312 run.py:483] Algo bellman_ford step 6424 current loss 0.701658, current_train_items 205600.
I0302 19:01:25.033648 22699590365312 run.py:483] Algo bellman_ford step 6425 current loss 0.312706, current_train_items 205632.
I0302 19:01:25.049976 22699590365312 run.py:483] Algo bellman_ford step 6426 current loss 0.455876, current_train_items 205664.
I0302 19:01:25.073313 22699590365312 run.py:483] Algo bellman_ford step 6427 current loss 0.578152, current_train_items 205696.
I0302 19:01:25.104444 22699590365312 run.py:483] Algo bellman_ford step 6428 current loss 0.612462, current_train_items 205728.
I0302 19:01:25.139709 22699590365312 run.py:483] Algo bellman_ford step 6429 current loss 0.819298, current_train_items 205760.
I0302 19:01:25.159452 22699590365312 run.py:483] Algo bellman_ford step 6430 current loss 0.275737, current_train_items 205792.
I0302 19:01:25.175041 22699590365312 run.py:483] Algo bellman_ford step 6431 current loss 0.472225, current_train_items 205824.
I0302 19:01:25.198212 22699590365312 run.py:483] Algo bellman_ford step 6432 current loss 0.643368, current_train_items 205856.
I0302 19:01:25.229078 22699590365312 run.py:483] Algo bellman_ford step 6433 current loss 0.567766, current_train_items 205888.
I0302 19:01:25.261306 22699590365312 run.py:483] Algo bellman_ford step 6434 current loss 0.719946, current_train_items 205920.
I0302 19:01:25.280496 22699590365312 run.py:483] Algo bellman_ford step 6435 current loss 0.304908, current_train_items 205952.
I0302 19:01:25.296455 22699590365312 run.py:483] Algo bellman_ford step 6436 current loss 0.558301, current_train_items 205984.
I0302 19:01:25.319207 22699590365312 run.py:483] Algo bellman_ford step 6437 current loss 0.573641, current_train_items 206016.
I0302 19:01:25.350661 22699590365312 run.py:483] Algo bellman_ford step 6438 current loss 0.715526, current_train_items 206048.
I0302 19:01:25.384289 22699590365312 run.py:483] Algo bellman_ford step 6439 current loss 0.741270, current_train_items 206080.
I0302 19:01:25.403528 22699590365312 run.py:483] Algo bellman_ford step 6440 current loss 0.255904, current_train_items 206112.
I0302 19:01:25.419830 22699590365312 run.py:483] Algo bellman_ford step 6441 current loss 0.457486, current_train_items 206144.
I0302 19:01:25.442064 22699590365312 run.py:483] Algo bellman_ford step 6442 current loss 0.528885, current_train_items 206176.
I0302 19:01:25.471068 22699590365312 run.py:483] Algo bellman_ford step 6443 current loss 0.600506, current_train_items 206208.
I0302 19:01:25.504551 22699590365312 run.py:483] Algo bellman_ford step 6444 current loss 0.694128, current_train_items 206240.
I0302 19:01:25.523861 22699590365312 run.py:483] Algo bellman_ford step 6445 current loss 0.254270, current_train_items 206272.
I0302 19:01:25.539624 22699590365312 run.py:483] Algo bellman_ford step 6446 current loss 0.483077, current_train_items 206304.
I0302 19:01:25.563549 22699590365312 run.py:483] Algo bellman_ford step 6447 current loss 0.538249, current_train_items 206336.
I0302 19:01:25.594205 22699590365312 run.py:483] Algo bellman_ford step 6448 current loss 0.662263, current_train_items 206368.
I0302 19:01:25.627732 22699590365312 run.py:483] Algo bellman_ford step 6449 current loss 0.744880, current_train_items 206400.
I0302 19:01:25.647194 22699590365312 run.py:483] Algo bellman_ford step 6450 current loss 0.314217, current_train_items 206432.
I0302 19:01:25.655268 22699590365312 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0302 19:01:25.655375 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:01:25.672240 22699590365312 run.py:483] Algo bellman_ford step 6451 current loss 0.457938, current_train_items 206464.
I0302 19:01:25.697272 22699590365312 run.py:483] Algo bellman_ford step 6452 current loss 0.635278, current_train_items 206496.
I0302 19:01:25.727462 22699590365312 run.py:483] Algo bellman_ford step 6453 current loss 0.599814, current_train_items 206528.
I0302 19:01:25.759530 22699590365312 run.py:483] Algo bellman_ford step 6454 current loss 0.591112, current_train_items 206560.
I0302 19:01:25.779498 22699590365312 run.py:483] Algo bellman_ford step 6455 current loss 0.279279, current_train_items 206592.
I0302 19:01:25.795331 22699590365312 run.py:483] Algo bellman_ford step 6456 current loss 0.378584, current_train_items 206624.
I0302 19:01:25.817566 22699590365312 run.py:483] Algo bellman_ford step 6457 current loss 0.643407, current_train_items 206656.
I0302 19:01:25.847939 22699590365312 run.py:483] Algo bellman_ford step 6458 current loss 0.613410, current_train_items 206688.
I0302 19:01:25.879796 22699590365312 run.py:483] Algo bellman_ford step 6459 current loss 0.658863, current_train_items 206720.
I0302 19:01:25.899875 22699590365312 run.py:483] Algo bellman_ford step 6460 current loss 0.355369, current_train_items 206752.
I0302 19:01:25.916106 22699590365312 run.py:483] Algo bellman_ford step 6461 current loss 0.527046, current_train_items 206784.
I0302 19:01:25.939213 22699590365312 run.py:483] Algo bellman_ford step 6462 current loss 0.543834, current_train_items 206816.
I0302 19:01:25.969398 22699590365312 run.py:483] Algo bellman_ford step 6463 current loss 0.566309, current_train_items 206848.
I0302 19:01:26.004870 22699590365312 run.py:483] Algo bellman_ford step 6464 current loss 0.789745, current_train_items 206880.
I0302 19:01:26.023995 22699590365312 run.py:483] Algo bellman_ford step 6465 current loss 0.222556, current_train_items 206912.
I0302 19:01:26.040384 22699590365312 run.py:483] Algo bellman_ford step 6466 current loss 0.402517, current_train_items 206944.
I0302 19:01:26.065097 22699590365312 run.py:483] Algo bellman_ford step 6467 current loss 0.722841, current_train_items 206976.
I0302 19:01:26.096916 22699590365312 run.py:483] Algo bellman_ford step 6468 current loss 0.641914, current_train_items 207008.
I0302 19:01:26.131169 22699590365312 run.py:483] Algo bellman_ford step 6469 current loss 0.778101, current_train_items 207040.
I0302 19:01:26.151311 22699590365312 run.py:483] Algo bellman_ford step 6470 current loss 0.338762, current_train_items 207072.
I0302 19:01:26.167903 22699590365312 run.py:483] Algo bellman_ford step 6471 current loss 0.442059, current_train_items 207104.
I0302 19:01:26.190382 22699590365312 run.py:483] Algo bellman_ford step 6472 current loss 0.545750, current_train_items 207136.
I0302 19:01:26.221338 22699590365312 run.py:483] Algo bellman_ford step 6473 current loss 0.950498, current_train_items 207168.
I0302 19:01:26.253617 22699590365312 run.py:483] Algo bellman_ford step 6474 current loss 0.762719, current_train_items 207200.
I0302 19:01:26.273362 22699590365312 run.py:483] Algo bellman_ford step 6475 current loss 0.322209, current_train_items 207232.
I0302 19:01:26.289709 22699590365312 run.py:483] Algo bellman_ford step 6476 current loss 0.528955, current_train_items 207264.
I0302 19:01:26.312031 22699590365312 run.py:483] Algo bellman_ford step 6477 current loss 0.531945, current_train_items 207296.
I0302 19:01:26.343001 22699590365312 run.py:483] Algo bellman_ford step 6478 current loss 0.682434, current_train_items 207328.
I0302 19:01:26.377671 22699590365312 run.py:483] Algo bellman_ford step 6479 current loss 0.879947, current_train_items 207360.
I0302 19:01:26.397595 22699590365312 run.py:483] Algo bellman_ford step 6480 current loss 0.380108, current_train_items 207392.
I0302 19:01:26.413547 22699590365312 run.py:483] Algo bellman_ford step 6481 current loss 0.402203, current_train_items 207424.
I0302 19:01:26.437007 22699590365312 run.py:483] Algo bellman_ford step 6482 current loss 0.616856, current_train_items 207456.
I0302 19:01:26.468523 22699590365312 run.py:483] Algo bellman_ford step 6483 current loss 0.748552, current_train_items 207488.
I0302 19:01:26.502780 22699590365312 run.py:483] Algo bellman_ford step 6484 current loss 0.764708, current_train_items 207520.
I0302 19:01:26.522602 22699590365312 run.py:483] Algo bellman_ford step 6485 current loss 0.344361, current_train_items 207552.
I0302 19:01:26.538933 22699590365312 run.py:483] Algo bellman_ford step 6486 current loss 0.405741, current_train_items 207584.
I0302 19:01:26.563484 22699590365312 run.py:483] Algo bellman_ford step 6487 current loss 0.668971, current_train_items 207616.
I0302 19:01:26.593052 22699590365312 run.py:483] Algo bellman_ford step 6488 current loss 0.590509, current_train_items 207648.
I0302 19:01:26.626603 22699590365312 run.py:483] Algo bellman_ford step 6489 current loss 0.716237, current_train_items 207680.
I0302 19:01:26.646680 22699590365312 run.py:483] Algo bellman_ford step 6490 current loss 0.336636, current_train_items 207712.
I0302 19:01:26.662755 22699590365312 run.py:483] Algo bellman_ford step 6491 current loss 0.399470, current_train_items 207744.
I0302 19:01:26.685388 22699590365312 run.py:483] Algo bellman_ford step 6492 current loss 0.634135, current_train_items 207776.
I0302 19:01:26.716119 22699590365312 run.py:483] Algo bellman_ford step 6493 current loss 0.623948, current_train_items 207808.
I0302 19:01:26.749200 22699590365312 run.py:483] Algo bellman_ford step 6494 current loss 0.851274, current_train_items 207840.
I0302 19:01:26.768407 22699590365312 run.py:483] Algo bellman_ford step 6495 current loss 0.297429, current_train_items 207872.
I0302 19:01:26.784266 22699590365312 run.py:483] Algo bellman_ford step 6496 current loss 0.431724, current_train_items 207904.
I0302 19:01:26.808519 22699590365312 run.py:483] Algo bellman_ford step 6497 current loss 0.586049, current_train_items 207936.
I0302 19:01:26.838642 22699590365312 run.py:483] Algo bellman_ford step 6498 current loss 0.580261, current_train_items 207968.
I0302 19:01:26.873668 22699590365312 run.py:483] Algo bellman_ford step 6499 current loss 0.675052, current_train_items 208000.
I0302 19:01:26.893831 22699590365312 run.py:483] Algo bellman_ford step 6500 current loss 0.364676, current_train_items 208032.
I0302 19:01:26.901356 22699590365312 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0302 19:01:26.901466 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:01:26.918169 22699590365312 run.py:483] Algo bellman_ford step 6501 current loss 0.392659, current_train_items 208064.
I0302 19:01:26.942671 22699590365312 run.py:483] Algo bellman_ford step 6502 current loss 0.597658, current_train_items 208096.
I0302 19:01:26.972932 22699590365312 run.py:483] Algo bellman_ford step 6503 current loss 0.593958, current_train_items 208128.
I0302 19:01:27.007519 22699590365312 run.py:483] Algo bellman_ford step 6504 current loss 0.682643, current_train_items 208160.
I0302 19:01:27.027808 22699590365312 run.py:483] Algo bellman_ford step 6505 current loss 0.284535, current_train_items 208192.
I0302 19:01:27.043018 22699590365312 run.py:483] Algo bellman_ford step 6506 current loss 0.320366, current_train_items 208224.
I0302 19:01:27.066450 22699590365312 run.py:483] Algo bellman_ford step 6507 current loss 0.600307, current_train_items 208256.
I0302 19:01:27.097770 22699590365312 run.py:483] Algo bellman_ford step 6508 current loss 0.607660, current_train_items 208288.
I0302 19:01:27.128742 22699590365312 run.py:483] Algo bellman_ford step 6509 current loss 0.742352, current_train_items 208320.
I0302 19:01:27.147984 22699590365312 run.py:483] Algo bellman_ford step 6510 current loss 0.281310, current_train_items 208352.
I0302 19:01:27.164257 22699590365312 run.py:483] Algo bellman_ford step 6511 current loss 0.422968, current_train_items 208384.
I0302 19:01:27.188344 22699590365312 run.py:483] Algo bellman_ford step 6512 current loss 0.579790, current_train_items 208416.
I0302 19:01:27.218642 22699590365312 run.py:483] Algo bellman_ford step 6513 current loss 0.546383, current_train_items 208448.
I0302 19:01:27.252851 22699590365312 run.py:483] Algo bellman_ford step 6514 current loss 0.765746, current_train_items 208480.
I0302 19:01:27.272004 22699590365312 run.py:483] Algo bellman_ford step 6515 current loss 0.264734, current_train_items 208512.
I0302 19:01:27.287942 22699590365312 run.py:483] Algo bellman_ford step 6516 current loss 0.465918, current_train_items 208544.
I0302 19:01:27.311415 22699590365312 run.py:483] Algo bellman_ford step 6517 current loss 0.537231, current_train_items 208576.
I0302 19:01:27.341860 22699590365312 run.py:483] Algo bellman_ford step 6518 current loss 0.612599, current_train_items 208608.
I0302 19:01:27.374184 22699590365312 run.py:483] Algo bellman_ford step 6519 current loss 0.716516, current_train_items 208640.
I0302 19:01:27.393416 22699590365312 run.py:483] Algo bellman_ford step 6520 current loss 0.316136, current_train_items 208672.
I0302 19:01:27.409452 22699590365312 run.py:483] Algo bellman_ford step 6521 current loss 0.458858, current_train_items 208704.
I0302 19:01:27.433367 22699590365312 run.py:483] Algo bellman_ford step 6522 current loss 0.650689, current_train_items 208736.
I0302 19:01:27.463450 22699590365312 run.py:483] Algo bellman_ford step 6523 current loss 0.632895, current_train_items 208768.
I0302 19:01:27.494278 22699590365312 run.py:483] Algo bellman_ford step 6524 current loss 0.661196, current_train_items 208800.
I0302 19:01:27.513766 22699590365312 run.py:483] Algo bellman_ford step 6525 current loss 0.222800, current_train_items 208832.
I0302 19:01:27.529606 22699590365312 run.py:483] Algo bellman_ford step 6526 current loss 0.373252, current_train_items 208864.
I0302 19:01:27.552430 22699590365312 run.py:483] Algo bellman_ford step 6527 current loss 0.554042, current_train_items 208896.
I0302 19:01:27.581895 22699590365312 run.py:483] Algo bellman_ford step 6528 current loss 0.571138, current_train_items 208928.
I0302 19:01:27.615268 22699590365312 run.py:483] Algo bellman_ford step 6529 current loss 0.691003, current_train_items 208960.
I0302 19:01:27.634459 22699590365312 run.py:483] Algo bellman_ford step 6530 current loss 0.278824, current_train_items 208992.
I0302 19:01:27.650696 22699590365312 run.py:483] Algo bellman_ford step 6531 current loss 0.432038, current_train_items 209024.
I0302 19:01:27.675243 22699590365312 run.py:483] Algo bellman_ford step 6532 current loss 0.699569, current_train_items 209056.
I0302 19:01:27.704952 22699590365312 run.py:483] Algo bellman_ford step 6533 current loss 0.655727, current_train_items 209088.
I0302 19:01:27.737831 22699590365312 run.py:483] Algo bellman_ford step 6534 current loss 0.767704, current_train_items 209120.
I0302 19:01:27.757285 22699590365312 run.py:483] Algo bellman_ford step 6535 current loss 0.261344, current_train_items 209152.
I0302 19:01:27.773465 22699590365312 run.py:483] Algo bellman_ford step 6536 current loss 0.459356, current_train_items 209184.
I0302 19:01:27.796308 22699590365312 run.py:483] Algo bellman_ford step 6537 current loss 0.546562, current_train_items 209216.
I0302 19:01:27.826194 22699590365312 run.py:483] Algo bellman_ford step 6538 current loss 0.671346, current_train_items 209248.
I0302 19:01:27.859332 22699590365312 run.py:483] Algo bellman_ford step 6539 current loss 0.638193, current_train_items 209280.
I0302 19:01:27.878603 22699590365312 run.py:483] Algo bellman_ford step 6540 current loss 0.299704, current_train_items 209312.
I0302 19:01:27.894635 22699590365312 run.py:483] Algo bellman_ford step 6541 current loss 0.459531, current_train_items 209344.
I0302 19:01:27.917635 22699590365312 run.py:483] Algo bellman_ford step 6542 current loss 0.606670, current_train_items 209376.
I0302 19:01:27.948591 22699590365312 run.py:483] Algo bellman_ford step 6543 current loss 0.710462, current_train_items 209408.
I0302 19:01:27.983364 22699590365312 run.py:483] Algo bellman_ford step 6544 current loss 0.847006, current_train_items 209440.
I0302 19:01:28.002764 22699590365312 run.py:483] Algo bellman_ford step 6545 current loss 0.245243, current_train_items 209472.
I0302 19:01:28.018586 22699590365312 run.py:483] Algo bellman_ford step 6546 current loss 0.406015, current_train_items 209504.
I0302 19:01:28.041284 22699590365312 run.py:483] Algo bellman_ford step 6547 current loss 0.470242, current_train_items 209536.
I0302 19:01:28.072911 22699590365312 run.py:483] Algo bellman_ford step 6548 current loss 0.657091, current_train_items 209568.
I0302 19:01:28.107028 22699590365312 run.py:483] Algo bellman_ford step 6549 current loss 0.694534, current_train_items 209600.
I0302 19:01:28.126384 22699590365312 run.py:483] Algo bellman_ford step 6550 current loss 0.263499, current_train_items 209632.
I0302 19:01:28.134676 22699590365312 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0302 19:01:28.134783 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:01:28.151843 22699590365312 run.py:483] Algo bellman_ford step 6551 current loss 0.437590, current_train_items 209664.
I0302 19:01:28.175297 22699590365312 run.py:483] Algo bellman_ford step 6552 current loss 0.503655, current_train_items 209696.
I0302 19:01:28.207263 22699590365312 run.py:483] Algo bellman_ford step 6553 current loss 0.639797, current_train_items 209728.
I0302 19:01:28.243024 22699590365312 run.py:483] Algo bellman_ford step 6554 current loss 0.924004, current_train_items 209760.
I0302 19:01:28.262825 22699590365312 run.py:483] Algo bellman_ford step 6555 current loss 0.302604, current_train_items 209792.
I0302 19:01:28.278588 22699590365312 run.py:483] Algo bellman_ford step 6556 current loss 0.498373, current_train_items 209824.
I0302 19:01:28.301970 22699590365312 run.py:483] Algo bellman_ford step 6557 current loss 0.596880, current_train_items 209856.
I0302 19:01:28.333149 22699590365312 run.py:483] Algo bellman_ford step 6558 current loss 0.674734, current_train_items 209888.
I0302 19:01:28.366582 22699590365312 run.py:483] Algo bellman_ford step 6559 current loss 0.733866, current_train_items 209920.
I0302 19:01:28.386639 22699590365312 run.py:483] Algo bellman_ford step 6560 current loss 0.277097, current_train_items 209952.
I0302 19:01:28.402377 22699590365312 run.py:483] Algo bellman_ford step 6561 current loss 0.433681, current_train_items 209984.
I0302 19:01:28.424469 22699590365312 run.py:483] Algo bellman_ford step 6562 current loss 0.493559, current_train_items 210016.
I0302 19:01:28.454627 22699590365312 run.py:483] Algo bellman_ford step 6563 current loss 0.671283, current_train_items 210048.
I0302 19:01:28.485012 22699590365312 run.py:483] Algo bellman_ford step 6564 current loss 0.739208, current_train_items 210080.
I0302 19:01:28.504577 22699590365312 run.py:483] Algo bellman_ford step 6565 current loss 0.291045, current_train_items 210112.
I0302 19:01:28.520326 22699590365312 run.py:483] Algo bellman_ford step 6566 current loss 0.431358, current_train_items 210144.
I0302 19:01:28.543550 22699590365312 run.py:483] Algo bellman_ford step 6567 current loss 0.617501, current_train_items 210176.
I0302 19:01:28.572336 22699590365312 run.py:483] Algo bellman_ford step 6568 current loss 0.726072, current_train_items 210208.
I0302 19:01:28.605437 22699590365312 run.py:483] Algo bellman_ford step 6569 current loss 0.697913, current_train_items 210240.
I0302 19:01:28.625357 22699590365312 run.py:483] Algo bellman_ford step 6570 current loss 0.294027, current_train_items 210272.
I0302 19:01:28.641394 22699590365312 run.py:483] Algo bellman_ford step 6571 current loss 0.466630, current_train_items 210304.
I0302 19:01:28.664096 22699590365312 run.py:483] Algo bellman_ford step 6572 current loss 0.515801, current_train_items 210336.
I0302 19:01:28.694135 22699590365312 run.py:483] Algo bellman_ford step 6573 current loss 0.506827, current_train_items 210368.
I0302 19:01:28.727134 22699590365312 run.py:483] Algo bellman_ford step 6574 current loss 0.730311, current_train_items 210400.
I0302 19:01:28.746937 22699590365312 run.py:483] Algo bellman_ford step 6575 current loss 0.309669, current_train_items 210432.
I0302 19:01:28.763288 22699590365312 run.py:483] Algo bellman_ford step 6576 current loss 0.393095, current_train_items 210464.
I0302 19:01:28.785778 22699590365312 run.py:483] Algo bellman_ford step 6577 current loss 0.519922, current_train_items 210496.
I0302 19:01:28.816483 22699590365312 run.py:483] Algo bellman_ford step 6578 current loss 0.600869, current_train_items 210528.
I0302 19:01:28.849878 22699590365312 run.py:483] Algo bellman_ford step 6579 current loss 0.673908, current_train_items 210560.
I0302 19:01:28.869431 22699590365312 run.py:483] Algo bellman_ford step 6580 current loss 0.318546, current_train_items 210592.
I0302 19:01:28.885533 22699590365312 run.py:483] Algo bellman_ford step 6581 current loss 0.409528, current_train_items 210624.
I0302 19:01:28.908956 22699590365312 run.py:483] Algo bellman_ford step 6582 current loss 0.611184, current_train_items 210656.
I0302 19:01:28.940809 22699590365312 run.py:483] Algo bellman_ford step 6583 current loss 0.755850, current_train_items 210688.
I0302 19:01:28.973428 22699590365312 run.py:483] Algo bellman_ford step 6584 current loss 0.748755, current_train_items 210720.
I0302 19:01:28.992891 22699590365312 run.py:483] Algo bellman_ford step 6585 current loss 0.280438, current_train_items 210752.
I0302 19:01:29.008510 22699590365312 run.py:483] Algo bellman_ford step 6586 current loss 0.426728, current_train_items 210784.
I0302 19:01:29.030197 22699590365312 run.py:483] Algo bellman_ford step 6587 current loss 0.614602, current_train_items 210816.
I0302 19:01:29.059772 22699590365312 run.py:483] Algo bellman_ford step 6588 current loss 0.780882, current_train_items 210848.
I0302 19:01:29.091373 22699590365312 run.py:483] Algo bellman_ford step 6589 current loss 0.721612, current_train_items 210880.
I0302 19:01:29.110936 22699590365312 run.py:483] Algo bellman_ford step 6590 current loss 0.355946, current_train_items 210912.
I0302 19:01:29.126795 22699590365312 run.py:483] Algo bellman_ford step 6591 current loss 0.437147, current_train_items 210944.
I0302 19:01:29.149872 22699590365312 run.py:483] Algo bellman_ford step 6592 current loss 0.555389, current_train_items 210976.
I0302 19:01:29.180338 22699590365312 run.py:483] Algo bellman_ford step 6593 current loss 0.528243, current_train_items 211008.
I0302 19:01:29.212560 22699590365312 run.py:483] Algo bellman_ford step 6594 current loss 0.771777, current_train_items 211040.
I0302 19:01:29.231846 22699590365312 run.py:483] Algo bellman_ford step 6595 current loss 0.290321, current_train_items 211072.
I0302 19:01:29.247774 22699590365312 run.py:483] Algo bellman_ford step 6596 current loss 0.403025, current_train_items 211104.
I0302 19:01:29.271564 22699590365312 run.py:483] Algo bellman_ford step 6597 current loss 0.521365, current_train_items 211136.
I0302 19:01:29.302581 22699590365312 run.py:483] Algo bellman_ford step 6598 current loss 0.602067, current_train_items 211168.
I0302 19:01:29.332340 22699590365312 run.py:483] Algo bellman_ford step 6599 current loss 0.678068, current_train_items 211200.
I0302 19:01:29.351984 22699590365312 run.py:483] Algo bellman_ford step 6600 current loss 0.243059, current_train_items 211232.
I0302 19:01:29.359763 22699590365312 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0302 19:01:29.359870 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:01:29.376555 22699590365312 run.py:483] Algo bellman_ford step 6601 current loss 0.489411, current_train_items 211264.
I0302 19:01:29.399049 22699590365312 run.py:483] Algo bellman_ford step 6602 current loss 0.533037, current_train_items 211296.
I0302 19:01:29.430478 22699590365312 run.py:483] Algo bellman_ford step 6603 current loss 0.658299, current_train_items 211328.
I0302 19:01:29.464469 22699590365312 run.py:483] Algo bellman_ford step 6604 current loss 0.693672, current_train_items 211360.
I0302 19:01:29.484460 22699590365312 run.py:483] Algo bellman_ford step 6605 current loss 0.263948, current_train_items 211392.
I0302 19:01:29.500169 22699590365312 run.py:483] Algo bellman_ford step 6606 current loss 0.405048, current_train_items 211424.
I0302 19:01:29.523571 22699590365312 run.py:483] Algo bellman_ford step 6607 current loss 0.623391, current_train_items 211456.
I0302 19:01:29.554611 22699590365312 run.py:483] Algo bellman_ford step 6608 current loss 0.515721, current_train_items 211488.
I0302 19:01:29.585943 22699590365312 run.py:483] Algo bellman_ford step 6609 current loss 0.592631, current_train_items 211520.
I0302 19:01:29.605839 22699590365312 run.py:483] Algo bellman_ford step 6610 current loss 0.322212, current_train_items 211552.
I0302 19:01:29.621643 22699590365312 run.py:483] Algo bellman_ford step 6611 current loss 0.450638, current_train_items 211584.
I0302 19:01:29.644356 22699590365312 run.py:483] Algo bellman_ford step 6612 current loss 0.717231, current_train_items 211616.
I0302 19:01:29.675392 22699590365312 run.py:483] Algo bellman_ford step 6613 current loss 0.949905, current_train_items 211648.
I0302 19:01:29.711011 22699590365312 run.py:483] Algo bellman_ford step 6614 current loss 1.343419, current_train_items 211680.
I0302 19:01:29.730889 22699590365312 run.py:483] Algo bellman_ford step 6615 current loss 0.334414, current_train_items 211712.
I0302 19:01:29.746819 22699590365312 run.py:483] Algo bellman_ford step 6616 current loss 0.390312, current_train_items 211744.
I0302 19:01:29.770536 22699590365312 run.py:483] Algo bellman_ford step 6617 current loss 0.672461, current_train_items 211776.
I0302 19:01:29.801092 22699590365312 run.py:483] Algo bellman_ford step 6618 current loss 0.618320, current_train_items 211808.
I0302 19:01:29.834577 22699590365312 run.py:483] Algo bellman_ford step 6619 current loss 0.682277, current_train_items 211840.
I0302 19:01:29.854001 22699590365312 run.py:483] Algo bellman_ford step 6620 current loss 0.320444, current_train_items 211872.
I0302 19:01:29.869767 22699590365312 run.py:483] Algo bellman_ford step 6621 current loss 0.420419, current_train_items 211904.
I0302 19:01:29.891842 22699590365312 run.py:483] Algo bellman_ford step 6622 current loss 0.575813, current_train_items 211936.
I0302 19:01:29.923046 22699590365312 run.py:483] Algo bellman_ford step 6623 current loss 0.618117, current_train_items 211968.
I0302 19:01:29.955167 22699590365312 run.py:483] Algo bellman_ford step 6624 current loss 0.665658, current_train_items 212000.
I0302 19:01:29.974622 22699590365312 run.py:483] Algo bellman_ford step 6625 current loss 0.310539, current_train_items 212032.
I0302 19:01:29.990820 22699590365312 run.py:483] Algo bellman_ford step 6626 current loss 0.452628, current_train_items 212064.
I0302 19:01:30.014220 22699590365312 run.py:483] Algo bellman_ford step 6627 current loss 0.606003, current_train_items 212096.
I0302 19:01:30.043724 22699590365312 run.py:483] Algo bellman_ford step 6628 current loss 0.665010, current_train_items 212128.
I0302 19:01:30.075852 22699590365312 run.py:483] Algo bellman_ford step 6629 current loss 0.690901, current_train_items 212160.
I0302 19:01:30.095482 22699590365312 run.py:483] Algo bellman_ford step 6630 current loss 0.332223, current_train_items 212192.
I0302 19:01:30.111599 22699590365312 run.py:483] Algo bellman_ford step 6631 current loss 0.514607, current_train_items 212224.
I0302 19:01:30.136195 22699590365312 run.py:483] Algo bellman_ford step 6632 current loss 0.577810, current_train_items 212256.
I0302 19:01:30.167693 22699590365312 run.py:483] Algo bellman_ford step 6633 current loss 0.642560, current_train_items 212288.
I0302 19:01:30.199617 22699590365312 run.py:483] Algo bellman_ford step 6634 current loss 0.729616, current_train_items 212320.
I0302 19:01:30.219084 22699590365312 run.py:483] Algo bellman_ford step 6635 current loss 0.274869, current_train_items 212352.
I0302 19:01:30.235334 22699590365312 run.py:483] Algo bellman_ford step 6636 current loss 0.459695, current_train_items 212384.
I0302 19:01:30.258749 22699590365312 run.py:483] Algo bellman_ford step 6637 current loss 0.516763, current_train_items 212416.
I0302 19:01:30.289210 22699590365312 run.py:483] Algo bellman_ford step 6638 current loss 0.616192, current_train_items 212448.
I0302 19:01:30.322887 22699590365312 run.py:483] Algo bellman_ford step 6639 current loss 0.825504, current_train_items 212480.
I0302 19:01:30.342259 22699590365312 run.py:483] Algo bellman_ford step 6640 current loss 0.308036, current_train_items 212512.
I0302 19:01:30.358515 22699590365312 run.py:483] Algo bellman_ford step 6641 current loss 0.421023, current_train_items 212544.
I0302 19:01:30.382898 22699590365312 run.py:483] Algo bellman_ford step 6642 current loss 0.635820, current_train_items 212576.
I0302 19:01:30.412981 22699590365312 run.py:483] Algo bellman_ford step 6643 current loss 0.638845, current_train_items 212608.
I0302 19:01:30.446955 22699590365312 run.py:483] Algo bellman_ford step 6644 current loss 0.757909, current_train_items 212640.
I0302 19:01:30.466124 22699590365312 run.py:483] Algo bellman_ford step 6645 current loss 0.337617, current_train_items 212672.
I0302 19:01:30.482122 22699590365312 run.py:483] Algo bellman_ford step 6646 current loss 0.419562, current_train_items 212704.
I0302 19:01:30.505299 22699590365312 run.py:483] Algo bellman_ford step 6647 current loss 0.623212, current_train_items 212736.
I0302 19:01:30.536207 22699590365312 run.py:483] Algo bellman_ford step 6648 current loss 0.660381, current_train_items 212768.
I0302 19:01:30.570106 22699590365312 run.py:483] Algo bellman_ford step 6649 current loss 0.673994, current_train_items 212800.
I0302 19:01:30.589679 22699590365312 run.py:483] Algo bellman_ford step 6650 current loss 0.360894, current_train_items 212832.
I0302 19:01:30.597971 22699590365312 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0302 19:01:30.598078 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:01:30.615290 22699590365312 run.py:483] Algo bellman_ford step 6651 current loss 0.434928, current_train_items 212864.
I0302 19:01:30.639982 22699590365312 run.py:483] Algo bellman_ford step 6652 current loss 0.609107, current_train_items 212896.
I0302 19:01:30.671473 22699590365312 run.py:483] Algo bellman_ford step 6653 current loss 0.608885, current_train_items 212928.
I0302 19:01:30.701573 22699590365312 run.py:483] Algo bellman_ford step 6654 current loss 0.534405, current_train_items 212960.
I0302 19:01:30.721597 22699590365312 run.py:483] Algo bellman_ford step 6655 current loss 0.315190, current_train_items 212992.
I0302 19:01:30.737316 22699590365312 run.py:483] Algo bellman_ford step 6656 current loss 0.546940, current_train_items 213024.
I0302 19:01:30.760528 22699590365312 run.py:483] Algo bellman_ford step 6657 current loss 0.532061, current_train_items 213056.
I0302 19:01:30.791681 22699590365312 run.py:483] Algo bellman_ford step 6658 current loss 0.751956, current_train_items 213088.
I0302 19:01:30.826606 22699590365312 run.py:483] Algo bellman_ford step 6659 current loss 0.843296, current_train_items 213120.
I0302 19:01:30.846764 22699590365312 run.py:483] Algo bellman_ford step 6660 current loss 0.350450, current_train_items 213152.
I0302 19:01:30.863103 22699590365312 run.py:483] Algo bellman_ford step 6661 current loss 0.465395, current_train_items 213184.
I0302 19:01:30.887448 22699590365312 run.py:483] Algo bellman_ford step 6662 current loss 0.665177, current_train_items 213216.
I0302 19:01:30.917220 22699590365312 run.py:483] Algo bellman_ford step 6663 current loss 0.616202, current_train_items 213248.
I0302 19:01:30.949932 22699590365312 run.py:483] Algo bellman_ford step 6664 current loss 0.659101, current_train_items 213280.
I0302 19:01:30.969560 22699590365312 run.py:483] Algo bellman_ford step 6665 current loss 0.271509, current_train_items 213312.
I0302 19:01:30.985637 22699590365312 run.py:483] Algo bellman_ford step 6666 current loss 0.455275, current_train_items 213344.
I0302 19:01:31.011027 22699590365312 run.py:483] Algo bellman_ford step 6667 current loss 0.611420, current_train_items 213376.
I0302 19:01:31.042951 22699590365312 run.py:483] Algo bellman_ford step 6668 current loss 0.684397, current_train_items 213408.
I0302 19:01:31.077907 22699590365312 run.py:483] Algo bellman_ford step 6669 current loss 1.161316, current_train_items 213440.
I0302 19:01:31.097992 22699590365312 run.py:483] Algo bellman_ford step 6670 current loss 0.294214, current_train_items 213472.
I0302 19:01:31.114102 22699590365312 run.py:483] Algo bellman_ford step 6671 current loss 0.412821, current_train_items 213504.
I0302 19:01:31.137708 22699590365312 run.py:483] Algo bellman_ford step 6672 current loss 0.620975, current_train_items 213536.
I0302 19:01:31.169347 22699590365312 run.py:483] Algo bellman_ford step 6673 current loss 0.729622, current_train_items 213568.
I0302 19:01:31.201420 22699590365312 run.py:483] Algo bellman_ford step 6674 current loss 0.720838, current_train_items 213600.
I0302 19:01:31.221316 22699590365312 run.py:483] Algo bellman_ford step 6675 current loss 0.406995, current_train_items 213632.
I0302 19:01:31.236859 22699590365312 run.py:483] Algo bellman_ford step 6676 current loss 0.436493, current_train_items 213664.
I0302 19:01:31.260558 22699590365312 run.py:483] Algo bellman_ford step 6677 current loss 0.591427, current_train_items 213696.
I0302 19:01:31.292382 22699590365312 run.py:483] Algo bellman_ford step 6678 current loss 0.780395, current_train_items 213728.
I0302 19:01:31.327777 22699590365312 run.py:483] Algo bellman_ford step 6679 current loss 0.891705, current_train_items 213760.
I0302 19:01:31.347419 22699590365312 run.py:483] Algo bellman_ford step 6680 current loss 0.289296, current_train_items 213792.
I0302 19:01:31.363565 22699590365312 run.py:483] Algo bellman_ford step 6681 current loss 0.475828, current_train_items 213824.
I0302 19:01:31.387829 22699590365312 run.py:483] Algo bellman_ford step 6682 current loss 0.654664, current_train_items 213856.
I0302 19:01:31.418161 22699590365312 run.py:483] Algo bellman_ford step 6683 current loss 0.800033, current_train_items 213888.
I0302 19:01:31.453836 22699590365312 run.py:483] Algo bellman_ford step 6684 current loss 0.943178, current_train_items 213920.
I0302 19:01:31.473827 22699590365312 run.py:483] Algo bellman_ford step 6685 current loss 0.300717, current_train_items 213952.
I0302 19:01:31.489833 22699590365312 run.py:483] Algo bellman_ford step 6686 current loss 0.483726, current_train_items 213984.
I0302 19:01:31.513173 22699590365312 run.py:483] Algo bellman_ford step 6687 current loss 0.579875, current_train_items 214016.
I0302 19:01:31.544412 22699590365312 run.py:483] Algo bellman_ford step 6688 current loss 0.662368, current_train_items 214048.
I0302 19:01:31.577385 22699590365312 run.py:483] Algo bellman_ford step 6689 current loss 0.845232, current_train_items 214080.
I0302 19:01:31.597213 22699590365312 run.py:483] Algo bellman_ford step 6690 current loss 0.264060, current_train_items 214112.
I0302 19:01:31.612960 22699590365312 run.py:483] Algo bellman_ford step 6691 current loss 0.413325, current_train_items 214144.
I0302 19:01:31.636883 22699590365312 run.py:483] Algo bellman_ford step 6692 current loss 0.699255, current_train_items 214176.
I0302 19:01:31.667873 22699590365312 run.py:483] Algo bellman_ford step 6693 current loss 0.604271, current_train_items 214208.
I0302 19:01:31.701886 22699590365312 run.py:483] Algo bellman_ford step 6694 current loss 0.634155, current_train_items 214240.
I0302 19:01:31.721590 22699590365312 run.py:483] Algo bellman_ford step 6695 current loss 0.239243, current_train_items 214272.
I0302 19:01:31.737836 22699590365312 run.py:483] Algo bellman_ford step 6696 current loss 0.519426, current_train_items 214304.
I0302 19:01:31.761366 22699590365312 run.py:483] Algo bellman_ford step 6697 current loss 0.635120, current_train_items 214336.
I0302 19:01:31.793070 22699590365312 run.py:483] Algo bellman_ford step 6698 current loss 0.650278, current_train_items 214368.
I0302 19:01:31.827749 22699590365312 run.py:483] Algo bellman_ford step 6699 current loss 0.731433, current_train_items 214400.
I0302 19:01:31.847244 22699590365312 run.py:483] Algo bellman_ford step 6700 current loss 0.414040, current_train_items 214432.
I0302 19:01:31.855015 22699590365312 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0302 19:01:31.855119 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 19:01:31.872318 22699590365312 run.py:483] Algo bellman_ford step 6701 current loss 0.510113, current_train_items 214464.
I0302 19:01:31.896084 22699590365312 run.py:483] Algo bellman_ford step 6702 current loss 0.599151, current_train_items 214496.
I0302 19:01:31.927763 22699590365312 run.py:483] Algo bellman_ford step 6703 current loss 0.651785, current_train_items 214528.
I0302 19:01:31.960741 22699590365312 run.py:483] Algo bellman_ford step 6704 current loss 0.652979, current_train_items 214560.
I0302 19:01:31.980569 22699590365312 run.py:483] Algo bellman_ford step 6705 current loss 0.256246, current_train_items 214592.
I0302 19:01:31.996587 22699590365312 run.py:483] Algo bellman_ford step 6706 current loss 0.430217, current_train_items 214624.
I0302 19:01:32.021260 22699590365312 run.py:483] Algo bellman_ford step 6707 current loss 0.571543, current_train_items 214656.
I0302 19:01:32.052420 22699590365312 run.py:483] Algo bellman_ford step 6708 current loss 0.549132, current_train_items 214688.
I0302 19:01:32.086846 22699590365312 run.py:483] Algo bellman_ford step 6709 current loss 0.745089, current_train_items 214720.
I0302 19:01:32.106629 22699590365312 run.py:483] Algo bellman_ford step 6710 current loss 0.324326, current_train_items 214752.
I0302 19:01:32.122988 22699590365312 run.py:483] Algo bellman_ford step 6711 current loss 0.446342, current_train_items 214784.
I0302 19:01:32.145677 22699590365312 run.py:483] Algo bellman_ford step 6712 current loss 0.534199, current_train_items 214816.
I0302 19:01:32.178091 22699590365312 run.py:483] Algo bellman_ford step 6713 current loss 0.677514, current_train_items 214848.
I0302 19:01:32.211331 22699590365312 run.py:483] Algo bellman_ford step 6714 current loss 0.635610, current_train_items 214880.
I0302 19:01:32.230823 22699590365312 run.py:483] Algo bellman_ford step 6715 current loss 0.274025, current_train_items 214912.
I0302 19:01:32.246913 22699590365312 run.py:483] Algo bellman_ford step 6716 current loss 0.442560, current_train_items 214944.
I0302 19:01:32.270642 22699590365312 run.py:483] Algo bellman_ford step 6717 current loss 0.547246, current_train_items 214976.
I0302 19:01:32.301513 22699590365312 run.py:483] Algo bellman_ford step 6718 current loss 0.622347, current_train_items 215008.
I0302 19:01:32.336611 22699590365312 run.py:483] Algo bellman_ford step 6719 current loss 0.794206, current_train_items 215040.
I0302 19:01:32.355907 22699590365312 run.py:483] Algo bellman_ford step 6720 current loss 0.270059, current_train_items 215072.
I0302 19:01:32.371990 22699590365312 run.py:483] Algo bellman_ford step 6721 current loss 0.428737, current_train_items 215104.
I0302 19:01:32.394562 22699590365312 run.py:483] Algo bellman_ford step 6722 current loss 0.534074, current_train_items 215136.
I0302 19:01:32.426287 22699590365312 run.py:483] Algo bellman_ford step 6723 current loss 0.692573, current_train_items 215168.
I0302 19:01:32.458620 22699590365312 run.py:483] Algo bellman_ford step 6724 current loss 0.626644, current_train_items 215200.
I0302 19:01:32.478011 22699590365312 run.py:483] Algo bellman_ford step 6725 current loss 0.225807, current_train_items 215232.
I0302 19:01:32.494149 22699590365312 run.py:483] Algo bellman_ford step 6726 current loss 0.459552, current_train_items 215264.
I0302 19:01:32.517295 22699590365312 run.py:483] Algo bellman_ford step 6727 current loss 0.524621, current_train_items 215296.
I0302 19:01:32.547921 22699590365312 run.py:483] Algo bellman_ford step 6728 current loss 0.649881, current_train_items 215328.
I0302 19:01:32.580084 22699590365312 run.py:483] Algo bellman_ford step 6729 current loss 0.662117, current_train_items 215360.
I0302 19:01:32.599509 22699590365312 run.py:483] Algo bellman_ford step 6730 current loss 0.292282, current_train_items 215392.
I0302 19:01:32.615376 22699590365312 run.py:483] Algo bellman_ford step 6731 current loss 0.459879, current_train_items 215424.
I0302 19:01:32.639573 22699590365312 run.py:483] Algo bellman_ford step 6732 current loss 0.659573, current_train_items 215456.
I0302 19:01:32.668752 22699590365312 run.py:483] Algo bellman_ford step 6733 current loss 0.611314, current_train_items 215488.
I0302 19:01:32.700312 22699590365312 run.py:483] Algo bellman_ford step 6734 current loss 0.681577, current_train_items 215520.
I0302 19:01:32.719768 22699590365312 run.py:483] Algo bellman_ford step 6735 current loss 0.264501, current_train_items 215552.
I0302 19:01:32.736017 22699590365312 run.py:483] Algo bellman_ford step 6736 current loss 0.468477, current_train_items 215584.
I0302 19:01:32.760247 22699590365312 run.py:483] Algo bellman_ford step 6737 current loss 0.565488, current_train_items 215616.
I0302 19:01:32.790147 22699590365312 run.py:483] Algo bellman_ford step 6738 current loss 0.587418, current_train_items 215648.
I0302 19:01:32.825190 22699590365312 run.py:483] Algo bellman_ford step 6739 current loss 0.707062, current_train_items 215680.
I0302 19:01:32.844672 22699590365312 run.py:483] Algo bellman_ford step 6740 current loss 0.281011, current_train_items 215712.
I0302 19:01:32.860939 22699590365312 run.py:483] Algo bellman_ford step 6741 current loss 0.482719, current_train_items 215744.
I0302 19:01:32.885235 22699590365312 run.py:483] Algo bellman_ford step 6742 current loss 0.567620, current_train_items 215776.
I0302 19:01:32.915243 22699590365312 run.py:483] Algo bellman_ford step 6743 current loss 0.624208, current_train_items 215808.
I0302 19:01:32.949635 22699590365312 run.py:483] Algo bellman_ford step 6744 current loss 0.855278, current_train_items 215840.
I0302 19:01:32.969321 22699590365312 run.py:483] Algo bellman_ford step 6745 current loss 0.299740, current_train_items 215872.
I0302 19:01:32.985482 22699590365312 run.py:483] Algo bellman_ford step 6746 current loss 0.512674, current_train_items 215904.
I0302 19:01:33.009071 22699590365312 run.py:483] Algo bellman_ford step 6747 current loss 0.532464, current_train_items 215936.
I0302 19:01:33.039125 22699590365312 run.py:483] Algo bellman_ford step 6748 current loss 0.589928, current_train_items 215968.
I0302 19:01:33.072757 22699590365312 run.py:483] Algo bellman_ford step 6749 current loss 0.814904, current_train_items 216000.
I0302 19:01:33.092206 22699590365312 run.py:483] Algo bellman_ford step 6750 current loss 0.362855, current_train_items 216032.
I0302 19:01:33.100186 22699590365312 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0302 19:01:33.100294 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:01:33.117161 22699590365312 run.py:483] Algo bellman_ford step 6751 current loss 0.398834, current_train_items 216064.
I0302 19:01:33.141725 22699590365312 run.py:483] Algo bellman_ford step 6752 current loss 0.624982, current_train_items 216096.
I0302 19:01:33.173802 22699590365312 run.py:483] Algo bellman_ford step 6753 current loss 0.675217, current_train_items 216128.
I0302 19:01:33.208397 22699590365312 run.py:483] Algo bellman_ford step 6754 current loss 0.714328, current_train_items 216160.
I0302 19:01:33.228613 22699590365312 run.py:483] Algo bellman_ford step 6755 current loss 0.326210, current_train_items 216192.
I0302 19:01:33.244927 22699590365312 run.py:483] Algo bellman_ford step 6756 current loss 0.525835, current_train_items 216224.
I0302 19:01:33.268247 22699590365312 run.py:483] Algo bellman_ford step 6757 current loss 0.497920, current_train_items 216256.
I0302 19:01:33.299476 22699590365312 run.py:483] Algo bellman_ford step 6758 current loss 0.699599, current_train_items 216288.
I0302 19:01:33.334249 22699590365312 run.py:483] Algo bellman_ford step 6759 current loss 0.842128, current_train_items 216320.
I0302 19:01:33.354222 22699590365312 run.py:483] Algo bellman_ford step 6760 current loss 0.282368, current_train_items 216352.
I0302 19:01:33.370715 22699590365312 run.py:483] Algo bellman_ford step 6761 current loss 0.354351, current_train_items 216384.
I0302 19:01:33.393949 22699590365312 run.py:483] Algo bellman_ford step 6762 current loss 0.509521, current_train_items 216416.
I0302 19:01:33.425227 22699590365312 run.py:483] Algo bellman_ford step 6763 current loss 0.594456, current_train_items 216448.
I0302 19:01:33.458069 22699590365312 run.py:483] Algo bellman_ford step 6764 current loss 0.718497, current_train_items 216480.
I0302 19:01:33.477945 22699590365312 run.py:483] Algo bellman_ford step 6765 current loss 0.262293, current_train_items 216512.
I0302 19:01:33.494334 22699590365312 run.py:483] Algo bellman_ford step 6766 current loss 0.467562, current_train_items 216544.
I0302 19:01:33.517806 22699590365312 run.py:483] Algo bellman_ford step 6767 current loss 0.530308, current_train_items 216576.
I0302 19:01:33.547673 22699590365312 run.py:483] Algo bellman_ford step 6768 current loss 0.576448, current_train_items 216608.
I0302 19:01:33.580684 22699590365312 run.py:483] Algo bellman_ford step 6769 current loss 0.736843, current_train_items 216640.
I0302 19:01:33.600544 22699590365312 run.py:483] Algo bellman_ford step 6770 current loss 0.235659, current_train_items 216672.
I0302 19:01:33.616580 22699590365312 run.py:483] Algo bellman_ford step 6771 current loss 0.428850, current_train_items 216704.
I0302 19:01:33.639131 22699590365312 run.py:483] Algo bellman_ford step 6772 current loss 0.511338, current_train_items 216736.
I0302 19:01:33.669896 22699590365312 run.py:483] Algo bellman_ford step 6773 current loss 0.637729, current_train_items 216768.
I0302 19:01:33.701644 22699590365312 run.py:483] Algo bellman_ford step 6774 current loss 0.661052, current_train_items 216800.
I0302 19:01:33.721576 22699590365312 run.py:483] Algo bellman_ford step 6775 current loss 0.276318, current_train_items 216832.
I0302 19:01:33.737670 22699590365312 run.py:483] Algo bellman_ford step 6776 current loss 0.424642, current_train_items 216864.
I0302 19:01:33.760315 22699590365312 run.py:483] Algo bellman_ford step 6777 current loss 0.535393, current_train_items 216896.
I0302 19:01:33.792360 22699590365312 run.py:483] Algo bellman_ford step 6778 current loss 0.775465, current_train_items 216928.
I0302 19:01:33.825104 22699590365312 run.py:483] Algo bellman_ford step 6779 current loss 0.812724, current_train_items 216960.
I0302 19:01:33.844686 22699590365312 run.py:483] Algo bellman_ford step 6780 current loss 0.287705, current_train_items 216992.
I0302 19:01:33.860600 22699590365312 run.py:483] Algo bellman_ford step 6781 current loss 0.478488, current_train_items 217024.
I0302 19:01:33.882997 22699590365312 run.py:483] Algo bellman_ford step 6782 current loss 0.633298, current_train_items 217056.
I0302 19:01:33.915387 22699590365312 run.py:483] Algo bellman_ford step 6783 current loss 0.650265, current_train_items 217088.
I0302 19:01:33.948553 22699590365312 run.py:483] Algo bellman_ford step 6784 current loss 0.837716, current_train_items 217120.
I0302 19:01:33.968362 22699590365312 run.py:483] Algo bellman_ford step 6785 current loss 0.234585, current_train_items 217152.
I0302 19:01:33.984102 22699590365312 run.py:483] Algo bellman_ford step 6786 current loss 0.465091, current_train_items 217184.
I0302 19:01:34.006639 22699590365312 run.py:483] Algo bellman_ford step 6787 current loss 0.549352, current_train_items 217216.
I0302 19:01:34.036100 22699590365312 run.py:483] Algo bellman_ford step 6788 current loss 0.647923, current_train_items 217248.
I0302 19:01:34.069871 22699590365312 run.py:483] Algo bellman_ford step 6789 current loss 0.774134, current_train_items 217280.
I0302 19:01:34.089708 22699590365312 run.py:483] Algo bellman_ford step 6790 current loss 0.338744, current_train_items 217312.
I0302 19:01:34.105706 22699590365312 run.py:483] Algo bellman_ford step 6791 current loss 0.444813, current_train_items 217344.
I0302 19:01:34.129274 22699590365312 run.py:483] Algo bellman_ford step 6792 current loss 0.519977, current_train_items 217376.
I0302 19:01:34.159151 22699590365312 run.py:483] Algo bellman_ford step 6793 current loss 0.569488, current_train_items 217408.
I0302 19:01:34.192351 22699590365312 run.py:483] Algo bellman_ford step 6794 current loss 0.685349, current_train_items 217440.
I0302 19:01:34.211749 22699590365312 run.py:483] Algo bellman_ford step 6795 current loss 0.268704, current_train_items 217472.
I0302 19:01:34.227637 22699590365312 run.py:483] Algo bellman_ford step 6796 current loss 0.373077, current_train_items 217504.
I0302 19:01:34.251906 22699590365312 run.py:483] Algo bellman_ford step 6797 current loss 0.559759, current_train_items 217536.
I0302 19:01:34.282152 22699590365312 run.py:483] Algo bellman_ford step 6798 current loss 0.622031, current_train_items 217568.
I0302 19:01:34.315345 22699590365312 run.py:483] Algo bellman_ford step 6799 current loss 0.697456, current_train_items 217600.
I0302 19:01:34.335146 22699590365312 run.py:483] Algo bellman_ford step 6800 current loss 0.349401, current_train_items 217632.
I0302 19:01:34.342909 22699590365312 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0302 19:01:34.343014 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:01:34.359581 22699590365312 run.py:483] Algo bellman_ford step 6801 current loss 0.398411, current_train_items 217664.
I0302 19:01:34.383752 22699590365312 run.py:483] Algo bellman_ford step 6802 current loss 0.584219, current_train_items 217696.
I0302 19:01:34.416423 22699590365312 run.py:483] Algo bellman_ford step 6803 current loss 0.677669, current_train_items 217728.
I0302 19:01:34.450235 22699590365312 run.py:483] Algo bellman_ford step 6804 current loss 0.659372, current_train_items 217760.
I0302 19:01:34.470085 22699590365312 run.py:483] Algo bellman_ford step 6805 current loss 0.260832, current_train_items 217792.
I0302 19:01:34.486164 22699590365312 run.py:483] Algo bellman_ford step 6806 current loss 0.330227, current_train_items 217824.
I0302 19:01:34.510249 22699590365312 run.py:483] Algo bellman_ford step 6807 current loss 0.617180, current_train_items 217856.
I0302 19:01:34.540165 22699590365312 run.py:483] Algo bellman_ford step 6808 current loss 0.571714, current_train_items 217888.
I0302 19:01:34.573123 22699590365312 run.py:483] Algo bellman_ford step 6809 current loss 0.636484, current_train_items 217920.
I0302 19:01:34.592648 22699590365312 run.py:483] Algo bellman_ford step 6810 current loss 0.342994, current_train_items 217952.
I0302 19:01:34.608289 22699590365312 run.py:483] Algo bellman_ford step 6811 current loss 0.356067, current_train_items 217984.
I0302 19:01:34.632812 22699590365312 run.py:483] Algo bellman_ford step 6812 current loss 0.622451, current_train_items 218016.
I0302 19:01:34.663162 22699590365312 run.py:483] Algo bellman_ford step 6813 current loss 0.627770, current_train_items 218048.
I0302 19:01:34.694403 22699590365312 run.py:483] Algo bellman_ford step 6814 current loss 0.673628, current_train_items 218080.
I0302 19:01:34.714138 22699590365312 run.py:483] Algo bellman_ford step 6815 current loss 0.326927, current_train_items 218112.
I0302 19:01:34.730258 22699590365312 run.py:483] Algo bellman_ford step 6816 current loss 0.416979, current_train_items 218144.
I0302 19:01:34.752400 22699590365312 run.py:483] Algo bellman_ford step 6817 current loss 0.575399, current_train_items 218176.
I0302 19:01:34.783204 22699590365312 run.py:483] Algo bellman_ford step 6818 current loss 0.624998, current_train_items 218208.
I0302 19:01:34.818333 22699590365312 run.py:483] Algo bellman_ford step 6819 current loss 0.833237, current_train_items 218240.
I0302 19:01:34.837896 22699590365312 run.py:483] Algo bellman_ford step 6820 current loss 0.264705, current_train_items 218272.
I0302 19:01:34.853878 22699590365312 run.py:483] Algo bellman_ford step 6821 current loss 0.407578, current_train_items 218304.
I0302 19:01:34.877001 22699590365312 run.py:483] Algo bellman_ford step 6822 current loss 0.574943, current_train_items 218336.
I0302 19:01:34.908214 22699590365312 run.py:483] Algo bellman_ford step 6823 current loss 0.747510, current_train_items 218368.
I0302 19:01:34.942886 22699590365312 run.py:483] Algo bellman_ford step 6824 current loss 0.696552, current_train_items 218400.
I0302 19:01:34.962747 22699590365312 run.py:483] Algo bellman_ford step 6825 current loss 0.287854, current_train_items 218432.
I0302 19:01:34.978947 22699590365312 run.py:483] Algo bellman_ford step 6826 current loss 0.414839, current_train_items 218464.
I0302 19:01:35.002279 22699590365312 run.py:483] Algo bellman_ford step 6827 current loss 0.601763, current_train_items 218496.
I0302 19:01:35.032594 22699590365312 run.py:483] Algo bellman_ford step 6828 current loss 0.642039, current_train_items 218528.
I0302 19:01:35.068176 22699590365312 run.py:483] Algo bellman_ford step 6829 current loss 0.797075, current_train_items 218560.
I0302 19:01:35.088014 22699590365312 run.py:483] Algo bellman_ford step 6830 current loss 0.383238, current_train_items 218592.
I0302 19:01:35.103857 22699590365312 run.py:483] Algo bellman_ford step 6831 current loss 0.415743, current_train_items 218624.
I0302 19:01:35.128908 22699590365312 run.py:483] Algo bellman_ford step 6832 current loss 0.664363, current_train_items 218656.
I0302 19:01:35.159245 22699590365312 run.py:483] Algo bellman_ford step 6833 current loss 0.627600, current_train_items 218688.
I0302 19:01:35.192436 22699590365312 run.py:483] Algo bellman_ford step 6834 current loss 0.747877, current_train_items 218720.
I0302 19:01:35.212172 22699590365312 run.py:483] Algo bellman_ford step 6835 current loss 0.305226, current_train_items 218752.
I0302 19:01:35.228622 22699590365312 run.py:483] Algo bellman_ford step 6836 current loss 0.429824, current_train_items 218784.
I0302 19:01:35.251946 22699590365312 run.py:483] Algo bellman_ford step 6837 current loss 0.545479, current_train_items 218816.
I0302 19:01:35.283710 22699590365312 run.py:483] Algo bellman_ford step 6838 current loss 0.653441, current_train_items 218848.
I0302 19:01:35.315523 22699590365312 run.py:483] Algo bellman_ford step 6839 current loss 0.689081, current_train_items 218880.
I0302 19:01:35.334688 22699590365312 run.py:483] Algo bellman_ford step 6840 current loss 0.244523, current_train_items 218912.
I0302 19:01:35.351122 22699590365312 run.py:483] Algo bellman_ford step 6841 current loss 0.491844, current_train_items 218944.
I0302 19:01:35.375290 22699590365312 run.py:483] Algo bellman_ford step 6842 current loss 0.571570, current_train_items 218976.
I0302 19:01:35.405098 22699590365312 run.py:483] Algo bellman_ford step 6843 current loss 0.539338, current_train_items 219008.
I0302 19:01:35.438492 22699590365312 run.py:483] Algo bellman_ford step 6844 current loss 0.799201, current_train_items 219040.
I0302 19:01:35.457922 22699590365312 run.py:483] Algo bellman_ford step 6845 current loss 0.324965, current_train_items 219072.
I0302 19:01:35.474619 22699590365312 run.py:483] Algo bellman_ford step 6846 current loss 0.430619, current_train_items 219104.
I0302 19:01:35.498714 22699590365312 run.py:483] Algo bellman_ford step 6847 current loss 0.601529, current_train_items 219136.
I0302 19:01:35.530012 22699590365312 run.py:483] Algo bellman_ford step 6848 current loss 0.596045, current_train_items 219168.
I0302 19:01:35.563966 22699590365312 run.py:483] Algo bellman_ford step 6849 current loss 0.696213, current_train_items 219200.
I0302 19:01:35.583379 22699590365312 run.py:483] Algo bellman_ford step 6850 current loss 0.345458, current_train_items 219232.
I0302 19:01:35.591569 22699590365312 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.9091796875, 'score': 0.9091796875, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0302 19:01:35.591675 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.909, val scores are: bellman_ford: 0.909
I0302 19:01:35.608711 22699590365312 run.py:483] Algo bellman_ford step 6851 current loss 0.469543, current_train_items 219264.
I0302 19:01:35.633312 22699590365312 run.py:483] Algo bellman_ford step 6852 current loss 0.679622, current_train_items 219296.
I0302 19:01:35.664761 22699590365312 run.py:483] Algo bellman_ford step 6853 current loss 0.614092, current_train_items 219328.
I0302 19:01:35.699105 22699590365312 run.py:483] Algo bellman_ford step 6854 current loss 0.727934, current_train_items 219360.
I0302 19:01:35.719244 22699590365312 run.py:483] Algo bellman_ford step 6855 current loss 0.301982, current_train_items 219392.
I0302 19:01:35.734875 22699590365312 run.py:483] Algo bellman_ford step 6856 current loss 0.391607, current_train_items 219424.
I0302 19:01:35.758438 22699590365312 run.py:483] Algo bellman_ford step 6857 current loss 0.630070, current_train_items 219456.
I0302 19:01:35.788794 22699590365312 run.py:483] Algo bellman_ford step 6858 current loss 0.624682, current_train_items 219488.
I0302 19:01:35.822384 22699590365312 run.py:483] Algo bellman_ford step 6859 current loss 0.704336, current_train_items 219520.
I0302 19:01:35.842057 22699590365312 run.py:483] Algo bellman_ford step 6860 current loss 0.343543, current_train_items 219552.
I0302 19:01:35.858107 22699590365312 run.py:483] Algo bellman_ford step 6861 current loss 0.420755, current_train_items 219584.
I0302 19:01:35.880715 22699590365312 run.py:483] Algo bellman_ford step 6862 current loss 0.511057, current_train_items 219616.
I0302 19:01:35.911029 22699590365312 run.py:483] Algo bellman_ford step 6863 current loss 0.519318, current_train_items 219648.
I0302 19:01:35.943126 22699590365312 run.py:483] Algo bellman_ford step 6864 current loss 0.713711, current_train_items 219680.
I0302 19:01:35.962873 22699590365312 run.py:483] Algo bellman_ford step 6865 current loss 0.303284, current_train_items 219712.
I0302 19:01:35.978976 22699590365312 run.py:483] Algo bellman_ford step 6866 current loss 0.397493, current_train_items 219744.
I0302 19:01:36.003522 22699590365312 run.py:483] Algo bellman_ford step 6867 current loss 0.581132, current_train_items 219776.
I0302 19:01:36.034718 22699590365312 run.py:483] Algo bellman_ford step 6868 current loss 0.684750, current_train_items 219808.
I0302 19:01:36.070589 22699590365312 run.py:483] Algo bellman_ford step 6869 current loss 0.837425, current_train_items 219840.
I0302 19:01:36.090595 22699590365312 run.py:483] Algo bellman_ford step 6870 current loss 0.320900, current_train_items 219872.
I0302 19:01:36.106996 22699590365312 run.py:483] Algo bellman_ford step 6871 current loss 0.411823, current_train_items 219904.
I0302 19:01:36.130074 22699590365312 run.py:483] Algo bellman_ford step 6872 current loss 0.582708, current_train_items 219936.
I0302 19:01:36.159484 22699590365312 run.py:483] Algo bellman_ford step 6873 current loss 0.578095, current_train_items 219968.
I0302 19:01:36.192825 22699590365312 run.py:483] Algo bellman_ford step 6874 current loss 0.693173, current_train_items 220000.
I0302 19:01:36.212726 22699590365312 run.py:483] Algo bellman_ford step 6875 current loss 0.278888, current_train_items 220032.
I0302 19:01:36.228554 22699590365312 run.py:483] Algo bellman_ford step 6876 current loss 0.439765, current_train_items 220064.
I0302 19:01:36.251887 22699590365312 run.py:483] Algo bellman_ford step 6877 current loss 0.718302, current_train_items 220096.
I0302 19:01:36.282730 22699590365312 run.py:483] Algo bellman_ford step 6878 current loss 0.682158, current_train_items 220128.
I0302 19:01:36.316871 22699590365312 run.py:483] Algo bellman_ford step 6879 current loss 0.736050, current_train_items 220160.
I0302 19:01:36.336528 22699590365312 run.py:483] Algo bellman_ford step 6880 current loss 0.332572, current_train_items 220192.
I0302 19:01:36.352270 22699590365312 run.py:483] Algo bellman_ford step 6881 current loss 0.442410, current_train_items 220224.
I0302 19:01:36.376287 22699590365312 run.py:483] Algo bellman_ford step 6882 current loss 0.549326, current_train_items 220256.
I0302 19:01:36.405652 22699590365312 run.py:483] Algo bellman_ford step 6883 current loss 0.571566, current_train_items 220288.
I0302 19:01:36.438390 22699590365312 run.py:483] Algo bellman_ford step 6884 current loss 0.694102, current_train_items 220320.
I0302 19:01:36.458235 22699590365312 run.py:483] Algo bellman_ford step 6885 current loss 0.286876, current_train_items 220352.
I0302 19:01:36.474452 22699590365312 run.py:483] Algo bellman_ford step 6886 current loss 0.425298, current_train_items 220384.
I0302 19:01:36.498384 22699590365312 run.py:483] Algo bellman_ford step 6887 current loss 0.548958, current_train_items 220416.
I0302 19:01:36.530216 22699590365312 run.py:483] Algo bellman_ford step 6888 current loss 0.623106, current_train_items 220448.
I0302 19:01:36.563990 22699590365312 run.py:483] Algo bellman_ford step 6889 current loss 0.676585, current_train_items 220480.
I0302 19:01:36.584073 22699590365312 run.py:483] Algo bellman_ford step 6890 current loss 0.266168, current_train_items 220512.
I0302 19:01:36.600503 22699590365312 run.py:483] Algo bellman_ford step 6891 current loss 0.466765, current_train_items 220544.
I0302 19:01:36.623987 22699590365312 run.py:483] Algo bellman_ford step 6892 current loss 0.600203, current_train_items 220576.
I0302 19:01:36.655197 22699590365312 run.py:483] Algo bellman_ford step 6893 current loss 0.710313, current_train_items 220608.
I0302 19:01:36.687638 22699590365312 run.py:483] Algo bellman_ford step 6894 current loss 0.754524, current_train_items 220640.
I0302 19:01:36.707485 22699590365312 run.py:483] Algo bellman_ford step 6895 current loss 0.273147, current_train_items 220672.
I0302 19:01:36.723495 22699590365312 run.py:483] Algo bellman_ford step 6896 current loss 0.427835, current_train_items 220704.
I0302 19:01:36.747696 22699590365312 run.py:483] Algo bellman_ford step 6897 current loss 0.652068, current_train_items 220736.
I0302 19:01:36.777481 22699590365312 run.py:483] Algo bellman_ford step 6898 current loss 0.517179, current_train_items 220768.
I0302 19:01:36.812051 22699590365312 run.py:483] Algo bellman_ford step 6899 current loss 0.750844, current_train_items 220800.
I0302 19:01:36.832140 22699590365312 run.py:483] Algo bellman_ford step 6900 current loss 0.303647, current_train_items 220832.
I0302 19:01:36.840035 22699590365312 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0302 19:01:36.840140 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0302 19:01:36.856671 22699590365312 run.py:483] Algo bellman_ford step 6901 current loss 0.393862, current_train_items 220864.
I0302 19:01:36.880286 22699590365312 run.py:483] Algo bellman_ford step 6902 current loss 0.620746, current_train_items 220896.
I0302 19:01:36.912456 22699590365312 run.py:483] Algo bellman_ford step 6903 current loss 0.635528, current_train_items 220928.
I0302 19:01:36.947872 22699590365312 run.py:483] Algo bellman_ford step 6904 current loss 0.635772, current_train_items 220960.
I0302 19:01:36.967715 22699590365312 run.py:483] Algo bellman_ford step 6905 current loss 0.331978, current_train_items 220992.
I0302 19:01:36.983159 22699590365312 run.py:483] Algo bellman_ford step 6906 current loss 0.394242, current_train_items 221024.
I0302 19:01:37.006663 22699590365312 run.py:483] Algo bellman_ford step 6907 current loss 0.512152, current_train_items 221056.
I0302 19:01:37.038558 22699590365312 run.py:483] Algo bellman_ford step 6908 current loss 0.675032, current_train_items 221088.
I0302 19:01:37.073255 22699590365312 run.py:483] Algo bellman_ford step 6909 current loss 0.841586, current_train_items 221120.
I0302 19:01:37.092932 22699590365312 run.py:483] Algo bellman_ford step 6910 current loss 0.242161, current_train_items 221152.
I0302 19:01:37.109623 22699590365312 run.py:483] Algo bellman_ford step 6911 current loss 0.513472, current_train_items 221184.
I0302 19:01:37.131962 22699590365312 run.py:483] Algo bellman_ford step 6912 current loss 0.555041, current_train_items 221216.
I0302 19:01:37.163644 22699590365312 run.py:483] Algo bellman_ford step 6913 current loss 0.554545, current_train_items 221248.
I0302 19:01:37.198560 22699590365312 run.py:483] Algo bellman_ford step 6914 current loss 0.767592, current_train_items 221280.
I0302 19:01:37.217898 22699590365312 run.py:483] Algo bellman_ford step 6915 current loss 0.355238, current_train_items 221312.
I0302 19:01:37.233501 22699590365312 run.py:483] Algo bellman_ford step 6916 current loss 0.398528, current_train_items 221344.
I0302 19:01:37.258019 22699590365312 run.py:483] Algo bellman_ford step 6917 current loss 0.600661, current_train_items 221376.
I0302 19:01:37.289532 22699590365312 run.py:483] Algo bellman_ford step 6918 current loss 0.635940, current_train_items 221408.
I0302 19:01:37.324818 22699590365312 run.py:483] Algo bellman_ford step 6919 current loss 0.697422, current_train_items 221440.
I0302 19:01:37.344490 22699590365312 run.py:483] Algo bellman_ford step 6920 current loss 0.295241, current_train_items 221472.
I0302 19:01:37.360526 22699590365312 run.py:483] Algo bellman_ford step 6921 current loss 0.428862, current_train_items 221504.
I0302 19:01:37.383550 22699590365312 run.py:483] Algo bellman_ford step 6922 current loss 0.567901, current_train_items 221536.
I0302 19:01:37.415015 22699590365312 run.py:483] Algo bellman_ford step 6923 current loss 0.650013, current_train_items 221568.
I0302 19:01:37.448114 22699590365312 run.py:483] Algo bellman_ford step 6924 current loss 0.669819, current_train_items 221600.
I0302 19:01:37.467813 22699590365312 run.py:483] Algo bellman_ford step 6925 current loss 0.261426, current_train_items 221632.
I0302 19:01:37.483669 22699590365312 run.py:483] Algo bellman_ford step 6926 current loss 0.334596, current_train_items 221664.
I0302 19:01:37.507707 22699590365312 run.py:483] Algo bellman_ford step 6927 current loss 0.634696, current_train_items 221696.
I0302 19:01:37.539024 22699590365312 run.py:483] Algo bellman_ford step 6928 current loss 0.717032, current_train_items 221728.
I0302 19:01:37.572095 22699590365312 run.py:483] Algo bellman_ford step 6929 current loss 0.857341, current_train_items 221760.
I0302 19:01:37.591912 22699590365312 run.py:483] Algo bellman_ford step 6930 current loss 0.290053, current_train_items 221792.
I0302 19:01:37.608207 22699590365312 run.py:483] Algo bellman_ford step 6931 current loss 0.447304, current_train_items 221824.
I0302 19:01:37.631134 22699590365312 run.py:483] Algo bellman_ford step 6932 current loss 0.509243, current_train_items 221856.
I0302 19:01:37.662138 22699590365312 run.py:483] Algo bellman_ford step 6933 current loss 0.642871, current_train_items 221888.
I0302 19:01:37.696939 22699590365312 run.py:483] Algo bellman_ford step 6934 current loss 0.831424, current_train_items 221920.
I0302 19:01:37.716380 22699590365312 run.py:483] Algo bellman_ford step 6935 current loss 0.353943, current_train_items 221952.
I0302 19:01:37.732245 22699590365312 run.py:483] Algo bellman_ford step 6936 current loss 0.421508, current_train_items 221984.
I0302 19:01:37.755749 22699590365312 run.py:483] Algo bellman_ford step 6937 current loss 0.539491, current_train_items 222016.
I0302 19:01:37.786719 22699590365312 run.py:483] Algo bellman_ford step 6938 current loss 0.625480, current_train_items 222048.
I0302 19:01:37.819235 22699590365312 run.py:483] Algo bellman_ford step 6939 current loss 0.609935, current_train_items 222080.
I0302 19:01:37.838790 22699590365312 run.py:483] Algo bellman_ford step 6940 current loss 0.306632, current_train_items 222112.
I0302 19:01:37.855240 22699590365312 run.py:483] Algo bellman_ford step 6941 current loss 0.563684, current_train_items 222144.
I0302 19:01:37.879144 22699590365312 run.py:483] Algo bellman_ford step 6942 current loss 0.660707, current_train_items 222176.
I0302 19:01:37.910293 22699590365312 run.py:483] Algo bellman_ford step 6943 current loss 0.629136, current_train_items 222208.
I0302 19:01:37.944676 22699590365312 run.py:483] Algo bellman_ford step 6944 current loss 0.894182, current_train_items 222240.
I0302 19:01:37.964445 22699590365312 run.py:483] Algo bellman_ford step 6945 current loss 0.296534, current_train_items 222272.
I0302 19:01:37.980737 22699590365312 run.py:483] Algo bellman_ford step 6946 current loss 0.537961, current_train_items 222304.
I0302 19:01:38.004550 22699590365312 run.py:483] Algo bellman_ford step 6947 current loss 0.633098, current_train_items 222336.
I0302 19:01:38.033376 22699590365312 run.py:483] Algo bellman_ford step 6948 current loss 0.539733, current_train_items 222368.
I0302 19:01:38.067545 22699590365312 run.py:483] Algo bellman_ford step 6949 current loss 0.780287, current_train_items 222400.
I0302 19:01:38.087043 22699590365312 run.py:483] Algo bellman_ford step 6950 current loss 0.285917, current_train_items 222432.
I0302 19:01:38.095292 22699590365312 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0302 19:01:38.095397 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 19:01:38.112376 22699590365312 run.py:483] Algo bellman_ford step 6951 current loss 0.477327, current_train_items 222464.
I0302 19:01:38.137024 22699590365312 run.py:483] Algo bellman_ford step 6952 current loss 0.617905, current_train_items 222496.
I0302 19:01:38.167798 22699590365312 run.py:483] Algo bellman_ford step 6953 current loss 0.667757, current_train_items 222528.
I0302 19:01:38.202728 22699590365312 run.py:483] Algo bellman_ford step 6954 current loss 0.789585, current_train_items 222560.
I0302 19:01:38.223134 22699590365312 run.py:483] Algo bellman_ford step 6955 current loss 0.307475, current_train_items 222592.
I0302 19:01:38.239115 22699590365312 run.py:483] Algo bellman_ford step 6956 current loss 0.414802, current_train_items 222624.
I0302 19:01:38.264133 22699590365312 run.py:483] Algo bellman_ford step 6957 current loss 0.661644, current_train_items 222656.
I0302 19:01:38.294666 22699590365312 run.py:483] Algo bellman_ford step 6958 current loss 0.693479, current_train_items 222688.
I0302 19:01:38.326443 22699590365312 run.py:483] Algo bellman_ford step 6959 current loss 0.653583, current_train_items 222720.
I0302 19:01:38.346457 22699590365312 run.py:483] Algo bellman_ford step 6960 current loss 0.274345, current_train_items 222752.
I0302 19:01:38.362958 22699590365312 run.py:483] Algo bellman_ford step 6961 current loss 0.453023, current_train_items 222784.
I0302 19:01:38.385910 22699590365312 run.py:483] Algo bellman_ford step 6962 current loss 0.535111, current_train_items 222816.
I0302 19:01:38.416922 22699590365312 run.py:483] Algo bellman_ford step 6963 current loss 0.739402, current_train_items 222848.
I0302 19:01:38.451954 22699590365312 run.py:483] Algo bellman_ford step 6964 current loss 0.695037, current_train_items 222880.
I0302 19:01:38.471744 22699590365312 run.py:483] Algo bellman_ford step 6965 current loss 0.338787, current_train_items 222912.
I0302 19:01:38.487959 22699590365312 run.py:483] Algo bellman_ford step 6966 current loss 0.454312, current_train_items 222944.
I0302 19:01:38.511551 22699590365312 run.py:483] Algo bellman_ford step 6967 current loss 0.588856, current_train_items 222976.
I0302 19:01:38.541412 22699590365312 run.py:483] Algo bellman_ford step 6968 current loss 0.615812, current_train_items 223008.
I0302 19:01:38.574976 22699590365312 run.py:483] Algo bellman_ford step 6969 current loss 0.794353, current_train_items 223040.
I0302 19:01:38.594835 22699590365312 run.py:483] Algo bellman_ford step 6970 current loss 0.246292, current_train_items 223072.
I0302 19:01:38.611072 22699590365312 run.py:483] Algo bellman_ford step 6971 current loss 0.400267, current_train_items 223104.
I0302 19:01:38.634523 22699590365312 run.py:483] Algo bellman_ford step 6972 current loss 0.585903, current_train_items 223136.
I0302 19:01:38.665120 22699590365312 run.py:483] Algo bellman_ford step 6973 current loss 0.579315, current_train_items 223168.
I0302 19:01:38.699426 22699590365312 run.py:483] Algo bellman_ford step 6974 current loss 0.703137, current_train_items 223200.
I0302 19:01:38.719058 22699590365312 run.py:483] Algo bellman_ford step 6975 current loss 0.347837, current_train_items 223232.
I0302 19:01:38.735318 22699590365312 run.py:483] Algo bellman_ford step 6976 current loss 0.434004, current_train_items 223264.
I0302 19:01:38.758463 22699590365312 run.py:483] Algo bellman_ford step 6977 current loss 0.573328, current_train_items 223296.
I0302 19:01:38.788525 22699590365312 run.py:483] Algo bellman_ford step 6978 current loss 0.625432, current_train_items 223328.
I0302 19:01:38.822771 22699590365312 run.py:483] Algo bellman_ford step 6979 current loss 0.700894, current_train_items 223360.
I0302 19:01:38.842306 22699590365312 run.py:483] Algo bellman_ford step 6980 current loss 0.387400, current_train_items 223392.
I0302 19:01:38.858957 22699590365312 run.py:483] Algo bellman_ford step 6981 current loss 0.447582, current_train_items 223424.
I0302 19:01:38.881410 22699590365312 run.py:483] Algo bellman_ford step 6982 current loss 0.492944, current_train_items 223456.
I0302 19:01:38.913607 22699590365312 run.py:483] Algo bellman_ford step 6983 current loss 0.646055, current_train_items 223488.
I0302 19:01:38.947263 22699590365312 run.py:483] Algo bellman_ford step 6984 current loss 0.613037, current_train_items 223520.
I0302 19:01:38.967070 22699590365312 run.py:483] Algo bellman_ford step 6985 current loss 0.271625, current_train_items 223552.
I0302 19:01:38.983053 22699590365312 run.py:483] Algo bellman_ford step 6986 current loss 0.395523, current_train_items 223584.
I0302 19:01:39.006312 22699590365312 run.py:483] Algo bellman_ford step 6987 current loss 0.618855, current_train_items 223616.
I0302 19:01:39.036940 22699590365312 run.py:483] Algo bellman_ford step 6988 current loss 0.605923, current_train_items 223648.
I0302 19:01:39.072462 22699590365312 run.py:483] Algo bellman_ford step 6989 current loss 0.724593, current_train_items 223680.
I0302 19:01:39.092032 22699590365312 run.py:483] Algo bellman_ford step 6990 current loss 0.270710, current_train_items 223712.
I0302 19:01:39.108036 22699590365312 run.py:483] Algo bellman_ford step 6991 current loss 0.485932, current_train_items 223744.
I0302 19:01:39.131292 22699590365312 run.py:483] Algo bellman_ford step 6992 current loss 0.671923, current_train_items 223776.
I0302 19:01:39.163006 22699590365312 run.py:483] Algo bellman_ford step 6993 current loss 0.683051, current_train_items 223808.
I0302 19:01:39.199312 22699590365312 run.py:483] Algo bellman_ford step 6994 current loss 0.725234, current_train_items 223840.
I0302 19:01:39.218970 22699590365312 run.py:483] Algo bellman_ford step 6995 current loss 0.315516, current_train_items 223872.
I0302 19:01:39.235422 22699590365312 run.py:483] Algo bellman_ford step 6996 current loss 0.385763, current_train_items 223904.
I0302 19:01:39.257421 22699590365312 run.py:483] Algo bellman_ford step 6997 current loss 0.515914, current_train_items 223936.
I0302 19:01:39.287292 22699590365312 run.py:483] Algo bellman_ford step 6998 current loss 0.569075, current_train_items 223968.
I0302 19:01:39.321074 22699590365312 run.py:483] Algo bellman_ford step 6999 current loss 0.994843, current_train_items 224000.
I0302 19:01:39.340794 22699590365312 run.py:483] Algo bellman_ford step 7000 current loss 0.306339, current_train_items 224032.
I0302 19:01:39.348706 22699590365312 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.9091796875, 'score': 0.9091796875, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0302 19:01:39.348810 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.909, val scores are: bellman_ford: 0.909
I0302 19:01:39.365369 22699590365312 run.py:483] Algo bellman_ford step 7001 current loss 0.406634, current_train_items 224064.
I0302 19:01:39.388599 22699590365312 run.py:483] Algo bellman_ford step 7002 current loss 0.579505, current_train_items 224096.
I0302 19:01:39.418298 22699590365312 run.py:483] Algo bellman_ford step 7003 current loss 0.556502, current_train_items 224128.
I0302 19:01:39.451150 22699590365312 run.py:483] Algo bellman_ford step 7004 current loss 0.706851, current_train_items 224160.
I0302 19:01:39.471199 22699590365312 run.py:483] Algo bellman_ford step 7005 current loss 0.274562, current_train_items 224192.
I0302 19:01:39.487287 22699590365312 run.py:483] Algo bellman_ford step 7006 current loss 0.454502, current_train_items 224224.
I0302 19:01:39.511236 22699590365312 run.py:483] Algo bellman_ford step 7007 current loss 0.524976, current_train_items 224256.
I0302 19:01:39.540704 22699590365312 run.py:483] Algo bellman_ford step 7008 current loss 0.584690, current_train_items 224288.
I0302 19:01:39.576467 22699590365312 run.py:483] Algo bellman_ford step 7009 current loss 0.809123, current_train_items 224320.
I0302 19:01:39.596071 22699590365312 run.py:483] Algo bellman_ford step 7010 current loss 0.320424, current_train_items 224352.
I0302 19:01:39.612314 22699590365312 run.py:483] Algo bellman_ford step 7011 current loss 0.431691, current_train_items 224384.
I0302 19:01:39.635502 22699590365312 run.py:483] Algo bellman_ford step 7012 current loss 0.497520, current_train_items 224416.
I0302 19:01:39.665597 22699590365312 run.py:483] Algo bellman_ford step 7013 current loss 0.574226, current_train_items 224448.
I0302 19:01:39.698960 22699590365312 run.py:483] Algo bellman_ford step 7014 current loss 0.651160, current_train_items 224480.
I0302 19:01:39.718794 22699590365312 run.py:483] Algo bellman_ford step 7015 current loss 0.258660, current_train_items 224512.
I0302 19:01:39.734746 22699590365312 run.py:483] Algo bellman_ford step 7016 current loss 0.403360, current_train_items 224544.
I0302 19:01:39.758070 22699590365312 run.py:483] Algo bellman_ford step 7017 current loss 0.532633, current_train_items 224576.
I0302 19:01:39.788191 22699590365312 run.py:483] Algo bellman_ford step 7018 current loss 0.605922, current_train_items 224608.
I0302 19:01:39.821095 22699590365312 run.py:483] Algo bellman_ford step 7019 current loss 0.663390, current_train_items 224640.
I0302 19:01:39.840717 22699590365312 run.py:483] Algo bellman_ford step 7020 current loss 0.291219, current_train_items 224672.
I0302 19:01:39.856646 22699590365312 run.py:483] Algo bellman_ford step 7021 current loss 0.515891, current_train_items 224704.
I0302 19:01:39.879588 22699590365312 run.py:483] Algo bellman_ford step 7022 current loss 0.556218, current_train_items 224736.
I0302 19:01:39.909690 22699590365312 run.py:483] Algo bellman_ford step 7023 current loss 0.601228, current_train_items 224768.
I0302 19:01:39.943759 22699590365312 run.py:483] Algo bellman_ford step 7024 current loss 0.654893, current_train_items 224800.
I0302 19:01:39.963417 22699590365312 run.py:483] Algo bellman_ford step 7025 current loss 0.326796, current_train_items 224832.
I0302 19:01:39.979851 22699590365312 run.py:483] Algo bellman_ford step 7026 current loss 0.439513, current_train_items 224864.
I0302 19:01:40.004762 22699590365312 run.py:483] Algo bellman_ford step 7027 current loss 0.735494, current_train_items 224896.
I0302 19:01:40.037358 22699590365312 run.py:483] Algo bellman_ford step 7028 current loss 0.798391, current_train_items 224928.
I0302 19:01:40.069016 22699590365312 run.py:483] Algo bellman_ford step 7029 current loss 0.711623, current_train_items 224960.
I0302 19:01:40.088624 22699590365312 run.py:483] Algo bellman_ford step 7030 current loss 0.318635, current_train_items 224992.
I0302 19:01:40.104769 22699590365312 run.py:483] Algo bellman_ford step 7031 current loss 0.382726, current_train_items 225024.
I0302 19:01:40.127941 22699590365312 run.py:483] Algo bellman_ford step 7032 current loss 0.522220, current_train_items 225056.
I0302 19:01:40.158507 22699590365312 run.py:483] Algo bellman_ford step 7033 current loss 0.649095, current_train_items 225088.
I0302 19:01:40.193610 22699590365312 run.py:483] Algo bellman_ford step 7034 current loss 0.811919, current_train_items 225120.
I0302 19:01:40.213221 22699590365312 run.py:483] Algo bellman_ford step 7035 current loss 0.230063, current_train_items 225152.
I0302 19:01:40.229413 22699590365312 run.py:483] Algo bellman_ford step 7036 current loss 0.421985, current_train_items 225184.
I0302 19:01:40.252631 22699590365312 run.py:483] Algo bellman_ford step 7037 current loss 0.580401, current_train_items 225216.
I0302 19:01:40.283961 22699590365312 run.py:483] Algo bellman_ford step 7038 current loss 0.673781, current_train_items 225248.
I0302 19:01:40.316598 22699590365312 run.py:483] Algo bellman_ford step 7039 current loss 0.696703, current_train_items 225280.
I0302 19:01:40.336218 22699590365312 run.py:483] Algo bellman_ford step 7040 current loss 0.270948, current_train_items 225312.
I0302 19:01:40.352359 22699590365312 run.py:483] Algo bellman_ford step 7041 current loss 0.411630, current_train_items 225344.
I0302 19:01:40.375838 22699590365312 run.py:483] Algo bellman_ford step 7042 current loss 0.541051, current_train_items 225376.
I0302 19:01:40.407176 22699590365312 run.py:483] Algo bellman_ford step 7043 current loss 0.656923, current_train_items 225408.
I0302 19:01:40.440881 22699590365312 run.py:483] Algo bellman_ford step 7044 current loss 0.692928, current_train_items 225440.
I0302 19:01:40.460479 22699590365312 run.py:483] Algo bellman_ford step 7045 current loss 0.324315, current_train_items 225472.
I0302 19:01:40.476876 22699590365312 run.py:483] Algo bellman_ford step 7046 current loss 0.486248, current_train_items 225504.
I0302 19:01:40.500062 22699590365312 run.py:483] Algo bellman_ford step 7047 current loss 0.589283, current_train_items 225536.
I0302 19:01:40.531512 22699590365312 run.py:483] Algo bellman_ford step 7048 current loss 0.586868, current_train_items 225568.
I0302 19:01:40.564261 22699590365312 run.py:483] Algo bellman_ford step 7049 current loss 0.697128, current_train_items 225600.
I0302 19:01:40.583818 22699590365312 run.py:483] Algo bellman_ford step 7050 current loss 0.251113, current_train_items 225632.
I0302 19:01:40.591861 22699590365312 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0302 19:01:40.591965 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:01:40.608608 22699590365312 run.py:483] Algo bellman_ford step 7051 current loss 0.405694, current_train_items 225664.
I0302 19:01:40.633330 22699590365312 run.py:483] Algo bellman_ford step 7052 current loss 0.634480, current_train_items 225696.
I0302 19:01:40.666019 22699590365312 run.py:483] Algo bellman_ford step 7053 current loss 0.731888, current_train_items 225728.
I0302 19:01:40.700971 22699590365312 run.py:483] Algo bellman_ford step 7054 current loss 0.726174, current_train_items 225760.
I0302 19:01:40.721046 22699590365312 run.py:483] Algo bellman_ford step 7055 current loss 0.334338, current_train_items 225792.
I0302 19:01:40.736896 22699590365312 run.py:483] Algo bellman_ford step 7056 current loss 0.431111, current_train_items 225824.
I0302 19:01:40.761312 22699590365312 run.py:483] Algo bellman_ford step 7057 current loss 0.578406, current_train_items 225856.
I0302 19:01:40.792317 22699590365312 run.py:483] Algo bellman_ford step 7058 current loss 0.637594, current_train_items 225888.
I0302 19:01:40.828199 22699590365312 run.py:483] Algo bellman_ford step 7059 current loss 0.671108, current_train_items 225920.
I0302 19:01:40.848105 22699590365312 run.py:483] Algo bellman_ford step 7060 current loss 0.305894, current_train_items 225952.
I0302 19:01:40.864271 22699590365312 run.py:483] Algo bellman_ford step 7061 current loss 0.466216, current_train_items 225984.
I0302 19:01:40.888239 22699590365312 run.py:483] Algo bellman_ford step 7062 current loss 0.552760, current_train_items 226016.
I0302 19:01:40.917116 22699590365312 run.py:483] Algo bellman_ford step 7063 current loss 0.498259, current_train_items 226048.
I0302 19:01:40.950309 22699590365312 run.py:483] Algo bellman_ford step 7064 current loss 0.708461, current_train_items 226080.
I0302 19:01:40.970090 22699590365312 run.py:483] Algo bellman_ford step 7065 current loss 0.281150, current_train_items 226112.
I0302 19:01:40.986639 22699590365312 run.py:483] Algo bellman_ford step 7066 current loss 0.422325, current_train_items 226144.
I0302 19:01:41.009836 22699590365312 run.py:483] Algo bellman_ford step 7067 current loss 0.488611, current_train_items 226176.
I0302 19:01:41.042516 22699590365312 run.py:483] Algo bellman_ford step 7068 current loss 0.702462, current_train_items 226208.
I0302 19:01:41.077681 22699590365312 run.py:483] Algo bellman_ford step 7069 current loss 0.778557, current_train_items 226240.
I0302 19:01:41.097745 22699590365312 run.py:483] Algo bellman_ford step 7070 current loss 0.292374, current_train_items 226272.
I0302 19:01:41.113778 22699590365312 run.py:483] Algo bellman_ford step 7071 current loss 0.548192, current_train_items 226304.
I0302 19:01:41.135500 22699590365312 run.py:483] Algo bellman_ford step 7072 current loss 0.616494, current_train_items 226336.
I0302 19:01:41.165641 22699590365312 run.py:483] Algo bellman_ford step 7073 current loss 0.579195, current_train_items 226368.
I0302 19:01:41.198601 22699590365312 run.py:483] Algo bellman_ford step 7074 current loss 0.679403, current_train_items 226400.
I0302 19:01:41.218369 22699590365312 run.py:483] Algo bellman_ford step 7075 current loss 0.302990, current_train_items 226432.
I0302 19:01:41.233855 22699590365312 run.py:483] Algo bellman_ford step 7076 current loss 0.413514, current_train_items 226464.
I0302 19:01:41.256426 22699590365312 run.py:483] Algo bellman_ford step 7077 current loss 0.644273, current_train_items 226496.
I0302 19:01:41.287821 22699590365312 run.py:483] Algo bellman_ford step 7078 current loss 0.596600, current_train_items 226528.
I0302 19:01:41.321935 22699590365312 run.py:483] Algo bellman_ford step 7079 current loss 0.884978, current_train_items 226560.
I0302 19:01:41.341165 22699590365312 run.py:483] Algo bellman_ford step 7080 current loss 0.219674, current_train_items 226592.
I0302 19:01:41.357017 22699590365312 run.py:483] Algo bellman_ford step 7081 current loss 0.399515, current_train_items 226624.
I0302 19:01:41.380519 22699590365312 run.py:483] Algo bellman_ford step 7082 current loss 0.658547, current_train_items 226656.
I0302 19:01:41.411375 22699590365312 run.py:483] Algo bellman_ford step 7083 current loss 0.750325, current_train_items 226688.
I0302 19:01:41.444450 22699590365312 run.py:483] Algo bellman_ford step 7084 current loss 0.756077, current_train_items 226720.
I0302 19:01:41.464413 22699590365312 run.py:483] Algo bellman_ford step 7085 current loss 0.242209, current_train_items 226752.
I0302 19:01:41.480262 22699590365312 run.py:483] Algo bellman_ford step 7086 current loss 0.455187, current_train_items 226784.
I0302 19:01:41.502537 22699590365312 run.py:483] Algo bellman_ford step 7087 current loss 0.496962, current_train_items 226816.
I0302 19:01:41.533555 22699590365312 run.py:483] Algo bellman_ford step 7088 current loss 0.632871, current_train_items 226848.
I0302 19:01:41.566878 22699590365312 run.py:483] Algo bellman_ford step 7089 current loss 0.669604, current_train_items 226880.
I0302 19:01:41.587025 22699590365312 run.py:483] Algo bellman_ford step 7090 current loss 0.268784, current_train_items 226912.
I0302 19:01:41.602869 22699590365312 run.py:483] Algo bellman_ford step 7091 current loss 0.395482, current_train_items 226944.
I0302 19:01:41.626344 22699590365312 run.py:483] Algo bellman_ford step 7092 current loss 0.579105, current_train_items 226976.
I0302 19:01:41.657979 22699590365312 run.py:483] Algo bellman_ford step 7093 current loss 0.721523, current_train_items 227008.
I0302 19:01:41.689747 22699590365312 run.py:483] Algo bellman_ford step 7094 current loss 0.793154, current_train_items 227040.
I0302 19:01:41.709716 22699590365312 run.py:483] Algo bellman_ford step 7095 current loss 0.332906, current_train_items 227072.
I0302 19:01:41.725758 22699590365312 run.py:483] Algo bellman_ford step 7096 current loss 0.442258, current_train_items 227104.
I0302 19:01:41.748181 22699590365312 run.py:483] Algo bellman_ford step 7097 current loss 0.513230, current_train_items 227136.
I0302 19:01:41.779532 22699590365312 run.py:483] Algo bellman_ford step 7098 current loss 0.671080, current_train_items 227168.
I0302 19:01:41.811722 22699590365312 run.py:483] Algo bellman_ford step 7099 current loss 0.676162, current_train_items 227200.
I0302 19:01:41.831704 22699590365312 run.py:483] Algo bellman_ford step 7100 current loss 0.272443, current_train_items 227232.
I0302 19:01:41.839705 22699590365312 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0302 19:01:41.839810 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 19:01:41.856521 22699590365312 run.py:483] Algo bellman_ford step 7101 current loss 0.432270, current_train_items 227264.
I0302 19:01:41.880368 22699590365312 run.py:483] Algo bellman_ford step 7102 current loss 0.574370, current_train_items 227296.
I0302 19:01:41.911724 22699590365312 run.py:483] Algo bellman_ford step 7103 current loss 0.633145, current_train_items 227328.
I0302 19:01:41.947047 22699590365312 run.py:483] Algo bellman_ford step 7104 current loss 0.760175, current_train_items 227360.
I0302 19:01:41.967193 22699590365312 run.py:483] Algo bellman_ford step 7105 current loss 0.322748, current_train_items 227392.
I0302 19:01:41.983179 22699590365312 run.py:483] Algo bellman_ford step 7106 current loss 0.423473, current_train_items 227424.
I0302 19:01:42.007092 22699590365312 run.py:483] Algo bellman_ford step 7107 current loss 0.622945, current_train_items 227456.
I0302 19:01:42.037611 22699590365312 run.py:483] Algo bellman_ford step 7108 current loss 0.622135, current_train_items 227488.
I0302 19:01:42.073514 22699590365312 run.py:483] Algo bellman_ford step 7109 current loss 0.697743, current_train_items 227520.
I0302 19:01:42.093403 22699590365312 run.py:483] Algo bellman_ford step 7110 current loss 0.308547, current_train_items 227552.
I0302 19:01:42.109655 22699590365312 run.py:483] Algo bellman_ford step 7111 current loss 0.430365, current_train_items 227584.
I0302 19:01:42.133140 22699590365312 run.py:483] Algo bellman_ford step 7112 current loss 0.545835, current_train_items 227616.
I0302 19:01:42.164711 22699590365312 run.py:483] Algo bellman_ford step 7113 current loss 0.768490, current_train_items 227648.
I0302 19:01:42.198695 22699590365312 run.py:483] Algo bellman_ford step 7114 current loss 0.834894, current_train_items 227680.
I0302 19:01:42.218476 22699590365312 run.py:483] Algo bellman_ford step 7115 current loss 0.307254, current_train_items 227712.
I0302 19:01:42.234607 22699590365312 run.py:483] Algo bellman_ford step 7116 current loss 0.476727, current_train_items 227744.
I0302 19:01:42.258724 22699590365312 run.py:483] Algo bellman_ford step 7117 current loss 0.543251, current_train_items 227776.
I0302 19:01:42.289872 22699590365312 run.py:483] Algo bellman_ford step 7118 current loss 0.689306, current_train_items 227808.
I0302 19:01:42.323575 22699590365312 run.py:483] Algo bellman_ford step 7119 current loss 0.767759, current_train_items 227840.
I0302 19:01:42.342624 22699590365312 run.py:483] Algo bellman_ford step 7120 current loss 0.285062, current_train_items 227872.
I0302 19:01:42.358879 22699590365312 run.py:483] Algo bellman_ford step 7121 current loss 0.424018, current_train_items 227904.
I0302 19:01:42.382640 22699590365312 run.py:483] Algo bellman_ford step 7122 current loss 0.619498, current_train_items 227936.
I0302 19:01:42.413065 22699590365312 run.py:483] Algo bellman_ford step 7123 current loss 0.634222, current_train_items 227968.
I0302 19:01:42.446901 22699590365312 run.py:483] Algo bellman_ford step 7124 current loss 0.789842, current_train_items 228000.
I0302 19:01:42.466084 22699590365312 run.py:483] Algo bellman_ford step 7125 current loss 0.276906, current_train_items 228032.
I0302 19:01:42.481694 22699590365312 run.py:483] Algo bellman_ford step 7126 current loss 0.436023, current_train_items 228064.
I0302 19:01:42.504248 22699590365312 run.py:483] Algo bellman_ford step 7127 current loss 0.579185, current_train_items 228096.
I0302 19:01:42.534459 22699590365312 run.py:483] Algo bellman_ford step 7128 current loss 0.682244, current_train_items 228128.
I0302 19:01:42.569216 22699590365312 run.py:483] Algo bellman_ford step 7129 current loss 0.775831, current_train_items 228160.
I0302 19:01:42.588698 22699590365312 run.py:483] Algo bellman_ford step 7130 current loss 0.304916, current_train_items 228192.
I0302 19:01:42.605003 22699590365312 run.py:483] Algo bellman_ford step 7131 current loss 0.412584, current_train_items 228224.
I0302 19:01:42.628820 22699590365312 run.py:483] Algo bellman_ford step 7132 current loss 0.567113, current_train_items 228256.
I0302 19:01:42.659848 22699590365312 run.py:483] Algo bellman_ford step 7133 current loss 0.740746, current_train_items 228288.
I0302 19:01:42.692874 22699590365312 run.py:483] Algo bellman_ford step 7134 current loss 0.636955, current_train_items 228320.
I0302 19:01:42.712554 22699590365312 run.py:483] Algo bellman_ford step 7135 current loss 0.272346, current_train_items 228352.
I0302 19:01:42.728305 22699590365312 run.py:483] Algo bellman_ford step 7136 current loss 0.363872, current_train_items 228384.
I0302 19:01:42.750740 22699590365312 run.py:483] Algo bellman_ford step 7137 current loss 0.511504, current_train_items 228416.
I0302 19:01:42.780443 22699590365312 run.py:483] Algo bellman_ford step 7138 current loss 0.540233, current_train_items 228448.
I0302 19:01:42.816349 22699590365312 run.py:483] Algo bellman_ford step 7139 current loss 0.760398, current_train_items 228480.
I0302 19:01:42.835866 22699590365312 run.py:483] Algo bellman_ford step 7140 current loss 0.315324, current_train_items 228512.
I0302 19:01:42.851640 22699590365312 run.py:483] Algo bellman_ford step 7141 current loss 0.504666, current_train_items 228544.
I0302 19:01:42.875242 22699590365312 run.py:483] Algo bellman_ford step 7142 current loss 0.542552, current_train_items 228576.
I0302 19:01:42.904451 22699590365312 run.py:483] Algo bellman_ford step 7143 current loss 0.568877, current_train_items 228608.
I0302 19:01:42.937161 22699590365312 run.py:483] Algo bellman_ford step 7144 current loss 0.674909, current_train_items 228640.
I0302 19:01:42.956942 22699590365312 run.py:483] Algo bellman_ford step 7145 current loss 0.325783, current_train_items 228672.
I0302 19:01:42.973389 22699590365312 run.py:483] Algo bellman_ford step 7146 current loss 0.448348, current_train_items 228704.
I0302 19:01:42.997486 22699590365312 run.py:483] Algo bellman_ford step 7147 current loss 0.543393, current_train_items 228736.
I0302 19:01:43.027254 22699590365312 run.py:483] Algo bellman_ford step 7148 current loss 0.590922, current_train_items 228768.
I0302 19:01:43.057950 22699590365312 run.py:483] Algo bellman_ford step 7149 current loss 0.620402, current_train_items 228800.
I0302 19:01:43.077365 22699590365312 run.py:483] Algo bellman_ford step 7150 current loss 0.275206, current_train_items 228832.
I0302 19:01:43.085478 22699590365312 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0302 19:01:43.085585 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:01:43.102878 22699590365312 run.py:483] Algo bellman_ford step 7151 current loss 0.453416, current_train_items 228864.
I0302 19:01:43.127798 22699590365312 run.py:483] Algo bellman_ford step 7152 current loss 0.543146, current_train_items 228896.
I0302 19:01:43.159293 22699590365312 run.py:483] Algo bellman_ford step 7153 current loss 0.669020, current_train_items 228928.
I0302 19:01:43.192692 22699590365312 run.py:483] Algo bellman_ford step 7154 current loss 0.570874, current_train_items 228960.
I0302 19:01:43.212837 22699590365312 run.py:483] Algo bellman_ford step 7155 current loss 0.277739, current_train_items 228992.
I0302 19:01:43.228896 22699590365312 run.py:483] Algo bellman_ford step 7156 current loss 0.477283, current_train_items 229024.
I0302 19:01:43.252410 22699590365312 run.py:483] Algo bellman_ford step 7157 current loss 0.539569, current_train_items 229056.
I0302 19:01:43.282649 22699590365312 run.py:483] Algo bellman_ford step 7158 current loss 0.539023, current_train_items 229088.
I0302 19:01:43.316329 22699590365312 run.py:483] Algo bellman_ford step 7159 current loss 0.774273, current_train_items 229120.
I0302 19:01:43.336149 22699590365312 run.py:483] Algo bellman_ford step 7160 current loss 0.247261, current_train_items 229152.
I0302 19:01:43.352762 22699590365312 run.py:483] Algo bellman_ford step 7161 current loss 0.475253, current_train_items 229184.
W0302 19:01:43.367250 22699590365312 samplers.py:155] Increasing hint lengh from 10 to 11
I0302 19:01:50.242382 22699590365312 run.py:483] Algo bellman_ford step 7162 current loss 0.600712, current_train_items 229216.
I0302 19:01:50.274531 22699590365312 run.py:483] Algo bellman_ford step 7163 current loss 0.658463, current_train_items 229248.
I0302 19:01:50.309863 22699590365312 run.py:483] Algo bellman_ford step 7164 current loss 0.784488, current_train_items 229280.
I0302 19:01:50.329680 22699590365312 run.py:483] Algo bellman_ford step 7165 current loss 0.327498, current_train_items 229312.
I0302 19:01:50.345919 22699590365312 run.py:483] Algo bellman_ford step 7166 current loss 0.469111, current_train_items 229344.
I0302 19:01:50.369482 22699590365312 run.py:483] Algo bellman_ford step 7167 current loss 0.576288, current_train_items 229376.
I0302 19:01:50.400995 22699590365312 run.py:483] Algo bellman_ford step 7168 current loss 0.543141, current_train_items 229408.
I0302 19:01:50.432150 22699590365312 run.py:483] Algo bellman_ford step 7169 current loss 0.684713, current_train_items 229440.
I0302 19:01:50.451937 22699590365312 run.py:483] Algo bellman_ford step 7170 current loss 0.309576, current_train_items 229472.
I0302 19:01:50.468415 22699590365312 run.py:483] Algo bellman_ford step 7171 current loss 0.463024, current_train_items 229504.
I0302 19:01:50.491396 22699590365312 run.py:483] Algo bellman_ford step 7172 current loss 0.560378, current_train_items 229536.
I0302 19:01:50.522560 22699590365312 run.py:483] Algo bellman_ford step 7173 current loss 0.563486, current_train_items 229568.
I0302 19:01:50.556974 22699590365312 run.py:483] Algo bellman_ford step 7174 current loss 0.689394, current_train_items 229600.
I0302 19:01:50.576992 22699590365312 run.py:483] Algo bellman_ford step 7175 current loss 0.286634, current_train_items 229632.
I0302 19:01:50.592940 22699590365312 run.py:483] Algo bellman_ford step 7176 current loss 0.494474, current_train_items 229664.
I0302 19:01:50.616575 22699590365312 run.py:483] Algo bellman_ford step 7177 current loss 0.584367, current_train_items 229696.
I0302 19:01:50.649430 22699590365312 run.py:483] Algo bellman_ford step 7178 current loss 0.590304, current_train_items 229728.
I0302 19:01:50.683774 22699590365312 run.py:483] Algo bellman_ford step 7179 current loss 0.651438, current_train_items 229760.
I0302 19:01:50.703326 22699590365312 run.py:483] Algo bellman_ford step 7180 current loss 0.226157, current_train_items 229792.
I0302 19:01:50.719308 22699590365312 run.py:483] Algo bellman_ford step 7181 current loss 0.442262, current_train_items 229824.
I0302 19:01:50.743627 22699590365312 run.py:483] Algo bellman_ford step 7182 current loss 0.613622, current_train_items 229856.
I0302 19:01:50.774303 22699590365312 run.py:483] Algo bellman_ford step 7183 current loss 0.572712, current_train_items 229888.
I0302 19:01:50.810896 22699590365312 run.py:483] Algo bellman_ford step 7184 current loss 0.758661, current_train_items 229920.
I0302 19:01:50.830467 22699590365312 run.py:483] Algo bellman_ford step 7185 current loss 0.339579, current_train_items 229952.
I0302 19:01:50.846519 22699590365312 run.py:483] Algo bellman_ford step 7186 current loss 0.470549, current_train_items 229984.
I0302 19:01:50.870766 22699590365312 run.py:483] Algo bellman_ford step 7187 current loss 0.598367, current_train_items 230016.
I0302 19:01:50.902230 22699590365312 run.py:483] Algo bellman_ford step 7188 current loss 0.625879, current_train_items 230048.
I0302 19:01:50.937204 22699590365312 run.py:483] Algo bellman_ford step 7189 current loss 0.708615, current_train_items 230080.
I0302 19:01:50.956808 22699590365312 run.py:483] Algo bellman_ford step 7190 current loss 0.288960, current_train_items 230112.
I0302 19:01:50.973110 22699590365312 run.py:483] Algo bellman_ford step 7191 current loss 0.414661, current_train_items 230144.
I0302 19:01:50.996475 22699590365312 run.py:483] Algo bellman_ford step 7192 current loss 0.622604, current_train_items 230176.
I0302 19:01:51.027698 22699590365312 run.py:483] Algo bellman_ford step 7193 current loss 0.566810, current_train_items 230208.
I0302 19:01:51.060683 22699590365312 run.py:483] Algo bellman_ford step 7194 current loss 0.617222, current_train_items 230240.
I0302 19:01:51.080402 22699590365312 run.py:483] Algo bellman_ford step 7195 current loss 0.268453, current_train_items 230272.
I0302 19:01:51.096148 22699590365312 run.py:483] Algo bellman_ford step 7196 current loss 0.523468, current_train_items 230304.
I0302 19:01:51.119503 22699590365312 run.py:483] Algo bellman_ford step 7197 current loss 0.562684, current_train_items 230336.
I0302 19:01:51.151131 22699590365312 run.py:483] Algo bellman_ford step 7198 current loss 0.681310, current_train_items 230368.
I0302 19:01:51.184951 22699590365312 run.py:483] Algo bellman_ford step 7199 current loss 0.738641, current_train_items 230400.
I0302 19:01:51.205062 22699590365312 run.py:483] Algo bellman_ford step 7200 current loss 0.342203, current_train_items 230432.
I0302 19:01:51.214825 22699590365312 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0302 19:01:51.214943 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:01:51.231960 22699590365312 run.py:483] Algo bellman_ford step 7201 current loss 0.532511, current_train_items 230464.
I0302 19:01:51.255626 22699590365312 run.py:483] Algo bellman_ford step 7202 current loss 0.538888, current_train_items 230496.
I0302 19:01:51.287677 22699590365312 run.py:483] Algo bellman_ford step 7203 current loss 0.615468, current_train_items 230528.
I0302 19:01:51.321398 22699590365312 run.py:483] Algo bellman_ford step 7204 current loss 0.627850, current_train_items 230560.
I0302 19:01:51.341332 22699590365312 run.py:483] Algo bellman_ford step 7205 current loss 0.334670, current_train_items 230592.
I0302 19:01:51.356887 22699590365312 run.py:483] Algo bellman_ford step 7206 current loss 0.438499, current_train_items 230624.
I0302 19:01:51.380612 22699590365312 run.py:483] Algo bellman_ford step 7207 current loss 0.584476, current_train_items 230656.
I0302 19:01:51.411642 22699590365312 run.py:483] Algo bellman_ford step 7208 current loss 0.636836, current_train_items 230688.
I0302 19:01:51.446192 22699590365312 run.py:483] Algo bellman_ford step 7209 current loss 0.770076, current_train_items 230720.
I0302 19:01:51.465774 22699590365312 run.py:483] Algo bellman_ford step 7210 current loss 0.343968, current_train_items 230752.
I0302 19:01:51.481602 22699590365312 run.py:483] Algo bellman_ford step 7211 current loss 0.384034, current_train_items 230784.
I0302 19:01:51.504731 22699590365312 run.py:483] Algo bellman_ford step 7212 current loss 0.530384, current_train_items 230816.
I0302 19:01:51.535788 22699590365312 run.py:483] Algo bellman_ford step 7213 current loss 0.559731, current_train_items 230848.
I0302 19:01:51.567653 22699590365312 run.py:483] Algo bellman_ford step 7214 current loss 0.751768, current_train_items 230880.
I0302 19:01:51.587273 22699590365312 run.py:483] Algo bellman_ford step 7215 current loss 0.378527, current_train_items 230912.
I0302 19:01:51.603354 22699590365312 run.py:483] Algo bellman_ford step 7216 current loss 0.429856, current_train_items 230944.
I0302 19:01:51.627166 22699590365312 run.py:483] Algo bellman_ford step 7217 current loss 0.554100, current_train_items 230976.
I0302 19:01:51.658649 22699590365312 run.py:483] Algo bellman_ford step 7218 current loss 0.607997, current_train_items 231008.
I0302 19:01:51.691216 22699590365312 run.py:483] Algo bellman_ford step 7219 current loss 0.629268, current_train_items 231040.
I0302 19:01:51.710558 22699590365312 run.py:483] Algo bellman_ford step 7220 current loss 0.342463, current_train_items 231072.
I0302 19:01:51.726709 22699590365312 run.py:483] Algo bellman_ford step 7221 current loss 0.448314, current_train_items 231104.
I0302 19:01:51.751646 22699590365312 run.py:483] Algo bellman_ford step 7222 current loss 0.530509, current_train_items 231136.
I0302 19:01:51.782603 22699590365312 run.py:483] Algo bellman_ford step 7223 current loss 0.648042, current_train_items 231168.
I0302 19:01:51.816779 22699590365312 run.py:483] Algo bellman_ford step 7224 current loss 0.788768, current_train_items 231200.
I0302 19:01:51.836433 22699590365312 run.py:483] Algo bellman_ford step 7225 current loss 0.286820, current_train_items 231232.
I0302 19:01:51.852631 22699590365312 run.py:483] Algo bellman_ford step 7226 current loss 0.402019, current_train_items 231264.
I0302 19:01:51.877487 22699590365312 run.py:483] Algo bellman_ford step 7227 current loss 0.605270, current_train_items 231296.
I0302 19:01:51.908799 22699590365312 run.py:483] Algo bellman_ford step 7228 current loss 0.551254, current_train_items 231328.
I0302 19:01:51.940400 22699590365312 run.py:483] Algo bellman_ford step 7229 current loss 0.704489, current_train_items 231360.
I0302 19:01:51.959730 22699590365312 run.py:483] Algo bellman_ford step 7230 current loss 0.281372, current_train_items 231392.
I0302 19:01:51.976090 22699590365312 run.py:483] Algo bellman_ford step 7231 current loss 0.417316, current_train_items 231424.
I0302 19:01:51.997915 22699590365312 run.py:483] Algo bellman_ford step 7232 current loss 0.467993, current_train_items 231456.
I0302 19:01:52.029719 22699590365312 run.py:483] Algo bellman_ford step 7233 current loss 0.650293, current_train_items 231488.
I0302 19:01:52.062323 22699590365312 run.py:483] Algo bellman_ford step 7234 current loss 0.668767, current_train_items 231520.
I0302 19:01:52.081897 22699590365312 run.py:483] Algo bellman_ford step 7235 current loss 0.263839, current_train_items 231552.
I0302 19:01:52.098332 22699590365312 run.py:483] Algo bellman_ford step 7236 current loss 0.527924, current_train_items 231584.
I0302 19:01:52.122923 22699590365312 run.py:483] Algo bellman_ford step 7237 current loss 0.538580, current_train_items 231616.
I0302 19:01:52.155942 22699590365312 run.py:483] Algo bellman_ford step 7238 current loss 0.705570, current_train_items 231648.
I0302 19:01:52.189594 22699590365312 run.py:483] Algo bellman_ford step 7239 current loss 0.663179, current_train_items 231680.
I0302 19:01:52.209195 22699590365312 run.py:483] Algo bellman_ford step 7240 current loss 0.281107, current_train_items 231712.
I0302 19:01:52.225127 22699590365312 run.py:483] Algo bellman_ford step 7241 current loss 0.428329, current_train_items 231744.
I0302 19:01:52.249736 22699590365312 run.py:483] Algo bellman_ford step 7242 current loss 0.635933, current_train_items 231776.
I0302 19:01:52.281087 22699590365312 run.py:483] Algo bellman_ford step 7243 current loss 0.579314, current_train_items 231808.
I0302 19:01:52.316339 22699590365312 run.py:483] Algo bellman_ford step 7244 current loss 0.676000, current_train_items 231840.
I0302 19:01:52.335829 22699590365312 run.py:483] Algo bellman_ford step 7245 current loss 0.297531, current_train_items 231872.
I0302 19:01:52.351865 22699590365312 run.py:483] Algo bellman_ford step 7246 current loss 0.408430, current_train_items 231904.
I0302 19:01:52.376228 22699590365312 run.py:483] Algo bellman_ford step 7247 current loss 0.505973, current_train_items 231936.
I0302 19:01:52.408984 22699590365312 run.py:483] Algo bellman_ford step 7248 current loss 0.616951, current_train_items 231968.
I0302 19:01:52.442871 22699590365312 run.py:483] Algo bellman_ford step 7249 current loss 0.660621, current_train_items 232000.
I0302 19:01:52.462580 22699590365312 run.py:483] Algo bellman_ford step 7250 current loss 0.306833, current_train_items 232032.
I0302 19:01:52.470774 22699590365312 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0302 19:01:52.470885 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:01:52.487817 22699590365312 run.py:483] Algo bellman_ford step 7251 current loss 0.400066, current_train_items 232064.
I0302 19:01:52.511966 22699590365312 run.py:483] Algo bellman_ford step 7252 current loss 0.520132, current_train_items 232096.
I0302 19:01:52.542311 22699590365312 run.py:483] Algo bellman_ford step 7253 current loss 0.603152, current_train_items 232128.
I0302 19:01:52.576176 22699590365312 run.py:483] Algo bellman_ford step 7254 current loss 0.734859, current_train_items 232160.
I0302 19:01:52.596305 22699590365312 run.py:483] Algo bellman_ford step 7255 current loss 0.292685, current_train_items 232192.
I0302 19:01:52.611883 22699590365312 run.py:483] Algo bellman_ford step 7256 current loss 0.427734, current_train_items 232224.
I0302 19:01:52.635439 22699590365312 run.py:483] Algo bellman_ford step 7257 current loss 0.570672, current_train_items 232256.
I0302 19:01:52.668833 22699590365312 run.py:483] Algo bellman_ford step 7258 current loss 0.689992, current_train_items 232288.
I0302 19:01:52.702893 22699590365312 run.py:483] Algo bellman_ford step 7259 current loss 0.725039, current_train_items 232320.
I0302 19:01:52.722696 22699590365312 run.py:483] Algo bellman_ford step 7260 current loss 0.290552, current_train_items 232352.
I0302 19:01:52.738942 22699590365312 run.py:483] Algo bellman_ford step 7261 current loss 0.431123, current_train_items 232384.
I0302 19:01:52.762438 22699590365312 run.py:483] Algo bellman_ford step 7262 current loss 0.547673, current_train_items 232416.
I0302 19:01:52.793997 22699590365312 run.py:483] Algo bellman_ford step 7263 current loss 0.659271, current_train_items 232448.
I0302 19:01:52.827610 22699590365312 run.py:483] Algo bellman_ford step 7264 current loss 0.708977, current_train_items 232480.
I0302 19:01:52.846947 22699590365312 run.py:483] Algo bellman_ford step 7265 current loss 0.294166, current_train_items 232512.
I0302 19:01:52.863060 22699590365312 run.py:483] Algo bellman_ford step 7266 current loss 0.481819, current_train_items 232544.
I0302 19:01:52.886518 22699590365312 run.py:483] Algo bellman_ford step 7267 current loss 0.644732, current_train_items 232576.
I0302 19:01:52.917912 22699590365312 run.py:483] Algo bellman_ford step 7268 current loss 0.607290, current_train_items 232608.
I0302 19:01:52.950626 22699590365312 run.py:483] Algo bellman_ford step 7269 current loss 0.751072, current_train_items 232640.
I0302 19:01:52.970356 22699590365312 run.py:483] Algo bellman_ford step 7270 current loss 0.290550, current_train_items 232672.
I0302 19:01:52.986758 22699590365312 run.py:483] Algo bellman_ford step 7271 current loss 0.500338, current_train_items 232704.
I0302 19:01:53.010302 22699590365312 run.py:483] Algo bellman_ford step 7272 current loss 0.498080, current_train_items 232736.
I0302 19:01:53.041436 22699590365312 run.py:483] Algo bellman_ford step 7273 current loss 0.619269, current_train_items 232768.
I0302 19:01:53.074606 22699590365312 run.py:483] Algo bellman_ford step 7274 current loss 0.733518, current_train_items 232800.
I0302 19:01:53.094520 22699590365312 run.py:483] Algo bellman_ford step 7275 current loss 0.303699, current_train_items 232832.
I0302 19:01:53.110501 22699590365312 run.py:483] Algo bellman_ford step 7276 current loss 0.391892, current_train_items 232864.
I0302 19:01:53.134400 22699590365312 run.py:483] Algo bellman_ford step 7277 current loss 0.566072, current_train_items 232896.
I0302 19:01:53.165913 22699590365312 run.py:483] Algo bellman_ford step 7278 current loss 0.641224, current_train_items 232928.
I0302 19:01:53.199717 22699590365312 run.py:483] Algo bellman_ford step 7279 current loss 0.658408, current_train_items 232960.
I0302 19:01:53.219123 22699590365312 run.py:483] Algo bellman_ford step 7280 current loss 0.322823, current_train_items 232992.
I0302 19:01:53.235563 22699590365312 run.py:483] Algo bellman_ford step 7281 current loss 0.417866, current_train_items 233024.
I0302 19:01:53.258835 22699590365312 run.py:483] Algo bellman_ford step 7282 current loss 0.555193, current_train_items 233056.
I0302 19:01:53.290458 22699590365312 run.py:483] Algo bellman_ford step 7283 current loss 0.594939, current_train_items 233088.
I0302 19:01:53.322919 22699590365312 run.py:483] Algo bellman_ford step 7284 current loss 0.605981, current_train_items 233120.
I0302 19:01:53.342894 22699590365312 run.py:483] Algo bellman_ford step 7285 current loss 0.276932, current_train_items 233152.
I0302 19:01:53.359977 22699590365312 run.py:483] Algo bellman_ford step 7286 current loss 0.609900, current_train_items 233184.
I0302 19:01:53.383004 22699590365312 run.py:483] Algo bellman_ford step 7287 current loss 0.462641, current_train_items 233216.
I0302 19:01:53.413968 22699590365312 run.py:483] Algo bellman_ford step 7288 current loss 0.576890, current_train_items 233248.
I0302 19:01:53.446210 22699590365312 run.py:483] Algo bellman_ford step 7289 current loss 0.605666, current_train_items 233280.
I0302 19:01:53.466378 22699590365312 run.py:483] Algo bellman_ford step 7290 current loss 0.325955, current_train_items 233312.
I0302 19:01:53.482648 22699590365312 run.py:483] Algo bellman_ford step 7291 current loss 0.461705, current_train_items 233344.
I0302 19:01:53.505338 22699590365312 run.py:483] Algo bellman_ford step 7292 current loss 0.453526, current_train_items 233376.
I0302 19:01:53.537497 22699590365312 run.py:483] Algo bellman_ford step 7293 current loss 0.600946, current_train_items 233408.
I0302 19:01:53.569660 22699590365312 run.py:483] Algo bellman_ford step 7294 current loss 0.675146, current_train_items 233440.
I0302 19:01:53.589122 22699590365312 run.py:483] Algo bellman_ford step 7295 current loss 0.310876, current_train_items 233472.
I0302 19:01:53.605326 22699590365312 run.py:483] Algo bellman_ford step 7296 current loss 0.448275, current_train_items 233504.
I0302 19:01:53.629266 22699590365312 run.py:483] Algo bellman_ford step 7297 current loss 0.618279, current_train_items 233536.
I0302 19:01:53.660745 22699590365312 run.py:483] Algo bellman_ford step 7298 current loss 0.691126, current_train_items 233568.
I0302 19:01:53.694243 22699590365312 run.py:483] Algo bellman_ford step 7299 current loss 0.817286, current_train_items 233600.
I0302 19:01:53.714075 22699590365312 run.py:483] Algo bellman_ford step 7300 current loss 0.297979, current_train_items 233632.
I0302 19:01:53.722198 22699590365312 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0302 19:01:53.722306 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:01:53.739645 22699590365312 run.py:483] Algo bellman_ford step 7301 current loss 0.432128, current_train_items 233664.
I0302 19:01:53.763376 22699590365312 run.py:483] Algo bellman_ford step 7302 current loss 0.523925, current_train_items 233696.
I0302 19:01:53.794653 22699590365312 run.py:483] Algo bellman_ford step 7303 current loss 0.590529, current_train_items 233728.
I0302 19:01:53.829159 22699590365312 run.py:483] Algo bellman_ford step 7304 current loss 0.662947, current_train_items 233760.
I0302 19:01:53.849054 22699590365312 run.py:483] Algo bellman_ford step 7305 current loss 0.312352, current_train_items 233792.
I0302 19:01:53.864737 22699590365312 run.py:483] Algo bellman_ford step 7306 current loss 0.372696, current_train_items 233824.
I0302 19:01:53.888180 22699590365312 run.py:483] Algo bellman_ford step 7307 current loss 0.701508, current_train_items 233856.
I0302 19:01:53.918501 22699590365312 run.py:483] Algo bellman_ford step 7308 current loss 0.560434, current_train_items 233888.
I0302 19:01:53.952980 22699590365312 run.py:483] Algo bellman_ford step 7309 current loss 0.858280, current_train_items 233920.
I0302 19:01:53.972542 22699590365312 run.py:483] Algo bellman_ford step 7310 current loss 0.256463, current_train_items 233952.
I0302 19:01:53.988385 22699590365312 run.py:483] Algo bellman_ford step 7311 current loss 0.386898, current_train_items 233984.
I0302 19:01:54.013465 22699590365312 run.py:483] Algo bellman_ford step 7312 current loss 0.646020, current_train_items 234016.
I0302 19:01:54.045061 22699590365312 run.py:483] Algo bellman_ford step 7313 current loss 0.623714, current_train_items 234048.
I0302 19:01:54.079312 22699590365312 run.py:483] Algo bellman_ford step 7314 current loss 0.739725, current_train_items 234080.
I0302 19:01:54.099111 22699590365312 run.py:483] Algo bellman_ford step 7315 current loss 0.270752, current_train_items 234112.
I0302 19:01:54.115006 22699590365312 run.py:483] Algo bellman_ford step 7316 current loss 0.442177, current_train_items 234144.
I0302 19:01:54.139231 22699590365312 run.py:483] Algo bellman_ford step 7317 current loss 0.666879, current_train_items 234176.
I0302 19:01:54.170191 22699590365312 run.py:483] Algo bellman_ford step 7318 current loss 0.638881, current_train_items 234208.
I0302 19:01:54.204857 22699590365312 run.py:483] Algo bellman_ford step 7319 current loss 0.783308, current_train_items 234240.
I0302 19:01:54.224618 22699590365312 run.py:483] Algo bellman_ford step 7320 current loss 0.303019, current_train_items 234272.
I0302 19:01:54.241218 22699590365312 run.py:483] Algo bellman_ford step 7321 current loss 0.447877, current_train_items 234304.
I0302 19:01:54.266012 22699590365312 run.py:483] Algo bellman_ford step 7322 current loss 0.610981, current_train_items 234336.
I0302 19:01:54.297098 22699590365312 run.py:483] Algo bellman_ford step 7323 current loss 0.606520, current_train_items 234368.
I0302 19:01:54.331745 22699590365312 run.py:483] Algo bellman_ford step 7324 current loss 0.658311, current_train_items 234400.
I0302 19:01:54.351422 22699590365312 run.py:483] Algo bellman_ford step 7325 current loss 0.285498, current_train_items 234432.
I0302 19:01:54.367906 22699590365312 run.py:483] Algo bellman_ford step 7326 current loss 0.502990, current_train_items 234464.
I0302 19:01:54.391855 22699590365312 run.py:483] Algo bellman_ford step 7327 current loss 0.558592, current_train_items 234496.
I0302 19:01:54.422445 22699590365312 run.py:483] Algo bellman_ford step 7328 current loss 0.613201, current_train_items 234528.
I0302 19:01:54.456532 22699590365312 run.py:483] Algo bellman_ford step 7329 current loss 0.639822, current_train_items 234560.
I0302 19:01:54.475841 22699590365312 run.py:483] Algo bellman_ford step 7330 current loss 0.305721, current_train_items 234592.
I0302 19:01:54.492170 22699590365312 run.py:483] Algo bellman_ford step 7331 current loss 0.488264, current_train_items 234624.
I0302 19:01:54.517110 22699590365312 run.py:483] Algo bellman_ford step 7332 current loss 0.589526, current_train_items 234656.
I0302 19:01:54.548841 22699590365312 run.py:483] Algo bellman_ford step 7333 current loss 0.632883, current_train_items 234688.
I0302 19:01:54.580964 22699590365312 run.py:483] Algo bellman_ford step 7334 current loss 0.632886, current_train_items 234720.
I0302 19:01:54.600367 22699590365312 run.py:483] Algo bellman_ford step 7335 current loss 0.298045, current_train_items 234752.
I0302 19:01:54.616148 22699590365312 run.py:483] Algo bellman_ford step 7336 current loss 0.418613, current_train_items 234784.
I0302 19:01:54.639939 22699590365312 run.py:483] Algo bellman_ford step 7337 current loss 0.479409, current_train_items 234816.
I0302 19:01:54.671526 22699590365312 run.py:483] Algo bellman_ford step 7338 current loss 0.545735, current_train_items 234848.
I0302 19:01:54.706967 22699590365312 run.py:483] Algo bellman_ford step 7339 current loss 0.736947, current_train_items 234880.
I0302 19:01:54.726514 22699590365312 run.py:483] Algo bellman_ford step 7340 current loss 0.260421, current_train_items 234912.
I0302 19:01:54.742406 22699590365312 run.py:483] Algo bellman_ford step 7341 current loss 0.459335, current_train_items 234944.
I0302 19:01:54.767349 22699590365312 run.py:483] Algo bellman_ford step 7342 current loss 0.586626, current_train_items 234976.
I0302 19:01:54.799116 22699590365312 run.py:483] Algo bellman_ford step 7343 current loss 0.571655, current_train_items 235008.
I0302 19:01:54.832355 22699590365312 run.py:483] Algo bellman_ford step 7344 current loss 0.695458, current_train_items 235040.
I0302 19:01:54.851972 22699590365312 run.py:483] Algo bellman_ford step 7345 current loss 0.329490, current_train_items 235072.
I0302 19:01:54.867791 22699590365312 run.py:483] Algo bellman_ford step 7346 current loss 0.367488, current_train_items 235104.
I0302 19:01:54.892231 22699590365312 run.py:483] Algo bellman_ford step 7347 current loss 0.606123, current_train_items 235136.
I0302 19:01:54.923982 22699590365312 run.py:483] Algo bellman_ford step 7348 current loss 0.547216, current_train_items 235168.
I0302 19:01:54.954248 22699590365312 run.py:483] Algo bellman_ford step 7349 current loss 0.574046, current_train_items 235200.
I0302 19:01:54.973823 22699590365312 run.py:483] Algo bellman_ford step 7350 current loss 0.271411, current_train_items 235232.
I0302 19:01:54.982113 22699590365312 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0302 19:01:54.982266 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:01:54.999614 22699590365312 run.py:483] Algo bellman_ford step 7351 current loss 0.488375, current_train_items 235264.
I0302 19:01:55.025080 22699590365312 run.py:483] Algo bellman_ford step 7352 current loss 0.597377, current_train_items 235296.
I0302 19:01:55.057773 22699590365312 run.py:483] Algo bellman_ford step 7353 current loss 0.599232, current_train_items 235328.
I0302 19:01:55.091495 22699590365312 run.py:483] Algo bellman_ford step 7354 current loss 0.676655, current_train_items 235360.
I0302 19:01:55.111447 22699590365312 run.py:483] Algo bellman_ford step 7355 current loss 0.314320, current_train_items 235392.
I0302 19:01:55.127077 22699590365312 run.py:483] Algo bellman_ford step 7356 current loss 0.401695, current_train_items 235424.
I0302 19:01:55.152054 22699590365312 run.py:483] Algo bellman_ford step 7357 current loss 0.588972, current_train_items 235456.
I0302 19:01:55.183187 22699590365312 run.py:483] Algo bellman_ford step 7358 current loss 0.670564, current_train_items 235488.
I0302 19:01:55.216473 22699590365312 run.py:483] Algo bellman_ford step 7359 current loss 0.627025, current_train_items 235520.
I0302 19:01:55.236215 22699590365312 run.py:483] Algo bellman_ford step 7360 current loss 0.299375, current_train_items 235552.
I0302 19:01:55.252823 22699590365312 run.py:483] Algo bellman_ford step 7361 current loss 0.462189, current_train_items 235584.
I0302 19:01:55.275548 22699590365312 run.py:483] Algo bellman_ford step 7362 current loss 0.569946, current_train_items 235616.
I0302 19:01:55.308583 22699590365312 run.py:483] Algo bellman_ford step 7363 current loss 0.753076, current_train_items 235648.
I0302 19:01:55.340011 22699590365312 run.py:483] Algo bellman_ford step 7364 current loss 0.689453, current_train_items 235680.
I0302 19:01:55.359706 22699590365312 run.py:483] Algo bellman_ford step 7365 current loss 0.302483, current_train_items 235712.
I0302 19:01:55.375818 22699590365312 run.py:483] Algo bellman_ford step 7366 current loss 0.417184, current_train_items 235744.
I0302 19:01:55.400267 22699590365312 run.py:483] Algo bellman_ford step 7367 current loss 0.501080, current_train_items 235776.
I0302 19:01:55.433703 22699590365312 run.py:483] Algo bellman_ford step 7368 current loss 0.647880, current_train_items 235808.
I0302 19:01:55.465775 22699590365312 run.py:483] Algo bellman_ford step 7369 current loss 0.614030, current_train_items 235840.
I0302 19:01:55.485819 22699590365312 run.py:483] Algo bellman_ford step 7370 current loss 0.244184, current_train_items 235872.
I0302 19:01:55.502051 22699590365312 run.py:483] Algo bellman_ford step 7371 current loss 0.632644, current_train_items 235904.
I0302 19:01:55.524742 22699590365312 run.py:483] Algo bellman_ford step 7372 current loss 0.612790, current_train_items 235936.
I0302 19:01:55.555547 22699590365312 run.py:483] Algo bellman_ford step 7373 current loss 0.659810, current_train_items 235968.
I0302 19:01:55.588994 22699590365312 run.py:483] Algo bellman_ford step 7374 current loss 0.775018, current_train_items 236000.
I0302 19:01:55.608629 22699590365312 run.py:483] Algo bellman_ford step 7375 current loss 0.250892, current_train_items 236032.
I0302 19:01:55.625290 22699590365312 run.py:483] Algo bellman_ford step 7376 current loss 0.402430, current_train_items 236064.
I0302 19:01:55.649086 22699590365312 run.py:483] Algo bellman_ford step 7377 current loss 0.553866, current_train_items 236096.
I0302 19:01:55.680369 22699590365312 run.py:483] Algo bellman_ford step 7378 current loss 0.575593, current_train_items 236128.
I0302 19:01:55.712225 22699590365312 run.py:483] Algo bellman_ford step 7379 current loss 0.609674, current_train_items 236160.
I0302 19:01:55.731369 22699590365312 run.py:483] Algo bellman_ford step 7380 current loss 0.334012, current_train_items 236192.
I0302 19:01:55.747067 22699590365312 run.py:483] Algo bellman_ford step 7381 current loss 0.372713, current_train_items 236224.
I0302 19:01:55.771794 22699590365312 run.py:483] Algo bellman_ford step 7382 current loss 0.592455, current_train_items 236256.
I0302 19:01:55.802418 22699590365312 run.py:483] Algo bellman_ford step 7383 current loss 0.627417, current_train_items 236288.
I0302 19:01:55.834679 22699590365312 run.py:483] Algo bellman_ford step 7384 current loss 0.616286, current_train_items 236320.
I0302 19:01:55.854657 22699590365312 run.py:483] Algo bellman_ford step 7385 current loss 0.246680, current_train_items 236352.
I0302 19:01:55.870775 22699590365312 run.py:483] Algo bellman_ford step 7386 current loss 0.407305, current_train_items 236384.
I0302 19:01:55.894911 22699590365312 run.py:483] Algo bellman_ford step 7387 current loss 0.505515, current_train_items 236416.
I0302 19:01:55.927248 22699590365312 run.py:483] Algo bellman_ford step 7388 current loss 0.543164, current_train_items 236448.
I0302 19:01:55.959300 22699590365312 run.py:483] Algo bellman_ford step 7389 current loss 0.665952, current_train_items 236480.
I0302 19:01:55.979288 22699590365312 run.py:483] Algo bellman_ford step 7390 current loss 0.279790, current_train_items 236512.
I0302 19:01:55.995694 22699590365312 run.py:483] Algo bellman_ford step 7391 current loss 0.489060, current_train_items 236544.
I0302 19:01:56.018749 22699590365312 run.py:483] Algo bellman_ford step 7392 current loss 0.596489, current_train_items 236576.
I0302 19:01:56.049074 22699590365312 run.py:483] Algo bellman_ford step 7393 current loss 0.615659, current_train_items 236608.
I0302 19:01:56.083079 22699590365312 run.py:483] Algo bellman_ford step 7394 current loss 0.778654, current_train_items 236640.
I0302 19:01:56.102562 22699590365312 run.py:483] Algo bellman_ford step 7395 current loss 0.308392, current_train_items 236672.
I0302 19:01:56.118884 22699590365312 run.py:483] Algo bellman_ford step 7396 current loss 0.383387, current_train_items 236704.
I0302 19:01:56.142974 22699590365312 run.py:483] Algo bellman_ford step 7397 current loss 0.565528, current_train_items 236736.
I0302 19:01:56.173575 22699590365312 run.py:483] Algo bellman_ford step 7398 current loss 0.551541, current_train_items 236768.
I0302 19:01:56.205521 22699590365312 run.py:483] Algo bellman_ford step 7399 current loss 0.575611, current_train_items 236800.
I0302 19:01:56.224895 22699590365312 run.py:483] Algo bellman_ford step 7400 current loss 0.287970, current_train_items 236832.
I0302 19:01:56.232656 22699590365312 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0302 19:01:56.232761 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:01:56.249826 22699590365312 run.py:483] Algo bellman_ford step 7401 current loss 0.489798, current_train_items 236864.
I0302 19:01:56.275174 22699590365312 run.py:483] Algo bellman_ford step 7402 current loss 0.613734, current_train_items 236896.
I0302 19:01:56.307887 22699590365312 run.py:483] Algo bellman_ford step 7403 current loss 0.628526, current_train_items 236928.
I0302 19:01:56.342301 22699590365312 run.py:483] Algo bellman_ford step 7404 current loss 0.773126, current_train_items 236960.
I0302 19:01:56.362361 22699590365312 run.py:483] Algo bellman_ford step 7405 current loss 0.297384, current_train_items 236992.
I0302 19:01:56.377802 22699590365312 run.py:483] Algo bellman_ford step 7406 current loss 0.365332, current_train_items 237024.
I0302 19:01:56.401884 22699590365312 run.py:483] Algo bellman_ford step 7407 current loss 0.473632, current_train_items 237056.
I0302 19:01:56.434056 22699590365312 run.py:483] Algo bellman_ford step 7408 current loss 0.577700, current_train_items 237088.
I0302 19:01:56.465794 22699590365312 run.py:483] Algo bellman_ford step 7409 current loss 0.652344, current_train_items 237120.
I0302 19:01:56.485596 22699590365312 run.py:483] Algo bellman_ford step 7410 current loss 0.292602, current_train_items 237152.
I0302 19:01:56.502207 22699590365312 run.py:483] Algo bellman_ford step 7411 current loss 0.423167, current_train_items 237184.
I0302 19:01:56.527178 22699590365312 run.py:483] Algo bellman_ford step 7412 current loss 0.563551, current_train_items 237216.
I0302 19:01:56.559209 22699590365312 run.py:483] Algo bellman_ford step 7413 current loss 0.650997, current_train_items 237248.
I0302 19:01:56.591238 22699590365312 run.py:483] Algo bellman_ford step 7414 current loss 0.731030, current_train_items 237280.
I0302 19:01:56.610831 22699590365312 run.py:483] Algo bellman_ford step 7415 current loss 0.316028, current_train_items 237312.
I0302 19:01:56.626607 22699590365312 run.py:483] Algo bellman_ford step 7416 current loss 0.433843, current_train_items 237344.
I0302 19:01:56.650439 22699590365312 run.py:483] Algo bellman_ford step 7417 current loss 0.589123, current_train_items 237376.
I0302 19:01:56.681899 22699590365312 run.py:483] Algo bellman_ford step 7418 current loss 0.645835, current_train_items 237408.
I0302 19:01:56.714322 22699590365312 run.py:483] Algo bellman_ford step 7419 current loss 0.746379, current_train_items 237440.
I0302 19:01:56.733996 22699590365312 run.py:483] Algo bellman_ford step 7420 current loss 0.280200, current_train_items 237472.
I0302 19:01:56.749793 22699590365312 run.py:483] Algo bellman_ford step 7421 current loss 0.470604, current_train_items 237504.
I0302 19:01:56.773382 22699590365312 run.py:483] Algo bellman_ford step 7422 current loss 0.525338, current_train_items 237536.
I0302 19:01:56.804267 22699590365312 run.py:483] Algo bellman_ford step 7423 current loss 0.645192, current_train_items 237568.
I0302 19:01:56.836998 22699590365312 run.py:483] Algo bellman_ford step 7424 current loss 0.641642, current_train_items 237600.
I0302 19:01:56.856561 22699590365312 run.py:483] Algo bellman_ford step 7425 current loss 0.254803, current_train_items 237632.
I0302 19:01:56.872486 22699590365312 run.py:483] Algo bellman_ford step 7426 current loss 0.407698, current_train_items 237664.
I0302 19:01:56.897199 22699590365312 run.py:483] Algo bellman_ford step 7427 current loss 0.576316, current_train_items 237696.
I0302 19:01:56.928244 22699590365312 run.py:483] Algo bellman_ford step 7428 current loss 0.649143, current_train_items 237728.
I0302 19:01:56.962139 22699590365312 run.py:483] Algo bellman_ford step 7429 current loss 0.791879, current_train_items 237760.
I0302 19:01:56.981841 22699590365312 run.py:483] Algo bellman_ford step 7430 current loss 0.261388, current_train_items 237792.
I0302 19:01:56.997576 22699590365312 run.py:483] Algo bellman_ford step 7431 current loss 0.415295, current_train_items 237824.
I0302 19:01:57.021802 22699590365312 run.py:483] Algo bellman_ford step 7432 current loss 0.644485, current_train_items 237856.
I0302 19:01:57.053337 22699590365312 run.py:483] Algo bellman_ford step 7433 current loss 0.616893, current_train_items 237888.
I0302 19:01:57.084216 22699590365312 run.py:483] Algo bellman_ford step 7434 current loss 0.786021, current_train_items 237920.
I0302 19:01:57.103880 22699590365312 run.py:483] Algo bellman_ford step 7435 current loss 0.297633, current_train_items 237952.
I0302 19:01:57.119886 22699590365312 run.py:483] Algo bellman_ford step 7436 current loss 0.484039, current_train_items 237984.
I0302 19:01:57.144363 22699590365312 run.py:483] Algo bellman_ford step 7437 current loss 0.589483, current_train_items 238016.
I0302 19:01:57.175402 22699590365312 run.py:483] Algo bellman_ford step 7438 current loss 0.707045, current_train_items 238048.
I0302 19:01:57.209783 22699590365312 run.py:483] Algo bellman_ford step 7439 current loss 0.774797, current_train_items 238080.
I0302 19:01:57.229373 22699590365312 run.py:483] Algo bellman_ford step 7440 current loss 0.358080, current_train_items 238112.
I0302 19:01:57.245831 22699590365312 run.py:483] Algo bellman_ford step 7441 current loss 0.519273, current_train_items 238144.
I0302 19:01:57.269002 22699590365312 run.py:483] Algo bellman_ford step 7442 current loss 0.597009, current_train_items 238176.
I0302 19:01:57.301072 22699590365312 run.py:483] Algo bellman_ford step 7443 current loss 0.881611, current_train_items 238208.
I0302 19:01:57.334899 22699590365312 run.py:483] Algo bellman_ford step 7444 current loss 0.903097, current_train_items 238240.
I0302 19:01:57.353935 22699590365312 run.py:483] Algo bellman_ford step 7445 current loss 0.275577, current_train_items 238272.
I0302 19:01:57.370002 22699590365312 run.py:483] Algo bellman_ford step 7446 current loss 0.446342, current_train_items 238304.
I0302 19:01:57.393493 22699590365312 run.py:483] Algo bellman_ford step 7447 current loss 0.545755, current_train_items 238336.
I0302 19:01:57.424279 22699590365312 run.py:483] Algo bellman_ford step 7448 current loss 0.580800, current_train_items 238368.
I0302 19:01:57.459070 22699590365312 run.py:483] Algo bellman_ford step 7449 current loss 0.717100, current_train_items 238400.
I0302 19:01:57.478691 22699590365312 run.py:483] Algo bellman_ford step 7450 current loss 0.314703, current_train_items 238432.
I0302 19:01:57.486772 22699590365312 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0302 19:01:57.486876 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 19:01:57.503696 22699590365312 run.py:483] Algo bellman_ford step 7451 current loss 0.484251, current_train_items 238464.
I0302 19:01:57.529220 22699590365312 run.py:483] Algo bellman_ford step 7452 current loss 0.621821, current_train_items 238496.
I0302 19:01:57.560346 22699590365312 run.py:483] Algo bellman_ford step 7453 current loss 0.624237, current_train_items 238528.
I0302 19:01:57.593195 22699590365312 run.py:483] Algo bellman_ford step 7454 current loss 0.810527, current_train_items 238560.
I0302 19:01:57.612829 22699590365312 run.py:483] Algo bellman_ford step 7455 current loss 0.279193, current_train_items 238592.
I0302 19:01:57.629068 22699590365312 run.py:483] Algo bellman_ford step 7456 current loss 0.388580, current_train_items 238624.
I0302 19:01:57.652703 22699590365312 run.py:483] Algo bellman_ford step 7457 current loss 0.546882, current_train_items 238656.
I0302 19:01:57.684341 22699590365312 run.py:483] Algo bellman_ford step 7458 current loss 0.704331, current_train_items 238688.
I0302 19:01:57.717473 22699590365312 run.py:483] Algo bellman_ford step 7459 current loss 1.097085, current_train_items 238720.
I0302 19:01:57.737455 22699590365312 run.py:483] Algo bellman_ford step 7460 current loss 0.218337, current_train_items 238752.
I0302 19:01:57.753988 22699590365312 run.py:483] Algo bellman_ford step 7461 current loss 0.475279, current_train_items 238784.
I0302 19:01:57.776726 22699590365312 run.py:483] Algo bellman_ford step 7462 current loss 0.583900, current_train_items 238816.
I0302 19:01:57.807656 22699590365312 run.py:483] Algo bellman_ford step 7463 current loss 0.648044, current_train_items 238848.
I0302 19:01:57.840850 22699590365312 run.py:483] Algo bellman_ford step 7464 current loss 0.812512, current_train_items 238880.
I0302 19:01:57.860385 22699590365312 run.py:483] Algo bellman_ford step 7465 current loss 0.293618, current_train_items 238912.
I0302 19:01:57.876224 22699590365312 run.py:483] Algo bellman_ford step 7466 current loss 0.394228, current_train_items 238944.
I0302 19:01:57.899146 22699590365312 run.py:483] Algo bellman_ford step 7467 current loss 0.608056, current_train_items 238976.
I0302 19:01:57.931418 22699590365312 run.py:483] Algo bellman_ford step 7468 current loss 0.678590, current_train_items 239008.
I0302 19:01:57.962915 22699590365312 run.py:483] Algo bellman_ford step 7469 current loss 0.761009, current_train_items 239040.
I0302 19:01:57.983055 22699590365312 run.py:483] Algo bellman_ford step 7470 current loss 0.244992, current_train_items 239072.
I0302 19:01:57.999152 22699590365312 run.py:483] Algo bellman_ford step 7471 current loss 0.417566, current_train_items 239104.
I0302 19:01:58.022283 22699590365312 run.py:483] Algo bellman_ford step 7472 current loss 0.652105, current_train_items 239136.
I0302 19:01:58.053760 22699590365312 run.py:483] Algo bellman_ford step 7473 current loss 0.709465, current_train_items 239168.
I0302 19:01:58.088453 22699590365312 run.py:483] Algo bellman_ford step 7474 current loss 0.754876, current_train_items 239200.
I0302 19:01:58.108302 22699590365312 run.py:483] Algo bellman_ford step 7475 current loss 0.247145, current_train_items 239232.
I0302 19:01:58.124552 22699590365312 run.py:483] Algo bellman_ford step 7476 current loss 0.472276, current_train_items 239264.
I0302 19:01:58.147281 22699590365312 run.py:483] Algo bellman_ford step 7477 current loss 0.586337, current_train_items 239296.
I0302 19:01:58.179146 22699590365312 run.py:483] Algo bellman_ford step 7478 current loss 0.657504, current_train_items 239328.
I0302 19:01:58.215443 22699590365312 run.py:483] Algo bellman_ford step 7479 current loss 0.909938, current_train_items 239360.
I0302 19:01:58.234763 22699590365312 run.py:483] Algo bellman_ford step 7480 current loss 0.327426, current_train_items 239392.
I0302 19:01:58.251015 22699590365312 run.py:483] Algo bellman_ford step 7481 current loss 0.456997, current_train_items 239424.
I0302 19:01:58.275955 22699590365312 run.py:483] Algo bellman_ford step 7482 current loss 0.650936, current_train_items 239456.
I0302 19:01:58.307813 22699590365312 run.py:483] Algo bellman_ford step 7483 current loss 0.662487, current_train_items 239488.
I0302 19:01:58.342075 22699590365312 run.py:483] Algo bellman_ford step 7484 current loss 0.777198, current_train_items 239520.
I0302 19:01:58.361852 22699590365312 run.py:483] Algo bellman_ford step 7485 current loss 0.283571, current_train_items 239552.
I0302 19:01:58.377895 22699590365312 run.py:483] Algo bellman_ford step 7486 current loss 0.524874, current_train_items 239584.
I0302 19:01:58.400628 22699590365312 run.py:483] Algo bellman_ford step 7487 current loss 0.475342, current_train_items 239616.
I0302 19:01:58.432266 22699590365312 run.py:483] Algo bellman_ford step 7488 current loss 0.691361, current_train_items 239648.
I0302 19:01:58.467260 22699590365312 run.py:483] Algo bellman_ford step 7489 current loss 0.748229, current_train_items 239680.
I0302 19:01:58.486895 22699590365312 run.py:483] Algo bellman_ford step 7490 current loss 0.355967, current_train_items 239712.
I0302 19:01:58.502565 22699590365312 run.py:483] Algo bellman_ford step 7491 current loss 0.518052, current_train_items 239744.
I0302 19:01:58.525080 22699590365312 run.py:483] Algo bellman_ford step 7492 current loss 0.492761, current_train_items 239776.
I0302 19:01:58.555902 22699590365312 run.py:483] Algo bellman_ford step 7493 current loss 0.578050, current_train_items 239808.
I0302 19:01:58.589041 22699590365312 run.py:483] Algo bellman_ford step 7494 current loss 0.717785, current_train_items 239840.
I0302 19:01:58.608675 22699590365312 run.py:483] Algo bellman_ford step 7495 current loss 0.356398, current_train_items 239872.
I0302 19:01:58.624855 22699590365312 run.py:483] Algo bellman_ford step 7496 current loss 0.401068, current_train_items 239904.
I0302 19:01:58.648725 22699590365312 run.py:483] Algo bellman_ford step 7497 current loss 0.489425, current_train_items 239936.
I0302 19:01:58.679223 22699590365312 run.py:483] Algo bellman_ford step 7498 current loss 0.605633, current_train_items 239968.
I0302 19:01:58.711953 22699590365312 run.py:483] Algo bellman_ford step 7499 current loss 0.754503, current_train_items 240000.
I0302 19:01:58.731606 22699590365312 run.py:483] Algo bellman_ford step 7500 current loss 0.311557, current_train_items 240032.
I0302 19:01:58.739436 22699590365312 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0302 19:01:58.739543 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:01:58.756201 22699590365312 run.py:483] Algo bellman_ford step 7501 current loss 0.376020, current_train_items 240064.
I0302 19:01:58.780102 22699590365312 run.py:483] Algo bellman_ford step 7502 current loss 0.588001, current_train_items 240096.
I0302 19:01:58.811815 22699590365312 run.py:483] Algo bellman_ford step 7503 current loss 0.589150, current_train_items 240128.
I0302 19:01:58.845422 22699590365312 run.py:483] Algo bellman_ford step 7504 current loss 0.662666, current_train_items 240160.
I0302 19:01:58.865431 22699590365312 run.py:483] Algo bellman_ford step 7505 current loss 0.276929, current_train_items 240192.
I0302 19:01:58.881342 22699590365312 run.py:483] Algo bellman_ford step 7506 current loss 0.449078, current_train_items 240224.
I0302 19:01:58.904934 22699590365312 run.py:483] Algo bellman_ford step 7507 current loss 0.498513, current_train_items 240256.
I0302 19:01:58.935927 22699590365312 run.py:483] Algo bellman_ford step 7508 current loss 0.599289, current_train_items 240288.
I0302 19:01:58.968386 22699590365312 run.py:483] Algo bellman_ford step 7509 current loss 0.718844, current_train_items 240320.
I0302 19:01:58.987806 22699590365312 run.py:483] Algo bellman_ford step 7510 current loss 0.249158, current_train_items 240352.
I0302 19:01:59.004258 22699590365312 run.py:483] Algo bellman_ford step 7511 current loss 0.487302, current_train_items 240384.
I0302 19:01:59.026877 22699590365312 run.py:483] Algo bellman_ford step 7512 current loss 0.602916, current_train_items 240416.
I0302 19:01:59.057997 22699590365312 run.py:483] Algo bellman_ford step 7513 current loss 0.682886, current_train_items 240448.
I0302 19:01:59.090194 22699590365312 run.py:483] Algo bellman_ford step 7514 current loss 0.630690, current_train_items 240480.
I0302 19:01:59.109728 22699590365312 run.py:483] Algo bellman_ford step 7515 current loss 0.310369, current_train_items 240512.
I0302 19:01:59.125114 22699590365312 run.py:483] Algo bellman_ford step 7516 current loss 0.498742, current_train_items 240544.
I0302 19:01:59.149269 22699590365312 run.py:483] Algo bellman_ford step 7517 current loss 0.570473, current_train_items 240576.
I0302 19:01:59.180944 22699590365312 run.py:483] Algo bellman_ford step 7518 current loss 0.639509, current_train_items 240608.
I0302 19:01:59.214418 22699590365312 run.py:483] Algo bellman_ford step 7519 current loss 0.808201, current_train_items 240640.
I0302 19:01:59.233914 22699590365312 run.py:483] Algo bellman_ford step 7520 current loss 0.280029, current_train_items 240672.
I0302 19:01:59.249982 22699590365312 run.py:483] Algo bellman_ford step 7521 current loss 0.471599, current_train_items 240704.
I0302 19:01:59.273761 22699590365312 run.py:483] Algo bellman_ford step 7522 current loss 0.522431, current_train_items 240736.
I0302 19:01:59.305261 22699590365312 run.py:483] Algo bellman_ford step 7523 current loss 0.725883, current_train_items 240768.
I0302 19:01:59.336086 22699590365312 run.py:483] Algo bellman_ford step 7524 current loss 0.591878, current_train_items 240800.
I0302 19:01:59.355218 22699590365312 run.py:483] Algo bellman_ford step 7525 current loss 0.200340, current_train_items 240832.
I0302 19:01:59.371085 22699590365312 run.py:483] Algo bellman_ford step 7526 current loss 0.442228, current_train_items 240864.
I0302 19:01:59.395555 22699590365312 run.py:483] Algo bellman_ford step 7527 current loss 0.679041, current_train_items 240896.
I0302 19:01:59.426129 22699590365312 run.py:483] Algo bellman_ford step 7528 current loss 0.637297, current_train_items 240928.
I0302 19:01:59.459089 22699590365312 run.py:483] Algo bellman_ford step 7529 current loss 0.695640, current_train_items 240960.
I0302 19:01:59.478572 22699590365312 run.py:483] Algo bellman_ford step 7530 current loss 0.258714, current_train_items 240992.
I0302 19:01:59.494435 22699590365312 run.py:483] Algo bellman_ford step 7531 current loss 0.398197, current_train_items 241024.
I0302 19:01:59.517284 22699590365312 run.py:483] Algo bellman_ford step 7532 current loss 0.567659, current_train_items 241056.
I0302 19:01:59.549807 22699590365312 run.py:483] Algo bellman_ford step 7533 current loss 0.613655, current_train_items 241088.
I0302 19:01:59.584401 22699590365312 run.py:483] Algo bellman_ford step 7534 current loss 0.791323, current_train_items 241120.
I0302 19:01:59.603924 22699590365312 run.py:483] Algo bellman_ford step 7535 current loss 0.267478, current_train_items 241152.
I0302 19:01:59.620085 22699590365312 run.py:483] Algo bellman_ford step 7536 current loss 0.433554, current_train_items 241184.
I0302 19:01:59.643674 22699590365312 run.py:483] Algo bellman_ford step 7537 current loss 0.597040, current_train_items 241216.
I0302 19:01:59.675393 22699590365312 run.py:483] Algo bellman_ford step 7538 current loss 0.602309, current_train_items 241248.
I0302 19:01:59.708271 22699590365312 run.py:483] Algo bellman_ford step 7539 current loss 0.663871, current_train_items 241280.
I0302 19:01:59.728066 22699590365312 run.py:483] Algo bellman_ford step 7540 current loss 0.268244, current_train_items 241312.
I0302 19:01:59.744487 22699590365312 run.py:483] Algo bellman_ford step 7541 current loss 0.490068, current_train_items 241344.
I0302 19:01:59.769168 22699590365312 run.py:483] Algo bellman_ford step 7542 current loss 0.540205, current_train_items 241376.
I0302 19:01:59.801185 22699590365312 run.py:483] Algo bellman_ford step 7543 current loss 0.642913, current_train_items 241408.
I0302 19:01:59.834661 22699590365312 run.py:483] Algo bellman_ford step 7544 current loss 0.635366, current_train_items 241440.
I0302 19:01:59.853929 22699590365312 run.py:483] Algo bellman_ford step 7545 current loss 0.217519, current_train_items 241472.
I0302 19:01:59.869911 22699590365312 run.py:483] Algo bellman_ford step 7546 current loss 0.528346, current_train_items 241504.
I0302 19:01:59.894255 22699590365312 run.py:483] Algo bellman_ford step 7547 current loss 0.671354, current_train_items 241536.
I0302 19:01:59.925961 22699590365312 run.py:483] Algo bellman_ford step 7548 current loss 0.814406, current_train_items 241568.
I0302 19:01:59.959445 22699590365312 run.py:483] Algo bellman_ford step 7549 current loss 0.702792, current_train_items 241600.
I0302 19:01:59.979326 22699590365312 run.py:483] Algo bellman_ford step 7550 current loss 0.277141, current_train_items 241632.
I0302 19:01:59.987360 22699590365312 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.9501953125, 'score': 0.9501953125, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0302 19:01:59.987466 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.945, current avg val score is 0.950, val scores are: bellman_ford: 0.950
I0302 19:02:00.017639 22699590365312 run.py:483] Algo bellman_ford step 7551 current loss 0.464993, current_train_items 241664.
I0302 19:02:00.041750 22699590365312 run.py:483] Algo bellman_ford step 7552 current loss 0.572268, current_train_items 241696.
I0302 19:02:00.074307 22699590365312 run.py:483] Algo bellman_ford step 7553 current loss 0.659937, current_train_items 241728.
I0302 19:02:00.108348 22699590365312 run.py:483] Algo bellman_ford step 7554 current loss 0.781361, current_train_items 241760.
I0302 19:02:00.128403 22699590365312 run.py:483] Algo bellman_ford step 7555 current loss 0.251652, current_train_items 241792.
I0302 19:02:00.144451 22699590365312 run.py:483] Algo bellman_ford step 7556 current loss 0.460851, current_train_items 241824.
I0302 19:02:00.168365 22699590365312 run.py:483] Algo bellman_ford step 7557 current loss 0.548719, current_train_items 241856.
I0302 19:02:00.200973 22699590365312 run.py:483] Algo bellman_ford step 7558 current loss 0.667356, current_train_items 241888.
I0302 19:02:00.233479 22699590365312 run.py:483] Algo bellman_ford step 7559 current loss 0.696421, current_train_items 241920.
I0302 19:02:00.253452 22699590365312 run.py:483] Algo bellman_ford step 7560 current loss 0.297893, current_train_items 241952.
I0302 19:02:00.270347 22699590365312 run.py:483] Algo bellman_ford step 7561 current loss 0.460052, current_train_items 241984.
I0302 19:02:00.293355 22699590365312 run.py:483] Algo bellman_ford step 7562 current loss 0.584911, current_train_items 242016.
I0302 19:02:00.325127 22699590365312 run.py:483] Algo bellman_ford step 7563 current loss 0.710614, current_train_items 242048.
I0302 19:02:00.358563 22699590365312 run.py:483] Algo bellman_ford step 7564 current loss 0.734341, current_train_items 242080.
I0302 19:02:00.378092 22699590365312 run.py:483] Algo bellman_ford step 7565 current loss 0.294294, current_train_items 242112.
I0302 19:02:00.393875 22699590365312 run.py:483] Algo bellman_ford step 7566 current loss 0.460218, current_train_items 242144.
I0302 19:02:00.417584 22699590365312 run.py:483] Algo bellman_ford step 7567 current loss 0.570545, current_train_items 242176.
I0302 19:02:00.448717 22699590365312 run.py:483] Algo bellman_ford step 7568 current loss 0.661689, current_train_items 242208.
I0302 19:02:00.481206 22699590365312 run.py:483] Algo bellman_ford step 7569 current loss 0.840204, current_train_items 242240.
I0302 19:02:00.500951 22699590365312 run.py:483] Algo bellman_ford step 7570 current loss 0.302456, current_train_items 242272.
I0302 19:02:00.517290 22699590365312 run.py:483] Algo bellman_ford step 7571 current loss 0.449477, current_train_items 242304.
I0302 19:02:00.540323 22699590365312 run.py:483] Algo bellman_ford step 7572 current loss 0.607269, current_train_items 242336.
I0302 19:02:00.570554 22699590365312 run.py:483] Algo bellman_ford step 7573 current loss 0.581697, current_train_items 242368.
I0302 19:02:00.601099 22699590365312 run.py:483] Algo bellman_ford step 7574 current loss 0.585154, current_train_items 242400.
I0302 19:02:00.620807 22699590365312 run.py:483] Algo bellman_ford step 7575 current loss 0.258265, current_train_items 242432.
I0302 19:02:00.637465 22699590365312 run.py:483] Algo bellman_ford step 7576 current loss 0.424045, current_train_items 242464.
I0302 19:02:00.659978 22699590365312 run.py:483] Algo bellman_ford step 7577 current loss 0.525270, current_train_items 242496.
I0302 19:02:00.691552 22699590365312 run.py:483] Algo bellman_ford step 7578 current loss 0.574968, current_train_items 242528.
I0302 19:02:00.725630 22699590365312 run.py:483] Algo bellman_ford step 7579 current loss 0.797651, current_train_items 242560.
I0302 19:02:00.744993 22699590365312 run.py:483] Algo bellman_ford step 7580 current loss 0.253992, current_train_items 242592.
I0302 19:02:00.761302 22699590365312 run.py:483] Algo bellman_ford step 7581 current loss 0.472270, current_train_items 242624.
I0302 19:02:00.784957 22699590365312 run.py:483] Algo bellman_ford step 7582 current loss 0.607436, current_train_items 242656.
I0302 19:02:00.816017 22699590365312 run.py:483] Algo bellman_ford step 7583 current loss 0.637469, current_train_items 242688.
I0302 19:02:00.848269 22699590365312 run.py:483] Algo bellman_ford step 7584 current loss 0.687439, current_train_items 242720.
I0302 19:02:00.868199 22699590365312 run.py:483] Algo bellman_ford step 7585 current loss 0.227228, current_train_items 242752.
I0302 19:02:00.883865 22699590365312 run.py:483] Algo bellman_ford step 7586 current loss 0.414212, current_train_items 242784.
I0302 19:02:00.907037 22699590365312 run.py:483] Algo bellman_ford step 7587 current loss 0.651532, current_train_items 242816.
I0302 19:02:00.935878 22699590365312 run.py:483] Algo bellman_ford step 7588 current loss 0.520598, current_train_items 242848.
I0302 19:02:00.969494 22699590365312 run.py:483] Algo bellman_ford step 7589 current loss 0.695012, current_train_items 242880.
I0302 19:02:00.989218 22699590365312 run.py:483] Algo bellman_ford step 7590 current loss 0.267314, current_train_items 242912.
I0302 19:02:01.005023 22699590365312 run.py:483] Algo bellman_ford step 7591 current loss 0.400988, current_train_items 242944.
I0302 19:02:01.029468 22699590365312 run.py:483] Algo bellman_ford step 7592 current loss 0.617474, current_train_items 242976.
I0302 19:02:01.059162 22699590365312 run.py:483] Algo bellman_ford step 7593 current loss 0.625363, current_train_items 243008.
I0302 19:02:01.092728 22699590365312 run.py:483] Algo bellman_ford step 7594 current loss 0.631833, current_train_items 243040.
I0302 19:02:01.112085 22699590365312 run.py:483] Algo bellman_ford step 7595 current loss 0.278280, current_train_items 243072.
I0302 19:02:01.127998 22699590365312 run.py:483] Algo bellman_ford step 7596 current loss 0.441849, current_train_items 243104.
I0302 19:02:01.152026 22699590365312 run.py:483] Algo bellman_ford step 7597 current loss 0.632014, current_train_items 243136.
I0302 19:02:01.183933 22699590365312 run.py:483] Algo bellman_ford step 7598 current loss 0.563345, current_train_items 243168.
I0302 19:02:01.214827 22699590365312 run.py:483] Algo bellman_ford step 7599 current loss 0.709068, current_train_items 243200.
I0302 19:02:01.234769 22699590365312 run.py:483] Algo bellman_ford step 7600 current loss 0.268405, current_train_items 243232.
I0302 19:02:01.242617 22699590365312 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0302 19:02:01.242725 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:01.259649 22699590365312 run.py:483] Algo bellman_ford step 7601 current loss 0.452775, current_train_items 243264.
I0302 19:02:01.285505 22699590365312 run.py:483] Algo bellman_ford step 7602 current loss 0.621952, current_train_items 243296.
I0302 19:02:01.316546 22699590365312 run.py:483] Algo bellman_ford step 7603 current loss 0.557119, current_train_items 243328.
I0302 19:02:01.350826 22699590365312 run.py:483] Algo bellman_ford step 7604 current loss 0.745488, current_train_items 243360.
I0302 19:02:01.370975 22699590365312 run.py:483] Algo bellman_ford step 7605 current loss 0.300967, current_train_items 243392.
I0302 19:02:01.386148 22699590365312 run.py:483] Algo bellman_ford step 7606 current loss 0.421910, current_train_items 243424.
I0302 19:02:01.409734 22699590365312 run.py:483] Algo bellman_ford step 7607 current loss 0.487213, current_train_items 243456.
I0302 19:02:01.441056 22699590365312 run.py:483] Algo bellman_ford step 7608 current loss 0.664581, current_train_items 243488.
I0302 19:02:01.475029 22699590365312 run.py:483] Algo bellman_ford step 7609 current loss 0.790245, current_train_items 243520.
I0302 19:02:01.494333 22699590365312 run.py:483] Algo bellman_ford step 7610 current loss 0.260139, current_train_items 243552.
I0302 19:02:01.510795 22699590365312 run.py:483] Algo bellman_ford step 7611 current loss 0.470228, current_train_items 243584.
I0302 19:02:01.534132 22699590365312 run.py:483] Algo bellman_ford step 7612 current loss 0.563516, current_train_items 243616.
I0302 19:02:01.563381 22699590365312 run.py:483] Algo bellman_ford step 7613 current loss 0.570229, current_train_items 243648.
I0302 19:02:01.595895 22699590365312 run.py:483] Algo bellman_ford step 7614 current loss 0.772980, current_train_items 243680.
I0302 19:02:01.615357 22699590365312 run.py:483] Algo bellman_ford step 7615 current loss 0.296644, current_train_items 243712.
I0302 19:02:01.631511 22699590365312 run.py:483] Algo bellman_ford step 7616 current loss 0.466283, current_train_items 243744.
I0302 19:02:01.655146 22699590365312 run.py:483] Algo bellman_ford step 7617 current loss 0.572192, current_train_items 243776.
I0302 19:02:01.685888 22699590365312 run.py:483] Algo bellman_ford step 7618 current loss 0.642888, current_train_items 243808.
I0302 19:02:01.719345 22699590365312 run.py:483] Algo bellman_ford step 7619 current loss 0.713252, current_train_items 243840.
I0302 19:02:01.738814 22699590365312 run.py:483] Algo bellman_ford step 7620 current loss 0.309399, current_train_items 243872.
I0302 19:02:01.754901 22699590365312 run.py:483] Algo bellman_ford step 7621 current loss 0.498219, current_train_items 243904.
I0302 19:02:01.778614 22699590365312 run.py:483] Algo bellman_ford step 7622 current loss 0.523069, current_train_items 243936.
I0302 19:02:01.811083 22699590365312 run.py:483] Algo bellman_ford step 7623 current loss 0.645205, current_train_items 243968.
I0302 19:02:01.843640 22699590365312 run.py:483] Algo bellman_ford step 7624 current loss 0.708998, current_train_items 244000.
I0302 19:02:01.862927 22699590365312 run.py:483] Algo bellman_ford step 7625 current loss 0.250025, current_train_items 244032.
I0302 19:02:01.878214 22699590365312 run.py:483] Algo bellman_ford step 7626 current loss 0.366994, current_train_items 244064.
I0302 19:02:01.902581 22699590365312 run.py:483] Algo bellman_ford step 7627 current loss 0.652983, current_train_items 244096.
I0302 19:02:01.935003 22699590365312 run.py:483] Algo bellman_ford step 7628 current loss 0.590791, current_train_items 244128.
I0302 19:02:01.967407 22699590365312 run.py:483] Algo bellman_ford step 7629 current loss 0.709446, current_train_items 244160.
I0302 19:02:01.987073 22699590365312 run.py:483] Algo bellman_ford step 7630 current loss 0.250370, current_train_items 244192.
I0302 19:02:02.003441 22699590365312 run.py:483] Algo bellman_ford step 7631 current loss 0.522745, current_train_items 244224.
I0302 19:02:02.026825 22699590365312 run.py:483] Algo bellman_ford step 7632 current loss 0.560790, current_train_items 244256.
I0302 19:02:02.058844 22699590365312 run.py:483] Algo bellman_ford step 7633 current loss 0.633406, current_train_items 244288.
I0302 19:02:02.092551 22699590365312 run.py:483] Algo bellman_ford step 7634 current loss 0.615311, current_train_items 244320.
I0302 19:02:02.112216 22699590365312 run.py:483] Algo bellman_ford step 7635 current loss 0.243165, current_train_items 244352.
I0302 19:02:02.128098 22699590365312 run.py:483] Algo bellman_ford step 7636 current loss 0.431000, current_train_items 244384.
I0302 19:02:02.151655 22699590365312 run.py:483] Algo bellman_ford step 7637 current loss 0.529667, current_train_items 244416.
I0302 19:02:02.181549 22699590365312 run.py:483] Algo bellman_ford step 7638 current loss 0.530439, current_train_items 244448.
I0302 19:02:02.216872 22699590365312 run.py:483] Algo bellman_ford step 7639 current loss 0.657020, current_train_items 244480.
I0302 19:02:02.236393 22699590365312 run.py:483] Algo bellman_ford step 7640 current loss 0.276366, current_train_items 244512.
I0302 19:02:02.252271 22699590365312 run.py:483] Algo bellman_ford step 7641 current loss 0.493070, current_train_items 244544.
I0302 19:02:02.274867 22699590365312 run.py:483] Algo bellman_ford step 7642 current loss 0.543222, current_train_items 244576.
I0302 19:02:02.308300 22699590365312 run.py:483] Algo bellman_ford step 7643 current loss 0.698559, current_train_items 244608.
I0302 19:02:02.343452 22699590365312 run.py:483] Algo bellman_ford step 7644 current loss 0.705442, current_train_items 244640.
I0302 19:02:02.362872 22699590365312 run.py:483] Algo bellman_ford step 7645 current loss 0.247290, current_train_items 244672.
I0302 19:02:02.378955 22699590365312 run.py:483] Algo bellman_ford step 7646 current loss 0.532118, current_train_items 244704.
I0302 19:02:02.403538 22699590365312 run.py:483] Algo bellman_ford step 7647 current loss 0.552119, current_train_items 244736.
I0302 19:02:02.436407 22699590365312 run.py:483] Algo bellman_ford step 7648 current loss 0.623848, current_train_items 244768.
I0302 19:02:02.471657 22699590365312 run.py:483] Algo bellman_ford step 7649 current loss 0.849503, current_train_items 244800.
I0302 19:02:02.491230 22699590365312 run.py:483] Algo bellman_ford step 7650 current loss 0.330754, current_train_items 244832.
I0302 19:02:02.499257 22699590365312 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.9404296875, 'score': 0.9404296875, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0302 19:02:02.499363 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.940, val scores are: bellman_ford: 0.940
I0302 19:02:02.515888 22699590365312 run.py:483] Algo bellman_ford step 7651 current loss 0.399799, current_train_items 244864.
I0302 19:02:02.541087 22699590365312 run.py:483] Algo bellman_ford step 7652 current loss 0.608330, current_train_items 244896.
I0302 19:02:02.572976 22699590365312 run.py:483] Algo bellman_ford step 7653 current loss 0.569469, current_train_items 244928.
I0302 19:02:02.606448 22699590365312 run.py:483] Algo bellman_ford step 7654 current loss 0.711854, current_train_items 244960.
I0302 19:02:02.626533 22699590365312 run.py:483] Algo bellman_ford step 7655 current loss 0.224473, current_train_items 244992.
I0302 19:02:02.642254 22699590365312 run.py:483] Algo bellman_ford step 7656 current loss 0.410919, current_train_items 245024.
I0302 19:02:02.666101 22699590365312 run.py:483] Algo bellman_ford step 7657 current loss 0.600288, current_train_items 245056.
I0302 19:02:02.697741 22699590365312 run.py:483] Algo bellman_ford step 7658 current loss 0.579953, current_train_items 245088.
I0302 19:02:02.732174 22699590365312 run.py:483] Algo bellman_ford step 7659 current loss 1.314526, current_train_items 245120.
I0302 19:02:02.752040 22699590365312 run.py:483] Algo bellman_ford step 7660 current loss 0.284777, current_train_items 245152.
I0302 19:02:02.768150 22699590365312 run.py:483] Algo bellman_ford step 7661 current loss 0.379568, current_train_items 245184.
I0302 19:02:02.791293 22699590365312 run.py:483] Algo bellman_ford step 7662 current loss 0.529906, current_train_items 245216.
I0302 19:02:02.822111 22699590365312 run.py:483] Algo bellman_ford step 7663 current loss 0.644572, current_train_items 245248.
I0302 19:02:02.855740 22699590365312 run.py:483] Algo bellman_ford step 7664 current loss 0.739979, current_train_items 245280.
I0302 19:02:02.875103 22699590365312 run.py:483] Algo bellman_ford step 7665 current loss 0.319200, current_train_items 245312.
I0302 19:02:02.891118 22699590365312 run.py:483] Algo bellman_ford step 7666 current loss 0.446870, current_train_items 245344.
I0302 19:02:02.914427 22699590365312 run.py:483] Algo bellman_ford step 7667 current loss 0.611232, current_train_items 245376.
I0302 19:02:02.948045 22699590365312 run.py:483] Algo bellman_ford step 7668 current loss 0.675139, current_train_items 245408.
I0302 19:02:02.982089 22699590365312 run.py:483] Algo bellman_ford step 7669 current loss 0.793830, current_train_items 245440.
I0302 19:02:03.001942 22699590365312 run.py:483] Algo bellman_ford step 7670 current loss 0.263143, current_train_items 245472.
I0302 19:02:03.018358 22699590365312 run.py:483] Algo bellman_ford step 7671 current loss 0.429249, current_train_items 245504.
I0302 19:02:03.041496 22699590365312 run.py:483] Algo bellman_ford step 7672 current loss 0.559767, current_train_items 245536.
I0302 19:02:03.073314 22699590365312 run.py:483] Algo bellman_ford step 7673 current loss 0.730832, current_train_items 245568.
I0302 19:02:03.107545 22699590365312 run.py:483] Algo bellman_ford step 7674 current loss 0.764921, current_train_items 245600.
I0302 19:02:03.127309 22699590365312 run.py:483] Algo bellman_ford step 7675 current loss 0.263580, current_train_items 245632.
I0302 19:02:03.143895 22699590365312 run.py:483] Algo bellman_ford step 7676 current loss 0.449602, current_train_items 245664.
I0302 19:02:03.168093 22699590365312 run.py:483] Algo bellman_ford step 7677 current loss 0.536869, current_train_items 245696.
I0302 19:02:03.199162 22699590365312 run.py:483] Algo bellman_ford step 7678 current loss 0.560321, current_train_items 245728.
I0302 19:02:03.233790 22699590365312 run.py:483] Algo bellman_ford step 7679 current loss 0.807611, current_train_items 245760.
I0302 19:02:03.253420 22699590365312 run.py:483] Algo bellman_ford step 7680 current loss 0.275914, current_train_items 245792.
I0302 19:02:03.269612 22699590365312 run.py:483] Algo bellman_ford step 7681 current loss 0.522120, current_train_items 245824.
I0302 19:02:03.292952 22699590365312 run.py:483] Algo bellman_ford step 7682 current loss 0.546145, current_train_items 245856.
I0302 19:02:03.324556 22699590365312 run.py:483] Algo bellman_ford step 7683 current loss 0.611923, current_train_items 245888.
I0302 19:02:03.358932 22699590365312 run.py:483] Algo bellman_ford step 7684 current loss 0.714942, current_train_items 245920.
I0302 19:02:03.379089 22699590365312 run.py:483] Algo bellman_ford step 7685 current loss 0.251805, current_train_items 245952.
I0302 19:02:03.395393 22699590365312 run.py:483] Algo bellman_ford step 7686 current loss 0.474920, current_train_items 245984.
I0302 19:02:03.417834 22699590365312 run.py:483] Algo bellman_ford step 7687 current loss 0.620528, current_train_items 246016.
I0302 19:02:03.450084 22699590365312 run.py:483] Algo bellman_ford step 7688 current loss 0.633557, current_train_items 246048.
I0302 19:02:03.483520 22699590365312 run.py:483] Algo bellman_ford step 7689 current loss 0.720763, current_train_items 246080.
I0302 19:02:03.503166 22699590365312 run.py:483] Algo bellman_ford step 7690 current loss 0.233438, current_train_items 246112.
I0302 19:02:03.519255 22699590365312 run.py:483] Algo bellman_ford step 7691 current loss 0.415924, current_train_items 246144.
I0302 19:02:03.542119 22699590365312 run.py:483] Algo bellman_ford step 7692 current loss 0.546300, current_train_items 246176.
I0302 19:02:03.574888 22699590365312 run.py:483] Algo bellman_ford step 7693 current loss 0.614674, current_train_items 246208.
I0302 19:02:03.607759 22699590365312 run.py:483] Algo bellman_ford step 7694 current loss 0.656555, current_train_items 246240.
I0302 19:02:03.626968 22699590365312 run.py:483] Algo bellman_ford step 7695 current loss 0.254993, current_train_items 246272.
I0302 19:02:03.643181 22699590365312 run.py:483] Algo bellman_ford step 7696 current loss 0.403051, current_train_items 246304.
I0302 19:02:03.667189 22699590365312 run.py:483] Algo bellman_ford step 7697 current loss 0.548853, current_train_items 246336.
I0302 19:02:03.699532 22699590365312 run.py:483] Algo bellman_ford step 7698 current loss 0.621686, current_train_items 246368.
I0302 19:02:03.734505 22699590365312 run.py:483] Algo bellman_ford step 7699 current loss 0.742571, current_train_items 246400.
I0302 19:02:03.754322 22699590365312 run.py:483] Algo bellman_ford step 7700 current loss 0.311324, current_train_items 246432.
I0302 19:02:03.762076 22699590365312 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0302 19:02:03.762191 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:02:03.779218 22699590365312 run.py:483] Algo bellman_ford step 7701 current loss 0.425280, current_train_items 246464.
I0302 19:02:03.803354 22699590365312 run.py:483] Algo bellman_ford step 7702 current loss 0.530210, current_train_items 246496.
I0302 19:02:03.834073 22699590365312 run.py:483] Algo bellman_ford step 7703 current loss 0.617154, current_train_items 246528.
I0302 19:02:03.867603 22699590365312 run.py:483] Algo bellman_ford step 7704 current loss 0.902440, current_train_items 246560.
I0302 19:02:03.887889 22699590365312 run.py:483] Algo bellman_ford step 7705 current loss 0.249975, current_train_items 246592.
I0302 19:02:03.903749 22699590365312 run.py:483] Algo bellman_ford step 7706 current loss 0.432961, current_train_items 246624.
I0302 19:02:03.929019 22699590365312 run.py:483] Algo bellman_ford step 7707 current loss 0.546531, current_train_items 246656.
I0302 19:02:03.959514 22699590365312 run.py:483] Algo bellman_ford step 7708 current loss 0.593497, current_train_items 246688.
I0302 19:02:03.992000 22699590365312 run.py:483] Algo bellman_ford step 7709 current loss 0.770849, current_train_items 246720.
I0302 19:02:04.011834 22699590365312 run.py:483] Algo bellman_ford step 7710 current loss 0.300971, current_train_items 246752.
I0302 19:02:04.027996 22699590365312 run.py:483] Algo bellman_ford step 7711 current loss 0.420636, current_train_items 246784.
I0302 19:02:04.051917 22699590365312 run.py:483] Algo bellman_ford step 7712 current loss 0.623309, current_train_items 246816.
I0302 19:02:04.082359 22699590365312 run.py:483] Algo bellman_ford step 7713 current loss 0.769541, current_train_items 246848.
I0302 19:02:04.115526 22699590365312 run.py:483] Algo bellman_ford step 7714 current loss 0.709624, current_train_items 246880.
I0302 19:02:04.135189 22699590365312 run.py:483] Algo bellman_ford step 7715 current loss 0.317358, current_train_items 246912.
I0302 19:02:04.151073 22699590365312 run.py:483] Algo bellman_ford step 7716 current loss 0.402190, current_train_items 246944.
I0302 19:02:04.174846 22699590365312 run.py:483] Algo bellman_ford step 7717 current loss 0.540103, current_train_items 246976.
I0302 19:02:04.206318 22699590365312 run.py:483] Algo bellman_ford step 7718 current loss 0.590780, current_train_items 247008.
I0302 19:02:04.238310 22699590365312 run.py:483] Algo bellman_ford step 7719 current loss 0.618465, current_train_items 247040.
I0302 19:02:04.257954 22699590365312 run.py:483] Algo bellman_ford step 7720 current loss 0.318497, current_train_items 247072.
I0302 19:02:04.273784 22699590365312 run.py:483] Algo bellman_ford step 7721 current loss 0.406472, current_train_items 247104.
I0302 19:02:04.298264 22699590365312 run.py:483] Algo bellman_ford step 7722 current loss 0.618911, current_train_items 247136.
I0302 19:02:04.330441 22699590365312 run.py:483] Algo bellman_ford step 7723 current loss 0.690921, current_train_items 247168.
I0302 19:02:04.364651 22699590365312 run.py:483] Algo bellman_ford step 7724 current loss 0.882162, current_train_items 247200.
I0302 19:02:04.384091 22699590365312 run.py:483] Algo bellman_ford step 7725 current loss 0.300080, current_train_items 247232.
I0302 19:02:04.400045 22699590365312 run.py:483] Algo bellman_ford step 7726 current loss 0.516424, current_train_items 247264.
I0302 19:02:04.422693 22699590365312 run.py:483] Algo bellman_ford step 7727 current loss 0.614752, current_train_items 247296.
I0302 19:02:04.453738 22699590365312 run.py:483] Algo bellman_ford step 7728 current loss 0.766742, current_train_items 247328.
I0302 19:02:04.486079 22699590365312 run.py:483] Algo bellman_ford step 7729 current loss 0.835616, current_train_items 247360.
I0302 19:02:04.505807 22699590365312 run.py:483] Algo bellman_ford step 7730 current loss 0.257489, current_train_items 247392.
I0302 19:02:04.521547 22699590365312 run.py:483] Algo bellman_ford step 7731 current loss 0.437508, current_train_items 247424.
I0302 19:02:04.544672 22699590365312 run.py:483] Algo bellman_ford step 7732 current loss 0.567915, current_train_items 247456.
I0302 19:02:04.575967 22699590365312 run.py:483] Algo bellman_ford step 7733 current loss 0.704879, current_train_items 247488.
I0302 19:02:04.607686 22699590365312 run.py:483] Algo bellman_ford step 7734 current loss 0.698111, current_train_items 247520.
I0302 19:02:04.626812 22699590365312 run.py:483] Algo bellman_ford step 7735 current loss 0.277746, current_train_items 247552.
I0302 19:02:04.642879 22699590365312 run.py:483] Algo bellman_ford step 7736 current loss 0.534088, current_train_items 247584.
I0302 19:02:04.666463 22699590365312 run.py:483] Algo bellman_ford step 7737 current loss 0.551050, current_train_items 247616.
I0302 19:02:04.697016 22699590365312 run.py:483] Algo bellman_ford step 7738 current loss 0.742875, current_train_items 247648.
I0302 19:02:04.730904 22699590365312 run.py:483] Algo bellman_ford step 7739 current loss 0.750287, current_train_items 247680.
I0302 19:02:04.750244 22699590365312 run.py:483] Algo bellman_ford step 7740 current loss 0.244783, current_train_items 247712.
I0302 19:02:04.766453 22699590365312 run.py:483] Algo bellman_ford step 7741 current loss 0.441805, current_train_items 247744.
I0302 19:02:04.791664 22699590365312 run.py:483] Algo bellman_ford step 7742 current loss 0.701433, current_train_items 247776.
I0302 19:02:04.823493 22699590365312 run.py:483] Algo bellman_ford step 7743 current loss 0.854118, current_train_items 247808.
I0302 19:02:04.856933 22699590365312 run.py:483] Algo bellman_ford step 7744 current loss 0.919269, current_train_items 247840.
I0302 19:02:04.876676 22699590365312 run.py:483] Algo bellman_ford step 7745 current loss 0.295392, current_train_items 247872.
I0302 19:02:04.892620 22699590365312 run.py:483] Algo bellman_ford step 7746 current loss 0.384112, current_train_items 247904.
I0302 19:02:04.916200 22699590365312 run.py:483] Algo bellman_ford step 7747 current loss 0.568900, current_train_items 247936.
I0302 19:02:04.949190 22699590365312 run.py:483] Algo bellman_ford step 7748 current loss 0.721133, current_train_items 247968.
I0302 19:02:04.983289 22699590365312 run.py:483] Algo bellman_ford step 7749 current loss 0.693870, current_train_items 248000.
I0302 19:02:05.003104 22699590365312 run.py:483] Algo bellman_ford step 7750 current loss 0.320413, current_train_items 248032.
I0302 19:02:05.011257 22699590365312 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0302 19:02:05.011362 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:02:05.028423 22699590365312 run.py:483] Algo bellman_ford step 7751 current loss 0.502549, current_train_items 248064.
I0302 19:02:05.052723 22699590365312 run.py:483] Algo bellman_ford step 7752 current loss 0.631120, current_train_items 248096.
I0302 19:02:05.083932 22699590365312 run.py:483] Algo bellman_ford step 7753 current loss 0.569842, current_train_items 248128.
I0302 19:02:05.119709 22699590365312 run.py:483] Algo bellman_ford step 7754 current loss 0.742622, current_train_items 248160.
I0302 19:02:05.139801 22699590365312 run.py:483] Algo bellman_ford step 7755 current loss 0.281637, current_train_items 248192.
I0302 19:02:05.155171 22699590365312 run.py:483] Algo bellman_ford step 7756 current loss 0.438428, current_train_items 248224.
I0302 19:02:05.179032 22699590365312 run.py:483] Algo bellman_ford step 7757 current loss 0.555640, current_train_items 248256.
I0302 19:02:05.209374 22699590365312 run.py:483] Algo bellman_ford step 7758 current loss 0.646950, current_train_items 248288.
I0302 19:02:05.242376 22699590365312 run.py:483] Algo bellman_ford step 7759 current loss 0.671396, current_train_items 248320.
I0302 19:02:05.262093 22699590365312 run.py:483] Algo bellman_ford step 7760 current loss 0.281387, current_train_items 248352.
I0302 19:02:05.278098 22699590365312 run.py:483] Algo bellman_ford step 7761 current loss 0.465223, current_train_items 248384.
I0302 19:02:05.302199 22699590365312 run.py:483] Algo bellman_ford step 7762 current loss 0.590433, current_train_items 248416.
I0302 19:02:05.333351 22699590365312 run.py:483] Algo bellman_ford step 7763 current loss 0.683083, current_train_items 248448.
I0302 19:02:05.365378 22699590365312 run.py:483] Algo bellman_ford step 7764 current loss 0.635871, current_train_items 248480.
I0302 19:02:05.384837 22699590365312 run.py:483] Algo bellman_ford step 7765 current loss 0.335315, current_train_items 248512.
I0302 19:02:05.400820 22699590365312 run.py:483] Algo bellman_ford step 7766 current loss 0.463638, current_train_items 248544.
I0302 19:02:05.423954 22699590365312 run.py:483] Algo bellman_ford step 7767 current loss 0.480987, current_train_items 248576.
I0302 19:02:05.455443 22699590365312 run.py:483] Algo bellman_ford step 7768 current loss 0.544332, current_train_items 248608.
I0302 19:02:05.488758 22699590365312 run.py:483] Algo bellman_ford step 7769 current loss 0.697773, current_train_items 248640.
I0302 19:02:05.508477 22699590365312 run.py:483] Algo bellman_ford step 7770 current loss 0.329551, current_train_items 248672.
I0302 19:02:05.524688 22699590365312 run.py:483] Algo bellman_ford step 7771 current loss 0.421588, current_train_items 248704.
I0302 19:02:05.547989 22699590365312 run.py:483] Algo bellman_ford step 7772 current loss 0.527886, current_train_items 248736.
I0302 19:02:05.579801 22699590365312 run.py:483] Algo bellman_ford step 7773 current loss 0.661497, current_train_items 248768.
I0302 19:02:05.612739 22699590365312 run.py:483] Algo bellman_ford step 7774 current loss 0.771582, current_train_items 248800.
I0302 19:02:05.632249 22699590365312 run.py:483] Algo bellman_ford step 7775 current loss 0.268151, current_train_items 248832.
I0302 19:02:05.648337 22699590365312 run.py:483] Algo bellman_ford step 7776 current loss 0.533635, current_train_items 248864.
I0302 19:02:05.673045 22699590365312 run.py:483] Algo bellman_ford step 7777 current loss 0.605378, current_train_items 248896.
I0302 19:02:05.704873 22699590365312 run.py:483] Algo bellman_ford step 7778 current loss 0.586497, current_train_items 248928.
I0302 19:02:05.740141 22699590365312 run.py:483] Algo bellman_ford step 7779 current loss 0.674557, current_train_items 248960.
I0302 19:02:05.759307 22699590365312 run.py:483] Algo bellman_ford step 7780 current loss 0.191366, current_train_items 248992.
I0302 19:02:05.775597 22699590365312 run.py:483] Algo bellman_ford step 7781 current loss 0.496694, current_train_items 249024.
I0302 19:02:05.799334 22699590365312 run.py:483] Algo bellman_ford step 7782 current loss 0.552112, current_train_items 249056.
I0302 19:02:05.831429 22699590365312 run.py:483] Algo bellman_ford step 7783 current loss 0.732151, current_train_items 249088.
I0302 19:02:05.865121 22699590365312 run.py:483] Algo bellman_ford step 7784 current loss 0.698080, current_train_items 249120.
I0302 19:02:05.884986 22699590365312 run.py:483] Algo bellman_ford step 7785 current loss 0.419890, current_train_items 249152.
I0302 19:02:05.901083 22699590365312 run.py:483] Algo bellman_ford step 7786 current loss 0.474372, current_train_items 249184.
I0302 19:02:05.924225 22699590365312 run.py:483] Algo bellman_ford step 7787 current loss 0.487593, current_train_items 249216.
I0302 19:02:05.955117 22699590365312 run.py:483] Algo bellman_ford step 7788 current loss 0.595169, current_train_items 249248.
I0302 19:02:05.986557 22699590365312 run.py:483] Algo bellman_ford step 7789 current loss 0.726823, current_train_items 249280.
I0302 19:02:06.006349 22699590365312 run.py:483] Algo bellman_ford step 7790 current loss 0.284888, current_train_items 249312.
I0302 19:02:06.022008 22699590365312 run.py:483] Algo bellman_ford step 7791 current loss 0.439370, current_train_items 249344.
I0302 19:02:06.045209 22699590365312 run.py:483] Algo bellman_ford step 7792 current loss 0.541118, current_train_items 249376.
I0302 19:02:06.076209 22699590365312 run.py:483] Algo bellman_ford step 7793 current loss 0.647272, current_train_items 249408.
I0302 19:02:06.109464 22699590365312 run.py:483] Algo bellman_ford step 7794 current loss 0.831347, current_train_items 249440.
I0302 19:02:06.129050 22699590365312 run.py:483] Algo bellman_ford step 7795 current loss 0.347263, current_train_items 249472.
I0302 19:02:06.144672 22699590365312 run.py:483] Algo bellman_ford step 7796 current loss 0.489025, current_train_items 249504.
I0302 19:02:06.167454 22699590365312 run.py:483] Algo bellman_ford step 7797 current loss 0.571624, current_train_items 249536.
I0302 19:02:06.198261 22699590365312 run.py:483] Algo bellman_ford step 7798 current loss 0.532383, current_train_items 249568.
I0302 19:02:06.232630 22699590365312 run.py:483] Algo bellman_ford step 7799 current loss 0.717691, current_train_items 249600.
I0302 19:02:06.252531 22699590365312 run.py:483] Algo bellman_ford step 7800 current loss 0.319626, current_train_items 249632.
I0302 19:02:06.260322 22699590365312 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.8828125, 'score': 0.8828125, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0302 19:02:06.260426 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.883, val scores are: bellman_ford: 0.883
I0302 19:02:06.277200 22699590365312 run.py:483] Algo bellman_ford step 7801 current loss 0.479351, current_train_items 249664.
I0302 19:02:06.301332 22699590365312 run.py:483] Algo bellman_ford step 7802 current loss 0.673442, current_train_items 249696.
I0302 19:02:06.334447 22699590365312 run.py:483] Algo bellman_ford step 7803 current loss 0.705526, current_train_items 249728.
I0302 19:02:06.370897 22699590365312 run.py:483] Algo bellman_ford step 7804 current loss 0.916169, current_train_items 249760.
I0302 19:02:06.391139 22699590365312 run.py:483] Algo bellman_ford step 7805 current loss 0.249055, current_train_items 249792.
I0302 19:02:06.407243 22699590365312 run.py:483] Algo bellman_ford step 7806 current loss 0.401683, current_train_items 249824.
I0302 19:02:06.429907 22699590365312 run.py:483] Algo bellman_ford step 7807 current loss 0.480025, current_train_items 249856.
I0302 19:02:06.460612 22699590365312 run.py:483] Algo bellman_ford step 7808 current loss 0.570731, current_train_items 249888.
I0302 19:02:06.494510 22699590365312 run.py:483] Algo bellman_ford step 7809 current loss 0.737622, current_train_items 249920.
I0302 19:02:06.514225 22699590365312 run.py:483] Algo bellman_ford step 7810 current loss 0.318526, current_train_items 249952.
I0302 19:02:06.530801 22699590365312 run.py:483] Algo bellman_ford step 7811 current loss 0.481422, current_train_items 249984.
I0302 19:02:06.554742 22699590365312 run.py:483] Algo bellman_ford step 7812 current loss 0.537770, current_train_items 250016.
I0302 19:02:06.587030 22699590365312 run.py:483] Algo bellman_ford step 7813 current loss 0.634656, current_train_items 250048.
I0302 19:02:06.620865 22699590365312 run.py:483] Algo bellman_ford step 7814 current loss 0.705478, current_train_items 250080.
I0302 19:02:06.640136 22699590365312 run.py:483] Algo bellman_ford step 7815 current loss 0.319348, current_train_items 250112.
I0302 19:02:06.656097 22699590365312 run.py:483] Algo bellman_ford step 7816 current loss 0.456244, current_train_items 250144.
I0302 19:02:06.680345 22699590365312 run.py:483] Algo bellman_ford step 7817 current loss 0.561082, current_train_items 250176.
I0302 19:02:06.710653 22699590365312 run.py:483] Algo bellman_ford step 7818 current loss 0.571723, current_train_items 250208.
I0302 19:02:06.743408 22699590365312 run.py:483] Algo bellman_ford step 7819 current loss 0.711308, current_train_items 250240.
I0302 19:02:06.763261 22699590365312 run.py:483] Algo bellman_ford step 7820 current loss 0.343913, current_train_items 250272.
I0302 19:02:06.779685 22699590365312 run.py:483] Algo bellman_ford step 7821 current loss 0.462333, current_train_items 250304.
I0302 19:02:06.804759 22699590365312 run.py:483] Algo bellman_ford step 7822 current loss 0.473429, current_train_items 250336.
I0302 19:02:06.837921 22699590365312 run.py:483] Algo bellman_ford step 7823 current loss 0.624908, current_train_items 250368.
I0302 19:02:06.870500 22699590365312 run.py:483] Algo bellman_ford step 7824 current loss 0.732622, current_train_items 250400.
I0302 19:02:06.890406 22699590365312 run.py:483] Algo bellman_ford step 7825 current loss 0.241303, current_train_items 250432.
I0302 19:02:06.906751 22699590365312 run.py:483] Algo bellman_ford step 7826 current loss 0.439094, current_train_items 250464.
I0302 19:02:06.930964 22699590365312 run.py:483] Algo bellman_ford step 7827 current loss 0.576828, current_train_items 250496.
I0302 19:02:06.962705 22699590365312 run.py:483] Algo bellman_ford step 7828 current loss 0.666525, current_train_items 250528.
I0302 19:02:06.996279 22699590365312 run.py:483] Algo bellman_ford step 7829 current loss 0.770611, current_train_items 250560.
I0302 19:02:07.015716 22699590365312 run.py:483] Algo bellman_ford step 7830 current loss 0.219063, current_train_items 250592.
I0302 19:02:07.031452 22699590365312 run.py:483] Algo bellman_ford step 7831 current loss 0.482323, current_train_items 250624.
I0302 19:02:07.054860 22699590365312 run.py:483] Algo bellman_ford step 7832 current loss 0.590339, current_train_items 250656.
I0302 19:02:07.086869 22699590365312 run.py:483] Algo bellman_ford step 7833 current loss 0.638851, current_train_items 250688.
I0302 19:02:07.118212 22699590365312 run.py:483] Algo bellman_ford step 7834 current loss 0.752615, current_train_items 250720.
I0302 19:02:07.137501 22699590365312 run.py:483] Algo bellman_ford step 7835 current loss 0.274621, current_train_items 250752.
I0302 19:02:07.153731 22699590365312 run.py:483] Algo bellman_ford step 7836 current loss 0.424328, current_train_items 250784.
I0302 19:02:07.177316 22699590365312 run.py:483] Algo bellman_ford step 7837 current loss 0.520516, current_train_items 250816.
I0302 19:02:07.210496 22699590365312 run.py:483] Algo bellman_ford step 7838 current loss 0.648473, current_train_items 250848.
I0302 19:02:07.245336 22699590365312 run.py:483] Algo bellman_ford step 7839 current loss 1.235387, current_train_items 250880.
I0302 19:02:07.264765 22699590365312 run.py:483] Algo bellman_ford step 7840 current loss 0.231473, current_train_items 250912.
I0302 19:02:07.280997 22699590365312 run.py:483] Algo bellman_ford step 7841 current loss 0.425182, current_train_items 250944.
I0302 19:02:07.304675 22699590365312 run.py:483] Algo bellman_ford step 7842 current loss 0.564343, current_train_items 250976.
I0302 19:02:07.337688 22699590365312 run.py:483] Algo bellman_ford step 7843 current loss 0.580584, current_train_items 251008.
I0302 19:02:07.372870 22699590365312 run.py:483] Algo bellman_ford step 7844 current loss 0.742628, current_train_items 251040.
I0302 19:02:07.392302 22699590365312 run.py:483] Algo bellman_ford step 7845 current loss 0.394523, current_train_items 251072.
I0302 19:02:07.408289 22699590365312 run.py:483] Algo bellman_ford step 7846 current loss 0.518219, current_train_items 251104.
I0302 19:02:07.431946 22699590365312 run.py:483] Algo bellman_ford step 7847 current loss 0.594101, current_train_items 251136.
I0302 19:02:07.463846 22699590365312 run.py:483] Algo bellman_ford step 7848 current loss 0.654066, current_train_items 251168.
I0302 19:02:07.497883 22699590365312 run.py:483] Algo bellman_ford step 7849 current loss 0.846065, current_train_items 251200.
I0302 19:02:07.517448 22699590365312 run.py:483] Algo bellman_ford step 7850 current loss 0.246951, current_train_items 251232.
I0302 19:02:07.525449 22699590365312 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0302 19:02:07.525555 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:02:07.542384 22699590365312 run.py:483] Algo bellman_ford step 7851 current loss 0.483105, current_train_items 251264.
I0302 19:02:07.566374 22699590365312 run.py:483] Algo bellman_ford step 7852 current loss 0.529388, current_train_items 251296.
I0302 19:02:07.599259 22699590365312 run.py:483] Algo bellman_ford step 7853 current loss 0.642765, current_train_items 251328.
I0302 19:02:07.635029 22699590365312 run.py:483] Algo bellman_ford step 7854 current loss 0.899857, current_train_items 251360.
I0302 19:02:07.654997 22699590365312 run.py:483] Algo bellman_ford step 7855 current loss 0.310184, current_train_items 251392.
I0302 19:02:07.671271 22699590365312 run.py:483] Algo bellman_ford step 7856 current loss 0.551896, current_train_items 251424.
I0302 19:02:07.695441 22699590365312 run.py:483] Algo bellman_ford step 7857 current loss 0.608822, current_train_items 251456.
I0302 19:02:07.727083 22699590365312 run.py:483] Algo bellman_ford step 7858 current loss 0.623906, current_train_items 251488.
I0302 19:02:07.761410 22699590365312 run.py:483] Algo bellman_ford step 7859 current loss 0.737340, current_train_items 251520.
I0302 19:02:07.781062 22699590365312 run.py:483] Algo bellman_ford step 7860 current loss 0.304615, current_train_items 251552.
I0302 19:02:07.796983 22699590365312 run.py:483] Algo bellman_ford step 7861 current loss 0.394637, current_train_items 251584.
I0302 19:02:07.820450 22699590365312 run.py:483] Algo bellman_ford step 7862 current loss 0.668435, current_train_items 251616.
I0302 19:02:07.851866 22699590365312 run.py:483] Algo bellman_ford step 7863 current loss 0.711339, current_train_items 251648.
I0302 19:02:07.884247 22699590365312 run.py:483] Algo bellman_ford step 7864 current loss 0.661279, current_train_items 251680.
I0302 19:02:07.903798 22699590365312 run.py:483] Algo bellman_ford step 7865 current loss 0.279555, current_train_items 251712.
I0302 19:02:07.920173 22699590365312 run.py:483] Algo bellman_ford step 7866 current loss 0.515728, current_train_items 251744.
I0302 19:02:07.944387 22699590365312 run.py:483] Algo bellman_ford step 7867 current loss 0.559196, current_train_items 251776.
I0302 19:02:07.975213 22699590365312 run.py:483] Algo bellman_ford step 7868 current loss 0.648301, current_train_items 251808.
I0302 19:02:08.007958 22699590365312 run.py:483] Algo bellman_ford step 7869 current loss 0.700319, current_train_items 251840.
I0302 19:02:08.027892 22699590365312 run.py:483] Algo bellman_ford step 7870 current loss 0.335578, current_train_items 251872.
I0302 19:02:08.043944 22699590365312 run.py:483] Algo bellman_ford step 7871 current loss 0.389260, current_train_items 251904.
I0302 19:02:08.067790 22699590365312 run.py:483] Algo bellman_ford step 7872 current loss 0.570358, current_train_items 251936.
I0302 19:02:08.098142 22699590365312 run.py:483] Algo bellman_ford step 7873 current loss 0.606285, current_train_items 251968.
I0302 19:02:08.131391 22699590365312 run.py:483] Algo bellman_ford step 7874 current loss 0.654325, current_train_items 252000.
I0302 19:02:08.151189 22699590365312 run.py:483] Algo bellman_ford step 7875 current loss 0.287768, current_train_items 252032.
I0302 19:02:08.166822 22699590365312 run.py:483] Algo bellman_ford step 7876 current loss 0.391725, current_train_items 252064.
I0302 19:02:08.189950 22699590365312 run.py:483] Algo bellman_ford step 7877 current loss 0.562823, current_train_items 252096.
I0302 19:02:08.221149 22699590365312 run.py:483] Algo bellman_ford step 7878 current loss 0.629645, current_train_items 252128.
I0302 19:02:08.255275 22699590365312 run.py:483] Algo bellman_ford step 7879 current loss 0.699135, current_train_items 252160.
I0302 19:02:08.274621 22699590365312 run.py:483] Algo bellman_ford step 7880 current loss 0.264873, current_train_items 252192.
I0302 19:02:08.290350 22699590365312 run.py:483] Algo bellman_ford step 7881 current loss 0.415138, current_train_items 252224.
I0302 19:02:08.313778 22699590365312 run.py:483] Algo bellman_ford step 7882 current loss 0.563180, current_train_items 252256.
I0302 19:02:08.345603 22699590365312 run.py:483] Algo bellman_ford step 7883 current loss 0.603916, current_train_items 252288.
I0302 19:02:08.379653 22699590365312 run.py:483] Algo bellman_ford step 7884 current loss 0.766008, current_train_items 252320.
I0302 19:02:08.399681 22699590365312 run.py:483] Algo bellman_ford step 7885 current loss 0.324104, current_train_items 252352.
I0302 19:02:08.415892 22699590365312 run.py:483] Algo bellman_ford step 7886 current loss 0.417100, current_train_items 252384.
I0302 19:02:08.438655 22699590365312 run.py:483] Algo bellman_ford step 7887 current loss 0.489266, current_train_items 252416.
I0302 19:02:08.468408 22699590365312 run.py:483] Algo bellman_ford step 7888 current loss 0.546327, current_train_items 252448.
I0302 19:02:08.501338 22699590365312 run.py:483] Algo bellman_ford step 7889 current loss 0.638154, current_train_items 252480.
I0302 19:02:08.521169 22699590365312 run.py:483] Algo bellman_ford step 7890 current loss 0.250287, current_train_items 252512.
I0302 19:02:08.537147 22699590365312 run.py:483] Algo bellman_ford step 7891 current loss 0.437583, current_train_items 252544.
I0302 19:02:08.560564 22699590365312 run.py:483] Algo bellman_ford step 7892 current loss 0.528130, current_train_items 252576.
I0302 19:02:08.592342 22699590365312 run.py:483] Algo bellman_ford step 7893 current loss 0.598419, current_train_items 252608.
I0302 19:02:08.625269 22699590365312 run.py:483] Algo bellman_ford step 7894 current loss 0.655734, current_train_items 252640.
I0302 19:02:08.644768 22699590365312 run.py:483] Algo bellman_ford step 7895 current loss 0.253678, current_train_items 252672.
I0302 19:02:08.660498 22699590365312 run.py:483] Algo bellman_ford step 7896 current loss 0.396486, current_train_items 252704.
I0302 19:02:08.684568 22699590365312 run.py:483] Algo bellman_ford step 7897 current loss 0.593583, current_train_items 252736.
I0302 19:02:08.716816 22699590365312 run.py:483] Algo bellman_ford step 7898 current loss 0.581176, current_train_items 252768.
I0302 19:02:08.750498 22699590365312 run.py:483] Algo bellman_ford step 7899 current loss 0.744247, current_train_items 252800.
I0302 19:02:08.770414 22699590365312 run.py:483] Algo bellman_ford step 7900 current loss 0.298782, current_train_items 252832.
I0302 19:02:08.778285 22699590365312 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0302 19:02:08.778390 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:02:08.795044 22699590365312 run.py:483] Algo bellman_ford step 7901 current loss 0.427781, current_train_items 252864.
I0302 19:02:08.819748 22699590365312 run.py:483] Algo bellman_ford step 7902 current loss 0.523389, current_train_items 252896.
I0302 19:02:08.852174 22699590365312 run.py:483] Algo bellman_ford step 7903 current loss 0.690394, current_train_items 252928.
I0302 19:02:08.888802 22699590365312 run.py:483] Algo bellman_ford step 7904 current loss 0.922957, current_train_items 252960.
I0302 19:02:08.908653 22699590365312 run.py:483] Algo bellman_ford step 7905 current loss 0.291555, current_train_items 252992.
I0302 19:02:08.925072 22699590365312 run.py:483] Algo bellman_ford step 7906 current loss 0.427573, current_train_items 253024.
I0302 19:02:08.949434 22699590365312 run.py:483] Algo bellman_ford step 7907 current loss 0.548690, current_train_items 253056.
I0302 19:02:08.980343 22699590365312 run.py:483] Algo bellman_ford step 7908 current loss 0.576413, current_train_items 253088.
I0302 19:02:09.011333 22699590365312 run.py:483] Algo bellman_ford step 7909 current loss 0.671323, current_train_items 253120.
I0302 19:02:09.030827 22699590365312 run.py:483] Algo bellman_ford step 7910 current loss 0.318057, current_train_items 253152.
I0302 19:02:09.047524 22699590365312 run.py:483] Algo bellman_ford step 7911 current loss 0.464456, current_train_items 253184.
I0302 19:02:09.072021 22699590365312 run.py:483] Algo bellman_ford step 7912 current loss 0.607485, current_train_items 253216.
I0302 19:02:09.103824 22699590365312 run.py:483] Algo bellman_ford step 7913 current loss 0.647217, current_train_items 253248.
I0302 19:02:09.137740 22699590365312 run.py:483] Algo bellman_ford step 7914 current loss 0.743190, current_train_items 253280.
I0302 19:02:09.157247 22699590365312 run.py:483] Algo bellman_ford step 7915 current loss 0.246337, current_train_items 253312.
I0302 19:02:09.173488 22699590365312 run.py:483] Algo bellman_ford step 7916 current loss 0.491048, current_train_items 253344.
I0302 19:02:09.197039 22699590365312 run.py:483] Algo bellman_ford step 7917 current loss 0.565812, current_train_items 253376.
I0302 19:02:09.228363 22699590365312 run.py:483] Algo bellman_ford step 7918 current loss 0.611452, current_train_items 253408.
I0302 19:02:09.263609 22699590365312 run.py:483] Algo bellman_ford step 7919 current loss 0.748695, current_train_items 253440.
I0302 19:02:09.283129 22699590365312 run.py:483] Algo bellman_ford step 7920 current loss 0.303908, current_train_items 253472.
I0302 19:02:09.299518 22699590365312 run.py:483] Algo bellman_ford step 7921 current loss 0.461786, current_train_items 253504.
I0302 19:02:09.323589 22699590365312 run.py:483] Algo bellman_ford step 7922 current loss 0.596572, current_train_items 253536.
I0302 19:02:09.354508 22699590365312 run.py:483] Algo bellman_ford step 7923 current loss 0.568563, current_train_items 253568.
I0302 19:02:09.388263 22699590365312 run.py:483] Algo bellman_ford step 7924 current loss 0.783528, current_train_items 253600.
I0302 19:02:09.407510 22699590365312 run.py:483] Algo bellman_ford step 7925 current loss 0.334361, current_train_items 253632.
I0302 19:02:09.423742 22699590365312 run.py:483] Algo bellman_ford step 7926 current loss 0.422086, current_train_items 253664.
I0302 19:02:09.447492 22699590365312 run.py:483] Algo bellman_ford step 7927 current loss 0.583958, current_train_items 253696.
I0302 19:02:09.479919 22699590365312 run.py:483] Algo bellman_ford step 7928 current loss 0.814756, current_train_items 253728.
I0302 19:02:09.512677 22699590365312 run.py:483] Algo bellman_ford step 7929 current loss 0.897358, current_train_items 253760.
I0302 19:02:09.532498 22699590365312 run.py:483] Algo bellman_ford step 7930 current loss 0.318498, current_train_items 253792.
I0302 19:02:09.548594 22699590365312 run.py:483] Algo bellman_ford step 7931 current loss 0.394507, current_train_items 253824.
I0302 19:02:09.571449 22699590365312 run.py:483] Algo bellman_ford step 7932 current loss 0.466228, current_train_items 253856.
I0302 19:02:09.603318 22699590365312 run.py:483] Algo bellman_ford step 7933 current loss 0.696286, current_train_items 253888.
I0302 19:02:09.637560 22699590365312 run.py:483] Algo bellman_ford step 7934 current loss 0.709400, current_train_items 253920.
I0302 19:02:09.656952 22699590365312 run.py:483] Algo bellman_ford step 7935 current loss 0.307746, current_train_items 253952.
I0302 19:02:09.672760 22699590365312 run.py:483] Algo bellman_ford step 7936 current loss 0.393121, current_train_items 253984.
I0302 19:02:09.696912 22699590365312 run.py:483] Algo bellman_ford step 7937 current loss 0.503751, current_train_items 254016.
I0302 19:02:09.727927 22699590365312 run.py:483] Algo bellman_ford step 7938 current loss 0.609942, current_train_items 254048.
I0302 19:02:09.762187 22699590365312 run.py:483] Algo bellman_ford step 7939 current loss 0.769196, current_train_items 254080.
I0302 19:02:09.781859 22699590365312 run.py:483] Algo bellman_ford step 7940 current loss 0.290963, current_train_items 254112.
I0302 19:02:09.797341 22699590365312 run.py:483] Algo bellman_ford step 7941 current loss 0.400348, current_train_items 254144.
I0302 19:02:09.821808 22699590365312 run.py:483] Algo bellman_ford step 7942 current loss 0.578764, current_train_items 254176.
I0302 19:02:09.854711 22699590365312 run.py:483] Algo bellman_ford step 7943 current loss 0.720320, current_train_items 254208.
I0302 19:02:09.887714 22699590365312 run.py:483] Algo bellman_ford step 7944 current loss 0.743791, current_train_items 254240.
I0302 19:02:09.907221 22699590365312 run.py:483] Algo bellman_ford step 7945 current loss 0.282754, current_train_items 254272.
I0302 19:02:09.922875 22699590365312 run.py:483] Algo bellman_ford step 7946 current loss 0.437837, current_train_items 254304.
I0302 19:02:09.945676 22699590365312 run.py:483] Algo bellman_ford step 7947 current loss 0.559392, current_train_items 254336.
I0302 19:02:09.975885 22699590365312 run.py:483] Algo bellman_ford step 7948 current loss 0.580660, current_train_items 254368.
I0302 19:02:10.009773 22699590365312 run.py:483] Algo bellman_ford step 7949 current loss 0.694287, current_train_items 254400.
I0302 19:02:10.029016 22699590365312 run.py:483] Algo bellman_ford step 7950 current loss 0.295912, current_train_items 254432.
I0302 19:02:10.037146 22699590365312 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0302 19:02:10.037260 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:02:10.054175 22699590365312 run.py:483] Algo bellman_ford step 7951 current loss 0.470223, current_train_items 254464.
I0302 19:02:10.079005 22699590365312 run.py:483] Algo bellman_ford step 7952 current loss 0.505540, current_train_items 254496.
I0302 19:02:10.111822 22699590365312 run.py:483] Algo bellman_ford step 7953 current loss 0.689167, current_train_items 254528.
I0302 19:02:10.145591 22699590365312 run.py:483] Algo bellman_ford step 7954 current loss 0.661621, current_train_items 254560.
I0302 19:02:10.166043 22699590365312 run.py:483] Algo bellman_ford step 7955 current loss 0.397507, current_train_items 254592.
I0302 19:02:10.182218 22699590365312 run.py:483] Algo bellman_ford step 7956 current loss 0.612123, current_train_items 254624.
I0302 19:02:10.206325 22699590365312 run.py:483] Algo bellman_ford step 7957 current loss 0.494059, current_train_items 254656.
I0302 19:02:10.238134 22699590365312 run.py:483] Algo bellman_ford step 7958 current loss 0.695565, current_train_items 254688.
I0302 19:02:10.271732 22699590365312 run.py:483] Algo bellman_ford step 7959 current loss 0.742319, current_train_items 254720.
I0302 19:02:10.291439 22699590365312 run.py:483] Algo bellman_ford step 7960 current loss 0.245459, current_train_items 254752.
I0302 19:02:10.307961 22699590365312 run.py:483] Algo bellman_ford step 7961 current loss 0.417285, current_train_items 254784.
I0302 19:02:10.331274 22699590365312 run.py:483] Algo bellman_ford step 7962 current loss 0.566489, current_train_items 254816.
I0302 19:02:10.362045 22699590365312 run.py:483] Algo bellman_ford step 7963 current loss 0.593614, current_train_items 254848.
I0302 19:02:10.395961 22699590365312 run.py:483] Algo bellman_ford step 7964 current loss 0.656482, current_train_items 254880.
I0302 19:02:10.415369 22699590365312 run.py:483] Algo bellman_ford step 7965 current loss 0.332205, current_train_items 254912.
I0302 19:02:10.431769 22699590365312 run.py:483] Algo bellman_ford step 7966 current loss 0.486569, current_train_items 254944.
I0302 19:02:10.456481 22699590365312 run.py:483] Algo bellman_ford step 7967 current loss 0.623845, current_train_items 254976.
I0302 19:02:10.487833 22699590365312 run.py:483] Algo bellman_ford step 7968 current loss 0.571960, current_train_items 255008.
I0302 19:02:10.520340 22699590365312 run.py:483] Algo bellman_ford step 7969 current loss 0.705613, current_train_items 255040.
I0302 19:02:10.539967 22699590365312 run.py:483] Algo bellman_ford step 7970 current loss 0.349234, current_train_items 255072.
I0302 19:02:10.556005 22699590365312 run.py:483] Algo bellman_ford step 7971 current loss 0.503959, current_train_items 255104.
I0302 19:02:10.580279 22699590365312 run.py:483] Algo bellman_ford step 7972 current loss 0.564757, current_train_items 255136.
I0302 19:02:10.611304 22699590365312 run.py:483] Algo bellman_ford step 7973 current loss 0.530462, current_train_items 255168.
I0302 19:02:10.646364 22699590365312 run.py:483] Algo bellman_ford step 7974 current loss 0.702613, current_train_items 255200.
I0302 19:02:10.666298 22699590365312 run.py:483] Algo bellman_ford step 7975 current loss 0.301392, current_train_items 255232.
I0302 19:02:10.682713 22699590365312 run.py:483] Algo bellman_ford step 7976 current loss 0.408713, current_train_items 255264.
I0302 19:02:10.706327 22699590365312 run.py:483] Algo bellman_ford step 7977 current loss 0.552936, current_train_items 255296.
I0302 19:02:10.737797 22699590365312 run.py:483] Algo bellman_ford step 7978 current loss 0.691278, current_train_items 255328.
I0302 19:02:10.769333 22699590365312 run.py:483] Algo bellman_ford step 7979 current loss 0.630951, current_train_items 255360.
I0302 19:02:10.789111 22699590365312 run.py:483] Algo bellman_ford step 7980 current loss 0.300454, current_train_items 255392.
I0302 19:02:10.804942 22699590365312 run.py:483] Algo bellman_ford step 7981 current loss 0.472825, current_train_items 255424.
I0302 19:02:10.828085 22699590365312 run.py:483] Algo bellman_ford step 7982 current loss 0.527789, current_train_items 255456.
I0302 19:02:10.858846 22699590365312 run.py:483] Algo bellman_ford step 7983 current loss 0.597225, current_train_items 255488.
I0302 19:02:10.892787 22699590365312 run.py:483] Algo bellman_ford step 7984 current loss 0.811237, current_train_items 255520.
I0302 19:02:10.912475 22699590365312 run.py:483] Algo bellman_ford step 7985 current loss 0.222324, current_train_items 255552.
I0302 19:02:10.928774 22699590365312 run.py:483] Algo bellman_ford step 7986 current loss 0.489594, current_train_items 255584.
I0302 19:02:10.951546 22699590365312 run.py:483] Algo bellman_ford step 7987 current loss 0.540511, current_train_items 255616.
I0302 19:02:10.983431 22699590365312 run.py:483] Algo bellman_ford step 7988 current loss 0.645934, current_train_items 255648.
I0302 19:02:11.016236 22699590365312 run.py:483] Algo bellman_ford step 7989 current loss 0.974269, current_train_items 255680.
I0302 19:02:11.035866 22699590365312 run.py:483] Algo bellman_ford step 7990 current loss 0.363324, current_train_items 255712.
I0302 19:02:11.052053 22699590365312 run.py:483] Algo bellman_ford step 7991 current loss 0.555849, current_train_items 255744.
I0302 19:02:11.075763 22699590365312 run.py:483] Algo bellman_ford step 7992 current loss 0.638005, current_train_items 255776.
I0302 19:02:11.108987 22699590365312 run.py:483] Algo bellman_ford step 7993 current loss 0.828919, current_train_items 255808.
I0302 19:02:11.143942 22699590365312 run.py:483] Algo bellman_ford step 7994 current loss 0.918064, current_train_items 255840.
I0302 19:02:11.163429 22699590365312 run.py:483] Algo bellman_ford step 7995 current loss 0.352474, current_train_items 255872.
I0302 19:02:11.178963 22699590365312 run.py:483] Algo bellman_ford step 7996 current loss 0.429538, current_train_items 255904.
I0302 19:02:11.202341 22699590365312 run.py:483] Algo bellman_ford step 7997 current loss 0.602962, current_train_items 255936.
I0302 19:02:11.233546 22699590365312 run.py:483] Algo bellman_ford step 7998 current loss 0.617650, current_train_items 255968.
I0302 19:02:11.265025 22699590365312 run.py:483] Algo bellman_ford step 7999 current loss 0.701458, current_train_items 256000.
I0302 19:02:11.284898 22699590365312 run.py:483] Algo bellman_ford step 8000 current loss 0.347567, current_train_items 256032.
I0302 19:02:11.292605 22699590365312 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0302 19:02:11.292711 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:02:11.309636 22699590365312 run.py:483] Algo bellman_ford step 8001 current loss 0.447884, current_train_items 256064.
I0302 19:02:11.333348 22699590365312 run.py:483] Algo bellman_ford step 8002 current loss 0.657886, current_train_items 256096.
I0302 19:02:11.365935 22699590365312 run.py:483] Algo bellman_ford step 8003 current loss 0.632068, current_train_items 256128.
I0302 19:02:11.398924 22699590365312 run.py:483] Algo bellman_ford step 8004 current loss 0.609466, current_train_items 256160.
I0302 19:02:11.418992 22699590365312 run.py:483] Algo bellman_ford step 8005 current loss 0.344108, current_train_items 256192.
I0302 19:02:11.434792 22699590365312 run.py:483] Algo bellman_ford step 8006 current loss 0.427032, current_train_items 256224.
I0302 19:02:11.458340 22699590365312 run.py:483] Algo bellman_ford step 8007 current loss 0.579261, current_train_items 256256.
I0302 19:02:11.490954 22699590365312 run.py:483] Algo bellman_ford step 8008 current loss 0.547645, current_train_items 256288.
I0302 19:02:11.524519 22699590365312 run.py:483] Algo bellman_ford step 8009 current loss 0.760982, current_train_items 256320.
I0302 19:02:11.543948 22699590365312 run.py:483] Algo bellman_ford step 8010 current loss 0.289668, current_train_items 256352.
I0302 19:02:11.559756 22699590365312 run.py:483] Algo bellman_ford step 8011 current loss 0.409576, current_train_items 256384.
I0302 19:02:11.584077 22699590365312 run.py:483] Algo bellman_ford step 8012 current loss 0.512075, current_train_items 256416.
I0302 19:02:11.615175 22699590365312 run.py:483] Algo bellman_ford step 8013 current loss 0.671268, current_train_items 256448.
I0302 19:02:11.646623 22699590365312 run.py:483] Algo bellman_ford step 8014 current loss 0.631359, current_train_items 256480.
I0302 19:02:11.666263 22699590365312 run.py:483] Algo bellman_ford step 8015 current loss 0.327910, current_train_items 256512.
I0302 19:02:11.682873 22699590365312 run.py:483] Algo bellman_ford step 8016 current loss 0.486405, current_train_items 256544.
I0302 19:02:11.705878 22699590365312 run.py:483] Algo bellman_ford step 8017 current loss 0.500750, current_train_items 256576.
I0302 19:02:11.737278 22699590365312 run.py:483] Algo bellman_ford step 8018 current loss 0.620124, current_train_items 256608.
I0302 19:02:11.769314 22699590365312 run.py:483] Algo bellman_ford step 8019 current loss 1.024796, current_train_items 256640.
I0302 19:02:11.788657 22699590365312 run.py:483] Algo bellman_ford step 8020 current loss 0.324258, current_train_items 256672.
I0302 19:02:11.804798 22699590365312 run.py:483] Algo bellman_ford step 8021 current loss 0.448564, current_train_items 256704.
I0302 19:02:11.828217 22699590365312 run.py:483] Algo bellman_ford step 8022 current loss 0.537969, current_train_items 256736.
I0302 19:02:11.858902 22699590365312 run.py:483] Algo bellman_ford step 8023 current loss 0.650718, current_train_items 256768.
I0302 19:02:11.893692 22699590365312 run.py:483] Algo bellman_ford step 8024 current loss 0.766508, current_train_items 256800.
I0302 19:02:11.913044 22699590365312 run.py:483] Algo bellman_ford step 8025 current loss 0.338621, current_train_items 256832.
I0302 19:02:11.929390 22699590365312 run.py:483] Algo bellman_ford step 8026 current loss 0.428686, current_train_items 256864.
I0302 19:02:11.953349 22699590365312 run.py:483] Algo bellman_ford step 8027 current loss 0.555408, current_train_items 256896.
I0302 19:02:11.983787 22699590365312 run.py:483] Algo bellman_ford step 8028 current loss 0.560609, current_train_items 256928.
I0302 19:02:12.017326 22699590365312 run.py:483] Algo bellman_ford step 8029 current loss 0.635055, current_train_items 256960.
I0302 19:02:12.036850 22699590365312 run.py:483] Algo bellman_ford step 8030 current loss 0.294914, current_train_items 256992.
I0302 19:02:12.052912 22699590365312 run.py:483] Algo bellman_ford step 8031 current loss 0.433515, current_train_items 257024.
I0302 19:02:12.076977 22699590365312 run.py:483] Algo bellman_ford step 8032 current loss 0.604675, current_train_items 257056.
I0302 19:02:12.108886 22699590365312 run.py:483] Algo bellman_ford step 8033 current loss 0.638215, current_train_items 257088.
I0302 19:02:12.143223 22699590365312 run.py:483] Algo bellman_ford step 8034 current loss 0.700985, current_train_items 257120.
I0302 19:02:12.162778 22699590365312 run.py:483] Algo bellman_ford step 8035 current loss 0.274657, current_train_items 257152.
I0302 19:02:12.179238 22699590365312 run.py:483] Algo bellman_ford step 8036 current loss 0.408477, current_train_items 257184.
I0302 19:02:12.202775 22699590365312 run.py:483] Algo bellman_ford step 8037 current loss 0.523777, current_train_items 257216.
I0302 19:02:12.233617 22699590365312 run.py:483] Algo bellman_ford step 8038 current loss 0.634315, current_train_items 257248.
I0302 19:02:12.268068 22699590365312 run.py:483] Algo bellman_ford step 8039 current loss 0.783726, current_train_items 257280.
I0302 19:02:12.287761 22699590365312 run.py:483] Algo bellman_ford step 8040 current loss 0.311901, current_train_items 257312.
I0302 19:02:12.303946 22699590365312 run.py:483] Algo bellman_ford step 8041 current loss 0.445838, current_train_items 257344.
I0302 19:02:12.327799 22699590365312 run.py:483] Algo bellman_ford step 8042 current loss 0.527847, current_train_items 257376.
I0302 19:02:12.359294 22699590365312 run.py:483] Algo bellman_ford step 8043 current loss 0.522885, current_train_items 257408.
I0302 19:02:12.393373 22699590365312 run.py:483] Algo bellman_ford step 8044 current loss 0.712305, current_train_items 257440.
I0302 19:02:12.413250 22699590365312 run.py:483] Algo bellman_ford step 8045 current loss 0.240323, current_train_items 257472.
I0302 19:02:12.429827 22699590365312 run.py:483] Algo bellman_ford step 8046 current loss 0.442247, current_train_items 257504.
I0302 19:02:12.452902 22699590365312 run.py:483] Algo bellman_ford step 8047 current loss 0.581763, current_train_items 257536.
I0302 19:02:12.484589 22699590365312 run.py:483] Algo bellman_ford step 8048 current loss 0.596804, current_train_items 257568.
I0302 19:02:12.516746 22699590365312 run.py:483] Algo bellman_ford step 8049 current loss 0.610453, current_train_items 257600.
I0302 19:02:12.536206 22699590365312 run.py:483] Algo bellman_ford step 8050 current loss 0.290640, current_train_items 257632.
I0302 19:02:12.544212 22699590365312 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0302 19:02:12.544317 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:02:12.561303 22699590365312 run.py:483] Algo bellman_ford step 8051 current loss 0.449932, current_train_items 257664.
I0302 19:02:12.585349 22699590365312 run.py:483] Algo bellman_ford step 8052 current loss 0.476406, current_train_items 257696.
I0302 19:02:12.617149 22699590365312 run.py:483] Algo bellman_ford step 8053 current loss 0.543125, current_train_items 257728.
I0302 19:02:12.652072 22699590365312 run.py:483] Algo bellman_ford step 8054 current loss 0.822847, current_train_items 257760.
I0302 19:02:12.671987 22699590365312 run.py:483] Algo bellman_ford step 8055 current loss 0.369208, current_train_items 257792.
I0302 19:02:12.687093 22699590365312 run.py:483] Algo bellman_ford step 8056 current loss 0.386513, current_train_items 257824.
I0302 19:02:12.709379 22699590365312 run.py:483] Algo bellman_ford step 8057 current loss 0.489236, current_train_items 257856.
I0302 19:02:12.741953 22699590365312 run.py:483] Algo bellman_ford step 8058 current loss 0.657864, current_train_items 257888.
I0302 19:02:12.777667 22699590365312 run.py:483] Algo bellman_ford step 8059 current loss 0.830386, current_train_items 257920.
I0302 19:02:12.797177 22699590365312 run.py:483] Algo bellman_ford step 8060 current loss 0.293643, current_train_items 257952.
I0302 19:02:12.812901 22699590365312 run.py:483] Algo bellman_ford step 8061 current loss 0.439877, current_train_items 257984.
I0302 19:02:12.837425 22699590365312 run.py:483] Algo bellman_ford step 8062 current loss 0.617691, current_train_items 258016.
I0302 19:02:12.868898 22699590365312 run.py:483] Algo bellman_ford step 8063 current loss 0.597167, current_train_items 258048.
I0302 19:02:12.901282 22699590365312 run.py:483] Algo bellman_ford step 8064 current loss 0.798316, current_train_items 258080.
I0302 19:02:12.920778 22699590365312 run.py:483] Algo bellman_ford step 8065 current loss 0.266300, current_train_items 258112.
I0302 19:02:12.936548 22699590365312 run.py:483] Algo bellman_ford step 8066 current loss 0.527069, current_train_items 258144.
I0302 19:02:12.959884 22699590365312 run.py:483] Algo bellman_ford step 8067 current loss 0.554267, current_train_items 258176.
I0302 19:02:12.991213 22699590365312 run.py:483] Algo bellman_ford step 8068 current loss 0.637739, current_train_items 258208.
I0302 19:02:13.023818 22699590365312 run.py:483] Algo bellman_ford step 8069 current loss 0.793530, current_train_items 258240.
I0302 19:02:13.043660 22699590365312 run.py:483] Algo bellman_ford step 8070 current loss 0.311455, current_train_items 258272.
I0302 19:02:13.060575 22699590365312 run.py:483] Algo bellman_ford step 8071 current loss 0.455855, current_train_items 258304.
I0302 19:02:13.084248 22699590365312 run.py:483] Algo bellman_ford step 8072 current loss 0.613271, current_train_items 258336.
I0302 19:02:13.115494 22699590365312 run.py:483] Algo bellman_ford step 8073 current loss 0.623623, current_train_items 258368.
I0302 19:02:13.150185 22699590365312 run.py:483] Algo bellman_ford step 8074 current loss 0.781871, current_train_items 258400.
I0302 19:02:13.169863 22699590365312 run.py:483] Algo bellman_ford step 8075 current loss 0.286147, current_train_items 258432.
I0302 19:02:13.185871 22699590365312 run.py:483] Algo bellman_ford step 8076 current loss 0.424822, current_train_items 258464.
I0302 19:02:13.209239 22699590365312 run.py:483] Algo bellman_ford step 8077 current loss 0.459343, current_train_items 258496.
I0302 19:02:13.239820 22699590365312 run.py:483] Algo bellman_ford step 8078 current loss 0.852068, current_train_items 258528.
I0302 19:02:13.271554 22699590365312 run.py:483] Algo bellman_ford step 8079 current loss 0.678938, current_train_items 258560.
I0302 19:02:13.290953 22699590365312 run.py:483] Algo bellman_ford step 8080 current loss 0.338572, current_train_items 258592.
I0302 19:02:13.307203 22699590365312 run.py:483] Algo bellman_ford step 8081 current loss 0.491601, current_train_items 258624.
I0302 19:02:13.330723 22699590365312 run.py:483] Algo bellman_ford step 8082 current loss 0.564425, current_train_items 258656.
I0302 19:02:13.362276 22699590365312 run.py:483] Algo bellman_ford step 8083 current loss 0.630565, current_train_items 258688.
I0302 19:02:13.398508 22699590365312 run.py:483] Algo bellman_ford step 8084 current loss 0.794635, current_train_items 258720.
I0302 19:02:13.418255 22699590365312 run.py:483] Algo bellman_ford step 8085 current loss 0.341006, current_train_items 258752.
I0302 19:02:13.435070 22699590365312 run.py:483] Algo bellman_ford step 8086 current loss 0.481318, current_train_items 258784.
I0302 19:02:13.457451 22699590365312 run.py:483] Algo bellman_ford step 8087 current loss 0.550361, current_train_items 258816.
I0302 19:02:13.490176 22699590365312 run.py:483] Algo bellman_ford step 8088 current loss 0.574838, current_train_items 258848.
I0302 19:02:13.522279 22699590365312 run.py:483] Algo bellman_ford step 8089 current loss 0.657810, current_train_items 258880.
I0302 19:02:13.542783 22699590365312 run.py:483] Algo bellman_ford step 8090 current loss 0.346220, current_train_items 258912.
I0302 19:02:13.558979 22699590365312 run.py:483] Algo bellman_ford step 8091 current loss 0.402218, current_train_items 258944.
I0302 19:02:13.583303 22699590365312 run.py:483] Algo bellman_ford step 8092 current loss 0.539808, current_train_items 258976.
I0302 19:02:13.613936 22699590365312 run.py:483] Algo bellman_ford step 8093 current loss 0.557860, current_train_items 259008.
I0302 19:02:13.647985 22699590365312 run.py:483] Algo bellman_ford step 8094 current loss 0.779209, current_train_items 259040.
I0302 19:02:13.667647 22699590365312 run.py:483] Algo bellman_ford step 8095 current loss 0.274609, current_train_items 259072.
I0302 19:02:13.683498 22699590365312 run.py:483] Algo bellman_ford step 8096 current loss 0.512681, current_train_items 259104.
I0302 19:02:13.708472 22699590365312 run.py:483] Algo bellman_ford step 8097 current loss 0.560448, current_train_items 259136.
I0302 19:02:13.740507 22699590365312 run.py:483] Algo bellman_ford step 8098 current loss 0.661056, current_train_items 259168.
I0302 19:02:13.773535 22699590365312 run.py:483] Algo bellman_ford step 8099 current loss 0.703542, current_train_items 259200.
I0302 19:02:13.793021 22699590365312 run.py:483] Algo bellman_ford step 8100 current loss 0.255107, current_train_items 259232.
I0302 19:02:13.800971 22699590365312 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0302 19:02:13.801084 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:02:13.817977 22699590365312 run.py:483] Algo bellman_ford step 8101 current loss 0.418555, current_train_items 259264.
I0302 19:02:13.843187 22699590365312 run.py:483] Algo bellman_ford step 8102 current loss 0.658318, current_train_items 259296.
I0302 19:02:13.874813 22699590365312 run.py:483] Algo bellman_ford step 8103 current loss 0.605186, current_train_items 259328.
I0302 19:02:13.908660 22699590365312 run.py:483] Algo bellman_ford step 8104 current loss 0.708560, current_train_items 259360.
I0302 19:02:13.929338 22699590365312 run.py:483] Algo bellman_ford step 8105 current loss 0.307640, current_train_items 259392.
I0302 19:02:13.945328 22699590365312 run.py:483] Algo bellman_ford step 8106 current loss 0.462696, current_train_items 259424.
I0302 19:02:13.968798 22699590365312 run.py:483] Algo bellman_ford step 8107 current loss 0.648506, current_train_items 259456.
I0302 19:02:13.999821 22699590365312 run.py:483] Algo bellman_ford step 8108 current loss 1.048617, current_train_items 259488.
I0302 19:02:14.033737 22699590365312 run.py:483] Algo bellman_ford step 8109 current loss 1.246097, current_train_items 259520.
I0302 19:02:14.053214 22699590365312 run.py:483] Algo bellman_ford step 8110 current loss 0.264530, current_train_items 259552.
I0302 19:02:14.068755 22699590365312 run.py:483] Algo bellman_ford step 8111 current loss 0.527564, current_train_items 259584.
I0302 19:02:14.091742 22699590365312 run.py:483] Algo bellman_ford step 8112 current loss 0.570899, current_train_items 259616.
I0302 19:02:14.121771 22699590365312 run.py:483] Algo bellman_ford step 8113 current loss 0.562888, current_train_items 259648.
I0302 19:02:14.156916 22699590365312 run.py:483] Algo bellman_ford step 8114 current loss 0.771056, current_train_items 259680.
I0302 19:02:14.176477 22699590365312 run.py:483] Algo bellman_ford step 8115 current loss 0.368522, current_train_items 259712.
I0302 19:02:14.192612 22699590365312 run.py:483] Algo bellman_ford step 8116 current loss 0.463989, current_train_items 259744.
I0302 19:02:14.216457 22699590365312 run.py:483] Algo bellman_ford step 8117 current loss 0.666864, current_train_items 259776.
I0302 19:02:14.248088 22699590365312 run.py:483] Algo bellman_ford step 8118 current loss 0.838856, current_train_items 259808.
I0302 19:02:14.281389 22699590365312 run.py:483] Algo bellman_ford step 8119 current loss 0.747606, current_train_items 259840.
I0302 19:02:14.300686 22699590365312 run.py:483] Algo bellman_ford step 8120 current loss 0.271113, current_train_items 259872.
I0302 19:02:14.316865 22699590365312 run.py:483] Algo bellman_ford step 8121 current loss 0.468954, current_train_items 259904.
I0302 19:02:14.340601 22699590365312 run.py:483] Algo bellman_ford step 8122 current loss 0.563086, current_train_items 259936.
I0302 19:02:14.372988 22699590365312 run.py:483] Algo bellman_ford step 8123 current loss 0.663984, current_train_items 259968.
I0302 19:02:14.405137 22699590365312 run.py:483] Algo bellman_ford step 8124 current loss 0.672448, current_train_items 260000.
I0302 19:02:14.424602 22699590365312 run.py:483] Algo bellman_ford step 8125 current loss 0.279286, current_train_items 260032.
I0302 19:02:14.440942 22699590365312 run.py:483] Algo bellman_ford step 8126 current loss 0.657969, current_train_items 260064.
I0302 19:02:14.465476 22699590365312 run.py:483] Algo bellman_ford step 8127 current loss 0.557677, current_train_items 260096.
I0302 19:02:14.497660 22699590365312 run.py:483] Algo bellman_ford step 8128 current loss 0.626696, current_train_items 260128.
I0302 19:02:14.533086 22699590365312 run.py:483] Algo bellman_ford step 8129 current loss 0.745252, current_train_items 260160.
I0302 19:02:14.552870 22699590365312 run.py:483] Algo bellman_ford step 8130 current loss 0.267392, current_train_items 260192.
I0302 19:02:14.568478 22699590365312 run.py:483] Algo bellman_ford step 8131 current loss 0.460255, current_train_items 260224.
I0302 19:02:14.592521 22699590365312 run.py:483] Algo bellman_ford step 8132 current loss 0.581281, current_train_items 260256.
I0302 19:02:14.623619 22699590365312 run.py:483] Algo bellman_ford step 8133 current loss 0.627161, current_train_items 260288.
I0302 19:02:14.657676 22699590365312 run.py:483] Algo bellman_ford step 8134 current loss 0.791703, current_train_items 260320.
I0302 19:02:14.677139 22699590365312 run.py:483] Algo bellman_ford step 8135 current loss 0.294762, current_train_items 260352.
I0302 19:02:14.693015 22699590365312 run.py:483] Algo bellman_ford step 8136 current loss 0.445230, current_train_items 260384.
I0302 19:02:14.716422 22699590365312 run.py:483] Algo bellman_ford step 8137 current loss 0.577354, current_train_items 260416.
I0302 19:02:14.747990 22699590365312 run.py:483] Algo bellman_ford step 8138 current loss 0.662233, current_train_items 260448.
I0302 19:02:14.783200 22699590365312 run.py:483] Algo bellman_ford step 8139 current loss 0.764337, current_train_items 260480.
I0302 19:02:14.802304 22699590365312 run.py:483] Algo bellman_ford step 8140 current loss 0.286586, current_train_items 260512.
I0302 19:02:14.818119 22699590365312 run.py:483] Algo bellman_ford step 8141 current loss 0.568284, current_train_items 260544.
I0302 19:02:14.842197 22699590365312 run.py:483] Algo bellman_ford step 8142 current loss 0.559530, current_train_items 260576.
I0302 19:02:14.874134 22699590365312 run.py:483] Algo bellman_ford step 8143 current loss 0.608872, current_train_items 260608.
I0302 19:02:14.908411 22699590365312 run.py:483] Algo bellman_ford step 8144 current loss 0.666656, current_train_items 260640.
I0302 19:02:14.927811 22699590365312 run.py:483] Algo bellman_ford step 8145 current loss 0.283165, current_train_items 260672.
I0302 19:02:14.944125 22699590365312 run.py:483] Algo bellman_ford step 8146 current loss 0.447778, current_train_items 260704.
I0302 19:02:14.967772 22699590365312 run.py:483] Algo bellman_ford step 8147 current loss 0.598071, current_train_items 260736.
I0302 19:02:14.999397 22699590365312 run.py:483] Algo bellman_ford step 8148 current loss 0.707010, current_train_items 260768.
I0302 19:02:15.032043 22699590365312 run.py:483] Algo bellman_ford step 8149 current loss 0.713245, current_train_items 260800.
I0302 19:02:15.051904 22699590365312 run.py:483] Algo bellman_ford step 8150 current loss 0.306421, current_train_items 260832.
I0302 19:02:15.059898 22699590365312 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.9482421875, 'score': 0.9482421875, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0302 19:02:15.060003 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.948, val scores are: bellman_ford: 0.948
I0302 19:02:15.076605 22699590365312 run.py:483] Algo bellman_ford step 8151 current loss 0.379859, current_train_items 260864.
I0302 19:02:15.101282 22699590365312 run.py:483] Algo bellman_ford step 8152 current loss 0.574126, current_train_items 260896.
I0302 19:02:15.131612 22699590365312 run.py:483] Algo bellman_ford step 8153 current loss 0.603064, current_train_items 260928.
I0302 19:02:15.166506 22699590365312 run.py:483] Algo bellman_ford step 8154 current loss 0.763869, current_train_items 260960.
I0302 19:02:15.186611 22699590365312 run.py:483] Algo bellman_ford step 8155 current loss 0.280281, current_train_items 260992.
I0302 19:02:15.201943 22699590365312 run.py:483] Algo bellman_ford step 8156 current loss 0.395488, current_train_items 261024.
I0302 19:02:15.225771 22699590365312 run.py:483] Algo bellman_ford step 8157 current loss 0.540694, current_train_items 261056.
I0302 19:02:15.257014 22699590365312 run.py:483] Algo bellman_ford step 8158 current loss 0.668735, current_train_items 261088.
I0302 19:02:15.290086 22699590365312 run.py:483] Algo bellman_ford step 8159 current loss 0.672848, current_train_items 261120.
I0302 19:02:15.309744 22699590365312 run.py:483] Algo bellman_ford step 8160 current loss 0.257404, current_train_items 261152.
I0302 19:02:15.326002 22699590365312 run.py:483] Algo bellman_ford step 8161 current loss 0.453990, current_train_items 261184.
I0302 19:02:15.349510 22699590365312 run.py:483] Algo bellman_ford step 8162 current loss 0.636156, current_train_items 261216.
I0302 19:02:15.381037 22699590365312 run.py:483] Algo bellman_ford step 8163 current loss 0.662491, current_train_items 261248.
I0302 19:02:15.410925 22699590365312 run.py:483] Algo bellman_ford step 8164 current loss 0.565951, current_train_items 261280.
I0302 19:02:15.430479 22699590365312 run.py:483] Algo bellman_ford step 8165 current loss 0.215704, current_train_items 261312.
I0302 19:02:15.446532 22699590365312 run.py:483] Algo bellman_ford step 8166 current loss 0.469189, current_train_items 261344.
I0302 19:02:15.469855 22699590365312 run.py:483] Algo bellman_ford step 8167 current loss 0.591370, current_train_items 261376.
I0302 19:02:15.500259 22699590365312 run.py:483] Algo bellman_ford step 8168 current loss 0.526665, current_train_items 261408.
I0302 19:02:15.534579 22699590365312 run.py:483] Algo bellman_ford step 8169 current loss 0.702963, current_train_items 261440.
I0302 19:02:15.554166 22699590365312 run.py:483] Algo bellman_ford step 8170 current loss 0.269088, current_train_items 261472.
I0302 19:02:15.570247 22699590365312 run.py:483] Algo bellman_ford step 8171 current loss 0.374452, current_train_items 261504.
I0302 19:02:15.594068 22699590365312 run.py:483] Algo bellman_ford step 8172 current loss 0.598494, current_train_items 261536.
I0302 19:02:15.625579 22699590365312 run.py:483] Algo bellman_ford step 8173 current loss 0.573586, current_train_items 261568.
I0302 19:02:15.657978 22699590365312 run.py:483] Algo bellman_ford step 8174 current loss 0.648580, current_train_items 261600.
I0302 19:02:15.677689 22699590365312 run.py:483] Algo bellman_ford step 8175 current loss 0.297902, current_train_items 261632.
I0302 19:02:15.694092 22699590365312 run.py:483] Algo bellman_ford step 8176 current loss 0.448008, current_train_items 261664.
I0302 19:02:15.717048 22699590365312 run.py:483] Algo bellman_ford step 8177 current loss 0.575264, current_train_items 261696.
I0302 19:02:15.749505 22699590365312 run.py:483] Algo bellman_ford step 8178 current loss 0.681771, current_train_items 261728.
I0302 19:02:15.780451 22699590365312 run.py:483] Algo bellman_ford step 8179 current loss 0.566551, current_train_items 261760.
I0302 19:02:15.800032 22699590365312 run.py:483] Algo bellman_ford step 8180 current loss 0.268715, current_train_items 261792.
I0302 19:02:15.815933 22699590365312 run.py:483] Algo bellman_ford step 8181 current loss 0.390346, current_train_items 261824.
I0302 19:02:15.839006 22699590365312 run.py:483] Algo bellman_ford step 8182 current loss 0.508850, current_train_items 261856.
I0302 19:02:15.871026 22699590365312 run.py:483] Algo bellman_ford step 8183 current loss 0.641040, current_train_items 261888.
I0302 19:02:15.906753 22699590365312 run.py:483] Algo bellman_ford step 8184 current loss 0.829305, current_train_items 261920.
I0302 19:02:15.926677 22699590365312 run.py:483] Algo bellman_ford step 8185 current loss 0.302126, current_train_items 261952.
I0302 19:02:15.942701 22699590365312 run.py:483] Algo bellman_ford step 8186 current loss 0.453395, current_train_items 261984.
I0302 19:02:15.966265 22699590365312 run.py:483] Algo bellman_ford step 8187 current loss 0.532128, current_train_items 262016.
I0302 19:02:15.997733 22699590365312 run.py:483] Algo bellman_ford step 8188 current loss 0.570380, current_train_items 262048.
I0302 19:02:16.030738 22699590365312 run.py:483] Algo bellman_ford step 8189 current loss 0.689428, current_train_items 262080.
I0302 19:02:16.050539 22699590365312 run.py:483] Algo bellman_ford step 8190 current loss 0.302800, current_train_items 262112.
I0302 19:02:16.066696 22699590365312 run.py:483] Algo bellman_ford step 8191 current loss 0.501266, current_train_items 262144.
I0302 19:02:16.090172 22699590365312 run.py:483] Algo bellman_ford step 8192 current loss 0.549167, current_train_items 262176.
I0302 19:02:16.121201 22699590365312 run.py:483] Algo bellman_ford step 8193 current loss 0.525525, current_train_items 262208.
I0302 19:02:16.154753 22699590365312 run.py:483] Algo bellman_ford step 8194 current loss 0.671667, current_train_items 262240.
I0302 19:02:16.174125 22699590365312 run.py:483] Algo bellman_ford step 8195 current loss 0.306877, current_train_items 262272.
I0302 19:02:16.190176 22699590365312 run.py:483] Algo bellman_ford step 8196 current loss 0.415933, current_train_items 262304.
I0302 19:02:16.213296 22699590365312 run.py:483] Algo bellman_ford step 8197 current loss 0.499909, current_train_items 262336.
I0302 19:02:16.244500 22699590365312 run.py:483] Algo bellman_ford step 8198 current loss 0.583959, current_train_items 262368.
I0302 19:02:16.277726 22699590365312 run.py:483] Algo bellman_ford step 8199 current loss 0.634647, current_train_items 262400.
I0302 19:02:16.297517 22699590365312 run.py:483] Algo bellman_ford step 8200 current loss 0.277586, current_train_items 262432.
I0302 19:02:16.305283 22699590365312 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0302 19:02:16.305392 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:02:16.321736 22699590365312 run.py:483] Algo bellman_ford step 8201 current loss 0.370555, current_train_items 262464.
I0302 19:02:16.345932 22699590365312 run.py:483] Algo bellman_ford step 8202 current loss 0.493742, current_train_items 262496.
I0302 19:02:16.377248 22699590365312 run.py:483] Algo bellman_ford step 8203 current loss 0.667123, current_train_items 262528.
I0302 19:02:16.412302 22699590365312 run.py:483] Algo bellman_ford step 8204 current loss 0.697710, current_train_items 262560.
I0302 19:02:16.432398 22699590365312 run.py:483] Algo bellman_ford step 8205 current loss 0.255952, current_train_items 262592.
I0302 19:02:16.448719 22699590365312 run.py:483] Algo bellman_ford step 8206 current loss 0.414955, current_train_items 262624.
I0302 19:02:16.473307 22699590365312 run.py:483] Algo bellman_ford step 8207 current loss 0.589008, current_train_items 262656.
I0302 19:02:16.504740 22699590365312 run.py:483] Algo bellman_ford step 8208 current loss 0.588762, current_train_items 262688.
I0302 19:02:16.535524 22699590365312 run.py:483] Algo bellman_ford step 8209 current loss 0.531327, current_train_items 262720.
I0302 19:02:16.555031 22699590365312 run.py:483] Algo bellman_ford step 8210 current loss 0.263214, current_train_items 262752.
I0302 19:02:16.570973 22699590365312 run.py:483] Algo bellman_ford step 8211 current loss 0.429913, current_train_items 262784.
I0302 19:02:16.594205 22699590365312 run.py:483] Algo bellman_ford step 8212 current loss 0.522942, current_train_items 262816.
I0302 19:02:16.625238 22699590365312 run.py:483] Algo bellman_ford step 8213 current loss 0.660822, current_train_items 262848.
I0302 19:02:16.658513 22699590365312 run.py:483] Algo bellman_ford step 8214 current loss 0.723236, current_train_items 262880.
I0302 19:02:16.677912 22699590365312 run.py:483] Algo bellman_ford step 8215 current loss 0.276689, current_train_items 262912.
I0302 19:02:16.693320 22699590365312 run.py:483] Algo bellman_ford step 8216 current loss 0.453097, current_train_items 262944.
I0302 19:02:16.715868 22699590365312 run.py:483] Algo bellman_ford step 8217 current loss 0.605598, current_train_items 262976.
I0302 19:02:16.748388 22699590365312 run.py:483] Algo bellman_ford step 8218 current loss 0.591206, current_train_items 263008.
I0302 19:02:16.782595 22699590365312 run.py:483] Algo bellman_ford step 8219 current loss 0.660757, current_train_items 263040.
I0302 19:02:16.802065 22699590365312 run.py:483] Algo bellman_ford step 8220 current loss 0.288916, current_train_items 263072.
I0302 19:02:16.817814 22699590365312 run.py:483] Algo bellman_ford step 8221 current loss 0.473996, current_train_items 263104.
I0302 19:02:16.842063 22699590365312 run.py:483] Algo bellman_ford step 8222 current loss 0.642219, current_train_items 263136.
I0302 19:02:16.873934 22699590365312 run.py:483] Algo bellman_ford step 8223 current loss 0.634429, current_train_items 263168.
I0302 19:02:16.904959 22699590365312 run.py:483] Algo bellman_ford step 8224 current loss 0.668473, current_train_items 263200.
I0302 19:02:16.924517 22699590365312 run.py:483] Algo bellman_ford step 8225 current loss 0.240739, current_train_items 263232.
I0302 19:02:16.940783 22699590365312 run.py:483] Algo bellman_ford step 8226 current loss 0.404405, current_train_items 263264.
I0302 19:02:16.965158 22699590365312 run.py:483] Algo bellman_ford step 8227 current loss 0.602971, current_train_items 263296.
I0302 19:02:16.995541 22699590365312 run.py:483] Algo bellman_ford step 8228 current loss 0.558188, current_train_items 263328.
I0302 19:02:17.028821 22699590365312 run.py:483] Algo bellman_ford step 8229 current loss 0.734971, current_train_items 263360.
I0302 19:02:17.048460 22699590365312 run.py:483] Algo bellman_ford step 8230 current loss 0.252901, current_train_items 263392.
I0302 19:02:17.064329 22699590365312 run.py:483] Algo bellman_ford step 8231 current loss 0.493382, current_train_items 263424.
I0302 19:02:17.088484 22699590365312 run.py:483] Algo bellman_ford step 8232 current loss 0.554750, current_train_items 263456.
I0302 19:02:17.119806 22699590365312 run.py:483] Algo bellman_ford step 8233 current loss 0.564326, current_train_items 263488.
I0302 19:02:17.152611 22699590365312 run.py:483] Algo bellman_ford step 8234 current loss 0.699453, current_train_items 263520.
I0302 19:02:17.171814 22699590365312 run.py:483] Algo bellman_ford step 8235 current loss 0.243836, current_train_items 263552.
I0302 19:02:17.188257 22699590365312 run.py:483] Algo bellman_ford step 8236 current loss 0.443355, current_train_items 263584.
I0302 19:02:17.212835 22699590365312 run.py:483] Algo bellman_ford step 8237 current loss 0.629128, current_train_items 263616.
I0302 19:02:17.244610 22699590365312 run.py:483] Algo bellman_ford step 8238 current loss 0.631321, current_train_items 263648.
I0302 19:02:17.277404 22699590365312 run.py:483] Algo bellman_ford step 8239 current loss 0.638720, current_train_items 263680.
I0302 19:02:17.296879 22699590365312 run.py:483] Algo bellman_ford step 8240 current loss 0.230540, current_train_items 263712.
I0302 19:02:17.313063 22699590365312 run.py:483] Algo bellman_ford step 8241 current loss 0.412943, current_train_items 263744.
I0302 19:02:17.335986 22699590365312 run.py:483] Algo bellman_ford step 8242 current loss 0.564010, current_train_items 263776.
I0302 19:02:17.367091 22699590365312 run.py:483] Algo bellman_ford step 8243 current loss 0.588020, current_train_items 263808.
I0302 19:02:17.402507 22699590365312 run.py:483] Algo bellman_ford step 8244 current loss 0.704022, current_train_items 263840.
I0302 19:02:17.422359 22699590365312 run.py:483] Algo bellman_ford step 8245 current loss 0.349255, current_train_items 263872.
I0302 19:02:17.438278 22699590365312 run.py:483] Algo bellman_ford step 8246 current loss 0.460395, current_train_items 263904.
I0302 19:02:17.460625 22699590365312 run.py:483] Algo bellman_ford step 8247 current loss 0.487807, current_train_items 263936.
I0302 19:02:17.491462 22699590365312 run.py:483] Algo bellman_ford step 8248 current loss 0.531322, current_train_items 263968.
I0302 19:02:17.524899 22699590365312 run.py:483] Algo bellman_ford step 8249 current loss 0.777608, current_train_items 264000.
I0302 19:02:17.544206 22699590365312 run.py:483] Algo bellman_ford step 8250 current loss 0.205027, current_train_items 264032.
I0302 19:02:17.552657 22699590365312 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0302 19:02:17.552762 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:02:17.569423 22699590365312 run.py:483] Algo bellman_ford step 8251 current loss 0.490884, current_train_items 264064.
I0302 19:02:17.593649 22699590365312 run.py:483] Algo bellman_ford step 8252 current loss 0.596737, current_train_items 264096.
I0302 19:02:17.626824 22699590365312 run.py:483] Algo bellman_ford step 8253 current loss 0.637128, current_train_items 264128.
I0302 19:02:17.660130 22699590365312 run.py:483] Algo bellman_ford step 8254 current loss 0.618342, current_train_items 264160.
I0302 19:02:17.680070 22699590365312 run.py:483] Algo bellman_ford step 8255 current loss 0.394574, current_train_items 264192.
I0302 19:02:17.696484 22699590365312 run.py:483] Algo bellman_ford step 8256 current loss 0.397264, current_train_items 264224.
I0302 19:02:17.720511 22699590365312 run.py:483] Algo bellman_ford step 8257 current loss 0.522341, current_train_items 264256.
I0302 19:02:17.752939 22699590365312 run.py:483] Algo bellman_ford step 8258 current loss 0.617181, current_train_items 264288.
I0302 19:02:17.787909 22699590365312 run.py:483] Algo bellman_ford step 8259 current loss 0.703531, current_train_items 264320.
I0302 19:02:17.807595 22699590365312 run.py:483] Algo bellman_ford step 8260 current loss 0.318531, current_train_items 264352.
I0302 19:02:17.823919 22699590365312 run.py:483] Algo bellman_ford step 8261 current loss 0.403909, current_train_items 264384.
I0302 19:02:17.848286 22699590365312 run.py:483] Algo bellman_ford step 8262 current loss 0.583305, current_train_items 264416.
I0302 19:02:17.880274 22699590365312 run.py:483] Algo bellman_ford step 8263 current loss 0.633259, current_train_items 264448.
I0302 19:02:17.915169 22699590365312 run.py:483] Algo bellman_ford step 8264 current loss 0.764041, current_train_items 264480.
I0302 19:02:17.934458 22699590365312 run.py:483] Algo bellman_ford step 8265 current loss 0.276651, current_train_items 264512.
I0302 19:02:17.950132 22699590365312 run.py:483] Algo bellman_ford step 8266 current loss 0.521911, current_train_items 264544.
I0302 19:02:17.973745 22699590365312 run.py:483] Algo bellman_ford step 8267 current loss 0.533384, current_train_items 264576.
I0302 19:02:18.005759 22699590365312 run.py:483] Algo bellman_ford step 8268 current loss 0.681615, current_train_items 264608.
I0302 19:02:18.038118 22699590365312 run.py:483] Algo bellman_ford step 8269 current loss 0.663133, current_train_items 264640.
I0302 19:02:18.058284 22699590365312 run.py:483] Algo bellman_ford step 8270 current loss 0.252115, current_train_items 264672.
I0302 19:02:18.073910 22699590365312 run.py:483] Algo bellman_ford step 8271 current loss 0.356199, current_train_items 264704.
I0302 19:02:18.097275 22699590365312 run.py:483] Algo bellman_ford step 8272 current loss 0.487655, current_train_items 264736.
I0302 19:02:18.127986 22699590365312 run.py:483] Algo bellman_ford step 8273 current loss 0.590353, current_train_items 264768.
I0302 19:02:18.162770 22699590365312 run.py:483] Algo bellman_ford step 8274 current loss 0.678944, current_train_items 264800.
I0302 19:02:18.182523 22699590365312 run.py:483] Algo bellman_ford step 8275 current loss 0.251213, current_train_items 264832.
I0302 19:02:18.199129 22699590365312 run.py:483] Algo bellman_ford step 8276 current loss 0.393703, current_train_items 264864.
I0302 19:02:18.222949 22699590365312 run.py:483] Algo bellman_ford step 8277 current loss 0.574026, current_train_items 264896.
I0302 19:02:18.253972 22699590365312 run.py:483] Algo bellman_ford step 8278 current loss 0.526649, current_train_items 264928.
I0302 19:02:18.286700 22699590365312 run.py:483] Algo bellman_ford step 8279 current loss 0.622891, current_train_items 264960.
I0302 19:02:18.306088 22699590365312 run.py:483] Algo bellman_ford step 8280 current loss 0.294203, current_train_items 264992.
I0302 19:02:18.322504 22699590365312 run.py:483] Algo bellman_ford step 8281 current loss 0.453090, current_train_items 265024.
I0302 19:02:18.346040 22699590365312 run.py:483] Algo bellman_ford step 8282 current loss 0.538136, current_train_items 265056.
I0302 19:02:18.377041 22699590365312 run.py:483] Algo bellman_ford step 8283 current loss 0.577729, current_train_items 265088.
I0302 19:02:18.409600 22699590365312 run.py:483] Algo bellman_ford step 8284 current loss 0.740986, current_train_items 265120.
I0302 19:02:18.429370 22699590365312 run.py:483] Algo bellman_ford step 8285 current loss 0.281709, current_train_items 265152.
I0302 19:02:18.445672 22699590365312 run.py:483] Algo bellman_ford step 8286 current loss 0.441517, current_train_items 265184.
I0302 19:02:18.469341 22699590365312 run.py:483] Algo bellman_ford step 8287 current loss 0.550196, current_train_items 265216.
I0302 19:02:18.502119 22699590365312 run.py:483] Algo bellman_ford step 8288 current loss 0.760016, current_train_items 265248.
I0302 19:02:18.535755 22699590365312 run.py:483] Algo bellman_ford step 8289 current loss 0.750423, current_train_items 265280.
I0302 19:02:18.555465 22699590365312 run.py:483] Algo bellman_ford step 8290 current loss 0.294309, current_train_items 265312.
I0302 19:02:18.571511 22699590365312 run.py:483] Algo bellman_ford step 8291 current loss 0.437403, current_train_items 265344.
I0302 19:02:18.595604 22699590365312 run.py:483] Algo bellman_ford step 8292 current loss 0.611024, current_train_items 265376.
I0302 19:02:18.625865 22699590365312 run.py:483] Algo bellman_ford step 8293 current loss 0.551625, current_train_items 265408.
I0302 19:02:18.660722 22699590365312 run.py:483] Algo bellman_ford step 8294 current loss 0.758657, current_train_items 265440.
I0302 19:02:18.680259 22699590365312 run.py:483] Algo bellman_ford step 8295 current loss 0.221770, current_train_items 265472.
I0302 19:02:18.696439 22699590365312 run.py:483] Algo bellman_ford step 8296 current loss 0.520812, current_train_items 265504.
I0302 19:02:18.720934 22699590365312 run.py:483] Algo bellman_ford step 8297 current loss 0.763326, current_train_items 265536.
I0302 19:02:18.752319 22699590365312 run.py:483] Algo bellman_ford step 8298 current loss 0.677088, current_train_items 265568.
I0302 19:02:18.785324 22699590365312 run.py:483] Algo bellman_ford step 8299 current loss 0.692001, current_train_items 265600.
I0302 19:02:18.805297 22699590365312 run.py:483] Algo bellman_ford step 8300 current loss 0.253756, current_train_items 265632.
I0302 19:02:18.813064 22699590365312 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0302 19:02:18.813176 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:02:18.829484 22699590365312 run.py:483] Algo bellman_ford step 8301 current loss 0.548454, current_train_items 265664.
I0302 19:02:18.854819 22699590365312 run.py:483] Algo bellman_ford step 8302 current loss 0.636598, current_train_items 265696.
I0302 19:02:18.887032 22699590365312 run.py:483] Algo bellman_ford step 8303 current loss 0.684165, current_train_items 265728.
I0302 19:02:18.920910 22699590365312 run.py:483] Algo bellman_ford step 8304 current loss 0.801126, current_train_items 265760.
I0302 19:02:18.940913 22699590365312 run.py:483] Algo bellman_ford step 8305 current loss 0.285455, current_train_items 265792.
I0302 19:02:18.956504 22699590365312 run.py:483] Algo bellman_ford step 8306 current loss 0.478967, current_train_items 265824.
I0302 19:02:18.980700 22699590365312 run.py:483] Algo bellman_ford step 8307 current loss 0.529380, current_train_items 265856.
I0302 19:02:19.010146 22699590365312 run.py:483] Algo bellman_ford step 8308 current loss 0.522878, current_train_items 265888.
I0302 19:02:19.045979 22699590365312 run.py:483] Algo bellman_ford step 8309 current loss 0.794116, current_train_items 265920.
I0302 19:02:19.065775 22699590365312 run.py:483] Algo bellman_ford step 8310 current loss 0.267298, current_train_items 265952.
I0302 19:02:19.081779 22699590365312 run.py:483] Algo bellman_ford step 8311 current loss 0.390624, current_train_items 265984.
I0302 19:02:19.105728 22699590365312 run.py:483] Algo bellman_ford step 8312 current loss 0.555936, current_train_items 266016.
I0302 19:02:19.136334 22699590365312 run.py:483] Algo bellman_ford step 8313 current loss 0.584534, current_train_items 266048.
I0302 19:02:19.167977 22699590365312 run.py:483] Algo bellman_ford step 8314 current loss 0.654687, current_train_items 266080.
I0302 19:02:19.187561 22699590365312 run.py:483] Algo bellman_ford step 8315 current loss 0.239210, current_train_items 266112.
I0302 19:02:19.203846 22699590365312 run.py:483] Algo bellman_ford step 8316 current loss 0.415609, current_train_items 266144.
I0302 19:02:19.227480 22699590365312 run.py:483] Algo bellman_ford step 8317 current loss 0.544929, current_train_items 266176.
I0302 19:02:19.257360 22699590365312 run.py:483] Algo bellman_ford step 8318 current loss 0.548466, current_train_items 266208.
I0302 19:02:19.290668 22699590365312 run.py:483] Algo bellman_ford step 8319 current loss 0.645272, current_train_items 266240.
I0302 19:02:19.309705 22699590365312 run.py:483] Algo bellman_ford step 8320 current loss 0.240804, current_train_items 266272.
I0302 19:02:19.325614 22699590365312 run.py:483] Algo bellman_ford step 8321 current loss 0.403365, current_train_items 266304.
I0302 19:02:19.350047 22699590365312 run.py:483] Algo bellman_ford step 8322 current loss 0.557589, current_train_items 266336.
I0302 19:02:19.380972 22699590365312 run.py:483] Algo bellman_ford step 8323 current loss 0.615975, current_train_items 266368.
I0302 19:02:19.415876 22699590365312 run.py:483] Algo bellman_ford step 8324 current loss 0.745142, current_train_items 266400.
I0302 19:02:19.435367 22699590365312 run.py:483] Algo bellman_ford step 8325 current loss 0.341731, current_train_items 266432.
I0302 19:02:19.450867 22699590365312 run.py:483] Algo bellman_ford step 8326 current loss 0.389206, current_train_items 266464.
I0302 19:02:19.474433 22699590365312 run.py:483] Algo bellman_ford step 8327 current loss 0.539061, current_train_items 266496.
I0302 19:02:19.504552 22699590365312 run.py:483] Algo bellman_ford step 8328 current loss 0.653581, current_train_items 266528.
I0302 19:02:19.538564 22699590365312 run.py:483] Algo bellman_ford step 8329 current loss 0.778098, current_train_items 266560.
I0302 19:02:19.558256 22699590365312 run.py:483] Algo bellman_ford step 8330 current loss 0.308744, current_train_items 266592.
I0302 19:02:19.574080 22699590365312 run.py:483] Algo bellman_ford step 8331 current loss 0.459599, current_train_items 266624.
I0302 19:02:19.597190 22699590365312 run.py:483] Algo bellman_ford step 8332 current loss 0.492408, current_train_items 266656.
I0302 19:02:19.629001 22699590365312 run.py:483] Algo bellman_ford step 8333 current loss 0.663744, current_train_items 266688.
I0302 19:02:19.662120 22699590365312 run.py:483] Algo bellman_ford step 8334 current loss 0.764901, current_train_items 266720.
I0302 19:02:19.681666 22699590365312 run.py:483] Algo bellman_ford step 8335 current loss 0.309992, current_train_items 266752.
I0302 19:02:19.697606 22699590365312 run.py:483] Algo bellman_ford step 8336 current loss 0.470367, current_train_items 266784.
I0302 19:02:19.722198 22699590365312 run.py:483] Algo bellman_ford step 8337 current loss 0.562122, current_train_items 266816.
I0302 19:02:19.753391 22699590365312 run.py:483] Algo bellman_ford step 8338 current loss 0.704538, current_train_items 266848.
I0302 19:02:19.785781 22699590365312 run.py:483] Algo bellman_ford step 8339 current loss 0.784695, current_train_items 266880.
I0302 19:02:19.805054 22699590365312 run.py:483] Algo bellman_ford step 8340 current loss 0.308519, current_train_items 266912.
I0302 19:02:19.820982 22699590365312 run.py:483] Algo bellman_ford step 8341 current loss 0.385869, current_train_items 266944.
I0302 19:02:19.844166 22699590365312 run.py:483] Algo bellman_ford step 8342 current loss 0.517667, current_train_items 266976.
I0302 19:02:19.874367 22699590365312 run.py:483] Algo bellman_ford step 8343 current loss 0.542767, current_train_items 267008.
I0302 19:02:19.909303 22699590365312 run.py:483] Algo bellman_ford step 8344 current loss 0.658245, current_train_items 267040.
I0302 19:02:19.928876 22699590365312 run.py:483] Algo bellman_ford step 8345 current loss 0.284819, current_train_items 267072.
I0302 19:02:19.944737 22699590365312 run.py:483] Algo bellman_ford step 8346 current loss 0.396973, current_train_items 267104.
I0302 19:02:19.969168 22699590365312 run.py:483] Algo bellman_ford step 8347 current loss 0.539881, current_train_items 267136.
I0302 19:02:20.000975 22699590365312 run.py:483] Algo bellman_ford step 8348 current loss 0.608719, current_train_items 267168.
I0302 19:02:20.035472 22699590365312 run.py:483] Algo bellman_ford step 8349 current loss 0.658618, current_train_items 267200.
I0302 19:02:20.054821 22699590365312 run.py:483] Algo bellman_ford step 8350 current loss 0.278359, current_train_items 267232.
I0302 19:02:20.063068 22699590365312 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0302 19:02:20.063184 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:20.079929 22699590365312 run.py:483] Algo bellman_ford step 8351 current loss 0.436003, current_train_items 267264.
I0302 19:02:20.104350 22699590365312 run.py:483] Algo bellman_ford step 8352 current loss 0.545869, current_train_items 267296.
I0302 19:02:20.136221 22699590365312 run.py:483] Algo bellman_ford step 8353 current loss 0.556055, current_train_items 267328.
I0302 19:02:20.171936 22699590365312 run.py:483] Algo bellman_ford step 8354 current loss 0.776433, current_train_items 267360.
I0302 19:02:20.192006 22699590365312 run.py:483] Algo bellman_ford step 8355 current loss 0.317045, current_train_items 267392.
I0302 19:02:20.207589 22699590365312 run.py:483] Algo bellman_ford step 8356 current loss 0.484502, current_train_items 267424.
I0302 19:02:20.231046 22699590365312 run.py:483] Algo bellman_ford step 8357 current loss 0.476356, current_train_items 267456.
I0302 19:02:20.262516 22699590365312 run.py:483] Algo bellman_ford step 8358 current loss 0.542062, current_train_items 267488.
I0302 19:02:20.295284 22699590365312 run.py:483] Algo bellman_ford step 8359 current loss 0.669875, current_train_items 267520.
I0302 19:02:20.315166 22699590365312 run.py:483] Algo bellman_ford step 8360 current loss 0.291647, current_train_items 267552.
I0302 19:02:20.330939 22699590365312 run.py:483] Algo bellman_ford step 8361 current loss 0.420261, current_train_items 267584.
I0302 19:02:20.354648 22699590365312 run.py:483] Algo bellman_ford step 8362 current loss 0.518885, current_train_items 267616.
I0302 19:02:20.385485 22699590365312 run.py:483] Algo bellman_ford step 8363 current loss 0.532182, current_train_items 267648.
I0302 19:02:20.419471 22699590365312 run.py:483] Algo bellman_ford step 8364 current loss 0.730152, current_train_items 267680.
I0302 19:02:20.439122 22699590365312 run.py:483] Algo bellman_ford step 8365 current loss 0.283388, current_train_items 267712.
I0302 19:02:20.455464 22699590365312 run.py:483] Algo bellman_ford step 8366 current loss 0.380182, current_train_items 267744.
I0302 19:02:20.478535 22699590365312 run.py:483] Algo bellman_ford step 8367 current loss 0.471616, current_train_items 267776.
I0302 19:02:20.510230 22699590365312 run.py:483] Algo bellman_ford step 8368 current loss 0.590057, current_train_items 267808.
I0302 19:02:20.545751 22699590365312 run.py:483] Algo bellman_ford step 8369 current loss 0.789348, current_train_items 267840.
I0302 19:02:20.565774 22699590365312 run.py:483] Algo bellman_ford step 8370 current loss 0.224259, current_train_items 267872.
I0302 19:02:20.582057 22699590365312 run.py:483] Algo bellman_ford step 8371 current loss 0.528885, current_train_items 267904.
I0302 19:02:20.604703 22699590365312 run.py:483] Algo bellman_ford step 8372 current loss 0.559486, current_train_items 267936.
I0302 19:02:20.635842 22699590365312 run.py:483] Algo bellman_ford step 8373 current loss 0.560330, current_train_items 267968.
I0302 19:02:20.670395 22699590365312 run.py:483] Algo bellman_ford step 8374 current loss 0.710568, current_train_items 268000.
I0302 19:02:20.690332 22699590365312 run.py:483] Algo bellman_ford step 8375 current loss 0.256632, current_train_items 268032.
I0302 19:02:20.705979 22699590365312 run.py:483] Algo bellman_ford step 8376 current loss 0.404569, current_train_items 268064.
I0302 19:02:20.728949 22699590365312 run.py:483] Algo bellman_ford step 8377 current loss 0.539113, current_train_items 268096.
I0302 19:02:20.760363 22699590365312 run.py:483] Algo bellman_ford step 8378 current loss 0.556036, current_train_items 268128.
I0302 19:02:20.794251 22699590365312 run.py:483] Algo bellman_ford step 8379 current loss 0.770987, current_train_items 268160.
I0302 19:02:20.814001 22699590365312 run.py:483] Algo bellman_ford step 8380 current loss 0.329856, current_train_items 268192.
I0302 19:02:20.830035 22699590365312 run.py:483] Algo bellman_ford step 8381 current loss 0.453757, current_train_items 268224.
I0302 19:02:20.852558 22699590365312 run.py:483] Algo bellman_ford step 8382 current loss 0.476501, current_train_items 268256.
I0302 19:02:20.884659 22699590365312 run.py:483] Algo bellman_ford step 8383 current loss 0.703738, current_train_items 268288.
I0302 19:02:20.917007 22699590365312 run.py:483] Algo bellman_ford step 8384 current loss 0.705880, current_train_items 268320.
I0302 19:02:20.936632 22699590365312 run.py:483] Algo bellman_ford step 8385 current loss 0.292023, current_train_items 268352.
I0302 19:02:20.952417 22699590365312 run.py:483] Algo bellman_ford step 8386 current loss 0.381909, current_train_items 268384.
I0302 19:02:20.975614 22699590365312 run.py:483] Algo bellman_ford step 8387 current loss 0.523709, current_train_items 268416.
I0302 19:02:21.007237 22699590365312 run.py:483] Algo bellman_ford step 8388 current loss 0.655429, current_train_items 268448.
I0302 19:02:21.040258 22699590365312 run.py:483] Algo bellman_ford step 8389 current loss 0.654059, current_train_items 268480.
I0302 19:02:21.060101 22699590365312 run.py:483] Algo bellman_ford step 8390 current loss 0.307417, current_train_items 268512.
I0302 19:02:21.076267 22699590365312 run.py:483] Algo bellman_ford step 8391 current loss 0.375035, current_train_items 268544.
I0302 19:02:21.098836 22699590365312 run.py:483] Algo bellman_ford step 8392 current loss 0.501862, current_train_items 268576.
I0302 19:02:21.131817 22699590365312 run.py:483] Algo bellman_ford step 8393 current loss 0.646135, current_train_items 268608.
I0302 19:02:21.164354 22699590365312 run.py:483] Algo bellman_ford step 8394 current loss 0.642881, current_train_items 268640.
I0302 19:02:21.183893 22699590365312 run.py:483] Algo bellman_ford step 8395 current loss 0.308389, current_train_items 268672.
I0302 19:02:21.199985 22699590365312 run.py:483] Algo bellman_ford step 8396 current loss 0.438669, current_train_items 268704.
I0302 19:02:21.223412 22699590365312 run.py:483] Algo bellman_ford step 8397 current loss 0.528261, current_train_items 268736.
I0302 19:02:21.255167 22699590365312 run.py:483] Algo bellman_ford step 8398 current loss 0.593365, current_train_items 268768.
I0302 19:02:21.289382 22699590365312 run.py:483] Algo bellman_ford step 8399 current loss 0.810280, current_train_items 268800.
I0302 19:02:21.309178 22699590365312 run.py:483] Algo bellman_ford step 8400 current loss 0.305611, current_train_items 268832.
I0302 19:02:21.316711 22699590365312 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0302 19:02:21.316817 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:02:21.333700 22699590365312 run.py:483] Algo bellman_ford step 8401 current loss 0.413667, current_train_items 268864.
I0302 19:02:21.358607 22699590365312 run.py:483] Algo bellman_ford step 8402 current loss 0.572904, current_train_items 268896.
I0302 19:02:21.391034 22699590365312 run.py:483] Algo bellman_ford step 8403 current loss 0.616053, current_train_items 268928.
I0302 19:02:21.427806 22699590365312 run.py:483] Algo bellman_ford step 8404 current loss 0.757650, current_train_items 268960.
I0302 19:02:21.447973 22699590365312 run.py:483] Algo bellman_ford step 8405 current loss 0.287502, current_train_items 268992.
I0302 19:02:21.463899 22699590365312 run.py:483] Algo bellman_ford step 8406 current loss 0.429414, current_train_items 269024.
I0302 19:02:21.487036 22699590365312 run.py:483] Algo bellman_ford step 8407 current loss 0.536480, current_train_items 269056.
I0302 19:02:21.517869 22699590365312 run.py:483] Algo bellman_ford step 8408 current loss 0.583045, current_train_items 269088.
I0302 19:02:21.552746 22699590365312 run.py:483] Algo bellman_ford step 8409 current loss 0.719928, current_train_items 269120.
I0302 19:02:21.572437 22699590365312 run.py:483] Algo bellman_ford step 8410 current loss 0.276864, current_train_items 269152.
I0302 19:02:21.588449 22699590365312 run.py:483] Algo bellman_ford step 8411 current loss 0.425607, current_train_items 269184.
I0302 19:02:21.612640 22699590365312 run.py:483] Algo bellman_ford step 8412 current loss 0.514086, current_train_items 269216.
I0302 19:02:21.644929 22699590365312 run.py:483] Algo bellman_ford step 8413 current loss 0.660226, current_train_items 269248.
I0302 19:02:21.679274 22699590365312 run.py:483] Algo bellman_ford step 8414 current loss 0.730665, current_train_items 269280.
I0302 19:02:21.698936 22699590365312 run.py:483] Algo bellman_ford step 8415 current loss 0.350003, current_train_items 269312.
I0302 19:02:21.715096 22699590365312 run.py:483] Algo bellman_ford step 8416 current loss 0.419587, current_train_items 269344.
I0302 19:02:21.738800 22699590365312 run.py:483] Algo bellman_ford step 8417 current loss 0.553189, current_train_items 269376.
I0302 19:02:21.770941 22699590365312 run.py:483] Algo bellman_ford step 8418 current loss 0.563378, current_train_items 269408.
I0302 19:02:21.807891 22699590365312 run.py:483] Algo bellman_ford step 8419 current loss 0.812993, current_train_items 269440.
I0302 19:02:21.827568 22699590365312 run.py:483] Algo bellman_ford step 8420 current loss 0.244241, current_train_items 269472.
I0302 19:02:21.843600 22699590365312 run.py:483] Algo bellman_ford step 8421 current loss 0.458073, current_train_items 269504.
I0302 19:02:21.868396 22699590365312 run.py:483] Algo bellman_ford step 8422 current loss 0.603061, current_train_items 269536.
I0302 19:02:21.900426 22699590365312 run.py:483] Algo bellman_ford step 8423 current loss 0.556335, current_train_items 269568.
I0302 19:02:21.934935 22699590365312 run.py:483] Algo bellman_ford step 8424 current loss 0.769960, current_train_items 269600.
I0302 19:02:21.954290 22699590365312 run.py:483] Algo bellman_ford step 8425 current loss 0.254479, current_train_items 269632.
I0302 19:02:21.969947 22699590365312 run.py:483] Algo bellman_ford step 8426 current loss 0.447362, current_train_items 269664.
I0302 19:02:21.993542 22699590365312 run.py:483] Algo bellman_ford step 8427 current loss 0.601781, current_train_items 269696.
I0302 19:02:22.024883 22699590365312 run.py:483] Algo bellman_ford step 8428 current loss 0.625938, current_train_items 269728.
I0302 19:02:22.058586 22699590365312 run.py:483] Algo bellman_ford step 8429 current loss 0.671734, current_train_items 269760.
I0302 19:02:22.078350 22699590365312 run.py:483] Algo bellman_ford step 8430 current loss 0.315884, current_train_items 269792.
I0302 19:02:22.094801 22699590365312 run.py:483] Algo bellman_ford step 8431 current loss 0.523432, current_train_items 269824.
I0302 19:02:22.118199 22699590365312 run.py:483] Algo bellman_ford step 8432 current loss 0.448504, current_train_items 269856.
I0302 19:02:22.150969 22699590365312 run.py:483] Algo bellman_ford step 8433 current loss 0.691676, current_train_items 269888.
I0302 19:02:22.183304 22699590365312 run.py:483] Algo bellman_ford step 8434 current loss 0.734584, current_train_items 269920.
I0302 19:02:22.202418 22699590365312 run.py:483] Algo bellman_ford step 8435 current loss 0.251910, current_train_items 269952.
I0302 19:02:22.218142 22699590365312 run.py:483] Algo bellman_ford step 8436 current loss 0.466546, current_train_items 269984.
I0302 19:02:22.241555 22699590365312 run.py:483] Algo bellman_ford step 8437 current loss 0.516668, current_train_items 270016.
I0302 19:02:22.272826 22699590365312 run.py:483] Algo bellman_ford step 8438 current loss 0.573641, current_train_items 270048.
I0302 19:02:22.305064 22699590365312 run.py:483] Algo bellman_ford step 8439 current loss 0.657234, current_train_items 270080.
I0302 19:02:22.324750 22699590365312 run.py:483] Algo bellman_ford step 8440 current loss 0.295121, current_train_items 270112.
I0302 19:02:22.340904 22699590365312 run.py:483] Algo bellman_ford step 8441 current loss 0.405345, current_train_items 270144.
I0302 19:02:22.364985 22699590365312 run.py:483] Algo bellman_ford step 8442 current loss 0.568373, current_train_items 270176.
I0302 19:02:22.396070 22699590365312 run.py:483] Algo bellman_ford step 8443 current loss 0.572336, current_train_items 270208.
I0302 19:02:22.430606 22699590365312 run.py:483] Algo bellman_ford step 8444 current loss 0.671107, current_train_items 270240.
I0302 19:02:22.449964 22699590365312 run.py:483] Algo bellman_ford step 8445 current loss 0.335249, current_train_items 270272.
I0302 19:02:22.465871 22699590365312 run.py:483] Algo bellman_ford step 8446 current loss 0.443669, current_train_items 270304.
I0302 19:02:22.489303 22699590365312 run.py:483] Algo bellman_ford step 8447 current loss 0.531658, current_train_items 270336.
I0302 19:02:22.520180 22699590365312 run.py:483] Algo bellman_ford step 8448 current loss 0.574120, current_train_items 270368.
I0302 19:02:22.553664 22699590365312 run.py:483] Algo bellman_ford step 8449 current loss 0.853587, current_train_items 270400.
I0302 19:02:22.573238 22699590365312 run.py:483] Algo bellman_ford step 8450 current loss 0.281361, current_train_items 270432.
I0302 19:02:22.581360 22699590365312 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0302 19:02:22.581465 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:02:22.597835 22699590365312 run.py:483] Algo bellman_ford step 8451 current loss 0.428091, current_train_items 270464.
I0302 19:02:22.622581 22699590365312 run.py:483] Algo bellman_ford step 8452 current loss 0.517780, current_train_items 270496.
I0302 19:02:22.655160 22699590365312 run.py:483] Algo bellman_ford step 8453 current loss 0.617170, current_train_items 270528.
I0302 19:02:22.689776 22699590365312 run.py:483] Algo bellman_ford step 8454 current loss 0.734203, current_train_items 270560.
I0302 19:02:22.709912 22699590365312 run.py:483] Algo bellman_ford step 8455 current loss 0.262681, current_train_items 270592.
I0302 19:02:22.726087 22699590365312 run.py:483] Algo bellman_ford step 8456 current loss 0.435314, current_train_items 270624.
I0302 19:02:22.750355 22699590365312 run.py:483] Algo bellman_ford step 8457 current loss 0.560361, current_train_items 270656.
I0302 19:02:22.780444 22699590365312 run.py:483] Algo bellman_ford step 8458 current loss 0.584688, current_train_items 270688.
I0302 19:02:22.811628 22699590365312 run.py:483] Algo bellman_ford step 8459 current loss 0.553185, current_train_items 270720.
I0302 19:02:22.831414 22699590365312 run.py:483] Algo bellman_ford step 8460 current loss 0.320589, current_train_items 270752.
I0302 19:02:22.847284 22699590365312 run.py:483] Algo bellman_ford step 8461 current loss 0.398246, current_train_items 270784.
I0302 19:02:22.869726 22699590365312 run.py:483] Algo bellman_ford step 8462 current loss 0.502243, current_train_items 270816.
I0302 19:02:22.900240 22699590365312 run.py:483] Algo bellman_ford step 8463 current loss 0.607123, current_train_items 270848.
I0302 19:02:22.934827 22699590365312 run.py:483] Algo bellman_ford step 8464 current loss 0.750673, current_train_items 270880.
I0302 19:02:22.954097 22699590365312 run.py:483] Algo bellman_ford step 8465 current loss 0.302181, current_train_items 270912.
I0302 19:02:22.970595 22699590365312 run.py:483] Algo bellman_ford step 8466 current loss 0.465790, current_train_items 270944.
I0302 19:02:22.993717 22699590365312 run.py:483] Algo bellman_ford step 8467 current loss 0.517409, current_train_items 270976.
I0302 19:02:23.024561 22699590365312 run.py:483] Algo bellman_ford step 8468 current loss 0.866324, current_train_items 271008.
I0302 19:02:23.057327 22699590365312 run.py:483] Algo bellman_ford step 8469 current loss 0.760703, current_train_items 271040.
I0302 19:02:23.077321 22699590365312 run.py:483] Algo bellman_ford step 8470 current loss 0.330469, current_train_items 271072.
I0302 19:02:23.093714 22699590365312 run.py:483] Algo bellman_ford step 8471 current loss 0.546470, current_train_items 271104.
I0302 19:02:23.116916 22699590365312 run.py:483] Algo bellman_ford step 8472 current loss 0.516216, current_train_items 271136.
I0302 19:02:23.145859 22699590365312 run.py:483] Algo bellman_ford step 8473 current loss 0.551662, current_train_items 271168.
I0302 19:02:23.180039 22699590365312 run.py:483] Algo bellman_ford step 8474 current loss 0.744820, current_train_items 271200.
I0302 19:02:23.199966 22699590365312 run.py:483] Algo bellman_ford step 8475 current loss 0.259455, current_train_items 271232.
I0302 19:02:23.216320 22699590365312 run.py:483] Algo bellman_ford step 8476 current loss 0.445581, current_train_items 271264.
I0302 19:02:23.241024 22699590365312 run.py:483] Algo bellman_ford step 8477 current loss 0.591849, current_train_items 271296.
I0302 19:02:23.272572 22699590365312 run.py:483] Algo bellman_ford step 8478 current loss 0.537309, current_train_items 271328.
I0302 19:02:23.306997 22699590365312 run.py:483] Algo bellman_ford step 8479 current loss 0.599845, current_train_items 271360.
I0302 19:02:23.326439 22699590365312 run.py:483] Algo bellman_ford step 8480 current loss 0.274139, current_train_items 271392.
I0302 19:02:23.342769 22699590365312 run.py:483] Algo bellman_ford step 8481 current loss 0.535323, current_train_items 271424.
I0302 19:02:23.366706 22699590365312 run.py:483] Algo bellman_ford step 8482 current loss 0.663810, current_train_items 271456.
I0302 19:02:23.398014 22699590365312 run.py:483] Algo bellman_ford step 8483 current loss 0.694039, current_train_items 271488.
I0302 19:02:23.430672 22699590365312 run.py:483] Algo bellman_ford step 8484 current loss 0.707617, current_train_items 271520.
I0302 19:02:23.450509 22699590365312 run.py:483] Algo bellman_ford step 8485 current loss 0.304009, current_train_items 271552.
I0302 19:02:23.466269 22699590365312 run.py:483] Algo bellman_ford step 8486 current loss 0.338963, current_train_items 271584.
I0302 19:02:23.489692 22699590365312 run.py:483] Algo bellman_ford step 8487 current loss 0.545763, current_train_items 271616.
I0302 19:02:23.521780 22699590365312 run.py:483] Algo bellman_ford step 8488 current loss 0.649845, current_train_items 271648.
I0302 19:02:23.555543 22699590365312 run.py:483] Algo bellman_ford step 8489 current loss 0.763702, current_train_items 271680.
I0302 19:02:23.575176 22699590365312 run.py:483] Algo bellman_ford step 8490 current loss 0.314858, current_train_items 271712.
I0302 19:02:23.590961 22699590365312 run.py:483] Algo bellman_ford step 8491 current loss 0.416111, current_train_items 271744.
I0302 19:02:23.614835 22699590365312 run.py:483] Algo bellman_ford step 8492 current loss 0.600126, current_train_items 271776.
I0302 19:02:23.645757 22699590365312 run.py:483] Algo bellman_ford step 8493 current loss 0.582971, current_train_items 271808.
I0302 19:02:23.678186 22699590365312 run.py:483] Algo bellman_ford step 8494 current loss 0.649737, current_train_items 271840.
I0302 19:02:23.697733 22699590365312 run.py:483] Algo bellman_ford step 8495 current loss 0.248446, current_train_items 271872.
I0302 19:02:23.714138 22699590365312 run.py:483] Algo bellman_ford step 8496 current loss 0.479867, current_train_items 271904.
I0302 19:02:23.737948 22699590365312 run.py:483] Algo bellman_ford step 8497 current loss 0.464296, current_train_items 271936.
I0302 19:02:23.769803 22699590365312 run.py:483] Algo bellman_ford step 8498 current loss 0.556798, current_train_items 271968.
I0302 19:02:23.801735 22699590365312 run.py:483] Algo bellman_ford step 8499 current loss 0.605907, current_train_items 272000.
I0302 19:02:23.821723 22699590365312 run.py:483] Algo bellman_ford step 8500 current loss 0.274931, current_train_items 272032.
I0302 19:02:23.829553 22699590365312 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0302 19:02:23.829686 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 19:02:23.846732 22699590365312 run.py:483] Algo bellman_ford step 8501 current loss 0.382024, current_train_items 272064.
I0302 19:02:23.871587 22699590365312 run.py:483] Algo bellman_ford step 8502 current loss 0.535468, current_train_items 272096.
I0302 19:02:23.902745 22699590365312 run.py:483] Algo bellman_ford step 8503 current loss 0.578059, current_train_items 272128.
I0302 19:02:23.936274 22699590365312 run.py:483] Algo bellman_ford step 8504 current loss 0.701688, current_train_items 272160.
I0302 19:02:23.956114 22699590365312 run.py:483] Algo bellman_ford step 8505 current loss 0.282120, current_train_items 272192.
I0302 19:02:23.971256 22699590365312 run.py:483] Algo bellman_ford step 8506 current loss 0.381815, current_train_items 272224.
I0302 19:02:23.993594 22699590365312 run.py:483] Algo bellman_ford step 8507 current loss 0.523311, current_train_items 272256.
I0302 19:02:24.025383 22699590365312 run.py:483] Algo bellman_ford step 8508 current loss 0.610180, current_train_items 272288.
I0302 19:02:24.060362 22699590365312 run.py:483] Algo bellman_ford step 8509 current loss 0.657029, current_train_items 272320.
I0302 19:02:24.079977 22699590365312 run.py:483] Algo bellman_ford step 8510 current loss 0.309309, current_train_items 272352.
I0302 19:02:24.095555 22699590365312 run.py:483] Algo bellman_ford step 8511 current loss 0.417597, current_train_items 272384.
I0302 19:02:24.120177 22699590365312 run.py:483] Algo bellman_ford step 8512 current loss 0.568066, current_train_items 272416.
I0302 19:02:24.151079 22699590365312 run.py:483] Algo bellman_ford step 8513 current loss 0.735671, current_train_items 272448.
I0302 19:02:24.185274 22699590365312 run.py:483] Algo bellman_ford step 8514 current loss 0.925827, current_train_items 272480.
I0302 19:02:24.204524 22699590365312 run.py:483] Algo bellman_ford step 8515 current loss 0.292746, current_train_items 272512.
I0302 19:02:24.220450 22699590365312 run.py:483] Algo bellman_ford step 8516 current loss 0.417184, current_train_items 272544.
I0302 19:02:24.244151 22699590365312 run.py:483] Algo bellman_ford step 8517 current loss 0.547305, current_train_items 272576.
I0302 19:02:24.276931 22699590365312 run.py:483] Algo bellman_ford step 8518 current loss 0.678858, current_train_items 272608.
I0302 19:02:24.309448 22699590365312 run.py:483] Algo bellman_ford step 8519 current loss 0.759193, current_train_items 272640.
I0302 19:02:24.328816 22699590365312 run.py:483] Algo bellman_ford step 8520 current loss 0.267070, current_train_items 272672.
I0302 19:02:24.344585 22699590365312 run.py:483] Algo bellman_ford step 8521 current loss 0.471024, current_train_items 272704.
I0302 19:02:24.368191 22699590365312 run.py:483] Algo bellman_ford step 8522 current loss 0.495553, current_train_items 272736.
I0302 19:02:24.399338 22699590365312 run.py:483] Algo bellman_ford step 8523 current loss 0.678978, current_train_items 272768.
I0302 19:02:24.433766 22699590365312 run.py:483] Algo bellman_ford step 8524 current loss 0.694547, current_train_items 272800.
I0302 19:02:24.453283 22699590365312 run.py:483] Algo bellman_ford step 8525 current loss 0.287482, current_train_items 272832.
I0302 19:02:24.469347 22699590365312 run.py:483] Algo bellman_ford step 8526 current loss 0.432660, current_train_items 272864.
I0302 19:02:24.492878 22699590365312 run.py:483] Algo bellman_ford step 8527 current loss 0.607910, current_train_items 272896.
I0302 19:02:24.523746 22699590365312 run.py:483] Algo bellman_ford step 8528 current loss 0.548640, current_train_items 272928.
I0302 19:02:24.557614 22699590365312 run.py:483] Algo bellman_ford step 8529 current loss 0.793202, current_train_items 272960.
I0302 19:02:24.577308 22699590365312 run.py:483] Algo bellman_ford step 8530 current loss 0.260791, current_train_items 272992.
I0302 19:02:24.593365 22699590365312 run.py:483] Algo bellman_ford step 8531 current loss 0.454807, current_train_items 273024.
I0302 19:02:24.615888 22699590365312 run.py:483] Algo bellman_ford step 8532 current loss 0.502981, current_train_items 273056.
I0302 19:02:24.646566 22699590365312 run.py:483] Algo bellman_ford step 8533 current loss 0.701051, current_train_items 273088.
I0302 19:02:24.679312 22699590365312 run.py:483] Algo bellman_ford step 8534 current loss 0.745782, current_train_items 273120.
I0302 19:02:24.698905 22699590365312 run.py:483] Algo bellman_ford step 8535 current loss 0.223093, current_train_items 273152.
I0302 19:02:24.714917 22699590365312 run.py:483] Algo bellman_ford step 8536 current loss 0.442852, current_train_items 273184.
I0302 19:02:24.738433 22699590365312 run.py:483] Algo bellman_ford step 8537 current loss 0.607240, current_train_items 273216.
I0302 19:02:24.769770 22699590365312 run.py:483] Algo bellman_ford step 8538 current loss 0.613651, current_train_items 273248.
I0302 19:02:24.805332 22699590365312 run.py:483] Algo bellman_ford step 8539 current loss 0.973500, current_train_items 273280.
I0302 19:02:24.824957 22699590365312 run.py:483] Algo bellman_ford step 8540 current loss 0.251637, current_train_items 273312.
I0302 19:02:24.840841 22699590365312 run.py:483] Algo bellman_ford step 8541 current loss 0.506617, current_train_items 273344.
I0302 19:02:24.864812 22699590365312 run.py:483] Algo bellman_ford step 8542 current loss 0.768265, current_train_items 273376.
I0302 19:02:24.896639 22699590365312 run.py:483] Algo bellman_ford step 8543 current loss 0.822718, current_train_items 273408.
I0302 19:02:24.929568 22699590365312 run.py:483] Algo bellman_ford step 8544 current loss 2.003336, current_train_items 273440.
I0302 19:02:24.948877 22699590365312 run.py:483] Algo bellman_ford step 8545 current loss 0.239979, current_train_items 273472.
I0302 19:02:24.964405 22699590365312 run.py:483] Algo bellman_ford step 8546 current loss 0.364579, current_train_items 273504.
I0302 19:02:24.988339 22699590365312 run.py:483] Algo bellman_ford step 8547 current loss 0.725289, current_train_items 273536.
I0302 19:02:25.019505 22699590365312 run.py:483] Algo bellman_ford step 8548 current loss 0.795403, current_train_items 273568.
I0302 19:02:25.053377 22699590365312 run.py:483] Algo bellman_ford step 8549 current loss 0.959399, current_train_items 273600.
I0302 19:02:25.072821 22699590365312 run.py:483] Algo bellman_ford step 8550 current loss 0.235340, current_train_items 273632.
I0302 19:02:25.080982 22699590365312 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0302 19:02:25.081088 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:02:25.098097 22699590365312 run.py:483] Algo bellman_ford step 8551 current loss 0.484970, current_train_items 273664.
I0302 19:02:25.122791 22699590365312 run.py:483] Algo bellman_ford step 8552 current loss 0.573994, current_train_items 273696.
I0302 19:02:25.154687 22699590365312 run.py:483] Algo bellman_ford step 8553 current loss 0.630178, current_train_items 273728.
I0302 19:02:25.186586 22699590365312 run.py:483] Algo bellman_ford step 8554 current loss 0.682834, current_train_items 273760.
I0302 19:02:25.206597 22699590365312 run.py:483] Algo bellman_ford step 8555 current loss 0.397537, current_train_items 273792.
I0302 19:02:25.222229 22699590365312 run.py:483] Algo bellman_ford step 8556 current loss 0.506474, current_train_items 273824.
I0302 19:02:25.245734 22699590365312 run.py:483] Algo bellman_ford step 8557 current loss 0.591541, current_train_items 273856.
I0302 19:02:25.277496 22699590365312 run.py:483] Algo bellman_ford step 8558 current loss 0.652524, current_train_items 273888.
I0302 19:02:25.311800 22699590365312 run.py:483] Algo bellman_ford step 8559 current loss 0.717792, current_train_items 273920.
I0302 19:02:25.331709 22699590365312 run.py:483] Algo bellman_ford step 8560 current loss 0.237879, current_train_items 273952.
I0302 19:02:25.348088 22699590365312 run.py:483] Algo bellman_ford step 8561 current loss 0.448296, current_train_items 273984.
I0302 19:02:25.371474 22699590365312 run.py:483] Algo bellman_ford step 8562 current loss 0.600350, current_train_items 274016.
I0302 19:02:25.403827 22699590365312 run.py:483] Algo bellman_ford step 8563 current loss 0.645988, current_train_items 274048.
I0302 19:02:25.436452 22699590365312 run.py:483] Algo bellman_ford step 8564 current loss 0.659263, current_train_items 274080.
I0302 19:02:25.455776 22699590365312 run.py:483] Algo bellman_ford step 8565 current loss 0.296924, current_train_items 274112.
I0302 19:02:25.472294 22699590365312 run.py:483] Algo bellman_ford step 8566 current loss 0.428317, current_train_items 274144.
I0302 19:02:25.496957 22699590365312 run.py:483] Algo bellman_ford step 8567 current loss 0.577790, current_train_items 274176.
I0302 19:02:25.528246 22699590365312 run.py:483] Algo bellman_ford step 8568 current loss 0.710280, current_train_items 274208.
I0302 19:02:25.562292 22699590365312 run.py:483] Algo bellman_ford step 8569 current loss 0.730589, current_train_items 274240.
I0302 19:02:25.582273 22699590365312 run.py:483] Algo bellman_ford step 8570 current loss 0.293600, current_train_items 274272.
I0302 19:02:25.598261 22699590365312 run.py:483] Algo bellman_ford step 8571 current loss 0.526271, current_train_items 274304.
I0302 19:02:25.621310 22699590365312 run.py:483] Algo bellman_ford step 8572 current loss 0.536035, current_train_items 274336.
I0302 19:02:25.652677 22699590365312 run.py:483] Algo bellman_ford step 8573 current loss 0.637041, current_train_items 274368.
I0302 19:02:25.684701 22699590365312 run.py:483] Algo bellman_ford step 8574 current loss 0.746273, current_train_items 274400.
I0302 19:02:25.704554 22699590365312 run.py:483] Algo bellman_ford step 8575 current loss 0.336948, current_train_items 274432.
I0302 19:02:25.720457 22699590365312 run.py:483] Algo bellman_ford step 8576 current loss 0.421737, current_train_items 274464.
I0302 19:02:25.744193 22699590365312 run.py:483] Algo bellman_ford step 8577 current loss 0.535824, current_train_items 274496.
I0302 19:02:25.775951 22699590365312 run.py:483] Algo bellman_ford step 8578 current loss 0.700663, current_train_items 274528.
I0302 19:02:25.809787 22699590365312 run.py:483] Algo bellman_ford step 8579 current loss 0.705151, current_train_items 274560.
I0302 19:02:25.829347 22699590365312 run.py:483] Algo bellman_ford step 8580 current loss 0.342322, current_train_items 274592.
I0302 19:02:25.845383 22699590365312 run.py:483] Algo bellman_ford step 8581 current loss 0.424688, current_train_items 274624.
I0302 19:02:25.868342 22699590365312 run.py:483] Algo bellman_ford step 8582 current loss 0.565912, current_train_items 274656.
I0302 19:02:25.899796 22699590365312 run.py:483] Algo bellman_ford step 8583 current loss 0.656543, current_train_items 274688.
I0302 19:02:25.934648 22699590365312 run.py:483] Algo bellman_ford step 8584 current loss 0.662640, current_train_items 274720.
I0302 19:02:25.954446 22699590365312 run.py:483] Algo bellman_ford step 8585 current loss 0.258092, current_train_items 274752.
I0302 19:02:25.970419 22699590365312 run.py:483] Algo bellman_ford step 8586 current loss 0.421476, current_train_items 274784.
I0302 19:02:25.995136 22699590365312 run.py:483] Algo bellman_ford step 8587 current loss 0.665575, current_train_items 274816.
I0302 19:02:26.026588 22699590365312 run.py:483] Algo bellman_ford step 8588 current loss 0.612976, current_train_items 274848.
I0302 19:02:26.059027 22699590365312 run.py:483] Algo bellman_ford step 8589 current loss 0.865692, current_train_items 274880.
I0302 19:02:26.079004 22699590365312 run.py:483] Algo bellman_ford step 8590 current loss 0.314854, current_train_items 274912.
I0302 19:02:26.094228 22699590365312 run.py:483] Algo bellman_ford step 8591 current loss 0.391241, current_train_items 274944.
I0302 19:02:26.117970 22699590365312 run.py:483] Algo bellman_ford step 8592 current loss 0.559326, current_train_items 274976.
I0302 19:02:26.149848 22699590365312 run.py:483] Algo bellman_ford step 8593 current loss 0.662913, current_train_items 275008.
I0302 19:02:26.183437 22699590365312 run.py:483] Algo bellman_ford step 8594 current loss 0.790582, current_train_items 275040.
I0302 19:02:26.202687 22699590365312 run.py:483] Algo bellman_ford step 8595 current loss 0.258574, current_train_items 275072.
I0302 19:02:26.218410 22699590365312 run.py:483] Algo bellman_ford step 8596 current loss 0.388985, current_train_items 275104.
I0302 19:02:26.242151 22699590365312 run.py:483] Algo bellman_ford step 8597 current loss 0.483793, current_train_items 275136.
I0302 19:02:26.272131 22699590365312 run.py:483] Algo bellman_ford step 8598 current loss 0.528829, current_train_items 275168.
I0302 19:02:26.306419 22699590365312 run.py:483] Algo bellman_ford step 8599 current loss 0.770930, current_train_items 275200.
I0302 19:02:26.326230 22699590365312 run.py:483] Algo bellman_ford step 8600 current loss 0.330951, current_train_items 275232.
I0302 19:02:26.334248 22699590365312 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0302 19:02:26.334356 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0302 19:02:26.351284 22699590365312 run.py:483] Algo bellman_ford step 8601 current loss 0.428267, current_train_items 275264.
I0302 19:02:26.376144 22699590365312 run.py:483] Algo bellman_ford step 8602 current loss 0.605249, current_train_items 275296.
I0302 19:02:26.408818 22699590365312 run.py:483] Algo bellman_ford step 8603 current loss 0.605166, current_train_items 275328.
I0302 19:02:26.443265 22699590365312 run.py:483] Algo bellman_ford step 8604 current loss 0.682485, current_train_items 275360.
I0302 19:02:26.463122 22699590365312 run.py:483] Algo bellman_ford step 8605 current loss 0.276745, current_train_items 275392.
I0302 19:02:26.478359 22699590365312 run.py:483] Algo bellman_ford step 8606 current loss 0.362656, current_train_items 275424.
I0302 19:02:26.501644 22699590365312 run.py:483] Algo bellman_ford step 8607 current loss 0.511650, current_train_items 275456.
I0302 19:02:26.533088 22699590365312 run.py:483] Algo bellman_ford step 8608 current loss 0.615239, current_train_items 275488.
I0302 19:02:26.568630 22699590365312 run.py:483] Algo bellman_ford step 8609 current loss 0.714322, current_train_items 275520.
I0302 19:02:26.588249 22699590365312 run.py:483] Algo bellman_ford step 8610 current loss 0.268303, current_train_items 275552.
I0302 19:02:26.604236 22699590365312 run.py:483] Algo bellman_ford step 8611 current loss 0.407956, current_train_items 275584.
I0302 19:02:26.628439 22699590365312 run.py:483] Algo bellman_ford step 8612 current loss 0.610759, current_train_items 275616.
I0302 19:02:26.660058 22699590365312 run.py:483] Algo bellman_ford step 8613 current loss 0.623819, current_train_items 275648.
I0302 19:02:26.691491 22699590365312 run.py:483] Algo bellman_ford step 8614 current loss 0.619249, current_train_items 275680.
I0302 19:02:26.711113 22699590365312 run.py:483] Algo bellman_ford step 8615 current loss 0.221221, current_train_items 275712.
I0302 19:02:26.727090 22699590365312 run.py:483] Algo bellman_ford step 8616 current loss 0.385851, current_train_items 275744.
I0302 19:02:26.750889 22699590365312 run.py:483] Algo bellman_ford step 8617 current loss 0.467448, current_train_items 275776.
I0302 19:02:26.782983 22699590365312 run.py:483] Algo bellman_ford step 8618 current loss 0.625918, current_train_items 275808.
I0302 19:02:26.816785 22699590365312 run.py:483] Algo bellman_ford step 8619 current loss 0.694617, current_train_items 275840.
I0302 19:02:26.836329 22699590365312 run.py:483] Algo bellman_ford step 8620 current loss 0.278499, current_train_items 275872.
I0302 19:02:26.852453 22699590365312 run.py:483] Algo bellman_ford step 8621 current loss 0.383770, current_train_items 275904.
I0302 19:02:26.876944 22699590365312 run.py:483] Algo bellman_ford step 8622 current loss 0.532381, current_train_items 275936.
I0302 19:02:26.909045 22699590365312 run.py:483] Algo bellman_ford step 8623 current loss 0.596954, current_train_items 275968.
I0302 19:02:26.942463 22699590365312 run.py:483] Algo bellman_ford step 8624 current loss 0.640506, current_train_items 276000.
I0302 19:02:26.961933 22699590365312 run.py:483] Algo bellman_ford step 8625 current loss 0.300055, current_train_items 276032.
I0302 19:02:26.977951 22699590365312 run.py:483] Algo bellman_ford step 8626 current loss 0.423100, current_train_items 276064.
I0302 19:02:27.001927 22699590365312 run.py:483] Algo bellman_ford step 8627 current loss 0.547739, current_train_items 276096.
I0302 19:02:27.034708 22699590365312 run.py:483] Algo bellman_ford step 8628 current loss 0.720707, current_train_items 276128.
I0302 19:02:27.067439 22699590365312 run.py:483] Algo bellman_ford step 8629 current loss 0.858921, current_train_items 276160.
I0302 19:02:27.087068 22699590365312 run.py:483] Algo bellman_ford step 8630 current loss 0.306576, current_train_items 276192.
I0302 19:02:27.103196 22699590365312 run.py:483] Algo bellman_ford step 8631 current loss 0.416497, current_train_items 276224.
I0302 19:02:27.126959 22699590365312 run.py:483] Algo bellman_ford step 8632 current loss 0.636426, current_train_items 276256.
I0302 19:02:27.157857 22699590365312 run.py:483] Algo bellman_ford step 8633 current loss 0.529514, current_train_items 276288.
I0302 19:02:27.191131 22699590365312 run.py:483] Algo bellman_ford step 8634 current loss 0.645700, current_train_items 276320.
I0302 19:02:27.211011 22699590365312 run.py:483] Algo bellman_ford step 8635 current loss 0.304681, current_train_items 276352.
I0302 19:02:27.227100 22699590365312 run.py:483] Algo bellman_ford step 8636 current loss 0.363744, current_train_items 276384.
I0302 19:02:27.251667 22699590365312 run.py:483] Algo bellman_ford step 8637 current loss 0.508929, current_train_items 276416.
I0302 19:02:27.282449 22699590365312 run.py:483] Algo bellman_ford step 8638 current loss 0.561460, current_train_items 276448.
I0302 19:02:27.316884 22699590365312 run.py:483] Algo bellman_ford step 8639 current loss 0.636211, current_train_items 276480.
I0302 19:02:27.336538 22699590365312 run.py:483] Algo bellman_ford step 8640 current loss 0.279156, current_train_items 276512.
I0302 19:02:27.352857 22699590365312 run.py:483] Algo bellman_ford step 8641 current loss 0.446755, current_train_items 276544.
I0302 19:02:27.376727 22699590365312 run.py:483] Algo bellman_ford step 8642 current loss 0.486418, current_train_items 276576.
I0302 19:02:27.408802 22699590365312 run.py:483] Algo bellman_ford step 8643 current loss 0.670770, current_train_items 276608.
I0302 19:02:27.442746 22699590365312 run.py:483] Algo bellman_ford step 8644 current loss 0.808745, current_train_items 276640.
I0302 19:02:27.462322 22699590365312 run.py:483] Algo bellman_ford step 8645 current loss 0.327072, current_train_items 276672.
I0302 19:02:27.478266 22699590365312 run.py:483] Algo bellman_ford step 8646 current loss 0.430480, current_train_items 276704.
I0302 19:02:27.501982 22699590365312 run.py:483] Algo bellman_ford step 8647 current loss 0.597941, current_train_items 276736.
I0302 19:02:27.534483 22699590365312 run.py:483] Algo bellman_ford step 8648 current loss 0.613793, current_train_items 276768.
I0302 19:02:27.567342 22699590365312 run.py:483] Algo bellman_ford step 8649 current loss 0.682494, current_train_items 276800.
I0302 19:02:27.586667 22699590365312 run.py:483] Algo bellman_ford step 8650 current loss 0.341271, current_train_items 276832.
I0302 19:02:27.594841 22699590365312 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0302 19:02:27.594945 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:02:27.611800 22699590365312 run.py:483] Algo bellman_ford step 8651 current loss 0.372029, current_train_items 276864.
I0302 19:02:27.635725 22699590365312 run.py:483] Algo bellman_ford step 8652 current loss 0.515440, current_train_items 276896.
I0302 19:02:27.668250 22699590365312 run.py:483] Algo bellman_ford step 8653 current loss 0.558215, current_train_items 276928.
I0302 19:02:27.703391 22699590365312 run.py:483] Algo bellman_ford step 8654 current loss 0.671899, current_train_items 276960.
I0302 19:02:27.723457 22699590365312 run.py:483] Algo bellman_ford step 8655 current loss 0.232910, current_train_items 276992.
I0302 19:02:27.739190 22699590365312 run.py:483] Algo bellman_ford step 8656 current loss 0.425982, current_train_items 277024.
I0302 19:02:27.764499 22699590365312 run.py:483] Algo bellman_ford step 8657 current loss 0.669991, current_train_items 277056.
I0302 19:02:27.796825 22699590365312 run.py:483] Algo bellman_ford step 8658 current loss 0.634059, current_train_items 277088.
I0302 19:02:27.830306 22699590365312 run.py:483] Algo bellman_ford step 8659 current loss 0.676397, current_train_items 277120.
I0302 19:02:27.850345 22699590365312 run.py:483] Algo bellman_ford step 8660 current loss 0.227370, current_train_items 277152.
I0302 19:02:27.866137 22699590365312 run.py:483] Algo bellman_ford step 8661 current loss 0.421508, current_train_items 277184.
I0302 19:02:27.889360 22699590365312 run.py:483] Algo bellman_ford step 8662 current loss 0.603189, current_train_items 277216.
I0302 19:02:27.920354 22699590365312 run.py:483] Algo bellman_ford step 8663 current loss 0.604421, current_train_items 277248.
I0302 19:02:27.954825 22699590365312 run.py:483] Algo bellman_ford step 8664 current loss 0.587846, current_train_items 277280.
I0302 19:02:27.974447 22699590365312 run.py:483] Algo bellman_ford step 8665 current loss 0.318918, current_train_items 277312.
I0302 19:02:27.989999 22699590365312 run.py:483] Algo bellman_ford step 8666 current loss 0.374756, current_train_items 277344.
I0302 19:02:28.013657 22699590365312 run.py:483] Algo bellman_ford step 8667 current loss 0.492016, current_train_items 277376.
I0302 19:02:28.044264 22699590365312 run.py:483] Algo bellman_ford step 8668 current loss 0.601217, current_train_items 277408.
I0302 19:02:28.077052 22699590365312 run.py:483] Algo bellman_ford step 8669 current loss 0.663082, current_train_items 277440.
I0302 19:02:28.096894 22699590365312 run.py:483] Algo bellman_ford step 8670 current loss 0.252430, current_train_items 277472.
I0302 19:02:28.113127 22699590365312 run.py:483] Algo bellman_ford step 8671 current loss 0.447355, current_train_items 277504.
I0302 19:02:28.136407 22699590365312 run.py:483] Algo bellman_ford step 8672 current loss 0.518992, current_train_items 277536.
I0302 19:02:28.167871 22699590365312 run.py:483] Algo bellman_ford step 8673 current loss 0.551333, current_train_items 277568.
I0302 19:02:28.199078 22699590365312 run.py:483] Algo bellman_ford step 8674 current loss 0.714727, current_train_items 277600.
I0302 19:02:28.219022 22699590365312 run.py:483] Algo bellman_ford step 8675 current loss 0.293000, current_train_items 277632.
I0302 19:02:28.234941 22699590365312 run.py:483] Algo bellman_ford step 8676 current loss 0.457566, current_train_items 277664.
I0302 19:02:28.258404 22699590365312 run.py:483] Algo bellman_ford step 8677 current loss 0.633563, current_train_items 277696.
I0302 19:02:28.290340 22699590365312 run.py:483] Algo bellman_ford step 8678 current loss 0.621991, current_train_items 277728.
I0302 19:02:28.323336 22699590365312 run.py:483] Algo bellman_ford step 8679 current loss 0.686470, current_train_items 277760.
I0302 19:02:28.342818 22699590365312 run.py:483] Algo bellman_ford step 8680 current loss 0.300786, current_train_items 277792.
I0302 19:02:28.359027 22699590365312 run.py:483] Algo bellman_ford step 8681 current loss 0.467513, current_train_items 277824.
I0302 19:02:28.382807 22699590365312 run.py:483] Algo bellman_ford step 8682 current loss 0.553987, current_train_items 277856.
I0302 19:02:28.413917 22699590365312 run.py:483] Algo bellman_ford step 8683 current loss 0.617760, current_train_items 277888.
I0302 19:02:28.446911 22699590365312 run.py:483] Algo bellman_ford step 8684 current loss 0.743856, current_train_items 277920.
I0302 19:02:28.466829 22699590365312 run.py:483] Algo bellman_ford step 8685 current loss 0.348838, current_train_items 277952.
I0302 19:02:28.482514 22699590365312 run.py:483] Algo bellman_ford step 8686 current loss 0.477906, current_train_items 277984.
I0302 19:02:28.506825 22699590365312 run.py:483] Algo bellman_ford step 8687 current loss 0.603112, current_train_items 278016.
I0302 19:02:28.539062 22699590365312 run.py:483] Algo bellman_ford step 8688 current loss 0.654806, current_train_items 278048.
I0302 19:02:28.571766 22699590365312 run.py:483] Algo bellman_ford step 8689 current loss 0.656498, current_train_items 278080.
I0302 19:02:28.591536 22699590365312 run.py:483] Algo bellman_ford step 8690 current loss 0.233305, current_train_items 278112.
I0302 19:02:28.607678 22699590365312 run.py:483] Algo bellman_ford step 8691 current loss 0.411905, current_train_items 278144.
I0302 19:02:28.630929 22699590365312 run.py:483] Algo bellman_ford step 8692 current loss 0.526329, current_train_items 278176.
I0302 19:02:28.662082 22699590365312 run.py:483] Algo bellman_ford step 8693 current loss 0.614575, current_train_items 278208.
I0302 19:02:28.695322 22699590365312 run.py:483] Algo bellman_ford step 8694 current loss 0.763300, current_train_items 278240.
I0302 19:02:28.714869 22699590365312 run.py:483] Algo bellman_ford step 8695 current loss 0.308347, current_train_items 278272.
I0302 19:02:28.730735 22699590365312 run.py:483] Algo bellman_ford step 8696 current loss 0.420981, current_train_items 278304.
I0302 19:02:28.754626 22699590365312 run.py:483] Algo bellman_ford step 8697 current loss 0.570502, current_train_items 278336.
I0302 19:02:28.786334 22699590365312 run.py:483] Algo bellman_ford step 8698 current loss 0.605585, current_train_items 278368.
I0302 19:02:28.821945 22699590365312 run.py:483] Algo bellman_ford step 8699 current loss 0.757271, current_train_items 278400.
I0302 19:02:28.841649 22699590365312 run.py:483] Algo bellman_ford step 8700 current loss 0.270995, current_train_items 278432.
I0302 19:02:28.849608 22699590365312 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.939453125, 'score': 0.939453125, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0302 19:02:28.849715 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.939, val scores are: bellman_ford: 0.939
I0302 19:02:28.866423 22699590365312 run.py:483] Algo bellman_ford step 8701 current loss 0.423868, current_train_items 278464.
I0302 19:02:28.890370 22699590365312 run.py:483] Algo bellman_ford step 8702 current loss 0.517685, current_train_items 278496.
I0302 19:02:28.922840 22699590365312 run.py:483] Algo bellman_ford step 8703 current loss 0.626346, current_train_items 278528.
I0302 19:02:28.958490 22699590365312 run.py:483] Algo bellman_ford step 8704 current loss 0.683259, current_train_items 278560.
I0302 19:02:28.978890 22699590365312 run.py:483] Algo bellman_ford step 8705 current loss 0.312340, current_train_items 278592.
I0302 19:02:28.994475 22699590365312 run.py:483] Algo bellman_ford step 8706 current loss 0.475678, current_train_items 278624.
I0302 19:02:29.018452 22699590365312 run.py:483] Algo bellman_ford step 8707 current loss 0.556573, current_train_items 278656.
I0302 19:02:29.050554 22699590365312 run.py:483] Algo bellman_ford step 8708 current loss 0.573727, current_train_items 278688.
I0302 19:02:29.083829 22699590365312 run.py:483] Algo bellman_ford step 8709 current loss 0.668666, current_train_items 278720.
I0302 19:02:29.103571 22699590365312 run.py:483] Algo bellman_ford step 8710 current loss 0.348305, current_train_items 278752.
I0302 19:02:29.119790 22699590365312 run.py:483] Algo bellman_ford step 8711 current loss 0.491823, current_train_items 278784.
I0302 19:02:29.143690 22699590365312 run.py:483] Algo bellman_ford step 8712 current loss 0.533217, current_train_items 278816.
I0302 19:02:29.177161 22699590365312 run.py:483] Algo bellman_ford step 8713 current loss 0.759305, current_train_items 278848.
I0302 19:02:29.209719 22699590365312 run.py:483] Algo bellman_ford step 8714 current loss 0.710039, current_train_items 278880.
I0302 19:02:29.228970 22699590365312 run.py:483] Algo bellman_ford step 8715 current loss 0.264612, current_train_items 278912.
I0302 19:02:29.245215 22699590365312 run.py:483] Algo bellman_ford step 8716 current loss 0.456565, current_train_items 278944.
I0302 19:02:29.268601 22699590365312 run.py:483] Algo bellman_ford step 8717 current loss 0.517233, current_train_items 278976.
I0302 19:02:29.299376 22699590365312 run.py:483] Algo bellman_ford step 8718 current loss 0.565015, current_train_items 279008.
I0302 19:02:29.332188 22699590365312 run.py:483] Algo bellman_ford step 8719 current loss 0.742764, current_train_items 279040.
I0302 19:02:29.351875 22699590365312 run.py:483] Algo bellman_ford step 8720 current loss 0.334356, current_train_items 279072.
I0302 19:02:29.368202 22699590365312 run.py:483] Algo bellman_ford step 8721 current loss 0.446882, current_train_items 279104.
I0302 19:02:29.392874 22699590365312 run.py:483] Algo bellman_ford step 8722 current loss 0.529092, current_train_items 279136.
I0302 19:02:29.423602 22699590365312 run.py:483] Algo bellman_ford step 8723 current loss 0.550159, current_train_items 279168.
I0302 19:02:29.456689 22699590365312 run.py:483] Algo bellman_ford step 8724 current loss 0.810180, current_train_items 279200.
I0302 19:02:29.476406 22699590365312 run.py:483] Algo bellman_ford step 8725 current loss 0.276745, current_train_items 279232.
I0302 19:02:29.492583 22699590365312 run.py:483] Algo bellman_ford step 8726 current loss 0.450485, current_train_items 279264.
I0302 19:02:29.516497 22699590365312 run.py:483] Algo bellman_ford step 8727 current loss 0.541757, current_train_items 279296.
I0302 19:02:29.547512 22699590365312 run.py:483] Algo bellman_ford step 8728 current loss 0.621689, current_train_items 279328.
I0302 19:02:29.580401 22699590365312 run.py:483] Algo bellman_ford step 8729 current loss 0.743750, current_train_items 279360.
I0302 19:02:29.599779 22699590365312 run.py:483] Algo bellman_ford step 8730 current loss 0.275690, current_train_items 279392.
I0302 19:02:29.615276 22699590365312 run.py:483] Algo bellman_ford step 8731 current loss 0.343106, current_train_items 279424.
I0302 19:02:29.639045 22699590365312 run.py:483] Algo bellman_ford step 8732 current loss 0.541092, current_train_items 279456.
I0302 19:02:29.671045 22699590365312 run.py:483] Algo bellman_ford step 8733 current loss 0.633123, current_train_items 279488.
I0302 19:02:29.706330 22699590365312 run.py:483] Algo bellman_ford step 8734 current loss 0.766823, current_train_items 279520.
I0302 19:02:29.725645 22699590365312 run.py:483] Algo bellman_ford step 8735 current loss 0.333999, current_train_items 279552.
I0302 19:02:29.741511 22699590365312 run.py:483] Algo bellman_ford step 8736 current loss 0.457426, current_train_items 279584.
I0302 19:02:29.766428 22699590365312 run.py:483] Algo bellman_ford step 8737 current loss 0.564848, current_train_items 279616.
I0302 19:02:29.797812 22699590365312 run.py:483] Algo bellman_ford step 8738 current loss 0.625441, current_train_items 279648.
I0302 19:02:29.831852 22699590365312 run.py:483] Algo bellman_ford step 8739 current loss 0.788309, current_train_items 279680.
I0302 19:02:29.851149 22699590365312 run.py:483] Algo bellman_ford step 8740 current loss 0.342280, current_train_items 279712.
I0302 19:02:29.867002 22699590365312 run.py:483] Algo bellman_ford step 8741 current loss 0.429534, current_train_items 279744.
I0302 19:02:29.891125 22699590365312 run.py:483] Algo bellman_ford step 8742 current loss 0.629521, current_train_items 279776.
I0302 19:02:29.922864 22699590365312 run.py:483] Algo bellman_ford step 8743 current loss 0.606660, current_train_items 279808.
I0302 19:02:29.956656 22699590365312 run.py:483] Algo bellman_ford step 8744 current loss 0.635983, current_train_items 279840.
I0302 19:02:29.976396 22699590365312 run.py:483] Algo bellman_ford step 8745 current loss 0.304995, current_train_items 279872.
I0302 19:02:29.992723 22699590365312 run.py:483] Algo bellman_ford step 8746 current loss 0.480843, current_train_items 279904.
I0302 19:02:30.016124 22699590365312 run.py:483] Algo bellman_ford step 8747 current loss 0.569535, current_train_items 279936.
I0302 19:02:30.048770 22699590365312 run.py:483] Algo bellman_ford step 8748 current loss 0.638488, current_train_items 279968.
I0302 19:02:30.083610 22699590365312 run.py:483] Algo bellman_ford step 8749 current loss 0.744024, current_train_items 280000.
I0302 19:02:30.103193 22699590365312 run.py:483] Algo bellman_ford step 8750 current loss 0.290689, current_train_items 280032.
I0302 19:02:30.111292 22699590365312 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0302 19:02:30.111398 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:02:30.128132 22699590365312 run.py:483] Algo bellman_ford step 8751 current loss 0.370272, current_train_items 280064.
I0302 19:02:30.151679 22699590365312 run.py:483] Algo bellman_ford step 8752 current loss 0.520353, current_train_items 280096.
I0302 19:02:30.183837 22699590365312 run.py:483] Algo bellman_ford step 8753 current loss 0.680249, current_train_items 280128.
I0302 19:02:30.217899 22699590365312 run.py:483] Algo bellman_ford step 8754 current loss 0.684840, current_train_items 280160.
I0302 19:02:30.237626 22699590365312 run.py:483] Algo bellman_ford step 8755 current loss 0.277038, current_train_items 280192.
I0302 19:02:30.253243 22699590365312 run.py:483] Algo bellman_ford step 8756 current loss 0.440133, current_train_items 280224.
I0302 19:02:30.277785 22699590365312 run.py:483] Algo bellman_ford step 8757 current loss 0.583289, current_train_items 280256.
I0302 19:02:30.308572 22699590365312 run.py:483] Algo bellman_ford step 8758 current loss 0.665573, current_train_items 280288.
I0302 19:02:30.341825 22699590365312 run.py:483] Algo bellman_ford step 8759 current loss 0.709662, current_train_items 280320.
I0302 19:02:30.361410 22699590365312 run.py:483] Algo bellman_ford step 8760 current loss 0.275208, current_train_items 280352.
I0302 19:02:30.377554 22699590365312 run.py:483] Algo bellman_ford step 8761 current loss 0.441857, current_train_items 280384.
I0302 19:02:30.401356 22699590365312 run.py:483] Algo bellman_ford step 8762 current loss 0.520732, current_train_items 280416.
I0302 19:02:30.432659 22699590365312 run.py:483] Algo bellman_ford step 8763 current loss 0.590752, current_train_items 280448.
I0302 19:02:30.467785 22699590365312 run.py:483] Algo bellman_ford step 8764 current loss 0.696396, current_train_items 280480.
I0302 19:02:30.487105 22699590365312 run.py:483] Algo bellman_ford step 8765 current loss 0.274910, current_train_items 280512.
I0302 19:02:30.503494 22699590365312 run.py:483] Algo bellman_ford step 8766 current loss 0.419419, current_train_items 280544.
I0302 19:02:30.525862 22699590365312 run.py:483] Algo bellman_ford step 8767 current loss 0.467268, current_train_items 280576.
I0302 19:02:30.556355 22699590365312 run.py:483] Algo bellman_ford step 8768 current loss 0.597477, current_train_items 280608.
I0302 19:02:30.588036 22699590365312 run.py:483] Algo bellman_ford step 8769 current loss 0.574966, current_train_items 280640.
I0302 19:02:30.607547 22699590365312 run.py:483] Algo bellman_ford step 8770 current loss 0.234949, current_train_items 280672.
I0302 19:02:30.623744 22699590365312 run.py:483] Algo bellman_ford step 8771 current loss 0.408625, current_train_items 280704.
I0302 19:02:30.647451 22699590365312 run.py:483] Algo bellman_ford step 8772 current loss 0.569921, current_train_items 280736.
I0302 19:02:30.679388 22699590365312 run.py:483] Algo bellman_ford step 8773 current loss 0.664779, current_train_items 280768.
I0302 19:02:30.711904 22699590365312 run.py:483] Algo bellman_ford step 8774 current loss 0.609482, current_train_items 280800.
I0302 19:02:30.731424 22699590365312 run.py:483] Algo bellman_ford step 8775 current loss 0.281248, current_train_items 280832.
I0302 19:02:30.747424 22699590365312 run.py:483] Algo bellman_ford step 8776 current loss 0.471752, current_train_items 280864.
I0302 19:02:30.770119 22699590365312 run.py:483] Algo bellman_ford step 8777 current loss 0.522115, current_train_items 280896.
I0302 19:02:30.801187 22699590365312 run.py:483] Algo bellman_ford step 8778 current loss 0.559530, current_train_items 280928.
I0302 19:02:30.835435 22699590365312 run.py:483] Algo bellman_ford step 8779 current loss 0.645064, current_train_items 280960.
I0302 19:02:30.854477 22699590365312 run.py:483] Algo bellman_ford step 8780 current loss 0.329759, current_train_items 280992.
I0302 19:02:30.870337 22699590365312 run.py:483] Algo bellman_ford step 8781 current loss 0.508019, current_train_items 281024.
I0302 19:02:30.893732 22699590365312 run.py:483] Algo bellman_ford step 8782 current loss 0.585452, current_train_items 281056.
I0302 19:02:30.925455 22699590365312 run.py:483] Algo bellman_ford step 8783 current loss 0.704970, current_train_items 281088.
I0302 19:02:30.959721 22699590365312 run.py:483] Algo bellman_ford step 8784 current loss 0.803485, current_train_items 281120.
I0302 19:02:30.979390 22699590365312 run.py:483] Algo bellman_ford step 8785 current loss 0.274699, current_train_items 281152.
I0302 19:02:30.995398 22699590365312 run.py:483] Algo bellman_ford step 8786 current loss 0.472800, current_train_items 281184.
I0302 19:02:31.019038 22699590365312 run.py:483] Algo bellman_ford step 8787 current loss 0.588376, current_train_items 281216.
I0302 19:02:31.050055 22699590365312 run.py:483] Algo bellman_ford step 8788 current loss 0.680447, current_train_items 281248.
I0302 19:02:31.083510 22699590365312 run.py:483] Algo bellman_ford step 8789 current loss 0.725253, current_train_items 281280.
I0302 19:02:31.103133 22699590365312 run.py:483] Algo bellman_ford step 8790 current loss 0.305582, current_train_items 281312.
I0302 19:02:31.119516 22699590365312 run.py:483] Algo bellman_ford step 8791 current loss 0.481398, current_train_items 281344.
I0302 19:02:31.142452 22699590365312 run.py:483] Algo bellman_ford step 8792 current loss 0.519263, current_train_items 281376.
I0302 19:02:31.173736 22699590365312 run.py:483] Algo bellman_ford step 8793 current loss 0.581106, current_train_items 281408.
I0302 19:02:31.208245 22699590365312 run.py:483] Algo bellman_ford step 8794 current loss 0.685642, current_train_items 281440.
I0302 19:02:31.227448 22699590365312 run.py:483] Algo bellman_ford step 8795 current loss 0.294579, current_train_items 281472.
I0302 19:02:31.243400 22699590365312 run.py:483] Algo bellman_ford step 8796 current loss 0.436769, current_train_items 281504.
I0302 19:02:31.266376 22699590365312 run.py:483] Algo bellman_ford step 8797 current loss 0.522091, current_train_items 281536.
I0302 19:02:31.298333 22699590365312 run.py:483] Algo bellman_ford step 8798 current loss 0.598202, current_train_items 281568.
I0302 19:02:31.333411 22699590365312 run.py:483] Algo bellman_ford step 8799 current loss 0.780643, current_train_items 281600.
I0302 19:02:31.352782 22699590365312 run.py:483] Algo bellman_ford step 8800 current loss 0.256651, current_train_items 281632.
I0302 19:02:31.360414 22699590365312 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0302 19:02:31.360518 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 19:02:31.376847 22699590365312 run.py:483] Algo bellman_ford step 8801 current loss 0.424960, current_train_items 281664.
I0302 19:02:31.401434 22699590365312 run.py:483] Algo bellman_ford step 8802 current loss 0.606888, current_train_items 281696.
I0302 19:02:31.434354 22699590365312 run.py:483] Algo bellman_ford step 8803 current loss 0.608485, current_train_items 281728.
I0302 19:02:31.468280 22699590365312 run.py:483] Algo bellman_ford step 8804 current loss 0.650261, current_train_items 281760.
I0302 19:02:31.488172 22699590365312 run.py:483] Algo bellman_ford step 8805 current loss 0.262487, current_train_items 281792.
I0302 19:02:31.504132 22699590365312 run.py:483] Algo bellman_ford step 8806 current loss 0.408388, current_train_items 281824.
I0302 19:02:31.528336 22699590365312 run.py:483] Algo bellman_ford step 8807 current loss 0.559233, current_train_items 281856.
I0302 19:02:31.559220 22699590365312 run.py:483] Algo bellman_ford step 8808 current loss 0.605494, current_train_items 281888.
I0302 19:02:31.594750 22699590365312 run.py:483] Algo bellman_ford step 8809 current loss 0.703196, current_train_items 281920.
I0302 19:02:31.614625 22699590365312 run.py:483] Algo bellman_ford step 8810 current loss 0.372044, current_train_items 281952.
I0302 19:02:31.630826 22699590365312 run.py:483] Algo bellman_ford step 8811 current loss 0.511547, current_train_items 281984.
I0302 19:02:31.654482 22699590365312 run.py:483] Algo bellman_ford step 8812 current loss 0.551480, current_train_items 282016.
I0302 19:02:31.686072 22699590365312 run.py:483] Algo bellman_ford step 8813 current loss 0.594833, current_train_items 282048.
I0302 19:02:31.719719 22699590365312 run.py:483] Algo bellman_ford step 8814 current loss 0.696529, current_train_items 282080.
I0302 19:02:31.739300 22699590365312 run.py:483] Algo bellman_ford step 8815 current loss 0.290622, current_train_items 282112.
I0302 19:02:31.755368 22699590365312 run.py:483] Algo bellman_ford step 8816 current loss 0.408492, current_train_items 282144.
I0302 19:02:31.778696 22699590365312 run.py:483] Algo bellman_ford step 8817 current loss 0.491364, current_train_items 282176.
I0302 19:02:31.809080 22699590365312 run.py:483] Algo bellman_ford step 8818 current loss 0.618591, current_train_items 282208.
I0302 19:02:31.846520 22699590365312 run.py:483] Algo bellman_ford step 8819 current loss 0.718191, current_train_items 282240.
I0302 19:02:31.866054 22699590365312 run.py:483] Algo bellman_ford step 8820 current loss 0.316064, current_train_items 282272.
I0302 19:02:31.881894 22699590365312 run.py:483] Algo bellman_ford step 8821 current loss 0.444728, current_train_items 282304.
I0302 19:02:31.905658 22699590365312 run.py:483] Algo bellman_ford step 8822 current loss 0.503964, current_train_items 282336.
I0302 19:02:31.937744 22699590365312 run.py:483] Algo bellman_ford step 8823 current loss 0.666198, current_train_items 282368.
I0302 19:02:31.970371 22699590365312 run.py:483] Algo bellman_ford step 8824 current loss 0.716291, current_train_items 282400.
I0302 19:02:31.989976 22699590365312 run.py:483] Algo bellman_ford step 8825 current loss 0.256425, current_train_items 282432.
I0302 19:02:32.006165 22699590365312 run.py:483] Algo bellman_ford step 8826 current loss 0.438269, current_train_items 282464.
I0302 19:02:32.030302 22699590365312 run.py:483] Algo bellman_ford step 8827 current loss 0.636328, current_train_items 282496.
I0302 19:02:32.061768 22699590365312 run.py:483] Algo bellman_ford step 8828 current loss 0.710958, current_train_items 282528.
I0302 19:02:32.094957 22699590365312 run.py:483] Algo bellman_ford step 8829 current loss 0.703710, current_train_items 282560.
I0302 19:02:32.114327 22699590365312 run.py:483] Algo bellman_ford step 8830 current loss 0.321722, current_train_items 282592.
I0302 19:02:32.130150 22699590365312 run.py:483] Algo bellman_ford step 8831 current loss 0.437662, current_train_items 282624.
I0302 19:02:32.154350 22699590365312 run.py:483] Algo bellman_ford step 8832 current loss 0.603459, current_train_items 282656.
I0302 19:02:32.184214 22699590365312 run.py:483] Algo bellman_ford step 8833 current loss 0.544661, current_train_items 282688.
I0302 19:02:32.216379 22699590365312 run.py:483] Algo bellman_ford step 8834 current loss 0.716021, current_train_items 282720.
I0302 19:02:32.236019 22699590365312 run.py:483] Algo bellman_ford step 8835 current loss 0.265631, current_train_items 282752.
I0302 19:02:32.251830 22699590365312 run.py:483] Algo bellman_ford step 8836 current loss 0.422756, current_train_items 282784.
I0302 19:02:32.276180 22699590365312 run.py:483] Algo bellman_ford step 8837 current loss 0.522792, current_train_items 282816.
I0302 19:02:32.307844 22699590365312 run.py:483] Algo bellman_ford step 8838 current loss 0.548402, current_train_items 282848.
I0302 19:02:32.341135 22699590365312 run.py:483] Algo bellman_ford step 8839 current loss 0.672110, current_train_items 282880.
I0302 19:02:32.360888 22699590365312 run.py:483] Algo bellman_ford step 8840 current loss 0.314095, current_train_items 282912.
I0302 19:02:32.377166 22699590365312 run.py:483] Algo bellman_ford step 8841 current loss 0.453249, current_train_items 282944.
I0302 19:02:32.401113 22699590365312 run.py:483] Algo bellman_ford step 8842 current loss 0.570418, current_train_items 282976.
I0302 19:02:32.432400 22699590365312 run.py:483] Algo bellman_ford step 8843 current loss 0.603853, current_train_items 283008.
I0302 19:02:32.467755 22699590365312 run.py:483] Algo bellman_ford step 8844 current loss 0.838251, current_train_items 283040.
I0302 19:02:32.487256 22699590365312 run.py:483] Algo bellman_ford step 8845 current loss 0.368047, current_train_items 283072.
I0302 19:02:32.503127 22699590365312 run.py:483] Algo bellman_ford step 8846 current loss 0.388414, current_train_items 283104.
I0302 19:02:32.526916 22699590365312 run.py:483] Algo bellman_ford step 8847 current loss 0.497708, current_train_items 283136.
I0302 19:02:32.556858 22699590365312 run.py:483] Algo bellman_ford step 8848 current loss 0.587278, current_train_items 283168.
I0302 19:02:32.590428 22699590365312 run.py:483] Algo bellman_ford step 8849 current loss 0.687518, current_train_items 283200.
I0302 19:02:32.609893 22699590365312 run.py:483] Algo bellman_ford step 8850 current loss 0.309634, current_train_items 283232.
I0302 19:02:32.617933 22699590365312 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0302 19:02:32.618040 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.950, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:02:32.634782 22699590365312 run.py:483] Algo bellman_ford step 8851 current loss 0.451870, current_train_items 283264.
I0302 19:02:32.659317 22699590365312 run.py:483] Algo bellman_ford step 8852 current loss 0.529927, current_train_items 283296.
I0302 19:02:32.691622 22699590365312 run.py:483] Algo bellman_ford step 8853 current loss 0.564181, current_train_items 283328.
I0302 19:02:32.725883 22699590365312 run.py:483] Algo bellman_ford step 8854 current loss 0.747736, current_train_items 283360.
I0302 19:02:32.745968 22699590365312 run.py:483] Algo bellman_ford step 8855 current loss 0.335130, current_train_items 283392.
I0302 19:02:32.761882 22699590365312 run.py:483] Algo bellman_ford step 8856 current loss 0.417553, current_train_items 283424.
I0302 19:02:32.785525 22699590365312 run.py:483] Algo bellman_ford step 8857 current loss 0.569851, current_train_items 283456.
I0302 19:02:32.817545 22699590365312 run.py:483] Algo bellman_ford step 8858 current loss 0.597525, current_train_items 283488.
I0302 19:02:32.849408 22699590365312 run.py:483] Algo bellman_ford step 8859 current loss 0.661050, current_train_items 283520.
I0302 19:02:32.869243 22699590365312 run.py:483] Algo bellman_ford step 8860 current loss 0.305916, current_train_items 283552.
I0302 19:02:32.885515 22699590365312 run.py:483] Algo bellman_ford step 8861 current loss 0.455407, current_train_items 283584.
I0302 19:02:32.909437 22699590365312 run.py:483] Algo bellman_ford step 8862 current loss 0.653542, current_train_items 283616.
I0302 19:02:32.939980 22699590365312 run.py:483] Algo bellman_ford step 8863 current loss 0.576060, current_train_items 283648.
I0302 19:02:32.972325 22699590365312 run.py:483] Algo bellman_ford step 8864 current loss 0.692733, current_train_items 283680.
I0302 19:02:32.992126 22699590365312 run.py:483] Algo bellman_ford step 8865 current loss 0.296053, current_train_items 283712.
I0302 19:02:33.008056 22699590365312 run.py:483] Algo bellman_ford step 8866 current loss 0.428607, current_train_items 283744.
I0302 19:02:33.031095 22699590365312 run.py:483] Algo bellman_ford step 8867 current loss 0.603841, current_train_items 283776.
I0302 19:02:33.062937 22699590365312 run.py:483] Algo bellman_ford step 8868 current loss 0.770910, current_train_items 283808.
I0302 19:02:33.095484 22699590365312 run.py:483] Algo bellman_ford step 8869 current loss 0.892097, current_train_items 283840.
I0302 19:02:33.115654 22699590365312 run.py:483] Algo bellman_ford step 8870 current loss 0.298637, current_train_items 283872.
I0302 19:02:33.131813 22699590365312 run.py:483] Algo bellman_ford step 8871 current loss 0.388269, current_train_items 283904.
I0302 19:02:33.155827 22699590365312 run.py:483] Algo bellman_ford step 8872 current loss 0.597431, current_train_items 283936.
I0302 19:02:33.186939 22699590365312 run.py:483] Algo bellman_ford step 8873 current loss 0.586850, current_train_items 283968.
I0302 19:02:33.221747 22699590365312 run.py:483] Algo bellman_ford step 8874 current loss 0.834843, current_train_items 284000.
I0302 19:02:33.241780 22699590365312 run.py:483] Algo bellman_ford step 8875 current loss 0.316056, current_train_items 284032.
I0302 19:02:33.257885 22699590365312 run.py:483] Algo bellman_ford step 8876 current loss 0.483133, current_train_items 284064.
I0302 19:02:33.281014 22699590365312 run.py:483] Algo bellman_ford step 8877 current loss 0.617350, current_train_items 284096.
I0302 19:02:33.312194 22699590365312 run.py:483] Algo bellman_ford step 8878 current loss 0.625120, current_train_items 284128.
I0302 19:02:33.346223 22699590365312 run.py:483] Algo bellman_ford step 8879 current loss 0.734150, current_train_items 284160.
I0302 19:02:33.365813 22699590365312 run.py:483] Algo bellman_ford step 8880 current loss 0.311876, current_train_items 284192.
I0302 19:02:33.381738 22699590365312 run.py:483] Algo bellman_ford step 8881 current loss 0.499895, current_train_items 284224.
I0302 19:02:33.404903 22699590365312 run.py:483] Algo bellman_ford step 8882 current loss 0.523162, current_train_items 284256.
I0302 19:02:33.437404 22699590365312 run.py:483] Algo bellman_ford step 8883 current loss 0.713630, current_train_items 284288.
I0302 19:02:33.471214 22699590365312 run.py:483] Algo bellman_ford step 8884 current loss 0.884077, current_train_items 284320.
I0302 19:02:33.491244 22699590365312 run.py:483] Algo bellman_ford step 8885 current loss 0.302732, current_train_items 284352.
I0302 19:02:33.507781 22699590365312 run.py:483] Algo bellman_ford step 8886 current loss 0.385931, current_train_items 284384.
I0302 19:02:33.531358 22699590365312 run.py:483] Algo bellman_ford step 8887 current loss 0.499579, current_train_items 284416.
I0302 19:02:33.562394 22699590365312 run.py:483] Algo bellman_ford step 8888 current loss 0.664548, current_train_items 284448.
I0302 19:02:33.597005 22699590365312 run.py:483] Algo bellman_ford step 8889 current loss 0.755132, current_train_items 284480.
I0302 19:02:33.616631 22699590365312 run.py:483] Algo bellman_ford step 8890 current loss 0.275095, current_train_items 284512.
I0302 19:02:33.632308 22699590365312 run.py:483] Algo bellman_ford step 8891 current loss 0.421180, current_train_items 284544.
I0302 19:02:33.655715 22699590365312 run.py:483] Algo bellman_ford step 8892 current loss 0.509183, current_train_items 284576.
I0302 19:02:33.688689 22699590365312 run.py:483] Algo bellman_ford step 8893 current loss 0.689465, current_train_items 284608.
I0302 19:02:33.722407 22699590365312 run.py:483] Algo bellman_ford step 8894 current loss 0.734812, current_train_items 284640.
I0302 19:02:33.741845 22699590365312 run.py:483] Algo bellman_ford step 8895 current loss 0.284664, current_train_items 284672.
I0302 19:02:33.757750 22699590365312 run.py:483] Algo bellman_ford step 8896 current loss 0.404920, current_train_items 284704.
I0302 19:02:33.780295 22699590365312 run.py:483] Algo bellman_ford step 8897 current loss 0.478212, current_train_items 284736.
I0302 19:02:33.812691 22699590365312 run.py:483] Algo bellman_ford step 8898 current loss 0.548596, current_train_items 284768.
I0302 19:02:33.847665 22699590365312 run.py:483] Algo bellman_ford step 8899 current loss 0.609721, current_train_items 284800.
I0302 19:02:33.867465 22699590365312 run.py:483] Algo bellman_ford step 8900 current loss 0.285492, current_train_items 284832.
I0302 19:02:33.875192 22699590365312 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.958984375, 'score': 0.958984375, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0302 19:02:33.875298 22699590365312 run.py:519] Checkpointing best model, best avg val score was 0.950, current avg val score is 0.959, val scores are: bellman_ford: 0.959
I0302 19:02:33.904948 22699590365312 run.py:483] Algo bellman_ford step 8901 current loss 0.411588, current_train_items 284864.
I0302 19:02:33.929297 22699590365312 run.py:483] Algo bellman_ford step 8902 current loss 0.496771, current_train_items 284896.
I0302 19:02:33.960664 22699590365312 run.py:483] Algo bellman_ford step 8903 current loss 0.571068, current_train_items 284928.
I0302 19:02:33.996292 22699590365312 run.py:483] Algo bellman_ford step 8904 current loss 0.652066, current_train_items 284960.
I0302 19:02:34.016211 22699590365312 run.py:483] Algo bellman_ford step 8905 current loss 0.301266, current_train_items 284992.
I0302 19:02:34.032304 22699590365312 run.py:483] Algo bellman_ford step 8906 current loss 0.374944, current_train_items 285024.
I0302 19:02:34.056270 22699590365312 run.py:483] Algo bellman_ford step 8907 current loss 0.518052, current_train_items 285056.
I0302 19:02:34.087849 22699590365312 run.py:483] Algo bellman_ford step 8908 current loss 0.604657, current_train_items 285088.
I0302 19:02:34.119959 22699590365312 run.py:483] Algo bellman_ford step 8909 current loss 0.651811, current_train_items 285120.
I0302 19:02:34.139889 22699590365312 run.py:483] Algo bellman_ford step 8910 current loss 0.289160, current_train_items 285152.
I0302 19:02:34.155935 22699590365312 run.py:483] Algo bellman_ford step 8911 current loss 0.402560, current_train_items 285184.
I0302 19:02:34.180121 22699590365312 run.py:483] Algo bellman_ford step 8912 current loss 0.534785, current_train_items 285216.
I0302 19:02:34.210579 22699590365312 run.py:483] Algo bellman_ford step 8913 current loss 0.528371, current_train_items 285248.
I0302 19:02:34.245836 22699590365312 run.py:483] Algo bellman_ford step 8914 current loss 0.767017, current_train_items 285280.
I0302 19:02:34.265539 22699590365312 run.py:483] Algo bellman_ford step 8915 current loss 0.303104, current_train_items 285312.
I0302 19:02:34.281277 22699590365312 run.py:483] Algo bellman_ford step 8916 current loss 0.407441, current_train_items 285344.
I0302 19:02:34.305240 22699590365312 run.py:483] Algo bellman_ford step 8917 current loss 0.531838, current_train_items 285376.
I0302 19:02:34.336694 22699590365312 run.py:483] Algo bellman_ford step 8918 current loss 0.567376, current_train_items 285408.
I0302 19:02:34.369926 22699590365312 run.py:483] Algo bellman_ford step 8919 current loss 0.695848, current_train_items 285440.
I0302 19:02:34.389647 22699590365312 run.py:483] Algo bellman_ford step 8920 current loss 0.283173, current_train_items 285472.
I0302 19:02:34.405833 22699590365312 run.py:483] Algo bellman_ford step 8921 current loss 0.466356, current_train_items 285504.
I0302 19:02:34.430444 22699590365312 run.py:483] Algo bellman_ford step 8922 current loss 0.562790, current_train_items 285536.
I0302 19:02:34.461416 22699590365312 run.py:483] Algo bellman_ford step 8923 current loss 0.636240, current_train_items 285568.
I0302 19:02:34.495472 22699590365312 run.py:483] Algo bellman_ford step 8924 current loss 0.867663, current_train_items 285600.
I0302 19:02:34.514964 22699590365312 run.py:483] Algo bellman_ford step 8925 current loss 0.226054, current_train_items 285632.
I0302 19:02:34.530656 22699590365312 run.py:483] Algo bellman_ford step 8926 current loss 0.527005, current_train_items 285664.
I0302 19:02:34.554613 22699590365312 run.py:483] Algo bellman_ford step 8927 current loss 0.589410, current_train_items 285696.
I0302 19:02:34.584717 22699590365312 run.py:483] Algo bellman_ford step 8928 current loss 0.521862, current_train_items 285728.
I0302 19:02:34.618434 22699590365312 run.py:483] Algo bellman_ford step 8929 current loss 0.812159, current_train_items 285760.
I0302 19:02:34.637757 22699590365312 run.py:483] Algo bellman_ford step 8930 current loss 0.304765, current_train_items 285792.
I0302 19:02:34.654273 22699590365312 run.py:483] Algo bellman_ford step 8931 current loss 0.478863, current_train_items 285824.
I0302 19:02:34.677856 22699590365312 run.py:483] Algo bellman_ford step 8932 current loss 0.538452, current_train_items 285856.
I0302 19:02:34.710682 22699590365312 run.py:483] Algo bellman_ford step 8933 current loss 0.708943, current_train_items 285888.
I0302 19:02:34.745004 22699590365312 run.py:483] Algo bellman_ford step 8934 current loss 0.673043, current_train_items 285920.
I0302 19:02:34.764768 22699590365312 run.py:483] Algo bellman_ford step 8935 current loss 0.303024, current_train_items 285952.
I0302 19:02:34.780557 22699590365312 run.py:483] Algo bellman_ford step 8936 current loss 0.438207, current_train_items 285984.
I0302 19:02:34.804287 22699590365312 run.py:483] Algo bellman_ford step 8937 current loss 0.555577, current_train_items 286016.
I0302 19:02:34.835097 22699590365312 run.py:483] Algo bellman_ford step 8938 current loss 0.617744, current_train_items 286048.
I0302 19:02:34.867793 22699590365312 run.py:483] Algo bellman_ford step 8939 current loss 0.594609, current_train_items 286080.
I0302 19:02:34.887497 22699590365312 run.py:483] Algo bellman_ford step 8940 current loss 0.311023, current_train_items 286112.
I0302 19:02:34.903270 22699590365312 run.py:483] Algo bellman_ford step 8941 current loss 0.391110, current_train_items 286144.
I0302 19:02:34.927150 22699590365312 run.py:483] Algo bellman_ford step 8942 current loss 0.575317, current_train_items 286176.
I0302 19:02:34.959548 22699590365312 run.py:483] Algo bellman_ford step 8943 current loss 0.664687, current_train_items 286208.
I0302 19:02:34.993001 22699590365312 run.py:483] Algo bellman_ford step 8944 current loss 0.708076, current_train_items 286240.
I0302 19:02:35.012656 22699590365312 run.py:483] Algo bellman_ford step 8945 current loss 0.336888, current_train_items 286272.
I0302 19:02:35.028686 22699590365312 run.py:483] Algo bellman_ford step 8946 current loss 0.418334, current_train_items 286304.
I0302 19:02:35.051697 22699590365312 run.py:483] Algo bellman_ford step 8947 current loss 0.594736, current_train_items 286336.
I0302 19:02:35.083402 22699590365312 run.py:483] Algo bellman_ford step 8948 current loss 0.644800, current_train_items 286368.
I0302 19:02:35.116487 22699590365312 run.py:483] Algo bellman_ford step 8949 current loss 0.678549, current_train_items 286400.
I0302 19:02:35.135879 22699590365312 run.py:483] Algo bellman_ford step 8950 current loss 0.302454, current_train_items 286432.
I0302 19:02:35.143996 22699590365312 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0302 19:02:35.144100 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:02:35.161059 22699590365312 run.py:483] Algo bellman_ford step 8951 current loss 0.431295, current_train_items 286464.
I0302 19:02:35.184751 22699590365312 run.py:483] Algo bellman_ford step 8952 current loss 0.506397, current_train_items 286496.
I0302 19:02:35.219200 22699590365312 run.py:483] Algo bellman_ford step 8953 current loss 0.651480, current_train_items 286528.
I0302 19:02:35.254750 22699590365312 run.py:483] Algo bellman_ford step 8954 current loss 0.742905, current_train_items 286560.
I0302 19:02:35.274829 22699590365312 run.py:483] Algo bellman_ford step 8955 current loss 0.294075, current_train_items 286592.
I0302 19:02:35.291203 22699590365312 run.py:483] Algo bellman_ford step 8956 current loss 0.502602, current_train_items 286624.
I0302 19:02:35.315287 22699590365312 run.py:483] Algo bellman_ford step 8957 current loss 0.610441, current_train_items 286656.
I0302 19:02:35.346302 22699590365312 run.py:483] Algo bellman_ford step 8958 current loss 0.678637, current_train_items 286688.
I0302 19:02:35.380920 22699590365312 run.py:483] Algo bellman_ford step 8959 current loss 0.834190, current_train_items 286720.
I0302 19:02:35.400956 22699590365312 run.py:483] Algo bellman_ford step 8960 current loss 0.249286, current_train_items 286752.
I0302 19:02:35.416515 22699590365312 run.py:483] Algo bellman_ford step 8961 current loss 0.406542, current_train_items 286784.
I0302 19:02:35.440894 22699590365312 run.py:483] Algo bellman_ford step 8962 current loss 0.537687, current_train_items 286816.
I0302 19:02:35.472818 22699590365312 run.py:483] Algo bellman_ford step 8963 current loss 0.557000, current_train_items 286848.
I0302 19:02:35.504961 22699590365312 run.py:483] Algo bellman_ford step 8964 current loss 0.714883, current_train_items 286880.
I0302 19:02:35.524685 22699590365312 run.py:483] Algo bellman_ford step 8965 current loss 0.273906, current_train_items 286912.
I0302 19:02:35.540786 22699590365312 run.py:483] Algo bellman_ford step 8966 current loss 0.453273, current_train_items 286944.
I0302 19:02:35.564800 22699590365312 run.py:483] Algo bellman_ford step 8967 current loss 0.592701, current_train_items 286976.
I0302 19:02:35.596395 22699590365312 run.py:483] Algo bellman_ford step 8968 current loss 0.612038, current_train_items 287008.
I0302 19:02:35.629842 22699590365312 run.py:483] Algo bellman_ford step 8969 current loss 0.677970, current_train_items 287040.
I0302 19:02:35.649339 22699590365312 run.py:483] Algo bellman_ford step 8970 current loss 0.294229, current_train_items 287072.
I0302 19:02:35.665258 22699590365312 run.py:483] Algo bellman_ford step 8971 current loss 0.361080, current_train_items 287104.
I0302 19:02:35.688656 22699590365312 run.py:483] Algo bellman_ford step 8972 current loss 0.547112, current_train_items 287136.
I0302 19:02:35.719698 22699590365312 run.py:483] Algo bellman_ford step 8973 current loss 0.545181, current_train_items 287168.
I0302 19:02:35.752655 22699590365312 run.py:483] Algo bellman_ford step 8974 current loss 0.673862, current_train_items 287200.
I0302 19:02:35.772551 22699590365312 run.py:483] Algo bellman_ford step 8975 current loss 0.337053, current_train_items 287232.
I0302 19:02:35.788286 22699590365312 run.py:483] Algo bellman_ford step 8976 current loss 0.388447, current_train_items 287264.
I0302 19:02:35.810673 22699590365312 run.py:483] Algo bellman_ford step 8977 current loss 0.496576, current_train_items 287296.
I0302 19:02:35.842686 22699590365312 run.py:483] Algo bellman_ford step 8978 current loss 0.672487, current_train_items 287328.
I0302 19:02:35.875741 22699590365312 run.py:483] Algo bellman_ford step 8979 current loss 0.811238, current_train_items 287360.
I0302 19:02:35.895350 22699590365312 run.py:483] Algo bellman_ford step 8980 current loss 0.285234, current_train_items 287392.
I0302 19:02:35.911508 22699590365312 run.py:483] Algo bellman_ford step 8981 current loss 0.470284, current_train_items 287424.
I0302 19:02:35.935551 22699590365312 run.py:483] Algo bellman_ford step 8982 current loss 0.562832, current_train_items 287456.
I0302 19:02:35.967165 22699590365312 run.py:483] Algo bellman_ford step 8983 current loss 0.603687, current_train_items 287488.
I0302 19:02:36.000624 22699590365312 run.py:483] Algo bellman_ford step 8984 current loss 0.675285, current_train_items 287520.
I0302 19:02:36.020326 22699590365312 run.py:483] Algo bellman_ford step 8985 current loss 0.271687, current_train_items 287552.
I0302 19:02:36.036278 22699590365312 run.py:483] Algo bellman_ford step 8986 current loss 0.406112, current_train_items 287584.
I0302 19:02:36.058995 22699590365312 run.py:483] Algo bellman_ford step 8987 current loss 0.505479, current_train_items 287616.
I0302 19:02:36.088866 22699590365312 run.py:483] Algo bellman_ford step 8988 current loss 0.701597, current_train_items 287648.
I0302 19:02:36.124065 22699590365312 run.py:483] Algo bellman_ford step 8989 current loss 0.793180, current_train_items 287680.
I0302 19:02:36.143908 22699590365312 run.py:483] Algo bellman_ford step 8990 current loss 0.276828, current_train_items 287712.
I0302 19:02:36.159565 22699590365312 run.py:483] Algo bellman_ford step 8991 current loss 0.415355, current_train_items 287744.
I0302 19:02:36.182621 22699590365312 run.py:483] Algo bellman_ford step 8992 current loss 0.570165, current_train_items 287776.
I0302 19:02:36.213896 22699590365312 run.py:483] Algo bellman_ford step 8993 current loss 0.667922, current_train_items 287808.
I0302 19:02:36.247032 22699590365312 run.py:483] Algo bellman_ford step 8994 current loss 0.743751, current_train_items 287840.
I0302 19:02:36.266086 22699590365312 run.py:483] Algo bellman_ford step 8995 current loss 0.284173, current_train_items 287872.
I0302 19:02:36.282090 22699590365312 run.py:483] Algo bellman_ford step 8996 current loss 0.375884, current_train_items 287904.
I0302 19:02:36.306096 22699590365312 run.py:483] Algo bellman_ford step 8997 current loss 0.583992, current_train_items 287936.
I0302 19:02:36.338884 22699590365312 run.py:483] Algo bellman_ford step 8998 current loss 0.640977, current_train_items 287968.
I0302 19:02:36.371189 22699590365312 run.py:483] Algo bellman_ford step 8999 current loss 0.671570, current_train_items 288000.
I0302 19:02:36.391128 22699590365312 run.py:483] Algo bellman_ford step 9000 current loss 0.264293, current_train_items 288032.
I0302 19:02:36.398864 22699590365312 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0302 19:02:36.398970 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:02:36.416048 22699590365312 run.py:483] Algo bellman_ford step 9001 current loss 0.430484, current_train_items 288064.
I0302 19:02:36.439976 22699590365312 run.py:483] Algo bellman_ford step 9002 current loss 0.561879, current_train_items 288096.
I0302 19:02:36.471356 22699590365312 run.py:483] Algo bellman_ford step 9003 current loss 0.636269, current_train_items 288128.
I0302 19:02:36.505569 22699590365312 run.py:483] Algo bellman_ford step 9004 current loss 0.774453, current_train_items 288160.
I0302 19:02:36.525970 22699590365312 run.py:483] Algo bellman_ford step 9005 current loss 0.316546, current_train_items 288192.
I0302 19:02:36.542109 22699590365312 run.py:483] Algo bellman_ford step 9006 current loss 0.382548, current_train_items 288224.
I0302 19:02:36.565320 22699590365312 run.py:483] Algo bellman_ford step 9007 current loss 0.540613, current_train_items 288256.
I0302 19:02:36.597913 22699590365312 run.py:483] Algo bellman_ford step 9008 current loss 0.703924, current_train_items 288288.
I0302 19:02:36.631632 22699590365312 run.py:483] Algo bellman_ford step 9009 current loss 0.669432, current_train_items 288320.
I0302 19:02:36.651296 22699590365312 run.py:483] Algo bellman_ford step 9010 current loss 0.295784, current_train_items 288352.
I0302 19:02:36.667405 22699590365312 run.py:483] Algo bellman_ford step 9011 current loss 0.457275, current_train_items 288384.
I0302 19:02:36.690623 22699590365312 run.py:483] Algo bellman_ford step 9012 current loss 0.638121, current_train_items 288416.
I0302 19:02:36.721460 22699590365312 run.py:483] Algo bellman_ford step 9013 current loss 0.584858, current_train_items 288448.
I0302 19:02:36.757347 22699590365312 run.py:483] Algo bellman_ford step 9014 current loss 0.875617, current_train_items 288480.
I0302 19:02:36.777035 22699590365312 run.py:483] Algo bellman_ford step 9015 current loss 0.258741, current_train_items 288512.
I0302 19:02:36.793284 22699590365312 run.py:483] Algo bellman_ford step 9016 current loss 0.495139, current_train_items 288544.
I0302 19:02:36.817827 22699590365312 run.py:483] Algo bellman_ford step 9017 current loss 0.586611, current_train_items 288576.
I0302 19:02:36.849693 22699590365312 run.py:483] Algo bellman_ford step 9018 current loss 0.615475, current_train_items 288608.
I0302 19:02:36.885010 22699590365312 run.py:483] Algo bellman_ford step 9019 current loss 0.658950, current_train_items 288640.
I0302 19:02:36.904411 22699590365312 run.py:483] Algo bellman_ford step 9020 current loss 0.338813, current_train_items 288672.
I0302 19:02:36.920402 22699590365312 run.py:483] Algo bellman_ford step 9021 current loss 0.430600, current_train_items 288704.
I0302 19:02:36.945543 22699590365312 run.py:483] Algo bellman_ford step 9022 current loss 0.647432, current_train_items 288736.
I0302 19:02:36.977707 22699590365312 run.py:483] Algo bellman_ford step 9023 current loss 0.675209, current_train_items 288768.
I0302 19:02:37.011440 22699590365312 run.py:483] Algo bellman_ford step 9024 current loss 0.859513, current_train_items 288800.
I0302 19:02:37.030817 22699590365312 run.py:483] Algo bellman_ford step 9025 current loss 0.321746, current_train_items 288832.
I0302 19:02:37.046608 22699590365312 run.py:483] Algo bellman_ford step 9026 current loss 0.336808, current_train_items 288864.
I0302 19:02:37.070530 22699590365312 run.py:483] Algo bellman_ford step 9027 current loss 0.504885, current_train_items 288896.
I0302 19:02:37.102510 22699590365312 run.py:483] Algo bellman_ford step 9028 current loss 0.588594, current_train_items 288928.
I0302 19:02:37.136081 22699590365312 run.py:483] Algo bellman_ford step 9029 current loss 0.721922, current_train_items 288960.
I0302 19:02:37.156113 22699590365312 run.py:483] Algo bellman_ford step 9030 current loss 0.276121, current_train_items 288992.
I0302 19:02:37.172205 22699590365312 run.py:483] Algo bellman_ford step 9031 current loss 0.372358, current_train_items 289024.
I0302 19:02:37.196783 22699590365312 run.py:483] Algo bellman_ford step 9032 current loss 0.702986, current_train_items 289056.
I0302 19:02:37.228856 22699590365312 run.py:483] Algo bellman_ford step 9033 current loss 0.679835, current_train_items 289088.
I0302 19:02:37.264538 22699590365312 run.py:483] Algo bellman_ford step 9034 current loss 0.976938, current_train_items 289120.
I0302 19:02:37.284071 22699590365312 run.py:483] Algo bellman_ford step 9035 current loss 0.322554, current_train_items 289152.
I0302 19:02:37.299773 22699590365312 run.py:483] Algo bellman_ford step 9036 current loss 0.602044, current_train_items 289184.
I0302 19:02:37.324342 22699590365312 run.py:483] Algo bellman_ford step 9037 current loss 0.596549, current_train_items 289216.
I0302 19:02:37.355553 22699590365312 run.py:483] Algo bellman_ford step 9038 current loss 0.623525, current_train_items 289248.
I0302 19:02:37.389417 22699590365312 run.py:483] Algo bellman_ford step 9039 current loss 0.802105, current_train_items 289280.
I0302 19:02:37.409151 22699590365312 run.py:483] Algo bellman_ford step 9040 current loss 0.247906, current_train_items 289312.
I0302 19:02:37.425376 22699590365312 run.py:483] Algo bellman_ford step 9041 current loss 0.427816, current_train_items 289344.
I0302 19:02:37.448744 22699590365312 run.py:483] Algo bellman_ford step 9042 current loss 0.510300, current_train_items 289376.
I0302 19:02:37.479935 22699590365312 run.py:483] Algo bellman_ford step 9043 current loss 0.703494, current_train_items 289408.
I0302 19:02:37.513976 22699590365312 run.py:483] Algo bellman_ford step 9044 current loss 0.794897, current_train_items 289440.
I0302 19:02:37.533820 22699590365312 run.py:483] Algo bellman_ford step 9045 current loss 0.291508, current_train_items 289472.
I0302 19:02:37.550189 22699590365312 run.py:483] Algo bellman_ford step 9046 current loss 0.401355, current_train_items 289504.
I0302 19:02:37.572810 22699590365312 run.py:483] Algo bellman_ford step 9047 current loss 0.596942, current_train_items 289536.
I0302 19:02:37.604760 22699590365312 run.py:483] Algo bellman_ford step 9048 current loss 0.655559, current_train_items 289568.
I0302 19:02:37.638305 22699590365312 run.py:483] Algo bellman_ford step 9049 current loss 0.743737, current_train_items 289600.
I0302 19:02:37.657893 22699590365312 run.py:483] Algo bellman_ford step 9050 current loss 0.256848, current_train_items 289632.
I0302 19:02:37.665857 22699590365312 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0302 19:02:37.665961 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:02:37.682451 22699590365312 run.py:483] Algo bellman_ford step 9051 current loss 0.382283, current_train_items 289664.
I0302 19:02:37.707317 22699590365312 run.py:483] Algo bellman_ford step 9052 current loss 0.573968, current_train_items 289696.
I0302 19:02:37.739809 22699590365312 run.py:483] Algo bellman_ford step 9053 current loss 0.644650, current_train_items 289728.
I0302 19:02:37.773009 22699590365312 run.py:483] Algo bellman_ford step 9054 current loss 0.793671, current_train_items 289760.
I0302 19:02:37.793273 22699590365312 run.py:483] Algo bellman_ford step 9055 current loss 0.284778, current_train_items 289792.
I0302 19:02:37.808935 22699590365312 run.py:483] Algo bellman_ford step 9056 current loss 0.384004, current_train_items 289824.
I0302 19:02:37.832177 22699590365312 run.py:483] Algo bellman_ford step 9057 current loss 0.587101, current_train_items 289856.
I0302 19:02:37.862235 22699590365312 run.py:483] Algo bellman_ford step 9058 current loss 0.589354, current_train_items 289888.
I0302 19:02:37.894052 22699590365312 run.py:483] Algo bellman_ford step 9059 current loss 0.605629, current_train_items 289920.
I0302 19:02:37.913640 22699590365312 run.py:483] Algo bellman_ford step 9060 current loss 0.309199, current_train_items 289952.
I0302 19:02:37.930214 22699590365312 run.py:483] Algo bellman_ford step 9061 current loss 0.466502, current_train_items 289984.
I0302 19:02:37.954985 22699590365312 run.py:483] Algo bellman_ford step 9062 current loss 0.633148, current_train_items 290016.
I0302 19:02:37.985193 22699590365312 run.py:483] Algo bellman_ford step 9063 current loss 0.557494, current_train_items 290048.
I0302 19:02:38.019698 22699590365312 run.py:483] Algo bellman_ford step 9064 current loss 0.645683, current_train_items 290080.
I0302 19:02:38.039664 22699590365312 run.py:483] Algo bellman_ford step 9065 current loss 0.302536, current_train_items 290112.
I0302 19:02:38.055739 22699590365312 run.py:483] Algo bellman_ford step 9066 current loss 0.490324, current_train_items 290144.
I0302 19:02:38.077788 22699590365312 run.py:483] Algo bellman_ford step 9067 current loss 0.493302, current_train_items 290176.
I0302 19:02:38.109420 22699590365312 run.py:483] Algo bellman_ford step 9068 current loss 0.536824, current_train_items 290208.
I0302 19:02:38.143821 22699590365312 run.py:483] Algo bellman_ford step 9069 current loss 0.696348, current_train_items 290240.
I0302 19:02:38.163485 22699590365312 run.py:483] Algo bellman_ford step 9070 current loss 0.255896, current_train_items 290272.
I0302 19:02:38.179719 22699590365312 run.py:483] Algo bellman_ford step 9071 current loss 0.467970, current_train_items 290304.
I0302 19:02:38.203858 22699590365312 run.py:483] Algo bellman_ford step 9072 current loss 0.512302, current_train_items 290336.
I0302 19:02:38.235686 22699590365312 run.py:483] Algo bellman_ford step 9073 current loss 0.577326, current_train_items 290368.
I0302 19:02:38.269059 22699590365312 run.py:483] Algo bellman_ford step 9074 current loss 0.596003, current_train_items 290400.
I0302 19:02:38.288478 22699590365312 run.py:483] Algo bellman_ford step 9075 current loss 0.221121, current_train_items 290432.
I0302 19:02:38.304566 22699590365312 run.py:483] Algo bellman_ford step 9076 current loss 0.436001, current_train_items 290464.
I0302 19:02:38.328452 22699590365312 run.py:483] Algo bellman_ford step 9077 current loss 0.500200, current_train_items 290496.
I0302 19:02:38.359870 22699590365312 run.py:483] Algo bellman_ford step 9078 current loss 0.521295, current_train_items 290528.
I0302 19:02:38.394085 22699590365312 run.py:483] Algo bellman_ford step 9079 current loss 0.794120, current_train_items 290560.
I0302 19:02:38.413581 22699590365312 run.py:483] Algo bellman_ford step 9080 current loss 0.267697, current_train_items 290592.
I0302 19:02:38.429484 22699590365312 run.py:483] Algo bellman_ford step 9081 current loss 0.387532, current_train_items 290624.
I0302 19:02:38.452712 22699590365312 run.py:483] Algo bellman_ford step 9082 current loss 0.468913, current_train_items 290656.
I0302 19:02:38.483625 22699590365312 run.py:483] Algo bellman_ford step 9083 current loss 0.580505, current_train_items 290688.
I0302 19:02:38.516612 22699590365312 run.py:483] Algo bellman_ford step 9084 current loss 0.640899, current_train_items 290720.
I0302 19:02:38.536617 22699590365312 run.py:483] Algo bellman_ford step 9085 current loss 0.291682, current_train_items 290752.
I0302 19:02:38.552658 22699590365312 run.py:483] Algo bellman_ford step 9086 current loss 0.402127, current_train_items 290784.
I0302 19:02:38.576599 22699590365312 run.py:483] Algo bellman_ford step 9087 current loss 0.532009, current_train_items 290816.
I0302 19:02:38.607195 22699590365312 run.py:483] Algo bellman_ford step 9088 current loss 0.474355, current_train_items 290848.
I0302 19:02:38.642342 22699590365312 run.py:483] Algo bellman_ford step 9089 current loss 0.794406, current_train_items 290880.
I0302 19:02:38.662042 22699590365312 run.py:483] Algo bellman_ford step 9090 current loss 0.212366, current_train_items 290912.
I0302 19:02:38.677998 22699590365312 run.py:483] Algo bellman_ford step 9091 current loss 0.488053, current_train_items 290944.
I0302 19:02:38.700941 22699590365312 run.py:483] Algo bellman_ford step 9092 current loss 0.516379, current_train_items 290976.
I0302 19:02:38.730371 22699590365312 run.py:483] Algo bellman_ford step 9093 current loss 0.545676, current_train_items 291008.
I0302 19:02:38.766031 22699590365312 run.py:483] Algo bellman_ford step 9094 current loss 0.796445, current_train_items 291040.
I0302 19:02:38.785310 22699590365312 run.py:483] Algo bellman_ford step 9095 current loss 0.378353, current_train_items 291072.
I0302 19:02:38.801175 22699590365312 run.py:483] Algo bellman_ford step 9096 current loss 0.449065, current_train_items 291104.
I0302 19:02:38.824614 22699590365312 run.py:483] Algo bellman_ford step 9097 current loss 0.566101, current_train_items 291136.
I0302 19:02:38.856432 22699590365312 run.py:483] Algo bellman_ford step 9098 current loss 0.619604, current_train_items 291168.
I0302 19:02:38.889391 22699590365312 run.py:483] Algo bellman_ford step 9099 current loss 0.633541, current_train_items 291200.
I0302 19:02:38.909270 22699590365312 run.py:483] Algo bellman_ford step 9100 current loss 0.242836, current_train_items 291232.
I0302 19:02:38.917008 22699590365312 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0302 19:02:38.917115 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:02:38.933793 22699590365312 run.py:483] Algo bellman_ford step 9101 current loss 0.369321, current_train_items 291264.
I0302 19:02:38.958843 22699590365312 run.py:483] Algo bellman_ford step 9102 current loss 0.565378, current_train_items 291296.
I0302 19:02:38.990720 22699590365312 run.py:483] Algo bellman_ford step 9103 current loss 0.606169, current_train_items 291328.
I0302 19:02:39.026175 22699590365312 run.py:483] Algo bellman_ford step 9104 current loss 0.725918, current_train_items 291360.
I0302 19:02:39.046444 22699590365312 run.py:483] Algo bellman_ford step 9105 current loss 0.255732, current_train_items 291392.
I0302 19:02:39.062607 22699590365312 run.py:483] Algo bellman_ford step 9106 current loss 0.496023, current_train_items 291424.
I0302 19:02:39.086959 22699590365312 run.py:483] Algo bellman_ford step 9107 current loss 0.551075, current_train_items 291456.
I0302 19:02:39.119144 22699590365312 run.py:483] Algo bellman_ford step 9108 current loss 0.608472, current_train_items 291488.
I0302 19:02:39.153273 22699590365312 run.py:483] Algo bellman_ford step 9109 current loss 0.634608, current_train_items 291520.
I0302 19:02:39.172908 22699590365312 run.py:483] Algo bellman_ford step 9110 current loss 0.293601, current_train_items 291552.
I0302 19:02:39.188847 22699590365312 run.py:483] Algo bellman_ford step 9111 current loss 0.394510, current_train_items 291584.
I0302 19:02:39.212366 22699590365312 run.py:483] Algo bellman_ford step 9112 current loss 0.688907, current_train_items 291616.
I0302 19:02:39.245357 22699590365312 run.py:483] Algo bellman_ford step 9113 current loss 0.586388, current_train_items 291648.
I0302 19:02:39.280342 22699590365312 run.py:483] Algo bellman_ford step 9114 current loss 0.636304, current_train_items 291680.
I0302 19:02:39.299962 22699590365312 run.py:483] Algo bellman_ford step 9115 current loss 0.321064, current_train_items 291712.
I0302 19:02:39.316209 22699590365312 run.py:483] Algo bellman_ford step 9116 current loss 0.377105, current_train_items 291744.
I0302 19:02:39.339884 22699590365312 run.py:483] Algo bellman_ford step 9117 current loss 0.593206, current_train_items 291776.
I0302 19:02:39.370809 22699590365312 run.py:483] Algo bellman_ford step 9118 current loss 0.732344, current_train_items 291808.
I0302 19:02:39.405993 22699590365312 run.py:483] Algo bellman_ford step 9119 current loss 1.043581, current_train_items 291840.
I0302 19:02:39.425349 22699590365312 run.py:483] Algo bellman_ford step 9120 current loss 0.243937, current_train_items 291872.
I0302 19:02:39.441227 22699590365312 run.py:483] Algo bellman_ford step 9121 current loss 0.418191, current_train_items 291904.
I0302 19:02:39.464938 22699590365312 run.py:483] Algo bellman_ford step 9122 current loss 0.589787, current_train_items 291936.
I0302 19:02:39.497872 22699590365312 run.py:483] Algo bellman_ford step 9123 current loss 0.781968, current_train_items 291968.
I0302 19:02:39.533665 22699590365312 run.py:483] Algo bellman_ford step 9124 current loss 0.738504, current_train_items 292000.
I0302 19:02:39.553406 22699590365312 run.py:483] Algo bellman_ford step 9125 current loss 0.248078, current_train_items 292032.
I0302 19:02:39.569059 22699590365312 run.py:483] Algo bellman_ford step 9126 current loss 0.430381, current_train_items 292064.
I0302 19:02:39.591722 22699590365312 run.py:483] Algo bellman_ford step 9127 current loss 0.553443, current_train_items 292096.
I0302 19:02:39.624042 22699590365312 run.py:483] Algo bellman_ford step 9128 current loss 0.700314, current_train_items 292128.
I0302 19:02:39.657235 22699590365312 run.py:483] Algo bellman_ford step 9129 current loss 0.704614, current_train_items 292160.
I0302 19:02:39.676550 22699590365312 run.py:483] Algo bellman_ford step 9130 current loss 0.261976, current_train_items 292192.
I0302 19:02:39.692546 22699590365312 run.py:483] Algo bellman_ford step 9131 current loss 0.480968, current_train_items 292224.
I0302 19:02:39.716797 22699590365312 run.py:483] Algo bellman_ford step 9132 current loss 0.589648, current_train_items 292256.
I0302 19:02:39.748638 22699590365312 run.py:483] Algo bellman_ford step 9133 current loss 0.674318, current_train_items 292288.
I0302 19:02:39.781027 22699590365312 run.py:483] Algo bellman_ford step 9134 current loss 0.817015, current_train_items 292320.
I0302 19:02:39.800459 22699590365312 run.py:483] Algo bellman_ford step 9135 current loss 0.284177, current_train_items 292352.
I0302 19:02:39.816170 22699590365312 run.py:483] Algo bellman_ford step 9136 current loss 0.439157, current_train_items 292384.
I0302 19:02:39.839134 22699590365312 run.py:483] Algo bellman_ford step 9137 current loss 0.480068, current_train_items 292416.
I0302 19:02:39.870148 22699590365312 run.py:483] Algo bellman_ford step 9138 current loss 0.635166, current_train_items 292448.
I0302 19:02:39.902392 22699590365312 run.py:483] Algo bellman_ford step 9139 current loss 0.717186, current_train_items 292480.
I0302 19:02:39.922176 22699590365312 run.py:483] Algo bellman_ford step 9140 current loss 0.285849, current_train_items 292512.
I0302 19:02:39.938077 22699590365312 run.py:483] Algo bellman_ford step 9141 current loss 0.470121, current_train_items 292544.
I0302 19:02:39.961177 22699590365312 run.py:483] Algo bellman_ford step 9142 current loss 0.561884, current_train_items 292576.
I0302 19:02:39.991986 22699590365312 run.py:483] Algo bellman_ford step 9143 current loss 0.707271, current_train_items 292608.
I0302 19:02:40.026463 22699590365312 run.py:483] Algo bellman_ford step 9144 current loss 0.671254, current_train_items 292640.
I0302 19:02:40.046051 22699590365312 run.py:483] Algo bellman_ford step 9145 current loss 0.282569, current_train_items 292672.
I0302 19:02:40.062085 22699590365312 run.py:483] Algo bellman_ford step 9146 current loss 0.389722, current_train_items 292704.
I0302 19:02:40.085859 22699590365312 run.py:483] Algo bellman_ford step 9147 current loss 0.570933, current_train_items 292736.
I0302 19:02:40.117494 22699590365312 run.py:483] Algo bellman_ford step 9148 current loss 0.700897, current_train_items 292768.
I0302 19:02:40.147553 22699590365312 run.py:483] Algo bellman_ford step 9149 current loss 0.636280, current_train_items 292800.
I0302 19:02:40.166698 22699590365312 run.py:483] Algo bellman_ford step 9150 current loss 0.284997, current_train_items 292832.
I0302 19:02:40.174586 22699590365312 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0302 19:02:40.174691 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:02:40.191491 22699590365312 run.py:483] Algo bellman_ford step 9151 current loss 0.471780, current_train_items 292864.
I0302 19:02:40.215578 22699590365312 run.py:483] Algo bellman_ford step 9152 current loss 0.565184, current_train_items 292896.
I0302 19:02:40.248102 22699590365312 run.py:483] Algo bellman_ford step 9153 current loss 0.683327, current_train_items 292928.
I0302 19:02:40.279695 22699590365312 run.py:483] Algo bellman_ford step 9154 current loss 0.743356, current_train_items 292960.
I0302 19:02:40.299640 22699590365312 run.py:483] Algo bellman_ford step 9155 current loss 0.246048, current_train_items 292992.
I0302 19:02:40.315965 22699590365312 run.py:483] Algo bellman_ford step 9156 current loss 0.484034, current_train_items 293024.
I0302 19:02:40.340597 22699590365312 run.py:483] Algo bellman_ford step 9157 current loss 0.620313, current_train_items 293056.
I0302 19:02:40.371662 22699590365312 run.py:483] Algo bellman_ford step 9158 current loss 0.612950, current_train_items 293088.
I0302 19:02:40.406364 22699590365312 run.py:483] Algo bellman_ford step 9159 current loss 0.709785, current_train_items 293120.
I0302 19:02:40.426189 22699590365312 run.py:483] Algo bellman_ford step 9160 current loss 0.301114, current_train_items 293152.
I0302 19:02:40.442337 22699590365312 run.py:483] Algo bellman_ford step 9161 current loss 0.443827, current_train_items 293184.
I0302 19:02:40.465559 22699590365312 run.py:483] Algo bellman_ford step 9162 current loss 0.591690, current_train_items 293216.
I0302 19:02:40.496715 22699590365312 run.py:483] Algo bellman_ford step 9163 current loss 0.544750, current_train_items 293248.
I0302 19:02:40.531983 22699590365312 run.py:483] Algo bellman_ford step 9164 current loss 0.686751, current_train_items 293280.
I0302 19:02:40.551470 22699590365312 run.py:483] Algo bellman_ford step 9165 current loss 0.323409, current_train_items 293312.
I0302 19:02:40.567612 22699590365312 run.py:483] Algo bellman_ford step 9166 current loss 0.445598, current_train_items 293344.
I0302 19:02:40.591372 22699590365312 run.py:483] Algo bellman_ford step 9167 current loss 0.524081, current_train_items 293376.
I0302 19:02:40.623041 22699590365312 run.py:483] Algo bellman_ford step 9168 current loss 0.587088, current_train_items 293408.
I0302 19:02:40.656064 22699590365312 run.py:483] Algo bellman_ford step 9169 current loss 0.639903, current_train_items 293440.
I0302 19:02:40.675532 22699590365312 run.py:483] Algo bellman_ford step 9170 current loss 0.252056, current_train_items 293472.
I0302 19:02:40.691377 22699590365312 run.py:483] Algo bellman_ford step 9171 current loss 0.444377, current_train_items 293504.
I0302 19:02:40.716532 22699590365312 run.py:483] Algo bellman_ford step 9172 current loss 0.654018, current_train_items 293536.
I0302 19:02:40.748055 22699590365312 run.py:483] Algo bellman_ford step 9173 current loss 0.565294, current_train_items 293568.
I0302 19:02:40.782738 22699590365312 run.py:483] Algo bellman_ford step 9174 current loss 0.679695, current_train_items 293600.
I0302 19:02:40.802360 22699590365312 run.py:483] Algo bellman_ford step 9175 current loss 0.258535, current_train_items 293632.
I0302 19:02:40.818581 22699590365312 run.py:483] Algo bellman_ford step 9176 current loss 0.313131, current_train_items 293664.
I0302 19:02:40.841199 22699590365312 run.py:483] Algo bellman_ford step 9177 current loss 0.495832, current_train_items 293696.
I0302 19:02:40.871383 22699590365312 run.py:483] Algo bellman_ford step 9178 current loss 0.559983, current_train_items 293728.
I0302 19:02:40.906462 22699590365312 run.py:483] Algo bellman_ford step 9179 current loss 0.730062, current_train_items 293760.
I0302 19:02:40.925781 22699590365312 run.py:483] Algo bellman_ford step 9180 current loss 0.335941, current_train_items 293792.
I0302 19:02:40.941796 22699590365312 run.py:483] Algo bellman_ford step 9181 current loss 0.517335, current_train_items 293824.
I0302 19:02:40.966204 22699590365312 run.py:483] Algo bellman_ford step 9182 current loss 0.577752, current_train_items 293856.
I0302 19:02:40.996532 22699590365312 run.py:483] Algo bellman_ford step 9183 current loss 0.567959, current_train_items 293888.
I0302 19:02:41.029643 22699590365312 run.py:483] Algo bellman_ford step 9184 current loss 0.708731, current_train_items 293920.
I0302 19:02:41.049434 22699590365312 run.py:483] Algo bellman_ford step 9185 current loss 0.271768, current_train_items 293952.
I0302 19:02:41.065675 22699590365312 run.py:483] Algo bellman_ford step 9186 current loss 0.421043, current_train_items 293984.
I0302 19:02:41.090018 22699590365312 run.py:483] Algo bellman_ford step 9187 current loss 0.503217, current_train_items 294016.
I0302 19:02:41.122756 22699590365312 run.py:483] Algo bellman_ford step 9188 current loss 0.695557, current_train_items 294048.
I0302 19:02:41.158443 22699590365312 run.py:483] Algo bellman_ford step 9189 current loss 0.822079, current_train_items 294080.
I0302 19:02:41.178031 22699590365312 run.py:483] Algo bellman_ford step 9190 current loss 0.407840, current_train_items 294112.
I0302 19:02:41.194098 22699590365312 run.py:483] Algo bellman_ford step 9191 current loss 0.607017, current_train_items 294144.
I0302 19:02:41.218104 22699590365312 run.py:483] Algo bellman_ford step 9192 current loss 0.519299, current_train_items 294176.
I0302 19:02:41.249828 22699590365312 run.py:483] Algo bellman_ford step 9193 current loss 0.627558, current_train_items 294208.
I0302 19:02:41.282305 22699590365312 run.py:483] Algo bellman_ford step 9194 current loss 0.627372, current_train_items 294240.
I0302 19:02:41.301976 22699590365312 run.py:483] Algo bellman_ford step 9195 current loss 0.279173, current_train_items 294272.
I0302 19:02:41.318449 22699590365312 run.py:483] Algo bellman_ford step 9196 current loss 0.399342, current_train_items 294304.
I0302 19:02:41.342427 22699590365312 run.py:483] Algo bellman_ford step 9197 current loss 0.578382, current_train_items 294336.
I0302 19:02:41.373176 22699590365312 run.py:483] Algo bellman_ford step 9198 current loss 0.555497, current_train_items 294368.
I0302 19:02:41.405671 22699590365312 run.py:483] Algo bellman_ford step 9199 current loss 0.680263, current_train_items 294400.
I0302 19:02:41.425552 22699590365312 run.py:483] Algo bellman_ford step 9200 current loss 0.271934, current_train_items 294432.
I0302 19:02:41.433298 22699590365312 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0302 19:02:41.433404 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 19:02:41.450128 22699590365312 run.py:483] Algo bellman_ford step 9201 current loss 0.437017, current_train_items 294464.
I0302 19:02:41.474484 22699590365312 run.py:483] Algo bellman_ford step 9202 current loss 0.454920, current_train_items 294496.
I0302 19:02:41.507431 22699590365312 run.py:483] Algo bellman_ford step 9203 current loss 0.658386, current_train_items 294528.
I0302 19:02:41.542419 22699590365312 run.py:483] Algo bellman_ford step 9204 current loss 0.703698, current_train_items 294560.
I0302 19:02:41.562345 22699590365312 run.py:483] Algo bellman_ford step 9205 current loss 0.227239, current_train_items 294592.
I0302 19:02:41.578312 22699590365312 run.py:483] Algo bellman_ford step 9206 current loss 0.411060, current_train_items 294624.
I0302 19:02:41.602314 22699590365312 run.py:483] Algo bellman_ford step 9207 current loss 0.460044, current_train_items 294656.
I0302 19:02:41.633712 22699590365312 run.py:483] Algo bellman_ford step 9208 current loss 0.557261, current_train_items 294688.
I0302 19:02:41.665325 22699590365312 run.py:483] Algo bellman_ford step 9209 current loss 0.698872, current_train_items 294720.
I0302 19:02:41.684794 22699590365312 run.py:483] Algo bellman_ford step 9210 current loss 0.280366, current_train_items 294752.
I0302 19:02:41.701312 22699590365312 run.py:483] Algo bellman_ford step 9211 current loss 0.444975, current_train_items 294784.
I0302 19:02:41.724631 22699590365312 run.py:483] Algo bellman_ford step 9212 current loss 0.554257, current_train_items 294816.
I0302 19:02:41.755943 22699590365312 run.py:483] Algo bellman_ford step 9213 current loss 0.609437, current_train_items 294848.
I0302 19:02:41.788491 22699590365312 run.py:483] Algo bellman_ford step 9214 current loss 0.647722, current_train_items 294880.
I0302 19:02:41.807665 22699590365312 run.py:483] Algo bellman_ford step 9215 current loss 0.350642, current_train_items 294912.
I0302 19:02:41.823846 22699590365312 run.py:483] Algo bellman_ford step 9216 current loss 0.458556, current_train_items 294944.
I0302 19:02:41.847350 22699590365312 run.py:483] Algo bellman_ford step 9217 current loss 0.527813, current_train_items 294976.
I0302 19:02:41.878611 22699590365312 run.py:483] Algo bellman_ford step 9218 current loss 0.550511, current_train_items 295008.
I0302 19:02:41.912231 22699590365312 run.py:483] Algo bellman_ford step 9219 current loss 0.669684, current_train_items 295040.
I0302 19:02:41.931950 22699590365312 run.py:483] Algo bellman_ford step 9220 current loss 0.264791, current_train_items 295072.
I0302 19:02:41.947718 22699590365312 run.py:483] Algo bellman_ford step 9221 current loss 0.357294, current_train_items 295104.
I0302 19:02:41.971761 22699590365312 run.py:483] Algo bellman_ford step 9222 current loss 0.604301, current_train_items 295136.
I0302 19:02:42.002703 22699590365312 run.py:483] Algo bellman_ford step 9223 current loss 0.620917, current_train_items 295168.
I0302 19:02:42.036390 22699590365312 run.py:483] Algo bellman_ford step 9224 current loss 0.649653, current_train_items 295200.
I0302 19:02:42.056074 22699590365312 run.py:483] Algo bellman_ford step 9225 current loss 0.267727, current_train_items 295232.
I0302 19:02:42.071976 22699590365312 run.py:483] Algo bellman_ford step 9226 current loss 0.366620, current_train_items 295264.
I0302 19:02:42.095954 22699590365312 run.py:483] Algo bellman_ford step 9227 current loss 0.560926, current_train_items 295296.
I0302 19:02:42.127564 22699590365312 run.py:483] Algo bellman_ford step 9228 current loss 0.596630, current_train_items 295328.
I0302 19:02:42.158804 22699590365312 run.py:483] Algo bellman_ford step 9229 current loss 0.686515, current_train_items 295360.
I0302 19:02:42.178304 22699590365312 run.py:483] Algo bellman_ford step 9230 current loss 0.283960, current_train_items 295392.
I0302 19:02:42.194619 22699590365312 run.py:483] Algo bellman_ford step 9231 current loss 0.410374, current_train_items 295424.
I0302 19:02:42.218885 22699590365312 run.py:483] Algo bellman_ford step 9232 current loss 0.568475, current_train_items 295456.
I0302 19:02:42.252796 22699590365312 run.py:483] Algo bellman_ford step 9233 current loss 0.602656, current_train_items 295488.
I0302 19:02:42.286790 22699590365312 run.py:483] Algo bellman_ford step 9234 current loss 0.718275, current_train_items 295520.
I0302 19:02:42.306307 22699590365312 run.py:483] Algo bellman_ford step 9235 current loss 0.228067, current_train_items 295552.
I0302 19:02:42.322439 22699590365312 run.py:483] Algo bellman_ford step 9236 current loss 0.423366, current_train_items 295584.
I0302 19:02:42.345971 22699590365312 run.py:483] Algo bellman_ford step 9237 current loss 0.503240, current_train_items 295616.
I0302 19:02:42.378595 22699590365312 run.py:483] Algo bellman_ford step 9238 current loss 0.675381, current_train_items 295648.
I0302 19:02:42.412463 22699590365312 run.py:483] Algo bellman_ford step 9239 current loss 0.626367, current_train_items 295680.
I0302 19:02:42.431965 22699590365312 run.py:483] Algo bellman_ford step 9240 current loss 0.244483, current_train_items 295712.
I0302 19:02:42.448478 22699590365312 run.py:483] Algo bellman_ford step 9241 current loss 0.479862, current_train_items 295744.
I0302 19:02:42.472936 22699590365312 run.py:483] Algo bellman_ford step 9242 current loss 0.600881, current_train_items 295776.
I0302 19:02:42.504605 22699590365312 run.py:483] Algo bellman_ford step 9243 current loss 0.563497, current_train_items 295808.
I0302 19:02:42.537066 22699590365312 run.py:483] Algo bellman_ford step 9244 current loss 0.632259, current_train_items 295840.
I0302 19:02:42.556584 22699590365312 run.py:483] Algo bellman_ford step 9245 current loss 0.281789, current_train_items 295872.
I0302 19:02:42.572650 22699590365312 run.py:483] Algo bellman_ford step 9246 current loss 0.467700, current_train_items 295904.
I0302 19:02:42.595638 22699590365312 run.py:483] Algo bellman_ford step 9247 current loss 0.599985, current_train_items 295936.
I0302 19:02:42.627264 22699590365312 run.py:483] Algo bellman_ford step 9248 current loss 0.689423, current_train_items 295968.
I0302 19:02:42.661023 22699590365312 run.py:483] Algo bellman_ford step 9249 current loss 0.733414, current_train_items 296000.
I0302 19:02:42.680644 22699590365312 run.py:483] Algo bellman_ford step 9250 current loss 0.267287, current_train_items 296032.
I0302 19:02:42.688802 22699590365312 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0302 19:02:42.688910 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:02:42.705621 22699590365312 run.py:483] Algo bellman_ford step 9251 current loss 0.473302, current_train_items 296064.
I0302 19:02:42.730107 22699590365312 run.py:483] Algo bellman_ford step 9252 current loss 0.556944, current_train_items 296096.
I0302 19:02:42.761365 22699590365312 run.py:483] Algo bellman_ford step 9253 current loss 0.585350, current_train_items 296128.
I0302 19:02:42.793415 22699590365312 run.py:483] Algo bellman_ford step 9254 current loss 0.616775, current_train_items 296160.
I0302 19:02:42.813578 22699590365312 run.py:483] Algo bellman_ford step 9255 current loss 0.277181, current_train_items 296192.
I0302 19:02:42.829396 22699590365312 run.py:483] Algo bellman_ford step 9256 current loss 0.359700, current_train_items 296224.
I0302 19:02:42.853247 22699590365312 run.py:483] Algo bellman_ford step 9257 current loss 0.592421, current_train_items 296256.
I0302 19:02:42.885309 22699590365312 run.py:483] Algo bellman_ford step 9258 current loss 0.670515, current_train_items 296288.
I0302 19:02:42.918835 22699590365312 run.py:483] Algo bellman_ford step 9259 current loss 0.671032, current_train_items 296320.
I0302 19:02:42.938646 22699590365312 run.py:483] Algo bellman_ford step 9260 current loss 0.332435, current_train_items 296352.
I0302 19:02:42.955441 22699590365312 run.py:483] Algo bellman_ford step 9261 current loss 0.469992, current_train_items 296384.
I0302 19:02:42.977944 22699590365312 run.py:483] Algo bellman_ford step 9262 current loss 0.451323, current_train_items 296416.
I0302 19:02:43.009514 22699590365312 run.py:483] Algo bellman_ford step 9263 current loss 0.600267, current_train_items 296448.
I0302 19:02:43.043099 22699590365312 run.py:483] Algo bellman_ford step 9264 current loss 0.682095, current_train_items 296480.
I0302 19:02:43.062438 22699590365312 run.py:483] Algo bellman_ford step 9265 current loss 0.318654, current_train_items 296512.
I0302 19:02:43.078526 22699590365312 run.py:483] Algo bellman_ford step 9266 current loss 0.415801, current_train_items 296544.
I0302 19:02:43.101072 22699590365312 run.py:483] Algo bellman_ford step 9267 current loss 0.515165, current_train_items 296576.
I0302 19:02:43.132480 22699590365312 run.py:483] Algo bellman_ford step 9268 current loss 0.605697, current_train_items 296608.
I0302 19:02:43.165733 22699590365312 run.py:483] Algo bellman_ford step 9269 current loss 0.704439, current_train_items 296640.
I0302 19:02:43.185524 22699590365312 run.py:483] Algo bellman_ford step 9270 current loss 0.300307, current_train_items 296672.
I0302 19:02:43.201475 22699590365312 run.py:483] Algo bellman_ford step 9271 current loss 0.451333, current_train_items 296704.
I0302 19:02:43.224639 22699590365312 run.py:483] Algo bellman_ford step 9272 current loss 0.554865, current_train_items 296736.
I0302 19:02:43.257634 22699590365312 run.py:483] Algo bellman_ford step 9273 current loss 0.673433, current_train_items 296768.
I0302 19:02:43.291892 22699590365312 run.py:483] Algo bellman_ford step 9274 current loss 0.754004, current_train_items 296800.
I0302 19:02:43.312081 22699590365312 run.py:483] Algo bellman_ford step 9275 current loss 0.341663, current_train_items 296832.
I0302 19:02:43.328200 22699590365312 run.py:483] Algo bellman_ford step 9276 current loss 0.433758, current_train_items 296864.
I0302 19:02:43.351589 22699590365312 run.py:483] Algo bellman_ford step 9277 current loss 0.547753, current_train_items 296896.
I0302 19:02:43.383343 22699590365312 run.py:483] Algo bellman_ford step 9278 current loss 0.595031, current_train_items 296928.
I0302 19:02:43.416959 22699590365312 run.py:483] Algo bellman_ford step 9279 current loss 0.775146, current_train_items 296960.
I0302 19:02:43.437025 22699590365312 run.py:483] Algo bellman_ford step 9280 current loss 0.321388, current_train_items 296992.
I0302 19:02:43.453526 22699590365312 run.py:483] Algo bellman_ford step 9281 current loss 0.385402, current_train_items 297024.
I0302 19:02:43.476677 22699590365312 run.py:483] Algo bellman_ford step 9282 current loss 0.498530, current_train_items 297056.
I0302 19:02:43.506474 22699590365312 run.py:483] Algo bellman_ford step 9283 current loss 0.508701, current_train_items 297088.
I0302 19:02:43.539690 22699590365312 run.py:483] Algo bellman_ford step 9284 current loss 0.704421, current_train_items 297120.
I0302 19:02:43.559627 22699590365312 run.py:483] Algo bellman_ford step 9285 current loss 0.231311, current_train_items 297152.
I0302 19:02:43.575121 22699590365312 run.py:483] Algo bellman_ford step 9286 current loss 0.424796, current_train_items 297184.
I0302 19:02:43.599468 22699590365312 run.py:483] Algo bellman_ford step 9287 current loss 0.590544, current_train_items 297216.
I0302 19:02:43.630364 22699590365312 run.py:483] Algo bellman_ford step 9288 current loss 0.555515, current_train_items 297248.
I0302 19:02:43.665570 22699590365312 run.py:483] Algo bellman_ford step 9289 current loss 0.729030, current_train_items 297280.
I0302 19:02:43.685348 22699590365312 run.py:483] Algo bellman_ford step 9290 current loss 0.326836, current_train_items 297312.
I0302 19:02:43.701312 22699590365312 run.py:483] Algo bellman_ford step 9291 current loss 0.431760, current_train_items 297344.
I0302 19:02:43.725265 22699590365312 run.py:483] Algo bellman_ford step 9292 current loss 0.575516, current_train_items 297376.
I0302 19:02:43.756354 22699590365312 run.py:483] Algo bellman_ford step 9293 current loss 0.549651, current_train_items 297408.
I0302 19:02:43.790797 22699590365312 run.py:483] Algo bellman_ford step 9294 current loss 0.754642, current_train_items 297440.
I0302 19:02:43.810593 22699590365312 run.py:483] Algo bellman_ford step 9295 current loss 0.236927, current_train_items 297472.
I0302 19:02:43.826688 22699590365312 run.py:483] Algo bellman_ford step 9296 current loss 0.396732, current_train_items 297504.
I0302 19:02:43.850324 22699590365312 run.py:483] Algo bellman_ford step 9297 current loss 0.564144, current_train_items 297536.
I0302 19:02:43.882266 22699590365312 run.py:483] Algo bellman_ford step 9298 current loss 0.563075, current_train_items 297568.
I0302 19:02:43.916360 22699590365312 run.py:483] Algo bellman_ford step 9299 current loss 0.739446, current_train_items 297600.
I0302 19:02:43.936181 22699590365312 run.py:483] Algo bellman_ford step 9300 current loss 0.322090, current_train_items 297632.
I0302 19:02:43.944288 22699590365312 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0302 19:02:43.944395 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:02:43.961302 22699590365312 run.py:483] Algo bellman_ford step 9301 current loss 0.408333, current_train_items 297664.
I0302 19:02:43.985525 22699590365312 run.py:483] Algo bellman_ford step 9302 current loss 0.484783, current_train_items 297696.
I0302 19:02:44.017521 22699590365312 run.py:483] Algo bellman_ford step 9303 current loss 0.604787, current_train_items 297728.
I0302 19:02:44.052319 22699590365312 run.py:483] Algo bellman_ford step 9304 current loss 0.735379, current_train_items 297760.
I0302 19:02:44.072170 22699590365312 run.py:483] Algo bellman_ford step 9305 current loss 0.256332, current_train_items 297792.
I0302 19:02:44.087985 22699590365312 run.py:483] Algo bellman_ford step 9306 current loss 0.461183, current_train_items 297824.
I0302 19:02:44.111952 22699590365312 run.py:483] Algo bellman_ford step 9307 current loss 0.653323, current_train_items 297856.
I0302 19:02:44.143488 22699590365312 run.py:483] Algo bellman_ford step 9308 current loss 0.621704, current_train_items 297888.
I0302 19:02:44.176686 22699590365312 run.py:483] Algo bellman_ford step 9309 current loss 0.626931, current_train_items 297920.
I0302 19:02:44.196208 22699590365312 run.py:483] Algo bellman_ford step 9310 current loss 0.280826, current_train_items 297952.
I0302 19:02:44.212077 22699590365312 run.py:483] Algo bellman_ford step 9311 current loss 0.377461, current_train_items 297984.
I0302 19:02:44.235372 22699590365312 run.py:483] Algo bellman_ford step 9312 current loss 0.533668, current_train_items 298016.
I0302 19:02:44.265752 22699590365312 run.py:483] Algo bellman_ford step 9313 current loss 0.598531, current_train_items 298048.
I0302 19:02:44.299803 22699590365312 run.py:483] Algo bellman_ford step 9314 current loss 0.638493, current_train_items 298080.
I0302 19:02:44.319413 22699590365312 run.py:483] Algo bellman_ford step 9315 current loss 0.300803, current_train_items 298112.
I0302 19:02:44.335128 22699590365312 run.py:483] Algo bellman_ford step 9316 current loss 0.358315, current_train_items 298144.
I0302 19:02:44.358730 22699590365312 run.py:483] Algo bellman_ford step 9317 current loss 0.520129, current_train_items 298176.
I0302 19:02:44.390836 22699590365312 run.py:483] Algo bellman_ford step 9318 current loss 0.649605, current_train_items 298208.
I0302 19:02:44.424809 22699590365312 run.py:483] Algo bellman_ford step 9319 current loss 0.565935, current_train_items 298240.
I0302 19:02:44.444567 22699590365312 run.py:483] Algo bellman_ford step 9320 current loss 0.316258, current_train_items 298272.
I0302 19:02:44.460677 22699590365312 run.py:483] Algo bellman_ford step 9321 current loss 0.431529, current_train_items 298304.
I0302 19:02:44.482991 22699590365312 run.py:483] Algo bellman_ford step 9322 current loss 0.543447, current_train_items 298336.
I0302 19:02:44.513273 22699590365312 run.py:483] Algo bellman_ford step 9323 current loss 0.635257, current_train_items 298368.
I0302 19:02:44.549611 22699590365312 run.py:483] Algo bellman_ford step 9324 current loss 0.861417, current_train_items 298400.
I0302 19:02:44.569032 22699590365312 run.py:483] Algo bellman_ford step 9325 current loss 0.278996, current_train_items 298432.
I0302 19:02:44.585008 22699590365312 run.py:483] Algo bellman_ford step 9326 current loss 0.456233, current_train_items 298464.
I0302 19:02:44.609306 22699590365312 run.py:483] Algo bellman_ford step 9327 current loss 0.607272, current_train_items 298496.
I0302 19:02:44.641556 22699590365312 run.py:483] Algo bellman_ford step 9328 current loss 0.630937, current_train_items 298528.
I0302 19:02:44.672371 22699590365312 run.py:483] Algo bellman_ford step 9329 current loss 0.645581, current_train_items 298560.
I0302 19:02:44.692415 22699590365312 run.py:483] Algo bellman_ford step 9330 current loss 0.262770, current_train_items 298592.
I0302 19:02:44.708580 22699590365312 run.py:483] Algo bellman_ford step 9331 current loss 0.435375, current_train_items 298624.
I0302 19:02:44.731912 22699590365312 run.py:483] Algo bellman_ford step 9332 current loss 0.525746, current_train_items 298656.
I0302 19:02:44.763106 22699590365312 run.py:483] Algo bellman_ford step 9333 current loss 0.644984, current_train_items 298688.
I0302 19:02:44.795772 22699590365312 run.py:483] Algo bellman_ford step 9334 current loss 0.702101, current_train_items 298720.
I0302 19:02:44.815496 22699590365312 run.py:483] Algo bellman_ford step 9335 current loss 0.232165, current_train_items 298752.
I0302 19:02:44.831503 22699590365312 run.py:483] Algo bellman_ford step 9336 current loss 0.472075, current_train_items 298784.
I0302 19:02:44.856164 22699590365312 run.py:483] Algo bellman_ford step 9337 current loss 0.611375, current_train_items 298816.
I0302 19:02:44.887820 22699590365312 run.py:483] Algo bellman_ford step 9338 current loss 0.606520, current_train_items 298848.
I0302 19:02:44.921695 22699590365312 run.py:483] Algo bellman_ford step 9339 current loss 0.661194, current_train_items 298880.
I0302 19:02:44.941187 22699590365312 run.py:483] Algo bellman_ford step 9340 current loss 0.275239, current_train_items 298912.
I0302 19:02:44.956858 22699590365312 run.py:483] Algo bellman_ford step 9341 current loss 0.418431, current_train_items 298944.
I0302 19:02:44.980886 22699590365312 run.py:483] Algo bellman_ford step 9342 current loss 0.498836, current_train_items 298976.
I0302 19:02:45.012675 22699590365312 run.py:483] Algo bellman_ford step 9343 current loss 0.582128, current_train_items 299008.
I0302 19:02:45.046728 22699590365312 run.py:483] Algo bellman_ford step 9344 current loss 0.670282, current_train_items 299040.
I0302 19:02:45.066133 22699590365312 run.py:483] Algo bellman_ford step 9345 current loss 0.319548, current_train_items 299072.
I0302 19:02:45.082077 22699590365312 run.py:483] Algo bellman_ford step 9346 current loss 0.428271, current_train_items 299104.
I0302 19:02:45.106229 22699590365312 run.py:483] Algo bellman_ford step 9347 current loss 0.489863, current_train_items 299136.
I0302 19:02:45.137969 22699590365312 run.py:483] Algo bellman_ford step 9348 current loss 0.579351, current_train_items 299168.
I0302 19:02:45.170301 22699590365312 run.py:483] Algo bellman_ford step 9349 current loss 0.628918, current_train_items 299200.
I0302 19:02:45.190074 22699590365312 run.py:483] Algo bellman_ford step 9350 current loss 0.315415, current_train_items 299232.
I0302 19:02:45.198123 22699590365312 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0302 19:02:45.198236 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:02:45.214769 22699590365312 run.py:483] Algo bellman_ford step 9351 current loss 0.422850, current_train_items 299264.
I0302 19:02:45.240168 22699590365312 run.py:483] Algo bellman_ford step 9352 current loss 0.667858, current_train_items 299296.
I0302 19:02:45.273164 22699590365312 run.py:483] Algo bellman_ford step 9353 current loss 0.625186, current_train_items 299328.
I0302 19:02:45.309015 22699590365312 run.py:483] Algo bellman_ford step 9354 current loss 0.802925, current_train_items 299360.
I0302 19:02:45.328866 22699590365312 run.py:483] Algo bellman_ford step 9355 current loss 0.273590, current_train_items 299392.
I0302 19:02:45.344290 22699590365312 run.py:483] Algo bellman_ford step 9356 current loss 0.452616, current_train_items 299424.
I0302 19:02:45.368035 22699590365312 run.py:483] Algo bellman_ford step 9357 current loss 0.672455, current_train_items 299456.
I0302 19:02:45.399775 22699590365312 run.py:483] Algo bellman_ford step 9358 current loss 0.699684, current_train_items 299488.
I0302 19:02:45.433069 22699590365312 run.py:483] Algo bellman_ford step 9359 current loss 0.610122, current_train_items 299520.
I0302 19:02:45.453228 22699590365312 run.py:483] Algo bellman_ford step 9360 current loss 0.287201, current_train_items 299552.
I0302 19:02:45.469146 22699590365312 run.py:483] Algo bellman_ford step 9361 current loss 0.423054, current_train_items 299584.
I0302 19:02:45.492484 22699590365312 run.py:483] Algo bellman_ford step 9362 current loss 0.534578, current_train_items 299616.
I0302 19:02:45.524508 22699590365312 run.py:483] Algo bellman_ford step 9363 current loss 0.621872, current_train_items 299648.
I0302 19:02:45.558879 22699590365312 run.py:483] Algo bellman_ford step 9364 current loss 0.765789, current_train_items 299680.
I0302 19:02:45.578083 22699590365312 run.py:483] Algo bellman_ford step 9365 current loss 0.306389, current_train_items 299712.
I0302 19:02:45.594471 22699590365312 run.py:483] Algo bellman_ford step 9366 current loss 0.471343, current_train_items 299744.
I0302 19:02:45.619648 22699590365312 run.py:483] Algo bellman_ford step 9367 current loss 0.632609, current_train_items 299776.
I0302 19:02:45.652010 22699590365312 run.py:483] Algo bellman_ford step 9368 current loss 0.603764, current_train_items 299808.
I0302 19:02:45.684743 22699590365312 run.py:483] Algo bellman_ford step 9369 current loss 0.691763, current_train_items 299840.
I0302 19:02:45.704822 22699590365312 run.py:483] Algo bellman_ford step 9370 current loss 0.299803, current_train_items 299872.
I0302 19:02:45.721078 22699590365312 run.py:483] Algo bellman_ford step 9371 current loss 0.484060, current_train_items 299904.
I0302 19:02:45.744707 22699590365312 run.py:483] Algo bellman_ford step 9372 current loss 0.560065, current_train_items 299936.
I0302 19:02:45.776959 22699590365312 run.py:483] Algo bellman_ford step 9373 current loss 0.587110, current_train_items 299968.
I0302 19:02:45.810949 22699590365312 run.py:483] Algo bellman_ford step 9374 current loss 0.631599, current_train_items 300000.
I0302 19:02:45.830647 22699590365312 run.py:483] Algo bellman_ford step 9375 current loss 0.243818, current_train_items 300032.
I0302 19:02:45.846213 22699590365312 run.py:483] Algo bellman_ford step 9376 current loss 0.481563, current_train_items 300064.
I0302 19:02:45.869455 22699590365312 run.py:483] Algo bellman_ford step 9377 current loss 0.502020, current_train_items 300096.
I0302 19:02:45.899830 22699590365312 run.py:483] Algo bellman_ford step 9378 current loss 0.587355, current_train_items 300128.
I0302 19:02:45.933349 22699590365312 run.py:483] Algo bellman_ford step 9379 current loss 0.699786, current_train_items 300160.
I0302 19:02:45.952734 22699590365312 run.py:483] Algo bellman_ford step 9380 current loss 0.246503, current_train_items 300192.
I0302 19:02:45.968206 22699590365312 run.py:483] Algo bellman_ford step 9381 current loss 0.389032, current_train_items 300224.
I0302 19:02:45.991919 22699590365312 run.py:483] Algo bellman_ford step 9382 current loss 0.545250, current_train_items 300256.
I0302 19:02:46.022504 22699590365312 run.py:483] Algo bellman_ford step 9383 current loss 0.516291, current_train_items 300288.
I0302 19:02:46.056137 22699590365312 run.py:483] Algo bellman_ford step 9384 current loss 0.623265, current_train_items 300320.
I0302 19:02:46.076190 22699590365312 run.py:483] Algo bellman_ford step 9385 current loss 0.256181, current_train_items 300352.
I0302 19:02:46.092606 22699590365312 run.py:483] Algo bellman_ford step 9386 current loss 0.473942, current_train_items 300384.
I0302 19:02:46.115983 22699590365312 run.py:483] Algo bellman_ford step 9387 current loss 0.580512, current_train_items 300416.
I0302 19:02:46.147250 22699590365312 run.py:483] Algo bellman_ford step 9388 current loss 0.606408, current_train_items 300448.
I0302 19:02:46.182888 22699590365312 run.py:483] Algo bellman_ford step 9389 current loss 0.604602, current_train_items 300480.
I0302 19:02:46.202659 22699590365312 run.py:483] Algo bellman_ford step 9390 current loss 0.279399, current_train_items 300512.
I0302 19:02:46.218852 22699590365312 run.py:483] Algo bellman_ford step 9391 current loss 0.397071, current_train_items 300544.
I0302 19:02:46.242027 22699590365312 run.py:483] Algo bellman_ford step 9392 current loss 0.541318, current_train_items 300576.
I0302 19:02:46.273271 22699590365312 run.py:483] Algo bellman_ford step 9393 current loss 0.569088, current_train_items 300608.
I0302 19:02:46.306255 22699590365312 run.py:483] Algo bellman_ford step 9394 current loss 0.693975, current_train_items 300640.
I0302 19:02:46.325610 22699590365312 run.py:483] Algo bellman_ford step 9395 current loss 0.336090, current_train_items 300672.
I0302 19:02:46.341755 22699590365312 run.py:483] Algo bellman_ford step 9396 current loss 0.445910, current_train_items 300704.
I0302 19:02:46.366323 22699590365312 run.py:483] Algo bellman_ford step 9397 current loss 0.567524, current_train_items 300736.
I0302 19:02:46.397514 22699590365312 run.py:483] Algo bellman_ford step 9398 current loss 0.544507, current_train_items 300768.
I0302 19:02:46.431143 22699590365312 run.py:483] Algo bellman_ford step 9399 current loss 0.679984, current_train_items 300800.
I0302 19:02:46.450958 22699590365312 run.py:483] Algo bellman_ford step 9400 current loss 0.273738, current_train_items 300832.
I0302 19:02:46.458704 22699590365312 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0302 19:02:46.458809 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:02:46.475543 22699590365312 run.py:483] Algo bellman_ford step 9401 current loss 0.457491, current_train_items 300864.
I0302 19:02:46.500684 22699590365312 run.py:483] Algo bellman_ford step 9402 current loss 0.580741, current_train_items 300896.
I0302 19:02:46.532361 22699590365312 run.py:483] Algo bellman_ford step 9403 current loss 0.605663, current_train_items 300928.
I0302 19:02:46.567032 22699590365312 run.py:483] Algo bellman_ford step 9404 current loss 0.670570, current_train_items 300960.
I0302 19:02:46.587062 22699590365312 run.py:483] Algo bellman_ford step 9405 current loss 0.284897, current_train_items 300992.
I0302 19:02:46.602854 22699590365312 run.py:483] Algo bellman_ford step 9406 current loss 0.404102, current_train_items 301024.
I0302 19:02:46.628234 22699590365312 run.py:483] Algo bellman_ford step 9407 current loss 0.554021, current_train_items 301056.
I0302 19:02:46.659640 22699590365312 run.py:483] Algo bellman_ford step 9408 current loss 0.605602, current_train_items 301088.
I0302 19:02:46.694397 22699590365312 run.py:483] Algo bellman_ford step 9409 current loss 0.774446, current_train_items 301120.
I0302 19:02:46.714056 22699590365312 run.py:483] Algo bellman_ford step 9410 current loss 0.284232, current_train_items 301152.
I0302 19:02:46.729995 22699590365312 run.py:483] Algo bellman_ford step 9411 current loss 0.477015, current_train_items 301184.
I0302 19:02:46.754086 22699590365312 run.py:483] Algo bellman_ford step 9412 current loss 0.527577, current_train_items 301216.
I0302 19:02:46.785537 22699590365312 run.py:483] Algo bellman_ford step 9413 current loss 0.536311, current_train_items 301248.
I0302 19:02:46.818512 22699590365312 run.py:483] Algo bellman_ford step 9414 current loss 0.662948, current_train_items 301280.
I0302 19:02:46.838077 22699590365312 run.py:483] Algo bellman_ford step 9415 current loss 0.339573, current_train_items 301312.
I0302 19:02:46.854328 22699590365312 run.py:483] Algo bellman_ford step 9416 current loss 0.378052, current_train_items 301344.
I0302 19:02:46.878616 22699590365312 run.py:483] Algo bellman_ford step 9417 current loss 0.532122, current_train_items 301376.
I0302 19:02:46.909557 22699590365312 run.py:483] Algo bellman_ford step 9418 current loss 0.621910, current_train_items 301408.
I0302 19:02:46.941004 22699590365312 run.py:483] Algo bellman_ford step 9419 current loss 0.618846, current_train_items 301440.
I0302 19:02:46.960355 22699590365312 run.py:483] Algo bellman_ford step 9420 current loss 0.302367, current_train_items 301472.
I0302 19:02:46.976517 22699590365312 run.py:483] Algo bellman_ford step 9421 current loss 0.388924, current_train_items 301504.
I0302 19:02:47.001034 22699590365312 run.py:483] Algo bellman_ford step 9422 current loss 0.565428, current_train_items 301536.
I0302 19:02:47.032702 22699590365312 run.py:483] Algo bellman_ford step 9423 current loss 0.584606, current_train_items 301568.
I0302 19:02:47.066127 22699590365312 run.py:483] Algo bellman_ford step 9424 current loss 0.722238, current_train_items 301600.
I0302 19:02:47.085923 22699590365312 run.py:483] Algo bellman_ford step 9425 current loss 0.313723, current_train_items 301632.
I0302 19:02:47.101618 22699590365312 run.py:483] Algo bellman_ford step 9426 current loss 0.398564, current_train_items 301664.
I0302 19:02:47.124754 22699590365312 run.py:483] Algo bellman_ford step 9427 current loss 0.573090, current_train_items 301696.
I0302 19:02:47.156737 22699590365312 run.py:483] Algo bellman_ford step 9428 current loss 0.666094, current_train_items 301728.
I0302 19:02:47.187669 22699590365312 run.py:483] Algo bellman_ford step 9429 current loss 0.848177, current_train_items 301760.
I0302 19:02:47.206971 22699590365312 run.py:483] Algo bellman_ford step 9430 current loss 0.296867, current_train_items 301792.
I0302 19:02:47.222931 22699590365312 run.py:483] Algo bellman_ford step 9431 current loss 0.523478, current_train_items 301824.
I0302 19:02:47.246381 22699590365312 run.py:483] Algo bellman_ford step 9432 current loss 0.651654, current_train_items 301856.
I0302 19:02:47.277054 22699590365312 run.py:483] Algo bellman_ford step 9433 current loss 0.690824, current_train_items 301888.
I0302 19:02:47.311291 22699590365312 run.py:483] Algo bellman_ford step 9434 current loss 0.970537, current_train_items 301920.
I0302 19:02:47.330926 22699590365312 run.py:483] Algo bellman_ford step 9435 current loss 0.274646, current_train_items 301952.
I0302 19:02:47.347120 22699590365312 run.py:483] Algo bellman_ford step 9436 current loss 0.437978, current_train_items 301984.
I0302 19:02:47.370674 22699590365312 run.py:483] Algo bellman_ford step 9437 current loss 0.540940, current_train_items 302016.
I0302 19:02:47.402759 22699590365312 run.py:483] Algo bellman_ford step 9438 current loss 0.605829, current_train_items 302048.
I0302 19:02:47.435487 22699590365312 run.py:483] Algo bellman_ford step 9439 current loss 0.705373, current_train_items 302080.
I0302 19:02:47.455332 22699590365312 run.py:483] Algo bellman_ford step 9440 current loss 0.277027, current_train_items 302112.
I0302 19:02:47.471671 22699590365312 run.py:483] Algo bellman_ford step 9441 current loss 0.445222, current_train_items 302144.
I0302 19:02:47.495353 22699590365312 run.py:483] Algo bellman_ford step 9442 current loss 0.524778, current_train_items 302176.
I0302 19:02:47.527482 22699590365312 run.py:483] Algo bellman_ford step 9443 current loss 0.535458, current_train_items 302208.
I0302 19:02:47.563078 22699590365312 run.py:483] Algo bellman_ford step 9444 current loss 0.714732, current_train_items 302240.
I0302 19:02:47.582603 22699590365312 run.py:483] Algo bellman_ford step 9445 current loss 0.319151, current_train_items 302272.
I0302 19:02:47.598201 22699590365312 run.py:483] Algo bellman_ford step 9446 current loss 0.397795, current_train_items 302304.
I0302 19:02:47.621493 22699590365312 run.py:483] Algo bellman_ford step 9447 current loss 0.557882, current_train_items 302336.
I0302 19:02:47.652501 22699590365312 run.py:483] Algo bellman_ford step 9448 current loss 0.630790, current_train_items 302368.
I0302 19:02:47.684491 22699590365312 run.py:483] Algo bellman_ford step 9449 current loss 0.598810, current_train_items 302400.
I0302 19:02:47.704033 22699590365312 run.py:483] Algo bellman_ford step 9450 current loss 0.270536, current_train_items 302432.
I0302 19:02:47.712089 22699590365312 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0302 19:02:47.712206 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:02:47.729008 22699590365312 run.py:483] Algo bellman_ford step 9451 current loss 0.426166, current_train_items 302464.
I0302 19:02:47.752334 22699590365312 run.py:483] Algo bellman_ford step 9452 current loss 0.542312, current_train_items 302496.
I0302 19:02:47.784504 22699590365312 run.py:483] Algo bellman_ford step 9453 current loss 0.570227, current_train_items 302528.
I0302 19:02:47.818936 22699590365312 run.py:483] Algo bellman_ford step 9454 current loss 0.655135, current_train_items 302560.
I0302 19:02:47.839352 22699590365312 run.py:483] Algo bellman_ford step 9455 current loss 0.292109, current_train_items 302592.
I0302 19:02:47.855160 22699590365312 run.py:483] Algo bellman_ford step 9456 current loss 0.410530, current_train_items 302624.
I0302 19:02:47.878843 22699590365312 run.py:483] Algo bellman_ford step 9457 current loss 0.494551, current_train_items 302656.
I0302 19:02:47.910544 22699590365312 run.py:483] Algo bellman_ford step 9458 current loss 0.567394, current_train_items 302688.
I0302 19:02:47.942734 22699590365312 run.py:483] Algo bellman_ford step 9459 current loss 0.628459, current_train_items 302720.
I0302 19:02:47.962940 22699590365312 run.py:483] Algo bellman_ford step 9460 current loss 0.249461, current_train_items 302752.
I0302 19:02:47.979366 22699590365312 run.py:483] Algo bellman_ford step 9461 current loss 0.417412, current_train_items 302784.
I0302 19:02:48.002387 22699590365312 run.py:483] Algo bellman_ford step 9462 current loss 0.507879, current_train_items 302816.
I0302 19:02:48.034607 22699590365312 run.py:483] Algo bellman_ford step 9463 current loss 0.611417, current_train_items 302848.
I0302 19:02:48.067553 22699590365312 run.py:483] Algo bellman_ford step 9464 current loss 0.797554, current_train_items 302880.
I0302 19:02:48.086920 22699590365312 run.py:483] Algo bellman_ford step 9465 current loss 0.319819, current_train_items 302912.
I0302 19:02:48.103347 22699590365312 run.py:483] Algo bellman_ford step 9466 current loss 0.463957, current_train_items 302944.
I0302 19:02:48.128387 22699590365312 run.py:483] Algo bellman_ford step 9467 current loss 0.602051, current_train_items 302976.
I0302 19:02:48.160564 22699590365312 run.py:483] Algo bellman_ford step 9468 current loss 0.588753, current_train_items 303008.
I0302 19:02:48.194799 22699590365312 run.py:483] Algo bellman_ford step 9469 current loss 0.628033, current_train_items 303040.
I0302 19:02:48.214597 22699590365312 run.py:483] Algo bellman_ford step 9470 current loss 0.282778, current_train_items 303072.
I0302 19:02:48.230313 22699590365312 run.py:483] Algo bellman_ford step 9471 current loss 0.452115, current_train_items 303104.
I0302 19:02:48.253869 22699590365312 run.py:483] Algo bellman_ford step 9472 current loss 0.545252, current_train_items 303136.
I0302 19:02:48.285812 22699590365312 run.py:483] Algo bellman_ford step 9473 current loss 0.621156, current_train_items 303168.
I0302 19:02:48.318587 22699590365312 run.py:483] Algo bellman_ford step 9474 current loss 0.663437, current_train_items 303200.
I0302 19:02:48.338271 22699590365312 run.py:483] Algo bellman_ford step 9475 current loss 0.294288, current_train_items 303232.
I0302 19:02:48.354369 22699590365312 run.py:483] Algo bellman_ford step 9476 current loss 0.435819, current_train_items 303264.
I0302 19:02:48.377501 22699590365312 run.py:483] Algo bellman_ford step 9477 current loss 0.541626, current_train_items 303296.
I0302 19:02:48.408645 22699590365312 run.py:483] Algo bellman_ford step 9478 current loss 0.658627, current_train_items 303328.
I0302 19:02:48.441410 22699590365312 run.py:483] Algo bellman_ford step 9479 current loss 0.738767, current_train_items 303360.
I0302 19:02:48.460859 22699590365312 run.py:483] Algo bellman_ford step 9480 current loss 0.323244, current_train_items 303392.
I0302 19:02:48.476689 22699590365312 run.py:483] Algo bellman_ford step 9481 current loss 0.448350, current_train_items 303424.
I0302 19:02:48.501885 22699590365312 run.py:483] Algo bellman_ford step 9482 current loss 0.700708, current_train_items 303456.
I0302 19:02:48.532100 22699590365312 run.py:483] Algo bellman_ford step 9483 current loss 0.607156, current_train_items 303488.
I0302 19:02:48.564551 22699590365312 run.py:483] Algo bellman_ford step 9484 current loss 0.777913, current_train_items 303520.
I0302 19:02:48.584002 22699590365312 run.py:483] Algo bellman_ford step 9485 current loss 0.271373, current_train_items 303552.
I0302 19:02:48.600420 22699590365312 run.py:483] Algo bellman_ford step 9486 current loss 0.370785, current_train_items 303584.
I0302 19:02:48.624986 22699590365312 run.py:483] Algo bellman_ford step 9487 current loss 0.643303, current_train_items 303616.
I0302 19:02:48.657589 22699590365312 run.py:483] Algo bellman_ford step 9488 current loss 0.682031, current_train_items 303648.
I0302 19:02:48.689917 22699590365312 run.py:483] Algo bellman_ford step 9489 current loss 0.783668, current_train_items 303680.
I0302 19:02:48.709645 22699590365312 run.py:483] Algo bellman_ford step 9490 current loss 0.228350, current_train_items 303712.
I0302 19:02:48.725501 22699590365312 run.py:483] Algo bellman_ford step 9491 current loss 0.483297, current_train_items 303744.
I0302 19:02:48.749594 22699590365312 run.py:483] Algo bellman_ford step 9492 current loss 0.553596, current_train_items 303776.
I0302 19:02:48.780183 22699590365312 run.py:483] Algo bellman_ford step 9493 current loss 0.589834, current_train_items 303808.
I0302 19:02:48.813817 22699590365312 run.py:483] Algo bellman_ford step 9494 current loss 0.834883, current_train_items 303840.
I0302 19:02:48.833322 22699590365312 run.py:483] Algo bellman_ford step 9495 current loss 0.279503, current_train_items 303872.
I0302 19:02:48.849882 22699590365312 run.py:483] Algo bellman_ford step 9496 current loss 0.459036, current_train_items 303904.
I0302 19:02:48.873330 22699590365312 run.py:483] Algo bellman_ford step 9497 current loss 0.530209, current_train_items 303936.
I0302 19:02:48.903614 22699590365312 run.py:483] Algo bellman_ford step 9498 current loss 0.591858, current_train_items 303968.
I0302 19:02:48.938018 22699590365312 run.py:483] Algo bellman_ford step 9499 current loss 0.775116, current_train_items 304000.
I0302 19:02:48.958128 22699590365312 run.py:483] Algo bellman_ford step 9500 current loss 0.237837, current_train_items 304032.
I0302 19:02:48.966151 22699590365312 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0302 19:02:48.966267 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:48.982726 22699590365312 run.py:483] Algo bellman_ford step 9501 current loss 0.405005, current_train_items 304064.
I0302 19:02:49.007211 22699590365312 run.py:483] Algo bellman_ford step 9502 current loss 0.561918, current_train_items 304096.
I0302 19:02:49.039022 22699590365312 run.py:483] Algo bellman_ford step 9503 current loss 0.581201, current_train_items 304128.
I0302 19:02:49.071774 22699590365312 run.py:483] Algo bellman_ford step 9504 current loss 0.678288, current_train_items 304160.
I0302 19:02:49.091829 22699590365312 run.py:483] Algo bellman_ford step 9505 current loss 0.269851, current_train_items 304192.
I0302 19:02:49.107924 22699590365312 run.py:483] Algo bellman_ford step 9506 current loss 0.441425, current_train_items 304224.
I0302 19:02:49.132432 22699590365312 run.py:483] Algo bellman_ford step 9507 current loss 0.534574, current_train_items 304256.
I0302 19:02:49.162717 22699590365312 run.py:483] Algo bellman_ford step 9508 current loss 0.592076, current_train_items 304288.
I0302 19:02:49.198296 22699590365312 run.py:483] Algo bellman_ford step 9509 current loss 0.766662, current_train_items 304320.
I0302 19:02:49.218252 22699590365312 run.py:483] Algo bellman_ford step 9510 current loss 0.284294, current_train_items 304352.
I0302 19:02:49.234845 22699590365312 run.py:483] Algo bellman_ford step 9511 current loss 0.445998, current_train_items 304384.
I0302 19:02:49.259131 22699590365312 run.py:483] Algo bellman_ford step 9512 current loss 0.592530, current_train_items 304416.
I0302 19:02:49.290747 22699590365312 run.py:483] Algo bellman_ford step 9513 current loss 0.679893, current_train_items 304448.
I0302 19:02:49.323142 22699590365312 run.py:483] Algo bellman_ford step 9514 current loss 0.743931, current_train_items 304480.
I0302 19:02:49.342522 22699590365312 run.py:483] Algo bellman_ford step 9515 current loss 0.338641, current_train_items 304512.
I0302 19:02:49.358165 22699590365312 run.py:483] Algo bellman_ford step 9516 current loss 0.385212, current_train_items 304544.
I0302 19:02:49.382213 22699590365312 run.py:483] Algo bellman_ford step 9517 current loss 0.522123, current_train_items 304576.
I0302 19:02:49.413523 22699590365312 run.py:483] Algo bellman_ford step 9518 current loss 0.615811, current_train_items 304608.
I0302 19:02:49.445918 22699590365312 run.py:483] Algo bellman_ford step 9519 current loss 0.614035, current_train_items 304640.
I0302 19:02:49.465342 22699590365312 run.py:483] Algo bellman_ford step 9520 current loss 0.304116, current_train_items 304672.
I0302 19:02:49.480653 22699590365312 run.py:483] Algo bellman_ford step 9521 current loss 0.396178, current_train_items 304704.
I0302 19:02:49.503929 22699590365312 run.py:483] Algo bellman_ford step 9522 current loss 0.522269, current_train_items 304736.
I0302 19:02:49.534006 22699590365312 run.py:483] Algo bellman_ford step 9523 current loss 0.551018, current_train_items 304768.
I0302 19:02:49.566693 22699590365312 run.py:483] Algo bellman_ford step 9524 current loss 0.637140, current_train_items 304800.
I0302 19:02:49.586050 22699590365312 run.py:483] Algo bellman_ford step 9525 current loss 0.188071, current_train_items 304832.
I0302 19:02:49.601992 22699590365312 run.py:483] Algo bellman_ford step 9526 current loss 0.380755, current_train_items 304864.
I0302 19:02:49.626121 22699590365312 run.py:483] Algo bellman_ford step 9527 current loss 0.621562, current_train_items 304896.
I0302 19:02:49.658271 22699590365312 run.py:483] Algo bellman_ford step 9528 current loss 0.636448, current_train_items 304928.
I0302 19:02:49.693162 22699590365312 run.py:483] Algo bellman_ford step 9529 current loss 0.713267, current_train_items 304960.
I0302 19:02:49.712752 22699590365312 run.py:483] Algo bellman_ford step 9530 current loss 0.243503, current_train_items 304992.
I0302 19:02:49.728869 22699590365312 run.py:483] Algo bellman_ford step 9531 current loss 0.398402, current_train_items 305024.
I0302 19:02:49.752573 22699590365312 run.py:483] Algo bellman_ford step 9532 current loss 0.560546, current_train_items 305056.
I0302 19:02:49.785614 22699590365312 run.py:483] Algo bellman_ford step 9533 current loss 0.646879, current_train_items 305088.
I0302 19:02:49.818833 22699590365312 run.py:483] Algo bellman_ford step 9534 current loss 0.616104, current_train_items 305120.
I0302 19:02:49.838420 22699590365312 run.py:483] Algo bellman_ford step 9535 current loss 0.287691, current_train_items 305152.
I0302 19:02:49.854568 22699590365312 run.py:483] Algo bellman_ford step 9536 current loss 0.471773, current_train_items 305184.
I0302 19:02:49.878367 22699590365312 run.py:483] Algo bellman_ford step 9537 current loss 0.532050, current_train_items 305216.
I0302 19:02:49.911107 22699590365312 run.py:483] Algo bellman_ford step 9538 current loss 0.610004, current_train_items 305248.
I0302 19:02:49.942295 22699590365312 run.py:483] Algo bellman_ford step 9539 current loss 0.625806, current_train_items 305280.
I0302 19:02:49.962204 22699590365312 run.py:483] Algo bellman_ford step 9540 current loss 0.268837, current_train_items 305312.
I0302 19:02:49.978010 22699590365312 run.py:483] Algo bellman_ford step 9541 current loss 0.401868, current_train_items 305344.
I0302 19:02:50.000924 22699590365312 run.py:483] Algo bellman_ford step 9542 current loss 0.546989, current_train_items 305376.
I0302 19:02:50.033821 22699590365312 run.py:483] Algo bellman_ford step 9543 current loss 0.703296, current_train_items 305408.
I0302 19:02:50.067333 22699590365312 run.py:483] Algo bellman_ford step 9544 current loss 0.762382, current_train_items 305440.
I0302 19:02:50.086699 22699590365312 run.py:483] Algo bellman_ford step 9545 current loss 0.267599, current_train_items 305472.
I0302 19:02:50.102200 22699590365312 run.py:483] Algo bellman_ford step 9546 current loss 0.443955, current_train_items 305504.
I0302 19:02:50.123660 22699590365312 run.py:483] Algo bellman_ford step 9547 current loss 0.464957, current_train_items 305536.
I0302 19:02:50.154551 22699590365312 run.py:483] Algo bellman_ford step 9548 current loss 0.632270, current_train_items 305568.
I0302 19:02:50.188278 22699590365312 run.py:483] Algo bellman_ford step 9549 current loss 0.684293, current_train_items 305600.
I0302 19:02:50.207686 22699590365312 run.py:483] Algo bellman_ford step 9550 current loss 0.236200, current_train_items 305632.
I0302 19:02:50.215702 22699590365312 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.951171875, 'score': 0.951171875, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0302 19:02:50.215806 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.951, val scores are: bellman_ford: 0.951
I0302 19:02:50.232459 22699590365312 run.py:483] Algo bellman_ford step 9551 current loss 0.491464, current_train_items 305664.
I0302 19:02:50.257202 22699590365312 run.py:483] Algo bellman_ford step 9552 current loss 0.595622, current_train_items 305696.
I0302 19:02:50.290683 22699590365312 run.py:483] Algo bellman_ford step 9553 current loss 0.609625, current_train_items 305728.
I0302 19:02:50.324138 22699590365312 run.py:483] Algo bellman_ford step 9554 current loss 0.621151, current_train_items 305760.
I0302 19:02:50.344249 22699590365312 run.py:483] Algo bellman_ford step 9555 current loss 0.286911, current_train_items 305792.
I0302 19:02:50.359601 22699590365312 run.py:483] Algo bellman_ford step 9556 current loss 0.408071, current_train_items 305824.
I0302 19:02:50.383763 22699590365312 run.py:483] Algo bellman_ford step 9557 current loss 0.565172, current_train_items 305856.
I0302 19:02:50.415072 22699590365312 run.py:483] Algo bellman_ford step 9558 current loss 0.654047, current_train_items 305888.
I0302 19:02:50.447505 22699590365312 run.py:483] Algo bellman_ford step 9559 current loss 0.713936, current_train_items 305920.
I0302 19:02:50.467190 22699590365312 run.py:483] Algo bellman_ford step 9560 current loss 0.278598, current_train_items 305952.
I0302 19:02:50.483316 22699590365312 run.py:483] Algo bellman_ford step 9561 current loss 0.447220, current_train_items 305984.
I0302 19:02:50.506369 22699590365312 run.py:483] Algo bellman_ford step 9562 current loss 0.584118, current_train_items 306016.
I0302 19:02:50.537326 22699590365312 run.py:483] Algo bellman_ford step 9563 current loss 0.593857, current_train_items 306048.
I0302 19:02:50.569727 22699590365312 run.py:483] Algo bellman_ford step 9564 current loss 0.662580, current_train_items 306080.
I0302 19:02:50.589356 22699590365312 run.py:483] Algo bellman_ford step 9565 current loss 0.363172, current_train_items 306112.
I0302 19:02:50.605841 22699590365312 run.py:483] Algo bellman_ford step 9566 current loss 0.489995, current_train_items 306144.
I0302 19:02:50.630566 22699590365312 run.py:483] Algo bellman_ford step 9567 current loss 0.752912, current_train_items 306176.
I0302 19:02:50.662225 22699590365312 run.py:483] Algo bellman_ford step 9568 current loss 0.763650, current_train_items 306208.
I0302 19:02:50.695458 22699590365312 run.py:483] Algo bellman_ford step 9569 current loss 0.638741, current_train_items 306240.
I0302 19:02:50.715260 22699590365312 run.py:483] Algo bellman_ford step 9570 current loss 0.290816, current_train_items 306272.
I0302 19:02:50.731323 22699590365312 run.py:483] Algo bellman_ford step 9571 current loss 0.438196, current_train_items 306304.
I0302 19:02:50.755903 22699590365312 run.py:483] Algo bellman_ford step 9572 current loss 0.619180, current_train_items 306336.
I0302 19:02:50.786496 22699590365312 run.py:483] Algo bellman_ford step 9573 current loss 0.581183, current_train_items 306368.
I0302 19:02:50.819950 22699590365312 run.py:483] Algo bellman_ford step 9574 current loss 0.796617, current_train_items 306400.
I0302 19:02:50.839977 22699590365312 run.py:483] Algo bellman_ford step 9575 current loss 0.280575, current_train_items 306432.
I0302 19:02:50.856740 22699590365312 run.py:483] Algo bellman_ford step 9576 current loss 0.415888, current_train_items 306464.
I0302 19:02:50.881127 22699590365312 run.py:483] Algo bellman_ford step 9577 current loss 0.750328, current_train_items 306496.
I0302 19:02:50.911971 22699590365312 run.py:483] Algo bellman_ford step 9578 current loss 0.481935, current_train_items 306528.
I0302 19:02:50.945543 22699590365312 run.py:483] Algo bellman_ford step 9579 current loss 0.643936, current_train_items 306560.
I0302 19:02:50.965087 22699590365312 run.py:483] Algo bellman_ford step 9580 current loss 0.299213, current_train_items 306592.
I0302 19:02:50.980863 22699590365312 run.py:483] Algo bellman_ford step 9581 current loss 0.432741, current_train_items 306624.
I0302 19:02:51.005800 22699590365312 run.py:483] Algo bellman_ford step 9582 current loss 0.523839, current_train_items 306656.
I0302 19:02:51.038426 22699590365312 run.py:483] Algo bellman_ford step 9583 current loss 0.628251, current_train_items 306688.
I0302 19:02:51.072703 22699590365312 run.py:483] Algo bellman_ford step 9584 current loss 0.658687, current_train_items 306720.
I0302 19:02:51.092690 22699590365312 run.py:483] Algo bellman_ford step 9585 current loss 0.303328, current_train_items 306752.
I0302 19:02:51.108847 22699590365312 run.py:483] Algo bellman_ford step 9586 current loss 0.373987, current_train_items 306784.
I0302 19:02:51.132773 22699590365312 run.py:483] Algo bellman_ford step 9587 current loss 0.511219, current_train_items 306816.
I0302 19:02:51.164865 22699590365312 run.py:483] Algo bellman_ford step 9588 current loss 0.683093, current_train_items 306848.
I0302 19:02:51.199590 22699590365312 run.py:483] Algo bellman_ford step 9589 current loss 0.920099, current_train_items 306880.
I0302 19:02:51.219272 22699590365312 run.py:483] Algo bellman_ford step 9590 current loss 0.212071, current_train_items 306912.
I0302 19:02:51.235333 22699590365312 run.py:483] Algo bellman_ford step 9591 current loss 0.371212, current_train_items 306944.
I0302 19:02:51.258982 22699590365312 run.py:483] Algo bellman_ford step 9592 current loss 0.639171, current_train_items 306976.
I0302 19:02:51.290814 22699590365312 run.py:483] Algo bellman_ford step 9593 current loss 0.549773, current_train_items 307008.
I0302 19:02:51.323683 22699590365312 run.py:483] Algo bellman_ford step 9594 current loss 0.648155, current_train_items 307040.
I0302 19:02:51.343301 22699590365312 run.py:483] Algo bellman_ford step 9595 current loss 0.320674, current_train_items 307072.
I0302 19:02:51.359497 22699590365312 run.py:483] Algo bellman_ford step 9596 current loss 0.436871, current_train_items 307104.
I0302 19:02:51.383874 22699590365312 run.py:483] Algo bellman_ford step 9597 current loss 0.537368, current_train_items 307136.
I0302 19:02:51.414074 22699590365312 run.py:483] Algo bellman_ford step 9598 current loss 0.482292, current_train_items 307168.
I0302 19:02:51.446980 22699590365312 run.py:483] Algo bellman_ford step 9599 current loss 0.642498, current_train_items 307200.
I0302 19:02:51.466732 22699590365312 run.py:483] Algo bellman_ford step 9600 current loss 0.266381, current_train_items 307232.
I0302 19:02:51.474703 22699590365312 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.9404296875, 'score': 0.9404296875, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0302 19:02:51.474809 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.940, val scores are: bellman_ford: 0.940
I0302 19:02:51.491879 22699590365312 run.py:483] Algo bellman_ford step 9601 current loss 0.446146, current_train_items 307264.
I0302 19:02:51.516797 22699590365312 run.py:483] Algo bellman_ford step 9602 current loss 0.489632, current_train_items 307296.
I0302 19:02:51.548022 22699590365312 run.py:483] Algo bellman_ford step 9603 current loss 0.553820, current_train_items 307328.
I0302 19:02:51.583500 22699590365312 run.py:483] Algo bellman_ford step 9604 current loss 0.679215, current_train_items 307360.
I0302 19:02:51.603512 22699590365312 run.py:483] Algo bellman_ford step 9605 current loss 0.259832, current_train_items 307392.
I0302 19:02:51.619152 22699590365312 run.py:483] Algo bellman_ford step 9606 current loss 0.403441, current_train_items 307424.
I0302 19:02:51.642645 22699590365312 run.py:483] Algo bellman_ford step 9607 current loss 0.501844, current_train_items 307456.
I0302 19:02:51.673532 22699590365312 run.py:483] Algo bellman_ford step 9608 current loss 0.571792, current_train_items 307488.
I0302 19:02:51.706923 22699590365312 run.py:483] Algo bellman_ford step 9609 current loss 0.722143, current_train_items 307520.
I0302 19:02:51.726730 22699590365312 run.py:483] Algo bellman_ford step 9610 current loss 0.339480, current_train_items 307552.
I0302 19:02:51.742944 22699590365312 run.py:483] Algo bellman_ford step 9611 current loss 0.425387, current_train_items 307584.
I0302 19:02:51.767466 22699590365312 run.py:483] Algo bellman_ford step 9612 current loss 0.549511, current_train_items 307616.
I0302 19:02:51.799111 22699590365312 run.py:483] Algo bellman_ford step 9613 current loss 0.646581, current_train_items 307648.
I0302 19:02:51.832907 22699590365312 run.py:483] Algo bellman_ford step 9614 current loss 0.673755, current_train_items 307680.
I0302 19:02:51.852846 22699590365312 run.py:483] Algo bellman_ford step 9615 current loss 0.279690, current_train_items 307712.
I0302 19:02:51.868896 22699590365312 run.py:483] Algo bellman_ford step 9616 current loss 0.563534, current_train_items 307744.
I0302 19:02:51.892748 22699590365312 run.py:483] Algo bellman_ford step 9617 current loss 0.659381, current_train_items 307776.
I0302 19:02:51.923755 22699590365312 run.py:483] Algo bellman_ford step 9618 current loss 0.582402, current_train_items 307808.
I0302 19:02:51.957363 22699590365312 run.py:483] Algo bellman_ford step 9619 current loss 0.703995, current_train_items 307840.
I0302 19:02:51.976690 22699590365312 run.py:483] Algo bellman_ford step 9620 current loss 0.291049, current_train_items 307872.
I0302 19:02:51.992596 22699590365312 run.py:483] Algo bellman_ford step 9621 current loss 0.383136, current_train_items 307904.
I0302 19:02:52.016779 22699590365312 run.py:483] Algo bellman_ford step 9622 current loss 0.567432, current_train_items 307936.
I0302 19:02:52.048346 22699590365312 run.py:483] Algo bellman_ford step 9623 current loss 0.570142, current_train_items 307968.
I0302 19:02:52.082896 22699590365312 run.py:483] Algo bellman_ford step 9624 current loss 0.692214, current_train_items 308000.
I0302 19:02:52.102378 22699590365312 run.py:483] Algo bellman_ford step 9625 current loss 0.311694, current_train_items 308032.
I0302 19:02:52.118387 22699590365312 run.py:483] Algo bellman_ford step 9626 current loss 0.442818, current_train_items 308064.
I0302 19:02:52.142234 22699590365312 run.py:483] Algo bellman_ford step 9627 current loss 0.503672, current_train_items 308096.
I0302 19:02:52.173130 22699590365312 run.py:483] Algo bellman_ford step 9628 current loss 0.575842, current_train_items 308128.
I0302 19:02:52.207438 22699590365312 run.py:483] Algo bellman_ford step 9629 current loss 0.776013, current_train_items 308160.
I0302 19:02:52.226777 22699590365312 run.py:483] Algo bellman_ford step 9630 current loss 0.252951, current_train_items 308192.
I0302 19:02:52.243172 22699590365312 run.py:483] Algo bellman_ford step 9631 current loss 0.387924, current_train_items 308224.
I0302 19:02:52.268080 22699590365312 run.py:483] Algo bellman_ford step 9632 current loss 0.602887, current_train_items 308256.
I0302 19:02:52.299020 22699590365312 run.py:483] Algo bellman_ford step 9633 current loss 0.542989, current_train_items 308288.
I0302 19:02:52.333516 22699590365312 run.py:483] Algo bellman_ford step 9634 current loss 0.736404, current_train_items 308320.
I0302 19:02:52.352960 22699590365312 run.py:483] Algo bellman_ford step 9635 current loss 0.248987, current_train_items 308352.
I0302 19:02:52.368955 22699590365312 run.py:483] Algo bellman_ford step 9636 current loss 0.435025, current_train_items 308384.
I0302 19:02:52.393031 22699590365312 run.py:483] Algo bellman_ford step 9637 current loss 0.599663, current_train_items 308416.
I0302 19:02:52.424895 22699590365312 run.py:483] Algo bellman_ford step 9638 current loss 0.754763, current_train_items 308448.
I0302 19:02:52.458165 22699590365312 run.py:483] Algo bellman_ford step 9639 current loss 0.810904, current_train_items 308480.
I0302 19:02:52.477934 22699590365312 run.py:483] Algo bellman_ford step 9640 current loss 0.213685, current_train_items 308512.
I0302 19:02:52.493592 22699590365312 run.py:483] Algo bellman_ford step 9641 current loss 0.364686, current_train_items 308544.
I0302 19:02:52.517650 22699590365312 run.py:483] Algo bellman_ford step 9642 current loss 0.576519, current_train_items 308576.
I0302 19:02:52.550704 22699590365312 run.py:483] Algo bellman_ford step 9643 current loss 0.660538, current_train_items 308608.
I0302 19:02:52.582268 22699590365312 run.py:483] Algo bellman_ford step 9644 current loss 0.873585, current_train_items 308640.
I0302 19:02:52.601838 22699590365312 run.py:483] Algo bellman_ford step 9645 current loss 0.241716, current_train_items 308672.
I0302 19:02:52.617916 22699590365312 run.py:483] Algo bellman_ford step 9646 current loss 0.473426, current_train_items 308704.
I0302 19:02:52.641075 22699590365312 run.py:483] Algo bellman_ford step 9647 current loss 0.545218, current_train_items 308736.
I0302 19:02:52.673172 22699590365312 run.py:483] Algo bellman_ford step 9648 current loss 0.595158, current_train_items 308768.
I0302 19:02:52.707192 22699590365312 run.py:483] Algo bellman_ford step 9649 current loss 0.717535, current_train_items 308800.
I0302 19:02:52.726470 22699590365312 run.py:483] Algo bellman_ford step 9650 current loss 0.368215, current_train_items 308832.
I0302 19:02:52.734401 22699590365312 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0302 19:02:52.734531 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:02:52.751505 22699590365312 run.py:483] Algo bellman_ford step 9651 current loss 0.415096, current_train_items 308864.
I0302 19:02:52.777390 22699590365312 run.py:483] Algo bellman_ford step 9652 current loss 0.649177, current_train_items 308896.
I0302 19:02:52.809865 22699590365312 run.py:483] Algo bellman_ford step 9653 current loss 0.727325, current_train_items 308928.
I0302 19:02:52.845277 22699590365312 run.py:483] Algo bellman_ford step 9654 current loss 0.749961, current_train_items 308960.
I0302 19:02:52.865409 22699590365312 run.py:483] Algo bellman_ford step 9655 current loss 0.318517, current_train_items 308992.
I0302 19:02:52.881476 22699590365312 run.py:483] Algo bellman_ford step 9656 current loss 0.439473, current_train_items 309024.
I0302 19:02:52.905202 22699590365312 run.py:483] Algo bellman_ford step 9657 current loss 0.494026, current_train_items 309056.
I0302 19:02:52.937782 22699590365312 run.py:483] Algo bellman_ford step 9658 current loss 0.608795, current_train_items 309088.
I0302 19:02:52.969770 22699590365312 run.py:483] Algo bellman_ford step 9659 current loss 0.714398, current_train_items 309120.
I0302 19:02:52.989402 22699590365312 run.py:483] Algo bellman_ford step 9660 current loss 0.269209, current_train_items 309152.
I0302 19:02:53.005890 22699590365312 run.py:483] Algo bellman_ford step 9661 current loss 0.435996, current_train_items 309184.
I0302 19:02:53.029560 22699590365312 run.py:483] Algo bellman_ford step 9662 current loss 0.582212, current_train_items 309216.
I0302 19:02:53.060874 22699590365312 run.py:483] Algo bellman_ford step 9663 current loss 0.560946, current_train_items 309248.
I0302 19:02:53.094749 22699590365312 run.py:483] Algo bellman_ford step 9664 current loss 0.785066, current_train_items 309280.
I0302 19:02:53.114353 22699590365312 run.py:483] Algo bellman_ford step 9665 current loss 0.308176, current_train_items 309312.
I0302 19:02:53.130479 22699590365312 run.py:483] Algo bellman_ford step 9666 current loss 0.418513, current_train_items 309344.
I0302 19:02:53.153523 22699590365312 run.py:483] Algo bellman_ford step 9667 current loss 0.591293, current_train_items 309376.
I0302 19:02:53.184873 22699590365312 run.py:483] Algo bellman_ford step 9668 current loss 0.547516, current_train_items 309408.
I0302 19:02:53.218269 22699590365312 run.py:483] Algo bellman_ford step 9669 current loss 0.713883, current_train_items 309440.
I0302 19:02:53.237941 22699590365312 run.py:483] Algo bellman_ford step 9670 current loss 0.238639, current_train_items 309472.
I0302 19:02:53.253965 22699590365312 run.py:483] Algo bellman_ford step 9671 current loss 0.382199, current_train_items 309504.
I0302 19:02:53.277909 22699590365312 run.py:483] Algo bellman_ford step 9672 current loss 0.487513, current_train_items 309536.
I0302 19:02:53.308966 22699590365312 run.py:483] Algo bellman_ford step 9673 current loss 0.504194, current_train_items 309568.
I0302 19:02:53.342378 22699590365312 run.py:483] Algo bellman_ford step 9674 current loss 0.697764, current_train_items 309600.
I0302 19:02:53.362020 22699590365312 run.py:483] Algo bellman_ford step 9675 current loss 0.235331, current_train_items 309632.
I0302 19:02:53.378556 22699590365312 run.py:483] Algo bellman_ford step 9676 current loss 0.408195, current_train_items 309664.
I0302 19:02:53.402325 22699590365312 run.py:483] Algo bellman_ford step 9677 current loss 0.484792, current_train_items 309696.
I0302 19:02:53.432542 22699590365312 run.py:483] Algo bellman_ford step 9678 current loss 0.533008, current_train_items 309728.
I0302 19:02:53.466350 22699590365312 run.py:483] Algo bellman_ford step 9679 current loss 0.728895, current_train_items 309760.
I0302 19:02:53.486006 22699590365312 run.py:483] Algo bellman_ford step 9680 current loss 0.300929, current_train_items 309792.
I0302 19:02:53.502253 22699590365312 run.py:483] Algo bellman_ford step 9681 current loss 0.398528, current_train_items 309824.
I0302 19:02:53.525849 22699590365312 run.py:483] Algo bellman_ford step 9682 current loss 0.428326, current_train_items 309856.
I0302 19:02:53.556550 22699590365312 run.py:483] Algo bellman_ford step 9683 current loss 0.554186, current_train_items 309888.
I0302 19:02:53.589059 22699590365312 run.py:483] Algo bellman_ford step 9684 current loss 0.578532, current_train_items 309920.
I0302 19:02:53.608658 22699590365312 run.py:483] Algo bellman_ford step 9685 current loss 0.268736, current_train_items 309952.
I0302 19:02:53.624750 22699590365312 run.py:483] Algo bellman_ford step 9686 current loss 0.436278, current_train_items 309984.
I0302 19:02:53.648621 22699590365312 run.py:483] Algo bellman_ford step 9687 current loss 0.570094, current_train_items 310016.
I0302 19:02:53.680002 22699590365312 run.py:483] Algo bellman_ford step 9688 current loss 0.676013, current_train_items 310048.
I0302 19:02:53.713487 22699590365312 run.py:483] Algo bellman_ford step 9689 current loss 0.697244, current_train_items 310080.
I0302 19:02:53.733581 22699590365312 run.py:483] Algo bellman_ford step 9690 current loss 0.270723, current_train_items 310112.
I0302 19:02:53.749289 22699590365312 run.py:483] Algo bellman_ford step 9691 current loss 0.408163, current_train_items 310144.
I0302 19:02:53.772795 22699590365312 run.py:483] Algo bellman_ford step 9692 current loss 0.464653, current_train_items 310176.
I0302 19:02:53.803715 22699590365312 run.py:483] Algo bellman_ford step 9693 current loss 0.582603, current_train_items 310208.
I0302 19:02:53.837557 22699590365312 run.py:483] Algo bellman_ford step 9694 current loss 0.663067, current_train_items 310240.
I0302 19:02:53.856873 22699590365312 run.py:483] Algo bellman_ford step 9695 current loss 0.299617, current_train_items 310272.
I0302 19:02:53.872884 22699590365312 run.py:483] Algo bellman_ford step 9696 current loss 0.472986, current_train_items 310304.
I0302 19:02:53.896037 22699590365312 run.py:483] Algo bellman_ford step 9697 current loss 0.480194, current_train_items 310336.
I0302 19:02:53.930140 22699590365312 run.py:483] Algo bellman_ford step 9698 current loss 0.666090, current_train_items 310368.
I0302 19:02:53.964966 22699590365312 run.py:483] Algo bellman_ford step 9699 current loss 0.668117, current_train_items 310400.
I0302 19:02:53.985013 22699590365312 run.py:483] Algo bellman_ford step 9700 current loss 0.296235, current_train_items 310432.
I0302 19:02:53.992724 22699590365312 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.9443359375, 'score': 0.9443359375, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0302 19:02:53.992830 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.944, val scores are: bellman_ford: 0.944
I0302 19:02:54.009372 22699590365312 run.py:483] Algo bellman_ford step 9701 current loss 0.482902, current_train_items 310464.
I0302 19:02:54.034512 22699590365312 run.py:483] Algo bellman_ford step 9702 current loss 0.556459, current_train_items 310496.
I0302 19:02:54.066976 22699590365312 run.py:483] Algo bellman_ford step 9703 current loss 0.626050, current_train_items 310528.
I0302 19:02:54.100785 22699590365312 run.py:483] Algo bellman_ford step 9704 current loss 0.642641, current_train_items 310560.
I0302 19:02:54.121344 22699590365312 run.py:483] Algo bellman_ford step 9705 current loss 0.237977, current_train_items 310592.
I0302 19:02:54.137247 22699590365312 run.py:483] Algo bellman_ford step 9706 current loss 0.393726, current_train_items 310624.
I0302 19:02:54.162381 22699590365312 run.py:483] Algo bellman_ford step 9707 current loss 0.572833, current_train_items 310656.
I0302 19:02:54.194488 22699590365312 run.py:483] Algo bellman_ford step 9708 current loss 0.609051, current_train_items 310688.
I0302 19:02:54.226189 22699590365312 run.py:483] Algo bellman_ford step 9709 current loss 0.600249, current_train_items 310720.
I0302 19:02:54.245809 22699590365312 run.py:483] Algo bellman_ford step 9710 current loss 0.287321, current_train_items 310752.
I0302 19:02:54.261585 22699590365312 run.py:483] Algo bellman_ford step 9711 current loss 0.426869, current_train_items 310784.
I0302 19:02:54.285697 22699590365312 run.py:483] Algo bellman_ford step 9712 current loss 0.586428, current_train_items 310816.
I0302 19:02:54.317611 22699590365312 run.py:483] Algo bellman_ford step 9713 current loss 0.502891, current_train_items 310848.
I0302 19:02:54.351760 22699590365312 run.py:483] Algo bellman_ford step 9714 current loss 0.650104, current_train_items 310880.
I0302 19:02:54.371375 22699590365312 run.py:483] Algo bellman_ford step 9715 current loss 0.257989, current_train_items 310912.
I0302 19:02:54.387781 22699590365312 run.py:483] Algo bellman_ford step 9716 current loss 0.440096, current_train_items 310944.
I0302 19:02:54.410523 22699590365312 run.py:483] Algo bellman_ford step 9717 current loss 0.559587, current_train_items 310976.
I0302 19:02:54.442367 22699590365312 run.py:483] Algo bellman_ford step 9718 current loss 0.658764, current_train_items 311008.
I0302 19:02:54.473142 22699590365312 run.py:483] Algo bellman_ford step 9719 current loss 0.552431, current_train_items 311040.
I0302 19:02:54.492809 22699590365312 run.py:483] Algo bellman_ford step 9720 current loss 0.273392, current_train_items 311072.
I0302 19:02:54.508383 22699590365312 run.py:483] Algo bellman_ford step 9721 current loss 0.382940, current_train_items 311104.
I0302 19:02:54.531564 22699590365312 run.py:483] Algo bellman_ford step 9722 current loss 0.567618, current_train_items 311136.
I0302 19:02:54.562947 22699590365312 run.py:483] Algo bellman_ford step 9723 current loss 0.642441, current_train_items 311168.
I0302 19:02:54.596237 22699590365312 run.py:483] Algo bellman_ford step 9724 current loss 0.633653, current_train_items 311200.
I0302 19:02:54.615938 22699590365312 run.py:483] Algo bellman_ford step 9725 current loss 0.262548, current_train_items 311232.
I0302 19:02:54.631686 22699590365312 run.py:483] Algo bellman_ford step 9726 current loss 0.452460, current_train_items 311264.
I0302 19:02:54.654321 22699590365312 run.py:483] Algo bellman_ford step 9727 current loss 0.467155, current_train_items 311296.
I0302 19:02:54.685962 22699590365312 run.py:483] Algo bellman_ford step 9728 current loss 0.579428, current_train_items 311328.
I0302 19:02:54.719998 22699590365312 run.py:483] Algo bellman_ford step 9729 current loss 0.731165, current_train_items 311360.
I0302 19:02:54.739662 22699590365312 run.py:483] Algo bellman_ford step 9730 current loss 0.300872, current_train_items 311392.
I0302 19:02:54.755964 22699590365312 run.py:483] Algo bellman_ford step 9731 current loss 0.439121, current_train_items 311424.
I0302 19:02:54.779258 22699590365312 run.py:483] Algo bellman_ford step 9732 current loss 0.564930, current_train_items 311456.
I0302 19:02:54.810331 22699590365312 run.py:483] Algo bellman_ford step 9733 current loss 0.631861, current_train_items 311488.
I0302 19:02:54.842610 22699590365312 run.py:483] Algo bellman_ford step 9734 current loss 0.642688, current_train_items 311520.
I0302 19:02:54.862367 22699590365312 run.py:483] Algo bellman_ford step 9735 current loss 0.267594, current_train_items 311552.
I0302 19:02:54.878321 22699590365312 run.py:483] Algo bellman_ford step 9736 current loss 0.473935, current_train_items 311584.
I0302 19:02:54.901461 22699590365312 run.py:483] Algo bellman_ford step 9737 current loss 0.552196, current_train_items 311616.
I0302 19:02:54.934424 22699590365312 run.py:483] Algo bellman_ford step 9738 current loss 0.600360, current_train_items 311648.
I0302 19:02:54.968380 22699590365312 run.py:483] Algo bellman_ford step 9739 current loss 0.661105, current_train_items 311680.
I0302 19:02:54.987774 22699590365312 run.py:483] Algo bellman_ford step 9740 current loss 0.256288, current_train_items 311712.
I0302 19:02:55.004488 22699590365312 run.py:483] Algo bellman_ford step 9741 current loss 0.398577, current_train_items 311744.
I0302 19:02:55.029589 22699590365312 run.py:483] Algo bellman_ford step 9742 current loss 0.624005, current_train_items 311776.
I0302 19:02:55.061623 22699590365312 run.py:483] Algo bellman_ford step 9743 current loss 0.628799, current_train_items 311808.
I0302 19:02:55.095411 22699590365312 run.py:483] Algo bellman_ford step 9744 current loss 0.661705, current_train_items 311840.
I0302 19:02:55.115119 22699590365312 run.py:483] Algo bellman_ford step 9745 current loss 0.215455, current_train_items 311872.
I0302 19:02:55.131866 22699590365312 run.py:483] Algo bellman_ford step 9746 current loss 0.440070, current_train_items 311904.
I0302 19:02:55.154972 22699590365312 run.py:483] Algo bellman_ford step 9747 current loss 0.494343, current_train_items 311936.
I0302 19:02:55.184458 22699590365312 run.py:483] Algo bellman_ford step 9748 current loss 0.449522, current_train_items 311968.
I0302 19:02:55.216727 22699590365312 run.py:483] Algo bellman_ford step 9749 current loss 0.612657, current_train_items 312000.
I0302 19:02:55.236176 22699590365312 run.py:483] Algo bellman_ford step 9750 current loss 0.254935, current_train_items 312032.
I0302 19:02:55.244379 22699590365312 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.9453125, 'score': 0.9453125, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0302 19:02:55.244484 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.945, val scores are: bellman_ford: 0.945
I0302 19:02:55.260965 22699590365312 run.py:483] Algo bellman_ford step 9751 current loss 0.446798, current_train_items 312064.
I0302 19:02:55.286345 22699590365312 run.py:483] Algo bellman_ford step 9752 current loss 0.566517, current_train_items 312096.
I0302 19:02:55.317686 22699590365312 run.py:483] Algo bellman_ford step 9753 current loss 0.631874, current_train_items 312128.
I0302 19:02:55.350798 22699590365312 run.py:483] Algo bellman_ford step 9754 current loss 0.575826, current_train_items 312160.
I0302 19:02:55.370642 22699590365312 run.py:483] Algo bellman_ford step 9755 current loss 0.320786, current_train_items 312192.
I0302 19:02:55.386945 22699590365312 run.py:483] Algo bellman_ford step 9756 current loss 0.530465, current_train_items 312224.
I0302 19:02:55.409539 22699590365312 run.py:483] Algo bellman_ford step 9757 current loss 0.451436, current_train_items 312256.
I0302 19:02:55.441318 22699590365312 run.py:483] Algo bellman_ford step 9758 current loss 0.634890, current_train_items 312288.
I0302 19:02:55.474247 22699590365312 run.py:483] Algo bellman_ford step 9759 current loss 0.670630, current_train_items 312320.
I0302 19:02:55.493944 22699590365312 run.py:483] Algo bellman_ford step 9760 current loss 0.258878, current_train_items 312352.
I0302 19:02:55.510328 22699590365312 run.py:483] Algo bellman_ford step 9761 current loss 0.445186, current_train_items 312384.
I0302 19:02:55.533942 22699590365312 run.py:483] Algo bellman_ford step 9762 current loss 0.645101, current_train_items 312416.
I0302 19:02:55.565710 22699590365312 run.py:483] Algo bellman_ford step 9763 current loss 0.604159, current_train_items 312448.
I0302 19:02:55.596108 22699590365312 run.py:483] Algo bellman_ford step 9764 current loss 0.643659, current_train_items 312480.
I0302 19:02:55.615299 22699590365312 run.py:483] Algo bellman_ford step 9765 current loss 0.231122, current_train_items 312512.
I0302 19:02:55.630955 22699590365312 run.py:483] Algo bellman_ford step 9766 current loss 0.434214, current_train_items 312544.
I0302 19:02:55.654896 22699590365312 run.py:483] Algo bellman_ford step 9767 current loss 0.665533, current_train_items 312576.
I0302 19:02:55.685720 22699590365312 run.py:483] Algo bellman_ford step 9768 current loss 0.660787, current_train_items 312608.
I0302 19:02:55.719318 22699590365312 run.py:483] Algo bellman_ford step 9769 current loss 0.802348, current_train_items 312640.
I0302 19:02:55.739405 22699590365312 run.py:483] Algo bellman_ford step 9770 current loss 0.252760, current_train_items 312672.
I0302 19:02:55.755140 22699590365312 run.py:483] Algo bellman_ford step 9771 current loss 0.556807, current_train_items 312704.
I0302 19:02:55.777695 22699590365312 run.py:483] Algo bellman_ford step 9772 current loss 0.538814, current_train_items 312736.
I0302 19:02:55.810213 22699590365312 run.py:483] Algo bellman_ford step 9773 current loss 0.627731, current_train_items 312768.
I0302 19:02:55.843321 22699590365312 run.py:483] Algo bellman_ford step 9774 current loss 0.714637, current_train_items 312800.
I0302 19:02:55.862811 22699590365312 run.py:483] Algo bellman_ford step 9775 current loss 0.322662, current_train_items 312832.
I0302 19:02:55.878601 22699590365312 run.py:483] Algo bellman_ford step 9776 current loss 0.419331, current_train_items 312864.
I0302 19:02:55.902459 22699590365312 run.py:483] Algo bellman_ford step 9777 current loss 0.581010, current_train_items 312896.
I0302 19:02:55.934401 22699590365312 run.py:483] Algo bellman_ford step 9778 current loss 0.645750, current_train_items 312928.
I0302 19:02:55.967665 22699590365312 run.py:483] Algo bellman_ford step 9779 current loss 0.674412, current_train_items 312960.
I0302 19:02:55.986846 22699590365312 run.py:483] Algo bellman_ford step 9780 current loss 0.283940, current_train_items 312992.
I0302 19:02:56.003100 22699590365312 run.py:483] Algo bellman_ford step 9781 current loss 0.460725, current_train_items 313024.
I0302 19:02:56.026007 22699590365312 run.py:483] Algo bellman_ford step 9782 current loss 0.534122, current_train_items 313056.
I0302 19:02:56.059191 22699590365312 run.py:483] Algo bellman_ford step 9783 current loss 0.624979, current_train_items 313088.
I0302 19:02:56.090738 22699590365312 run.py:483] Algo bellman_ford step 9784 current loss 0.711163, current_train_items 313120.
I0302 19:02:56.110357 22699590365312 run.py:483] Algo bellman_ford step 9785 current loss 0.236615, current_train_items 313152.
I0302 19:02:56.126747 22699590365312 run.py:483] Algo bellman_ford step 9786 current loss 0.481757, current_train_items 313184.
I0302 19:02:56.149960 22699590365312 run.py:483] Algo bellman_ford step 9787 current loss 0.475137, current_train_items 313216.
I0302 19:02:56.180409 22699590365312 run.py:483] Algo bellman_ford step 9788 current loss 0.576827, current_train_items 313248.
I0302 19:02:56.213793 22699590365312 run.py:483] Algo bellman_ford step 9789 current loss 0.692332, current_train_items 313280.
I0302 19:02:56.233088 22699590365312 run.py:483] Algo bellman_ford step 9790 current loss 0.322712, current_train_items 313312.
I0302 19:02:56.248911 22699590365312 run.py:483] Algo bellman_ford step 9791 current loss 0.388377, current_train_items 313344.
I0302 19:02:56.271788 22699590365312 run.py:483] Algo bellman_ford step 9792 current loss 0.607019, current_train_items 313376.
I0302 19:02:56.303008 22699590365312 run.py:483] Algo bellman_ford step 9793 current loss 0.554708, current_train_items 313408.
I0302 19:02:56.334149 22699590365312 run.py:483] Algo bellman_ford step 9794 current loss 0.621349, current_train_items 313440.
I0302 19:02:56.353489 22699590365312 run.py:483] Algo bellman_ford step 9795 current loss 0.282715, current_train_items 313472.
I0302 19:02:56.369858 22699590365312 run.py:483] Algo bellman_ford step 9796 current loss 0.424594, current_train_items 313504.
I0302 19:02:56.393312 22699590365312 run.py:483] Algo bellman_ford step 9797 current loss 0.542363, current_train_items 313536.
I0302 19:02:56.423990 22699590365312 run.py:483] Algo bellman_ford step 9798 current loss 0.557942, current_train_items 313568.
I0302 19:02:56.456194 22699590365312 run.py:483] Algo bellman_ford step 9799 current loss 0.595793, current_train_items 313600.
I0302 19:02:56.475911 22699590365312 run.py:483] Algo bellman_ford step 9800 current loss 0.348954, current_train_items 313632.
I0302 19:02:56.483520 22699590365312 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.935546875, 'score': 0.935546875, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0302 19:02:56.483623 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.936, val scores are: bellman_ford: 0.936
I0302 19:02:56.500067 22699590365312 run.py:483] Algo bellman_ford step 9801 current loss 0.395404, current_train_items 313664.
I0302 19:02:56.524681 22699590365312 run.py:483] Algo bellman_ford step 9802 current loss 0.606436, current_train_items 313696.
I0302 19:02:56.557541 22699590365312 run.py:483] Algo bellman_ford step 9803 current loss 0.797470, current_train_items 313728.
I0302 19:02:56.590680 22699590365312 run.py:483] Algo bellman_ford step 9804 current loss 0.646096, current_train_items 313760.
I0302 19:02:56.610725 22699590365312 run.py:483] Algo bellman_ford step 9805 current loss 0.351169, current_train_items 313792.
I0302 19:02:56.626784 22699590365312 run.py:483] Algo bellman_ford step 9806 current loss 0.455234, current_train_items 313824.
I0302 19:02:56.649876 22699590365312 run.py:483] Algo bellman_ford step 9807 current loss 0.503247, current_train_items 313856.
I0302 19:02:56.681285 22699590365312 run.py:483] Algo bellman_ford step 9808 current loss 0.586517, current_train_items 313888.
I0302 19:02:56.716873 22699590365312 run.py:483] Algo bellman_ford step 9809 current loss 0.714898, current_train_items 313920.
I0302 19:02:56.736489 22699590365312 run.py:483] Algo bellman_ford step 9810 current loss 0.213195, current_train_items 313952.
I0302 19:02:56.752173 22699590365312 run.py:483] Algo bellman_ford step 9811 current loss 0.402132, current_train_items 313984.
I0302 19:02:56.777031 22699590365312 run.py:483] Algo bellman_ford step 9812 current loss 0.679434, current_train_items 314016.
I0302 19:02:56.806205 22699590365312 run.py:483] Algo bellman_ford step 9813 current loss 0.608656, current_train_items 314048.
I0302 19:02:56.840034 22699590365312 run.py:483] Algo bellman_ford step 9814 current loss 0.641577, current_train_items 314080.
I0302 19:02:56.859342 22699590365312 run.py:483] Algo bellman_ford step 9815 current loss 0.271092, current_train_items 314112.
I0302 19:02:56.875669 22699590365312 run.py:483] Algo bellman_ford step 9816 current loss 0.417750, current_train_items 314144.
I0302 19:02:56.898997 22699590365312 run.py:483] Algo bellman_ford step 9817 current loss 0.519026, current_train_items 314176.
I0302 19:02:56.931590 22699590365312 run.py:483] Algo bellman_ford step 9818 current loss 0.657465, current_train_items 314208.
I0302 19:02:56.964393 22699590365312 run.py:483] Algo bellman_ford step 9819 current loss 0.662649, current_train_items 314240.
I0302 19:02:56.983902 22699590365312 run.py:483] Algo bellman_ford step 9820 current loss 0.307521, current_train_items 314272.
I0302 19:02:56.999762 22699590365312 run.py:483] Algo bellman_ford step 9821 current loss 0.485421, current_train_items 314304.
I0302 19:02:57.023954 22699590365312 run.py:483] Algo bellman_ford step 9822 current loss 0.526973, current_train_items 314336.
I0302 19:02:57.054957 22699590365312 run.py:483] Algo bellman_ford step 9823 current loss 0.503227, current_train_items 314368.
I0302 19:02:57.088106 22699590365312 run.py:483] Algo bellman_ford step 9824 current loss 0.663083, current_train_items 314400.
I0302 19:02:57.107694 22699590365312 run.py:483] Algo bellman_ford step 9825 current loss 0.248231, current_train_items 314432.
I0302 19:02:57.123471 22699590365312 run.py:483] Algo bellman_ford step 9826 current loss 0.388471, current_train_items 314464.
I0302 19:02:57.146813 22699590365312 run.py:483] Algo bellman_ford step 9827 current loss 0.537198, current_train_items 314496.
I0302 19:02:57.177196 22699590365312 run.py:483] Algo bellman_ford step 9828 current loss 0.561194, current_train_items 314528.
I0302 19:02:57.211866 22699590365312 run.py:483] Algo bellman_ford step 9829 current loss 0.695279, current_train_items 314560.
I0302 19:02:57.230958 22699590365312 run.py:483] Algo bellman_ford step 9830 current loss 0.228067, current_train_items 314592.
I0302 19:02:57.247452 22699590365312 run.py:483] Algo bellman_ford step 9831 current loss 0.420802, current_train_items 314624.
I0302 19:02:57.271114 22699590365312 run.py:483] Algo bellman_ford step 9832 current loss 0.531820, current_train_items 314656.
I0302 19:02:57.302894 22699590365312 run.py:483] Algo bellman_ford step 9833 current loss 0.596897, current_train_items 314688.
I0302 19:02:57.334570 22699590365312 run.py:483] Algo bellman_ford step 9834 current loss 0.587693, current_train_items 314720.
I0302 19:02:57.353821 22699590365312 run.py:483] Algo bellman_ford step 9835 current loss 0.247316, current_train_items 314752.
I0302 19:02:57.369805 22699590365312 run.py:483] Algo bellman_ford step 9836 current loss 0.425498, current_train_items 314784.
I0302 19:02:57.393335 22699590365312 run.py:483] Algo bellman_ford step 9837 current loss 0.598185, current_train_items 314816.
I0302 19:02:57.425825 22699590365312 run.py:483] Algo bellman_ford step 9838 current loss 0.614468, current_train_items 314848.
I0302 19:02:57.460838 22699590365312 run.py:483] Algo bellman_ford step 9839 current loss 0.691512, current_train_items 314880.
I0302 19:02:57.480310 22699590365312 run.py:483] Algo bellman_ford step 9840 current loss 0.268498, current_train_items 314912.
I0302 19:02:57.496296 22699590365312 run.py:483] Algo bellman_ford step 9841 current loss 0.417183, current_train_items 314944.
I0302 19:02:57.520137 22699590365312 run.py:483] Algo bellman_ford step 9842 current loss 0.510016, current_train_items 314976.
I0302 19:02:57.551348 22699590365312 run.py:483] Algo bellman_ford step 9843 current loss 0.534321, current_train_items 315008.
I0302 19:02:57.582268 22699590365312 run.py:483] Algo bellman_ford step 9844 current loss 0.563901, current_train_items 315040.
I0302 19:02:57.601803 22699590365312 run.py:483] Algo bellman_ford step 9845 current loss 0.253373, current_train_items 315072.
I0302 19:02:57.618004 22699590365312 run.py:483] Algo bellman_ford step 9846 current loss 0.413094, current_train_items 315104.
I0302 19:02:57.642373 22699590365312 run.py:483] Algo bellman_ford step 9847 current loss 0.571740, current_train_items 315136.
I0302 19:02:57.673703 22699590365312 run.py:483] Algo bellman_ford step 9848 current loss 0.605439, current_train_items 315168.
I0302 19:02:57.707058 22699590365312 run.py:483] Algo bellman_ford step 9849 current loss 0.695149, current_train_items 315200.
I0302 19:02:57.726912 22699590365312 run.py:483] Algo bellman_ford step 9850 current loss 0.254655, current_train_items 315232.
I0302 19:02:57.734932 22699590365312 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.9541015625, 'score': 0.9541015625, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0302 19:02:57.735038 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.954, val scores are: bellman_ford: 0.954
I0302 19:02:57.752480 22699590365312 run.py:483] Algo bellman_ford step 9851 current loss 0.425648, current_train_items 315264.
I0302 19:02:57.778164 22699590365312 run.py:483] Algo bellman_ford step 9852 current loss 0.602138, current_train_items 315296.
I0302 19:02:57.809534 22699590365312 run.py:483] Algo bellman_ford step 9853 current loss 0.531784, current_train_items 315328.
I0302 19:02:57.843920 22699590365312 run.py:483] Algo bellman_ford step 9854 current loss 0.711201, current_train_items 315360.
I0302 19:02:57.863879 22699590365312 run.py:483] Algo bellman_ford step 9855 current loss 0.285691, current_train_items 315392.
I0302 19:02:57.879784 22699590365312 run.py:483] Algo bellman_ford step 9856 current loss 0.412872, current_train_items 315424.
I0302 19:02:57.903999 22699590365312 run.py:483] Algo bellman_ford step 9857 current loss 0.577132, current_train_items 315456.
I0302 19:02:57.935070 22699590365312 run.py:483] Algo bellman_ford step 9858 current loss 0.587245, current_train_items 315488.
I0302 19:02:57.968307 22699590365312 run.py:483] Algo bellman_ford step 9859 current loss 0.692605, current_train_items 315520.
I0302 19:02:57.988363 22699590365312 run.py:483] Algo bellman_ford step 9860 current loss 0.279820, current_train_items 315552.
I0302 19:02:58.004417 22699590365312 run.py:483] Algo bellman_ford step 9861 current loss 0.425734, current_train_items 315584.
I0302 19:02:58.028436 22699590365312 run.py:483] Algo bellman_ford step 9862 current loss 0.665033, current_train_items 315616.
I0302 19:02:58.059916 22699590365312 run.py:483] Algo bellman_ford step 9863 current loss 0.522675, current_train_items 315648.
I0302 19:02:58.093184 22699590365312 run.py:483] Algo bellman_ford step 9864 current loss 0.632114, current_train_items 315680.
I0302 19:02:58.112785 22699590365312 run.py:483] Algo bellman_ford step 9865 current loss 0.252972, current_train_items 315712.
I0302 19:02:58.128620 22699590365312 run.py:483] Algo bellman_ford step 9866 current loss 0.440993, current_train_items 315744.
I0302 19:02:58.152654 22699590365312 run.py:483] Algo bellman_ford step 9867 current loss 0.610923, current_train_items 315776.
I0302 19:02:58.184710 22699590365312 run.py:483] Algo bellman_ford step 9868 current loss 0.679726, current_train_items 315808.
I0302 19:02:58.218102 22699590365312 run.py:483] Algo bellman_ford step 9869 current loss 0.760941, current_train_items 315840.
I0302 19:02:58.238268 22699590365312 run.py:483] Algo bellman_ford step 9870 current loss 0.341438, current_train_items 315872.
I0302 19:02:58.254896 22699590365312 run.py:483] Algo bellman_ford step 9871 current loss 0.448895, current_train_items 315904.
I0302 19:02:58.278698 22699590365312 run.py:483] Algo bellman_ford step 9872 current loss 0.500522, current_train_items 315936.
I0302 19:02:58.310553 22699590365312 run.py:483] Algo bellman_ford step 9873 current loss 0.552084, current_train_items 315968.
I0302 19:02:58.343200 22699590365312 run.py:483] Algo bellman_ford step 9874 current loss 0.619875, current_train_items 316000.
I0302 19:02:58.362913 22699590365312 run.py:483] Algo bellman_ford step 9875 current loss 0.241276, current_train_items 316032.
I0302 19:02:58.378896 22699590365312 run.py:483] Algo bellman_ford step 9876 current loss 0.359111, current_train_items 316064.
I0302 19:02:58.400951 22699590365312 run.py:483] Algo bellman_ford step 9877 current loss 0.536780, current_train_items 316096.
I0302 19:02:58.432013 22699590365312 run.py:483] Algo bellman_ford step 9878 current loss 0.569684, current_train_items 316128.
I0302 19:02:58.464758 22699590365312 run.py:483] Algo bellman_ford step 9879 current loss 0.648601, current_train_items 316160.
I0302 19:02:58.484314 22699590365312 run.py:483] Algo bellman_ford step 9880 current loss 0.246756, current_train_items 316192.
I0302 19:02:58.500478 22699590365312 run.py:483] Algo bellman_ford step 9881 current loss 0.431124, current_train_items 316224.
I0302 19:02:58.524334 22699590365312 run.py:483] Algo bellman_ford step 9882 current loss 0.473427, current_train_items 316256.
I0302 19:02:58.555891 22699590365312 run.py:483] Algo bellman_ford step 9883 current loss 0.662267, current_train_items 316288.
I0302 19:02:58.589221 22699590365312 run.py:483] Algo bellman_ford step 9884 current loss 0.582855, current_train_items 316320.
I0302 19:02:58.609213 22699590365312 run.py:483] Algo bellman_ford step 9885 current loss 0.312765, current_train_items 316352.
I0302 19:02:58.625085 22699590365312 run.py:483] Algo bellman_ford step 9886 current loss 0.401243, current_train_items 316384.
I0302 19:02:58.648811 22699590365312 run.py:483] Algo bellman_ford step 9887 current loss 0.504926, current_train_items 316416.
I0302 19:02:58.679774 22699590365312 run.py:483] Algo bellman_ford step 9888 current loss 0.573540, current_train_items 316448.
I0302 19:02:58.711623 22699590365312 run.py:483] Algo bellman_ford step 9889 current loss 0.618110, current_train_items 316480.
I0302 19:02:58.731380 22699590365312 run.py:483] Algo bellman_ford step 9890 current loss 0.267484, current_train_items 316512.
I0302 19:02:58.747380 22699590365312 run.py:483] Algo bellman_ford step 9891 current loss 0.429730, current_train_items 316544.
I0302 19:02:58.769984 22699590365312 run.py:483] Algo bellman_ford step 9892 current loss 0.518137, current_train_items 316576.
I0302 19:02:58.801953 22699590365312 run.py:483] Algo bellman_ford step 9893 current loss 0.598842, current_train_items 316608.
I0302 19:02:58.836505 22699590365312 run.py:483] Algo bellman_ford step 9894 current loss 0.712249, current_train_items 316640.
I0302 19:02:58.856211 22699590365312 run.py:483] Algo bellman_ford step 9895 current loss 0.246662, current_train_items 316672.
I0302 19:02:58.872046 22699590365312 run.py:483] Algo bellman_ford step 9896 current loss 0.409531, current_train_items 316704.
I0302 19:02:58.896537 22699590365312 run.py:483] Algo bellman_ford step 9897 current loss 0.564402, current_train_items 316736.
I0302 19:02:58.928337 22699590365312 run.py:483] Algo bellman_ford step 9898 current loss 0.588326, current_train_items 316768.
I0302 19:02:58.963358 22699590365312 run.py:483] Algo bellman_ford step 9899 current loss 0.694347, current_train_items 316800.
I0302 19:02:58.983117 22699590365312 run.py:483] Algo bellman_ford step 9900 current loss 0.259740, current_train_items 316832.
I0302 19:02:58.991026 22699590365312 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0302 19:02:58.991131 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:02:59.007865 22699590365312 run.py:483] Algo bellman_ford step 9901 current loss 0.374058, current_train_items 316864.
I0302 19:02:59.032078 22699590365312 run.py:483] Algo bellman_ford step 9902 current loss 0.550341, current_train_items 316896.
I0302 19:02:59.063266 22699590365312 run.py:483] Algo bellman_ford step 9903 current loss 0.541694, current_train_items 316928.
I0302 19:02:59.098150 22699590365312 run.py:483] Algo bellman_ford step 9904 current loss 0.589882, current_train_items 316960.
I0302 19:02:59.118058 22699590365312 run.py:483] Algo bellman_ford step 9905 current loss 0.239145, current_train_items 316992.
I0302 19:02:59.134278 22699590365312 run.py:483] Algo bellman_ford step 9906 current loss 0.426423, current_train_items 317024.
I0302 19:02:59.158609 22699590365312 run.py:483] Algo bellman_ford step 9907 current loss 0.655412, current_train_items 317056.
I0302 19:02:59.190032 22699590365312 run.py:483] Algo bellman_ford step 9908 current loss 0.696894, current_train_items 317088.
I0302 19:02:59.221599 22699590365312 run.py:483] Algo bellman_ford step 9909 current loss 0.728412, current_train_items 317120.
I0302 19:02:59.241022 22699590365312 run.py:483] Algo bellman_ford step 9910 current loss 0.258549, current_train_items 317152.
I0302 19:02:59.257376 22699590365312 run.py:483] Algo bellman_ford step 9911 current loss 0.481316, current_train_items 317184.
I0302 19:02:59.281282 22699590365312 run.py:483] Algo bellman_ford step 9912 current loss 0.608449, current_train_items 317216.
I0302 19:02:59.313501 22699590365312 run.py:483] Algo bellman_ford step 9913 current loss 0.609122, current_train_items 317248.
I0302 19:02:59.346767 22699590365312 run.py:483] Algo bellman_ford step 9914 current loss 0.688162, current_train_items 317280.
I0302 19:02:59.366384 22699590365312 run.py:483] Algo bellman_ford step 9915 current loss 0.270868, current_train_items 317312.
I0302 19:02:59.382678 22699590365312 run.py:483] Algo bellman_ford step 9916 current loss 0.420546, current_train_items 317344.
I0302 19:02:59.406723 22699590365312 run.py:483] Algo bellman_ford step 9917 current loss 0.530703, current_train_items 317376.
I0302 19:02:59.438493 22699590365312 run.py:483] Algo bellman_ford step 9918 current loss 0.627231, current_train_items 317408.
I0302 19:02:59.471504 22699590365312 run.py:483] Algo bellman_ford step 9919 current loss 0.629658, current_train_items 317440.
I0302 19:02:59.491141 22699590365312 run.py:483] Algo bellman_ford step 9920 current loss 0.278916, current_train_items 317472.
I0302 19:02:59.507645 22699590365312 run.py:483] Algo bellman_ford step 9921 current loss 0.469442, current_train_items 317504.
I0302 19:02:59.531507 22699590365312 run.py:483] Algo bellman_ford step 9922 current loss 0.585079, current_train_items 317536.
I0302 19:02:59.561374 22699590365312 run.py:483] Algo bellman_ford step 9923 current loss 0.574850, current_train_items 317568.
I0302 19:02:59.597168 22699590365312 run.py:483] Algo bellman_ford step 9924 current loss 0.708296, current_train_items 317600.
I0302 19:02:59.616352 22699590365312 run.py:483] Algo bellman_ford step 9925 current loss 0.266097, current_train_items 317632.
I0302 19:02:59.632232 22699590365312 run.py:483] Algo bellman_ford step 9926 current loss 0.419897, current_train_items 317664.
I0302 19:02:59.656511 22699590365312 run.py:483] Algo bellman_ford step 9927 current loss 0.507308, current_train_items 317696.
I0302 19:02:59.689361 22699590365312 run.py:483] Algo bellman_ford step 9928 current loss 0.572924, current_train_items 317728.
I0302 19:02:59.720390 22699590365312 run.py:483] Algo bellman_ford step 9929 current loss 0.639297, current_train_items 317760.
I0302 19:02:59.739733 22699590365312 run.py:483] Algo bellman_ford step 9930 current loss 0.232556, current_train_items 317792.
I0302 19:02:59.755129 22699590365312 run.py:483] Algo bellman_ford step 9931 current loss 0.353630, current_train_items 317824.
I0302 19:02:59.778931 22699590365312 run.py:483] Algo bellman_ford step 9932 current loss 0.544937, current_train_items 317856.
I0302 19:02:59.809787 22699590365312 run.py:483] Algo bellman_ford step 9933 current loss 0.663451, current_train_items 317888.
I0302 19:02:59.841714 22699590365312 run.py:483] Algo bellman_ford step 9934 current loss 0.715183, current_train_items 317920.
I0302 19:02:59.861060 22699590365312 run.py:483] Algo bellman_ford step 9935 current loss 0.243197, current_train_items 317952.
I0302 19:02:59.876797 22699590365312 run.py:483] Algo bellman_ford step 9936 current loss 0.453971, current_train_items 317984.
I0302 19:02:59.900089 22699590365312 run.py:483] Algo bellman_ford step 9937 current loss 0.529688, current_train_items 318016.
I0302 19:02:59.930981 22699590365312 run.py:483] Algo bellman_ford step 9938 current loss 0.664369, current_train_items 318048.
I0302 19:02:59.964757 22699590365312 run.py:483] Algo bellman_ford step 9939 current loss 0.820309, current_train_items 318080.
I0302 19:02:59.984347 22699590365312 run.py:483] Algo bellman_ford step 9940 current loss 0.296387, current_train_items 318112.
I0302 19:03:00.000011 22699590365312 run.py:483] Algo bellman_ford step 9941 current loss 0.359644, current_train_items 318144.
I0302 19:03:00.023183 22699590365312 run.py:483] Algo bellman_ford step 9942 current loss 0.560740, current_train_items 318176.
I0302 19:03:00.054913 22699590365312 run.py:483] Algo bellman_ford step 9943 current loss 0.552096, current_train_items 318208.
I0302 19:03:00.086856 22699590365312 run.py:483] Algo bellman_ford step 9944 current loss 0.687102, current_train_items 318240.
I0302 19:03:00.106341 22699590365312 run.py:483] Algo bellman_ford step 9945 current loss 0.257419, current_train_items 318272.
I0302 19:03:00.121926 22699590365312 run.py:483] Algo bellman_ford step 9946 current loss 0.460257, current_train_items 318304.
I0302 19:03:00.145874 22699590365312 run.py:483] Algo bellman_ford step 9947 current loss 0.524121, current_train_items 318336.
I0302 19:03:00.177723 22699590365312 run.py:483] Algo bellman_ford step 9948 current loss 0.599757, current_train_items 318368.
I0302 19:03:00.211826 22699590365312 run.py:483] Algo bellman_ford step 9949 current loss 0.722172, current_train_items 318400.
I0302 19:03:00.231363 22699590365312 run.py:483] Algo bellman_ford step 9950 current loss 0.234919, current_train_items 318432.
I0302 19:03:00.239514 22699590365312 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0302 19:03:00.239619 22699590365312 run.py:522] Not saving new best model, best avg val score was 0.959, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:03:00.256708 22699590365312 run.py:483] Algo bellman_ford step 9951 current loss 0.415430, current_train_items 318464.
I0302 19:03:00.280779 22699590365312 run.py:483] Algo bellman_ford step 9952 current loss 0.498361, current_train_items 318496.
I0302 19:03:00.313564 22699590365312 run.py:483] Algo bellman_ford step 9953 current loss 0.661223, current_train_items 318528.
I0302 19:03:00.348612 22699590365312 run.py:483] Algo bellman_ford step 9954 current loss 0.588734, current_train_items 318560.
I0302 19:03:00.368669 22699590365312 run.py:483] Algo bellman_ford step 9955 current loss 0.295422, current_train_items 318592.
I0302 19:03:00.384913 22699590365312 run.py:483] Algo bellman_ford step 9956 current loss 0.441662, current_train_items 318624.
I0302 19:03:00.408711 22699590365312 run.py:483] Algo bellman_ford step 9957 current loss 0.571932, current_train_items 318656.
I0302 19:03:00.440776 22699590365312 run.py:483] Algo bellman_ford step 9958 current loss 0.617281, current_train_items 318688.
I0302 19:03:00.475129 22699590365312 run.py:483] Algo bellman_ford step 9959 current loss 0.711144, current_train_items 318720.
I0302 19:03:00.494794 22699590365312 run.py:483] Algo bellman_ford step 9960 current loss 0.334929, current_train_items 318752.
I0302 19:03:00.510349 22699590365312 run.py:483] Algo bellman_ford step 9961 current loss 0.447165, current_train_items 318784.
I0302 19:03:00.534612 22699590365312 run.py:483] Algo bellman_ford step 9962 current loss 0.586717, current_train_items 318816.
I0302 19:03:00.566816 22699590365312 run.py:483] Algo bellman_ford step 9963 current loss 0.598494, current_train_items 318848.
I0302 19:03:00.599898 22699590365312 run.py:483] Algo bellman_ford step 9964 current loss 0.656643, current_train_items 318880.
I0302 19:03:00.619062 22699590365312 run.py:483] Algo bellman_ford step 9965 current loss 0.276416, current_train_items 318912.
I0302 19:03:00.635381 22699590365312 run.py:483] Algo bellman_ford step 9966 current loss 0.483639, current_train_items 318944.
I0302 19:03:00.659585 22699590365312 run.py:483] Algo bellman_ford step 9967 current loss 0.585233, current_train_items 318976.
I0302 19:03:00.690714 22699590365312 run.py:483] Algo bellman_ford step 9968 current loss 0.669843, current_train_items 319008.
I0302 19:03:00.724112 22699590365312 run.py:483] Algo bellman_ford step 9969 current loss 0.634625, current_train_items 319040.
I0302 19:03:00.743789 22699590365312 run.py:483] Algo bellman_ford step 9970 current loss 0.248510, current_train_items 319072.
I0302 19:03:00.759709 22699590365312 run.py:483] Algo bellman_ford step 9971 current loss 0.454746, current_train_items 319104.
I0302 19:03:00.782687 22699590365312 run.py:483] Algo bellman_ford step 9972 current loss 0.613900, current_train_items 319136.
I0302 19:03:00.813091 22699590365312 run.py:483] Algo bellman_ford step 9973 current loss 0.650159, current_train_items 319168.
I0302 19:03:00.846446 22699590365312 run.py:483] Algo bellman_ford step 9974 current loss 0.642273, current_train_items 319200.
I0302 19:03:00.865903 22699590365312 run.py:483] Algo bellman_ford step 9975 current loss 0.282612, current_train_items 319232.
I0302 19:03:00.882291 22699590365312 run.py:483] Algo bellman_ford step 9976 current loss 0.406354, current_train_items 319264.
I0302 19:03:00.905853 22699590365312 run.py:483] Algo bellman_ford step 9977 current loss 0.501711, current_train_items 319296.
I0302 19:03:00.937215 22699590365312 run.py:483] Algo bellman_ford step 9978 current loss 0.575462, current_train_items 319328.
I0302 19:03:00.969163 22699590365312 run.py:483] Algo bellman_ford step 9979 current loss 0.624918, current_train_items 319360.
I0302 19:03:00.988528 22699590365312 run.py:483] Algo bellman_ford step 9980 current loss 0.276997, current_train_items 319392.
I0302 19:03:01.004478 22699590365312 run.py:483] Algo bellman_ford step 9981 current loss 0.438547, current_train_items 319424.
I0302 19:03:01.028733 22699590365312 run.py:483] Algo bellman_ford step 9982 current loss 0.518285, current_train_items 319456.
I0302 19:03:01.061573 22699590365312 run.py:483] Algo bellman_ford step 9983 current loss 0.685451, current_train_items 319488.
I0302 19:03:01.095446 22699590365312 run.py:483] Algo bellman_ford step 9984 current loss 0.725633, current_train_items 319520.
I0302 19:03:01.115016 22699590365312 run.py:483] Algo bellman_ford step 9985 current loss 0.267617, current_train_items 319552.
I0302 19:03:01.130757 22699590365312 run.py:483] Algo bellman_ford step 9986 current loss 0.339988, current_train_items 319584.
I0302 19:03:01.154018 22699590365312 run.py:483] Algo bellman_ford step 9987 current loss 0.567132, current_train_items 319616.
I0302 19:03:01.185678 22699590365312 run.py:483] Algo bellman_ford step 9988 current loss 0.599602, current_train_items 319648.
I0302 19:03:01.218434 22699590365312 run.py:483] Algo bellman_ford step 9989 current loss 0.620170, current_train_items 319680.
I0302 19:03:01.237902 22699590365312 run.py:483] Algo bellman_ford step 9990 current loss 0.239200, current_train_items 319712.
I0302 19:03:01.254034 22699590365312 run.py:483] Algo bellman_ford step 9991 current loss 0.410374, current_train_items 319744.
I0302 19:03:01.277690 22699590365312 run.py:483] Algo bellman_ford step 9992 current loss 0.627151, current_train_items 319776.
I0302 19:03:01.308078 22699590365312 run.py:483] Algo bellman_ford step 9993 current loss 0.625642, current_train_items 319808.
I0302 19:03:01.340388 22699590365312 run.py:483] Algo bellman_ford step 9994 current loss 0.596463, current_train_items 319840.
I0302 19:03:01.359834 22699590365312 run.py:483] Algo bellman_ford step 9995 current loss 0.284523, current_train_items 319872.
I0302 19:03:01.376032 22699590365312 run.py:483] Algo bellman_ford step 9996 current loss 0.432213, current_train_items 319904.
I0302 19:03:01.399938 22699590365312 run.py:483] Algo bellman_ford step 9997 current loss 0.480575, current_train_items 319936.
I0302 19:03:01.430970 22699590365312 run.py:483] Algo bellman_ford step 9998 current loss 0.589668, current_train_items 319968.
I0302 19:03:01.461984 22699590365312 run.py:483] Algo bellman_ford step 9999 current loss 0.664562, current_train_items 320000.
I0302 19:03:01.467877 22699590365312 run.py:527] Restoring best model from checkpoint...
I0302 19:03:03.890007 22699590365312 run.py:542] (test) algo bellman_ford : {'pi': 0.22119140625, 'score': 0.22119140625, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0302 19:03:03.890290 22699590365312 run.py:544] Done!

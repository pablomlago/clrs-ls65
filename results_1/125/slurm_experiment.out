Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-04 19:24:58.439586: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-04 19:24:58.439855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-04 19:24:58.482587: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-04 19:25:19.202642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0304 19:26:25.666312 22849303695488 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0304 19:26:25.672388 22849303695488 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0304 19:26:27.030812 22849303695488 run.py:307] Creating samplers for algo bellman_ford
W0304 19:26:27.031243 22849303695488 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.031528 22849303695488 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.248089 22849303695488 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.248348 22849303695488 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.499852 22849303695488 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.500105 22849303695488 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.847702 22849303695488 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.847942 22849303695488 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:28.272022 22849303695488 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:28.272260 22849303695488 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:28.812864 22849303695488 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0304 19:26:28.813119 22849303695488 samplers.py:112] Creating a dataset with 64 samples.
I0304 19:26:28.852060 22849303695488 run.py:166] Dataset not found in ./datasets_1/125/CLRS30_v1.0.0. Downloading...
I0304 19:26:45.422592 22849303695488 dataset_info.py:482] Load dataset info from ./datasets_1/125/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:26:45.425237 22849303695488 dataset_info.py:482] Load dataset info from ./datasets_1/125/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:26:45.426068 22849303695488 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/125/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0304 19:26:45.426154 22849303695488 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/125/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:27:01.328711 22849303695488 run.py:483] Algo bellman_ford step 0 current loss 4.292734, current_train_items 32.
I0304 19:27:04.332092 22849303695488 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.546875, 'score': 0.546875, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0304 19:27:04.332309 22849303695488 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.547, val scores are: bellman_ford: 0.547
I0304 19:27:14.633704 22849303695488 run.py:483] Algo bellman_ford step 1 current loss 4.246543, current_train_items 64.
I0304 19:27:25.676636 22849303695488 run.py:483] Algo bellman_ford step 2 current loss 4.206859, current_train_items 96.
I0304 19:27:36.741962 22849303695488 run.py:483] Algo bellman_ford step 3 current loss 3.747272, current_train_items 128.
I0304 19:27:46.693516 22849303695488 run.py:483] Algo bellman_ford step 4 current loss 3.826755, current_train_items 160.
I0304 19:27:46.712378 22849303695488 run.py:483] Algo bellman_ford step 5 current loss 1.278220, current_train_items 192.
I0304 19:27:46.729645 22849303695488 run.py:483] Algo bellman_ford step 6 current loss 1.935434, current_train_items 224.
I0304 19:27:46.752802 22849303695488 run.py:483] Algo bellman_ford step 7 current loss 2.334599, current_train_items 256.
I0304 19:27:46.783017 22849303695488 run.py:483] Algo bellman_ford step 8 current loss 2.675277, current_train_items 288.
I0304 19:27:46.815166 22849303695488 run.py:483] Algo bellman_ford step 9 current loss 2.843255, current_train_items 320.
I0304 19:27:46.833601 22849303695488 run.py:483] Algo bellman_ford step 10 current loss 1.200334, current_train_items 352.
I0304 19:27:46.850754 22849303695488 run.py:483] Algo bellman_ford step 11 current loss 1.691906, current_train_items 384.
I0304 19:27:46.873457 22849303695488 run.py:483] Algo bellman_ford step 12 current loss 1.768509, current_train_items 416.
I0304 19:27:46.904966 22849303695488 run.py:483] Algo bellman_ford step 13 current loss 2.218847, current_train_items 448.
I0304 19:27:46.933232 22849303695488 run.py:483] Algo bellman_ford step 14 current loss 1.977714, current_train_items 480.
I0304 19:27:46.951970 22849303695488 run.py:483] Algo bellman_ford step 15 current loss 0.895941, current_train_items 512.
I0304 19:27:46.968466 22849303695488 run.py:483] Algo bellman_ford step 16 current loss 1.238477, current_train_items 544.
I0304 19:27:46.994138 22849303695488 run.py:483] Algo bellman_ford step 17 current loss 1.976120, current_train_items 576.
I0304 19:27:47.023890 22849303695488 run.py:483] Algo bellman_ford step 18 current loss 1.754141, current_train_items 608.
I0304 19:27:47.056751 22849303695488 run.py:483] Algo bellman_ford step 19 current loss 1.979749, current_train_items 640.
I0304 19:27:47.074971 22849303695488 run.py:483] Algo bellman_ford step 20 current loss 0.651147, current_train_items 672.
I0304 19:27:47.091267 22849303695488 run.py:483] Algo bellman_ford step 21 current loss 0.901662, current_train_items 704.
I0304 19:27:47.115249 22849303695488 run.py:483] Algo bellman_ford step 22 current loss 1.446321, current_train_items 736.
I0304 19:27:47.145123 22849303695488 run.py:483] Algo bellman_ford step 23 current loss 1.515113, current_train_items 768.
I0304 19:27:47.176311 22849303695488 run.py:483] Algo bellman_ford step 24 current loss 1.860577, current_train_items 800.
I0304 19:27:47.194491 22849303695488 run.py:483] Algo bellman_ford step 25 current loss 0.600503, current_train_items 832.
I0304 19:27:47.211061 22849303695488 run.py:483] Algo bellman_ford step 26 current loss 0.939300, current_train_items 864.
I0304 19:27:47.235470 22849303695488 run.py:483] Algo bellman_ford step 27 current loss 1.271853, current_train_items 896.
I0304 19:27:47.266366 22849303695488 run.py:483] Algo bellman_ford step 28 current loss 1.356693, current_train_items 928.
I0304 19:27:47.299557 22849303695488 run.py:483] Algo bellman_ford step 29 current loss 1.559139, current_train_items 960.
I0304 19:27:47.318297 22849303695488 run.py:483] Algo bellman_ford step 30 current loss 0.475312, current_train_items 992.
I0304 19:27:47.334027 22849303695488 run.py:483] Algo bellman_ford step 31 current loss 0.711652, current_train_items 1024.
I0304 19:27:47.357104 22849303695488 run.py:483] Algo bellman_ford step 32 current loss 1.105912, current_train_items 1056.
I0304 19:27:47.388125 22849303695488 run.py:483] Algo bellman_ford step 33 current loss 1.251308, current_train_items 1088.
I0304 19:27:47.420107 22849303695488 run.py:483] Algo bellman_ford step 34 current loss 1.349039, current_train_items 1120.
I0304 19:27:47.438140 22849303695488 run.py:483] Algo bellman_ford step 35 current loss 0.494249, current_train_items 1152.
I0304 19:27:47.454221 22849303695488 run.py:483] Algo bellman_ford step 36 current loss 0.585812, current_train_items 1184.
I0304 19:27:47.478257 22849303695488 run.py:483] Algo bellman_ford step 37 current loss 1.003808, current_train_items 1216.
I0304 19:27:47.508649 22849303695488 run.py:483] Algo bellman_ford step 38 current loss 1.032201, current_train_items 1248.
W0304 19:27:47.531936 22849303695488 samplers.py:155] Increasing hint lengh from 9 to 11
I0304 19:27:54.313480 22849303695488 run.py:483] Algo bellman_ford step 39 current loss 1.607847, current_train_items 1280.
I0304 19:27:54.333981 22849303695488 run.py:483] Algo bellman_ford step 40 current loss 0.442972, current_train_items 1312.
I0304 19:27:54.351109 22849303695488 run.py:483] Algo bellman_ford step 41 current loss 0.640289, current_train_items 1344.
I0304 19:27:54.374371 22849303695488 run.py:483] Algo bellman_ford step 42 current loss 0.880536, current_train_items 1376.
I0304 19:27:54.405468 22849303695488 run.py:483] Algo bellman_ford step 43 current loss 1.025052, current_train_items 1408.
I0304 19:27:54.438057 22849303695488 run.py:483] Algo bellman_ford step 44 current loss 1.220905, current_train_items 1440.
I0304 19:27:54.457658 22849303695488 run.py:483] Algo bellman_ford step 45 current loss 0.342462, current_train_items 1472.
I0304 19:27:54.474490 22849303695488 run.py:483] Algo bellman_ford step 46 current loss 0.637171, current_train_items 1504.
I0304 19:27:54.497287 22849303695488 run.py:483] Algo bellman_ford step 47 current loss 0.804554, current_train_items 1536.
I0304 19:27:54.525042 22849303695488 run.py:483] Algo bellman_ford step 48 current loss 0.778430, current_train_items 1568.
I0304 19:27:54.554504 22849303695488 run.py:483] Algo bellman_ford step 49 current loss 1.013423, current_train_items 1600.
I0304 19:27:54.573296 22849303695488 run.py:483] Algo bellman_ford step 50 current loss 0.293964, current_train_items 1632.
I0304 19:27:54.583289 22849303695488 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.8388671875, 'score': 0.8388671875, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0304 19:27:54.583404 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.547, current avg val score is 0.839, val scores are: bellman_ford: 0.839
I0304 19:27:54.613137 22849303695488 run.py:483] Algo bellman_ford step 51 current loss 0.542807, current_train_items 1664.
I0304 19:27:54.637191 22849303695488 run.py:483] Algo bellman_ford step 52 current loss 1.127070, current_train_items 1696.
I0304 19:27:54.666849 22849303695488 run.py:483] Algo bellman_ford step 53 current loss 0.946902, current_train_items 1728.
I0304 19:27:54.699618 22849303695488 run.py:483] Algo bellman_ford step 54 current loss 1.116915, current_train_items 1760.
I0304 19:27:54.719511 22849303695488 run.py:483] Algo bellman_ford step 55 current loss 0.369447, current_train_items 1792.
I0304 19:27:54.735923 22849303695488 run.py:483] Algo bellman_ford step 56 current loss 0.507088, current_train_items 1824.
I0304 19:27:54.759191 22849303695488 run.py:483] Algo bellman_ford step 57 current loss 0.917020, current_train_items 1856.
I0304 19:27:54.787688 22849303695488 run.py:483] Algo bellman_ford step 58 current loss 0.697020, current_train_items 1888.
I0304 19:27:54.820875 22849303695488 run.py:483] Algo bellman_ford step 59 current loss 1.065474, current_train_items 1920.
I0304 19:27:54.839979 22849303695488 run.py:483] Algo bellman_ford step 60 current loss 0.255390, current_train_items 1952.
W0304 19:27:54.849379 22849303695488 samplers.py:155] Increasing hint lengh from 6 to 7
I0304 19:28:01.634074 22849303695488 run.py:483] Algo bellman_ford step 61 current loss 0.530864, current_train_items 1984.
I0304 19:28:01.659469 22849303695488 run.py:483] Algo bellman_ford step 62 current loss 0.769940, current_train_items 2016.
I0304 19:28:01.689384 22849303695488 run.py:483] Algo bellman_ford step 63 current loss 0.902295, current_train_items 2048.
I0304 19:28:01.723740 22849303695488 run.py:483] Algo bellman_ford step 64 current loss 1.095499, current_train_items 2080.
I0304 19:28:01.743955 22849303695488 run.py:483] Algo bellman_ford step 65 current loss 0.286894, current_train_items 2112.
I0304 19:28:01.760432 22849303695488 run.py:483] Algo bellman_ford step 66 current loss 0.444503, current_train_items 2144.
I0304 19:28:01.785667 22849303695488 run.py:483] Algo bellman_ford step 67 current loss 0.905764, current_train_items 2176.
I0304 19:28:01.814808 22849303695488 run.py:483] Algo bellman_ford step 68 current loss 0.724899, current_train_items 2208.
I0304 19:28:01.847215 22849303695488 run.py:483] Algo bellman_ford step 69 current loss 0.896732, current_train_items 2240.
I0304 19:28:01.866385 22849303695488 run.py:483] Algo bellman_ford step 70 current loss 0.267618, current_train_items 2272.
I0304 19:28:01.883102 22849303695488 run.py:483] Algo bellman_ford step 71 current loss 0.426011, current_train_items 2304.
I0304 19:28:01.906957 22849303695488 run.py:483] Algo bellman_ford step 72 current loss 0.725199, current_train_items 2336.
I0304 19:28:01.937217 22849303695488 run.py:483] Algo bellman_ford step 73 current loss 0.767664, current_train_items 2368.
I0304 19:28:01.970358 22849303695488 run.py:483] Algo bellman_ford step 74 current loss 0.925925, current_train_items 2400.
I0304 19:28:01.989250 22849303695488 run.py:483] Algo bellman_ford step 75 current loss 0.173987, current_train_items 2432.
I0304 19:28:02.006433 22849303695488 run.py:483] Algo bellman_ford step 76 current loss 0.442350, current_train_items 2464.
I0304 19:28:02.030079 22849303695488 run.py:483] Algo bellman_ford step 77 current loss 0.680421, current_train_items 2496.
I0304 19:28:02.059350 22849303695488 run.py:483] Algo bellman_ford step 78 current loss 0.699883, current_train_items 2528.
I0304 19:28:02.089522 22849303695488 run.py:483] Algo bellman_ford step 79 current loss 0.764654, current_train_items 2560.
I0304 19:28:02.109090 22849303695488 run.py:483] Algo bellman_ford step 80 current loss 0.225036, current_train_items 2592.
I0304 19:28:02.125599 22849303695488 run.py:483] Algo bellman_ford step 81 current loss 0.426899, current_train_items 2624.
I0304 19:28:02.149488 22849303695488 run.py:483] Algo bellman_ford step 82 current loss 0.791237, current_train_items 2656.
I0304 19:28:02.178924 22849303695488 run.py:483] Algo bellman_ford step 83 current loss 0.696809, current_train_items 2688.
I0304 19:28:02.209792 22849303695488 run.py:483] Algo bellman_ford step 84 current loss 0.750584, current_train_items 2720.
I0304 19:28:02.228940 22849303695488 run.py:483] Algo bellman_ford step 85 current loss 0.248526, current_train_items 2752.
I0304 19:28:02.245604 22849303695488 run.py:483] Algo bellman_ford step 86 current loss 0.417893, current_train_items 2784.
I0304 19:28:02.270545 22849303695488 run.py:483] Algo bellman_ford step 87 current loss 0.765549, current_train_items 2816.
I0304 19:28:02.300638 22849303695488 run.py:483] Algo bellman_ford step 88 current loss 0.656774, current_train_items 2848.
I0304 19:28:02.333329 22849303695488 run.py:483] Algo bellman_ford step 89 current loss 0.856623, current_train_items 2880.
I0304 19:28:02.352600 22849303695488 run.py:483] Algo bellman_ford step 90 current loss 0.246677, current_train_items 2912.
I0304 19:28:02.369238 22849303695488 run.py:483] Algo bellman_ford step 91 current loss 0.467480, current_train_items 2944.
I0304 19:28:02.393085 22849303695488 run.py:483] Algo bellman_ford step 92 current loss 0.554612, current_train_items 2976.
I0304 19:28:02.424023 22849303695488 run.py:483] Algo bellman_ford step 93 current loss 0.738145, current_train_items 3008.
I0304 19:28:02.455595 22849303695488 run.py:483] Algo bellman_ford step 94 current loss 0.734158, current_train_items 3040.
I0304 19:28:02.475399 22849303695488 run.py:483] Algo bellman_ford step 95 current loss 0.196944, current_train_items 3072.
I0304 19:28:02.491735 22849303695488 run.py:483] Algo bellman_ford step 96 current loss 0.346327, current_train_items 3104.
I0304 19:28:02.515738 22849303695488 run.py:483] Algo bellman_ford step 97 current loss 0.524656, current_train_items 3136.
I0304 19:28:02.545589 22849303695488 run.py:483] Algo bellman_ford step 98 current loss 0.651063, current_train_items 3168.
I0304 19:28:02.577996 22849303695488 run.py:483] Algo bellman_ford step 99 current loss 0.849575, current_train_items 3200.
I0304 19:28:02.597228 22849303695488 run.py:483] Algo bellman_ford step 100 current loss 0.246333, current_train_items 3232.
I0304 19:28:02.607158 22849303695488 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.8671875, 'score': 0.8671875, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0304 19:28:02.607271 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.839, current avg val score is 0.867, val scores are: bellman_ford: 0.867
I0304 19:28:02.637557 22849303695488 run.py:483] Algo bellman_ford step 101 current loss 0.454724, current_train_items 3264.
I0304 19:28:02.661271 22849303695488 run.py:483] Algo bellman_ford step 102 current loss 0.498802, current_train_items 3296.
I0304 19:28:02.691550 22849303695488 run.py:483] Algo bellman_ford step 103 current loss 0.583101, current_train_items 3328.
I0304 19:28:02.726158 22849303695488 run.py:483] Algo bellman_ford step 104 current loss 0.855057, current_train_items 3360.
I0304 19:28:02.745319 22849303695488 run.py:483] Algo bellman_ford step 105 current loss 0.167498, current_train_items 3392.
I0304 19:28:02.761999 22849303695488 run.py:483] Algo bellman_ford step 106 current loss 0.341480, current_train_items 3424.
I0304 19:28:02.786207 22849303695488 run.py:483] Algo bellman_ford step 107 current loss 0.510388, current_train_items 3456.
I0304 19:28:02.815309 22849303695488 run.py:483] Algo bellman_ford step 108 current loss 0.586039, current_train_items 3488.
I0304 19:28:02.846480 22849303695488 run.py:483] Algo bellman_ford step 109 current loss 0.614030, current_train_items 3520.
I0304 19:28:02.865586 22849303695488 run.py:483] Algo bellman_ford step 110 current loss 0.208284, current_train_items 3552.
I0304 19:28:02.882220 22849303695488 run.py:483] Algo bellman_ford step 111 current loss 0.362348, current_train_items 3584.
I0304 19:28:02.905393 22849303695488 run.py:483] Algo bellman_ford step 112 current loss 0.425071, current_train_items 3616.
I0304 19:28:02.935454 22849303695488 run.py:483] Algo bellman_ford step 113 current loss 0.621999, current_train_items 3648.
I0304 19:28:02.966613 22849303695488 run.py:483] Algo bellman_ford step 114 current loss 0.728680, current_train_items 3680.
I0304 19:28:02.985442 22849303695488 run.py:483] Algo bellman_ford step 115 current loss 0.166478, current_train_items 3712.
I0304 19:28:03.002251 22849303695488 run.py:483] Algo bellman_ford step 116 current loss 0.338524, current_train_items 3744.
I0304 19:28:03.026831 22849303695488 run.py:483] Algo bellman_ford step 117 current loss 0.683127, current_train_items 3776.
I0304 19:28:03.056057 22849303695488 run.py:483] Algo bellman_ford step 118 current loss 0.586210, current_train_items 3808.
I0304 19:28:03.085842 22849303695488 run.py:483] Algo bellman_ford step 119 current loss 0.509739, current_train_items 3840.
I0304 19:28:03.104716 22849303695488 run.py:483] Algo bellman_ford step 120 current loss 0.162573, current_train_items 3872.
I0304 19:28:03.121571 22849303695488 run.py:483] Algo bellman_ford step 121 current loss 0.337753, current_train_items 3904.
I0304 19:28:03.145722 22849303695488 run.py:483] Algo bellman_ford step 122 current loss 0.498208, current_train_items 3936.
I0304 19:28:03.176436 22849303695488 run.py:483] Algo bellman_ford step 123 current loss 0.547970, current_train_items 3968.
I0304 19:28:03.212117 22849303695488 run.py:483] Algo bellman_ford step 124 current loss 0.895534, current_train_items 4000.
I0304 19:28:03.231096 22849303695488 run.py:483] Algo bellman_ford step 125 current loss 0.215404, current_train_items 4032.
I0304 19:28:03.247941 22849303695488 run.py:483] Algo bellman_ford step 126 current loss 0.367078, current_train_items 4064.
I0304 19:28:03.271746 22849303695488 run.py:483] Algo bellman_ford step 127 current loss 0.462862, current_train_items 4096.
I0304 19:28:03.301523 22849303695488 run.py:483] Algo bellman_ford step 128 current loss 0.511035, current_train_items 4128.
I0304 19:28:03.333832 22849303695488 run.py:483] Algo bellman_ford step 129 current loss 0.560435, current_train_items 4160.
I0304 19:28:03.352654 22849303695488 run.py:483] Algo bellman_ford step 130 current loss 0.202562, current_train_items 4192.
I0304 19:28:03.369121 22849303695488 run.py:483] Algo bellman_ford step 131 current loss 0.317538, current_train_items 4224.
I0304 19:28:03.393244 22849303695488 run.py:483] Algo bellman_ford step 132 current loss 0.591093, current_train_items 4256.
I0304 19:28:03.422793 22849303695488 run.py:483] Algo bellman_ford step 133 current loss 0.609150, current_train_items 4288.
I0304 19:28:03.454202 22849303695488 run.py:483] Algo bellman_ford step 134 current loss 0.587819, current_train_items 4320.
I0304 19:28:03.472974 22849303695488 run.py:483] Algo bellman_ford step 135 current loss 0.130701, current_train_items 4352.
I0304 19:28:03.489436 22849303695488 run.py:483] Algo bellman_ford step 136 current loss 0.297190, current_train_items 4384.
I0304 19:28:03.513423 22849303695488 run.py:483] Algo bellman_ford step 137 current loss 0.425958, current_train_items 4416.
I0304 19:28:03.542853 22849303695488 run.py:483] Algo bellman_ford step 138 current loss 0.527574, current_train_items 4448.
I0304 19:28:03.575459 22849303695488 run.py:483] Algo bellman_ford step 139 current loss 0.620327, current_train_items 4480.
I0304 19:28:03.594212 22849303695488 run.py:483] Algo bellman_ford step 140 current loss 0.148142, current_train_items 4512.
I0304 19:28:03.610974 22849303695488 run.py:483] Algo bellman_ford step 141 current loss 0.438847, current_train_items 4544.
I0304 19:28:03.634427 22849303695488 run.py:483] Algo bellman_ford step 142 current loss 0.636020, current_train_items 4576.
I0304 19:28:03.664603 22849303695488 run.py:483] Algo bellman_ford step 143 current loss 0.618465, current_train_items 4608.
I0304 19:28:03.696192 22849303695488 run.py:483] Algo bellman_ford step 144 current loss 0.567711, current_train_items 4640.
I0304 19:28:03.714993 22849303695488 run.py:483] Algo bellman_ford step 145 current loss 0.128967, current_train_items 4672.
I0304 19:28:03.731443 22849303695488 run.py:483] Algo bellman_ford step 146 current loss 0.290138, current_train_items 4704.
I0304 19:28:03.754849 22849303695488 run.py:483] Algo bellman_ford step 147 current loss 0.393420, current_train_items 4736.
I0304 19:28:03.784149 22849303695488 run.py:483] Algo bellman_ford step 148 current loss 0.552160, current_train_items 4768.
I0304 19:28:03.814672 22849303695488 run.py:483] Algo bellman_ford step 149 current loss 0.682316, current_train_items 4800.
I0304 19:28:03.834216 22849303695488 run.py:483] Algo bellman_ford step 150 current loss 0.135917, current_train_items 4832.
I0304 19:28:03.842647 22849303695488 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0304 19:28:03.842761 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.867, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0304 19:28:03.872210 22849303695488 run.py:483] Algo bellman_ford step 151 current loss 0.213542, current_train_items 4864.
I0304 19:28:03.896238 22849303695488 run.py:483] Algo bellman_ford step 152 current loss 0.356232, current_train_items 4896.
I0304 19:28:03.925858 22849303695488 run.py:483] Algo bellman_ford step 153 current loss 0.429865, current_train_items 4928.
I0304 19:28:03.961853 22849303695488 run.py:483] Algo bellman_ford step 154 current loss 0.693212, current_train_items 4960.
I0304 19:28:03.981658 22849303695488 run.py:483] Algo bellman_ford step 155 current loss 0.123945, current_train_items 4992.
I0304 19:28:03.998501 22849303695488 run.py:483] Algo bellman_ford step 156 current loss 0.314078, current_train_items 5024.
I0304 19:28:04.022379 22849303695488 run.py:483] Algo bellman_ford step 157 current loss 0.391386, current_train_items 5056.
I0304 19:28:04.051318 22849303695488 run.py:483] Algo bellman_ford step 158 current loss 0.385151, current_train_items 5088.
I0304 19:28:04.083608 22849303695488 run.py:483] Algo bellman_ford step 159 current loss 0.459764, current_train_items 5120.
I0304 19:28:04.102945 22849303695488 run.py:483] Algo bellman_ford step 160 current loss 0.150179, current_train_items 5152.
I0304 19:28:04.119756 22849303695488 run.py:483] Algo bellman_ford step 161 current loss 0.191681, current_train_items 5184.
I0304 19:28:04.142697 22849303695488 run.py:483] Algo bellman_ford step 162 current loss 0.370935, current_train_items 5216.
I0304 19:28:04.172664 22849303695488 run.py:483] Algo bellman_ford step 163 current loss 0.471600, current_train_items 5248.
I0304 19:28:04.204583 22849303695488 run.py:483] Algo bellman_ford step 164 current loss 0.497991, current_train_items 5280.
I0304 19:28:04.224259 22849303695488 run.py:483] Algo bellman_ford step 165 current loss 0.108550, current_train_items 5312.
I0304 19:28:04.241267 22849303695488 run.py:483] Algo bellman_ford step 166 current loss 0.405710, current_train_items 5344.
I0304 19:28:04.264514 22849303695488 run.py:483] Algo bellman_ford step 167 current loss 0.413602, current_train_items 5376.
I0304 19:28:04.294800 22849303695488 run.py:483] Algo bellman_ford step 168 current loss 0.449424, current_train_items 5408.
I0304 19:28:04.327089 22849303695488 run.py:483] Algo bellman_ford step 169 current loss 0.547511, current_train_items 5440.
I0304 19:28:04.346241 22849303695488 run.py:483] Algo bellman_ford step 170 current loss 0.129975, current_train_items 5472.
I0304 19:28:04.362887 22849303695488 run.py:483] Algo bellman_ford step 171 current loss 0.221523, current_train_items 5504.
I0304 19:28:04.386088 22849303695488 run.py:483] Algo bellman_ford step 172 current loss 0.388590, current_train_items 5536.
I0304 19:28:04.416545 22849303695488 run.py:483] Algo bellman_ford step 173 current loss 0.499073, current_train_items 5568.
I0304 19:28:04.452451 22849303695488 run.py:483] Algo bellman_ford step 174 current loss 0.626669, current_train_items 5600.
I0304 19:28:04.471603 22849303695488 run.py:483] Algo bellman_ford step 175 current loss 0.110579, current_train_items 5632.
I0304 19:28:04.488158 22849303695488 run.py:483] Algo bellman_ford step 176 current loss 0.256288, current_train_items 5664.
I0304 19:28:04.512923 22849303695488 run.py:483] Algo bellman_ford step 177 current loss 0.498628, current_train_items 5696.
I0304 19:28:04.541489 22849303695488 run.py:483] Algo bellman_ford step 178 current loss 0.352031, current_train_items 5728.
I0304 19:28:04.572744 22849303695488 run.py:483] Algo bellman_ford step 179 current loss 0.562858, current_train_items 5760.
I0304 19:28:04.592331 22849303695488 run.py:483] Algo bellman_ford step 180 current loss 0.124796, current_train_items 5792.
I0304 19:28:04.609040 22849303695488 run.py:483] Algo bellman_ford step 181 current loss 0.212241, current_train_items 5824.
I0304 19:28:04.633634 22849303695488 run.py:483] Algo bellman_ford step 182 current loss 0.390817, current_train_items 5856.
I0304 19:28:04.663550 22849303695488 run.py:483] Algo bellman_ford step 183 current loss 0.490179, current_train_items 5888.
I0304 19:28:04.694739 22849303695488 run.py:483] Algo bellman_ford step 184 current loss 0.509660, current_train_items 5920.
I0304 19:28:04.713624 22849303695488 run.py:483] Algo bellman_ford step 185 current loss 0.122713, current_train_items 5952.
I0304 19:28:04.730527 22849303695488 run.py:483] Algo bellman_ford step 186 current loss 0.244077, current_train_items 5984.
I0304 19:28:04.753874 22849303695488 run.py:483] Algo bellman_ford step 187 current loss 0.368096, current_train_items 6016.
I0304 19:28:04.783519 22849303695488 run.py:483] Algo bellman_ford step 188 current loss 0.601049, current_train_items 6048.
I0304 19:28:04.817225 22849303695488 run.py:483] Algo bellman_ford step 189 current loss 0.557968, current_train_items 6080.
I0304 19:28:04.836536 22849303695488 run.py:483] Algo bellman_ford step 190 current loss 0.089333, current_train_items 6112.
I0304 19:28:04.853327 22849303695488 run.py:483] Algo bellman_ford step 191 current loss 0.328629, current_train_items 6144.
I0304 19:28:04.877974 22849303695488 run.py:483] Algo bellman_ford step 192 current loss 0.705345, current_train_items 6176.
I0304 19:28:04.907992 22849303695488 run.py:483] Algo bellman_ford step 193 current loss 0.689919, current_train_items 6208.
I0304 19:28:04.940030 22849303695488 run.py:483] Algo bellman_ford step 194 current loss 0.488534, current_train_items 6240.
I0304 19:28:04.959284 22849303695488 run.py:483] Algo bellman_ford step 195 current loss 0.106143, current_train_items 6272.
I0304 19:28:04.975884 22849303695488 run.py:483] Algo bellman_ford step 196 current loss 0.272734, current_train_items 6304.
I0304 19:28:04.999305 22849303695488 run.py:483] Algo bellman_ford step 197 current loss 0.549813, current_train_items 6336.
I0304 19:28:05.030081 22849303695488 run.py:483] Algo bellman_ford step 198 current loss 0.640485, current_train_items 6368.
I0304 19:28:05.063192 22849303695488 run.py:483] Algo bellman_ford step 199 current loss 0.603791, current_train_items 6400.
I0304 19:28:05.082515 22849303695488 run.py:483] Algo bellman_ford step 200 current loss 0.149030, current_train_items 6432.
I0304 19:28:05.090615 22849303695488 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0304 19:28:05.090722 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.893, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0304 19:28:05.119985 22849303695488 run.py:483] Algo bellman_ford step 201 current loss 0.322834, current_train_items 6464.
I0304 19:28:05.144330 22849303695488 run.py:483] Algo bellman_ford step 202 current loss 0.755306, current_train_items 6496.
I0304 19:28:05.176327 22849303695488 run.py:483] Algo bellman_ford step 203 current loss 0.633988, current_train_items 6528.
I0304 19:28:05.212210 22849303695488 run.py:483] Algo bellman_ford step 204 current loss 0.604027, current_train_items 6560.
I0304 19:28:05.231441 22849303695488 run.py:483] Algo bellman_ford step 205 current loss 0.133252, current_train_items 6592.
I0304 19:28:05.247203 22849303695488 run.py:483] Algo bellman_ford step 206 current loss 0.161469, current_train_items 6624.
I0304 19:28:05.271767 22849303695488 run.py:483] Algo bellman_ford step 207 current loss 0.588095, current_train_items 6656.
I0304 19:28:05.301855 22849303695488 run.py:483] Algo bellman_ford step 208 current loss 0.508153, current_train_items 6688.
I0304 19:28:05.333334 22849303695488 run.py:483] Algo bellman_ford step 209 current loss 0.510733, current_train_items 6720.
I0304 19:28:05.352296 22849303695488 run.py:483] Algo bellman_ford step 210 current loss 0.169960, current_train_items 6752.
I0304 19:28:05.368962 22849303695488 run.py:483] Algo bellman_ford step 211 current loss 0.292343, current_train_items 6784.
I0304 19:28:05.393273 22849303695488 run.py:483] Algo bellman_ford step 212 current loss 0.628318, current_train_items 6816.
I0304 19:28:05.423716 22849303695488 run.py:483] Algo bellman_ford step 213 current loss 0.571325, current_train_items 6848.
I0304 19:28:05.452131 22849303695488 run.py:483] Algo bellman_ford step 214 current loss 0.330156, current_train_items 6880.
I0304 19:28:05.471255 22849303695488 run.py:483] Algo bellman_ford step 215 current loss 0.094127, current_train_items 6912.
I0304 19:28:05.487772 22849303695488 run.py:483] Algo bellman_ford step 216 current loss 0.272324, current_train_items 6944.
I0304 19:28:05.511261 22849303695488 run.py:483] Algo bellman_ford step 217 current loss 0.370963, current_train_items 6976.
I0304 19:28:05.540823 22849303695488 run.py:483] Algo bellman_ford step 218 current loss 0.468183, current_train_items 7008.
I0304 19:28:05.571724 22849303695488 run.py:483] Algo bellman_ford step 219 current loss 0.467796, current_train_items 7040.
I0304 19:28:05.590646 22849303695488 run.py:483] Algo bellman_ford step 220 current loss 0.129055, current_train_items 7072.
I0304 19:28:05.607189 22849303695488 run.py:483] Algo bellman_ford step 221 current loss 0.268819, current_train_items 7104.
I0304 19:28:05.631638 22849303695488 run.py:483] Algo bellman_ford step 222 current loss 0.573208, current_train_items 7136.
I0304 19:28:05.660644 22849303695488 run.py:483] Algo bellman_ford step 223 current loss 0.634877, current_train_items 7168.
I0304 19:28:05.692948 22849303695488 run.py:483] Algo bellman_ford step 224 current loss 0.701816, current_train_items 7200.
I0304 19:28:05.711957 22849303695488 run.py:483] Algo bellman_ford step 225 current loss 0.108061, current_train_items 7232.
I0304 19:28:05.728840 22849303695488 run.py:483] Algo bellman_ford step 226 current loss 0.222035, current_train_items 7264.
I0304 19:28:05.752590 22849303695488 run.py:483] Algo bellman_ford step 227 current loss 0.538079, current_train_items 7296.
I0304 19:28:05.782829 22849303695488 run.py:483] Algo bellman_ford step 228 current loss 0.442065, current_train_items 7328.
I0304 19:28:05.815586 22849303695488 run.py:483] Algo bellman_ford step 229 current loss 0.590153, current_train_items 7360.
I0304 19:28:05.834412 22849303695488 run.py:483] Algo bellman_ford step 230 current loss 0.091013, current_train_items 7392.
I0304 19:28:05.851065 22849303695488 run.py:483] Algo bellman_ford step 231 current loss 0.210760, current_train_items 7424.
I0304 19:28:05.875045 22849303695488 run.py:483] Algo bellman_ford step 232 current loss 0.382301, current_train_items 7456.
I0304 19:28:05.905537 22849303695488 run.py:483] Algo bellman_ford step 233 current loss 0.420093, current_train_items 7488.
I0304 19:28:05.939116 22849303695488 run.py:483] Algo bellman_ford step 234 current loss 0.443233, current_train_items 7520.
I0304 19:28:05.958437 22849303695488 run.py:483] Algo bellman_ford step 235 current loss 0.121666, current_train_items 7552.
I0304 19:28:05.974617 22849303695488 run.py:483] Algo bellman_ford step 236 current loss 0.230872, current_train_items 7584.
I0304 19:28:05.998403 22849303695488 run.py:483] Algo bellman_ford step 237 current loss 0.290642, current_train_items 7616.
I0304 19:28:06.028046 22849303695488 run.py:483] Algo bellman_ford step 238 current loss 0.341369, current_train_items 7648.
I0304 19:28:06.060314 22849303695488 run.py:483] Algo bellman_ford step 239 current loss 0.390057, current_train_items 7680.
I0304 19:28:06.079101 22849303695488 run.py:483] Algo bellman_ford step 240 current loss 0.085980, current_train_items 7712.
I0304 19:28:06.095764 22849303695488 run.py:483] Algo bellman_ford step 241 current loss 0.240287, current_train_items 7744.
I0304 19:28:06.119523 22849303695488 run.py:483] Algo bellman_ford step 242 current loss 0.316239, current_train_items 7776.
I0304 19:28:06.148854 22849303695488 run.py:483] Algo bellman_ford step 243 current loss 0.362003, current_train_items 7808.
I0304 19:28:06.178354 22849303695488 run.py:483] Algo bellman_ford step 244 current loss 0.443013, current_train_items 7840.
I0304 19:28:06.197463 22849303695488 run.py:483] Algo bellman_ford step 245 current loss 0.111599, current_train_items 7872.
I0304 19:28:06.213875 22849303695488 run.py:483] Algo bellman_ford step 246 current loss 0.154937, current_train_items 7904.
I0304 19:28:06.236886 22849303695488 run.py:483] Algo bellman_ford step 247 current loss 0.239728, current_train_items 7936.
I0304 19:28:06.267604 22849303695488 run.py:483] Algo bellman_ford step 248 current loss 0.383142, current_train_items 7968.
I0304 19:28:06.300349 22849303695488 run.py:483] Algo bellman_ford step 249 current loss 0.336273, current_train_items 8000.
I0304 19:28:06.319055 22849303695488 run.py:483] Algo bellman_ford step 250 current loss 0.086209, current_train_items 8032.
I0304 19:28:06.327508 22849303695488 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.935546875, 'score': 0.935546875, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0304 19:28:06.327618 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.908, current avg val score is 0.936, val scores are: bellman_ford: 0.936
I0304 19:28:06.357196 22849303695488 run.py:483] Algo bellman_ford step 251 current loss 0.221072, current_train_items 8064.
I0304 19:28:06.381810 22849303695488 run.py:483] Algo bellman_ford step 252 current loss 0.267449, current_train_items 8096.
I0304 19:28:06.412846 22849303695488 run.py:483] Algo bellman_ford step 253 current loss 0.362360, current_train_items 8128.
I0304 19:28:06.446468 22849303695488 run.py:483] Algo bellman_ford step 254 current loss 0.428688, current_train_items 8160.
I0304 19:28:06.465938 22849303695488 run.py:483] Algo bellman_ford step 255 current loss 0.123489, current_train_items 8192.
I0304 19:28:06.481952 22849303695488 run.py:483] Algo bellman_ford step 256 current loss 0.181813, current_train_items 8224.
I0304 19:28:06.504812 22849303695488 run.py:483] Algo bellman_ford step 257 current loss 0.250712, current_train_items 8256.
I0304 19:28:06.534996 22849303695488 run.py:483] Algo bellman_ford step 258 current loss 0.317988, current_train_items 8288.
I0304 19:28:06.565616 22849303695488 run.py:483] Algo bellman_ford step 259 current loss 0.361575, current_train_items 8320.
I0304 19:28:06.585049 22849303695488 run.py:483] Algo bellman_ford step 260 current loss 0.081839, current_train_items 8352.
I0304 19:28:06.601606 22849303695488 run.py:483] Algo bellman_ford step 261 current loss 0.188024, current_train_items 8384.
I0304 19:28:06.624634 22849303695488 run.py:483] Algo bellman_ford step 262 current loss 0.264115, current_train_items 8416.
I0304 19:28:06.653871 22849303695488 run.py:483] Algo bellman_ford step 263 current loss 0.340397, current_train_items 8448.
I0304 19:28:06.684695 22849303695488 run.py:483] Algo bellman_ford step 264 current loss 0.358300, current_train_items 8480.
I0304 19:28:06.703685 22849303695488 run.py:483] Algo bellman_ford step 265 current loss 0.074570, current_train_items 8512.
I0304 19:28:06.720384 22849303695488 run.py:483] Algo bellman_ford step 266 current loss 0.194822, current_train_items 8544.
I0304 19:28:06.745015 22849303695488 run.py:483] Algo bellman_ford step 267 current loss 0.337089, current_train_items 8576.
I0304 19:28:06.775846 22849303695488 run.py:483] Algo bellman_ford step 268 current loss 0.419724, current_train_items 8608.
I0304 19:28:06.806346 22849303695488 run.py:483] Algo bellman_ford step 269 current loss 0.325654, current_train_items 8640.
I0304 19:28:06.825652 22849303695488 run.py:483] Algo bellman_ford step 270 current loss 0.065285, current_train_items 8672.
I0304 19:28:06.842059 22849303695488 run.py:483] Algo bellman_ford step 271 current loss 0.125862, current_train_items 8704.
I0304 19:28:06.866044 22849303695488 run.py:483] Algo bellman_ford step 272 current loss 0.266506, current_train_items 8736.
I0304 19:28:06.895429 22849303695488 run.py:483] Algo bellman_ford step 273 current loss 0.286660, current_train_items 8768.
I0304 19:28:06.926100 22849303695488 run.py:483] Algo bellman_ford step 274 current loss 0.373404, current_train_items 8800.
I0304 19:28:06.945431 22849303695488 run.py:483] Algo bellman_ford step 275 current loss 0.070926, current_train_items 8832.
I0304 19:28:06.962085 22849303695488 run.py:483] Algo bellman_ford step 276 current loss 0.159993, current_train_items 8864.
I0304 19:28:06.986617 22849303695488 run.py:483] Algo bellman_ford step 277 current loss 0.319233, current_train_items 8896.
I0304 19:28:07.017217 22849303695488 run.py:483] Algo bellman_ford step 278 current loss 0.337581, current_train_items 8928.
I0304 19:28:07.048850 22849303695488 run.py:483] Algo bellman_ford step 279 current loss 0.354609, current_train_items 8960.
I0304 19:28:07.067607 22849303695488 run.py:483] Algo bellman_ford step 280 current loss 0.073374, current_train_items 8992.
I0304 19:28:07.084415 22849303695488 run.py:483] Algo bellman_ford step 281 current loss 0.219991, current_train_items 9024.
I0304 19:28:07.108523 22849303695488 run.py:483] Algo bellman_ford step 282 current loss 0.285496, current_train_items 9056.
I0304 19:28:07.138013 22849303695488 run.py:483] Algo bellman_ford step 283 current loss 0.343167, current_train_items 9088.
I0304 19:28:07.171167 22849303695488 run.py:483] Algo bellman_ford step 284 current loss 0.516868, current_train_items 9120.
I0304 19:28:07.190510 22849303695488 run.py:483] Algo bellman_ford step 285 current loss 0.088733, current_train_items 9152.
I0304 19:28:07.207354 22849303695488 run.py:483] Algo bellman_ford step 286 current loss 0.155948, current_train_items 9184.
I0304 19:28:07.230578 22849303695488 run.py:483] Algo bellman_ford step 287 current loss 0.323842, current_train_items 9216.
I0304 19:28:07.260971 22849303695488 run.py:483] Algo bellman_ford step 288 current loss 0.509636, current_train_items 9248.
I0304 19:28:07.293909 22849303695488 run.py:483] Algo bellman_ford step 289 current loss 0.446924, current_train_items 9280.
I0304 19:28:07.313071 22849303695488 run.py:483] Algo bellman_ford step 290 current loss 0.053277, current_train_items 9312.
I0304 19:28:07.329582 22849303695488 run.py:483] Algo bellman_ford step 291 current loss 0.164571, current_train_items 9344.
I0304 19:28:07.352912 22849303695488 run.py:483] Algo bellman_ford step 292 current loss 0.446665, current_train_items 9376.
I0304 19:28:07.382258 22849303695488 run.py:483] Algo bellman_ford step 293 current loss 0.370280, current_train_items 9408.
I0304 19:28:07.413877 22849303695488 run.py:483] Algo bellman_ford step 294 current loss 0.354269, current_train_items 9440.
I0304 19:28:07.432693 22849303695488 run.py:483] Algo bellman_ford step 295 current loss 0.082267, current_train_items 9472.
I0304 19:28:07.448954 22849303695488 run.py:483] Algo bellman_ford step 296 current loss 0.139403, current_train_items 9504.
I0304 19:28:07.473052 22849303695488 run.py:483] Algo bellman_ford step 297 current loss 0.270774, current_train_items 9536.
I0304 19:28:07.503903 22849303695488 run.py:483] Algo bellman_ford step 298 current loss 0.324566, current_train_items 9568.
I0304 19:28:07.534920 22849303695488 run.py:483] Algo bellman_ford step 299 current loss 0.434756, current_train_items 9600.
I0304 19:28:07.554215 22849303695488 run.py:483] Algo bellman_ford step 300 current loss 0.126051, current_train_items 9632.
I0304 19:28:07.562244 22849303695488 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0304 19:28:07.562354 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.936, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0304 19:28:07.579515 22849303695488 run.py:483] Algo bellman_ford step 301 current loss 0.173434, current_train_items 9664.
I0304 19:28:07.602937 22849303695488 run.py:483] Algo bellman_ford step 302 current loss 0.238796, current_train_items 9696.
I0304 19:28:07.632046 22849303695488 run.py:483] Algo bellman_ford step 303 current loss 0.228504, current_train_items 9728.
I0304 19:28:07.665563 22849303695488 run.py:483] Algo bellman_ford step 304 current loss 0.574971, current_train_items 9760.
I0304 19:28:07.684721 22849303695488 run.py:483] Algo bellman_ford step 305 current loss 0.076075, current_train_items 9792.
I0304 19:28:07.701608 22849303695488 run.py:483] Algo bellman_ford step 306 current loss 0.201460, current_train_items 9824.
I0304 19:28:07.726130 22849303695488 run.py:483] Algo bellman_ford step 307 current loss 0.292844, current_train_items 9856.
I0304 19:28:07.756232 22849303695488 run.py:483] Algo bellman_ford step 308 current loss 0.353652, current_train_items 9888.
I0304 19:28:07.787985 22849303695488 run.py:483] Algo bellman_ford step 309 current loss 0.343602, current_train_items 9920.
I0304 19:28:07.807202 22849303695488 run.py:483] Algo bellman_ford step 310 current loss 0.093237, current_train_items 9952.
I0304 19:28:07.823519 22849303695488 run.py:483] Algo bellman_ford step 311 current loss 0.149811, current_train_items 9984.
I0304 19:28:07.847145 22849303695488 run.py:483] Algo bellman_ford step 312 current loss 0.256649, current_train_items 10016.
I0304 19:28:07.877705 22849303695488 run.py:483] Algo bellman_ford step 313 current loss 0.318968, current_train_items 10048.
I0304 19:28:07.909571 22849303695488 run.py:483] Algo bellman_ford step 314 current loss 0.368445, current_train_items 10080.
I0304 19:28:07.928467 22849303695488 run.py:483] Algo bellman_ford step 315 current loss 0.079642, current_train_items 10112.
I0304 19:28:07.945178 22849303695488 run.py:483] Algo bellman_ford step 316 current loss 0.159175, current_train_items 10144.
I0304 19:28:07.968286 22849303695488 run.py:483] Algo bellman_ford step 317 current loss 0.238330, current_train_items 10176.
I0304 19:28:07.996916 22849303695488 run.py:483] Algo bellman_ford step 318 current loss 0.311650, current_train_items 10208.
I0304 19:28:08.027991 22849303695488 run.py:483] Algo bellman_ford step 319 current loss 0.357166, current_train_items 10240.
I0304 19:28:08.046900 22849303695488 run.py:483] Algo bellman_ford step 320 current loss 0.054384, current_train_items 10272.
I0304 19:28:08.063480 22849303695488 run.py:483] Algo bellman_ford step 321 current loss 0.230349, current_train_items 10304.
I0304 19:28:08.087549 22849303695488 run.py:483] Algo bellman_ford step 322 current loss 0.348238, current_train_items 10336.
I0304 19:28:08.116404 22849303695488 run.py:483] Algo bellman_ford step 323 current loss 0.228652, current_train_items 10368.
I0304 19:28:08.148324 22849303695488 run.py:483] Algo bellman_ford step 324 current loss 0.404687, current_train_items 10400.
I0304 19:28:08.167328 22849303695488 run.py:483] Algo bellman_ford step 325 current loss 0.083032, current_train_items 10432.
I0304 19:28:08.183592 22849303695488 run.py:483] Algo bellman_ford step 326 current loss 0.129746, current_train_items 10464.
I0304 19:28:08.207077 22849303695488 run.py:483] Algo bellman_ford step 327 current loss 0.237703, current_train_items 10496.
I0304 19:28:08.237768 22849303695488 run.py:483] Algo bellman_ford step 328 current loss 0.307017, current_train_items 10528.
I0304 19:28:08.270413 22849303695488 run.py:483] Algo bellman_ford step 329 current loss 0.439748, current_train_items 10560.
I0304 19:28:08.289424 22849303695488 run.py:483] Algo bellman_ford step 330 current loss 0.093000, current_train_items 10592.
I0304 19:28:08.305828 22849303695488 run.py:483] Algo bellman_ford step 331 current loss 0.149901, current_train_items 10624.
I0304 19:28:08.329407 22849303695488 run.py:483] Algo bellman_ford step 332 current loss 0.369256, current_train_items 10656.
I0304 19:28:08.359742 22849303695488 run.py:483] Algo bellman_ford step 333 current loss 0.394269, current_train_items 10688.
I0304 19:28:08.393035 22849303695488 run.py:483] Algo bellman_ford step 334 current loss 0.423497, current_train_items 10720.
I0304 19:28:08.412112 22849303695488 run.py:483] Algo bellman_ford step 335 current loss 0.081122, current_train_items 10752.
I0304 19:28:08.429061 22849303695488 run.py:483] Algo bellman_ford step 336 current loss 0.261361, current_train_items 10784.
I0304 19:28:08.453888 22849303695488 run.py:483] Algo bellman_ford step 337 current loss 0.336473, current_train_items 10816.
I0304 19:28:08.483572 22849303695488 run.py:483] Algo bellman_ford step 338 current loss 0.272494, current_train_items 10848.
I0304 19:28:08.516651 22849303695488 run.py:483] Algo bellman_ford step 339 current loss 0.369605, current_train_items 10880.
I0304 19:28:08.535549 22849303695488 run.py:483] Algo bellman_ford step 340 current loss 0.070162, current_train_items 10912.
I0304 19:28:08.551829 22849303695488 run.py:483] Algo bellman_ford step 341 current loss 0.191114, current_train_items 10944.
I0304 19:28:08.575239 22849303695488 run.py:483] Algo bellman_ford step 342 current loss 0.294175, current_train_items 10976.
I0304 19:28:08.603834 22849303695488 run.py:483] Algo bellman_ford step 343 current loss 0.241008, current_train_items 11008.
I0304 19:28:08.636338 22849303695488 run.py:483] Algo bellman_ford step 344 current loss 0.391701, current_train_items 11040.
I0304 19:28:08.655498 22849303695488 run.py:483] Algo bellman_ford step 345 current loss 0.118105, current_train_items 11072.
I0304 19:28:08.671998 22849303695488 run.py:483] Algo bellman_ford step 346 current loss 0.179822, current_train_items 11104.
I0304 19:28:08.696904 22849303695488 run.py:483] Algo bellman_ford step 347 current loss 0.258670, current_train_items 11136.
I0304 19:28:08.726511 22849303695488 run.py:483] Algo bellman_ford step 348 current loss 0.355562, current_train_items 11168.
I0304 19:28:08.759784 22849303695488 run.py:483] Algo bellman_ford step 349 current loss 0.427744, current_train_items 11200.
I0304 19:28:08.778942 22849303695488 run.py:483] Algo bellman_ford step 350 current loss 0.079046, current_train_items 11232.
I0304 19:28:08.787448 22849303695488 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0304 19:28:08.787557 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.936, current avg val score is 0.955, val scores are: bellman_ford: 0.955
I0304 19:28:08.816011 22849303695488 run.py:483] Algo bellman_ford step 351 current loss 0.151520, current_train_items 11264.
I0304 19:28:08.840865 22849303695488 run.py:483] Algo bellman_ford step 352 current loss 0.261668, current_train_items 11296.
I0304 19:28:08.871211 22849303695488 run.py:483] Algo bellman_ford step 353 current loss 0.312946, current_train_items 11328.
I0304 19:28:08.905660 22849303695488 run.py:483] Algo bellman_ford step 354 current loss 0.543250, current_train_items 11360.
I0304 19:28:08.925095 22849303695488 run.py:483] Algo bellman_ford step 355 current loss 0.078028, current_train_items 11392.
I0304 19:28:08.941395 22849303695488 run.py:483] Algo bellman_ford step 356 current loss 0.199992, current_train_items 11424.
I0304 19:28:08.964840 22849303695488 run.py:483] Algo bellman_ford step 357 current loss 0.207661, current_train_items 11456.
I0304 19:28:08.996101 22849303695488 run.py:483] Algo bellman_ford step 358 current loss 0.423166, current_train_items 11488.
I0304 19:28:09.030058 22849303695488 run.py:483] Algo bellman_ford step 359 current loss 0.413149, current_train_items 11520.
I0304 19:28:09.049666 22849303695488 run.py:483] Algo bellman_ford step 360 current loss 0.074241, current_train_items 11552.
I0304 19:28:09.066555 22849303695488 run.py:483] Algo bellman_ford step 361 current loss 0.148180, current_train_items 11584.
I0304 19:28:09.089077 22849303695488 run.py:483] Algo bellman_ford step 362 current loss 0.243654, current_train_items 11616.
I0304 19:28:09.118727 22849303695488 run.py:483] Algo bellman_ford step 363 current loss 0.281780, current_train_items 11648.
I0304 19:28:09.149299 22849303695488 run.py:483] Algo bellman_ford step 364 current loss 0.254533, current_train_items 11680.
I0304 19:28:09.168246 22849303695488 run.py:483] Algo bellman_ford step 365 current loss 0.056353, current_train_items 11712.
I0304 19:28:09.184772 22849303695488 run.py:483] Algo bellman_ford step 366 current loss 0.159355, current_train_items 11744.
I0304 19:28:09.208113 22849303695488 run.py:483] Algo bellman_ford step 367 current loss 0.170245, current_train_items 11776.
I0304 19:28:09.239104 22849303695488 run.py:483] Algo bellman_ford step 368 current loss 0.281060, current_train_items 11808.
I0304 19:28:09.269377 22849303695488 run.py:483] Algo bellman_ford step 369 current loss 0.245767, current_train_items 11840.
I0304 19:28:09.288644 22849303695488 run.py:483] Algo bellman_ford step 370 current loss 0.058205, current_train_items 11872.
I0304 19:28:09.305138 22849303695488 run.py:483] Algo bellman_ford step 371 current loss 0.095577, current_train_items 11904.
I0304 19:28:09.328324 22849303695488 run.py:483] Algo bellman_ford step 372 current loss 0.186611, current_train_items 11936.
I0304 19:28:09.359206 22849303695488 run.py:483] Algo bellman_ford step 373 current loss 0.345075, current_train_items 11968.
I0304 19:28:09.392796 22849303695488 run.py:483] Algo bellman_ford step 374 current loss 0.478566, current_train_items 12000.
I0304 19:28:09.412070 22849303695488 run.py:483] Algo bellman_ford step 375 current loss 0.040451, current_train_items 12032.
I0304 19:28:09.428422 22849303695488 run.py:483] Algo bellman_ford step 376 current loss 0.086828, current_train_items 12064.
I0304 19:28:09.452715 22849303695488 run.py:483] Algo bellman_ford step 377 current loss 0.234893, current_train_items 12096.
I0304 19:28:09.482791 22849303695488 run.py:483] Algo bellman_ford step 378 current loss 0.255335, current_train_items 12128.
I0304 19:28:09.516224 22849303695488 run.py:483] Algo bellman_ford step 379 current loss 0.370373, current_train_items 12160.
I0304 19:28:09.535146 22849303695488 run.py:483] Algo bellman_ford step 380 current loss 0.044451, current_train_items 12192.
I0304 19:28:09.552046 22849303695488 run.py:483] Algo bellman_ford step 381 current loss 0.172971, current_train_items 12224.
I0304 19:28:09.576023 22849303695488 run.py:483] Algo bellman_ford step 382 current loss 0.211146, current_train_items 12256.
I0304 19:28:09.606109 22849303695488 run.py:483] Algo bellman_ford step 383 current loss 0.328939, current_train_items 12288.
I0304 19:28:09.639336 22849303695488 run.py:483] Algo bellman_ford step 384 current loss 0.310622, current_train_items 12320.
I0304 19:28:09.658780 22849303695488 run.py:483] Algo bellman_ford step 385 current loss 0.053246, current_train_items 12352.
I0304 19:28:09.675500 22849303695488 run.py:483] Algo bellman_ford step 386 current loss 0.078086, current_train_items 12384.
I0304 19:28:09.698719 22849303695488 run.py:483] Algo bellman_ford step 387 current loss 0.178865, current_train_items 12416.
I0304 19:28:09.728270 22849303695488 run.py:483] Algo bellman_ford step 388 current loss 0.271467, current_train_items 12448.
I0304 19:28:09.761106 22849303695488 run.py:483] Algo bellman_ford step 389 current loss 0.342262, current_train_items 12480.
I0304 19:28:09.780279 22849303695488 run.py:483] Algo bellman_ford step 390 current loss 0.064348, current_train_items 12512.
I0304 19:28:09.796638 22849303695488 run.py:483] Algo bellman_ford step 391 current loss 0.119072, current_train_items 12544.
I0304 19:28:09.820557 22849303695488 run.py:483] Algo bellman_ford step 392 current loss 0.250915, current_train_items 12576.
I0304 19:28:09.851949 22849303695488 run.py:483] Algo bellman_ford step 393 current loss 0.328609, current_train_items 12608.
I0304 19:28:09.883840 22849303695488 run.py:483] Algo bellman_ford step 394 current loss 0.346022, current_train_items 12640.
I0304 19:28:09.902785 22849303695488 run.py:483] Algo bellman_ford step 395 current loss 0.054707, current_train_items 12672.
I0304 19:28:09.919401 22849303695488 run.py:483] Algo bellman_ford step 396 current loss 0.135476, current_train_items 12704.
I0304 19:28:09.943522 22849303695488 run.py:483] Algo bellman_ford step 397 current loss 0.269147, current_train_items 12736.
I0304 19:28:09.974516 22849303695488 run.py:483] Algo bellman_ford step 398 current loss 0.303739, current_train_items 12768.
I0304 19:28:10.004774 22849303695488 run.py:483] Algo bellman_ford step 399 current loss 0.281420, current_train_items 12800.
I0304 19:28:10.023971 22849303695488 run.py:483] Algo bellman_ford step 400 current loss 0.040621, current_train_items 12832.
I0304 19:28:10.032017 22849303695488 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0304 19:28:10.032155 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0304 19:28:10.049596 22849303695488 run.py:483] Algo bellman_ford step 401 current loss 0.197835, current_train_items 12864.
I0304 19:28:10.074188 22849303695488 run.py:483] Algo bellman_ford step 402 current loss 0.265076, current_train_items 12896.
I0304 19:28:10.105114 22849303695488 run.py:483] Algo bellman_ford step 403 current loss 0.345258, current_train_items 12928.
I0304 19:28:10.136543 22849303695488 run.py:483] Algo bellman_ford step 404 current loss 0.406046, current_train_items 12960.
I0304 19:28:10.155990 22849303695488 run.py:483] Algo bellman_ford step 405 current loss 0.086752, current_train_items 12992.
I0304 19:28:10.171673 22849303695488 run.py:483] Algo bellman_ford step 406 current loss 0.114830, current_train_items 13024.
I0304 19:28:10.195815 22849303695488 run.py:483] Algo bellman_ford step 407 current loss 0.222771, current_train_items 13056.
I0304 19:28:10.227278 22849303695488 run.py:483] Algo bellman_ford step 408 current loss 0.366352, current_train_items 13088.
I0304 19:28:10.260059 22849303695488 run.py:483] Algo bellman_ford step 409 current loss 0.373830, current_train_items 13120.
I0304 19:28:10.279426 22849303695488 run.py:483] Algo bellman_ford step 410 current loss 0.052002, current_train_items 13152.
I0304 19:28:10.295703 22849303695488 run.py:483] Algo bellman_ford step 411 current loss 0.077851, current_train_items 13184.
I0304 19:28:10.320102 22849303695488 run.py:483] Algo bellman_ford step 412 current loss 0.184563, current_train_items 13216.
I0304 19:28:10.350788 22849303695488 run.py:483] Algo bellman_ford step 413 current loss 0.242474, current_train_items 13248.
I0304 19:28:10.383681 22849303695488 run.py:483] Algo bellman_ford step 414 current loss 0.292868, current_train_items 13280.
I0304 19:28:10.402897 22849303695488 run.py:483] Algo bellman_ford step 415 current loss 0.067316, current_train_items 13312.
I0304 19:28:10.419214 22849303695488 run.py:483] Algo bellman_ford step 416 current loss 0.081006, current_train_items 13344.
I0304 19:28:10.444022 22849303695488 run.py:483] Algo bellman_ford step 417 current loss 0.163505, current_train_items 13376.
I0304 19:28:10.475845 22849303695488 run.py:483] Algo bellman_ford step 418 current loss 0.277570, current_train_items 13408.
I0304 19:28:10.508351 22849303695488 run.py:483] Algo bellman_ford step 419 current loss 0.337722, current_train_items 13440.
I0304 19:28:10.527318 22849303695488 run.py:483] Algo bellman_ford step 420 current loss 0.101364, current_train_items 13472.
I0304 19:28:10.543840 22849303695488 run.py:483] Algo bellman_ford step 421 current loss 0.066875, current_train_items 13504.
I0304 19:28:10.568227 22849303695488 run.py:483] Algo bellman_ford step 422 current loss 0.202036, current_train_items 13536.
I0304 19:28:10.597478 22849303695488 run.py:483] Algo bellman_ford step 423 current loss 0.313575, current_train_items 13568.
I0304 19:28:10.629578 22849303695488 run.py:483] Algo bellman_ford step 424 current loss 0.326860, current_train_items 13600.
I0304 19:28:10.648592 22849303695488 run.py:483] Algo bellman_ford step 425 current loss 0.099077, current_train_items 13632.
I0304 19:28:10.664826 22849303695488 run.py:483] Algo bellman_ford step 426 current loss 0.109591, current_train_items 13664.
I0304 19:28:10.688151 22849303695488 run.py:483] Algo bellman_ford step 427 current loss 0.448493, current_train_items 13696.
I0304 19:28:10.717732 22849303695488 run.py:483] Algo bellman_ford step 428 current loss 0.371546, current_train_items 13728.
I0304 19:28:10.749780 22849303695488 run.py:483] Algo bellman_ford step 429 current loss 0.301915, current_train_items 13760.
I0304 19:28:10.769319 22849303695488 run.py:483] Algo bellman_ford step 430 current loss 0.052358, current_train_items 13792.
I0304 19:28:10.785727 22849303695488 run.py:483] Algo bellman_ford step 431 current loss 0.234376, current_train_items 13824.
I0304 19:28:10.810214 22849303695488 run.py:483] Algo bellman_ford step 432 current loss 0.445001, current_train_items 13856.
I0304 19:28:10.839439 22849303695488 run.py:483] Algo bellman_ford step 433 current loss 0.238436, current_train_items 13888.
I0304 19:28:10.872041 22849303695488 run.py:483] Algo bellman_ford step 434 current loss 0.276647, current_train_items 13920.
I0304 19:28:10.890841 22849303695488 run.py:483] Algo bellman_ford step 435 current loss 0.091608, current_train_items 13952.
I0304 19:28:10.907518 22849303695488 run.py:483] Algo bellman_ford step 436 current loss 0.162463, current_train_items 13984.
I0304 19:28:10.930961 22849303695488 run.py:483] Algo bellman_ford step 437 current loss 0.233402, current_train_items 14016.
I0304 19:28:10.960626 22849303695488 run.py:483] Algo bellman_ford step 438 current loss 0.200346, current_train_items 14048.
I0304 19:28:10.993170 22849303695488 run.py:483] Algo bellman_ford step 439 current loss 0.327250, current_train_items 14080.
I0304 19:28:11.012193 22849303695488 run.py:483] Algo bellman_ford step 440 current loss 0.046901, current_train_items 14112.
I0304 19:28:11.028541 22849303695488 run.py:483] Algo bellman_ford step 441 current loss 0.148066, current_train_items 14144.
I0304 19:28:11.051926 22849303695488 run.py:483] Algo bellman_ford step 442 current loss 0.169050, current_train_items 14176.
I0304 19:28:11.081915 22849303695488 run.py:483] Algo bellman_ford step 443 current loss 0.291602, current_train_items 14208.
I0304 19:28:11.113727 22849303695488 run.py:483] Algo bellman_ford step 444 current loss 0.259938, current_train_items 14240.
I0304 19:28:11.132928 22849303695488 run.py:483] Algo bellman_ford step 445 current loss 0.052469, current_train_items 14272.
I0304 19:28:11.150182 22849303695488 run.py:483] Algo bellman_ford step 446 current loss 0.209756, current_train_items 14304.
I0304 19:28:11.174448 22849303695488 run.py:483] Algo bellman_ford step 447 current loss 0.238292, current_train_items 14336.
I0304 19:28:11.203301 22849303695488 run.py:483] Algo bellman_ford step 448 current loss 0.212394, current_train_items 14368.
I0304 19:28:11.233940 22849303695488 run.py:483] Algo bellman_ford step 449 current loss 0.251124, current_train_items 14400.
I0304 19:28:11.253356 22849303695488 run.py:483] Algo bellman_ford step 450 current loss 0.059827, current_train_items 14432.
I0304 19:28:11.261752 22849303695488 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.943359375, 'score': 0.943359375, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0304 19:28:11.261862 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.943, val scores are: bellman_ford: 0.943
I0304 19:28:11.279228 22849303695488 run.py:483] Algo bellman_ford step 451 current loss 0.106945, current_train_items 14464.
I0304 19:28:11.303224 22849303695488 run.py:483] Algo bellman_ford step 452 current loss 0.153853, current_train_items 14496.
I0304 19:28:11.334400 22849303695488 run.py:483] Algo bellman_ford step 453 current loss 0.268023, current_train_items 14528.
I0304 19:28:11.366789 22849303695488 run.py:483] Algo bellman_ford step 454 current loss 0.292080, current_train_items 14560.
I0304 19:28:11.385824 22849303695488 run.py:483] Algo bellman_ford step 455 current loss 0.043952, current_train_items 14592.
I0304 19:28:11.402313 22849303695488 run.py:483] Algo bellman_ford step 456 current loss 0.220444, current_train_items 14624.
I0304 19:28:11.425605 22849303695488 run.py:483] Algo bellman_ford step 457 current loss 0.208190, current_train_items 14656.
I0304 19:28:11.455104 22849303695488 run.py:483] Algo bellman_ford step 458 current loss 0.197618, current_train_items 14688.
I0304 19:28:11.486133 22849303695488 run.py:483] Algo bellman_ford step 459 current loss 0.325071, current_train_items 14720.
I0304 19:28:11.505149 22849303695488 run.py:483] Algo bellman_ford step 460 current loss 0.031169, current_train_items 14752.
I0304 19:28:11.522058 22849303695488 run.py:483] Algo bellman_ford step 461 current loss 0.104703, current_train_items 14784.
I0304 19:28:11.545837 22849303695488 run.py:483] Algo bellman_ford step 462 current loss 0.177127, current_train_items 14816.
I0304 19:28:11.575985 22849303695488 run.py:483] Algo bellman_ford step 463 current loss 0.215303, current_train_items 14848.
I0304 19:28:11.609582 22849303695488 run.py:483] Algo bellman_ford step 464 current loss 0.268435, current_train_items 14880.
I0304 19:28:11.628573 22849303695488 run.py:483] Algo bellman_ford step 465 current loss 0.051546, current_train_items 14912.
I0304 19:28:11.644647 22849303695488 run.py:483] Algo bellman_ford step 466 current loss 0.108673, current_train_items 14944.
I0304 19:28:11.668934 22849303695488 run.py:483] Algo bellman_ford step 467 current loss 0.303532, current_train_items 14976.
I0304 19:28:11.697646 22849303695488 run.py:483] Algo bellman_ford step 468 current loss 0.181084, current_train_items 15008.
I0304 19:28:11.731189 22849303695488 run.py:483] Algo bellman_ford step 469 current loss 0.415083, current_train_items 15040.
I0304 19:28:11.750334 22849303695488 run.py:483] Algo bellman_ford step 470 current loss 0.070715, current_train_items 15072.
I0304 19:28:11.766924 22849303695488 run.py:483] Algo bellman_ford step 471 current loss 0.119860, current_train_items 15104.
I0304 19:28:11.791176 22849303695488 run.py:483] Algo bellman_ford step 472 current loss 0.230773, current_train_items 15136.
I0304 19:28:11.821603 22849303695488 run.py:483] Algo bellman_ford step 473 current loss 0.247861, current_train_items 15168.
I0304 19:28:11.854703 22849303695488 run.py:483] Algo bellman_ford step 474 current loss 0.359133, current_train_items 15200.
I0304 19:28:11.874090 22849303695488 run.py:483] Algo bellman_ford step 475 current loss 0.037372, current_train_items 15232.
I0304 19:28:11.890267 22849303695488 run.py:483] Algo bellman_ford step 476 current loss 0.122432, current_train_items 15264.
I0304 19:28:11.914634 22849303695488 run.py:483] Algo bellman_ford step 477 current loss 0.251880, current_train_items 15296.
I0304 19:28:11.943149 22849303695488 run.py:483] Algo bellman_ford step 478 current loss 0.150163, current_train_items 15328.
I0304 19:28:11.976069 22849303695488 run.py:483] Algo bellman_ford step 479 current loss 0.398887, current_train_items 15360.
I0304 19:28:11.994726 22849303695488 run.py:483] Algo bellman_ford step 480 current loss 0.064766, current_train_items 15392.
I0304 19:28:12.011163 22849303695488 run.py:483] Algo bellman_ford step 481 current loss 0.131405, current_train_items 15424.
I0304 19:28:12.034389 22849303695488 run.py:483] Algo bellman_ford step 482 current loss 0.213767, current_train_items 15456.
I0304 19:28:12.064699 22849303695488 run.py:483] Algo bellman_ford step 483 current loss 0.157780, current_train_items 15488.
I0304 19:28:12.097340 22849303695488 run.py:483] Algo bellman_ford step 484 current loss 0.280815, current_train_items 15520.
I0304 19:28:12.116380 22849303695488 run.py:483] Algo bellman_ford step 485 current loss 0.056854, current_train_items 15552.
I0304 19:28:12.132852 22849303695488 run.py:483] Algo bellman_ford step 486 current loss 0.131554, current_train_items 15584.
I0304 19:28:12.156656 22849303695488 run.py:483] Algo bellman_ford step 487 current loss 0.224378, current_train_items 15616.
I0304 19:28:12.186180 22849303695488 run.py:483] Algo bellman_ford step 488 current loss 0.230027, current_train_items 15648.
I0304 19:28:12.217645 22849303695488 run.py:483] Algo bellman_ford step 489 current loss 0.324576, current_train_items 15680.
I0304 19:28:12.236681 22849303695488 run.py:483] Algo bellman_ford step 490 current loss 0.066595, current_train_items 15712.
I0304 19:28:12.253372 22849303695488 run.py:483] Algo bellman_ford step 491 current loss 0.077006, current_train_items 15744.
I0304 19:28:12.276662 22849303695488 run.py:483] Algo bellman_ford step 492 current loss 0.185227, current_train_items 15776.
I0304 19:28:12.306582 22849303695488 run.py:483] Algo bellman_ford step 493 current loss 0.276343, current_train_items 15808.
I0304 19:28:12.339985 22849303695488 run.py:483] Algo bellman_ford step 494 current loss 0.384486, current_train_items 15840.
I0304 19:28:12.358852 22849303695488 run.py:483] Algo bellman_ford step 495 current loss 0.030614, current_train_items 15872.
I0304 19:28:12.375111 22849303695488 run.py:483] Algo bellman_ford step 496 current loss 0.114222, current_train_items 15904.
I0304 19:28:12.400225 22849303695488 run.py:483] Algo bellman_ford step 497 current loss 0.276963, current_train_items 15936.
I0304 19:28:12.431130 22849303695488 run.py:483] Algo bellman_ford step 498 current loss 0.200734, current_train_items 15968.
I0304 19:28:12.463115 22849303695488 run.py:483] Algo bellman_ford step 499 current loss 0.237094, current_train_items 16000.
I0304 19:28:12.482298 22849303695488 run.py:483] Algo bellman_ford step 500 current loss 0.041779, current_train_items 16032.
I0304 19:28:12.490262 22849303695488 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0304 19:28:12.490370 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0304 19:28:12.506987 22849303695488 run.py:483] Algo bellman_ford step 501 current loss 0.100983, current_train_items 16064.
I0304 19:28:12.531632 22849303695488 run.py:483] Algo bellman_ford step 502 current loss 0.222819, current_train_items 16096.
I0304 19:28:12.563162 22849303695488 run.py:483] Algo bellman_ford step 503 current loss 0.274279, current_train_items 16128.
I0304 19:28:12.598114 22849303695488 run.py:483] Algo bellman_ford step 504 current loss 0.401388, current_train_items 16160.
I0304 19:28:12.617442 22849303695488 run.py:483] Algo bellman_ford step 505 current loss 0.076717, current_train_items 16192.
I0304 19:28:12.633140 22849303695488 run.py:483] Algo bellman_ford step 506 current loss 0.067228, current_train_items 16224.
I0304 19:28:12.656213 22849303695488 run.py:483] Algo bellman_ford step 507 current loss 0.149963, current_train_items 16256.
I0304 19:28:12.686920 22849303695488 run.py:483] Algo bellman_ford step 508 current loss 0.206472, current_train_items 16288.
I0304 19:28:12.720280 22849303695488 run.py:483] Algo bellman_ford step 509 current loss 0.306657, current_train_items 16320.
I0304 19:28:12.739700 22849303695488 run.py:483] Algo bellman_ford step 510 current loss 0.038759, current_train_items 16352.
I0304 19:28:12.756417 22849303695488 run.py:483] Algo bellman_ford step 511 current loss 0.108583, current_train_items 16384.
I0304 19:28:12.780673 22849303695488 run.py:483] Algo bellman_ford step 512 current loss 0.249811, current_train_items 16416.
I0304 19:28:12.811373 22849303695488 run.py:483] Algo bellman_ford step 513 current loss 0.223787, current_train_items 16448.
I0304 19:28:12.844334 22849303695488 run.py:483] Algo bellman_ford step 514 current loss 0.246479, current_train_items 16480.
I0304 19:28:12.863437 22849303695488 run.py:483] Algo bellman_ford step 515 current loss 0.043297, current_train_items 16512.
I0304 19:28:12.879687 22849303695488 run.py:483] Algo bellman_ford step 516 current loss 0.084092, current_train_items 16544.
I0304 19:28:12.904466 22849303695488 run.py:483] Algo bellman_ford step 517 current loss 0.157994, current_train_items 16576.
I0304 19:28:12.935691 22849303695488 run.py:483] Algo bellman_ford step 518 current loss 0.212974, current_train_items 16608.
I0304 19:28:12.968306 22849303695488 run.py:483] Algo bellman_ford step 519 current loss 0.215553, current_train_items 16640.
I0304 19:28:12.987504 22849303695488 run.py:483] Algo bellman_ford step 520 current loss 0.043733, current_train_items 16672.
I0304 19:28:13.004236 22849303695488 run.py:483] Algo bellman_ford step 521 current loss 0.133104, current_train_items 16704.
I0304 19:28:13.028061 22849303695488 run.py:483] Algo bellman_ford step 522 current loss 0.152893, current_train_items 16736.
I0304 19:28:13.059169 22849303695488 run.py:483] Algo bellman_ford step 523 current loss 0.210624, current_train_items 16768.
I0304 19:28:13.090937 22849303695488 run.py:483] Algo bellman_ford step 524 current loss 0.313966, current_train_items 16800.
I0304 19:28:13.109950 22849303695488 run.py:483] Algo bellman_ford step 525 current loss 0.035592, current_train_items 16832.
I0304 19:28:13.126669 22849303695488 run.py:483] Algo bellman_ford step 526 current loss 0.146780, current_train_items 16864.
I0304 19:28:13.150595 22849303695488 run.py:483] Algo bellman_ford step 527 current loss 0.205915, current_train_items 16896.
I0304 19:28:13.180231 22849303695488 run.py:483] Algo bellman_ford step 528 current loss 0.212827, current_train_items 16928.
I0304 19:28:13.213224 22849303695488 run.py:483] Algo bellman_ford step 529 current loss 0.266011, current_train_items 16960.
I0304 19:28:13.232479 22849303695488 run.py:483] Algo bellman_ford step 530 current loss 0.038880, current_train_items 16992.
I0304 19:28:13.249073 22849303695488 run.py:483] Algo bellman_ford step 531 current loss 0.083925, current_train_items 17024.
I0304 19:28:13.272107 22849303695488 run.py:483] Algo bellman_ford step 532 current loss 0.159309, current_train_items 17056.
I0304 19:28:13.302461 22849303695488 run.py:483] Algo bellman_ford step 533 current loss 0.287868, current_train_items 17088.
I0304 19:28:13.335223 22849303695488 run.py:483] Algo bellman_ford step 534 current loss 0.284013, current_train_items 17120.
I0304 19:28:13.354642 22849303695488 run.py:483] Algo bellman_ford step 535 current loss 0.068957, current_train_items 17152.
I0304 19:28:13.370761 22849303695488 run.py:483] Algo bellman_ford step 536 current loss 0.081891, current_train_items 17184.
I0304 19:28:13.394037 22849303695488 run.py:483] Algo bellman_ford step 537 current loss 0.140630, current_train_items 17216.
I0304 19:28:13.423594 22849303695488 run.py:483] Algo bellman_ford step 538 current loss 0.174653, current_train_items 17248.
I0304 19:28:13.455080 22849303695488 run.py:483] Algo bellman_ford step 539 current loss 0.244560, current_train_items 17280.
I0304 19:28:13.473860 22849303695488 run.py:483] Algo bellman_ford step 540 current loss 0.038073, current_train_items 17312.
I0304 19:28:13.489823 22849303695488 run.py:483] Algo bellman_ford step 541 current loss 0.059111, current_train_items 17344.
I0304 19:28:13.514327 22849303695488 run.py:483] Algo bellman_ford step 542 current loss 0.214145, current_train_items 17376.
I0304 19:28:13.544945 22849303695488 run.py:483] Algo bellman_ford step 543 current loss 0.251106, current_train_items 17408.
I0304 19:28:13.577836 22849303695488 run.py:483] Algo bellman_ford step 544 current loss 0.262978, current_train_items 17440.
I0304 19:28:13.597072 22849303695488 run.py:483] Algo bellman_ford step 545 current loss 0.037164, current_train_items 17472.
I0304 19:28:13.613849 22849303695488 run.py:483] Algo bellman_ford step 546 current loss 0.129723, current_train_items 17504.
I0304 19:28:13.637620 22849303695488 run.py:483] Algo bellman_ford step 547 current loss 0.173575, current_train_items 17536.
I0304 19:28:13.668416 22849303695488 run.py:483] Algo bellman_ford step 548 current loss 0.344929, current_train_items 17568.
I0304 19:28:13.698519 22849303695488 run.py:483] Algo bellman_ford step 549 current loss 0.254765, current_train_items 17600.
I0304 19:28:13.717646 22849303695488 run.py:483] Algo bellman_ford step 550 current loss 0.038931, current_train_items 17632.
I0304 19:28:13.725766 22849303695488 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.951171875, 'score': 0.951171875, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0304 19:28:13.725875 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.951, val scores are: bellman_ford: 0.951
I0304 19:28:13.742907 22849303695488 run.py:483] Algo bellman_ford step 551 current loss 0.063817, current_train_items 17664.
I0304 19:28:13.766780 22849303695488 run.py:483] Algo bellman_ford step 552 current loss 0.177513, current_train_items 17696.
I0304 19:28:13.798571 22849303695488 run.py:483] Algo bellman_ford step 553 current loss 0.380976, current_train_items 17728.
I0304 19:28:13.830766 22849303695488 run.py:483] Algo bellman_ford step 554 current loss 0.389863, current_train_items 17760.
I0304 19:28:13.850016 22849303695488 run.py:483] Algo bellman_ford step 555 current loss 0.094868, current_train_items 17792.
I0304 19:28:13.866209 22849303695488 run.py:483] Algo bellman_ford step 556 current loss 0.154618, current_train_items 17824.
I0304 19:28:13.890744 22849303695488 run.py:483] Algo bellman_ford step 557 current loss 0.345703, current_train_items 17856.
I0304 19:28:13.920247 22849303695488 run.py:483] Algo bellman_ford step 558 current loss 0.218712, current_train_items 17888.
I0304 19:28:13.954549 22849303695488 run.py:483] Algo bellman_ford step 559 current loss 0.334488, current_train_items 17920.
I0304 19:28:13.973957 22849303695488 run.py:483] Algo bellman_ford step 560 current loss 0.068733, current_train_items 17952.
I0304 19:28:13.990548 22849303695488 run.py:483] Algo bellman_ford step 561 current loss 0.101778, current_train_items 17984.
I0304 19:28:14.014503 22849303695488 run.py:483] Algo bellman_ford step 562 current loss 0.158967, current_train_items 18016.
I0304 19:28:14.044289 22849303695488 run.py:483] Algo bellman_ford step 563 current loss 0.177756, current_train_items 18048.
I0304 19:28:14.074553 22849303695488 run.py:483] Algo bellman_ford step 564 current loss 0.191135, current_train_items 18080.
I0304 19:28:14.093537 22849303695488 run.py:483] Algo bellman_ford step 565 current loss 0.028367, current_train_items 18112.
I0304 19:28:14.110502 22849303695488 run.py:483] Algo bellman_ford step 566 current loss 0.070397, current_train_items 18144.
I0304 19:28:14.135169 22849303695488 run.py:483] Algo bellman_ford step 567 current loss 0.217591, current_train_items 18176.
I0304 19:28:14.165132 22849303695488 run.py:483] Algo bellman_ford step 568 current loss 0.208466, current_train_items 18208.
I0304 19:28:14.196553 22849303695488 run.py:483] Algo bellman_ford step 569 current loss 0.285400, current_train_items 18240.
I0304 19:28:14.215908 22849303695488 run.py:483] Algo bellman_ford step 570 current loss 0.076975, current_train_items 18272.
I0304 19:28:14.232308 22849303695488 run.py:483] Algo bellman_ford step 571 current loss 0.114560, current_train_items 18304.
I0304 19:28:14.256402 22849303695488 run.py:483] Algo bellman_ford step 572 current loss 0.211364, current_train_items 18336.
I0304 19:28:14.287952 22849303695488 run.py:483] Algo bellman_ford step 573 current loss 0.248035, current_train_items 18368.
I0304 19:28:14.319505 22849303695488 run.py:483] Algo bellman_ford step 574 current loss 0.160499, current_train_items 18400.
I0304 19:28:14.339098 22849303695488 run.py:483] Algo bellman_ford step 575 current loss 0.032972, current_train_items 18432.
I0304 19:28:14.355364 22849303695488 run.py:483] Algo bellman_ford step 576 current loss 0.057248, current_train_items 18464.
I0304 19:28:14.379320 22849303695488 run.py:483] Algo bellman_ford step 577 current loss 0.155883, current_train_items 18496.
I0304 19:28:14.409595 22849303695488 run.py:483] Algo bellman_ford step 578 current loss 0.238380, current_train_items 18528.
I0304 19:28:14.442852 22849303695488 run.py:483] Algo bellman_ford step 579 current loss 0.232688, current_train_items 18560.
I0304 19:28:14.461873 22849303695488 run.py:483] Algo bellman_ford step 580 current loss 0.037215, current_train_items 18592.
I0304 19:28:14.478383 22849303695488 run.py:483] Algo bellman_ford step 581 current loss 0.123870, current_train_items 18624.
I0304 19:28:14.501084 22849303695488 run.py:483] Algo bellman_ford step 582 current loss 0.108687, current_train_items 18656.
I0304 19:28:14.529876 22849303695488 run.py:483] Algo bellman_ford step 583 current loss 0.145666, current_train_items 18688.
I0304 19:28:14.561771 22849303695488 run.py:483] Algo bellman_ford step 584 current loss 0.260361, current_train_items 18720.
I0304 19:28:14.581252 22849303695488 run.py:483] Algo bellman_ford step 585 current loss 0.069401, current_train_items 18752.
I0304 19:28:14.597808 22849303695488 run.py:483] Algo bellman_ford step 586 current loss 0.069283, current_train_items 18784.
I0304 19:28:14.620558 22849303695488 run.py:483] Algo bellman_ford step 587 current loss 0.055842, current_train_items 18816.
I0304 19:28:14.650027 22849303695488 run.py:483] Algo bellman_ford step 588 current loss 0.202734, current_train_items 18848.
I0304 19:28:14.683784 22849303695488 run.py:483] Algo bellman_ford step 589 current loss 0.250879, current_train_items 18880.
I0304 19:28:14.703190 22849303695488 run.py:483] Algo bellman_ford step 590 current loss 0.028719, current_train_items 18912.
I0304 19:28:14.719821 22849303695488 run.py:483] Algo bellman_ford step 591 current loss 0.092192, current_train_items 18944.
I0304 19:28:14.743929 22849303695488 run.py:483] Algo bellman_ford step 592 current loss 0.314022, current_train_items 18976.
I0304 19:28:14.773664 22849303695488 run.py:483] Algo bellman_ford step 593 current loss 0.204489, current_train_items 19008.
I0304 19:28:14.805653 22849303695488 run.py:483] Algo bellman_ford step 594 current loss 0.272080, current_train_items 19040.
I0304 19:28:14.824508 22849303695488 run.py:483] Algo bellman_ford step 595 current loss 0.035545, current_train_items 19072.
I0304 19:28:14.841002 22849303695488 run.py:483] Algo bellman_ford step 596 current loss 0.086306, current_train_items 19104.
I0304 19:28:14.865427 22849303695488 run.py:483] Algo bellman_ford step 597 current loss 0.151832, current_train_items 19136.
I0304 19:28:14.895336 22849303695488 run.py:483] Algo bellman_ford step 598 current loss 0.131971, current_train_items 19168.
I0304 19:28:14.927705 22849303695488 run.py:483] Algo bellman_ford step 599 current loss 0.243839, current_train_items 19200.
I0304 19:28:14.946962 22849303695488 run.py:483] Algo bellman_ford step 600 current loss 0.040407, current_train_items 19232.
I0304 19:28:14.954880 22849303695488 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.9560546875, 'score': 0.9560546875, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0304 19:28:14.954990 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.955, current avg val score is 0.956, val scores are: bellman_ford: 0.956
I0304 19:28:14.989482 22849303695488 run.py:483] Algo bellman_ford step 601 current loss 0.063977, current_train_items 19264.
I0304 19:28:15.013428 22849303695488 run.py:483] Algo bellman_ford step 602 current loss 0.176693, current_train_items 19296.
I0304 19:28:15.043184 22849303695488 run.py:483] Algo bellman_ford step 603 current loss 0.195785, current_train_items 19328.
I0304 19:28:15.074848 22849303695488 run.py:483] Algo bellman_ford step 604 current loss 0.213932, current_train_items 19360.
I0304 19:28:15.094210 22849303695488 run.py:483] Algo bellman_ford step 605 current loss 0.024655, current_train_items 19392.
I0304 19:28:15.110274 22849303695488 run.py:483] Algo bellman_ford step 606 current loss 0.134352, current_train_items 19424.
I0304 19:28:15.134967 22849303695488 run.py:483] Algo bellman_ford step 607 current loss 0.241470, current_train_items 19456.
I0304 19:28:15.163958 22849303695488 run.py:483] Algo bellman_ford step 608 current loss 0.207037, current_train_items 19488.
I0304 19:28:15.196358 22849303695488 run.py:483] Algo bellman_ford step 609 current loss 0.223274, current_train_items 19520.
I0304 19:28:15.215313 22849303695488 run.py:483] Algo bellman_ford step 610 current loss 0.042143, current_train_items 19552.
I0304 19:28:15.232192 22849303695488 run.py:483] Algo bellman_ford step 611 current loss 0.060804, current_train_items 19584.
I0304 19:28:15.256379 22849303695488 run.py:483] Algo bellman_ford step 612 current loss 0.319946, current_train_items 19616.
I0304 19:28:15.284742 22849303695488 run.py:483] Algo bellman_ford step 613 current loss 0.277714, current_train_items 19648.
I0304 19:28:15.316407 22849303695488 run.py:483] Algo bellman_ford step 614 current loss 0.409279, current_train_items 19680.
I0304 19:28:15.335246 22849303695488 run.py:483] Algo bellman_ford step 615 current loss 0.027261, current_train_items 19712.
I0304 19:28:15.352161 22849303695488 run.py:483] Algo bellman_ford step 616 current loss 0.117575, current_train_items 19744.
I0304 19:28:15.375492 22849303695488 run.py:483] Algo bellman_ford step 617 current loss 0.234076, current_train_items 19776.
I0304 19:28:15.405272 22849303695488 run.py:483] Algo bellman_ford step 618 current loss 0.208302, current_train_items 19808.
I0304 19:28:15.437623 22849303695488 run.py:483] Algo bellman_ford step 619 current loss 0.209329, current_train_items 19840.
I0304 19:28:15.456610 22849303695488 run.py:483] Algo bellman_ford step 620 current loss 0.080315, current_train_items 19872.
I0304 19:28:15.473440 22849303695488 run.py:483] Algo bellman_ford step 621 current loss 0.119574, current_train_items 19904.
I0304 19:28:15.497604 22849303695488 run.py:483] Algo bellman_ford step 622 current loss 0.154377, current_train_items 19936.
I0304 19:28:15.527219 22849303695488 run.py:483] Algo bellman_ford step 623 current loss 0.180577, current_train_items 19968.
I0304 19:28:15.558874 22849303695488 run.py:483] Algo bellman_ford step 624 current loss 0.298074, current_train_items 20000.
I0304 19:28:15.577787 22849303695488 run.py:483] Algo bellman_ford step 625 current loss 0.036634, current_train_items 20032.
I0304 19:28:15.594080 22849303695488 run.py:483] Algo bellman_ford step 626 current loss 0.054510, current_train_items 20064.
I0304 19:28:15.618663 22849303695488 run.py:483] Algo bellman_ford step 627 current loss 0.144038, current_train_items 20096.
I0304 19:28:15.649328 22849303695488 run.py:483] Algo bellman_ford step 628 current loss 0.186837, current_train_items 20128.
I0304 19:28:15.681550 22849303695488 run.py:483] Algo bellman_ford step 629 current loss 0.274246, current_train_items 20160.
I0304 19:28:15.700834 22849303695488 run.py:483] Algo bellman_ford step 630 current loss 0.024774, current_train_items 20192.
I0304 19:28:15.717214 22849303695488 run.py:483] Algo bellman_ford step 631 current loss 0.101109, current_train_items 20224.
I0304 19:28:15.740778 22849303695488 run.py:483] Algo bellman_ford step 632 current loss 0.214399, current_train_items 20256.
I0304 19:28:15.770347 22849303695488 run.py:483] Algo bellman_ford step 633 current loss 0.176474, current_train_items 20288.
I0304 19:28:15.801832 22849303695488 run.py:483] Algo bellman_ford step 634 current loss 0.185179, current_train_items 20320.
I0304 19:28:15.820863 22849303695488 run.py:483] Algo bellman_ford step 635 current loss 0.041829, current_train_items 20352.
I0304 19:28:15.837290 22849303695488 run.py:483] Algo bellman_ford step 636 current loss 0.055407, current_train_items 20384.
I0304 19:28:15.860921 22849303695488 run.py:483] Algo bellman_ford step 637 current loss 0.152452, current_train_items 20416.
I0304 19:28:15.891751 22849303695488 run.py:483] Algo bellman_ford step 638 current loss 0.227617, current_train_items 20448.
I0304 19:28:15.920991 22849303695488 run.py:483] Algo bellman_ford step 639 current loss 0.176170, current_train_items 20480.
I0304 19:28:15.940017 22849303695488 run.py:483] Algo bellman_ford step 640 current loss 0.019734, current_train_items 20512.
I0304 19:28:15.956442 22849303695488 run.py:483] Algo bellman_ford step 641 current loss 0.088501, current_train_items 20544.
I0304 19:28:15.979058 22849303695488 run.py:483] Algo bellman_ford step 642 current loss 0.220658, current_train_items 20576.
I0304 19:28:16.008436 22849303695488 run.py:483] Algo bellman_ford step 643 current loss 0.182328, current_train_items 20608.
I0304 19:28:16.040805 22849303695488 run.py:483] Algo bellman_ford step 644 current loss 0.284996, current_train_items 20640.
I0304 19:28:16.059590 22849303695488 run.py:483] Algo bellman_ford step 645 current loss 0.020881, current_train_items 20672.
I0304 19:28:16.077065 22849303695488 run.py:483] Algo bellman_ford step 646 current loss 0.092474, current_train_items 20704.
I0304 19:28:16.100549 22849303695488 run.py:483] Algo bellman_ford step 647 current loss 0.102613, current_train_items 20736.
I0304 19:28:16.131321 22849303695488 run.py:483] Algo bellman_ford step 648 current loss 0.285626, current_train_items 20768.
I0304 19:28:16.163984 22849303695488 run.py:483] Algo bellman_ford step 649 current loss 0.203033, current_train_items 20800.
I0304 19:28:16.182941 22849303695488 run.py:483] Algo bellman_ford step 650 current loss 0.091870, current_train_items 20832.
I0304 19:28:16.191168 22849303695488 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.947265625, 'score': 0.947265625, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0304 19:28:16.191279 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.956, current avg val score is 0.947, val scores are: bellman_ford: 0.947
I0304 19:28:16.208900 22849303695488 run.py:483] Algo bellman_ford step 651 current loss 0.066420, current_train_items 20864.
I0304 19:28:16.232549 22849303695488 run.py:483] Algo bellman_ford step 652 current loss 0.143608, current_train_items 20896.
I0304 19:28:16.261609 22849303695488 run.py:483] Algo bellman_ford step 653 current loss 0.159471, current_train_items 20928.
I0304 19:28:16.293532 22849303695488 run.py:483] Algo bellman_ford step 654 current loss 0.233868, current_train_items 20960.
I0304 19:28:16.313024 22849303695488 run.py:483] Algo bellman_ford step 655 current loss 0.023066, current_train_items 20992.
I0304 19:28:16.328967 22849303695488 run.py:483] Algo bellman_ford step 656 current loss 0.056274, current_train_items 21024.
I0304 19:28:16.353079 22849303695488 run.py:483] Algo bellman_ford step 657 current loss 0.292810, current_train_items 21056.
I0304 19:28:16.383379 22849303695488 run.py:483] Algo bellman_ford step 658 current loss 0.211639, current_train_items 21088.
I0304 19:28:16.414682 22849303695488 run.py:483] Algo bellman_ford step 659 current loss 0.256299, current_train_items 21120.
I0304 19:28:16.434026 22849303695488 run.py:483] Algo bellman_ford step 660 current loss 0.058355, current_train_items 21152.
I0304 19:28:16.450484 22849303695488 run.py:483] Algo bellman_ford step 661 current loss 0.098928, current_train_items 21184.
I0304 19:28:16.473459 22849303695488 run.py:483] Algo bellman_ford step 662 current loss 0.199978, current_train_items 21216.
I0304 19:28:16.502975 22849303695488 run.py:483] Algo bellman_ford step 663 current loss 0.246232, current_train_items 21248.
I0304 19:28:16.537558 22849303695488 run.py:483] Algo bellman_ford step 664 current loss 0.282458, current_train_items 21280.
I0304 19:28:16.556676 22849303695488 run.py:483] Algo bellman_ford step 665 current loss 0.081961, current_train_items 21312.
I0304 19:28:16.573222 22849303695488 run.py:483] Algo bellman_ford step 666 current loss 0.146327, current_train_items 21344.
I0304 19:28:16.597613 22849303695488 run.py:483] Algo bellman_ford step 667 current loss 0.345036, current_train_items 21376.
I0304 19:28:16.626792 22849303695488 run.py:483] Algo bellman_ford step 668 current loss 0.283673, current_train_items 21408.
I0304 19:28:16.658371 22849303695488 run.py:483] Algo bellman_ford step 669 current loss 0.310819, current_train_items 21440.
I0304 19:28:16.677623 22849303695488 run.py:483] Algo bellman_ford step 670 current loss 0.034919, current_train_items 21472.
I0304 19:28:16.693948 22849303695488 run.py:483] Algo bellman_ford step 671 current loss 0.108620, current_train_items 21504.
I0304 19:28:16.717962 22849303695488 run.py:483] Algo bellman_ford step 672 current loss 0.179366, current_train_items 21536.
I0304 19:28:16.747608 22849303695488 run.py:483] Algo bellman_ford step 673 current loss 0.178194, current_train_items 21568.
I0304 19:28:16.780877 22849303695488 run.py:483] Algo bellman_ford step 674 current loss 0.279163, current_train_items 21600.
I0304 19:28:16.800318 22849303695488 run.py:483] Algo bellman_ford step 675 current loss 0.069205, current_train_items 21632.
I0304 19:28:16.816292 22849303695488 run.py:483] Algo bellman_ford step 676 current loss 0.150503, current_train_items 21664.
I0304 19:28:16.839459 22849303695488 run.py:483] Algo bellman_ford step 677 current loss 0.161745, current_train_items 21696.
I0304 19:28:16.869980 22849303695488 run.py:483] Algo bellman_ford step 678 current loss 0.229901, current_train_items 21728.
I0304 19:28:16.903076 22849303695488 run.py:483] Algo bellman_ford step 679 current loss 0.239436, current_train_items 21760.
I0304 19:28:16.922019 22849303695488 run.py:483] Algo bellman_ford step 680 current loss 0.052294, current_train_items 21792.
I0304 19:28:16.938532 22849303695488 run.py:483] Algo bellman_ford step 681 current loss 0.070854, current_train_items 21824.
I0304 19:28:16.962315 22849303695488 run.py:483] Algo bellman_ford step 682 current loss 0.193170, current_train_items 21856.
I0304 19:28:16.991273 22849303695488 run.py:483] Algo bellman_ford step 683 current loss 0.206907, current_train_items 21888.
I0304 19:28:17.024553 22849303695488 run.py:483] Algo bellman_ford step 684 current loss 0.247322, current_train_items 21920.
I0304 19:28:17.043651 22849303695488 run.py:483] Algo bellman_ford step 685 current loss 0.043722, current_train_items 21952.
I0304 19:28:17.060045 22849303695488 run.py:483] Algo bellman_ford step 686 current loss 0.062213, current_train_items 21984.
I0304 19:28:17.083858 22849303695488 run.py:483] Algo bellman_ford step 687 current loss 0.155835, current_train_items 22016.
I0304 19:28:17.112321 22849303695488 run.py:483] Algo bellman_ford step 688 current loss 0.173490, current_train_items 22048.
I0304 19:28:17.146210 22849303695488 run.py:483] Algo bellman_ford step 689 current loss 0.350233, current_train_items 22080.
I0304 19:28:17.165274 22849303695488 run.py:483] Algo bellman_ford step 690 current loss 0.045342, current_train_items 22112.
I0304 19:28:17.182282 22849303695488 run.py:483] Algo bellman_ford step 691 current loss 0.128302, current_train_items 22144.
I0304 19:28:17.206576 22849303695488 run.py:483] Algo bellman_ford step 692 current loss 0.182294, current_train_items 22176.
I0304 19:28:17.235742 22849303695488 run.py:483] Algo bellman_ford step 693 current loss 0.238584, current_train_items 22208.
I0304 19:28:17.269443 22849303695488 run.py:483] Algo bellman_ford step 694 current loss 0.523636, current_train_items 22240.
I0304 19:28:17.288463 22849303695488 run.py:483] Algo bellman_ford step 695 current loss 0.035303, current_train_items 22272.
I0304 19:28:17.304735 22849303695488 run.py:483] Algo bellman_ford step 696 current loss 0.111795, current_train_items 22304.
I0304 19:28:17.326965 22849303695488 run.py:483] Algo bellman_ford step 697 current loss 0.198534, current_train_items 22336.
I0304 19:28:17.356967 22849303695488 run.py:483] Algo bellman_ford step 698 current loss 0.198734, current_train_items 22368.
I0304 19:28:17.390299 22849303695488 run.py:483] Algo bellman_ford step 699 current loss 0.307489, current_train_items 22400.
I0304 19:28:17.409465 22849303695488 run.py:483] Algo bellman_ford step 700 current loss 0.036065, current_train_items 22432.
I0304 19:28:17.417550 22849303695488 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.9580078125, 'score': 0.9580078125, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0304 19:28:17.417658 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.956, current avg val score is 0.958, val scores are: bellman_ford: 0.958
I0304 19:28:17.446452 22849303695488 run.py:483] Algo bellman_ford step 701 current loss 0.062287, current_train_items 22464.
I0304 19:28:17.470384 22849303695488 run.py:483] Algo bellman_ford step 702 current loss 0.233573, current_train_items 22496.
I0304 19:28:17.499684 22849303695488 run.py:483] Algo bellman_ford step 703 current loss 0.197843, current_train_items 22528.
I0304 19:28:17.531316 22849303695488 run.py:483] Algo bellman_ford step 704 current loss 0.189158, current_train_items 22560.
I0304 19:28:17.551056 22849303695488 run.py:483] Algo bellman_ford step 705 current loss 0.038583, current_train_items 22592.
I0304 19:28:17.567361 22849303695488 run.py:483] Algo bellman_ford step 706 current loss 0.065310, current_train_items 22624.
I0304 19:28:17.591141 22849303695488 run.py:483] Algo bellman_ford step 707 current loss 0.115783, current_train_items 22656.
I0304 19:28:17.621177 22849303695488 run.py:483] Algo bellman_ford step 708 current loss 0.163107, current_train_items 22688.
I0304 19:28:17.654072 22849303695488 run.py:483] Algo bellman_ford step 709 current loss 0.226573, current_train_items 22720.
I0304 19:28:17.672954 22849303695488 run.py:483] Algo bellman_ford step 710 current loss 0.024025, current_train_items 22752.
I0304 19:28:17.689531 22849303695488 run.py:483] Algo bellman_ford step 711 current loss 0.050004, current_train_items 22784.
I0304 19:28:17.714047 22849303695488 run.py:483] Algo bellman_ford step 712 current loss 0.147677, current_train_items 22816.
I0304 19:28:17.745026 22849303695488 run.py:483] Algo bellman_ford step 713 current loss 0.184930, current_train_items 22848.
I0304 19:28:17.775676 22849303695488 run.py:483] Algo bellman_ford step 714 current loss 0.169463, current_train_items 22880.
I0304 19:28:17.795013 22849303695488 run.py:483] Algo bellman_ford step 715 current loss 0.020980, current_train_items 22912.
I0304 19:28:17.811895 22849303695488 run.py:483] Algo bellman_ford step 716 current loss 0.131283, current_train_items 22944.
I0304 19:28:17.836897 22849303695488 run.py:483] Algo bellman_ford step 717 current loss 0.292371, current_train_items 22976.
I0304 19:28:17.865824 22849303695488 run.py:483] Algo bellman_ford step 718 current loss 0.126905, current_train_items 23008.
I0304 19:28:17.898315 22849303695488 run.py:483] Algo bellman_ford step 719 current loss 0.195883, current_train_items 23040.
I0304 19:28:17.917241 22849303695488 run.py:483] Algo bellman_ford step 720 current loss 0.034088, current_train_items 23072.
I0304 19:28:17.934179 22849303695488 run.py:483] Algo bellman_ford step 721 current loss 0.163963, current_train_items 23104.
I0304 19:28:17.957665 22849303695488 run.py:483] Algo bellman_ford step 722 current loss 0.087492, current_train_items 23136.
I0304 19:28:17.987197 22849303695488 run.py:483] Algo bellman_ford step 723 current loss 0.125550, current_train_items 23168.
I0304 19:28:18.018877 22849303695488 run.py:483] Algo bellman_ford step 724 current loss 0.222206, current_train_items 23200.
I0304 19:28:18.038345 22849303695488 run.py:483] Algo bellman_ford step 725 current loss 0.041245, current_train_items 23232.
I0304 19:28:18.055315 22849303695488 run.py:483] Algo bellman_ford step 726 current loss 0.060020, current_train_items 23264.
I0304 19:28:18.079606 22849303695488 run.py:483] Algo bellman_ford step 727 current loss 0.183561, current_train_items 23296.
I0304 19:28:18.109060 22849303695488 run.py:483] Algo bellman_ford step 728 current loss 0.145611, current_train_items 23328.
I0304 19:28:18.141382 22849303695488 run.py:483] Algo bellman_ford step 729 current loss 0.193886, current_train_items 23360.
I0304 19:28:18.160211 22849303695488 run.py:483] Algo bellman_ford step 730 current loss 0.017371, current_train_items 23392.
I0304 19:28:18.176707 22849303695488 run.py:483] Algo bellman_ford step 731 current loss 0.085918, current_train_items 23424.
I0304 19:28:18.200755 22849303695488 run.py:483] Algo bellman_ford step 732 current loss 0.149398, current_train_items 23456.
I0304 19:28:18.230433 22849303695488 run.py:483] Algo bellman_ford step 733 current loss 0.218184, current_train_items 23488.
I0304 19:28:18.263247 22849303695488 run.py:483] Algo bellman_ford step 734 current loss 0.247916, current_train_items 23520.
I0304 19:28:18.282150 22849303695488 run.py:483] Algo bellman_ford step 735 current loss 0.038032, current_train_items 23552.
I0304 19:28:18.299088 22849303695488 run.py:483] Algo bellman_ford step 736 current loss 0.043298, current_train_items 23584.
I0304 19:28:18.322641 22849303695488 run.py:483] Algo bellman_ford step 737 current loss 0.108454, current_train_items 23616.
I0304 19:28:18.352373 22849303695488 run.py:483] Algo bellman_ford step 738 current loss 0.224812, current_train_items 23648.
I0304 19:28:18.382534 22849303695488 run.py:483] Algo bellman_ford step 739 current loss 0.152832, current_train_items 23680.
I0304 19:28:18.401553 22849303695488 run.py:483] Algo bellman_ford step 740 current loss 0.020110, current_train_items 23712.
I0304 19:28:18.418311 22849303695488 run.py:483] Algo bellman_ford step 741 current loss 0.054003, current_train_items 23744.
I0304 19:28:18.441690 22849303695488 run.py:483] Algo bellman_ford step 742 current loss 0.181633, current_train_items 23776.
I0304 19:28:18.470909 22849303695488 run.py:483] Algo bellman_ford step 743 current loss 0.137121, current_train_items 23808.
I0304 19:28:18.504836 22849303695488 run.py:483] Algo bellman_ford step 744 current loss 0.224797, current_train_items 23840.
I0304 19:28:18.524264 22849303695488 run.py:483] Algo bellman_ford step 745 current loss 0.019729, current_train_items 23872.
I0304 19:28:18.540715 22849303695488 run.py:483] Algo bellman_ford step 746 current loss 0.121859, current_train_items 23904.
I0304 19:28:18.564484 22849303695488 run.py:483] Algo bellman_ford step 747 current loss 0.152125, current_train_items 23936.
I0304 19:28:18.594940 22849303695488 run.py:483] Algo bellman_ford step 748 current loss 0.196532, current_train_items 23968.
I0304 19:28:18.626800 22849303695488 run.py:483] Algo bellman_ford step 749 current loss 0.245034, current_train_items 24000.
I0304 19:28:18.645587 22849303695488 run.py:483] Algo bellman_ford step 750 current loss 0.015293, current_train_items 24032.
I0304 19:28:18.653942 22849303695488 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0304 19:28:18.654060 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.958, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:28:18.683187 22849303695488 run.py:483] Algo bellman_ford step 751 current loss 0.088432, current_train_items 24064.
I0304 19:28:18.706504 22849303695488 run.py:483] Algo bellman_ford step 752 current loss 0.132203, current_train_items 24096.
I0304 19:28:18.736238 22849303695488 run.py:483] Algo bellman_ford step 753 current loss 0.175689, current_train_items 24128.
I0304 19:28:18.769250 22849303695488 run.py:483] Algo bellman_ford step 754 current loss 0.233406, current_train_items 24160.
I0304 19:28:18.788704 22849303695488 run.py:483] Algo bellman_ford step 755 current loss 0.086904, current_train_items 24192.
I0304 19:28:18.804822 22849303695488 run.py:483] Algo bellman_ford step 756 current loss 0.070071, current_train_items 24224.
I0304 19:28:18.829705 22849303695488 run.py:483] Algo bellman_ford step 757 current loss 0.142532, current_train_items 24256.
I0304 19:28:18.859223 22849303695488 run.py:483] Algo bellman_ford step 758 current loss 0.204363, current_train_items 24288.
I0304 19:28:18.890799 22849303695488 run.py:483] Algo bellman_ford step 759 current loss 0.190947, current_train_items 24320.
I0304 19:28:18.910159 22849303695488 run.py:483] Algo bellman_ford step 760 current loss 0.012062, current_train_items 24352.
I0304 19:28:18.926783 22849303695488 run.py:483] Algo bellman_ford step 761 current loss 0.064665, current_train_items 24384.
I0304 19:28:18.950524 22849303695488 run.py:483] Algo bellman_ford step 762 current loss 0.170121, current_train_items 24416.
I0304 19:28:18.979657 22849303695488 run.py:483] Algo bellman_ford step 763 current loss 0.144864, current_train_items 24448.
I0304 19:28:19.012191 22849303695488 run.py:483] Algo bellman_ford step 764 current loss 0.244126, current_train_items 24480.
I0304 19:28:19.031278 22849303695488 run.py:483] Algo bellman_ford step 765 current loss 0.021971, current_train_items 24512.
I0304 19:28:19.048301 22849303695488 run.py:483] Algo bellman_ford step 766 current loss 0.095553, current_train_items 24544.
I0304 19:28:19.072115 22849303695488 run.py:483] Algo bellman_ford step 767 current loss 0.129759, current_train_items 24576.
I0304 19:28:19.102109 22849303695488 run.py:483] Algo bellman_ford step 768 current loss 0.132397, current_train_items 24608.
I0304 19:28:19.135269 22849303695488 run.py:483] Algo bellman_ford step 769 current loss 0.222003, current_train_items 24640.
I0304 19:28:19.154802 22849303695488 run.py:483] Algo bellman_ford step 770 current loss 0.025166, current_train_items 24672.
I0304 19:28:19.171520 22849303695488 run.py:483] Algo bellman_ford step 771 current loss 0.100471, current_train_items 24704.
I0304 19:28:19.194764 22849303695488 run.py:483] Algo bellman_ford step 772 current loss 0.233808, current_train_items 24736.
I0304 19:28:19.224312 22849303695488 run.py:483] Algo bellman_ford step 773 current loss 0.112416, current_train_items 24768.
I0304 19:28:19.257139 22849303695488 run.py:483] Algo bellman_ford step 774 current loss 0.228050, current_train_items 24800.
I0304 19:28:19.276570 22849303695488 run.py:483] Algo bellman_ford step 775 current loss 0.025668, current_train_items 24832.
I0304 19:28:19.293962 22849303695488 run.py:483] Algo bellman_ford step 776 current loss 0.052847, current_train_items 24864.
I0304 19:28:19.317993 22849303695488 run.py:483] Algo bellman_ford step 777 current loss 0.098578, current_train_items 24896.
I0304 19:28:19.347029 22849303695488 run.py:483] Algo bellman_ford step 778 current loss 0.194170, current_train_items 24928.
I0304 19:28:19.378662 22849303695488 run.py:483] Algo bellman_ford step 779 current loss 0.175250, current_train_items 24960.
I0304 19:28:19.397562 22849303695488 run.py:483] Algo bellman_ford step 780 current loss 0.031266, current_train_items 24992.
I0304 19:28:19.414386 22849303695488 run.py:483] Algo bellman_ford step 781 current loss 0.062840, current_train_items 25024.
I0304 19:28:19.437869 22849303695488 run.py:483] Algo bellman_ford step 782 current loss 0.080949, current_train_items 25056.
I0304 19:28:19.467517 22849303695488 run.py:483] Algo bellman_ford step 783 current loss 0.221519, current_train_items 25088.
I0304 19:28:19.500511 22849303695488 run.py:483] Algo bellman_ford step 784 current loss 0.244164, current_train_items 25120.
I0304 19:28:19.519661 22849303695488 run.py:483] Algo bellman_ford step 785 current loss 0.020696, current_train_items 25152.
I0304 19:28:19.536565 22849303695488 run.py:483] Algo bellman_ford step 786 current loss 0.084380, current_train_items 25184.
I0304 19:28:19.559888 22849303695488 run.py:483] Algo bellman_ford step 787 current loss 0.106731, current_train_items 25216.
I0304 19:28:19.589918 22849303695488 run.py:483] Algo bellman_ford step 788 current loss 0.195881, current_train_items 25248.
I0304 19:28:19.620066 22849303695488 run.py:483] Algo bellman_ford step 789 current loss 0.249847, current_train_items 25280.
I0304 19:28:19.639472 22849303695488 run.py:483] Algo bellman_ford step 790 current loss 0.023582, current_train_items 25312.
I0304 19:28:19.656464 22849303695488 run.py:483] Algo bellman_ford step 791 current loss 0.112298, current_train_items 25344.
I0304 19:28:19.678892 22849303695488 run.py:483] Algo bellman_ford step 792 current loss 0.120395, current_train_items 25376.
I0304 19:28:19.708781 22849303695488 run.py:483] Algo bellman_ford step 793 current loss 0.165385, current_train_items 25408.
I0304 19:28:19.739791 22849303695488 run.py:483] Algo bellman_ford step 794 current loss 0.196176, current_train_items 25440.
I0304 19:28:19.758553 22849303695488 run.py:483] Algo bellman_ford step 795 current loss 0.026667, current_train_items 25472.
I0304 19:28:19.774605 22849303695488 run.py:483] Algo bellman_ford step 796 current loss 0.062179, current_train_items 25504.
I0304 19:28:19.798615 22849303695488 run.py:483] Algo bellman_ford step 797 current loss 0.175634, current_train_items 25536.
I0304 19:28:19.827420 22849303695488 run.py:483] Algo bellman_ford step 798 current loss 0.155300, current_train_items 25568.
I0304 19:28:19.858974 22849303695488 run.py:483] Algo bellman_ford step 799 current loss 0.166985, current_train_items 25600.
I0304 19:28:19.878047 22849303695488 run.py:483] Algo bellman_ford step 800 current loss 0.013387, current_train_items 25632.
I0304 19:28:19.886070 22849303695488 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.9482421875, 'score': 0.9482421875, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0304 19:28:19.886178 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.975, current avg val score is 0.948, val scores are: bellman_ford: 0.948
I0304 19:28:19.903263 22849303695488 run.py:483] Algo bellman_ford step 801 current loss 0.099381, current_train_items 25664.
I0304 19:28:19.928418 22849303695488 run.py:483] Algo bellman_ford step 802 current loss 0.203928, current_train_items 25696.
I0304 19:28:19.958719 22849303695488 run.py:483] Algo bellman_ford step 803 current loss 0.240646, current_train_items 25728.
I0304 19:28:19.990557 22849303695488 run.py:483] Algo bellman_ford step 804 current loss 0.276782, current_train_items 25760.
I0304 19:28:20.010049 22849303695488 run.py:483] Algo bellman_ford step 805 current loss 0.023120, current_train_items 25792.
I0304 19:28:20.026234 22849303695488 run.py:483] Algo bellman_ford step 806 current loss 0.069778, current_train_items 25824.
I0304 19:28:20.050750 22849303695488 run.py:483] Algo bellman_ford step 807 current loss 0.109853, current_train_items 25856.
I0304 19:28:20.080601 22849303695488 run.py:483] Algo bellman_ford step 808 current loss 0.200272, current_train_items 25888.
I0304 19:28:20.111765 22849303695488 run.py:483] Algo bellman_ford step 809 current loss 0.204804, current_train_items 25920.
I0304 19:28:20.130877 22849303695488 run.py:483] Algo bellman_ford step 810 current loss 0.031226, current_train_items 25952.
I0304 19:28:20.147996 22849303695488 run.py:483] Algo bellman_ford step 811 current loss 0.115580, current_train_items 25984.
I0304 19:28:20.171550 22849303695488 run.py:483] Algo bellman_ford step 812 current loss 0.125167, current_train_items 26016.
I0304 19:28:20.200694 22849303695488 run.py:483] Algo bellman_ford step 813 current loss 0.165148, current_train_items 26048.
I0304 19:28:20.232692 22849303695488 run.py:483] Algo bellman_ford step 814 current loss 0.193307, current_train_items 26080.
I0304 19:28:20.251987 22849303695488 run.py:483] Algo bellman_ford step 815 current loss 0.076677, current_train_items 26112.
I0304 19:28:20.268956 22849303695488 run.py:483] Algo bellman_ford step 816 current loss 0.221641, current_train_items 26144.
I0304 19:28:20.292906 22849303695488 run.py:483] Algo bellman_ford step 817 current loss 0.105794, current_train_items 26176.
I0304 19:28:20.323703 22849303695488 run.py:483] Algo bellman_ford step 818 current loss 0.207132, current_train_items 26208.
I0304 19:28:20.357716 22849303695488 run.py:483] Algo bellman_ford step 819 current loss 0.269769, current_train_items 26240.
I0304 19:28:20.376991 22849303695488 run.py:483] Algo bellman_ford step 820 current loss 0.024873, current_train_items 26272.
I0304 19:28:20.393272 22849303695488 run.py:483] Algo bellman_ford step 821 current loss 0.045855, current_train_items 26304.
I0304 19:28:20.416735 22849303695488 run.py:483] Algo bellman_ford step 822 current loss 0.094824, current_train_items 26336.
I0304 19:28:20.447023 22849303695488 run.py:483] Algo bellman_ford step 823 current loss 0.225337, current_train_items 26368.
I0304 19:28:20.478142 22849303695488 run.py:483] Algo bellman_ford step 824 current loss 0.326601, current_train_items 26400.
I0304 19:28:20.497171 22849303695488 run.py:483] Algo bellman_ford step 825 current loss 0.042323, current_train_items 26432.
I0304 19:28:20.514040 22849303695488 run.py:483] Algo bellman_ford step 826 current loss 0.108498, current_train_items 26464.
I0304 19:28:20.538577 22849303695488 run.py:483] Algo bellman_ford step 827 current loss 0.223577, current_train_items 26496.
I0304 19:28:20.568682 22849303695488 run.py:483] Algo bellman_ford step 828 current loss 0.281759, current_train_items 26528.
I0304 19:28:20.600545 22849303695488 run.py:483] Algo bellman_ford step 829 current loss 0.172323, current_train_items 26560.
I0304 19:28:20.619489 22849303695488 run.py:483] Algo bellman_ford step 830 current loss 0.015870, current_train_items 26592.
I0304 19:28:20.636023 22849303695488 run.py:483] Algo bellman_ford step 831 current loss 0.063757, current_train_items 26624.
I0304 19:28:20.659953 22849303695488 run.py:483] Algo bellman_ford step 832 current loss 0.201640, current_train_items 26656.
I0304 19:28:20.689896 22849303695488 run.py:483] Algo bellman_ford step 833 current loss 0.161978, current_train_items 26688.
I0304 19:28:20.720876 22849303695488 run.py:483] Algo bellman_ford step 834 current loss 0.150294, current_train_items 26720.
I0304 19:28:20.739766 22849303695488 run.py:483] Algo bellman_ford step 835 current loss 0.025554, current_train_items 26752.
I0304 19:28:20.756157 22849303695488 run.py:483] Algo bellman_ford step 836 current loss 0.055923, current_train_items 26784.
I0304 19:28:20.780132 22849303695488 run.py:483] Algo bellman_ford step 837 current loss 0.204307, current_train_items 26816.
I0304 19:28:20.809997 22849303695488 run.py:483] Algo bellman_ford step 838 current loss 0.119155, current_train_items 26848.
I0304 19:28:20.842779 22849303695488 run.py:483] Algo bellman_ford step 839 current loss 0.269040, current_train_items 26880.
I0304 19:28:20.861440 22849303695488 run.py:483] Algo bellman_ford step 840 current loss 0.057358, current_train_items 26912.
I0304 19:28:20.878031 22849303695488 run.py:483] Algo bellman_ford step 841 current loss 0.070332, current_train_items 26944.
I0304 19:28:20.902494 22849303695488 run.py:483] Algo bellman_ford step 842 current loss 0.135877, current_train_items 26976.
I0304 19:28:20.931344 22849303695488 run.py:483] Algo bellman_ford step 843 current loss 0.140335, current_train_items 27008.
I0304 19:28:20.962398 22849303695488 run.py:483] Algo bellman_ford step 844 current loss 0.154792, current_train_items 27040.
I0304 19:28:20.981295 22849303695488 run.py:483] Algo bellman_ford step 845 current loss 0.044344, current_train_items 27072.
I0304 19:28:20.997952 22849303695488 run.py:483] Algo bellman_ford step 846 current loss 0.069244, current_train_items 27104.
I0304 19:28:21.023097 22849303695488 run.py:483] Algo bellman_ford step 847 current loss 0.178580, current_train_items 27136.
I0304 19:28:21.052367 22849303695488 run.py:483] Algo bellman_ford step 848 current loss 0.249117, current_train_items 27168.
I0304 19:28:21.084780 22849303695488 run.py:483] Algo bellman_ford step 849 current loss 0.303401, current_train_items 27200.
I0304 19:28:21.103831 22849303695488 run.py:483] Algo bellman_ford step 850 current loss 0.017315, current_train_items 27232.
I0304 19:28:21.112085 22849303695488 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0304 19:28:21.112193 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.975, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0304 19:28:21.128876 22849303695488 run.py:483] Algo bellman_ford step 851 current loss 0.040696, current_train_items 27264.
I0304 19:28:21.153145 22849303695488 run.py:483] Algo bellman_ford step 852 current loss 0.207014, current_train_items 27296.
I0304 19:28:21.184800 22849303695488 run.py:483] Algo bellman_ford step 853 current loss 0.270618, current_train_items 27328.
I0304 19:28:21.216993 22849303695488 run.py:483] Algo bellman_ford step 854 current loss 0.214830, current_train_items 27360.
I0304 19:28:21.236548 22849303695488 run.py:483] Algo bellman_ford step 855 current loss 0.043447, current_train_items 27392.
I0304 19:28:21.252991 22849303695488 run.py:483] Algo bellman_ford step 856 current loss 0.065651, current_train_items 27424.
I0304 19:28:21.276467 22849303695488 run.py:483] Algo bellman_ford step 857 current loss 0.257276, current_train_items 27456.
I0304 19:28:21.307620 22849303695488 run.py:483] Algo bellman_ford step 858 current loss 0.403125, current_train_items 27488.
I0304 19:28:21.339162 22849303695488 run.py:483] Algo bellman_ford step 859 current loss 0.258682, current_train_items 27520.
I0304 19:28:21.358498 22849303695488 run.py:483] Algo bellman_ford step 860 current loss 0.019007, current_train_items 27552.
I0304 19:28:21.375378 22849303695488 run.py:483] Algo bellman_ford step 861 current loss 0.099333, current_train_items 27584.
I0304 19:28:21.399239 22849303695488 run.py:483] Algo bellman_ford step 862 current loss 0.181885, current_train_items 27616.
I0304 19:28:21.429393 22849303695488 run.py:483] Algo bellman_ford step 863 current loss 0.314043, current_train_items 27648.
I0304 19:28:21.460756 22849303695488 run.py:483] Algo bellman_ford step 864 current loss 0.285538, current_train_items 27680.
I0304 19:28:21.479916 22849303695488 run.py:483] Algo bellman_ford step 865 current loss 0.016865, current_train_items 27712.
I0304 19:28:21.496029 22849303695488 run.py:483] Algo bellman_ford step 866 current loss 0.112216, current_train_items 27744.
I0304 19:28:21.520555 22849303695488 run.py:483] Algo bellman_ford step 867 current loss 0.324548, current_train_items 27776.
I0304 19:28:21.550396 22849303695488 run.py:483] Algo bellman_ford step 868 current loss 0.216084, current_train_items 27808.
I0304 19:28:21.583919 22849303695488 run.py:483] Algo bellman_ford step 869 current loss 0.213139, current_train_items 27840.
I0304 19:28:21.603653 22849303695488 run.py:483] Algo bellman_ford step 870 current loss 0.084847, current_train_items 27872.
I0304 19:28:21.620441 22849303695488 run.py:483] Algo bellman_ford step 871 current loss 0.094574, current_train_items 27904.
I0304 19:28:21.643154 22849303695488 run.py:483] Algo bellman_ford step 872 current loss 0.160641, current_train_items 27936.
I0304 19:28:21.673642 22849303695488 run.py:483] Algo bellman_ford step 873 current loss 0.226129, current_train_items 27968.
I0304 19:28:21.705344 22849303695488 run.py:483] Algo bellman_ford step 874 current loss 0.246502, current_train_items 28000.
I0304 19:28:21.724856 22849303695488 run.py:483] Algo bellman_ford step 875 current loss 0.032010, current_train_items 28032.
I0304 19:28:21.741972 22849303695488 run.py:483] Algo bellman_ford step 876 current loss 0.194637, current_train_items 28064.
I0304 19:28:21.767994 22849303695488 run.py:483] Algo bellman_ford step 877 current loss 0.318917, current_train_items 28096.
I0304 19:28:21.797210 22849303695488 run.py:483] Algo bellman_ford step 878 current loss 0.161832, current_train_items 28128.
I0304 19:28:21.830550 22849303695488 run.py:483] Algo bellman_ford step 879 current loss 0.247191, current_train_items 28160.
I0304 19:28:21.849720 22849303695488 run.py:483] Algo bellman_ford step 880 current loss 0.017157, current_train_items 28192.
I0304 19:28:21.865977 22849303695488 run.py:483] Algo bellman_ford step 881 current loss 0.036284, current_train_items 28224.
I0304 19:28:21.889291 22849303695488 run.py:483] Algo bellman_ford step 882 current loss 0.256122, current_train_items 28256.
I0304 19:28:21.919934 22849303695488 run.py:483] Algo bellman_ford step 883 current loss 0.283729, current_train_items 28288.
I0304 19:28:21.953641 22849303695488 run.py:483] Algo bellman_ford step 884 current loss 0.263127, current_train_items 28320.
I0304 19:28:21.973413 22849303695488 run.py:483] Algo bellman_ford step 885 current loss 0.036336, current_train_items 28352.
I0304 19:28:21.989991 22849303695488 run.py:483] Algo bellman_ford step 886 current loss 0.154072, current_train_items 28384.
I0304 19:28:22.014123 22849303695488 run.py:483] Algo bellman_ford step 887 current loss 0.180418, current_train_items 28416.
I0304 19:28:22.044225 22849303695488 run.py:483] Algo bellman_ford step 888 current loss 0.172860, current_train_items 28448.
I0304 19:28:22.077604 22849303695488 run.py:483] Algo bellman_ford step 889 current loss 0.245582, current_train_items 28480.
I0304 19:28:22.097444 22849303695488 run.py:483] Algo bellman_ford step 890 current loss 0.045574, current_train_items 28512.
I0304 19:28:22.114185 22849303695488 run.py:483] Algo bellman_ford step 891 current loss 0.088063, current_train_items 28544.
I0304 19:28:22.138234 22849303695488 run.py:483] Algo bellman_ford step 892 current loss 0.349935, current_train_items 28576.
I0304 19:28:22.169362 22849303695488 run.py:483] Algo bellman_ford step 893 current loss 0.311370, current_train_items 28608.
I0304 19:28:22.201285 22849303695488 run.py:483] Algo bellman_ford step 894 current loss 0.208544, current_train_items 28640.
I0304 19:28:22.220424 22849303695488 run.py:483] Algo bellman_ford step 895 current loss 0.041010, current_train_items 28672.
I0304 19:28:22.236761 22849303695488 run.py:483] Algo bellman_ford step 896 current loss 0.123783, current_train_items 28704.
I0304 19:28:22.261443 22849303695488 run.py:483] Algo bellman_ford step 897 current loss 0.414894, current_train_items 28736.
I0304 19:28:22.292091 22849303695488 run.py:483] Algo bellman_ford step 898 current loss 0.366525, current_train_items 28768.
I0304 19:28:22.326108 22849303695488 run.py:483] Algo bellman_ford step 899 current loss 0.364648, current_train_items 28800.
I0304 19:28:22.345800 22849303695488 run.py:483] Algo bellman_ford step 900 current loss 0.019606, current_train_items 28832.
I0304 19:28:22.353894 22849303695488 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.9560546875, 'score': 0.9560546875, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0304 19:28:22.354016 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.975, current avg val score is 0.956, val scores are: bellman_ford: 0.956
I0304 19:28:22.371543 22849303695488 run.py:483] Algo bellman_ford step 901 current loss 0.116336, current_train_items 28864.
I0304 19:28:22.396322 22849303695488 run.py:483] Algo bellman_ford step 902 current loss 0.236734, current_train_items 28896.
I0304 19:28:22.427361 22849303695488 run.py:483] Algo bellman_ford step 903 current loss 0.231361, current_train_items 28928.
I0304 19:28:22.460405 22849303695488 run.py:483] Algo bellman_ford step 904 current loss 0.288990, current_train_items 28960.
I0304 19:28:22.479632 22849303695488 run.py:483] Algo bellman_ford step 905 current loss 0.015456, current_train_items 28992.
I0304 19:28:22.496326 22849303695488 run.py:483] Algo bellman_ford step 906 current loss 0.105306, current_train_items 29024.
I0304 19:28:22.520467 22849303695488 run.py:483] Algo bellman_ford step 907 current loss 0.169475, current_train_items 29056.
I0304 19:28:22.551342 22849303695488 run.py:483] Algo bellman_ford step 908 current loss 0.259093, current_train_items 29088.
I0304 19:28:22.584101 22849303695488 run.py:483] Algo bellman_ford step 909 current loss 0.170521, current_train_items 29120.
I0304 19:28:22.603061 22849303695488 run.py:483] Algo bellman_ford step 910 current loss 0.025860, current_train_items 29152.
I0304 19:28:22.620296 22849303695488 run.py:483] Algo bellman_ford step 911 current loss 0.171786, current_train_items 29184.
I0304 19:28:22.644714 22849303695488 run.py:483] Algo bellman_ford step 912 current loss 0.318457, current_train_items 29216.
I0304 19:28:22.673127 22849303695488 run.py:483] Algo bellman_ford step 913 current loss 0.278456, current_train_items 29248.
I0304 19:28:22.706176 22849303695488 run.py:483] Algo bellman_ford step 914 current loss 0.351154, current_train_items 29280.
I0304 19:28:22.724966 22849303695488 run.py:483] Algo bellman_ford step 915 current loss 0.014262, current_train_items 29312.
I0304 19:28:22.741804 22849303695488 run.py:483] Algo bellman_ford step 916 current loss 0.060886, current_train_items 29344.
I0304 19:28:22.766040 22849303695488 run.py:483] Algo bellman_ford step 917 current loss 0.247201, current_train_items 29376.
I0304 19:28:22.796084 22849303695488 run.py:483] Algo bellman_ford step 918 current loss 0.203362, current_train_items 29408.
I0304 19:28:22.829587 22849303695488 run.py:483] Algo bellman_ford step 919 current loss 0.243372, current_train_items 29440.
I0304 19:28:22.848735 22849303695488 run.py:483] Algo bellman_ford step 920 current loss 0.043647, current_train_items 29472.
I0304 19:28:22.865924 22849303695488 run.py:483] Algo bellman_ford step 921 current loss 0.089496, current_train_items 29504.
I0304 19:28:22.890291 22849303695488 run.py:483] Algo bellman_ford step 922 current loss 0.196089, current_train_items 29536.
I0304 19:28:22.919647 22849303695488 run.py:483] Algo bellman_ford step 923 current loss 0.177118, current_train_items 29568.
I0304 19:28:22.951665 22849303695488 run.py:483] Algo bellman_ford step 924 current loss 0.273223, current_train_items 29600.
I0304 19:28:22.970622 22849303695488 run.py:483] Algo bellman_ford step 925 current loss 0.022974, current_train_items 29632.
I0304 19:28:22.986808 22849303695488 run.py:483] Algo bellman_ford step 926 current loss 0.067720, current_train_items 29664.
I0304 19:28:23.010858 22849303695488 run.py:483] Algo bellman_ford step 927 current loss 0.232608, current_train_items 29696.
I0304 19:28:23.041859 22849303695488 run.py:483] Algo bellman_ford step 928 current loss 0.231516, current_train_items 29728.
I0304 19:28:23.072732 22849303695488 run.py:483] Algo bellman_ford step 929 current loss 0.198813, current_train_items 29760.
I0304 19:28:23.091732 22849303695488 run.py:483] Algo bellman_ford step 930 current loss 0.033782, current_train_items 29792.
I0304 19:28:23.108340 22849303695488 run.py:483] Algo bellman_ford step 931 current loss 0.072161, current_train_items 29824.
I0304 19:28:23.132369 22849303695488 run.py:483] Algo bellman_ford step 932 current loss 0.155681, current_train_items 29856.
I0304 19:28:23.161590 22849303695488 run.py:483] Algo bellman_ford step 933 current loss 0.149703, current_train_items 29888.
I0304 19:28:23.194380 22849303695488 run.py:483] Algo bellman_ford step 934 current loss 0.228774, current_train_items 29920.
I0304 19:28:23.212966 22849303695488 run.py:483] Algo bellman_ford step 935 current loss 0.017106, current_train_items 29952.
I0304 19:28:23.229474 22849303695488 run.py:483] Algo bellman_ford step 936 current loss 0.106511, current_train_items 29984.
I0304 19:28:23.252719 22849303695488 run.py:483] Algo bellman_ford step 937 current loss 0.128428, current_train_items 30016.
I0304 19:28:23.282379 22849303695488 run.py:483] Algo bellman_ford step 938 current loss 0.156792, current_train_items 30048.
I0304 19:28:23.314655 22849303695488 run.py:483] Algo bellman_ford step 939 current loss 0.287054, current_train_items 30080.
I0304 19:28:23.333361 22849303695488 run.py:483] Algo bellman_ford step 940 current loss 0.029097, current_train_items 30112.
I0304 19:28:23.349994 22849303695488 run.py:483] Algo bellman_ford step 941 current loss 0.115974, current_train_items 30144.
I0304 19:28:23.374359 22849303695488 run.py:483] Algo bellman_ford step 942 current loss 0.185147, current_train_items 30176.
I0304 19:28:23.404474 22849303695488 run.py:483] Algo bellman_ford step 943 current loss 0.160089, current_train_items 30208.
I0304 19:28:23.437492 22849303695488 run.py:483] Algo bellman_ford step 944 current loss 0.219586, current_train_items 30240.
I0304 19:28:23.456423 22849303695488 run.py:483] Algo bellman_ford step 945 current loss 0.018152, current_train_items 30272.
I0304 19:28:23.472388 22849303695488 run.py:483] Algo bellman_ford step 946 current loss 0.171776, current_train_items 30304.
I0304 19:28:23.497202 22849303695488 run.py:483] Algo bellman_ford step 947 current loss 0.370884, current_train_items 30336.
I0304 19:28:23.526823 22849303695488 run.py:483] Algo bellman_ford step 948 current loss 0.275443, current_train_items 30368.
I0304 19:28:23.558679 22849303695488 run.py:483] Algo bellman_ford step 949 current loss 0.235877, current_train_items 30400.
I0304 19:28:23.577708 22849303695488 run.py:483] Algo bellman_ford step 950 current loss 0.021041, current_train_items 30432.
I0304 19:28:23.586064 22849303695488 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.9521484375, 'score': 0.9521484375, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0304 19:28:23.586171 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.975, current avg val score is 0.952, val scores are: bellman_ford: 0.952
I0304 19:28:23.603272 22849303695488 run.py:483] Algo bellman_ford step 951 current loss 0.253498, current_train_items 30464.
I0304 19:28:23.627653 22849303695488 run.py:483] Algo bellman_ford step 952 current loss 0.301702, current_train_items 30496.
I0304 19:28:23.658158 22849303695488 run.py:483] Algo bellman_ford step 953 current loss 0.355898, current_train_items 30528.
I0304 19:28:23.688227 22849303695488 run.py:483] Algo bellman_ford step 954 current loss 0.199154, current_train_items 30560.
I0304 19:28:23.707385 22849303695488 run.py:483] Algo bellman_ford step 955 current loss 0.041699, current_train_items 30592.
I0304 19:28:23.723788 22849303695488 run.py:483] Algo bellman_ford step 956 current loss 0.071482, current_train_items 30624.
I0304 19:28:23.748043 22849303695488 run.py:483] Algo bellman_ford step 957 current loss 0.123508, current_train_items 30656.
I0304 19:28:23.776650 22849303695488 run.py:483] Algo bellman_ford step 958 current loss 0.202036, current_train_items 30688.
I0304 19:28:23.809013 22849303695488 run.py:483] Algo bellman_ford step 959 current loss 0.228784, current_train_items 30720.
I0304 19:28:23.828580 22849303695488 run.py:483] Algo bellman_ford step 960 current loss 0.035168, current_train_items 30752.
I0304 19:28:23.845745 22849303695488 run.py:483] Algo bellman_ford step 961 current loss 0.087099, current_train_items 30784.
I0304 19:28:23.867941 22849303695488 run.py:483] Algo bellman_ford step 962 current loss 0.085012, current_train_items 30816.
I0304 19:28:23.896345 22849303695488 run.py:483] Algo bellman_ford step 963 current loss 0.155982, current_train_items 30848.
I0304 19:28:23.929515 22849303695488 run.py:483] Algo bellman_ford step 964 current loss 0.223104, current_train_items 30880.
I0304 19:28:23.948587 22849303695488 run.py:483] Algo bellman_ford step 965 current loss 0.056119, current_train_items 30912.
I0304 19:28:23.965134 22849303695488 run.py:483] Algo bellman_ford step 966 current loss 0.090240, current_train_items 30944.
I0304 19:28:23.989741 22849303695488 run.py:483] Algo bellman_ford step 967 current loss 0.210131, current_train_items 30976.
I0304 19:28:24.020159 22849303695488 run.py:483] Algo bellman_ford step 968 current loss 0.176373, current_train_items 31008.
I0304 19:28:24.051879 22849303695488 run.py:483] Algo bellman_ford step 969 current loss 0.200326, current_train_items 31040.
I0304 19:28:24.071106 22849303695488 run.py:483] Algo bellman_ford step 970 current loss 0.013449, current_train_items 31072.
I0304 19:28:24.088013 22849303695488 run.py:483] Algo bellman_ford step 971 current loss 0.051071, current_train_items 31104.
I0304 19:28:24.112243 22849303695488 run.py:483] Algo bellman_ford step 972 current loss 0.186676, current_train_items 31136.
I0304 19:28:24.143182 22849303695488 run.py:483] Algo bellman_ford step 973 current loss 0.381505, current_train_items 31168.
I0304 19:28:24.176895 22849303695488 run.py:483] Algo bellman_ford step 974 current loss 0.391896, current_train_items 31200.
I0304 19:28:24.196129 22849303695488 run.py:483] Algo bellman_ford step 975 current loss 0.024759, current_train_items 31232.
I0304 19:28:24.212589 22849303695488 run.py:483] Algo bellman_ford step 976 current loss 0.044788, current_train_items 31264.
I0304 19:28:24.236620 22849303695488 run.py:483] Algo bellman_ford step 977 current loss 0.114435, current_train_items 31296.
I0304 19:28:24.266021 22849303695488 run.py:483] Algo bellman_ford step 978 current loss 0.156314, current_train_items 31328.
I0304 19:28:24.297406 22849303695488 run.py:483] Algo bellman_ford step 979 current loss 0.184351, current_train_items 31360.
I0304 19:28:24.316071 22849303695488 run.py:483] Algo bellman_ford step 980 current loss 0.036612, current_train_items 31392.
I0304 19:28:24.332736 22849303695488 run.py:483] Algo bellman_ford step 981 current loss 0.057430, current_train_items 31424.
I0304 19:28:24.356436 22849303695488 run.py:483] Algo bellman_ford step 982 current loss 0.122529, current_train_items 31456.
I0304 19:28:24.385484 22849303695488 run.py:483] Algo bellman_ford step 983 current loss 0.121823, current_train_items 31488.
I0304 19:28:24.417945 22849303695488 run.py:483] Algo bellman_ford step 984 current loss 0.160122, current_train_items 31520.
I0304 19:28:24.437296 22849303695488 run.py:483] Algo bellman_ford step 985 current loss 0.017307, current_train_items 31552.
I0304 19:28:24.454389 22849303695488 run.py:483] Algo bellman_ford step 986 current loss 0.103262, current_train_items 31584.
I0304 19:28:24.477594 22849303695488 run.py:483] Algo bellman_ford step 987 current loss 0.082141, current_train_items 31616.
I0304 19:28:24.507481 22849303695488 run.py:483] Algo bellman_ford step 988 current loss 0.149542, current_train_items 31648.
I0304 19:28:24.538899 22849303695488 run.py:483] Algo bellman_ford step 989 current loss 0.155546, current_train_items 31680.
I0304 19:28:24.558208 22849303695488 run.py:483] Algo bellman_ford step 990 current loss 0.024861, current_train_items 31712.
I0304 19:28:24.574472 22849303695488 run.py:483] Algo bellman_ford step 991 current loss 0.033531, current_train_items 31744.
I0304 19:28:24.598608 22849303695488 run.py:483] Algo bellman_ford step 992 current loss 0.145177, current_train_items 31776.
I0304 19:28:24.627084 22849303695488 run.py:483] Algo bellman_ford step 993 current loss 0.130173, current_train_items 31808.
I0304 19:28:24.661626 22849303695488 run.py:483] Algo bellman_ford step 994 current loss 0.245901, current_train_items 31840.
I0304 19:28:24.680549 22849303695488 run.py:483] Algo bellman_ford step 995 current loss 0.022028, current_train_items 31872.
I0304 19:28:24.696901 22849303695488 run.py:483] Algo bellman_ford step 996 current loss 0.034364, current_train_items 31904.
I0304 19:28:24.720554 22849303695488 run.py:483] Algo bellman_ford step 997 current loss 0.282523, current_train_items 31936.
I0304 19:28:24.751069 22849303695488 run.py:483] Algo bellman_ford step 998 current loss 0.175204, current_train_items 31968.
I0304 19:28:24.782836 22849303695488 run.py:483] Algo bellman_ford step 999 current loss 0.174942, current_train_items 32000.
I0304 19:28:24.802093 22849303695488 run.py:483] Algo bellman_ford step 1000 current loss 0.037125, current_train_items 32032.
I0304 19:28:24.810053 22849303695488 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0304 19:28:24.810162 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.975, current avg val score is 0.955, val scores are: bellman_ford: 0.955
I0304 19:28:24.827304 22849303695488 run.py:483] Algo bellman_ford step 1001 current loss 0.106388, current_train_items 32064.
I0304 19:28:24.851547 22849303695488 run.py:483] Algo bellman_ford step 1002 current loss 0.202325, current_train_items 32096.
I0304 19:28:24.880131 22849303695488 run.py:483] Algo bellman_ford step 1003 current loss 0.239553, current_train_items 32128.
I0304 19:28:24.915253 22849303695488 run.py:483] Algo bellman_ford step 1004 current loss 0.362907, current_train_items 32160.
I0304 19:28:24.934557 22849303695488 run.py:483] Algo bellman_ford step 1005 current loss 0.040015, current_train_items 32192.
I0304 19:28:24.950664 22849303695488 run.py:483] Algo bellman_ford step 1006 current loss 0.073153, current_train_items 32224.
I0304 19:28:24.975898 22849303695488 run.py:483] Algo bellman_ford step 1007 current loss 0.226848, current_train_items 32256.
I0304 19:28:25.004888 22849303695488 run.py:483] Algo bellman_ford step 1008 current loss 0.248157, current_train_items 32288.
I0304 19:28:25.038161 22849303695488 run.py:483] Algo bellman_ford step 1009 current loss 0.253456, current_train_items 32320.
I0304 19:28:25.056979 22849303695488 run.py:483] Algo bellman_ford step 1010 current loss 0.021270, current_train_items 32352.
I0304 19:28:25.073626 22849303695488 run.py:483] Algo bellman_ford step 1011 current loss 0.115255, current_train_items 32384.
I0304 19:28:25.098783 22849303695488 run.py:483] Algo bellman_ford step 1012 current loss 0.199998, current_train_items 32416.
I0304 19:28:25.128906 22849303695488 run.py:483] Algo bellman_ford step 1013 current loss 0.266927, current_train_items 32448.
I0304 19:28:25.161770 22849303695488 run.py:483] Algo bellman_ford step 1014 current loss 0.161747, current_train_items 32480.
I0304 19:28:25.180781 22849303695488 run.py:483] Algo bellman_ford step 1015 current loss 0.018972, current_train_items 32512.
I0304 19:28:25.197687 22849303695488 run.py:483] Algo bellman_ford step 1016 current loss 0.095022, current_train_items 32544.
I0304 19:28:25.221815 22849303695488 run.py:483] Algo bellman_ford step 1017 current loss 0.341057, current_train_items 32576.
I0304 19:28:25.252156 22849303695488 run.py:483] Algo bellman_ford step 1018 current loss 0.279972, current_train_items 32608.
I0304 19:28:25.287226 22849303695488 run.py:483] Algo bellman_ford step 1019 current loss 0.380893, current_train_items 32640.
I0304 19:28:25.306057 22849303695488 run.py:483] Algo bellman_ford step 1020 current loss 0.030468, current_train_items 32672.
I0304 19:28:25.322422 22849303695488 run.py:483] Algo bellman_ford step 1021 current loss 0.095581, current_train_items 32704.
I0304 19:28:25.346658 22849303695488 run.py:483] Algo bellman_ford step 1022 current loss 0.157935, current_train_items 32736.
I0304 19:28:25.375523 22849303695488 run.py:483] Algo bellman_ford step 1023 current loss 0.197762, current_train_items 32768.
I0304 19:28:25.405995 22849303695488 run.py:483] Algo bellman_ford step 1024 current loss 0.230897, current_train_items 32800.
I0304 19:28:25.425072 22849303695488 run.py:483] Algo bellman_ford step 1025 current loss 0.045977, current_train_items 32832.
I0304 19:28:25.442114 22849303695488 run.py:483] Algo bellman_ford step 1026 current loss 0.099836, current_train_items 32864.
I0304 19:28:25.466115 22849303695488 run.py:483] Algo bellman_ford step 1027 current loss 0.156796, current_train_items 32896.
I0304 19:28:25.496742 22849303695488 run.py:483] Algo bellman_ford step 1028 current loss 0.158050, current_train_items 32928.
I0304 19:28:25.528254 22849303695488 run.py:483] Algo bellman_ford step 1029 current loss 0.221247, current_train_items 32960.
I0304 19:28:25.547093 22849303695488 run.py:483] Algo bellman_ford step 1030 current loss 0.024871, current_train_items 32992.
I0304 19:28:25.563252 22849303695488 run.py:483] Algo bellman_ford step 1031 current loss 0.035000, current_train_items 33024.
I0304 19:28:25.586746 22849303695488 run.py:483] Algo bellman_ford step 1032 current loss 0.077661, current_train_items 33056.
I0304 19:28:25.615617 22849303695488 run.py:483] Algo bellman_ford step 1033 current loss 0.185104, current_train_items 33088.
I0304 19:28:25.648503 22849303695488 run.py:483] Algo bellman_ford step 1034 current loss 0.222287, current_train_items 33120.
I0304 19:28:25.667331 22849303695488 run.py:483] Algo bellman_ford step 1035 current loss 0.021527, current_train_items 33152.
I0304 19:28:25.683854 22849303695488 run.py:483] Algo bellman_ford step 1036 current loss 0.041454, current_train_items 33184.
I0304 19:28:25.709023 22849303695488 run.py:483] Algo bellman_ford step 1037 current loss 0.129649, current_train_items 33216.
I0304 19:28:25.739277 22849303695488 run.py:483] Algo bellman_ford step 1038 current loss 0.184840, current_train_items 33248.
I0304 19:28:25.773253 22849303695488 run.py:483] Algo bellman_ford step 1039 current loss 0.204301, current_train_items 33280.
I0304 19:28:25.791913 22849303695488 run.py:483] Algo bellman_ford step 1040 current loss 0.021303, current_train_items 33312.
I0304 19:28:25.809112 22849303695488 run.py:483] Algo bellman_ford step 1041 current loss 0.075722, current_train_items 33344.
I0304 19:28:25.833253 22849303695488 run.py:483] Algo bellman_ford step 1042 current loss 0.223447, current_train_items 33376.
I0304 19:28:25.862720 22849303695488 run.py:483] Algo bellman_ford step 1043 current loss 0.178496, current_train_items 33408.
I0304 19:28:25.894517 22849303695488 run.py:483] Algo bellman_ford step 1044 current loss 0.142631, current_train_items 33440.
I0304 19:28:25.913307 22849303695488 run.py:483] Algo bellman_ford step 1045 current loss 0.009755, current_train_items 33472.
I0304 19:28:25.929592 22849303695488 run.py:483] Algo bellman_ford step 1046 current loss 0.042001, current_train_items 33504.
I0304 19:28:25.954323 22849303695488 run.py:483] Algo bellman_ford step 1047 current loss 0.141138, current_train_items 33536.
I0304 19:28:25.984978 22849303695488 run.py:483] Algo bellman_ford step 1048 current loss 0.165812, current_train_items 33568.
I0304 19:28:26.017958 22849303695488 run.py:483] Algo bellman_ford step 1049 current loss 0.192881, current_train_items 33600.
I0304 19:28:26.037000 22849303695488 run.py:483] Algo bellman_ford step 1050 current loss 0.084353, current_train_items 33632.
I0304 19:28:26.045338 22849303695488 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0304 19:28:26.045448 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.975, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:28:26.062330 22849303695488 run.py:483] Algo bellman_ford step 1051 current loss 0.054732, current_train_items 33664.
I0304 19:28:26.087042 22849303695488 run.py:483] Algo bellman_ford step 1052 current loss 0.159581, current_train_items 33696.
I0304 19:28:26.116465 22849303695488 run.py:483] Algo bellman_ford step 1053 current loss 0.166940, current_train_items 33728.
I0304 19:28:26.149301 22849303695488 run.py:483] Algo bellman_ford step 1054 current loss 0.275820, current_train_items 33760.
I0304 19:28:26.169081 22849303695488 run.py:483] Algo bellman_ford step 1055 current loss 0.030663, current_train_items 33792.
I0304 19:28:26.185573 22849303695488 run.py:483] Algo bellman_ford step 1056 current loss 0.064948, current_train_items 33824.
I0304 19:28:26.209998 22849303695488 run.py:483] Algo bellman_ford step 1057 current loss 0.150727, current_train_items 33856.
I0304 19:28:26.240696 22849303695488 run.py:483] Algo bellman_ford step 1058 current loss 0.136311, current_train_items 33888.
I0304 19:28:26.271216 22849303695488 run.py:483] Algo bellman_ford step 1059 current loss 0.112202, current_train_items 33920.
I0304 19:28:26.290211 22849303695488 run.py:483] Algo bellman_ford step 1060 current loss 0.010406, current_train_items 33952.
I0304 19:28:26.306722 22849303695488 run.py:483] Algo bellman_ford step 1061 current loss 0.047838, current_train_items 33984.
I0304 19:28:26.329034 22849303695488 run.py:483] Algo bellman_ford step 1062 current loss 0.074594, current_train_items 34016.
I0304 19:28:26.358331 22849303695488 run.py:483] Algo bellman_ford step 1063 current loss 0.184381, current_train_items 34048.
I0304 19:28:26.390892 22849303695488 run.py:483] Algo bellman_ford step 1064 current loss 0.192848, current_train_items 34080.
I0304 19:28:26.409786 22849303695488 run.py:483] Algo bellman_ford step 1065 current loss 0.017946, current_train_items 34112.
I0304 19:28:26.426246 22849303695488 run.py:483] Algo bellman_ford step 1066 current loss 0.074535, current_train_items 34144.
I0304 19:28:26.450225 22849303695488 run.py:483] Algo bellman_ford step 1067 current loss 0.134346, current_train_items 34176.
I0304 19:28:26.480167 22849303695488 run.py:483] Algo bellman_ford step 1068 current loss 0.121824, current_train_items 34208.
I0304 19:28:26.513821 22849303695488 run.py:483] Algo bellman_ford step 1069 current loss 0.183601, current_train_items 34240.
I0304 19:28:26.532851 22849303695488 run.py:483] Algo bellman_ford step 1070 current loss 0.012002, current_train_items 34272.
I0304 19:28:26.549783 22849303695488 run.py:483] Algo bellman_ford step 1071 current loss 0.098009, current_train_items 34304.
I0304 19:28:26.573421 22849303695488 run.py:483] Algo bellman_ford step 1072 current loss 0.142701, current_train_items 34336.
I0304 19:28:26.603235 22849303695488 run.py:483] Algo bellman_ford step 1073 current loss 0.113743, current_train_items 34368.
I0304 19:28:26.634278 22849303695488 run.py:483] Algo bellman_ford step 1074 current loss 0.159702, current_train_items 34400.
I0304 19:28:26.653418 22849303695488 run.py:483] Algo bellman_ford step 1075 current loss 0.015445, current_train_items 34432.
I0304 19:28:26.670227 22849303695488 run.py:483] Algo bellman_ford step 1076 current loss 0.102209, current_train_items 34464.
I0304 19:28:26.694062 22849303695488 run.py:483] Algo bellman_ford step 1077 current loss 0.117013, current_train_items 34496.
I0304 19:28:26.724425 22849303695488 run.py:483] Algo bellman_ford step 1078 current loss 0.151852, current_train_items 34528.
I0304 19:28:26.756076 22849303695488 run.py:483] Algo bellman_ford step 1079 current loss 0.172022, current_train_items 34560.
I0304 19:28:26.774873 22849303695488 run.py:483] Algo bellman_ford step 1080 current loss 0.033316, current_train_items 34592.
I0304 19:28:26.791737 22849303695488 run.py:483] Algo bellman_ford step 1081 current loss 0.068871, current_train_items 34624.
I0304 19:28:26.815065 22849303695488 run.py:483] Algo bellman_ford step 1082 current loss 0.134587, current_train_items 34656.
I0304 19:28:26.844375 22849303695488 run.py:483] Algo bellman_ford step 1083 current loss 0.098870, current_train_items 34688.
I0304 19:28:26.877466 22849303695488 run.py:483] Algo bellman_ford step 1084 current loss 0.189265, current_train_items 34720.
I0304 19:28:26.896599 22849303695488 run.py:483] Algo bellman_ford step 1085 current loss 0.013673, current_train_items 34752.
I0304 19:28:26.913356 22849303695488 run.py:483] Algo bellman_ford step 1086 current loss 0.046258, current_train_items 34784.
I0304 19:28:26.937082 22849303695488 run.py:483] Algo bellman_ford step 1087 current loss 0.110898, current_train_items 34816.
I0304 19:28:26.965867 22849303695488 run.py:483] Algo bellman_ford step 1088 current loss 0.084880, current_train_items 34848.
I0304 19:28:26.996393 22849303695488 run.py:483] Algo bellman_ford step 1089 current loss 0.169383, current_train_items 34880.
I0304 19:28:27.015706 22849303695488 run.py:483] Algo bellman_ford step 1090 current loss 0.026380, current_train_items 34912.
I0304 19:28:27.032742 22849303695488 run.py:483] Algo bellman_ford step 1091 current loss 0.140099, current_train_items 34944.
I0304 19:28:27.055874 22849303695488 run.py:483] Algo bellman_ford step 1092 current loss 0.121047, current_train_items 34976.
I0304 19:28:27.086830 22849303695488 run.py:483] Algo bellman_ford step 1093 current loss 0.207848, current_train_items 35008.
I0304 19:28:27.120269 22849303695488 run.py:483] Algo bellman_ford step 1094 current loss 0.150818, current_train_items 35040.
I0304 19:28:27.139389 22849303695488 run.py:483] Algo bellman_ford step 1095 current loss 0.019707, current_train_items 35072.
I0304 19:28:27.155833 22849303695488 run.py:483] Algo bellman_ford step 1096 current loss 0.099591, current_train_items 35104.
I0304 19:28:27.179694 22849303695488 run.py:483] Algo bellman_ford step 1097 current loss 0.157362, current_train_items 35136.
I0304 19:28:27.210068 22849303695488 run.py:483] Algo bellman_ford step 1098 current loss 0.288997, current_train_items 35168.
I0304 19:28:27.241683 22849303695488 run.py:483] Algo bellman_ford step 1099 current loss 0.284558, current_train_items 35200.
I0304 19:28:27.261144 22849303695488 run.py:483] Algo bellman_ford step 1100 current loss 0.022510, current_train_items 35232.
I0304 19:28:27.269072 22849303695488 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0304 19:28:27.269182 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.975, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:28:27.300053 22849303695488 run.py:483] Algo bellman_ford step 1101 current loss 0.054306, current_train_items 35264.
I0304 19:28:27.323691 22849303695488 run.py:483] Algo bellman_ford step 1102 current loss 0.068626, current_train_items 35296.
I0304 19:28:27.354938 22849303695488 run.py:483] Algo bellman_ford step 1103 current loss 0.128849, current_train_items 35328.
I0304 19:28:27.389707 22849303695488 run.py:483] Algo bellman_ford step 1104 current loss 0.235926, current_train_items 35360.
I0304 19:28:27.409079 22849303695488 run.py:483] Algo bellman_ford step 1105 current loss 0.023506, current_train_items 35392.
I0304 19:28:27.425377 22849303695488 run.py:483] Algo bellman_ford step 1106 current loss 0.053096, current_train_items 35424.
I0304 19:28:27.449683 22849303695488 run.py:483] Algo bellman_ford step 1107 current loss 0.169591, current_train_items 35456.
I0304 19:28:27.479197 22849303695488 run.py:483] Algo bellman_ford step 1108 current loss 0.099944, current_train_items 35488.
I0304 19:28:27.509713 22849303695488 run.py:483] Algo bellman_ford step 1109 current loss 0.129028, current_train_items 35520.
I0304 19:28:27.528787 22849303695488 run.py:483] Algo bellman_ford step 1110 current loss 0.035042, current_train_items 35552.
I0304 19:28:27.545481 22849303695488 run.py:483] Algo bellman_ford step 1111 current loss 0.073215, current_train_items 35584.
I0304 19:28:27.569540 22849303695488 run.py:483] Algo bellman_ford step 1112 current loss 0.135286, current_train_items 35616.
I0304 19:28:27.598014 22849303695488 run.py:483] Algo bellman_ford step 1113 current loss 0.109707, current_train_items 35648.
I0304 19:28:27.631329 22849303695488 run.py:483] Algo bellman_ford step 1114 current loss 0.171699, current_train_items 35680.
I0304 19:28:27.650628 22849303695488 run.py:483] Algo bellman_ford step 1115 current loss 0.017646, current_train_items 35712.
I0304 19:28:27.667078 22849303695488 run.py:483] Algo bellman_ford step 1116 current loss 0.040004, current_train_items 35744.
I0304 19:28:27.691827 22849303695488 run.py:483] Algo bellman_ford step 1117 current loss 0.119243, current_train_items 35776.
I0304 19:28:27.721931 22849303695488 run.py:483] Algo bellman_ford step 1118 current loss 0.068656, current_train_items 35808.
I0304 19:28:27.754758 22849303695488 run.py:483] Algo bellman_ford step 1119 current loss 0.182217, current_train_items 35840.
I0304 19:28:27.774239 22849303695488 run.py:483] Algo bellman_ford step 1120 current loss 0.016093, current_train_items 35872.
I0304 19:28:27.790425 22849303695488 run.py:483] Algo bellman_ford step 1121 current loss 0.049822, current_train_items 35904.
I0304 19:28:27.814980 22849303695488 run.py:483] Algo bellman_ford step 1122 current loss 0.185817, current_train_items 35936.
I0304 19:28:27.843732 22849303695488 run.py:483] Algo bellman_ford step 1123 current loss 0.111300, current_train_items 35968.
I0304 19:28:27.877536 22849303695488 run.py:483] Algo bellman_ford step 1124 current loss 0.173937, current_train_items 36000.
I0304 19:28:27.896559 22849303695488 run.py:483] Algo bellman_ford step 1125 current loss 0.030714, current_train_items 36032.
I0304 19:28:27.912730 22849303695488 run.py:483] Algo bellman_ford step 1126 current loss 0.113000, current_train_items 36064.
I0304 19:28:27.936617 22849303695488 run.py:483] Algo bellman_ford step 1127 current loss 0.298803, current_train_items 36096.
I0304 19:28:27.966861 22849303695488 run.py:483] Algo bellman_ford step 1128 current loss 0.271701, current_train_items 36128.
I0304 19:28:27.997108 22849303695488 run.py:483] Algo bellman_ford step 1129 current loss 0.156783, current_train_items 36160.
I0304 19:28:28.016128 22849303695488 run.py:483] Algo bellman_ford step 1130 current loss 0.023546, current_train_items 36192.
I0304 19:28:28.032758 22849303695488 run.py:483] Algo bellman_ford step 1131 current loss 0.076467, current_train_items 36224.
I0304 19:28:28.057119 22849303695488 run.py:483] Algo bellman_ford step 1132 current loss 0.408728, current_train_items 36256.
I0304 19:28:28.087195 22849303695488 run.py:483] Algo bellman_ford step 1133 current loss 0.351397, current_train_items 36288.
I0304 19:28:28.118655 22849303695488 run.py:483] Algo bellman_ford step 1134 current loss 0.228781, current_train_items 36320.
I0304 19:28:28.137473 22849303695488 run.py:483] Algo bellman_ford step 1135 current loss 0.034315, current_train_items 36352.
I0304 19:28:28.153931 22849303695488 run.py:483] Algo bellman_ford step 1136 current loss 0.055881, current_train_items 36384.
I0304 19:28:28.177466 22849303695488 run.py:483] Algo bellman_ford step 1137 current loss 0.116541, current_train_items 36416.
I0304 19:28:28.208451 22849303695488 run.py:483] Algo bellman_ford step 1138 current loss 0.171480, current_train_items 36448.
I0304 19:28:28.240317 22849303695488 run.py:483] Algo bellman_ford step 1139 current loss 0.217942, current_train_items 36480.
I0304 19:28:28.259253 22849303695488 run.py:483] Algo bellman_ford step 1140 current loss 0.016257, current_train_items 36512.
I0304 19:28:28.276398 22849303695488 run.py:483] Algo bellman_ford step 1141 current loss 0.039366, current_train_items 36544.
I0304 19:28:28.299380 22849303695488 run.py:483] Algo bellman_ford step 1142 current loss 0.202445, current_train_items 36576.
I0304 19:28:28.328747 22849303695488 run.py:483] Algo bellman_ford step 1143 current loss 0.239163, current_train_items 36608.
I0304 19:28:28.360791 22849303695488 run.py:483] Algo bellman_ford step 1144 current loss 0.139046, current_train_items 36640.
I0304 19:28:28.380277 22849303695488 run.py:483] Algo bellman_ford step 1145 current loss 0.036699, current_train_items 36672.
I0304 19:28:28.396797 22849303695488 run.py:483] Algo bellman_ford step 1146 current loss 0.096910, current_train_items 36704.
I0304 19:28:28.420831 22849303695488 run.py:483] Algo bellman_ford step 1147 current loss 0.156082, current_train_items 36736.
I0304 19:28:28.450904 22849303695488 run.py:483] Algo bellman_ford step 1148 current loss 0.192037, current_train_items 36768.
I0304 19:28:28.480677 22849303695488 run.py:483] Algo bellman_ford step 1149 current loss 0.181330, current_train_items 36800.
I0304 19:28:28.499572 22849303695488 run.py:483] Algo bellman_ford step 1150 current loss 0.036616, current_train_items 36832.
I0304 19:28:28.508001 22849303695488 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0304 19:28:28.508122 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.955, val scores are: bellman_ford: 0.955
I0304 19:28:28.525036 22849303695488 run.py:483] Algo bellman_ford step 1151 current loss 0.036149, current_train_items 36864.
I0304 19:28:28.549671 22849303695488 run.py:483] Algo bellman_ford step 1152 current loss 0.176748, current_train_items 36896.
I0304 19:28:28.581347 22849303695488 run.py:483] Algo bellman_ford step 1153 current loss 0.192879, current_train_items 36928.
W0304 19:28:28.602220 22849303695488 samplers.py:155] Increasing hint lengh from 11 to 12
I0304 19:28:35.420282 22849303695488 run.py:483] Algo bellman_ford step 1154 current loss 0.152972, current_train_items 36960.
I0304 19:28:35.441546 22849303695488 run.py:483] Algo bellman_ford step 1155 current loss 0.019960, current_train_items 36992.
I0304 19:28:35.457957 22849303695488 run.py:483] Algo bellman_ford step 1156 current loss 0.099034, current_train_items 37024.
I0304 19:28:35.482016 22849303695488 run.py:483] Algo bellman_ford step 1157 current loss 0.237559, current_train_items 37056.
I0304 19:28:35.512420 22849303695488 run.py:483] Algo bellman_ford step 1158 current loss 0.282862, current_train_items 37088.
I0304 19:28:35.543199 22849303695488 run.py:483] Algo bellman_ford step 1159 current loss 0.245552, current_train_items 37120.
I0304 19:28:35.563369 22849303695488 run.py:483] Algo bellman_ford step 1160 current loss 0.049868, current_train_items 37152.
I0304 19:28:35.580518 22849303695488 run.py:483] Algo bellman_ford step 1161 current loss 0.083973, current_train_items 37184.
I0304 19:28:35.603675 22849303695488 run.py:483] Algo bellman_ford step 1162 current loss 0.138236, current_train_items 37216.
I0304 19:28:35.633276 22849303695488 run.py:483] Algo bellman_ford step 1163 current loss 0.243822, current_train_items 37248.
I0304 19:28:35.666031 22849303695488 run.py:483] Algo bellman_ford step 1164 current loss 0.358848, current_train_items 37280.
I0304 19:28:35.685601 22849303695488 run.py:483] Algo bellman_ford step 1165 current loss 0.012695, current_train_items 37312.
I0304 19:28:35.702254 22849303695488 run.py:483] Algo bellman_ford step 1166 current loss 0.068463, current_train_items 37344.
I0304 19:28:35.726392 22849303695488 run.py:483] Algo bellman_ford step 1167 current loss 0.138025, current_train_items 37376.
I0304 19:28:35.756725 22849303695488 run.py:483] Algo bellman_ford step 1168 current loss 0.174983, current_train_items 37408.
I0304 19:28:35.790688 22849303695488 run.py:483] Algo bellman_ford step 1169 current loss 0.217026, current_train_items 37440.
I0304 19:28:35.810485 22849303695488 run.py:483] Algo bellman_ford step 1170 current loss 0.031467, current_train_items 37472.
I0304 19:28:35.827592 22849303695488 run.py:483] Algo bellman_ford step 1171 current loss 0.062368, current_train_items 37504.
I0304 19:28:35.851119 22849303695488 run.py:483] Algo bellman_ford step 1172 current loss 0.214380, current_train_items 37536.
I0304 19:28:35.882013 22849303695488 run.py:483] Algo bellman_ford step 1173 current loss 0.148634, current_train_items 37568.
I0304 19:28:35.914774 22849303695488 run.py:483] Algo bellman_ford step 1174 current loss 0.163361, current_train_items 37600.
I0304 19:28:35.934818 22849303695488 run.py:483] Algo bellman_ford step 1175 current loss 0.010815, current_train_items 37632.
I0304 19:28:35.951743 22849303695488 run.py:483] Algo bellman_ford step 1176 current loss 0.055287, current_train_items 37664.
I0304 19:28:35.974616 22849303695488 run.py:483] Algo bellman_ford step 1177 current loss 0.179067, current_train_items 37696.
I0304 19:28:36.004026 22849303695488 run.py:483] Algo bellman_ford step 1178 current loss 0.145558, current_train_items 37728.
I0304 19:28:36.038106 22849303695488 run.py:483] Algo bellman_ford step 1179 current loss 0.245446, current_train_items 37760.
I0304 19:28:36.057463 22849303695488 run.py:483] Algo bellman_ford step 1180 current loss 0.013742, current_train_items 37792.
I0304 19:28:36.074268 22849303695488 run.py:483] Algo bellman_ford step 1181 current loss 0.102604, current_train_items 37824.
I0304 19:28:36.097882 22849303695488 run.py:483] Algo bellman_ford step 1182 current loss 0.080855, current_train_items 37856.
I0304 19:28:36.126827 22849303695488 run.py:483] Algo bellman_ford step 1183 current loss 0.150878, current_train_items 37888.
I0304 19:28:36.159715 22849303695488 run.py:483] Algo bellman_ford step 1184 current loss 0.213416, current_train_items 37920.
I0304 19:28:36.179429 22849303695488 run.py:483] Algo bellman_ford step 1185 current loss 0.018168, current_train_items 37952.
I0304 19:28:36.195839 22849303695488 run.py:483] Algo bellman_ford step 1186 current loss 0.047360, current_train_items 37984.
I0304 19:28:36.219316 22849303695488 run.py:483] Algo bellman_ford step 1187 current loss 0.102153, current_train_items 38016.
I0304 19:28:36.248899 22849303695488 run.py:483] Algo bellman_ford step 1188 current loss 0.173077, current_train_items 38048.
I0304 19:28:36.280274 22849303695488 run.py:483] Algo bellman_ford step 1189 current loss 0.208846, current_train_items 38080.
I0304 19:28:36.300340 22849303695488 run.py:483] Algo bellman_ford step 1190 current loss 0.030420, current_train_items 38112.
I0304 19:28:36.317089 22849303695488 run.py:483] Algo bellman_ford step 1191 current loss 0.083572, current_train_items 38144.
I0304 19:28:36.341482 22849303695488 run.py:483] Algo bellman_ford step 1192 current loss 0.127799, current_train_items 38176.
I0304 19:28:36.371829 22849303695488 run.py:483] Algo bellman_ford step 1193 current loss 0.166843, current_train_items 38208.
I0304 19:28:36.404216 22849303695488 run.py:483] Algo bellman_ford step 1194 current loss 0.210446, current_train_items 38240.
I0304 19:28:36.423336 22849303695488 run.py:483] Algo bellman_ford step 1195 current loss 0.018712, current_train_items 38272.
I0304 19:28:36.439897 22849303695488 run.py:483] Algo bellman_ford step 1196 current loss 0.050620, current_train_items 38304.
I0304 19:28:36.463500 22849303695488 run.py:483] Algo bellman_ford step 1197 current loss 0.079761, current_train_items 38336.
I0304 19:28:36.494466 22849303695488 run.py:483] Algo bellman_ford step 1198 current loss 0.188099, current_train_items 38368.
I0304 19:28:36.528383 22849303695488 run.py:483] Algo bellman_ford step 1199 current loss 0.200973, current_train_items 38400.
I0304 19:28:36.548229 22849303695488 run.py:483] Algo bellman_ford step 1200 current loss 0.025617, current_train_items 38432.
I0304 19:28:36.558089 22849303695488 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0304 19:28:36.558197 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:28:36.575538 22849303695488 run.py:483] Algo bellman_ford step 1201 current loss 0.061609, current_train_items 38464.
I0304 19:28:36.600307 22849303695488 run.py:483] Algo bellman_ford step 1202 current loss 0.077015, current_train_items 38496.
I0304 19:28:36.631811 22849303695488 run.py:483] Algo bellman_ford step 1203 current loss 0.148223, current_train_items 38528.
I0304 19:28:36.665665 22849303695488 run.py:483] Algo bellman_ford step 1204 current loss 0.176843, current_train_items 38560.
I0304 19:28:36.685853 22849303695488 run.py:483] Algo bellman_ford step 1205 current loss 0.017064, current_train_items 38592.
I0304 19:28:36.702285 22849303695488 run.py:483] Algo bellman_ford step 1206 current loss 0.041289, current_train_items 38624.
I0304 19:28:36.725913 22849303695488 run.py:483] Algo bellman_ford step 1207 current loss 0.082227, current_train_items 38656.
I0304 19:28:36.756330 22849303695488 run.py:483] Algo bellman_ford step 1208 current loss 0.145083, current_train_items 38688.
I0304 19:28:36.790311 22849303695488 run.py:483] Algo bellman_ford step 1209 current loss 0.129628, current_train_items 38720.
I0304 19:28:36.809895 22849303695488 run.py:483] Algo bellman_ford step 1210 current loss 0.013011, current_train_items 38752.
I0304 19:28:36.826539 22849303695488 run.py:483] Algo bellman_ford step 1211 current loss 0.098932, current_train_items 38784.
I0304 19:28:36.849838 22849303695488 run.py:483] Algo bellman_ford step 1212 current loss 0.090788, current_train_items 38816.
I0304 19:28:36.878971 22849303695488 run.py:483] Algo bellman_ford step 1213 current loss 0.084137, current_train_items 38848.
I0304 19:28:36.912519 22849303695488 run.py:483] Algo bellman_ford step 1214 current loss 0.162764, current_train_items 38880.
I0304 19:28:36.932237 22849303695488 run.py:483] Algo bellman_ford step 1215 current loss 0.016955, current_train_items 38912.
I0304 19:28:36.948384 22849303695488 run.py:483] Algo bellman_ford step 1216 current loss 0.029554, current_train_items 38944.
I0304 19:28:36.971985 22849303695488 run.py:483] Algo bellman_ford step 1217 current loss 0.110780, current_train_items 38976.
I0304 19:28:37.003176 22849303695488 run.py:483] Algo bellman_ford step 1218 current loss 0.189484, current_train_items 39008.
I0304 19:28:37.036542 22849303695488 run.py:483] Algo bellman_ford step 1219 current loss 0.148771, current_train_items 39040.
I0304 19:28:37.056401 22849303695488 run.py:483] Algo bellman_ford step 1220 current loss 0.011324, current_train_items 39072.
I0304 19:28:37.072670 22849303695488 run.py:483] Algo bellman_ford step 1221 current loss 0.079422, current_train_items 39104.
I0304 19:28:37.097182 22849303695488 run.py:483] Algo bellman_ford step 1222 current loss 0.184059, current_train_items 39136.
I0304 19:28:37.128069 22849303695488 run.py:483] Algo bellman_ford step 1223 current loss 0.168248, current_train_items 39168.
I0304 19:28:37.160684 22849303695488 run.py:483] Algo bellman_ford step 1224 current loss 0.117012, current_train_items 39200.
I0304 19:28:37.179942 22849303695488 run.py:483] Algo bellman_ford step 1225 current loss 0.016808, current_train_items 39232.
I0304 19:28:37.196593 22849303695488 run.py:483] Algo bellman_ford step 1226 current loss 0.101641, current_train_items 39264.
I0304 19:28:37.221293 22849303695488 run.py:483] Algo bellman_ford step 1227 current loss 0.268252, current_train_items 39296.
I0304 19:28:37.251312 22849303695488 run.py:483] Algo bellman_ford step 1228 current loss 0.204953, current_train_items 39328.
I0304 19:28:37.285766 22849303695488 run.py:483] Algo bellman_ford step 1229 current loss 0.185235, current_train_items 39360.
I0304 19:28:37.305240 22849303695488 run.py:483] Algo bellman_ford step 1230 current loss 0.016342, current_train_items 39392.
I0304 19:28:37.321853 22849303695488 run.py:483] Algo bellman_ford step 1231 current loss 0.083610, current_train_items 39424.
I0304 19:28:37.346385 22849303695488 run.py:483] Algo bellman_ford step 1232 current loss 0.214044, current_train_items 39456.
I0304 19:28:37.376633 22849303695488 run.py:483] Algo bellman_ford step 1233 current loss 0.116260, current_train_items 39488.
I0304 19:28:37.407727 22849303695488 run.py:483] Algo bellman_ford step 1234 current loss 0.140426, current_train_items 39520.
I0304 19:28:37.427159 22849303695488 run.py:483] Algo bellman_ford step 1235 current loss 0.014007, current_train_items 39552.
I0304 19:28:37.443199 22849303695488 run.py:483] Algo bellman_ford step 1236 current loss 0.065678, current_train_items 39584.
I0304 19:28:37.466763 22849303695488 run.py:483] Algo bellman_ford step 1237 current loss 0.191843, current_train_items 39616.
I0304 19:28:37.497140 22849303695488 run.py:483] Algo bellman_ford step 1238 current loss 0.148425, current_train_items 39648.
I0304 19:28:37.531398 22849303695488 run.py:483] Algo bellman_ford step 1239 current loss 0.122036, current_train_items 39680.
I0304 19:28:37.550802 22849303695488 run.py:483] Algo bellman_ford step 1240 current loss 0.028476, current_train_items 39712.
I0304 19:28:37.567311 22849303695488 run.py:483] Algo bellman_ford step 1241 current loss 0.062259, current_train_items 39744.
I0304 19:28:37.591759 22849303695488 run.py:483] Algo bellman_ford step 1242 current loss 0.077221, current_train_items 39776.
I0304 19:28:37.622386 22849303695488 run.py:483] Algo bellman_ford step 1243 current loss 0.112816, current_train_items 39808.
I0304 19:28:37.655841 22849303695488 run.py:483] Algo bellman_ford step 1244 current loss 0.206822, current_train_items 39840.
I0304 19:28:37.675342 22849303695488 run.py:483] Algo bellman_ford step 1245 current loss 0.030192, current_train_items 39872.
I0304 19:28:37.691821 22849303695488 run.py:483] Algo bellman_ford step 1246 current loss 0.073571, current_train_items 39904.
I0304 19:28:37.715033 22849303695488 run.py:483] Algo bellman_ford step 1247 current loss 0.064087, current_train_items 39936.
I0304 19:28:37.746047 22849303695488 run.py:483] Algo bellman_ford step 1248 current loss 0.152595, current_train_items 39968.
I0304 19:28:37.778423 22849303695488 run.py:483] Algo bellman_ford step 1249 current loss 0.169790, current_train_items 40000.
I0304 19:28:37.798062 22849303695488 run.py:483] Algo bellman_ford step 1250 current loss 0.015400, current_train_items 40032.
I0304 19:28:37.806651 22849303695488 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0304 19:28:37.806760 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0304 19:28:37.823863 22849303695488 run.py:483] Algo bellman_ford step 1251 current loss 0.068588, current_train_items 40064.
I0304 19:28:37.848632 22849303695488 run.py:483] Algo bellman_ford step 1252 current loss 0.104948, current_train_items 40096.
I0304 19:28:37.877291 22849303695488 run.py:483] Algo bellman_ford step 1253 current loss 0.125851, current_train_items 40128.
I0304 19:28:37.907823 22849303695488 run.py:483] Algo bellman_ford step 1254 current loss 0.114652, current_train_items 40160.
I0304 19:28:37.927409 22849303695488 run.py:483] Algo bellman_ford step 1255 current loss 0.014624, current_train_items 40192.
I0304 19:28:37.943997 22849303695488 run.py:483] Algo bellman_ford step 1256 current loss 0.153414, current_train_items 40224.
I0304 19:28:37.967798 22849303695488 run.py:483] Algo bellman_ford step 1257 current loss 0.311590, current_train_items 40256.
I0304 19:28:37.997259 22849303695488 run.py:483] Algo bellman_ford step 1258 current loss 0.387777, current_train_items 40288.
I0304 19:28:38.029049 22849303695488 run.py:483] Algo bellman_ford step 1259 current loss 0.232573, current_train_items 40320.
I0304 19:28:38.048793 22849303695488 run.py:483] Algo bellman_ford step 1260 current loss 0.012503, current_train_items 40352.
I0304 19:28:38.066142 22849303695488 run.py:483] Algo bellman_ford step 1261 current loss 0.076884, current_train_items 40384.
I0304 19:28:38.090247 22849303695488 run.py:483] Algo bellman_ford step 1262 current loss 0.129071, current_train_items 40416.
I0304 19:28:38.117588 22849303695488 run.py:483] Algo bellman_ford step 1263 current loss 0.171710, current_train_items 40448.
I0304 19:28:38.152124 22849303695488 run.py:483] Algo bellman_ford step 1264 current loss 0.416226, current_train_items 40480.
I0304 19:28:38.171561 22849303695488 run.py:483] Algo bellman_ford step 1265 current loss 0.009392, current_train_items 40512.
I0304 19:28:38.188094 22849303695488 run.py:483] Algo bellman_ford step 1266 current loss 0.101618, current_train_items 40544.
I0304 19:28:38.211045 22849303695488 run.py:483] Algo bellman_ford step 1267 current loss 0.114580, current_train_items 40576.
I0304 19:28:38.240391 22849303695488 run.py:483] Algo bellman_ford step 1268 current loss 0.115166, current_train_items 40608.
I0304 19:28:38.273583 22849303695488 run.py:483] Algo bellman_ford step 1269 current loss 0.170037, current_train_items 40640.
I0304 19:28:38.293414 22849303695488 run.py:483] Algo bellman_ford step 1270 current loss 0.017636, current_train_items 40672.
I0304 19:28:38.309807 22849303695488 run.py:483] Algo bellman_ford step 1271 current loss 0.045824, current_train_items 40704.
I0304 19:28:38.332206 22849303695488 run.py:483] Algo bellman_ford step 1272 current loss 0.098245, current_train_items 40736.
I0304 19:28:38.360975 22849303695488 run.py:483] Algo bellman_ford step 1273 current loss 0.100992, current_train_items 40768.
I0304 19:28:38.394189 22849303695488 run.py:483] Algo bellman_ford step 1274 current loss 0.172512, current_train_items 40800.
I0304 19:28:38.413851 22849303695488 run.py:483] Algo bellman_ford step 1275 current loss 0.015442, current_train_items 40832.
I0304 19:28:38.430531 22849303695488 run.py:483] Algo bellman_ford step 1276 current loss 0.056973, current_train_items 40864.
I0304 19:28:38.452864 22849303695488 run.py:483] Algo bellman_ford step 1277 current loss 0.096566, current_train_items 40896.
I0304 19:28:38.482671 22849303695488 run.py:483] Algo bellman_ford step 1278 current loss 0.119329, current_train_items 40928.
I0304 19:28:38.514723 22849303695488 run.py:483] Algo bellman_ford step 1279 current loss 0.177728, current_train_items 40960.
I0304 19:28:38.534400 22849303695488 run.py:483] Algo bellman_ford step 1280 current loss 0.022197, current_train_items 40992.
I0304 19:28:38.550792 22849303695488 run.py:483] Algo bellman_ford step 1281 current loss 0.056148, current_train_items 41024.
I0304 19:28:38.574551 22849303695488 run.py:483] Algo bellman_ford step 1282 current loss 0.101146, current_train_items 41056.
I0304 19:28:38.604854 22849303695488 run.py:483] Algo bellman_ford step 1283 current loss 0.155173, current_train_items 41088.
I0304 19:28:38.636632 22849303695488 run.py:483] Algo bellman_ford step 1284 current loss 0.149027, current_train_items 41120.
I0304 19:28:38.656435 22849303695488 run.py:483] Algo bellman_ford step 1285 current loss 0.033677, current_train_items 41152.
I0304 19:28:38.673026 22849303695488 run.py:483] Algo bellman_ford step 1286 current loss 0.037958, current_train_items 41184.
I0304 19:28:38.697577 22849303695488 run.py:483] Algo bellman_ford step 1287 current loss 0.129059, current_train_items 41216.
I0304 19:28:38.727662 22849303695488 run.py:483] Algo bellman_ford step 1288 current loss 0.168785, current_train_items 41248.
I0304 19:28:38.760735 22849303695488 run.py:483] Algo bellman_ford step 1289 current loss 0.157864, current_train_items 41280.
I0304 19:28:38.781138 22849303695488 run.py:483] Algo bellman_ford step 1290 current loss 0.012588, current_train_items 41312.
I0304 19:28:38.797983 22849303695488 run.py:483] Algo bellman_ford step 1291 current loss 0.070627, current_train_items 41344.
I0304 19:28:38.820842 22849303695488 run.py:483] Algo bellman_ford step 1292 current loss 0.100887, current_train_items 41376.
I0304 19:28:38.850507 22849303695488 run.py:483] Algo bellman_ford step 1293 current loss 0.096848, current_train_items 41408.
I0304 19:28:38.884290 22849303695488 run.py:483] Algo bellman_ford step 1294 current loss 0.175479, current_train_items 41440.
I0304 19:28:38.904058 22849303695488 run.py:483] Algo bellman_ford step 1295 current loss 0.029900, current_train_items 41472.
I0304 19:28:38.920661 22849303695488 run.py:483] Algo bellman_ford step 1296 current loss 0.028921, current_train_items 41504.
I0304 19:28:38.944701 22849303695488 run.py:483] Algo bellman_ford step 1297 current loss 0.084662, current_train_items 41536.
I0304 19:28:38.973242 22849303695488 run.py:483] Algo bellman_ford step 1298 current loss 0.122827, current_train_items 41568.
I0304 19:28:39.003794 22849303695488 run.py:483] Algo bellman_ford step 1299 current loss 0.171377, current_train_items 41600.
I0304 19:28:39.023305 22849303695488 run.py:483] Algo bellman_ford step 1300 current loss 0.017049, current_train_items 41632.
I0304 19:28:39.043993 22849303695488 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0304 19:28:39.044110 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:28:39.061075 22849303695488 run.py:483] Algo bellman_ford step 1301 current loss 0.057732, current_train_items 41664.
I0304 19:28:39.085727 22849303695488 run.py:483] Algo bellman_ford step 1302 current loss 0.163570, current_train_items 41696.
I0304 19:28:39.116574 22849303695488 run.py:483] Algo bellman_ford step 1303 current loss 0.156561, current_train_items 41728.
I0304 19:28:39.148441 22849303695488 run.py:483] Algo bellman_ford step 1304 current loss 0.178405, current_train_items 41760.
I0304 19:28:39.168345 22849303695488 run.py:483] Algo bellman_ford step 1305 current loss 0.009147, current_train_items 41792.
I0304 19:28:39.184862 22849303695488 run.py:483] Algo bellman_ford step 1306 current loss 0.072847, current_train_items 41824.
I0304 19:28:39.208380 22849303695488 run.py:483] Algo bellman_ford step 1307 current loss 0.120351, current_train_items 41856.
I0304 19:28:39.237981 22849303695488 run.py:483] Algo bellman_ford step 1308 current loss 0.125977, current_train_items 41888.
I0304 19:28:39.270247 22849303695488 run.py:483] Algo bellman_ford step 1309 current loss 0.152440, current_train_items 41920.
I0304 19:28:39.290234 22849303695488 run.py:483] Algo bellman_ford step 1310 current loss 0.013848, current_train_items 41952.
I0304 19:28:39.306753 22849303695488 run.py:483] Algo bellman_ford step 1311 current loss 0.094559, current_train_items 41984.
I0304 19:28:39.329905 22849303695488 run.py:483] Algo bellman_ford step 1312 current loss 0.099200, current_train_items 42016.
I0304 19:28:39.360521 22849303695488 run.py:483] Algo bellman_ford step 1313 current loss 0.150343, current_train_items 42048.
I0304 19:28:39.393515 22849303695488 run.py:483] Algo bellman_ford step 1314 current loss 0.131583, current_train_items 42080.
I0304 19:28:39.412889 22849303695488 run.py:483] Algo bellman_ford step 1315 current loss 0.008024, current_train_items 42112.
I0304 19:28:39.429132 22849303695488 run.py:483] Algo bellman_ford step 1316 current loss 0.035448, current_train_items 42144.
I0304 19:28:39.452879 22849303695488 run.py:483] Algo bellman_ford step 1317 current loss 0.170460, current_train_items 42176.
I0304 19:28:39.483147 22849303695488 run.py:483] Algo bellman_ford step 1318 current loss 0.129925, current_train_items 42208.
I0304 19:28:39.515890 22849303695488 run.py:483] Algo bellman_ford step 1319 current loss 0.303372, current_train_items 42240.
I0304 19:28:39.535305 22849303695488 run.py:483] Algo bellman_ford step 1320 current loss 0.011166, current_train_items 42272.
I0304 19:28:39.551784 22849303695488 run.py:483] Algo bellman_ford step 1321 current loss 0.058537, current_train_items 42304.
I0304 19:28:39.576576 22849303695488 run.py:483] Algo bellman_ford step 1322 current loss 0.170001, current_train_items 42336.
I0304 19:28:39.607326 22849303695488 run.py:483] Algo bellman_ford step 1323 current loss 0.104322, current_train_items 42368.
I0304 19:28:39.639669 22849303695488 run.py:483] Algo bellman_ford step 1324 current loss 0.179990, current_train_items 42400.
I0304 19:28:39.659440 22849303695488 run.py:483] Algo bellman_ford step 1325 current loss 0.030106, current_train_items 42432.
I0304 19:28:39.675958 22849303695488 run.py:483] Algo bellman_ford step 1326 current loss 0.073813, current_train_items 42464.
I0304 19:28:39.699395 22849303695488 run.py:483] Algo bellman_ford step 1327 current loss 0.107114, current_train_items 42496.
I0304 19:28:39.729164 22849303695488 run.py:483] Algo bellman_ford step 1328 current loss 0.164132, current_train_items 42528.
I0304 19:28:39.762158 22849303695488 run.py:483] Algo bellman_ford step 1329 current loss 0.206106, current_train_items 42560.
I0304 19:28:39.781560 22849303695488 run.py:483] Algo bellman_ford step 1330 current loss 0.016182, current_train_items 42592.
I0304 19:28:39.797851 22849303695488 run.py:483] Algo bellman_ford step 1331 current loss 0.080466, current_train_items 42624.
I0304 19:28:39.820968 22849303695488 run.py:483] Algo bellman_ford step 1332 current loss 0.143667, current_train_items 42656.
I0304 19:28:39.850983 22849303695488 run.py:483] Algo bellman_ford step 1333 current loss 0.206629, current_train_items 42688.
I0304 19:28:39.882703 22849303695488 run.py:483] Algo bellman_ford step 1334 current loss 0.162817, current_train_items 42720.
I0304 19:28:39.902099 22849303695488 run.py:483] Algo bellman_ford step 1335 current loss 0.024124, current_train_items 42752.
I0304 19:28:39.918190 22849303695488 run.py:483] Algo bellman_ford step 1336 current loss 0.053230, current_train_items 42784.
I0304 19:28:39.943235 22849303695488 run.py:483] Algo bellman_ford step 1337 current loss 0.091352, current_train_items 42816.
I0304 19:28:39.973363 22849303695488 run.py:483] Algo bellman_ford step 1338 current loss 0.219004, current_train_items 42848.
I0304 19:28:40.008205 22849303695488 run.py:483] Algo bellman_ford step 1339 current loss 0.150481, current_train_items 42880.
I0304 19:28:40.027729 22849303695488 run.py:483] Algo bellman_ford step 1340 current loss 0.103972, current_train_items 42912.
I0304 19:28:40.044238 22849303695488 run.py:483] Algo bellman_ford step 1341 current loss 0.041316, current_train_items 42944.
I0304 19:28:40.068281 22849303695488 run.py:483] Algo bellman_ford step 1342 current loss 0.188129, current_train_items 42976.
I0304 19:28:40.097851 22849303695488 run.py:483] Algo bellman_ford step 1343 current loss 0.268766, current_train_items 43008.
I0304 19:28:40.130171 22849303695488 run.py:483] Algo bellman_ford step 1344 current loss 0.250963, current_train_items 43040.
I0304 19:28:40.149811 22849303695488 run.py:483] Algo bellman_ford step 1345 current loss 0.013500, current_train_items 43072.
I0304 19:28:40.166435 22849303695488 run.py:483] Algo bellman_ford step 1346 current loss 0.079532, current_train_items 43104.
I0304 19:28:40.191311 22849303695488 run.py:483] Algo bellman_ford step 1347 current loss 0.192555, current_train_items 43136.
I0304 19:28:40.220147 22849303695488 run.py:483] Algo bellman_ford step 1348 current loss 0.184777, current_train_items 43168.
I0304 19:28:40.251451 22849303695488 run.py:483] Algo bellman_ford step 1349 current loss 0.145415, current_train_items 43200.
I0304 19:28:40.270988 22849303695488 run.py:483] Algo bellman_ford step 1350 current loss 0.056437, current_train_items 43232.
I0304 19:28:40.279264 22849303695488 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.9541015625, 'score': 0.9541015625, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0304 19:28:40.279374 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.954, val scores are: bellman_ford: 0.954
I0304 19:28:40.296607 22849303695488 run.py:483] Algo bellman_ford step 1351 current loss 0.066313, current_train_items 43264.
I0304 19:28:40.321806 22849303695488 run.py:483] Algo bellman_ford step 1352 current loss 0.093380, current_train_items 43296.
I0304 19:28:40.350780 22849303695488 run.py:483] Algo bellman_ford step 1353 current loss 0.145518, current_train_items 43328.
I0304 19:28:40.385683 22849303695488 run.py:483] Algo bellman_ford step 1354 current loss 0.182384, current_train_items 43360.
I0304 19:28:40.405421 22849303695488 run.py:483] Algo bellman_ford step 1355 current loss 0.026166, current_train_items 43392.
I0304 19:28:40.421605 22849303695488 run.py:483] Algo bellman_ford step 1356 current loss 0.056664, current_train_items 43424.
I0304 19:28:40.444934 22849303695488 run.py:483] Algo bellman_ford step 1357 current loss 0.074058, current_train_items 43456.
I0304 19:28:40.474865 22849303695488 run.py:483] Algo bellman_ford step 1358 current loss 0.154421, current_train_items 43488.
I0304 19:28:40.509060 22849303695488 run.py:483] Algo bellman_ford step 1359 current loss 0.277275, current_train_items 43520.
I0304 19:28:40.528841 22849303695488 run.py:483] Algo bellman_ford step 1360 current loss 0.011475, current_train_items 43552.
I0304 19:28:40.546145 22849303695488 run.py:483] Algo bellman_ford step 1361 current loss 0.112469, current_train_items 43584.
I0304 19:28:40.570404 22849303695488 run.py:483] Algo bellman_ford step 1362 current loss 0.145068, current_train_items 43616.
I0304 19:28:40.600497 22849303695488 run.py:483] Algo bellman_ford step 1363 current loss 0.154399, current_train_items 43648.
I0304 19:28:40.634323 22849303695488 run.py:483] Algo bellman_ford step 1364 current loss 0.167006, current_train_items 43680.
I0304 19:28:40.653966 22849303695488 run.py:483] Algo bellman_ford step 1365 current loss 0.009548, current_train_items 43712.
I0304 19:28:40.670187 22849303695488 run.py:483] Algo bellman_ford step 1366 current loss 0.075408, current_train_items 43744.
I0304 19:28:40.693499 22849303695488 run.py:483] Algo bellman_ford step 1367 current loss 0.161017, current_train_items 43776.
I0304 19:28:40.721768 22849303695488 run.py:483] Algo bellman_ford step 1368 current loss 0.134374, current_train_items 43808.
I0304 19:28:40.754637 22849303695488 run.py:483] Algo bellman_ford step 1369 current loss 0.237494, current_train_items 43840.
I0304 19:28:40.774897 22849303695488 run.py:483] Algo bellman_ford step 1370 current loss 0.008782, current_train_items 43872.
I0304 19:28:40.791759 22849303695488 run.py:483] Algo bellman_ford step 1371 current loss 0.089001, current_train_items 43904.
I0304 19:28:40.815955 22849303695488 run.py:483] Algo bellman_ford step 1372 current loss 0.109830, current_train_items 43936.
I0304 19:28:40.847277 22849303695488 run.py:483] Algo bellman_ford step 1373 current loss 0.145549, current_train_items 43968.
I0304 19:28:40.880436 22849303695488 run.py:483] Algo bellman_ford step 1374 current loss 0.186464, current_train_items 44000.
I0304 19:28:40.900399 22849303695488 run.py:483] Algo bellman_ford step 1375 current loss 0.014928, current_train_items 44032.
I0304 19:28:40.916660 22849303695488 run.py:483] Algo bellman_ford step 1376 current loss 0.079706, current_train_items 44064.
I0304 19:28:40.940231 22849303695488 run.py:483] Algo bellman_ford step 1377 current loss 0.130743, current_train_items 44096.
I0304 19:28:40.970881 22849303695488 run.py:483] Algo bellman_ford step 1378 current loss 0.115498, current_train_items 44128.
I0304 19:28:41.004052 22849303695488 run.py:483] Algo bellman_ford step 1379 current loss 0.156570, current_train_items 44160.
I0304 19:28:41.023549 22849303695488 run.py:483] Algo bellman_ford step 1380 current loss 0.020983, current_train_items 44192.
I0304 19:28:41.040238 22849303695488 run.py:483] Algo bellman_ford step 1381 current loss 0.062106, current_train_items 44224.
I0304 19:28:41.063606 22849303695488 run.py:483] Algo bellman_ford step 1382 current loss 0.097384, current_train_items 44256.
I0304 19:28:41.093984 22849303695488 run.py:483] Algo bellman_ford step 1383 current loss 0.135490, current_train_items 44288.
I0304 19:28:41.124691 22849303695488 run.py:483] Algo bellman_ford step 1384 current loss 0.127136, current_train_items 44320.
I0304 19:28:41.144499 22849303695488 run.py:483] Algo bellman_ford step 1385 current loss 0.008498, current_train_items 44352.
I0304 19:28:41.160644 22849303695488 run.py:483] Algo bellman_ford step 1386 current loss 0.034660, current_train_items 44384.
I0304 19:28:41.183164 22849303695488 run.py:483] Algo bellman_ford step 1387 current loss 0.051054, current_train_items 44416.
I0304 19:28:41.213774 22849303695488 run.py:483] Algo bellman_ford step 1388 current loss 0.137996, current_train_items 44448.
I0304 19:28:41.246422 22849303695488 run.py:483] Algo bellman_ford step 1389 current loss 0.152404, current_train_items 44480.
I0304 19:28:41.266122 22849303695488 run.py:483] Algo bellman_ford step 1390 current loss 0.013526, current_train_items 44512.
I0304 19:28:41.282423 22849303695488 run.py:483] Algo bellman_ford step 1391 current loss 0.068251, current_train_items 44544.
I0304 19:28:41.306879 22849303695488 run.py:483] Algo bellman_ford step 1392 current loss 0.097474, current_train_items 44576.
I0304 19:28:41.337146 22849303695488 run.py:483] Algo bellman_ford step 1393 current loss 0.144471, current_train_items 44608.
I0304 19:28:41.369590 22849303695488 run.py:483] Algo bellman_ford step 1394 current loss 0.170883, current_train_items 44640.
I0304 19:28:41.388709 22849303695488 run.py:483] Algo bellman_ford step 1395 current loss 0.013559, current_train_items 44672.
I0304 19:28:41.404975 22849303695488 run.py:483] Algo bellman_ford step 1396 current loss 0.040085, current_train_items 44704.
I0304 19:28:41.429419 22849303695488 run.py:483] Algo bellman_ford step 1397 current loss 0.107167, current_train_items 44736.
I0304 19:28:41.460572 22849303695488 run.py:483] Algo bellman_ford step 1398 current loss 0.104478, current_train_items 44768.
I0304 19:28:41.492233 22849303695488 run.py:483] Algo bellman_ford step 1399 current loss 0.131757, current_train_items 44800.
I0304 19:28:41.512498 22849303695488 run.py:483] Algo bellman_ford step 1400 current loss 0.017545, current_train_items 44832.
I0304 19:28:41.520707 22849303695488 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0304 19:28:41.520823 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:28:41.537564 22849303695488 run.py:483] Algo bellman_ford step 1401 current loss 0.046217, current_train_items 44864.
I0304 19:28:41.562475 22849303695488 run.py:483] Algo bellman_ford step 1402 current loss 0.164065, current_train_items 44896.
I0304 19:28:41.593450 22849303695488 run.py:483] Algo bellman_ford step 1403 current loss 0.138890, current_train_items 44928.
I0304 19:28:41.627378 22849303695488 run.py:483] Algo bellman_ford step 1404 current loss 0.166104, current_train_items 44960.
I0304 19:28:41.647363 22849303695488 run.py:483] Algo bellman_ford step 1405 current loss 0.008963, current_train_items 44992.
I0304 19:28:41.663958 22849303695488 run.py:483] Algo bellman_ford step 1406 current loss 0.058577, current_train_items 45024.
I0304 19:28:41.687971 22849303695488 run.py:483] Algo bellman_ford step 1407 current loss 0.094317, current_train_items 45056.
I0304 19:28:41.718578 22849303695488 run.py:483] Algo bellman_ford step 1408 current loss 0.119142, current_train_items 45088.
I0304 19:28:41.751496 22849303695488 run.py:483] Algo bellman_ford step 1409 current loss 0.143005, current_train_items 45120.
I0304 19:28:41.770830 22849303695488 run.py:483] Algo bellman_ford step 1410 current loss 0.014013, current_train_items 45152.
I0304 19:28:41.787642 22849303695488 run.py:483] Algo bellman_ford step 1411 current loss 0.038700, current_train_items 45184.
I0304 19:28:41.811258 22849303695488 run.py:483] Algo bellman_ford step 1412 current loss 0.056246, current_train_items 45216.
I0304 19:28:41.839638 22849303695488 run.py:483] Algo bellman_ford step 1413 current loss 0.093713, current_train_items 45248.
I0304 19:28:41.871977 22849303695488 run.py:483] Algo bellman_ford step 1414 current loss 0.159854, current_train_items 45280.
I0304 19:28:41.891700 22849303695488 run.py:483] Algo bellman_ford step 1415 current loss 0.014727, current_train_items 45312.
I0304 19:28:41.908672 22849303695488 run.py:483] Algo bellman_ford step 1416 current loss 0.056045, current_train_items 45344.
I0304 19:28:41.931853 22849303695488 run.py:483] Algo bellman_ford step 1417 current loss 0.045146, current_train_items 45376.
I0304 19:28:41.960808 22849303695488 run.py:483] Algo bellman_ford step 1418 current loss 0.123205, current_train_items 45408.
I0304 19:28:41.994226 22849303695488 run.py:483] Algo bellman_ford step 1419 current loss 0.148987, current_train_items 45440.
I0304 19:28:42.014411 22849303695488 run.py:483] Algo bellman_ford step 1420 current loss 0.035013, current_train_items 45472.
I0304 19:28:42.030884 22849303695488 run.py:483] Algo bellman_ford step 1421 current loss 0.056104, current_train_items 45504.
I0304 19:28:42.055502 22849303695488 run.py:483] Algo bellman_ford step 1422 current loss 0.136328, current_train_items 45536.
I0304 19:28:42.086738 22849303695488 run.py:483] Algo bellman_ford step 1423 current loss 0.163696, current_train_items 45568.
I0304 19:28:42.118889 22849303695488 run.py:483] Algo bellman_ford step 1424 current loss 0.165746, current_train_items 45600.
I0304 19:28:42.138315 22849303695488 run.py:483] Algo bellman_ford step 1425 current loss 0.011404, current_train_items 45632.
I0304 19:28:42.155194 22849303695488 run.py:483] Algo bellman_ford step 1426 current loss 0.108692, current_train_items 45664.
I0304 19:28:42.179827 22849303695488 run.py:483] Algo bellman_ford step 1427 current loss 0.089062, current_train_items 45696.
I0304 19:28:42.210183 22849303695488 run.py:483] Algo bellman_ford step 1428 current loss 0.107991, current_train_items 45728.
I0304 19:28:42.241648 22849303695488 run.py:483] Algo bellman_ford step 1429 current loss 0.119917, current_train_items 45760.
I0304 19:28:42.261236 22849303695488 run.py:483] Algo bellman_ford step 1430 current loss 0.013553, current_train_items 45792.
I0304 19:28:42.277570 22849303695488 run.py:483] Algo bellman_ford step 1431 current loss 0.044658, current_train_items 45824.
I0304 19:28:42.302779 22849303695488 run.py:483] Algo bellman_ford step 1432 current loss 0.148953, current_train_items 45856.
I0304 19:28:42.331496 22849303695488 run.py:483] Algo bellman_ford step 1433 current loss 0.100491, current_train_items 45888.
I0304 19:28:42.362970 22849303695488 run.py:483] Algo bellman_ford step 1434 current loss 0.121209, current_train_items 45920.
I0304 19:28:42.382331 22849303695488 run.py:483] Algo bellman_ford step 1435 current loss 0.008431, current_train_items 45952.
I0304 19:28:42.398934 22849303695488 run.py:483] Algo bellman_ford step 1436 current loss 0.045520, current_train_items 45984.
I0304 19:28:42.422666 22849303695488 run.py:483] Algo bellman_ford step 1437 current loss 0.085907, current_train_items 46016.
I0304 19:28:42.451911 22849303695488 run.py:483] Algo bellman_ford step 1438 current loss 0.138874, current_train_items 46048.
I0304 19:28:42.485887 22849303695488 run.py:483] Algo bellman_ford step 1439 current loss 0.219838, current_train_items 46080.
I0304 19:28:42.505249 22849303695488 run.py:483] Algo bellman_ford step 1440 current loss 0.010924, current_train_items 46112.
I0304 19:28:42.521996 22849303695488 run.py:483] Algo bellman_ford step 1441 current loss 0.057063, current_train_items 46144.
I0304 19:28:42.545320 22849303695488 run.py:483] Algo bellman_ford step 1442 current loss 0.070400, current_train_items 46176.
I0304 19:28:42.575549 22849303695488 run.py:483] Algo bellman_ford step 1443 current loss 0.119187, current_train_items 46208.
I0304 19:28:42.611781 22849303695488 run.py:483] Algo bellman_ford step 1444 current loss 0.224286, current_train_items 46240.
I0304 19:28:42.631397 22849303695488 run.py:483] Algo bellman_ford step 1445 current loss 0.021869, current_train_items 46272.
I0304 19:28:42.647330 22849303695488 run.py:483] Algo bellman_ford step 1446 current loss 0.092743, current_train_items 46304.
I0304 19:28:42.671886 22849303695488 run.py:483] Algo bellman_ford step 1447 current loss 0.097063, current_train_items 46336.
I0304 19:28:42.701637 22849303695488 run.py:483] Algo bellman_ford step 1448 current loss 0.222312, current_train_items 46368.
I0304 19:28:42.733400 22849303695488 run.py:483] Algo bellman_ford step 1449 current loss 0.142588, current_train_items 46400.
I0304 19:28:42.753231 22849303695488 run.py:483] Algo bellman_ford step 1450 current loss 0.015712, current_train_items 46432.
I0304 19:28:42.761701 22849303695488 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0304 19:28:42.761810 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:28:42.778417 22849303695488 run.py:483] Algo bellman_ford step 1451 current loss 0.059312, current_train_items 46464.
I0304 19:28:42.802165 22849303695488 run.py:483] Algo bellman_ford step 1452 current loss 0.063206, current_train_items 46496.
I0304 19:28:42.833503 22849303695488 run.py:483] Algo bellman_ford step 1453 current loss 0.164351, current_train_items 46528.
I0304 19:28:42.867783 22849303695488 run.py:483] Algo bellman_ford step 1454 current loss 0.192176, current_train_items 46560.
I0304 19:28:42.887433 22849303695488 run.py:483] Algo bellman_ford step 1455 current loss 0.043959, current_train_items 46592.
I0304 19:28:42.903715 22849303695488 run.py:483] Algo bellman_ford step 1456 current loss 0.056015, current_train_items 46624.
I0304 19:28:42.926384 22849303695488 run.py:483] Algo bellman_ford step 1457 current loss 0.085674, current_train_items 46656.
I0304 19:28:42.956935 22849303695488 run.py:483] Algo bellman_ford step 1458 current loss 0.173019, current_train_items 46688.
I0304 19:28:42.988944 22849303695488 run.py:483] Algo bellman_ford step 1459 current loss 0.190653, current_train_items 46720.
I0304 19:28:43.008645 22849303695488 run.py:483] Algo bellman_ford step 1460 current loss 0.020324, current_train_items 46752.
I0304 19:28:43.024792 22849303695488 run.py:483] Algo bellman_ford step 1461 current loss 0.064360, current_train_items 46784.
I0304 19:28:43.047659 22849303695488 run.py:483] Algo bellman_ford step 1462 current loss 0.167794, current_train_items 46816.
I0304 19:28:43.076997 22849303695488 run.py:483] Algo bellman_ford step 1463 current loss 0.207977, current_train_items 46848.
I0304 19:28:43.108438 22849303695488 run.py:483] Algo bellman_ford step 1464 current loss 0.156376, current_train_items 46880.
I0304 19:28:43.128131 22849303695488 run.py:483] Algo bellman_ford step 1465 current loss 0.023418, current_train_items 46912.
I0304 19:28:43.144818 22849303695488 run.py:483] Algo bellman_ford step 1466 current loss 0.080223, current_train_items 46944.
I0304 19:28:43.168299 22849303695488 run.py:483] Algo bellman_ford step 1467 current loss 0.133717, current_train_items 46976.
I0304 19:28:43.196189 22849303695488 run.py:483] Algo bellman_ford step 1468 current loss 0.131661, current_train_items 47008.
I0304 19:28:43.227391 22849303695488 run.py:483] Algo bellman_ford step 1469 current loss 0.194144, current_train_items 47040.
I0304 19:28:43.247567 22849303695488 run.py:483] Algo bellman_ford step 1470 current loss 0.011983, current_train_items 47072.
I0304 19:28:43.264476 22849303695488 run.py:483] Algo bellman_ford step 1471 current loss 0.106077, current_train_items 47104.
I0304 19:28:43.287446 22849303695488 run.py:483] Algo bellman_ford step 1472 current loss 0.085536, current_train_items 47136.
I0304 19:28:43.317027 22849303695488 run.py:483] Algo bellman_ford step 1473 current loss 0.243075, current_train_items 47168.
I0304 19:28:43.349923 22849303695488 run.py:483] Algo bellman_ford step 1474 current loss 0.217984, current_train_items 47200.
I0304 19:28:43.369759 22849303695488 run.py:483] Algo bellman_ford step 1475 current loss 0.030204, current_train_items 47232.
I0304 19:28:43.386162 22849303695488 run.py:483] Algo bellman_ford step 1476 current loss 0.033788, current_train_items 47264.
I0304 19:28:43.409828 22849303695488 run.py:483] Algo bellman_ford step 1477 current loss 0.112714, current_train_items 47296.
I0304 19:28:43.439482 22849303695488 run.py:483] Algo bellman_ford step 1478 current loss 0.194363, current_train_items 47328.
I0304 19:28:43.473599 22849303695488 run.py:483] Algo bellman_ford step 1479 current loss 0.180446, current_train_items 47360.
I0304 19:28:43.492619 22849303695488 run.py:483] Algo bellman_ford step 1480 current loss 0.012874, current_train_items 47392.
I0304 19:28:43.508778 22849303695488 run.py:483] Algo bellman_ford step 1481 current loss 0.033064, current_train_items 47424.
I0304 19:28:43.531827 22849303695488 run.py:483] Algo bellman_ford step 1482 current loss 0.109807, current_train_items 47456.
I0304 19:28:43.561699 22849303695488 run.py:483] Algo bellman_ford step 1483 current loss 0.192076, current_train_items 47488.
I0304 19:28:43.592479 22849303695488 run.py:483] Algo bellman_ford step 1484 current loss 0.135710, current_train_items 47520.
I0304 19:28:43.611996 22849303695488 run.py:483] Algo bellman_ford step 1485 current loss 0.016904, current_train_items 47552.
I0304 19:28:43.628030 22849303695488 run.py:483] Algo bellman_ford step 1486 current loss 0.067294, current_train_items 47584.
I0304 19:28:43.652038 22849303695488 run.py:483] Algo bellman_ford step 1487 current loss 0.200232, current_train_items 47616.
I0304 19:28:43.680581 22849303695488 run.py:483] Algo bellman_ford step 1488 current loss 0.285603, current_train_items 47648.
I0304 19:28:43.713886 22849303695488 run.py:483] Algo bellman_ford step 1489 current loss 0.290700, current_train_items 47680.
I0304 19:28:43.733724 22849303695488 run.py:483] Algo bellman_ford step 1490 current loss 0.018413, current_train_items 47712.
I0304 19:28:43.750225 22849303695488 run.py:483] Algo bellman_ford step 1491 current loss 0.064386, current_train_items 47744.
I0304 19:28:43.773687 22849303695488 run.py:483] Algo bellman_ford step 1492 current loss 0.094431, current_train_items 47776.
I0304 19:28:43.803498 22849303695488 run.py:483] Algo bellman_ford step 1493 current loss 0.186911, current_train_items 47808.
I0304 19:28:43.837315 22849303695488 run.py:483] Algo bellman_ford step 1494 current loss 0.204649, current_train_items 47840.
I0304 19:28:43.856523 22849303695488 run.py:483] Algo bellman_ford step 1495 current loss 0.014588, current_train_items 47872.
I0304 19:28:43.873350 22849303695488 run.py:483] Algo bellman_ford step 1496 current loss 0.094321, current_train_items 47904.
I0304 19:28:43.897274 22849303695488 run.py:483] Algo bellman_ford step 1497 current loss 0.155574, current_train_items 47936.
I0304 19:28:43.927708 22849303695488 run.py:483] Algo bellman_ford step 1498 current loss 0.198674, current_train_items 47968.
I0304 19:28:43.961771 22849303695488 run.py:483] Algo bellman_ford step 1499 current loss 0.188281, current_train_items 48000.
I0304 19:28:43.981508 22849303695488 run.py:483] Algo bellman_ford step 1500 current loss 0.013574, current_train_items 48032.
I0304 19:28:43.989421 22849303695488 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0304 19:28:43.989533 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:28:44.006800 22849303695488 run.py:483] Algo bellman_ford step 1501 current loss 0.076578, current_train_items 48064.
I0304 19:28:44.031451 22849303695488 run.py:483] Algo bellman_ford step 1502 current loss 0.120528, current_train_items 48096.
I0304 19:28:44.061889 22849303695488 run.py:483] Algo bellman_ford step 1503 current loss 0.117588, current_train_items 48128.
I0304 19:28:44.094799 22849303695488 run.py:483] Algo bellman_ford step 1504 current loss 0.124415, current_train_items 48160.
I0304 19:28:44.114649 22849303695488 run.py:483] Algo bellman_ford step 1505 current loss 0.013742, current_train_items 48192.
I0304 19:28:44.130841 22849303695488 run.py:483] Algo bellman_ford step 1506 current loss 0.075804, current_train_items 48224.
I0304 19:28:44.154058 22849303695488 run.py:483] Algo bellman_ford step 1507 current loss 0.096121, current_train_items 48256.
I0304 19:28:44.183631 22849303695488 run.py:483] Algo bellman_ford step 1508 current loss 0.168906, current_train_items 48288.
I0304 19:28:44.215932 22849303695488 run.py:483] Algo bellman_ford step 1509 current loss 0.152554, current_train_items 48320.
I0304 19:28:44.235180 22849303695488 run.py:483] Algo bellman_ford step 1510 current loss 0.043523, current_train_items 48352.
I0304 19:28:44.251744 22849303695488 run.py:483] Algo bellman_ford step 1511 current loss 0.059028, current_train_items 48384.
I0304 19:28:44.275553 22849303695488 run.py:483] Algo bellman_ford step 1512 current loss 0.136780, current_train_items 48416.
I0304 19:28:44.304444 22849303695488 run.py:483] Algo bellman_ford step 1513 current loss 0.104211, current_train_items 48448.
I0304 19:28:44.338673 22849303695488 run.py:483] Algo bellman_ford step 1514 current loss 0.243390, current_train_items 48480.
I0304 19:28:44.358085 22849303695488 run.py:483] Algo bellman_ford step 1515 current loss 0.008317, current_train_items 48512.
I0304 19:28:44.374187 22849303695488 run.py:483] Algo bellman_ford step 1516 current loss 0.044216, current_train_items 48544.
I0304 19:28:44.397818 22849303695488 run.py:483] Algo bellman_ford step 1517 current loss 0.175259, current_train_items 48576.
I0304 19:28:44.428606 22849303695488 run.py:483] Algo bellman_ford step 1518 current loss 0.214528, current_train_items 48608.
I0304 19:28:44.461065 22849303695488 run.py:483] Algo bellman_ford step 1519 current loss 0.164227, current_train_items 48640.
I0304 19:28:44.480686 22849303695488 run.py:483] Algo bellman_ford step 1520 current loss 0.010161, current_train_items 48672.
I0304 19:28:44.496764 22849303695488 run.py:483] Algo bellman_ford step 1521 current loss 0.079612, current_train_items 48704.
I0304 19:28:44.520781 22849303695488 run.py:483] Algo bellman_ford step 1522 current loss 0.157968, current_train_items 48736.
I0304 19:28:44.550435 22849303695488 run.py:483] Algo bellman_ford step 1523 current loss 0.200787, current_train_items 48768.
I0304 19:28:44.583035 22849303695488 run.py:483] Algo bellman_ford step 1524 current loss 0.152928, current_train_items 48800.
I0304 19:28:44.602651 22849303695488 run.py:483] Algo bellman_ford step 1525 current loss 0.014652, current_train_items 48832.
I0304 19:28:44.618796 22849303695488 run.py:483] Algo bellman_ford step 1526 current loss 0.060173, current_train_items 48864.
I0304 19:28:44.642994 22849303695488 run.py:483] Algo bellman_ford step 1527 current loss 0.188546, current_train_items 48896.
I0304 19:28:44.672543 22849303695488 run.py:483] Algo bellman_ford step 1528 current loss 0.243595, current_train_items 48928.
I0304 19:28:44.705610 22849303695488 run.py:483] Algo bellman_ford step 1529 current loss 0.163574, current_train_items 48960.
I0304 19:28:44.725221 22849303695488 run.py:483] Algo bellman_ford step 1530 current loss 0.013367, current_train_items 48992.
I0304 19:28:44.741451 22849303695488 run.py:483] Algo bellman_ford step 1531 current loss 0.071258, current_train_items 49024.
I0304 19:28:44.764836 22849303695488 run.py:483] Algo bellman_ford step 1532 current loss 0.097768, current_train_items 49056.
I0304 19:28:44.795059 22849303695488 run.py:483] Algo bellman_ford step 1533 current loss 0.328657, current_train_items 49088.
I0304 19:28:44.828598 22849303695488 run.py:483] Algo bellman_ford step 1534 current loss 0.285744, current_train_items 49120.
I0304 19:28:44.848337 22849303695488 run.py:483] Algo bellman_ford step 1535 current loss 0.035800, current_train_items 49152.
I0304 19:28:44.865055 22849303695488 run.py:483] Algo bellman_ford step 1536 current loss 0.095160, current_train_items 49184.
I0304 19:28:44.888885 22849303695488 run.py:483] Algo bellman_ford step 1537 current loss 0.171713, current_train_items 49216.
I0304 19:28:44.919011 22849303695488 run.py:483] Algo bellman_ford step 1538 current loss 0.192548, current_train_items 49248.
I0304 19:28:44.954477 22849303695488 run.py:483] Algo bellman_ford step 1539 current loss 0.302841, current_train_items 49280.
I0304 19:28:44.974108 22849303695488 run.py:483] Algo bellman_ford step 1540 current loss 0.025916, current_train_items 49312.
I0304 19:28:44.990156 22849303695488 run.py:483] Algo bellman_ford step 1541 current loss 0.067974, current_train_items 49344.
I0304 19:28:45.013694 22849303695488 run.py:483] Algo bellman_ford step 1542 current loss 0.178657, current_train_items 49376.
I0304 19:28:45.042792 22849303695488 run.py:483] Algo bellman_ford step 1543 current loss 0.188605, current_train_items 49408.
I0304 19:28:45.075490 22849303695488 run.py:483] Algo bellman_ford step 1544 current loss 0.259136, current_train_items 49440.
I0304 19:28:45.095026 22849303695488 run.py:483] Algo bellman_ford step 1545 current loss 0.032232, current_train_items 49472.
I0304 19:28:45.111444 22849303695488 run.py:483] Algo bellman_ford step 1546 current loss 0.074359, current_train_items 49504.
I0304 19:28:45.135254 22849303695488 run.py:483] Algo bellman_ford step 1547 current loss 0.167117, current_train_items 49536.
I0304 19:28:45.166099 22849303695488 run.py:483] Algo bellman_ford step 1548 current loss 0.108373, current_train_items 49568.
I0304 19:28:45.200217 22849303695488 run.py:483] Algo bellman_ford step 1549 current loss 0.223365, current_train_items 49600.
I0304 19:28:45.219823 22849303695488 run.py:483] Algo bellman_ford step 1550 current loss 0.048216, current_train_items 49632.
I0304 19:28:45.227888 22849303695488 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.9619140625, 'score': 0.9619140625, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0304 19:28:45.227995 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.962, val scores are: bellman_ford: 0.962
I0304 19:28:45.245638 22849303695488 run.py:483] Algo bellman_ford step 1551 current loss 0.104944, current_train_items 49664.
I0304 19:28:45.269702 22849303695488 run.py:483] Algo bellman_ford step 1552 current loss 0.094669, current_train_items 49696.
I0304 19:28:45.300290 22849303695488 run.py:483] Algo bellman_ford step 1553 current loss 0.176596, current_train_items 49728.
I0304 19:28:45.331835 22849303695488 run.py:483] Algo bellman_ford step 1554 current loss 0.196705, current_train_items 49760.
I0304 19:28:45.351504 22849303695488 run.py:483] Algo bellman_ford step 1555 current loss 0.009061, current_train_items 49792.
I0304 19:28:45.367845 22849303695488 run.py:483] Algo bellman_ford step 1556 current loss 0.058872, current_train_items 49824.
I0304 19:28:45.392224 22849303695488 run.py:483] Algo bellman_ford step 1557 current loss 0.195748, current_train_items 49856.
I0304 19:28:45.422812 22849303695488 run.py:483] Algo bellman_ford step 1558 current loss 0.151760, current_train_items 49888.
I0304 19:28:45.456338 22849303695488 run.py:483] Algo bellman_ford step 1559 current loss 0.127580, current_train_items 49920.
I0304 19:28:45.476231 22849303695488 run.py:483] Algo bellman_ford step 1560 current loss 0.041321, current_train_items 49952.
I0304 19:28:45.492852 22849303695488 run.py:483] Algo bellman_ford step 1561 current loss 0.030895, current_train_items 49984.
I0304 19:28:45.516486 22849303695488 run.py:483] Algo bellman_ford step 1562 current loss 0.096099, current_train_items 50016.
I0304 19:28:45.547110 22849303695488 run.py:483] Algo bellman_ford step 1563 current loss 0.122325, current_train_items 50048.
I0304 19:28:45.581025 22849303695488 run.py:483] Algo bellman_ford step 1564 current loss 0.138979, current_train_items 50080.
I0304 19:28:45.600495 22849303695488 run.py:483] Algo bellman_ford step 1565 current loss 0.012658, current_train_items 50112.
I0304 19:28:45.617136 22849303695488 run.py:483] Algo bellman_ford step 1566 current loss 0.068842, current_train_items 50144.
I0304 19:28:45.641381 22849303695488 run.py:483] Algo bellman_ford step 1567 current loss 0.077003, current_train_items 50176.
I0304 19:28:45.671308 22849303695488 run.py:483] Algo bellman_ford step 1568 current loss 0.167461, current_train_items 50208.
I0304 19:28:45.705431 22849303695488 run.py:483] Algo bellman_ford step 1569 current loss 0.190127, current_train_items 50240.
I0304 19:28:45.725201 22849303695488 run.py:483] Algo bellman_ford step 1570 current loss 0.010224, current_train_items 50272.
I0304 19:28:45.741979 22849303695488 run.py:483] Algo bellman_ford step 1571 current loss 0.024248, current_train_items 50304.
I0304 19:28:45.766470 22849303695488 run.py:483] Algo bellman_ford step 1572 current loss 0.098829, current_train_items 50336.
I0304 19:28:45.796569 22849303695488 run.py:483] Algo bellman_ford step 1573 current loss 0.109287, current_train_items 50368.
I0304 19:28:45.827239 22849303695488 run.py:483] Algo bellman_ford step 1574 current loss 0.160787, current_train_items 50400.
I0304 19:28:45.847619 22849303695488 run.py:483] Algo bellman_ford step 1575 current loss 0.011105, current_train_items 50432.
I0304 19:28:45.864206 22849303695488 run.py:483] Algo bellman_ford step 1576 current loss 0.041179, current_train_items 50464.
I0304 19:28:45.886991 22849303695488 run.py:483] Algo bellman_ford step 1577 current loss 0.064226, current_train_items 50496.
I0304 19:28:45.918058 22849303695488 run.py:483] Algo bellman_ford step 1578 current loss 0.293414, current_train_items 50528.
I0304 19:28:45.950607 22849303695488 run.py:483] Algo bellman_ford step 1579 current loss 0.119418, current_train_items 50560.
I0304 19:28:45.970355 22849303695488 run.py:483] Algo bellman_ford step 1580 current loss 0.035288, current_train_items 50592.
I0304 19:28:45.986530 22849303695488 run.py:483] Algo bellman_ford step 1581 current loss 0.092046, current_train_items 50624.
I0304 19:28:46.009909 22849303695488 run.py:483] Algo bellman_ford step 1582 current loss 0.075697, current_train_items 50656.
I0304 19:28:46.040350 22849303695488 run.py:483] Algo bellman_ford step 1583 current loss 0.098699, current_train_items 50688.
I0304 19:28:46.074343 22849303695488 run.py:483] Algo bellman_ford step 1584 current loss 0.186684, current_train_items 50720.
I0304 19:28:46.094087 22849303695488 run.py:483] Algo bellman_ford step 1585 current loss 0.010469, current_train_items 50752.
I0304 19:28:46.110134 22849303695488 run.py:483] Algo bellman_ford step 1586 current loss 0.047258, current_train_items 50784.
I0304 19:28:46.133821 22849303695488 run.py:483] Algo bellman_ford step 1587 current loss 0.101558, current_train_items 50816.
I0304 19:28:46.163470 22849303695488 run.py:483] Algo bellman_ford step 1588 current loss 0.183021, current_train_items 50848.
I0304 19:28:46.196069 22849303695488 run.py:483] Algo bellman_ford step 1589 current loss 0.158255, current_train_items 50880.
I0304 19:28:46.216143 22849303695488 run.py:483] Algo bellman_ford step 1590 current loss 0.022714, current_train_items 50912.
I0304 19:28:46.232685 22849303695488 run.py:483] Algo bellman_ford step 1591 current loss 0.052466, current_train_items 50944.
I0304 19:28:46.255753 22849303695488 run.py:483] Algo bellman_ford step 1592 current loss 0.095957, current_train_items 50976.
I0304 19:28:46.286808 22849303695488 run.py:483] Algo bellman_ford step 1593 current loss 0.125521, current_train_items 51008.
I0304 19:28:46.320953 22849303695488 run.py:483] Algo bellman_ford step 1594 current loss 0.148943, current_train_items 51040.
I0304 19:28:46.340492 22849303695488 run.py:483] Algo bellman_ford step 1595 current loss 0.020539, current_train_items 51072.
I0304 19:28:46.356456 22849303695488 run.py:483] Algo bellman_ford step 1596 current loss 0.039165, current_train_items 51104.
I0304 19:28:46.381116 22849303695488 run.py:483] Algo bellman_ford step 1597 current loss 0.101647, current_train_items 51136.
I0304 19:28:46.410140 22849303695488 run.py:483] Algo bellman_ford step 1598 current loss 0.106015, current_train_items 51168.
I0304 19:28:46.443387 22849303695488 run.py:483] Algo bellman_ford step 1599 current loss 0.160118, current_train_items 51200.
I0304 19:28:46.463065 22849303695488 run.py:483] Algo bellman_ford step 1600 current loss 0.022546, current_train_items 51232.
I0304 19:28:46.470831 22849303695488 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0304 19:28:46.470939 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:28:46.487841 22849303695488 run.py:483] Algo bellman_ford step 1601 current loss 0.051845, current_train_items 51264.
I0304 19:28:46.513253 22849303695488 run.py:483] Algo bellman_ford step 1602 current loss 0.054877, current_train_items 51296.
I0304 19:28:46.543789 22849303695488 run.py:483] Algo bellman_ford step 1603 current loss 0.115723, current_train_items 51328.
I0304 19:28:46.577084 22849303695488 run.py:483] Algo bellman_ford step 1604 current loss 0.168896, current_train_items 51360.
I0304 19:28:46.597026 22849303695488 run.py:483] Algo bellman_ford step 1605 current loss 0.047695, current_train_items 51392.
I0304 19:28:46.613044 22849303695488 run.py:483] Algo bellman_ford step 1606 current loss 0.067291, current_train_items 51424.
I0304 19:28:46.636953 22849303695488 run.py:483] Algo bellman_ford step 1607 current loss 0.074680, current_train_items 51456.
I0304 19:28:46.668267 22849303695488 run.py:483] Algo bellman_ford step 1608 current loss 0.132485, current_train_items 51488.
I0304 19:28:46.702038 22849303695488 run.py:483] Algo bellman_ford step 1609 current loss 0.254081, current_train_items 51520.
I0304 19:28:46.721948 22849303695488 run.py:483] Algo bellman_ford step 1610 current loss 0.027444, current_train_items 51552.
I0304 19:28:46.738689 22849303695488 run.py:483] Algo bellman_ford step 1611 current loss 0.068026, current_train_items 51584.
I0304 19:28:46.762683 22849303695488 run.py:483] Algo bellman_ford step 1612 current loss 0.091535, current_train_items 51616.
I0304 19:28:46.793127 22849303695488 run.py:483] Algo bellman_ford step 1613 current loss 0.210705, current_train_items 51648.
I0304 19:28:46.825399 22849303695488 run.py:483] Algo bellman_ford step 1614 current loss 0.172725, current_train_items 51680.
I0304 19:28:46.844918 22849303695488 run.py:483] Algo bellman_ford step 1615 current loss 0.014464, current_train_items 51712.
I0304 19:28:46.861553 22849303695488 run.py:483] Algo bellman_ford step 1616 current loss 0.043546, current_train_items 51744.
I0304 19:28:46.885587 22849303695488 run.py:483] Algo bellman_ford step 1617 current loss 0.317468, current_train_items 51776.
I0304 19:28:46.915210 22849303695488 run.py:483] Algo bellman_ford step 1618 current loss 0.165846, current_train_items 51808.
I0304 19:28:46.949935 22849303695488 run.py:483] Algo bellman_ford step 1619 current loss 0.248705, current_train_items 51840.
I0304 19:28:46.969425 22849303695488 run.py:483] Algo bellman_ford step 1620 current loss 0.031168, current_train_items 51872.
I0304 19:28:46.985662 22849303695488 run.py:483] Algo bellman_ford step 1621 current loss 0.063362, current_train_items 51904.
I0304 19:28:47.009968 22849303695488 run.py:483] Algo bellman_ford step 1622 current loss 0.151270, current_train_items 51936.
I0304 19:28:47.040549 22849303695488 run.py:483] Algo bellman_ford step 1623 current loss 0.147015, current_train_items 51968.
I0304 19:28:47.073882 22849303695488 run.py:483] Algo bellman_ford step 1624 current loss 0.147465, current_train_items 52000.
I0304 19:28:47.093647 22849303695488 run.py:483] Algo bellman_ford step 1625 current loss 0.014520, current_train_items 52032.
I0304 19:28:47.110242 22849303695488 run.py:483] Algo bellman_ford step 1626 current loss 0.081698, current_train_items 52064.
I0304 19:28:47.134081 22849303695488 run.py:483] Algo bellman_ford step 1627 current loss 0.190667, current_train_items 52096.
I0304 19:28:47.165729 22849303695488 run.py:483] Algo bellman_ford step 1628 current loss 0.245785, current_train_items 52128.
I0304 19:28:47.198263 22849303695488 run.py:483] Algo bellman_ford step 1629 current loss 0.197339, current_train_items 52160.
I0304 19:28:47.217928 22849303695488 run.py:483] Algo bellman_ford step 1630 current loss 0.011014, current_train_items 52192.
I0304 19:28:47.234862 22849303695488 run.py:483] Algo bellman_ford step 1631 current loss 0.064632, current_train_items 52224.
I0304 19:28:47.259187 22849303695488 run.py:483] Algo bellman_ford step 1632 current loss 0.193445, current_train_items 52256.
I0304 19:28:47.288921 22849303695488 run.py:483] Algo bellman_ford step 1633 current loss 0.122856, current_train_items 52288.
I0304 19:28:47.322466 22849303695488 run.py:483] Algo bellman_ford step 1634 current loss 0.185836, current_train_items 52320.
I0304 19:28:47.342079 22849303695488 run.py:483] Algo bellman_ford step 1635 current loss 0.016523, current_train_items 52352.
I0304 19:28:47.358652 22849303695488 run.py:483] Algo bellman_ford step 1636 current loss 0.093211, current_train_items 52384.
I0304 19:28:47.382955 22849303695488 run.py:483] Algo bellman_ford step 1637 current loss 0.154731, current_train_items 52416.
I0304 19:28:47.413519 22849303695488 run.py:483] Algo bellman_ford step 1638 current loss 0.187997, current_train_items 52448.
I0304 19:28:47.448902 22849303695488 run.py:483] Algo bellman_ford step 1639 current loss 0.160971, current_train_items 52480.
I0304 19:28:47.468300 22849303695488 run.py:483] Algo bellman_ford step 1640 current loss 0.022926, current_train_items 52512.
I0304 19:28:47.484409 22849303695488 run.py:483] Algo bellman_ford step 1641 current loss 0.076265, current_train_items 52544.
I0304 19:28:47.508292 22849303695488 run.py:483] Algo bellman_ford step 1642 current loss 0.257538, current_train_items 52576.
I0304 19:28:47.538518 22849303695488 run.py:483] Algo bellman_ford step 1643 current loss 0.174717, current_train_items 52608.
I0304 19:28:47.570632 22849303695488 run.py:483] Algo bellman_ford step 1644 current loss 0.166879, current_train_items 52640.
I0304 19:28:47.590193 22849303695488 run.py:483] Algo bellman_ford step 1645 current loss 0.030454, current_train_items 52672.
I0304 19:28:47.606609 22849303695488 run.py:483] Algo bellman_ford step 1646 current loss 0.044962, current_train_items 52704.
I0304 19:28:47.631446 22849303695488 run.py:483] Algo bellman_ford step 1647 current loss 0.184169, current_train_items 52736.
I0304 19:28:47.660242 22849303695488 run.py:483] Algo bellman_ford step 1648 current loss 0.209414, current_train_items 52768.
I0304 19:28:47.693904 22849303695488 run.py:483] Algo bellman_ford step 1649 current loss 0.362526, current_train_items 52800.
I0304 19:28:47.713595 22849303695488 run.py:483] Algo bellman_ford step 1650 current loss 0.047344, current_train_items 52832.
I0304 19:28:47.721741 22849303695488 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0304 19:28:47.721849 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:28:47.739758 22849303695488 run.py:483] Algo bellman_ford step 1651 current loss 0.087475, current_train_items 52864.
I0304 19:28:47.764871 22849303695488 run.py:483] Algo bellman_ford step 1652 current loss 0.157484, current_train_items 52896.
I0304 19:28:47.794939 22849303695488 run.py:483] Algo bellman_ford step 1653 current loss 0.204772, current_train_items 52928.
I0304 19:28:47.826678 22849303695488 run.py:483] Algo bellman_ford step 1654 current loss 0.159355, current_train_items 52960.
I0304 19:28:47.846670 22849303695488 run.py:483] Algo bellman_ford step 1655 current loss 0.013968, current_train_items 52992.
I0304 19:28:47.862906 22849303695488 run.py:483] Algo bellman_ford step 1656 current loss 0.083270, current_train_items 53024.
I0304 19:28:47.886305 22849303695488 run.py:483] Algo bellman_ford step 1657 current loss 0.136665, current_train_items 53056.
I0304 19:28:47.914737 22849303695488 run.py:483] Algo bellman_ford step 1658 current loss 0.175767, current_train_items 53088.
I0304 19:28:47.946256 22849303695488 run.py:483] Algo bellman_ford step 1659 current loss 0.321247, current_train_items 53120.
I0304 19:28:47.966199 22849303695488 run.py:483] Algo bellman_ford step 1660 current loss 0.016957, current_train_items 53152.
I0304 19:28:47.982796 22849303695488 run.py:483] Algo bellman_ford step 1661 current loss 0.116324, current_train_items 53184.
I0304 19:28:48.005664 22849303695488 run.py:483] Algo bellman_ford step 1662 current loss 0.149049, current_train_items 53216.
I0304 19:28:48.037308 22849303695488 run.py:483] Algo bellman_ford step 1663 current loss 0.169733, current_train_items 53248.
I0304 19:28:48.070923 22849303695488 run.py:483] Algo bellman_ford step 1664 current loss 0.197037, current_train_items 53280.
I0304 19:28:48.090240 22849303695488 run.py:483] Algo bellman_ford step 1665 current loss 0.047590, current_train_items 53312.
I0304 19:28:48.106862 22849303695488 run.py:483] Algo bellman_ford step 1666 current loss 0.060842, current_train_items 53344.
I0304 19:28:48.130573 22849303695488 run.py:483] Algo bellman_ford step 1667 current loss 0.059633, current_train_items 53376.
I0304 19:28:48.159436 22849303695488 run.py:483] Algo bellman_ford step 1668 current loss 0.088167, current_train_items 53408.
I0304 19:28:48.190726 22849303695488 run.py:483] Algo bellman_ford step 1669 current loss 0.179559, current_train_items 53440.
I0304 19:28:48.210279 22849303695488 run.py:483] Algo bellman_ford step 1670 current loss 0.012624, current_train_items 53472.
I0304 19:28:48.226639 22849303695488 run.py:483] Algo bellman_ford step 1671 current loss 0.050369, current_train_items 53504.
I0304 19:28:48.249631 22849303695488 run.py:483] Algo bellman_ford step 1672 current loss 0.088477, current_train_items 53536.
I0304 19:28:48.280398 22849303695488 run.py:483] Algo bellman_ford step 1673 current loss 0.165632, current_train_items 53568.
I0304 19:28:48.313431 22849303695488 run.py:483] Algo bellman_ford step 1674 current loss 0.130151, current_train_items 53600.
I0304 19:28:48.332921 22849303695488 run.py:483] Algo bellman_ford step 1675 current loss 0.014874, current_train_items 53632.
I0304 19:28:48.349654 22849303695488 run.py:483] Algo bellman_ford step 1676 current loss 0.097668, current_train_items 53664.
I0304 19:28:48.373042 22849303695488 run.py:483] Algo bellman_ford step 1677 current loss 0.088683, current_train_items 53696.
I0304 19:28:48.402878 22849303695488 run.py:483] Algo bellman_ford step 1678 current loss 0.100620, current_train_items 53728.
I0304 19:28:48.436776 22849303695488 run.py:483] Algo bellman_ford step 1679 current loss 0.122604, current_train_items 53760.
I0304 19:28:48.456072 22849303695488 run.py:483] Algo bellman_ford step 1680 current loss 0.013805, current_train_items 53792.
I0304 19:28:48.472289 22849303695488 run.py:483] Algo bellman_ford step 1681 current loss 0.026340, current_train_items 53824.
I0304 19:28:48.495111 22849303695488 run.py:483] Algo bellman_ford step 1682 current loss 0.148332, current_train_items 53856.
I0304 19:28:48.523983 22849303695488 run.py:483] Algo bellman_ford step 1683 current loss 0.114913, current_train_items 53888.
I0304 19:28:48.556801 22849303695488 run.py:483] Algo bellman_ford step 1684 current loss 0.137697, current_train_items 53920.
I0304 19:28:48.576733 22849303695488 run.py:483] Algo bellman_ford step 1685 current loss 0.021934, current_train_items 53952.
I0304 19:28:48.593362 22849303695488 run.py:483] Algo bellman_ford step 1686 current loss 0.095671, current_train_items 53984.
I0304 19:28:48.617167 22849303695488 run.py:483] Algo bellman_ford step 1687 current loss 0.078616, current_train_items 54016.
I0304 19:28:48.646929 22849303695488 run.py:483] Algo bellman_ford step 1688 current loss 0.115891, current_train_items 54048.
I0304 19:28:48.681515 22849303695488 run.py:483] Algo bellman_ford step 1689 current loss 0.149350, current_train_items 54080.
I0304 19:28:48.701044 22849303695488 run.py:483] Algo bellman_ford step 1690 current loss 0.009977, current_train_items 54112.
I0304 19:28:48.717861 22849303695488 run.py:483] Algo bellman_ford step 1691 current loss 0.062909, current_train_items 54144.
I0304 19:28:48.741708 22849303695488 run.py:483] Algo bellman_ford step 1692 current loss 0.088824, current_train_items 54176.
I0304 19:28:48.772527 22849303695488 run.py:483] Algo bellman_ford step 1693 current loss 0.145931, current_train_items 54208.
I0304 19:28:48.805610 22849303695488 run.py:483] Algo bellman_ford step 1694 current loss 0.168668, current_train_items 54240.
I0304 19:28:48.824800 22849303695488 run.py:483] Algo bellman_ford step 1695 current loss 0.012491, current_train_items 54272.
I0304 19:28:48.841542 22849303695488 run.py:483] Algo bellman_ford step 1696 current loss 0.047834, current_train_items 54304.
I0304 19:28:48.863648 22849303695488 run.py:483] Algo bellman_ford step 1697 current loss 0.102310, current_train_items 54336.
I0304 19:28:48.892553 22849303695488 run.py:483] Algo bellman_ford step 1698 current loss 0.188597, current_train_items 54368.
I0304 19:28:48.925738 22849303695488 run.py:483] Algo bellman_ford step 1699 current loss 0.336315, current_train_items 54400.
I0304 19:28:48.945384 22849303695488 run.py:483] Algo bellman_ford step 1700 current loss 0.017300, current_train_items 54432.
I0304 19:28:48.953350 22849303695488 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.962890625, 'score': 0.962890625, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0304 19:28:48.953458 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.963, val scores are: bellman_ford: 0.963
I0304 19:28:48.970486 22849303695488 run.py:483] Algo bellman_ford step 1701 current loss 0.054838, current_train_items 54464.
I0304 19:28:48.994024 22849303695488 run.py:483] Algo bellman_ford step 1702 current loss 0.095187, current_train_items 54496.
I0304 19:28:49.024178 22849303695488 run.py:483] Algo bellman_ford step 1703 current loss 0.087736, current_train_items 54528.
I0304 19:28:49.056692 22849303695488 run.py:483] Algo bellman_ford step 1704 current loss 0.157318, current_train_items 54560.
I0304 19:28:49.076487 22849303695488 run.py:483] Algo bellman_ford step 1705 current loss 0.009367, current_train_items 54592.
I0304 19:28:49.092397 22849303695488 run.py:483] Algo bellman_ford step 1706 current loss 0.020642, current_train_items 54624.
I0304 19:28:49.116020 22849303695488 run.py:483] Algo bellman_ford step 1707 current loss 0.131036, current_train_items 54656.
I0304 19:28:49.145938 22849303695488 run.py:483] Algo bellman_ford step 1708 current loss 0.123431, current_train_items 54688.
I0304 19:28:49.179954 22849303695488 run.py:483] Algo bellman_ford step 1709 current loss 0.110039, current_train_items 54720.
I0304 19:28:49.199390 22849303695488 run.py:483] Algo bellman_ford step 1710 current loss 0.046819, current_train_items 54752.
I0304 19:28:49.216282 22849303695488 run.py:483] Algo bellman_ford step 1711 current loss 0.027703, current_train_items 54784.
I0304 19:28:49.240058 22849303695488 run.py:483] Algo bellman_ford step 1712 current loss 0.102667, current_train_items 54816.
I0304 19:28:49.269394 22849303695488 run.py:483] Algo bellman_ford step 1713 current loss 0.117397, current_train_items 54848.
I0304 19:28:49.301555 22849303695488 run.py:483] Algo bellman_ford step 1714 current loss 0.137378, current_train_items 54880.
I0304 19:28:49.320847 22849303695488 run.py:483] Algo bellman_ford step 1715 current loss 0.011400, current_train_items 54912.
I0304 19:28:49.336784 22849303695488 run.py:483] Algo bellman_ford step 1716 current loss 0.022812, current_train_items 54944.
I0304 19:28:49.360744 22849303695488 run.py:483] Algo bellman_ford step 1717 current loss 0.073409, current_train_items 54976.
I0304 19:28:49.389160 22849303695488 run.py:483] Algo bellman_ford step 1718 current loss 0.129072, current_train_items 55008.
I0304 19:28:49.421897 22849303695488 run.py:483] Algo bellman_ford step 1719 current loss 0.106360, current_train_items 55040.
I0304 19:28:49.441058 22849303695488 run.py:483] Algo bellman_ford step 1720 current loss 0.036739, current_train_items 55072.
I0304 19:28:49.457273 22849303695488 run.py:483] Algo bellman_ford step 1721 current loss 0.068798, current_train_items 55104.
I0304 19:28:49.480603 22849303695488 run.py:483] Algo bellman_ford step 1722 current loss 0.120021, current_train_items 55136.
I0304 19:28:49.510405 22849303695488 run.py:483] Algo bellman_ford step 1723 current loss 0.170918, current_train_items 55168.
I0304 19:28:49.542607 22849303695488 run.py:483] Algo bellman_ford step 1724 current loss 0.226387, current_train_items 55200.
I0304 19:28:49.561719 22849303695488 run.py:483] Algo bellman_ford step 1725 current loss 0.007858, current_train_items 55232.
I0304 19:28:49.578199 22849303695488 run.py:483] Algo bellman_ford step 1726 current loss 0.141724, current_train_items 55264.
I0304 19:28:49.603142 22849303695488 run.py:483] Algo bellman_ford step 1727 current loss 0.173968, current_train_items 55296.
I0304 19:28:49.632946 22849303695488 run.py:483] Algo bellman_ford step 1728 current loss 0.156165, current_train_items 55328.
I0304 19:28:49.665451 22849303695488 run.py:483] Algo bellman_ford step 1729 current loss 0.161854, current_train_items 55360.
I0304 19:28:49.684624 22849303695488 run.py:483] Algo bellman_ford step 1730 current loss 0.014997, current_train_items 55392.
I0304 19:28:49.701073 22849303695488 run.py:483] Algo bellman_ford step 1731 current loss 0.044376, current_train_items 55424.
I0304 19:28:49.724691 22849303695488 run.py:483] Algo bellman_ford step 1732 current loss 0.239453, current_train_items 55456.
I0304 19:28:49.754547 22849303695488 run.py:483] Algo bellman_ford step 1733 current loss 0.323719, current_train_items 55488.
I0304 19:28:49.785121 22849303695488 run.py:483] Algo bellman_ford step 1734 current loss 0.243031, current_train_items 55520.
I0304 19:28:49.804600 22849303695488 run.py:483] Algo bellman_ford step 1735 current loss 0.014670, current_train_items 55552.
I0304 19:28:49.821191 22849303695488 run.py:483] Algo bellman_ford step 1736 current loss 0.052313, current_train_items 55584.
I0304 19:28:49.845349 22849303695488 run.py:483] Algo bellman_ford step 1737 current loss 0.173504, current_train_items 55616.
I0304 19:28:49.875098 22849303695488 run.py:483] Algo bellman_ford step 1738 current loss 0.137996, current_train_items 55648.
I0304 19:28:49.908384 22849303695488 run.py:483] Algo bellman_ford step 1739 current loss 0.211181, current_train_items 55680.
I0304 19:28:49.927458 22849303695488 run.py:483] Algo bellman_ford step 1740 current loss 0.013372, current_train_items 55712.
I0304 19:28:49.943955 22849303695488 run.py:483] Algo bellman_ford step 1741 current loss 0.032684, current_train_items 55744.
I0304 19:28:49.967521 22849303695488 run.py:483] Algo bellman_ford step 1742 current loss 0.068295, current_train_items 55776.
I0304 19:28:49.998044 22849303695488 run.py:483] Algo bellman_ford step 1743 current loss 0.068162, current_train_items 55808.
I0304 19:28:50.029570 22849303695488 run.py:483] Algo bellman_ford step 1744 current loss 0.136558, current_train_items 55840.
I0304 19:28:50.048867 22849303695488 run.py:483] Algo bellman_ford step 1745 current loss 0.012134, current_train_items 55872.
I0304 19:28:50.065468 22849303695488 run.py:483] Algo bellman_ford step 1746 current loss 0.048555, current_train_items 55904.
I0304 19:28:50.089069 22849303695488 run.py:483] Algo bellman_ford step 1747 current loss 0.107658, current_train_items 55936.
I0304 19:28:50.119038 22849303695488 run.py:483] Algo bellman_ford step 1748 current loss 0.096242, current_train_items 55968.
I0304 19:28:50.150182 22849303695488 run.py:483] Algo bellman_ford step 1749 current loss 0.088196, current_train_items 56000.
I0304 19:28:50.169548 22849303695488 run.py:483] Algo bellman_ford step 1750 current loss 0.029135, current_train_items 56032.
I0304 19:28:50.177662 22849303695488 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0304 19:28:50.177773 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.978, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:28:50.207963 22849303695488 run.py:483] Algo bellman_ford step 1751 current loss 0.058362, current_train_items 56064.
I0304 19:28:50.232387 22849303695488 run.py:483] Algo bellman_ford step 1752 current loss 0.054190, current_train_items 56096.
I0304 19:28:50.264155 22849303695488 run.py:483] Algo bellman_ford step 1753 current loss 0.143570, current_train_items 56128.
I0304 19:28:50.298955 22849303695488 run.py:483] Algo bellman_ford step 1754 current loss 0.169744, current_train_items 56160.
I0304 19:28:50.319139 22849303695488 run.py:483] Algo bellman_ford step 1755 current loss 0.008449, current_train_items 56192.
I0304 19:28:50.335914 22849303695488 run.py:483] Algo bellman_ford step 1756 current loss 0.040956, current_train_items 56224.
I0304 19:28:50.360243 22849303695488 run.py:483] Algo bellman_ford step 1757 current loss 0.082068, current_train_items 56256.
I0304 19:28:50.391396 22849303695488 run.py:483] Algo bellman_ford step 1758 current loss 0.104616, current_train_items 56288.
I0304 19:28:50.421947 22849303695488 run.py:483] Algo bellman_ford step 1759 current loss 0.120846, current_train_items 56320.
I0304 19:28:50.441656 22849303695488 run.py:483] Algo bellman_ford step 1760 current loss 0.010534, current_train_items 56352.
I0304 19:28:50.458181 22849303695488 run.py:483] Algo bellman_ford step 1761 current loss 0.038326, current_train_items 56384.
I0304 19:28:50.482629 22849303695488 run.py:483] Algo bellman_ford step 1762 current loss 0.143154, current_train_items 56416.
I0304 19:28:50.514556 22849303695488 run.py:483] Algo bellman_ford step 1763 current loss 0.090977, current_train_items 56448.
I0304 19:28:50.549088 22849303695488 run.py:483] Algo bellman_ford step 1764 current loss 0.200039, current_train_items 56480.
I0304 19:28:50.568700 22849303695488 run.py:483] Algo bellman_ford step 1765 current loss 0.014493, current_train_items 56512.
I0304 19:28:50.584942 22849303695488 run.py:483] Algo bellman_ford step 1766 current loss 0.017882, current_train_items 56544.
I0304 19:28:50.608456 22849303695488 run.py:483] Algo bellman_ford step 1767 current loss 0.091356, current_train_items 56576.
I0304 19:28:50.638176 22849303695488 run.py:483] Algo bellman_ford step 1768 current loss 0.090295, current_train_items 56608.
I0304 19:28:50.670234 22849303695488 run.py:483] Algo bellman_ford step 1769 current loss 0.158685, current_train_items 56640.
I0304 19:28:50.690200 22849303695488 run.py:483] Algo bellman_ford step 1770 current loss 0.031271, current_train_items 56672.
I0304 19:28:50.707134 22849303695488 run.py:483] Algo bellman_ford step 1771 current loss 0.083536, current_train_items 56704.
I0304 19:28:50.731571 22849303695488 run.py:483] Algo bellman_ford step 1772 current loss 0.146505, current_train_items 56736.
I0304 19:28:50.762309 22849303695488 run.py:483] Algo bellman_ford step 1773 current loss 0.143008, current_train_items 56768.
I0304 19:28:50.794278 22849303695488 run.py:483] Algo bellman_ford step 1774 current loss 0.170605, current_train_items 56800.
I0304 19:28:50.814141 22849303695488 run.py:483] Algo bellman_ford step 1775 current loss 0.012080, current_train_items 56832.
I0304 19:28:50.830875 22849303695488 run.py:483] Algo bellman_ford step 1776 current loss 0.113401, current_train_items 56864.
I0304 19:28:50.854824 22849303695488 run.py:483] Algo bellman_ford step 1777 current loss 0.177053, current_train_items 56896.
I0304 19:28:50.884706 22849303695488 run.py:483] Algo bellman_ford step 1778 current loss 0.117300, current_train_items 56928.
I0304 19:28:50.916133 22849303695488 run.py:483] Algo bellman_ford step 1779 current loss 0.118479, current_train_items 56960.
I0304 19:28:50.935744 22849303695488 run.py:483] Algo bellman_ford step 1780 current loss 0.009626, current_train_items 56992.
I0304 19:28:50.952074 22849303695488 run.py:483] Algo bellman_ford step 1781 current loss 0.054965, current_train_items 57024.
I0304 19:28:50.975573 22849303695488 run.py:483] Algo bellman_ford step 1782 current loss 0.179336, current_train_items 57056.
I0304 19:28:51.006156 22849303695488 run.py:483] Algo bellman_ford step 1783 current loss 0.206363, current_train_items 57088.
I0304 19:28:51.038742 22849303695488 run.py:483] Algo bellman_ford step 1784 current loss 0.138278, current_train_items 57120.
I0304 19:28:51.058489 22849303695488 run.py:483] Algo bellman_ford step 1785 current loss 0.008495, current_train_items 57152.
I0304 19:28:51.075220 22849303695488 run.py:483] Algo bellman_ford step 1786 current loss 0.037298, current_train_items 57184.
I0304 19:28:51.098218 22849303695488 run.py:483] Algo bellman_ford step 1787 current loss 0.190300, current_train_items 57216.
I0304 19:28:51.127260 22849303695488 run.py:483] Algo bellman_ford step 1788 current loss 0.138381, current_train_items 57248.
I0304 19:28:51.160386 22849303695488 run.py:483] Algo bellman_ford step 1789 current loss 0.136480, current_train_items 57280.
I0304 19:28:51.179981 22849303695488 run.py:483] Algo bellman_ford step 1790 current loss 0.013839, current_train_items 57312.
I0304 19:28:51.197238 22849303695488 run.py:483] Algo bellman_ford step 1791 current loss 0.067717, current_train_items 57344.
I0304 19:28:51.221189 22849303695488 run.py:483] Algo bellman_ford step 1792 current loss 0.144359, current_train_items 57376.
I0304 19:28:51.250229 22849303695488 run.py:483] Algo bellman_ford step 1793 current loss 0.221714, current_train_items 57408.
I0304 19:28:51.282562 22849303695488 run.py:483] Algo bellman_ford step 1794 current loss 0.183384, current_train_items 57440.
I0304 19:28:51.302064 22849303695488 run.py:483] Algo bellman_ford step 1795 current loss 0.026938, current_train_items 57472.
I0304 19:28:51.318423 22849303695488 run.py:483] Algo bellman_ford step 1796 current loss 0.075545, current_train_items 57504.
I0304 19:28:51.343036 22849303695488 run.py:483] Algo bellman_ford step 1797 current loss 0.093823, current_train_items 57536.
I0304 19:28:51.373809 22849303695488 run.py:483] Algo bellman_ford step 1798 current loss 0.229136, current_train_items 57568.
I0304 19:28:51.407660 22849303695488 run.py:483] Algo bellman_ford step 1799 current loss 0.201450, current_train_items 57600.
I0304 19:28:51.427420 22849303695488 run.py:483] Algo bellman_ford step 1800 current loss 0.012989, current_train_items 57632.
I0304 19:28:51.435582 22849303695488 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0304 19:28:51.435691 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:28:51.452591 22849303695488 run.py:483] Algo bellman_ford step 1801 current loss 0.029692, current_train_items 57664.
I0304 19:28:51.477219 22849303695488 run.py:483] Algo bellman_ford step 1802 current loss 0.143769, current_train_items 57696.
I0304 19:28:51.507106 22849303695488 run.py:483] Algo bellman_ford step 1803 current loss 0.178838, current_train_items 57728.
I0304 19:28:51.539189 22849303695488 run.py:483] Algo bellman_ford step 1804 current loss 0.205234, current_train_items 57760.
I0304 19:28:51.559221 22849303695488 run.py:483] Algo bellman_ford step 1805 current loss 0.006607, current_train_items 57792.
I0304 19:28:51.575231 22849303695488 run.py:483] Algo bellman_ford step 1806 current loss 0.023625, current_train_items 57824.
I0304 19:28:51.598576 22849303695488 run.py:483] Algo bellman_ford step 1807 current loss 0.064884, current_train_items 57856.
I0304 19:28:51.627728 22849303695488 run.py:483] Algo bellman_ford step 1808 current loss 0.098882, current_train_items 57888.
I0304 19:28:51.661293 22849303695488 run.py:483] Algo bellman_ford step 1809 current loss 0.164312, current_train_items 57920.
I0304 19:28:51.680553 22849303695488 run.py:483] Algo bellman_ford step 1810 current loss 0.007064, current_train_items 57952.
I0304 19:28:51.697053 22849303695488 run.py:483] Algo bellman_ford step 1811 current loss 0.039762, current_train_items 57984.
I0304 19:28:51.721181 22849303695488 run.py:483] Algo bellman_ford step 1812 current loss 0.089091, current_train_items 58016.
I0304 19:28:51.749201 22849303695488 run.py:483] Algo bellman_ford step 1813 current loss 0.100655, current_train_items 58048.
I0304 19:28:51.782065 22849303695488 run.py:483] Algo bellman_ford step 1814 current loss 0.141869, current_train_items 58080.
I0304 19:28:51.801702 22849303695488 run.py:483] Algo bellman_ford step 1815 current loss 0.011072, current_train_items 58112.
I0304 19:28:51.818672 22849303695488 run.py:483] Algo bellman_ford step 1816 current loss 0.084466, current_train_items 58144.
I0304 19:28:51.841921 22849303695488 run.py:483] Algo bellman_ford step 1817 current loss 0.080128, current_train_items 58176.
I0304 19:28:51.872086 22849303695488 run.py:483] Algo bellman_ford step 1818 current loss 0.098468, current_train_items 58208.
I0304 19:28:51.904731 22849303695488 run.py:483] Algo bellman_ford step 1819 current loss 0.138895, current_train_items 58240.
I0304 19:28:51.924128 22849303695488 run.py:483] Algo bellman_ford step 1820 current loss 0.022759, current_train_items 58272.
I0304 19:28:51.940960 22849303695488 run.py:483] Algo bellman_ford step 1821 current loss 0.061772, current_train_items 58304.
I0304 19:28:51.966522 22849303695488 run.py:483] Algo bellman_ford step 1822 current loss 0.097498, current_train_items 58336.
I0304 19:28:51.997055 22849303695488 run.py:483] Algo bellman_ford step 1823 current loss 0.093275, current_train_items 58368.
I0304 19:28:52.028745 22849303695488 run.py:483] Algo bellman_ford step 1824 current loss 0.107377, current_train_items 58400.
I0304 19:28:52.048029 22849303695488 run.py:483] Algo bellman_ford step 1825 current loss 0.006639, current_train_items 58432.
I0304 19:28:52.064862 22849303695488 run.py:483] Algo bellman_ford step 1826 current loss 0.043590, current_train_items 58464.
I0304 19:28:52.088479 22849303695488 run.py:483] Algo bellman_ford step 1827 current loss 0.076815, current_train_items 58496.
I0304 19:28:52.119781 22849303695488 run.py:483] Algo bellman_ford step 1828 current loss 0.154571, current_train_items 58528.
I0304 19:28:52.155158 22849303695488 run.py:483] Algo bellman_ford step 1829 current loss 0.183772, current_train_items 58560.
I0304 19:28:52.174527 22849303695488 run.py:483] Algo bellman_ford step 1830 current loss 0.012700, current_train_items 58592.
I0304 19:28:52.191126 22849303695488 run.py:483] Algo bellman_ford step 1831 current loss 0.070248, current_train_items 58624.
I0304 19:28:52.216493 22849303695488 run.py:483] Algo bellman_ford step 1832 current loss 0.101385, current_train_items 58656.
I0304 19:28:52.246636 22849303695488 run.py:483] Algo bellman_ford step 1833 current loss 0.134993, current_train_items 58688.
I0304 19:28:52.278937 22849303695488 run.py:483] Algo bellman_ford step 1834 current loss 0.135673, current_train_items 58720.
I0304 19:28:52.298400 22849303695488 run.py:483] Algo bellman_ford step 1835 current loss 0.016796, current_train_items 58752.
I0304 19:28:52.315022 22849303695488 run.py:483] Algo bellman_ford step 1836 current loss 0.045532, current_train_items 58784.
I0304 19:28:52.340313 22849303695488 run.py:483] Algo bellman_ford step 1837 current loss 0.120514, current_train_items 58816.
I0304 19:28:52.369826 22849303695488 run.py:483] Algo bellman_ford step 1838 current loss 0.104132, current_train_items 58848.
I0304 19:28:52.403021 22849303695488 run.py:483] Algo bellman_ford step 1839 current loss 0.163923, current_train_items 58880.
I0304 19:28:52.422552 22849303695488 run.py:483] Algo bellman_ford step 1840 current loss 0.017427, current_train_items 58912.
I0304 19:28:52.439140 22849303695488 run.py:483] Algo bellman_ford step 1841 current loss 0.036401, current_train_items 58944.
I0304 19:28:52.463610 22849303695488 run.py:483] Algo bellman_ford step 1842 current loss 0.130485, current_train_items 58976.
I0304 19:28:52.494381 22849303695488 run.py:483] Algo bellman_ford step 1843 current loss 0.115422, current_train_items 59008.
I0304 19:28:52.526973 22849303695488 run.py:483] Algo bellman_ford step 1844 current loss 0.120284, current_train_items 59040.
I0304 19:28:52.546308 22849303695488 run.py:483] Algo bellman_ford step 1845 current loss 0.013260, current_train_items 59072.
I0304 19:28:52.562660 22849303695488 run.py:483] Algo bellman_ford step 1846 current loss 0.067493, current_train_items 59104.
I0304 19:28:52.586780 22849303695488 run.py:483] Algo bellman_ford step 1847 current loss 0.112391, current_train_items 59136.
I0304 19:28:52.616996 22849303695488 run.py:483] Algo bellman_ford step 1848 current loss 0.096762, current_train_items 59168.
I0304 19:28:52.651326 22849303695488 run.py:483] Algo bellman_ford step 1849 current loss 0.141795, current_train_items 59200.
I0304 19:28:52.671273 22849303695488 run.py:483] Algo bellman_ford step 1850 current loss 0.015502, current_train_items 59232.
I0304 19:28:52.679569 22849303695488 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0304 19:28:52.679675 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:28:52.696877 22849303695488 run.py:483] Algo bellman_ford step 1851 current loss 0.031539, current_train_items 59264.
I0304 19:28:52.721528 22849303695488 run.py:483] Algo bellman_ford step 1852 current loss 0.098518, current_train_items 59296.
I0304 19:28:52.752396 22849303695488 run.py:483] Algo bellman_ford step 1853 current loss 0.106228, current_train_items 59328.
I0304 19:28:52.785480 22849303695488 run.py:483] Algo bellman_ford step 1854 current loss 0.122993, current_train_items 59360.
I0304 19:28:52.805607 22849303695488 run.py:483] Algo bellman_ford step 1855 current loss 0.019434, current_train_items 59392.
I0304 19:28:52.821551 22849303695488 run.py:483] Algo bellman_ford step 1856 current loss 0.037915, current_train_items 59424.
I0304 19:28:52.845914 22849303695488 run.py:483] Algo bellman_ford step 1857 current loss 0.152814, current_train_items 59456.
I0304 19:28:52.875345 22849303695488 run.py:483] Algo bellman_ford step 1858 current loss 0.088839, current_train_items 59488.
I0304 19:28:52.908865 22849303695488 run.py:483] Algo bellman_ford step 1859 current loss 0.131306, current_train_items 59520.
I0304 19:28:52.929175 22849303695488 run.py:483] Algo bellman_ford step 1860 current loss 0.015924, current_train_items 59552.
I0304 19:28:52.946141 22849303695488 run.py:483] Algo bellman_ford step 1861 current loss 0.048910, current_train_items 59584.
I0304 19:28:52.969440 22849303695488 run.py:483] Algo bellman_ford step 1862 current loss 0.114352, current_train_items 59616.
I0304 19:28:52.999566 22849303695488 run.py:483] Algo bellman_ford step 1863 current loss 0.086708, current_train_items 59648.
I0304 19:28:53.033478 22849303695488 run.py:483] Algo bellman_ford step 1864 current loss 0.133515, current_train_items 59680.
I0304 19:28:53.053250 22849303695488 run.py:483] Algo bellman_ford step 1865 current loss 0.022301, current_train_items 59712.
I0304 19:28:53.069912 22849303695488 run.py:483] Algo bellman_ford step 1866 current loss 0.060795, current_train_items 59744.
I0304 19:28:53.094936 22849303695488 run.py:483] Algo bellman_ford step 1867 current loss 0.137418, current_train_items 59776.
I0304 19:28:53.125341 22849303695488 run.py:483] Algo bellman_ford step 1868 current loss 0.181537, current_train_items 59808.
I0304 19:28:53.158126 22849303695488 run.py:483] Algo bellman_ford step 1869 current loss 0.135478, current_train_items 59840.
I0304 19:28:53.177731 22849303695488 run.py:483] Algo bellman_ford step 1870 current loss 0.007404, current_train_items 59872.
I0304 19:28:53.194189 22849303695488 run.py:483] Algo bellman_ford step 1871 current loss 0.046949, current_train_items 59904.
I0304 19:28:53.217591 22849303695488 run.py:483] Algo bellman_ford step 1872 current loss 0.060399, current_train_items 59936.
I0304 19:28:53.248227 22849303695488 run.py:483] Algo bellman_ford step 1873 current loss 0.166762, current_train_items 59968.
I0304 19:28:53.280741 22849303695488 run.py:483] Algo bellman_ford step 1874 current loss 0.132571, current_train_items 60000.
I0304 19:28:53.300785 22849303695488 run.py:483] Algo bellman_ford step 1875 current loss 0.030099, current_train_items 60032.
I0304 19:28:53.317300 22849303695488 run.py:483] Algo bellman_ford step 1876 current loss 0.024267, current_train_items 60064.
I0304 19:28:53.341266 22849303695488 run.py:483] Algo bellman_ford step 1877 current loss 0.136416, current_train_items 60096.
I0304 19:28:53.370358 22849303695488 run.py:483] Algo bellman_ford step 1878 current loss 0.108073, current_train_items 60128.
I0304 19:28:53.405884 22849303695488 run.py:483] Algo bellman_ford step 1879 current loss 0.255130, current_train_items 60160.
I0304 19:28:53.425563 22849303695488 run.py:483] Algo bellman_ford step 1880 current loss 0.008192, current_train_items 60192.
I0304 19:28:53.442369 22849303695488 run.py:483] Algo bellman_ford step 1881 current loss 0.037358, current_train_items 60224.
I0304 19:28:53.465894 22849303695488 run.py:483] Algo bellman_ford step 1882 current loss 0.118709, current_train_items 60256.
I0304 19:28:53.495950 22849303695488 run.py:483] Algo bellman_ford step 1883 current loss 0.180628, current_train_items 60288.
I0304 19:28:53.529256 22849303695488 run.py:483] Algo bellman_ford step 1884 current loss 0.169465, current_train_items 60320.
I0304 19:28:53.549171 22849303695488 run.py:483] Algo bellman_ford step 1885 current loss 0.019559, current_train_items 60352.
I0304 19:28:53.566185 22849303695488 run.py:483] Algo bellman_ford step 1886 current loss 0.042000, current_train_items 60384.
I0304 19:28:53.590124 22849303695488 run.py:483] Algo bellman_ford step 1887 current loss 0.177452, current_train_items 60416.
I0304 19:28:53.618952 22849303695488 run.py:483] Algo bellman_ford step 1888 current loss 0.117227, current_train_items 60448.
I0304 19:28:53.653490 22849303695488 run.py:483] Algo bellman_ford step 1889 current loss 0.244391, current_train_items 60480.
I0304 19:28:53.673234 22849303695488 run.py:483] Algo bellman_ford step 1890 current loss 0.005253, current_train_items 60512.
I0304 19:28:53.690119 22849303695488 run.py:483] Algo bellman_ford step 1891 current loss 0.028281, current_train_items 60544.
I0304 19:28:53.713241 22849303695488 run.py:483] Algo bellman_ford step 1892 current loss 0.077179, current_train_items 60576.
I0304 19:28:53.743916 22849303695488 run.py:483] Algo bellman_ford step 1893 current loss 0.129976, current_train_items 60608.
I0304 19:28:53.779443 22849303695488 run.py:483] Algo bellman_ford step 1894 current loss 0.158525, current_train_items 60640.
I0304 19:28:53.799288 22849303695488 run.py:483] Algo bellman_ford step 1895 current loss 0.014440, current_train_items 60672.
I0304 19:28:53.815811 22849303695488 run.py:483] Algo bellman_ford step 1896 current loss 0.019764, current_train_items 60704.
I0304 19:28:53.840128 22849303695488 run.py:483] Algo bellman_ford step 1897 current loss 0.116667, current_train_items 60736.
I0304 19:28:53.870527 22849303695488 run.py:483] Algo bellman_ford step 1898 current loss 0.143051, current_train_items 60768.
I0304 19:28:53.903735 22849303695488 run.py:483] Algo bellman_ford step 1899 current loss 0.164622, current_train_items 60800.
I0304 19:28:53.924104 22849303695488 run.py:483] Algo bellman_ford step 1900 current loss 0.012028, current_train_items 60832.
I0304 19:28:53.932232 22849303695488 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.9404296875, 'score': 0.9404296875, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0304 19:28:53.932339 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.940, val scores are: bellman_ford: 0.940
I0304 19:28:53.949248 22849303695488 run.py:483] Algo bellman_ford step 1901 current loss 0.044444, current_train_items 60864.
I0304 19:28:53.973143 22849303695488 run.py:483] Algo bellman_ford step 1902 current loss 0.126571, current_train_items 60896.
I0304 19:28:54.001737 22849303695488 run.py:483] Algo bellman_ford step 1903 current loss 0.146066, current_train_items 60928.
I0304 19:28:54.034524 22849303695488 run.py:483] Algo bellman_ford step 1904 current loss 0.248125, current_train_items 60960.
I0304 19:28:54.054132 22849303695488 run.py:483] Algo bellman_ford step 1905 current loss 0.010634, current_train_items 60992.
I0304 19:28:54.070463 22849303695488 run.py:483] Algo bellman_ford step 1906 current loss 0.040471, current_train_items 61024.
I0304 19:28:54.093612 22849303695488 run.py:483] Algo bellman_ford step 1907 current loss 0.163155, current_train_items 61056.
I0304 19:28:54.122980 22849303695488 run.py:483] Algo bellman_ford step 1908 current loss 0.196670, current_train_items 61088.
I0304 19:28:54.155123 22849303695488 run.py:483] Algo bellman_ford step 1909 current loss 0.155310, current_train_items 61120.
I0304 19:28:54.174617 22849303695488 run.py:483] Algo bellman_ford step 1910 current loss 0.011542, current_train_items 61152.
I0304 19:28:54.191351 22849303695488 run.py:483] Algo bellman_ford step 1911 current loss 0.029550, current_train_items 61184.
I0304 19:28:54.216078 22849303695488 run.py:483] Algo bellman_ford step 1912 current loss 0.114342, current_train_items 61216.
I0304 19:28:54.245796 22849303695488 run.py:483] Algo bellman_ford step 1913 current loss 0.088639, current_train_items 61248.
I0304 19:28:54.278486 22849303695488 run.py:483] Algo bellman_ford step 1914 current loss 0.120174, current_train_items 61280.
I0304 19:28:54.297667 22849303695488 run.py:483] Algo bellman_ford step 1915 current loss 0.009486, current_train_items 61312.
I0304 19:28:54.314402 22849303695488 run.py:483] Algo bellman_ford step 1916 current loss 0.019467, current_train_items 61344.
I0304 19:28:54.338311 22849303695488 run.py:483] Algo bellman_ford step 1917 current loss 0.068188, current_train_items 61376.
I0304 19:28:54.368533 22849303695488 run.py:483] Algo bellman_ford step 1918 current loss 0.095250, current_train_items 61408.
I0304 19:28:54.402591 22849303695488 run.py:483] Algo bellman_ford step 1919 current loss 0.165591, current_train_items 61440.
I0304 19:28:54.421904 22849303695488 run.py:483] Algo bellman_ford step 1920 current loss 0.013208, current_train_items 61472.
I0304 19:28:54.438595 22849303695488 run.py:483] Algo bellman_ford step 1921 current loss 0.062826, current_train_items 61504.
I0304 19:28:54.461909 22849303695488 run.py:483] Algo bellman_ford step 1922 current loss 0.116016, current_train_items 61536.
I0304 19:28:54.491081 22849303695488 run.py:483] Algo bellman_ford step 1923 current loss 0.131567, current_train_items 61568.
I0304 19:28:54.524013 22849303695488 run.py:483] Algo bellman_ford step 1924 current loss 0.115586, current_train_items 61600.
I0304 19:28:54.543390 22849303695488 run.py:483] Algo bellman_ford step 1925 current loss 0.010912, current_train_items 61632.
I0304 19:28:54.559930 22849303695488 run.py:483] Algo bellman_ford step 1926 current loss 0.096748, current_train_items 61664.
I0304 19:28:54.582439 22849303695488 run.py:483] Algo bellman_ford step 1927 current loss 0.055376, current_train_items 61696.
I0304 19:28:54.611697 22849303695488 run.py:483] Algo bellman_ford step 1928 current loss 0.060848, current_train_items 61728.
I0304 19:28:54.642985 22849303695488 run.py:483] Algo bellman_ford step 1929 current loss 0.136001, current_train_items 61760.
I0304 19:28:54.662189 22849303695488 run.py:483] Algo bellman_ford step 1930 current loss 0.015521, current_train_items 61792.
I0304 19:28:54.678168 22849303695488 run.py:483] Algo bellman_ford step 1931 current loss 0.072936, current_train_items 61824.
I0304 19:28:54.702818 22849303695488 run.py:483] Algo bellman_ford step 1932 current loss 0.079961, current_train_items 61856.
I0304 19:28:54.732621 22849303695488 run.py:483] Algo bellman_ford step 1933 current loss 0.133642, current_train_items 61888.
I0304 19:28:54.763788 22849303695488 run.py:483] Algo bellman_ford step 1934 current loss 0.101842, current_train_items 61920.
I0304 19:28:54.782909 22849303695488 run.py:483] Algo bellman_ford step 1935 current loss 0.027634, current_train_items 61952.
I0304 19:28:54.799430 22849303695488 run.py:483] Algo bellman_ford step 1936 current loss 0.049131, current_train_items 61984.
I0304 19:28:54.824038 22849303695488 run.py:483] Algo bellman_ford step 1937 current loss 0.119316, current_train_items 62016.
I0304 19:28:54.853071 22849303695488 run.py:483] Algo bellman_ford step 1938 current loss 0.187830, current_train_items 62048.
I0304 19:28:54.886345 22849303695488 run.py:483] Algo bellman_ford step 1939 current loss 0.223134, current_train_items 62080.
I0304 19:28:54.905711 22849303695488 run.py:483] Algo bellman_ford step 1940 current loss 0.016942, current_train_items 62112.
I0304 19:28:54.922486 22849303695488 run.py:483] Algo bellman_ford step 1941 current loss 0.044967, current_train_items 62144.
I0304 19:28:54.945897 22849303695488 run.py:483] Algo bellman_ford step 1942 current loss 0.126478, current_train_items 62176.
I0304 19:28:54.974670 22849303695488 run.py:483] Algo bellman_ford step 1943 current loss 0.094309, current_train_items 62208.
I0304 19:28:55.007441 22849303695488 run.py:483] Algo bellman_ford step 1944 current loss 0.182169, current_train_items 62240.
I0304 19:28:55.026938 22849303695488 run.py:483] Algo bellman_ford step 1945 current loss 0.021690, current_train_items 62272.
I0304 19:28:55.043478 22849303695488 run.py:483] Algo bellman_ford step 1946 current loss 0.061133, current_train_items 62304.
I0304 19:28:55.067502 22849303695488 run.py:483] Algo bellman_ford step 1947 current loss 0.079308, current_train_items 62336.
I0304 19:28:55.098116 22849303695488 run.py:483] Algo bellman_ford step 1948 current loss 0.151355, current_train_items 62368.
I0304 19:28:55.129992 22849303695488 run.py:483] Algo bellman_ford step 1949 current loss 0.136883, current_train_items 62400.
I0304 19:28:55.149488 22849303695488 run.py:483] Algo bellman_ford step 1950 current loss 0.031851, current_train_items 62432.
I0304 19:28:55.157775 22849303695488 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0304 19:28:55.157883 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:28:55.175299 22849303695488 run.py:483] Algo bellman_ford step 1951 current loss 0.027804, current_train_items 62464.
I0304 19:28:55.199784 22849303695488 run.py:483] Algo bellman_ford step 1952 current loss 0.180526, current_train_items 62496.
I0304 19:28:55.231697 22849303695488 run.py:483] Algo bellman_ford step 1953 current loss 0.162980, current_train_items 62528.
I0304 19:28:55.264921 22849303695488 run.py:483] Algo bellman_ford step 1954 current loss 0.179068, current_train_items 62560.
I0304 19:28:55.284749 22849303695488 run.py:483] Algo bellman_ford step 1955 current loss 0.026676, current_train_items 62592.
I0304 19:28:55.300400 22849303695488 run.py:483] Algo bellman_ford step 1956 current loss 0.059460, current_train_items 62624.
I0304 19:28:55.324821 22849303695488 run.py:483] Algo bellman_ford step 1957 current loss 0.109226, current_train_items 62656.
I0304 19:28:55.355800 22849303695488 run.py:483] Algo bellman_ford step 1958 current loss 0.103828, current_train_items 62688.
I0304 19:28:55.389470 22849303695488 run.py:483] Algo bellman_ford step 1959 current loss 0.108868, current_train_items 62720.
I0304 19:28:55.409675 22849303695488 run.py:483] Algo bellman_ford step 1960 current loss 0.014485, current_train_items 62752.
I0304 19:28:55.426386 22849303695488 run.py:483] Algo bellman_ford step 1961 current loss 0.068011, current_train_items 62784.
I0304 19:28:55.449760 22849303695488 run.py:483] Algo bellman_ford step 1962 current loss 0.116338, current_train_items 62816.
I0304 19:28:55.478981 22849303695488 run.py:483] Algo bellman_ford step 1963 current loss 0.175114, current_train_items 62848.
I0304 19:28:55.514725 22849303695488 run.py:483] Algo bellman_ford step 1964 current loss 0.198352, current_train_items 62880.
I0304 19:28:55.534693 22849303695488 run.py:483] Algo bellman_ford step 1965 current loss 0.013164, current_train_items 62912.
I0304 19:28:55.551198 22849303695488 run.py:483] Algo bellman_ford step 1966 current loss 0.083015, current_train_items 62944.
I0304 19:28:55.575125 22849303695488 run.py:483] Algo bellman_ford step 1967 current loss 0.219180, current_train_items 62976.
I0304 19:28:55.604749 22849303695488 run.py:483] Algo bellman_ford step 1968 current loss 0.132976, current_train_items 63008.
I0304 19:28:55.637215 22849303695488 run.py:483] Algo bellman_ford step 1969 current loss 0.180466, current_train_items 63040.
I0304 19:28:55.657054 22849303695488 run.py:483] Algo bellman_ford step 1970 current loss 0.011586, current_train_items 63072.
I0304 19:28:55.673436 22849303695488 run.py:483] Algo bellman_ford step 1971 current loss 0.072407, current_train_items 63104.
I0304 19:28:55.696478 22849303695488 run.py:483] Algo bellman_ford step 1972 current loss 0.107521, current_train_items 63136.
I0304 19:28:55.726603 22849303695488 run.py:483] Algo bellman_ford step 1973 current loss 0.134844, current_train_items 63168.
I0304 19:28:55.762307 22849303695488 run.py:483] Algo bellman_ford step 1974 current loss 0.213571, current_train_items 63200.
I0304 19:28:55.782246 22849303695488 run.py:483] Algo bellman_ford step 1975 current loss 0.011326, current_train_items 63232.
I0304 19:28:55.798599 22849303695488 run.py:483] Algo bellman_ford step 1976 current loss 0.092085, current_train_items 63264.
I0304 19:28:55.821083 22849303695488 run.py:483] Algo bellman_ford step 1977 current loss 0.127740, current_train_items 63296.
I0304 19:28:55.850592 22849303695488 run.py:483] Algo bellman_ford step 1978 current loss 0.142712, current_train_items 63328.
I0304 19:28:55.884298 22849303695488 run.py:483] Algo bellman_ford step 1979 current loss 0.241109, current_train_items 63360.
I0304 19:28:55.903658 22849303695488 run.py:483] Algo bellman_ford step 1980 current loss 0.011365, current_train_items 63392.
I0304 19:28:55.920225 22849303695488 run.py:483] Algo bellman_ford step 1981 current loss 0.037665, current_train_items 63424.
I0304 19:28:55.943775 22849303695488 run.py:483] Algo bellman_ford step 1982 current loss 0.121676, current_train_items 63456.
I0304 19:28:55.972974 22849303695488 run.py:483] Algo bellman_ford step 1983 current loss 0.099247, current_train_items 63488.
I0304 19:28:56.008742 22849303695488 run.py:483] Algo bellman_ford step 1984 current loss 0.192893, current_train_items 63520.
I0304 19:28:56.028859 22849303695488 run.py:483] Algo bellman_ford step 1985 current loss 0.013021, current_train_items 63552.
I0304 19:28:56.045513 22849303695488 run.py:483] Algo bellman_ford step 1986 current loss 0.076932, current_train_items 63584.
I0304 19:28:56.068075 22849303695488 run.py:483] Algo bellman_ford step 1987 current loss 0.141786, current_train_items 63616.
I0304 19:28:56.097231 22849303695488 run.py:483] Algo bellman_ford step 1988 current loss 0.125444, current_train_items 63648.
I0304 19:28:56.131091 22849303695488 run.py:483] Algo bellman_ford step 1989 current loss 0.189173, current_train_items 63680.
I0304 19:28:56.151052 22849303695488 run.py:483] Algo bellman_ford step 1990 current loss 0.012134, current_train_items 63712.
I0304 19:28:56.167621 22849303695488 run.py:483] Algo bellman_ford step 1991 current loss 0.091757, current_train_items 63744.
I0304 19:28:56.191703 22849303695488 run.py:483] Algo bellman_ford step 1992 current loss 0.157257, current_train_items 63776.
I0304 19:28:56.222158 22849303695488 run.py:483] Algo bellman_ford step 1993 current loss 0.179990, current_train_items 63808.
I0304 19:28:56.251246 22849303695488 run.py:483] Algo bellman_ford step 1994 current loss 0.121295, current_train_items 63840.
I0304 19:28:56.271125 22849303695488 run.py:483] Algo bellman_ford step 1995 current loss 0.033081, current_train_items 63872.
I0304 19:28:56.288291 22849303695488 run.py:483] Algo bellman_ford step 1996 current loss 0.043478, current_train_items 63904.
I0304 19:28:56.311851 22849303695488 run.py:483] Algo bellman_ford step 1997 current loss 0.060629, current_train_items 63936.
I0304 19:28:56.343194 22849303695488 run.py:483] Algo bellman_ford step 1998 current loss 0.175004, current_train_items 63968.
I0304 19:28:56.375534 22849303695488 run.py:483] Algo bellman_ford step 1999 current loss 0.176673, current_train_items 64000.
I0304 19:28:56.395587 22849303695488 run.py:483] Algo bellman_ford step 2000 current loss 0.012923, current_train_items 64032.
I0304 19:28:56.403995 22849303695488 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.9462890625, 'score': 0.9462890625, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0304 19:28:56.404109 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.946, val scores are: bellman_ford: 0.946
I0304 19:28:56.421031 22849303695488 run.py:483] Algo bellman_ford step 2001 current loss 0.046783, current_train_items 64064.
I0304 19:28:56.445584 22849303695488 run.py:483] Algo bellman_ford step 2002 current loss 0.227603, current_train_items 64096.
I0304 19:28:56.476114 22849303695488 run.py:483] Algo bellman_ford step 2003 current loss 0.225545, current_train_items 64128.
I0304 19:28:56.508633 22849303695488 run.py:483] Algo bellman_ford step 2004 current loss 0.111618, current_train_items 64160.
I0304 19:28:56.528046 22849303695488 run.py:483] Algo bellman_ford step 2005 current loss 0.007992, current_train_items 64192.
I0304 19:28:56.544528 22849303695488 run.py:483] Algo bellman_ford step 2006 current loss 0.043912, current_train_items 64224.
I0304 19:28:56.568057 22849303695488 run.py:483] Algo bellman_ford step 2007 current loss 0.060795, current_train_items 64256.
I0304 19:28:56.598879 22849303695488 run.py:483] Algo bellman_ford step 2008 current loss 0.230880, current_train_items 64288.
I0304 19:28:56.631233 22849303695488 run.py:483] Algo bellman_ford step 2009 current loss 0.145041, current_train_items 64320.
I0304 19:28:56.650689 22849303695488 run.py:483] Algo bellman_ford step 2010 current loss 0.022671, current_train_items 64352.
I0304 19:28:56.667219 22849303695488 run.py:483] Algo bellman_ford step 2011 current loss 0.036358, current_train_items 64384.
I0304 19:28:56.691105 22849303695488 run.py:483] Algo bellman_ford step 2012 current loss 0.138387, current_train_items 64416.
I0304 19:28:56.720238 22849303695488 run.py:483] Algo bellman_ford step 2013 current loss 0.141937, current_train_items 64448.
I0304 19:28:56.752758 22849303695488 run.py:483] Algo bellman_ford step 2014 current loss 0.118237, current_train_items 64480.
I0304 19:28:56.771848 22849303695488 run.py:483] Algo bellman_ford step 2015 current loss 0.015766, current_train_items 64512.
I0304 19:28:56.788852 22849303695488 run.py:483] Algo bellman_ford step 2016 current loss 0.061862, current_train_items 64544.
I0304 19:28:56.812913 22849303695488 run.py:483] Algo bellman_ford step 2017 current loss 0.180408, current_train_items 64576.
I0304 19:28:56.840995 22849303695488 run.py:483] Algo bellman_ford step 2018 current loss 0.115544, current_train_items 64608.
I0304 19:28:56.875077 22849303695488 run.py:483] Algo bellman_ford step 2019 current loss 0.168681, current_train_items 64640.
I0304 19:28:56.894370 22849303695488 run.py:483] Algo bellman_ford step 2020 current loss 0.014327, current_train_items 64672.
I0304 19:28:56.910901 22849303695488 run.py:483] Algo bellman_ford step 2021 current loss 0.024901, current_train_items 64704.
I0304 19:28:56.934800 22849303695488 run.py:483] Algo bellman_ford step 2022 current loss 0.169933, current_train_items 64736.
I0304 19:28:56.964804 22849303695488 run.py:483] Algo bellman_ford step 2023 current loss 0.148990, current_train_items 64768.
I0304 19:28:57.000126 22849303695488 run.py:483] Algo bellman_ford step 2024 current loss 0.166042, current_train_items 64800.
I0304 19:28:57.019756 22849303695488 run.py:483] Algo bellman_ford step 2025 current loss 0.011357, current_train_items 64832.
I0304 19:28:57.036000 22849303695488 run.py:483] Algo bellman_ford step 2026 current loss 0.055391, current_train_items 64864.
I0304 19:28:57.059928 22849303695488 run.py:483] Algo bellman_ford step 2027 current loss 0.158122, current_train_items 64896.
I0304 19:28:57.089495 22849303695488 run.py:483] Algo bellman_ford step 2028 current loss 0.124338, current_train_items 64928.
I0304 19:28:57.121817 22849303695488 run.py:483] Algo bellman_ford step 2029 current loss 0.145270, current_train_items 64960.
I0304 19:28:57.141422 22849303695488 run.py:483] Algo bellman_ford step 2030 current loss 0.022708, current_train_items 64992.
I0304 19:28:57.158246 22849303695488 run.py:483] Algo bellman_ford step 2031 current loss 0.081566, current_train_items 65024.
I0304 19:28:57.181201 22849303695488 run.py:483] Algo bellman_ford step 2032 current loss 0.078272, current_train_items 65056.
I0304 19:28:57.212291 22849303695488 run.py:483] Algo bellman_ford step 2033 current loss 0.135776, current_train_items 65088.
I0304 19:28:57.244108 22849303695488 run.py:483] Algo bellman_ford step 2034 current loss 0.127927, current_train_items 65120.
I0304 19:28:57.263405 22849303695488 run.py:483] Algo bellman_ford step 2035 current loss 0.012686, current_train_items 65152.
I0304 19:28:57.279538 22849303695488 run.py:483] Algo bellman_ford step 2036 current loss 0.017330, current_train_items 65184.
I0304 19:28:57.302555 22849303695488 run.py:483] Algo bellman_ford step 2037 current loss 0.114721, current_train_items 65216.
I0304 19:28:57.332517 22849303695488 run.py:483] Algo bellman_ford step 2038 current loss 0.150963, current_train_items 65248.
I0304 19:28:57.365434 22849303695488 run.py:483] Algo bellman_ford step 2039 current loss 0.122455, current_train_items 65280.
I0304 19:28:57.384740 22849303695488 run.py:483] Algo bellman_ford step 2040 current loss 0.025303, current_train_items 65312.
I0304 19:28:57.400889 22849303695488 run.py:483] Algo bellman_ford step 2041 current loss 0.096399, current_train_items 65344.
I0304 19:28:57.425436 22849303695488 run.py:483] Algo bellman_ford step 2042 current loss 0.211773, current_train_items 65376.
I0304 19:28:57.455591 22849303695488 run.py:483] Algo bellman_ford step 2043 current loss 0.195462, current_train_items 65408.
I0304 19:28:57.483055 22849303695488 run.py:483] Algo bellman_ford step 2044 current loss 0.121856, current_train_items 65440.
I0304 19:28:57.502348 22849303695488 run.py:483] Algo bellman_ford step 2045 current loss 0.007930, current_train_items 65472.
I0304 19:28:57.519341 22849303695488 run.py:483] Algo bellman_ford step 2046 current loss 0.047760, current_train_items 65504.
I0304 19:28:57.543116 22849303695488 run.py:483] Algo bellman_ford step 2047 current loss 0.156230, current_train_items 65536.
I0304 19:28:57.572873 22849303695488 run.py:483] Algo bellman_ford step 2048 current loss 0.098525, current_train_items 65568.
I0304 19:28:57.603296 22849303695488 run.py:483] Algo bellman_ford step 2049 current loss 0.094469, current_train_items 65600.
I0304 19:28:57.622799 22849303695488 run.py:483] Algo bellman_ford step 2050 current loss 0.018524, current_train_items 65632.
I0304 19:28:57.631255 22849303695488 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0304 19:28:57.631397 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:28:57.649049 22849303695488 run.py:483] Algo bellman_ford step 2051 current loss 0.032795, current_train_items 65664.
I0304 19:28:57.672943 22849303695488 run.py:483] Algo bellman_ford step 2052 current loss 0.058325, current_train_items 65696.
I0304 19:28:57.702949 22849303695488 run.py:483] Algo bellman_ford step 2053 current loss 0.128789, current_train_items 65728.
I0304 19:28:57.738801 22849303695488 run.py:483] Algo bellman_ford step 2054 current loss 0.197753, current_train_items 65760.
I0304 19:28:57.758995 22849303695488 run.py:483] Algo bellman_ford step 2055 current loss 0.007147, current_train_items 65792.
I0304 19:28:57.775103 22849303695488 run.py:483] Algo bellman_ford step 2056 current loss 0.026052, current_train_items 65824.
I0304 19:28:57.798326 22849303695488 run.py:483] Algo bellman_ford step 2057 current loss 0.059132, current_train_items 65856.
I0304 19:28:57.828220 22849303695488 run.py:483] Algo bellman_ford step 2058 current loss 0.067978, current_train_items 65888.
I0304 19:28:57.862023 22849303695488 run.py:483] Algo bellman_ford step 2059 current loss 0.145344, current_train_items 65920.
I0304 19:28:57.881824 22849303695488 run.py:483] Algo bellman_ford step 2060 current loss 0.015670, current_train_items 65952.
I0304 19:28:57.898508 22849303695488 run.py:483] Algo bellman_ford step 2061 current loss 0.035970, current_train_items 65984.
I0304 19:28:57.922028 22849303695488 run.py:483] Algo bellman_ford step 2062 current loss 0.074090, current_train_items 66016.
I0304 19:28:57.952438 22849303695488 run.py:483] Algo bellman_ford step 2063 current loss 0.110623, current_train_items 66048.
I0304 19:28:57.987020 22849303695488 run.py:483] Algo bellman_ford step 2064 current loss 0.158969, current_train_items 66080.
I0304 19:28:58.006431 22849303695488 run.py:483] Algo bellman_ford step 2065 current loss 0.011934, current_train_items 66112.
I0304 19:28:58.022952 22849303695488 run.py:483] Algo bellman_ford step 2066 current loss 0.038064, current_train_items 66144.
I0304 19:28:58.047194 22849303695488 run.py:483] Algo bellman_ford step 2067 current loss 0.076821, current_train_items 66176.
I0304 19:28:58.077961 22849303695488 run.py:483] Algo bellman_ford step 2068 current loss 0.101402, current_train_items 66208.
I0304 19:28:58.110840 22849303695488 run.py:483] Algo bellman_ford step 2069 current loss 0.096201, current_train_items 66240.
I0304 19:28:58.130770 22849303695488 run.py:483] Algo bellman_ford step 2070 current loss 0.007826, current_train_items 66272.
I0304 19:28:58.147711 22849303695488 run.py:483] Algo bellman_ford step 2071 current loss 0.028078, current_train_items 66304.
I0304 19:28:58.171544 22849303695488 run.py:483] Algo bellman_ford step 2072 current loss 0.098655, current_train_items 66336.
I0304 19:28:58.201889 22849303695488 run.py:483] Algo bellman_ford step 2073 current loss 0.157812, current_train_items 66368.
I0304 19:28:58.235112 22849303695488 run.py:483] Algo bellman_ford step 2074 current loss 0.190612, current_train_items 66400.
I0304 19:28:58.254930 22849303695488 run.py:483] Algo bellman_ford step 2075 current loss 0.041471, current_train_items 66432.
I0304 19:28:58.271943 22849303695488 run.py:483] Algo bellman_ford step 2076 current loss 0.057030, current_train_items 66464.
I0304 19:28:58.296195 22849303695488 run.py:483] Algo bellman_ford step 2077 current loss 0.097236, current_train_items 66496.
I0304 19:28:58.324486 22849303695488 run.py:483] Algo bellman_ford step 2078 current loss 0.066550, current_train_items 66528.
I0304 19:28:58.356666 22849303695488 run.py:483] Algo bellman_ford step 2079 current loss 0.101082, current_train_items 66560.
I0304 19:28:58.376097 22849303695488 run.py:483] Algo bellman_ford step 2080 current loss 0.008631, current_train_items 66592.
I0304 19:28:58.392726 22849303695488 run.py:483] Algo bellman_ford step 2081 current loss 0.035483, current_train_items 66624.
I0304 19:28:58.416659 22849303695488 run.py:483] Algo bellman_ford step 2082 current loss 0.071434, current_train_items 66656.
I0304 19:28:58.448250 22849303695488 run.py:483] Algo bellman_ford step 2083 current loss 0.109499, current_train_items 66688.
I0304 19:28:58.481995 22849303695488 run.py:483] Algo bellman_ford step 2084 current loss 0.117108, current_train_items 66720.
I0304 19:28:58.501983 22849303695488 run.py:483] Algo bellman_ford step 2085 current loss 0.009262, current_train_items 66752.
I0304 19:28:58.518599 22849303695488 run.py:483] Algo bellman_ford step 2086 current loss 0.080115, current_train_items 66784.
I0304 19:28:58.542619 22849303695488 run.py:483] Algo bellman_ford step 2087 current loss 0.078633, current_train_items 66816.
I0304 19:28:58.570947 22849303695488 run.py:483] Algo bellman_ford step 2088 current loss 0.053136, current_train_items 66848.
I0304 19:28:58.604302 22849303695488 run.py:483] Algo bellman_ford step 2089 current loss 0.185701, current_train_items 66880.
I0304 19:28:58.624087 22849303695488 run.py:483] Algo bellman_ford step 2090 current loss 0.012156, current_train_items 66912.
I0304 19:28:58.640906 22849303695488 run.py:483] Algo bellman_ford step 2091 current loss 0.042846, current_train_items 66944.
I0304 19:28:58.664224 22849303695488 run.py:483] Algo bellman_ford step 2092 current loss 0.091476, current_train_items 66976.
I0304 19:28:58.694257 22849303695488 run.py:483] Algo bellman_ford step 2093 current loss 0.094285, current_train_items 67008.
I0304 19:28:58.727240 22849303695488 run.py:483] Algo bellman_ford step 2094 current loss 0.137770, current_train_items 67040.
I0304 19:28:58.746895 22849303695488 run.py:483] Algo bellman_ford step 2095 current loss 0.018135, current_train_items 67072.
I0304 19:28:58.763475 22849303695488 run.py:483] Algo bellman_ford step 2096 current loss 0.041217, current_train_items 67104.
I0304 19:28:58.787103 22849303695488 run.py:483] Algo bellman_ford step 2097 current loss 0.116734, current_train_items 67136.
I0304 19:28:58.816661 22849303695488 run.py:483] Algo bellman_ford step 2098 current loss 0.119961, current_train_items 67168.
I0304 19:28:58.849333 22849303695488 run.py:483] Algo bellman_ford step 2099 current loss 0.171907, current_train_items 67200.
I0304 19:28:58.869545 22849303695488 run.py:483] Algo bellman_ford step 2100 current loss 0.009520, current_train_items 67232.
I0304 19:28:58.877471 22849303695488 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0304 19:28:58.877578 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:28:58.894443 22849303695488 run.py:483] Algo bellman_ford step 2101 current loss 0.122347, current_train_items 67264.
I0304 19:28:58.918177 22849303695488 run.py:483] Algo bellman_ford step 2102 current loss 0.212564, current_train_items 67296.
I0304 19:28:58.948436 22849303695488 run.py:483] Algo bellman_ford step 2103 current loss 0.123031, current_train_items 67328.
I0304 19:28:58.980823 22849303695488 run.py:483] Algo bellman_ford step 2104 current loss 0.143560, current_train_items 67360.
I0304 19:28:59.000855 22849303695488 run.py:483] Algo bellman_ford step 2105 current loss 0.013383, current_train_items 67392.
I0304 19:28:59.016714 22849303695488 run.py:483] Algo bellman_ford step 2106 current loss 0.013100, current_train_items 67424.
I0304 19:28:59.040965 22849303695488 run.py:483] Algo bellman_ford step 2107 current loss 0.135545, current_train_items 67456.
I0304 19:28:59.072235 22849303695488 run.py:483] Algo bellman_ford step 2108 current loss 0.241345, current_train_items 67488.
I0304 19:28:59.104115 22849303695488 run.py:483] Algo bellman_ford step 2109 current loss 0.121491, current_train_items 67520.
I0304 19:28:59.123764 22849303695488 run.py:483] Algo bellman_ford step 2110 current loss 0.023113, current_train_items 67552.
I0304 19:28:59.140433 22849303695488 run.py:483] Algo bellman_ford step 2111 current loss 0.042597, current_train_items 67584.
I0304 19:28:59.163394 22849303695488 run.py:483] Algo bellman_ford step 2112 current loss 0.129484, current_train_items 67616.
I0304 19:28:59.192209 22849303695488 run.py:483] Algo bellman_ford step 2113 current loss 0.154746, current_train_items 67648.
I0304 19:28:59.223575 22849303695488 run.py:483] Algo bellman_ford step 2114 current loss 0.153900, current_train_items 67680.
I0304 19:28:59.242825 22849303695488 run.py:483] Algo bellman_ford step 2115 current loss 0.030959, current_train_items 67712.
I0304 19:28:59.259349 22849303695488 run.py:483] Algo bellman_ford step 2116 current loss 0.053729, current_train_items 67744.
I0304 19:28:59.283858 22849303695488 run.py:483] Algo bellman_ford step 2117 current loss 0.100758, current_train_items 67776.
I0304 19:28:59.314107 22849303695488 run.py:483] Algo bellman_ford step 2118 current loss 0.116799, current_train_items 67808.
I0304 19:28:59.346018 22849303695488 run.py:483] Algo bellman_ford step 2119 current loss 0.092323, current_train_items 67840.
I0304 19:28:59.365570 22849303695488 run.py:483] Algo bellman_ford step 2120 current loss 0.012046, current_train_items 67872.
I0304 19:28:59.381855 22849303695488 run.py:483] Algo bellman_ford step 2121 current loss 0.041512, current_train_items 67904.
I0304 19:28:59.405194 22849303695488 run.py:483] Algo bellman_ford step 2122 current loss 0.077021, current_train_items 67936.
I0304 19:28:59.435433 22849303695488 run.py:483] Algo bellman_ford step 2123 current loss 0.127020, current_train_items 67968.
I0304 19:28:59.464494 22849303695488 run.py:483] Algo bellman_ford step 2124 current loss 0.096981, current_train_items 68000.
I0304 19:28:59.483649 22849303695488 run.py:483] Algo bellman_ford step 2125 current loss 0.007430, current_train_items 68032.
I0304 19:28:59.500451 22849303695488 run.py:483] Algo bellman_ford step 2126 current loss 0.124900, current_train_items 68064.
I0304 19:28:59.525759 22849303695488 run.py:483] Algo bellman_ford step 2127 current loss 0.105137, current_train_items 68096.
I0304 19:28:59.555053 22849303695488 run.py:483] Algo bellman_ford step 2128 current loss 0.153457, current_train_items 68128.
I0304 19:28:59.586974 22849303695488 run.py:483] Algo bellman_ford step 2129 current loss 0.095375, current_train_items 68160.
I0304 19:28:59.606251 22849303695488 run.py:483] Algo bellman_ford step 2130 current loss 0.009628, current_train_items 68192.
I0304 19:28:59.623357 22849303695488 run.py:483] Algo bellman_ford step 2131 current loss 0.055257, current_train_items 68224.
I0304 19:28:59.647487 22849303695488 run.py:483] Algo bellman_ford step 2132 current loss 0.141779, current_train_items 68256.
I0304 19:28:59.676744 22849303695488 run.py:483] Algo bellman_ford step 2133 current loss 0.153951, current_train_items 68288.
I0304 19:28:59.710518 22849303695488 run.py:483] Algo bellman_ford step 2134 current loss 0.132669, current_train_items 68320.
I0304 19:28:59.729803 22849303695488 run.py:483] Algo bellman_ford step 2135 current loss 0.015802, current_train_items 68352.
I0304 19:28:59.746423 22849303695488 run.py:483] Algo bellman_ford step 2136 current loss 0.070076, current_train_items 68384.
I0304 19:28:59.769895 22849303695488 run.py:483] Algo bellman_ford step 2137 current loss 0.115958, current_train_items 68416.
I0304 19:28:59.800022 22849303695488 run.py:483] Algo bellman_ford step 2138 current loss 0.155483, current_train_items 68448.
I0304 19:28:59.835434 22849303695488 run.py:483] Algo bellman_ford step 2139 current loss 0.164882, current_train_items 68480.
I0304 19:28:59.854726 22849303695488 run.py:483] Algo bellman_ford step 2140 current loss 0.080444, current_train_items 68512.
I0304 19:28:59.871114 22849303695488 run.py:483] Algo bellman_ford step 2141 current loss 0.090200, current_train_items 68544.
I0304 19:28:59.893690 22849303695488 run.py:483] Algo bellman_ford step 2142 current loss 0.073826, current_train_items 68576.
I0304 19:28:59.923106 22849303695488 run.py:483] Algo bellman_ford step 2143 current loss 0.205132, current_train_items 68608.
I0304 19:28:59.955882 22849303695488 run.py:483] Algo bellman_ford step 2144 current loss 0.258819, current_train_items 68640.
I0304 19:28:59.974995 22849303695488 run.py:483] Algo bellman_ford step 2145 current loss 0.034414, current_train_items 68672.
I0304 19:28:59.991887 22849303695488 run.py:483] Algo bellman_ford step 2146 current loss 0.096155, current_train_items 68704.
I0304 19:29:00.016022 22849303695488 run.py:483] Algo bellman_ford step 2147 current loss 0.146991, current_train_items 68736.
I0304 19:29:00.046081 22849303695488 run.py:483] Algo bellman_ford step 2148 current loss 0.169342, current_train_items 68768.
I0304 19:29:00.077935 22849303695488 run.py:483] Algo bellman_ford step 2149 current loss 0.171481, current_train_items 68800.
I0304 19:29:00.097198 22849303695488 run.py:483] Algo bellman_ford step 2150 current loss 0.022030, current_train_items 68832.
I0304 19:29:00.105438 22849303695488 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.9560546875, 'score': 0.9560546875, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0304 19:29:00.105547 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.956, val scores are: bellman_ford: 0.956
I0304 19:29:00.122715 22849303695488 run.py:483] Algo bellman_ford step 2151 current loss 0.024081, current_train_items 68864.
I0304 19:29:00.147061 22849303695488 run.py:483] Algo bellman_ford step 2152 current loss 0.137470, current_train_items 68896.
I0304 19:29:00.177021 22849303695488 run.py:483] Algo bellman_ford step 2153 current loss 0.196902, current_train_items 68928.
I0304 19:29:00.209708 22849303695488 run.py:483] Algo bellman_ford step 2154 current loss 0.183816, current_train_items 68960.
I0304 19:29:00.229566 22849303695488 run.py:483] Algo bellman_ford step 2155 current loss 0.016306, current_train_items 68992.
I0304 19:29:00.245708 22849303695488 run.py:483] Algo bellman_ford step 2156 current loss 0.033838, current_train_items 69024.
I0304 19:29:00.269720 22849303695488 run.py:483] Algo bellman_ford step 2157 current loss 0.074087, current_train_items 69056.
I0304 19:29:00.300198 22849303695488 run.py:483] Algo bellman_ford step 2158 current loss 0.095886, current_train_items 69088.
I0304 19:29:00.333503 22849303695488 run.py:483] Algo bellman_ford step 2159 current loss 0.198412, current_train_items 69120.
I0304 19:29:00.353510 22849303695488 run.py:483] Algo bellman_ford step 2160 current loss 0.035526, current_train_items 69152.
I0304 19:29:00.370059 22849303695488 run.py:483] Algo bellman_ford step 2161 current loss 0.024895, current_train_items 69184.
I0304 19:29:00.393768 22849303695488 run.py:483] Algo bellman_ford step 2162 current loss 0.097582, current_train_items 69216.
I0304 19:29:00.422596 22849303695488 run.py:483] Algo bellman_ford step 2163 current loss 0.078199, current_train_items 69248.
I0304 19:29:00.456360 22849303695488 run.py:483] Algo bellman_ford step 2164 current loss 0.174771, current_train_items 69280.
I0304 19:29:00.476017 22849303695488 run.py:483] Algo bellman_ford step 2165 current loss 0.013153, current_train_items 69312.
I0304 19:29:00.492974 22849303695488 run.py:483] Algo bellman_ford step 2166 current loss 0.085389, current_train_items 69344.
I0304 19:29:00.516217 22849303695488 run.py:483] Algo bellman_ford step 2167 current loss 0.071178, current_train_items 69376.
I0304 19:29:00.545043 22849303695488 run.py:483] Algo bellman_ford step 2168 current loss 0.071786, current_train_items 69408.
I0304 19:29:00.578162 22849303695488 run.py:483] Algo bellman_ford step 2169 current loss 0.138960, current_train_items 69440.
I0304 19:29:00.597759 22849303695488 run.py:483] Algo bellman_ford step 2170 current loss 0.015447, current_train_items 69472.
I0304 19:29:00.614206 22849303695488 run.py:483] Algo bellman_ford step 2171 current loss 0.080833, current_train_items 69504.
I0304 19:29:00.637643 22849303695488 run.py:483] Algo bellman_ford step 2172 current loss 0.098935, current_train_items 69536.
I0304 19:29:00.668766 22849303695488 run.py:483] Algo bellman_ford step 2173 current loss 0.118327, current_train_items 69568.
I0304 19:29:00.699804 22849303695488 run.py:483] Algo bellman_ford step 2174 current loss 0.124528, current_train_items 69600.
I0304 19:29:00.719750 22849303695488 run.py:483] Algo bellman_ford step 2175 current loss 0.010406, current_train_items 69632.
I0304 19:29:00.735806 22849303695488 run.py:483] Algo bellman_ford step 2176 current loss 0.014147, current_train_items 69664.
I0304 19:29:00.760062 22849303695488 run.py:483] Algo bellman_ford step 2177 current loss 0.174778, current_train_items 69696.
I0304 19:29:00.789488 22849303695488 run.py:483] Algo bellman_ford step 2178 current loss 0.150090, current_train_items 69728.
I0304 19:29:00.823312 22849303695488 run.py:483] Algo bellman_ford step 2179 current loss 0.093862, current_train_items 69760.
I0304 19:29:00.842609 22849303695488 run.py:483] Algo bellman_ford step 2180 current loss 0.041967, current_train_items 69792.
I0304 19:29:00.859192 22849303695488 run.py:483] Algo bellman_ford step 2181 current loss 0.048639, current_train_items 69824.
I0304 19:29:00.883240 22849303695488 run.py:483] Algo bellman_ford step 2182 current loss 0.076769, current_train_items 69856.
I0304 19:29:00.912781 22849303695488 run.py:483] Algo bellman_ford step 2183 current loss 0.143578, current_train_items 69888.
I0304 19:29:00.947071 22849303695488 run.py:483] Algo bellman_ford step 2184 current loss 0.218046, current_train_items 69920.
I0304 19:29:00.967389 22849303695488 run.py:483] Algo bellman_ford step 2185 current loss 0.015123, current_train_items 69952.
I0304 19:29:00.983551 22849303695488 run.py:483] Algo bellman_ford step 2186 current loss 0.028344, current_train_items 69984.
I0304 19:29:01.007000 22849303695488 run.py:483] Algo bellman_ford step 2187 current loss 0.109890, current_train_items 70016.
I0304 19:29:01.037224 22849303695488 run.py:483] Algo bellman_ford step 2188 current loss 0.106889, current_train_items 70048.
W0304 19:29:01.062083 22849303695488 samplers.py:155] Increasing hint lengh from 12 to 13
I0304 19:29:07.882024 22849303695488 run.py:483] Algo bellman_ford step 2189 current loss 0.228020, current_train_items 70080.
I0304 19:29:07.902949 22849303695488 run.py:483] Algo bellman_ford step 2190 current loss 0.014924, current_train_items 70112.
I0304 19:29:07.919942 22849303695488 run.py:483] Algo bellman_ford step 2191 current loss 0.063035, current_train_items 70144.
I0304 19:29:07.944602 22849303695488 run.py:483] Algo bellman_ford step 2192 current loss 0.089893, current_train_items 70176.
W0304 19:29:07.966734 22849303695488 samplers.py:155] Increasing hint lengh from 10 to 12
I0304 19:29:15.135636 22849303695488 run.py:483] Algo bellman_ford step 2193 current loss 0.273423, current_train_items 70208.
I0304 19:29:15.168821 22849303695488 run.py:483] Algo bellman_ford step 2194 current loss 0.139132, current_train_items 70240.
I0304 19:29:15.189712 22849303695488 run.py:483] Algo bellman_ford step 2195 current loss 0.038531, current_train_items 70272.
I0304 19:29:15.206707 22849303695488 run.py:483] Algo bellman_ford step 2196 current loss 0.029094, current_train_items 70304.
I0304 19:29:15.230700 22849303695488 run.py:483] Algo bellman_ford step 2197 current loss 0.055350, current_train_items 70336.
I0304 19:29:15.260437 22849303695488 run.py:483] Algo bellman_ford step 2198 current loss 0.102373, current_train_items 70368.
I0304 19:29:15.293852 22849303695488 run.py:483] Algo bellman_ford step 2199 current loss 0.113603, current_train_items 70400.
I0304 19:29:15.314369 22849303695488 run.py:483] Algo bellman_ford step 2200 current loss 0.015642, current_train_items 70432.
I0304 19:29:15.323925 22849303695488 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0304 19:29:15.324045 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:29:15.341624 22849303695488 run.py:483] Algo bellman_ford step 2201 current loss 0.048788, current_train_items 70464.
I0304 19:29:15.365596 22849303695488 run.py:483] Algo bellman_ford step 2202 current loss 0.056260, current_train_items 70496.
I0304 19:29:15.402845 22849303695488 run.py:483] Algo bellman_ford step 2203 current loss 0.089028, current_train_items 70528.
I0304 19:29:15.436850 22849303695488 run.py:483] Algo bellman_ford step 2204 current loss 0.088576, current_train_items 70560.
I0304 19:29:15.457257 22849303695488 run.py:483] Algo bellman_ford step 2205 current loss 0.008065, current_train_items 70592.
I0304 19:29:15.474022 22849303695488 run.py:483] Algo bellman_ford step 2206 current loss 0.046552, current_train_items 70624.
I0304 19:29:15.498084 22849303695488 run.py:483] Algo bellman_ford step 2207 current loss 0.086238, current_train_items 70656.
I0304 19:29:15.529016 22849303695488 run.py:483] Algo bellman_ford step 2208 current loss 0.123996, current_train_items 70688.
I0304 19:29:15.563829 22849303695488 run.py:483] Algo bellman_ford step 2209 current loss 0.119222, current_train_items 70720.
I0304 19:29:15.584101 22849303695488 run.py:483] Algo bellman_ford step 2210 current loss 0.026325, current_train_items 70752.
I0304 19:29:15.600801 22849303695488 run.py:483] Algo bellman_ford step 2211 current loss 0.024381, current_train_items 70784.
I0304 19:29:15.625324 22849303695488 run.py:483] Algo bellman_ford step 2212 current loss 0.085214, current_train_items 70816.
I0304 19:29:15.656723 22849303695488 run.py:483] Algo bellman_ford step 2213 current loss 0.151276, current_train_items 70848.
I0304 19:29:15.691937 22849303695488 run.py:483] Algo bellman_ford step 2214 current loss 0.126510, current_train_items 70880.
I0304 19:29:15.712410 22849303695488 run.py:483] Algo bellman_ford step 2215 current loss 0.022167, current_train_items 70912.
I0304 19:29:15.729076 22849303695488 run.py:483] Algo bellman_ford step 2216 current loss 0.020371, current_train_items 70944.
I0304 19:29:15.751832 22849303695488 run.py:483] Algo bellman_ford step 2217 current loss 0.069238, current_train_items 70976.
I0304 19:29:15.783676 22849303695488 run.py:483] Algo bellman_ford step 2218 current loss 0.130804, current_train_items 71008.
I0304 19:29:15.816429 22849303695488 run.py:483] Algo bellman_ford step 2219 current loss 0.132524, current_train_items 71040.
I0304 19:29:15.836565 22849303695488 run.py:483] Algo bellman_ford step 2220 current loss 0.012691, current_train_items 71072.
I0304 19:29:15.853079 22849303695488 run.py:483] Algo bellman_ford step 2221 current loss 0.028269, current_train_items 71104.
I0304 19:29:15.876687 22849303695488 run.py:483] Algo bellman_ford step 2222 current loss 0.111860, current_train_items 71136.
I0304 19:29:15.908424 22849303695488 run.py:483] Algo bellman_ford step 2223 current loss 0.074404, current_train_items 71168.
I0304 19:29:15.943562 22849303695488 run.py:483] Algo bellman_ford step 2224 current loss 0.114276, current_train_items 71200.
I0304 19:29:15.963643 22849303695488 run.py:483] Algo bellman_ford step 2225 current loss 0.015325, current_train_items 71232.
I0304 19:29:15.980383 22849303695488 run.py:483] Algo bellman_ford step 2226 current loss 0.029973, current_train_items 71264.
I0304 19:29:16.004698 22849303695488 run.py:483] Algo bellman_ford step 2227 current loss 0.189873, current_train_items 71296.
I0304 19:29:16.035692 22849303695488 run.py:483] Algo bellman_ford step 2228 current loss 0.329351, current_train_items 71328.
I0304 19:29:16.067170 22849303695488 run.py:483] Algo bellman_ford step 2229 current loss 0.246355, current_train_items 71360.
I0304 19:29:16.087385 22849303695488 run.py:483] Algo bellman_ford step 2230 current loss 0.010344, current_train_items 71392.
I0304 19:29:16.104279 22849303695488 run.py:483] Algo bellman_ford step 2231 current loss 0.059422, current_train_items 71424.
I0304 19:29:16.128153 22849303695488 run.py:483] Algo bellman_ford step 2232 current loss 0.122502, current_train_items 71456.
I0304 19:29:16.158907 22849303695488 run.py:483] Algo bellman_ford step 2233 current loss 0.111058, current_train_items 71488.
I0304 19:29:16.192053 22849303695488 run.py:483] Algo bellman_ford step 2234 current loss 0.088129, current_train_items 71520.
I0304 19:29:16.212179 22849303695488 run.py:483] Algo bellman_ford step 2235 current loss 0.014113, current_train_items 71552.
I0304 19:29:16.228936 22849303695488 run.py:483] Algo bellman_ford step 2236 current loss 0.055232, current_train_items 71584.
I0304 19:29:16.254040 22849303695488 run.py:483] Algo bellman_ford step 2237 current loss 0.095152, current_train_items 71616.
I0304 19:29:16.285279 22849303695488 run.py:483] Algo bellman_ford step 2238 current loss 0.065425, current_train_items 71648.
I0304 19:29:16.317999 22849303695488 run.py:483] Algo bellman_ford step 2239 current loss 0.125580, current_train_items 71680.
I0304 19:29:16.338256 22849303695488 run.py:483] Algo bellman_ford step 2240 current loss 0.057270, current_train_items 71712.
I0304 19:29:16.355367 22849303695488 run.py:483] Algo bellman_ford step 2241 current loss 0.042876, current_train_items 71744.
I0304 19:29:16.380201 22849303695488 run.py:483] Algo bellman_ford step 2242 current loss 0.102871, current_train_items 71776.
I0304 19:29:16.411279 22849303695488 run.py:483] Algo bellman_ford step 2243 current loss 0.108915, current_train_items 71808.
I0304 19:29:16.443521 22849303695488 run.py:483] Algo bellman_ford step 2244 current loss 0.066617, current_train_items 71840.
I0304 19:29:16.463446 22849303695488 run.py:483] Algo bellman_ford step 2245 current loss 0.027302, current_train_items 71872.
I0304 19:29:16.480305 22849303695488 run.py:483] Algo bellman_ford step 2246 current loss 0.069143, current_train_items 71904.
I0304 19:29:16.503616 22849303695488 run.py:483] Algo bellman_ford step 2247 current loss 0.067067, current_train_items 71936.
I0304 19:29:16.535492 22849303695488 run.py:483] Algo bellman_ford step 2248 current loss 0.108840, current_train_items 71968.
I0304 19:29:16.569827 22849303695488 run.py:483] Algo bellman_ford step 2249 current loss 0.143092, current_train_items 72000.
I0304 19:29:16.590214 22849303695488 run.py:483] Algo bellman_ford step 2250 current loss 0.010608, current_train_items 72032.
I0304 19:29:16.599088 22849303695488 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0304 19:29:16.599196 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:29:16.616255 22849303695488 run.py:483] Algo bellman_ford step 2251 current loss 0.052693, current_train_items 72064.
I0304 19:29:16.639457 22849303695488 run.py:483] Algo bellman_ford step 2252 current loss 0.036189, current_train_items 72096.
I0304 19:29:16.671774 22849303695488 run.py:483] Algo bellman_ford step 2253 current loss 0.111757, current_train_items 72128.
I0304 19:29:16.706663 22849303695488 run.py:483] Algo bellman_ford step 2254 current loss 0.105892, current_train_items 72160.
I0304 19:29:16.726865 22849303695488 run.py:483] Algo bellman_ford step 2255 current loss 0.021613, current_train_items 72192.
I0304 19:29:16.743413 22849303695488 run.py:483] Algo bellman_ford step 2256 current loss 0.034048, current_train_items 72224.
I0304 19:29:16.767320 22849303695488 run.py:483] Algo bellman_ford step 2257 current loss 0.071654, current_train_items 72256.
I0304 19:29:16.796941 22849303695488 run.py:483] Algo bellman_ford step 2258 current loss 0.097534, current_train_items 72288.
I0304 19:29:16.831532 22849303695488 run.py:483] Algo bellman_ford step 2259 current loss 0.149056, current_train_items 72320.
I0304 19:29:16.851835 22849303695488 run.py:483] Algo bellman_ford step 2260 current loss 0.012252, current_train_items 72352.
I0304 19:29:16.869079 22849303695488 run.py:483] Algo bellman_ford step 2261 current loss 0.041362, current_train_items 72384.
I0304 19:29:16.893973 22849303695488 run.py:483] Algo bellman_ford step 2262 current loss 0.094709, current_train_items 72416.
I0304 19:29:16.923848 22849303695488 run.py:483] Algo bellman_ford step 2263 current loss 0.065128, current_train_items 72448.
I0304 19:29:16.957467 22849303695488 run.py:483] Algo bellman_ford step 2264 current loss 0.183549, current_train_items 72480.
I0304 19:29:16.977501 22849303695488 run.py:483] Algo bellman_ford step 2265 current loss 0.053388, current_train_items 72512.
I0304 19:29:16.993992 22849303695488 run.py:483] Algo bellman_ford step 2266 current loss 0.067109, current_train_items 72544.
I0304 19:29:17.018717 22849303695488 run.py:483] Algo bellman_ford step 2267 current loss 0.065712, current_train_items 72576.
I0304 19:29:17.049506 22849303695488 run.py:483] Algo bellman_ford step 2268 current loss 0.168337, current_train_items 72608.
I0304 19:29:17.082820 22849303695488 run.py:483] Algo bellman_ford step 2269 current loss 0.178872, current_train_items 72640.
I0304 19:29:17.103241 22849303695488 run.py:483] Algo bellman_ford step 2270 current loss 0.031924, current_train_items 72672.
I0304 19:29:17.119885 22849303695488 run.py:483] Algo bellman_ford step 2271 current loss 0.016835, current_train_items 72704.
I0304 19:29:17.142436 22849303695488 run.py:483] Algo bellman_ford step 2272 current loss 0.080321, current_train_items 72736.
I0304 19:29:17.173854 22849303695488 run.py:483] Algo bellman_ford step 2273 current loss 0.095851, current_train_items 72768.
I0304 19:29:17.209901 22849303695488 run.py:483] Algo bellman_ford step 2274 current loss 0.150760, current_train_items 72800.
I0304 19:29:17.230269 22849303695488 run.py:483] Algo bellman_ford step 2275 current loss 0.006104, current_train_items 72832.
I0304 19:29:17.246746 22849303695488 run.py:483] Algo bellman_ford step 2276 current loss 0.043830, current_train_items 72864.
I0304 19:29:17.269162 22849303695488 run.py:483] Algo bellman_ford step 2277 current loss 0.030397, current_train_items 72896.
I0304 19:29:17.299094 22849303695488 run.py:483] Algo bellman_ford step 2278 current loss 0.093638, current_train_items 72928.
I0304 19:29:17.333962 22849303695488 run.py:483] Algo bellman_ford step 2279 current loss 0.119290, current_train_items 72960.
I0304 19:29:17.353984 22849303695488 run.py:483] Algo bellman_ford step 2280 current loss 0.003785, current_train_items 72992.
I0304 19:29:17.370398 22849303695488 run.py:483] Algo bellman_ford step 2281 current loss 0.061907, current_train_items 73024.
I0304 19:29:17.394773 22849303695488 run.py:483] Algo bellman_ford step 2282 current loss 0.064113, current_train_items 73056.
I0304 19:29:17.428361 22849303695488 run.py:483] Algo bellman_ford step 2283 current loss 0.143435, current_train_items 73088.
I0304 19:29:17.463088 22849303695488 run.py:483] Algo bellman_ford step 2284 current loss 0.105607, current_train_items 73120.
I0304 19:29:17.483706 22849303695488 run.py:483] Algo bellman_ford step 2285 current loss 0.011761, current_train_items 73152.
I0304 19:29:17.500368 22849303695488 run.py:483] Algo bellman_ford step 2286 current loss 0.065721, current_train_items 73184.
I0304 19:29:17.523716 22849303695488 run.py:483] Algo bellman_ford step 2287 current loss 0.106800, current_train_items 73216.
I0304 19:29:17.554101 22849303695488 run.py:483] Algo bellman_ford step 2288 current loss 0.103410, current_train_items 73248.
I0304 19:29:17.590223 22849303695488 run.py:483] Algo bellman_ford step 2289 current loss 0.140660, current_train_items 73280.
I0304 19:29:17.610834 22849303695488 run.py:483] Algo bellman_ford step 2290 current loss 0.006029, current_train_items 73312.
I0304 19:29:17.626945 22849303695488 run.py:483] Algo bellman_ford step 2291 current loss 0.038273, current_train_items 73344.
I0304 19:29:17.649980 22849303695488 run.py:483] Algo bellman_ford step 2292 current loss 0.087072, current_train_items 73376.
I0304 19:29:17.681542 22849303695488 run.py:483] Algo bellman_ford step 2293 current loss 0.111427, current_train_items 73408.
I0304 19:29:17.715954 22849303695488 run.py:483] Algo bellman_ford step 2294 current loss 0.176176, current_train_items 73440.
I0304 19:29:17.735896 22849303695488 run.py:483] Algo bellman_ford step 2295 current loss 0.010940, current_train_items 73472.
I0304 19:29:17.752404 22849303695488 run.py:483] Algo bellman_ford step 2296 current loss 0.078488, current_train_items 73504.
I0304 19:29:17.775501 22849303695488 run.py:483] Algo bellman_ford step 2297 current loss 0.063166, current_train_items 73536.
I0304 19:29:17.805359 22849303695488 run.py:483] Algo bellman_ford step 2298 current loss 0.112996, current_train_items 73568.
I0304 19:29:17.839273 22849303695488 run.py:483] Algo bellman_ford step 2299 current loss 0.123954, current_train_items 73600.
I0304 19:29:17.859738 22849303695488 run.py:483] Algo bellman_ford step 2300 current loss 0.022185, current_train_items 73632.
I0304 19:29:17.867865 22849303695488 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0304 19:29:17.867972 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:29:17.885614 22849303695488 run.py:483] Algo bellman_ford step 2301 current loss 0.078086, current_train_items 73664.
I0304 19:29:17.910189 22849303695488 run.py:483] Algo bellman_ford step 2302 current loss 0.101586, current_train_items 73696.
I0304 19:29:17.942935 22849303695488 run.py:483] Algo bellman_ford step 2303 current loss 0.099229, current_train_items 73728.
I0304 19:29:17.977483 22849303695488 run.py:483] Algo bellman_ford step 2304 current loss 0.120690, current_train_items 73760.
I0304 19:29:17.997911 22849303695488 run.py:483] Algo bellman_ford step 2305 current loss 0.099060, current_train_items 73792.
I0304 19:29:18.014664 22849303695488 run.py:483] Algo bellman_ford step 2306 current loss 0.060832, current_train_items 73824.
I0304 19:29:18.038674 22849303695488 run.py:483] Algo bellman_ford step 2307 current loss 0.129608, current_train_items 73856.
I0304 19:29:18.068613 22849303695488 run.py:483] Algo bellman_ford step 2308 current loss 0.073495, current_train_items 73888.
I0304 19:29:18.103345 22849303695488 run.py:483] Algo bellman_ford step 2309 current loss 0.116915, current_train_items 73920.
I0304 19:29:18.123729 22849303695488 run.py:483] Algo bellman_ford step 2310 current loss 0.011042, current_train_items 73952.
I0304 19:29:18.140363 22849303695488 run.py:483] Algo bellman_ford step 2311 current loss 0.036236, current_train_items 73984.
I0304 19:29:18.164555 22849303695488 run.py:483] Algo bellman_ford step 2312 current loss 0.125731, current_train_items 74016.
I0304 19:29:18.195681 22849303695488 run.py:483] Algo bellman_ford step 2313 current loss 0.145546, current_train_items 74048.
I0304 19:29:18.230248 22849303695488 run.py:483] Algo bellman_ford step 2314 current loss 0.115977, current_train_items 74080.
I0304 19:29:18.250281 22849303695488 run.py:483] Algo bellman_ford step 2315 current loss 0.010310, current_train_items 74112.
I0304 19:29:18.266932 22849303695488 run.py:483] Algo bellman_ford step 2316 current loss 0.087264, current_train_items 74144.
I0304 19:29:18.290216 22849303695488 run.py:483] Algo bellman_ford step 2317 current loss 0.074333, current_train_items 74176.
I0304 19:29:18.322249 22849303695488 run.py:483] Algo bellman_ford step 2318 current loss 0.104346, current_train_items 74208.
I0304 19:29:18.357090 22849303695488 run.py:483] Algo bellman_ford step 2319 current loss 0.119353, current_train_items 74240.
I0304 19:29:18.377415 22849303695488 run.py:483] Algo bellman_ford step 2320 current loss 0.013290, current_train_items 74272.
I0304 19:29:18.393781 22849303695488 run.py:483] Algo bellman_ford step 2321 current loss 0.020926, current_train_items 74304.
I0304 19:29:18.418469 22849303695488 run.py:483] Algo bellman_ford step 2322 current loss 0.110429, current_train_items 74336.
I0304 19:29:18.449020 22849303695488 run.py:483] Algo bellman_ford step 2323 current loss 0.145606, current_train_items 74368.
I0304 19:29:18.483669 22849303695488 run.py:483] Algo bellman_ford step 2324 current loss 0.254971, current_train_items 74400.
I0304 19:29:18.503796 22849303695488 run.py:483] Algo bellman_ford step 2325 current loss 0.012596, current_train_items 74432.
I0304 19:29:18.520155 22849303695488 run.py:483] Algo bellman_ford step 2326 current loss 0.031625, current_train_items 74464.
I0304 19:29:18.543349 22849303695488 run.py:483] Algo bellman_ford step 2327 current loss 0.072507, current_train_items 74496.
I0304 19:29:18.572904 22849303695488 run.py:483] Algo bellman_ford step 2328 current loss 0.069947, current_train_items 74528.
I0304 19:29:18.605633 22849303695488 run.py:483] Algo bellman_ford step 2329 current loss 0.097999, current_train_items 74560.
I0304 19:29:18.625734 22849303695488 run.py:483] Algo bellman_ford step 2330 current loss 0.020145, current_train_items 74592.
I0304 19:29:18.641921 22849303695488 run.py:483] Algo bellman_ford step 2331 current loss 0.056863, current_train_items 74624.
I0304 19:29:18.666592 22849303695488 run.py:483] Algo bellman_ford step 2332 current loss 0.103006, current_train_items 74656.
I0304 19:29:18.698322 22849303695488 run.py:483] Algo bellman_ford step 2333 current loss 0.120873, current_train_items 74688.
I0304 19:29:18.733310 22849303695488 run.py:483] Algo bellman_ford step 2334 current loss 0.107496, current_train_items 74720.
I0304 19:29:18.753192 22849303695488 run.py:483] Algo bellman_ford step 2335 current loss 0.019025, current_train_items 74752.
I0304 19:29:18.770024 22849303695488 run.py:483] Algo bellman_ford step 2336 current loss 0.033306, current_train_items 74784.
I0304 19:29:18.793911 22849303695488 run.py:483] Algo bellman_ford step 2337 current loss 0.042846, current_train_items 74816.
I0304 19:29:18.824434 22849303695488 run.py:483] Algo bellman_ford step 2338 current loss 0.091430, current_train_items 74848.
I0304 19:29:18.857848 22849303695488 run.py:483] Algo bellman_ford step 2339 current loss 0.114558, current_train_items 74880.
I0304 19:29:18.878327 22849303695488 run.py:483] Algo bellman_ford step 2340 current loss 0.010488, current_train_items 74912.
I0304 19:29:18.895034 22849303695488 run.py:483] Algo bellman_ford step 2341 current loss 0.021598, current_train_items 74944.
I0304 19:29:18.917978 22849303695488 run.py:483] Algo bellman_ford step 2342 current loss 0.060057, current_train_items 74976.
I0304 19:29:18.948118 22849303695488 run.py:483] Algo bellman_ford step 2343 current loss 0.094773, current_train_items 75008.
I0304 19:29:18.980678 22849303695488 run.py:483] Algo bellman_ford step 2344 current loss 0.124121, current_train_items 75040.
I0304 19:29:19.000695 22849303695488 run.py:483] Algo bellman_ford step 2345 current loss 0.024992, current_train_items 75072.
I0304 19:29:19.017046 22849303695488 run.py:483] Algo bellman_ford step 2346 current loss 0.045734, current_train_items 75104.
I0304 19:29:19.040940 22849303695488 run.py:483] Algo bellman_ford step 2347 current loss 0.056296, current_train_items 75136.
I0304 19:29:19.070524 22849303695488 run.py:483] Algo bellman_ford step 2348 current loss 0.074346, current_train_items 75168.
I0304 19:29:19.103087 22849303695488 run.py:483] Algo bellman_ford step 2349 current loss 0.139234, current_train_items 75200.
I0304 19:29:19.123461 22849303695488 run.py:483] Algo bellman_ford step 2350 current loss 0.029436, current_train_items 75232.
I0304 19:29:19.131738 22849303695488 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.96484375, 'score': 0.96484375, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0304 19:29:19.131846 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.965, val scores are: bellman_ford: 0.965
I0304 19:29:19.148896 22849303695488 run.py:483] Algo bellman_ford step 2351 current loss 0.025265, current_train_items 75264.
I0304 19:29:19.173084 22849303695488 run.py:483] Algo bellman_ford step 2352 current loss 0.095489, current_train_items 75296.
I0304 19:29:19.202764 22849303695488 run.py:483] Algo bellman_ford step 2353 current loss 0.063122, current_train_items 75328.
I0304 19:29:19.238898 22849303695488 run.py:483] Algo bellman_ford step 2354 current loss 0.120019, current_train_items 75360.
I0304 19:29:19.259443 22849303695488 run.py:483] Algo bellman_ford step 2355 current loss 0.046496, current_train_items 75392.
I0304 19:29:19.275313 22849303695488 run.py:483] Algo bellman_ford step 2356 current loss 0.027673, current_train_items 75424.
I0304 19:29:19.299577 22849303695488 run.py:483] Algo bellman_ford step 2357 current loss 0.080061, current_train_items 75456.
I0304 19:29:19.329236 22849303695488 run.py:483] Algo bellman_ford step 2358 current loss 0.127123, current_train_items 75488.
I0304 19:29:19.364845 22849303695488 run.py:483] Algo bellman_ford step 2359 current loss 0.112770, current_train_items 75520.
I0304 19:29:19.385324 22849303695488 run.py:483] Algo bellman_ford step 2360 current loss 0.010174, current_train_items 75552.
I0304 19:29:19.402641 22849303695488 run.py:483] Algo bellman_ford step 2361 current loss 0.058022, current_train_items 75584.
I0304 19:29:19.427228 22849303695488 run.py:483] Algo bellman_ford step 2362 current loss 0.218603, current_train_items 75616.
I0304 19:29:19.457220 22849303695488 run.py:483] Algo bellman_ford step 2363 current loss 0.187435, current_train_items 75648.
I0304 19:29:19.491188 22849303695488 run.py:483] Algo bellman_ford step 2364 current loss 0.213600, current_train_items 75680.
I0304 19:29:19.511235 22849303695488 run.py:483] Algo bellman_ford step 2365 current loss 0.006830, current_train_items 75712.
I0304 19:29:19.527911 22849303695488 run.py:483] Algo bellman_ford step 2366 current loss 0.045333, current_train_items 75744.
I0304 19:29:19.552083 22849303695488 run.py:483] Algo bellman_ford step 2367 current loss 0.188601, current_train_items 75776.
I0304 19:29:19.583884 22849303695488 run.py:483] Algo bellman_ford step 2368 current loss 0.158781, current_train_items 75808.
I0304 19:29:19.618994 22849303695488 run.py:483] Algo bellman_ford step 2369 current loss 0.151456, current_train_items 75840.
I0304 19:29:19.639533 22849303695488 run.py:483] Algo bellman_ford step 2370 current loss 0.007670, current_train_items 75872.
I0304 19:29:19.656376 22849303695488 run.py:483] Algo bellman_ford step 2371 current loss 0.053268, current_train_items 75904.
I0304 19:29:19.680124 22849303695488 run.py:483] Algo bellman_ford step 2372 current loss 0.163403, current_train_items 75936.
I0304 19:29:19.710216 22849303695488 run.py:483] Algo bellman_ford step 2373 current loss 0.168864, current_train_items 75968.
I0304 19:29:19.743479 22849303695488 run.py:483] Algo bellman_ford step 2374 current loss 0.101686, current_train_items 76000.
I0304 19:29:19.763970 22849303695488 run.py:483] Algo bellman_ford step 2375 current loss 0.005883, current_train_items 76032.
I0304 19:29:19.780683 22849303695488 run.py:483] Algo bellman_ford step 2376 current loss 0.026118, current_train_items 76064.
I0304 19:29:19.804595 22849303695488 run.py:483] Algo bellman_ford step 2377 current loss 0.144940, current_train_items 76096.
I0304 19:29:19.836824 22849303695488 run.py:483] Algo bellman_ford step 2378 current loss 0.106547, current_train_items 76128.
I0304 19:29:19.872478 22849303695488 run.py:483] Algo bellman_ford step 2379 current loss 0.132754, current_train_items 76160.
I0304 19:29:19.892613 22849303695488 run.py:483] Algo bellman_ford step 2380 current loss 0.029166, current_train_items 76192.
I0304 19:29:19.909627 22849303695488 run.py:483] Algo bellman_ford step 2381 current loss 0.064219, current_train_items 76224.
I0304 19:29:19.934142 22849303695488 run.py:483] Algo bellman_ford step 2382 current loss 0.085862, current_train_items 76256.
I0304 19:29:19.964217 22849303695488 run.py:483] Algo bellman_ford step 2383 current loss 0.097459, current_train_items 76288.
I0304 19:29:19.998749 22849303695488 run.py:483] Algo bellman_ford step 2384 current loss 0.172783, current_train_items 76320.
I0304 19:29:20.019061 22849303695488 run.py:483] Algo bellman_ford step 2385 current loss 0.014226, current_train_items 76352.
I0304 19:29:20.035454 22849303695488 run.py:483] Algo bellman_ford step 2386 current loss 0.030639, current_train_items 76384.
I0304 19:29:20.058409 22849303695488 run.py:483] Algo bellman_ford step 2387 current loss 0.094650, current_train_items 76416.
I0304 19:29:20.089469 22849303695488 run.py:483] Algo bellman_ford step 2388 current loss 0.097656, current_train_items 76448.
I0304 19:29:20.125177 22849303695488 run.py:483] Algo bellman_ford step 2389 current loss 0.148284, current_train_items 76480.
I0304 19:29:20.145916 22849303695488 run.py:483] Algo bellman_ford step 2390 current loss 0.016000, current_train_items 76512.
I0304 19:29:20.162149 22849303695488 run.py:483] Algo bellman_ford step 2391 current loss 0.043388, current_train_items 76544.
I0304 19:29:20.184723 22849303695488 run.py:483] Algo bellman_ford step 2392 current loss 0.073803, current_train_items 76576.
I0304 19:29:20.215332 22849303695488 run.py:483] Algo bellman_ford step 2393 current loss 0.101971, current_train_items 76608.
I0304 19:29:20.251987 22849303695488 run.py:483] Algo bellman_ford step 2394 current loss 0.145440, current_train_items 76640.
I0304 19:29:20.272190 22849303695488 run.py:483] Algo bellman_ford step 2395 current loss 0.014334, current_train_items 76672.
I0304 19:29:20.288628 22849303695488 run.py:483] Algo bellman_ford step 2396 current loss 0.029833, current_train_items 76704.
I0304 19:29:20.311435 22849303695488 run.py:483] Algo bellman_ford step 2397 current loss 0.062146, current_train_items 76736.
I0304 19:29:20.341946 22849303695488 run.py:483] Algo bellman_ford step 2398 current loss 0.101432, current_train_items 76768.
I0304 19:29:20.376919 22849303695488 run.py:483] Algo bellman_ford step 2399 current loss 0.123645, current_train_items 76800.
I0304 19:29:20.397584 22849303695488 run.py:483] Algo bellman_ford step 2400 current loss 0.007243, current_train_items 76832.
I0304 19:29:20.406035 22849303695488 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.9501953125, 'score': 0.9501953125, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0304 19:29:20.406144 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.950, val scores are: bellman_ford: 0.950
I0304 19:29:20.423115 22849303695488 run.py:483] Algo bellman_ford step 2401 current loss 0.075499, current_train_items 76864.
I0304 19:29:20.447417 22849303695488 run.py:483] Algo bellman_ford step 2402 current loss 0.093459, current_train_items 76896.
I0304 19:29:20.479351 22849303695488 run.py:483] Algo bellman_ford step 2403 current loss 0.150452, current_train_items 76928.
I0304 19:29:20.516695 22849303695488 run.py:483] Algo bellman_ford step 2404 current loss 0.149317, current_train_items 76960.
I0304 19:29:20.537079 22849303695488 run.py:483] Algo bellman_ford step 2405 current loss 0.008878, current_train_items 76992.
I0304 19:29:20.553463 22849303695488 run.py:483] Algo bellman_ford step 2406 current loss 0.068322, current_train_items 77024.
I0304 19:29:20.577341 22849303695488 run.py:483] Algo bellman_ford step 2407 current loss 0.165326, current_train_items 77056.
I0304 19:29:20.606994 22849303695488 run.py:483] Algo bellman_ford step 2408 current loss 0.186556, current_train_items 77088.
I0304 19:29:20.640357 22849303695488 run.py:483] Algo bellman_ford step 2409 current loss 0.129728, current_train_items 77120.
I0304 19:29:20.660107 22849303695488 run.py:483] Algo bellman_ford step 2410 current loss 0.003231, current_train_items 77152.
I0304 19:29:20.676494 22849303695488 run.py:483] Algo bellman_ford step 2411 current loss 0.044273, current_train_items 77184.
I0304 19:29:20.701478 22849303695488 run.py:483] Algo bellman_ford step 2412 current loss 0.143359, current_train_items 77216.
I0304 19:29:20.733015 22849303695488 run.py:483] Algo bellman_ford step 2413 current loss 0.098335, current_train_items 77248.
I0304 19:29:20.767805 22849303695488 run.py:483] Algo bellman_ford step 2414 current loss 0.118215, current_train_items 77280.
I0304 19:29:20.787856 22849303695488 run.py:483] Algo bellman_ford step 2415 current loss 0.005760, current_train_items 77312.
I0304 19:29:20.803764 22849303695488 run.py:483] Algo bellman_ford step 2416 current loss 0.051451, current_train_items 77344.
I0304 19:29:20.827523 22849303695488 run.py:483] Algo bellman_ford step 2417 current loss 0.109423, current_train_items 77376.
I0304 19:29:20.859528 22849303695488 run.py:483] Algo bellman_ford step 2418 current loss 0.159034, current_train_items 77408.
I0304 19:29:20.891925 22849303695488 run.py:483] Algo bellman_ford step 2419 current loss 0.074805, current_train_items 77440.
I0304 19:29:20.911633 22849303695488 run.py:483] Algo bellman_ford step 2420 current loss 0.019805, current_train_items 77472.
I0304 19:29:20.927937 22849303695488 run.py:483] Algo bellman_ford step 2421 current loss 0.064866, current_train_items 77504.
I0304 19:29:20.952522 22849303695488 run.py:483] Algo bellman_ford step 2422 current loss 0.134450, current_train_items 77536.
I0304 19:29:20.983530 22849303695488 run.py:483] Algo bellman_ford step 2423 current loss 0.111675, current_train_items 77568.
I0304 19:29:21.018152 22849303695488 run.py:483] Algo bellman_ford step 2424 current loss 0.144999, current_train_items 77600.
I0304 19:29:21.038127 22849303695488 run.py:483] Algo bellman_ford step 2425 current loss 0.040421, current_train_items 77632.
I0304 19:29:21.054596 22849303695488 run.py:483] Algo bellman_ford step 2426 current loss 0.101720, current_train_items 77664.
I0304 19:29:21.079604 22849303695488 run.py:483] Algo bellman_ford step 2427 current loss 0.196711, current_train_items 77696.
I0304 19:29:21.109464 22849303695488 run.py:483] Algo bellman_ford step 2428 current loss 0.160600, current_train_items 77728.
I0304 19:29:21.143191 22849303695488 run.py:483] Algo bellman_ford step 2429 current loss 0.098361, current_train_items 77760.
I0304 19:29:21.163264 22849303695488 run.py:483] Algo bellman_ford step 2430 current loss 0.014514, current_train_items 77792.
I0304 19:29:21.179877 22849303695488 run.py:483] Algo bellman_ford step 2431 current loss 0.041404, current_train_items 77824.
I0304 19:29:21.204368 22849303695488 run.py:483] Algo bellman_ford step 2432 current loss 0.080572, current_train_items 77856.
I0304 19:29:21.234851 22849303695488 run.py:483] Algo bellman_ford step 2433 current loss 0.084944, current_train_items 77888.
I0304 19:29:21.268848 22849303695488 run.py:483] Algo bellman_ford step 2434 current loss 0.094118, current_train_items 77920.
I0304 19:29:21.288565 22849303695488 run.py:483] Algo bellman_ford step 2435 current loss 0.038912, current_train_items 77952.
I0304 19:29:21.305258 22849303695488 run.py:483] Algo bellman_ford step 2436 current loss 0.049883, current_train_items 77984.
I0304 19:29:21.329098 22849303695488 run.py:483] Algo bellman_ford step 2437 current loss 0.119194, current_train_items 78016.
I0304 19:29:21.358536 22849303695488 run.py:483] Algo bellman_ford step 2438 current loss 0.129885, current_train_items 78048.
I0304 19:29:21.391639 22849303695488 run.py:483] Algo bellman_ford step 2439 current loss 0.087857, current_train_items 78080.
I0304 19:29:21.411669 22849303695488 run.py:483] Algo bellman_ford step 2440 current loss 0.013418, current_train_items 78112.
I0304 19:29:21.428138 22849303695488 run.py:483] Algo bellman_ford step 2441 current loss 0.038202, current_train_items 78144.
I0304 19:29:21.453310 22849303695488 run.py:483] Algo bellman_ford step 2442 current loss 0.123242, current_train_items 78176.
I0304 19:29:21.484146 22849303695488 run.py:483] Algo bellman_ford step 2443 current loss 0.109624, current_train_items 78208.
I0304 19:29:21.519516 22849303695488 run.py:483] Algo bellman_ford step 2444 current loss 0.119658, current_train_items 78240.
I0304 19:29:21.539435 22849303695488 run.py:483] Algo bellman_ford step 2445 current loss 0.016195, current_train_items 78272.
I0304 19:29:21.555866 22849303695488 run.py:483] Algo bellman_ford step 2446 current loss 0.036233, current_train_items 78304.
I0304 19:29:21.580259 22849303695488 run.py:483] Algo bellman_ford step 2447 current loss 0.109511, current_train_items 78336.
I0304 19:29:21.610114 22849303695488 run.py:483] Algo bellman_ford step 2448 current loss 0.090405, current_train_items 78368.
I0304 19:29:21.643736 22849303695488 run.py:483] Algo bellman_ford step 2449 current loss 0.108196, current_train_items 78400.
I0304 19:29:21.663306 22849303695488 run.py:483] Algo bellman_ford step 2450 current loss 0.017212, current_train_items 78432.
I0304 19:29:21.671774 22849303695488 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0304 19:29:21.671883 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.979, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:29:21.702542 22849303695488 run.py:483] Algo bellman_ford step 2451 current loss 0.030209, current_train_items 78464.
I0304 19:29:21.726753 22849303695488 run.py:483] Algo bellman_ford step 2452 current loss 0.058174, current_train_items 78496.
I0304 19:29:21.757904 22849303695488 run.py:483] Algo bellman_ford step 2453 current loss 0.076472, current_train_items 78528.
I0304 19:29:21.793358 22849303695488 run.py:483] Algo bellman_ford step 2454 current loss 0.116868, current_train_items 78560.
I0304 19:29:21.814461 22849303695488 run.py:483] Algo bellman_ford step 2455 current loss 0.008675, current_train_items 78592.
I0304 19:29:21.831065 22849303695488 run.py:483] Algo bellman_ford step 2456 current loss 0.050247, current_train_items 78624.
I0304 19:29:21.854130 22849303695488 run.py:483] Algo bellman_ford step 2457 current loss 0.040550, current_train_items 78656.
I0304 19:29:21.885473 22849303695488 run.py:483] Algo bellman_ford step 2458 current loss 0.113947, current_train_items 78688.
I0304 19:29:21.920984 22849303695488 run.py:483] Algo bellman_ford step 2459 current loss 0.226576, current_train_items 78720.
I0304 19:29:21.942020 22849303695488 run.py:483] Algo bellman_ford step 2460 current loss 0.011405, current_train_items 78752.
I0304 19:29:21.959216 22849303695488 run.py:483] Algo bellman_ford step 2461 current loss 0.048697, current_train_items 78784.
I0304 19:29:21.982729 22849303695488 run.py:483] Algo bellman_ford step 2462 current loss 0.075809, current_train_items 78816.
I0304 19:29:22.014566 22849303695488 run.py:483] Algo bellman_ford step 2463 current loss 0.146196, current_train_items 78848.
I0304 19:29:22.048585 22849303695488 run.py:483] Algo bellman_ford step 2464 current loss 0.128580, current_train_items 78880.
I0304 19:29:22.068443 22849303695488 run.py:483] Algo bellman_ford step 2465 current loss 0.009978, current_train_items 78912.
I0304 19:29:22.084985 22849303695488 run.py:483] Algo bellman_ford step 2466 current loss 0.070057, current_train_items 78944.
I0304 19:29:22.109325 22849303695488 run.py:483] Algo bellman_ford step 2467 current loss 0.073072, current_train_items 78976.
I0304 19:29:22.140847 22849303695488 run.py:483] Algo bellman_ford step 2468 current loss 0.180659, current_train_items 79008.
I0304 19:29:22.176474 22849303695488 run.py:483] Algo bellman_ford step 2469 current loss 0.128042, current_train_items 79040.
I0304 19:29:22.197063 22849303695488 run.py:483] Algo bellman_ford step 2470 current loss 0.008046, current_train_items 79072.
I0304 19:29:22.213690 22849303695488 run.py:483] Algo bellman_ford step 2471 current loss 0.047716, current_train_items 79104.
I0304 19:29:22.238109 22849303695488 run.py:483] Algo bellman_ford step 2472 current loss 0.231835, current_train_items 79136.
I0304 19:29:22.270273 22849303695488 run.py:483] Algo bellman_ford step 2473 current loss 0.170024, current_train_items 79168.
I0304 19:29:22.303992 22849303695488 run.py:483] Algo bellman_ford step 2474 current loss 0.130202, current_train_items 79200.
I0304 19:29:22.324426 22849303695488 run.py:483] Algo bellman_ford step 2475 current loss 0.038049, current_train_items 79232.
I0304 19:29:22.341208 22849303695488 run.py:483] Algo bellman_ford step 2476 current loss 0.063315, current_train_items 79264.
I0304 19:29:22.363525 22849303695488 run.py:483] Algo bellman_ford step 2477 current loss 0.054070, current_train_items 79296.
I0304 19:29:22.393760 22849303695488 run.py:483] Algo bellman_ford step 2478 current loss 0.148279, current_train_items 79328.
I0304 19:29:22.429514 22849303695488 run.py:483] Algo bellman_ford step 2479 current loss 0.149069, current_train_items 79360.
I0304 19:29:22.449920 22849303695488 run.py:483] Algo bellman_ford step 2480 current loss 0.007812, current_train_items 79392.
I0304 19:29:22.466137 22849303695488 run.py:483] Algo bellman_ford step 2481 current loss 0.033245, current_train_items 79424.
I0304 19:29:22.491436 22849303695488 run.py:483] Algo bellman_ford step 2482 current loss 0.099214, current_train_items 79456.
I0304 19:29:22.522842 22849303695488 run.py:483] Algo bellman_ford step 2483 current loss 0.176149, current_train_items 79488.
I0304 19:29:22.558646 22849303695488 run.py:483] Algo bellman_ford step 2484 current loss 0.145713, current_train_items 79520.
I0304 19:29:22.579574 22849303695488 run.py:483] Algo bellman_ford step 2485 current loss 0.011272, current_train_items 79552.
I0304 19:29:22.596257 22849303695488 run.py:483] Algo bellman_ford step 2486 current loss 0.051562, current_train_items 79584.
I0304 19:29:22.621035 22849303695488 run.py:483] Algo bellman_ford step 2487 current loss 0.105155, current_train_items 79616.
I0304 19:29:22.649500 22849303695488 run.py:483] Algo bellman_ford step 2488 current loss 0.068647, current_train_items 79648.
I0304 19:29:22.684542 22849303695488 run.py:483] Algo bellman_ford step 2489 current loss 0.160261, current_train_items 79680.
I0304 19:29:22.705034 22849303695488 run.py:483] Algo bellman_ford step 2490 current loss 0.017399, current_train_items 79712.
I0304 19:29:22.721618 22849303695488 run.py:483] Algo bellman_ford step 2491 current loss 0.051225, current_train_items 79744.
I0304 19:29:22.744954 22849303695488 run.py:483] Algo bellman_ford step 2492 current loss 0.108892, current_train_items 79776.
I0304 19:29:22.776070 22849303695488 run.py:483] Algo bellman_ford step 2493 current loss 0.084550, current_train_items 79808.
I0304 19:29:22.809216 22849303695488 run.py:483] Algo bellman_ford step 2494 current loss 0.164426, current_train_items 79840.
I0304 19:29:22.829452 22849303695488 run.py:483] Algo bellman_ford step 2495 current loss 0.013332, current_train_items 79872.
I0304 19:29:22.846507 22849303695488 run.py:483] Algo bellman_ford step 2496 current loss 0.039028, current_train_items 79904.
I0304 19:29:22.870313 22849303695488 run.py:483] Algo bellman_ford step 2497 current loss 0.068539, current_train_items 79936.
I0304 19:29:22.899960 22849303695488 run.py:483] Algo bellman_ford step 2498 current loss 0.155898, current_train_items 79968.
I0304 19:29:22.934203 22849303695488 run.py:483] Algo bellman_ford step 2499 current loss 0.115129, current_train_items 80000.
I0304 19:29:22.954686 22849303695488 run.py:483] Algo bellman_ford step 2500 current loss 0.010408, current_train_items 80032.
I0304 19:29:22.962598 22849303695488 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0304 19:29:22.962708 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.981, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:29:22.979953 22849303695488 run.py:483] Algo bellman_ford step 2501 current loss 0.050596, current_train_items 80064.
I0304 19:29:23.004613 22849303695488 run.py:483] Algo bellman_ford step 2502 current loss 0.130341, current_train_items 80096.
I0304 19:29:23.034781 22849303695488 run.py:483] Algo bellman_ford step 2503 current loss 0.108806, current_train_items 80128.
I0304 19:29:23.070101 22849303695488 run.py:483] Algo bellman_ford step 2504 current loss 0.117890, current_train_items 80160.
I0304 19:29:23.090240 22849303695488 run.py:483] Algo bellman_ford step 2505 current loss 0.015915, current_train_items 80192.
I0304 19:29:23.106738 22849303695488 run.py:483] Algo bellman_ford step 2506 current loss 0.042537, current_train_items 80224.
I0304 19:29:23.131059 22849303695488 run.py:483] Algo bellman_ford step 2507 current loss 0.058710, current_train_items 80256.
I0304 19:29:23.160835 22849303695488 run.py:483] Algo bellman_ford step 2508 current loss 0.119530, current_train_items 80288.
I0304 19:29:23.195690 22849303695488 run.py:483] Algo bellman_ford step 2509 current loss 0.137085, current_train_items 80320.
I0304 19:29:23.215581 22849303695488 run.py:483] Algo bellman_ford step 2510 current loss 0.038918, current_train_items 80352.
I0304 19:29:23.232311 22849303695488 run.py:483] Algo bellman_ford step 2511 current loss 0.036169, current_train_items 80384.
I0304 19:29:23.256047 22849303695488 run.py:483] Algo bellman_ford step 2512 current loss 0.081612, current_train_items 80416.
I0304 19:29:23.285983 22849303695488 run.py:483] Algo bellman_ford step 2513 current loss 0.089618, current_train_items 80448.
I0304 19:29:23.320757 22849303695488 run.py:483] Algo bellman_ford step 2514 current loss 0.109552, current_train_items 80480.
I0304 19:29:23.340544 22849303695488 run.py:483] Algo bellman_ford step 2515 current loss 0.007880, current_train_items 80512.
I0304 19:29:23.357383 22849303695488 run.py:483] Algo bellman_ford step 2516 current loss 0.028686, current_train_items 80544.
I0304 19:29:23.381467 22849303695488 run.py:483] Algo bellman_ford step 2517 current loss 0.108901, current_train_items 80576.
I0304 19:29:23.411386 22849303695488 run.py:483] Algo bellman_ford step 2518 current loss 0.068900, current_train_items 80608.
I0304 19:29:23.445680 22849303695488 run.py:483] Algo bellman_ford step 2519 current loss 0.121241, current_train_items 80640.
I0304 19:29:23.465716 22849303695488 run.py:483] Algo bellman_ford step 2520 current loss 0.009474, current_train_items 80672.
I0304 19:29:23.482210 22849303695488 run.py:483] Algo bellman_ford step 2521 current loss 0.043802, current_train_items 80704.
I0304 19:29:23.504709 22849303695488 run.py:483] Algo bellman_ford step 2522 current loss 0.044794, current_train_items 80736.
I0304 19:29:23.534766 22849303695488 run.py:483] Algo bellman_ford step 2523 current loss 0.060866, current_train_items 80768.
I0304 19:29:23.568308 22849303695488 run.py:483] Algo bellman_ford step 2524 current loss 0.115945, current_train_items 80800.
I0304 19:29:23.588262 22849303695488 run.py:483] Algo bellman_ford step 2525 current loss 0.006280, current_train_items 80832.
I0304 19:29:23.604644 22849303695488 run.py:483] Algo bellman_ford step 2526 current loss 0.023299, current_train_items 80864.
I0304 19:29:23.628435 22849303695488 run.py:483] Algo bellman_ford step 2527 current loss 0.105016, current_train_items 80896.
I0304 19:29:23.658360 22849303695488 run.py:483] Algo bellman_ford step 2528 current loss 0.045245, current_train_items 80928.
I0304 19:29:23.693340 22849303695488 run.py:483] Algo bellman_ford step 2529 current loss 0.119129, current_train_items 80960.
I0304 19:29:23.713203 22849303695488 run.py:483] Algo bellman_ford step 2530 current loss 0.006796, current_train_items 80992.
I0304 19:29:23.729336 22849303695488 run.py:483] Algo bellman_ford step 2531 current loss 0.036953, current_train_items 81024.
I0304 19:29:23.753219 22849303695488 run.py:483] Algo bellman_ford step 2532 current loss 0.080777, current_train_items 81056.
I0304 19:29:23.783780 22849303695488 run.py:483] Algo bellman_ford step 2533 current loss 0.071879, current_train_items 81088.
I0304 19:29:23.817097 22849303695488 run.py:483] Algo bellman_ford step 2534 current loss 0.081525, current_train_items 81120.
I0304 19:29:23.837046 22849303695488 run.py:483] Algo bellman_ford step 2535 current loss 0.004747, current_train_items 81152.
I0304 19:29:23.853178 22849303695488 run.py:483] Algo bellman_ford step 2536 current loss 0.025151, current_train_items 81184.
W0304 19:29:23.869936 22849303695488 samplers.py:155] Increasing hint lengh from 9 to 10
I0304 19:29:30.742526 22849303695488 run.py:483] Algo bellman_ford step 2537 current loss 0.102094, current_train_items 81216.
I0304 19:29:30.775614 22849303695488 run.py:483] Algo bellman_ford step 2538 current loss 0.070281, current_train_items 81248.
I0304 19:29:30.811145 22849303695488 run.py:483] Algo bellman_ford step 2539 current loss 0.125575, current_train_items 81280.
I0304 19:29:30.831782 22849303695488 run.py:483] Algo bellman_ford step 2540 current loss 0.042774, current_train_items 81312.
I0304 19:29:30.848310 22849303695488 run.py:483] Algo bellman_ford step 2541 current loss 0.032273, current_train_items 81344.
I0304 19:29:30.872611 22849303695488 run.py:483] Algo bellman_ford step 2542 current loss 0.082506, current_train_items 81376.
I0304 19:29:30.902614 22849303695488 run.py:483] Algo bellman_ford step 2543 current loss 0.086971, current_train_items 81408.
I0304 19:29:30.936486 22849303695488 run.py:483] Algo bellman_ford step 2544 current loss 0.155785, current_train_items 81440.
I0304 19:29:30.956738 22849303695488 run.py:483] Algo bellman_ford step 2545 current loss 0.009519, current_train_items 81472.
I0304 19:29:30.973217 22849303695488 run.py:483] Algo bellman_ford step 2546 current loss 0.089577, current_train_items 81504.
I0304 19:29:30.997773 22849303695488 run.py:483] Algo bellman_ford step 2547 current loss 0.095979, current_train_items 81536.
I0304 19:29:31.030400 22849303695488 run.py:483] Algo bellman_ford step 2548 current loss 0.110376, current_train_items 81568.
I0304 19:29:31.063435 22849303695488 run.py:483] Algo bellman_ford step 2549 current loss 0.106107, current_train_items 81600.
I0304 19:29:31.083600 22849303695488 run.py:483] Algo bellman_ford step 2550 current loss 0.014186, current_train_items 81632.
I0304 19:29:31.093801 22849303695488 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0304 19:29:31.093912 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.981, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:29:31.111225 22849303695488 run.py:483] Algo bellman_ford step 2551 current loss 0.061695, current_train_items 81664.
I0304 19:29:31.136332 22849303695488 run.py:483] Algo bellman_ford step 2552 current loss 0.101989, current_train_items 81696.
I0304 19:29:31.169447 22849303695488 run.py:483] Algo bellman_ford step 2553 current loss 0.097297, current_train_items 81728.
I0304 19:29:31.203889 22849303695488 run.py:483] Algo bellman_ford step 2554 current loss 0.099095, current_train_items 81760.
I0304 19:29:31.224179 22849303695488 run.py:483] Algo bellman_ford step 2555 current loss 0.010214, current_train_items 81792.
I0304 19:29:31.240808 22849303695488 run.py:483] Algo bellman_ford step 2556 current loss 0.064265, current_train_items 81824.
I0304 19:29:31.265707 22849303695488 run.py:483] Algo bellman_ford step 2557 current loss 0.088500, current_train_items 81856.
I0304 19:29:31.297829 22849303695488 run.py:483] Algo bellman_ford step 2558 current loss 0.104652, current_train_items 81888.
I0304 19:29:31.333808 22849303695488 run.py:483] Algo bellman_ford step 2559 current loss 0.176829, current_train_items 81920.
I0304 19:29:31.354221 22849303695488 run.py:483] Algo bellman_ford step 2560 current loss 0.022717, current_train_items 81952.
I0304 19:29:31.371257 22849303695488 run.py:483] Algo bellman_ford step 2561 current loss 0.038058, current_train_items 81984.
I0304 19:29:31.395443 22849303695488 run.py:483] Algo bellman_ford step 2562 current loss 0.124006, current_train_items 82016.
I0304 19:29:31.428671 22849303695488 run.py:483] Algo bellman_ford step 2563 current loss 0.211350, current_train_items 82048.
I0304 19:29:31.463017 22849303695488 run.py:483] Algo bellman_ford step 2564 current loss 0.154106, current_train_items 82080.
I0304 19:29:31.482793 22849303695488 run.py:483] Algo bellman_ford step 2565 current loss 0.005588, current_train_items 82112.
I0304 19:29:31.499776 22849303695488 run.py:483] Algo bellman_ford step 2566 current loss 0.045349, current_train_items 82144.
I0304 19:29:31.523692 22849303695488 run.py:483] Algo bellman_ford step 2567 current loss 0.071490, current_train_items 82176.
I0304 19:29:31.555840 22849303695488 run.py:483] Algo bellman_ford step 2568 current loss 0.138506, current_train_items 82208.
I0304 19:29:31.589249 22849303695488 run.py:483] Algo bellman_ford step 2569 current loss 0.103714, current_train_items 82240.
I0304 19:29:31.609757 22849303695488 run.py:483] Algo bellman_ford step 2570 current loss 0.008550, current_train_items 82272.
I0304 19:29:31.626451 22849303695488 run.py:483] Algo bellman_ford step 2571 current loss 0.049084, current_train_items 82304.
I0304 19:29:31.649177 22849303695488 run.py:483] Algo bellman_ford step 2572 current loss 0.065956, current_train_items 82336.
I0304 19:29:31.680649 22849303695488 run.py:483] Algo bellman_ford step 2573 current loss 0.083017, current_train_items 82368.
I0304 19:29:31.716189 22849303695488 run.py:483] Algo bellman_ford step 2574 current loss 0.130646, current_train_items 82400.
I0304 19:29:31.736985 22849303695488 run.py:483] Algo bellman_ford step 2575 current loss 0.020656, current_train_items 82432.
I0304 19:29:31.753417 22849303695488 run.py:483] Algo bellman_ford step 2576 current loss 0.115820, current_train_items 82464.
I0304 19:29:31.776999 22849303695488 run.py:483] Algo bellman_ford step 2577 current loss 0.142858, current_train_items 82496.
I0304 19:29:31.809550 22849303695488 run.py:483] Algo bellman_ford step 2578 current loss 0.259669, current_train_items 82528.
I0304 19:29:31.842629 22849303695488 run.py:483] Algo bellman_ford step 2579 current loss 0.192577, current_train_items 82560.
I0304 19:29:31.862759 22849303695488 run.py:483] Algo bellman_ford step 2580 current loss 0.015152, current_train_items 82592.
I0304 19:29:31.879404 22849303695488 run.py:483] Algo bellman_ford step 2581 current loss 0.021046, current_train_items 82624.
I0304 19:29:31.903309 22849303695488 run.py:483] Algo bellman_ford step 2582 current loss 0.130545, current_train_items 82656.
I0304 19:29:31.934643 22849303695488 run.py:483] Algo bellman_ford step 2583 current loss 0.186834, current_train_items 82688.
I0304 19:29:31.969806 22849303695488 run.py:483] Algo bellman_ford step 2584 current loss 0.221388, current_train_items 82720.
I0304 19:29:31.990017 22849303695488 run.py:483] Algo bellman_ford step 2585 current loss 0.007885, current_train_items 82752.
I0304 19:29:32.006791 22849303695488 run.py:483] Algo bellman_ford step 2586 current loss 0.027969, current_train_items 82784.
I0304 19:29:32.030628 22849303695488 run.py:483] Algo bellman_ford step 2587 current loss 0.039193, current_train_items 82816.
I0304 19:29:32.062154 22849303695488 run.py:483] Algo bellman_ford step 2588 current loss 0.118402, current_train_items 82848.
I0304 19:29:32.096763 22849303695488 run.py:483] Algo bellman_ford step 2589 current loss 0.168800, current_train_items 82880.
I0304 19:29:32.116947 22849303695488 run.py:483] Algo bellman_ford step 2590 current loss 0.005871, current_train_items 82912.
I0304 19:29:32.133825 22849303695488 run.py:483] Algo bellman_ford step 2591 current loss 0.043496, current_train_items 82944.
I0304 19:29:32.158420 22849303695488 run.py:483] Algo bellman_ford step 2592 current loss 0.080383, current_train_items 82976.
I0304 19:29:32.190957 22849303695488 run.py:483] Algo bellman_ford step 2593 current loss 0.091641, current_train_items 83008.
I0304 19:29:32.223270 22849303695488 run.py:483] Algo bellman_ford step 2594 current loss 0.176263, current_train_items 83040.
I0304 19:29:32.243001 22849303695488 run.py:483] Algo bellman_ford step 2595 current loss 0.018196, current_train_items 83072.
I0304 19:29:32.259933 22849303695488 run.py:483] Algo bellman_ford step 2596 current loss 0.075448, current_train_items 83104.
I0304 19:29:32.285177 22849303695488 run.py:483] Algo bellman_ford step 2597 current loss 0.061674, current_train_items 83136.
I0304 19:29:32.317428 22849303695488 run.py:483] Algo bellman_ford step 2598 current loss 0.077584, current_train_items 83168.
I0304 19:29:32.351372 22849303695488 run.py:483] Algo bellman_ford step 2599 current loss 0.090210, current_train_items 83200.
I0304 19:29:32.371864 22849303695488 run.py:483] Algo bellman_ford step 2600 current loss 0.013872, current_train_items 83232.
I0304 19:29:32.380154 22849303695488 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0304 19:29:32.380262 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.981, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:29:32.397439 22849303695488 run.py:483] Algo bellman_ford step 2601 current loss 0.021939, current_train_items 83264.
I0304 19:29:32.421657 22849303695488 run.py:483] Algo bellman_ford step 2602 current loss 0.058118, current_train_items 83296.
I0304 19:29:32.452183 22849303695488 run.py:483] Algo bellman_ford step 2603 current loss 0.069224, current_train_items 83328.
I0304 19:29:32.486576 22849303695488 run.py:483] Algo bellman_ford step 2604 current loss 0.097602, current_train_items 83360.
I0304 19:29:32.507101 22849303695488 run.py:483] Algo bellman_ford step 2605 current loss 0.027306, current_train_items 83392.
I0304 19:29:32.523327 22849303695488 run.py:483] Algo bellman_ford step 2606 current loss 0.016723, current_train_items 83424.
I0304 19:29:32.548149 22849303695488 run.py:483] Algo bellman_ford step 2607 current loss 0.070895, current_train_items 83456.
I0304 19:29:32.580634 22849303695488 run.py:483] Algo bellman_ford step 2608 current loss 0.130624, current_train_items 83488.
I0304 19:29:32.615442 22849303695488 run.py:483] Algo bellman_ford step 2609 current loss 0.167028, current_train_items 83520.
I0304 19:29:32.635714 22849303695488 run.py:483] Algo bellman_ford step 2610 current loss 0.011662, current_train_items 83552.
I0304 19:29:32.652198 22849303695488 run.py:483] Algo bellman_ford step 2611 current loss 0.027715, current_train_items 83584.
I0304 19:29:32.675992 22849303695488 run.py:483] Algo bellman_ford step 2612 current loss 0.056325, current_train_items 83616.
I0304 19:29:32.707357 22849303695488 run.py:483] Algo bellman_ford step 2613 current loss 0.100901, current_train_items 83648.
I0304 19:29:32.741021 22849303695488 run.py:483] Algo bellman_ford step 2614 current loss 0.123867, current_train_items 83680.
I0304 19:29:32.761426 22849303695488 run.py:483] Algo bellman_ford step 2615 current loss 0.006412, current_train_items 83712.
I0304 19:29:32.777731 22849303695488 run.py:483] Algo bellman_ford step 2616 current loss 0.027628, current_train_items 83744.
I0304 19:29:32.803326 22849303695488 run.py:483] Algo bellman_ford step 2617 current loss 0.081171, current_train_items 83776.
I0304 19:29:32.835937 22849303695488 run.py:483] Algo bellman_ford step 2618 current loss 0.093603, current_train_items 83808.
I0304 19:29:32.868857 22849303695488 run.py:483] Algo bellman_ford step 2619 current loss 0.107493, current_train_items 83840.
I0304 19:29:32.889042 22849303695488 run.py:483] Algo bellman_ford step 2620 current loss 0.007463, current_train_items 83872.
I0304 19:29:32.905645 22849303695488 run.py:483] Algo bellman_ford step 2621 current loss 0.020601, current_train_items 83904.
I0304 19:29:32.930189 22849303695488 run.py:483] Algo bellman_ford step 2622 current loss 0.078115, current_train_items 83936.
I0304 19:29:32.961635 22849303695488 run.py:483] Algo bellman_ford step 2623 current loss 0.067313, current_train_items 83968.
I0304 19:29:32.995145 22849303695488 run.py:483] Algo bellman_ford step 2624 current loss 0.087195, current_train_items 84000.
I0304 19:29:33.015295 22849303695488 run.py:483] Algo bellman_ford step 2625 current loss 0.013713, current_train_items 84032.
I0304 19:29:33.031831 22849303695488 run.py:483] Algo bellman_ford step 2626 current loss 0.036579, current_train_items 84064.
I0304 19:29:33.056298 22849303695488 run.py:483] Algo bellman_ford step 2627 current loss 0.067501, current_train_items 84096.
I0304 19:29:33.088124 22849303695488 run.py:483] Algo bellman_ford step 2628 current loss 0.139333, current_train_items 84128.
I0304 19:29:33.122375 22849303695488 run.py:483] Algo bellman_ford step 2629 current loss 0.092981, current_train_items 84160.
I0304 19:29:33.142558 22849303695488 run.py:483] Algo bellman_ford step 2630 current loss 0.008062, current_train_items 84192.
I0304 19:29:33.159510 22849303695488 run.py:483] Algo bellman_ford step 2631 current loss 0.052638, current_train_items 84224.
I0304 19:29:33.183670 22849303695488 run.py:483] Algo bellman_ford step 2632 current loss 0.085379, current_train_items 84256.
I0304 19:29:33.214854 22849303695488 run.py:483] Algo bellman_ford step 2633 current loss 0.079887, current_train_items 84288.
I0304 19:29:33.250954 22849303695488 run.py:483] Algo bellman_ford step 2634 current loss 0.197365, current_train_items 84320.
I0304 19:29:33.271524 22849303695488 run.py:483] Algo bellman_ford step 2635 current loss 0.015392, current_train_items 84352.
I0304 19:29:33.287954 22849303695488 run.py:483] Algo bellman_ford step 2636 current loss 0.057227, current_train_items 84384.
I0304 19:29:33.312751 22849303695488 run.py:483] Algo bellman_ford step 2637 current loss 0.134116, current_train_items 84416.
I0304 19:29:33.345050 22849303695488 run.py:483] Algo bellman_ford step 2638 current loss 0.141376, current_train_items 84448.
I0304 19:29:33.379753 22849303695488 run.py:483] Algo bellman_ford step 2639 current loss 0.166011, current_train_items 84480.
I0304 19:29:33.399901 22849303695488 run.py:483] Algo bellman_ford step 2640 current loss 0.005520, current_train_items 84512.
I0304 19:29:33.416456 22849303695488 run.py:483] Algo bellman_ford step 2641 current loss 0.025896, current_train_items 84544.
I0304 19:29:33.439870 22849303695488 run.py:483] Algo bellman_ford step 2642 current loss 0.066081, current_train_items 84576.
I0304 19:29:33.471172 22849303695488 run.py:483] Algo bellman_ford step 2643 current loss 0.152950, current_train_items 84608.
I0304 19:29:33.506448 22849303695488 run.py:483] Algo bellman_ford step 2644 current loss 0.165961, current_train_items 84640.
I0304 19:29:33.526437 22849303695488 run.py:483] Algo bellman_ford step 2645 current loss 0.011400, current_train_items 84672.
I0304 19:29:33.542816 22849303695488 run.py:483] Algo bellman_ford step 2646 current loss 0.045885, current_train_items 84704.
I0304 19:29:33.567081 22849303695488 run.py:483] Algo bellman_ford step 2647 current loss 0.089088, current_train_items 84736.
I0304 19:29:33.599041 22849303695488 run.py:483] Algo bellman_ford step 2648 current loss 0.091670, current_train_items 84768.
I0304 19:29:33.635649 22849303695488 run.py:483] Algo bellman_ford step 2649 current loss 0.177882, current_train_items 84800.
I0304 19:29:33.655737 22849303695488 run.py:483] Algo bellman_ford step 2650 current loss 0.010434, current_train_items 84832.
I0304 19:29:33.664066 22849303695488 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0304 19:29:33.664177 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.981, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:29:33.681102 22849303695488 run.py:483] Algo bellman_ford step 2651 current loss 0.031123, current_train_items 84864.
I0304 19:29:33.705346 22849303695488 run.py:483] Algo bellman_ford step 2652 current loss 0.070825, current_train_items 84896.
I0304 19:29:33.738301 22849303695488 run.py:483] Algo bellman_ford step 2653 current loss 0.092895, current_train_items 84928.
I0304 19:29:33.772699 22849303695488 run.py:483] Algo bellman_ford step 2654 current loss 0.088704, current_train_items 84960.
I0304 19:29:33.792742 22849303695488 run.py:483] Algo bellman_ford step 2655 current loss 0.019186, current_train_items 84992.
I0304 19:29:33.809195 22849303695488 run.py:483] Algo bellman_ford step 2656 current loss 0.042964, current_train_items 85024.
I0304 19:29:33.833284 22849303695488 run.py:483] Algo bellman_ford step 2657 current loss 0.059071, current_train_items 85056.
I0304 19:29:33.864149 22849303695488 run.py:483] Algo bellman_ford step 2658 current loss 0.056396, current_train_items 85088.
I0304 19:29:33.900638 22849303695488 run.py:483] Algo bellman_ford step 2659 current loss 0.132423, current_train_items 85120.
I0304 19:29:33.921333 22849303695488 run.py:483] Algo bellman_ford step 2660 current loss 0.005138, current_train_items 85152.
I0304 19:29:33.938553 22849303695488 run.py:483] Algo bellman_ford step 2661 current loss 0.055148, current_train_items 85184.
I0304 19:29:33.962234 22849303695488 run.py:483] Algo bellman_ford step 2662 current loss 0.114445, current_train_items 85216.
I0304 19:29:33.993897 22849303695488 run.py:483] Algo bellman_ford step 2663 current loss 0.165121, current_train_items 85248.
I0304 19:29:34.030661 22849303695488 run.py:483] Algo bellman_ford step 2664 current loss 0.167956, current_train_items 85280.
I0304 19:29:34.050614 22849303695488 run.py:483] Algo bellman_ford step 2665 current loss 0.007527, current_train_items 85312.
I0304 19:29:34.067227 22849303695488 run.py:483] Algo bellman_ford step 2666 current loss 0.072016, current_train_items 85344.
I0304 19:29:34.092216 22849303695488 run.py:483] Algo bellman_ford step 2667 current loss 0.078114, current_train_items 85376.
I0304 19:29:34.124056 22849303695488 run.py:483] Algo bellman_ford step 2668 current loss 0.162618, current_train_items 85408.
I0304 19:29:34.157522 22849303695488 run.py:483] Algo bellman_ford step 2669 current loss 0.076648, current_train_items 85440.
I0304 19:29:34.177688 22849303695488 run.py:483] Algo bellman_ford step 2670 current loss 0.011168, current_train_items 85472.
I0304 19:29:34.194592 22849303695488 run.py:483] Algo bellman_ford step 2671 current loss 0.025379, current_train_items 85504.
I0304 19:29:34.218591 22849303695488 run.py:483] Algo bellman_ford step 2672 current loss 0.085273, current_train_items 85536.
I0304 19:29:34.251155 22849303695488 run.py:483] Algo bellman_ford step 2673 current loss 0.164775, current_train_items 85568.
I0304 19:29:34.286857 22849303695488 run.py:483] Algo bellman_ford step 2674 current loss 0.120705, current_train_items 85600.
I0304 19:29:34.307203 22849303695488 run.py:483] Algo bellman_ford step 2675 current loss 0.010759, current_train_items 85632.
I0304 19:29:34.323904 22849303695488 run.py:483] Algo bellman_ford step 2676 current loss 0.071042, current_train_items 85664.
I0304 19:29:34.348904 22849303695488 run.py:483] Algo bellman_ford step 2677 current loss 0.093494, current_train_items 85696.
I0304 19:29:34.379370 22849303695488 run.py:483] Algo bellman_ford step 2678 current loss 0.087639, current_train_items 85728.
I0304 19:29:34.412744 22849303695488 run.py:483] Algo bellman_ford step 2679 current loss 0.083304, current_train_items 85760.
I0304 19:29:34.432841 22849303695488 run.py:483] Algo bellman_ford step 2680 current loss 0.005663, current_train_items 85792.
I0304 19:29:34.449551 22849303695488 run.py:483] Algo bellman_ford step 2681 current loss 0.037865, current_train_items 85824.
I0304 19:29:34.473833 22849303695488 run.py:483] Algo bellman_ford step 2682 current loss 0.105154, current_train_items 85856.
I0304 19:29:34.506262 22849303695488 run.py:483] Algo bellman_ford step 2683 current loss 0.094625, current_train_items 85888.
I0304 19:29:34.540693 22849303695488 run.py:483] Algo bellman_ford step 2684 current loss 0.115856, current_train_items 85920.
I0304 19:29:34.561129 22849303695488 run.py:483] Algo bellman_ford step 2685 current loss 0.007032, current_train_items 85952.
I0304 19:29:34.577624 22849303695488 run.py:483] Algo bellman_ford step 2686 current loss 0.062831, current_train_items 85984.
I0304 19:29:34.601881 22849303695488 run.py:483] Algo bellman_ford step 2687 current loss 0.122037, current_train_items 86016.
I0304 19:29:34.633354 22849303695488 run.py:483] Algo bellman_ford step 2688 current loss 0.093767, current_train_items 86048.
I0304 19:29:34.666302 22849303695488 run.py:483] Algo bellman_ford step 2689 current loss 0.074885, current_train_items 86080.
I0304 19:29:34.686647 22849303695488 run.py:483] Algo bellman_ford step 2690 current loss 0.024160, current_train_items 86112.
I0304 19:29:34.703575 22849303695488 run.py:483] Algo bellman_ford step 2691 current loss 0.023994, current_train_items 86144.
I0304 19:29:34.728585 22849303695488 run.py:483] Algo bellman_ford step 2692 current loss 0.110958, current_train_items 86176.
I0304 19:29:34.761154 22849303695488 run.py:483] Algo bellman_ford step 2693 current loss 0.118598, current_train_items 86208.
I0304 19:29:34.796129 22849303695488 run.py:483] Algo bellman_ford step 2694 current loss 0.115626, current_train_items 86240.
I0304 19:29:34.816637 22849303695488 run.py:483] Algo bellman_ford step 2695 current loss 0.007203, current_train_items 86272.
I0304 19:29:34.832964 22849303695488 run.py:483] Algo bellman_ford step 2696 current loss 0.023804, current_train_items 86304.
I0304 19:29:34.857445 22849303695488 run.py:483] Algo bellman_ford step 2697 current loss 0.137586, current_train_items 86336.
I0304 19:29:34.888659 22849303695488 run.py:483] Algo bellman_ford step 2698 current loss 0.204866, current_train_items 86368.
I0304 19:29:34.921133 22849303695488 run.py:483] Algo bellman_ford step 2699 current loss 0.190437, current_train_items 86400.
I0304 19:29:34.941267 22849303695488 run.py:483] Algo bellman_ford step 2700 current loss 0.014131, current_train_items 86432.
I0304 19:29:34.949662 22849303695488 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.9599609375, 'score': 0.9599609375, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0304 19:29:34.949768 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.981, current avg val score is 0.960, val scores are: bellman_ford: 0.960
I0304 19:29:34.966853 22849303695488 run.py:483] Algo bellman_ford step 2701 current loss 0.037171, current_train_items 86464.
I0304 19:29:34.991480 22849303695488 run.py:483] Algo bellman_ford step 2702 current loss 0.080721, current_train_items 86496.
I0304 19:29:35.025155 22849303695488 run.py:483] Algo bellman_ford step 2703 current loss 0.131219, current_train_items 86528.
I0304 19:29:35.061994 22849303695488 run.py:483] Algo bellman_ford step 2704 current loss 0.109915, current_train_items 86560.
I0304 19:29:35.081984 22849303695488 run.py:483] Algo bellman_ford step 2705 current loss 0.009406, current_train_items 86592.
I0304 19:29:35.098148 22849303695488 run.py:483] Algo bellman_ford step 2706 current loss 0.032393, current_train_items 86624.
I0304 19:29:35.122664 22849303695488 run.py:483] Algo bellman_ford step 2707 current loss 0.083515, current_train_items 86656.
I0304 19:29:35.152810 22849303695488 run.py:483] Algo bellman_ford step 2708 current loss 0.140515, current_train_items 86688.
I0304 19:29:35.185367 22849303695488 run.py:483] Algo bellman_ford step 2709 current loss 0.102715, current_train_items 86720.
I0304 19:29:35.205677 22849303695488 run.py:483] Algo bellman_ford step 2710 current loss 0.007343, current_train_items 86752.
I0304 19:29:35.222577 22849303695488 run.py:483] Algo bellman_ford step 2711 current loss 0.040565, current_train_items 86784.
I0304 19:29:35.247440 22849303695488 run.py:483] Algo bellman_ford step 2712 current loss 0.051957, current_train_items 86816.
I0304 19:29:35.279544 22849303695488 run.py:483] Algo bellman_ford step 2713 current loss 0.092911, current_train_items 86848.
I0304 19:29:35.316108 22849303695488 run.py:483] Algo bellman_ford step 2714 current loss 0.124011, current_train_items 86880.
I0304 19:29:35.336132 22849303695488 run.py:483] Algo bellman_ford step 2715 current loss 0.019356, current_train_items 86912.
I0304 19:29:35.352248 22849303695488 run.py:483] Algo bellman_ford step 2716 current loss 0.009005, current_train_items 86944.
I0304 19:29:35.377651 22849303695488 run.py:483] Algo bellman_ford step 2717 current loss 0.123511, current_train_items 86976.
I0304 19:29:35.407604 22849303695488 run.py:483] Algo bellman_ford step 2718 current loss 0.082936, current_train_items 87008.
I0304 19:29:35.441768 22849303695488 run.py:483] Algo bellman_ford step 2719 current loss 0.099430, current_train_items 87040.
I0304 19:29:35.461584 22849303695488 run.py:483] Algo bellman_ford step 2720 current loss 0.009792, current_train_items 87072.
I0304 19:29:35.478130 22849303695488 run.py:483] Algo bellman_ford step 2721 current loss 0.095581, current_train_items 87104.
I0304 19:29:35.502378 22849303695488 run.py:483] Algo bellman_ford step 2722 current loss 0.127432, current_train_items 87136.
I0304 19:29:35.535017 22849303695488 run.py:483] Algo bellman_ford step 2723 current loss 0.100062, current_train_items 87168.
I0304 19:29:35.568845 22849303695488 run.py:483] Algo bellman_ford step 2724 current loss 0.108714, current_train_items 87200.
I0304 19:29:35.588722 22849303695488 run.py:483] Algo bellman_ford step 2725 current loss 0.009026, current_train_items 87232.
I0304 19:29:35.605110 22849303695488 run.py:483] Algo bellman_ford step 2726 current loss 0.086202, current_train_items 87264.
I0304 19:29:35.629774 22849303695488 run.py:483] Algo bellman_ford step 2727 current loss 0.133421, current_train_items 87296.
I0304 19:29:35.661690 22849303695488 run.py:483] Algo bellman_ford step 2728 current loss 0.116186, current_train_items 87328.
I0304 19:29:35.695447 22849303695488 run.py:483] Algo bellman_ford step 2729 current loss 0.088501, current_train_items 87360.
I0304 19:29:35.715641 22849303695488 run.py:483] Algo bellman_ford step 2730 current loss 0.007314, current_train_items 87392.
I0304 19:29:35.732175 22849303695488 run.py:483] Algo bellman_ford step 2731 current loss 0.019267, current_train_items 87424.
I0304 19:29:35.755452 22849303695488 run.py:483] Algo bellman_ford step 2732 current loss 0.052917, current_train_items 87456.
I0304 19:29:35.785910 22849303695488 run.py:483] Algo bellman_ford step 2733 current loss 0.095142, current_train_items 87488.
I0304 19:29:35.820184 22849303695488 run.py:483] Algo bellman_ford step 2734 current loss 0.106987, current_train_items 87520.
I0304 19:29:35.840576 22849303695488 run.py:483] Algo bellman_ford step 2735 current loss 0.028678, current_train_items 87552.
I0304 19:29:35.856656 22849303695488 run.py:483] Algo bellman_ford step 2736 current loss 0.040025, current_train_items 87584.
I0304 19:29:35.880995 22849303695488 run.py:483] Algo bellman_ford step 2737 current loss 0.108196, current_train_items 87616.
I0304 19:29:35.911704 22849303695488 run.py:483] Algo bellman_ford step 2738 current loss 0.156173, current_train_items 87648.
I0304 19:29:35.945375 22849303695488 run.py:483] Algo bellman_ford step 2739 current loss 0.173250, current_train_items 87680.
I0304 19:29:35.965346 22849303695488 run.py:483] Algo bellman_ford step 2740 current loss 0.017437, current_train_items 87712.
I0304 19:29:35.981684 22849303695488 run.py:483] Algo bellman_ford step 2741 current loss 0.029464, current_train_items 87744.
I0304 19:29:36.005932 22849303695488 run.py:483] Algo bellman_ford step 2742 current loss 0.083386, current_train_items 87776.
I0304 19:29:36.039080 22849303695488 run.py:483] Algo bellman_ford step 2743 current loss 0.116502, current_train_items 87808.
I0304 19:29:36.072881 22849303695488 run.py:483] Algo bellman_ford step 2744 current loss 0.206105, current_train_items 87840.
I0304 19:29:36.093085 22849303695488 run.py:483] Algo bellman_ford step 2745 current loss 0.010517, current_train_items 87872.
I0304 19:29:36.109433 22849303695488 run.py:483] Algo bellman_ford step 2746 current loss 0.029853, current_train_items 87904.
I0304 19:29:36.133995 22849303695488 run.py:483] Algo bellman_ford step 2747 current loss 0.058194, current_train_items 87936.
I0304 19:29:36.166449 22849303695488 run.py:483] Algo bellman_ford step 2748 current loss 0.114522, current_train_items 87968.
I0304 19:29:36.199829 22849303695488 run.py:483] Algo bellman_ford step 2749 current loss 0.121688, current_train_items 88000.
I0304 19:29:36.220067 22849303695488 run.py:483] Algo bellman_ford step 2750 current loss 0.016621, current_train_items 88032.
I0304 19:29:36.228382 22849303695488 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0304 19:29:36.228490 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.981, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:29:36.245731 22849303695488 run.py:483] Algo bellman_ford step 2751 current loss 0.034211, current_train_items 88064.
I0304 19:29:36.270644 22849303695488 run.py:483] Algo bellman_ford step 2752 current loss 0.088724, current_train_items 88096.
I0304 19:29:36.303064 22849303695488 run.py:483] Algo bellman_ford step 2753 current loss 0.106447, current_train_items 88128.
I0304 19:29:36.336879 22849303695488 run.py:483] Algo bellman_ford step 2754 current loss 0.091821, current_train_items 88160.
I0304 19:29:36.356924 22849303695488 run.py:483] Algo bellman_ford step 2755 current loss 0.008364, current_train_items 88192.
I0304 19:29:36.373478 22849303695488 run.py:483] Algo bellman_ford step 2756 current loss 0.032325, current_train_items 88224.
I0304 19:29:36.397131 22849303695488 run.py:483] Algo bellman_ford step 2757 current loss 0.036254, current_train_items 88256.
I0304 19:29:36.429151 22849303695488 run.py:483] Algo bellman_ford step 2758 current loss 0.089204, current_train_items 88288.
I0304 19:29:36.465954 22849303695488 run.py:483] Algo bellman_ford step 2759 current loss 0.117389, current_train_items 88320.
I0304 19:29:36.486076 22849303695488 run.py:483] Algo bellman_ford step 2760 current loss 0.007897, current_train_items 88352.
I0304 19:29:36.502993 22849303695488 run.py:483] Algo bellman_ford step 2761 current loss 0.054583, current_train_items 88384.
I0304 19:29:36.527480 22849303695488 run.py:483] Algo bellman_ford step 2762 current loss 0.160003, current_train_items 88416.
I0304 19:29:36.559540 22849303695488 run.py:483] Algo bellman_ford step 2763 current loss 0.157436, current_train_items 88448.
I0304 19:29:36.593464 22849303695488 run.py:483] Algo bellman_ford step 2764 current loss 0.127754, current_train_items 88480.
I0304 19:29:36.613388 22849303695488 run.py:483] Algo bellman_ford step 2765 current loss 0.017586, current_train_items 88512.
I0304 19:29:36.630282 22849303695488 run.py:483] Algo bellman_ford step 2766 current loss 0.032261, current_train_items 88544.
I0304 19:29:36.654752 22849303695488 run.py:483] Algo bellman_ford step 2767 current loss 0.132613, current_train_items 88576.
I0304 19:29:36.687225 22849303695488 run.py:483] Algo bellman_ford step 2768 current loss 0.175175, current_train_items 88608.
I0304 19:29:36.721096 22849303695488 run.py:483] Algo bellman_ford step 2769 current loss 0.159087, current_train_items 88640.
I0304 19:29:36.741096 22849303695488 run.py:483] Algo bellman_ford step 2770 current loss 0.010266, current_train_items 88672.
I0304 19:29:36.758110 22849303695488 run.py:483] Algo bellman_ford step 2771 current loss 0.063801, current_train_items 88704.
I0304 19:29:36.782549 22849303695488 run.py:483] Algo bellman_ford step 2772 current loss 0.154166, current_train_items 88736.
I0304 19:29:36.813410 22849303695488 run.py:483] Algo bellman_ford step 2773 current loss 0.128164, current_train_items 88768.
I0304 19:29:36.847608 22849303695488 run.py:483] Algo bellman_ford step 2774 current loss 0.140313, current_train_items 88800.
I0304 19:29:36.867748 22849303695488 run.py:483] Algo bellman_ford step 2775 current loss 0.005683, current_train_items 88832.
I0304 19:29:36.884928 22849303695488 run.py:483] Algo bellman_ford step 2776 current loss 0.080915, current_train_items 88864.
I0304 19:29:36.908936 22849303695488 run.py:483] Algo bellman_ford step 2777 current loss 0.093898, current_train_items 88896.
I0304 19:29:36.940156 22849303695488 run.py:483] Algo bellman_ford step 2778 current loss 0.189507, current_train_items 88928.
I0304 19:29:36.974859 22849303695488 run.py:483] Algo bellman_ford step 2779 current loss 0.266899, current_train_items 88960.
I0304 19:29:36.994859 22849303695488 run.py:483] Algo bellman_ford step 2780 current loss 0.026628, current_train_items 88992.
I0304 19:29:37.011398 22849303695488 run.py:483] Algo bellman_ford step 2781 current loss 0.083418, current_train_items 89024.
I0304 19:29:37.036037 22849303695488 run.py:483] Algo bellman_ford step 2782 current loss 0.089869, current_train_items 89056.
I0304 19:29:37.067296 22849303695488 run.py:483] Algo bellman_ford step 2783 current loss 0.081766, current_train_items 89088.
I0304 19:29:37.102266 22849303695488 run.py:483] Algo bellman_ford step 2784 current loss 0.097604, current_train_items 89120.
I0304 19:29:37.123166 22849303695488 run.py:483] Algo bellman_ford step 2785 current loss 0.015759, current_train_items 89152.
I0304 19:29:37.139609 22849303695488 run.py:483] Algo bellman_ford step 2786 current loss 0.048086, current_train_items 89184.
I0304 19:29:37.162472 22849303695488 run.py:483] Algo bellman_ford step 2787 current loss 0.051068, current_train_items 89216.
I0304 19:29:37.193563 22849303695488 run.py:483] Algo bellman_ford step 2788 current loss 0.109038, current_train_items 89248.
I0304 19:29:37.224815 22849303695488 run.py:483] Algo bellman_ford step 2789 current loss 0.161957, current_train_items 89280.
I0304 19:29:37.245243 22849303695488 run.py:483] Algo bellman_ford step 2790 current loss 0.010513, current_train_items 89312.
I0304 19:29:37.262076 22849303695488 run.py:483] Algo bellman_ford step 2791 current loss 0.016247, current_train_items 89344.
I0304 19:29:37.286030 22849303695488 run.py:483] Algo bellman_ford step 2792 current loss 0.086123, current_train_items 89376.
I0304 19:29:37.316820 22849303695488 run.py:483] Algo bellman_ford step 2793 current loss 0.188084, current_train_items 89408.
I0304 19:29:37.352959 22849303695488 run.py:483] Algo bellman_ford step 2794 current loss 0.148109, current_train_items 89440.
I0304 19:29:37.373041 22849303695488 run.py:483] Algo bellman_ford step 2795 current loss 0.010325, current_train_items 89472.
I0304 19:29:37.390053 22849303695488 run.py:483] Algo bellman_ford step 2796 current loss 0.050859, current_train_items 89504.
I0304 19:29:37.413823 22849303695488 run.py:483] Algo bellman_ford step 2797 current loss 0.096628, current_train_items 89536.
I0304 19:29:37.446348 22849303695488 run.py:483] Algo bellman_ford step 2798 current loss 0.092806, current_train_items 89568.
I0304 19:29:37.480365 22849303695488 run.py:483] Algo bellman_ford step 2799 current loss 0.088284, current_train_items 89600.
I0304 19:29:37.500661 22849303695488 run.py:483] Algo bellman_ford step 2800 current loss 0.006909, current_train_items 89632.
I0304 19:29:37.508643 22849303695488 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0304 19:29:37.508752 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.981, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:29:37.525839 22849303695488 run.py:483] Algo bellman_ford step 2801 current loss 0.021896, current_train_items 89664.
I0304 19:29:37.551322 22849303695488 run.py:483] Algo bellman_ford step 2802 current loss 0.156196, current_train_items 89696.
I0304 19:29:37.583772 22849303695488 run.py:483] Algo bellman_ford step 2803 current loss 0.163463, current_train_items 89728.
I0304 19:29:37.618108 22849303695488 run.py:483] Algo bellman_ford step 2804 current loss 0.111339, current_train_items 89760.
I0304 19:29:37.638363 22849303695488 run.py:483] Algo bellman_ford step 2805 current loss 0.008978, current_train_items 89792.
I0304 19:29:37.655024 22849303695488 run.py:483] Algo bellman_ford step 2806 current loss 0.081688, current_train_items 89824.
I0304 19:29:37.678867 22849303695488 run.py:483] Algo bellman_ford step 2807 current loss 0.055981, current_train_items 89856.
I0304 19:29:37.711048 22849303695488 run.py:483] Algo bellman_ford step 2808 current loss 0.147905, current_train_items 89888.
I0304 19:29:37.745819 22849303695488 run.py:483] Algo bellman_ford step 2809 current loss 0.125573, current_train_items 89920.
I0304 19:29:37.765846 22849303695488 run.py:483] Algo bellman_ford step 2810 current loss 0.007507, current_train_items 89952.
I0304 19:29:37.782808 22849303695488 run.py:483] Algo bellman_ford step 2811 current loss 0.105532, current_train_items 89984.
I0304 19:29:37.806115 22849303695488 run.py:483] Algo bellman_ford step 2812 current loss 0.055015, current_train_items 90016.
I0304 19:29:37.838621 22849303695488 run.py:483] Algo bellman_ford step 2813 current loss 0.128241, current_train_items 90048.
I0304 19:29:37.871807 22849303695488 run.py:483] Algo bellman_ford step 2814 current loss 0.106161, current_train_items 90080.
I0304 19:29:37.891581 22849303695488 run.py:483] Algo bellman_ford step 2815 current loss 0.010448, current_train_items 90112.
I0304 19:29:37.908401 22849303695488 run.py:483] Algo bellman_ford step 2816 current loss 0.038611, current_train_items 90144.
I0304 19:29:37.932824 22849303695488 run.py:483] Algo bellman_ford step 2817 current loss 0.084322, current_train_items 90176.
I0304 19:29:37.961620 22849303695488 run.py:483] Algo bellman_ford step 2818 current loss 0.057674, current_train_items 90208.
I0304 19:29:37.995615 22849303695488 run.py:483] Algo bellman_ford step 2819 current loss 0.085852, current_train_items 90240.
I0304 19:29:38.015386 22849303695488 run.py:483] Algo bellman_ford step 2820 current loss 0.007525, current_train_items 90272.
I0304 19:29:38.031764 22849303695488 run.py:483] Algo bellman_ford step 2821 current loss 0.056764, current_train_items 90304.
I0304 19:29:38.055833 22849303695488 run.py:483] Algo bellman_ford step 2822 current loss 0.122059, current_train_items 90336.
I0304 19:29:38.087161 22849303695488 run.py:483] Algo bellman_ford step 2823 current loss 0.202564, current_train_items 90368.
I0304 19:29:38.123452 22849303695488 run.py:483] Algo bellman_ford step 2824 current loss 0.166166, current_train_items 90400.
I0304 19:29:38.143539 22849303695488 run.py:483] Algo bellman_ford step 2825 current loss 0.006885, current_train_items 90432.
I0304 19:29:38.159809 22849303695488 run.py:483] Algo bellman_ford step 2826 current loss 0.046219, current_train_items 90464.
I0304 19:29:38.184066 22849303695488 run.py:483] Algo bellman_ford step 2827 current loss 0.199075, current_train_items 90496.
I0304 19:29:38.215790 22849303695488 run.py:483] Algo bellman_ford step 2828 current loss 0.137080, current_train_items 90528.
I0304 19:29:38.248783 22849303695488 run.py:483] Algo bellman_ford step 2829 current loss 0.108964, current_train_items 90560.
I0304 19:29:38.268712 22849303695488 run.py:483] Algo bellman_ford step 2830 current loss 0.012051, current_train_items 90592.
I0304 19:29:38.285554 22849303695488 run.py:483] Algo bellman_ford step 2831 current loss 0.069332, current_train_items 90624.
I0304 19:29:38.309405 22849303695488 run.py:483] Algo bellman_ford step 2832 current loss 0.137335, current_train_items 90656.
I0304 19:29:38.340044 22849303695488 run.py:483] Algo bellman_ford step 2833 current loss 0.070298, current_train_items 90688.
I0304 19:29:38.374025 22849303695488 run.py:483] Algo bellman_ford step 2834 current loss 0.116765, current_train_items 90720.
I0304 19:29:38.393903 22849303695488 run.py:483] Algo bellman_ford step 2835 current loss 0.039837, current_train_items 90752.
I0304 19:29:38.410069 22849303695488 run.py:483] Algo bellman_ford step 2836 current loss 0.090139, current_train_items 90784.
I0304 19:29:38.433598 22849303695488 run.py:483] Algo bellman_ford step 2837 current loss 0.153451, current_train_items 90816.
I0304 19:29:38.464961 22849303695488 run.py:483] Algo bellman_ford step 2838 current loss 0.266473, current_train_items 90848.
I0304 19:29:38.501284 22849303695488 run.py:483] Algo bellman_ford step 2839 current loss 0.347267, current_train_items 90880.
I0304 19:29:38.521281 22849303695488 run.py:483] Algo bellman_ford step 2840 current loss 0.012191, current_train_items 90912.
I0304 19:29:38.537660 22849303695488 run.py:483] Algo bellman_ford step 2841 current loss 0.028124, current_train_items 90944.
I0304 19:29:38.561904 22849303695488 run.py:483] Algo bellman_ford step 2842 current loss 0.073250, current_train_items 90976.
I0304 19:29:38.594312 22849303695488 run.py:483] Algo bellman_ford step 2843 current loss 0.171342, current_train_items 91008.
I0304 19:29:38.629292 22849303695488 run.py:483] Algo bellman_ford step 2844 current loss 0.168796, current_train_items 91040.
I0304 19:29:38.649508 22849303695488 run.py:483] Algo bellman_ford step 2845 current loss 0.008280, current_train_items 91072.
I0304 19:29:38.666452 22849303695488 run.py:483] Algo bellman_ford step 2846 current loss 0.049688, current_train_items 91104.
I0304 19:29:38.690240 22849303695488 run.py:483] Algo bellman_ford step 2847 current loss 0.034399, current_train_items 91136.
I0304 19:29:38.721627 22849303695488 run.py:483] Algo bellman_ford step 2848 current loss 0.095799, current_train_items 91168.
I0304 19:29:38.757212 22849303695488 run.py:483] Algo bellman_ford step 2849 current loss 0.119435, current_train_items 91200.
I0304 19:29:38.777214 22849303695488 run.py:483] Algo bellman_ford step 2850 current loss 0.022834, current_train_items 91232.
I0304 19:29:38.785599 22849303695488 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0304 19:29:38.785707 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.981, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:29:38.802263 22849303695488 run.py:483] Algo bellman_ford step 2851 current loss 0.016048, current_train_items 91264.
I0304 19:29:38.826507 22849303695488 run.py:483] Algo bellman_ford step 2852 current loss 0.044030, current_train_items 91296.
I0304 19:29:38.859108 22849303695488 run.py:483] Algo bellman_ford step 2853 current loss 0.071742, current_train_items 91328.
I0304 19:29:38.893418 22849303695488 run.py:483] Algo bellman_ford step 2854 current loss 0.114611, current_train_items 91360.
I0304 19:29:38.913394 22849303695488 run.py:483] Algo bellman_ford step 2855 current loss 0.006063, current_train_items 91392.
I0304 19:29:38.929877 22849303695488 run.py:483] Algo bellman_ford step 2856 current loss 0.041508, current_train_items 91424.
I0304 19:29:38.954298 22849303695488 run.py:483] Algo bellman_ford step 2857 current loss 0.098109, current_train_items 91456.
I0304 19:29:38.983822 22849303695488 run.py:483] Algo bellman_ford step 2858 current loss 0.072203, current_train_items 91488.
I0304 19:29:39.016656 22849303695488 run.py:483] Algo bellman_ford step 2859 current loss 0.069270, current_train_items 91520.
I0304 19:29:39.037065 22849303695488 run.py:483] Algo bellman_ford step 2860 current loss 0.010486, current_train_items 91552.
I0304 19:29:39.053778 22849303695488 run.py:483] Algo bellman_ford step 2861 current loss 0.033851, current_train_items 91584.
I0304 19:29:39.077779 22849303695488 run.py:483] Algo bellman_ford step 2862 current loss 0.077214, current_train_items 91616.
I0304 19:29:39.109953 22849303695488 run.py:483] Algo bellman_ford step 2863 current loss 0.120493, current_train_items 91648.
I0304 19:29:39.141343 22849303695488 run.py:483] Algo bellman_ford step 2864 current loss 0.109660, current_train_items 91680.
I0304 19:29:39.161524 22849303695488 run.py:483] Algo bellman_ford step 2865 current loss 0.016826, current_train_items 91712.
I0304 19:29:39.178189 22849303695488 run.py:483] Algo bellman_ford step 2866 current loss 0.045201, current_train_items 91744.
I0304 19:29:39.202091 22849303695488 run.py:483] Algo bellman_ford step 2867 current loss 0.156218, current_train_items 91776.
I0304 19:29:39.232725 22849303695488 run.py:483] Algo bellman_ford step 2868 current loss 0.085359, current_train_items 91808.
I0304 19:29:39.267607 22849303695488 run.py:483] Algo bellman_ford step 2869 current loss 0.132770, current_train_items 91840.
I0304 19:29:39.288267 22849303695488 run.py:483] Algo bellman_ford step 2870 current loss 0.006531, current_train_items 91872.
I0304 19:29:39.304908 22849303695488 run.py:483] Algo bellman_ford step 2871 current loss 0.043576, current_train_items 91904.
I0304 19:29:39.329635 22849303695488 run.py:483] Algo bellman_ford step 2872 current loss 0.112171, current_train_items 91936.
I0304 19:29:39.362087 22849303695488 run.py:483] Algo bellman_ford step 2873 current loss 0.109614, current_train_items 91968.
I0304 19:29:39.395333 22849303695488 run.py:483] Algo bellman_ford step 2874 current loss 0.114059, current_train_items 92000.
I0304 19:29:39.415673 22849303695488 run.py:483] Algo bellman_ford step 2875 current loss 0.006354, current_train_items 92032.
I0304 19:29:39.432424 22849303695488 run.py:483] Algo bellman_ford step 2876 current loss 0.025171, current_train_items 92064.
I0304 19:29:39.455938 22849303695488 run.py:483] Algo bellman_ford step 2877 current loss 0.070470, current_train_items 92096.
I0304 19:29:39.488591 22849303695488 run.py:483] Algo bellman_ford step 2878 current loss 0.236187, current_train_items 92128.
I0304 19:29:39.521830 22849303695488 run.py:483] Algo bellman_ford step 2879 current loss 0.103651, current_train_items 92160.
I0304 19:29:39.541790 22849303695488 run.py:483] Algo bellman_ford step 2880 current loss 0.074177, current_train_items 92192.
I0304 19:29:39.558339 22849303695488 run.py:483] Algo bellman_ford step 2881 current loss 0.014196, current_train_items 92224.
I0304 19:29:39.582445 22849303695488 run.py:483] Algo bellman_ford step 2882 current loss 0.065859, current_train_items 92256.
I0304 19:29:39.614328 22849303695488 run.py:483] Algo bellman_ford step 2883 current loss 0.131625, current_train_items 92288.
I0304 19:29:39.646486 22849303695488 run.py:483] Algo bellman_ford step 2884 current loss 0.162297, current_train_items 92320.
I0304 19:29:39.666676 22849303695488 run.py:483] Algo bellman_ford step 2885 current loss 0.010923, current_train_items 92352.
I0304 19:29:39.683253 22849303695488 run.py:483] Algo bellman_ford step 2886 current loss 0.051286, current_train_items 92384.
I0304 19:29:39.707523 22849303695488 run.py:483] Algo bellman_ford step 2887 current loss 0.073025, current_train_items 92416.
I0304 19:29:39.738557 22849303695488 run.py:483] Algo bellman_ford step 2888 current loss 0.084592, current_train_items 92448.
I0304 19:29:39.773247 22849303695488 run.py:483] Algo bellman_ford step 2889 current loss 0.188660, current_train_items 92480.
I0304 19:29:39.793745 22849303695488 run.py:483] Algo bellman_ford step 2890 current loss 0.007927, current_train_items 92512.
I0304 19:29:39.810258 22849303695488 run.py:483] Algo bellman_ford step 2891 current loss 0.021545, current_train_items 92544.
I0304 19:29:39.835479 22849303695488 run.py:483] Algo bellman_ford step 2892 current loss 0.072961, current_train_items 92576.
I0304 19:29:39.865896 22849303695488 run.py:483] Algo bellman_ford step 2893 current loss 0.069643, current_train_items 92608.
I0304 19:29:39.900639 22849303695488 run.py:483] Algo bellman_ford step 2894 current loss 0.100461, current_train_items 92640.
I0304 19:29:39.920359 22849303695488 run.py:483] Algo bellman_ford step 2895 current loss 0.009510, current_train_items 92672.
I0304 19:29:39.937548 22849303695488 run.py:483] Algo bellman_ford step 2896 current loss 0.055270, current_train_items 92704.
I0304 19:29:39.960664 22849303695488 run.py:483] Algo bellman_ford step 2897 current loss 0.079912, current_train_items 92736.
I0304 19:29:39.991647 22849303695488 run.py:483] Algo bellman_ford step 2898 current loss 0.058723, current_train_items 92768.
I0304 19:29:40.023113 22849303695488 run.py:483] Algo bellman_ford step 2899 current loss 0.067661, current_train_items 92800.
I0304 19:29:40.043154 22849303695488 run.py:483] Algo bellman_ford step 2900 current loss 0.003383, current_train_items 92832.
I0304 19:29:40.050974 22849303695488 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0304 19:29:40.051094 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.981, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:40.068081 22849303695488 run.py:483] Algo bellman_ford step 2901 current loss 0.016320, current_train_items 92864.
I0304 19:29:40.092227 22849303695488 run.py:483] Algo bellman_ford step 2902 current loss 0.101879, current_train_items 92896.
I0304 19:29:40.123998 22849303695488 run.py:483] Algo bellman_ford step 2903 current loss 0.158290, current_train_items 92928.
I0304 19:29:40.159106 22849303695488 run.py:483] Algo bellman_ford step 2904 current loss 0.164083, current_train_items 92960.
I0304 19:29:40.179732 22849303695488 run.py:483] Algo bellman_ford step 2905 current loss 0.038461, current_train_items 92992.
I0304 19:29:40.196237 22849303695488 run.py:483] Algo bellman_ford step 2906 current loss 0.037513, current_train_items 93024.
I0304 19:29:40.220817 22849303695488 run.py:483] Algo bellman_ford step 2907 current loss 0.092483, current_train_items 93056.
I0304 19:29:40.253910 22849303695488 run.py:483] Algo bellman_ford step 2908 current loss 0.120663, current_train_items 93088.
I0304 19:29:40.288661 22849303695488 run.py:483] Algo bellman_ford step 2909 current loss 0.104365, current_train_items 93120.
I0304 19:29:40.308622 22849303695488 run.py:483] Algo bellman_ford step 2910 current loss 0.034489, current_train_items 93152.
I0304 19:29:40.325666 22849303695488 run.py:483] Algo bellman_ford step 2911 current loss 0.054228, current_train_items 93184.
I0304 19:29:40.350033 22849303695488 run.py:483] Algo bellman_ford step 2912 current loss 0.076912, current_train_items 93216.
I0304 19:29:40.381403 22849303695488 run.py:483] Algo bellman_ford step 2913 current loss 0.085365, current_train_items 93248.
I0304 19:29:40.416544 22849303695488 run.py:483] Algo bellman_ford step 2914 current loss 0.163817, current_train_items 93280.
I0304 19:29:40.436444 22849303695488 run.py:483] Algo bellman_ford step 2915 current loss 0.007396, current_train_items 93312.
I0304 19:29:40.453328 22849303695488 run.py:483] Algo bellman_ford step 2916 current loss 0.024085, current_train_items 93344.
I0304 19:29:40.477161 22849303695488 run.py:483] Algo bellman_ford step 2917 current loss 0.094484, current_train_items 93376.
I0304 19:29:40.509872 22849303695488 run.py:483] Algo bellman_ford step 2918 current loss 0.130999, current_train_items 93408.
I0304 19:29:40.545599 22849303695488 run.py:483] Algo bellman_ford step 2919 current loss 0.141136, current_train_items 93440.
I0304 19:29:40.565968 22849303695488 run.py:483] Algo bellman_ford step 2920 current loss 0.004932, current_train_items 93472.
I0304 19:29:40.582617 22849303695488 run.py:483] Algo bellman_ford step 2921 current loss 0.048503, current_train_items 93504.
I0304 19:29:40.607830 22849303695488 run.py:483] Algo bellman_ford step 2922 current loss 0.108258, current_train_items 93536.
I0304 19:29:40.638530 22849303695488 run.py:483] Algo bellman_ford step 2923 current loss 0.091204, current_train_items 93568.
I0304 19:29:40.674710 22849303695488 run.py:483] Algo bellman_ford step 2924 current loss 0.149623, current_train_items 93600.
I0304 19:29:40.695057 22849303695488 run.py:483] Algo bellman_ford step 2925 current loss 0.033689, current_train_items 93632.
I0304 19:29:40.711675 22849303695488 run.py:483] Algo bellman_ford step 2926 current loss 0.088734, current_train_items 93664.
I0304 19:29:40.736812 22849303695488 run.py:483] Algo bellman_ford step 2927 current loss 0.128506, current_train_items 93696.
I0304 19:29:40.768956 22849303695488 run.py:483] Algo bellman_ford step 2928 current loss 0.138421, current_train_items 93728.
I0304 19:29:40.803684 22849303695488 run.py:483] Algo bellman_ford step 2929 current loss 0.096261, current_train_items 93760.
I0304 19:29:40.824094 22849303695488 run.py:483] Algo bellman_ford step 2930 current loss 0.010296, current_train_items 93792.
I0304 19:29:40.840518 22849303695488 run.py:483] Algo bellman_ford step 2931 current loss 0.027320, current_train_items 93824.
I0304 19:29:40.866151 22849303695488 run.py:483] Algo bellman_ford step 2932 current loss 0.160519, current_train_items 93856.
I0304 19:29:40.896968 22849303695488 run.py:483] Algo bellman_ford step 2933 current loss 0.113966, current_train_items 93888.
I0304 19:29:40.930559 22849303695488 run.py:483] Algo bellman_ford step 2934 current loss 0.127964, current_train_items 93920.
I0304 19:29:40.950740 22849303695488 run.py:483] Algo bellman_ford step 2935 current loss 0.007775, current_train_items 93952.
I0304 19:29:40.967063 22849303695488 run.py:483] Algo bellman_ford step 2936 current loss 0.028931, current_train_items 93984.
I0304 19:29:40.991574 22849303695488 run.py:483] Algo bellman_ford step 2937 current loss 0.099222, current_train_items 94016.
I0304 19:29:41.021431 22849303695488 run.py:483] Algo bellman_ford step 2938 current loss 0.072214, current_train_items 94048.
I0304 19:29:41.055365 22849303695488 run.py:483] Algo bellman_ford step 2939 current loss 0.178046, current_train_items 94080.
I0304 19:29:41.075171 22849303695488 run.py:483] Algo bellman_ford step 2940 current loss 0.052209, current_train_items 94112.
I0304 19:29:41.091991 22849303695488 run.py:483] Algo bellman_ford step 2941 current loss 0.042760, current_train_items 94144.
I0304 19:29:41.117103 22849303695488 run.py:483] Algo bellman_ford step 2942 current loss 0.102745, current_train_items 94176.
I0304 19:29:41.149654 22849303695488 run.py:483] Algo bellman_ford step 2943 current loss 0.132624, current_train_items 94208.
I0304 19:29:41.185253 22849303695488 run.py:483] Algo bellman_ford step 2944 current loss 0.124346, current_train_items 94240.
I0304 19:29:41.205580 22849303695488 run.py:483] Algo bellman_ford step 2945 current loss 0.034395, current_train_items 94272.
I0304 19:29:41.222391 22849303695488 run.py:483] Algo bellman_ford step 2946 current loss 0.020087, current_train_items 94304.
I0304 19:29:41.247014 22849303695488 run.py:483] Algo bellman_ford step 2947 current loss 0.118521, current_train_items 94336.
I0304 19:29:41.278403 22849303695488 run.py:483] Algo bellman_ford step 2948 current loss 0.116680, current_train_items 94368.
I0304 19:29:41.312095 22849303695488 run.py:483] Algo bellman_ford step 2949 current loss 0.087929, current_train_items 94400.
I0304 19:29:41.332230 22849303695488 run.py:483] Algo bellman_ford step 2950 current loss 0.006991, current_train_items 94432.
I0304 19:29:41.340545 22849303695488 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.962890625, 'score': 0.962890625, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0304 19:29:41.340654 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.981, current avg val score is 0.963, val scores are: bellman_ford: 0.963
I0304 19:29:41.357890 22849303695488 run.py:483] Algo bellman_ford step 2951 current loss 0.048758, current_train_items 94464.
I0304 19:29:41.381155 22849303695488 run.py:483] Algo bellman_ford step 2952 current loss 0.050273, current_train_items 94496.
I0304 19:29:41.412581 22849303695488 run.py:483] Algo bellman_ford step 2953 current loss 0.087853, current_train_items 94528.
I0304 19:29:41.447713 22849303695488 run.py:483] Algo bellman_ford step 2954 current loss 0.085621, current_train_items 94560.
I0304 19:29:41.468312 22849303695488 run.py:483] Algo bellman_ford step 2955 current loss 0.005229, current_train_items 94592.
I0304 19:29:41.484320 22849303695488 run.py:483] Algo bellman_ford step 2956 current loss 0.055281, current_train_items 94624.
I0304 19:29:41.509757 22849303695488 run.py:483] Algo bellman_ford step 2957 current loss 0.078423, current_train_items 94656.
I0304 19:29:41.542674 22849303695488 run.py:483] Algo bellman_ford step 2958 current loss 0.076639, current_train_items 94688.
I0304 19:29:41.577273 22849303695488 run.py:483] Algo bellman_ford step 2959 current loss 0.114833, current_train_items 94720.
I0304 19:29:41.597877 22849303695488 run.py:483] Algo bellman_ford step 2960 current loss 0.027261, current_train_items 94752.
I0304 19:29:41.614823 22849303695488 run.py:483] Algo bellman_ford step 2961 current loss 0.029202, current_train_items 94784.
I0304 19:29:41.638434 22849303695488 run.py:483] Algo bellman_ford step 2962 current loss 0.085086, current_train_items 94816.
I0304 19:29:41.669069 22849303695488 run.py:483] Algo bellman_ford step 2963 current loss 0.101166, current_train_items 94848.
I0304 19:29:41.704070 22849303695488 run.py:483] Algo bellman_ford step 2964 current loss 0.141438, current_train_items 94880.
I0304 19:29:41.723808 22849303695488 run.py:483] Algo bellman_ford step 2965 current loss 0.004823, current_train_items 94912.
I0304 19:29:41.740348 22849303695488 run.py:483] Algo bellman_ford step 2966 current loss 0.034920, current_train_items 94944.
I0304 19:29:41.764962 22849303695488 run.py:483] Algo bellman_ford step 2967 current loss 0.091106, current_train_items 94976.
I0304 19:29:41.795983 22849303695488 run.py:483] Algo bellman_ford step 2968 current loss 0.085029, current_train_items 95008.
I0304 19:29:41.831096 22849303695488 run.py:483] Algo bellman_ford step 2969 current loss 0.101798, current_train_items 95040.
I0304 19:29:41.851558 22849303695488 run.py:483] Algo bellman_ford step 2970 current loss 0.009821, current_train_items 95072.
I0304 19:29:41.868171 22849303695488 run.py:483] Algo bellman_ford step 2971 current loss 0.044484, current_train_items 95104.
I0304 19:29:41.892318 22849303695488 run.py:483] Algo bellman_ford step 2972 current loss 0.073010, current_train_items 95136.
I0304 19:29:41.923482 22849303695488 run.py:483] Algo bellman_ford step 2973 current loss 0.076464, current_train_items 95168.
I0304 19:29:41.960631 22849303695488 run.py:483] Algo bellman_ford step 2974 current loss 0.121505, current_train_items 95200.
I0304 19:29:41.981288 22849303695488 run.py:483] Algo bellman_ford step 2975 current loss 0.009210, current_train_items 95232.
I0304 19:29:41.998652 22849303695488 run.py:483] Algo bellman_ford step 2976 current loss 0.040763, current_train_items 95264.
I0304 19:29:42.023162 22849303695488 run.py:483] Algo bellman_ford step 2977 current loss 0.059110, current_train_items 95296.
I0304 19:29:42.054442 22849303695488 run.py:483] Algo bellman_ford step 2978 current loss 0.116532, current_train_items 95328.
I0304 19:29:42.089726 22849303695488 run.py:483] Algo bellman_ford step 2979 current loss 0.120814, current_train_items 95360.
I0304 19:29:42.109610 22849303695488 run.py:483] Algo bellman_ford step 2980 current loss 0.007240, current_train_items 95392.
I0304 19:29:42.126044 22849303695488 run.py:483] Algo bellman_ford step 2981 current loss 0.025631, current_train_items 95424.
I0304 19:29:42.151046 22849303695488 run.py:483] Algo bellman_ford step 2982 current loss 0.117062, current_train_items 95456.
I0304 19:29:42.184724 22849303695488 run.py:483] Algo bellman_ford step 2983 current loss 0.090089, current_train_items 95488.
I0304 19:29:42.218970 22849303695488 run.py:483] Algo bellman_ford step 2984 current loss 0.097759, current_train_items 95520.
I0304 19:29:42.239159 22849303695488 run.py:483] Algo bellman_ford step 2985 current loss 0.005588, current_train_items 95552.
I0304 19:29:42.256337 22849303695488 run.py:483] Algo bellman_ford step 2986 current loss 0.074860, current_train_items 95584.
I0304 19:29:42.281498 22849303695488 run.py:483] Algo bellman_ford step 2987 current loss 0.159461, current_train_items 95616.
I0304 19:29:42.313083 22849303695488 run.py:483] Algo bellman_ford step 2988 current loss 0.085971, current_train_items 95648.
I0304 19:29:42.347928 22849303695488 run.py:483] Algo bellman_ford step 2989 current loss 0.107320, current_train_items 95680.
I0304 19:29:42.368365 22849303695488 run.py:483] Algo bellman_ford step 2990 current loss 0.014820, current_train_items 95712.
I0304 19:29:42.385623 22849303695488 run.py:483] Algo bellman_ford step 2991 current loss 0.038527, current_train_items 95744.
I0304 19:29:42.410426 22849303695488 run.py:483] Algo bellman_ford step 2992 current loss 0.080454, current_train_items 95776.
I0304 19:29:42.443012 22849303695488 run.py:483] Algo bellman_ford step 2993 current loss 0.139956, current_train_items 95808.
I0304 19:29:42.478948 22849303695488 run.py:483] Algo bellman_ford step 2994 current loss 0.209281, current_train_items 95840.
I0304 19:29:42.499068 22849303695488 run.py:483] Algo bellman_ford step 2995 current loss 0.023707, current_train_items 95872.
I0304 19:29:42.515817 22849303695488 run.py:483] Algo bellman_ford step 2996 current loss 0.065367, current_train_items 95904.
I0304 19:29:42.541542 22849303695488 run.py:483] Algo bellman_ford step 2997 current loss 0.088714, current_train_items 95936.
I0304 19:29:42.574257 22849303695488 run.py:483] Algo bellman_ford step 2998 current loss 0.073481, current_train_items 95968.
I0304 19:29:42.607761 22849303695488 run.py:483] Algo bellman_ford step 2999 current loss 0.131198, current_train_items 96000.
I0304 19:29:42.627973 22849303695488 run.py:483] Algo bellman_ford step 3000 current loss 0.005706, current_train_items 96032.
I0304 19:29:42.636177 22849303695488 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0304 19:29:42.636286 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.981, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:29:42.667360 22849303695488 run.py:483] Algo bellman_ford step 3001 current loss 0.036889, current_train_items 96064.
I0304 19:29:42.691298 22849303695488 run.py:483] Algo bellman_ford step 3002 current loss 0.039897, current_train_items 96096.
I0304 19:29:42.723353 22849303695488 run.py:483] Algo bellman_ford step 3003 current loss 0.116420, current_train_items 96128.
I0304 19:29:42.757274 22849303695488 run.py:483] Algo bellman_ford step 3004 current loss 0.093360, current_train_items 96160.
I0304 19:29:42.777631 22849303695488 run.py:483] Algo bellman_ford step 3005 current loss 0.026110, current_train_items 96192.
I0304 19:29:42.793923 22849303695488 run.py:483] Algo bellman_ford step 3006 current loss 0.034562, current_train_items 96224.
I0304 19:29:42.819313 22849303695488 run.py:483] Algo bellman_ford step 3007 current loss 0.070197, current_train_items 96256.
I0304 19:29:42.851712 22849303695488 run.py:483] Algo bellman_ford step 3008 current loss 0.076890, current_train_items 96288.
I0304 19:29:42.888561 22849303695488 run.py:483] Algo bellman_ford step 3009 current loss 0.108653, current_train_items 96320.
I0304 19:29:42.908544 22849303695488 run.py:483] Algo bellman_ford step 3010 current loss 0.005550, current_train_items 96352.
I0304 19:29:42.925132 22849303695488 run.py:483] Algo bellman_ford step 3011 current loss 0.024826, current_train_items 96384.
I0304 19:29:42.949632 22849303695488 run.py:483] Algo bellman_ford step 3012 current loss 0.054056, current_train_items 96416.
I0304 19:29:42.980739 22849303695488 run.py:483] Algo bellman_ford step 3013 current loss 0.120731, current_train_items 96448.
I0304 19:29:43.016412 22849303695488 run.py:483] Algo bellman_ford step 3014 current loss 0.169615, current_train_items 96480.
I0304 19:29:43.036311 22849303695488 run.py:483] Algo bellman_ford step 3015 current loss 0.013357, current_train_items 96512.
I0304 19:29:43.053333 22849303695488 run.py:483] Algo bellman_ford step 3016 current loss 0.033460, current_train_items 96544.
I0304 19:29:43.077452 22849303695488 run.py:483] Algo bellman_ford step 3017 current loss 0.126998, current_train_items 96576.
I0304 19:29:43.108716 22849303695488 run.py:483] Algo bellman_ford step 3018 current loss 0.137899, current_train_items 96608.
I0304 19:29:43.141405 22849303695488 run.py:483] Algo bellman_ford step 3019 current loss 0.146486, current_train_items 96640.
I0304 19:29:43.161137 22849303695488 run.py:483] Algo bellman_ford step 3020 current loss 0.005961, current_train_items 96672.
I0304 19:29:43.177295 22849303695488 run.py:483] Algo bellman_ford step 3021 current loss 0.033786, current_train_items 96704.
I0304 19:29:43.202589 22849303695488 run.py:483] Algo bellman_ford step 3022 current loss 0.119493, current_train_items 96736.
I0304 19:29:43.234203 22849303695488 run.py:483] Algo bellman_ford step 3023 current loss 0.141623, current_train_items 96768.
I0304 19:29:43.268559 22849303695488 run.py:483] Algo bellman_ford step 3024 current loss 0.091111, current_train_items 96800.
I0304 19:29:43.288762 22849303695488 run.py:483] Algo bellman_ford step 3025 current loss 0.006923, current_train_items 96832.
I0304 19:29:43.305335 22849303695488 run.py:483] Algo bellman_ford step 3026 current loss 0.022395, current_train_items 96864.
I0304 19:29:43.328575 22849303695488 run.py:483] Algo bellman_ford step 3027 current loss 0.044317, current_train_items 96896.
I0304 19:29:43.359833 22849303695488 run.py:483] Algo bellman_ford step 3028 current loss 0.073118, current_train_items 96928.
I0304 19:29:43.392705 22849303695488 run.py:483] Algo bellman_ford step 3029 current loss 0.065547, current_train_items 96960.
I0304 19:29:43.412766 22849303695488 run.py:483] Algo bellman_ford step 3030 current loss 0.010364, current_train_items 96992.
I0304 19:29:43.429056 22849303695488 run.py:483] Algo bellman_ford step 3031 current loss 0.029673, current_train_items 97024.
I0304 19:29:43.454390 22849303695488 run.py:483] Algo bellman_ford step 3032 current loss 0.088279, current_train_items 97056.
I0304 19:29:43.486301 22849303695488 run.py:483] Algo bellman_ford step 3033 current loss 0.125318, current_train_items 97088.
I0304 19:29:43.521296 22849303695488 run.py:483] Algo bellman_ford step 3034 current loss 0.093400, current_train_items 97120.
I0304 19:29:43.541213 22849303695488 run.py:483] Algo bellman_ford step 3035 current loss 0.037190, current_train_items 97152.
I0304 19:29:43.558028 22849303695488 run.py:483] Algo bellman_ford step 3036 current loss 0.028880, current_train_items 97184.
I0304 19:29:43.582123 22849303695488 run.py:483] Algo bellman_ford step 3037 current loss 0.083842, current_train_items 97216.
I0304 19:29:43.614161 22849303695488 run.py:483] Algo bellman_ford step 3038 current loss 0.118910, current_train_items 97248.
I0304 19:29:43.646942 22849303695488 run.py:483] Algo bellman_ford step 3039 current loss 0.127084, current_train_items 97280.
I0304 19:29:43.667383 22849303695488 run.py:483] Algo bellman_ford step 3040 current loss 0.007210, current_train_items 97312.
I0304 19:29:43.683868 22849303695488 run.py:483] Algo bellman_ford step 3041 current loss 0.023607, current_train_items 97344.
I0304 19:29:43.708226 22849303695488 run.py:483] Algo bellman_ford step 3042 current loss 0.088394, current_train_items 97376.
I0304 19:29:43.739632 22849303695488 run.py:483] Algo bellman_ford step 3043 current loss 0.119510, current_train_items 97408.
I0304 19:29:43.772525 22849303695488 run.py:483] Algo bellman_ford step 3044 current loss 0.150449, current_train_items 97440.
I0304 19:29:43.792418 22849303695488 run.py:483] Algo bellman_ford step 3045 current loss 0.006820, current_train_items 97472.
I0304 19:29:43.808857 22849303695488 run.py:483] Algo bellman_ford step 3046 current loss 0.035140, current_train_items 97504.
I0304 19:29:43.832712 22849303695488 run.py:483] Algo bellman_ford step 3047 current loss 0.093435, current_train_items 97536.
I0304 19:29:43.864805 22849303695488 run.py:483] Algo bellman_ford step 3048 current loss 0.094886, current_train_items 97568.
I0304 19:29:43.898751 22849303695488 run.py:483] Algo bellman_ford step 3049 current loss 0.142294, current_train_items 97600.
I0304 19:29:43.918830 22849303695488 run.py:483] Algo bellman_ford step 3050 current loss 0.012757, current_train_items 97632.
I0304 19:29:43.927155 22849303695488 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0304 19:29:43.927265 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:29:43.944260 22849303695488 run.py:483] Algo bellman_ford step 3051 current loss 0.017805, current_train_items 97664.
I0304 19:29:43.968863 22849303695488 run.py:483] Algo bellman_ford step 3052 current loss 0.109916, current_train_items 97696.
I0304 19:29:44.001954 22849303695488 run.py:483] Algo bellman_ford step 3053 current loss 0.077982, current_train_items 97728.
I0304 19:29:44.034585 22849303695488 run.py:483] Algo bellman_ford step 3054 current loss 0.102507, current_train_items 97760.
I0304 19:29:44.055143 22849303695488 run.py:483] Algo bellman_ford step 3055 current loss 0.005855, current_train_items 97792.
I0304 19:29:44.071370 22849303695488 run.py:483] Algo bellman_ford step 3056 current loss 0.085758, current_train_items 97824.
I0304 19:29:44.095862 22849303695488 run.py:483] Algo bellman_ford step 3057 current loss 0.066329, current_train_items 97856.
I0304 19:29:44.127721 22849303695488 run.py:483] Algo bellman_ford step 3058 current loss 0.118348, current_train_items 97888.
I0304 19:29:44.160713 22849303695488 run.py:483] Algo bellman_ford step 3059 current loss 0.154217, current_train_items 97920.
I0304 19:29:44.180814 22849303695488 run.py:483] Algo bellman_ford step 3060 current loss 0.005547, current_train_items 97952.
I0304 19:29:44.197673 22849303695488 run.py:483] Algo bellman_ford step 3061 current loss 0.057855, current_train_items 97984.
I0304 19:29:44.221392 22849303695488 run.py:483] Algo bellman_ford step 3062 current loss 0.073970, current_train_items 98016.
I0304 19:29:44.254180 22849303695488 run.py:483] Algo bellman_ford step 3063 current loss 0.133535, current_train_items 98048.
I0304 19:29:44.289669 22849303695488 run.py:483] Algo bellman_ford step 3064 current loss 0.082132, current_train_items 98080.
I0304 19:29:44.309836 22849303695488 run.py:483] Algo bellman_ford step 3065 current loss 0.007318, current_train_items 98112.
I0304 19:29:44.326539 22849303695488 run.py:483] Algo bellman_ford step 3066 current loss 0.034237, current_train_items 98144.
I0304 19:29:44.351678 22849303695488 run.py:483] Algo bellman_ford step 3067 current loss 0.058858, current_train_items 98176.
I0304 19:29:44.382824 22849303695488 run.py:483] Algo bellman_ford step 3068 current loss 0.074085, current_train_items 98208.
I0304 19:29:44.419749 22849303695488 run.py:483] Algo bellman_ford step 3069 current loss 0.102800, current_train_items 98240.
I0304 19:29:44.440482 22849303695488 run.py:483] Algo bellman_ford step 3070 current loss 0.006960, current_train_items 98272.
I0304 19:29:44.457181 22849303695488 run.py:483] Algo bellman_ford step 3071 current loss 0.063071, current_train_items 98304.
I0304 19:29:44.480797 22849303695488 run.py:483] Algo bellman_ford step 3072 current loss 0.069090, current_train_items 98336.
I0304 19:29:44.511527 22849303695488 run.py:483] Algo bellman_ford step 3073 current loss 0.081141, current_train_items 98368.
I0304 19:29:44.547431 22849303695488 run.py:483] Algo bellman_ford step 3074 current loss 0.176247, current_train_items 98400.
I0304 19:29:44.567703 22849303695488 run.py:483] Algo bellman_ford step 3075 current loss 0.006811, current_train_items 98432.
I0304 19:29:44.584727 22849303695488 run.py:483] Algo bellman_ford step 3076 current loss 0.039839, current_train_items 98464.
I0304 19:29:44.609262 22849303695488 run.py:483] Algo bellman_ford step 3077 current loss 0.088097, current_train_items 98496.
I0304 19:29:44.641236 22849303695488 run.py:483] Algo bellman_ford step 3078 current loss 0.156159, current_train_items 98528.
I0304 19:29:44.673925 22849303695488 run.py:483] Algo bellman_ford step 3079 current loss 0.133587, current_train_items 98560.
I0304 19:29:44.694246 22849303695488 run.py:483] Algo bellman_ford step 3080 current loss 0.040875, current_train_items 98592.
I0304 19:29:44.711171 22849303695488 run.py:483] Algo bellman_ford step 3081 current loss 0.054986, current_train_items 98624.
I0304 19:29:44.735293 22849303695488 run.py:483] Algo bellman_ford step 3082 current loss 0.067615, current_train_items 98656.
I0304 19:29:44.766863 22849303695488 run.py:483] Algo bellman_ford step 3083 current loss 0.092333, current_train_items 98688.
I0304 19:29:44.800968 22849303695488 run.py:483] Algo bellman_ford step 3084 current loss 0.097449, current_train_items 98720.
I0304 19:29:44.821699 22849303695488 run.py:483] Algo bellman_ford step 3085 current loss 0.051603, current_train_items 98752.
I0304 19:29:44.838548 22849303695488 run.py:483] Algo bellman_ford step 3086 current loss 0.030189, current_train_items 98784.
I0304 19:29:44.861589 22849303695488 run.py:483] Algo bellman_ford step 3087 current loss 0.050357, current_train_items 98816.
I0304 19:29:44.893439 22849303695488 run.py:483] Algo bellman_ford step 3088 current loss 0.129936, current_train_items 98848.
I0304 19:29:44.929449 22849303695488 run.py:483] Algo bellman_ford step 3089 current loss 0.237520, current_train_items 98880.
I0304 19:29:44.949960 22849303695488 run.py:483] Algo bellman_ford step 3090 current loss 0.005545, current_train_items 98912.
I0304 19:29:44.966584 22849303695488 run.py:483] Algo bellman_ford step 3091 current loss 0.041629, current_train_items 98944.
I0304 19:29:44.989806 22849303695488 run.py:483] Algo bellman_ford step 3092 current loss 0.120659, current_train_items 98976.
I0304 19:29:45.021585 22849303695488 run.py:483] Algo bellman_ford step 3093 current loss 0.312942, current_train_items 99008.
I0304 19:29:45.056124 22849303695488 run.py:483] Algo bellman_ford step 3094 current loss 0.192689, current_train_items 99040.
I0304 19:29:45.076028 22849303695488 run.py:483] Algo bellman_ford step 3095 current loss 0.013313, current_train_items 99072.
I0304 19:29:45.093121 22849303695488 run.py:483] Algo bellman_ford step 3096 current loss 0.061464, current_train_items 99104.
I0304 19:29:45.118325 22849303695488 run.py:483] Algo bellman_ford step 3097 current loss 0.130315, current_train_items 99136.
I0304 19:29:45.149605 22849303695488 run.py:483] Algo bellman_ford step 3098 current loss 0.091401, current_train_items 99168.
I0304 19:29:45.183258 22849303695488 run.py:483] Algo bellman_ford step 3099 current loss 0.133637, current_train_items 99200.
I0304 19:29:45.203746 22849303695488 run.py:483] Algo bellman_ford step 3100 current loss 0.012881, current_train_items 99232.
I0304 19:29:45.211841 22849303695488 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0304 19:29:45.211950 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:29:45.230077 22849303695488 run.py:483] Algo bellman_ford step 3101 current loss 0.061043, current_train_items 99264.
I0304 19:29:45.255117 22849303695488 run.py:483] Algo bellman_ford step 3102 current loss 0.083627, current_train_items 99296.
I0304 19:29:45.287443 22849303695488 run.py:483] Algo bellman_ford step 3103 current loss 0.069755, current_train_items 99328.
I0304 19:29:45.322961 22849303695488 run.py:483] Algo bellman_ford step 3104 current loss 0.106828, current_train_items 99360.
I0304 19:29:45.343393 22849303695488 run.py:483] Algo bellman_ford step 3105 current loss 0.030940, current_train_items 99392.
I0304 19:29:45.359181 22849303695488 run.py:483] Algo bellman_ford step 3106 current loss 0.008908, current_train_items 99424.
I0304 19:29:45.383297 22849303695488 run.py:483] Algo bellman_ford step 3107 current loss 0.058976, current_train_items 99456.
I0304 19:29:45.415800 22849303695488 run.py:483] Algo bellman_ford step 3108 current loss 0.117090, current_train_items 99488.
I0304 19:29:45.448529 22849303695488 run.py:483] Algo bellman_ford step 3109 current loss 0.105928, current_train_items 99520.
I0304 19:29:45.468485 22849303695488 run.py:483] Algo bellman_ford step 3110 current loss 0.007011, current_train_items 99552.
I0304 19:29:45.485089 22849303695488 run.py:483] Algo bellman_ford step 3111 current loss 0.045851, current_train_items 99584.
I0304 19:29:45.509135 22849303695488 run.py:483] Algo bellman_ford step 3112 current loss 0.068038, current_train_items 99616.
I0304 19:29:45.540897 22849303695488 run.py:483] Algo bellman_ford step 3113 current loss 0.089995, current_train_items 99648.
I0304 19:29:45.575789 22849303695488 run.py:483] Algo bellman_ford step 3114 current loss 0.096195, current_train_items 99680.
I0304 19:29:45.595835 22849303695488 run.py:483] Algo bellman_ford step 3115 current loss 0.011583, current_train_items 99712.
I0304 19:29:45.612539 22849303695488 run.py:483] Algo bellman_ford step 3116 current loss 0.026684, current_train_items 99744.
I0304 19:29:45.636644 22849303695488 run.py:483] Algo bellman_ford step 3117 current loss 0.040116, current_train_items 99776.
I0304 19:29:45.668942 22849303695488 run.py:483] Algo bellman_ford step 3118 current loss 0.116092, current_train_items 99808.
I0304 19:29:45.702471 22849303695488 run.py:483] Algo bellman_ford step 3119 current loss 0.115500, current_train_items 99840.
I0304 19:29:45.722192 22849303695488 run.py:483] Algo bellman_ford step 3120 current loss 0.005114, current_train_items 99872.
I0304 19:29:45.738638 22849303695488 run.py:483] Algo bellman_ford step 3121 current loss 0.049847, current_train_items 99904.
I0304 19:29:45.762913 22849303695488 run.py:483] Algo bellman_ford step 3122 current loss 0.040281, current_train_items 99936.
I0304 19:29:45.794837 22849303695488 run.py:483] Algo bellman_ford step 3123 current loss 0.069624, current_train_items 99968.
I0304 19:29:45.828773 22849303695488 run.py:483] Algo bellman_ford step 3124 current loss 0.089931, current_train_items 100000.
I0304 19:29:45.848683 22849303695488 run.py:483] Algo bellman_ford step 3125 current loss 0.007431, current_train_items 100032.
I0304 19:29:45.865293 22849303695488 run.py:483] Algo bellman_ford step 3126 current loss 0.043586, current_train_items 100064.
I0304 19:29:45.888738 22849303695488 run.py:483] Algo bellman_ford step 3127 current loss 0.036865, current_train_items 100096.
I0304 19:29:45.921238 22849303695488 run.py:483] Algo bellman_ford step 3128 current loss 0.079272, current_train_items 100128.
I0304 19:29:45.954009 22849303695488 run.py:483] Algo bellman_ford step 3129 current loss 0.074198, current_train_items 100160.
I0304 19:29:45.973955 22849303695488 run.py:483] Algo bellman_ford step 3130 current loss 0.007527, current_train_items 100192.
I0304 19:29:45.990549 22849303695488 run.py:483] Algo bellman_ford step 3131 current loss 0.022363, current_train_items 100224.
I0304 19:29:46.014800 22849303695488 run.py:483] Algo bellman_ford step 3132 current loss 0.054858, current_train_items 100256.
I0304 19:29:46.045570 22849303695488 run.py:483] Algo bellman_ford step 3133 current loss 0.091919, current_train_items 100288.
I0304 19:29:46.081467 22849303695488 run.py:483] Algo bellman_ford step 3134 current loss 0.144182, current_train_items 100320.
I0304 19:29:46.101349 22849303695488 run.py:483] Algo bellman_ford step 3135 current loss 0.050534, current_train_items 100352.
I0304 19:29:46.117920 22849303695488 run.py:483] Algo bellman_ford step 3136 current loss 0.034454, current_train_items 100384.
I0304 19:29:46.142458 22849303695488 run.py:483] Algo bellman_ford step 3137 current loss 0.081495, current_train_items 100416.
I0304 19:29:46.172938 22849303695488 run.py:483] Algo bellman_ford step 3138 current loss 0.062356, current_train_items 100448.
I0304 19:29:46.207430 22849303695488 run.py:483] Algo bellman_ford step 3139 current loss 0.104855, current_train_items 100480.
I0304 19:29:46.227762 22849303695488 run.py:483] Algo bellman_ford step 3140 current loss 0.006625, current_train_items 100512.
I0304 19:29:46.244808 22849303695488 run.py:483] Algo bellman_ford step 3141 current loss 0.092390, current_train_items 100544.
I0304 19:29:46.267687 22849303695488 run.py:483] Algo bellman_ford step 3142 current loss 0.100119, current_train_items 100576.
I0304 19:29:46.300412 22849303695488 run.py:483] Algo bellman_ford step 3143 current loss 0.113124, current_train_items 100608.
I0304 19:29:46.334286 22849303695488 run.py:483] Algo bellman_ford step 3144 current loss 0.101591, current_train_items 100640.
I0304 19:29:46.354199 22849303695488 run.py:483] Algo bellman_ford step 3145 current loss 0.007743, current_train_items 100672.
I0304 19:29:46.370632 22849303695488 run.py:483] Algo bellman_ford step 3146 current loss 0.024723, current_train_items 100704.
I0304 19:29:46.394353 22849303695488 run.py:483] Algo bellman_ford step 3147 current loss 0.042914, current_train_items 100736.
I0304 19:29:46.423949 22849303695488 run.py:483] Algo bellman_ford step 3148 current loss 0.059110, current_train_items 100768.
I0304 19:29:46.459881 22849303695488 run.py:483] Algo bellman_ford step 3149 current loss 0.133262, current_train_items 100800.
I0304 19:29:46.479641 22849303695488 run.py:483] Algo bellman_ford step 3150 current loss 0.010343, current_train_items 100832.
I0304 19:29:46.488023 22849303695488 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0304 19:29:46.488133 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:29:46.505409 22849303695488 run.py:483] Algo bellman_ford step 3151 current loss 0.072868, current_train_items 100864.
I0304 19:29:46.530922 22849303695488 run.py:483] Algo bellman_ford step 3152 current loss 0.237843, current_train_items 100896.
I0304 19:29:46.562968 22849303695488 run.py:483] Algo bellman_ford step 3153 current loss 0.123041, current_train_items 100928.
I0304 19:29:46.597936 22849303695488 run.py:483] Algo bellman_ford step 3154 current loss 0.164776, current_train_items 100960.
I0304 19:29:46.618147 22849303695488 run.py:483] Algo bellman_ford step 3155 current loss 0.010028, current_train_items 100992.
I0304 19:29:46.634414 22849303695488 run.py:483] Algo bellman_ford step 3156 current loss 0.032524, current_train_items 101024.
I0304 19:29:46.659318 22849303695488 run.py:483] Algo bellman_ford step 3157 current loss 0.061806, current_train_items 101056.
I0304 19:29:46.690101 22849303695488 run.py:483] Algo bellman_ford step 3158 current loss 0.132460, current_train_items 101088.
I0304 19:29:46.723551 22849303695488 run.py:483] Algo bellman_ford step 3159 current loss 0.125051, current_train_items 101120.
I0304 19:29:46.743956 22849303695488 run.py:483] Algo bellman_ford step 3160 current loss 0.007267, current_train_items 101152.
I0304 19:29:46.760320 22849303695488 run.py:483] Algo bellman_ford step 3161 current loss 0.015598, current_train_items 101184.
I0304 19:29:46.784201 22849303695488 run.py:483] Algo bellman_ford step 3162 current loss 0.051753, current_train_items 101216.
I0304 19:29:46.814897 22849303695488 run.py:483] Algo bellman_ford step 3163 current loss 0.065689, current_train_items 101248.
I0304 19:29:46.850155 22849303695488 run.py:483] Algo bellman_ford step 3164 current loss 0.124673, current_train_items 101280.
I0304 19:29:46.870134 22849303695488 run.py:483] Algo bellman_ford step 3165 current loss 0.007741, current_train_items 101312.
I0304 19:29:46.887385 22849303695488 run.py:483] Algo bellman_ford step 3166 current loss 0.072564, current_train_items 101344.
I0304 19:29:46.911787 22849303695488 run.py:483] Algo bellman_ford step 3167 current loss 0.110398, current_train_items 101376.
I0304 19:29:46.943445 22849303695488 run.py:483] Algo bellman_ford step 3168 current loss 0.087889, current_train_items 101408.
I0304 19:29:46.977093 22849303695488 run.py:483] Algo bellman_ford step 3169 current loss 0.124540, current_train_items 101440.
I0304 19:29:46.997398 22849303695488 run.py:483] Algo bellman_ford step 3170 current loss 0.011458, current_train_items 101472.
I0304 19:29:47.014423 22849303695488 run.py:483] Algo bellman_ford step 3171 current loss 0.111200, current_train_items 101504.
I0304 19:29:47.038707 22849303695488 run.py:483] Algo bellman_ford step 3172 current loss 0.100644, current_train_items 101536.
I0304 19:29:47.070579 22849303695488 run.py:483] Algo bellman_ford step 3173 current loss 0.118420, current_train_items 101568.
I0304 19:29:47.102922 22849303695488 run.py:483] Algo bellman_ford step 3174 current loss 0.113658, current_train_items 101600.
I0304 19:29:47.123322 22849303695488 run.py:483] Algo bellman_ford step 3175 current loss 0.006507, current_train_items 101632.
I0304 19:29:47.140027 22849303695488 run.py:483] Algo bellman_ford step 3176 current loss 0.063344, current_train_items 101664.
I0304 19:29:47.163458 22849303695488 run.py:483] Algo bellman_ford step 3177 current loss 0.163572, current_train_items 101696.
I0304 19:29:47.193385 22849303695488 run.py:483] Algo bellman_ford step 3178 current loss 0.079677, current_train_items 101728.
I0304 19:29:47.228551 22849303695488 run.py:483] Algo bellman_ford step 3179 current loss 0.137802, current_train_items 101760.
I0304 19:29:47.248223 22849303695488 run.py:483] Algo bellman_ford step 3180 current loss 0.010422, current_train_items 101792.
I0304 19:29:47.265067 22849303695488 run.py:483] Algo bellman_ford step 3181 current loss 0.035677, current_train_items 101824.
I0304 19:29:47.289492 22849303695488 run.py:483] Algo bellman_ford step 3182 current loss 0.076885, current_train_items 101856.
I0304 19:29:47.321338 22849303695488 run.py:483] Algo bellman_ford step 3183 current loss 0.103485, current_train_items 101888.
I0304 19:29:47.358322 22849303695488 run.py:483] Algo bellman_ford step 3184 current loss 0.091310, current_train_items 101920.
I0304 19:29:47.379014 22849303695488 run.py:483] Algo bellman_ford step 3185 current loss 0.090321, current_train_items 101952.
I0304 19:29:47.395622 22849303695488 run.py:483] Algo bellman_ford step 3186 current loss 0.026777, current_train_items 101984.
I0304 19:29:47.420355 22849303695488 run.py:483] Algo bellman_ford step 3187 current loss 0.170882, current_train_items 102016.
I0304 19:29:47.451691 22849303695488 run.py:483] Algo bellman_ford step 3188 current loss 0.169990, current_train_items 102048.
I0304 19:29:47.485952 22849303695488 run.py:483] Algo bellman_ford step 3189 current loss 0.176122, current_train_items 102080.
I0304 19:29:47.506352 22849303695488 run.py:483] Algo bellman_ford step 3190 current loss 0.012250, current_train_items 102112.
I0304 19:29:47.523175 22849303695488 run.py:483] Algo bellman_ford step 3191 current loss 0.018710, current_train_items 102144.
I0304 19:29:47.547966 22849303695488 run.py:483] Algo bellman_ford step 3192 current loss 0.123130, current_train_items 102176.
I0304 19:29:47.579231 22849303695488 run.py:483] Algo bellman_ford step 3193 current loss 0.079043, current_train_items 102208.
I0304 19:29:47.613875 22849303695488 run.py:483] Algo bellman_ford step 3194 current loss 0.105973, current_train_items 102240.
I0304 19:29:47.634187 22849303695488 run.py:483] Algo bellman_ford step 3195 current loss 0.008272, current_train_items 102272.
I0304 19:29:47.650433 22849303695488 run.py:483] Algo bellman_ford step 3196 current loss 0.039801, current_train_items 102304.
I0304 19:29:47.674753 22849303695488 run.py:483] Algo bellman_ford step 3197 current loss 0.095996, current_train_items 102336.
I0304 19:29:47.706754 22849303695488 run.py:483] Algo bellman_ford step 3198 current loss 0.111848, current_train_items 102368.
I0304 19:29:47.741575 22849303695488 run.py:483] Algo bellman_ford step 3199 current loss 0.143390, current_train_items 102400.
I0304 19:29:47.761854 22849303695488 run.py:483] Algo bellman_ford step 3200 current loss 0.022613, current_train_items 102432.
I0304 19:29:47.770016 22849303695488 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0304 19:29:47.770128 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:29:47.786772 22849303695488 run.py:483] Algo bellman_ford step 3201 current loss 0.019124, current_train_items 102464.
I0304 19:29:47.810856 22849303695488 run.py:483] Algo bellman_ford step 3202 current loss 0.043251, current_train_items 102496.
I0304 19:29:47.844717 22849303695488 run.py:483] Algo bellman_ford step 3203 current loss 0.161474, current_train_items 102528.
I0304 19:29:47.879824 22849303695488 run.py:483] Algo bellman_ford step 3204 current loss 0.123462, current_train_items 102560.
I0304 19:29:47.900588 22849303695488 run.py:483] Algo bellman_ford step 3205 current loss 0.007706, current_train_items 102592.
I0304 19:29:47.916725 22849303695488 run.py:483] Algo bellman_ford step 3206 current loss 0.029059, current_train_items 102624.
I0304 19:29:47.940169 22849303695488 run.py:483] Algo bellman_ford step 3207 current loss 0.040721, current_train_items 102656.
I0304 19:29:47.973030 22849303695488 run.py:483] Algo bellman_ford step 3208 current loss 0.100764, current_train_items 102688.
I0304 19:29:48.007754 22849303695488 run.py:483] Algo bellman_ford step 3209 current loss 0.135974, current_train_items 102720.
I0304 19:29:48.027847 22849303695488 run.py:483] Algo bellman_ford step 3210 current loss 0.040610, current_train_items 102752.
I0304 19:29:48.044102 22849303695488 run.py:483] Algo bellman_ford step 3211 current loss 0.014131, current_train_items 102784.
I0304 19:29:48.068306 22849303695488 run.py:483] Algo bellman_ford step 3212 current loss 0.051235, current_train_items 102816.
I0304 19:29:48.100088 22849303695488 run.py:483] Algo bellman_ford step 3213 current loss 0.101658, current_train_items 102848.
I0304 19:29:48.134159 22849303695488 run.py:483] Algo bellman_ford step 3214 current loss 0.106226, current_train_items 102880.
I0304 19:29:48.154741 22849303695488 run.py:483] Algo bellman_ford step 3215 current loss 0.046224, current_train_items 102912.
I0304 19:29:48.171275 22849303695488 run.py:483] Algo bellman_ford step 3216 current loss 0.036026, current_train_items 102944.
I0304 19:29:48.196451 22849303695488 run.py:483] Algo bellman_ford step 3217 current loss 0.054485, current_train_items 102976.
I0304 19:29:48.227152 22849303695488 run.py:483] Algo bellman_ford step 3218 current loss 0.060600, current_train_items 103008.
I0304 19:29:48.260932 22849303695488 run.py:483] Algo bellman_ford step 3219 current loss 0.068263, current_train_items 103040.
I0304 19:29:48.281320 22849303695488 run.py:483] Algo bellman_ford step 3220 current loss 0.009289, current_train_items 103072.
I0304 19:29:48.297867 22849303695488 run.py:483] Algo bellman_ford step 3221 current loss 0.023790, current_train_items 103104.
I0304 19:29:48.322932 22849303695488 run.py:483] Algo bellman_ford step 3222 current loss 0.084317, current_train_items 103136.
I0304 19:29:48.354983 22849303695488 run.py:483] Algo bellman_ford step 3223 current loss 0.074755, current_train_items 103168.
I0304 19:29:48.387629 22849303695488 run.py:483] Algo bellman_ford step 3224 current loss 0.077048, current_train_items 103200.
I0304 19:29:48.407583 22849303695488 run.py:483] Algo bellman_ford step 3225 current loss 0.009841, current_train_items 103232.
I0304 19:29:48.423375 22849303695488 run.py:483] Algo bellman_ford step 3226 current loss 0.027747, current_train_items 103264.
I0304 19:29:48.448276 22849303695488 run.py:483] Algo bellman_ford step 3227 current loss 0.037086, current_train_items 103296.
I0304 19:29:48.480235 22849303695488 run.py:483] Algo bellman_ford step 3228 current loss 0.083491, current_train_items 103328.
I0304 19:29:48.518411 22849303695488 run.py:483] Algo bellman_ford step 3229 current loss 0.115416, current_train_items 103360.
I0304 19:29:48.538563 22849303695488 run.py:483] Algo bellman_ford step 3230 current loss 0.009545, current_train_items 103392.
I0304 19:29:48.555289 22849303695488 run.py:483] Algo bellman_ford step 3231 current loss 0.074367, current_train_items 103424.
I0304 19:29:48.578883 22849303695488 run.py:483] Algo bellman_ford step 3232 current loss 0.086660, current_train_items 103456.
I0304 19:29:48.610717 22849303695488 run.py:483] Algo bellman_ford step 3233 current loss 0.067169, current_train_items 103488.
I0304 19:29:48.646334 22849303695488 run.py:483] Algo bellman_ford step 3234 current loss 0.079051, current_train_items 103520.
I0304 19:29:48.666644 22849303695488 run.py:483] Algo bellman_ford step 3235 current loss 0.033786, current_train_items 103552.
I0304 19:29:48.683337 22849303695488 run.py:483] Algo bellman_ford step 3236 current loss 0.035381, current_train_items 103584.
I0304 19:29:48.707097 22849303695488 run.py:483] Algo bellman_ford step 3237 current loss 0.058966, current_train_items 103616.
I0304 19:29:48.739835 22849303695488 run.py:483] Algo bellman_ford step 3238 current loss 0.090875, current_train_items 103648.
I0304 19:29:48.774014 22849303695488 run.py:483] Algo bellman_ford step 3239 current loss 0.101948, current_train_items 103680.
I0304 19:29:48.794085 22849303695488 run.py:483] Algo bellman_ford step 3240 current loss 0.032730, current_train_items 103712.
I0304 19:29:48.810809 22849303695488 run.py:483] Algo bellman_ford step 3241 current loss 0.027716, current_train_items 103744.
I0304 19:29:48.834936 22849303695488 run.py:483] Algo bellman_ford step 3242 current loss 0.053956, current_train_items 103776.
I0304 19:29:48.865674 22849303695488 run.py:483] Algo bellman_ford step 3243 current loss 0.082059, current_train_items 103808.
I0304 19:29:48.900532 22849303695488 run.py:483] Algo bellman_ford step 3244 current loss 0.110658, current_train_items 103840.
I0304 19:29:48.920626 22849303695488 run.py:483] Algo bellman_ford step 3245 current loss 0.016451, current_train_items 103872.
I0304 19:29:48.937381 22849303695488 run.py:483] Algo bellman_ford step 3246 current loss 0.013636, current_train_items 103904.
I0304 19:29:48.961376 22849303695488 run.py:483] Algo bellman_ford step 3247 current loss 0.093167, current_train_items 103936.
I0304 19:29:48.993635 22849303695488 run.py:483] Algo bellman_ford step 3248 current loss 0.070632, current_train_items 103968.
I0304 19:29:49.026824 22849303695488 run.py:483] Algo bellman_ford step 3249 current loss 0.075108, current_train_items 104000.
I0304 19:29:49.047031 22849303695488 run.py:483] Algo bellman_ford step 3250 current loss 0.032732, current_train_items 104032.
I0304 19:29:49.055287 22849303695488 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0304 19:29:49.055396 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:29:49.072520 22849303695488 run.py:483] Algo bellman_ford step 3251 current loss 0.091710, current_train_items 104064.
I0304 19:29:49.095902 22849303695488 run.py:483] Algo bellman_ford step 3252 current loss 0.075369, current_train_items 104096.
I0304 19:29:49.129318 22849303695488 run.py:483] Algo bellman_ford step 3253 current loss 0.099033, current_train_items 104128.
I0304 19:29:49.164376 22849303695488 run.py:483] Algo bellman_ford step 3254 current loss 0.076430, current_train_items 104160.
I0304 19:29:49.185145 22849303695488 run.py:483] Algo bellman_ford step 3255 current loss 0.022759, current_train_items 104192.
I0304 19:29:49.201170 22849303695488 run.py:483] Algo bellman_ford step 3256 current loss 0.048625, current_train_items 104224.
I0304 19:29:49.224999 22849303695488 run.py:483] Algo bellman_ford step 3257 current loss 0.083055, current_train_items 104256.
I0304 19:29:49.258074 22849303695488 run.py:483] Algo bellman_ford step 3258 current loss 0.184140, current_train_items 104288.
I0304 19:29:49.293919 22849303695488 run.py:483] Algo bellman_ford step 3259 current loss 0.109632, current_train_items 104320.
I0304 19:29:49.314365 22849303695488 run.py:483] Algo bellman_ford step 3260 current loss 0.008115, current_train_items 104352.
I0304 19:29:49.330983 22849303695488 run.py:483] Algo bellman_ford step 3261 current loss 0.014462, current_train_items 104384.
I0304 19:29:49.354709 22849303695488 run.py:483] Algo bellman_ford step 3262 current loss 0.123031, current_train_items 104416.
I0304 19:29:49.387176 22849303695488 run.py:483] Algo bellman_ford step 3263 current loss 0.155135, current_train_items 104448.
I0304 19:29:49.422224 22849303695488 run.py:483] Algo bellman_ford step 3264 current loss 0.190636, current_train_items 104480.
I0304 19:29:49.442523 22849303695488 run.py:483] Algo bellman_ford step 3265 current loss 0.020967, current_train_items 104512.
I0304 19:29:49.459329 22849303695488 run.py:483] Algo bellman_ford step 3266 current loss 0.064756, current_train_items 104544.
I0304 19:29:49.483160 22849303695488 run.py:483] Algo bellman_ford step 3267 current loss 0.058821, current_train_items 104576.
I0304 19:29:49.513362 22849303695488 run.py:483] Algo bellman_ford step 3268 current loss 0.129324, current_train_items 104608.
I0304 19:29:49.550137 22849303695488 run.py:483] Algo bellman_ford step 3269 current loss 0.226068, current_train_items 104640.
I0304 19:29:49.570859 22849303695488 run.py:483] Algo bellman_ford step 3270 current loss 0.009430, current_train_items 104672.
I0304 19:29:49.587829 22849303695488 run.py:483] Algo bellman_ford step 3271 current loss 0.023238, current_train_items 104704.
I0304 19:29:49.611768 22849303695488 run.py:483] Algo bellman_ford step 3272 current loss 0.042405, current_train_items 104736.
I0304 19:29:49.642319 22849303695488 run.py:483] Algo bellman_ford step 3273 current loss 0.072796, current_train_items 104768.
I0304 19:29:49.674710 22849303695488 run.py:483] Algo bellman_ford step 3274 current loss 0.049141, current_train_items 104800.
I0304 19:29:49.695417 22849303695488 run.py:483] Algo bellman_ford step 3275 current loss 0.007322, current_train_items 104832.
I0304 19:29:49.711938 22849303695488 run.py:483] Algo bellman_ford step 3276 current loss 0.035390, current_train_items 104864.
I0304 19:29:49.735547 22849303695488 run.py:483] Algo bellman_ford step 3277 current loss 0.077270, current_train_items 104896.
I0304 19:29:49.768311 22849303695488 run.py:483] Algo bellman_ford step 3278 current loss 0.097419, current_train_items 104928.
I0304 19:29:49.803561 22849303695488 run.py:483] Algo bellman_ford step 3279 current loss 0.118357, current_train_items 104960.
I0304 19:29:49.823846 22849303695488 run.py:483] Algo bellman_ford step 3280 current loss 0.012342, current_train_items 104992.
I0304 19:29:49.840216 22849303695488 run.py:483] Algo bellman_ford step 3281 current loss 0.019790, current_train_items 105024.
I0304 19:29:49.864422 22849303695488 run.py:483] Algo bellman_ford step 3282 current loss 0.054815, current_train_items 105056.
I0304 19:29:49.896729 22849303695488 run.py:483] Algo bellman_ford step 3283 current loss 0.204957, current_train_items 105088.
I0304 19:29:49.931387 22849303695488 run.py:483] Algo bellman_ford step 3284 current loss 0.111218, current_train_items 105120.
I0304 19:29:49.951867 22849303695488 run.py:483] Algo bellman_ford step 3285 current loss 0.006446, current_train_items 105152.
I0304 19:29:49.968930 22849303695488 run.py:483] Algo bellman_ford step 3286 current loss 0.028636, current_train_items 105184.
I0304 19:29:49.993442 22849303695488 run.py:483] Algo bellman_ford step 3287 current loss 0.075874, current_train_items 105216.
I0304 19:29:50.023996 22849303695488 run.py:483] Algo bellman_ford step 3288 current loss 0.033771, current_train_items 105248.
I0304 19:29:50.056448 22849303695488 run.py:483] Algo bellman_ford step 3289 current loss 0.083512, current_train_items 105280.
I0304 19:29:50.077014 22849303695488 run.py:483] Algo bellman_ford step 3290 current loss 0.027886, current_train_items 105312.
I0304 19:29:50.094218 22849303695488 run.py:483] Algo bellman_ford step 3291 current loss 0.024704, current_train_items 105344.
I0304 19:29:50.118486 22849303695488 run.py:483] Algo bellman_ford step 3292 current loss 0.085077, current_train_items 105376.
I0304 19:29:50.150660 22849303695488 run.py:483] Algo bellman_ford step 3293 current loss 0.124713, current_train_items 105408.
I0304 19:29:50.185398 22849303695488 run.py:483] Algo bellman_ford step 3294 current loss 0.110206, current_train_items 105440.
I0304 19:29:50.205820 22849303695488 run.py:483] Algo bellman_ford step 3295 current loss 0.008449, current_train_items 105472.
I0304 19:29:50.221948 22849303695488 run.py:483] Algo bellman_ford step 3296 current loss 0.008501, current_train_items 105504.
I0304 19:29:50.247288 22849303695488 run.py:483] Algo bellman_ford step 3297 current loss 0.175456, current_train_items 105536.
I0304 19:29:50.280638 22849303695488 run.py:483] Algo bellman_ford step 3298 current loss 0.273731, current_train_items 105568.
I0304 19:29:50.316532 22849303695488 run.py:483] Algo bellman_ford step 3299 current loss 0.172568, current_train_items 105600.
I0304 19:29:50.337157 22849303695488 run.py:483] Algo bellman_ford step 3300 current loss 0.041682, current_train_items 105632.
I0304 19:29:50.345445 22849303695488 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0304 19:29:50.345554 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:29:50.362745 22849303695488 run.py:483] Algo bellman_ford step 3301 current loss 0.024754, current_train_items 105664.
I0304 19:29:50.387052 22849303695488 run.py:483] Algo bellman_ford step 3302 current loss 0.151998, current_train_items 105696.
I0304 19:29:50.417848 22849303695488 run.py:483] Algo bellman_ford step 3303 current loss 0.070658, current_train_items 105728.
I0304 19:29:50.451036 22849303695488 run.py:483] Algo bellman_ford step 3304 current loss 0.075914, current_train_items 105760.
I0304 19:29:50.471114 22849303695488 run.py:483] Algo bellman_ford step 3305 current loss 0.005570, current_train_items 105792.
I0304 19:29:50.487562 22849303695488 run.py:483] Algo bellman_ford step 3306 current loss 0.034215, current_train_items 105824.
I0304 19:29:50.512686 22849303695488 run.py:483] Algo bellman_ford step 3307 current loss 0.049741, current_train_items 105856.
I0304 19:29:50.544754 22849303695488 run.py:483] Algo bellman_ford step 3308 current loss 0.066273, current_train_items 105888.
I0304 19:29:50.580655 22849303695488 run.py:483] Algo bellman_ford step 3309 current loss 0.184005, current_train_items 105920.
I0304 19:29:50.600831 22849303695488 run.py:483] Algo bellman_ford step 3310 current loss 0.099201, current_train_items 105952.
I0304 19:29:50.617737 22849303695488 run.py:483] Algo bellman_ford step 3311 current loss 0.057675, current_train_items 105984.
I0304 19:29:50.642041 22849303695488 run.py:483] Algo bellman_ford step 3312 current loss 0.069362, current_train_items 106016.
I0304 19:29:50.674054 22849303695488 run.py:483] Algo bellman_ford step 3313 current loss 0.120871, current_train_items 106048.
I0304 19:29:50.706949 22849303695488 run.py:483] Algo bellman_ford step 3314 current loss 0.119990, current_train_items 106080.
I0304 19:29:50.726730 22849303695488 run.py:483] Algo bellman_ford step 3315 current loss 0.007720, current_train_items 106112.
I0304 19:29:50.743887 22849303695488 run.py:483] Algo bellman_ford step 3316 current loss 0.044550, current_train_items 106144.
I0304 19:29:50.767663 22849303695488 run.py:483] Algo bellman_ford step 3317 current loss 0.037543, current_train_items 106176.
I0304 19:29:50.799318 22849303695488 run.py:483] Algo bellman_ford step 3318 current loss 0.093217, current_train_items 106208.
I0304 19:29:50.834249 22849303695488 run.py:483] Algo bellman_ford step 3319 current loss 0.165753, current_train_items 106240.
I0304 19:29:50.854396 22849303695488 run.py:483] Algo bellman_ford step 3320 current loss 0.004709, current_train_items 106272.
I0304 19:29:50.871046 22849303695488 run.py:483] Algo bellman_ford step 3321 current loss 0.019686, current_train_items 106304.
I0304 19:29:50.896218 22849303695488 run.py:483] Algo bellman_ford step 3322 current loss 0.090004, current_train_items 106336.
I0304 19:29:50.928330 22849303695488 run.py:483] Algo bellman_ford step 3323 current loss 0.123896, current_train_items 106368.
I0304 19:29:50.961574 22849303695488 run.py:483] Algo bellman_ford step 3324 current loss 0.088165, current_train_items 106400.
I0304 19:29:50.981245 22849303695488 run.py:483] Algo bellman_ford step 3325 current loss 0.006952, current_train_items 106432.
I0304 19:29:50.997935 22849303695488 run.py:483] Algo bellman_ford step 3326 current loss 0.066611, current_train_items 106464.
I0304 19:29:51.021724 22849303695488 run.py:483] Algo bellman_ford step 3327 current loss 0.069595, current_train_items 106496.
I0304 19:29:51.052868 22849303695488 run.py:483] Algo bellman_ford step 3328 current loss 0.094078, current_train_items 106528.
I0304 19:29:51.086650 22849303695488 run.py:483] Algo bellman_ford step 3329 current loss 0.139683, current_train_items 106560.
I0304 19:29:51.106647 22849303695488 run.py:483] Algo bellman_ford step 3330 current loss 0.024114, current_train_items 106592.
I0304 19:29:51.123084 22849303695488 run.py:483] Algo bellman_ford step 3331 current loss 0.040284, current_train_items 106624.
I0304 19:29:51.146996 22849303695488 run.py:483] Algo bellman_ford step 3332 current loss 0.054602, current_train_items 106656.
I0304 19:29:51.177903 22849303695488 run.py:483] Algo bellman_ford step 3333 current loss 0.045961, current_train_items 106688.
I0304 19:29:51.210732 22849303695488 run.py:483] Algo bellman_ford step 3334 current loss 0.081281, current_train_items 106720.
I0304 19:29:51.230540 22849303695488 run.py:483] Algo bellman_ford step 3335 current loss 0.005228, current_train_items 106752.
I0304 19:29:51.246626 22849303695488 run.py:483] Algo bellman_ford step 3336 current loss 0.034891, current_train_items 106784.
I0304 19:29:51.271250 22849303695488 run.py:483] Algo bellman_ford step 3337 current loss 0.067481, current_train_items 106816.
I0304 19:29:51.304205 22849303695488 run.py:483] Algo bellman_ford step 3338 current loss 0.094417, current_train_items 106848.
I0304 19:29:51.339693 22849303695488 run.py:483] Algo bellman_ford step 3339 current loss 0.071069, current_train_items 106880.
I0304 19:29:51.359417 22849303695488 run.py:483] Algo bellman_ford step 3340 current loss 0.003327, current_train_items 106912.
I0304 19:29:51.376346 22849303695488 run.py:483] Algo bellman_ford step 3341 current loss 0.034496, current_train_items 106944.
I0304 19:29:51.401394 22849303695488 run.py:483] Algo bellman_ford step 3342 current loss 0.083909, current_train_items 106976.
I0304 19:29:51.433875 22849303695488 run.py:483] Algo bellman_ford step 3343 current loss 0.116229, current_train_items 107008.
I0304 19:29:51.466290 22849303695488 run.py:483] Algo bellman_ford step 3344 current loss 0.111100, current_train_items 107040.
I0304 19:29:51.486376 22849303695488 run.py:483] Algo bellman_ford step 3345 current loss 0.015401, current_train_items 107072.
I0304 19:29:51.502908 22849303695488 run.py:483] Algo bellman_ford step 3346 current loss 0.023485, current_train_items 107104.
I0304 19:29:51.526971 22849303695488 run.py:483] Algo bellman_ford step 3347 current loss 0.091847, current_train_items 107136.
I0304 19:29:51.558073 22849303695488 run.py:483] Algo bellman_ford step 3348 current loss 0.094899, current_train_items 107168.
I0304 19:29:51.590150 22849303695488 run.py:483] Algo bellman_ford step 3349 current loss 0.098198, current_train_items 107200.
I0304 19:29:51.610465 22849303695488 run.py:483] Algo bellman_ford step 3350 current loss 0.019073, current_train_items 107232.
I0304 19:29:51.618732 22849303695488 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0304 19:29:51.618840 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:29:51.636502 22849303695488 run.py:483] Algo bellman_ford step 3351 current loss 0.054543, current_train_items 107264.
I0304 19:29:51.661871 22849303695488 run.py:483] Algo bellman_ford step 3352 current loss 0.074986, current_train_items 107296.
I0304 19:29:51.694744 22849303695488 run.py:483] Algo bellman_ford step 3353 current loss 0.064279, current_train_items 107328.
I0304 19:29:51.729671 22849303695488 run.py:483] Algo bellman_ford step 3354 current loss 0.084575, current_train_items 107360.
I0304 19:29:51.749749 22849303695488 run.py:483] Algo bellman_ford step 3355 current loss 0.004359, current_train_items 107392.
I0304 19:29:51.766028 22849303695488 run.py:483] Algo bellman_ford step 3356 current loss 0.069150, current_train_items 107424.
I0304 19:29:51.790543 22849303695488 run.py:483] Algo bellman_ford step 3357 current loss 0.072436, current_train_items 107456.
I0304 19:29:51.822219 22849303695488 run.py:483] Algo bellman_ford step 3358 current loss 0.084221, current_train_items 107488.
I0304 19:29:51.858081 22849303695488 run.py:483] Algo bellman_ford step 3359 current loss 0.088120, current_train_items 107520.
I0304 19:29:51.878516 22849303695488 run.py:483] Algo bellman_ford step 3360 current loss 0.009519, current_train_items 107552.
I0304 19:29:51.895112 22849303695488 run.py:483] Algo bellman_ford step 3361 current loss 0.021251, current_train_items 107584.
I0304 19:29:51.919738 22849303695488 run.py:483] Algo bellman_ford step 3362 current loss 0.139154, current_train_items 107616.
I0304 19:29:51.951264 22849303695488 run.py:483] Algo bellman_ford step 3363 current loss 0.179849, current_train_items 107648.
I0304 19:29:51.984806 22849303695488 run.py:483] Algo bellman_ford step 3364 current loss 0.126379, current_train_items 107680.
I0304 19:29:52.005081 22849303695488 run.py:483] Algo bellman_ford step 3365 current loss 0.020521, current_train_items 107712.
I0304 19:29:52.021472 22849303695488 run.py:483] Algo bellman_ford step 3366 current loss 0.017389, current_train_items 107744.
I0304 19:29:52.046675 22849303695488 run.py:483] Algo bellman_ford step 3367 current loss 0.103218, current_train_items 107776.
I0304 19:29:52.077308 22849303695488 run.py:483] Algo bellman_ford step 3368 current loss 0.079645, current_train_items 107808.
I0304 19:29:52.110697 22849303695488 run.py:483] Algo bellman_ford step 3369 current loss 0.085544, current_train_items 107840.
I0304 19:29:52.131418 22849303695488 run.py:483] Algo bellman_ford step 3370 current loss 0.004729, current_train_items 107872.
I0304 19:29:52.148236 22849303695488 run.py:483] Algo bellman_ford step 3371 current loss 0.042116, current_train_items 107904.
I0304 19:29:52.172451 22849303695488 run.py:483] Algo bellman_ford step 3372 current loss 0.078602, current_train_items 107936.
I0304 19:29:52.205604 22849303695488 run.py:483] Algo bellman_ford step 3373 current loss 0.136540, current_train_items 107968.
I0304 19:29:52.239335 22849303695488 run.py:483] Algo bellman_ford step 3374 current loss 0.193783, current_train_items 108000.
I0304 19:29:52.259854 22849303695488 run.py:483] Algo bellman_ford step 3375 current loss 0.008928, current_train_items 108032.
I0304 19:29:52.276228 22849303695488 run.py:483] Algo bellman_ford step 3376 current loss 0.017909, current_train_items 108064.
I0304 19:29:52.299877 22849303695488 run.py:483] Algo bellman_ford step 3377 current loss 0.076295, current_train_items 108096.
I0304 19:29:52.330573 22849303695488 run.py:483] Algo bellman_ford step 3378 current loss 0.055088, current_train_items 108128.
I0304 19:29:52.366522 22849303695488 run.py:483] Algo bellman_ford step 3379 current loss 0.092094, current_train_items 108160.
I0304 19:29:52.386699 22849303695488 run.py:483] Algo bellman_ford step 3380 current loss 0.004046, current_train_items 108192.
I0304 19:29:52.403565 22849303695488 run.py:483] Algo bellman_ford step 3381 current loss 0.063305, current_train_items 108224.
I0304 19:29:52.428921 22849303695488 run.py:483] Algo bellman_ford step 3382 current loss 0.032943, current_train_items 108256.
I0304 19:29:52.459858 22849303695488 run.py:483] Algo bellman_ford step 3383 current loss 0.119468, current_train_items 108288.
I0304 19:29:52.495892 22849303695488 run.py:483] Algo bellman_ford step 3384 current loss 0.098580, current_train_items 108320.
I0304 19:29:52.516638 22849303695488 run.py:483] Algo bellman_ford step 3385 current loss 0.004386, current_train_items 108352.
I0304 19:29:52.533535 22849303695488 run.py:483] Algo bellman_ford step 3386 current loss 0.029464, current_train_items 108384.
I0304 19:29:52.557402 22849303695488 run.py:483] Algo bellman_ford step 3387 current loss 0.124964, current_train_items 108416.
I0304 19:29:52.588399 22849303695488 run.py:483] Algo bellman_ford step 3388 current loss 0.119364, current_train_items 108448.
I0304 19:29:52.621223 22849303695488 run.py:483] Algo bellman_ford step 3389 current loss 0.196377, current_train_items 108480.
I0304 19:29:52.641794 22849303695488 run.py:483] Algo bellman_ford step 3390 current loss 0.016150, current_train_items 108512.
I0304 19:29:52.658517 22849303695488 run.py:483] Algo bellman_ford step 3391 current loss 0.019882, current_train_items 108544.
I0304 19:29:52.682616 22849303695488 run.py:483] Algo bellman_ford step 3392 current loss 0.056652, current_train_items 108576.
I0304 19:29:52.713578 22849303695488 run.py:483] Algo bellman_ford step 3393 current loss 0.063930, current_train_items 108608.
I0304 19:29:52.745656 22849303695488 run.py:483] Algo bellman_ford step 3394 current loss 0.063662, current_train_items 108640.
I0304 19:29:52.765795 22849303695488 run.py:483] Algo bellman_ford step 3395 current loss 0.003824, current_train_items 108672.
I0304 19:29:52.782556 22849303695488 run.py:483] Algo bellman_ford step 3396 current loss 0.021126, current_train_items 108704.
I0304 19:29:52.806579 22849303695488 run.py:483] Algo bellman_ford step 3397 current loss 0.038198, current_train_items 108736.
I0304 19:29:52.838775 22849303695488 run.py:483] Algo bellman_ford step 3398 current loss 0.096541, current_train_items 108768.
I0304 19:29:52.872130 22849303695488 run.py:483] Algo bellman_ford step 3399 current loss 0.083551, current_train_items 108800.
I0304 19:29:52.892783 22849303695488 run.py:483] Algo bellman_ford step 3400 current loss 0.008752, current_train_items 108832.
I0304 19:29:52.900741 22849303695488 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0304 19:29:52.900848 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:29:52.918394 22849303695488 run.py:483] Algo bellman_ford step 3401 current loss 0.025823, current_train_items 108864.
I0304 19:29:52.943068 22849303695488 run.py:483] Algo bellman_ford step 3402 current loss 0.051425, current_train_items 108896.
I0304 19:29:52.975460 22849303695488 run.py:483] Algo bellman_ford step 3403 current loss 0.087445, current_train_items 108928.
I0304 19:29:53.011330 22849303695488 run.py:483] Algo bellman_ford step 3404 current loss 0.115327, current_train_items 108960.
I0304 19:29:53.031486 22849303695488 run.py:483] Algo bellman_ford step 3405 current loss 0.004463, current_train_items 108992.
I0304 19:29:53.047456 22849303695488 run.py:483] Algo bellman_ford step 3406 current loss 0.048746, current_train_items 109024.
I0304 19:29:53.072416 22849303695488 run.py:483] Algo bellman_ford step 3407 current loss 0.140647, current_train_items 109056.
I0304 19:29:53.105512 22849303695488 run.py:483] Algo bellman_ford step 3408 current loss 0.186406, current_train_items 109088.
I0304 19:29:53.139043 22849303695488 run.py:483] Algo bellman_ford step 3409 current loss 0.143169, current_train_items 109120.
I0304 19:29:53.159210 22849303695488 run.py:483] Algo bellman_ford step 3410 current loss 0.006277, current_train_items 109152.
I0304 19:29:53.176148 22849303695488 run.py:483] Algo bellman_ford step 3411 current loss 0.053376, current_train_items 109184.
I0304 19:29:53.201283 22849303695488 run.py:483] Algo bellman_ford step 3412 current loss 0.067508, current_train_items 109216.
I0304 19:29:53.234453 22849303695488 run.py:483] Algo bellman_ford step 3413 current loss 0.126100, current_train_items 109248.
I0304 19:29:53.271460 22849303695488 run.py:483] Algo bellman_ford step 3414 current loss 0.182418, current_train_items 109280.
I0304 19:29:53.291786 22849303695488 run.py:483] Algo bellman_ford step 3415 current loss 0.009854, current_train_items 109312.
I0304 19:29:53.308486 22849303695488 run.py:483] Algo bellman_ford step 3416 current loss 0.034688, current_train_items 109344.
I0304 19:29:53.333219 22849303695488 run.py:483] Algo bellman_ford step 3417 current loss 0.111725, current_train_items 109376.
I0304 19:29:53.365681 22849303695488 run.py:483] Algo bellman_ford step 3418 current loss 0.090093, current_train_items 109408.
I0304 19:29:53.399270 22849303695488 run.py:483] Algo bellman_ford step 3419 current loss 0.082269, current_train_items 109440.
I0304 19:29:53.419376 22849303695488 run.py:483] Algo bellman_ford step 3420 current loss 0.009567, current_train_items 109472.
I0304 19:29:53.435681 22849303695488 run.py:483] Algo bellman_ford step 3421 current loss 0.031056, current_train_items 109504.
I0304 19:29:53.460212 22849303695488 run.py:483] Algo bellman_ford step 3422 current loss 0.091421, current_train_items 109536.
I0304 19:29:53.492762 22849303695488 run.py:483] Algo bellman_ford step 3423 current loss 0.067764, current_train_items 109568.
I0304 19:29:53.526773 22849303695488 run.py:483] Algo bellman_ford step 3424 current loss 0.093788, current_train_items 109600.
I0304 19:29:53.546984 22849303695488 run.py:483] Algo bellman_ford step 3425 current loss 0.013741, current_train_items 109632.
I0304 19:29:53.564033 22849303695488 run.py:483] Algo bellman_ford step 3426 current loss 0.031660, current_train_items 109664.
I0304 19:29:53.588124 22849303695488 run.py:483] Algo bellman_ford step 3427 current loss 0.062966, current_train_items 109696.
I0304 19:29:53.620125 22849303695488 run.py:483] Algo bellman_ford step 3428 current loss 0.069271, current_train_items 109728.
I0304 19:29:53.651833 22849303695488 run.py:483] Algo bellman_ford step 3429 current loss 0.041074, current_train_items 109760.
I0304 19:29:53.672292 22849303695488 run.py:483] Algo bellman_ford step 3430 current loss 0.006491, current_train_items 109792.
I0304 19:29:53.689112 22849303695488 run.py:483] Algo bellman_ford step 3431 current loss 0.011015, current_train_items 109824.
I0304 19:29:53.713095 22849303695488 run.py:483] Algo bellman_ford step 3432 current loss 0.061372, current_train_items 109856.
I0304 19:29:53.746961 22849303695488 run.py:483] Algo bellman_ford step 3433 current loss 0.092716, current_train_items 109888.
I0304 19:29:53.779963 22849303695488 run.py:483] Algo bellman_ford step 3434 current loss 0.115575, current_train_items 109920.
I0304 19:29:53.799923 22849303695488 run.py:483] Algo bellman_ford step 3435 current loss 0.004265, current_train_items 109952.
I0304 19:29:53.816523 22849303695488 run.py:483] Algo bellman_ford step 3436 current loss 0.019026, current_train_items 109984.
I0304 19:29:53.840980 22849303695488 run.py:483] Algo bellman_ford step 3437 current loss 0.071458, current_train_items 110016.
I0304 19:29:53.871986 22849303695488 run.py:483] Algo bellman_ford step 3438 current loss 0.064134, current_train_items 110048.
I0304 19:29:53.904301 22849303695488 run.py:483] Algo bellman_ford step 3439 current loss 0.131187, current_train_items 110080.
I0304 19:29:53.924844 22849303695488 run.py:483] Algo bellman_ford step 3440 current loss 0.031048, current_train_items 110112.
I0304 19:29:53.941686 22849303695488 run.py:483] Algo bellman_ford step 3441 current loss 0.048192, current_train_items 110144.
I0304 19:29:53.965525 22849303695488 run.py:483] Algo bellman_ford step 3442 current loss 0.072129, current_train_items 110176.
I0304 19:29:53.997960 22849303695488 run.py:483] Algo bellman_ford step 3443 current loss 0.164875, current_train_items 110208.
I0304 19:29:54.033402 22849303695488 run.py:483] Algo bellman_ford step 3444 current loss 0.146486, current_train_items 110240.
I0304 19:29:54.053375 22849303695488 run.py:483] Algo bellman_ford step 3445 current loss 0.006670, current_train_items 110272.
I0304 19:29:54.070273 22849303695488 run.py:483] Algo bellman_ford step 3446 current loss 0.026281, current_train_items 110304.
I0304 19:29:54.095275 22849303695488 run.py:483] Algo bellman_ford step 3447 current loss 0.094527, current_train_items 110336.
I0304 19:29:54.126658 22849303695488 run.py:483] Algo bellman_ford step 3448 current loss 0.118624, current_train_items 110368.
I0304 19:29:54.160929 22849303695488 run.py:483] Algo bellman_ford step 3449 current loss 0.106724, current_train_items 110400.
I0304 19:29:54.181197 22849303695488 run.py:483] Algo bellman_ford step 3450 current loss 0.027565, current_train_items 110432.
I0304 19:29:54.189630 22849303695488 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0304 19:29:54.189739 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:29:54.206821 22849303695488 run.py:483] Algo bellman_ford step 3451 current loss 0.029106, current_train_items 110464.
I0304 19:29:54.231865 22849303695488 run.py:483] Algo bellman_ford step 3452 current loss 0.131560, current_train_items 110496.
I0304 19:29:54.262732 22849303695488 run.py:483] Algo bellman_ford step 3453 current loss 0.128065, current_train_items 110528.
I0304 19:29:54.299206 22849303695488 run.py:483] Algo bellman_ford step 3454 current loss 0.187309, current_train_items 110560.
I0304 19:29:54.319675 22849303695488 run.py:483] Algo bellman_ford step 3455 current loss 0.004704, current_train_items 110592.
I0304 19:29:54.335694 22849303695488 run.py:483] Algo bellman_ford step 3456 current loss 0.043111, current_train_items 110624.
I0304 19:29:54.359969 22849303695488 run.py:483] Algo bellman_ford step 3457 current loss 0.061860, current_train_items 110656.
I0304 19:29:54.393418 22849303695488 run.py:483] Algo bellman_ford step 3458 current loss 0.130810, current_train_items 110688.
I0304 19:29:54.426856 22849303695488 run.py:483] Algo bellman_ford step 3459 current loss 0.119448, current_train_items 110720.
I0304 19:29:54.447320 22849303695488 run.py:483] Algo bellman_ford step 3460 current loss 0.047259, current_train_items 110752.
I0304 19:29:54.464236 22849303695488 run.py:483] Algo bellman_ford step 3461 current loss 0.031526, current_train_items 110784.
I0304 19:29:54.488176 22849303695488 run.py:483] Algo bellman_ford step 3462 current loss 0.067273, current_train_items 110816.
I0304 19:29:54.519814 22849303695488 run.py:483] Algo bellman_ford step 3463 current loss 0.080686, current_train_items 110848.
I0304 19:29:54.553075 22849303695488 run.py:483] Algo bellman_ford step 3464 current loss 0.103348, current_train_items 110880.
I0304 19:29:54.573264 22849303695488 run.py:483] Algo bellman_ford step 3465 current loss 0.026543, current_train_items 110912.
I0304 19:29:54.590125 22849303695488 run.py:483] Algo bellman_ford step 3466 current loss 0.029832, current_train_items 110944.
I0304 19:29:54.614944 22849303695488 run.py:483] Algo bellman_ford step 3467 current loss 0.071589, current_train_items 110976.
I0304 19:29:54.648088 22849303695488 run.py:483] Algo bellman_ford step 3468 current loss 0.082468, current_train_items 111008.
I0304 19:29:54.684489 22849303695488 run.py:483] Algo bellman_ford step 3469 current loss 0.114970, current_train_items 111040.
I0304 19:29:54.704933 22849303695488 run.py:483] Algo bellman_ford step 3470 current loss 0.013552, current_train_items 111072.
I0304 19:29:54.722171 22849303695488 run.py:483] Algo bellman_ford step 3471 current loss 0.038418, current_train_items 111104.
I0304 19:29:54.746133 22849303695488 run.py:483] Algo bellman_ford step 3472 current loss 0.055519, current_train_items 111136.
I0304 19:29:54.779201 22849303695488 run.py:483] Algo bellman_ford step 3473 current loss 0.085133, current_train_items 111168.
I0304 19:29:54.813566 22849303695488 run.py:483] Algo bellman_ford step 3474 current loss 0.079346, current_train_items 111200.
I0304 19:29:54.834096 22849303695488 run.py:483] Algo bellman_ford step 3475 current loss 0.008315, current_train_items 111232.
I0304 19:29:54.850633 22849303695488 run.py:483] Algo bellman_ford step 3476 current loss 0.037693, current_train_items 111264.
I0304 19:29:54.874167 22849303695488 run.py:483] Algo bellman_ford step 3477 current loss 0.082359, current_train_items 111296.
I0304 19:29:54.905234 22849303695488 run.py:483] Algo bellman_ford step 3478 current loss 0.074332, current_train_items 111328.
I0304 19:29:54.940474 22849303695488 run.py:483] Algo bellman_ford step 3479 current loss 0.083059, current_train_items 111360.
I0304 19:29:54.960900 22849303695488 run.py:483] Algo bellman_ford step 3480 current loss 0.006447, current_train_items 111392.
I0304 19:29:54.977369 22849303695488 run.py:483] Algo bellman_ford step 3481 current loss 0.036036, current_train_items 111424.
I0304 19:29:55.001636 22849303695488 run.py:483] Algo bellman_ford step 3482 current loss 0.052452, current_train_items 111456.
I0304 19:29:55.032341 22849303695488 run.py:483] Algo bellman_ford step 3483 current loss 0.076399, current_train_items 111488.
I0304 19:29:55.065803 22849303695488 run.py:483] Algo bellman_ford step 3484 current loss 0.068079, current_train_items 111520.
I0304 19:29:55.086331 22849303695488 run.py:483] Algo bellman_ford step 3485 current loss 0.007748, current_train_items 111552.
I0304 19:29:55.103204 22849303695488 run.py:483] Algo bellman_ford step 3486 current loss 0.027438, current_train_items 111584.
I0304 19:29:55.127756 22849303695488 run.py:483] Algo bellman_ford step 3487 current loss 0.074942, current_train_items 111616.
I0304 19:29:55.158728 22849303695488 run.py:483] Algo bellman_ford step 3488 current loss 0.094082, current_train_items 111648.
I0304 19:29:55.191068 22849303695488 run.py:483] Algo bellman_ford step 3489 current loss 0.093846, current_train_items 111680.
I0304 19:29:55.211689 22849303695488 run.py:483] Algo bellman_ford step 3490 current loss 0.004316, current_train_items 111712.
I0304 19:29:55.228137 22849303695488 run.py:483] Algo bellman_ford step 3491 current loss 0.009250, current_train_items 111744.
I0304 19:29:55.252278 22849303695488 run.py:483] Algo bellman_ford step 3492 current loss 0.050945, current_train_items 111776.
I0304 19:29:55.284855 22849303695488 run.py:483] Algo bellman_ford step 3493 current loss 0.101017, current_train_items 111808.
I0304 19:29:55.318137 22849303695488 run.py:483] Algo bellman_ford step 3494 current loss 0.097064, current_train_items 111840.
I0304 19:29:55.338348 22849303695488 run.py:483] Algo bellman_ford step 3495 current loss 0.004824, current_train_items 111872.
I0304 19:29:55.355109 22849303695488 run.py:483] Algo bellman_ford step 3496 current loss 0.039789, current_train_items 111904.
I0304 19:29:55.379071 22849303695488 run.py:483] Algo bellman_ford step 3497 current loss 0.064340, current_train_items 111936.
I0304 19:29:55.410963 22849303695488 run.py:483] Algo bellman_ford step 3498 current loss 0.140685, current_train_items 111968.
I0304 19:29:55.445694 22849303695488 run.py:483] Algo bellman_ford step 3499 current loss 0.109481, current_train_items 112000.
I0304 19:29:55.466217 22849303695488 run.py:483] Algo bellman_ford step 3500 current loss 0.006639, current_train_items 112032.
I0304 19:29:55.474154 22849303695488 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.9453125, 'score': 0.9453125, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0304 19:29:55.474263 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.945, val scores are: bellman_ford: 0.945
I0304 19:29:55.490878 22849303695488 run.py:483] Algo bellman_ford step 3501 current loss 0.031146, current_train_items 112064.
I0304 19:29:55.515403 22849303695488 run.py:483] Algo bellman_ford step 3502 current loss 0.149793, current_train_items 112096.
I0304 19:29:55.547578 22849303695488 run.py:483] Algo bellman_ford step 3503 current loss 0.141583, current_train_items 112128.
I0304 19:29:55.584279 22849303695488 run.py:483] Algo bellman_ford step 3504 current loss 0.145233, current_train_items 112160.
I0304 19:29:55.604689 22849303695488 run.py:483] Algo bellman_ford step 3505 current loss 0.029643, current_train_items 112192.
I0304 19:29:55.620322 22849303695488 run.py:483] Algo bellman_ford step 3506 current loss 0.029287, current_train_items 112224.
I0304 19:29:55.645422 22849303695488 run.py:483] Algo bellman_ford step 3507 current loss 0.072650, current_train_items 112256.
I0304 19:29:55.676438 22849303695488 run.py:483] Algo bellman_ford step 3508 current loss 0.116515, current_train_items 112288.
I0304 19:29:55.707847 22849303695488 run.py:483] Algo bellman_ford step 3509 current loss 0.089118, current_train_items 112320.
I0304 19:29:55.727807 22849303695488 run.py:483] Algo bellman_ford step 3510 current loss 0.024664, current_train_items 112352.
I0304 19:29:55.744234 22849303695488 run.py:483] Algo bellman_ford step 3511 current loss 0.015641, current_train_items 112384.
I0304 19:29:55.767854 22849303695488 run.py:483] Algo bellman_ford step 3512 current loss 0.046552, current_train_items 112416.
I0304 19:29:55.798470 22849303695488 run.py:483] Algo bellman_ford step 3513 current loss 0.061263, current_train_items 112448.
I0304 19:29:55.831228 22849303695488 run.py:483] Algo bellman_ford step 3514 current loss 0.088942, current_train_items 112480.
I0304 19:29:55.851142 22849303695488 run.py:483] Algo bellman_ford step 3515 current loss 0.003310, current_train_items 112512.
I0304 19:29:55.867199 22849303695488 run.py:483] Algo bellman_ford step 3516 current loss 0.011835, current_train_items 112544.
I0304 19:29:55.891422 22849303695488 run.py:483] Algo bellman_ford step 3517 current loss 0.075697, current_train_items 112576.
I0304 19:29:55.923357 22849303695488 run.py:483] Algo bellman_ford step 3518 current loss 0.183793, current_train_items 112608.
I0304 19:29:55.959604 22849303695488 run.py:483] Algo bellman_ford step 3519 current loss 0.170074, current_train_items 112640.
I0304 19:29:55.979683 22849303695488 run.py:483] Algo bellman_ford step 3520 current loss 0.006087, current_train_items 112672.
I0304 19:29:55.996188 22849303695488 run.py:483] Algo bellman_ford step 3521 current loss 0.032275, current_train_items 112704.
I0304 19:29:56.019719 22849303695488 run.py:483] Algo bellman_ford step 3522 current loss 0.047401, current_train_items 112736.
I0304 19:29:56.051299 22849303695488 run.py:483] Algo bellman_ford step 3523 current loss 0.114237, current_train_items 112768.
I0304 19:29:56.084836 22849303695488 run.py:483] Algo bellman_ford step 3524 current loss 0.253009, current_train_items 112800.
I0304 19:29:56.104865 22849303695488 run.py:483] Algo bellman_ford step 3525 current loss 0.121608, current_train_items 112832.
I0304 19:29:56.121388 22849303695488 run.py:483] Algo bellman_ford step 3526 current loss 0.035887, current_train_items 112864.
I0304 19:29:56.145733 22849303695488 run.py:483] Algo bellman_ford step 3527 current loss 0.059026, current_train_items 112896.
I0304 19:29:56.178946 22849303695488 run.py:483] Algo bellman_ford step 3528 current loss 0.081862, current_train_items 112928.
I0304 19:29:56.211877 22849303695488 run.py:483] Algo bellman_ford step 3529 current loss 0.084024, current_train_items 112960.
I0304 19:29:56.231769 22849303695488 run.py:483] Algo bellman_ford step 3530 current loss 0.007475, current_train_items 112992.
I0304 19:29:56.248494 22849303695488 run.py:483] Algo bellman_ford step 3531 current loss 0.020243, current_train_items 113024.
I0304 19:29:56.274108 22849303695488 run.py:483] Algo bellman_ford step 3532 current loss 0.077257, current_train_items 113056.
I0304 19:29:56.308843 22849303695488 run.py:483] Algo bellman_ford step 3533 current loss 0.098227, current_train_items 113088.
I0304 19:29:56.343130 22849303695488 run.py:483] Algo bellman_ford step 3534 current loss 0.122936, current_train_items 113120.
I0304 19:29:56.363113 22849303695488 run.py:483] Algo bellman_ford step 3535 current loss 0.049851, current_train_items 113152.
I0304 19:29:56.379766 22849303695488 run.py:483] Algo bellman_ford step 3536 current loss 0.049853, current_train_items 113184.
I0304 19:29:56.403902 22849303695488 run.py:483] Algo bellman_ford step 3537 current loss 0.064026, current_train_items 113216.
I0304 19:29:56.436224 22849303695488 run.py:483] Algo bellman_ford step 3538 current loss 0.091009, current_train_items 113248.
I0304 19:29:56.472238 22849303695488 run.py:483] Algo bellman_ford step 3539 current loss 0.074397, current_train_items 113280.
I0304 19:29:56.492238 22849303695488 run.py:483] Algo bellman_ford step 3540 current loss 0.008592, current_train_items 113312.
I0304 19:29:56.508675 22849303695488 run.py:483] Algo bellman_ford step 3541 current loss 0.025080, current_train_items 113344.
I0304 19:29:56.533299 22849303695488 run.py:483] Algo bellman_ford step 3542 current loss 0.077278, current_train_items 113376.
I0304 19:29:56.564379 22849303695488 run.py:483] Algo bellman_ford step 3543 current loss 0.062337, current_train_items 113408.
I0304 19:29:56.597779 22849303695488 run.py:483] Algo bellman_ford step 3544 current loss 0.100687, current_train_items 113440.
I0304 19:29:56.617588 22849303695488 run.py:483] Algo bellman_ford step 3545 current loss 0.006838, current_train_items 113472.
I0304 19:29:56.634165 22849303695488 run.py:483] Algo bellman_ford step 3546 current loss 0.016533, current_train_items 113504.
I0304 19:29:56.658660 22849303695488 run.py:483] Algo bellman_ford step 3547 current loss 0.094638, current_train_items 113536.
I0304 19:29:56.689052 22849303695488 run.py:483] Algo bellman_ford step 3548 current loss 0.084851, current_train_items 113568.
I0304 19:29:56.722834 22849303695488 run.py:483] Algo bellman_ford step 3549 current loss 0.120438, current_train_items 113600.
I0304 19:29:56.742974 22849303695488 run.py:483] Algo bellman_ford step 3550 current loss 0.023961, current_train_items 113632.
I0304 19:29:56.751651 22849303695488 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0304 19:29:56.751759 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:29:56.769421 22849303695488 run.py:483] Algo bellman_ford step 3551 current loss 0.034212, current_train_items 113664.
I0304 19:29:56.795653 22849303695488 run.py:483] Algo bellman_ford step 3552 current loss 0.081127, current_train_items 113696.
I0304 19:29:56.827534 22849303695488 run.py:483] Algo bellman_ford step 3553 current loss 0.053103, current_train_items 113728.
I0304 19:29:56.862067 22849303695488 run.py:483] Algo bellman_ford step 3554 current loss 0.073547, current_train_items 113760.
I0304 19:29:56.882401 22849303695488 run.py:483] Algo bellman_ford step 3555 current loss 0.010637, current_train_items 113792.
I0304 19:29:56.899000 22849303695488 run.py:483] Algo bellman_ford step 3556 current loss 0.034249, current_train_items 113824.
I0304 19:29:56.923331 22849303695488 run.py:483] Algo bellman_ford step 3557 current loss 0.077801, current_train_items 113856.
I0304 19:29:56.955503 22849303695488 run.py:483] Algo bellman_ford step 3558 current loss 0.106589, current_train_items 113888.
I0304 19:29:56.988778 22849303695488 run.py:483] Algo bellman_ford step 3559 current loss 0.083862, current_train_items 113920.
I0304 19:29:57.009136 22849303695488 run.py:483] Algo bellman_ford step 3560 current loss 0.005803, current_train_items 113952.
I0304 19:29:57.025750 22849303695488 run.py:483] Algo bellman_ford step 3561 current loss 0.033425, current_train_items 113984.
I0304 19:29:57.050718 22849303695488 run.py:483] Algo bellman_ford step 3562 current loss 0.120367, current_train_items 114016.
I0304 19:29:57.081412 22849303695488 run.py:483] Algo bellman_ford step 3563 current loss 0.089354, current_train_items 114048.
I0304 19:29:57.118859 22849303695488 run.py:483] Algo bellman_ford step 3564 current loss 0.182231, current_train_items 114080.
I0304 19:29:57.138828 22849303695488 run.py:483] Algo bellman_ford step 3565 current loss 0.006418, current_train_items 114112.
I0304 19:29:57.155581 22849303695488 run.py:483] Algo bellman_ford step 3566 current loss 0.031869, current_train_items 114144.
I0304 19:29:57.179710 22849303695488 run.py:483] Algo bellman_ford step 3567 current loss 0.039462, current_train_items 114176.
I0304 19:29:57.211163 22849303695488 run.py:483] Algo bellman_ford step 3568 current loss 0.078205, current_train_items 114208.
I0304 19:29:57.245573 22849303695488 run.py:483] Algo bellman_ford step 3569 current loss 0.130172, current_train_items 114240.
I0304 19:29:57.265583 22849303695488 run.py:483] Algo bellman_ford step 3570 current loss 0.003527, current_train_items 114272.
I0304 19:29:57.281938 22849303695488 run.py:483] Algo bellman_ford step 3571 current loss 0.020709, current_train_items 114304.
I0304 19:29:57.305846 22849303695488 run.py:483] Algo bellman_ford step 3572 current loss 0.094097, current_train_items 114336.
I0304 19:29:57.336493 22849303695488 run.py:483] Algo bellman_ford step 3573 current loss 0.085592, current_train_items 114368.
I0304 19:29:57.372401 22849303695488 run.py:483] Algo bellman_ford step 3574 current loss 0.150555, current_train_items 114400.
I0304 19:29:57.392503 22849303695488 run.py:483] Algo bellman_ford step 3575 current loss 0.007547, current_train_items 114432.
I0304 19:29:57.409398 22849303695488 run.py:483] Algo bellman_ford step 3576 current loss 0.020629, current_train_items 114464.
I0304 19:29:57.433155 22849303695488 run.py:483] Algo bellman_ford step 3577 current loss 0.132260, current_train_items 114496.
I0304 19:29:57.464482 22849303695488 run.py:483] Algo bellman_ford step 3578 current loss 0.133289, current_train_items 114528.
I0304 19:29:57.497631 22849303695488 run.py:483] Algo bellman_ford step 3579 current loss 0.087261, current_train_items 114560.
I0304 19:29:57.517709 22849303695488 run.py:483] Algo bellman_ford step 3580 current loss 0.004871, current_train_items 114592.
I0304 19:29:57.534401 22849303695488 run.py:483] Algo bellman_ford step 3581 current loss 0.090196, current_train_items 114624.
I0304 19:29:57.559653 22849303695488 run.py:483] Algo bellman_ford step 3582 current loss 0.097572, current_train_items 114656.
I0304 19:29:57.591728 22849303695488 run.py:483] Algo bellman_ford step 3583 current loss 0.086198, current_train_items 114688.
I0304 19:29:57.626327 22849303695488 run.py:483] Algo bellman_ford step 3584 current loss 0.077031, current_train_items 114720.
I0304 19:29:57.646444 22849303695488 run.py:483] Algo bellman_ford step 3585 current loss 0.002930, current_train_items 114752.
I0304 19:29:57.663159 22849303695488 run.py:483] Algo bellman_ford step 3586 current loss 0.061594, current_train_items 114784.
I0304 19:29:57.686528 22849303695488 run.py:483] Algo bellman_ford step 3587 current loss 0.106876, current_train_items 114816.
I0304 19:29:57.719211 22849303695488 run.py:483] Algo bellman_ford step 3588 current loss 0.092384, current_train_items 114848.
I0304 19:29:57.754274 22849303695488 run.py:483] Algo bellman_ford step 3589 current loss 0.096959, current_train_items 114880.
I0304 19:29:57.774259 22849303695488 run.py:483] Algo bellman_ford step 3590 current loss 0.005302, current_train_items 114912.
I0304 19:29:57.790708 22849303695488 run.py:483] Algo bellman_ford step 3591 current loss 0.014244, current_train_items 114944.
I0304 19:29:57.816164 22849303695488 run.py:483] Algo bellman_ford step 3592 current loss 0.077169, current_train_items 114976.
I0304 19:29:57.849154 22849303695488 run.py:483] Algo bellman_ford step 3593 current loss 0.152820, current_train_items 115008.
I0304 19:29:57.884173 22849303695488 run.py:483] Algo bellman_ford step 3594 current loss 0.107356, current_train_items 115040.
I0304 19:29:57.904101 22849303695488 run.py:483] Algo bellman_ford step 3595 current loss 0.004182, current_train_items 115072.
I0304 19:29:57.921012 22849303695488 run.py:483] Algo bellman_ford step 3596 current loss 0.053177, current_train_items 115104.
I0304 19:29:57.944826 22849303695488 run.py:483] Algo bellman_ford step 3597 current loss 0.055552, current_train_items 115136.
I0304 19:29:57.975341 22849303695488 run.py:483] Algo bellman_ford step 3598 current loss 0.081550, current_train_items 115168.
I0304 19:29:58.010063 22849303695488 run.py:483] Algo bellman_ford step 3599 current loss 0.100293, current_train_items 115200.
I0304 19:29:58.030299 22849303695488 run.py:483] Algo bellman_ford step 3600 current loss 0.003104, current_train_items 115232.
I0304 19:29:58.038221 22849303695488 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0304 19:29:58.038332 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:29:58.055876 22849303695488 run.py:483] Algo bellman_ford step 3601 current loss 0.022872, current_train_items 115264.
I0304 19:29:58.081122 22849303695488 run.py:483] Algo bellman_ford step 3602 current loss 0.038890, current_train_items 115296.
I0304 19:29:58.112751 22849303695488 run.py:483] Algo bellman_ford step 3603 current loss 0.071801, current_train_items 115328.
I0304 19:29:58.149678 22849303695488 run.py:483] Algo bellman_ford step 3604 current loss 0.105300, current_train_items 115360.
I0304 19:29:58.169792 22849303695488 run.py:483] Algo bellman_ford step 3605 current loss 0.005304, current_train_items 115392.
I0304 19:29:58.186587 22849303695488 run.py:483] Algo bellman_ford step 3606 current loss 0.028256, current_train_items 115424.
I0304 19:29:58.209888 22849303695488 run.py:483] Algo bellman_ford step 3607 current loss 0.018746, current_train_items 115456.
I0304 19:29:58.240544 22849303695488 run.py:483] Algo bellman_ford step 3608 current loss 0.057378, current_train_items 115488.
I0304 19:29:58.272720 22849303695488 run.py:483] Algo bellman_ford step 3609 current loss 0.086952, current_train_items 115520.
I0304 19:29:58.292895 22849303695488 run.py:483] Algo bellman_ford step 3610 current loss 0.005707, current_train_items 115552.
I0304 19:29:58.309753 22849303695488 run.py:483] Algo bellman_ford step 3611 current loss 0.019968, current_train_items 115584.
I0304 19:29:58.335265 22849303695488 run.py:483] Algo bellman_ford step 3612 current loss 0.083861, current_train_items 115616.
I0304 19:29:58.366137 22849303695488 run.py:483] Algo bellman_ford step 3613 current loss 0.075301, current_train_items 115648.
I0304 19:29:58.398779 22849303695488 run.py:483] Algo bellman_ford step 3614 current loss 0.069566, current_train_items 115680.
I0304 19:29:58.418597 22849303695488 run.py:483] Algo bellman_ford step 3615 current loss 0.012550, current_train_items 115712.
I0304 19:29:58.435605 22849303695488 run.py:483] Algo bellman_ford step 3616 current loss 0.034288, current_train_items 115744.
I0304 19:29:58.459502 22849303695488 run.py:483] Algo bellman_ford step 3617 current loss 0.042861, current_train_items 115776.
I0304 19:29:58.491601 22849303695488 run.py:483] Algo bellman_ford step 3618 current loss 0.101349, current_train_items 115808.
I0304 19:29:58.522510 22849303695488 run.py:483] Algo bellman_ford step 3619 current loss 0.080088, current_train_items 115840.
I0304 19:29:58.542546 22849303695488 run.py:483] Algo bellman_ford step 3620 current loss 0.004836, current_train_items 115872.
I0304 19:29:58.559151 22849303695488 run.py:483] Algo bellman_ford step 3621 current loss 0.040017, current_train_items 115904.
I0304 19:29:58.583320 22849303695488 run.py:483] Algo bellman_ford step 3622 current loss 0.076892, current_train_items 115936.
I0304 19:29:58.615558 22849303695488 run.py:483] Algo bellman_ford step 3623 current loss 0.098999, current_train_items 115968.
I0304 19:29:58.649300 22849303695488 run.py:483] Algo bellman_ford step 3624 current loss 0.110897, current_train_items 116000.
I0304 19:29:58.669125 22849303695488 run.py:483] Algo bellman_ford step 3625 current loss 0.009679, current_train_items 116032.
I0304 19:29:58.685804 22849303695488 run.py:483] Algo bellman_ford step 3626 current loss 0.057734, current_train_items 116064.
I0304 19:29:58.710336 22849303695488 run.py:483] Algo bellman_ford step 3627 current loss 0.063958, current_train_items 116096.
I0304 19:29:58.742685 22849303695488 run.py:483] Algo bellman_ford step 3628 current loss 0.101133, current_train_items 116128.
I0304 19:29:58.776142 22849303695488 run.py:483] Algo bellman_ford step 3629 current loss 0.084196, current_train_items 116160.
I0304 19:29:58.796242 22849303695488 run.py:483] Algo bellman_ford step 3630 current loss 0.018079, current_train_items 116192.
I0304 19:29:58.812618 22849303695488 run.py:483] Algo bellman_ford step 3631 current loss 0.054443, current_train_items 116224.
I0304 19:29:58.837092 22849303695488 run.py:483] Algo bellman_ford step 3632 current loss 0.053326, current_train_items 116256.
I0304 19:29:58.868622 22849303695488 run.py:483] Algo bellman_ford step 3633 current loss 0.080852, current_train_items 116288.
I0304 19:29:58.901922 22849303695488 run.py:483] Algo bellman_ford step 3634 current loss 0.117744, current_train_items 116320.
I0304 19:29:58.921908 22849303695488 run.py:483] Algo bellman_ford step 3635 current loss 0.008178, current_train_items 116352.
I0304 19:29:58.938506 22849303695488 run.py:483] Algo bellman_ford step 3636 current loss 0.064790, current_train_items 116384.
I0304 19:29:58.961788 22849303695488 run.py:483] Algo bellman_ford step 3637 current loss 0.105891, current_train_items 116416.
I0304 19:29:58.993696 22849303695488 run.py:483] Algo bellman_ford step 3638 current loss 0.088553, current_train_items 116448.
I0304 19:29:59.026983 22849303695488 run.py:483] Algo bellman_ford step 3639 current loss 0.082277, current_train_items 116480.
I0304 19:29:59.046764 22849303695488 run.py:483] Algo bellman_ford step 3640 current loss 0.005272, current_train_items 116512.
I0304 19:29:59.062969 22849303695488 run.py:483] Algo bellman_ford step 3641 current loss 0.029402, current_train_items 116544.
I0304 19:29:59.087949 22849303695488 run.py:483] Algo bellman_ford step 3642 current loss 0.123377, current_train_items 116576.
I0304 19:29:59.122591 22849303695488 run.py:483] Algo bellman_ford step 3643 current loss 0.145808, current_train_items 116608.
I0304 19:29:59.157590 22849303695488 run.py:483] Algo bellman_ford step 3644 current loss 0.144015, current_train_items 116640.
I0304 19:29:59.177690 22849303695488 run.py:483] Algo bellman_ford step 3645 current loss 0.005064, current_train_items 116672.
I0304 19:29:59.194231 22849303695488 run.py:483] Algo bellman_ford step 3646 current loss 0.027458, current_train_items 116704.
I0304 19:29:59.220068 22849303695488 run.py:483] Algo bellman_ford step 3647 current loss 0.107465, current_train_items 116736.
I0304 19:29:59.250941 22849303695488 run.py:483] Algo bellman_ford step 3648 current loss 0.088177, current_train_items 116768.
I0304 19:29:59.285331 22849303695488 run.py:483] Algo bellman_ford step 3649 current loss 0.129481, current_train_items 116800.
I0304 19:29:59.304895 22849303695488 run.py:483] Algo bellman_ford step 3650 current loss 0.002505, current_train_items 116832.
I0304 19:29:59.313084 22849303695488 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0304 19:29:59.313193 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:29:59.330832 22849303695488 run.py:483] Algo bellman_ford step 3651 current loss 0.018035, current_train_items 116864.
I0304 19:29:59.354356 22849303695488 run.py:483] Algo bellman_ford step 3652 current loss 0.034727, current_train_items 116896.
I0304 19:29:59.385876 22849303695488 run.py:483] Algo bellman_ford step 3653 current loss 0.037718, current_train_items 116928.
I0304 19:29:59.419853 22849303695488 run.py:483] Algo bellman_ford step 3654 current loss 0.090945, current_train_items 116960.
I0304 19:29:59.439919 22849303695488 run.py:483] Algo bellman_ford step 3655 current loss 0.003456, current_train_items 116992.
I0304 19:29:59.455793 22849303695488 run.py:483] Algo bellman_ford step 3656 current loss 0.033504, current_train_items 117024.
I0304 19:29:59.480357 22849303695488 run.py:483] Algo bellman_ford step 3657 current loss 0.055282, current_train_items 117056.
I0304 19:29:59.511612 22849303695488 run.py:483] Algo bellman_ford step 3658 current loss 0.062082, current_train_items 117088.
I0304 19:29:59.545773 22849303695488 run.py:483] Algo bellman_ford step 3659 current loss 0.066729, current_train_items 117120.
I0304 19:29:59.566185 22849303695488 run.py:483] Algo bellman_ford step 3660 current loss 0.003950, current_train_items 117152.
I0304 19:29:59.583135 22849303695488 run.py:483] Algo bellman_ford step 3661 current loss 0.016358, current_train_items 117184.
I0304 19:29:59.607018 22849303695488 run.py:483] Algo bellman_ford step 3662 current loss 0.036492, current_train_items 117216.
I0304 19:29:59.639513 22849303695488 run.py:483] Algo bellman_ford step 3663 current loss 0.080652, current_train_items 117248.
I0304 19:29:59.674675 22849303695488 run.py:483] Algo bellman_ford step 3664 current loss 0.134153, current_train_items 117280.
I0304 19:29:59.694498 22849303695488 run.py:483] Algo bellman_ford step 3665 current loss 0.047784, current_train_items 117312.
I0304 19:29:59.710851 22849303695488 run.py:483] Algo bellman_ford step 3666 current loss 0.036614, current_train_items 117344.
I0304 19:29:59.735224 22849303695488 run.py:483] Algo bellman_ford step 3667 current loss 0.041270, current_train_items 117376.
I0304 19:29:59.767381 22849303695488 run.py:483] Algo bellman_ford step 3668 current loss 0.083111, current_train_items 117408.
I0304 19:29:59.802822 22849303695488 run.py:483] Algo bellman_ford step 3669 current loss 0.088145, current_train_items 117440.
I0304 19:29:59.823194 22849303695488 run.py:483] Algo bellman_ford step 3670 current loss 0.014385, current_train_items 117472.
I0304 19:29:59.840123 22849303695488 run.py:483] Algo bellman_ford step 3671 current loss 0.080102, current_train_items 117504.
I0304 19:29:59.863774 22849303695488 run.py:483] Algo bellman_ford step 3672 current loss 0.106629, current_train_items 117536.
I0304 19:29:59.895369 22849303695488 run.py:483] Algo bellman_ford step 3673 current loss 0.141089, current_train_items 117568.
I0304 19:29:59.930481 22849303695488 run.py:483] Algo bellman_ford step 3674 current loss 0.168064, current_train_items 117600.
I0304 19:29:59.950534 22849303695488 run.py:483] Algo bellman_ford step 3675 current loss 0.008790, current_train_items 117632.
I0304 19:29:59.966813 22849303695488 run.py:483] Algo bellman_ford step 3676 current loss 0.024518, current_train_items 117664.
I0304 19:29:59.990763 22849303695488 run.py:483] Algo bellman_ford step 3677 current loss 0.048005, current_train_items 117696.
I0304 19:30:00.023092 22849303695488 run.py:483] Algo bellman_ford step 3678 current loss 0.105492, current_train_items 117728.
I0304 19:30:00.057612 22849303695488 run.py:483] Algo bellman_ford step 3679 current loss 0.181355, current_train_items 117760.
I0304 19:30:00.077379 22849303695488 run.py:483] Algo bellman_ford step 3680 current loss 0.007369, current_train_items 117792.
I0304 19:30:00.094125 22849303695488 run.py:483] Algo bellman_ford step 3681 current loss 0.030519, current_train_items 117824.
I0304 19:30:00.118689 22849303695488 run.py:483] Algo bellman_ford step 3682 current loss 0.060582, current_train_items 117856.
I0304 19:30:00.149660 22849303695488 run.py:483] Algo bellman_ford step 3683 current loss 0.089264, current_train_items 117888.
I0304 19:30:00.184057 22849303695488 run.py:483] Algo bellman_ford step 3684 current loss 0.084609, current_train_items 117920.
I0304 19:30:00.204149 22849303695488 run.py:483] Algo bellman_ford step 3685 current loss 0.004316, current_train_items 117952.
I0304 19:30:00.220862 22849303695488 run.py:483] Algo bellman_ford step 3686 current loss 0.022457, current_train_items 117984.
I0304 19:30:00.244400 22849303695488 run.py:483] Algo bellman_ford step 3687 current loss 0.120198, current_train_items 118016.
I0304 19:30:00.277354 22849303695488 run.py:483] Algo bellman_ford step 3688 current loss 0.089520, current_train_items 118048.
I0304 19:30:00.313249 22849303695488 run.py:483] Algo bellman_ford step 3689 current loss 0.068746, current_train_items 118080.
I0304 19:30:00.333596 22849303695488 run.py:483] Algo bellman_ford step 3690 current loss 0.007151, current_train_items 118112.
I0304 19:30:00.350218 22849303695488 run.py:483] Algo bellman_ford step 3691 current loss 0.029142, current_train_items 118144.
I0304 19:30:00.372668 22849303695488 run.py:483] Algo bellman_ford step 3692 current loss 0.019303, current_train_items 118176.
I0304 19:30:00.405679 22849303695488 run.py:483] Algo bellman_ford step 3693 current loss 0.050493, current_train_items 118208.
I0304 19:30:00.439536 22849303695488 run.py:483] Algo bellman_ford step 3694 current loss 0.078262, current_train_items 118240.
I0304 19:30:00.459507 22849303695488 run.py:483] Algo bellman_ford step 3695 current loss 0.007701, current_train_items 118272.
I0304 19:30:00.476644 22849303695488 run.py:483] Algo bellman_ford step 3696 current loss 0.031649, current_train_items 118304.
I0304 19:30:00.500859 22849303695488 run.py:483] Algo bellman_ford step 3697 current loss 0.037180, current_train_items 118336.
I0304 19:30:00.533228 22849303695488 run.py:483] Algo bellman_ford step 3698 current loss 0.042113, current_train_items 118368.
I0304 19:30:00.565885 22849303695488 run.py:483] Algo bellman_ford step 3699 current loss 0.126643, current_train_items 118400.
I0304 19:30:00.586178 22849303695488 run.py:483] Algo bellman_ford step 3700 current loss 0.020316, current_train_items 118432.
I0304 19:30:00.594473 22849303695488 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0304 19:30:00.594583 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:30:00.611530 22849303695488 run.py:483] Algo bellman_ford step 3701 current loss 0.015329, current_train_items 118464.
I0304 19:30:00.636339 22849303695488 run.py:483] Algo bellman_ford step 3702 current loss 0.101834, current_train_items 118496.
I0304 19:30:00.669971 22849303695488 run.py:483] Algo bellman_ford step 3703 current loss 0.083339, current_train_items 118528.
I0304 19:30:00.704080 22849303695488 run.py:483] Algo bellman_ford step 3704 current loss 0.068544, current_train_items 118560.
I0304 19:30:00.724156 22849303695488 run.py:483] Algo bellman_ford step 3705 current loss 0.004521, current_train_items 118592.
I0304 19:30:00.740621 22849303695488 run.py:483] Algo bellman_ford step 3706 current loss 0.031213, current_train_items 118624.
I0304 19:30:00.765250 22849303695488 run.py:483] Algo bellman_ford step 3707 current loss 0.049927, current_train_items 118656.
I0304 19:30:00.795920 22849303695488 run.py:483] Algo bellman_ford step 3708 current loss 0.069317, current_train_items 118688.
I0304 19:30:00.829880 22849303695488 run.py:483] Algo bellman_ford step 3709 current loss 0.084093, current_train_items 118720.
I0304 19:30:00.850277 22849303695488 run.py:483] Algo bellman_ford step 3710 current loss 0.010885, current_train_items 118752.
I0304 19:30:00.867034 22849303695488 run.py:483] Algo bellman_ford step 3711 current loss 0.030042, current_train_items 118784.
I0304 19:30:00.891239 22849303695488 run.py:483] Algo bellman_ford step 3712 current loss 0.053111, current_train_items 118816.
I0304 19:30:00.924349 22849303695488 run.py:483] Algo bellman_ford step 3713 current loss 0.098628, current_train_items 118848.
I0304 19:30:00.959010 22849303695488 run.py:483] Algo bellman_ford step 3714 current loss 0.089493, current_train_items 118880.
I0304 19:30:00.978735 22849303695488 run.py:483] Algo bellman_ford step 3715 current loss 0.004664, current_train_items 118912.
I0304 19:30:00.995258 22849303695488 run.py:483] Algo bellman_ford step 3716 current loss 0.028974, current_train_items 118944.
I0304 19:30:01.019347 22849303695488 run.py:483] Algo bellman_ford step 3717 current loss 0.052798, current_train_items 118976.
I0304 19:30:01.051133 22849303695488 run.py:483] Algo bellman_ford step 3718 current loss 0.128712, current_train_items 119008.
I0304 19:30:01.084073 22849303695488 run.py:483] Algo bellman_ford step 3719 current loss 0.084040, current_train_items 119040.
I0304 19:30:01.104206 22849303695488 run.py:483] Algo bellman_ford step 3720 current loss 0.007823, current_train_items 119072.
I0304 19:30:01.120930 22849303695488 run.py:483] Algo bellman_ford step 3721 current loss 0.034697, current_train_items 119104.
I0304 19:30:01.145055 22849303695488 run.py:483] Algo bellman_ford step 3722 current loss 0.037001, current_train_items 119136.
I0304 19:30:01.176294 22849303695488 run.py:483] Algo bellman_ford step 3723 current loss 0.123231, current_train_items 119168.
I0304 19:30:01.210173 22849303695488 run.py:483] Algo bellman_ford step 3724 current loss 0.073501, current_train_items 119200.
I0304 19:30:01.230051 22849303695488 run.py:483] Algo bellman_ford step 3725 current loss 0.004856, current_train_items 119232.
I0304 19:30:01.246935 22849303695488 run.py:483] Algo bellman_ford step 3726 current loss 0.014055, current_train_items 119264.
I0304 19:30:01.271899 22849303695488 run.py:483] Algo bellman_ford step 3727 current loss 0.108325, current_train_items 119296.
I0304 19:30:01.303815 22849303695488 run.py:483] Algo bellman_ford step 3728 current loss 0.066996, current_train_items 119328.
I0304 19:30:01.337725 22849303695488 run.py:483] Algo bellman_ford step 3729 current loss 0.070365, current_train_items 119360.
I0304 19:30:01.357641 22849303695488 run.py:483] Algo bellman_ford step 3730 current loss 0.009164, current_train_items 119392.
I0304 19:30:01.374253 22849303695488 run.py:483] Algo bellman_ford step 3731 current loss 0.035948, current_train_items 119424.
I0304 19:30:01.398581 22849303695488 run.py:483] Algo bellman_ford step 3732 current loss 0.082466, current_train_items 119456.
I0304 19:30:01.428414 22849303695488 run.py:483] Algo bellman_ford step 3733 current loss 0.078733, current_train_items 119488.
I0304 19:30:01.463146 22849303695488 run.py:483] Algo bellman_ford step 3734 current loss 0.087898, current_train_items 119520.
I0304 19:30:01.482863 22849303695488 run.py:483] Algo bellman_ford step 3735 current loss 0.007764, current_train_items 119552.
I0304 19:30:01.499150 22849303695488 run.py:483] Algo bellman_ford step 3736 current loss 0.071552, current_train_items 119584.
I0304 19:30:01.522454 22849303695488 run.py:483] Algo bellman_ford step 3737 current loss 0.061095, current_train_items 119616.
I0304 19:30:01.554256 22849303695488 run.py:483] Algo bellman_ford step 3738 current loss 0.124970, current_train_items 119648.
I0304 19:30:01.588191 22849303695488 run.py:483] Algo bellman_ford step 3739 current loss 0.139315, current_train_items 119680.
I0304 19:30:01.608325 22849303695488 run.py:483] Algo bellman_ford step 3740 current loss 0.029374, current_train_items 119712.
I0304 19:30:01.624727 22849303695488 run.py:483] Algo bellman_ford step 3741 current loss 0.033665, current_train_items 119744.
I0304 19:30:01.648544 22849303695488 run.py:483] Algo bellman_ford step 3742 current loss 0.128504, current_train_items 119776.
I0304 19:30:01.679837 22849303695488 run.py:483] Algo bellman_ford step 3743 current loss 0.188694, current_train_items 119808.
I0304 19:30:01.711494 22849303695488 run.py:483] Algo bellman_ford step 3744 current loss 0.132063, current_train_items 119840.
I0304 19:30:01.731496 22849303695488 run.py:483] Algo bellman_ford step 3745 current loss 0.047492, current_train_items 119872.
I0304 19:30:01.747694 22849303695488 run.py:483] Algo bellman_ford step 3746 current loss 0.012768, current_train_items 119904.
I0304 19:30:01.772105 22849303695488 run.py:483] Algo bellman_ford step 3747 current loss 0.061155, current_train_items 119936.
I0304 19:30:01.804583 22849303695488 run.py:483] Algo bellman_ford step 3748 current loss 0.137395, current_train_items 119968.
I0304 19:30:01.839926 22849303695488 run.py:483] Algo bellman_ford step 3749 current loss 0.230439, current_train_items 120000.
I0304 19:30:01.859946 22849303695488 run.py:483] Algo bellman_ford step 3750 current loss 0.024749, current_train_items 120032.
I0304 19:30:01.868311 22849303695488 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0304 19:30:01.868447 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:30:01.885799 22849303695488 run.py:483] Algo bellman_ford step 3751 current loss 0.050886, current_train_items 120064.
I0304 19:30:01.910927 22849303695488 run.py:483] Algo bellman_ford step 3752 current loss 0.052149, current_train_items 120096.
I0304 19:30:01.942639 22849303695488 run.py:483] Algo bellman_ford step 3753 current loss 0.080413, current_train_items 120128.
I0304 19:30:01.979105 22849303695488 run.py:483] Algo bellman_ford step 3754 current loss 0.095924, current_train_items 120160.
I0304 19:30:01.999048 22849303695488 run.py:483] Algo bellman_ford step 3755 current loss 0.005383, current_train_items 120192.
I0304 19:30:02.015419 22849303695488 run.py:483] Algo bellman_ford step 3756 current loss 0.077616, current_train_items 120224.
I0304 19:30:02.040022 22849303695488 run.py:483] Algo bellman_ford step 3757 current loss 0.082960, current_train_items 120256.
I0304 19:30:02.071921 22849303695488 run.py:483] Algo bellman_ford step 3758 current loss 0.098323, current_train_items 120288.
I0304 19:30:02.104224 22849303695488 run.py:483] Algo bellman_ford step 3759 current loss 0.108668, current_train_items 120320.
I0304 19:30:02.124496 22849303695488 run.py:483] Algo bellman_ford step 3760 current loss 0.003727, current_train_items 120352.
I0304 19:30:02.141213 22849303695488 run.py:483] Algo bellman_ford step 3761 current loss 0.011084, current_train_items 120384.
I0304 19:30:02.164742 22849303695488 run.py:483] Algo bellman_ford step 3762 current loss 0.022857, current_train_items 120416.
I0304 19:30:02.196032 22849303695488 run.py:483] Algo bellman_ford step 3763 current loss 0.041634, current_train_items 120448.
I0304 19:30:02.230469 22849303695488 run.py:483] Algo bellman_ford step 3764 current loss 0.084886, current_train_items 120480.
I0304 19:30:02.250514 22849303695488 run.py:483] Algo bellman_ford step 3765 current loss 0.002881, current_train_items 120512.
I0304 19:30:02.267534 22849303695488 run.py:483] Algo bellman_ford step 3766 current loss 0.025578, current_train_items 120544.
I0304 19:30:02.291946 22849303695488 run.py:483] Algo bellman_ford step 3767 current loss 0.141619, current_train_items 120576.
I0304 19:30:02.324338 22849303695488 run.py:483] Algo bellman_ford step 3768 current loss 0.091922, current_train_items 120608.
I0304 19:30:02.357112 22849303695488 run.py:483] Algo bellman_ford step 3769 current loss 0.063857, current_train_items 120640.
I0304 19:30:02.377793 22849303695488 run.py:483] Algo bellman_ford step 3770 current loss 0.006765, current_train_items 120672.
I0304 19:30:02.394557 22849303695488 run.py:483] Algo bellman_ford step 3771 current loss 0.081498, current_train_items 120704.
I0304 19:30:02.417709 22849303695488 run.py:483] Algo bellman_ford step 3772 current loss 0.077434, current_train_items 120736.
I0304 19:30:02.448662 22849303695488 run.py:483] Algo bellman_ford step 3773 current loss 0.091094, current_train_items 120768.
I0304 19:30:02.479601 22849303695488 run.py:483] Algo bellman_ford step 3774 current loss 0.101863, current_train_items 120800.
I0304 19:30:02.499997 22849303695488 run.py:483] Algo bellman_ford step 3775 current loss 0.003408, current_train_items 120832.
I0304 19:30:02.517099 22849303695488 run.py:483] Algo bellman_ford step 3776 current loss 0.037847, current_train_items 120864.
I0304 19:30:02.540375 22849303695488 run.py:483] Algo bellman_ford step 3777 current loss 0.062073, current_train_items 120896.
I0304 19:30:02.572591 22849303695488 run.py:483] Algo bellman_ford step 3778 current loss 0.075204, current_train_items 120928.
I0304 19:30:02.607139 22849303695488 run.py:483] Algo bellman_ford step 3779 current loss 0.077967, current_train_items 120960.
I0304 19:30:02.627070 22849303695488 run.py:483] Algo bellman_ford step 3780 current loss 0.005305, current_train_items 120992.
I0304 19:30:02.643681 22849303695488 run.py:483] Algo bellman_ford step 3781 current loss 0.070656, current_train_items 121024.
I0304 19:30:02.667630 22849303695488 run.py:483] Algo bellman_ford step 3782 current loss 0.109923, current_train_items 121056.
I0304 19:30:02.697500 22849303695488 run.py:483] Algo bellman_ford step 3783 current loss 0.114964, current_train_items 121088.
I0304 19:30:02.734035 22849303695488 run.py:483] Algo bellman_ford step 3784 current loss 0.189577, current_train_items 121120.
I0304 19:30:02.754075 22849303695488 run.py:483] Algo bellman_ford step 3785 current loss 0.006441, current_train_items 121152.
I0304 19:30:02.770628 22849303695488 run.py:483] Algo bellman_ford step 3786 current loss 0.015860, current_train_items 121184.
I0304 19:30:02.794892 22849303695488 run.py:483] Algo bellman_ford step 3787 current loss 0.097566, current_train_items 121216.
I0304 19:30:02.826349 22849303695488 run.py:483] Algo bellman_ford step 3788 current loss 0.073766, current_train_items 121248.
I0304 19:30:02.860412 22849303695488 run.py:483] Algo bellman_ford step 3789 current loss 0.173333, current_train_items 121280.
I0304 19:30:02.881038 22849303695488 run.py:483] Algo bellman_ford step 3790 current loss 0.017460, current_train_items 121312.
I0304 19:30:02.897725 22849303695488 run.py:483] Algo bellman_ford step 3791 current loss 0.035316, current_train_items 121344.
I0304 19:30:02.921448 22849303695488 run.py:483] Algo bellman_ford step 3792 current loss 0.072451, current_train_items 121376.
I0304 19:30:02.953985 22849303695488 run.py:483] Algo bellman_ford step 3793 current loss 0.096475, current_train_items 121408.
I0304 19:30:02.987724 22849303695488 run.py:483] Algo bellman_ford step 3794 current loss 0.058946, current_train_items 121440.
I0304 19:30:03.007730 22849303695488 run.py:483] Algo bellman_ford step 3795 current loss 0.018348, current_train_items 121472.
I0304 19:30:03.024572 22849303695488 run.py:483] Algo bellman_ford step 3796 current loss 0.045011, current_train_items 121504.
I0304 19:30:03.049133 22849303695488 run.py:483] Algo bellman_ford step 3797 current loss 0.060169, current_train_items 121536.
I0304 19:30:03.081581 22849303695488 run.py:483] Algo bellman_ford step 3798 current loss 0.073636, current_train_items 121568.
I0304 19:30:03.114280 22849303695488 run.py:483] Algo bellman_ford step 3799 current loss 0.085835, current_train_items 121600.
I0304 19:30:03.134557 22849303695488 run.py:483] Algo bellman_ford step 3800 current loss 0.006466, current_train_items 121632.
I0304 19:30:03.142528 22849303695488 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.95703125, 'score': 0.95703125, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0304 19:30:03.142636 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.983, current avg val score is 0.957, val scores are: bellman_ford: 0.957
I0304 19:30:03.160081 22849303695488 run.py:483] Algo bellman_ford step 3801 current loss 0.040324, current_train_items 121664.
I0304 19:30:03.185554 22849303695488 run.py:483] Algo bellman_ford step 3802 current loss 0.060226, current_train_items 121696.
I0304 19:30:03.217666 22849303695488 run.py:483] Algo bellman_ford step 3803 current loss 0.120417, current_train_items 121728.
I0304 19:30:03.250257 22849303695488 run.py:483] Algo bellman_ford step 3804 current loss 0.071714, current_train_items 121760.
I0304 19:30:03.270770 22849303695488 run.py:483] Algo bellman_ford step 3805 current loss 0.007987, current_train_items 121792.
I0304 19:30:03.287283 22849303695488 run.py:483] Algo bellman_ford step 3806 current loss 0.072209, current_train_items 121824.
I0304 19:30:03.311867 22849303695488 run.py:483] Algo bellman_ford step 3807 current loss 0.152667, current_train_items 121856.
I0304 19:30:03.343703 22849303695488 run.py:483] Algo bellman_ford step 3808 current loss 0.139185, current_train_items 121888.
I0304 19:30:03.379514 22849303695488 run.py:483] Algo bellman_ford step 3809 current loss 0.079288, current_train_items 121920.
I0304 19:30:03.399761 22849303695488 run.py:483] Algo bellman_ford step 3810 current loss 0.037541, current_train_items 121952.
I0304 19:30:03.416751 22849303695488 run.py:483] Algo bellman_ford step 3811 current loss 0.015946, current_train_items 121984.
I0304 19:30:03.441295 22849303695488 run.py:483] Algo bellman_ford step 3812 current loss 0.054754, current_train_items 122016.
I0304 19:30:03.475232 22849303695488 run.py:483] Algo bellman_ford step 3813 current loss 0.094373, current_train_items 122048.
I0304 19:30:03.511547 22849303695488 run.py:483] Algo bellman_ford step 3814 current loss 0.158999, current_train_items 122080.
I0304 19:30:03.532168 22849303695488 run.py:483] Algo bellman_ford step 3815 current loss 0.031586, current_train_items 122112.
I0304 19:30:03.549207 22849303695488 run.py:483] Algo bellman_ford step 3816 current loss 0.051166, current_train_items 122144.
I0304 19:30:03.573862 22849303695488 run.py:483] Algo bellman_ford step 3817 current loss 0.085000, current_train_items 122176.
I0304 19:30:03.605880 22849303695488 run.py:483] Algo bellman_ford step 3818 current loss 0.114645, current_train_items 122208.
I0304 19:30:03.640121 22849303695488 run.py:483] Algo bellman_ford step 3819 current loss 0.149219, current_train_items 122240.
I0304 19:30:03.660289 22849303695488 run.py:483] Algo bellman_ford step 3820 current loss 0.004803, current_train_items 122272.
I0304 19:30:03.676636 22849303695488 run.py:483] Algo bellman_ford step 3821 current loss 0.018471, current_train_items 122304.
I0304 19:30:03.701283 22849303695488 run.py:483] Algo bellman_ford step 3822 current loss 0.081561, current_train_items 122336.
I0304 19:30:03.733276 22849303695488 run.py:483] Algo bellman_ford step 3823 current loss 0.086679, current_train_items 122368.
I0304 19:30:03.768290 22849303695488 run.py:483] Algo bellman_ford step 3824 current loss 0.099971, current_train_items 122400.
I0304 19:30:03.788744 22849303695488 run.py:483] Algo bellman_ford step 3825 current loss 0.031391, current_train_items 122432.
I0304 19:30:03.805024 22849303695488 run.py:483] Algo bellman_ford step 3826 current loss 0.017065, current_train_items 122464.
I0304 19:30:03.829977 22849303695488 run.py:483] Algo bellman_ford step 3827 current loss 0.116625, current_train_items 122496.
I0304 19:30:03.862592 22849303695488 run.py:483] Algo bellman_ford step 3828 current loss 0.187799, current_train_items 122528.
I0304 19:30:03.895562 22849303695488 run.py:483] Algo bellman_ford step 3829 current loss 0.182427, current_train_items 122560.
I0304 19:30:03.915839 22849303695488 run.py:483] Algo bellman_ford step 3830 current loss 0.008479, current_train_items 122592.
I0304 19:30:03.932543 22849303695488 run.py:483] Algo bellman_ford step 3831 current loss 0.033676, current_train_items 122624.
I0304 19:30:03.955981 22849303695488 run.py:483] Algo bellman_ford step 3832 current loss 0.039914, current_train_items 122656.
I0304 19:30:03.988701 22849303695488 run.py:483] Algo bellman_ford step 3833 current loss 0.070248, current_train_items 122688.
I0304 19:30:04.022448 22849303695488 run.py:483] Algo bellman_ford step 3834 current loss 0.097176, current_train_items 122720.
I0304 19:30:04.042317 22849303695488 run.py:483] Algo bellman_ford step 3835 current loss 0.015891, current_train_items 122752.
I0304 19:30:04.058286 22849303695488 run.py:483] Algo bellman_ford step 3836 current loss 0.012738, current_train_items 122784.
I0304 19:30:04.083854 22849303695488 run.py:483] Algo bellman_ford step 3837 current loss 0.083783, current_train_items 122816.
I0304 19:30:04.116170 22849303695488 run.py:483] Algo bellman_ford step 3838 current loss 0.116343, current_train_items 122848.
I0304 19:30:04.150114 22849303695488 run.py:483] Algo bellman_ford step 3839 current loss 0.092079, current_train_items 122880.
I0304 19:30:04.170310 22849303695488 run.py:483] Algo bellman_ford step 3840 current loss 0.011006, current_train_items 122912.
I0304 19:30:04.186548 22849303695488 run.py:483] Algo bellman_ford step 3841 current loss 0.013842, current_train_items 122944.
I0304 19:30:04.210417 22849303695488 run.py:483] Algo bellman_ford step 3842 current loss 0.023868, current_train_items 122976.
I0304 19:30:04.242232 22849303695488 run.py:483] Algo bellman_ford step 3843 current loss 0.064947, current_train_items 123008.
I0304 19:30:04.277814 22849303695488 run.py:483] Algo bellman_ford step 3844 current loss 0.122941, current_train_items 123040.
I0304 19:30:04.297802 22849303695488 run.py:483] Algo bellman_ford step 3845 current loss 0.004472, current_train_items 123072.
I0304 19:30:04.314288 22849303695488 run.py:483] Algo bellman_ford step 3846 current loss 0.031458, current_train_items 123104.
I0304 19:30:04.340022 22849303695488 run.py:483] Algo bellman_ford step 3847 current loss 0.110655, current_train_items 123136.
I0304 19:30:04.371642 22849303695488 run.py:483] Algo bellman_ford step 3848 current loss 0.061135, current_train_items 123168.
I0304 19:30:04.405997 22849303695488 run.py:483] Algo bellman_ford step 3849 current loss 0.096098, current_train_items 123200.
I0304 19:30:04.426120 22849303695488 run.py:483] Algo bellman_ford step 3850 current loss 0.010002, current_train_items 123232.
I0304 19:30:04.434485 22849303695488 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0304 19:30:04.434595 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.983, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:04.464663 22849303695488 run.py:483] Algo bellman_ford step 3851 current loss 0.011409, current_train_items 123264.
I0304 19:30:04.488445 22849303695488 run.py:483] Algo bellman_ford step 3852 current loss 0.036976, current_train_items 123296.
I0304 19:30:04.521744 22849303695488 run.py:483] Algo bellman_ford step 3853 current loss 0.085724, current_train_items 123328.
I0304 19:30:04.558333 22849303695488 run.py:483] Algo bellman_ford step 3854 current loss 0.086200, current_train_items 123360.
I0304 19:30:04.578899 22849303695488 run.py:483] Algo bellman_ford step 3855 current loss 0.015457, current_train_items 123392.
I0304 19:30:04.595210 22849303695488 run.py:483] Algo bellman_ford step 3856 current loss 0.007735, current_train_items 123424.
I0304 19:30:04.618697 22849303695488 run.py:483] Algo bellman_ford step 3857 current loss 0.035531, current_train_items 123456.
I0304 19:30:04.651200 22849303695488 run.py:483] Algo bellman_ford step 3858 current loss 0.136225, current_train_items 123488.
I0304 19:30:04.688013 22849303695488 run.py:483] Algo bellman_ford step 3859 current loss 0.124326, current_train_items 123520.
I0304 19:30:04.708677 22849303695488 run.py:483] Algo bellman_ford step 3860 current loss 0.005394, current_train_items 123552.
I0304 19:30:04.725665 22849303695488 run.py:483] Algo bellman_ford step 3861 current loss 0.081613, current_train_items 123584.
I0304 19:30:04.749599 22849303695488 run.py:483] Algo bellman_ford step 3862 current loss 0.085070, current_train_items 123616.
I0304 19:30:04.781266 22849303695488 run.py:483] Algo bellman_ford step 3863 current loss 0.111259, current_train_items 123648.
I0304 19:30:04.817960 22849303695488 run.py:483] Algo bellman_ford step 3864 current loss 0.073351, current_train_items 123680.
I0304 19:30:04.837854 22849303695488 run.py:483] Algo bellman_ford step 3865 current loss 0.013944, current_train_items 123712.
I0304 19:30:04.854294 22849303695488 run.py:483] Algo bellman_ford step 3866 current loss 0.022638, current_train_items 123744.
I0304 19:30:04.878421 22849303695488 run.py:483] Algo bellman_ford step 3867 current loss 0.084338, current_train_items 123776.
I0304 19:30:04.909847 22849303695488 run.py:483] Algo bellman_ford step 3868 current loss 0.113184, current_train_items 123808.
I0304 19:30:04.943862 22849303695488 run.py:483] Algo bellman_ford step 3869 current loss 0.129566, current_train_items 123840.
I0304 19:30:04.964504 22849303695488 run.py:483] Algo bellman_ford step 3870 current loss 0.010371, current_train_items 123872.
I0304 19:30:04.980781 22849303695488 run.py:483] Algo bellman_ford step 3871 current loss 0.014941, current_train_items 123904.
I0304 19:30:05.005450 22849303695488 run.py:483] Algo bellman_ford step 3872 current loss 0.044960, current_train_items 123936.
I0304 19:30:05.036016 22849303695488 run.py:483] Algo bellman_ford step 3873 current loss 0.041972, current_train_items 123968.
I0304 19:30:05.069400 22849303695488 run.py:483] Algo bellman_ford step 3874 current loss 0.115739, current_train_items 124000.
I0304 19:30:05.089821 22849303695488 run.py:483] Algo bellman_ford step 3875 current loss 0.010275, current_train_items 124032.
I0304 19:30:05.106793 22849303695488 run.py:483] Algo bellman_ford step 3876 current loss 0.020489, current_train_items 124064.
I0304 19:30:05.131670 22849303695488 run.py:483] Algo bellman_ford step 3877 current loss 0.045987, current_train_items 124096.
I0304 19:30:05.163978 22849303695488 run.py:483] Algo bellman_ford step 3878 current loss 0.067566, current_train_items 124128.
I0304 19:30:05.198381 22849303695488 run.py:483] Algo bellman_ford step 3879 current loss 0.118906, current_train_items 124160.
I0304 19:30:05.218626 22849303695488 run.py:483] Algo bellman_ford step 3880 current loss 0.041613, current_train_items 124192.
I0304 19:30:05.235000 22849303695488 run.py:483] Algo bellman_ford step 3881 current loss 0.021255, current_train_items 124224.
I0304 19:30:05.259457 22849303695488 run.py:483] Algo bellman_ford step 3882 current loss 0.059334, current_train_items 124256.
I0304 19:30:05.291358 22849303695488 run.py:483] Algo bellman_ford step 3883 current loss 0.082913, current_train_items 124288.
I0304 19:30:05.324571 22849303695488 run.py:483] Algo bellman_ford step 3884 current loss 0.124329, current_train_items 124320.
I0304 19:30:05.345157 22849303695488 run.py:483] Algo bellman_ford step 3885 current loss 0.013128, current_train_items 124352.
I0304 19:30:05.362238 22849303695488 run.py:483] Algo bellman_ford step 3886 current loss 0.017629, current_train_items 124384.
I0304 19:30:05.386655 22849303695488 run.py:483] Algo bellman_ford step 3887 current loss 0.057319, current_train_items 124416.
I0304 19:30:05.418409 22849303695488 run.py:483] Algo bellman_ford step 3888 current loss 0.106256, current_train_items 124448.
I0304 19:30:05.453884 22849303695488 run.py:483] Algo bellman_ford step 3889 current loss 0.091304, current_train_items 124480.
I0304 19:30:05.474226 22849303695488 run.py:483] Algo bellman_ford step 3890 current loss 0.008344, current_train_items 124512.
I0304 19:30:05.490978 22849303695488 run.py:483] Algo bellman_ford step 3891 current loss 0.034293, current_train_items 124544.
I0304 19:30:05.515887 22849303695488 run.py:483] Algo bellman_ford step 3892 current loss 0.075692, current_train_items 124576.
I0304 19:30:05.545549 22849303695488 run.py:483] Algo bellman_ford step 3893 current loss 0.050759, current_train_items 124608.
I0304 19:30:05.580079 22849303695488 run.py:483] Algo bellman_ford step 3894 current loss 0.072449, current_train_items 124640.
I0304 19:30:05.600093 22849303695488 run.py:483] Algo bellman_ford step 3895 current loss 0.008443, current_train_items 124672.
I0304 19:30:05.616894 22849303695488 run.py:483] Algo bellman_ford step 3896 current loss 0.048992, current_train_items 124704.
I0304 19:30:05.642160 22849303695488 run.py:483] Algo bellman_ford step 3897 current loss 0.079392, current_train_items 124736.
I0304 19:30:05.674322 22849303695488 run.py:483] Algo bellman_ford step 3898 current loss 0.112189, current_train_items 124768.
I0304 19:30:05.708026 22849303695488 run.py:483] Algo bellman_ford step 3899 current loss 0.152159, current_train_items 124800.
I0304 19:30:05.728868 22849303695488 run.py:483] Algo bellman_ford step 3900 current loss 0.007549, current_train_items 124832.
I0304 19:30:05.736947 22849303695488 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0304 19:30:05.737065 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.985, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:05.766937 22849303695488 run.py:483] Algo bellman_ford step 3901 current loss 0.075931, current_train_items 124864.
I0304 19:30:05.791615 22849303695488 run.py:483] Algo bellman_ford step 3902 current loss 0.105691, current_train_items 124896.
I0304 19:30:05.824340 22849303695488 run.py:483] Algo bellman_ford step 3903 current loss 0.252993, current_train_items 124928.
I0304 19:30:05.858986 22849303695488 run.py:483] Algo bellman_ford step 3904 current loss 0.172389, current_train_items 124960.
I0304 19:30:05.879941 22849303695488 run.py:483] Algo bellman_ford step 3905 current loss 0.006647, current_train_items 124992.
I0304 19:30:05.896203 22849303695488 run.py:483] Algo bellman_ford step 3906 current loss 0.025011, current_train_items 125024.
I0304 19:30:05.919769 22849303695488 run.py:483] Algo bellman_ford step 3907 current loss 0.041469, current_train_items 125056.
I0304 19:30:05.952154 22849303695488 run.py:483] Algo bellman_ford step 3908 current loss 0.066842, current_train_items 125088.
I0304 19:30:05.986812 22849303695488 run.py:483] Algo bellman_ford step 3909 current loss 0.081929, current_train_items 125120.
I0304 19:30:06.006756 22849303695488 run.py:483] Algo bellman_ford step 3910 current loss 0.004616, current_train_items 125152.
I0304 19:30:06.023204 22849303695488 run.py:483] Algo bellman_ford step 3911 current loss 0.017011, current_train_items 125184.
I0304 19:30:06.048229 22849303695488 run.py:483] Algo bellman_ford step 3912 current loss 0.047299, current_train_items 125216.
I0304 19:30:06.079898 22849303695488 run.py:483] Algo bellman_ford step 3913 current loss 0.120506, current_train_items 125248.
I0304 19:30:06.111213 22849303695488 run.py:483] Algo bellman_ford step 3914 current loss 0.089313, current_train_items 125280.
I0304 19:30:06.131160 22849303695488 run.py:483] Algo bellman_ford step 3915 current loss 0.011963, current_train_items 125312.
I0304 19:30:06.147638 22849303695488 run.py:483] Algo bellman_ford step 3916 current loss 0.014081, current_train_items 125344.
I0304 19:30:06.173336 22849303695488 run.py:483] Algo bellman_ford step 3917 current loss 0.097194, current_train_items 125376.
I0304 19:30:06.203846 22849303695488 run.py:483] Algo bellman_ford step 3918 current loss 0.107576, current_train_items 125408.
I0304 19:30:06.238472 22849303695488 run.py:483] Algo bellman_ford step 3919 current loss 0.063272, current_train_items 125440.
I0304 19:30:06.258733 22849303695488 run.py:483] Algo bellman_ford step 3920 current loss 0.005096, current_train_items 125472.
I0304 19:30:06.275578 22849303695488 run.py:483] Algo bellman_ford step 3921 current loss 0.013903, current_train_items 125504.
I0304 19:30:06.300764 22849303695488 run.py:483] Algo bellman_ford step 3922 current loss 0.084278, current_train_items 125536.
I0304 19:30:06.332951 22849303695488 run.py:483] Algo bellman_ford step 3923 current loss 0.207393, current_train_items 125568.
I0304 19:30:06.369779 22849303695488 run.py:483] Algo bellman_ford step 3924 current loss 0.203734, current_train_items 125600.
I0304 19:30:06.390026 22849303695488 run.py:483] Algo bellman_ford step 3925 current loss 0.006294, current_train_items 125632.
I0304 19:30:06.406820 22849303695488 run.py:483] Algo bellman_ford step 3926 current loss 0.021083, current_train_items 125664.
I0304 19:30:06.431171 22849303695488 run.py:483] Algo bellman_ford step 3927 current loss 0.058945, current_train_items 125696.
I0304 19:30:06.462499 22849303695488 run.py:483] Algo bellman_ford step 3928 current loss 0.088558, current_train_items 125728.
I0304 19:30:06.496375 22849303695488 run.py:483] Algo bellman_ford step 3929 current loss 0.092899, current_train_items 125760.
I0304 19:30:06.516238 22849303695488 run.py:483] Algo bellman_ford step 3930 current loss 0.011250, current_train_items 125792.
I0304 19:30:06.532647 22849303695488 run.py:483] Algo bellman_ford step 3931 current loss 0.067757, current_train_items 125824.
I0304 19:30:06.556772 22849303695488 run.py:483] Algo bellman_ford step 3932 current loss 0.059845, current_train_items 125856.
I0304 19:30:06.590096 22849303695488 run.py:483] Algo bellman_ford step 3933 current loss 0.095469, current_train_items 125888.
I0304 19:30:06.624289 22849303695488 run.py:483] Algo bellman_ford step 3934 current loss 0.086730, current_train_items 125920.
I0304 19:30:06.644183 22849303695488 run.py:483] Algo bellman_ford step 3935 current loss 0.014301, current_train_items 125952.
I0304 19:30:06.660963 22849303695488 run.py:483] Algo bellman_ford step 3936 current loss 0.048358, current_train_items 125984.
I0304 19:30:06.685255 22849303695488 run.py:483] Algo bellman_ford step 3937 current loss 0.066656, current_train_items 126016.
I0304 19:30:06.717417 22849303695488 run.py:483] Algo bellman_ford step 3938 current loss 0.096040, current_train_items 126048.
I0304 19:30:06.752399 22849303695488 run.py:483] Algo bellman_ford step 3939 current loss 0.136478, current_train_items 126080.
I0304 19:30:06.772191 22849303695488 run.py:483] Algo bellman_ford step 3940 current loss 0.005579, current_train_items 126112.
I0304 19:30:06.788572 22849303695488 run.py:483] Algo bellman_ford step 3941 current loss 0.023216, current_train_items 126144.
I0304 19:30:06.812838 22849303695488 run.py:483] Algo bellman_ford step 3942 current loss 0.066104, current_train_items 126176.
I0304 19:30:06.843701 22849303695488 run.py:483] Algo bellman_ford step 3943 current loss 0.065175, current_train_items 126208.
I0304 19:30:06.878600 22849303695488 run.py:483] Algo bellman_ford step 3944 current loss 0.099260, current_train_items 126240.
I0304 19:30:06.898372 22849303695488 run.py:483] Algo bellman_ford step 3945 current loss 0.010171, current_train_items 126272.
I0304 19:30:06.915164 22849303695488 run.py:483] Algo bellman_ford step 3946 current loss 0.027570, current_train_items 126304.
I0304 19:30:06.939109 22849303695488 run.py:483] Algo bellman_ford step 3947 current loss 0.086903, current_train_items 126336.
I0304 19:30:06.969389 22849303695488 run.py:483] Algo bellman_ford step 3948 current loss 0.092704, current_train_items 126368.
I0304 19:30:07.004084 22849303695488 run.py:483] Algo bellman_ford step 3949 current loss 0.057050, current_train_items 126400.
I0304 19:30:07.024030 22849303695488 run.py:483] Algo bellman_ford step 3950 current loss 0.008500, current_train_items 126432.
I0304 19:30:07.032581 22849303695488 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0304 19:30:07.032688 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.986, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:07.062682 22849303695488 run.py:483] Algo bellman_ford step 3951 current loss 0.022419, current_train_items 126464.
I0304 19:30:07.087537 22849303695488 run.py:483] Algo bellman_ford step 3952 current loss 0.056995, current_train_items 126496.
I0304 19:30:07.119050 22849303695488 run.py:483] Algo bellman_ford step 3953 current loss 0.040015, current_train_items 126528.
I0304 19:30:07.151929 22849303695488 run.py:483] Algo bellman_ford step 3954 current loss 0.072795, current_train_items 126560.
I0304 19:30:07.172154 22849303695488 run.py:483] Algo bellman_ford step 3955 current loss 0.004574, current_train_items 126592.
I0304 19:30:07.188789 22849303695488 run.py:483] Algo bellman_ford step 3956 current loss 0.034492, current_train_items 126624.
I0304 19:30:07.213406 22849303695488 run.py:483] Algo bellman_ford step 3957 current loss 0.078367, current_train_items 126656.
I0304 19:30:07.246073 22849303695488 run.py:483] Algo bellman_ford step 3958 current loss 0.086524, current_train_items 126688.
I0304 19:30:07.282320 22849303695488 run.py:483] Algo bellman_ford step 3959 current loss 0.068096, current_train_items 126720.
I0304 19:30:07.302627 22849303695488 run.py:483] Algo bellman_ford step 3960 current loss 0.019153, current_train_items 126752.
I0304 19:30:07.319207 22849303695488 run.py:483] Algo bellman_ford step 3961 current loss 0.032523, current_train_items 126784.
I0304 19:30:07.342016 22849303695488 run.py:483] Algo bellman_ford step 3962 current loss 0.048581, current_train_items 126816.
I0304 19:30:07.374098 22849303695488 run.py:483] Algo bellman_ford step 3963 current loss 0.117004, current_train_items 126848.
I0304 19:30:07.408961 22849303695488 run.py:483] Algo bellman_ford step 3964 current loss 0.125061, current_train_items 126880.
I0304 19:30:07.428944 22849303695488 run.py:483] Algo bellman_ford step 3965 current loss 0.009425, current_train_items 126912.
I0304 19:30:07.445665 22849303695488 run.py:483] Algo bellman_ford step 3966 current loss 0.044891, current_train_items 126944.
I0304 19:30:07.471187 22849303695488 run.py:483] Algo bellman_ford step 3967 current loss 0.091403, current_train_items 126976.
I0304 19:30:07.502833 22849303695488 run.py:483] Algo bellman_ford step 3968 current loss 0.094906, current_train_items 127008.
I0304 19:30:07.536491 22849303695488 run.py:483] Algo bellman_ford step 3969 current loss 0.078997, current_train_items 127040.
I0304 19:30:07.557017 22849303695488 run.py:483] Algo bellman_ford step 3970 current loss 0.004722, current_train_items 127072.
I0304 19:30:07.573618 22849303695488 run.py:483] Algo bellman_ford step 3971 current loss 0.014196, current_train_items 127104.
I0304 19:30:07.598505 22849303695488 run.py:483] Algo bellman_ford step 3972 current loss 0.057641, current_train_items 127136.
I0304 19:30:07.631054 22849303695488 run.py:483] Algo bellman_ford step 3973 current loss 0.125993, current_train_items 127168.
I0304 19:30:07.663817 22849303695488 run.py:483] Algo bellman_ford step 3974 current loss 0.090492, current_train_items 127200.
I0304 19:30:07.684224 22849303695488 run.py:483] Algo bellman_ford step 3975 current loss 0.011713, current_train_items 127232.
I0304 19:30:07.701171 22849303695488 run.py:483] Algo bellman_ford step 3976 current loss 0.033616, current_train_items 127264.
I0304 19:30:07.724975 22849303695488 run.py:483] Algo bellman_ford step 3977 current loss 0.066238, current_train_items 127296.
I0304 19:30:07.757196 22849303695488 run.py:483] Algo bellman_ford step 3978 current loss 0.134194, current_train_items 127328.
I0304 19:30:07.790682 22849303695488 run.py:483] Algo bellman_ford step 3979 current loss 0.090651, current_train_items 127360.
I0304 19:30:07.810698 22849303695488 run.py:483] Algo bellman_ford step 3980 current loss 0.002562, current_train_items 127392.
I0304 19:30:07.827165 22849303695488 run.py:483] Algo bellman_ford step 3981 current loss 0.031534, current_train_items 127424.
I0304 19:30:07.850664 22849303695488 run.py:483] Algo bellman_ford step 3982 current loss 0.045669, current_train_items 127456.
I0304 19:30:07.882812 22849303695488 run.py:483] Algo bellman_ford step 3983 current loss 0.102399, current_train_items 127488.
I0304 19:30:07.915716 22849303695488 run.py:483] Algo bellman_ford step 3984 current loss 0.085198, current_train_items 127520.
I0304 19:30:07.935954 22849303695488 run.py:483] Algo bellman_ford step 3985 current loss 0.013262, current_train_items 127552.
I0304 19:30:07.952766 22849303695488 run.py:483] Algo bellman_ford step 3986 current loss 0.055312, current_train_items 127584.
I0304 19:30:07.976368 22849303695488 run.py:483] Algo bellman_ford step 3987 current loss 0.102307, current_train_items 127616.
I0304 19:30:08.008286 22849303695488 run.py:483] Algo bellman_ford step 3988 current loss 0.143068, current_train_items 127648.
I0304 19:30:08.043152 22849303695488 run.py:483] Algo bellman_ford step 3989 current loss 0.097554, current_train_items 127680.
I0304 19:30:08.063420 22849303695488 run.py:483] Algo bellman_ford step 3990 current loss 0.013913, current_train_items 127712.
I0304 19:30:08.080328 22849303695488 run.py:483] Algo bellman_ford step 3991 current loss 0.057921, current_train_items 127744.
I0304 19:30:08.103547 22849303695488 run.py:483] Algo bellman_ford step 3992 current loss 0.057852, current_train_items 127776.
I0304 19:30:08.134744 22849303695488 run.py:483] Algo bellman_ford step 3993 current loss 0.066740, current_train_items 127808.
I0304 19:30:08.169261 22849303695488 run.py:483] Algo bellman_ford step 3994 current loss 0.092217, current_train_items 127840.
I0304 19:30:08.189273 22849303695488 run.py:483] Algo bellman_ford step 3995 current loss 0.008461, current_train_items 127872.
I0304 19:30:08.205946 22849303695488 run.py:483] Algo bellman_ford step 3996 current loss 0.024651, current_train_items 127904.
I0304 19:30:08.230256 22849303695488 run.py:483] Algo bellman_ford step 3997 current loss 0.090660, current_train_items 127936.
I0304 19:30:08.261309 22849303695488 run.py:483] Algo bellman_ford step 3998 current loss 0.112050, current_train_items 127968.
I0304 19:30:08.296374 22849303695488 run.py:483] Algo bellman_ford step 3999 current loss 0.104638, current_train_items 128000.
I0304 19:30:08.316775 22849303695488 run.py:483] Algo bellman_ford step 4000 current loss 0.017832, current_train_items 128032.
I0304 19:30:08.324779 22849303695488 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0304 19:30:08.324886 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:30:08.342099 22849303695488 run.py:483] Algo bellman_ford step 4001 current loss 0.033897, current_train_items 128064.
I0304 19:30:08.367260 22849303695488 run.py:483] Algo bellman_ford step 4002 current loss 0.052096, current_train_items 128096.
I0304 19:30:08.398839 22849303695488 run.py:483] Algo bellman_ford step 4003 current loss 0.076569, current_train_items 128128.
I0304 19:30:08.432438 22849303695488 run.py:483] Algo bellman_ford step 4004 current loss 0.075192, current_train_items 128160.
I0304 19:30:08.452565 22849303695488 run.py:483] Algo bellman_ford step 4005 current loss 0.003647, current_train_items 128192.
I0304 19:30:08.469018 22849303695488 run.py:483] Algo bellman_ford step 4006 current loss 0.031478, current_train_items 128224.
I0304 19:30:08.493405 22849303695488 run.py:483] Algo bellman_ford step 4007 current loss 0.137242, current_train_items 128256.
I0304 19:30:08.525840 22849303695488 run.py:483] Algo bellman_ford step 4008 current loss 0.072493, current_train_items 128288.
I0304 19:30:08.558741 22849303695488 run.py:483] Algo bellman_ford step 4009 current loss 0.118374, current_train_items 128320.
I0304 19:30:08.578988 22849303695488 run.py:483] Algo bellman_ford step 4010 current loss 0.013120, current_train_items 128352.
I0304 19:30:08.595474 22849303695488 run.py:483] Algo bellman_ford step 4011 current loss 0.035287, current_train_items 128384.
I0304 19:30:08.618767 22849303695488 run.py:483] Algo bellman_ford step 4012 current loss 0.072983, current_train_items 128416.
I0304 19:30:08.650733 22849303695488 run.py:483] Algo bellman_ford step 4013 current loss 0.140221, current_train_items 128448.
I0304 19:30:08.684960 22849303695488 run.py:483] Algo bellman_ford step 4014 current loss 0.106571, current_train_items 128480.
I0304 19:30:08.704986 22849303695488 run.py:483] Algo bellman_ford step 4015 current loss 0.005746, current_train_items 128512.
I0304 19:30:08.721521 22849303695488 run.py:483] Algo bellman_ford step 4016 current loss 0.026155, current_train_items 128544.
I0304 19:30:08.745794 22849303695488 run.py:483] Algo bellman_ford step 4017 current loss 0.056627, current_train_items 128576.
I0304 19:30:08.777578 22849303695488 run.py:483] Algo bellman_ford step 4018 current loss 0.078126, current_train_items 128608.
I0304 19:30:08.810866 22849303695488 run.py:483] Algo bellman_ford step 4019 current loss 0.097492, current_train_items 128640.
I0304 19:30:08.830804 22849303695488 run.py:483] Algo bellman_ford step 4020 current loss 0.003418, current_train_items 128672.
I0304 19:30:08.847331 22849303695488 run.py:483] Algo bellman_ford step 4021 current loss 0.043553, current_train_items 128704.
I0304 19:30:08.872498 22849303695488 run.py:483] Algo bellman_ford step 4022 current loss 0.059482, current_train_items 128736.
I0304 19:30:08.903361 22849303695488 run.py:483] Algo bellman_ford step 4023 current loss 0.049019, current_train_items 128768.
I0304 19:30:08.939956 22849303695488 run.py:483] Algo bellman_ford step 4024 current loss 0.160952, current_train_items 128800.
I0304 19:30:08.960566 22849303695488 run.py:483] Algo bellman_ford step 4025 current loss 0.004215, current_train_items 128832.
I0304 19:30:08.977411 22849303695488 run.py:483] Algo bellman_ford step 4026 current loss 0.032662, current_train_items 128864.
I0304 19:30:09.001240 22849303695488 run.py:483] Algo bellman_ford step 4027 current loss 0.038990, current_train_items 128896.
I0304 19:30:09.032037 22849303695488 run.py:483] Algo bellman_ford step 4028 current loss 0.056193, current_train_items 128928.
I0304 19:30:09.065247 22849303695488 run.py:483] Algo bellman_ford step 4029 current loss 0.088807, current_train_items 128960.
I0304 19:30:09.085419 22849303695488 run.py:483] Algo bellman_ford step 4030 current loss 0.005937, current_train_items 128992.
I0304 19:30:09.101738 22849303695488 run.py:483] Algo bellman_ford step 4031 current loss 0.048543, current_train_items 129024.
I0304 19:30:09.126322 22849303695488 run.py:483] Algo bellman_ford step 4032 current loss 0.054665, current_train_items 129056.
I0304 19:30:09.159206 22849303695488 run.py:483] Algo bellman_ford step 4033 current loss 0.072337, current_train_items 129088.
I0304 19:30:09.192657 22849303695488 run.py:483] Algo bellman_ford step 4034 current loss 0.083318, current_train_items 129120.
I0304 19:30:09.212912 22849303695488 run.py:483] Algo bellman_ford step 4035 current loss 0.003537, current_train_items 129152.
I0304 19:30:09.229266 22849303695488 run.py:483] Algo bellman_ford step 4036 current loss 0.033883, current_train_items 129184.
I0304 19:30:09.253394 22849303695488 run.py:483] Algo bellman_ford step 4037 current loss 0.086780, current_train_items 129216.
I0304 19:30:09.285432 22849303695488 run.py:483] Algo bellman_ford step 4038 current loss 0.107643, current_train_items 129248.
I0304 19:30:09.319691 22849303695488 run.py:483] Algo bellman_ford step 4039 current loss 0.107349, current_train_items 129280.
I0304 19:30:09.340128 22849303695488 run.py:483] Algo bellman_ford step 4040 current loss 0.006654, current_train_items 129312.
I0304 19:30:09.356735 22849303695488 run.py:483] Algo bellman_ford step 4041 current loss 0.018372, current_train_items 129344.
I0304 19:30:09.380588 22849303695488 run.py:483] Algo bellman_ford step 4042 current loss 0.145795, current_train_items 129376.
I0304 19:30:09.413844 22849303695488 run.py:483] Algo bellman_ford step 4043 current loss 0.164367, current_train_items 129408.
I0304 19:30:09.446741 22849303695488 run.py:483] Algo bellman_ford step 4044 current loss 0.151413, current_train_items 129440.
I0304 19:30:09.466494 22849303695488 run.py:483] Algo bellman_ford step 4045 current loss 0.014320, current_train_items 129472.
I0304 19:30:09.483551 22849303695488 run.py:483] Algo bellman_ford step 4046 current loss 0.039539, current_train_items 129504.
I0304 19:30:09.507653 22849303695488 run.py:483] Algo bellman_ford step 4047 current loss 0.063418, current_train_items 129536.
I0304 19:30:09.538218 22849303695488 run.py:483] Algo bellman_ford step 4048 current loss 0.056675, current_train_items 129568.
I0304 19:30:09.573833 22849303695488 run.py:483] Algo bellman_ford step 4049 current loss 0.119507, current_train_items 129600.
I0304 19:30:09.593636 22849303695488 run.py:483] Algo bellman_ford step 4050 current loss 0.010974, current_train_items 129632.
I0304 19:30:09.601993 22849303695488 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0304 19:30:09.602113 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:30:09.619548 22849303695488 run.py:483] Algo bellman_ford step 4051 current loss 0.041951, current_train_items 129664.
I0304 19:30:09.643444 22849303695488 run.py:483] Algo bellman_ford step 4052 current loss 0.055774, current_train_items 129696.
I0304 19:30:09.675054 22849303695488 run.py:483] Algo bellman_ford step 4053 current loss 0.071230, current_train_items 129728.
I0304 19:30:09.711064 22849303695488 run.py:483] Algo bellman_ford step 4054 current loss 0.139217, current_train_items 129760.
I0304 19:30:09.731424 22849303695488 run.py:483] Algo bellman_ford step 4055 current loss 0.010530, current_train_items 129792.
I0304 19:30:09.747673 22849303695488 run.py:483] Algo bellman_ford step 4056 current loss 0.022090, current_train_items 129824.
I0304 19:30:09.771590 22849303695488 run.py:483] Algo bellman_ford step 4057 current loss 0.063084, current_train_items 129856.
I0304 19:30:09.802238 22849303695488 run.py:483] Algo bellman_ford step 4058 current loss 0.078389, current_train_items 129888.
I0304 19:30:09.834681 22849303695488 run.py:483] Algo bellman_ford step 4059 current loss 0.052387, current_train_items 129920.
I0304 19:30:09.855038 22849303695488 run.py:483] Algo bellman_ford step 4060 current loss 0.012231, current_train_items 129952.
I0304 19:30:09.872052 22849303695488 run.py:483] Algo bellman_ford step 4061 current loss 0.026318, current_train_items 129984.
I0304 19:30:09.895197 22849303695488 run.py:483] Algo bellman_ford step 4062 current loss 0.057340, current_train_items 130016.
I0304 19:30:09.925649 22849303695488 run.py:483] Algo bellman_ford step 4063 current loss 0.051005, current_train_items 130048.
I0304 19:30:09.961123 22849303695488 run.py:483] Algo bellman_ford step 4064 current loss 0.104878, current_train_items 130080.
I0304 19:30:09.980952 22849303695488 run.py:483] Algo bellman_ford step 4065 current loss 0.004662, current_train_items 130112.
I0304 19:30:09.997121 22849303695488 run.py:483] Algo bellman_ford step 4066 current loss 0.188058, current_train_items 130144.
I0304 19:30:10.022510 22849303695488 run.py:483] Algo bellman_ford step 4067 current loss 0.124964, current_train_items 130176.
I0304 19:30:10.053683 22849303695488 run.py:483] Algo bellman_ford step 4068 current loss 0.084442, current_train_items 130208.
I0304 19:30:10.088842 22849303695488 run.py:483] Algo bellman_ford step 4069 current loss 0.109632, current_train_items 130240.
I0304 19:30:10.109012 22849303695488 run.py:483] Algo bellman_ford step 4070 current loss 0.006663, current_train_items 130272.
I0304 19:30:10.125715 22849303695488 run.py:483] Algo bellman_ford step 4071 current loss 0.113253, current_train_items 130304.
I0304 19:30:10.149450 22849303695488 run.py:483] Algo bellman_ford step 4072 current loss 0.139505, current_train_items 130336.
I0304 19:30:10.179552 22849303695488 run.py:483] Algo bellman_ford step 4073 current loss 0.111254, current_train_items 130368.
I0304 19:30:10.212694 22849303695488 run.py:483] Algo bellman_ford step 4074 current loss 0.092086, current_train_items 130400.
I0304 19:30:10.233019 22849303695488 run.py:483] Algo bellman_ford step 4075 current loss 0.013830, current_train_items 130432.
I0304 19:30:10.250193 22849303695488 run.py:483] Algo bellman_ford step 4076 current loss 0.024886, current_train_items 130464.
I0304 19:30:10.274901 22849303695488 run.py:483] Algo bellman_ford step 4077 current loss 0.076001, current_train_items 130496.
I0304 19:30:10.305815 22849303695488 run.py:483] Algo bellman_ford step 4078 current loss 0.066264, current_train_items 130528.
I0304 19:30:10.339213 22849303695488 run.py:483] Algo bellman_ford step 4079 current loss 0.064320, current_train_items 130560.
I0304 19:30:10.358934 22849303695488 run.py:483] Algo bellman_ford step 4080 current loss 0.003569, current_train_items 130592.
I0304 19:30:10.375825 22849303695488 run.py:483] Algo bellman_ford step 4081 current loss 0.024439, current_train_items 130624.
I0304 19:30:10.400664 22849303695488 run.py:483] Algo bellman_ford step 4082 current loss 0.059727, current_train_items 130656.
I0304 19:30:10.433805 22849303695488 run.py:483] Algo bellman_ford step 4083 current loss 0.088098, current_train_items 130688.
I0304 19:30:10.465291 22849303695488 run.py:483] Algo bellman_ford step 4084 current loss 0.073842, current_train_items 130720.
I0304 19:30:10.485802 22849303695488 run.py:483] Algo bellman_ford step 4085 current loss 0.016704, current_train_items 130752.
I0304 19:30:10.502582 22849303695488 run.py:483] Algo bellman_ford step 4086 current loss 0.029136, current_train_items 130784.
I0304 19:30:10.525988 22849303695488 run.py:483] Algo bellman_ford step 4087 current loss 0.100774, current_train_items 130816.
I0304 19:30:10.557207 22849303695488 run.py:483] Algo bellman_ford step 4088 current loss 0.052319, current_train_items 130848.
I0304 19:30:10.589647 22849303695488 run.py:483] Algo bellman_ford step 4089 current loss 0.058598, current_train_items 130880.
I0304 19:30:10.609698 22849303695488 run.py:483] Algo bellman_ford step 4090 current loss 0.005080, current_train_items 130912.
I0304 19:30:10.626213 22849303695488 run.py:483] Algo bellman_ford step 4091 current loss 0.023910, current_train_items 130944.
I0304 19:30:10.650693 22849303695488 run.py:483] Algo bellman_ford step 4092 current loss 0.045490, current_train_items 130976.
I0304 19:30:10.683033 22849303695488 run.py:483] Algo bellman_ford step 4093 current loss 0.111947, current_train_items 131008.
I0304 19:30:10.717434 22849303695488 run.py:483] Algo bellman_ford step 4094 current loss 0.090488, current_train_items 131040.
I0304 19:30:10.737339 22849303695488 run.py:483] Algo bellman_ford step 4095 current loss 0.005496, current_train_items 131072.
I0304 19:30:10.754190 22849303695488 run.py:483] Algo bellman_ford step 4096 current loss 0.011659, current_train_items 131104.
I0304 19:30:10.778905 22849303695488 run.py:483] Algo bellman_ford step 4097 current loss 0.062629, current_train_items 131136.
I0304 19:30:10.810583 22849303695488 run.py:483] Algo bellman_ford step 4098 current loss 0.066839, current_train_items 131168.
I0304 19:30:10.844894 22849303695488 run.py:483] Algo bellman_ford step 4099 current loss 0.092234, current_train_items 131200.
I0304 19:30:10.865354 22849303695488 run.py:483] Algo bellman_ford step 4100 current loss 0.007965, current_train_items 131232.
I0304 19:30:10.873252 22849303695488 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0304 19:30:10.873360 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:10.890012 22849303695488 run.py:483] Algo bellman_ford step 4101 current loss 0.010431, current_train_items 131264.
I0304 19:30:10.912691 22849303695488 run.py:483] Algo bellman_ford step 4102 current loss 0.062069, current_train_items 131296.
I0304 19:30:10.945501 22849303695488 run.py:483] Algo bellman_ford step 4103 current loss 0.103087, current_train_items 131328.
I0304 19:30:10.981105 22849303695488 run.py:483] Algo bellman_ford step 4104 current loss 0.089661, current_train_items 131360.
I0304 19:30:11.001164 22849303695488 run.py:483] Algo bellman_ford step 4105 current loss 0.003795, current_train_items 131392.
I0304 19:30:11.017700 22849303695488 run.py:483] Algo bellman_ford step 4106 current loss 0.033622, current_train_items 131424.
I0304 19:30:11.042403 22849303695488 run.py:483] Algo bellman_ford step 4107 current loss 0.083801, current_train_items 131456.
I0304 19:30:11.073895 22849303695488 run.py:483] Algo bellman_ford step 4108 current loss 0.082908, current_train_items 131488.
I0304 19:30:11.110206 22849303695488 run.py:483] Algo bellman_ford step 4109 current loss 0.103828, current_train_items 131520.
I0304 19:30:11.130163 22849303695488 run.py:483] Algo bellman_ford step 4110 current loss 0.019558, current_train_items 131552.
I0304 19:30:11.147252 22849303695488 run.py:483] Algo bellman_ford step 4111 current loss 0.018562, current_train_items 131584.
I0304 19:30:11.170635 22849303695488 run.py:483] Algo bellman_ford step 4112 current loss 0.051786, current_train_items 131616.
I0304 19:30:11.203133 22849303695488 run.py:483] Algo bellman_ford step 4113 current loss 0.171753, current_train_items 131648.
I0304 19:30:11.237843 22849303695488 run.py:483] Algo bellman_ford step 4114 current loss 0.113685, current_train_items 131680.
I0304 19:30:11.257653 22849303695488 run.py:483] Algo bellman_ford step 4115 current loss 0.007768, current_train_items 131712.
I0304 19:30:11.274473 22849303695488 run.py:483] Algo bellman_ford step 4116 current loss 0.052053, current_train_items 131744.
I0304 19:30:11.299711 22849303695488 run.py:483] Algo bellman_ford step 4117 current loss 0.125884, current_train_items 131776.
I0304 19:30:11.330419 22849303695488 run.py:483] Algo bellman_ford step 4118 current loss 0.059345, current_train_items 131808.
I0304 19:30:11.364779 22849303695488 run.py:483] Algo bellman_ford step 4119 current loss 0.104202, current_train_items 131840.
I0304 19:30:11.384617 22849303695488 run.py:483] Algo bellman_ford step 4120 current loss 0.012218, current_train_items 131872.
I0304 19:30:11.401145 22849303695488 run.py:483] Algo bellman_ford step 4121 current loss 0.008053, current_train_items 131904.
I0304 19:30:11.426137 22849303695488 run.py:483] Algo bellman_ford step 4122 current loss 0.067299, current_train_items 131936.
I0304 19:30:11.458816 22849303695488 run.py:483] Algo bellman_ford step 4123 current loss 0.136496, current_train_items 131968.
I0304 19:30:11.492998 22849303695488 run.py:483] Algo bellman_ford step 4124 current loss 0.101095, current_train_items 132000.
I0304 19:30:11.512925 22849303695488 run.py:483] Algo bellman_ford step 4125 current loss 0.003378, current_train_items 132032.
I0304 19:30:11.529470 22849303695488 run.py:483] Algo bellman_ford step 4126 current loss 0.049093, current_train_items 132064.
I0304 19:30:11.555134 22849303695488 run.py:483] Algo bellman_ford step 4127 current loss 0.068701, current_train_items 132096.
I0304 19:30:11.586583 22849303695488 run.py:483] Algo bellman_ford step 4128 current loss 0.077503, current_train_items 132128.
I0304 19:30:11.619353 22849303695488 run.py:483] Algo bellman_ford step 4129 current loss 0.079309, current_train_items 132160.
I0304 19:30:11.639436 22849303695488 run.py:483] Algo bellman_ford step 4130 current loss 0.003051, current_train_items 132192.
I0304 19:30:11.656244 22849303695488 run.py:483] Algo bellman_ford step 4131 current loss 0.019453, current_train_items 132224.
I0304 19:30:11.680635 22849303695488 run.py:483] Algo bellman_ford step 4132 current loss 0.079420, current_train_items 132256.
I0304 19:30:11.713103 22849303695488 run.py:483] Algo bellman_ford step 4133 current loss 0.085699, current_train_items 132288.
I0304 19:30:11.748476 22849303695488 run.py:483] Algo bellman_ford step 4134 current loss 0.078759, current_train_items 132320.
I0304 19:30:11.768568 22849303695488 run.py:483] Algo bellman_ford step 4135 current loss 0.005203, current_train_items 132352.
I0304 19:30:11.785061 22849303695488 run.py:483] Algo bellman_ford step 4136 current loss 0.021554, current_train_items 132384.
I0304 19:30:11.809486 22849303695488 run.py:483] Algo bellman_ford step 4137 current loss 0.024444, current_train_items 132416.
I0304 19:30:11.840787 22849303695488 run.py:483] Algo bellman_ford step 4138 current loss 0.090492, current_train_items 132448.
I0304 19:30:11.875630 22849303695488 run.py:483] Algo bellman_ford step 4139 current loss 0.169156, current_train_items 132480.
I0304 19:30:11.895956 22849303695488 run.py:483] Algo bellman_ford step 4140 current loss 0.004028, current_train_items 132512.
I0304 19:30:11.912152 22849303695488 run.py:483] Algo bellman_ford step 4141 current loss 0.008947, current_train_items 132544.
I0304 19:30:11.937716 22849303695488 run.py:483] Algo bellman_ford step 4142 current loss 0.052442, current_train_items 132576.
I0304 19:30:11.968669 22849303695488 run.py:483] Algo bellman_ford step 4143 current loss 0.049775, current_train_items 132608.
I0304 19:30:12.002361 22849303695488 run.py:483] Algo bellman_ford step 4144 current loss 0.090141, current_train_items 132640.
I0304 19:30:12.022378 22849303695488 run.py:483] Algo bellman_ford step 4145 current loss 0.002828, current_train_items 132672.
I0304 19:30:12.038638 22849303695488 run.py:483] Algo bellman_ford step 4146 current loss 0.015659, current_train_items 132704.
I0304 19:30:12.062629 22849303695488 run.py:483] Algo bellman_ford step 4147 current loss 0.087827, current_train_items 132736.
I0304 19:30:12.092935 22849303695488 run.py:483] Algo bellman_ford step 4148 current loss 0.032814, current_train_items 132768.
I0304 19:30:12.126332 22849303695488 run.py:483] Algo bellman_ford step 4149 current loss 0.117931, current_train_items 132800.
I0304 19:30:12.146494 22849303695488 run.py:483] Algo bellman_ford step 4150 current loss 0.008290, current_train_items 132832.
I0304 19:30:12.154946 22849303695488 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0304 19:30:12.155060 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:12.172351 22849303695488 run.py:483] Algo bellman_ford step 4151 current loss 0.052522, current_train_items 132864.
I0304 19:30:12.197471 22849303695488 run.py:483] Algo bellman_ford step 4152 current loss 0.107630, current_train_items 132896.
I0304 19:30:12.229227 22849303695488 run.py:483] Algo bellman_ford step 4153 current loss 0.095410, current_train_items 132928.
I0304 19:30:12.264989 22849303695488 run.py:483] Algo bellman_ford step 4154 current loss 0.138156, current_train_items 132960.
I0304 19:30:12.285209 22849303695488 run.py:483] Algo bellman_ford step 4155 current loss 0.006076, current_train_items 132992.
I0304 19:30:12.301981 22849303695488 run.py:483] Algo bellman_ford step 4156 current loss 0.044684, current_train_items 133024.
I0304 19:30:12.325649 22849303695488 run.py:483] Algo bellman_ford step 4157 current loss 0.046668, current_train_items 133056.
I0304 19:30:12.356585 22849303695488 run.py:483] Algo bellman_ford step 4158 current loss 0.060055, current_train_items 133088.
I0304 19:30:12.390867 22849303695488 run.py:483] Algo bellman_ford step 4159 current loss 0.082708, current_train_items 133120.
I0304 19:30:12.411290 22849303695488 run.py:483] Algo bellman_ford step 4160 current loss 0.009146, current_train_items 133152.
I0304 19:30:12.428043 22849303695488 run.py:483] Algo bellman_ford step 4161 current loss 0.038274, current_train_items 133184.
I0304 19:30:12.450784 22849303695488 run.py:483] Algo bellman_ford step 4162 current loss 0.053945, current_train_items 133216.
I0304 19:30:12.482708 22849303695488 run.py:483] Algo bellman_ford step 4163 current loss 0.071293, current_train_items 133248.
I0304 19:30:12.517115 22849303695488 run.py:483] Algo bellman_ford step 4164 current loss 0.071684, current_train_items 133280.
I0304 19:30:12.537253 22849303695488 run.py:483] Algo bellman_ford step 4165 current loss 0.005821, current_train_items 133312.
I0304 19:30:12.553892 22849303695488 run.py:483] Algo bellman_ford step 4166 current loss 0.015461, current_train_items 133344.
I0304 19:30:12.579057 22849303695488 run.py:483] Algo bellman_ford step 4167 current loss 0.177656, current_train_items 133376.
I0304 19:30:12.610306 22849303695488 run.py:483] Algo bellman_ford step 4168 current loss 0.066387, current_train_items 133408.
I0304 19:30:12.643407 22849303695488 run.py:483] Algo bellman_ford step 4169 current loss 0.079448, current_train_items 133440.
I0304 19:30:12.663652 22849303695488 run.py:483] Algo bellman_ford step 4170 current loss 0.004294, current_train_items 133472.
I0304 19:30:12.679694 22849303695488 run.py:483] Algo bellman_ford step 4171 current loss 0.038027, current_train_items 133504.
I0304 19:30:12.703301 22849303695488 run.py:483] Algo bellman_ford step 4172 current loss 0.034533, current_train_items 133536.
I0304 19:30:12.734393 22849303695488 run.py:483] Algo bellman_ford step 4173 current loss 0.059421, current_train_items 133568.
I0304 19:30:12.770292 22849303695488 run.py:483] Algo bellman_ford step 4174 current loss 0.099950, current_train_items 133600.
I0304 19:30:12.790569 22849303695488 run.py:483] Algo bellman_ford step 4175 current loss 0.003506, current_train_items 133632.
I0304 19:30:12.807852 22849303695488 run.py:483] Algo bellman_ford step 4176 current loss 0.021000, current_train_items 133664.
I0304 19:30:12.831829 22849303695488 run.py:483] Algo bellman_ford step 4177 current loss 0.090301, current_train_items 133696.
I0304 19:30:12.863597 22849303695488 run.py:483] Algo bellman_ford step 4178 current loss 0.078199, current_train_items 133728.
I0304 19:30:12.898469 22849303695488 run.py:483] Algo bellman_ford step 4179 current loss 0.096539, current_train_items 133760.
I0304 19:30:12.918577 22849303695488 run.py:483] Algo bellman_ford step 4180 current loss 0.005056, current_train_items 133792.
I0304 19:30:12.935179 22849303695488 run.py:483] Algo bellman_ford step 4181 current loss 0.063675, current_train_items 133824.
I0304 19:30:12.960439 22849303695488 run.py:483] Algo bellman_ford step 4182 current loss 0.077326, current_train_items 133856.
I0304 19:30:12.990507 22849303695488 run.py:483] Algo bellman_ford step 4183 current loss 0.102490, current_train_items 133888.
I0304 19:30:13.025127 22849303695488 run.py:483] Algo bellman_ford step 4184 current loss 0.095673, current_train_items 133920.
I0304 19:30:13.045397 22849303695488 run.py:483] Algo bellman_ford step 4185 current loss 0.005221, current_train_items 133952.
I0304 19:30:13.061834 22849303695488 run.py:483] Algo bellman_ford step 4186 current loss 0.066391, current_train_items 133984.
I0304 19:30:13.085626 22849303695488 run.py:483] Algo bellman_ford step 4187 current loss 0.051598, current_train_items 134016.
I0304 19:30:13.116930 22849303695488 run.py:483] Algo bellman_ford step 4188 current loss 0.104397, current_train_items 134048.
I0304 19:30:13.150361 22849303695488 run.py:483] Algo bellman_ford step 4189 current loss 0.072297, current_train_items 134080.
I0304 19:30:13.170558 22849303695488 run.py:483] Algo bellman_ford step 4190 current loss 0.017843, current_train_items 134112.
I0304 19:30:13.187523 22849303695488 run.py:483] Algo bellman_ford step 4191 current loss 0.011918, current_train_items 134144.
I0304 19:30:13.209246 22849303695488 run.py:483] Algo bellman_ford step 4192 current loss 0.023483, current_train_items 134176.
I0304 19:30:13.241457 22849303695488 run.py:483] Algo bellman_ford step 4193 current loss 0.094677, current_train_items 134208.
I0304 19:30:13.277714 22849303695488 run.py:483] Algo bellman_ford step 4194 current loss 0.187538, current_train_items 134240.
I0304 19:30:13.297439 22849303695488 run.py:483] Algo bellman_ford step 4195 current loss 0.012264, current_train_items 134272.
I0304 19:30:13.313797 22849303695488 run.py:483] Algo bellman_ford step 4196 current loss 0.033939, current_train_items 134304.
I0304 19:30:13.338158 22849303695488 run.py:483] Algo bellman_ford step 4197 current loss 0.029101, current_train_items 134336.
I0304 19:30:13.369393 22849303695488 run.py:483] Algo bellman_ford step 4198 current loss 0.084713, current_train_items 134368.
I0304 19:30:13.403209 22849303695488 run.py:483] Algo bellman_ford step 4199 current loss 0.135625, current_train_items 134400.
I0304 19:30:13.422999 22849303695488 run.py:483] Algo bellman_ford step 4200 current loss 0.002544, current_train_items 134432.
I0304 19:30:13.431302 22849303695488 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0304 19:30:13.431411 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:30:13.449043 22849303695488 run.py:483] Algo bellman_ford step 4201 current loss 0.037117, current_train_items 134464.
I0304 19:30:13.473646 22849303695488 run.py:483] Algo bellman_ford step 4202 current loss 0.067308, current_train_items 134496.
I0304 19:30:13.505447 22849303695488 run.py:483] Algo bellman_ford step 4203 current loss 0.049437, current_train_items 134528.
I0304 19:30:13.539897 22849303695488 run.py:483] Algo bellman_ford step 4204 current loss 0.102178, current_train_items 134560.
I0304 19:30:13.560216 22849303695488 run.py:483] Algo bellman_ford step 4205 current loss 0.005662, current_train_items 134592.
I0304 19:30:13.576767 22849303695488 run.py:483] Algo bellman_ford step 4206 current loss 0.079544, current_train_items 134624.
I0304 19:30:13.601597 22849303695488 run.py:483] Algo bellman_ford step 4207 current loss 0.045715, current_train_items 134656.
I0304 19:30:13.634521 22849303695488 run.py:483] Algo bellman_ford step 4208 current loss 0.082436, current_train_items 134688.
I0304 19:30:13.669815 22849303695488 run.py:483] Algo bellman_ford step 4209 current loss 0.131264, current_train_items 134720.
I0304 19:30:13.689828 22849303695488 run.py:483] Algo bellman_ford step 4210 current loss 0.011521, current_train_items 134752.
I0304 19:30:13.706587 22849303695488 run.py:483] Algo bellman_ford step 4211 current loss 0.126698, current_train_items 134784.
I0304 19:30:13.731011 22849303695488 run.py:483] Algo bellman_ford step 4212 current loss 0.047119, current_train_items 134816.
I0304 19:30:13.763030 22849303695488 run.py:483] Algo bellman_ford step 4213 current loss 0.082880, current_train_items 134848.
I0304 19:30:13.796897 22849303695488 run.py:483] Algo bellman_ford step 4214 current loss 0.121115, current_train_items 134880.
I0304 19:30:13.816855 22849303695488 run.py:483] Algo bellman_ford step 4215 current loss 0.010386, current_train_items 134912.
I0304 19:30:13.833470 22849303695488 run.py:483] Algo bellman_ford step 4216 current loss 0.020706, current_train_items 134944.
I0304 19:30:13.857475 22849303695488 run.py:483] Algo bellman_ford step 4217 current loss 0.053509, current_train_items 134976.
I0304 19:30:13.889219 22849303695488 run.py:483] Algo bellman_ford step 4218 current loss 0.177698, current_train_items 135008.
I0304 19:30:13.923219 22849303695488 run.py:483] Algo bellman_ford step 4219 current loss 0.077178, current_train_items 135040.
I0304 19:30:13.943275 22849303695488 run.py:483] Algo bellman_ford step 4220 current loss 0.004082, current_train_items 135072.
I0304 19:30:13.960042 22849303695488 run.py:483] Algo bellman_ford step 4221 current loss 0.037684, current_train_items 135104.
I0304 19:30:13.984661 22849303695488 run.py:483] Algo bellman_ford step 4222 current loss 0.082125, current_train_items 135136.
I0304 19:30:14.017448 22849303695488 run.py:483] Algo bellman_ford step 4223 current loss 0.060993, current_train_items 135168.
I0304 19:30:14.053236 22849303695488 run.py:483] Algo bellman_ford step 4224 current loss 0.085871, current_train_items 135200.
I0304 19:30:14.073123 22849303695488 run.py:483] Algo bellman_ford step 4225 current loss 0.003593, current_train_items 135232.
I0304 19:30:14.089578 22849303695488 run.py:483] Algo bellman_ford step 4226 current loss 0.026327, current_train_items 135264.
I0304 19:30:14.113757 22849303695488 run.py:483] Algo bellman_ford step 4227 current loss 0.099403, current_train_items 135296.
I0304 19:30:14.144678 22849303695488 run.py:483] Algo bellman_ford step 4228 current loss 0.065781, current_train_items 135328.
I0304 19:30:14.176964 22849303695488 run.py:483] Algo bellman_ford step 4229 current loss 0.061948, current_train_items 135360.
I0304 19:30:14.197190 22849303695488 run.py:483] Algo bellman_ford step 4230 current loss 0.004098, current_train_items 135392.
I0304 19:30:14.213758 22849303695488 run.py:483] Algo bellman_ford step 4231 current loss 0.045085, current_train_items 135424.
I0304 19:30:14.238292 22849303695488 run.py:483] Algo bellman_ford step 4232 current loss 0.085162, current_train_items 135456.
I0304 19:30:14.270069 22849303695488 run.py:483] Algo bellman_ford step 4233 current loss 0.127950, current_train_items 135488.
I0304 19:30:14.303580 22849303695488 run.py:483] Algo bellman_ford step 4234 current loss 0.105108, current_train_items 135520.
I0304 19:30:14.323963 22849303695488 run.py:483] Algo bellman_ford step 4235 current loss 0.003863, current_train_items 135552.
I0304 19:30:14.341242 22849303695488 run.py:483] Algo bellman_ford step 4236 current loss 0.021523, current_train_items 135584.
I0304 19:30:14.365739 22849303695488 run.py:483] Algo bellman_ford step 4237 current loss 0.049516, current_train_items 135616.
I0304 19:30:14.397836 22849303695488 run.py:483] Algo bellman_ford step 4238 current loss 0.050155, current_train_items 135648.
I0304 19:30:14.434103 22849303695488 run.py:483] Algo bellman_ford step 4239 current loss 0.122874, current_train_items 135680.
I0304 19:30:14.454188 22849303695488 run.py:483] Algo bellman_ford step 4240 current loss 0.034909, current_train_items 135712.
I0304 19:30:14.470816 22849303695488 run.py:483] Algo bellman_ford step 4241 current loss 0.010143, current_train_items 135744.
I0304 19:30:14.494568 22849303695488 run.py:483] Algo bellman_ford step 4242 current loss 0.059041, current_train_items 135776.
I0304 19:30:14.527192 22849303695488 run.py:483] Algo bellman_ford step 4243 current loss 0.105223, current_train_items 135808.
I0304 19:30:14.560858 22849303695488 run.py:483] Algo bellman_ford step 4244 current loss 0.079560, current_train_items 135840.
I0304 19:30:14.581277 22849303695488 run.py:483] Algo bellman_ford step 4245 current loss 0.006418, current_train_items 135872.
I0304 19:30:14.597504 22849303695488 run.py:483] Algo bellman_ford step 4246 current loss 0.034757, current_train_items 135904.
I0304 19:30:14.623348 22849303695488 run.py:483] Algo bellman_ford step 4247 current loss 0.052946, current_train_items 135936.
I0304 19:30:14.655468 22849303695488 run.py:483] Algo bellman_ford step 4248 current loss 0.097932, current_train_items 135968.
I0304 19:30:14.691256 22849303695488 run.py:483] Algo bellman_ford step 4249 current loss 0.109346, current_train_items 136000.
I0304 19:30:14.711539 22849303695488 run.py:483] Algo bellman_ford step 4250 current loss 0.008824, current_train_items 136032.
I0304 19:30:14.719737 22849303695488 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0304 19:30:14.719845 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:30:14.736793 22849303695488 run.py:483] Algo bellman_ford step 4251 current loss 0.033359, current_train_items 136064.
I0304 19:30:14.761538 22849303695488 run.py:483] Algo bellman_ford step 4252 current loss 0.061355, current_train_items 136096.
I0304 19:30:14.793926 22849303695488 run.py:483] Algo bellman_ford step 4253 current loss 0.209126, current_train_items 136128.
I0304 19:30:14.828712 22849303695488 run.py:483] Algo bellman_ford step 4254 current loss 0.149259, current_train_items 136160.
I0304 19:30:14.848774 22849303695488 run.py:483] Algo bellman_ford step 4255 current loss 0.004647, current_train_items 136192.
I0304 19:30:14.864979 22849303695488 run.py:483] Algo bellman_ford step 4256 current loss 0.021523, current_train_items 136224.
I0304 19:30:14.889070 22849303695488 run.py:483] Algo bellman_ford step 4257 current loss 0.065477, current_train_items 136256.
I0304 19:30:14.921335 22849303695488 run.py:483] Algo bellman_ford step 4258 current loss 0.088372, current_train_items 136288.
I0304 19:30:14.956053 22849303695488 run.py:483] Algo bellman_ford step 4259 current loss 0.120513, current_train_items 136320.
I0304 19:30:14.976571 22849303695488 run.py:483] Algo bellman_ford step 4260 current loss 0.008982, current_train_items 136352.
I0304 19:30:14.993700 22849303695488 run.py:483] Algo bellman_ford step 4261 current loss 0.081131, current_train_items 136384.
I0304 19:30:15.017817 22849303695488 run.py:483] Algo bellman_ford step 4262 current loss 0.092118, current_train_items 136416.
I0304 19:30:15.049706 22849303695488 run.py:483] Algo bellman_ford step 4263 current loss 0.081602, current_train_items 136448.
I0304 19:30:15.084321 22849303695488 run.py:483] Algo bellman_ford step 4264 current loss 0.069966, current_train_items 136480.
I0304 19:30:15.104445 22849303695488 run.py:483] Algo bellman_ford step 4265 current loss 0.008934, current_train_items 136512.
I0304 19:30:15.121100 22849303695488 run.py:483] Algo bellman_ford step 4266 current loss 0.043635, current_train_items 136544.
I0304 19:30:15.145179 22849303695488 run.py:483] Algo bellman_ford step 4267 current loss 0.045029, current_train_items 136576.
I0304 19:30:15.176579 22849303695488 run.py:483] Algo bellman_ford step 4268 current loss 0.101717, current_train_items 136608.
I0304 19:30:15.210188 22849303695488 run.py:483] Algo bellman_ford step 4269 current loss 0.093415, current_train_items 136640.
I0304 19:30:15.230958 22849303695488 run.py:483] Algo bellman_ford step 4270 current loss 0.004874, current_train_items 136672.
I0304 19:30:15.247564 22849303695488 run.py:483] Algo bellman_ford step 4271 current loss 0.023596, current_train_items 136704.
I0304 19:30:15.270745 22849303695488 run.py:483] Algo bellman_ford step 4272 current loss 0.019830, current_train_items 136736.
I0304 19:30:15.302394 22849303695488 run.py:483] Algo bellman_ford step 4273 current loss 0.111370, current_train_items 136768.
I0304 19:30:15.336163 22849303695488 run.py:483] Algo bellman_ford step 4274 current loss 0.079553, current_train_items 136800.
I0304 19:30:15.356876 22849303695488 run.py:483] Algo bellman_ford step 4275 current loss 0.006781, current_train_items 136832.
I0304 19:30:15.373557 22849303695488 run.py:483] Algo bellman_ford step 4276 current loss 0.025680, current_train_items 136864.
I0304 19:30:15.397293 22849303695488 run.py:483] Algo bellman_ford step 4277 current loss 0.050510, current_train_items 136896.
I0304 19:30:15.430896 22849303695488 run.py:483] Algo bellman_ford step 4278 current loss 0.109237, current_train_items 136928.
I0304 19:30:15.465933 22849303695488 run.py:483] Algo bellman_ford step 4279 current loss 0.085873, current_train_items 136960.
I0304 19:30:15.486229 22849303695488 run.py:483] Algo bellman_ford step 4280 current loss 0.019407, current_train_items 136992.
I0304 19:30:15.502718 22849303695488 run.py:483] Algo bellman_ford step 4281 current loss 0.028101, current_train_items 137024.
I0304 19:30:15.526986 22849303695488 run.py:483] Algo bellman_ford step 4282 current loss 0.025915, current_train_items 137056.
I0304 19:30:15.557903 22849303695488 run.py:483] Algo bellman_ford step 4283 current loss 0.051316, current_train_items 137088.
I0304 19:30:15.594348 22849303695488 run.py:483] Algo bellman_ford step 4284 current loss 0.117634, current_train_items 137120.
I0304 19:30:15.614764 22849303695488 run.py:483] Algo bellman_ford step 4285 current loss 0.004791, current_train_items 137152.
I0304 19:30:15.631601 22849303695488 run.py:483] Algo bellman_ford step 4286 current loss 0.005320, current_train_items 137184.
I0304 19:30:15.655728 22849303695488 run.py:483] Algo bellman_ford step 4287 current loss 0.082761, current_train_items 137216.
I0304 19:30:15.688827 22849303695488 run.py:483] Algo bellman_ford step 4288 current loss 0.076684, current_train_items 137248.
I0304 19:30:15.723904 22849303695488 run.py:483] Algo bellman_ford step 4289 current loss 0.115977, current_train_items 137280.
I0304 19:30:15.744489 22849303695488 run.py:483] Algo bellman_ford step 4290 current loss 0.007713, current_train_items 137312.
I0304 19:30:15.761521 22849303695488 run.py:483] Algo bellman_ford step 4291 current loss 0.024467, current_train_items 137344.
I0304 19:30:15.786674 22849303695488 run.py:483] Algo bellman_ford step 4292 current loss 0.061670, current_train_items 137376.
I0304 19:30:15.820012 22849303695488 run.py:483] Algo bellman_ford step 4293 current loss 0.132926, current_train_items 137408.
I0304 19:30:15.856622 22849303695488 run.py:483] Algo bellman_ford step 4294 current loss 0.113221, current_train_items 137440.
I0304 19:30:15.876835 22849303695488 run.py:483] Algo bellman_ford step 4295 current loss 0.003894, current_train_items 137472.
I0304 19:30:15.893285 22849303695488 run.py:483] Algo bellman_ford step 4296 current loss 0.023212, current_train_items 137504.
I0304 19:30:15.917098 22849303695488 run.py:483] Algo bellman_ford step 4297 current loss 0.079108, current_train_items 137536.
I0304 19:30:15.949580 22849303695488 run.py:483] Algo bellman_ford step 4298 current loss 0.088208, current_train_items 137568.
I0304 19:30:15.980562 22849303695488 run.py:483] Algo bellman_ford step 4299 current loss 0.049649, current_train_items 137600.
I0304 19:30:16.001155 22849303695488 run.py:483] Algo bellman_ford step 4300 current loss 0.004529, current_train_items 137632.
I0304 19:30:16.009163 22849303695488 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0304 19:30:16.009273 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:16.026264 22849303695488 run.py:483] Algo bellman_ford step 4301 current loss 0.030324, current_train_items 137664.
I0304 19:30:16.050729 22849303695488 run.py:483] Algo bellman_ford step 4302 current loss 0.056341, current_train_items 137696.
I0304 19:30:16.083911 22849303695488 run.py:483] Algo bellman_ford step 4303 current loss 0.269769, current_train_items 137728.
I0304 19:30:16.120884 22849303695488 run.py:483] Algo bellman_ford step 4304 current loss 0.084939, current_train_items 137760.
I0304 19:30:16.141119 22849303695488 run.py:483] Algo bellman_ford step 4305 current loss 0.006245, current_train_items 137792.
I0304 19:30:16.157472 22849303695488 run.py:483] Algo bellman_ford step 4306 current loss 0.017330, current_train_items 137824.
I0304 19:30:16.183247 22849303695488 run.py:483] Algo bellman_ford step 4307 current loss 0.086351, current_train_items 137856.
I0304 19:30:16.215843 22849303695488 run.py:483] Algo bellman_ford step 4308 current loss 0.154419, current_train_items 137888.
I0304 19:30:16.252201 22849303695488 run.py:483] Algo bellman_ford step 4309 current loss 0.107859, current_train_items 137920.
I0304 19:30:16.272387 22849303695488 run.py:483] Algo bellman_ford step 4310 current loss 0.011595, current_train_items 137952.
I0304 19:30:16.289014 22849303695488 run.py:483] Algo bellman_ford step 4311 current loss 0.029274, current_train_items 137984.
I0304 19:30:16.312137 22849303695488 run.py:483] Algo bellman_ford step 4312 current loss 0.044270, current_train_items 138016.
I0304 19:30:16.344318 22849303695488 run.py:483] Algo bellman_ford step 4313 current loss 0.057593, current_train_items 138048.
I0304 19:30:16.379163 22849303695488 run.py:483] Algo bellman_ford step 4314 current loss 0.078916, current_train_items 138080.
I0304 19:30:16.399113 22849303695488 run.py:483] Algo bellman_ford step 4315 current loss 0.005918, current_train_items 138112.
I0304 19:30:16.415505 22849303695488 run.py:483] Algo bellman_ford step 4316 current loss 0.066105, current_train_items 138144.
I0304 19:30:16.440158 22849303695488 run.py:483] Algo bellman_ford step 4317 current loss 0.092383, current_train_items 138176.
I0304 19:30:16.471266 22849303695488 run.py:483] Algo bellman_ford step 4318 current loss 0.108429, current_train_items 138208.
I0304 19:30:16.505201 22849303695488 run.py:483] Algo bellman_ford step 4319 current loss 0.095677, current_train_items 138240.
I0304 19:30:16.525010 22849303695488 run.py:483] Algo bellman_ford step 4320 current loss 0.006018, current_train_items 138272.
I0304 19:30:16.541814 22849303695488 run.py:483] Algo bellman_ford step 4321 current loss 0.048587, current_train_items 138304.
I0304 19:30:16.566071 22849303695488 run.py:483] Algo bellman_ford step 4322 current loss 0.071298, current_train_items 138336.
I0304 19:30:16.596418 22849303695488 run.py:483] Algo bellman_ford step 4323 current loss 0.073632, current_train_items 138368.
I0304 19:30:16.630830 22849303695488 run.py:483] Algo bellman_ford step 4324 current loss 0.086254, current_train_items 138400.
I0304 19:30:16.650799 22849303695488 run.py:483] Algo bellman_ford step 4325 current loss 0.002550, current_train_items 138432.
I0304 19:30:16.667469 22849303695488 run.py:483] Algo bellman_ford step 4326 current loss 0.020308, current_train_items 138464.
I0304 19:30:16.691813 22849303695488 run.py:483] Algo bellman_ford step 4327 current loss 0.082406, current_train_items 138496.
I0304 19:30:16.725129 22849303695488 run.py:483] Algo bellman_ford step 4328 current loss 0.147944, current_train_items 138528.
I0304 19:30:16.758246 22849303695488 run.py:483] Algo bellman_ford step 4329 current loss 0.099705, current_train_items 138560.
I0304 19:30:16.778255 22849303695488 run.py:483] Algo bellman_ford step 4330 current loss 0.003960, current_train_items 138592.
I0304 19:30:16.795267 22849303695488 run.py:483] Algo bellman_ford step 4331 current loss 0.017380, current_train_items 138624.
I0304 19:30:16.820028 22849303695488 run.py:483] Algo bellman_ford step 4332 current loss 0.089247, current_train_items 138656.
I0304 19:30:16.851390 22849303695488 run.py:483] Algo bellman_ford step 4333 current loss 0.086014, current_train_items 138688.
I0304 19:30:16.883538 22849303695488 run.py:483] Algo bellman_ford step 4334 current loss 0.085049, current_train_items 138720.
I0304 19:30:16.903511 22849303695488 run.py:483] Algo bellman_ford step 4335 current loss 0.010794, current_train_items 138752.
I0304 19:30:16.919992 22849303695488 run.py:483] Algo bellman_ford step 4336 current loss 0.015949, current_train_items 138784.
I0304 19:30:16.944531 22849303695488 run.py:483] Algo bellman_ford step 4337 current loss 0.071970, current_train_items 138816.
I0304 19:30:16.977780 22849303695488 run.py:483] Algo bellman_ford step 4338 current loss 0.095971, current_train_items 138848.
I0304 19:30:17.010877 22849303695488 run.py:483] Algo bellman_ford step 4339 current loss 0.098511, current_train_items 138880.
I0304 19:30:17.030764 22849303695488 run.py:483] Algo bellman_ford step 4340 current loss 0.010145, current_train_items 138912.
I0304 19:30:17.047124 22849303695488 run.py:483] Algo bellman_ford step 4341 current loss 0.008989, current_train_items 138944.
I0304 19:30:17.071524 22849303695488 run.py:483] Algo bellman_ford step 4342 current loss 0.034257, current_train_items 138976.
I0304 19:30:17.103566 22849303695488 run.py:483] Algo bellman_ford step 4343 current loss 0.102241, current_train_items 139008.
I0304 19:30:17.138528 22849303695488 run.py:483] Algo bellman_ford step 4344 current loss 0.081087, current_train_items 139040.
I0304 19:30:17.158635 22849303695488 run.py:483] Algo bellman_ford step 4345 current loss 0.014610, current_train_items 139072.
I0304 19:30:17.175559 22849303695488 run.py:483] Algo bellman_ford step 4346 current loss 0.037323, current_train_items 139104.
I0304 19:30:17.200246 22849303695488 run.py:483] Algo bellman_ford step 4347 current loss 0.135316, current_train_items 139136.
I0304 19:30:17.231860 22849303695488 run.py:483] Algo bellman_ford step 4348 current loss 0.085901, current_train_items 139168.
I0304 19:30:17.265269 22849303695488 run.py:483] Algo bellman_ford step 4349 current loss 0.123024, current_train_items 139200.
I0304 19:30:17.285250 22849303695488 run.py:483] Algo bellman_ford step 4350 current loss 0.010648, current_train_items 139232.
I0304 19:30:17.293597 22849303695488 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0304 19:30:17.293707 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:17.310982 22849303695488 run.py:483] Algo bellman_ford step 4351 current loss 0.033135, current_train_items 139264.
I0304 19:30:17.336927 22849303695488 run.py:483] Algo bellman_ford step 4352 current loss 0.044127, current_train_items 139296.
I0304 19:30:17.368755 22849303695488 run.py:483] Algo bellman_ford step 4353 current loss 0.049079, current_train_items 139328.
I0304 19:30:17.403828 22849303695488 run.py:483] Algo bellman_ford step 4354 current loss 0.082995, current_train_items 139360.
I0304 19:30:17.424391 22849303695488 run.py:483] Algo bellman_ford step 4355 current loss 0.006122, current_train_items 139392.
I0304 19:30:17.440880 22849303695488 run.py:483] Algo bellman_ford step 4356 current loss 0.009636, current_train_items 139424.
I0304 19:30:17.466015 22849303695488 run.py:483] Algo bellman_ford step 4357 current loss 0.108947, current_train_items 139456.
I0304 19:30:17.496357 22849303695488 run.py:483] Algo bellman_ford step 4358 current loss 0.053059, current_train_items 139488.
I0304 19:30:17.529760 22849303695488 run.py:483] Algo bellman_ford step 4359 current loss 0.094369, current_train_items 139520.
I0304 19:30:17.550447 22849303695488 run.py:483] Algo bellman_ford step 4360 current loss 0.003950, current_train_items 139552.
I0304 19:30:17.567623 22849303695488 run.py:483] Algo bellman_ford step 4361 current loss 0.056942, current_train_items 139584.
I0304 19:30:17.591000 22849303695488 run.py:483] Algo bellman_ford step 4362 current loss 0.056696, current_train_items 139616.
I0304 19:30:17.623927 22849303695488 run.py:483] Algo bellman_ford step 4363 current loss 0.071199, current_train_items 139648.
I0304 19:30:17.660117 22849303695488 run.py:483] Algo bellman_ford step 4364 current loss 0.105353, current_train_items 139680.
I0304 19:30:17.680099 22849303695488 run.py:483] Algo bellman_ford step 4365 current loss 0.010135, current_train_items 139712.
I0304 19:30:17.696870 22849303695488 run.py:483] Algo bellman_ford step 4366 current loss 0.019430, current_train_items 139744.
I0304 19:30:17.721141 22849303695488 run.py:483] Algo bellman_ford step 4367 current loss 0.034007, current_train_items 139776.
I0304 19:30:17.751874 22849303695488 run.py:483] Algo bellman_ford step 4368 current loss 0.056249, current_train_items 139808.
I0304 19:30:17.786986 22849303695488 run.py:483] Algo bellman_ford step 4369 current loss 0.069377, current_train_items 139840.
I0304 19:30:17.807361 22849303695488 run.py:483] Algo bellman_ford step 4370 current loss 0.002337, current_train_items 139872.
I0304 19:30:17.823928 22849303695488 run.py:483] Algo bellman_ford step 4371 current loss 0.040837, current_train_items 139904.
I0304 19:30:17.847875 22849303695488 run.py:483] Algo bellman_ford step 4372 current loss 0.038150, current_train_items 139936.
I0304 19:30:17.879346 22849303695488 run.py:483] Algo bellman_ford step 4373 current loss 0.077261, current_train_items 139968.
I0304 19:30:17.913560 22849303695488 run.py:483] Algo bellman_ford step 4374 current loss 0.109980, current_train_items 140000.
I0304 19:30:17.934582 22849303695488 run.py:483] Algo bellman_ford step 4375 current loss 0.010899, current_train_items 140032.
I0304 19:30:17.951166 22849303695488 run.py:483] Algo bellman_ford step 4376 current loss 0.011499, current_train_items 140064.
I0304 19:30:17.975153 22849303695488 run.py:483] Algo bellman_ford step 4377 current loss 0.048488, current_train_items 140096.
I0304 19:30:18.006520 22849303695488 run.py:483] Algo bellman_ford step 4378 current loss 0.056397, current_train_items 140128.
I0304 19:30:18.041278 22849303695488 run.py:483] Algo bellman_ford step 4379 current loss 0.097854, current_train_items 140160.
I0304 19:30:18.061418 22849303695488 run.py:483] Algo bellman_ford step 4380 current loss 0.004105, current_train_items 140192.
I0304 19:30:18.077930 22849303695488 run.py:483] Algo bellman_ford step 4381 current loss 0.082180, current_train_items 140224.
I0304 19:30:18.102818 22849303695488 run.py:483] Algo bellman_ford step 4382 current loss 0.081200, current_train_items 140256.
I0304 19:30:18.134581 22849303695488 run.py:483] Algo bellman_ford step 4383 current loss 0.061293, current_train_items 140288.
I0304 19:30:18.169288 22849303695488 run.py:483] Algo bellman_ford step 4384 current loss 0.060792, current_train_items 140320.
I0304 19:30:18.189682 22849303695488 run.py:483] Algo bellman_ford step 4385 current loss 0.036725, current_train_items 140352.
I0304 19:30:18.206185 22849303695488 run.py:483] Algo bellman_ford step 4386 current loss 0.034628, current_train_items 140384.
I0304 19:30:18.230397 22849303695488 run.py:483] Algo bellman_ford step 4387 current loss 0.094513, current_train_items 140416.
I0304 19:30:18.262597 22849303695488 run.py:483] Algo bellman_ford step 4388 current loss 0.069424, current_train_items 140448.
I0304 19:30:18.295281 22849303695488 run.py:483] Algo bellman_ford step 4389 current loss 0.096267, current_train_items 140480.
I0304 19:30:18.315830 22849303695488 run.py:483] Algo bellman_ford step 4390 current loss 0.008114, current_train_items 140512.
I0304 19:30:18.332327 22849303695488 run.py:483] Algo bellman_ford step 4391 current loss 0.011714, current_train_items 140544.
I0304 19:30:18.356085 22849303695488 run.py:483] Algo bellman_ford step 4392 current loss 0.098159, current_train_items 140576.
I0304 19:30:18.388041 22849303695488 run.py:483] Algo bellman_ford step 4393 current loss 0.124232, current_train_items 140608.
I0304 19:30:18.423029 22849303695488 run.py:483] Algo bellman_ford step 4394 current loss 0.201015, current_train_items 140640.
I0304 19:30:18.443367 22849303695488 run.py:483] Algo bellman_ford step 4395 current loss 0.015195, current_train_items 140672.
I0304 19:30:18.460249 22849303695488 run.py:483] Algo bellman_ford step 4396 current loss 0.063231, current_train_items 140704.
I0304 19:30:18.484849 22849303695488 run.py:483] Algo bellman_ford step 4397 current loss 0.074351, current_train_items 140736.
I0304 19:30:18.516989 22849303695488 run.py:483] Algo bellman_ford step 4398 current loss 0.068623, current_train_items 140768.
I0304 19:30:18.550900 22849303695488 run.py:483] Algo bellman_ford step 4399 current loss 0.152957, current_train_items 140800.
I0304 19:30:18.571282 22849303695488 run.py:483] Algo bellman_ford step 4400 current loss 0.004512, current_train_items 140832.
I0304 19:30:18.579378 22849303695488 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0304 19:30:18.579488 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:30:18.597095 22849303695488 run.py:483] Algo bellman_ford step 4401 current loss 0.054044, current_train_items 140864.
I0304 19:30:18.621553 22849303695488 run.py:483] Algo bellman_ford step 4402 current loss 0.123391, current_train_items 140896.
I0304 19:30:18.652853 22849303695488 run.py:483] Algo bellman_ford step 4403 current loss 0.079772, current_train_items 140928.
I0304 19:30:18.691684 22849303695488 run.py:483] Algo bellman_ford step 4404 current loss 0.242686, current_train_items 140960.
I0304 19:30:18.711996 22849303695488 run.py:483] Algo bellman_ford step 4405 current loss 0.007546, current_train_items 140992.
I0304 19:30:18.728625 22849303695488 run.py:483] Algo bellman_ford step 4406 current loss 0.045292, current_train_items 141024.
I0304 19:30:18.752839 22849303695488 run.py:483] Algo bellman_ford step 4407 current loss 0.077148, current_train_items 141056.
I0304 19:30:18.784409 22849303695488 run.py:483] Algo bellman_ford step 4408 current loss 0.113060, current_train_items 141088.
I0304 19:30:18.820388 22849303695488 run.py:483] Algo bellman_ford step 4409 current loss 0.154097, current_train_items 141120.
I0304 19:30:18.840580 22849303695488 run.py:483] Algo bellman_ford step 4410 current loss 0.004708, current_train_items 141152.
I0304 19:30:18.857213 22849303695488 run.py:483] Algo bellman_ford step 4411 current loss 0.025538, current_train_items 141184.
I0304 19:30:18.881644 22849303695488 run.py:483] Algo bellman_ford step 4412 current loss 0.047141, current_train_items 141216.
I0304 19:30:18.914752 22849303695488 run.py:483] Algo bellman_ford step 4413 current loss 0.102156, current_train_items 141248.
I0304 19:30:18.947786 22849303695488 run.py:483] Algo bellman_ford step 4414 current loss 0.073342, current_train_items 141280.
I0304 19:30:18.967853 22849303695488 run.py:483] Algo bellman_ford step 4415 current loss 0.005955, current_train_items 141312.
I0304 19:30:18.984978 22849303695488 run.py:483] Algo bellman_ford step 4416 current loss 0.032900, current_train_items 141344.
I0304 19:30:19.009014 22849303695488 run.py:483] Algo bellman_ford step 4417 current loss 0.063377, current_train_items 141376.
I0304 19:30:19.040502 22849303695488 run.py:483] Algo bellman_ford step 4418 current loss 0.056672, current_train_items 141408.
I0304 19:30:19.074372 22849303695488 run.py:483] Algo bellman_ford step 4419 current loss 0.071199, current_train_items 141440.
I0304 19:30:19.094292 22849303695488 run.py:483] Algo bellman_ford step 4420 current loss 0.012471, current_train_items 141472.
I0304 19:30:19.110590 22849303695488 run.py:483] Algo bellman_ford step 4421 current loss 0.012084, current_train_items 141504.
I0304 19:30:19.134615 22849303695488 run.py:483] Algo bellman_ford step 4422 current loss 0.096151, current_train_items 141536.
I0304 19:30:19.166059 22849303695488 run.py:483] Algo bellman_ford step 4423 current loss 0.048210, current_train_items 141568.
I0304 19:30:19.200912 22849303695488 run.py:483] Algo bellman_ford step 4424 current loss 0.080047, current_train_items 141600.
I0304 19:30:19.221101 22849303695488 run.py:483] Algo bellman_ford step 4425 current loss 0.008586, current_train_items 141632.
I0304 19:30:19.236999 22849303695488 run.py:483] Algo bellman_ford step 4426 current loss 0.027192, current_train_items 141664.
I0304 19:30:19.261560 22849303695488 run.py:483] Algo bellman_ford step 4427 current loss 0.091455, current_train_items 141696.
I0304 19:30:19.293500 22849303695488 run.py:483] Algo bellman_ford step 4428 current loss 0.042278, current_train_items 141728.
I0304 19:30:19.327391 22849303695488 run.py:483] Algo bellman_ford step 4429 current loss 0.060381, current_train_items 141760.
I0304 19:30:19.347246 22849303695488 run.py:483] Algo bellman_ford step 4430 current loss 0.004195, current_train_items 141792.
I0304 19:30:19.363364 22849303695488 run.py:483] Algo bellman_ford step 4431 current loss 0.026642, current_train_items 141824.
I0304 19:30:19.387738 22849303695488 run.py:483] Algo bellman_ford step 4432 current loss 0.083961, current_train_items 141856.
I0304 19:30:19.418545 22849303695488 run.py:483] Algo bellman_ford step 4433 current loss 0.095859, current_train_items 141888.
I0304 19:30:19.452489 22849303695488 run.py:483] Algo bellman_ford step 4434 current loss 0.167144, current_train_items 141920.
I0304 19:30:19.472496 22849303695488 run.py:483] Algo bellman_ford step 4435 current loss 0.006193, current_train_items 141952.
I0304 19:30:19.489165 22849303695488 run.py:483] Algo bellman_ford step 4436 current loss 0.032178, current_train_items 141984.
I0304 19:30:19.514073 22849303695488 run.py:483] Algo bellman_ford step 4437 current loss 0.064231, current_train_items 142016.
I0304 19:30:19.544699 22849303695488 run.py:483] Algo bellman_ford step 4438 current loss 0.124596, current_train_items 142048.
I0304 19:30:19.579639 22849303695488 run.py:483] Algo bellman_ford step 4439 current loss 0.236409, current_train_items 142080.
I0304 19:30:19.599404 22849303695488 run.py:483] Algo bellman_ford step 4440 current loss 0.005322, current_train_items 142112.
I0304 19:30:19.616301 22849303695488 run.py:483] Algo bellman_ford step 4441 current loss 0.050258, current_train_items 142144.
I0304 19:30:19.640163 22849303695488 run.py:483] Algo bellman_ford step 4442 current loss 0.052131, current_train_items 142176.
I0304 19:30:19.671420 22849303695488 run.py:483] Algo bellman_ford step 4443 current loss 0.087502, current_train_items 142208.
I0304 19:30:19.706759 22849303695488 run.py:483] Algo bellman_ford step 4444 current loss 0.153149, current_train_items 142240.
I0304 19:30:19.726822 22849303695488 run.py:483] Algo bellman_ford step 4445 current loss 0.028061, current_train_items 142272.
I0304 19:30:19.743108 22849303695488 run.py:483] Algo bellman_ford step 4446 current loss 0.046409, current_train_items 142304.
I0304 19:30:19.767673 22849303695488 run.py:483] Algo bellman_ford step 4447 current loss 0.139699, current_train_items 142336.
I0304 19:30:19.799882 22849303695488 run.py:483] Algo bellman_ford step 4448 current loss 0.128825, current_train_items 142368.
I0304 19:30:19.834085 22849303695488 run.py:483] Algo bellman_ford step 4449 current loss 0.119171, current_train_items 142400.
I0304 19:30:19.854103 22849303695488 run.py:483] Algo bellman_ford step 4450 current loss 0.055263, current_train_items 142432.
I0304 19:30:19.862473 22849303695488 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0304 19:30:19.862580 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:30:19.879127 22849303695488 run.py:483] Algo bellman_ford step 4451 current loss 0.032491, current_train_items 142464.
I0304 19:30:19.904493 22849303695488 run.py:483] Algo bellman_ford step 4452 current loss 0.097661, current_train_items 142496.
I0304 19:30:19.935550 22849303695488 run.py:483] Algo bellman_ford step 4453 current loss 0.082885, current_train_items 142528.
I0304 19:30:19.968961 22849303695488 run.py:483] Algo bellman_ford step 4454 current loss 0.104056, current_train_items 142560.
I0304 19:30:19.989492 22849303695488 run.py:483] Algo bellman_ford step 4455 current loss 0.022614, current_train_items 142592.
I0304 19:30:20.005956 22849303695488 run.py:483] Algo bellman_ford step 4456 current loss 0.081865, current_train_items 142624.
I0304 19:30:20.030702 22849303695488 run.py:483] Algo bellman_ford step 4457 current loss 0.139279, current_train_items 142656.
I0304 19:30:20.060902 22849303695488 run.py:483] Algo bellman_ford step 4458 current loss 0.159223, current_train_items 142688.
I0304 19:30:20.098270 22849303695488 run.py:483] Algo bellman_ford step 4459 current loss 0.248786, current_train_items 142720.
I0304 19:30:20.119334 22849303695488 run.py:483] Algo bellman_ford step 4460 current loss 0.048861, current_train_items 142752.
I0304 19:30:20.136181 22849303695488 run.py:483] Algo bellman_ford step 4461 current loss 0.030955, current_train_items 142784.
I0304 19:30:20.161027 22849303695488 run.py:483] Algo bellman_ford step 4462 current loss 0.119772, current_train_items 142816.
I0304 19:30:20.191222 22849303695488 run.py:483] Algo bellman_ford step 4463 current loss 0.069094, current_train_items 142848.
I0304 19:30:20.225514 22849303695488 run.py:483] Algo bellman_ford step 4464 current loss 0.130161, current_train_items 142880.
I0304 19:30:20.245961 22849303695488 run.py:483] Algo bellman_ford step 4465 current loss 0.007860, current_train_items 142912.
I0304 19:30:20.262703 22849303695488 run.py:483] Algo bellman_ford step 4466 current loss 0.028233, current_train_items 142944.
I0304 19:30:20.286901 22849303695488 run.py:483] Algo bellman_ford step 4467 current loss 0.055312, current_train_items 142976.
I0304 19:30:20.320146 22849303695488 run.py:483] Algo bellman_ford step 4468 current loss 0.074767, current_train_items 143008.
I0304 19:30:20.355529 22849303695488 run.py:483] Algo bellman_ford step 4469 current loss 0.102203, current_train_items 143040.
I0304 19:30:20.375997 22849303695488 run.py:483] Algo bellman_ford step 4470 current loss 0.011123, current_train_items 143072.
I0304 19:30:20.392464 22849303695488 run.py:483] Algo bellman_ford step 4471 current loss 0.012856, current_train_items 143104.
I0304 19:30:20.416306 22849303695488 run.py:483] Algo bellman_ford step 4472 current loss 0.063996, current_train_items 143136.
I0304 19:30:20.448034 22849303695488 run.py:483] Algo bellman_ford step 4473 current loss 0.075914, current_train_items 143168.
I0304 19:30:20.484946 22849303695488 run.py:483] Algo bellman_ford step 4474 current loss 0.117888, current_train_items 143200.
I0304 19:30:20.505783 22849303695488 run.py:483] Algo bellman_ford step 4475 current loss 0.028397, current_train_items 143232.
I0304 19:30:20.522350 22849303695488 run.py:483] Algo bellman_ford step 4476 current loss 0.032634, current_train_items 143264.
I0304 19:30:20.546089 22849303695488 run.py:483] Algo bellman_ford step 4477 current loss 0.053140, current_train_items 143296.
I0304 19:30:20.577926 22849303695488 run.py:483] Algo bellman_ford step 4478 current loss 0.064990, current_train_items 143328.
I0304 19:30:20.614238 22849303695488 run.py:483] Algo bellman_ford step 4479 current loss 0.114361, current_train_items 143360.
I0304 19:30:20.634547 22849303695488 run.py:483] Algo bellman_ford step 4480 current loss 0.007739, current_train_items 143392.
I0304 19:30:20.651013 22849303695488 run.py:483] Algo bellman_ford step 4481 current loss 0.035749, current_train_items 143424.
I0304 19:30:20.676077 22849303695488 run.py:483] Algo bellman_ford step 4482 current loss 0.045192, current_train_items 143456.
I0304 19:30:20.708507 22849303695488 run.py:483] Algo bellman_ford step 4483 current loss 0.072027, current_train_items 143488.
I0304 19:30:20.740815 22849303695488 run.py:483] Algo bellman_ford step 4484 current loss 0.087172, current_train_items 143520.
I0304 19:30:20.761507 22849303695488 run.py:483] Algo bellman_ford step 4485 current loss 0.009032, current_train_items 143552.
I0304 19:30:20.778542 22849303695488 run.py:483] Algo bellman_ford step 4486 current loss 0.018250, current_train_items 143584.
I0304 19:30:20.804032 22849303695488 run.py:483] Algo bellman_ford step 4487 current loss 0.070339, current_train_items 143616.
I0304 19:30:20.836233 22849303695488 run.py:483] Algo bellman_ford step 4488 current loss 0.070048, current_train_items 143648.
I0304 19:30:20.870138 22849303695488 run.py:483] Algo bellman_ford step 4489 current loss 0.060523, current_train_items 143680.
I0304 19:30:20.890794 22849303695488 run.py:483] Algo bellman_ford step 4490 current loss 0.019448, current_train_items 143712.
I0304 19:30:20.907130 22849303695488 run.py:483] Algo bellman_ford step 4491 current loss 0.021691, current_train_items 143744.
I0304 19:30:20.931763 22849303695488 run.py:483] Algo bellman_ford step 4492 current loss 0.048642, current_train_items 143776.
I0304 19:30:20.964077 22849303695488 run.py:483] Algo bellman_ford step 4493 current loss 0.111301, current_train_items 143808.
I0304 19:30:20.999125 22849303695488 run.py:483] Algo bellman_ford step 4494 current loss 0.130888, current_train_items 143840.
I0304 19:30:21.019402 22849303695488 run.py:483] Algo bellman_ford step 4495 current loss 0.015931, current_train_items 143872.
I0304 19:30:21.036528 22849303695488 run.py:483] Algo bellman_ford step 4496 current loss 0.034759, current_train_items 143904.
I0304 19:30:21.060915 22849303695488 run.py:483] Algo bellman_ford step 4497 current loss 0.058871, current_train_items 143936.
I0304 19:30:21.091541 22849303695488 run.py:483] Algo bellman_ford step 4498 current loss 0.073627, current_train_items 143968.
I0304 19:30:21.123772 22849303695488 run.py:483] Algo bellman_ford step 4499 current loss 0.061000, current_train_items 144000.
I0304 19:30:21.144569 22849303695488 run.py:483] Algo bellman_ford step 4500 current loss 0.009494, current_train_items 144032.
I0304 19:30:21.152422 22849303695488 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0304 19:30:21.152531 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:21.169668 22849303695488 run.py:483] Algo bellman_ford step 4501 current loss 0.076863, current_train_items 144064.
I0304 19:30:21.194997 22849303695488 run.py:483] Algo bellman_ford step 4502 current loss 0.101782, current_train_items 144096.
I0304 19:30:21.227086 22849303695488 run.py:483] Algo bellman_ford step 4503 current loss 0.135263, current_train_items 144128.
I0304 19:30:21.261847 22849303695488 run.py:483] Algo bellman_ford step 4504 current loss 0.046038, current_train_items 144160.
I0304 19:30:21.282239 22849303695488 run.py:483] Algo bellman_ford step 4505 current loss 0.004129, current_train_items 144192.
I0304 19:30:21.298687 22849303695488 run.py:483] Algo bellman_ford step 4506 current loss 0.038757, current_train_items 144224.
I0304 19:30:21.322422 22849303695488 run.py:483] Algo bellman_ford step 4507 current loss 0.052119, current_train_items 144256.
I0304 19:30:21.353266 22849303695488 run.py:483] Algo bellman_ford step 4508 current loss 0.060888, current_train_items 144288.
I0304 19:30:21.386684 22849303695488 run.py:483] Algo bellman_ford step 4509 current loss 0.067741, current_train_items 144320.
I0304 19:30:21.407120 22849303695488 run.py:483] Algo bellman_ford step 4510 current loss 0.049237, current_train_items 144352.
I0304 19:30:21.424069 22849303695488 run.py:483] Algo bellman_ford step 4511 current loss 0.062032, current_train_items 144384.
I0304 19:30:21.448392 22849303695488 run.py:483] Algo bellman_ford step 4512 current loss 0.068291, current_train_items 144416.
I0304 19:30:21.479754 22849303695488 run.py:483] Algo bellman_ford step 4513 current loss 0.059335, current_train_items 144448.
I0304 19:30:21.513154 22849303695488 run.py:483] Algo bellman_ford step 4514 current loss 0.063677, current_train_items 144480.
I0304 19:30:21.533048 22849303695488 run.py:483] Algo bellman_ford step 4515 current loss 0.007633, current_train_items 144512.
I0304 19:30:21.549880 22849303695488 run.py:483] Algo bellman_ford step 4516 current loss 0.017713, current_train_items 144544.
I0304 19:30:21.573035 22849303695488 run.py:483] Algo bellman_ford step 4517 current loss 0.117502, current_train_items 144576.
I0304 19:30:21.605507 22849303695488 run.py:483] Algo bellman_ford step 4518 current loss 0.073097, current_train_items 144608.
I0304 19:30:21.642573 22849303695488 run.py:483] Algo bellman_ford step 4519 current loss 0.082869, current_train_items 144640.
I0304 19:30:21.662540 22849303695488 run.py:483] Algo bellman_ford step 4520 current loss 0.010177, current_train_items 144672.
I0304 19:30:21.679326 22849303695488 run.py:483] Algo bellman_ford step 4521 current loss 0.016237, current_train_items 144704.
I0304 19:30:21.703263 22849303695488 run.py:483] Algo bellman_ford step 4522 current loss 0.078075, current_train_items 144736.
I0304 19:30:21.736760 22849303695488 run.py:483] Algo bellman_ford step 4523 current loss 0.171502, current_train_items 144768.
I0304 19:30:21.770530 22849303695488 run.py:483] Algo bellman_ford step 4524 current loss 0.105933, current_train_items 144800.
I0304 19:30:21.790418 22849303695488 run.py:483] Algo bellman_ford step 4525 current loss 0.008679, current_train_items 144832.
I0304 19:30:21.806931 22849303695488 run.py:483] Algo bellman_ford step 4526 current loss 0.017278, current_train_items 144864.
I0304 19:30:21.831997 22849303695488 run.py:483] Algo bellman_ford step 4527 current loss 0.057329, current_train_items 144896.
I0304 19:30:21.863646 22849303695488 run.py:483] Algo bellman_ford step 4528 current loss 0.083416, current_train_items 144928.
I0304 19:30:21.898103 22849303695488 run.py:483] Algo bellman_ford step 4529 current loss 0.121576, current_train_items 144960.
I0304 19:30:21.918147 22849303695488 run.py:483] Algo bellman_ford step 4530 current loss 0.019796, current_train_items 144992.
I0304 19:30:21.934473 22849303695488 run.py:483] Algo bellman_ford step 4531 current loss 0.026248, current_train_items 145024.
I0304 19:30:21.958204 22849303695488 run.py:483] Algo bellman_ford step 4532 current loss 0.049106, current_train_items 145056.
I0304 19:30:21.988236 22849303695488 run.py:483] Algo bellman_ford step 4533 current loss 0.037118, current_train_items 145088.
I0304 19:30:22.023720 22849303695488 run.py:483] Algo bellman_ford step 4534 current loss 0.138078, current_train_items 145120.
I0304 19:30:22.043679 22849303695488 run.py:483] Algo bellman_ford step 4535 current loss 0.006252, current_train_items 145152.
I0304 19:30:22.060336 22849303695488 run.py:483] Algo bellman_ford step 4536 current loss 0.025200, current_train_items 145184.
I0304 19:30:22.084533 22849303695488 run.py:483] Algo bellman_ford step 4537 current loss 0.052848, current_train_items 145216.
I0304 19:30:22.115725 22849303695488 run.py:483] Algo bellman_ford step 4538 current loss 0.028183, current_train_items 145248.
I0304 19:30:22.151213 22849303695488 run.py:483] Algo bellman_ford step 4539 current loss 0.120207, current_train_items 145280.
I0304 19:30:22.171432 22849303695488 run.py:483] Algo bellman_ford step 4540 current loss 0.012120, current_train_items 145312.
I0304 19:30:22.187917 22849303695488 run.py:483] Algo bellman_ford step 4541 current loss 0.017987, current_train_items 145344.
I0304 19:30:22.211273 22849303695488 run.py:483] Algo bellman_ford step 4542 current loss 0.036381, current_train_items 145376.
I0304 19:30:22.242757 22849303695488 run.py:483] Algo bellman_ford step 4543 current loss 0.054497, current_train_items 145408.
I0304 19:30:22.277766 22849303695488 run.py:483] Algo bellman_ford step 4544 current loss 0.072335, current_train_items 145440.
I0304 19:30:22.297663 22849303695488 run.py:483] Algo bellman_ford step 4545 current loss 0.016192, current_train_items 145472.
I0304 19:30:22.314298 22849303695488 run.py:483] Algo bellman_ford step 4546 current loss 0.013082, current_train_items 145504.
I0304 19:30:22.338141 22849303695488 run.py:483] Algo bellman_ford step 4547 current loss 0.021477, current_train_items 145536.
I0304 19:30:22.368402 22849303695488 run.py:483] Algo bellman_ford step 4548 current loss 0.051159, current_train_items 145568.
I0304 19:30:22.403353 22849303695488 run.py:483] Algo bellman_ford step 4549 current loss 0.067442, current_train_items 145600.
I0304 19:30:22.423290 22849303695488 run.py:483] Algo bellman_ford step 4550 current loss 0.003413, current_train_items 145632.
I0304 19:30:22.431658 22849303695488 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0304 19:30:22.431766 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:22.449313 22849303695488 run.py:483] Algo bellman_ford step 4551 current loss 0.019780, current_train_items 145664.
I0304 19:30:22.473925 22849303695488 run.py:483] Algo bellman_ford step 4552 current loss 0.025952, current_train_items 145696.
I0304 19:30:22.506397 22849303695488 run.py:483] Algo bellman_ford step 4553 current loss 0.145647, current_train_items 145728.
I0304 19:30:22.541126 22849303695488 run.py:483] Algo bellman_ford step 4554 current loss 0.080707, current_train_items 145760.
I0304 19:30:22.561514 22849303695488 run.py:483] Algo bellman_ford step 4555 current loss 0.012032, current_train_items 145792.
I0304 19:30:22.577430 22849303695488 run.py:483] Algo bellman_ford step 4556 current loss 0.057664, current_train_items 145824.
I0304 19:30:22.600749 22849303695488 run.py:483] Algo bellman_ford step 4557 current loss 0.095001, current_train_items 145856.
I0304 19:30:22.633465 22849303695488 run.py:483] Algo bellman_ford step 4558 current loss 0.060011, current_train_items 145888.
I0304 19:30:22.668947 22849303695488 run.py:483] Algo bellman_ford step 4559 current loss 0.083608, current_train_items 145920.
I0304 19:30:22.689124 22849303695488 run.py:483] Algo bellman_ford step 4560 current loss 0.006541, current_train_items 145952.
I0304 19:30:22.705995 22849303695488 run.py:483] Algo bellman_ford step 4561 current loss 0.037490, current_train_items 145984.
I0304 19:30:22.729412 22849303695488 run.py:483] Algo bellman_ford step 4562 current loss 0.045237, current_train_items 146016.
I0304 19:30:22.760295 22849303695488 run.py:483] Algo bellman_ford step 4563 current loss 0.084264, current_train_items 146048.
I0304 19:30:22.795716 22849303695488 run.py:483] Algo bellman_ford step 4564 current loss 0.073845, current_train_items 146080.
I0304 19:30:22.815918 22849303695488 run.py:483] Algo bellman_ford step 4565 current loss 0.006242, current_train_items 146112.
I0304 19:30:22.832473 22849303695488 run.py:483] Algo bellman_ford step 4566 current loss 0.058931, current_train_items 146144.
I0304 19:30:22.857093 22849303695488 run.py:483] Algo bellman_ford step 4567 current loss 0.043920, current_train_items 146176.
I0304 19:30:22.889641 22849303695488 run.py:483] Algo bellman_ford step 4568 current loss 0.115097, current_train_items 146208.
I0304 19:30:22.923350 22849303695488 run.py:483] Algo bellman_ford step 4569 current loss 0.090819, current_train_items 146240.
I0304 19:30:22.943400 22849303695488 run.py:483] Algo bellman_ford step 4570 current loss 0.003449, current_train_items 146272.
I0304 19:30:22.959937 22849303695488 run.py:483] Algo bellman_ford step 4571 current loss 0.060889, current_train_items 146304.
I0304 19:30:22.984159 22849303695488 run.py:483] Algo bellman_ford step 4572 current loss 0.068053, current_train_items 146336.
I0304 19:30:23.014900 22849303695488 run.py:483] Algo bellman_ford step 4573 current loss 0.074301, current_train_items 146368.
I0304 19:30:23.048078 22849303695488 run.py:483] Algo bellman_ford step 4574 current loss 0.126993, current_train_items 146400.
I0304 19:30:23.068210 22849303695488 run.py:483] Algo bellman_ford step 4575 current loss 0.003061, current_train_items 146432.
I0304 19:30:23.084768 22849303695488 run.py:483] Algo bellman_ford step 4576 current loss 0.034983, current_train_items 146464.
I0304 19:30:23.108400 22849303695488 run.py:483] Algo bellman_ford step 4577 current loss 0.087952, current_train_items 146496.
I0304 19:30:23.140396 22849303695488 run.py:483] Algo bellman_ford step 4578 current loss 0.125326, current_train_items 146528.
I0304 19:30:23.175054 22849303695488 run.py:483] Algo bellman_ford step 4579 current loss 0.118300, current_train_items 146560.
I0304 19:30:23.194939 22849303695488 run.py:483] Algo bellman_ford step 4580 current loss 0.005290, current_train_items 146592.
I0304 19:30:23.211910 22849303695488 run.py:483] Algo bellman_ford step 4581 current loss 0.043819, current_train_items 146624.
I0304 19:30:23.236086 22849303695488 run.py:483] Algo bellman_ford step 4582 current loss 0.124318, current_train_items 146656.
I0304 19:30:23.267818 22849303695488 run.py:483] Algo bellman_ford step 4583 current loss 0.120490, current_train_items 146688.
I0304 19:30:23.302525 22849303695488 run.py:483] Algo bellman_ford step 4584 current loss 0.191208, current_train_items 146720.
I0304 19:30:23.322957 22849303695488 run.py:483] Algo bellman_ford step 4585 current loss 0.012715, current_train_items 146752.
I0304 19:30:23.340221 22849303695488 run.py:483] Algo bellman_ford step 4586 current loss 0.028526, current_train_items 146784.
I0304 19:30:23.363306 22849303695488 run.py:483] Algo bellman_ford step 4587 current loss 0.053736, current_train_items 146816.
I0304 19:30:23.394958 22849303695488 run.py:483] Algo bellman_ford step 4588 current loss 0.086869, current_train_items 146848.
I0304 19:30:23.428521 22849303695488 run.py:483] Algo bellman_ford step 4589 current loss 0.094038, current_train_items 146880.
I0304 19:30:23.449245 22849303695488 run.py:483] Algo bellman_ford step 4590 current loss 0.012157, current_train_items 146912.
I0304 19:30:23.465856 22849303695488 run.py:483] Algo bellman_ford step 4591 current loss 0.018280, current_train_items 146944.
I0304 19:30:23.489231 22849303695488 run.py:483] Algo bellman_ford step 4592 current loss 0.060031, current_train_items 146976.
I0304 19:30:23.521435 22849303695488 run.py:483] Algo bellman_ford step 4593 current loss 0.111083, current_train_items 147008.
I0304 19:30:23.556389 22849303695488 run.py:483] Algo bellman_ford step 4594 current loss 0.144442, current_train_items 147040.
I0304 19:30:23.576357 22849303695488 run.py:483] Algo bellman_ford step 4595 current loss 0.006044, current_train_items 147072.
I0304 19:30:23.592880 22849303695488 run.py:483] Algo bellman_ford step 4596 current loss 0.018925, current_train_items 147104.
I0304 19:30:23.618992 22849303695488 run.py:483] Algo bellman_ford step 4597 current loss 0.098641, current_train_items 147136.
I0304 19:30:23.650513 22849303695488 run.py:483] Algo bellman_ford step 4598 current loss 0.093227, current_train_items 147168.
I0304 19:30:23.682862 22849303695488 run.py:483] Algo bellman_ford step 4599 current loss 0.097500, current_train_items 147200.
I0304 19:30:23.703217 22849303695488 run.py:483] Algo bellman_ford step 4600 current loss 0.007754, current_train_items 147232.
I0304 19:30:23.711125 22849303695488 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0304 19:30:23.711234 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:30:23.728680 22849303695488 run.py:483] Algo bellman_ford step 4601 current loss 0.022173, current_train_items 147264.
I0304 19:30:23.754176 22849303695488 run.py:483] Algo bellman_ford step 4602 current loss 0.052792, current_train_items 147296.
I0304 19:30:23.785997 22849303695488 run.py:483] Algo bellman_ford step 4603 current loss 0.069306, current_train_items 147328.
I0304 19:30:23.821072 22849303695488 run.py:483] Algo bellman_ford step 4604 current loss 0.106561, current_train_items 147360.
I0304 19:30:23.841618 22849303695488 run.py:483] Algo bellman_ford step 4605 current loss 0.003668, current_train_items 147392.
I0304 19:30:23.858182 22849303695488 run.py:483] Algo bellman_ford step 4606 current loss 0.024588, current_train_items 147424.
I0304 19:30:23.882291 22849303695488 run.py:483] Algo bellman_ford step 4607 current loss 0.036678, current_train_items 147456.
I0304 19:30:23.915173 22849303695488 run.py:483] Algo bellman_ford step 4608 current loss 0.034719, current_train_items 147488.
I0304 19:30:23.947535 22849303695488 run.py:483] Algo bellman_ford step 4609 current loss 0.078616, current_train_items 147520.
I0304 19:30:23.967756 22849303695488 run.py:483] Algo bellman_ford step 4610 current loss 0.002988, current_train_items 147552.
I0304 19:30:23.984142 22849303695488 run.py:483] Algo bellman_ford step 4611 current loss 0.022423, current_train_items 147584.
I0304 19:30:24.008910 22849303695488 run.py:483] Algo bellman_ford step 4612 current loss 0.061454, current_train_items 147616.
I0304 19:30:24.041305 22849303695488 run.py:483] Algo bellman_ford step 4613 current loss 0.091055, current_train_items 147648.
I0304 19:30:24.076838 22849303695488 run.py:483] Algo bellman_ford step 4614 current loss 0.056967, current_train_items 147680.
I0304 19:30:24.097257 22849303695488 run.py:483] Algo bellman_ford step 4615 current loss 0.024777, current_train_items 147712.
I0304 19:30:24.113952 22849303695488 run.py:483] Algo bellman_ford step 4616 current loss 0.045213, current_train_items 147744.
I0304 19:30:24.138610 22849303695488 run.py:483] Algo bellman_ford step 4617 current loss 0.046570, current_train_items 147776.
I0304 19:30:24.169569 22849303695488 run.py:483] Algo bellman_ford step 4618 current loss 0.056572, current_train_items 147808.
I0304 19:30:24.203812 22849303695488 run.py:483] Algo bellman_ford step 4619 current loss 0.063180, current_train_items 147840.
I0304 19:30:24.223633 22849303695488 run.py:483] Algo bellman_ford step 4620 current loss 0.008992, current_train_items 147872.
I0304 19:30:24.239987 22849303695488 run.py:483] Algo bellman_ford step 4621 current loss 0.026460, current_train_items 147904.
I0304 19:30:24.264023 22849303695488 run.py:483] Algo bellman_ford step 4622 current loss 0.058753, current_train_items 147936.
I0304 19:30:24.295876 22849303695488 run.py:483] Algo bellman_ford step 4623 current loss 0.111435, current_train_items 147968.
I0304 19:30:24.329316 22849303695488 run.py:483] Algo bellman_ford step 4624 current loss 0.201924, current_train_items 148000.
I0304 19:30:24.349329 22849303695488 run.py:483] Algo bellman_ford step 4625 current loss 0.007560, current_train_items 148032.
I0304 19:30:24.365873 22849303695488 run.py:483] Algo bellman_ford step 4626 current loss 0.031031, current_train_items 148064.
I0304 19:30:24.390726 22849303695488 run.py:483] Algo bellman_ford step 4627 current loss 0.118978, current_train_items 148096.
I0304 19:30:24.423206 22849303695488 run.py:483] Algo bellman_ford step 4628 current loss 0.135078, current_train_items 148128.
I0304 19:30:24.458976 22849303695488 run.py:483] Algo bellman_ford step 4629 current loss 0.068533, current_train_items 148160.
I0304 19:30:24.478847 22849303695488 run.py:483] Algo bellman_ford step 4630 current loss 0.006783, current_train_items 148192.
I0304 19:30:24.495632 22849303695488 run.py:483] Algo bellman_ford step 4631 current loss 0.029832, current_train_items 148224.
I0304 19:30:24.520923 22849303695488 run.py:483] Algo bellman_ford step 4632 current loss 0.077312, current_train_items 148256.
I0304 19:30:24.551078 22849303695488 run.py:483] Algo bellman_ford step 4633 current loss 0.045095, current_train_items 148288.
I0304 19:30:24.585700 22849303695488 run.py:483] Algo bellman_ford step 4634 current loss 0.083558, current_train_items 148320.
I0304 19:30:24.605974 22849303695488 run.py:483] Algo bellman_ford step 4635 current loss 0.008064, current_train_items 148352.
I0304 19:30:24.623146 22849303695488 run.py:483] Algo bellman_ford step 4636 current loss 0.015148, current_train_items 148384.
I0304 19:30:24.647675 22849303695488 run.py:483] Algo bellman_ford step 4637 current loss 0.061158, current_train_items 148416.
I0304 19:30:24.680945 22849303695488 run.py:483] Algo bellman_ford step 4638 current loss 0.073950, current_train_items 148448.
I0304 19:30:24.715429 22849303695488 run.py:483] Algo bellman_ford step 4639 current loss 0.076604, current_train_items 148480.
I0304 19:30:24.735414 22849303695488 run.py:483] Algo bellman_ford step 4640 current loss 0.011649, current_train_items 148512.
I0304 19:30:24.751970 22849303695488 run.py:483] Algo bellman_ford step 4641 current loss 0.041571, current_train_items 148544.
I0304 19:30:24.776536 22849303695488 run.py:483] Algo bellman_ford step 4642 current loss 0.072046, current_train_items 148576.
I0304 19:30:24.808404 22849303695488 run.py:483] Algo bellman_ford step 4643 current loss 0.155418, current_train_items 148608.
I0304 19:30:24.843410 22849303695488 run.py:483] Algo bellman_ford step 4644 current loss 0.116079, current_train_items 148640.
I0304 19:30:24.863565 22849303695488 run.py:483] Algo bellman_ford step 4645 current loss 0.004058, current_train_items 148672.
I0304 19:30:24.880206 22849303695488 run.py:483] Algo bellman_ford step 4646 current loss 0.012268, current_train_items 148704.
I0304 19:30:24.903418 22849303695488 run.py:483] Algo bellman_ford step 4647 current loss 0.041646, current_train_items 148736.
I0304 19:30:24.933365 22849303695488 run.py:483] Algo bellman_ford step 4648 current loss 0.085307, current_train_items 148768.
I0304 19:30:24.965805 22849303695488 run.py:483] Algo bellman_ford step 4649 current loss 0.067317, current_train_items 148800.
I0304 19:30:24.985754 22849303695488 run.py:483] Algo bellman_ford step 4650 current loss 0.006643, current_train_items 148832.
I0304 19:30:24.994000 22849303695488 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0304 19:30:24.994117 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:30:25.011460 22849303695488 run.py:483] Algo bellman_ford step 4651 current loss 0.047983, current_train_items 148864.
I0304 19:30:25.036185 22849303695488 run.py:483] Algo bellman_ford step 4652 current loss 0.042015, current_train_items 148896.
I0304 19:30:25.069163 22849303695488 run.py:483] Algo bellman_ford step 4653 current loss 0.064103, current_train_items 148928.
I0304 19:30:25.103954 22849303695488 run.py:483] Algo bellman_ford step 4654 current loss 0.107126, current_train_items 148960.
I0304 19:30:25.124150 22849303695488 run.py:483] Algo bellman_ford step 4655 current loss 0.009016, current_train_items 148992.
I0304 19:30:25.140546 22849303695488 run.py:483] Algo bellman_ford step 4656 current loss 0.012328, current_train_items 149024.
I0304 19:30:25.165722 22849303695488 run.py:483] Algo bellman_ford step 4657 current loss 0.035383, current_train_items 149056.
I0304 19:30:25.197121 22849303695488 run.py:483] Algo bellman_ford step 4658 current loss 0.060196, current_train_items 149088.
I0304 19:30:25.230340 22849303695488 run.py:483] Algo bellman_ford step 4659 current loss 0.069594, current_train_items 149120.
I0304 19:30:25.250655 22849303695488 run.py:483] Algo bellman_ford step 4660 current loss 0.005224, current_train_items 149152.
I0304 19:30:25.267456 22849303695488 run.py:483] Algo bellman_ford step 4661 current loss 0.024349, current_train_items 149184.
I0304 19:30:25.292543 22849303695488 run.py:483] Algo bellman_ford step 4662 current loss 0.065403, current_train_items 149216.
I0304 19:30:25.323602 22849303695488 run.py:483] Algo bellman_ford step 4663 current loss 0.056926, current_train_items 149248.
I0304 19:30:25.361022 22849303695488 run.py:483] Algo bellman_ford step 4664 current loss 0.081846, current_train_items 149280.
I0304 19:30:25.381248 22849303695488 run.py:483] Algo bellman_ford step 4665 current loss 0.009780, current_train_items 149312.
I0304 19:30:25.397620 22849303695488 run.py:483] Algo bellman_ford step 4666 current loss 0.007565, current_train_items 149344.
I0304 19:30:25.421438 22849303695488 run.py:483] Algo bellman_ford step 4667 current loss 0.062136, current_train_items 149376.
I0304 19:30:25.454246 22849303695488 run.py:483] Algo bellman_ford step 4668 current loss 0.049068, current_train_items 149408.
I0304 19:30:25.489260 22849303695488 run.py:483] Algo bellman_ford step 4669 current loss 0.079490, current_train_items 149440.
I0304 19:30:25.509502 22849303695488 run.py:483] Algo bellman_ford step 4670 current loss 0.004400, current_train_items 149472.
I0304 19:30:25.525785 22849303695488 run.py:483] Algo bellman_ford step 4671 current loss 0.018087, current_train_items 149504.
I0304 19:30:25.549300 22849303695488 run.py:483] Algo bellman_ford step 4672 current loss 0.044018, current_train_items 149536.
I0304 19:30:25.579855 22849303695488 run.py:483] Algo bellman_ford step 4673 current loss 0.077992, current_train_items 149568.
I0304 19:30:25.613932 22849303695488 run.py:483] Algo bellman_ford step 4674 current loss 0.052284, current_train_items 149600.
I0304 19:30:25.634638 22849303695488 run.py:483] Algo bellman_ford step 4675 current loss 0.018862, current_train_items 149632.
I0304 19:30:25.651728 22849303695488 run.py:483] Algo bellman_ford step 4676 current loss 0.039285, current_train_items 149664.
I0304 19:30:25.676232 22849303695488 run.py:483] Algo bellman_ford step 4677 current loss 0.095007, current_train_items 149696.
I0304 19:30:25.709416 22849303695488 run.py:483] Algo bellman_ford step 4678 current loss 0.103791, current_train_items 149728.
I0304 19:30:25.743777 22849303695488 run.py:483] Algo bellman_ford step 4679 current loss 0.079725, current_train_items 149760.
I0304 19:30:25.763685 22849303695488 run.py:483] Algo bellman_ford step 4680 current loss 0.004639, current_train_items 149792.
I0304 19:30:25.780010 22849303695488 run.py:483] Algo bellman_ford step 4681 current loss 0.017250, current_train_items 149824.
I0304 19:30:25.804023 22849303695488 run.py:483] Algo bellman_ford step 4682 current loss 0.066201, current_train_items 149856.
I0304 19:30:25.835423 22849303695488 run.py:483] Algo bellman_ford step 4683 current loss 0.096518, current_train_items 149888.
I0304 19:30:25.871168 22849303695488 run.py:483] Algo bellman_ford step 4684 current loss 0.080342, current_train_items 149920.
I0304 19:30:25.891772 22849303695488 run.py:483] Algo bellman_ford step 4685 current loss 0.018381, current_train_items 149952.
I0304 19:30:25.908399 22849303695488 run.py:483] Algo bellman_ford step 4686 current loss 0.014095, current_train_items 149984.
I0304 19:30:25.932367 22849303695488 run.py:483] Algo bellman_ford step 4687 current loss 0.111680, current_train_items 150016.
I0304 19:30:25.964573 22849303695488 run.py:483] Algo bellman_ford step 4688 current loss 0.147059, current_train_items 150048.
I0304 19:30:26.000417 22849303695488 run.py:483] Algo bellman_ford step 4689 current loss 0.190731, current_train_items 150080.
I0304 19:30:26.021045 22849303695488 run.py:483] Algo bellman_ford step 4690 current loss 0.007176, current_train_items 150112.
I0304 19:30:26.037843 22849303695488 run.py:483] Algo bellman_ford step 4691 current loss 0.015636, current_train_items 150144.
I0304 19:30:26.061664 22849303695488 run.py:483] Algo bellman_ford step 4692 current loss 0.051628, current_train_items 150176.
I0304 19:30:26.093720 22849303695488 run.py:483] Algo bellman_ford step 4693 current loss 0.184542, current_train_items 150208.
I0304 19:30:26.127365 22849303695488 run.py:483] Algo bellman_ford step 4694 current loss 0.176320, current_train_items 150240.
I0304 19:30:26.147356 22849303695488 run.py:483] Algo bellman_ford step 4695 current loss 0.003085, current_train_items 150272.
I0304 19:30:26.163962 22849303695488 run.py:483] Algo bellman_ford step 4696 current loss 0.013314, current_train_items 150304.
I0304 19:30:26.188836 22849303695488 run.py:483] Algo bellman_ford step 4697 current loss 0.059414, current_train_items 150336.
I0304 19:30:26.220475 22849303695488 run.py:483] Algo bellman_ford step 4698 current loss 0.056877, current_train_items 150368.
I0304 19:30:26.253103 22849303695488 run.py:483] Algo bellman_ford step 4699 current loss 0.077555, current_train_items 150400.
I0304 19:30:26.273444 22849303695488 run.py:483] Algo bellman_ford step 4700 current loss 0.004076, current_train_items 150432.
I0304 19:30:26.281347 22849303695488 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0304 19:30:26.281456 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:30:26.298208 22849303695488 run.py:483] Algo bellman_ford step 4701 current loss 0.019706, current_train_items 150464.
I0304 19:30:26.323894 22849303695488 run.py:483] Algo bellman_ford step 4702 current loss 0.097415, current_train_items 150496.
I0304 19:30:26.357319 22849303695488 run.py:483] Algo bellman_ford step 4703 current loss 0.070449, current_train_items 150528.
I0304 19:30:26.393525 22849303695488 run.py:483] Algo bellman_ford step 4704 current loss 0.062817, current_train_items 150560.
I0304 19:30:26.413967 22849303695488 run.py:483] Algo bellman_ford step 4705 current loss 0.009990, current_train_items 150592.
I0304 19:30:26.430175 22849303695488 run.py:483] Algo bellman_ford step 4706 current loss 0.010618, current_train_items 150624.
I0304 19:30:26.455158 22849303695488 run.py:483] Algo bellman_ford step 4707 current loss 0.051629, current_train_items 150656.
I0304 19:30:26.488394 22849303695488 run.py:483] Algo bellman_ford step 4708 current loss 0.106837, current_train_items 150688.
I0304 19:30:26.522425 22849303695488 run.py:483] Algo bellman_ford step 4709 current loss 0.055642, current_train_items 150720.
I0304 19:30:26.542729 22849303695488 run.py:483] Algo bellman_ford step 4710 current loss 0.011042, current_train_items 150752.
I0304 19:30:26.559370 22849303695488 run.py:483] Algo bellman_ford step 4711 current loss 0.018151, current_train_items 150784.
I0304 19:30:26.584677 22849303695488 run.py:483] Algo bellman_ford step 4712 current loss 0.061814, current_train_items 150816.
I0304 19:30:26.616864 22849303695488 run.py:483] Algo bellman_ford step 4713 current loss 0.096194, current_train_items 150848.
I0304 19:30:26.649913 22849303695488 run.py:483] Algo bellman_ford step 4714 current loss 0.096551, current_train_items 150880.
I0304 19:30:26.670018 22849303695488 run.py:483] Algo bellman_ford step 4715 current loss 0.002586, current_train_items 150912.
I0304 19:30:26.686794 22849303695488 run.py:483] Algo bellman_ford step 4716 current loss 0.044557, current_train_items 150944.
I0304 19:30:26.710032 22849303695488 run.py:483] Algo bellman_ford step 4717 current loss 0.018692, current_train_items 150976.
I0304 19:30:26.741760 22849303695488 run.py:483] Algo bellman_ford step 4718 current loss 0.058570, current_train_items 151008.
I0304 19:30:26.777928 22849303695488 run.py:483] Algo bellman_ford step 4719 current loss 0.099244, current_train_items 151040.
I0304 19:30:26.798228 22849303695488 run.py:483] Algo bellman_ford step 4720 current loss 0.004419, current_train_items 151072.
I0304 19:30:26.814764 22849303695488 run.py:483] Algo bellman_ford step 4721 current loss 0.016189, current_train_items 151104.
I0304 19:30:26.839970 22849303695488 run.py:483] Algo bellman_ford step 4722 current loss 0.051500, current_train_items 151136.
I0304 19:30:26.870608 22849303695488 run.py:483] Algo bellman_ford step 4723 current loss 0.068888, current_train_items 151168.
I0304 19:30:26.905766 22849303695488 run.py:483] Algo bellman_ford step 4724 current loss 0.076849, current_train_items 151200.
I0304 19:30:26.925680 22849303695488 run.py:483] Algo bellman_ford step 4725 current loss 0.002415, current_train_items 151232.
I0304 19:30:26.942625 22849303695488 run.py:483] Algo bellman_ford step 4726 current loss 0.040097, current_train_items 151264.
I0304 19:30:26.968254 22849303695488 run.py:483] Algo bellman_ford step 4727 current loss 0.060950, current_train_items 151296.
I0304 19:30:26.998606 22849303695488 run.py:483] Algo bellman_ford step 4728 current loss 0.048328, current_train_items 151328.
I0304 19:30:27.029416 22849303695488 run.py:483] Algo bellman_ford step 4729 current loss 0.065812, current_train_items 151360.
I0304 19:30:27.049717 22849303695488 run.py:483] Algo bellman_ford step 4730 current loss 0.006910, current_train_items 151392.
I0304 19:30:27.066164 22849303695488 run.py:483] Algo bellman_ford step 4731 current loss 0.017041, current_train_items 151424.
I0304 19:30:27.090086 22849303695488 run.py:483] Algo bellman_ford step 4732 current loss 0.082328, current_train_items 151456.
I0304 19:30:27.121713 22849303695488 run.py:483] Algo bellman_ford step 4733 current loss 0.099702, current_train_items 151488.
I0304 19:30:27.156888 22849303695488 run.py:483] Algo bellman_ford step 4734 current loss 0.136254, current_train_items 151520.
I0304 19:30:27.176985 22849303695488 run.py:483] Algo bellman_ford step 4735 current loss 0.002340, current_train_items 151552.
I0304 19:30:27.193918 22849303695488 run.py:483] Algo bellman_ford step 4736 current loss 0.046985, current_train_items 151584.
I0304 19:30:27.217941 22849303695488 run.py:483] Algo bellman_ford step 4737 current loss 0.068193, current_train_items 151616.
I0304 19:30:27.249168 22849303695488 run.py:483] Algo bellman_ford step 4738 current loss 0.106068, current_train_items 151648.
I0304 19:30:27.282913 22849303695488 run.py:483] Algo bellman_ford step 4739 current loss 0.096000, current_train_items 151680.
I0304 19:30:27.303373 22849303695488 run.py:483] Algo bellman_ford step 4740 current loss 0.050192, current_train_items 151712.
I0304 19:30:27.319641 22849303695488 run.py:483] Algo bellman_ford step 4741 current loss 0.047563, current_train_items 151744.
I0304 19:30:27.343646 22849303695488 run.py:483] Algo bellman_ford step 4742 current loss 0.095907, current_train_items 151776.
I0304 19:30:27.375888 22849303695488 run.py:483] Algo bellman_ford step 4743 current loss 0.111028, current_train_items 151808.
I0304 19:30:27.408653 22849303695488 run.py:483] Algo bellman_ford step 4744 current loss 0.080403, current_train_items 151840.
I0304 19:30:27.428704 22849303695488 run.py:483] Algo bellman_ford step 4745 current loss 0.025400, current_train_items 151872.
I0304 19:30:27.445748 22849303695488 run.py:483] Algo bellman_ford step 4746 current loss 0.039497, current_train_items 151904.
I0304 19:30:27.471692 22849303695488 run.py:483] Algo bellman_ford step 4747 current loss 0.056273, current_train_items 151936.
I0304 19:30:27.503586 22849303695488 run.py:483] Algo bellman_ford step 4748 current loss 0.068378, current_train_items 151968.
I0304 19:30:27.537819 22849303695488 run.py:483] Algo bellman_ford step 4749 current loss 0.105408, current_train_items 152000.
I0304 19:30:27.557949 22849303695488 run.py:483] Algo bellman_ford step 4750 current loss 0.036374, current_train_items 152032.
I0304 19:30:27.566266 22849303695488 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0304 19:30:27.566390 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:30:27.583918 22849303695488 run.py:483] Algo bellman_ford step 4751 current loss 0.026820, current_train_items 152064.
I0304 19:30:27.609086 22849303695488 run.py:483] Algo bellman_ford step 4752 current loss 0.052190, current_train_items 152096.
I0304 19:30:27.643283 22849303695488 run.py:483] Algo bellman_ford step 4753 current loss 0.073691, current_train_items 152128.
I0304 19:30:27.677046 22849303695488 run.py:483] Algo bellman_ford step 4754 current loss 0.076209, current_train_items 152160.
I0304 19:30:27.697353 22849303695488 run.py:483] Algo bellman_ford step 4755 current loss 0.004643, current_train_items 152192.
I0304 19:30:27.713716 22849303695488 run.py:483] Algo bellman_ford step 4756 current loss 0.012150, current_train_items 152224.
I0304 19:30:27.738976 22849303695488 run.py:483] Algo bellman_ford step 4757 current loss 0.094330, current_train_items 152256.
I0304 19:30:27.770929 22849303695488 run.py:483] Algo bellman_ford step 4758 current loss 0.090342, current_train_items 152288.
I0304 19:30:27.805920 22849303695488 run.py:483] Algo bellman_ford step 4759 current loss 0.115772, current_train_items 152320.
I0304 19:30:27.826215 22849303695488 run.py:483] Algo bellman_ford step 4760 current loss 0.006056, current_train_items 152352.
I0304 19:30:27.842998 22849303695488 run.py:483] Algo bellman_ford step 4761 current loss 0.020531, current_train_items 152384.
I0304 19:30:27.867203 22849303695488 run.py:483] Algo bellman_ford step 4762 current loss 0.086797, current_train_items 152416.
I0304 19:30:27.898205 22849303695488 run.py:483] Algo bellman_ford step 4763 current loss 0.077627, current_train_items 152448.
I0304 19:30:27.932914 22849303695488 run.py:483] Algo bellman_ford step 4764 current loss 0.184579, current_train_items 152480.
I0304 19:30:27.952661 22849303695488 run.py:483] Algo bellman_ford step 4765 current loss 0.011264, current_train_items 152512.
I0304 19:30:27.968714 22849303695488 run.py:483] Algo bellman_ford step 4766 current loss 0.031360, current_train_items 152544.
I0304 19:30:27.993841 22849303695488 run.py:483] Algo bellman_ford step 4767 current loss 0.053949, current_train_items 152576.
I0304 19:30:28.024325 22849303695488 run.py:483] Algo bellman_ford step 4768 current loss 0.054039, current_train_items 152608.
I0304 19:30:28.057474 22849303695488 run.py:483] Algo bellman_ford step 4769 current loss 0.090650, current_train_items 152640.
I0304 19:30:28.077773 22849303695488 run.py:483] Algo bellman_ford step 4770 current loss 0.006268, current_train_items 152672.
I0304 19:30:28.094311 22849303695488 run.py:483] Algo bellman_ford step 4771 current loss 0.017413, current_train_items 152704.
I0304 19:30:28.117321 22849303695488 run.py:483] Algo bellman_ford step 4772 current loss 0.044770, current_train_items 152736.
I0304 19:30:28.149080 22849303695488 run.py:483] Algo bellman_ford step 4773 current loss 0.100880, current_train_items 152768.
I0304 19:30:28.183846 22849303695488 run.py:483] Algo bellman_ford step 4774 current loss 0.078529, current_train_items 152800.
I0304 19:30:28.204106 22849303695488 run.py:483] Algo bellman_ford step 4775 current loss 0.003952, current_train_items 152832.
I0304 19:30:28.220725 22849303695488 run.py:483] Algo bellman_ford step 4776 current loss 0.017383, current_train_items 152864.
I0304 19:30:28.244594 22849303695488 run.py:483] Algo bellman_ford step 4777 current loss 0.129282, current_train_items 152896.
I0304 19:30:28.276503 22849303695488 run.py:483] Algo bellman_ford step 4778 current loss 0.128952, current_train_items 152928.
I0304 19:30:28.308952 22849303695488 run.py:483] Algo bellman_ford step 4779 current loss 0.148752, current_train_items 152960.
I0304 19:30:28.329090 22849303695488 run.py:483] Algo bellman_ford step 4780 current loss 0.010029, current_train_items 152992.
I0304 19:30:28.345685 22849303695488 run.py:483] Algo bellman_ford step 4781 current loss 0.026433, current_train_items 153024.
I0304 19:30:28.370250 22849303695488 run.py:483] Algo bellman_ford step 4782 current loss 0.081896, current_train_items 153056.
I0304 19:30:28.402401 22849303695488 run.py:483] Algo bellman_ford step 4783 current loss 0.161258, current_train_items 153088.
I0304 19:30:28.437279 22849303695488 run.py:483] Algo bellman_ford step 4784 current loss 0.115578, current_train_items 153120.
I0304 19:30:28.457621 22849303695488 run.py:483] Algo bellman_ford step 4785 current loss 0.029194, current_train_items 153152.
I0304 19:30:28.474643 22849303695488 run.py:483] Algo bellman_ford step 4786 current loss 0.042077, current_train_items 153184.
I0304 19:30:28.499329 22849303695488 run.py:483] Algo bellman_ford step 4787 current loss 0.061712, current_train_items 153216.
I0304 19:30:28.530416 22849303695488 run.py:483] Algo bellman_ford step 4788 current loss 0.066480, current_train_items 153248.
I0304 19:30:28.563373 22849303695488 run.py:483] Algo bellman_ford step 4789 current loss 0.082362, current_train_items 153280.
I0304 19:30:28.583650 22849303695488 run.py:483] Algo bellman_ford step 4790 current loss 0.002028, current_train_items 153312.
I0304 19:30:28.600193 22849303695488 run.py:483] Algo bellman_ford step 4791 current loss 0.022802, current_train_items 153344.
I0304 19:30:28.624253 22849303695488 run.py:483] Algo bellman_ford step 4792 current loss 0.117095, current_train_items 153376.
I0304 19:30:28.655682 22849303695488 run.py:483] Algo bellman_ford step 4793 current loss 0.161849, current_train_items 153408.
I0304 19:30:28.692124 22849303695488 run.py:483] Algo bellman_ford step 4794 current loss 0.091505, current_train_items 153440.
I0304 19:30:28.711918 22849303695488 run.py:483] Algo bellman_ford step 4795 current loss 0.007758, current_train_items 153472.
I0304 19:30:28.728607 22849303695488 run.py:483] Algo bellman_ford step 4796 current loss 0.019685, current_train_items 153504.
I0304 19:30:28.753248 22849303695488 run.py:483] Algo bellman_ford step 4797 current loss 0.064894, current_train_items 153536.
I0304 19:30:28.784531 22849303695488 run.py:483] Algo bellman_ford step 4798 current loss 0.132705, current_train_items 153568.
I0304 19:30:28.818072 22849303695488 run.py:483] Algo bellman_ford step 4799 current loss 0.138222, current_train_items 153600.
I0304 19:30:28.838703 22849303695488 run.py:483] Algo bellman_ford step 4800 current loss 0.007838, current_train_items 153632.
I0304 19:30:28.846685 22849303695488 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0304 19:30:28.846795 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:30:28.864069 22849303695488 run.py:483] Algo bellman_ford step 4801 current loss 0.060697, current_train_items 153664.
I0304 19:30:28.889674 22849303695488 run.py:483] Algo bellman_ford step 4802 current loss 0.090266, current_train_items 153696.
I0304 19:30:28.921641 22849303695488 run.py:483] Algo bellman_ford step 4803 current loss 0.057712, current_train_items 153728.
I0304 19:30:28.958775 22849303695488 run.py:483] Algo bellman_ford step 4804 current loss 0.088712, current_train_items 153760.
I0304 19:30:28.979439 22849303695488 run.py:483] Algo bellman_ford step 4805 current loss 0.023271, current_train_items 153792.
I0304 19:30:28.996154 22849303695488 run.py:483] Algo bellman_ford step 4806 current loss 0.024357, current_train_items 153824.
I0304 19:30:29.021085 22849303695488 run.py:483] Algo bellman_ford step 4807 current loss 0.054736, current_train_items 153856.
I0304 19:30:29.053631 22849303695488 run.py:483] Algo bellman_ford step 4808 current loss 0.091653, current_train_items 153888.
I0304 19:30:29.088188 22849303695488 run.py:483] Algo bellman_ford step 4809 current loss 0.077453, current_train_items 153920.
I0304 19:30:29.108438 22849303695488 run.py:483] Algo bellman_ford step 4810 current loss 0.004667, current_train_items 153952.
I0304 19:30:29.125166 22849303695488 run.py:483] Algo bellman_ford step 4811 current loss 0.018707, current_train_items 153984.
I0304 19:30:29.149438 22849303695488 run.py:483] Algo bellman_ford step 4812 current loss 0.041827, current_train_items 154016.
I0304 19:30:29.181098 22849303695488 run.py:483] Algo bellman_ford step 4813 current loss 0.054326, current_train_items 154048.
I0304 19:30:29.216875 22849303695488 run.py:483] Algo bellman_ford step 4814 current loss 0.094927, current_train_items 154080.
I0304 19:30:29.237201 22849303695488 run.py:483] Algo bellman_ford step 4815 current loss 0.024223, current_train_items 154112.
I0304 19:30:29.253801 22849303695488 run.py:483] Algo bellman_ford step 4816 current loss 0.043275, current_train_items 154144.
I0304 19:30:29.279478 22849303695488 run.py:483] Algo bellman_ford step 4817 current loss 0.086297, current_train_items 154176.
I0304 19:30:29.311610 22849303695488 run.py:483] Algo bellman_ford step 4818 current loss 0.043407, current_train_items 154208.
I0304 19:30:29.342184 22849303695488 run.py:483] Algo bellman_ford step 4819 current loss 0.047019, current_train_items 154240.
I0304 19:30:29.362535 22849303695488 run.py:483] Algo bellman_ford step 4820 current loss 0.004254, current_train_items 154272.
I0304 19:30:29.378983 22849303695488 run.py:483] Algo bellman_ford step 4821 current loss 0.030345, current_train_items 154304.
I0304 19:30:29.403865 22849303695488 run.py:483] Algo bellman_ford step 4822 current loss 0.057571, current_train_items 154336.
I0304 19:30:29.435626 22849303695488 run.py:483] Algo bellman_ford step 4823 current loss 0.066529, current_train_items 154368.
I0304 19:30:29.470940 22849303695488 run.py:483] Algo bellman_ford step 4824 current loss 0.118061, current_train_items 154400.
I0304 19:30:29.491568 22849303695488 run.py:483] Algo bellman_ford step 4825 current loss 0.010078, current_train_items 154432.
I0304 19:30:29.508784 22849303695488 run.py:483] Algo bellman_ford step 4826 current loss 0.025071, current_train_items 154464.
I0304 19:30:29.532889 22849303695488 run.py:483] Algo bellman_ford step 4827 current loss 0.042973, current_train_items 154496.
I0304 19:30:29.565278 22849303695488 run.py:483] Algo bellman_ford step 4828 current loss 0.074418, current_train_items 154528.
I0304 19:30:29.600280 22849303695488 run.py:483] Algo bellman_ford step 4829 current loss 0.092695, current_train_items 154560.
I0304 19:30:29.620699 22849303695488 run.py:483] Algo bellman_ford step 4830 current loss 0.004735, current_train_items 154592.
I0304 19:30:29.637486 22849303695488 run.py:483] Algo bellman_ford step 4831 current loss 0.040136, current_train_items 154624.
I0304 19:30:29.662519 22849303695488 run.py:483] Algo bellman_ford step 4832 current loss 0.030211, current_train_items 154656.
I0304 19:30:29.694923 22849303695488 run.py:483] Algo bellman_ford step 4833 current loss 0.075145, current_train_items 154688.
I0304 19:30:29.728992 22849303695488 run.py:483] Algo bellman_ford step 4834 current loss 0.066662, current_train_items 154720.
I0304 19:30:29.748917 22849303695488 run.py:483] Algo bellman_ford step 4835 current loss 0.002128, current_train_items 154752.
I0304 19:30:29.765573 22849303695488 run.py:483] Algo bellman_ford step 4836 current loss 0.021319, current_train_items 154784.
I0304 19:30:29.788997 22849303695488 run.py:483] Algo bellman_ford step 4837 current loss 0.070196, current_train_items 154816.
I0304 19:30:29.820699 22849303695488 run.py:483] Algo bellman_ford step 4838 current loss 0.134080, current_train_items 154848.
I0304 19:30:29.852810 22849303695488 run.py:483] Algo bellman_ford step 4839 current loss 0.087544, current_train_items 154880.
I0304 19:30:29.873179 22849303695488 run.py:483] Algo bellman_ford step 4840 current loss 0.005348, current_train_items 154912.
I0304 19:30:29.889776 22849303695488 run.py:483] Algo bellman_ford step 4841 current loss 0.026845, current_train_items 154944.
I0304 19:30:29.914381 22849303695488 run.py:483] Algo bellman_ford step 4842 current loss 0.048650, current_train_items 154976.
I0304 19:30:29.945485 22849303695488 run.py:483] Algo bellman_ford step 4843 current loss 0.098365, current_train_items 155008.
I0304 19:30:29.981086 22849303695488 run.py:483] Algo bellman_ford step 4844 current loss 0.099970, current_train_items 155040.
I0304 19:30:30.001543 22849303695488 run.py:483] Algo bellman_ford step 4845 current loss 0.003515, current_train_items 155072.
I0304 19:30:30.018434 22849303695488 run.py:483] Algo bellman_ford step 4846 current loss 0.017038, current_train_items 155104.
I0304 19:30:30.042015 22849303695488 run.py:483] Algo bellman_ford step 4847 current loss 0.049201, current_train_items 155136.
I0304 19:30:30.073350 22849303695488 run.py:483] Algo bellman_ford step 4848 current loss 0.055203, current_train_items 155168.
I0304 19:30:30.107379 22849303695488 run.py:483] Algo bellman_ford step 4849 current loss 0.086070, current_train_items 155200.
I0304 19:30:30.127805 22849303695488 run.py:483] Algo bellman_ford step 4850 current loss 0.002616, current_train_items 155232.
I0304 19:30:30.136064 22849303695488 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0304 19:30:30.136173 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:30.153659 22849303695488 run.py:483] Algo bellman_ford step 4851 current loss 0.040680, current_train_items 155264.
I0304 19:30:30.177210 22849303695488 run.py:483] Algo bellman_ford step 4852 current loss 0.073891, current_train_items 155296.
I0304 19:30:30.208632 22849303695488 run.py:483] Algo bellman_ford step 4853 current loss 0.053937, current_train_items 155328.
I0304 19:30:30.243148 22849303695488 run.py:483] Algo bellman_ford step 4854 current loss 0.083887, current_train_items 155360.
I0304 19:30:30.263243 22849303695488 run.py:483] Algo bellman_ford step 4855 current loss 0.002438, current_train_items 155392.
I0304 19:30:30.280152 22849303695488 run.py:483] Algo bellman_ford step 4856 current loss 0.025662, current_train_items 155424.
I0304 19:30:30.305393 22849303695488 run.py:483] Algo bellman_ford step 4857 current loss 0.066055, current_train_items 155456.
I0304 19:30:30.337944 22849303695488 run.py:483] Algo bellman_ford step 4858 current loss 0.088663, current_train_items 155488.
I0304 19:30:30.374166 22849303695488 run.py:483] Algo bellman_ford step 4859 current loss 0.112908, current_train_items 155520.
I0304 19:30:30.394328 22849303695488 run.py:483] Algo bellman_ford step 4860 current loss 0.002599, current_train_items 155552.
I0304 19:30:30.411755 22849303695488 run.py:483] Algo bellman_ford step 4861 current loss 0.023750, current_train_items 155584.
I0304 19:30:30.435551 22849303695488 run.py:483] Algo bellman_ford step 4862 current loss 0.115582, current_train_items 155616.
I0304 19:30:30.467969 22849303695488 run.py:483] Algo bellman_ford step 4863 current loss 0.064926, current_train_items 155648.
I0304 19:30:30.500910 22849303695488 run.py:483] Algo bellman_ford step 4864 current loss 0.051939, current_train_items 155680.
I0304 19:30:30.520609 22849303695488 run.py:483] Algo bellman_ford step 4865 current loss 0.003172, current_train_items 155712.
I0304 19:30:30.536920 22849303695488 run.py:483] Algo bellman_ford step 4866 current loss 0.071278, current_train_items 155744.
I0304 19:30:30.561995 22849303695488 run.py:483] Algo bellman_ford step 4867 current loss 0.060247, current_train_items 155776.
I0304 19:30:30.594815 22849303695488 run.py:483] Algo bellman_ford step 4868 current loss 0.085114, current_train_items 155808.
I0304 19:30:30.627715 22849303695488 run.py:483] Algo bellman_ford step 4869 current loss 0.043598, current_train_items 155840.
I0304 19:30:30.648486 22849303695488 run.py:483] Algo bellman_ford step 4870 current loss 0.004075, current_train_items 155872.
I0304 19:30:30.665226 22849303695488 run.py:483] Algo bellman_ford step 4871 current loss 0.061787, current_train_items 155904.
I0304 19:30:30.688928 22849303695488 run.py:483] Algo bellman_ford step 4872 current loss 0.070939, current_train_items 155936.
I0304 19:30:30.718766 22849303695488 run.py:483] Algo bellman_ford step 4873 current loss 0.056358, current_train_items 155968.
I0304 19:30:30.751629 22849303695488 run.py:483] Algo bellman_ford step 4874 current loss 0.092523, current_train_items 156000.
I0304 19:30:30.772077 22849303695488 run.py:483] Algo bellman_ford step 4875 current loss 0.004018, current_train_items 156032.
I0304 19:30:30.788462 22849303695488 run.py:483] Algo bellman_ford step 4876 current loss 0.027053, current_train_items 156064.
I0304 19:30:30.811560 22849303695488 run.py:483] Algo bellman_ford step 4877 current loss 0.020679, current_train_items 156096.
I0304 19:30:30.842212 22849303695488 run.py:483] Algo bellman_ford step 4878 current loss 0.055206, current_train_items 156128.
I0304 19:30:30.876409 22849303695488 run.py:483] Algo bellman_ford step 4879 current loss 0.095404, current_train_items 156160.
I0304 19:30:30.896439 22849303695488 run.py:483] Algo bellman_ford step 4880 current loss 0.007675, current_train_items 156192.
I0304 19:30:30.912963 22849303695488 run.py:483] Algo bellman_ford step 4881 current loss 0.021083, current_train_items 156224.
I0304 19:30:30.937815 22849303695488 run.py:483] Algo bellman_ford step 4882 current loss 0.062812, current_train_items 156256.
I0304 19:30:30.969978 22849303695488 run.py:483] Algo bellman_ford step 4883 current loss 0.093307, current_train_items 156288.
I0304 19:30:31.004834 22849303695488 run.py:483] Algo bellman_ford step 4884 current loss 0.080672, current_train_items 156320.
I0304 19:30:31.024945 22849303695488 run.py:483] Algo bellman_ford step 4885 current loss 0.007375, current_train_items 156352.
I0304 19:30:31.041976 22849303695488 run.py:483] Algo bellman_ford step 4886 current loss 0.030043, current_train_items 156384.
I0304 19:30:31.065866 22849303695488 run.py:483] Algo bellman_ford step 4887 current loss 0.052769, current_train_items 156416.
I0304 19:30:31.097537 22849303695488 run.py:483] Algo bellman_ford step 4888 current loss 0.037962, current_train_items 156448.
I0304 19:30:31.130772 22849303695488 run.py:483] Algo bellman_ford step 4889 current loss 0.070398, current_train_items 156480.
I0304 19:30:31.151052 22849303695488 run.py:483] Algo bellman_ford step 4890 current loss 0.004977, current_train_items 156512.
I0304 19:30:31.167608 22849303695488 run.py:483] Algo bellman_ford step 4891 current loss 0.050688, current_train_items 156544.
I0304 19:30:31.191467 22849303695488 run.py:483] Algo bellman_ford step 4892 current loss 0.053338, current_train_items 156576.
I0304 19:30:31.221647 22849303695488 run.py:483] Algo bellman_ford step 4893 current loss 0.088415, current_train_items 156608.
I0304 19:30:31.254636 22849303695488 run.py:483] Algo bellman_ford step 4894 current loss 0.108375, current_train_items 156640.
I0304 19:30:31.274510 22849303695488 run.py:483] Algo bellman_ford step 4895 current loss 0.052008, current_train_items 156672.
I0304 19:30:31.291204 22849303695488 run.py:483] Algo bellman_ford step 4896 current loss 0.010543, current_train_items 156704.
I0304 19:30:31.315028 22849303695488 run.py:483] Algo bellman_ford step 4897 current loss 0.030992, current_train_items 156736.
I0304 19:30:31.345757 22849303695488 run.py:483] Algo bellman_ford step 4898 current loss 0.052880, current_train_items 156768.
I0304 19:30:31.380774 22849303695488 run.py:483] Algo bellman_ford step 4899 current loss 0.115332, current_train_items 156800.
I0304 19:30:31.401093 22849303695488 run.py:483] Algo bellman_ford step 4900 current loss 0.010264, current_train_items 156832.
I0304 19:30:31.409171 22849303695488 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0304 19:30:31.409311 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:31.426188 22849303695488 run.py:483] Algo bellman_ford step 4901 current loss 0.036876, current_train_items 156864.
I0304 19:30:31.451159 22849303695488 run.py:483] Algo bellman_ford step 4902 current loss 0.059986, current_train_items 156896.
I0304 19:30:31.484949 22849303695488 run.py:483] Algo bellman_ford step 4903 current loss 0.055635, current_train_items 156928.
I0304 19:30:31.519264 22849303695488 run.py:483] Algo bellman_ford step 4904 current loss 0.056348, current_train_items 156960.
I0304 19:30:31.539430 22849303695488 run.py:483] Algo bellman_ford step 4905 current loss 0.024436, current_train_items 156992.
I0304 19:30:31.555672 22849303695488 run.py:483] Algo bellman_ford step 4906 current loss 0.035534, current_train_items 157024.
I0304 19:30:31.580634 22849303695488 run.py:483] Algo bellman_ford step 4907 current loss 0.055499, current_train_items 157056.
I0304 19:30:31.612389 22849303695488 run.py:483] Algo bellman_ford step 4908 current loss 0.056219, current_train_items 157088.
I0304 19:30:31.646178 22849303695488 run.py:483] Algo bellman_ford step 4909 current loss 0.074898, current_train_items 157120.
I0304 19:30:31.666255 22849303695488 run.py:483] Algo bellman_ford step 4910 current loss 0.004753, current_train_items 157152.
I0304 19:30:31.683156 22849303695488 run.py:483] Algo bellman_ford step 4911 current loss 0.024320, current_train_items 157184.
I0304 19:30:31.707595 22849303695488 run.py:483] Algo bellman_ford step 4912 current loss 0.040760, current_train_items 157216.
I0304 19:30:31.740172 22849303695488 run.py:483] Algo bellman_ford step 4913 current loss 0.114428, current_train_items 157248.
I0304 19:30:31.773775 22849303695488 run.py:483] Algo bellman_ford step 4914 current loss 0.084235, current_train_items 157280.
I0304 19:30:31.793735 22849303695488 run.py:483] Algo bellman_ford step 4915 current loss 0.010115, current_train_items 157312.
I0304 19:30:31.810458 22849303695488 run.py:483] Algo bellman_ford step 4916 current loss 0.045875, current_train_items 157344.
I0304 19:30:31.834649 22849303695488 run.py:483] Algo bellman_ford step 4917 current loss 0.077344, current_train_items 157376.
I0304 19:30:31.865315 22849303695488 run.py:483] Algo bellman_ford step 4918 current loss 0.059464, current_train_items 157408.
I0304 19:30:31.899551 22849303695488 run.py:483] Algo bellman_ford step 4919 current loss 0.061355, current_train_items 157440.
I0304 19:30:31.919287 22849303695488 run.py:483] Algo bellman_ford step 4920 current loss 0.004017, current_train_items 157472.
I0304 19:30:31.935202 22849303695488 run.py:483] Algo bellman_ford step 4921 current loss 0.018830, current_train_items 157504.
I0304 19:30:31.959683 22849303695488 run.py:483] Algo bellman_ford step 4922 current loss 0.075691, current_train_items 157536.
I0304 19:30:31.991399 22849303695488 run.py:483] Algo bellman_ford step 4923 current loss 0.058147, current_train_items 157568.
I0304 19:30:32.025156 22849303695488 run.py:483] Algo bellman_ford step 4924 current loss 0.089202, current_train_items 157600.
I0304 19:30:32.045222 22849303695488 run.py:483] Algo bellman_ford step 4925 current loss 0.009218, current_train_items 157632.
I0304 19:30:32.061905 22849303695488 run.py:483] Algo bellman_ford step 4926 current loss 0.029820, current_train_items 157664.
I0304 19:30:32.087092 22849303695488 run.py:483] Algo bellman_ford step 4927 current loss 0.047045, current_train_items 157696.
I0304 19:30:32.117599 22849303695488 run.py:483] Algo bellman_ford step 4928 current loss 0.073604, current_train_items 157728.
I0304 19:30:32.151819 22849303695488 run.py:483] Algo bellman_ford step 4929 current loss 0.058964, current_train_items 157760.
I0304 19:30:32.171916 22849303695488 run.py:483] Algo bellman_ford step 4930 current loss 0.004129, current_train_items 157792.
I0304 19:30:32.188203 22849303695488 run.py:483] Algo bellman_ford step 4931 current loss 0.036096, current_train_items 157824.
I0304 19:30:32.212408 22849303695488 run.py:483] Algo bellman_ford step 4932 current loss 0.072494, current_train_items 157856.
I0304 19:30:32.243620 22849303695488 run.py:483] Algo bellman_ford step 4933 current loss 0.038389, current_train_items 157888.
I0304 19:30:32.277644 22849303695488 run.py:483] Algo bellman_ford step 4934 current loss 0.066932, current_train_items 157920.
I0304 19:30:32.297510 22849303695488 run.py:483] Algo bellman_ford step 4935 current loss 0.007117, current_train_items 157952.
I0304 19:30:32.314079 22849303695488 run.py:483] Algo bellman_ford step 4936 current loss 0.020198, current_train_items 157984.
I0304 19:30:32.340077 22849303695488 run.py:483] Algo bellman_ford step 4937 current loss 0.039372, current_train_items 158016.
I0304 19:30:32.371347 22849303695488 run.py:483] Algo bellman_ford step 4938 current loss 0.049723, current_train_items 158048.
I0304 19:30:32.407422 22849303695488 run.py:483] Algo bellman_ford step 4939 current loss 0.075671, current_train_items 158080.
I0304 19:30:32.427304 22849303695488 run.py:483] Algo bellman_ford step 4940 current loss 0.003600, current_train_items 158112.
I0304 19:30:32.443530 22849303695488 run.py:483] Algo bellman_ford step 4941 current loss 0.014766, current_train_items 158144.
I0304 19:30:32.467800 22849303695488 run.py:483] Algo bellman_ford step 4942 current loss 0.054093, current_train_items 158176.
I0304 19:30:32.500441 22849303695488 run.py:483] Algo bellman_ford step 4943 current loss 0.067775, current_train_items 158208.
I0304 19:30:32.532567 22849303695488 run.py:483] Algo bellman_ford step 4944 current loss 0.051010, current_train_items 158240.
I0304 19:30:32.552492 22849303695488 run.py:483] Algo bellman_ford step 4945 current loss 0.003732, current_train_items 158272.
I0304 19:30:32.569053 22849303695488 run.py:483] Algo bellman_ford step 4946 current loss 0.041178, current_train_items 158304.
I0304 19:30:32.592918 22849303695488 run.py:483] Algo bellman_ford step 4947 current loss 0.073652, current_train_items 158336.
I0304 19:30:32.624654 22849303695488 run.py:483] Algo bellman_ford step 4948 current loss 0.086729, current_train_items 158368.
I0304 19:30:32.660248 22849303695488 run.py:483] Algo bellman_ford step 4949 current loss 0.101035, current_train_items 158400.
I0304 19:30:32.680264 22849303695488 run.py:483] Algo bellman_ford step 4950 current loss 0.005904, current_train_items 158432.
I0304 19:30:32.688531 22849303695488 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0304 19:30:32.688657 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.988, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:30:32.719200 22849303695488 run.py:483] Algo bellman_ford step 4951 current loss 0.010247, current_train_items 158464.
I0304 19:30:32.743446 22849303695488 run.py:483] Algo bellman_ford step 4952 current loss 0.047538, current_train_items 158496.
I0304 19:30:32.775761 22849303695488 run.py:483] Algo bellman_ford step 4953 current loss 0.097716, current_train_items 158528.
I0304 19:30:32.812730 22849303695488 run.py:483] Algo bellman_ford step 4954 current loss 0.067505, current_train_items 158560.
I0304 19:30:32.833577 22849303695488 run.py:483] Algo bellman_ford step 4955 current loss 0.006633, current_train_items 158592.
I0304 19:30:32.850187 22849303695488 run.py:483] Algo bellman_ford step 4956 current loss 0.015912, current_train_items 158624.
I0304 19:30:32.876085 22849303695488 run.py:483] Algo bellman_ford step 4957 current loss 0.088174, current_train_items 158656.
I0304 19:30:32.908533 22849303695488 run.py:483] Algo bellman_ford step 4958 current loss 0.099000, current_train_items 158688.
I0304 19:30:32.940533 22849303695488 run.py:483] Algo bellman_ford step 4959 current loss 0.055930, current_train_items 158720.
I0304 19:30:32.960844 22849303695488 run.py:483] Algo bellman_ford step 4960 current loss 0.047258, current_train_items 158752.
I0304 19:30:32.977743 22849303695488 run.py:483] Algo bellman_ford step 4961 current loss 0.013023, current_train_items 158784.
I0304 19:30:33.002106 22849303695488 run.py:483] Algo bellman_ford step 4962 current loss 0.049976, current_train_items 158816.
I0304 19:30:33.032803 22849303695488 run.py:483] Algo bellman_ford step 4963 current loss 0.048710, current_train_items 158848.
I0304 19:30:33.066833 22849303695488 run.py:483] Algo bellman_ford step 4964 current loss 0.083432, current_train_items 158880.
I0304 19:30:33.086801 22849303695488 run.py:483] Algo bellman_ford step 4965 current loss 0.002293, current_train_items 158912.
I0304 19:30:33.103462 22849303695488 run.py:483] Algo bellman_ford step 4966 current loss 0.038213, current_train_items 158944.
I0304 19:30:33.128613 22849303695488 run.py:483] Algo bellman_ford step 4967 current loss 0.049715, current_train_items 158976.
I0304 19:30:33.161471 22849303695488 run.py:483] Algo bellman_ford step 4968 current loss 0.088321, current_train_items 159008.
I0304 19:30:33.194261 22849303695488 run.py:483] Algo bellman_ford step 4969 current loss 0.096148, current_train_items 159040.
I0304 19:30:33.215111 22849303695488 run.py:483] Algo bellman_ford step 4970 current loss 0.003582, current_train_items 159072.
I0304 19:30:33.231707 22849303695488 run.py:483] Algo bellman_ford step 4971 current loss 0.070879, current_train_items 159104.
I0304 19:30:33.256601 22849303695488 run.py:483] Algo bellman_ford step 4972 current loss 0.067699, current_train_items 159136.
I0304 19:30:33.289186 22849303695488 run.py:483] Algo bellman_ford step 4973 current loss 0.075595, current_train_items 159168.
I0304 19:30:33.323208 22849303695488 run.py:483] Algo bellman_ford step 4974 current loss 0.066007, current_train_items 159200.
I0304 19:30:33.343619 22849303695488 run.py:483] Algo bellman_ford step 4975 current loss 0.006072, current_train_items 159232.
I0304 19:30:33.360716 22849303695488 run.py:483] Algo bellman_ford step 4976 current loss 0.022761, current_train_items 159264.
I0304 19:30:33.385594 22849303695488 run.py:483] Algo bellman_ford step 4977 current loss 0.131876, current_train_items 159296.
I0304 19:30:33.418375 22849303695488 run.py:483] Algo bellman_ford step 4978 current loss 0.146479, current_train_items 159328.
I0304 19:30:33.453515 22849303695488 run.py:483] Algo bellman_ford step 4979 current loss 0.083551, current_train_items 159360.
I0304 19:30:33.473561 22849303695488 run.py:483] Algo bellman_ford step 4980 current loss 0.015600, current_train_items 159392.
I0304 19:30:33.490357 22849303695488 run.py:483] Algo bellman_ford step 4981 current loss 0.027929, current_train_items 159424.
I0304 19:30:33.516228 22849303695488 run.py:483] Algo bellman_ford step 4982 current loss 0.076510, current_train_items 159456.
I0304 19:30:33.546252 22849303695488 run.py:483] Algo bellman_ford step 4983 current loss 0.072587, current_train_items 159488.
I0304 19:30:33.580378 22849303695488 run.py:483] Algo bellman_ford step 4984 current loss 0.079002, current_train_items 159520.
I0304 19:30:33.600987 22849303695488 run.py:483] Algo bellman_ford step 4985 current loss 0.005788, current_train_items 159552.
I0304 19:30:33.617794 22849303695488 run.py:483] Algo bellman_ford step 4986 current loss 0.055293, current_train_items 159584.
I0304 19:30:33.640518 22849303695488 run.py:483] Algo bellman_ford step 4987 current loss 0.083070, current_train_items 159616.
I0304 19:30:33.671268 22849303695488 run.py:483] Algo bellman_ford step 4988 current loss 0.072836, current_train_items 159648.
I0304 19:30:33.708288 22849303695488 run.py:483] Algo bellman_ford step 4989 current loss 0.118201, current_train_items 159680.
I0304 19:30:33.728780 22849303695488 run.py:483] Algo bellman_ford step 4990 current loss 0.030476, current_train_items 159712.
I0304 19:30:33.745202 22849303695488 run.py:483] Algo bellman_ford step 4991 current loss 0.023580, current_train_items 159744.
I0304 19:30:33.769856 22849303695488 run.py:483] Algo bellman_ford step 4992 current loss 0.071739, current_train_items 159776.
I0304 19:30:33.801360 22849303695488 run.py:483] Algo bellman_ford step 4993 current loss 0.074680, current_train_items 159808.
I0304 19:30:33.835905 22849303695488 run.py:483] Algo bellman_ford step 4994 current loss 0.100960, current_train_items 159840.
I0304 19:30:33.855926 22849303695488 run.py:483] Algo bellman_ford step 4995 current loss 0.006177, current_train_items 159872.
I0304 19:30:33.872766 22849303695488 run.py:483] Algo bellman_ford step 4996 current loss 0.030972, current_train_items 159904.
I0304 19:30:33.897169 22849303695488 run.py:483] Algo bellman_ford step 4997 current loss 0.039531, current_train_items 159936.
I0304 19:30:33.929035 22849303695488 run.py:483] Algo bellman_ford step 4998 current loss 0.091672, current_train_items 159968.
I0304 19:30:33.964264 22849303695488 run.py:483] Algo bellman_ford step 4999 current loss 0.102310, current_train_items 160000.
I0304 19:30:33.984795 22849303695488 run.py:483] Algo bellman_ford step 5000 current loss 0.006286, current_train_items 160032.
I0304 19:30:33.992711 22849303695488 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0304 19:30:33.992821 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:30:34.010509 22849303695488 run.py:483] Algo bellman_ford step 5001 current loss 0.070411, current_train_items 160064.
I0304 19:30:34.034907 22849303695488 run.py:483] Algo bellman_ford step 5002 current loss 0.091371, current_train_items 160096.
I0304 19:30:34.065525 22849303695488 run.py:483] Algo bellman_ford step 5003 current loss 0.038156, current_train_items 160128.
I0304 19:30:34.099175 22849303695488 run.py:483] Algo bellman_ford step 5004 current loss 0.091417, current_train_items 160160.
I0304 19:30:34.119923 22849303695488 run.py:483] Algo bellman_ford step 5005 current loss 0.011355, current_train_items 160192.
I0304 19:30:34.136336 22849303695488 run.py:483] Algo bellman_ford step 5006 current loss 0.030874, current_train_items 160224.
I0304 19:30:34.161110 22849303695488 run.py:483] Algo bellman_ford step 5007 current loss 0.069202, current_train_items 160256.
I0304 19:30:34.194719 22849303695488 run.py:483] Algo bellman_ford step 5008 current loss 0.110017, current_train_items 160288.
I0304 19:30:34.227061 22849303695488 run.py:483] Algo bellman_ford step 5009 current loss 0.043007, current_train_items 160320.
I0304 19:30:34.246982 22849303695488 run.py:483] Algo bellman_ford step 5010 current loss 0.006564, current_train_items 160352.
I0304 19:30:34.263323 22849303695488 run.py:483] Algo bellman_ford step 5011 current loss 0.012018, current_train_items 160384.
I0304 19:30:34.287883 22849303695488 run.py:483] Algo bellman_ford step 5012 current loss 0.043739, current_train_items 160416.
I0304 19:30:34.319000 22849303695488 run.py:483] Algo bellman_ford step 5013 current loss 0.155275, current_train_items 160448.
I0304 19:30:34.354434 22849303695488 run.py:483] Algo bellman_ford step 5014 current loss 0.085846, current_train_items 160480.
I0304 19:30:34.374732 22849303695488 run.py:483] Algo bellman_ford step 5015 current loss 0.015768, current_train_items 160512.
I0304 19:30:34.391860 22849303695488 run.py:483] Algo bellman_ford step 5016 current loss 0.033366, current_train_items 160544.
I0304 19:30:34.415618 22849303695488 run.py:483] Algo bellman_ford step 5017 current loss 0.039570, current_train_items 160576.
I0304 19:30:34.448085 22849303695488 run.py:483] Algo bellman_ford step 5018 current loss 0.119650, current_train_items 160608.
I0304 19:30:34.483934 22849303695488 run.py:483] Algo bellman_ford step 5019 current loss 0.105217, current_train_items 160640.
I0304 19:30:34.503880 22849303695488 run.py:483] Algo bellman_ford step 5020 current loss 0.002677, current_train_items 160672.
I0304 19:30:34.520139 22849303695488 run.py:483] Algo bellman_ford step 5021 current loss 0.011821, current_train_items 160704.
I0304 19:30:34.545334 22849303695488 run.py:483] Algo bellman_ford step 5022 current loss 0.035541, current_train_items 160736.
I0304 19:30:34.577183 22849303695488 run.py:483] Algo bellman_ford step 5023 current loss 0.078885, current_train_items 160768.
I0304 19:30:34.611537 22849303695488 run.py:483] Algo bellman_ford step 5024 current loss 0.075361, current_train_items 160800.
I0304 19:30:34.631263 22849303695488 run.py:483] Algo bellman_ford step 5025 current loss 0.005465, current_train_items 160832.
I0304 19:30:34.647454 22849303695488 run.py:483] Algo bellman_ford step 5026 current loss 0.013340, current_train_items 160864.
I0304 19:30:34.671062 22849303695488 run.py:483] Algo bellman_ford step 5027 current loss 0.078349, current_train_items 160896.
I0304 19:30:34.701182 22849303695488 run.py:483] Algo bellman_ford step 5028 current loss 0.076946, current_train_items 160928.
I0304 19:30:34.734805 22849303695488 run.py:483] Algo bellman_ford step 5029 current loss 0.106535, current_train_items 160960.
I0304 19:30:34.754935 22849303695488 run.py:483] Algo bellman_ford step 5030 current loss 0.003828, current_train_items 160992.
I0304 19:30:34.771672 22849303695488 run.py:483] Algo bellman_ford step 5031 current loss 0.036333, current_train_items 161024.
I0304 19:30:34.796243 22849303695488 run.py:483] Algo bellman_ford step 5032 current loss 0.072601, current_train_items 161056.
I0304 19:30:34.826570 22849303695488 run.py:483] Algo bellman_ford step 5033 current loss 0.051760, current_train_items 161088.
I0304 19:30:34.861977 22849303695488 run.py:483] Algo bellman_ford step 5034 current loss 0.105149, current_train_items 161120.
I0304 19:30:34.882125 22849303695488 run.py:483] Algo bellman_ford step 5035 current loss 0.066482, current_train_items 161152.
I0304 19:30:34.898813 22849303695488 run.py:483] Algo bellman_ford step 5036 current loss 0.012266, current_train_items 161184.
I0304 19:30:34.922902 22849303695488 run.py:483] Algo bellman_ford step 5037 current loss 0.082888, current_train_items 161216.
I0304 19:30:34.955796 22849303695488 run.py:483] Algo bellman_ford step 5038 current loss 0.240297, current_train_items 161248.
I0304 19:30:34.989984 22849303695488 run.py:483] Algo bellman_ford step 5039 current loss 0.140981, current_train_items 161280.
I0304 19:30:35.009809 22849303695488 run.py:483] Algo bellman_ford step 5040 current loss 0.007320, current_train_items 161312.
I0304 19:30:35.025959 22849303695488 run.py:483] Algo bellman_ford step 5041 current loss 0.015060, current_train_items 161344.
I0304 19:30:35.051187 22849303695488 run.py:483] Algo bellman_ford step 5042 current loss 0.090129, current_train_items 161376.
I0304 19:30:35.081601 22849303695488 run.py:483] Algo bellman_ford step 5043 current loss 0.057820, current_train_items 161408.
I0304 19:30:35.115125 22849303695488 run.py:483] Algo bellman_ford step 5044 current loss 0.148312, current_train_items 161440.
I0304 19:30:35.135184 22849303695488 run.py:483] Algo bellman_ford step 5045 current loss 0.008079, current_train_items 161472.
I0304 19:30:35.151789 22849303695488 run.py:483] Algo bellman_ford step 5046 current loss 0.054211, current_train_items 161504.
I0304 19:30:35.174998 22849303695488 run.py:483] Algo bellman_ford step 5047 current loss 0.069055, current_train_items 161536.
I0304 19:30:35.206427 22849303695488 run.py:483] Algo bellman_ford step 5048 current loss 0.080169, current_train_items 161568.
I0304 19:30:35.240629 22849303695488 run.py:483] Algo bellman_ford step 5049 current loss 0.115404, current_train_items 161600.
I0304 19:30:35.260452 22849303695488 run.py:483] Algo bellman_ford step 5050 current loss 0.004846, current_train_items 161632.
I0304 19:30:35.268867 22849303695488 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0304 19:30:35.268977 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:35.285982 22849303695488 run.py:483] Algo bellman_ford step 5051 current loss 0.020538, current_train_items 161664.
I0304 19:30:35.311346 22849303695488 run.py:483] Algo bellman_ford step 5052 current loss 0.060799, current_train_items 161696.
I0304 19:30:35.343447 22849303695488 run.py:483] Algo bellman_ford step 5053 current loss 0.081762, current_train_items 161728.
I0304 19:30:35.376677 22849303695488 run.py:483] Algo bellman_ford step 5054 current loss 0.091825, current_train_items 161760.
I0304 19:30:35.397304 22849303695488 run.py:483] Algo bellman_ford step 5055 current loss 0.020505, current_train_items 161792.
I0304 19:30:35.414688 22849303695488 run.py:483] Algo bellman_ford step 5056 current loss 0.066940, current_train_items 161824.
I0304 19:30:35.440328 22849303695488 run.py:483] Algo bellman_ford step 5057 current loss 0.071965, current_train_items 161856.
I0304 19:30:35.471615 22849303695488 run.py:483] Algo bellman_ford step 5058 current loss 0.086462, current_train_items 161888.
I0304 19:30:35.506327 22849303695488 run.py:483] Algo bellman_ford step 5059 current loss 0.051710, current_train_items 161920.
I0304 19:30:35.526543 22849303695488 run.py:483] Algo bellman_ford step 5060 current loss 0.086525, current_train_items 161952.
I0304 19:30:35.543175 22849303695488 run.py:483] Algo bellman_ford step 5061 current loss 0.049916, current_train_items 161984.
I0304 19:30:35.565679 22849303695488 run.py:483] Algo bellman_ford step 5062 current loss 0.030647, current_train_items 162016.
I0304 19:30:35.597196 22849303695488 run.py:483] Algo bellman_ford step 5063 current loss 0.067753, current_train_items 162048.
I0304 19:30:35.629397 22849303695488 run.py:483] Algo bellman_ford step 5064 current loss 0.070220, current_train_items 162080.
I0304 19:30:35.649237 22849303695488 run.py:483] Algo bellman_ford step 5065 current loss 0.017135, current_train_items 162112.
I0304 19:30:35.665753 22849303695488 run.py:483] Algo bellman_ford step 5066 current loss 0.010582, current_train_items 162144.
I0304 19:30:35.691125 22849303695488 run.py:483] Algo bellman_ford step 5067 current loss 0.047462, current_train_items 162176.
I0304 19:30:35.722516 22849303695488 run.py:483] Algo bellman_ford step 5068 current loss 0.054733, current_train_items 162208.
I0304 19:30:35.757730 22849303695488 run.py:483] Algo bellman_ford step 5069 current loss 0.080623, current_train_items 162240.
I0304 19:30:35.777947 22849303695488 run.py:483] Algo bellman_ford step 5070 current loss 0.004272, current_train_items 162272.
I0304 19:30:35.794987 22849303695488 run.py:483] Algo bellman_ford step 5071 current loss 0.018017, current_train_items 162304.
I0304 19:30:35.817674 22849303695488 run.py:483] Algo bellman_ford step 5072 current loss 0.048281, current_train_items 162336.
I0304 19:30:35.849930 22849303695488 run.py:483] Algo bellman_ford step 5073 current loss 0.118471, current_train_items 162368.
I0304 19:30:35.881395 22849303695488 run.py:483] Algo bellman_ford step 5074 current loss 0.055233, current_train_items 162400.
I0304 19:30:35.901766 22849303695488 run.py:483] Algo bellman_ford step 5075 current loss 0.004520, current_train_items 162432.
I0304 19:30:35.918434 22849303695488 run.py:483] Algo bellman_ford step 5076 current loss 0.037663, current_train_items 162464.
I0304 19:30:35.942888 22849303695488 run.py:483] Algo bellman_ford step 5077 current loss 0.054755, current_train_items 162496.
I0304 19:30:35.974249 22849303695488 run.py:483] Algo bellman_ford step 5078 current loss 0.093417, current_train_items 162528.
I0304 19:30:36.007281 22849303695488 run.py:483] Algo bellman_ford step 5079 current loss 0.085791, current_train_items 162560.
I0304 19:30:36.027239 22849303695488 run.py:483] Algo bellman_ford step 5080 current loss 0.007232, current_train_items 162592.
I0304 19:30:36.043735 22849303695488 run.py:483] Algo bellman_ford step 5081 current loss 0.011017, current_train_items 162624.
I0304 19:30:36.067778 22849303695488 run.py:483] Algo bellman_ford step 5082 current loss 0.057098, current_train_items 162656.
I0304 19:30:36.099694 22849303695488 run.py:483] Algo bellman_ford step 5083 current loss 0.110073, current_train_items 162688.
I0304 19:30:36.133601 22849303695488 run.py:483] Algo bellman_ford step 5084 current loss 0.117327, current_train_items 162720.
I0304 19:30:36.154726 22849303695488 run.py:483] Algo bellman_ford step 5085 current loss 0.007026, current_train_items 162752.
I0304 19:30:36.171314 22849303695488 run.py:483] Algo bellman_ford step 5086 current loss 0.025420, current_train_items 162784.
I0304 19:30:36.195713 22849303695488 run.py:483] Algo bellman_ford step 5087 current loss 0.086723, current_train_items 162816.
I0304 19:30:36.227724 22849303695488 run.py:483] Algo bellman_ford step 5088 current loss 0.058400, current_train_items 162848.
I0304 19:30:36.262712 22849303695488 run.py:483] Algo bellman_ford step 5089 current loss 0.103045, current_train_items 162880.
I0304 19:30:36.282677 22849303695488 run.py:483] Algo bellman_ford step 5090 current loss 0.007505, current_train_items 162912.
I0304 19:30:36.299543 22849303695488 run.py:483] Algo bellman_ford step 5091 current loss 0.022920, current_train_items 162944.
I0304 19:30:36.322085 22849303695488 run.py:483] Algo bellman_ford step 5092 current loss 0.045255, current_train_items 162976.
I0304 19:30:36.352912 22849303695488 run.py:483] Algo bellman_ford step 5093 current loss 0.065416, current_train_items 163008.
I0304 19:30:36.385881 22849303695488 run.py:483] Algo bellman_ford step 5094 current loss 0.084698, current_train_items 163040.
I0304 19:30:36.405688 22849303695488 run.py:483] Algo bellman_ford step 5095 current loss 0.004586, current_train_items 163072.
I0304 19:30:36.422239 22849303695488 run.py:483] Algo bellman_ford step 5096 current loss 0.018709, current_train_items 163104.
I0304 19:30:36.447314 22849303695488 run.py:483] Algo bellman_ford step 5097 current loss 0.056119, current_train_items 163136.
I0304 19:30:36.479812 22849303695488 run.py:483] Algo bellman_ford step 5098 current loss 0.065995, current_train_items 163168.
I0304 19:30:36.515475 22849303695488 run.py:483] Algo bellman_ford step 5099 current loss 0.058448, current_train_items 163200.
I0304 19:30:36.535855 22849303695488 run.py:483] Algo bellman_ford step 5100 current loss 0.003859, current_train_items 163232.
I0304 19:30:36.543915 22849303695488 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0304 19:30:36.544039 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:30:36.560945 22849303695488 run.py:483] Algo bellman_ford step 5101 current loss 0.052234, current_train_items 163264.
I0304 19:30:36.583630 22849303695488 run.py:483] Algo bellman_ford step 5102 current loss 0.014662, current_train_items 163296.
I0304 19:30:36.616069 22849303695488 run.py:483] Algo bellman_ford step 5103 current loss 0.052902, current_train_items 163328.
I0304 19:30:36.651575 22849303695488 run.py:483] Algo bellman_ford step 5104 current loss 0.106359, current_train_items 163360.
I0304 19:30:36.671850 22849303695488 run.py:483] Algo bellman_ford step 5105 current loss 0.004001, current_train_items 163392.
I0304 19:30:36.688702 22849303695488 run.py:483] Algo bellman_ford step 5106 current loss 0.030007, current_train_items 163424.
I0304 19:30:36.712852 22849303695488 run.py:483] Algo bellman_ford step 5107 current loss 0.074596, current_train_items 163456.
I0304 19:30:36.743191 22849303695488 run.py:483] Algo bellman_ford step 5108 current loss 0.040099, current_train_items 163488.
I0304 19:30:36.777850 22849303695488 run.py:483] Algo bellman_ford step 5109 current loss 0.082630, current_train_items 163520.
I0304 19:30:36.798087 22849303695488 run.py:483] Algo bellman_ford step 5110 current loss 0.002493, current_train_items 163552.
I0304 19:30:36.814684 22849303695488 run.py:483] Algo bellman_ford step 5111 current loss 0.033249, current_train_items 163584.
I0304 19:30:36.838547 22849303695488 run.py:483] Algo bellman_ford step 5112 current loss 0.032937, current_train_items 163616.
I0304 19:30:36.869237 22849303695488 run.py:483] Algo bellman_ford step 5113 current loss 0.054100, current_train_items 163648.
I0304 19:30:36.902605 22849303695488 run.py:483] Algo bellman_ford step 5114 current loss 0.053206, current_train_items 163680.
I0304 19:30:36.922632 22849303695488 run.py:483] Algo bellman_ford step 5115 current loss 0.006804, current_train_items 163712.
I0304 19:30:36.939507 22849303695488 run.py:483] Algo bellman_ford step 5116 current loss 0.034323, current_train_items 163744.
I0304 19:30:36.964171 22849303695488 run.py:483] Algo bellman_ford step 5117 current loss 0.059597, current_train_items 163776.
I0304 19:30:36.994965 22849303695488 run.py:483] Algo bellman_ford step 5118 current loss 0.049867, current_train_items 163808.
I0304 19:30:37.029981 22849303695488 run.py:483] Algo bellman_ford step 5119 current loss 0.052990, current_train_items 163840.
I0304 19:30:37.050013 22849303695488 run.py:483] Algo bellman_ford step 5120 current loss 0.026195, current_train_items 163872.
I0304 19:30:37.066139 22849303695488 run.py:483] Algo bellman_ford step 5121 current loss 0.010246, current_train_items 163904.
I0304 19:30:37.090044 22849303695488 run.py:483] Algo bellman_ford step 5122 current loss 0.041499, current_train_items 163936.
I0304 19:30:37.120691 22849303695488 run.py:483] Algo bellman_ford step 5123 current loss 0.031534, current_train_items 163968.
I0304 19:30:37.154728 22849303695488 run.py:483] Algo bellman_ford step 5124 current loss 0.108392, current_train_items 164000.
I0304 19:30:37.174432 22849303695488 run.py:483] Algo bellman_ford step 5125 current loss 0.002189, current_train_items 164032.
I0304 19:30:37.190950 22849303695488 run.py:483] Algo bellman_ford step 5126 current loss 0.043853, current_train_items 164064.
I0304 19:30:37.215116 22849303695488 run.py:483] Algo bellman_ford step 5127 current loss 0.074372, current_train_items 164096.
I0304 19:30:37.246637 22849303695488 run.py:483] Algo bellman_ford step 5128 current loss 0.041678, current_train_items 164128.
I0304 19:30:37.281801 22849303695488 run.py:483] Algo bellman_ford step 5129 current loss 0.065098, current_train_items 164160.
I0304 19:30:37.301794 22849303695488 run.py:483] Algo bellman_ford step 5130 current loss 0.001683, current_train_items 164192.
I0304 19:30:37.318519 22849303695488 run.py:483] Algo bellman_ford step 5131 current loss 0.028920, current_train_items 164224.
I0304 19:30:37.341799 22849303695488 run.py:483] Algo bellman_ford step 5132 current loss 0.132443, current_train_items 164256.
I0304 19:30:37.373996 22849303695488 run.py:483] Algo bellman_ford step 5133 current loss 0.087627, current_train_items 164288.
I0304 19:30:37.407307 22849303695488 run.py:483] Algo bellman_ford step 5134 current loss 0.069774, current_train_items 164320.
I0304 19:30:37.427515 22849303695488 run.py:483] Algo bellman_ford step 5135 current loss 0.005371, current_train_items 164352.
I0304 19:30:37.444110 22849303695488 run.py:483] Algo bellman_ford step 5136 current loss 0.028548, current_train_items 164384.
I0304 19:30:37.468370 22849303695488 run.py:483] Algo bellman_ford step 5137 current loss 0.043356, current_train_items 164416.
I0304 19:30:37.499547 22849303695488 run.py:483] Algo bellman_ford step 5138 current loss 0.053431, current_train_items 164448.
I0304 19:30:37.534043 22849303695488 run.py:483] Algo bellman_ford step 5139 current loss 0.053881, current_train_items 164480.
I0304 19:30:37.553992 22849303695488 run.py:483] Algo bellman_ford step 5140 current loss 0.003604, current_train_items 164512.
I0304 19:30:37.571038 22849303695488 run.py:483] Algo bellman_ford step 5141 current loss 0.015977, current_train_items 164544.
I0304 19:30:37.595937 22849303695488 run.py:483] Algo bellman_ford step 5142 current loss 0.052821, current_train_items 164576.
I0304 19:30:37.627259 22849303695488 run.py:483] Algo bellman_ford step 5143 current loss 0.067411, current_train_items 164608.
I0304 19:30:37.661418 22849303695488 run.py:483] Algo bellman_ford step 5144 current loss 0.064845, current_train_items 164640.
I0304 19:30:37.681102 22849303695488 run.py:483] Algo bellman_ford step 5145 current loss 0.003856, current_train_items 164672.
I0304 19:30:37.697484 22849303695488 run.py:483] Algo bellman_ford step 5146 current loss 0.039353, current_train_items 164704.
I0304 19:30:37.720350 22849303695488 run.py:483] Algo bellman_ford step 5147 current loss 0.026730, current_train_items 164736.
I0304 19:30:37.750849 22849303695488 run.py:483] Algo bellman_ford step 5148 current loss 0.104731, current_train_items 164768.
I0304 19:30:37.783861 22849303695488 run.py:483] Algo bellman_ford step 5149 current loss 0.089831, current_train_items 164800.
I0304 19:30:37.803890 22849303695488 run.py:483] Algo bellman_ford step 5150 current loss 0.007580, current_train_items 164832.
I0304 19:30:37.812394 22849303695488 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0304 19:30:37.812504 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:37.829682 22849303695488 run.py:483] Algo bellman_ford step 5151 current loss 0.014324, current_train_items 164864.
I0304 19:30:37.854302 22849303695488 run.py:483] Algo bellman_ford step 5152 current loss 0.032226, current_train_items 164896.
I0304 19:30:37.885065 22849303695488 run.py:483] Algo bellman_ford step 5153 current loss 0.079725, current_train_items 164928.
I0304 19:30:37.919107 22849303695488 run.py:483] Algo bellman_ford step 5154 current loss 0.095400, current_train_items 164960.
I0304 19:30:37.939364 22849303695488 run.py:483] Algo bellman_ford step 5155 current loss 0.007154, current_train_items 164992.
I0304 19:30:37.955787 22849303695488 run.py:483] Algo bellman_ford step 5156 current loss 0.010510, current_train_items 165024.
I0304 19:30:37.981456 22849303695488 run.py:483] Algo bellman_ford step 5157 current loss 0.046229, current_train_items 165056.
I0304 19:30:38.014895 22849303695488 run.py:483] Algo bellman_ford step 5158 current loss 0.123867, current_train_items 165088.
I0304 19:30:38.048398 22849303695488 run.py:483] Algo bellman_ford step 5159 current loss 0.072786, current_train_items 165120.
I0304 19:30:38.069254 22849303695488 run.py:483] Algo bellman_ford step 5160 current loss 0.005234, current_train_items 165152.
I0304 19:30:38.085709 22849303695488 run.py:483] Algo bellman_ford step 5161 current loss 0.016533, current_train_items 165184.
I0304 19:30:38.111057 22849303695488 run.py:483] Algo bellman_ford step 5162 current loss 0.090748, current_train_items 165216.
I0304 19:30:38.143910 22849303695488 run.py:483] Algo bellman_ford step 5163 current loss 0.074438, current_train_items 165248.
I0304 19:30:38.177033 22849303695488 run.py:483] Algo bellman_ford step 5164 current loss 0.062433, current_train_items 165280.
I0304 19:30:38.197060 22849303695488 run.py:483] Algo bellman_ford step 5165 current loss 0.006616, current_train_items 165312.
I0304 19:30:38.213481 22849303695488 run.py:483] Algo bellman_ford step 5166 current loss 0.021509, current_train_items 165344.
I0304 19:30:38.237044 22849303695488 run.py:483] Algo bellman_ford step 5167 current loss 0.108217, current_train_items 165376.
I0304 19:30:38.269456 22849303695488 run.py:483] Algo bellman_ford step 5168 current loss 0.087128, current_train_items 165408.
I0304 19:30:38.304888 22849303695488 run.py:483] Algo bellman_ford step 5169 current loss 0.116493, current_train_items 165440.
I0304 19:30:38.325619 22849303695488 run.py:483] Algo bellman_ford step 5170 current loss 0.004216, current_train_items 165472.
I0304 19:30:38.342422 22849303695488 run.py:483] Algo bellman_ford step 5171 current loss 0.015946, current_train_items 165504.
I0304 19:30:38.366897 22849303695488 run.py:483] Algo bellman_ford step 5172 current loss 0.083536, current_train_items 165536.
I0304 19:30:38.399884 22849303695488 run.py:483] Algo bellman_ford step 5173 current loss 0.109705, current_train_items 165568.
I0304 19:30:38.433335 22849303695488 run.py:483] Algo bellman_ford step 5174 current loss 0.083479, current_train_items 165600.
I0304 19:30:38.453600 22849303695488 run.py:483] Algo bellman_ford step 5175 current loss 0.005563, current_train_items 165632.
I0304 19:30:38.470505 22849303695488 run.py:483] Algo bellman_ford step 5176 current loss 0.020570, current_train_items 165664.
I0304 19:30:38.495048 22849303695488 run.py:483] Algo bellman_ford step 5177 current loss 0.067037, current_train_items 165696.
I0304 19:30:38.526931 22849303695488 run.py:483] Algo bellman_ford step 5178 current loss 0.110495, current_train_items 165728.
I0304 19:30:38.559943 22849303695488 run.py:483] Algo bellman_ford step 5179 current loss 0.061511, current_train_items 165760.
I0304 19:30:38.579931 22849303695488 run.py:483] Algo bellman_ford step 5180 current loss 0.003996, current_train_items 165792.
I0304 19:30:38.596503 22849303695488 run.py:483] Algo bellman_ford step 5181 current loss 0.018777, current_train_items 165824.
I0304 19:30:38.621064 22849303695488 run.py:483] Algo bellman_ford step 5182 current loss 0.054050, current_train_items 165856.
I0304 19:30:38.652688 22849303695488 run.py:483] Algo bellman_ford step 5183 current loss 0.084919, current_train_items 165888.
I0304 19:30:38.685844 22849303695488 run.py:483] Algo bellman_ford step 5184 current loss 0.066810, current_train_items 165920.
I0304 19:30:38.706549 22849303695488 run.py:483] Algo bellman_ford step 5185 current loss 0.026593, current_train_items 165952.
I0304 19:30:38.723589 22849303695488 run.py:483] Algo bellman_ford step 5186 current loss 0.034780, current_train_items 165984.
I0304 19:30:38.746957 22849303695488 run.py:483] Algo bellman_ford step 5187 current loss 0.065145, current_train_items 166016.
I0304 19:30:38.779493 22849303695488 run.py:483] Algo bellman_ford step 5188 current loss 0.122995, current_train_items 166048.
I0304 19:30:38.812817 22849303695488 run.py:483] Algo bellman_ford step 5189 current loss 0.096058, current_train_items 166080.
I0304 19:30:38.833578 22849303695488 run.py:483] Algo bellman_ford step 5190 current loss 0.007003, current_train_items 166112.
I0304 19:30:38.850111 22849303695488 run.py:483] Algo bellman_ford step 5191 current loss 0.033884, current_train_items 166144.
I0304 19:30:38.874809 22849303695488 run.py:483] Algo bellman_ford step 5192 current loss 0.096989, current_train_items 166176.
I0304 19:30:38.905922 22849303695488 run.py:483] Algo bellman_ford step 5193 current loss 0.084801, current_train_items 166208.
I0304 19:30:38.940182 22849303695488 run.py:483] Algo bellman_ford step 5194 current loss 0.098800, current_train_items 166240.
I0304 19:30:38.960452 22849303695488 run.py:483] Algo bellman_ford step 5195 current loss 0.003417, current_train_items 166272.
I0304 19:30:38.977160 22849303695488 run.py:483] Algo bellman_ford step 5196 current loss 0.012962, current_train_items 166304.
I0304 19:30:39.001890 22849303695488 run.py:483] Algo bellman_ford step 5197 current loss 0.074821, current_train_items 166336.
I0304 19:30:39.034634 22849303695488 run.py:483] Algo bellman_ford step 5198 current loss 0.136748, current_train_items 166368.
I0304 19:30:39.068351 22849303695488 run.py:483] Algo bellman_ford step 5199 current loss 0.108146, current_train_items 166400.
I0304 19:30:39.089049 22849303695488 run.py:483] Algo bellman_ford step 5200 current loss 0.004014, current_train_items 166432.
I0304 19:30:39.097075 22849303695488 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0304 19:30:39.097184 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:39.114620 22849303695488 run.py:483] Algo bellman_ford step 5201 current loss 0.020678, current_train_items 166464.
I0304 19:30:39.140436 22849303695488 run.py:483] Algo bellman_ford step 5202 current loss 0.042423, current_train_items 166496.
I0304 19:30:39.173542 22849303695488 run.py:483] Algo bellman_ford step 5203 current loss 0.091415, current_train_items 166528.
I0304 19:30:39.209564 22849303695488 run.py:483] Algo bellman_ford step 5204 current loss 0.093358, current_train_items 166560.
I0304 19:30:39.230325 22849303695488 run.py:483] Algo bellman_ford step 5205 current loss 0.005878, current_train_items 166592.
I0304 19:30:39.246452 22849303695488 run.py:483] Algo bellman_ford step 5206 current loss 0.021443, current_train_items 166624.
I0304 19:30:39.271156 22849303695488 run.py:483] Algo bellman_ford step 5207 current loss 0.075810, current_train_items 166656.
I0304 19:30:39.301866 22849303695488 run.py:483] Algo bellman_ford step 5208 current loss 0.060720, current_train_items 166688.
I0304 19:30:39.338989 22849303695488 run.py:483] Algo bellman_ford step 5209 current loss 0.089259, current_train_items 166720.
I0304 19:30:39.359415 22849303695488 run.py:483] Algo bellman_ford step 5210 current loss 0.010798, current_train_items 166752.
I0304 19:30:39.375657 22849303695488 run.py:483] Algo bellman_ford step 5211 current loss 0.015939, current_train_items 166784.
I0304 19:30:39.400539 22849303695488 run.py:483] Algo bellman_ford step 5212 current loss 0.037591, current_train_items 166816.
I0304 19:30:39.433893 22849303695488 run.py:483] Algo bellman_ford step 5213 current loss 0.090621, current_train_items 166848.
I0304 19:30:39.467817 22849303695488 run.py:483] Algo bellman_ford step 5214 current loss 0.085334, current_train_items 166880.
I0304 19:30:39.487581 22849303695488 run.py:483] Algo bellman_ford step 5215 current loss 0.003145, current_train_items 166912.
I0304 19:30:39.504539 22849303695488 run.py:483] Algo bellman_ford step 5216 current loss 0.022825, current_train_items 166944.
I0304 19:30:39.528719 22849303695488 run.py:483] Algo bellman_ford step 5217 current loss 0.031564, current_train_items 166976.
I0304 19:30:39.559751 22849303695488 run.py:483] Algo bellman_ford step 5218 current loss 0.056401, current_train_items 167008.
I0304 19:30:39.593794 22849303695488 run.py:483] Algo bellman_ford step 5219 current loss 0.076248, current_train_items 167040.
I0304 19:30:39.613945 22849303695488 run.py:483] Algo bellman_ford step 5220 current loss 0.034094, current_train_items 167072.
I0304 19:30:39.630369 22849303695488 run.py:483] Algo bellman_ford step 5221 current loss 0.036013, current_train_items 167104.
I0304 19:30:39.653773 22849303695488 run.py:483] Algo bellman_ford step 5222 current loss 0.036076, current_train_items 167136.
I0304 19:30:39.686583 22849303695488 run.py:483] Algo bellman_ford step 5223 current loss 0.085733, current_train_items 167168.
I0304 19:30:39.720548 22849303695488 run.py:483] Algo bellman_ford step 5224 current loss 0.069018, current_train_items 167200.
I0304 19:30:39.740979 22849303695488 run.py:483] Algo bellman_ford step 5225 current loss 0.005175, current_train_items 167232.
I0304 19:30:39.757347 22849303695488 run.py:483] Algo bellman_ford step 5226 current loss 0.017486, current_train_items 167264.
I0304 19:30:39.782617 22849303695488 run.py:483] Algo bellman_ford step 5227 current loss 0.091770, current_train_items 167296.
I0304 19:30:39.815798 22849303695488 run.py:483] Algo bellman_ford step 5228 current loss 0.119003, current_train_items 167328.
I0304 19:30:39.851499 22849303695488 run.py:483] Algo bellman_ford step 5229 current loss 0.114466, current_train_items 167360.
I0304 19:30:39.871538 22849303695488 run.py:483] Algo bellman_ford step 5230 current loss 0.004426, current_train_items 167392.
I0304 19:30:39.888192 22849303695488 run.py:483] Algo bellman_ford step 5231 current loss 0.053026, current_train_items 167424.
I0304 19:30:39.912982 22849303695488 run.py:483] Algo bellman_ford step 5232 current loss 0.046613, current_train_items 167456.
I0304 19:30:39.944437 22849303695488 run.py:483] Algo bellman_ford step 5233 current loss 0.088326, current_train_items 167488.
I0304 19:30:39.980702 22849303695488 run.py:483] Algo bellman_ford step 5234 current loss 0.100054, current_train_items 167520.
I0304 19:30:40.000719 22849303695488 run.py:483] Algo bellman_ford step 5235 current loss 0.004593, current_train_items 167552.
I0304 19:30:40.017053 22849303695488 run.py:483] Algo bellman_ford step 5236 current loss 0.012156, current_train_items 167584.
I0304 19:30:40.041548 22849303695488 run.py:483] Algo bellman_ford step 5237 current loss 0.040264, current_train_items 167616.
I0304 19:30:40.075202 22849303695488 run.py:483] Algo bellman_ford step 5238 current loss 0.070911, current_train_items 167648.
I0304 19:30:40.110177 22849303695488 run.py:483] Algo bellman_ford step 5239 current loss 0.071171, current_train_items 167680.
I0304 19:30:40.130262 22849303695488 run.py:483] Algo bellman_ford step 5240 current loss 0.011291, current_train_items 167712.
I0304 19:30:40.146803 22849303695488 run.py:483] Algo bellman_ford step 5241 current loss 0.035591, current_train_items 167744.
I0304 19:30:40.171236 22849303695488 run.py:483] Algo bellman_ford step 5242 current loss 0.038926, current_train_items 167776.
I0304 19:30:40.203328 22849303695488 run.py:483] Algo bellman_ford step 5243 current loss 0.052111, current_train_items 167808.
I0304 19:30:40.235074 22849303695488 run.py:483] Algo bellman_ford step 5244 current loss 0.031501, current_train_items 167840.
I0304 19:30:40.254970 22849303695488 run.py:483] Algo bellman_ford step 5245 current loss 0.010638, current_train_items 167872.
I0304 19:30:40.271544 22849303695488 run.py:483] Algo bellman_ford step 5246 current loss 0.026088, current_train_items 167904.
I0304 19:30:40.296810 22849303695488 run.py:483] Algo bellman_ford step 5247 current loss 0.056399, current_train_items 167936.
I0304 19:30:40.329489 22849303695488 run.py:483] Algo bellman_ford step 5248 current loss 0.057957, current_train_items 167968.
I0304 19:30:40.364155 22849303695488 run.py:483] Algo bellman_ford step 5249 current loss 0.070337, current_train_items 168000.
I0304 19:30:40.384507 22849303695488 run.py:483] Algo bellman_ford step 5250 current loss 0.004660, current_train_items 168032.
I0304 19:30:40.392686 22849303695488 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0304 19:30:40.392794 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:30:40.409943 22849303695488 run.py:483] Algo bellman_ford step 5251 current loss 0.032364, current_train_items 168064.
I0304 19:30:40.435248 22849303695488 run.py:483] Algo bellman_ford step 5252 current loss 0.160370, current_train_items 168096.
I0304 19:30:40.467738 22849303695488 run.py:483] Algo bellman_ford step 5253 current loss 0.116590, current_train_items 168128.
I0304 19:30:40.500166 22849303695488 run.py:483] Algo bellman_ford step 5254 current loss 0.075252, current_train_items 168160.
I0304 19:30:40.520620 22849303695488 run.py:483] Algo bellman_ford step 5255 current loss 0.069683, current_train_items 168192.
I0304 19:30:40.537001 22849303695488 run.py:483] Algo bellman_ford step 5256 current loss 0.013789, current_train_items 168224.
I0304 19:30:40.560460 22849303695488 run.py:483] Algo bellman_ford step 5257 current loss 0.057076, current_train_items 168256.
I0304 19:30:40.593034 22849303695488 run.py:483] Algo bellman_ford step 5258 current loss 0.077888, current_train_items 168288.
I0304 19:30:40.625220 22849303695488 run.py:483] Algo bellman_ford step 5259 current loss 0.044384, current_train_items 168320.
I0304 19:30:40.645715 22849303695488 run.py:483] Algo bellman_ford step 5260 current loss 0.004733, current_train_items 168352.
I0304 19:30:40.662712 22849303695488 run.py:483] Algo bellman_ford step 5261 current loss 0.024569, current_train_items 168384.
I0304 19:30:40.686223 22849303695488 run.py:483] Algo bellman_ford step 5262 current loss 0.060736, current_train_items 168416.
I0304 19:30:40.718910 22849303695488 run.py:483] Algo bellman_ford step 5263 current loss 0.065245, current_train_items 168448.
I0304 19:30:40.751896 22849303695488 run.py:483] Algo bellman_ford step 5264 current loss 0.049192, current_train_items 168480.
I0304 19:30:40.771981 22849303695488 run.py:483] Algo bellman_ford step 5265 current loss 0.004267, current_train_items 168512.
I0304 19:30:40.788588 22849303695488 run.py:483] Algo bellman_ford step 5266 current loss 0.028406, current_train_items 168544.
I0304 19:30:40.813862 22849303695488 run.py:483] Algo bellman_ford step 5267 current loss 0.099225, current_train_items 168576.
I0304 19:30:40.844828 22849303695488 run.py:483] Algo bellman_ford step 5268 current loss 0.085331, current_train_items 168608.
I0304 19:30:40.877699 22849303695488 run.py:483] Algo bellman_ford step 5269 current loss 0.063713, current_train_items 168640.
I0304 19:30:40.898178 22849303695488 run.py:483] Algo bellman_ford step 5270 current loss 0.005209, current_train_items 168672.
I0304 19:30:40.914650 22849303695488 run.py:483] Algo bellman_ford step 5271 current loss 0.017522, current_train_items 168704.
I0304 19:30:40.937870 22849303695488 run.py:483] Algo bellman_ford step 5272 current loss 0.040990, current_train_items 168736.
I0304 19:30:40.969291 22849303695488 run.py:483] Algo bellman_ford step 5273 current loss 0.045078, current_train_items 168768.
I0304 19:30:41.002713 22849303695488 run.py:483] Algo bellman_ford step 5274 current loss 0.053736, current_train_items 168800.
I0304 19:30:41.022863 22849303695488 run.py:483] Algo bellman_ford step 5275 current loss 0.006712, current_train_items 168832.
I0304 19:30:41.039523 22849303695488 run.py:483] Algo bellman_ford step 5276 current loss 0.015826, current_train_items 168864.
I0304 19:30:41.063060 22849303695488 run.py:483] Algo bellman_ford step 5277 current loss 0.027160, current_train_items 168896.
I0304 19:30:41.095235 22849303695488 run.py:483] Algo bellman_ford step 5278 current loss 0.061729, current_train_items 168928.
I0304 19:30:41.130128 22849303695488 run.py:483] Algo bellman_ford step 5279 current loss 0.057361, current_train_items 168960.
I0304 19:30:41.149745 22849303695488 run.py:483] Algo bellman_ford step 5280 current loss 0.004056, current_train_items 168992.
I0304 19:30:41.166468 22849303695488 run.py:483] Algo bellman_ford step 5281 current loss 0.040010, current_train_items 169024.
I0304 19:30:41.192067 22849303695488 run.py:483] Algo bellman_ford step 5282 current loss 0.055719, current_train_items 169056.
I0304 19:30:41.224104 22849303695488 run.py:483] Algo bellman_ford step 5283 current loss 0.068279, current_train_items 169088.
I0304 19:30:41.257542 22849303695488 run.py:483] Algo bellman_ford step 5284 current loss 0.092846, current_train_items 169120.
I0304 19:30:41.277949 22849303695488 run.py:483] Algo bellman_ford step 5285 current loss 0.002970, current_train_items 169152.
I0304 19:30:41.294895 22849303695488 run.py:483] Algo bellman_ford step 5286 current loss 0.024227, current_train_items 169184.
I0304 19:30:41.320182 22849303695488 run.py:483] Algo bellman_ford step 5287 current loss 0.062555, current_train_items 169216.
I0304 19:30:41.351497 22849303695488 run.py:483] Algo bellman_ford step 5288 current loss 0.089193, current_train_items 169248.
I0304 19:30:41.386016 22849303695488 run.py:483] Algo bellman_ford step 5289 current loss 0.085496, current_train_items 169280.
I0304 19:30:41.406373 22849303695488 run.py:483] Algo bellman_ford step 5290 current loss 0.004102, current_train_items 169312.
I0304 19:30:41.423431 22849303695488 run.py:483] Algo bellman_ford step 5291 current loss 0.017907, current_train_items 169344.
I0304 19:30:41.447148 22849303695488 run.py:483] Algo bellman_ford step 5292 current loss 0.087848, current_train_items 169376.
I0304 19:30:41.479172 22849303695488 run.py:483] Algo bellman_ford step 5293 current loss 0.082728, current_train_items 169408.
I0304 19:30:41.512438 22849303695488 run.py:483] Algo bellman_ford step 5294 current loss 0.077680, current_train_items 169440.
I0304 19:30:41.532115 22849303695488 run.py:483] Algo bellman_ford step 5295 current loss 0.004252, current_train_items 169472.
I0304 19:30:41.548589 22849303695488 run.py:483] Algo bellman_ford step 5296 current loss 0.029997, current_train_items 169504.
I0304 19:30:41.572264 22849303695488 run.py:483] Algo bellman_ford step 5297 current loss 0.056223, current_train_items 169536.
I0304 19:30:41.604665 22849303695488 run.py:483] Algo bellman_ford step 5298 current loss 0.080103, current_train_items 169568.
I0304 19:30:41.641210 22849303695488 run.py:483] Algo bellman_ford step 5299 current loss 0.093169, current_train_items 169600.
I0304 19:30:41.661532 22849303695488 run.py:483] Algo bellman_ford step 5300 current loss 0.004766, current_train_items 169632.
I0304 19:30:41.669636 22849303695488 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0304 19:30:41.669746 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:30:41.686522 22849303695488 run.py:483] Algo bellman_ford step 5301 current loss 0.013872, current_train_items 169664.
I0304 19:30:41.712017 22849303695488 run.py:483] Algo bellman_ford step 5302 current loss 0.110172, current_train_items 169696.
I0304 19:30:41.745355 22849303695488 run.py:483] Algo bellman_ford step 5303 current loss 0.089797, current_train_items 169728.
I0304 19:30:41.781266 22849303695488 run.py:483] Algo bellman_ford step 5304 current loss 0.063507, current_train_items 169760.
I0304 19:30:41.802016 22849303695488 run.py:483] Algo bellman_ford step 5305 current loss 0.005942, current_train_items 169792.
I0304 19:30:41.818613 22849303695488 run.py:483] Algo bellman_ford step 5306 current loss 0.039351, current_train_items 169824.
I0304 19:30:41.844192 22849303695488 run.py:483] Algo bellman_ford step 5307 current loss 0.054159, current_train_items 169856.
I0304 19:30:41.877268 22849303695488 run.py:483] Algo bellman_ford step 5308 current loss 0.042970, current_train_items 169888.
I0304 19:30:41.910382 22849303695488 run.py:483] Algo bellman_ford step 5309 current loss 0.055442, current_train_items 169920.
I0304 19:30:41.930497 22849303695488 run.py:483] Algo bellman_ford step 5310 current loss 0.006113, current_train_items 169952.
I0304 19:30:41.946614 22849303695488 run.py:483] Algo bellman_ford step 5311 current loss 0.011233, current_train_items 169984.
I0304 19:30:41.971222 22849303695488 run.py:483] Algo bellman_ford step 5312 current loss 0.052725, current_train_items 170016.
I0304 19:30:42.003223 22849303695488 run.py:483] Algo bellman_ford step 5313 current loss 0.124444, current_train_items 170048.
I0304 19:30:42.038946 22849303695488 run.py:483] Algo bellman_ford step 5314 current loss 0.237651, current_train_items 170080.
I0304 19:30:42.059429 22849303695488 run.py:483] Algo bellman_ford step 5315 current loss 0.009410, current_train_items 170112.
I0304 19:30:42.076035 22849303695488 run.py:483] Algo bellman_ford step 5316 current loss 0.020210, current_train_items 170144.
I0304 19:30:42.100565 22849303695488 run.py:483] Algo bellman_ford step 5317 current loss 0.143010, current_train_items 170176.
I0304 19:30:42.131170 22849303695488 run.py:483] Algo bellman_ford step 5318 current loss 0.050145, current_train_items 170208.
I0304 19:30:42.165893 22849303695488 run.py:483] Algo bellman_ford step 5319 current loss 0.083121, current_train_items 170240.
I0304 19:30:42.185756 22849303695488 run.py:483] Algo bellman_ford step 5320 current loss 0.041366, current_train_items 170272.
I0304 19:30:42.202242 22849303695488 run.py:483] Algo bellman_ford step 5321 current loss 0.090518, current_train_items 170304.
I0304 19:30:42.227699 22849303695488 run.py:483] Algo bellman_ford step 5322 current loss 0.119310, current_train_items 170336.
I0304 19:30:42.258094 22849303695488 run.py:483] Algo bellman_ford step 5323 current loss 0.081141, current_train_items 170368.
I0304 19:30:42.293477 22849303695488 run.py:483] Algo bellman_ford step 5324 current loss 0.121398, current_train_items 170400.
I0304 19:30:42.313920 22849303695488 run.py:483] Algo bellman_ford step 5325 current loss 0.004368, current_train_items 170432.
I0304 19:30:42.330378 22849303695488 run.py:483] Algo bellman_ford step 5326 current loss 0.053704, current_train_items 170464.
I0304 19:30:42.354180 22849303695488 run.py:483] Algo bellman_ford step 5327 current loss 0.039706, current_train_items 170496.
I0304 19:30:42.386978 22849303695488 run.py:483] Algo bellman_ford step 5328 current loss 0.071824, current_train_items 170528.
I0304 19:30:42.420779 22849303695488 run.py:483] Algo bellman_ford step 5329 current loss 0.179378, current_train_items 170560.
I0304 19:30:42.441030 22849303695488 run.py:483] Algo bellman_ford step 5330 current loss 0.012549, current_train_items 170592.
I0304 19:30:42.457660 22849303695488 run.py:483] Algo bellman_ford step 5331 current loss 0.013346, current_train_items 170624.
I0304 19:30:42.482044 22849303695488 run.py:483] Algo bellman_ford step 5332 current loss 0.028504, current_train_items 170656.
I0304 19:30:42.512609 22849303695488 run.py:483] Algo bellman_ford step 5333 current loss 0.061191, current_train_items 170688.
I0304 19:30:42.545897 22849303695488 run.py:483] Algo bellman_ford step 5334 current loss 0.063799, current_train_items 170720.
I0304 19:30:42.566370 22849303695488 run.py:483] Algo bellman_ford step 5335 current loss 0.014614, current_train_items 170752.
I0304 19:30:42.583146 22849303695488 run.py:483] Algo bellman_ford step 5336 current loss 0.040841, current_train_items 170784.
I0304 19:30:42.607191 22849303695488 run.py:483] Algo bellman_ford step 5337 current loss 0.025655, current_train_items 170816.
I0304 19:30:42.638268 22849303695488 run.py:483] Algo bellman_ford step 5338 current loss 0.072933, current_train_items 170848.
I0304 19:30:42.673625 22849303695488 run.py:483] Algo bellman_ford step 5339 current loss 0.130209, current_train_items 170880.
I0304 19:30:42.693565 22849303695488 run.py:483] Algo bellman_ford step 5340 current loss 0.002231, current_train_items 170912.
I0304 19:30:42.710546 22849303695488 run.py:483] Algo bellman_ford step 5341 current loss 0.050366, current_train_items 170944.
I0304 19:30:42.736509 22849303695488 run.py:483] Algo bellman_ford step 5342 current loss 0.056750, current_train_items 170976.
I0304 19:30:42.768706 22849303695488 run.py:483] Algo bellman_ford step 5343 current loss 0.090384, current_train_items 171008.
I0304 19:30:42.802911 22849303695488 run.py:483] Algo bellman_ford step 5344 current loss 0.082457, current_train_items 171040.
I0304 19:30:42.822782 22849303695488 run.py:483] Algo bellman_ford step 5345 current loss 0.001494, current_train_items 171072.
I0304 19:30:42.838756 22849303695488 run.py:483] Algo bellman_ford step 5346 current loss 0.015886, current_train_items 171104.
I0304 19:30:42.863203 22849303695488 run.py:483] Algo bellman_ford step 5347 current loss 0.047578, current_train_items 171136.
I0304 19:30:42.896123 22849303695488 run.py:483] Algo bellman_ford step 5348 current loss 0.078406, current_train_items 171168.
I0304 19:30:42.930669 22849303695488 run.py:483] Algo bellman_ford step 5349 current loss 0.078820, current_train_items 171200.
I0304 19:30:42.950974 22849303695488 run.py:483] Algo bellman_ford step 5350 current loss 0.008059, current_train_items 171232.
I0304 19:30:42.959445 22849303695488 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0304 19:30:42.959555 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:30:42.976275 22849303695488 run.py:483] Algo bellman_ford step 5351 current loss 0.015411, current_train_items 171264.
I0304 19:30:43.001754 22849303695488 run.py:483] Algo bellman_ford step 5352 current loss 0.105105, current_train_items 171296.
I0304 19:30:43.033246 22849303695488 run.py:483] Algo bellman_ford step 5353 current loss 0.112377, current_train_items 171328.
I0304 19:30:43.067456 22849303695488 run.py:483] Algo bellman_ford step 5354 current loss 0.074486, current_train_items 171360.
I0304 19:30:43.087693 22849303695488 run.py:483] Algo bellman_ford step 5355 current loss 0.003374, current_train_items 171392.
I0304 19:30:43.104132 22849303695488 run.py:483] Algo bellman_ford step 5356 current loss 0.031422, current_train_items 171424.
I0304 19:30:43.128727 22849303695488 run.py:483] Algo bellman_ford step 5357 current loss 0.067368, current_train_items 171456.
I0304 19:30:43.159475 22849303695488 run.py:483] Algo bellman_ford step 5358 current loss 0.059945, current_train_items 171488.
I0304 19:30:43.193526 22849303695488 run.py:483] Algo bellman_ford step 5359 current loss 0.059179, current_train_items 171520.
I0304 19:30:43.214145 22849303695488 run.py:483] Algo bellman_ford step 5360 current loss 0.005710, current_train_items 171552.
I0304 19:30:43.230947 22849303695488 run.py:483] Algo bellman_ford step 5361 current loss 0.036267, current_train_items 171584.
I0304 19:30:43.255303 22849303695488 run.py:483] Algo bellman_ford step 5362 current loss 0.072384, current_train_items 171616.
I0304 19:30:43.287441 22849303695488 run.py:483] Algo bellman_ford step 5363 current loss 0.076935, current_train_items 171648.
I0304 19:30:43.321885 22849303695488 run.py:483] Algo bellman_ford step 5364 current loss 0.094385, current_train_items 171680.
I0304 19:30:43.342293 22849303695488 run.py:483] Algo bellman_ford step 5365 current loss 0.007908, current_train_items 171712.
I0304 19:30:43.358896 22849303695488 run.py:483] Algo bellman_ford step 5366 current loss 0.026869, current_train_items 171744.
I0304 19:30:43.383000 22849303695488 run.py:483] Algo bellman_ford step 5367 current loss 0.044217, current_train_items 171776.
I0304 19:30:43.414849 22849303695488 run.py:483] Algo bellman_ford step 5368 current loss 0.051609, current_train_items 171808.
I0304 19:30:43.448240 22849303695488 run.py:483] Algo bellman_ford step 5369 current loss 0.065043, current_train_items 171840.
I0304 19:30:43.468631 22849303695488 run.py:483] Algo bellman_ford step 5370 current loss 0.007283, current_train_items 171872.
I0304 19:30:43.485258 22849303695488 run.py:483] Algo bellman_ford step 5371 current loss 0.011229, current_train_items 171904.
I0304 19:30:43.509020 22849303695488 run.py:483] Algo bellman_ford step 5372 current loss 0.030094, current_train_items 171936.
I0304 19:30:43.540595 22849303695488 run.py:483] Algo bellman_ford step 5373 current loss 0.059270, current_train_items 171968.
I0304 19:30:43.572401 22849303695488 run.py:483] Algo bellman_ford step 5374 current loss 0.075393, current_train_items 172000.
I0304 19:30:43.592617 22849303695488 run.py:483] Algo bellman_ford step 5375 current loss 0.004772, current_train_items 172032.
I0304 19:30:43.609325 22849303695488 run.py:483] Algo bellman_ford step 5376 current loss 0.028190, current_train_items 172064.
I0304 19:30:43.633575 22849303695488 run.py:483] Algo bellman_ford step 5377 current loss 0.083254, current_train_items 172096.
I0304 19:30:43.664211 22849303695488 run.py:483] Algo bellman_ford step 5378 current loss 0.042977, current_train_items 172128.
I0304 19:30:43.695709 22849303695488 run.py:483] Algo bellman_ford step 5379 current loss 0.059935, current_train_items 172160.
I0304 19:30:43.715822 22849303695488 run.py:483] Algo bellman_ford step 5380 current loss 0.021329, current_train_items 172192.
I0304 19:30:43.732687 22849303695488 run.py:483] Algo bellman_ford step 5381 current loss 0.035516, current_train_items 172224.
I0304 19:30:43.757648 22849303695488 run.py:483] Algo bellman_ford step 5382 current loss 0.064116, current_train_items 172256.
I0304 19:30:43.789444 22849303695488 run.py:483] Algo bellman_ford step 5383 current loss 0.096948, current_train_items 172288.
I0304 19:30:43.822600 22849303695488 run.py:483] Algo bellman_ford step 5384 current loss 0.059531, current_train_items 172320.
I0304 19:30:43.843351 22849303695488 run.py:483] Algo bellman_ford step 5385 current loss 0.004569, current_train_items 172352.
I0304 19:30:43.859668 22849303695488 run.py:483] Algo bellman_ford step 5386 current loss 0.018280, current_train_items 172384.
I0304 19:30:43.883305 22849303695488 run.py:483] Algo bellman_ford step 5387 current loss 0.077876, current_train_items 172416.
I0304 19:30:43.914311 22849303695488 run.py:483] Algo bellman_ford step 5388 current loss 0.051870, current_train_items 172448.
I0304 19:30:43.948784 22849303695488 run.py:483] Algo bellman_ford step 5389 current loss 0.093540, current_train_items 172480.
I0304 19:30:43.969663 22849303695488 run.py:483] Algo bellman_ford step 5390 current loss 0.006208, current_train_items 172512.
I0304 19:30:43.986809 22849303695488 run.py:483] Algo bellman_ford step 5391 current loss 0.021414, current_train_items 172544.
I0304 19:30:44.010504 22849303695488 run.py:483] Algo bellman_ford step 5392 current loss 0.033017, current_train_items 172576.
I0304 19:30:44.043543 22849303695488 run.py:483] Algo bellman_ford step 5393 current loss 0.092118, current_train_items 172608.
I0304 19:30:44.078521 22849303695488 run.py:483] Algo bellman_ford step 5394 current loss 0.072471, current_train_items 172640.
I0304 19:30:44.098659 22849303695488 run.py:483] Algo bellman_ford step 5395 current loss 0.004913, current_train_items 172672.
I0304 19:30:44.115609 22849303695488 run.py:483] Algo bellman_ford step 5396 current loss 0.064502, current_train_items 172704.
I0304 19:30:44.140359 22849303695488 run.py:483] Algo bellman_ford step 5397 current loss 0.045396, current_train_items 172736.
I0304 19:30:44.171919 22849303695488 run.py:483] Algo bellman_ford step 5398 current loss 0.057227, current_train_items 172768.
I0304 19:30:44.207090 22849303695488 run.py:483] Algo bellman_ford step 5399 current loss 0.110543, current_train_items 172800.
I0304 19:30:44.227700 22849303695488 run.py:483] Algo bellman_ford step 5400 current loss 0.002253, current_train_items 172832.
I0304 19:30:44.235660 22849303695488 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0304 19:30:44.235769 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:30:44.253474 22849303695488 run.py:483] Algo bellman_ford step 5401 current loss 0.033597, current_train_items 172864.
I0304 19:30:44.278652 22849303695488 run.py:483] Algo bellman_ford step 5402 current loss 0.042663, current_train_items 172896.
I0304 19:30:44.312166 22849303695488 run.py:483] Algo bellman_ford step 5403 current loss 0.107039, current_train_items 172928.
I0304 19:30:44.347576 22849303695488 run.py:483] Algo bellman_ford step 5404 current loss 0.052829, current_train_items 172960.
I0304 19:30:44.367688 22849303695488 run.py:483] Algo bellman_ford step 5405 current loss 0.002029, current_train_items 172992.
I0304 19:30:44.383785 22849303695488 run.py:483] Algo bellman_ford step 5406 current loss 0.026065, current_train_items 173024.
I0304 19:30:44.407429 22849303695488 run.py:483] Algo bellman_ford step 5407 current loss 0.015174, current_train_items 173056.
I0304 19:30:44.437864 22849303695488 run.py:483] Algo bellman_ford step 5408 current loss 0.112048, current_train_items 173088.
I0304 19:30:44.471335 22849303695488 run.py:483] Algo bellman_ford step 5409 current loss 0.084893, current_train_items 173120.
I0304 19:30:44.491388 22849303695488 run.py:483] Algo bellman_ford step 5410 current loss 0.009181, current_train_items 173152.
I0304 19:30:44.508394 22849303695488 run.py:483] Algo bellman_ford step 5411 current loss 0.027174, current_train_items 173184.
I0304 19:30:44.533132 22849303695488 run.py:483] Algo bellman_ford step 5412 current loss 0.126750, current_train_items 173216.
I0304 19:30:44.565245 22849303695488 run.py:483] Algo bellman_ford step 5413 current loss 0.059619, current_train_items 173248.
I0304 19:30:44.599772 22849303695488 run.py:483] Algo bellman_ford step 5414 current loss 0.075164, current_train_items 173280.
I0304 19:30:44.619898 22849303695488 run.py:483] Algo bellman_ford step 5415 current loss 0.014526, current_train_items 173312.
I0304 19:30:44.636711 22849303695488 run.py:483] Algo bellman_ford step 5416 current loss 0.025236, current_train_items 173344.
I0304 19:30:44.661403 22849303695488 run.py:483] Algo bellman_ford step 5417 current loss 0.068547, current_train_items 173376.
I0304 19:30:44.694259 22849303695488 run.py:483] Algo bellman_ford step 5418 current loss 0.048782, current_train_items 173408.
I0304 19:30:44.730110 22849303695488 run.py:483] Algo bellman_ford step 5419 current loss 0.091253, current_train_items 173440.
I0304 19:30:44.750139 22849303695488 run.py:483] Algo bellman_ford step 5420 current loss 0.061616, current_train_items 173472.
I0304 19:30:44.766899 22849303695488 run.py:483] Algo bellman_ford step 5421 current loss 0.052143, current_train_items 173504.
I0304 19:30:44.790859 22849303695488 run.py:483] Algo bellman_ford step 5422 current loss 0.047940, current_train_items 173536.
I0304 19:30:44.821647 22849303695488 run.py:483] Algo bellman_ford step 5423 current loss 0.046682, current_train_items 173568.
I0304 19:30:44.853829 22849303695488 run.py:483] Algo bellman_ford step 5424 current loss 0.081411, current_train_items 173600.
I0304 19:30:44.873967 22849303695488 run.py:483] Algo bellman_ford step 5425 current loss 0.006499, current_train_items 173632.
I0304 19:30:44.890336 22849303695488 run.py:483] Algo bellman_ford step 5426 current loss 0.030263, current_train_items 173664.
I0304 19:30:44.915121 22849303695488 run.py:483] Algo bellman_ford step 5427 current loss 0.070361, current_train_items 173696.
I0304 19:30:44.946428 22849303695488 run.py:483] Algo bellman_ford step 5428 current loss 0.079053, current_train_items 173728.
I0304 19:30:44.981209 22849303695488 run.py:483] Algo bellman_ford step 5429 current loss 0.053754, current_train_items 173760.
I0304 19:30:45.000890 22849303695488 run.py:483] Algo bellman_ford step 5430 current loss 0.003592, current_train_items 173792.
I0304 19:30:45.017699 22849303695488 run.py:483] Algo bellman_ford step 5431 current loss 0.021196, current_train_items 173824.
I0304 19:30:45.042315 22849303695488 run.py:483] Algo bellman_ford step 5432 current loss 0.054355, current_train_items 173856.
I0304 19:30:45.075870 22849303695488 run.py:483] Algo bellman_ford step 5433 current loss 0.095622, current_train_items 173888.
I0304 19:30:45.112550 22849303695488 run.py:483] Algo bellman_ford step 5434 current loss 0.139498, current_train_items 173920.
I0304 19:30:45.132339 22849303695488 run.py:483] Algo bellman_ford step 5435 current loss 0.004869, current_train_items 173952.
I0304 19:30:45.148945 22849303695488 run.py:483] Algo bellman_ford step 5436 current loss 0.049104, current_train_items 173984.
I0304 19:30:45.173293 22849303695488 run.py:483] Algo bellman_ford step 5437 current loss 0.077986, current_train_items 174016.
I0304 19:30:45.204371 22849303695488 run.py:483] Algo bellman_ford step 5438 current loss 0.168401, current_train_items 174048.
I0304 19:30:45.241918 22849303695488 run.py:483] Algo bellman_ford step 5439 current loss 0.201845, current_train_items 174080.
I0304 19:30:45.261811 22849303695488 run.py:483] Algo bellman_ford step 5440 current loss 0.016255, current_train_items 174112.
I0304 19:30:45.278152 22849303695488 run.py:483] Algo bellman_ford step 5441 current loss 0.013086, current_train_items 174144.
I0304 19:30:45.302094 22849303695488 run.py:483] Algo bellman_ford step 5442 current loss 0.042544, current_train_items 174176.
I0304 19:30:45.332959 22849303695488 run.py:483] Algo bellman_ford step 5443 current loss 0.082270, current_train_items 174208.
I0304 19:30:45.367180 22849303695488 run.py:483] Algo bellman_ford step 5444 current loss 0.067137, current_train_items 174240.
I0304 19:30:45.386878 22849303695488 run.py:483] Algo bellman_ford step 5445 current loss 0.004457, current_train_items 174272.
I0304 19:30:45.403410 22849303695488 run.py:483] Algo bellman_ford step 5446 current loss 0.052258, current_train_items 174304.
I0304 19:30:45.428066 22849303695488 run.py:483] Algo bellman_ford step 5447 current loss 0.051819, current_train_items 174336.
I0304 19:30:45.460653 22849303695488 run.py:483] Algo bellman_ford step 5448 current loss 0.072262, current_train_items 174368.
I0304 19:30:45.494491 22849303695488 run.py:483] Algo bellman_ford step 5449 current loss 0.039123, current_train_items 174400.
I0304 19:30:45.514298 22849303695488 run.py:483] Algo bellman_ford step 5450 current loss 0.009947, current_train_items 174432.
I0304 19:30:45.522850 22849303695488 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0304 19:30:45.522960 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:30:45.539818 22849303695488 run.py:483] Algo bellman_ford step 5451 current loss 0.019477, current_train_items 174464.
I0304 19:30:45.564940 22849303695488 run.py:483] Algo bellman_ford step 5452 current loss 0.075652, current_train_items 174496.
I0304 19:30:45.595914 22849303695488 run.py:483] Algo bellman_ford step 5453 current loss 0.061185, current_train_items 174528.
I0304 19:30:45.630450 22849303695488 run.py:483] Algo bellman_ford step 5454 current loss 0.108603, current_train_items 174560.
I0304 19:30:45.650743 22849303695488 run.py:483] Algo bellman_ford step 5455 current loss 0.004014, current_train_items 174592.
I0304 19:30:45.667356 22849303695488 run.py:483] Algo bellman_ford step 5456 current loss 0.010029, current_train_items 174624.
I0304 19:30:45.692144 22849303695488 run.py:483] Algo bellman_ford step 5457 current loss 0.066043, current_train_items 174656.
I0304 19:30:45.724526 22849303695488 run.py:483] Algo bellman_ford step 5458 current loss 0.077319, current_train_items 174688.
I0304 19:30:45.756292 22849303695488 run.py:483] Algo bellman_ford step 5459 current loss 0.055304, current_train_items 174720.
I0304 19:30:45.776914 22849303695488 run.py:483] Algo bellman_ford step 5460 current loss 0.004691, current_train_items 174752.
I0304 19:30:45.793872 22849303695488 run.py:483] Algo bellman_ford step 5461 current loss 0.035254, current_train_items 174784.
I0304 19:30:45.818300 22849303695488 run.py:483] Algo bellman_ford step 5462 current loss 0.024419, current_train_items 174816.
I0304 19:30:45.849593 22849303695488 run.py:483] Algo bellman_ford step 5463 current loss 0.059702, current_train_items 174848.
I0304 19:30:45.884459 22849303695488 run.py:483] Algo bellman_ford step 5464 current loss 0.120514, current_train_items 174880.
I0304 19:30:45.904879 22849303695488 run.py:483] Algo bellman_ford step 5465 current loss 0.006320, current_train_items 174912.
I0304 19:30:45.921283 22849303695488 run.py:483] Algo bellman_ford step 5466 current loss 0.035621, current_train_items 174944.
I0304 19:30:45.945555 22849303695488 run.py:483] Algo bellman_ford step 5467 current loss 0.031656, current_train_items 174976.
I0304 19:30:45.976579 22849303695488 run.py:483] Algo bellman_ford step 5468 current loss 0.120176, current_train_items 175008.
I0304 19:30:46.011059 22849303695488 run.py:483] Algo bellman_ford step 5469 current loss 0.083254, current_train_items 175040.
I0304 19:30:46.031597 22849303695488 run.py:483] Algo bellman_ford step 5470 current loss 0.009104, current_train_items 175072.
I0304 19:30:46.048330 22849303695488 run.py:483] Algo bellman_ford step 5471 current loss 0.040623, current_train_items 175104.
I0304 19:30:46.071828 22849303695488 run.py:483] Algo bellman_ford step 5472 current loss 0.041726, current_train_items 175136.
I0304 19:30:46.103651 22849303695488 run.py:483] Algo bellman_ford step 5473 current loss 0.047542, current_train_items 175168.
I0304 19:30:46.138556 22849303695488 run.py:483] Algo bellman_ford step 5474 current loss 0.094403, current_train_items 175200.
I0304 19:30:46.158857 22849303695488 run.py:483] Algo bellman_ford step 5475 current loss 0.005325, current_train_items 175232.
I0304 19:30:46.175590 22849303695488 run.py:483] Algo bellman_ford step 5476 current loss 0.028622, current_train_items 175264.
I0304 19:30:46.199546 22849303695488 run.py:483] Algo bellman_ford step 5477 current loss 0.099974, current_train_items 175296.
I0304 19:30:46.231635 22849303695488 run.py:483] Algo bellman_ford step 5478 current loss 0.114479, current_train_items 175328.
I0304 19:30:46.265380 22849303695488 run.py:483] Algo bellman_ford step 5479 current loss 0.161396, current_train_items 175360.
I0304 19:30:46.286239 22849303695488 run.py:483] Algo bellman_ford step 5480 current loss 0.010643, current_train_items 175392.
I0304 19:30:46.303383 22849303695488 run.py:483] Algo bellman_ford step 5481 current loss 0.053692, current_train_items 175424.
I0304 19:30:46.329113 22849303695488 run.py:483] Algo bellman_ford step 5482 current loss 0.073080, current_train_items 175456.
I0304 19:30:46.361835 22849303695488 run.py:483] Algo bellman_ford step 5483 current loss 0.108395, current_train_items 175488.
I0304 19:30:46.398188 22849303695488 run.py:483] Algo bellman_ford step 5484 current loss 0.095511, current_train_items 175520.
I0304 19:30:46.418388 22849303695488 run.py:483] Algo bellman_ford step 5485 current loss 0.004922, current_train_items 175552.
I0304 19:30:46.434874 22849303695488 run.py:483] Algo bellman_ford step 5486 current loss 0.025113, current_train_items 175584.
I0304 19:30:46.458487 22849303695488 run.py:483] Algo bellman_ford step 5487 current loss 0.041385, current_train_items 175616.
I0304 19:30:46.492083 22849303695488 run.py:483] Algo bellman_ford step 5488 current loss 0.068945, current_train_items 175648.
I0304 19:30:46.525383 22849303695488 run.py:483] Algo bellman_ford step 5489 current loss 0.051519, current_train_items 175680.
I0304 19:30:46.545724 22849303695488 run.py:483] Algo bellman_ford step 5490 current loss 0.005861, current_train_items 175712.
I0304 19:30:46.562419 22849303695488 run.py:483] Algo bellman_ford step 5491 current loss 0.013331, current_train_items 175744.
I0304 19:30:46.586756 22849303695488 run.py:483] Algo bellman_ford step 5492 current loss 0.047167, current_train_items 175776.
I0304 19:30:46.618549 22849303695488 run.py:483] Algo bellman_ford step 5493 current loss 0.037646, current_train_items 175808.
I0304 19:30:46.652370 22849303695488 run.py:483] Algo bellman_ford step 5494 current loss 0.075156, current_train_items 175840.
I0304 19:30:46.672437 22849303695488 run.py:483] Algo bellman_ford step 5495 current loss 0.002843, current_train_items 175872.
I0304 19:30:46.689230 22849303695488 run.py:483] Algo bellman_ford step 5496 current loss 0.004775, current_train_items 175904.
I0304 19:30:46.714146 22849303695488 run.py:483] Algo bellman_ford step 5497 current loss 0.048378, current_train_items 175936.
I0304 19:30:46.746024 22849303695488 run.py:483] Algo bellman_ford step 5498 current loss 0.048211, current_train_items 175968.
I0304 19:30:46.781463 22849303695488 run.py:483] Algo bellman_ford step 5499 current loss 0.127602, current_train_items 176000.
I0304 19:30:46.802139 22849303695488 run.py:483] Algo bellman_ford step 5500 current loss 0.006322, current_train_items 176032.
I0304 19:30:46.810275 22849303695488 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0304 19:30:46.810385 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:46.828242 22849303695488 run.py:483] Algo bellman_ford step 5501 current loss 0.047728, current_train_items 176064.
I0304 19:30:46.853353 22849303695488 run.py:483] Algo bellman_ford step 5502 current loss 0.073700, current_train_items 176096.
I0304 19:30:46.885199 22849303695488 run.py:483] Algo bellman_ford step 5503 current loss 0.052032, current_train_items 176128.
I0304 19:30:46.920062 22849303695488 run.py:483] Algo bellman_ford step 5504 current loss 0.065282, current_train_items 176160.
I0304 19:30:46.940326 22849303695488 run.py:483] Algo bellman_ford step 5505 current loss 0.007084, current_train_items 176192.
I0304 19:30:46.957071 22849303695488 run.py:483] Algo bellman_ford step 5506 current loss 0.028491, current_train_items 176224.
I0304 19:30:46.980947 22849303695488 run.py:483] Algo bellman_ford step 5507 current loss 0.040817, current_train_items 176256.
I0304 19:30:47.011295 22849303695488 run.py:483] Algo bellman_ford step 5508 current loss 0.070174, current_train_items 176288.
I0304 19:30:47.044636 22849303695488 run.py:483] Algo bellman_ford step 5509 current loss 0.045998, current_train_items 176320.
I0304 19:30:47.064803 22849303695488 run.py:483] Algo bellman_ford step 5510 current loss 0.003307, current_train_items 176352.
I0304 19:30:47.081734 22849303695488 run.py:483] Algo bellman_ford step 5511 current loss 0.049593, current_train_items 176384.
I0304 19:30:47.107101 22849303695488 run.py:483] Algo bellman_ford step 5512 current loss 0.103605, current_train_items 176416.
I0304 19:30:47.138332 22849303695488 run.py:483] Algo bellman_ford step 5513 current loss 0.062057, current_train_items 176448.
I0304 19:30:47.170758 22849303695488 run.py:483] Algo bellman_ford step 5514 current loss 0.084479, current_train_items 176480.
I0304 19:30:47.191071 22849303695488 run.py:483] Algo bellman_ford step 5515 current loss 0.004333, current_train_items 176512.
I0304 19:30:47.207301 22849303695488 run.py:483] Algo bellman_ford step 5516 current loss 0.006452, current_train_items 176544.
I0304 19:30:47.231848 22849303695488 run.py:483] Algo bellman_ford step 5517 current loss 0.048051, current_train_items 176576.
I0304 19:30:47.263060 22849303695488 run.py:483] Algo bellman_ford step 5518 current loss 0.120685, current_train_items 176608.
I0304 19:30:47.298700 22849303695488 run.py:483] Algo bellman_ford step 5519 current loss 0.132904, current_train_items 176640.
I0304 19:30:47.318603 22849303695488 run.py:483] Algo bellman_ford step 5520 current loss 0.012071, current_train_items 176672.
I0304 19:30:47.335592 22849303695488 run.py:483] Algo bellman_ford step 5521 current loss 0.024863, current_train_items 176704.
I0304 19:30:47.359641 22849303695488 run.py:483] Algo bellman_ford step 5522 current loss 0.046236, current_train_items 176736.
I0304 19:30:47.391867 22849303695488 run.py:483] Algo bellman_ford step 5523 current loss 0.077944, current_train_items 176768.
I0304 19:30:47.427309 22849303695488 run.py:483] Algo bellman_ford step 5524 current loss 0.119526, current_train_items 176800.
I0304 19:30:47.447693 22849303695488 run.py:483] Algo bellman_ford step 5525 current loss 0.006709, current_train_items 176832.
I0304 19:30:47.463840 22849303695488 run.py:483] Algo bellman_ford step 5526 current loss 0.036336, current_train_items 176864.
I0304 19:30:47.489111 22849303695488 run.py:483] Algo bellman_ford step 5527 current loss 0.057207, current_train_items 176896.
I0304 19:30:47.519925 22849303695488 run.py:483] Algo bellman_ford step 5528 current loss 0.062602, current_train_items 176928.
I0304 19:30:47.552834 22849303695488 run.py:483] Algo bellman_ford step 5529 current loss 0.054868, current_train_items 176960.
I0304 19:30:47.572999 22849303695488 run.py:483] Algo bellman_ford step 5530 current loss 0.004349, current_train_items 176992.
I0304 19:30:47.589618 22849303695488 run.py:483] Algo bellman_ford step 5531 current loss 0.018436, current_train_items 177024.
I0304 19:30:47.614660 22849303695488 run.py:483] Algo bellman_ford step 5532 current loss 0.091523, current_train_items 177056.
I0304 19:30:47.646034 22849303695488 run.py:483] Algo bellman_ford step 5533 current loss 0.105469, current_train_items 177088.
I0304 19:30:47.681970 22849303695488 run.py:483] Algo bellman_ford step 5534 current loss 0.152562, current_train_items 177120.
I0304 19:30:47.701934 22849303695488 run.py:483] Algo bellman_ford step 5535 current loss 0.020636, current_train_items 177152.
I0304 19:30:47.718793 22849303695488 run.py:483] Algo bellman_ford step 5536 current loss 0.048361, current_train_items 177184.
I0304 19:30:47.742354 22849303695488 run.py:483] Algo bellman_ford step 5537 current loss 0.067297, current_train_items 177216.
I0304 19:30:47.773248 22849303695488 run.py:483] Algo bellman_ford step 5538 current loss 0.125294, current_train_items 177248.
I0304 19:30:47.809125 22849303695488 run.py:483] Algo bellman_ford step 5539 current loss 0.081173, current_train_items 177280.
I0304 19:30:47.829495 22849303695488 run.py:483] Algo bellman_ford step 5540 current loss 0.005263, current_train_items 177312.
I0304 19:30:47.846457 22849303695488 run.py:483] Algo bellman_ford step 5541 current loss 0.040237, current_train_items 177344.
I0304 19:30:47.871668 22849303695488 run.py:483] Algo bellman_ford step 5542 current loss 0.076932, current_train_items 177376.
I0304 19:30:47.903217 22849303695488 run.py:483] Algo bellman_ford step 5543 current loss 0.055672, current_train_items 177408.
I0304 19:30:47.936830 22849303695488 run.py:483] Algo bellman_ford step 5544 current loss 0.148161, current_train_items 177440.
I0304 19:30:47.956727 22849303695488 run.py:483] Algo bellman_ford step 5545 current loss 0.005188, current_train_items 177472.
I0304 19:30:47.973970 22849303695488 run.py:483] Algo bellman_ford step 5546 current loss 0.039564, current_train_items 177504.
I0304 19:30:47.998319 22849303695488 run.py:483] Algo bellman_ford step 5547 current loss 0.026354, current_train_items 177536.
I0304 19:30:48.029444 22849303695488 run.py:483] Algo bellman_ford step 5548 current loss 0.045773, current_train_items 177568.
I0304 19:30:48.062754 22849303695488 run.py:483] Algo bellman_ford step 5549 current loss 0.098501, current_train_items 177600.
I0304 19:30:48.082626 22849303695488 run.py:483] Algo bellman_ford step 5550 current loss 0.007106, current_train_items 177632.
I0304 19:30:48.090842 22849303695488 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0304 19:30:48.090952 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:30:48.108150 22849303695488 run.py:483] Algo bellman_ford step 5551 current loss 0.015753, current_train_items 177664.
I0304 19:30:48.134188 22849303695488 run.py:483] Algo bellman_ford step 5552 current loss 0.077328, current_train_items 177696.
I0304 19:30:48.165995 22849303695488 run.py:483] Algo bellman_ford step 5553 current loss 0.066197, current_train_items 177728.
I0304 19:30:48.203212 22849303695488 run.py:483] Algo bellman_ford step 5554 current loss 0.097849, current_train_items 177760.
I0304 19:30:48.223738 22849303695488 run.py:483] Algo bellman_ford step 5555 current loss 0.015114, current_train_items 177792.
I0304 19:30:48.239942 22849303695488 run.py:483] Algo bellman_ford step 5556 current loss 0.031984, current_train_items 177824.
I0304 19:30:48.264828 22849303695488 run.py:483] Algo bellman_ford step 5557 current loss 0.097317, current_train_items 177856.
I0304 19:30:48.296301 22849303695488 run.py:483] Algo bellman_ford step 5558 current loss 0.076885, current_train_items 177888.
I0304 19:30:48.331366 22849303695488 run.py:483] Algo bellman_ford step 5559 current loss 0.076183, current_train_items 177920.
I0304 19:30:48.352012 22849303695488 run.py:483] Algo bellman_ford step 5560 current loss 0.015882, current_train_items 177952.
I0304 19:30:48.368541 22849303695488 run.py:483] Algo bellman_ford step 5561 current loss 0.022962, current_train_items 177984.
I0304 19:30:48.392520 22849303695488 run.py:483] Algo bellman_ford step 5562 current loss 0.063622, current_train_items 178016.
I0304 19:30:48.424883 22849303695488 run.py:483] Algo bellman_ford step 5563 current loss 0.056879, current_train_items 178048.
I0304 19:30:48.460479 22849303695488 run.py:483] Algo bellman_ford step 5564 current loss 0.075995, current_train_items 178080.
I0304 19:30:48.480799 22849303695488 run.py:483] Algo bellman_ford step 5565 current loss 0.005019, current_train_items 178112.
I0304 19:30:48.497318 22849303695488 run.py:483] Algo bellman_ford step 5566 current loss 0.043238, current_train_items 178144.
I0304 19:30:48.522304 22849303695488 run.py:483] Algo bellman_ford step 5567 current loss 0.069821, current_train_items 178176.
I0304 19:30:48.555352 22849303695488 run.py:483] Algo bellman_ford step 5568 current loss 0.104326, current_train_items 178208.
I0304 19:30:48.589942 22849303695488 run.py:483] Algo bellman_ford step 5569 current loss 0.107342, current_train_items 178240.
I0304 19:30:48.610627 22849303695488 run.py:483] Algo bellman_ford step 5570 current loss 0.012135, current_train_items 178272.
I0304 19:30:48.627071 22849303695488 run.py:483] Algo bellman_ford step 5571 current loss 0.022224, current_train_items 178304.
I0304 19:30:48.651191 22849303695488 run.py:483] Algo bellman_ford step 5572 current loss 0.036050, current_train_items 178336.
I0304 19:30:48.682588 22849303695488 run.py:483] Algo bellman_ford step 5573 current loss 0.104612, current_train_items 178368.
I0304 19:30:48.717056 22849303695488 run.py:483] Algo bellman_ford step 5574 current loss 0.123965, current_train_items 178400.
I0304 19:30:48.737745 22849303695488 run.py:483] Algo bellman_ford step 5575 current loss 0.006536, current_train_items 178432.
I0304 19:30:48.754513 22849303695488 run.py:483] Algo bellman_ford step 5576 current loss 0.015224, current_train_items 178464.
I0304 19:30:48.777780 22849303695488 run.py:483] Algo bellman_ford step 5577 current loss 0.058955, current_train_items 178496.
I0304 19:30:48.809792 22849303695488 run.py:483] Algo bellman_ford step 5578 current loss 0.061970, current_train_items 178528.
I0304 19:30:48.845935 22849303695488 run.py:483] Algo bellman_ford step 5579 current loss 0.061847, current_train_items 178560.
I0304 19:30:48.866354 22849303695488 run.py:483] Algo bellman_ford step 5580 current loss 0.003919, current_train_items 178592.
I0304 19:30:48.882546 22849303695488 run.py:483] Algo bellman_ford step 5581 current loss 0.031410, current_train_items 178624.
I0304 19:30:48.908147 22849303695488 run.py:483] Algo bellman_ford step 5582 current loss 0.040691, current_train_items 178656.
I0304 19:30:48.939897 22849303695488 run.py:483] Algo bellman_ford step 5583 current loss 0.113048, current_train_items 178688.
I0304 19:30:48.974394 22849303695488 run.py:483] Algo bellman_ford step 5584 current loss 0.096291, current_train_items 178720.
I0304 19:30:48.994981 22849303695488 run.py:483] Algo bellman_ford step 5585 current loss 0.013188, current_train_items 178752.
I0304 19:30:49.011616 22849303695488 run.py:483] Algo bellman_ford step 5586 current loss 0.021400, current_train_items 178784.
I0304 19:30:49.037170 22849303695488 run.py:483] Algo bellman_ford step 5587 current loss 0.073013, current_train_items 178816.
I0304 19:30:49.069977 22849303695488 run.py:483] Algo bellman_ford step 5588 current loss 0.080085, current_train_items 178848.
I0304 19:30:49.102292 22849303695488 run.py:483] Algo bellman_ford step 5589 current loss 0.079177, current_train_items 178880.
I0304 19:30:49.122650 22849303695488 run.py:483] Algo bellman_ford step 5590 current loss 0.007218, current_train_items 178912.
I0304 19:30:49.139219 22849303695488 run.py:483] Algo bellman_ford step 5591 current loss 0.017112, current_train_items 178944.
I0304 19:30:49.162161 22849303695488 run.py:483] Algo bellman_ford step 5592 current loss 0.069102, current_train_items 178976.
I0304 19:30:49.193924 22849303695488 run.py:483] Algo bellman_ford step 5593 current loss 0.092784, current_train_items 179008.
I0304 19:30:49.229372 22849303695488 run.py:483] Algo bellman_ford step 5594 current loss 0.051461, current_train_items 179040.
I0304 19:30:49.249311 22849303695488 run.py:483] Algo bellman_ford step 5595 current loss 0.018745, current_train_items 179072.
I0304 19:30:49.265749 22849303695488 run.py:483] Algo bellman_ford step 5596 current loss 0.017296, current_train_items 179104.
I0304 19:30:49.290064 22849303695488 run.py:483] Algo bellman_ford step 5597 current loss 0.071503, current_train_items 179136.
I0304 19:30:49.322226 22849303695488 run.py:483] Algo bellman_ford step 5598 current loss 0.104865, current_train_items 179168.
I0304 19:30:49.352936 22849303695488 run.py:483] Algo bellman_ford step 5599 current loss 0.096210, current_train_items 179200.
I0304 19:30:49.373713 22849303695488 run.py:483] Algo bellman_ford step 5600 current loss 0.006035, current_train_items 179232.
I0304 19:30:49.381637 22849303695488 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0304 19:30:49.381745 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:30:49.398942 22849303695488 run.py:483] Algo bellman_ford step 5601 current loss 0.018759, current_train_items 179264.
I0304 19:30:49.424191 22849303695488 run.py:483] Algo bellman_ford step 5602 current loss 0.141122, current_train_items 179296.
I0304 19:30:49.456184 22849303695488 run.py:483] Algo bellman_ford step 5603 current loss 0.083968, current_train_items 179328.
I0304 19:30:49.489214 22849303695488 run.py:483] Algo bellman_ford step 5604 current loss 0.043773, current_train_items 179360.
I0304 19:30:49.509881 22849303695488 run.py:483] Algo bellman_ford step 5605 current loss 0.002525, current_train_items 179392.
I0304 19:30:49.526082 22849303695488 run.py:483] Algo bellman_ford step 5606 current loss 0.037432, current_train_items 179424.
I0304 19:30:49.550790 22849303695488 run.py:483] Algo bellman_ford step 5607 current loss 0.044073, current_train_items 179456.
I0304 19:30:49.581277 22849303695488 run.py:483] Algo bellman_ford step 5608 current loss 0.034675, current_train_items 179488.
I0304 19:30:49.616178 22849303695488 run.py:483] Algo bellman_ford step 5609 current loss 0.099968, current_train_items 179520.
I0304 19:30:49.636532 22849303695488 run.py:483] Algo bellman_ford step 5610 current loss 0.010251, current_train_items 179552.
I0304 19:30:49.653187 22849303695488 run.py:483] Algo bellman_ford step 5611 current loss 0.019465, current_train_items 179584.
I0304 19:30:49.677140 22849303695488 run.py:483] Algo bellman_ford step 5612 current loss 0.051896, current_train_items 179616.
I0304 19:30:49.709570 22849303695488 run.py:483] Algo bellman_ford step 5613 current loss 0.051397, current_train_items 179648.
I0304 19:30:49.742523 22849303695488 run.py:483] Algo bellman_ford step 5614 current loss 0.053313, current_train_items 179680.
I0304 19:30:49.762574 22849303695488 run.py:483] Algo bellman_ford step 5615 current loss 0.016025, current_train_items 179712.
I0304 19:30:49.779604 22849303695488 run.py:483] Algo bellman_ford step 5616 current loss 0.020864, current_train_items 179744.
I0304 19:30:49.804373 22849303695488 run.py:483] Algo bellman_ford step 5617 current loss 0.022347, current_train_items 179776.
I0304 19:30:49.835473 22849303695488 run.py:483] Algo bellman_ford step 5618 current loss 0.032674, current_train_items 179808.
I0304 19:30:49.871061 22849303695488 run.py:483] Algo bellman_ford step 5619 current loss 0.088068, current_train_items 179840.
I0304 19:30:49.891553 22849303695488 run.py:483] Algo bellman_ford step 5620 current loss 0.005336, current_train_items 179872.
I0304 19:30:49.908120 22849303695488 run.py:483] Algo bellman_ford step 5621 current loss 0.014675, current_train_items 179904.
I0304 19:30:49.932250 22849303695488 run.py:483] Algo bellman_ford step 5622 current loss 0.029573, current_train_items 179936.
I0304 19:30:49.963826 22849303695488 run.py:483] Algo bellman_ford step 5623 current loss 0.079424, current_train_items 179968.
I0304 19:30:49.998740 22849303695488 run.py:483] Algo bellman_ford step 5624 current loss 0.067385, current_train_items 180000.
I0304 19:30:50.019015 22849303695488 run.py:483] Algo bellman_ford step 5625 current loss 0.002568, current_train_items 180032.
I0304 19:30:50.036001 22849303695488 run.py:483] Algo bellman_ford step 5626 current loss 0.024876, current_train_items 180064.
I0304 19:30:50.060963 22849303695488 run.py:483] Algo bellman_ford step 5627 current loss 0.043842, current_train_items 180096.
I0304 19:30:50.093296 22849303695488 run.py:483] Algo bellman_ford step 5628 current loss 0.076899, current_train_items 180128.
I0304 19:30:50.128145 22849303695488 run.py:483] Algo bellman_ford step 5629 current loss 0.068219, current_train_items 180160.
I0304 19:30:50.148079 22849303695488 run.py:483] Algo bellman_ford step 5630 current loss 0.004491, current_train_items 180192.
I0304 19:30:50.164525 22849303695488 run.py:483] Algo bellman_ford step 5631 current loss 0.006312, current_train_items 180224.
I0304 19:30:50.189512 22849303695488 run.py:483] Algo bellman_ford step 5632 current loss 0.062941, current_train_items 180256.
I0304 19:30:50.221242 22849303695488 run.py:483] Algo bellman_ford step 5633 current loss 0.042094, current_train_items 180288.
I0304 19:30:50.255830 22849303695488 run.py:483] Algo bellman_ford step 5634 current loss 0.102780, current_train_items 180320.
I0304 19:30:50.276108 22849303695488 run.py:483] Algo bellman_ford step 5635 current loss 0.014487, current_train_items 180352.
I0304 19:30:50.292949 22849303695488 run.py:483] Algo bellman_ford step 5636 current loss 0.017097, current_train_items 180384.
I0304 19:30:50.317253 22849303695488 run.py:483] Algo bellman_ford step 5637 current loss 0.031318, current_train_items 180416.
I0304 19:30:50.348024 22849303695488 run.py:483] Algo bellman_ford step 5638 current loss 0.069318, current_train_items 180448.
I0304 19:30:50.381370 22849303695488 run.py:483] Algo bellman_ford step 5639 current loss 0.099218, current_train_items 180480.
I0304 19:30:50.401490 22849303695488 run.py:483] Algo bellman_ford step 5640 current loss 0.004811, current_train_items 180512.
I0304 19:30:50.417909 22849303695488 run.py:483] Algo bellman_ford step 5641 current loss 0.043528, current_train_items 180544.
I0304 19:30:50.442415 22849303695488 run.py:483] Algo bellman_ford step 5642 current loss 0.043781, current_train_items 180576.
I0304 19:30:50.474300 22849303695488 run.py:483] Algo bellman_ford step 5643 current loss 0.063535, current_train_items 180608.
I0304 19:30:50.508045 22849303695488 run.py:483] Algo bellman_ford step 5644 current loss 0.057434, current_train_items 180640.
I0304 19:30:50.528383 22849303695488 run.py:483] Algo bellman_ford step 5645 current loss 0.003793, current_train_items 180672.
I0304 19:30:50.545453 22849303695488 run.py:483] Algo bellman_ford step 5646 current loss 0.031780, current_train_items 180704.
I0304 19:30:50.569383 22849303695488 run.py:483] Algo bellman_ford step 5647 current loss 0.031978, current_train_items 180736.
I0304 19:30:50.601133 22849303695488 run.py:483] Algo bellman_ford step 5648 current loss 0.077759, current_train_items 180768.
I0304 19:30:50.636838 22849303695488 run.py:483] Algo bellman_ford step 5649 current loss 0.103301, current_train_items 180800.
I0304 19:30:50.656866 22849303695488 run.py:483] Algo bellman_ford step 5650 current loss 0.028792, current_train_items 180832.
I0304 19:30:50.665877 22849303695488 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0304 19:30:50.665983 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:50.683440 22849303695488 run.py:483] Algo bellman_ford step 5651 current loss 0.020200, current_train_items 180864.
I0304 19:30:50.707818 22849303695488 run.py:483] Algo bellman_ford step 5652 current loss 0.059254, current_train_items 180896.
I0304 19:30:50.741755 22849303695488 run.py:483] Algo bellman_ford step 5653 current loss 0.070371, current_train_items 180928.
I0304 19:30:50.777931 22849303695488 run.py:483] Algo bellman_ford step 5654 current loss 0.068811, current_train_items 180960.
I0304 19:30:50.798425 22849303695488 run.py:483] Algo bellman_ford step 5655 current loss 0.006252, current_train_items 180992.
I0304 19:30:50.815032 22849303695488 run.py:483] Algo bellman_ford step 5656 current loss 0.036427, current_train_items 181024.
I0304 19:30:50.838515 22849303695488 run.py:483] Algo bellman_ford step 5657 current loss 0.037416, current_train_items 181056.
I0304 19:30:50.871654 22849303695488 run.py:483] Algo bellman_ford step 5658 current loss 0.057855, current_train_items 181088.
I0304 19:30:50.906204 22849303695488 run.py:483] Algo bellman_ford step 5659 current loss 0.082935, current_train_items 181120.
I0304 19:30:50.926867 22849303695488 run.py:483] Algo bellman_ford step 5660 current loss 0.010260, current_train_items 181152.
I0304 19:30:50.943377 22849303695488 run.py:483] Algo bellman_ford step 5661 current loss 0.013260, current_train_items 181184.
I0304 19:30:50.967087 22849303695488 run.py:483] Algo bellman_ford step 5662 current loss 0.056886, current_train_items 181216.
I0304 19:30:51.001051 22849303695488 run.py:483] Algo bellman_ford step 5663 current loss 0.101235, current_train_items 181248.
I0304 19:30:51.035685 22849303695488 run.py:483] Algo bellman_ford step 5664 current loss 0.065908, current_train_items 181280.
I0304 19:30:51.055820 22849303695488 run.py:483] Algo bellman_ford step 5665 current loss 0.051282, current_train_items 181312.
I0304 19:30:51.072812 22849303695488 run.py:483] Algo bellman_ford step 5666 current loss 0.013132, current_train_items 181344.
I0304 19:30:51.097207 22849303695488 run.py:483] Algo bellman_ford step 5667 current loss 0.043167, current_train_items 181376.
I0304 19:30:51.130171 22849303695488 run.py:483] Algo bellman_ford step 5668 current loss 0.081309, current_train_items 181408.
I0304 19:30:51.163672 22849303695488 run.py:483] Algo bellman_ford step 5669 current loss 0.076789, current_train_items 181440.
I0304 19:30:51.184164 22849303695488 run.py:483] Algo bellman_ford step 5670 current loss 0.009287, current_train_items 181472.
I0304 19:30:51.200739 22849303695488 run.py:483] Algo bellman_ford step 5671 current loss 0.011712, current_train_items 181504.
I0304 19:30:51.224473 22849303695488 run.py:483] Algo bellman_ford step 5672 current loss 0.070559, current_train_items 181536.
I0304 19:30:51.256181 22849303695488 run.py:483] Algo bellman_ford step 5673 current loss 0.087454, current_train_items 181568.
I0304 19:30:51.289828 22849303695488 run.py:483] Algo bellman_ford step 5674 current loss 0.074285, current_train_items 181600.
I0304 19:30:51.310214 22849303695488 run.py:483] Algo bellman_ford step 5675 current loss 0.016884, current_train_items 181632.
I0304 19:30:51.326699 22849303695488 run.py:483] Algo bellman_ford step 5676 current loss 0.033946, current_train_items 181664.
I0304 19:30:51.351316 22849303695488 run.py:483] Algo bellman_ford step 5677 current loss 0.129533, current_train_items 181696.
I0304 19:30:51.384848 22849303695488 run.py:483] Algo bellman_ford step 5678 current loss 0.202296, current_train_items 181728.
I0304 19:30:51.419187 22849303695488 run.py:483] Algo bellman_ford step 5679 current loss 0.143184, current_train_items 181760.
I0304 19:30:51.439536 22849303695488 run.py:483] Algo bellman_ford step 5680 current loss 0.004587, current_train_items 181792.
I0304 19:30:51.456258 22849303695488 run.py:483] Algo bellman_ford step 5681 current loss 0.057898, current_train_items 181824.
I0304 19:30:51.480827 22849303695488 run.py:483] Algo bellman_ford step 5682 current loss 0.087724, current_train_items 181856.
I0304 19:30:51.512805 22849303695488 run.py:483] Algo bellman_ford step 5683 current loss 0.104372, current_train_items 181888.
I0304 19:30:51.545825 22849303695488 run.py:483] Algo bellman_ford step 5684 current loss 0.090974, current_train_items 181920.
I0304 19:30:51.566368 22849303695488 run.py:483] Algo bellman_ford step 5685 current loss 0.008294, current_train_items 181952.
I0304 19:30:51.583013 22849303695488 run.py:483] Algo bellman_ford step 5686 current loss 0.038604, current_train_items 181984.
I0304 19:30:51.607324 22849303695488 run.py:483] Algo bellman_ford step 5687 current loss 0.080008, current_train_items 182016.
I0304 19:30:51.639939 22849303695488 run.py:483] Algo bellman_ford step 5688 current loss 0.059451, current_train_items 182048.
I0304 19:30:51.673475 22849303695488 run.py:483] Algo bellman_ford step 5689 current loss 0.090598, current_train_items 182080.
I0304 19:30:51.693997 22849303695488 run.py:483] Algo bellman_ford step 5690 current loss 0.003125, current_train_items 182112.
I0304 19:30:51.711181 22849303695488 run.py:483] Algo bellman_ford step 5691 current loss 0.031856, current_train_items 182144.
I0304 19:30:51.735275 22849303695488 run.py:483] Algo bellman_ford step 5692 current loss 0.061258, current_train_items 182176.
I0304 19:30:51.767246 22849303695488 run.py:483] Algo bellman_ford step 5693 current loss 0.060282, current_train_items 182208.
I0304 19:30:51.800571 22849303695488 run.py:483] Algo bellman_ford step 5694 current loss 0.122828, current_train_items 182240.
I0304 19:30:51.821042 22849303695488 run.py:483] Algo bellman_ford step 5695 current loss 0.003313, current_train_items 182272.
I0304 19:30:51.837973 22849303695488 run.py:483] Algo bellman_ford step 5696 current loss 0.047808, current_train_items 182304.
I0304 19:30:51.862234 22849303695488 run.py:483] Algo bellman_ford step 5697 current loss 0.120328, current_train_items 182336.
I0304 19:30:51.895024 22849303695488 run.py:483] Algo bellman_ford step 5698 current loss 0.163335, current_train_items 182368.
I0304 19:30:51.929842 22849303695488 run.py:483] Algo bellman_ford step 5699 current loss 0.108487, current_train_items 182400.
I0304 19:30:51.950736 22849303695488 run.py:483] Algo bellman_ford step 5700 current loss 0.006954, current_train_items 182432.
I0304 19:30:51.958763 22849303695488 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0304 19:30:51.958872 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:30:51.975605 22849303695488 run.py:483] Algo bellman_ford step 5701 current loss 0.013505, current_train_items 182464.
I0304 19:30:51.999269 22849303695488 run.py:483] Algo bellman_ford step 5702 current loss 0.046046, current_train_items 182496.
I0304 19:30:52.031508 22849303695488 run.py:483] Algo bellman_ford step 5703 current loss 0.099557, current_train_items 182528.
I0304 19:30:52.067423 22849303695488 run.py:483] Algo bellman_ford step 5704 current loss 0.137097, current_train_items 182560.
I0304 19:30:52.087777 22849303695488 run.py:483] Algo bellman_ford step 5705 current loss 0.006066, current_train_items 182592.
I0304 19:30:52.104048 22849303695488 run.py:483] Algo bellman_ford step 5706 current loss 0.007080, current_train_items 182624.
I0304 19:30:52.129198 22849303695488 run.py:483] Algo bellman_ford step 5707 current loss 0.049364, current_train_items 182656.
I0304 19:30:52.160503 22849303695488 run.py:483] Algo bellman_ford step 5708 current loss 0.089405, current_train_items 182688.
I0304 19:30:52.195137 22849303695488 run.py:483] Algo bellman_ford step 5709 current loss 0.066363, current_train_items 182720.
I0304 19:30:52.214964 22849303695488 run.py:483] Algo bellman_ford step 5710 current loss 0.003536, current_train_items 182752.
I0304 19:30:52.231329 22849303695488 run.py:483] Algo bellman_ford step 5711 current loss 0.025699, current_train_items 182784.
I0304 19:30:52.255239 22849303695488 run.py:483] Algo bellman_ford step 5712 current loss 0.033705, current_train_items 182816.
I0304 19:30:52.286231 22849303695488 run.py:483] Algo bellman_ford step 5713 current loss 0.041396, current_train_items 182848.
I0304 19:30:52.319763 22849303695488 run.py:483] Algo bellman_ford step 5714 current loss 0.053443, current_train_items 182880.
I0304 19:30:52.339709 22849303695488 run.py:483] Algo bellman_ford step 5715 current loss 0.008400, current_train_items 182912.
I0304 19:30:52.356276 22849303695488 run.py:483] Algo bellman_ford step 5716 current loss 0.015140, current_train_items 182944.
I0304 19:30:52.380800 22849303695488 run.py:483] Algo bellman_ford step 5717 current loss 0.039862, current_train_items 182976.
I0304 19:30:52.412408 22849303695488 run.py:483] Algo bellman_ford step 5718 current loss 0.091637, current_train_items 183008.
I0304 19:30:52.446905 22849303695488 run.py:483] Algo bellman_ford step 5719 current loss 0.093904, current_train_items 183040.
I0304 19:30:52.466906 22849303695488 run.py:483] Algo bellman_ford step 5720 current loss 0.004437, current_train_items 183072.
I0304 19:30:52.483438 22849303695488 run.py:483] Algo bellman_ford step 5721 current loss 0.015431, current_train_items 183104.
I0304 19:30:52.507143 22849303695488 run.py:483] Algo bellman_ford step 5722 current loss 0.103920, current_train_items 183136.
I0304 19:30:52.538380 22849303695488 run.py:483] Algo bellman_ford step 5723 current loss 0.072350, current_train_items 183168.
I0304 19:30:52.571668 22849303695488 run.py:483] Algo bellman_ford step 5724 current loss 0.085290, current_train_items 183200.
I0304 19:30:52.591555 22849303695488 run.py:483] Algo bellman_ford step 5725 current loss 0.005627, current_train_items 183232.
I0304 19:30:52.608239 22849303695488 run.py:483] Algo bellman_ford step 5726 current loss 0.050782, current_train_items 183264.
I0304 19:30:52.633185 22849303695488 run.py:483] Algo bellman_ford step 5727 current loss 0.106317, current_train_items 183296.
I0304 19:30:52.663930 22849303695488 run.py:483] Algo bellman_ford step 5728 current loss 0.050998, current_train_items 183328.
I0304 19:30:52.697952 22849303695488 run.py:483] Algo bellman_ford step 5729 current loss 0.070092, current_train_items 183360.
I0304 19:30:52.717871 22849303695488 run.py:483] Algo bellman_ford step 5730 current loss 0.004020, current_train_items 183392.
I0304 19:30:52.734808 22849303695488 run.py:483] Algo bellman_ford step 5731 current loss 0.031558, current_train_items 183424.
I0304 19:30:52.759768 22849303695488 run.py:483] Algo bellman_ford step 5732 current loss 0.055323, current_train_items 183456.
I0304 19:30:52.790727 22849303695488 run.py:483] Algo bellman_ford step 5733 current loss 0.098069, current_train_items 183488.
I0304 19:30:52.825687 22849303695488 run.py:483] Algo bellman_ford step 5734 current loss 0.071225, current_train_items 183520.
I0304 19:30:52.846330 22849303695488 run.py:483] Algo bellman_ford step 5735 current loss 0.003685, current_train_items 183552.
I0304 19:30:52.863118 22849303695488 run.py:483] Algo bellman_ford step 5736 current loss 0.045792, current_train_items 183584.
I0304 19:30:52.887472 22849303695488 run.py:483] Algo bellman_ford step 5737 current loss 0.095331, current_train_items 183616.
I0304 19:30:52.918814 22849303695488 run.py:483] Algo bellman_ford step 5738 current loss 0.111704, current_train_items 183648.
I0304 19:30:52.953337 22849303695488 run.py:483] Algo bellman_ford step 5739 current loss 0.125170, current_train_items 183680.
I0304 19:30:52.973171 22849303695488 run.py:483] Algo bellman_ford step 5740 current loss 0.028838, current_train_items 183712.
I0304 19:30:52.990023 22849303695488 run.py:483] Algo bellman_ford step 5741 current loss 0.010829, current_train_items 183744.
I0304 19:30:53.015287 22849303695488 run.py:483] Algo bellman_ford step 5742 current loss 0.057006, current_train_items 183776.
I0304 19:30:53.047730 22849303695488 run.py:483] Algo bellman_ford step 5743 current loss 0.092355, current_train_items 183808.
I0304 19:30:53.082302 22849303695488 run.py:483] Algo bellman_ford step 5744 current loss 0.090006, current_train_items 183840.
I0304 19:30:53.102337 22849303695488 run.py:483] Algo bellman_ford step 5745 current loss 0.043041, current_train_items 183872.
I0304 19:30:53.119546 22849303695488 run.py:483] Algo bellman_ford step 5746 current loss 0.017584, current_train_items 183904.
I0304 19:30:53.144029 22849303695488 run.py:483] Algo bellman_ford step 5747 current loss 0.062484, current_train_items 183936.
I0304 19:30:53.175878 22849303695488 run.py:483] Algo bellman_ford step 5748 current loss 0.076502, current_train_items 183968.
I0304 19:30:53.209412 22849303695488 run.py:483] Algo bellman_ford step 5749 current loss 0.068950, current_train_items 184000.
I0304 19:30:53.229744 22849303695488 run.py:483] Algo bellman_ford step 5750 current loss 0.003894, current_train_items 184032.
I0304 19:30:53.238090 22849303695488 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0304 19:30:53.238200 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:30:53.255830 22849303695488 run.py:483] Algo bellman_ford step 5751 current loss 0.008600, current_train_items 184064.
I0304 19:30:53.280290 22849303695488 run.py:483] Algo bellman_ford step 5752 current loss 0.102951, current_train_items 184096.
I0304 19:30:53.311477 22849303695488 run.py:483] Algo bellman_ford step 5753 current loss 0.111456, current_train_items 184128.
I0304 19:30:53.347455 22849303695488 run.py:483] Algo bellman_ford step 5754 current loss 0.174461, current_train_items 184160.
I0304 19:30:53.368396 22849303695488 run.py:483] Algo bellman_ford step 5755 current loss 0.003091, current_train_items 184192.
I0304 19:30:53.384524 22849303695488 run.py:483] Algo bellman_ford step 5756 current loss 0.027923, current_train_items 184224.
I0304 19:30:53.409584 22849303695488 run.py:483] Algo bellman_ford step 5757 current loss 0.077122, current_train_items 184256.
I0304 19:30:53.442587 22849303695488 run.py:483] Algo bellman_ford step 5758 current loss 0.079892, current_train_items 184288.
I0304 19:30:53.477206 22849303695488 run.py:483] Algo bellman_ford step 5759 current loss 0.051762, current_train_items 184320.
I0304 19:30:53.497473 22849303695488 run.py:483] Algo bellman_ford step 5760 current loss 0.086450, current_train_items 184352.
I0304 19:30:53.514457 22849303695488 run.py:483] Algo bellman_ford step 5761 current loss 0.039529, current_train_items 184384.
I0304 19:30:53.538544 22849303695488 run.py:483] Algo bellman_ford step 5762 current loss 0.049158, current_train_items 184416.
I0304 19:30:53.569369 22849303695488 run.py:483] Algo bellman_ford step 5763 current loss 0.043048, current_train_items 184448.
I0304 19:30:53.604755 22849303695488 run.py:483] Algo bellman_ford step 5764 current loss 0.067693, current_train_items 184480.
I0304 19:30:53.625148 22849303695488 run.py:483] Algo bellman_ford step 5765 current loss 0.011807, current_train_items 184512.
I0304 19:30:53.641837 22849303695488 run.py:483] Algo bellman_ford step 5766 current loss 0.027770, current_train_items 184544.
I0304 19:30:53.666299 22849303695488 run.py:483] Algo bellman_ford step 5767 current loss 0.029943, current_train_items 184576.
I0304 19:30:53.699477 22849303695488 run.py:483] Algo bellman_ford step 5768 current loss 0.050895, current_train_items 184608.
I0304 19:30:53.733488 22849303695488 run.py:483] Algo bellman_ford step 5769 current loss 0.088804, current_train_items 184640.
I0304 19:30:53.754174 22849303695488 run.py:483] Algo bellman_ford step 5770 current loss 0.003592, current_train_items 184672.
I0304 19:30:53.770836 22849303695488 run.py:483] Algo bellman_ford step 5771 current loss 0.032947, current_train_items 184704.
I0304 19:30:53.793792 22849303695488 run.py:483] Algo bellman_ford step 5772 current loss 0.075008, current_train_items 184736.
I0304 19:30:53.825094 22849303695488 run.py:483] Algo bellman_ford step 5773 current loss 0.055995, current_train_items 184768.
I0304 19:30:53.858461 22849303695488 run.py:483] Algo bellman_ford step 5774 current loss 0.058604, current_train_items 184800.
I0304 19:30:53.879148 22849303695488 run.py:483] Algo bellman_ford step 5775 current loss 0.003561, current_train_items 184832.
I0304 19:30:53.896151 22849303695488 run.py:483] Algo bellman_ford step 5776 current loss 0.055703, current_train_items 184864.
I0304 19:30:53.919383 22849303695488 run.py:483] Algo bellman_ford step 5777 current loss 0.040271, current_train_items 184896.
I0304 19:30:53.951546 22849303695488 run.py:483] Algo bellman_ford step 5778 current loss 0.069439, current_train_items 184928.
I0304 19:30:53.986307 22849303695488 run.py:483] Algo bellman_ford step 5779 current loss 0.107880, current_train_items 184960.
I0304 19:30:54.006338 22849303695488 run.py:483] Algo bellman_ford step 5780 current loss 0.003714, current_train_items 184992.
I0304 19:30:54.022942 22849303695488 run.py:483] Algo bellman_ford step 5781 current loss 0.017995, current_train_items 185024.
I0304 19:30:54.048081 22849303695488 run.py:483] Algo bellman_ford step 5782 current loss 0.038844, current_train_items 185056.
I0304 19:30:54.079051 22849303695488 run.py:483] Algo bellman_ford step 5783 current loss 0.039039, current_train_items 185088.
I0304 19:30:54.111809 22849303695488 run.py:483] Algo bellman_ford step 5784 current loss 0.083311, current_train_items 185120.
I0304 19:30:54.132142 22849303695488 run.py:483] Algo bellman_ford step 5785 current loss 0.005800, current_train_items 185152.
I0304 19:30:54.148811 22849303695488 run.py:483] Algo bellman_ford step 5786 current loss 0.022771, current_train_items 185184.
I0304 19:30:54.171726 22849303695488 run.py:483] Algo bellman_ford step 5787 current loss 0.035104, current_train_items 185216.
I0304 19:30:54.204097 22849303695488 run.py:483] Algo bellman_ford step 5788 current loss 0.069394, current_train_items 185248.
I0304 19:30:54.236701 22849303695488 run.py:483] Algo bellman_ford step 5789 current loss 0.075210, current_train_items 185280.
I0304 19:30:54.257181 22849303695488 run.py:483] Algo bellman_ford step 5790 current loss 0.005737, current_train_items 185312.
I0304 19:30:54.274169 22849303695488 run.py:483] Algo bellman_ford step 5791 current loss 0.051210, current_train_items 185344.
I0304 19:30:54.297613 22849303695488 run.py:483] Algo bellman_ford step 5792 current loss 0.067369, current_train_items 185376.
I0304 19:30:54.329400 22849303695488 run.py:483] Algo bellman_ford step 5793 current loss 0.064221, current_train_items 185408.
I0304 19:30:54.363120 22849303695488 run.py:483] Algo bellman_ford step 5794 current loss 0.049230, current_train_items 185440.
I0304 19:30:54.383302 22849303695488 run.py:483] Algo bellman_ford step 5795 current loss 0.002718, current_train_items 185472.
I0304 19:30:54.399677 22849303695488 run.py:483] Algo bellman_ford step 5796 current loss 0.025788, current_train_items 185504.
I0304 19:30:54.424556 22849303695488 run.py:483] Algo bellman_ford step 5797 current loss 0.031221, current_train_items 185536.
I0304 19:30:54.456564 22849303695488 run.py:483] Algo bellman_ford step 5798 current loss 0.036354, current_train_items 185568.
I0304 19:30:54.491458 22849303695488 run.py:483] Algo bellman_ford step 5799 current loss 0.081974, current_train_items 185600.
I0304 19:30:54.511652 22849303695488 run.py:483] Algo bellman_ford step 5800 current loss 0.003231, current_train_items 185632.
I0304 19:30:54.519718 22849303695488 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0304 19:30:54.519825 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:54.537212 22849303695488 run.py:483] Algo bellman_ford step 5801 current loss 0.024474, current_train_items 185664.
I0304 19:30:54.561110 22849303695488 run.py:483] Algo bellman_ford step 5802 current loss 0.026345, current_train_items 185696.
I0304 19:30:54.593104 22849303695488 run.py:483] Algo bellman_ford step 5803 current loss 0.090645, current_train_items 185728.
I0304 19:30:54.629509 22849303695488 run.py:483] Algo bellman_ford step 5804 current loss 0.061193, current_train_items 185760.
I0304 19:30:54.649662 22849303695488 run.py:483] Algo bellman_ford step 5805 current loss 0.002768, current_train_items 185792.
I0304 19:30:54.666383 22849303695488 run.py:483] Algo bellman_ford step 5806 current loss 0.010371, current_train_items 185824.
I0304 19:30:54.691262 22849303695488 run.py:483] Algo bellman_ford step 5807 current loss 0.075037, current_train_items 185856.
I0304 19:30:54.723463 22849303695488 run.py:483] Algo bellman_ford step 5808 current loss 0.076453, current_train_items 185888.
I0304 19:30:54.757203 22849303695488 run.py:483] Algo bellman_ford step 5809 current loss 0.054903, current_train_items 185920.
I0304 19:30:54.777556 22849303695488 run.py:483] Algo bellman_ford step 5810 current loss 0.005049, current_train_items 185952.
I0304 19:30:54.794067 22849303695488 run.py:483] Algo bellman_ford step 5811 current loss 0.056014, current_train_items 185984.
I0304 19:30:54.818577 22849303695488 run.py:483] Algo bellman_ford step 5812 current loss 0.063229, current_train_items 186016.
I0304 19:30:54.851127 22849303695488 run.py:483] Algo bellman_ford step 5813 current loss 0.087884, current_train_items 186048.
I0304 19:30:54.886256 22849303695488 run.py:483] Algo bellman_ford step 5814 current loss 0.069780, current_train_items 186080.
I0304 19:30:54.906355 22849303695488 run.py:483] Algo bellman_ford step 5815 current loss 0.004808, current_train_items 186112.
I0304 19:30:54.923094 22849303695488 run.py:483] Algo bellman_ford step 5816 current loss 0.042646, current_train_items 186144.
I0304 19:30:54.947447 22849303695488 run.py:483] Algo bellman_ford step 5817 current loss 0.038891, current_train_items 186176.
I0304 19:30:54.978384 22849303695488 run.py:483] Algo bellman_ford step 5818 current loss 0.036101, current_train_items 186208.
I0304 19:30:55.012963 22849303695488 run.py:483] Algo bellman_ford step 5819 current loss 0.082534, current_train_items 186240.
I0304 19:30:55.032833 22849303695488 run.py:483] Algo bellman_ford step 5820 current loss 0.002433, current_train_items 186272.
I0304 19:30:55.049355 22849303695488 run.py:483] Algo bellman_ford step 5821 current loss 0.003937, current_train_items 186304.
I0304 19:30:55.073787 22849303695488 run.py:483] Algo bellman_ford step 5822 current loss 0.053281, current_train_items 186336.
I0304 19:30:55.105725 22849303695488 run.py:483] Algo bellman_ford step 5823 current loss 0.087322, current_train_items 186368.
I0304 19:30:55.139567 22849303695488 run.py:483] Algo bellman_ford step 5824 current loss 0.094083, current_train_items 186400.
I0304 19:30:55.159772 22849303695488 run.py:483] Algo bellman_ford step 5825 current loss 0.032159, current_train_items 186432.
I0304 19:30:55.176543 22849303695488 run.py:483] Algo bellman_ford step 5826 current loss 0.014451, current_train_items 186464.
I0304 19:30:55.201240 22849303695488 run.py:483] Algo bellman_ford step 5827 current loss 0.052126, current_train_items 186496.
I0304 19:30:55.233749 22849303695488 run.py:483] Algo bellman_ford step 5828 current loss 0.056169, current_train_items 186528.
I0304 19:30:55.269105 22849303695488 run.py:483] Algo bellman_ford step 5829 current loss 0.096415, current_train_items 186560.
I0304 19:30:55.289244 22849303695488 run.py:483] Algo bellman_ford step 5830 current loss 0.008686, current_train_items 186592.
I0304 19:30:55.306065 22849303695488 run.py:483] Algo bellman_ford step 5831 current loss 0.043942, current_train_items 186624.
I0304 19:30:55.329972 22849303695488 run.py:483] Algo bellman_ford step 5832 current loss 0.086269, current_train_items 186656.
I0304 19:30:55.361321 22849303695488 run.py:483] Algo bellman_ford step 5833 current loss 0.070449, current_train_items 186688.
I0304 19:30:55.391876 22849303695488 run.py:483] Algo bellman_ford step 5834 current loss 0.050709, current_train_items 186720.
I0304 19:30:55.412178 22849303695488 run.py:483] Algo bellman_ford step 5835 current loss 0.005438, current_train_items 186752.
I0304 19:30:55.428168 22849303695488 run.py:483] Algo bellman_ford step 5836 current loss 0.010339, current_train_items 186784.
I0304 19:30:55.453928 22849303695488 run.py:483] Algo bellman_ford step 5837 current loss 0.146445, current_train_items 186816.
I0304 19:30:55.483865 22849303695488 run.py:483] Algo bellman_ford step 5838 current loss 0.081255, current_train_items 186848.
I0304 19:30:55.519351 22849303695488 run.py:483] Algo bellman_ford step 5839 current loss 0.170267, current_train_items 186880.
I0304 19:30:55.539367 22849303695488 run.py:483] Algo bellman_ford step 5840 current loss 0.008281, current_train_items 186912.
I0304 19:30:55.556172 22849303695488 run.py:483] Algo bellman_ford step 5841 current loss 0.018319, current_train_items 186944.
I0304 19:30:55.579764 22849303695488 run.py:483] Algo bellman_ford step 5842 current loss 0.069938, current_train_items 186976.
I0304 19:30:55.610160 22849303695488 run.py:483] Algo bellman_ford step 5843 current loss 0.053820, current_train_items 187008.
I0304 19:30:55.643522 22849303695488 run.py:483] Algo bellman_ford step 5844 current loss 0.072601, current_train_items 187040.
I0304 19:30:55.663643 22849303695488 run.py:483] Algo bellman_ford step 5845 current loss 0.009041, current_train_items 187072.
I0304 19:30:55.680409 22849303695488 run.py:483] Algo bellman_ford step 5846 current loss 0.012387, current_train_items 187104.
I0304 19:30:55.705679 22849303695488 run.py:483] Algo bellman_ford step 5847 current loss 0.030953, current_train_items 187136.
I0304 19:30:55.736132 22849303695488 run.py:483] Algo bellman_ford step 5848 current loss 0.053281, current_train_items 187168.
I0304 19:30:55.770556 22849303695488 run.py:483] Algo bellman_ford step 5849 current loss 0.093830, current_train_items 187200.
I0304 19:30:55.790333 22849303695488 run.py:483] Algo bellman_ford step 5850 current loss 0.007989, current_train_items 187232.
I0304 19:30:55.798689 22849303695488 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0304 19:30:55.798797 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:55.816077 22849303695488 run.py:483] Algo bellman_ford step 5851 current loss 0.020820, current_train_items 187264.
I0304 19:30:55.840559 22849303695488 run.py:483] Algo bellman_ford step 5852 current loss 0.079335, current_train_items 187296.
I0304 19:30:55.871601 22849303695488 run.py:483] Algo bellman_ford step 5853 current loss 0.033248, current_train_items 187328.
I0304 19:30:55.906339 22849303695488 run.py:483] Algo bellman_ford step 5854 current loss 0.106173, current_train_items 187360.
I0304 19:30:55.926676 22849303695488 run.py:483] Algo bellman_ford step 5855 current loss 0.003195, current_train_items 187392.
I0304 19:30:55.943103 22849303695488 run.py:483] Algo bellman_ford step 5856 current loss 0.018962, current_train_items 187424.
I0304 19:30:55.968061 22849303695488 run.py:483] Algo bellman_ford step 5857 current loss 0.060405, current_train_items 187456.
I0304 19:30:55.998800 22849303695488 run.py:483] Algo bellman_ford step 5858 current loss 0.035177, current_train_items 187488.
I0304 19:30:56.033358 22849303695488 run.py:483] Algo bellman_ford step 5859 current loss 0.052096, current_train_items 187520.
I0304 19:30:56.053815 22849303695488 run.py:483] Algo bellman_ford step 5860 current loss 0.001978, current_train_items 187552.
I0304 19:30:56.070743 22849303695488 run.py:483] Algo bellman_ford step 5861 current loss 0.011701, current_train_items 187584.
I0304 19:30:56.093946 22849303695488 run.py:483] Algo bellman_ford step 5862 current loss 0.049128, current_train_items 187616.
I0304 19:30:56.126023 22849303695488 run.py:483] Algo bellman_ford step 5863 current loss 0.040679, current_train_items 187648.
I0304 19:30:56.161072 22849303695488 run.py:483] Algo bellman_ford step 5864 current loss 0.081870, current_train_items 187680.
I0304 19:30:56.180935 22849303695488 run.py:483] Algo bellman_ford step 5865 current loss 0.003270, current_train_items 187712.
I0304 19:30:56.198272 22849303695488 run.py:483] Algo bellman_ford step 5866 current loss 0.029982, current_train_items 187744.
I0304 19:30:56.222675 22849303695488 run.py:483] Algo bellman_ford step 5867 current loss 0.040240, current_train_items 187776.
I0304 19:30:56.253829 22849303695488 run.py:483] Algo bellman_ford step 5868 current loss 0.140366, current_train_items 187808.
I0304 19:30:56.289493 22849303695488 run.py:483] Algo bellman_ford step 5869 current loss 0.100646, current_train_items 187840.
I0304 19:30:56.310097 22849303695488 run.py:483] Algo bellman_ford step 5870 current loss 0.001950, current_train_items 187872.
I0304 19:30:56.326815 22849303695488 run.py:483] Algo bellman_ford step 5871 current loss 0.021522, current_train_items 187904.
I0304 19:30:56.349643 22849303695488 run.py:483] Algo bellman_ford step 5872 current loss 0.043645, current_train_items 187936.
I0304 19:30:56.380719 22849303695488 run.py:483] Algo bellman_ford step 5873 current loss 0.057965, current_train_items 187968.
I0304 19:30:56.414973 22849303695488 run.py:483] Algo bellman_ford step 5874 current loss 0.059456, current_train_items 188000.
I0304 19:30:56.435141 22849303695488 run.py:483] Algo bellman_ford step 5875 current loss 0.002546, current_train_items 188032.
I0304 19:30:56.451775 22849303695488 run.py:483] Algo bellman_ford step 5876 current loss 0.030586, current_train_items 188064.
I0304 19:30:56.475131 22849303695488 run.py:483] Algo bellman_ford step 5877 current loss 0.034972, current_train_items 188096.
I0304 19:30:56.506273 22849303695488 run.py:483] Algo bellman_ford step 5878 current loss 0.055825, current_train_items 188128.
I0304 19:30:56.541146 22849303695488 run.py:483] Algo bellman_ford step 5879 current loss 0.052558, current_train_items 188160.
I0304 19:30:56.561398 22849303695488 run.py:483] Algo bellman_ford step 5880 current loss 0.015475, current_train_items 188192.
I0304 19:30:56.577960 22849303695488 run.py:483] Algo bellman_ford step 5881 current loss 0.015976, current_train_items 188224.
I0304 19:30:56.602440 22849303695488 run.py:483] Algo bellman_ford step 5882 current loss 0.101712, current_train_items 188256.
I0304 19:30:56.633933 22849303695488 run.py:483] Algo bellman_ford step 5883 current loss 0.050476, current_train_items 188288.
I0304 19:30:56.669330 22849303695488 run.py:483] Algo bellman_ford step 5884 current loss 0.119686, current_train_items 188320.
I0304 19:30:56.689807 22849303695488 run.py:483] Algo bellman_ford step 5885 current loss 0.003581, current_train_items 188352.
I0304 19:30:56.706363 22849303695488 run.py:483] Algo bellman_ford step 5886 current loss 0.030135, current_train_items 188384.
I0304 19:30:56.729974 22849303695488 run.py:483] Algo bellman_ford step 5887 current loss 0.037616, current_train_items 188416.
I0304 19:30:56.761042 22849303695488 run.py:483] Algo bellman_ford step 5888 current loss 0.058638, current_train_items 188448.
I0304 19:30:56.793843 22849303695488 run.py:483] Algo bellman_ford step 5889 current loss 0.115102, current_train_items 188480.
I0304 19:30:56.814147 22849303695488 run.py:483] Algo bellman_ford step 5890 current loss 0.004556, current_train_items 188512.
I0304 19:30:56.830787 22849303695488 run.py:483] Algo bellman_ford step 5891 current loss 0.036973, current_train_items 188544.
I0304 19:30:56.854714 22849303695488 run.py:483] Algo bellman_ford step 5892 current loss 0.072247, current_train_items 188576.
I0304 19:30:56.885197 22849303695488 run.py:483] Algo bellman_ford step 5893 current loss 0.019912, current_train_items 188608.
I0304 19:30:56.916859 22849303695488 run.py:483] Algo bellman_ford step 5894 current loss 0.044224, current_train_items 188640.
I0304 19:30:56.937036 22849303695488 run.py:483] Algo bellman_ford step 5895 current loss 0.005204, current_train_items 188672.
I0304 19:30:56.953738 22849303695488 run.py:483] Algo bellman_ford step 5896 current loss 0.054150, current_train_items 188704.
I0304 19:30:56.978339 22849303695488 run.py:483] Algo bellman_ford step 5897 current loss 0.072789, current_train_items 188736.
I0304 19:30:57.010470 22849303695488 run.py:483] Algo bellman_ford step 5898 current loss 0.080029, current_train_items 188768.
I0304 19:30:57.043947 22849303695488 run.py:483] Algo bellman_ford step 5899 current loss 0.047452, current_train_items 188800.
I0304 19:30:57.064332 22849303695488 run.py:483] Algo bellman_ford step 5900 current loss 0.003779, current_train_items 188832.
I0304 19:30:57.072743 22849303695488 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0304 19:30:57.072851 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:57.089752 22849303695488 run.py:483] Algo bellman_ford step 5901 current loss 0.022457, current_train_items 188864.
I0304 19:30:57.115236 22849303695488 run.py:483] Algo bellman_ford step 5902 current loss 0.021536, current_train_items 188896.
I0304 19:30:57.148016 22849303695488 run.py:483] Algo bellman_ford step 5903 current loss 0.048878, current_train_items 188928.
I0304 19:30:57.180927 22849303695488 run.py:483] Algo bellman_ford step 5904 current loss 0.041189, current_train_items 188960.
I0304 19:30:57.201422 22849303695488 run.py:483] Algo bellman_ford step 5905 current loss 0.005613, current_train_items 188992.
I0304 19:30:57.218161 22849303695488 run.py:483] Algo bellman_ford step 5906 current loss 0.026326, current_train_items 189024.
I0304 19:30:57.242177 22849303695488 run.py:483] Algo bellman_ford step 5907 current loss 0.056356, current_train_items 189056.
I0304 19:30:57.274166 22849303695488 run.py:483] Algo bellman_ford step 5908 current loss 0.047025, current_train_items 189088.
I0304 19:30:57.307366 22849303695488 run.py:483] Algo bellman_ford step 5909 current loss 0.128937, current_train_items 189120.
I0304 19:30:57.327701 22849303695488 run.py:483] Algo bellman_ford step 5910 current loss 0.004294, current_train_items 189152.
I0304 19:30:57.344463 22849303695488 run.py:483] Algo bellman_ford step 5911 current loss 0.068979, current_train_items 189184.
I0304 19:30:57.369242 22849303695488 run.py:483] Algo bellman_ford step 5912 current loss 0.059861, current_train_items 189216.
I0304 19:30:57.400402 22849303695488 run.py:483] Algo bellman_ford step 5913 current loss 0.120276, current_train_items 189248.
I0304 19:30:57.433650 22849303695488 run.py:483] Algo bellman_ford step 5914 current loss 0.090322, current_train_items 189280.
I0304 19:30:57.453891 22849303695488 run.py:483] Algo bellman_ford step 5915 current loss 0.008727, current_train_items 189312.
I0304 19:30:57.470706 22849303695488 run.py:483] Algo bellman_ford step 5916 current loss 0.031262, current_train_items 189344.
I0304 19:30:57.494782 22849303695488 run.py:483] Algo bellman_ford step 5917 current loss 0.085745, current_train_items 189376.
I0304 19:30:57.525807 22849303695488 run.py:483] Algo bellman_ford step 5918 current loss 0.048777, current_train_items 189408.
I0304 19:30:57.560626 22849303695488 run.py:483] Algo bellman_ford step 5919 current loss 0.072279, current_train_items 189440.
I0304 19:30:57.581031 22849303695488 run.py:483] Algo bellman_ford step 5920 current loss 0.005759, current_train_items 189472.
I0304 19:30:57.597602 22849303695488 run.py:483] Algo bellman_ford step 5921 current loss 0.026516, current_train_items 189504.
I0304 19:30:57.621382 22849303695488 run.py:483] Algo bellman_ford step 5922 current loss 0.059697, current_train_items 189536.
I0304 19:30:57.651771 22849303695488 run.py:483] Algo bellman_ford step 5923 current loss 0.048962, current_train_items 189568.
I0304 19:30:57.686069 22849303695488 run.py:483] Algo bellman_ford step 5924 current loss 0.054496, current_train_items 189600.
I0304 19:30:57.706028 22849303695488 run.py:483] Algo bellman_ford step 5925 current loss 0.004355, current_train_items 189632.
I0304 19:30:57.722284 22849303695488 run.py:483] Algo bellman_ford step 5926 current loss 0.020815, current_train_items 189664.
I0304 19:30:57.745609 22849303695488 run.py:483] Algo bellman_ford step 5927 current loss 0.071270, current_train_items 189696.
I0304 19:30:57.777845 22849303695488 run.py:483] Algo bellman_ford step 5928 current loss 0.101403, current_train_items 189728.
I0304 19:30:57.811064 22849303695488 run.py:483] Algo bellman_ford step 5929 current loss 0.085519, current_train_items 189760.
I0304 19:30:57.831336 22849303695488 run.py:483] Algo bellman_ford step 5930 current loss 0.005130, current_train_items 189792.
I0304 19:30:57.847993 22849303695488 run.py:483] Algo bellman_ford step 5931 current loss 0.024271, current_train_items 189824.
I0304 19:30:57.872017 22849303695488 run.py:483] Algo bellman_ford step 5932 current loss 0.054859, current_train_items 189856.
I0304 19:30:57.902804 22849303695488 run.py:483] Algo bellman_ford step 5933 current loss 0.042950, current_train_items 189888.
I0304 19:30:57.937924 22849303695488 run.py:483] Algo bellman_ford step 5934 current loss 0.079837, current_train_items 189920.
I0304 19:30:57.958107 22849303695488 run.py:483] Algo bellman_ford step 5935 current loss 0.005947, current_train_items 189952.
I0304 19:30:57.974205 22849303695488 run.py:483] Algo bellman_ford step 5936 current loss 0.031712, current_train_items 189984.
I0304 19:30:57.998607 22849303695488 run.py:483] Algo bellman_ford step 5937 current loss 0.090700, current_train_items 190016.
I0304 19:30:58.030958 22849303695488 run.py:483] Algo bellman_ford step 5938 current loss 0.080303, current_train_items 190048.
I0304 19:30:58.063549 22849303695488 run.py:483] Algo bellman_ford step 5939 current loss 0.102156, current_train_items 190080.
I0304 19:30:58.084150 22849303695488 run.py:483] Algo bellman_ford step 5940 current loss 0.006312, current_train_items 190112.
I0304 19:30:58.100447 22849303695488 run.py:483] Algo bellman_ford step 5941 current loss 0.033944, current_train_items 190144.
I0304 19:30:58.125256 22849303695488 run.py:483] Algo bellman_ford step 5942 current loss 0.034033, current_train_items 190176.
I0304 19:30:58.156968 22849303695488 run.py:483] Algo bellman_ford step 5943 current loss 0.053680, current_train_items 190208.
I0304 19:30:58.191828 22849303695488 run.py:483] Algo bellman_ford step 5944 current loss 0.065025, current_train_items 190240.
I0304 19:30:58.212081 22849303695488 run.py:483] Algo bellman_ford step 5945 current loss 0.011789, current_train_items 190272.
I0304 19:30:58.228860 22849303695488 run.py:483] Algo bellman_ford step 5946 current loss 0.028536, current_train_items 190304.
I0304 19:30:58.253414 22849303695488 run.py:483] Algo bellman_ford step 5947 current loss 0.038818, current_train_items 190336.
I0304 19:30:58.283738 22849303695488 run.py:483] Algo bellman_ford step 5948 current loss 0.043003, current_train_items 190368.
I0304 19:30:58.317854 22849303695488 run.py:483] Algo bellman_ford step 5949 current loss 0.088734, current_train_items 190400.
I0304 19:30:58.337688 22849303695488 run.py:483] Algo bellman_ford step 5950 current loss 0.005871, current_train_items 190432.
I0304 19:30:58.346296 22849303695488 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0304 19:30:58.346407 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:58.363578 22849303695488 run.py:483] Algo bellman_ford step 5951 current loss 0.017556, current_train_items 190464.
I0304 19:30:58.388519 22849303695488 run.py:483] Algo bellman_ford step 5952 current loss 0.074224, current_train_items 190496.
I0304 19:30:58.420384 22849303695488 run.py:483] Algo bellman_ford step 5953 current loss 0.059110, current_train_items 190528.
I0304 19:30:58.454581 22849303695488 run.py:483] Algo bellman_ford step 5954 current loss 0.124205, current_train_items 190560.
I0304 19:30:58.475025 22849303695488 run.py:483] Algo bellman_ford step 5955 current loss 0.004381, current_train_items 190592.
I0304 19:30:58.491671 22849303695488 run.py:483] Algo bellman_ford step 5956 current loss 0.018954, current_train_items 190624.
I0304 19:30:58.514861 22849303695488 run.py:483] Algo bellman_ford step 5957 current loss 0.033665, current_train_items 190656.
I0304 19:30:58.545967 22849303695488 run.py:483] Algo bellman_ford step 5958 current loss 0.025896, current_train_items 190688.
I0304 19:30:58.579911 22849303695488 run.py:483] Algo bellman_ford step 5959 current loss 0.061323, current_train_items 190720.
I0304 19:30:58.600497 22849303695488 run.py:483] Algo bellman_ford step 5960 current loss 0.003263, current_train_items 190752.
I0304 19:30:58.617191 22849303695488 run.py:483] Algo bellman_ford step 5961 current loss 0.041471, current_train_items 190784.
I0304 19:30:58.641750 22849303695488 run.py:483] Algo bellman_ford step 5962 current loss 0.048116, current_train_items 190816.
I0304 19:30:58.671614 22849303695488 run.py:483] Algo bellman_ford step 5963 current loss 0.054438, current_train_items 190848.
I0304 19:30:58.705112 22849303695488 run.py:483] Algo bellman_ford step 5964 current loss 0.166840, current_train_items 190880.
I0304 19:30:58.725049 22849303695488 run.py:483] Algo bellman_ford step 5965 current loss 0.077912, current_train_items 190912.
I0304 19:30:58.741571 22849303695488 run.py:483] Algo bellman_ford step 5966 current loss 0.043684, current_train_items 190944.
I0304 19:30:58.766136 22849303695488 run.py:483] Algo bellman_ford step 5967 current loss 0.033687, current_train_items 190976.
I0304 19:30:58.797321 22849303695488 run.py:483] Algo bellman_ford step 5968 current loss 0.069916, current_train_items 191008.
I0304 19:30:58.832018 22849303695488 run.py:483] Algo bellman_ford step 5969 current loss 0.100250, current_train_items 191040.
I0304 19:30:58.852454 22849303695488 run.py:483] Algo bellman_ford step 5970 current loss 0.006070, current_train_items 191072.
I0304 19:30:58.869036 22849303695488 run.py:483] Algo bellman_ford step 5971 current loss 0.013957, current_train_items 191104.
I0304 19:30:58.892661 22849303695488 run.py:483] Algo bellman_ford step 5972 current loss 0.022229, current_train_items 191136.
I0304 19:30:58.924476 22849303695488 run.py:483] Algo bellman_ford step 5973 current loss 0.050478, current_train_items 191168.
I0304 19:30:58.958091 22849303695488 run.py:483] Algo bellman_ford step 5974 current loss 0.062175, current_train_items 191200.
I0304 19:30:58.978292 22849303695488 run.py:483] Algo bellman_ford step 5975 current loss 0.008794, current_train_items 191232.
I0304 19:30:58.994717 22849303695488 run.py:483] Algo bellman_ford step 5976 current loss 0.003876, current_train_items 191264.
I0304 19:30:59.017993 22849303695488 run.py:483] Algo bellman_ford step 5977 current loss 0.044505, current_train_items 191296.
I0304 19:30:59.049787 22849303695488 run.py:483] Algo bellman_ford step 5978 current loss 0.086912, current_train_items 191328.
I0304 19:30:59.082416 22849303695488 run.py:483] Algo bellman_ford step 5979 current loss 0.042891, current_train_items 191360.
I0304 19:30:59.102100 22849303695488 run.py:483] Algo bellman_ford step 5980 current loss 0.002302, current_train_items 191392.
I0304 19:30:59.119240 22849303695488 run.py:483] Algo bellman_ford step 5981 current loss 0.025909, current_train_items 191424.
I0304 19:30:59.142726 22849303695488 run.py:483] Algo bellman_ford step 5982 current loss 0.022146, current_train_items 191456.
I0304 19:30:59.173814 22849303695488 run.py:483] Algo bellman_ford step 5983 current loss 0.034584, current_train_items 191488.
I0304 19:30:59.207196 22849303695488 run.py:483] Algo bellman_ford step 5984 current loss 0.049356, current_train_items 191520.
I0304 19:30:59.227350 22849303695488 run.py:483] Algo bellman_ford step 5985 current loss 0.009703, current_train_items 191552.
I0304 19:30:59.244504 22849303695488 run.py:483] Algo bellman_ford step 5986 current loss 0.016694, current_train_items 191584.
I0304 19:30:59.268171 22849303695488 run.py:483] Algo bellman_ford step 5987 current loss 0.042664, current_train_items 191616.
I0304 19:30:59.298416 22849303695488 run.py:483] Algo bellman_ford step 5988 current loss 0.049358, current_train_items 191648.
I0304 19:30:59.331748 22849303695488 run.py:483] Algo bellman_ford step 5989 current loss 0.050216, current_train_items 191680.
I0304 19:30:59.351882 22849303695488 run.py:483] Algo bellman_ford step 5990 current loss 0.003666, current_train_items 191712.
I0304 19:30:59.367992 22849303695488 run.py:483] Algo bellman_ford step 5991 current loss 0.024778, current_train_items 191744.
I0304 19:30:59.392246 22849303695488 run.py:483] Algo bellman_ford step 5992 current loss 0.086481, current_train_items 191776.
I0304 19:30:59.425299 22849303695488 run.py:483] Algo bellman_ford step 5993 current loss 0.057489, current_train_items 191808.
I0304 19:30:59.459293 22849303695488 run.py:483] Algo bellman_ford step 5994 current loss 0.058286, current_train_items 191840.
I0304 19:30:59.479049 22849303695488 run.py:483] Algo bellman_ford step 5995 current loss 0.004611, current_train_items 191872.
I0304 19:30:59.495803 22849303695488 run.py:483] Algo bellman_ford step 5996 current loss 0.011925, current_train_items 191904.
I0304 19:30:59.520083 22849303695488 run.py:483] Algo bellman_ford step 5997 current loss 0.042461, current_train_items 191936.
I0304 19:30:59.551071 22849303695488 run.py:483] Algo bellman_ford step 5998 current loss 0.031963, current_train_items 191968.
I0304 19:30:59.583971 22849303695488 run.py:483] Algo bellman_ford step 5999 current loss 0.030731, current_train_items 192000.
I0304 19:30:59.604498 22849303695488 run.py:483] Algo bellman_ford step 6000 current loss 0.011507, current_train_items 192032.
I0304 19:30:59.612618 22849303695488 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0304 19:30:59.612726 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:59.629911 22849303695488 run.py:483] Algo bellman_ford step 6001 current loss 0.033473, current_train_items 192064.
I0304 19:30:59.654513 22849303695488 run.py:483] Algo bellman_ford step 6002 current loss 0.069954, current_train_items 192096.
I0304 19:30:59.685166 22849303695488 run.py:483] Algo bellman_ford step 6003 current loss 0.102848, current_train_items 192128.
I0304 19:30:59.718781 22849303695488 run.py:483] Algo bellman_ford step 6004 current loss 0.055345, current_train_items 192160.
I0304 19:30:59.738926 22849303695488 run.py:483] Algo bellman_ford step 6005 current loss 0.003766, current_train_items 192192.
I0304 19:30:59.755155 22849303695488 run.py:483] Algo bellman_ford step 6006 current loss 0.013456, current_train_items 192224.
I0304 19:30:59.780180 22849303695488 run.py:483] Algo bellman_ford step 6007 current loss 0.111152, current_train_items 192256.
I0304 19:30:59.813525 22849303695488 run.py:483] Algo bellman_ford step 6008 current loss 0.113625, current_train_items 192288.
I0304 19:30:59.849064 22849303695488 run.py:483] Algo bellman_ford step 6009 current loss 0.063681, current_train_items 192320.
I0304 19:30:59.869311 22849303695488 run.py:483] Algo bellman_ford step 6010 current loss 0.007005, current_train_items 192352.
I0304 19:30:59.885905 22849303695488 run.py:483] Algo bellman_ford step 6011 current loss 0.039669, current_train_items 192384.
I0304 19:30:59.910210 22849303695488 run.py:483] Algo bellman_ford step 6012 current loss 0.080594, current_train_items 192416.
I0304 19:30:59.940869 22849303695488 run.py:483] Algo bellman_ford step 6013 current loss 0.096020, current_train_items 192448.
I0304 19:30:59.976568 22849303695488 run.py:483] Algo bellman_ford step 6014 current loss 0.143854, current_train_items 192480.
I0304 19:30:59.996435 22849303695488 run.py:483] Algo bellman_ford step 6015 current loss 0.007035, current_train_items 192512.
I0304 19:31:00.013282 22849303695488 run.py:483] Algo bellman_ford step 6016 current loss 0.015752, current_train_items 192544.
I0304 19:31:00.037199 22849303695488 run.py:483] Algo bellman_ford step 6017 current loss 0.022570, current_train_items 192576.
I0304 19:31:00.069079 22849303695488 run.py:483] Algo bellman_ford step 6018 current loss 0.084985, current_train_items 192608.
I0304 19:31:00.103010 22849303695488 run.py:483] Algo bellman_ford step 6019 current loss 0.075883, current_train_items 192640.
I0304 19:31:00.123081 22849303695488 run.py:483] Algo bellman_ford step 6020 current loss 0.005078, current_train_items 192672.
I0304 19:31:00.139947 22849303695488 run.py:483] Algo bellman_ford step 6021 current loss 0.017600, current_train_items 192704.
I0304 19:31:00.164329 22849303695488 run.py:483] Algo bellman_ford step 6022 current loss 0.029921, current_train_items 192736.
I0304 19:31:00.197431 22849303695488 run.py:483] Algo bellman_ford step 6023 current loss 0.052425, current_train_items 192768.
I0304 19:31:00.231676 22849303695488 run.py:483] Algo bellman_ford step 6024 current loss 0.098137, current_train_items 192800.
I0304 19:31:00.251507 22849303695488 run.py:483] Algo bellman_ford step 6025 current loss 0.003566, current_train_items 192832.
I0304 19:31:00.268291 22849303695488 run.py:483] Algo bellman_ford step 6026 current loss 0.028710, current_train_items 192864.
I0304 19:31:00.292342 22849303695488 run.py:483] Algo bellman_ford step 6027 current loss 0.032502, current_train_items 192896.
I0304 19:31:00.323775 22849303695488 run.py:483] Algo bellman_ford step 6028 current loss 0.101626, current_train_items 192928.
I0304 19:31:00.357632 22849303695488 run.py:483] Algo bellman_ford step 6029 current loss 0.144999, current_train_items 192960.
I0304 19:31:00.377350 22849303695488 run.py:483] Algo bellman_ford step 6030 current loss 0.003122, current_train_items 192992.
I0304 19:31:00.393772 22849303695488 run.py:483] Algo bellman_ford step 6031 current loss 0.020818, current_train_items 193024.
I0304 19:31:00.417781 22849303695488 run.py:483] Algo bellman_ford step 6032 current loss 0.027755, current_train_items 193056.
I0304 19:31:00.451223 22849303695488 run.py:483] Algo bellman_ford step 6033 current loss 0.089023, current_train_items 193088.
I0304 19:31:00.484921 22849303695488 run.py:483] Algo bellman_ford step 6034 current loss 0.063572, current_train_items 193120.
I0304 19:31:00.504938 22849303695488 run.py:483] Algo bellman_ford step 6035 current loss 0.002427, current_train_items 193152.
I0304 19:31:00.521395 22849303695488 run.py:483] Algo bellman_ford step 6036 current loss 0.016914, current_train_items 193184.
I0304 19:31:00.545733 22849303695488 run.py:483] Algo bellman_ford step 6037 current loss 0.024568, current_train_items 193216.
I0304 19:31:00.577758 22849303695488 run.py:483] Algo bellman_ford step 6038 current loss 0.101182, current_train_items 193248.
I0304 19:31:00.610541 22849303695488 run.py:483] Algo bellman_ford step 6039 current loss 0.072846, current_train_items 193280.
I0304 19:31:00.630288 22849303695488 run.py:483] Algo bellman_ford step 6040 current loss 0.002283, current_train_items 193312.
I0304 19:31:00.646957 22849303695488 run.py:483] Algo bellman_ford step 6041 current loss 0.045037, current_train_items 193344.
I0304 19:31:00.672034 22849303695488 run.py:483] Algo bellman_ford step 6042 current loss 0.065638, current_train_items 193376.
I0304 19:31:00.703322 22849303695488 run.py:483] Algo bellman_ford step 6043 current loss 0.089757, current_train_items 193408.
I0304 19:31:00.740011 22849303695488 run.py:483] Algo bellman_ford step 6044 current loss 0.084290, current_train_items 193440.
I0304 19:31:00.759954 22849303695488 run.py:483] Algo bellman_ford step 6045 current loss 0.003179, current_train_items 193472.
I0304 19:31:00.777018 22849303695488 run.py:483] Algo bellman_ford step 6046 current loss 0.024014, current_train_items 193504.
I0304 19:31:00.800953 22849303695488 run.py:483] Algo bellman_ford step 6047 current loss 0.114360, current_train_items 193536.
I0304 19:31:00.833581 22849303695488 run.py:483] Algo bellman_ford step 6048 current loss 0.075304, current_train_items 193568.
I0304 19:31:00.868597 22849303695488 run.py:483] Algo bellman_ford step 6049 current loss 0.056370, current_train_items 193600.
I0304 19:31:00.888396 22849303695488 run.py:483] Algo bellman_ford step 6050 current loss 0.005616, current_train_items 193632.
I0304 19:31:00.896692 22849303695488 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0304 19:31:00.896834 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:31:00.914199 22849303695488 run.py:483] Algo bellman_ford step 6051 current loss 0.025503, current_train_items 193664.
I0304 19:31:00.939993 22849303695488 run.py:483] Algo bellman_ford step 6052 current loss 0.156650, current_train_items 193696.
I0304 19:31:00.971864 22849303695488 run.py:483] Algo bellman_ford step 6053 current loss 0.073281, current_train_items 193728.
I0304 19:31:01.007662 22849303695488 run.py:483] Algo bellman_ford step 6054 current loss 0.068787, current_train_items 193760.
I0304 19:31:01.028145 22849303695488 run.py:483] Algo bellman_ford step 6055 current loss 0.008439, current_train_items 193792.
I0304 19:31:01.044156 22849303695488 run.py:483] Algo bellman_ford step 6056 current loss 0.047226, current_train_items 193824.
I0304 19:31:01.067934 22849303695488 run.py:483] Algo bellman_ford step 6057 current loss 0.090233, current_train_items 193856.
I0304 19:31:01.099518 22849303695488 run.py:483] Algo bellman_ford step 6058 current loss 0.090488, current_train_items 193888.
I0304 19:31:01.131464 22849303695488 run.py:483] Algo bellman_ford step 6059 current loss 0.070472, current_train_items 193920.
I0304 19:31:01.152484 22849303695488 run.py:483] Algo bellman_ford step 6060 current loss 0.005427, current_train_items 193952.
I0304 19:31:01.169243 22849303695488 run.py:483] Algo bellman_ford step 6061 current loss 0.018500, current_train_items 193984.
I0304 19:31:01.191758 22849303695488 run.py:483] Algo bellman_ford step 6062 current loss 0.041366, current_train_items 194016.
I0304 19:31:01.222186 22849303695488 run.py:483] Algo bellman_ford step 6063 current loss 0.068401, current_train_items 194048.
I0304 19:31:01.257086 22849303695488 run.py:483] Algo bellman_ford step 6064 current loss 0.092114, current_train_items 194080.
I0304 19:31:01.277472 22849303695488 run.py:483] Algo bellman_ford step 6065 current loss 0.003477, current_train_items 194112.
I0304 19:31:01.294129 22849303695488 run.py:483] Algo bellman_ford step 6066 current loss 0.031591, current_train_items 194144.
I0304 19:31:01.319001 22849303695488 run.py:483] Algo bellman_ford step 6067 current loss 0.047107, current_train_items 194176.
I0304 19:31:01.351341 22849303695488 run.py:483] Algo bellman_ford step 6068 current loss 0.059168, current_train_items 194208.
I0304 19:31:01.384910 22849303695488 run.py:483] Algo bellman_ford step 6069 current loss 0.081839, current_train_items 194240.
I0304 19:31:01.405713 22849303695488 run.py:483] Algo bellman_ford step 6070 current loss 0.004148, current_train_items 194272.
I0304 19:31:01.422163 22849303695488 run.py:483] Algo bellman_ford step 6071 current loss 0.022310, current_train_items 194304.
I0304 19:31:01.446498 22849303695488 run.py:483] Algo bellman_ford step 6072 current loss 0.040215, current_train_items 194336.
I0304 19:31:01.477321 22849303695488 run.py:483] Algo bellman_ford step 6073 current loss 0.101650, current_train_items 194368.
I0304 19:31:01.511997 22849303695488 run.py:483] Algo bellman_ford step 6074 current loss 0.059353, current_train_items 194400.
I0304 19:31:01.532702 22849303695488 run.py:483] Algo bellman_ford step 6075 current loss 0.006170, current_train_items 194432.
I0304 19:31:01.549371 22849303695488 run.py:483] Algo bellman_ford step 6076 current loss 0.029516, current_train_items 194464.
I0304 19:31:01.572497 22849303695488 run.py:483] Algo bellman_ford step 6077 current loss 0.052179, current_train_items 194496.
I0304 19:31:01.603702 22849303695488 run.py:483] Algo bellman_ford step 6078 current loss 0.057023, current_train_items 194528.
I0304 19:31:01.638820 22849303695488 run.py:483] Algo bellman_ford step 6079 current loss 0.115576, current_train_items 194560.
I0304 19:31:01.658816 22849303695488 run.py:483] Algo bellman_ford step 6080 current loss 0.010137, current_train_items 194592.
I0304 19:31:01.675574 22849303695488 run.py:483] Algo bellman_ford step 6081 current loss 0.015350, current_train_items 194624.
I0304 19:31:01.699529 22849303695488 run.py:483] Algo bellman_ford step 6082 current loss 0.029485, current_train_items 194656.
I0304 19:31:01.732653 22849303695488 run.py:483] Algo bellman_ford step 6083 current loss 0.047456, current_train_items 194688.
I0304 19:31:01.765744 22849303695488 run.py:483] Algo bellman_ford step 6084 current loss 0.040834, current_train_items 194720.
I0304 19:31:01.786320 22849303695488 run.py:483] Algo bellman_ford step 6085 current loss 0.011874, current_train_items 194752.
I0304 19:31:01.803144 22849303695488 run.py:483] Algo bellman_ford step 6086 current loss 0.014700, current_train_items 194784.
I0304 19:31:01.826959 22849303695488 run.py:483] Algo bellman_ford step 6087 current loss 0.024422, current_train_items 194816.
I0304 19:31:01.858156 22849303695488 run.py:483] Algo bellman_ford step 6088 current loss 0.073071, current_train_items 194848.
I0304 19:31:01.893135 22849303695488 run.py:483] Algo bellman_ford step 6089 current loss 0.105139, current_train_items 194880.
I0304 19:31:01.913758 22849303695488 run.py:483] Algo bellman_ford step 6090 current loss 0.012952, current_train_items 194912.
I0304 19:31:01.930380 22849303695488 run.py:483] Algo bellman_ford step 6091 current loss 0.010829, current_train_items 194944.
I0304 19:31:01.954180 22849303695488 run.py:483] Algo bellman_ford step 6092 current loss 0.115285, current_train_items 194976.
I0304 19:31:01.985630 22849303695488 run.py:483] Algo bellman_ford step 6093 current loss 0.096206, current_train_items 195008.
I0304 19:31:02.019001 22849303695488 run.py:483] Algo bellman_ford step 6094 current loss 0.098567, current_train_items 195040.
I0304 19:31:02.039460 22849303695488 run.py:483] Algo bellman_ford step 6095 current loss 0.004474, current_train_items 195072.
I0304 19:31:02.056259 22849303695488 run.py:483] Algo bellman_ford step 6096 current loss 0.020599, current_train_items 195104.
I0304 19:31:02.080920 22849303695488 run.py:483] Algo bellman_ford step 6097 current loss 0.060274, current_train_items 195136.
I0304 19:31:02.111896 22849303695488 run.py:483] Algo bellman_ford step 6098 current loss 0.093579, current_train_items 195168.
I0304 19:31:02.146166 22849303695488 run.py:483] Algo bellman_ford step 6099 current loss 0.102666, current_train_items 195200.
I0304 19:31:02.166314 22849303695488 run.py:483] Algo bellman_ford step 6100 current loss 0.003115, current_train_items 195232.
I0304 19:31:02.174427 22849303695488 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0304 19:31:02.174536 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:02.191347 22849303695488 run.py:483] Algo bellman_ford step 6101 current loss 0.010653, current_train_items 195264.
I0304 19:31:02.217045 22849303695488 run.py:483] Algo bellman_ford step 6102 current loss 0.049224, current_train_items 195296.
I0304 19:31:02.250121 22849303695488 run.py:483] Algo bellman_ford step 6103 current loss 0.062777, current_train_items 195328.
I0304 19:31:02.283584 22849303695488 run.py:483] Algo bellman_ford step 6104 current loss 0.054828, current_train_items 195360.
I0304 19:31:02.303883 22849303695488 run.py:483] Algo bellman_ford step 6105 current loss 0.002093, current_train_items 195392.
I0304 19:31:02.319727 22849303695488 run.py:483] Algo bellman_ford step 6106 current loss 0.026555, current_train_items 195424.
I0304 19:31:02.344012 22849303695488 run.py:483] Algo bellman_ford step 6107 current loss 0.029292, current_train_items 195456.
I0304 19:31:02.375654 22849303695488 run.py:483] Algo bellman_ford step 6108 current loss 0.072037, current_train_items 195488.
I0304 19:31:02.409100 22849303695488 run.py:483] Algo bellman_ford step 6109 current loss 0.080769, current_train_items 195520.
I0304 19:31:02.429163 22849303695488 run.py:483] Algo bellman_ford step 6110 current loss 0.002385, current_train_items 195552.
I0304 19:31:02.445561 22849303695488 run.py:483] Algo bellman_ford step 6111 current loss 0.022844, current_train_items 195584.
I0304 19:31:02.469971 22849303695488 run.py:483] Algo bellman_ford step 6112 current loss 0.051826, current_train_items 195616.
I0304 19:31:02.501014 22849303695488 run.py:483] Algo bellman_ford step 6113 current loss 0.041389, current_train_items 195648.
I0304 19:31:02.535173 22849303695488 run.py:483] Algo bellman_ford step 6114 current loss 0.053774, current_train_items 195680.
I0304 19:31:02.555464 22849303695488 run.py:483] Algo bellman_ford step 6115 current loss 0.044119, current_train_items 195712.
I0304 19:31:02.572013 22849303695488 run.py:483] Algo bellman_ford step 6116 current loss 0.034663, current_train_items 195744.
I0304 19:31:02.596346 22849303695488 run.py:483] Algo bellman_ford step 6117 current loss 0.041022, current_train_items 195776.
I0304 19:31:02.628958 22849303695488 run.py:483] Algo bellman_ford step 6118 current loss 0.074973, current_train_items 195808.
I0304 19:31:02.662297 22849303695488 run.py:483] Algo bellman_ford step 6119 current loss 0.082059, current_train_items 195840.
I0304 19:31:02.682216 22849303695488 run.py:483] Algo bellman_ford step 6120 current loss 0.013260, current_train_items 195872.
I0304 19:31:02.698935 22849303695488 run.py:483] Algo bellman_ford step 6121 current loss 0.019719, current_train_items 195904.
I0304 19:31:02.722891 22849303695488 run.py:483] Algo bellman_ford step 6122 current loss 0.028795, current_train_items 195936.
I0304 19:31:02.755536 22849303695488 run.py:483] Algo bellman_ford step 6123 current loss 0.125482, current_train_items 195968.
I0304 19:31:02.789359 22849303695488 run.py:483] Algo bellman_ford step 6124 current loss 0.112616, current_train_items 196000.
I0304 19:31:02.809356 22849303695488 run.py:483] Algo bellman_ford step 6125 current loss 0.005709, current_train_items 196032.
I0304 19:31:02.825478 22849303695488 run.py:483] Algo bellman_ford step 6126 current loss 0.033759, current_train_items 196064.
I0304 19:31:02.851385 22849303695488 run.py:483] Algo bellman_ford step 6127 current loss 0.102346, current_train_items 196096.
I0304 19:31:02.883273 22849303695488 run.py:483] Algo bellman_ford step 6128 current loss 0.045307, current_train_items 196128.
I0304 19:31:02.916743 22849303695488 run.py:483] Algo bellman_ford step 6129 current loss 0.073440, current_train_items 196160.
I0304 19:31:02.936800 22849303695488 run.py:483] Algo bellman_ford step 6130 current loss 0.004038, current_train_items 196192.
I0304 19:31:02.953202 22849303695488 run.py:483] Algo bellman_ford step 6131 current loss 0.011563, current_train_items 196224.
I0304 19:31:02.978160 22849303695488 run.py:483] Algo bellman_ford step 6132 current loss 0.058979, current_train_items 196256.
I0304 19:31:03.008787 22849303695488 run.py:483] Algo bellman_ford step 6133 current loss 0.060168, current_train_items 196288.
I0304 19:31:03.041651 22849303695488 run.py:483] Algo bellman_ford step 6134 current loss 0.060078, current_train_items 196320.
I0304 19:31:03.061430 22849303695488 run.py:483] Algo bellman_ford step 6135 current loss 0.013366, current_train_items 196352.
I0304 19:31:03.077745 22849303695488 run.py:483] Algo bellman_ford step 6136 current loss 0.024224, current_train_items 196384.
I0304 19:31:03.102225 22849303695488 run.py:483] Algo bellman_ford step 6137 current loss 0.044684, current_train_items 196416.
I0304 19:31:03.135046 22849303695488 run.py:483] Algo bellman_ford step 6138 current loss 0.057654, current_train_items 196448.
I0304 19:31:03.169213 22849303695488 run.py:483] Algo bellman_ford step 6139 current loss 0.061266, current_train_items 196480.
I0304 19:31:03.189295 22849303695488 run.py:483] Algo bellman_ford step 6140 current loss 0.003073, current_train_items 196512.
I0304 19:31:03.205830 22849303695488 run.py:483] Algo bellman_ford step 6141 current loss 0.027544, current_train_items 196544.
I0304 19:31:03.231095 22849303695488 run.py:483] Algo bellman_ford step 6142 current loss 0.026901, current_train_items 196576.
I0304 19:31:03.263218 22849303695488 run.py:483] Algo bellman_ford step 6143 current loss 0.046291, current_train_items 196608.
I0304 19:31:03.297284 22849303695488 run.py:483] Algo bellman_ford step 6144 current loss 0.059271, current_train_items 196640.
I0304 19:31:03.317058 22849303695488 run.py:483] Algo bellman_ford step 6145 current loss 0.021919, current_train_items 196672.
I0304 19:31:03.333735 22849303695488 run.py:483] Algo bellman_ford step 6146 current loss 0.019195, current_train_items 196704.
I0304 19:31:03.357451 22849303695488 run.py:483] Algo bellman_ford step 6147 current loss 0.021420, current_train_items 196736.
I0304 19:31:03.389226 22849303695488 run.py:483] Algo bellman_ford step 6148 current loss 0.045550, current_train_items 196768.
I0304 19:31:03.424329 22849303695488 run.py:483] Algo bellman_ford step 6149 current loss 0.083373, current_train_items 196800.
I0304 19:31:03.444362 22849303695488 run.py:483] Algo bellman_ford step 6150 current loss 0.003462, current_train_items 196832.
I0304 19:31:03.452788 22849303695488 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0304 19:31:03.452897 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:03.470045 22849303695488 run.py:483] Algo bellman_ford step 6151 current loss 0.012772, current_train_items 196864.
I0304 19:31:03.494043 22849303695488 run.py:483] Algo bellman_ford step 6152 current loss 0.043859, current_train_items 196896.
I0304 19:31:03.524565 22849303695488 run.py:483] Algo bellman_ford step 6153 current loss 0.172461, current_train_items 196928.
I0304 19:31:03.558701 22849303695488 run.py:483] Algo bellman_ford step 6154 current loss 0.060018, current_train_items 196960.
I0304 19:31:03.578630 22849303695488 run.py:483] Algo bellman_ford step 6155 current loss 0.005039, current_train_items 196992.
I0304 19:31:03.594985 22849303695488 run.py:483] Algo bellman_ford step 6156 current loss 0.037082, current_train_items 197024.
I0304 19:31:03.619122 22849303695488 run.py:483] Algo bellman_ford step 6157 current loss 0.029789, current_train_items 197056.
I0304 19:31:03.650146 22849303695488 run.py:483] Algo bellman_ford step 6158 current loss 0.096050, current_train_items 197088.
I0304 19:31:03.684524 22849303695488 run.py:483] Algo bellman_ford step 6159 current loss 0.080075, current_train_items 197120.
I0304 19:31:03.705035 22849303695488 run.py:483] Algo bellman_ford step 6160 current loss 0.021271, current_train_items 197152.
I0304 19:31:03.722039 22849303695488 run.py:483] Algo bellman_ford step 6161 current loss 0.027160, current_train_items 197184.
I0304 19:31:03.745826 22849303695488 run.py:483] Algo bellman_ford step 6162 current loss 0.104364, current_train_items 197216.
I0304 19:31:03.778859 22849303695488 run.py:483] Algo bellman_ford step 6163 current loss 0.093407, current_train_items 197248.
I0304 19:31:03.813591 22849303695488 run.py:483] Algo bellman_ford step 6164 current loss 0.111454, current_train_items 197280.
I0304 19:31:03.833865 22849303695488 run.py:483] Algo bellman_ford step 6165 current loss 0.015800, current_train_items 197312.
I0304 19:31:03.850625 22849303695488 run.py:483] Algo bellman_ford step 6166 current loss 0.020569, current_train_items 197344.
I0304 19:31:03.875354 22849303695488 run.py:483] Algo bellman_ford step 6167 current loss 0.057431, current_train_items 197376.
I0304 19:31:03.908140 22849303695488 run.py:483] Algo bellman_ford step 6168 current loss 0.033394, current_train_items 197408.
I0304 19:31:03.942620 22849303695488 run.py:483] Algo bellman_ford step 6169 current loss 0.067782, current_train_items 197440.
I0304 19:31:03.963386 22849303695488 run.py:483] Algo bellman_ford step 6170 current loss 0.006351, current_train_items 197472.
I0304 19:31:03.979810 22849303695488 run.py:483] Algo bellman_ford step 6171 current loss 0.017662, current_train_items 197504.
I0304 19:31:04.004209 22849303695488 run.py:483] Algo bellman_ford step 6172 current loss 0.027576, current_train_items 197536.
I0304 19:31:04.037173 22849303695488 run.py:483] Algo bellman_ford step 6173 current loss 0.093483, current_train_items 197568.
I0304 19:31:04.071478 22849303695488 run.py:483] Algo bellman_ford step 6174 current loss 0.075572, current_train_items 197600.
I0304 19:31:04.091873 22849303695488 run.py:483] Algo bellman_ford step 6175 current loss 0.023219, current_train_items 197632.
I0304 19:31:04.108765 22849303695488 run.py:483] Algo bellman_ford step 6176 current loss 0.020251, current_train_items 197664.
I0304 19:31:04.132065 22849303695488 run.py:483] Algo bellman_ford step 6177 current loss 0.036823, current_train_items 197696.
I0304 19:31:04.164064 22849303695488 run.py:483] Algo bellman_ford step 6178 current loss 0.042323, current_train_items 197728.
I0304 19:31:04.198468 22849303695488 run.py:483] Algo bellman_ford step 6179 current loss 0.125965, current_train_items 197760.
I0304 19:31:04.218609 22849303695488 run.py:483] Algo bellman_ford step 6180 current loss 0.004897, current_train_items 197792.
I0304 19:31:04.235257 22849303695488 run.py:483] Algo bellman_ford step 6181 current loss 0.017282, current_train_items 197824.
I0304 19:31:04.259343 22849303695488 run.py:483] Algo bellman_ford step 6182 current loss 0.102561, current_train_items 197856.
I0304 19:31:04.291169 22849303695488 run.py:483] Algo bellman_ford step 6183 current loss 0.157690, current_train_items 197888.
I0304 19:31:04.327609 22849303695488 run.py:483] Algo bellman_ford step 6184 current loss 0.104381, current_train_items 197920.
I0304 19:31:04.348433 22849303695488 run.py:483] Algo bellman_ford step 6185 current loss 0.003051, current_train_items 197952.
I0304 19:31:04.365129 22849303695488 run.py:483] Algo bellman_ford step 6186 current loss 0.074262, current_train_items 197984.
I0304 19:31:04.388353 22849303695488 run.py:483] Algo bellman_ford step 6187 current loss 0.065893, current_train_items 198016.
I0304 19:31:04.419850 22849303695488 run.py:483] Algo bellman_ford step 6188 current loss 0.033007, current_train_items 198048.
I0304 19:31:04.454609 22849303695488 run.py:483] Algo bellman_ford step 6189 current loss 0.117976, current_train_items 198080.
I0304 19:31:04.475435 22849303695488 run.py:483] Algo bellman_ford step 6190 current loss 0.006039, current_train_items 198112.
I0304 19:31:04.492376 22849303695488 run.py:483] Algo bellman_ford step 6191 current loss 0.032502, current_train_items 198144.
I0304 19:31:04.516744 22849303695488 run.py:483] Algo bellman_ford step 6192 current loss 0.025898, current_train_items 198176.
I0304 19:31:04.549442 22849303695488 run.py:483] Algo bellman_ford step 6193 current loss 0.057103, current_train_items 198208.
I0304 19:31:04.582679 22849303695488 run.py:483] Algo bellman_ford step 6194 current loss 0.068586, current_train_items 198240.
I0304 19:31:04.602534 22849303695488 run.py:483] Algo bellman_ford step 6195 current loss 0.002316, current_train_items 198272.
I0304 19:31:04.618524 22849303695488 run.py:483] Algo bellman_ford step 6196 current loss 0.028457, current_train_items 198304.
I0304 19:31:04.643927 22849303695488 run.py:483] Algo bellman_ford step 6197 current loss 0.080034, current_train_items 198336.
I0304 19:31:04.674692 22849303695488 run.py:483] Algo bellman_ford step 6198 current loss 0.027575, current_train_items 198368.
I0304 19:31:04.709831 22849303695488 run.py:483] Algo bellman_ford step 6199 current loss 0.149674, current_train_items 198400.
I0304 19:31:04.730587 22849303695488 run.py:483] Algo bellman_ford step 6200 current loss 0.004947, current_train_items 198432.
I0304 19:31:04.738656 22849303695488 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0304 19:31:04.738769 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:31:04.756350 22849303695488 run.py:483] Algo bellman_ford step 6201 current loss 0.021390, current_train_items 198464.
I0304 19:31:04.781631 22849303695488 run.py:483] Algo bellman_ford step 6202 current loss 0.073622, current_train_items 198496.
I0304 19:31:04.812528 22849303695488 run.py:483] Algo bellman_ford step 6203 current loss 0.056233, current_train_items 198528.
I0304 19:31:04.846088 22849303695488 run.py:483] Algo bellman_ford step 6204 current loss 0.103068, current_train_items 198560.
I0304 19:31:04.866368 22849303695488 run.py:483] Algo bellman_ford step 6205 current loss 0.004537, current_train_items 198592.
I0304 19:31:04.881971 22849303695488 run.py:483] Algo bellman_ford step 6206 current loss 0.009906, current_train_items 198624.
I0304 19:31:04.906134 22849303695488 run.py:483] Algo bellman_ford step 6207 current loss 0.062037, current_train_items 198656.
I0304 19:31:04.937615 22849303695488 run.py:483] Algo bellman_ford step 6208 current loss 0.079723, current_train_items 198688.
I0304 19:31:04.972510 22849303695488 run.py:483] Algo bellman_ford step 6209 current loss 0.085495, current_train_items 198720.
I0304 19:31:04.992303 22849303695488 run.py:483] Algo bellman_ford step 6210 current loss 0.004659, current_train_items 198752.
I0304 19:31:05.008925 22849303695488 run.py:483] Algo bellman_ford step 6211 current loss 0.042321, current_train_items 198784.
I0304 19:31:05.032431 22849303695488 run.py:483] Algo bellman_ford step 6212 current loss 0.036165, current_train_items 198816.
I0304 19:31:05.062603 22849303695488 run.py:483] Algo bellman_ford step 6213 current loss 0.040499, current_train_items 198848.
I0304 19:31:05.098154 22849303695488 run.py:483] Algo bellman_ford step 6214 current loss 0.087855, current_train_items 198880.
I0304 19:31:05.118039 22849303695488 run.py:483] Algo bellman_ford step 6215 current loss 0.008784, current_train_items 198912.
I0304 19:31:05.134631 22849303695488 run.py:483] Algo bellman_ford step 6216 current loss 0.017528, current_train_items 198944.
I0304 19:31:05.158164 22849303695488 run.py:483] Algo bellman_ford step 6217 current loss 0.040330, current_train_items 198976.
I0304 19:31:05.189470 22849303695488 run.py:483] Algo bellman_ford step 6218 current loss 0.073984, current_train_items 199008.
I0304 19:31:05.222625 22849303695488 run.py:483] Algo bellman_ford step 6219 current loss 0.047223, current_train_items 199040.
I0304 19:31:05.242717 22849303695488 run.py:483] Algo bellman_ford step 6220 current loss 0.014060, current_train_items 199072.
I0304 19:31:05.258668 22849303695488 run.py:483] Algo bellman_ford step 6221 current loss 0.029626, current_train_items 199104.
I0304 19:31:05.283414 22849303695488 run.py:483] Algo bellman_ford step 6222 current loss 0.093517, current_train_items 199136.
I0304 19:31:05.313709 22849303695488 run.py:483] Algo bellman_ford step 6223 current loss 0.111484, current_train_items 199168.
I0304 19:31:05.348715 22849303695488 run.py:483] Algo bellman_ford step 6224 current loss 0.196062, current_train_items 199200.
I0304 19:31:05.368681 22849303695488 run.py:483] Algo bellman_ford step 6225 current loss 0.002955, current_train_items 199232.
I0304 19:31:05.385154 22849303695488 run.py:483] Algo bellman_ford step 6226 current loss 0.036496, current_train_items 199264.
I0304 19:31:05.408520 22849303695488 run.py:483] Algo bellman_ford step 6227 current loss 0.030217, current_train_items 199296.
I0304 19:31:05.438905 22849303695488 run.py:483] Algo bellman_ford step 6228 current loss 0.059309, current_train_items 199328.
I0304 19:31:05.471110 22849303695488 run.py:483] Algo bellman_ford step 6229 current loss 0.063961, current_train_items 199360.
I0304 19:31:05.490998 22849303695488 run.py:483] Algo bellman_ford step 6230 current loss 0.002717, current_train_items 199392.
I0304 19:31:05.507613 22849303695488 run.py:483] Algo bellman_ford step 6231 current loss 0.058854, current_train_items 199424.
I0304 19:31:05.532217 22849303695488 run.py:483] Algo bellman_ford step 6232 current loss 0.066108, current_train_items 199456.
I0304 19:31:05.562541 22849303695488 run.py:483] Algo bellman_ford step 6233 current loss 0.050343, current_train_items 199488.
I0304 19:31:05.596164 22849303695488 run.py:483] Algo bellman_ford step 6234 current loss 0.075347, current_train_items 199520.
I0304 19:31:05.615883 22849303695488 run.py:483] Algo bellman_ford step 6235 current loss 0.002573, current_train_items 199552.
I0304 19:31:05.632428 22849303695488 run.py:483] Algo bellman_ford step 6236 current loss 0.029683, current_train_items 199584.
I0304 19:31:05.655932 22849303695488 run.py:483] Algo bellman_ford step 6237 current loss 0.054088, current_train_items 199616.
I0304 19:31:05.687952 22849303695488 run.py:483] Algo bellman_ford step 6238 current loss 0.112543, current_train_items 199648.
I0304 19:31:05.719294 22849303695488 run.py:483] Algo bellman_ford step 6239 current loss 0.120678, current_train_items 199680.
I0304 19:31:05.739173 22849303695488 run.py:483] Algo bellman_ford step 6240 current loss 0.004626, current_train_items 199712.
I0304 19:31:05.755816 22849303695488 run.py:483] Algo bellman_ford step 6241 current loss 0.061150, current_train_items 199744.
I0304 19:31:05.780280 22849303695488 run.py:483] Algo bellman_ford step 6242 current loss 0.036755, current_train_items 199776.
I0304 19:31:05.812321 22849303695488 run.py:483] Algo bellman_ford step 6243 current loss 0.066217, current_train_items 199808.
I0304 19:31:05.845083 22849303695488 run.py:483] Algo bellman_ford step 6244 current loss 0.055965, current_train_items 199840.
I0304 19:31:05.865196 22849303695488 run.py:483] Algo bellman_ford step 6245 current loss 0.011783, current_train_items 199872.
I0304 19:31:05.881738 22849303695488 run.py:483] Algo bellman_ford step 6246 current loss 0.045662, current_train_items 199904.
I0304 19:31:05.905304 22849303695488 run.py:483] Algo bellman_ford step 6247 current loss 0.030654, current_train_items 199936.
I0304 19:31:05.938154 22849303695488 run.py:483] Algo bellman_ford step 6248 current loss 0.051755, current_train_items 199968.
I0304 19:31:05.970753 22849303695488 run.py:483] Algo bellman_ford step 6249 current loss 0.083839, current_train_items 200000.
I0304 19:31:05.990625 22849303695488 run.py:483] Algo bellman_ford step 6250 current loss 0.005741, current_train_items 200032.
I0304 19:31:05.998595 22849303695488 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0304 19:31:05.998705 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0304 19:31:06.016181 22849303695488 run.py:483] Algo bellman_ford step 6251 current loss 0.042070, current_train_items 200064.
I0304 19:31:06.040773 22849303695488 run.py:483] Algo bellman_ford step 6252 current loss 0.073655, current_train_items 200096.
I0304 19:31:06.073420 22849303695488 run.py:483] Algo bellman_ford step 6253 current loss 0.154863, current_train_items 200128.
I0304 19:31:06.106118 22849303695488 run.py:483] Algo bellman_ford step 6254 current loss 0.049978, current_train_items 200160.
I0304 19:31:06.126095 22849303695488 run.py:483] Algo bellman_ford step 6255 current loss 0.044321, current_train_items 200192.
I0304 19:31:06.142595 22849303695488 run.py:483] Algo bellman_ford step 6256 current loss 0.021360, current_train_items 200224.
I0304 19:31:06.167694 22849303695488 run.py:483] Algo bellman_ford step 6257 current loss 0.067138, current_train_items 200256.
I0304 19:31:06.198536 22849303695488 run.py:483] Algo bellman_ford step 6258 current loss 0.104012, current_train_items 200288.
I0304 19:31:06.231420 22849303695488 run.py:483] Algo bellman_ford step 6259 current loss 0.148589, current_train_items 200320.
I0304 19:31:06.252034 22849303695488 run.py:483] Algo bellman_ford step 6260 current loss 0.042994, current_train_items 200352.
I0304 19:31:06.268889 22849303695488 run.py:483] Algo bellman_ford step 6261 current loss 0.009482, current_train_items 200384.
I0304 19:31:06.292546 22849303695488 run.py:483] Algo bellman_ford step 6262 current loss 0.020874, current_train_items 200416.
I0304 19:31:06.324031 22849303695488 run.py:483] Algo bellman_ford step 6263 current loss 0.094063, current_train_items 200448.
I0304 19:31:06.358066 22849303695488 run.py:483] Algo bellman_ford step 6264 current loss 0.109327, current_train_items 200480.
I0304 19:31:06.377968 22849303695488 run.py:483] Algo bellman_ford step 6265 current loss 0.018492, current_train_items 200512.
I0304 19:31:06.395258 22849303695488 run.py:483] Algo bellman_ford step 6266 current loss 0.064804, current_train_items 200544.
I0304 19:31:06.420681 22849303695488 run.py:483] Algo bellman_ford step 6267 current loss 0.048479, current_train_items 200576.
I0304 19:31:06.452055 22849303695488 run.py:483] Algo bellman_ford step 6268 current loss 0.039572, current_train_items 200608.
I0304 19:31:06.483287 22849303695488 run.py:483] Algo bellman_ford step 6269 current loss 0.052850, current_train_items 200640.
I0304 19:31:06.503380 22849303695488 run.py:483] Algo bellman_ford step 6270 current loss 0.002453, current_train_items 200672.
I0304 19:31:06.520240 22849303695488 run.py:483] Algo bellman_ford step 6271 current loss 0.086457, current_train_items 200704.
I0304 19:31:06.544013 22849303695488 run.py:483] Algo bellman_ford step 6272 current loss 0.025350, current_train_items 200736.
I0304 19:31:06.574476 22849303695488 run.py:483] Algo bellman_ford step 6273 current loss 0.041424, current_train_items 200768.
I0304 19:31:06.609798 22849303695488 run.py:483] Algo bellman_ford step 6274 current loss 0.096540, current_train_items 200800.
I0304 19:31:06.630442 22849303695488 run.py:483] Algo bellman_ford step 6275 current loss 0.003569, current_train_items 200832.
I0304 19:31:06.647422 22849303695488 run.py:483] Algo bellman_ford step 6276 current loss 0.028758, current_train_items 200864.
I0304 19:31:06.671192 22849303695488 run.py:483] Algo bellman_ford step 6277 current loss 0.059502, current_train_items 200896.
I0304 19:31:06.701868 22849303695488 run.py:483] Algo bellman_ford step 6278 current loss 0.076052, current_train_items 200928.
I0304 19:31:06.736652 22849303695488 run.py:483] Algo bellman_ford step 6279 current loss 0.077115, current_train_items 200960.
I0304 19:31:06.756756 22849303695488 run.py:483] Algo bellman_ford step 6280 current loss 0.003954, current_train_items 200992.
I0304 19:31:06.773216 22849303695488 run.py:483] Algo bellman_ford step 6281 current loss 0.035782, current_train_items 201024.
I0304 19:31:06.796998 22849303695488 run.py:483] Algo bellman_ford step 6282 current loss 0.064457, current_train_items 201056.
I0304 19:31:06.828603 22849303695488 run.py:483] Algo bellman_ford step 6283 current loss 0.101007, current_train_items 201088.
I0304 19:31:06.863658 22849303695488 run.py:483] Algo bellman_ford step 6284 current loss 0.149436, current_train_items 201120.
I0304 19:31:06.883809 22849303695488 run.py:483] Algo bellman_ford step 6285 current loss 0.006647, current_train_items 201152.
I0304 19:31:06.900747 22849303695488 run.py:483] Algo bellman_ford step 6286 current loss 0.027689, current_train_items 201184.
I0304 19:31:06.924644 22849303695488 run.py:483] Algo bellman_ford step 6287 current loss 0.072143, current_train_items 201216.
I0304 19:31:06.956452 22849303695488 run.py:483] Algo bellman_ford step 6288 current loss 0.067366, current_train_items 201248.
I0304 19:31:06.990836 22849303695488 run.py:483] Algo bellman_ford step 6289 current loss 0.086164, current_train_items 201280.
I0304 19:31:07.010990 22849303695488 run.py:483] Algo bellman_ford step 6290 current loss 0.005117, current_train_items 201312.
I0304 19:31:07.027894 22849303695488 run.py:483] Algo bellman_ford step 6291 current loss 0.092164, current_train_items 201344.
I0304 19:31:07.051991 22849303695488 run.py:483] Algo bellman_ford step 6292 current loss 0.055863, current_train_items 201376.
I0304 19:31:07.084308 22849303695488 run.py:483] Algo bellman_ford step 6293 current loss 0.084227, current_train_items 201408.
I0304 19:31:07.119787 22849303695488 run.py:483] Algo bellman_ford step 6294 current loss 0.065726, current_train_items 201440.
I0304 19:31:07.139773 22849303695488 run.py:483] Algo bellman_ford step 6295 current loss 0.003149, current_train_items 201472.
I0304 19:31:07.157072 22849303695488 run.py:483] Algo bellman_ford step 6296 current loss 0.042593, current_train_items 201504.
I0304 19:31:07.180804 22849303695488 run.py:483] Algo bellman_ford step 6297 current loss 0.074631, current_train_items 201536.
I0304 19:31:07.212831 22849303695488 run.py:483] Algo bellman_ford step 6298 current loss 0.097768, current_train_items 201568.
I0304 19:31:07.245846 22849303695488 run.py:483] Algo bellman_ford step 6299 current loss 0.079157, current_train_items 201600.
I0304 19:31:07.265789 22849303695488 run.py:483] Algo bellman_ford step 6300 current loss 0.003080, current_train_items 201632.
I0304 19:31:07.273845 22849303695488 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0304 19:31:07.273956 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:31:07.291336 22849303695488 run.py:483] Algo bellman_ford step 6301 current loss 0.021181, current_train_items 201664.
I0304 19:31:07.315492 22849303695488 run.py:483] Algo bellman_ford step 6302 current loss 0.066915, current_train_items 201696.
I0304 19:31:07.346956 22849303695488 run.py:483] Algo bellman_ford step 6303 current loss 0.077269, current_train_items 201728.
I0304 19:31:07.379110 22849303695488 run.py:483] Algo bellman_ford step 6304 current loss 0.055211, current_train_items 201760.
I0304 19:31:07.399329 22849303695488 run.py:483] Algo bellman_ford step 6305 current loss 0.014862, current_train_items 201792.
I0304 19:31:07.415524 22849303695488 run.py:483] Algo bellman_ford step 6306 current loss 0.009044, current_train_items 201824.
I0304 19:31:07.439642 22849303695488 run.py:483] Algo bellman_ford step 6307 current loss 0.043907, current_train_items 201856.
I0304 19:31:07.471826 22849303695488 run.py:483] Algo bellman_ford step 6308 current loss 0.058867, current_train_items 201888.
I0304 19:31:07.505261 22849303695488 run.py:483] Algo bellman_ford step 6309 current loss 0.064949, current_train_items 201920.
I0304 19:31:07.525096 22849303695488 run.py:483] Algo bellman_ford step 6310 current loss 0.008734, current_train_items 201952.
I0304 19:31:07.541550 22849303695488 run.py:483] Algo bellman_ford step 6311 current loss 0.017498, current_train_items 201984.
I0304 19:31:07.564396 22849303695488 run.py:483] Algo bellman_ford step 6312 current loss 0.051653, current_train_items 202016.
I0304 19:31:07.597151 22849303695488 run.py:483] Algo bellman_ford step 6313 current loss 0.089482, current_train_items 202048.
I0304 19:31:07.631788 22849303695488 run.py:483] Algo bellman_ford step 6314 current loss 0.089680, current_train_items 202080.
I0304 19:31:07.651907 22849303695488 run.py:483] Algo bellman_ford step 6315 current loss 0.013281, current_train_items 202112.
I0304 19:31:07.668250 22849303695488 run.py:483] Algo bellman_ford step 6316 current loss 0.015075, current_train_items 202144.
I0304 19:31:07.691878 22849303695488 run.py:483] Algo bellman_ford step 6317 current loss 0.079006, current_train_items 202176.
I0304 19:31:07.722469 22849303695488 run.py:483] Algo bellman_ford step 6318 current loss 0.036837, current_train_items 202208.
I0304 19:31:07.755766 22849303695488 run.py:483] Algo bellman_ford step 6319 current loss 0.063124, current_train_items 202240.
I0304 19:31:07.775751 22849303695488 run.py:483] Algo bellman_ford step 6320 current loss 0.011390, current_train_items 202272.
I0304 19:31:07.792527 22849303695488 run.py:483] Algo bellman_ford step 6321 current loss 0.031796, current_train_items 202304.
I0304 19:31:07.817571 22849303695488 run.py:483] Algo bellman_ford step 6322 current loss 0.074657, current_train_items 202336.
I0304 19:31:07.850018 22849303695488 run.py:483] Algo bellman_ford step 6323 current loss 0.071834, current_train_items 202368.
I0304 19:31:07.884450 22849303695488 run.py:483] Algo bellman_ford step 6324 current loss 0.107210, current_train_items 202400.
I0304 19:31:07.904528 22849303695488 run.py:483] Algo bellman_ford step 6325 current loss 0.007082, current_train_items 202432.
I0304 19:31:07.920737 22849303695488 run.py:483] Algo bellman_ford step 6326 current loss 0.020453, current_train_items 202464.
I0304 19:31:07.946036 22849303695488 run.py:483] Algo bellman_ford step 6327 current loss 0.087890, current_train_items 202496.
I0304 19:31:07.977370 22849303695488 run.py:483] Algo bellman_ford step 6328 current loss 0.109633, current_train_items 202528.
I0304 19:31:08.011797 22849303695488 run.py:483] Algo bellman_ford step 6329 current loss 0.082135, current_train_items 202560.
I0304 19:31:08.031670 22849303695488 run.py:483] Algo bellman_ford step 6330 current loss 0.005811, current_train_items 202592.
I0304 19:31:08.048341 22849303695488 run.py:483] Algo bellman_ford step 6331 current loss 0.017703, current_train_items 202624.
I0304 19:31:08.071922 22849303695488 run.py:483] Algo bellman_ford step 6332 current loss 0.079894, current_train_items 202656.
I0304 19:31:08.103307 22849303695488 run.py:483] Algo bellman_ford step 6333 current loss 0.031385, current_train_items 202688.
I0304 19:31:08.136270 22849303695488 run.py:483] Algo bellman_ford step 6334 current loss 0.038074, current_train_items 202720.
I0304 19:31:08.156126 22849303695488 run.py:483] Algo bellman_ford step 6335 current loss 0.002569, current_train_items 202752.
I0304 19:31:08.172807 22849303695488 run.py:483] Algo bellman_ford step 6336 current loss 0.015719, current_train_items 202784.
I0304 19:31:08.197350 22849303695488 run.py:483] Algo bellman_ford step 6337 current loss 0.043542, current_train_items 202816.
I0304 19:31:08.229686 22849303695488 run.py:483] Algo bellman_ford step 6338 current loss 0.070301, current_train_items 202848.
I0304 19:31:08.263154 22849303695488 run.py:483] Algo bellman_ford step 6339 current loss 0.064922, current_train_items 202880.
I0304 19:31:08.282911 22849303695488 run.py:483] Algo bellman_ford step 6340 current loss 0.005025, current_train_items 202912.
I0304 19:31:08.299041 22849303695488 run.py:483] Algo bellman_ford step 6341 current loss 0.010677, current_train_items 202944.
I0304 19:31:08.323831 22849303695488 run.py:483] Algo bellman_ford step 6342 current loss 0.044611, current_train_items 202976.
I0304 19:31:08.354859 22849303695488 run.py:483] Algo bellman_ford step 6343 current loss 0.025133, current_train_items 203008.
I0304 19:31:08.386256 22849303695488 run.py:483] Algo bellman_ford step 6344 current loss 0.040873, current_train_items 203040.
I0304 19:31:08.406438 22849303695488 run.py:483] Algo bellman_ford step 6345 current loss 0.002517, current_train_items 203072.
I0304 19:31:08.423209 22849303695488 run.py:483] Algo bellman_ford step 6346 current loss 0.042369, current_train_items 203104.
I0304 19:31:08.447138 22849303695488 run.py:483] Algo bellman_ford step 6347 current loss 0.055344, current_train_items 203136.
I0304 19:31:08.477975 22849303695488 run.py:483] Algo bellman_ford step 6348 current loss 0.052366, current_train_items 203168.
I0304 19:31:08.513725 22849303695488 run.py:483] Algo bellman_ford step 6349 current loss 0.047966, current_train_items 203200.
I0304 19:31:08.533582 22849303695488 run.py:483] Algo bellman_ford step 6350 current loss 0.028227, current_train_items 203232.
I0304 19:31:08.542036 22849303695488 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0304 19:31:08.542147 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:31:08.559713 22849303695488 run.py:483] Algo bellman_ford step 6351 current loss 0.020912, current_train_items 203264.
I0304 19:31:08.583889 22849303695488 run.py:483] Algo bellman_ford step 6352 current loss 0.049635, current_train_items 203296.
I0304 19:31:08.616204 22849303695488 run.py:483] Algo bellman_ford step 6353 current loss 0.065791, current_train_items 203328.
I0304 19:31:08.650272 22849303695488 run.py:483] Algo bellman_ford step 6354 current loss 0.083253, current_train_items 203360.
I0304 19:31:08.670434 22849303695488 run.py:483] Algo bellman_ford step 6355 current loss 0.008378, current_train_items 203392.
I0304 19:31:08.686919 22849303695488 run.py:483] Algo bellman_ford step 6356 current loss 0.011076, current_train_items 203424.
I0304 19:31:08.711849 22849303695488 run.py:483] Algo bellman_ford step 6357 current loss 0.043912, current_train_items 203456.
I0304 19:31:08.743430 22849303695488 run.py:483] Algo bellman_ford step 6358 current loss 0.073982, current_train_items 203488.
I0304 19:31:08.777961 22849303695488 run.py:483] Algo bellman_ford step 6359 current loss 0.101089, current_train_items 203520.
I0304 19:31:08.798723 22849303695488 run.py:483] Algo bellman_ford step 6360 current loss 0.004079, current_train_items 203552.
I0304 19:31:08.815746 22849303695488 run.py:483] Algo bellman_ford step 6361 current loss 0.013529, current_train_items 203584.
I0304 19:31:08.838964 22849303695488 run.py:483] Algo bellman_ford step 6362 current loss 0.048605, current_train_items 203616.
I0304 19:31:08.870568 22849303695488 run.py:483] Algo bellman_ford step 6363 current loss 0.049950, current_train_items 203648.
I0304 19:31:08.904072 22849303695488 run.py:483] Algo bellman_ford step 6364 current loss 0.069215, current_train_items 203680.
I0304 19:31:08.924436 22849303695488 run.py:483] Algo bellman_ford step 6365 current loss 0.002079, current_train_items 203712.
I0304 19:31:08.941306 22849303695488 run.py:483] Algo bellman_ford step 6366 current loss 0.015123, current_train_items 203744.
I0304 19:31:08.965353 22849303695488 run.py:483] Algo bellman_ford step 6367 current loss 0.048979, current_train_items 203776.
I0304 19:31:08.998174 22849303695488 run.py:483] Algo bellman_ford step 6368 current loss 0.074629, current_train_items 203808.
I0304 19:31:09.030776 22849303695488 run.py:483] Algo bellman_ford step 6369 current loss 0.022171, current_train_items 203840.
I0304 19:31:09.051264 22849303695488 run.py:483] Algo bellman_ford step 6370 current loss 0.002171, current_train_items 203872.
I0304 19:31:09.068133 22849303695488 run.py:483] Algo bellman_ford step 6371 current loss 0.009269, current_train_items 203904.
I0304 19:31:09.092581 22849303695488 run.py:483] Algo bellman_ford step 6372 current loss 0.127216, current_train_items 203936.
I0304 19:31:09.123811 22849303695488 run.py:483] Algo bellman_ford step 6373 current loss 0.096189, current_train_items 203968.
I0304 19:31:09.157927 22849303695488 run.py:483] Algo bellman_ford step 6374 current loss 0.054415, current_train_items 204000.
I0304 19:31:09.178459 22849303695488 run.py:483] Algo bellman_ford step 6375 current loss 0.003410, current_train_items 204032.
I0304 19:31:09.194967 22849303695488 run.py:483] Algo bellman_ford step 6376 current loss 0.008616, current_train_items 204064.
I0304 19:31:09.219840 22849303695488 run.py:483] Algo bellman_ford step 6377 current loss 0.060325, current_train_items 204096.
I0304 19:31:09.251450 22849303695488 run.py:483] Algo bellman_ford step 6378 current loss 0.155566, current_train_items 204128.
I0304 19:31:09.287561 22849303695488 run.py:483] Algo bellman_ford step 6379 current loss 0.152275, current_train_items 204160.
I0304 19:31:09.307593 22849303695488 run.py:483] Algo bellman_ford step 6380 current loss 0.009528, current_train_items 204192.
I0304 19:31:09.323995 22849303695488 run.py:483] Algo bellman_ford step 6381 current loss 0.041504, current_train_items 204224.
I0304 19:31:09.348264 22849303695488 run.py:483] Algo bellman_ford step 6382 current loss 0.051056, current_train_items 204256.
I0304 19:31:09.378787 22849303695488 run.py:483] Algo bellman_ford step 6383 current loss 0.074329, current_train_items 204288.
I0304 19:31:09.412975 22849303695488 run.py:483] Algo bellman_ford step 6384 current loss 0.086917, current_train_items 204320.
I0304 19:31:09.433737 22849303695488 run.py:483] Algo bellman_ford step 6385 current loss 0.002961, current_train_items 204352.
I0304 19:31:09.449961 22849303695488 run.py:483] Algo bellman_ford step 6386 current loss 0.010739, current_train_items 204384.
I0304 19:31:09.474399 22849303695488 run.py:483] Algo bellman_ford step 6387 current loss 0.055847, current_train_items 204416.
I0304 19:31:09.505610 22849303695488 run.py:483] Algo bellman_ford step 6388 current loss 0.045325, current_train_items 204448.
I0304 19:31:09.539628 22849303695488 run.py:483] Algo bellman_ford step 6389 current loss 0.036193, current_train_items 204480.
I0304 19:31:09.560078 22849303695488 run.py:483] Algo bellman_ford step 6390 current loss 0.003470, current_train_items 204512.
I0304 19:31:09.576949 22849303695488 run.py:483] Algo bellman_ford step 6391 current loss 0.016723, current_train_items 204544.
I0304 19:31:09.600668 22849303695488 run.py:483] Algo bellman_ford step 6392 current loss 0.036491, current_train_items 204576.
I0304 19:31:09.630821 22849303695488 run.py:483] Algo bellman_ford step 6393 current loss 0.023017, current_train_items 204608.
I0304 19:31:09.665462 22849303695488 run.py:483] Algo bellman_ford step 6394 current loss 0.040609, current_train_items 204640.
I0304 19:31:09.685603 22849303695488 run.py:483] Algo bellman_ford step 6395 current loss 0.004140, current_train_items 204672.
I0304 19:31:09.702194 22849303695488 run.py:483] Algo bellman_ford step 6396 current loss 0.008494, current_train_items 204704.
I0304 19:31:09.726137 22849303695488 run.py:483] Algo bellman_ford step 6397 current loss 0.035502, current_train_items 204736.
I0304 19:31:09.757048 22849303695488 run.py:483] Algo bellman_ford step 6398 current loss 0.039543, current_train_items 204768.
I0304 19:31:09.789398 22849303695488 run.py:483] Algo bellman_ford step 6399 current loss 0.086175, current_train_items 204800.
I0304 19:31:09.809827 22849303695488 run.py:483] Algo bellman_ford step 6400 current loss 0.002212, current_train_items 204832.
I0304 19:31:09.817904 22849303695488 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0304 19:31:09.818023 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.990, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:31:09.847972 22849303695488 run.py:483] Algo bellman_ford step 6401 current loss 0.010041, current_train_items 204864.
I0304 19:31:09.872273 22849303695488 run.py:483] Algo bellman_ford step 6402 current loss 0.018651, current_train_items 204896.
I0304 19:31:09.904501 22849303695488 run.py:483] Algo bellman_ford step 6403 current loss 0.070054, current_train_items 204928.
I0304 19:31:09.939592 22849303695488 run.py:483] Algo bellman_ford step 6404 current loss 0.052994, current_train_items 204960.
I0304 19:31:09.959933 22849303695488 run.py:483] Algo bellman_ford step 6405 current loss 0.014829, current_train_items 204992.
I0304 19:31:09.976280 22849303695488 run.py:483] Algo bellman_ford step 6406 current loss 0.025621, current_train_items 205024.
I0304 19:31:10.000382 22849303695488 run.py:483] Algo bellman_ford step 6407 current loss 0.022454, current_train_items 205056.
I0304 19:31:10.032051 22849303695488 run.py:483] Algo bellman_ford step 6408 current loss 0.050409, current_train_items 205088.
I0304 19:31:10.067432 22849303695488 run.py:483] Algo bellman_ford step 6409 current loss 0.081196, current_train_items 205120.
I0304 19:31:10.087869 22849303695488 run.py:483] Algo bellman_ford step 6410 current loss 0.006432, current_train_items 205152.
I0304 19:31:10.104197 22849303695488 run.py:483] Algo bellman_ford step 6411 current loss 0.017364, current_train_items 205184.
I0304 19:31:10.127906 22849303695488 run.py:483] Algo bellman_ford step 6412 current loss 0.038863, current_train_items 205216.
I0304 19:31:10.159622 22849303695488 run.py:483] Algo bellman_ford step 6413 current loss 0.066546, current_train_items 205248.
I0304 19:31:10.194248 22849303695488 run.py:483] Algo bellman_ford step 6414 current loss 0.043203, current_train_items 205280.
I0304 19:31:10.214388 22849303695488 run.py:483] Algo bellman_ford step 6415 current loss 0.003180, current_train_items 205312.
I0304 19:31:10.230909 22849303695488 run.py:483] Algo bellman_ford step 6416 current loss 0.026436, current_train_items 205344.
I0304 19:31:10.256261 22849303695488 run.py:483] Algo bellman_ford step 6417 current loss 0.044670, current_train_items 205376.
I0304 19:31:10.289074 22849303695488 run.py:483] Algo bellman_ford step 6418 current loss 0.058915, current_train_items 205408.
I0304 19:31:10.323910 22849303695488 run.py:483] Algo bellman_ford step 6419 current loss 0.116150, current_train_items 205440.
I0304 19:31:10.344401 22849303695488 run.py:483] Algo bellman_ford step 6420 current loss 0.003336, current_train_items 205472.
I0304 19:31:10.361010 22849303695488 run.py:483] Algo bellman_ford step 6421 current loss 0.029627, current_train_items 205504.
I0304 19:31:10.384934 22849303695488 run.py:483] Algo bellman_ford step 6422 current loss 0.046409, current_train_items 205536.
I0304 19:31:10.416392 22849303695488 run.py:483] Algo bellman_ford step 6423 current loss 0.110395, current_train_items 205568.
I0304 19:31:10.450157 22849303695488 run.py:483] Algo bellman_ford step 6424 current loss 0.042752, current_train_items 205600.
I0304 19:31:10.470326 22849303695488 run.py:483] Algo bellman_ford step 6425 current loss 0.003485, current_train_items 205632.
I0304 19:31:10.487203 22849303695488 run.py:483] Algo bellman_ford step 6426 current loss 0.022009, current_train_items 205664.
I0304 19:31:10.511667 22849303695488 run.py:483] Algo bellman_ford step 6427 current loss 0.034210, current_train_items 205696.
I0304 19:31:10.543956 22849303695488 run.py:483] Algo bellman_ford step 6428 current loss 0.040657, current_train_items 205728.
I0304 19:31:10.580322 22849303695488 run.py:483] Algo bellman_ford step 6429 current loss 0.088900, current_train_items 205760.
I0304 19:31:10.600810 22849303695488 run.py:483] Algo bellman_ford step 6430 current loss 0.002808, current_train_items 205792.
I0304 19:31:10.616891 22849303695488 run.py:483] Algo bellman_ford step 6431 current loss 0.062351, current_train_items 205824.
I0304 19:31:10.641479 22849303695488 run.py:483] Algo bellman_ford step 6432 current loss 0.036641, current_train_items 205856.
I0304 19:31:10.673570 22849303695488 run.py:483] Algo bellman_ford step 6433 current loss 0.040049, current_train_items 205888.
I0304 19:31:10.706956 22849303695488 run.py:483] Algo bellman_ford step 6434 current loss 0.058405, current_train_items 205920.
I0304 19:31:10.727076 22849303695488 run.py:483] Algo bellman_ford step 6435 current loss 0.002069, current_train_items 205952.
I0304 19:31:10.743580 22849303695488 run.py:483] Algo bellman_ford step 6436 current loss 0.007237, current_train_items 205984.
I0304 19:31:10.767605 22849303695488 run.py:483] Algo bellman_ford step 6437 current loss 0.044691, current_train_items 206016.
I0304 19:31:10.800291 22849303695488 run.py:483] Algo bellman_ford step 6438 current loss 0.067327, current_train_items 206048.
I0304 19:31:10.834836 22849303695488 run.py:483] Algo bellman_ford step 6439 current loss 0.082109, current_train_items 206080.
I0304 19:31:10.855087 22849303695488 run.py:483] Algo bellman_ford step 6440 current loss 0.008284, current_train_items 206112.
I0304 19:31:10.871936 22849303695488 run.py:483] Algo bellman_ford step 6441 current loss 0.017074, current_train_items 206144.
I0304 19:31:10.895357 22849303695488 run.py:483] Algo bellman_ford step 6442 current loss 0.018554, current_train_items 206176.
I0304 19:31:10.925601 22849303695488 run.py:483] Algo bellman_ford step 6443 current loss 0.058743, current_train_items 206208.
I0304 19:31:10.960228 22849303695488 run.py:483] Algo bellman_ford step 6444 current loss 0.044310, current_train_items 206240.
I0304 19:31:10.980551 22849303695488 run.py:483] Algo bellman_ford step 6445 current loss 0.010882, current_train_items 206272.
I0304 19:31:10.996857 22849303695488 run.py:483] Algo bellman_ford step 6446 current loss 0.045066, current_train_items 206304.
I0304 19:31:11.022179 22849303695488 run.py:483] Algo bellman_ford step 6447 current loss 0.051735, current_train_items 206336.
I0304 19:31:11.054250 22849303695488 run.py:483] Algo bellman_ford step 6448 current loss 0.065260, current_train_items 206368.
I0304 19:31:11.088362 22849303695488 run.py:483] Algo bellman_ford step 6449 current loss 0.053397, current_train_items 206400.
I0304 19:31:11.108785 22849303695488 run.py:483] Algo bellman_ford step 6450 current loss 0.012348, current_train_items 206432.
I0304 19:31:11.116993 22849303695488 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0304 19:31:11.117112 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:31:11.134282 22849303695488 run.py:483] Algo bellman_ford step 6451 current loss 0.008580, current_train_items 206464.
I0304 19:31:11.160012 22849303695488 run.py:483] Algo bellman_ford step 6452 current loss 0.087485, current_train_items 206496.
I0304 19:31:11.190922 22849303695488 run.py:483] Algo bellman_ford step 6453 current loss 0.082013, current_train_items 206528.
I0304 19:31:11.223484 22849303695488 run.py:483] Algo bellman_ford step 6454 current loss 0.046678, current_train_items 206560.
I0304 19:31:11.243592 22849303695488 run.py:483] Algo bellman_ford step 6455 current loss 0.081344, current_train_items 206592.
I0304 19:31:11.259983 22849303695488 run.py:483] Algo bellman_ford step 6456 current loss 0.006314, current_train_items 206624.
I0304 19:31:11.283216 22849303695488 run.py:483] Algo bellman_ford step 6457 current loss 0.041104, current_train_items 206656.
I0304 19:31:11.314372 22849303695488 run.py:483] Algo bellman_ford step 6458 current loss 0.034273, current_train_items 206688.
I0304 19:31:11.346601 22849303695488 run.py:483] Algo bellman_ford step 6459 current loss 0.063581, current_train_items 206720.
I0304 19:31:11.366746 22849303695488 run.py:483] Algo bellman_ford step 6460 current loss 0.088352, current_train_items 206752.
I0304 19:31:11.383395 22849303695488 run.py:483] Algo bellman_ford step 6461 current loss 0.009075, current_train_items 206784.
I0304 19:31:11.406789 22849303695488 run.py:483] Algo bellman_ford step 6462 current loss 0.056696, current_train_items 206816.
I0304 19:31:11.437787 22849303695488 run.py:483] Algo bellman_ford step 6463 current loss 0.028336, current_train_items 206848.
I0304 19:31:11.473823 22849303695488 run.py:483] Algo bellman_ford step 6464 current loss 0.077370, current_train_items 206880.
I0304 19:31:11.493530 22849303695488 run.py:483] Algo bellman_ford step 6465 current loss 0.001782, current_train_items 206912.
I0304 19:31:11.510272 22849303695488 run.py:483] Algo bellman_ford step 6466 current loss 0.011162, current_train_items 206944.
I0304 19:31:11.535944 22849303695488 run.py:483] Algo bellman_ford step 6467 current loss 0.047148, current_train_items 206976.
I0304 19:31:11.568826 22849303695488 run.py:483] Algo bellman_ford step 6468 current loss 0.070814, current_train_items 207008.
I0304 19:31:11.603959 22849303695488 run.py:483] Algo bellman_ford step 6469 current loss 0.090469, current_train_items 207040.
I0304 19:31:11.624333 22849303695488 run.py:483] Algo bellman_ford step 6470 current loss 0.005954, current_train_items 207072.
I0304 19:31:11.641461 22849303695488 run.py:483] Algo bellman_ford step 6471 current loss 0.017806, current_train_items 207104.
I0304 19:31:11.664671 22849303695488 run.py:483] Algo bellman_ford step 6472 current loss 0.025165, current_train_items 207136.
I0304 19:31:11.696308 22849303695488 run.py:483] Algo bellman_ford step 6473 current loss 0.071146, current_train_items 207168.
I0304 19:31:11.729194 22849303695488 run.py:483] Algo bellman_ford step 6474 current loss 0.089435, current_train_items 207200.
I0304 19:31:11.749184 22849303695488 run.py:483] Algo bellman_ford step 6475 current loss 0.041112, current_train_items 207232.
I0304 19:31:11.766380 22849303695488 run.py:483] Algo bellman_ford step 6476 current loss 0.051642, current_train_items 207264.
I0304 19:31:11.789313 22849303695488 run.py:483] Algo bellman_ford step 6477 current loss 0.055466, current_train_items 207296.
I0304 19:31:11.821170 22849303695488 run.py:483] Algo bellman_ford step 6478 current loss 0.061635, current_train_items 207328.
I0304 19:31:11.856641 22849303695488 run.py:483] Algo bellman_ford step 6479 current loss 0.065605, current_train_items 207360.
I0304 19:31:11.876726 22849303695488 run.py:483] Algo bellman_ford step 6480 current loss 0.004933, current_train_items 207392.
I0304 19:31:11.892919 22849303695488 run.py:483] Algo bellman_ford step 6481 current loss 0.064183, current_train_items 207424.
I0304 19:31:11.916882 22849303695488 run.py:483] Algo bellman_ford step 6482 current loss 0.025780, current_train_items 207456.
I0304 19:31:11.949141 22849303695488 run.py:483] Algo bellman_ford step 6483 current loss 0.062291, current_train_items 207488.
I0304 19:31:11.984251 22849303695488 run.py:483] Algo bellman_ford step 6484 current loss 0.076759, current_train_items 207520.
I0304 19:31:12.004721 22849303695488 run.py:483] Algo bellman_ford step 6485 current loss 0.005299, current_train_items 207552.
I0304 19:31:12.021720 22849303695488 run.py:483] Algo bellman_ford step 6486 current loss 0.049500, current_train_items 207584.
I0304 19:31:12.046889 22849303695488 run.py:483] Algo bellman_ford step 6487 current loss 0.048185, current_train_items 207616.
I0304 19:31:12.077365 22849303695488 run.py:483] Algo bellman_ford step 6488 current loss 0.058063, current_train_items 207648.
I0304 19:31:12.111788 22849303695488 run.py:483] Algo bellman_ford step 6489 current loss 0.061035, current_train_items 207680.
I0304 19:31:12.132257 22849303695488 run.py:483] Algo bellman_ford step 6490 current loss 0.006445, current_train_items 207712.
I0304 19:31:12.148782 22849303695488 run.py:483] Algo bellman_ford step 6491 current loss 0.017849, current_train_items 207744.
I0304 19:31:12.171988 22849303695488 run.py:483] Algo bellman_ford step 6492 current loss 0.029339, current_train_items 207776.
I0304 19:31:12.203312 22849303695488 run.py:483] Algo bellman_ford step 6493 current loss 0.030162, current_train_items 207808.
I0304 19:31:12.236718 22849303695488 run.py:483] Algo bellman_ford step 6494 current loss 0.089215, current_train_items 207840.
I0304 19:31:12.256351 22849303695488 run.py:483] Algo bellman_ford step 6495 current loss 0.004586, current_train_items 207872.
I0304 19:31:12.272552 22849303695488 run.py:483] Algo bellman_ford step 6496 current loss 0.016608, current_train_items 207904.
I0304 19:31:12.297309 22849303695488 run.py:483] Algo bellman_ford step 6497 current loss 0.039725, current_train_items 207936.
I0304 19:31:12.328225 22849303695488 run.py:483] Algo bellman_ford step 6498 current loss 0.052664, current_train_items 207968.
I0304 19:31:12.363585 22849303695488 run.py:483] Algo bellman_ford step 6499 current loss 0.061825, current_train_items 208000.
I0304 19:31:12.383868 22849303695488 run.py:483] Algo bellman_ford step 6500 current loss 0.004128, current_train_items 208032.
I0304 19:31:12.391837 22849303695488 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0304 19:31:12.391948 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:31:12.409089 22849303695488 run.py:483] Algo bellman_ford step 6501 current loss 0.019775, current_train_items 208064.
I0304 19:31:12.434435 22849303695488 run.py:483] Algo bellman_ford step 6502 current loss 0.103393, current_train_items 208096.
I0304 19:31:12.466121 22849303695488 run.py:483] Algo bellman_ford step 6503 current loss 0.046429, current_train_items 208128.
I0304 19:31:12.501522 22849303695488 run.py:483] Algo bellman_ford step 6504 current loss 0.069587, current_train_items 208160.
I0304 19:31:12.521699 22849303695488 run.py:483] Algo bellman_ford step 6505 current loss 0.002726, current_train_items 208192.
I0304 19:31:12.537522 22849303695488 run.py:483] Algo bellman_ford step 6506 current loss 0.006230, current_train_items 208224.
I0304 19:31:12.561745 22849303695488 run.py:483] Algo bellman_ford step 6507 current loss 0.043454, current_train_items 208256.
I0304 19:31:12.593991 22849303695488 run.py:483] Algo bellman_ford step 6508 current loss 0.125379, current_train_items 208288.
I0304 19:31:12.625882 22849303695488 run.py:483] Algo bellman_ford step 6509 current loss 0.120203, current_train_items 208320.
I0304 19:31:12.646074 22849303695488 run.py:483] Algo bellman_ford step 6510 current loss 0.007812, current_train_items 208352.
I0304 19:31:12.662679 22849303695488 run.py:483] Algo bellman_ford step 6511 current loss 0.029755, current_train_items 208384.
I0304 19:31:12.687482 22849303695488 run.py:483] Algo bellman_ford step 6512 current loss 0.042115, current_train_items 208416.
I0304 19:31:12.718829 22849303695488 run.py:483] Algo bellman_ford step 6513 current loss 0.055314, current_train_items 208448.
I0304 19:31:12.754150 22849303695488 run.py:483] Algo bellman_ford step 6514 current loss 0.057490, current_train_items 208480.
I0304 19:31:12.773840 22849303695488 run.py:483] Algo bellman_ford step 6515 current loss 0.004931, current_train_items 208512.
I0304 19:31:12.790141 22849303695488 run.py:483] Algo bellman_ford step 6516 current loss 0.022207, current_train_items 208544.
I0304 19:31:12.814949 22849303695488 run.py:483] Algo bellman_ford step 6517 current loss 0.029110, current_train_items 208576.
I0304 19:31:12.846322 22849303695488 run.py:483] Algo bellman_ford step 6518 current loss 0.078970, current_train_items 208608.
I0304 19:31:12.879779 22849303695488 run.py:483] Algo bellman_ford step 6519 current loss 0.052051, current_train_items 208640.
I0304 19:31:12.899802 22849303695488 run.py:483] Algo bellman_ford step 6520 current loss 0.005890, current_train_items 208672.
I0304 19:31:12.916328 22849303695488 run.py:483] Algo bellman_ford step 6521 current loss 0.010001, current_train_items 208704.
I0304 19:31:12.941198 22849303695488 run.py:483] Algo bellman_ford step 6522 current loss 0.048805, current_train_items 208736.
I0304 19:31:12.972322 22849303695488 run.py:483] Algo bellman_ford step 6523 current loss 0.037846, current_train_items 208768.
I0304 19:31:13.003951 22849303695488 run.py:483] Algo bellman_ford step 6524 current loss 0.095710, current_train_items 208800.
I0304 19:31:13.024206 22849303695488 run.py:483] Algo bellman_ford step 6525 current loss 0.002995, current_train_items 208832.
I0304 19:31:13.040535 22849303695488 run.py:483] Algo bellman_ford step 6526 current loss 0.008981, current_train_items 208864.
I0304 19:31:13.064619 22849303695488 run.py:483] Algo bellman_ford step 6527 current loss 0.049572, current_train_items 208896.
I0304 19:31:13.095153 22849303695488 run.py:483] Algo bellman_ford step 6528 current loss 0.059725, current_train_items 208928.
I0304 19:31:13.129405 22849303695488 run.py:483] Algo bellman_ford step 6529 current loss 0.061637, current_train_items 208960.
I0304 19:31:13.149228 22849303695488 run.py:483] Algo bellman_ford step 6530 current loss 0.002888, current_train_items 208992.
I0304 19:31:13.165983 22849303695488 run.py:483] Algo bellman_ford step 6531 current loss 0.018904, current_train_items 209024.
I0304 19:31:13.191824 22849303695488 run.py:483] Algo bellman_ford step 6532 current loss 0.064007, current_train_items 209056.
I0304 19:31:13.222538 22849303695488 run.py:483] Algo bellman_ford step 6533 current loss 0.081680, current_train_items 209088.
I0304 19:31:13.256360 22849303695488 run.py:483] Algo bellman_ford step 6534 current loss 0.080868, current_train_items 209120.
I0304 19:31:13.276438 22849303695488 run.py:483] Algo bellman_ford step 6535 current loss 0.002205, current_train_items 209152.
I0304 19:31:13.293088 22849303695488 run.py:483] Algo bellman_ford step 6536 current loss 0.026187, current_train_items 209184.
I0304 19:31:13.317012 22849303695488 run.py:483] Algo bellman_ford step 6537 current loss 0.028975, current_train_items 209216.
I0304 19:31:13.347950 22849303695488 run.py:483] Algo bellman_ford step 6538 current loss 0.040850, current_train_items 209248.
I0304 19:31:13.381951 22849303695488 run.py:483] Algo bellman_ford step 6539 current loss 0.070494, current_train_items 209280.
I0304 19:31:13.401928 22849303695488 run.py:483] Algo bellman_ford step 6540 current loss 0.002995, current_train_items 209312.
I0304 19:31:13.418512 22849303695488 run.py:483] Algo bellman_ford step 6541 current loss 0.018870, current_train_items 209344.
I0304 19:31:13.442800 22849303695488 run.py:483] Algo bellman_ford step 6542 current loss 0.037132, current_train_items 209376.
I0304 19:31:13.475035 22849303695488 run.py:483] Algo bellman_ford step 6543 current loss 0.067288, current_train_items 209408.
I0304 19:31:13.511230 22849303695488 run.py:483] Algo bellman_ford step 6544 current loss 0.127369, current_train_items 209440.
I0304 19:31:13.531281 22849303695488 run.py:483] Algo bellman_ford step 6545 current loss 0.003413, current_train_items 209472.
I0304 19:31:13.547713 22849303695488 run.py:483] Algo bellman_ford step 6546 current loss 0.052613, current_train_items 209504.
I0304 19:31:13.571260 22849303695488 run.py:483] Algo bellman_ford step 6547 current loss 0.027608, current_train_items 209536.
I0304 19:31:13.603909 22849303695488 run.py:483] Algo bellman_ford step 6548 current loss 0.075544, current_train_items 209568.
I0304 19:31:13.638820 22849303695488 run.py:483] Algo bellman_ford step 6549 current loss 0.094310, current_train_items 209600.
I0304 19:31:13.658721 22849303695488 run.py:483] Algo bellman_ford step 6550 current loss 0.002780, current_train_items 209632.
I0304 19:31:13.667155 22849303695488 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0304 19:31:13.667265 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:31:13.684759 22849303695488 run.py:483] Algo bellman_ford step 6551 current loss 0.012032, current_train_items 209664.
I0304 19:31:13.709436 22849303695488 run.py:483] Algo bellman_ford step 6552 current loss 0.044432, current_train_items 209696.
I0304 19:31:13.742269 22849303695488 run.py:483] Algo bellman_ford step 6553 current loss 0.054741, current_train_items 209728.
I0304 19:31:13.778857 22849303695488 run.py:483] Algo bellman_ford step 6554 current loss 0.153630, current_train_items 209760.
I0304 19:31:13.799087 22849303695488 run.py:483] Algo bellman_ford step 6555 current loss 0.009862, current_train_items 209792.
I0304 19:31:13.815380 22849303695488 run.py:483] Algo bellman_ford step 6556 current loss 0.022320, current_train_items 209824.
I0304 19:31:13.839758 22849303695488 run.py:483] Algo bellman_ford step 6557 current loss 0.042374, current_train_items 209856.
I0304 19:31:13.871958 22849303695488 run.py:483] Algo bellman_ford step 6558 current loss 0.073703, current_train_items 209888.
I0304 19:31:13.906627 22849303695488 run.py:483] Algo bellman_ford step 6559 current loss 0.174636, current_train_items 209920.
I0304 19:31:13.927420 22849303695488 run.py:483] Algo bellman_ford step 6560 current loss 0.008694, current_train_items 209952.
I0304 19:31:13.943786 22849303695488 run.py:483] Algo bellman_ford step 6561 current loss 0.010423, current_train_items 209984.
I0304 19:31:13.966968 22849303695488 run.py:483] Algo bellman_ford step 6562 current loss 0.043090, current_train_items 210016.
I0304 19:31:13.998428 22849303695488 run.py:483] Algo bellman_ford step 6563 current loss 0.040292, current_train_items 210048.
I0304 19:31:14.030126 22849303695488 run.py:483] Algo bellman_ford step 6564 current loss 0.058867, current_train_items 210080.
I0304 19:31:14.050380 22849303695488 run.py:483] Algo bellman_ford step 6565 current loss 0.003437, current_train_items 210112.
I0304 19:31:14.066585 22849303695488 run.py:483] Algo bellman_ford step 6566 current loss 0.006797, current_train_items 210144.
I0304 19:31:14.091348 22849303695488 run.py:483] Algo bellman_ford step 6567 current loss 0.043353, current_train_items 210176.
I0304 19:31:14.121200 22849303695488 run.py:483] Algo bellman_ford step 6568 current loss 0.062733, current_train_items 210208.
I0304 19:31:14.155293 22849303695488 run.py:483] Algo bellman_ford step 6569 current loss 0.115001, current_train_items 210240.
I0304 19:31:14.175737 22849303695488 run.py:483] Algo bellman_ford step 6570 current loss 0.003198, current_train_items 210272.
I0304 19:31:14.192384 22849303695488 run.py:483] Algo bellman_ford step 6571 current loss 0.031730, current_train_items 210304.
I0304 19:31:14.216140 22849303695488 run.py:483] Algo bellman_ford step 6572 current loss 0.051161, current_train_items 210336.
I0304 19:31:14.247017 22849303695488 run.py:483] Algo bellman_ford step 6573 current loss 0.038952, current_train_items 210368.
I0304 19:31:14.280421 22849303695488 run.py:483] Algo bellman_ford step 6574 current loss 0.111606, current_train_items 210400.
I0304 19:31:14.300755 22849303695488 run.py:483] Algo bellman_ford step 6575 current loss 0.048979, current_train_items 210432.
I0304 19:31:14.317481 22849303695488 run.py:483] Algo bellman_ford step 6576 current loss 0.035899, current_train_items 210464.
I0304 19:31:14.341238 22849303695488 run.py:483] Algo bellman_ford step 6577 current loss 0.025296, current_train_items 210496.
I0304 19:31:14.372931 22849303695488 run.py:483] Algo bellman_ford step 6578 current loss 0.088958, current_train_items 210528.
I0304 19:31:14.407124 22849303695488 run.py:483] Algo bellman_ford step 6579 current loss 0.068077, current_train_items 210560.
I0304 19:31:14.427123 22849303695488 run.py:483] Algo bellman_ford step 6580 current loss 0.019143, current_train_items 210592.
I0304 19:31:14.443631 22849303695488 run.py:483] Algo bellman_ford step 6581 current loss 0.032245, current_train_items 210624.
I0304 19:31:14.468062 22849303695488 run.py:483] Algo bellman_ford step 6582 current loss 0.063663, current_train_items 210656.
I0304 19:31:14.500776 22849303695488 run.py:483] Algo bellman_ford step 6583 current loss 0.083167, current_train_items 210688.
I0304 19:31:14.534272 22849303695488 run.py:483] Algo bellman_ford step 6584 current loss 0.105671, current_train_items 210720.
I0304 19:31:14.554456 22849303695488 run.py:483] Algo bellman_ford step 6585 current loss 0.072005, current_train_items 210752.
I0304 19:31:14.570537 22849303695488 run.py:483] Algo bellman_ford step 6586 current loss 0.020662, current_train_items 210784.
I0304 19:31:14.593134 22849303695488 run.py:483] Algo bellman_ford step 6587 current loss 0.041700, current_train_items 210816.
I0304 19:31:14.623711 22849303695488 run.py:483] Algo bellman_ford step 6588 current loss 0.062435, current_train_items 210848.
I0304 19:31:14.656101 22849303695488 run.py:483] Algo bellman_ford step 6589 current loss 0.081491, current_train_items 210880.
I0304 19:31:14.676671 22849303695488 run.py:483] Algo bellman_ford step 6590 current loss 0.027940, current_train_items 210912.
I0304 19:31:14.692961 22849303695488 run.py:483] Algo bellman_ford step 6591 current loss 0.013658, current_train_items 210944.
I0304 19:31:14.717148 22849303695488 run.py:483] Algo bellman_ford step 6592 current loss 0.049000, current_train_items 210976.
I0304 19:31:14.748831 22849303695488 run.py:483] Algo bellman_ford step 6593 current loss 0.071381, current_train_items 211008.
I0304 19:31:14.781756 22849303695488 run.py:483] Algo bellman_ford step 6594 current loss 0.150069, current_train_items 211040.
I0304 19:31:14.801863 22849303695488 run.py:483] Algo bellman_ford step 6595 current loss 0.006557, current_train_items 211072.
I0304 19:31:14.818464 22849303695488 run.py:483] Algo bellman_ford step 6596 current loss 0.012674, current_train_items 211104.
I0304 19:31:14.843357 22849303695488 run.py:483] Algo bellman_ford step 6597 current loss 0.074855, current_train_items 211136.
I0304 19:31:14.875431 22849303695488 run.py:483] Algo bellman_ford step 6598 current loss 0.053679, current_train_items 211168.
I0304 19:31:14.906230 22849303695488 run.py:483] Algo bellman_ford step 6599 current loss 0.077100, current_train_items 211200.
I0304 19:31:14.926366 22849303695488 run.py:483] Algo bellman_ford step 6600 current loss 0.013568, current_train_items 211232.
I0304 19:31:14.934388 22849303695488 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0304 19:31:14.934497 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:31:14.951592 22849303695488 run.py:483] Algo bellman_ford step 6601 current loss 0.012983, current_train_items 211264.
I0304 19:31:14.975106 22849303695488 run.py:483] Algo bellman_ford step 6602 current loss 0.056362, current_train_items 211296.
I0304 19:31:15.007208 22849303695488 run.py:483] Algo bellman_ford step 6603 current loss 0.059511, current_train_items 211328.
I0304 19:31:15.041780 22849303695488 run.py:483] Algo bellman_ford step 6604 current loss 0.057272, current_train_items 211360.
I0304 19:31:15.061922 22849303695488 run.py:483] Algo bellman_ford step 6605 current loss 0.004284, current_train_items 211392.
I0304 19:31:15.078193 22849303695488 run.py:483] Algo bellman_ford step 6606 current loss 0.028467, current_train_items 211424.
I0304 19:31:15.102479 22849303695488 run.py:483] Algo bellman_ford step 6607 current loss 0.054980, current_train_items 211456.
I0304 19:31:15.134494 22849303695488 run.py:483] Algo bellman_ford step 6608 current loss 0.033378, current_train_items 211488.
I0304 19:31:15.166480 22849303695488 run.py:483] Algo bellman_ford step 6609 current loss 0.052348, current_train_items 211520.
I0304 19:31:15.186480 22849303695488 run.py:483] Algo bellman_ford step 6610 current loss 0.005865, current_train_items 211552.
I0304 19:31:15.202739 22849303695488 run.py:483] Algo bellman_ford step 6611 current loss 0.010683, current_train_items 211584.
I0304 19:31:15.225862 22849303695488 run.py:483] Algo bellman_ford step 6612 current loss 0.041600, current_train_items 211616.
I0304 19:31:15.258102 22849303695488 run.py:483] Algo bellman_ford step 6613 current loss 0.072668, current_train_items 211648.
I0304 19:31:15.294279 22849303695488 run.py:483] Algo bellman_ford step 6614 current loss 0.074585, current_train_items 211680.
I0304 19:31:15.314373 22849303695488 run.py:483] Algo bellman_ford step 6615 current loss 0.008900, current_train_items 211712.
I0304 19:31:15.330790 22849303695488 run.py:483] Algo bellman_ford step 6616 current loss 0.017479, current_train_items 211744.
I0304 19:31:15.355129 22849303695488 run.py:483] Algo bellman_ford step 6617 current loss 0.033763, current_train_items 211776.
I0304 19:31:15.386478 22849303695488 run.py:483] Algo bellman_ford step 6618 current loss 0.075018, current_train_items 211808.
I0304 19:31:15.420690 22849303695488 run.py:483] Algo bellman_ford step 6619 current loss 0.060486, current_train_items 211840.
I0304 19:31:15.440588 22849303695488 run.py:483] Algo bellman_ford step 6620 current loss 0.004242, current_train_items 211872.
I0304 19:31:15.456864 22849303695488 run.py:483] Algo bellman_ford step 6621 current loss 0.022359, current_train_items 211904.
I0304 19:31:15.479584 22849303695488 run.py:483] Algo bellman_ford step 6622 current loss 0.061114, current_train_items 211936.
I0304 19:31:15.511480 22849303695488 run.py:483] Algo bellman_ford step 6623 current loss 0.046755, current_train_items 211968.
I0304 19:31:15.543948 22849303695488 run.py:483] Algo bellman_ford step 6624 current loss 0.045772, current_train_items 212000.
I0304 19:31:15.564238 22849303695488 run.py:483] Algo bellman_ford step 6625 current loss 0.002676, current_train_items 212032.
I0304 19:31:15.580828 22849303695488 run.py:483] Algo bellman_ford step 6626 current loss 0.008127, current_train_items 212064.
I0304 19:31:15.604891 22849303695488 run.py:483] Algo bellman_ford step 6627 current loss 0.064885, current_train_items 212096.
I0304 19:31:15.635237 22849303695488 run.py:483] Algo bellman_ford step 6628 current loss 0.062538, current_train_items 212128.
I0304 19:31:15.667799 22849303695488 run.py:483] Algo bellman_ford step 6629 current loss 0.064244, current_train_items 212160.
I0304 19:31:15.687532 22849303695488 run.py:483] Algo bellman_ford step 6630 current loss 0.017820, current_train_items 212192.
I0304 19:31:15.703974 22849303695488 run.py:483] Algo bellman_ford step 6631 current loss 0.066195, current_train_items 212224.
I0304 19:31:15.728800 22849303695488 run.py:483] Algo bellman_ford step 6632 current loss 0.058960, current_train_items 212256.
I0304 19:31:15.761372 22849303695488 run.py:483] Algo bellman_ford step 6633 current loss 0.076303, current_train_items 212288.
I0304 19:31:15.794469 22849303695488 run.py:483] Algo bellman_ford step 6634 current loss 0.057587, current_train_items 212320.
I0304 19:31:15.814432 22849303695488 run.py:483] Algo bellman_ford step 6635 current loss 0.004373, current_train_items 212352.
I0304 19:31:15.831163 22849303695488 run.py:483] Algo bellman_ford step 6636 current loss 0.062277, current_train_items 212384.
I0304 19:31:15.855248 22849303695488 run.py:483] Algo bellman_ford step 6637 current loss 0.044735, current_train_items 212416.
I0304 19:31:15.886706 22849303695488 run.py:483] Algo bellman_ford step 6638 current loss 0.093315, current_train_items 212448.
I0304 19:31:15.921112 22849303695488 run.py:483] Algo bellman_ford step 6639 current loss 0.079978, current_train_items 212480.
I0304 19:31:15.941117 22849303695488 run.py:483] Algo bellman_ford step 6640 current loss 0.002457, current_train_items 212512.
I0304 19:31:15.957759 22849303695488 run.py:483] Algo bellman_ford step 6641 current loss 0.015145, current_train_items 212544.
I0304 19:31:15.982851 22849303695488 run.py:483] Algo bellman_ford step 6642 current loss 0.121900, current_train_items 212576.
I0304 19:31:16.013994 22849303695488 run.py:483] Algo bellman_ford step 6643 current loss 0.091694, current_train_items 212608.
I0304 19:31:16.048650 22849303695488 run.py:483] Algo bellman_ford step 6644 current loss 0.056123, current_train_items 212640.
I0304 19:31:16.068485 22849303695488 run.py:483] Algo bellman_ford step 6645 current loss 0.004827, current_train_items 212672.
I0304 19:31:16.084818 22849303695488 run.py:483] Algo bellman_ford step 6646 current loss 0.022578, current_train_items 212704.
I0304 19:31:16.108949 22849303695488 run.py:483] Algo bellman_ford step 6647 current loss 0.052875, current_train_items 212736.
I0304 19:31:16.140526 22849303695488 run.py:483] Algo bellman_ford step 6648 current loss 0.068551, current_train_items 212768.
I0304 19:31:16.175216 22849303695488 run.py:483] Algo bellman_ford step 6649 current loss 0.082852, current_train_items 212800.
I0304 19:31:16.195217 22849303695488 run.py:483] Algo bellman_ford step 6650 current loss 0.003919, current_train_items 212832.
I0304 19:31:16.203431 22849303695488 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0304 19:31:16.203539 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:31:16.221230 22849303695488 run.py:483] Algo bellman_ford step 6651 current loss 0.035436, current_train_items 212864.
I0304 19:31:16.246572 22849303695488 run.py:483] Algo bellman_ford step 6652 current loss 0.073943, current_train_items 212896.
I0304 19:31:16.279468 22849303695488 run.py:483] Algo bellman_ford step 6653 current loss 0.074070, current_train_items 212928.
I0304 19:31:16.309644 22849303695488 run.py:483] Algo bellman_ford step 6654 current loss 0.052765, current_train_items 212960.
I0304 19:31:16.329678 22849303695488 run.py:483] Algo bellman_ford step 6655 current loss 0.003985, current_train_items 212992.
I0304 19:31:16.345841 22849303695488 run.py:483] Algo bellman_ford step 6656 current loss 0.044451, current_train_items 213024.
I0304 19:31:16.370349 22849303695488 run.py:483] Algo bellman_ford step 6657 current loss 0.073545, current_train_items 213056.
I0304 19:31:16.402258 22849303695488 run.py:483] Algo bellman_ford step 6658 current loss 0.043394, current_train_items 213088.
I0304 19:31:16.437786 22849303695488 run.py:483] Algo bellman_ford step 6659 current loss 0.114208, current_train_items 213120.
I0304 19:31:16.458248 22849303695488 run.py:483] Algo bellman_ford step 6660 current loss 0.005322, current_train_items 213152.
I0304 19:31:16.475271 22849303695488 run.py:483] Algo bellman_ford step 6661 current loss 0.054373, current_train_items 213184.
I0304 19:31:16.500240 22849303695488 run.py:483] Algo bellman_ford step 6662 current loss 0.066795, current_train_items 213216.
I0304 19:31:16.530690 22849303695488 run.py:483] Algo bellman_ford step 6663 current loss 0.041130, current_train_items 213248.
I0304 19:31:16.564437 22849303695488 run.py:483] Algo bellman_ford step 6664 current loss 0.053237, current_train_items 213280.
I0304 19:31:16.584783 22849303695488 run.py:483] Algo bellman_ford step 6665 current loss 0.005184, current_train_items 213312.
I0304 19:31:16.601279 22849303695488 run.py:483] Algo bellman_ford step 6666 current loss 0.013141, current_train_items 213344.
I0304 19:31:16.627469 22849303695488 run.py:483] Algo bellman_ford step 6667 current loss 0.093399, current_train_items 213376.
I0304 19:31:16.660249 22849303695488 run.py:483] Algo bellman_ford step 6668 current loss 0.113713, current_train_items 213408.
I0304 19:31:16.695904 22849303695488 run.py:483] Algo bellman_ford step 6669 current loss 0.082727, current_train_items 213440.
I0304 19:31:16.716816 22849303695488 run.py:483] Algo bellman_ford step 6670 current loss 0.069378, current_train_items 213472.
I0304 19:31:16.733414 22849303695488 run.py:483] Algo bellman_ford step 6671 current loss 0.018590, current_train_items 213504.
I0304 19:31:16.757574 22849303695488 run.py:483] Algo bellman_ford step 6672 current loss 0.041374, current_train_items 213536.
I0304 19:31:16.789687 22849303695488 run.py:483] Algo bellman_ford step 6673 current loss 0.103661, current_train_items 213568.
I0304 19:31:16.822578 22849303695488 run.py:483] Algo bellman_ford step 6674 current loss 0.108258, current_train_items 213600.
I0304 19:31:16.842953 22849303695488 run.py:483] Algo bellman_ford step 6675 current loss 0.015834, current_train_items 213632.
I0304 19:31:16.859079 22849303695488 run.py:483] Algo bellman_ford step 6676 current loss 0.024694, current_train_items 213664.
I0304 19:31:16.883876 22849303695488 run.py:483] Algo bellman_ford step 6677 current loss 0.073267, current_train_items 213696.
I0304 19:31:16.916376 22849303695488 run.py:483] Algo bellman_ford step 6678 current loss 0.095231, current_train_items 213728.
I0304 19:31:16.952069 22849303695488 run.py:483] Algo bellman_ford step 6679 current loss 0.104116, current_train_items 213760.
I0304 19:31:16.972291 22849303695488 run.py:483] Algo bellman_ford step 6680 current loss 0.006683, current_train_items 213792.
I0304 19:31:16.988888 22849303695488 run.py:483] Algo bellman_ford step 6681 current loss 0.021928, current_train_items 213824.
I0304 19:31:17.013935 22849303695488 run.py:483] Algo bellman_ford step 6682 current loss 0.064941, current_train_items 213856.
I0304 19:31:17.045196 22849303695488 run.py:483] Algo bellman_ford step 6683 current loss 0.062390, current_train_items 213888.
I0304 19:31:17.081873 22849303695488 run.py:483] Algo bellman_ford step 6684 current loss 0.147017, current_train_items 213920.
I0304 19:31:17.102545 22849303695488 run.py:483] Algo bellman_ford step 6685 current loss 0.009696, current_train_items 213952.
I0304 19:31:17.118997 22849303695488 run.py:483] Algo bellman_ford step 6686 current loss 0.162794, current_train_items 213984.
I0304 19:31:17.142880 22849303695488 run.py:483] Algo bellman_ford step 6687 current loss 0.089251, current_train_items 214016.
I0304 19:31:17.175288 22849303695488 run.py:483] Algo bellman_ford step 6688 current loss 0.055820, current_train_items 214048.
I0304 19:31:17.208864 22849303695488 run.py:483] Algo bellman_ford step 6689 current loss 0.055741, current_train_items 214080.
I0304 19:31:17.229631 22849303695488 run.py:483] Algo bellman_ford step 6690 current loss 0.004814, current_train_items 214112.
I0304 19:31:17.245856 22849303695488 run.py:483] Algo bellman_ford step 6691 current loss 0.006439, current_train_items 214144.
I0304 19:31:17.270283 22849303695488 run.py:483] Algo bellman_ford step 6692 current loss 0.096923, current_train_items 214176.
I0304 19:31:17.301992 22849303695488 run.py:483] Algo bellman_ford step 6693 current loss 0.149560, current_train_items 214208.
I0304 19:31:17.336529 22849303695488 run.py:483] Algo bellman_ford step 6694 current loss 0.077862, current_train_items 214240.
I0304 19:31:17.356437 22849303695488 run.py:483] Algo bellman_ford step 6695 current loss 0.003102, current_train_items 214272.
I0304 19:31:17.373079 22849303695488 run.py:483] Algo bellman_ford step 6696 current loss 0.011335, current_train_items 214304.
I0304 19:31:17.397544 22849303695488 run.py:483] Algo bellman_ford step 6697 current loss 0.046202, current_train_items 214336.
I0304 19:31:17.430011 22849303695488 run.py:483] Algo bellman_ford step 6698 current loss 0.081197, current_train_items 214368.
I0304 19:31:17.465022 22849303695488 run.py:483] Algo bellman_ford step 6699 current loss 0.110282, current_train_items 214400.
I0304 19:31:17.484905 22849303695488 run.py:483] Algo bellman_ford step 6700 current loss 0.077064, current_train_items 214432.
I0304 19:31:17.492776 22849303695488 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0304 19:31:17.492884 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:17.510489 22849303695488 run.py:483] Algo bellman_ford step 6701 current loss 0.023369, current_train_items 214464.
I0304 19:31:17.535027 22849303695488 run.py:483] Algo bellman_ford step 6702 current loss 0.050261, current_train_items 214496.
I0304 19:31:17.567309 22849303695488 run.py:483] Algo bellman_ford step 6703 current loss 0.081653, current_train_items 214528.
I0304 19:31:17.601000 22849303695488 run.py:483] Algo bellman_ford step 6704 current loss 0.082344, current_train_items 214560.
I0304 19:31:17.621376 22849303695488 run.py:483] Algo bellman_ford step 6705 current loss 0.025603, current_train_items 214592.
I0304 19:31:17.637754 22849303695488 run.py:483] Algo bellman_ford step 6706 current loss 0.012646, current_train_items 214624.
I0304 19:31:17.663057 22849303695488 run.py:483] Algo bellman_ford step 6707 current loss 0.091503, current_train_items 214656.
I0304 19:31:17.694711 22849303695488 run.py:483] Algo bellman_ford step 6708 current loss 0.060344, current_train_items 214688.
I0304 19:31:17.729849 22849303695488 run.py:483] Algo bellman_ford step 6709 current loss 0.082204, current_train_items 214720.
I0304 19:31:17.750098 22849303695488 run.py:483] Algo bellman_ford step 6710 current loss 0.005096, current_train_items 214752.
I0304 19:31:17.766868 22849303695488 run.py:483] Algo bellman_ford step 6711 current loss 0.024023, current_train_items 214784.
I0304 19:31:17.790642 22849303695488 run.py:483] Algo bellman_ford step 6712 current loss 0.121821, current_train_items 214816.
I0304 19:31:17.823554 22849303695488 run.py:483] Algo bellman_ford step 6713 current loss 0.108239, current_train_items 214848.
I0304 19:31:17.857557 22849303695488 run.py:483] Algo bellman_ford step 6714 current loss 0.074795, current_train_items 214880.
I0304 19:31:17.877512 22849303695488 run.py:483] Algo bellman_ford step 6715 current loss 0.026143, current_train_items 214912.
I0304 19:31:17.894071 22849303695488 run.py:483] Algo bellman_ford step 6716 current loss 0.014024, current_train_items 214944.
I0304 19:31:17.918394 22849303695488 run.py:483] Algo bellman_ford step 6717 current loss 0.037061, current_train_items 214976.
I0304 19:31:17.950525 22849303695488 run.py:483] Algo bellman_ford step 6718 current loss 0.056221, current_train_items 215008.
I0304 19:31:17.986669 22849303695488 run.py:483] Algo bellman_ford step 6719 current loss 0.105942, current_train_items 215040.
I0304 19:31:18.006718 22849303695488 run.py:483] Algo bellman_ford step 6720 current loss 0.006208, current_train_items 215072.
I0304 19:31:18.023207 22849303695488 run.py:483] Algo bellman_ford step 6721 current loss 0.015699, current_train_items 215104.
I0304 19:31:18.046779 22849303695488 run.py:483] Algo bellman_ford step 6722 current loss 0.056368, current_train_items 215136.
I0304 19:31:18.079382 22849303695488 run.py:483] Algo bellman_ford step 6723 current loss 0.058425, current_train_items 215168.
I0304 19:31:18.112156 22849303695488 run.py:483] Algo bellman_ford step 6724 current loss 0.093497, current_train_items 215200.
I0304 19:31:18.131884 22849303695488 run.py:483] Algo bellman_ford step 6725 current loss 0.007217, current_train_items 215232.
I0304 19:31:18.148387 22849303695488 run.py:483] Algo bellman_ford step 6726 current loss 0.038519, current_train_items 215264.
I0304 19:31:18.172444 22849303695488 run.py:483] Algo bellman_ford step 6727 current loss 0.032990, current_train_items 215296.
I0304 19:31:18.203877 22849303695488 run.py:483] Algo bellman_ford step 6728 current loss 0.035230, current_train_items 215328.
I0304 19:31:18.236437 22849303695488 run.py:483] Algo bellman_ford step 6729 current loss 0.060261, current_train_items 215360.
I0304 19:31:18.256315 22849303695488 run.py:483] Algo bellman_ford step 6730 current loss 0.004622, current_train_items 215392.
I0304 19:31:18.272596 22849303695488 run.py:483] Algo bellman_ford step 6731 current loss 0.011734, current_train_items 215424.
I0304 19:31:18.297358 22849303695488 run.py:483] Algo bellman_ford step 6732 current loss 0.103672, current_train_items 215456.
I0304 19:31:18.327126 22849303695488 run.py:483] Algo bellman_ford step 6733 current loss 0.048681, current_train_items 215488.
I0304 19:31:18.359057 22849303695488 run.py:483] Algo bellman_ford step 6734 current loss 0.063462, current_train_items 215520.
I0304 19:31:18.379030 22849303695488 run.py:483] Algo bellman_ford step 6735 current loss 0.002867, current_train_items 215552.
I0304 19:31:18.396096 22849303695488 run.py:483] Algo bellman_ford step 6736 current loss 0.044582, current_train_items 215584.
I0304 19:31:18.421050 22849303695488 run.py:483] Algo bellman_ford step 6737 current loss 0.086695, current_train_items 215616.
I0304 19:31:18.451931 22849303695488 run.py:483] Algo bellman_ford step 6738 current loss 0.053652, current_train_items 215648.
I0304 19:31:18.487639 22849303695488 run.py:483] Algo bellman_ford step 6739 current loss 0.120960, current_train_items 215680.
I0304 19:31:18.507532 22849303695488 run.py:483] Algo bellman_ford step 6740 current loss 0.020903, current_train_items 215712.
I0304 19:31:18.524254 22849303695488 run.py:483] Algo bellman_ford step 6741 current loss 0.022818, current_train_items 215744.
I0304 19:31:18.549155 22849303695488 run.py:483] Algo bellman_ford step 6742 current loss 0.115676, current_train_items 215776.
I0304 19:31:18.579709 22849303695488 run.py:483] Algo bellman_ford step 6743 current loss 0.087347, current_train_items 215808.
I0304 19:31:18.614589 22849303695488 run.py:483] Algo bellman_ford step 6744 current loss 0.119192, current_train_items 215840.
I0304 19:31:18.634633 22849303695488 run.py:483] Algo bellman_ford step 6745 current loss 0.003522, current_train_items 215872.
I0304 19:31:18.651209 22849303695488 run.py:483] Algo bellman_ford step 6746 current loss 0.051024, current_train_items 215904.
I0304 19:31:18.675704 22849303695488 run.py:483] Algo bellman_ford step 6747 current loss 0.025563, current_train_items 215936.
I0304 19:31:18.706539 22849303695488 run.py:483] Algo bellman_ford step 6748 current loss 0.052633, current_train_items 215968.
I0304 19:31:18.741065 22849303695488 run.py:483] Algo bellman_ford step 6749 current loss 0.088179, current_train_items 216000.
I0304 19:31:18.760832 22849303695488 run.py:483] Algo bellman_ford step 6750 current loss 0.004062, current_train_items 216032.
I0304 19:31:18.769243 22849303695488 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0304 19:31:18.769366 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.991, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:31:18.799281 22849303695488 run.py:483] Algo bellman_ford step 6751 current loss 0.012825, current_train_items 216064.
I0304 19:31:18.824489 22849303695488 run.py:483] Algo bellman_ford step 6752 current loss 0.052096, current_train_items 216096.
I0304 19:31:18.857758 22849303695488 run.py:483] Algo bellman_ford step 6753 current loss 0.062356, current_train_items 216128.
I0304 19:31:18.893158 22849303695488 run.py:483] Algo bellman_ford step 6754 current loss 0.064955, current_train_items 216160.
I0304 19:31:18.914095 22849303695488 run.py:483] Algo bellman_ford step 6755 current loss 0.031301, current_train_items 216192.
I0304 19:31:18.931083 22849303695488 run.py:483] Algo bellman_ford step 6756 current loss 0.037377, current_train_items 216224.
I0304 19:31:18.955502 22849303695488 run.py:483] Algo bellman_ford step 6757 current loss 0.032031, current_train_items 216256.
I0304 19:31:18.987751 22849303695488 run.py:483] Algo bellman_ford step 6758 current loss 0.106336, current_train_items 216288.
I0304 19:31:19.023807 22849303695488 run.py:483] Algo bellman_ford step 6759 current loss 0.128656, current_train_items 216320.
I0304 19:31:19.044543 22849303695488 run.py:483] Algo bellman_ford step 6760 current loss 0.007859, current_train_items 216352.
I0304 19:31:19.061621 22849303695488 run.py:483] Algo bellman_ford step 6761 current loss 0.016309, current_train_items 216384.
I0304 19:31:19.086037 22849303695488 run.py:483] Algo bellman_ford step 6762 current loss 0.021763, current_train_items 216416.
I0304 19:31:19.118181 22849303695488 run.py:483] Algo bellman_ford step 6763 current loss 0.056129, current_train_items 216448.
I0304 19:31:19.151679 22849303695488 run.py:483] Algo bellman_ford step 6764 current loss 0.058681, current_train_items 216480.
I0304 19:31:19.172238 22849303695488 run.py:483] Algo bellman_ford step 6765 current loss 0.041324, current_train_items 216512.
I0304 19:31:19.189213 22849303695488 run.py:483] Algo bellman_ford step 6766 current loss 0.046059, current_train_items 216544.
I0304 19:31:19.213505 22849303695488 run.py:483] Algo bellman_ford step 6767 current loss 0.035253, current_train_items 216576.
I0304 19:31:19.244619 22849303695488 run.py:483] Algo bellman_ford step 6768 current loss 0.030792, current_train_items 216608.
I0304 19:31:19.278219 22849303695488 run.py:483] Algo bellman_ford step 6769 current loss 0.120779, current_train_items 216640.
I0304 19:31:19.298839 22849303695488 run.py:483] Algo bellman_ford step 6770 current loss 0.004347, current_train_items 216672.
I0304 19:31:19.315541 22849303695488 run.py:483] Algo bellman_ford step 6771 current loss 0.014071, current_train_items 216704.
I0304 19:31:19.338618 22849303695488 run.py:483] Algo bellman_ford step 6772 current loss 0.026910, current_train_items 216736.
I0304 19:31:19.370373 22849303695488 run.py:483] Algo bellman_ford step 6773 current loss 0.051397, current_train_items 216768.
I0304 19:31:19.403058 22849303695488 run.py:483] Algo bellman_ford step 6774 current loss 0.085977, current_train_items 216800.
I0304 19:31:19.423805 22849303695488 run.py:483] Algo bellman_ford step 6775 current loss 0.010323, current_train_items 216832.
I0304 19:31:19.440765 22849303695488 run.py:483] Algo bellman_ford step 6776 current loss 0.025927, current_train_items 216864.
I0304 19:31:19.464320 22849303695488 run.py:483] Algo bellman_ford step 6777 current loss 0.026105, current_train_items 216896.
I0304 19:31:19.497557 22849303695488 run.py:483] Algo bellman_ford step 6778 current loss 0.061132, current_train_items 216928.
I0304 19:31:19.531357 22849303695488 run.py:483] Algo bellman_ford step 6779 current loss 0.055098, current_train_items 216960.
I0304 19:31:19.551682 22849303695488 run.py:483] Algo bellman_ford step 6780 current loss 0.007483, current_train_items 216992.
I0304 19:31:19.568319 22849303695488 run.py:483] Algo bellman_ford step 6781 current loss 0.010771, current_train_items 217024.
I0304 19:31:19.592031 22849303695488 run.py:483] Algo bellman_ford step 6782 current loss 0.063245, current_train_items 217056.
I0304 19:31:19.624904 22849303695488 run.py:483] Algo bellman_ford step 6783 current loss 0.105654, current_train_items 217088.
I0304 19:31:19.658676 22849303695488 run.py:483] Algo bellman_ford step 6784 current loss 0.130784, current_train_items 217120.
I0304 19:31:19.679199 22849303695488 run.py:483] Algo bellman_ford step 6785 current loss 0.007268, current_train_items 217152.
I0304 19:31:19.695557 22849303695488 run.py:483] Algo bellman_ford step 6786 current loss 0.010118, current_train_items 217184.
I0304 19:31:19.719370 22849303695488 run.py:483] Algo bellman_ford step 6787 current loss 0.054283, current_train_items 217216.
I0304 19:31:19.749815 22849303695488 run.py:483] Algo bellman_ford step 6788 current loss 0.051028, current_train_items 217248.
I0304 19:31:19.784600 22849303695488 run.py:483] Algo bellman_ford step 6789 current loss 0.185686, current_train_items 217280.
I0304 19:31:19.805018 22849303695488 run.py:483] Algo bellman_ford step 6790 current loss 0.007772, current_train_items 217312.
I0304 19:31:19.821498 22849303695488 run.py:483] Algo bellman_ford step 6791 current loss 0.008189, current_train_items 217344.
I0304 19:31:19.845735 22849303695488 run.py:483] Algo bellman_ford step 6792 current loss 0.033992, current_train_items 217376.
I0304 19:31:19.876899 22849303695488 run.py:483] Algo bellman_ford step 6793 current loss 0.071037, current_train_items 217408.
I0304 19:31:19.910819 22849303695488 run.py:483] Algo bellman_ford step 6794 current loss 0.109474, current_train_items 217440.
I0304 19:31:19.930743 22849303695488 run.py:483] Algo bellman_ford step 6795 current loss 0.005813, current_train_items 217472.
I0304 19:31:19.947067 22849303695488 run.py:483] Algo bellman_ford step 6796 current loss 0.007671, current_train_items 217504.
I0304 19:31:19.972456 22849303695488 run.py:483] Algo bellman_ford step 6797 current loss 0.037097, current_train_items 217536.
I0304 19:31:20.003535 22849303695488 run.py:483] Algo bellman_ford step 6798 current loss 0.083076, current_train_items 217568.
I0304 19:31:20.037360 22849303695488 run.py:483] Algo bellman_ford step 6799 current loss 0.074297, current_train_items 217600.
I0304 19:31:20.058012 22849303695488 run.py:483] Algo bellman_ford step 6800 current loss 0.015987, current_train_items 217632.
I0304 19:31:20.066014 22849303695488 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0304 19:31:20.066125 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:31:20.083100 22849303695488 run.py:483] Algo bellman_ford step 6801 current loss 0.040151, current_train_items 217664.
I0304 19:31:20.108172 22849303695488 run.py:483] Algo bellman_ford step 6802 current loss 0.037569, current_train_items 217696.
I0304 19:31:20.142046 22849303695488 run.py:483] Algo bellman_ford step 6803 current loss 0.065057, current_train_items 217728.
I0304 19:31:20.176847 22849303695488 run.py:483] Algo bellman_ford step 6804 current loss 0.068820, current_train_items 217760.
I0304 19:31:20.197310 22849303695488 run.py:483] Algo bellman_ford step 6805 current loss 0.002839, current_train_items 217792.
I0304 19:31:20.213763 22849303695488 run.py:483] Algo bellman_ford step 6806 current loss 0.009527, current_train_items 217824.
I0304 19:31:20.238583 22849303695488 run.py:483] Algo bellman_ford step 6807 current loss 0.034610, current_train_items 217856.
I0304 19:31:20.269411 22849303695488 run.py:483] Algo bellman_ford step 6808 current loss 0.034603, current_train_items 217888.
I0304 19:31:20.303688 22849303695488 run.py:483] Algo bellman_ford step 6809 current loss 0.078480, current_train_items 217920.
I0304 19:31:20.323744 22849303695488 run.py:483] Algo bellman_ford step 6810 current loss 0.038217, current_train_items 217952.
I0304 19:31:20.339866 22849303695488 run.py:483] Algo bellman_ford step 6811 current loss 0.014614, current_train_items 217984.
I0304 19:31:20.365270 22849303695488 run.py:483] Algo bellman_ford step 6812 current loss 0.052872, current_train_items 218016.
I0304 19:31:20.396668 22849303695488 run.py:483] Algo bellman_ford step 6813 current loss 0.084421, current_train_items 218048.
I0304 19:31:20.428751 22849303695488 run.py:483] Algo bellman_ford step 6814 current loss 0.126146, current_train_items 218080.
I0304 19:31:20.448762 22849303695488 run.py:483] Algo bellman_ford step 6815 current loss 0.008835, current_train_items 218112.
I0304 19:31:20.465413 22849303695488 run.py:483] Algo bellman_ford step 6816 current loss 0.014208, current_train_items 218144.
I0304 19:31:20.488672 22849303695488 run.py:483] Algo bellman_ford step 6817 current loss 0.086955, current_train_items 218176.
I0304 19:31:20.520749 22849303695488 run.py:483] Algo bellman_ford step 6818 current loss 0.091039, current_train_items 218208.
I0304 19:31:20.556939 22849303695488 run.py:483] Algo bellman_ford step 6819 current loss 0.096432, current_train_items 218240.
I0304 19:31:20.577147 22849303695488 run.py:483] Algo bellman_ford step 6820 current loss 0.034175, current_train_items 218272.
I0304 19:31:20.593613 22849303695488 run.py:483] Algo bellman_ford step 6821 current loss 0.008686, current_train_items 218304.
I0304 19:31:20.617618 22849303695488 run.py:483] Algo bellman_ford step 6822 current loss 0.065338, current_train_items 218336.
I0304 19:31:20.649619 22849303695488 run.py:483] Algo bellman_ford step 6823 current loss 0.077828, current_train_items 218368.
I0304 19:31:20.685173 22849303695488 run.py:483] Algo bellman_ford step 6824 current loss 0.115034, current_train_items 218400.
I0304 19:31:20.705323 22849303695488 run.py:483] Algo bellman_ford step 6825 current loss 0.004673, current_train_items 218432.
I0304 19:31:20.722020 22849303695488 run.py:483] Algo bellman_ford step 6826 current loss 0.020759, current_train_items 218464.
I0304 19:31:20.746304 22849303695488 run.py:483] Algo bellman_ford step 6827 current loss 0.063172, current_train_items 218496.
I0304 19:31:20.777453 22849303695488 run.py:483] Algo bellman_ford step 6828 current loss 0.071333, current_train_items 218528.
I0304 19:31:20.813612 22849303695488 run.py:483] Algo bellman_ford step 6829 current loss 0.140173, current_train_items 218560.
I0304 19:31:20.833643 22849303695488 run.py:483] Algo bellman_ford step 6830 current loss 0.009951, current_train_items 218592.
I0304 19:31:20.849993 22849303695488 run.py:483] Algo bellman_ford step 6831 current loss 0.015766, current_train_items 218624.
I0304 19:31:20.876128 22849303695488 run.py:483] Algo bellman_ford step 6832 current loss 0.064623, current_train_items 218656.
I0304 19:31:20.907708 22849303695488 run.py:483] Algo bellman_ford step 6833 current loss 0.069372, current_train_items 218688.
I0304 19:31:20.942069 22849303695488 run.py:483] Algo bellman_ford step 6834 current loss 0.057918, current_train_items 218720.
I0304 19:31:20.962368 22849303695488 run.py:483] Algo bellman_ford step 6835 current loss 0.003758, current_train_items 218752.
I0304 19:31:20.979041 22849303695488 run.py:483] Algo bellman_ford step 6836 current loss 0.018764, current_train_items 218784.
I0304 19:31:21.003570 22849303695488 run.py:483] Algo bellman_ford step 6837 current loss 0.034201, current_train_items 218816.
I0304 19:31:21.036234 22849303695488 run.py:483] Algo bellman_ford step 6838 current loss 0.037439, current_train_items 218848.
I0304 19:31:21.068417 22849303695488 run.py:483] Algo bellman_ford step 6839 current loss 0.058086, current_train_items 218880.
I0304 19:31:21.088467 22849303695488 run.py:483] Algo bellman_ford step 6840 current loss 0.003654, current_train_items 218912.
I0304 19:31:21.105541 22849303695488 run.py:483] Algo bellman_ford step 6841 current loss 0.035280, current_train_items 218944.
I0304 19:31:21.130659 22849303695488 run.py:483] Algo bellman_ford step 6842 current loss 0.046318, current_train_items 218976.
I0304 19:31:21.161367 22849303695488 run.py:483] Algo bellman_ford step 6843 current loss 0.037230, current_train_items 219008.
I0304 19:31:21.195042 22849303695488 run.py:483] Algo bellman_ford step 6844 current loss 0.094378, current_train_items 219040.
I0304 19:31:21.215050 22849303695488 run.py:483] Algo bellman_ford step 6845 current loss 0.011409, current_train_items 219072.
I0304 19:31:21.232236 22849303695488 run.py:483] Algo bellman_ford step 6846 current loss 0.035041, current_train_items 219104.
I0304 19:31:21.257568 22849303695488 run.py:483] Algo bellman_ford step 6847 current loss 0.065585, current_train_items 219136.
I0304 19:31:21.290103 22849303695488 run.py:483] Algo bellman_ford step 6848 current loss 0.075054, current_train_items 219168.
I0304 19:31:21.325325 22849303695488 run.py:483] Algo bellman_ford step 6849 current loss 0.093561, current_train_items 219200.
I0304 19:31:21.345230 22849303695488 run.py:483] Algo bellman_ford step 6850 current loss 0.005009, current_train_items 219232.
I0304 19:31:21.353601 22849303695488 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0304 19:31:21.353710 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:31:21.370966 22849303695488 run.py:483] Algo bellman_ford step 6851 current loss 0.041520, current_train_items 219264.
I0304 19:31:21.395939 22849303695488 run.py:483] Algo bellman_ford step 6852 current loss 0.047607, current_train_items 219296.
I0304 19:31:21.428149 22849303695488 run.py:483] Algo bellman_ford step 6853 current loss 0.062498, current_train_items 219328.
I0304 19:31:21.463616 22849303695488 run.py:483] Algo bellman_ford step 6854 current loss 0.060922, current_train_items 219360.
I0304 19:31:21.483835 22849303695488 run.py:483] Algo bellman_ford step 6855 current loss 0.006705, current_train_items 219392.
I0304 19:31:21.499845 22849303695488 run.py:483] Algo bellman_ford step 6856 current loss 0.011190, current_train_items 219424.
I0304 19:31:21.524578 22849303695488 run.py:483] Algo bellman_ford step 6857 current loss 0.057419, current_train_items 219456.
I0304 19:31:21.555757 22849303695488 run.py:483] Algo bellman_ford step 6858 current loss 0.084655, current_train_items 219488.
I0304 19:31:21.590371 22849303695488 run.py:483] Algo bellman_ford step 6859 current loss 0.103832, current_train_items 219520.
I0304 19:31:21.610964 22849303695488 run.py:483] Algo bellman_ford step 6860 current loss 0.012729, current_train_items 219552.
I0304 19:31:21.627467 22849303695488 run.py:483] Algo bellman_ford step 6861 current loss 0.020036, current_train_items 219584.
I0304 19:31:21.650999 22849303695488 run.py:483] Algo bellman_ford step 6862 current loss 0.032166, current_train_items 219616.
I0304 19:31:21.682018 22849303695488 run.py:483] Algo bellman_ford step 6863 current loss 0.072660, current_train_items 219648.
I0304 19:31:21.714730 22849303695488 run.py:483] Algo bellman_ford step 6864 current loss 0.086051, current_train_items 219680.
I0304 19:31:21.735081 22849303695488 run.py:483] Algo bellman_ford step 6865 current loss 0.022739, current_train_items 219712.
I0304 19:31:21.751650 22849303695488 run.py:483] Algo bellman_ford step 6866 current loss 0.015833, current_train_items 219744.
I0304 19:31:21.777270 22849303695488 run.py:483] Algo bellman_ford step 6867 current loss 0.063279, current_train_items 219776.
I0304 19:31:21.809188 22849303695488 run.py:483] Algo bellman_ford step 6868 current loss 0.082251, current_train_items 219808.
I0304 19:31:21.845838 22849303695488 run.py:483] Algo bellman_ford step 6869 current loss 0.118520, current_train_items 219840.
I0304 19:31:21.866761 22849303695488 run.py:483] Algo bellman_ford step 6870 current loss 0.019438, current_train_items 219872.
I0304 19:31:21.883602 22849303695488 run.py:483] Algo bellman_ford step 6871 current loss 0.017046, current_train_items 219904.
I0304 19:31:21.907376 22849303695488 run.py:483] Algo bellman_ford step 6872 current loss 0.027050, current_train_items 219936.
I0304 19:31:21.938092 22849303695488 run.py:483] Algo bellman_ford step 6873 current loss 0.075174, current_train_items 219968.
I0304 19:31:21.972244 22849303695488 run.py:483] Algo bellman_ford step 6874 current loss 0.068462, current_train_items 220000.
I0304 19:31:21.992778 22849303695488 run.py:483] Algo bellman_ford step 6875 current loss 0.011867, current_train_items 220032.
I0304 19:31:22.009166 22849303695488 run.py:483] Algo bellman_ford step 6876 current loss 0.008234, current_train_items 220064.
I0304 19:31:22.033523 22849303695488 run.py:483] Algo bellman_ford step 6877 current loss 0.055079, current_train_items 220096.
I0304 19:31:22.065390 22849303695488 run.py:483] Algo bellman_ford step 6878 current loss 0.029715, current_train_items 220128.
I0304 19:31:22.100141 22849303695488 run.py:483] Algo bellman_ford step 6879 current loss 0.066266, current_train_items 220160.
I0304 19:31:22.120498 22849303695488 run.py:483] Algo bellman_ford step 6880 current loss 0.008536, current_train_items 220192.
I0304 19:31:22.136876 22849303695488 run.py:483] Algo bellman_ford step 6881 current loss 0.006619, current_train_items 220224.
I0304 19:31:22.162261 22849303695488 run.py:483] Algo bellman_ford step 6882 current loss 0.025336, current_train_items 220256.
I0304 19:31:22.193013 22849303695488 run.py:483] Algo bellman_ford step 6883 current loss 0.042602, current_train_items 220288.
I0304 19:31:22.226886 22849303695488 run.py:483] Algo bellman_ford step 6884 current loss 0.096108, current_train_items 220320.
I0304 19:31:22.247405 22849303695488 run.py:483] Algo bellman_ford step 6885 current loss 0.002016, current_train_items 220352.
I0304 19:31:22.264241 22849303695488 run.py:483] Algo bellman_ford step 6886 current loss 0.016133, current_train_items 220384.
I0304 19:31:22.288367 22849303695488 run.py:483] Algo bellman_ford step 6887 current loss 0.037209, current_train_items 220416.
I0304 19:31:22.321187 22849303695488 run.py:483] Algo bellman_ford step 6888 current loss 0.063079, current_train_items 220448.
I0304 19:31:22.355479 22849303695488 run.py:483] Algo bellman_ford step 6889 current loss 0.027908, current_train_items 220480.
I0304 19:31:22.375811 22849303695488 run.py:483] Algo bellman_ford step 6890 current loss 0.043838, current_train_items 220512.
I0304 19:31:22.392729 22849303695488 run.py:483] Algo bellman_ford step 6891 current loss 0.029154, current_train_items 220544.
I0304 19:31:22.417172 22849303695488 run.py:483] Algo bellman_ford step 6892 current loss 0.046211, current_train_items 220576.
I0304 19:31:22.449234 22849303695488 run.py:483] Algo bellman_ford step 6893 current loss 0.092658, current_train_items 220608.
I0304 19:31:22.482399 22849303695488 run.py:483] Algo bellman_ford step 6894 current loss 0.078029, current_train_items 220640.
I0304 19:31:22.502481 22849303695488 run.py:483] Algo bellman_ford step 6895 current loss 0.002255, current_train_items 220672.
I0304 19:31:22.518938 22849303695488 run.py:483] Algo bellman_ford step 6896 current loss 0.007601, current_train_items 220704.
I0304 19:31:22.544106 22849303695488 run.py:483] Algo bellman_ford step 6897 current loss 0.076779, current_train_items 220736.
I0304 19:31:22.574960 22849303695488 run.py:483] Algo bellman_ford step 6898 current loss 0.042630, current_train_items 220768.
I0304 19:31:22.609896 22849303695488 run.py:483] Algo bellman_ford step 6899 current loss 0.111737, current_train_items 220800.
I0304 19:31:22.630381 22849303695488 run.py:483] Algo bellman_ford step 6900 current loss 0.006400, current_train_items 220832.
I0304 19:31:22.638566 22849303695488 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0304 19:31:22.638674 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:31:22.655485 22849303695488 run.py:483] Algo bellman_ford step 6901 current loss 0.004587, current_train_items 220864.
I0304 19:31:22.680277 22849303695488 run.py:483] Algo bellman_ford step 6902 current loss 0.049438, current_train_items 220896.
I0304 19:31:22.713465 22849303695488 run.py:483] Algo bellman_ford step 6903 current loss 0.056687, current_train_items 220928.
I0304 19:31:22.749721 22849303695488 run.py:483] Algo bellman_ford step 6904 current loss 0.050898, current_train_items 220960.
I0304 19:31:22.770202 22849303695488 run.py:483] Algo bellman_ford step 6905 current loss 0.003097, current_train_items 220992.
I0304 19:31:22.786116 22849303695488 run.py:483] Algo bellman_ford step 6906 current loss 0.012907, current_train_items 221024.
I0304 19:31:22.810932 22849303695488 run.py:483] Algo bellman_ford step 6907 current loss 0.043985, current_train_items 221056.
I0304 19:31:22.843893 22849303695488 run.py:483] Algo bellman_ford step 6908 current loss 0.083086, current_train_items 221088.
I0304 19:31:22.879686 22849303695488 run.py:483] Algo bellman_ford step 6909 current loss 0.137406, current_train_items 221120.
I0304 19:31:22.900139 22849303695488 run.py:483] Algo bellman_ford step 6910 current loss 0.010765, current_train_items 221152.
I0304 19:31:22.917338 22849303695488 run.py:483] Algo bellman_ford step 6911 current loss 0.045575, current_train_items 221184.
I0304 19:31:22.941032 22849303695488 run.py:483] Algo bellman_ford step 6912 current loss 0.057697, current_train_items 221216.
I0304 19:31:22.973734 22849303695488 run.py:483] Algo bellman_ford step 6913 current loss 0.077481, current_train_items 221248.
I0304 19:31:23.009665 22849303695488 run.py:483] Algo bellman_ford step 6914 current loss 0.071885, current_train_items 221280.
I0304 19:31:23.029474 22849303695488 run.py:483] Algo bellman_ford step 6915 current loss 0.009686, current_train_items 221312.
I0304 19:31:23.045697 22849303695488 run.py:483] Algo bellman_ford step 6916 current loss 0.003878, current_train_items 221344.
I0304 19:31:23.071365 22849303695488 run.py:483] Algo bellman_ford step 6917 current loss 0.107162, current_train_items 221376.
I0304 19:31:23.103902 22849303695488 run.py:483] Algo bellman_ford step 6918 current loss 0.206983, current_train_items 221408.
I0304 19:31:23.139770 22849303695488 run.py:483] Algo bellman_ford step 6919 current loss 0.150993, current_train_items 221440.
I0304 19:31:23.159838 22849303695488 run.py:483] Algo bellman_ford step 6920 current loss 0.005510, current_train_items 221472.
I0304 19:31:23.176356 22849303695488 run.py:483] Algo bellman_ford step 6921 current loss 0.004923, current_train_items 221504.
I0304 19:31:23.200110 22849303695488 run.py:483] Algo bellman_ford step 6922 current loss 0.027936, current_train_items 221536.
I0304 19:31:23.232695 22849303695488 run.py:483] Algo bellman_ford step 6923 current loss 0.113204, current_train_items 221568.
I0304 19:31:23.266742 22849303695488 run.py:483] Algo bellman_ford step 6924 current loss 0.077396, current_train_items 221600.
I0304 19:31:23.287058 22849303695488 run.py:483] Algo bellman_ford step 6925 current loss 0.003353, current_train_items 221632.
I0304 19:31:23.303499 22849303695488 run.py:483] Algo bellman_ford step 6926 current loss 0.012525, current_train_items 221664.
I0304 19:31:23.328359 22849303695488 run.py:483] Algo bellman_ford step 6927 current loss 0.072921, current_train_items 221696.
I0304 19:31:23.360892 22849303695488 run.py:483] Algo bellman_ford step 6928 current loss 0.059745, current_train_items 221728.
I0304 19:31:23.395230 22849303695488 run.py:483] Algo bellman_ford step 6929 current loss 0.062123, current_train_items 221760.
I0304 19:31:23.415389 22849303695488 run.py:483] Algo bellman_ford step 6930 current loss 0.002296, current_train_items 221792.
I0304 19:31:23.432070 22849303695488 run.py:483] Algo bellman_ford step 6931 current loss 0.015070, current_train_items 221824.
I0304 19:31:23.455776 22849303695488 run.py:483] Algo bellman_ford step 6932 current loss 0.044473, current_train_items 221856.
I0304 19:31:23.488338 22849303695488 run.py:483] Algo bellman_ford step 6933 current loss 0.057507, current_train_items 221888.
I0304 19:31:23.523986 22849303695488 run.py:483] Algo bellman_ford step 6934 current loss 0.083940, current_train_items 221920.
I0304 19:31:23.544353 22849303695488 run.py:483] Algo bellman_ford step 6935 current loss 0.002756, current_train_items 221952.
I0304 19:31:23.560715 22849303695488 run.py:483] Algo bellman_ford step 6936 current loss 0.009288, current_train_items 221984.
I0304 19:31:23.585346 22849303695488 run.py:483] Algo bellman_ford step 6937 current loss 0.022422, current_train_items 222016.
I0304 19:31:23.617291 22849303695488 run.py:483] Algo bellman_ford step 6938 current loss 0.040602, current_train_items 222048.
I0304 19:31:23.650840 22849303695488 run.py:483] Algo bellman_ford step 6939 current loss 0.063463, current_train_items 222080.
I0304 19:31:23.671025 22849303695488 run.py:483] Algo bellman_ford step 6940 current loss 0.010640, current_train_items 222112.
I0304 19:31:23.687929 22849303695488 run.py:483] Algo bellman_ford step 6941 current loss 0.026518, current_train_items 222144.
I0304 19:31:23.713037 22849303695488 run.py:483] Algo bellman_ford step 6942 current loss 0.094166, current_train_items 222176.
I0304 19:31:23.745458 22849303695488 run.py:483] Algo bellman_ford step 6943 current loss 0.102453, current_train_items 222208.
I0304 19:31:23.781176 22849303695488 run.py:483] Algo bellman_ford step 6944 current loss 0.119718, current_train_items 222240.
I0304 19:31:23.801108 22849303695488 run.py:483] Algo bellman_ford step 6945 current loss 0.011703, current_train_items 222272.
I0304 19:31:23.817789 22849303695488 run.py:483] Algo bellman_ford step 6946 current loss 0.016883, current_train_items 222304.
I0304 19:31:23.842259 22849303695488 run.py:483] Algo bellman_ford step 6947 current loss 0.115330, current_train_items 222336.
I0304 19:31:23.872550 22849303695488 run.py:483] Algo bellman_ford step 6948 current loss 0.091208, current_train_items 222368.
I0304 19:31:23.907279 22849303695488 run.py:483] Algo bellman_ford step 6949 current loss 0.113193, current_train_items 222400.
I0304 19:31:23.927295 22849303695488 run.py:483] Algo bellman_ford step 6950 current loss 0.007152, current_train_items 222432.
I0304 19:31:23.935633 22849303695488 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0304 19:31:23.935743 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:23.952982 22849303695488 run.py:483] Algo bellman_ford step 6951 current loss 0.019266, current_train_items 222464.
I0304 19:31:23.978428 22849303695488 run.py:483] Algo bellman_ford step 6952 current loss 0.071623, current_train_items 222496.
I0304 19:31:24.010039 22849303695488 run.py:483] Algo bellman_ford step 6953 current loss 0.062711, current_train_items 222528.
I0304 19:31:24.045336 22849303695488 run.py:483] Algo bellman_ford step 6954 current loss 0.087007, current_train_items 222560.
I0304 19:31:24.065977 22849303695488 run.py:483] Algo bellman_ford step 6955 current loss 0.013475, current_train_items 222592.
I0304 19:31:24.082409 22849303695488 run.py:483] Algo bellman_ford step 6956 current loss 0.015236, current_train_items 222624.
I0304 19:31:24.107990 22849303695488 run.py:483] Algo bellman_ford step 6957 current loss 0.084106, current_train_items 222656.
I0304 19:31:24.139707 22849303695488 run.py:483] Algo bellman_ford step 6958 current loss 0.050212, current_train_items 222688.
I0304 19:31:24.172484 22849303695488 run.py:483] Algo bellman_ford step 6959 current loss 0.049722, current_train_items 222720.
I0304 19:31:24.192883 22849303695488 run.py:483] Algo bellman_ford step 6960 current loss 0.002237, current_train_items 222752.
I0304 19:31:24.209887 22849303695488 run.py:483] Algo bellman_ford step 6961 current loss 0.025376, current_train_items 222784.
I0304 19:31:24.233533 22849303695488 run.py:483] Algo bellman_ford step 6962 current loss 0.049640, current_train_items 222816.
I0304 19:31:24.265705 22849303695488 run.py:483] Algo bellman_ford step 6963 current loss 0.067411, current_train_items 222848.
I0304 19:31:24.301328 22849303695488 run.py:483] Algo bellman_ford step 6964 current loss 0.074344, current_train_items 222880.
I0304 19:31:24.321488 22849303695488 run.py:483] Algo bellman_ford step 6965 current loss 0.004774, current_train_items 222912.
I0304 19:31:24.338102 22849303695488 run.py:483] Algo bellman_ford step 6966 current loss 0.028759, current_train_items 222944.
I0304 19:31:24.362960 22849303695488 run.py:483] Algo bellman_ford step 6967 current loss 0.016667, current_train_items 222976.
I0304 19:31:24.393842 22849303695488 run.py:483] Algo bellman_ford step 6968 current loss 0.043785, current_train_items 223008.
I0304 19:31:24.428168 22849303695488 run.py:483] Algo bellman_ford step 6969 current loss 0.054404, current_train_items 223040.
I0304 19:31:24.448511 22849303695488 run.py:483] Algo bellman_ford step 6970 current loss 0.003006, current_train_items 223072.
I0304 19:31:24.465140 22849303695488 run.py:483] Algo bellman_ford step 6971 current loss 0.025343, current_train_items 223104.
I0304 19:31:24.489035 22849303695488 run.py:483] Algo bellman_ford step 6972 current loss 0.037404, current_train_items 223136.
I0304 19:31:24.520800 22849303695488 run.py:483] Algo bellman_ford step 6973 current loss 0.039747, current_train_items 223168.
I0304 19:31:24.555446 22849303695488 run.py:483] Algo bellman_ford step 6974 current loss 0.042680, current_train_items 223200.
I0304 19:31:24.575656 22849303695488 run.py:483] Algo bellman_ford step 6975 current loss 0.002734, current_train_items 223232.
I0304 19:31:24.592532 22849303695488 run.py:483] Algo bellman_ford step 6976 current loss 0.020317, current_train_items 223264.
I0304 19:31:24.616372 22849303695488 run.py:483] Algo bellman_ford step 6977 current loss 0.044966, current_train_items 223296.
I0304 19:31:24.647232 22849303695488 run.py:483] Algo bellman_ford step 6978 current loss 0.048755, current_train_items 223328.
I0304 19:31:24.682348 22849303695488 run.py:483] Algo bellman_ford step 6979 current loss 0.032577, current_train_items 223360.
I0304 19:31:24.702584 22849303695488 run.py:483] Algo bellman_ford step 6980 current loss 0.033806, current_train_items 223392.
I0304 19:31:24.719669 22849303695488 run.py:483] Algo bellman_ford step 6981 current loss 0.007738, current_train_items 223424.
I0304 19:31:24.742996 22849303695488 run.py:483] Algo bellman_ford step 6982 current loss 0.035892, current_train_items 223456.
I0304 19:31:24.776172 22849303695488 run.py:483] Algo bellman_ford step 6983 current loss 0.086221, current_train_items 223488.
I0304 19:31:24.810383 22849303695488 run.py:483] Algo bellman_ford step 6984 current loss 0.072722, current_train_items 223520.
I0304 19:31:24.831079 22849303695488 run.py:483] Algo bellman_ford step 6985 current loss 0.003266, current_train_items 223552.
I0304 19:31:24.847510 22849303695488 run.py:483] Algo bellman_ford step 6986 current loss 0.014112, current_train_items 223584.
I0304 19:31:24.871783 22849303695488 run.py:483] Algo bellman_ford step 6987 current loss 0.036681, current_train_items 223616.
I0304 19:31:24.903344 22849303695488 run.py:483] Algo bellman_ford step 6988 current loss 0.073127, current_train_items 223648.
I0304 19:31:24.939548 22849303695488 run.py:483] Algo bellman_ford step 6989 current loss 0.080077, current_train_items 223680.
I0304 19:31:24.960208 22849303695488 run.py:483] Algo bellman_ford step 6990 current loss 0.004696, current_train_items 223712.
I0304 19:31:24.976670 22849303695488 run.py:483] Algo bellman_ford step 6991 current loss 0.035629, current_train_items 223744.
I0304 19:31:25.000922 22849303695488 run.py:483] Algo bellman_ford step 6992 current loss 0.059121, current_train_items 223776.
I0304 19:31:25.033821 22849303695488 run.py:483] Algo bellman_ford step 6993 current loss 0.094859, current_train_items 223808.
I0304 19:31:25.070691 22849303695488 run.py:483] Algo bellman_ford step 6994 current loss 0.073780, current_train_items 223840.
I0304 19:31:25.090647 22849303695488 run.py:483] Algo bellman_ford step 6995 current loss 0.002277, current_train_items 223872.
I0304 19:31:25.107537 22849303695488 run.py:483] Algo bellman_ford step 6996 current loss 0.017481, current_train_items 223904.
I0304 19:31:25.130676 22849303695488 run.py:483] Algo bellman_ford step 6997 current loss 0.051485, current_train_items 223936.
I0304 19:31:25.161446 22849303695488 run.py:483] Algo bellman_ford step 6998 current loss 0.033819, current_train_items 223968.
I0304 19:31:25.195824 22849303695488 run.py:483] Algo bellman_ford step 6999 current loss 0.112866, current_train_items 224000.
I0304 19:31:25.216183 22849303695488 run.py:483] Algo bellman_ford step 7000 current loss 0.003259, current_train_items 224032.
I0304 19:31:25.224297 22849303695488 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0304 19:31:25.224405 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:25.241312 22849303695488 run.py:483] Algo bellman_ford step 7001 current loss 0.028058, current_train_items 224064.
I0304 19:31:25.265110 22849303695488 run.py:483] Algo bellman_ford step 7002 current loss 0.016338, current_train_items 224096.
I0304 19:31:25.295650 22849303695488 run.py:483] Algo bellman_ford step 7003 current loss 0.055117, current_train_items 224128.
I0304 19:31:25.329361 22849303695488 run.py:483] Algo bellman_ford step 7004 current loss 0.080295, current_train_items 224160.
I0304 19:31:25.349828 22849303695488 run.py:483] Algo bellman_ford step 7005 current loss 0.002760, current_train_items 224192.
I0304 19:31:25.366359 22849303695488 run.py:483] Algo bellman_ford step 7006 current loss 0.034488, current_train_items 224224.
I0304 19:31:25.391617 22849303695488 run.py:483] Algo bellman_ford step 7007 current loss 0.052425, current_train_items 224256.
I0304 19:31:25.422096 22849303695488 run.py:483] Algo bellman_ford step 7008 current loss 0.054007, current_train_items 224288.
I0304 19:31:25.458682 22849303695488 run.py:483] Algo bellman_ford step 7009 current loss 0.070714, current_train_items 224320.
I0304 19:31:25.479153 22849303695488 run.py:483] Algo bellman_ford step 7010 current loss 0.007951, current_train_items 224352.
I0304 19:31:25.495841 22849303695488 run.py:483] Algo bellman_ford step 7011 current loss 0.008990, current_train_items 224384.
I0304 19:31:25.520091 22849303695488 run.py:483] Algo bellman_ford step 7012 current loss 0.045377, current_train_items 224416.
I0304 19:31:25.551242 22849303695488 run.py:483] Algo bellman_ford step 7013 current loss 0.025351, current_train_items 224448.
I0304 19:31:25.585884 22849303695488 run.py:483] Algo bellman_ford step 7014 current loss 0.076330, current_train_items 224480.
I0304 19:31:25.606262 22849303695488 run.py:483] Algo bellman_ford step 7015 current loss 0.002011, current_train_items 224512.
I0304 19:31:25.622579 22849303695488 run.py:483] Algo bellman_ford step 7016 current loss 0.012015, current_train_items 224544.
I0304 19:31:25.647349 22849303695488 run.py:483] Algo bellman_ford step 7017 current loss 0.053746, current_train_items 224576.
I0304 19:31:25.678442 22849303695488 run.py:483] Algo bellman_ford step 7018 current loss 0.046467, current_train_items 224608.
I0304 19:31:25.711970 22849303695488 run.py:483] Algo bellman_ford step 7019 current loss 0.052105, current_train_items 224640.
I0304 19:31:25.731735 22849303695488 run.py:483] Algo bellman_ford step 7020 current loss 0.002277, current_train_items 224672.
I0304 19:31:25.748201 22849303695488 run.py:483] Algo bellman_ford step 7021 current loss 0.023768, current_train_items 224704.
I0304 19:31:25.772221 22849303695488 run.py:483] Algo bellman_ford step 7022 current loss 0.081869, current_train_items 224736.
I0304 19:31:25.803375 22849303695488 run.py:483] Algo bellman_ford step 7023 current loss 0.100085, current_train_items 224768.
I0304 19:31:25.838581 22849303695488 run.py:483] Algo bellman_ford step 7024 current loss 0.133081, current_train_items 224800.
I0304 19:31:25.858843 22849303695488 run.py:483] Algo bellman_ford step 7025 current loss 0.002151, current_train_items 224832.
I0304 19:31:25.875700 22849303695488 run.py:483] Algo bellman_ford step 7026 current loss 0.018672, current_train_items 224864.
I0304 19:31:25.901430 22849303695488 run.py:483] Algo bellman_ford step 7027 current loss 0.065063, current_train_items 224896.
I0304 19:31:25.935458 22849303695488 run.py:483] Algo bellman_ford step 7028 current loss 0.076402, current_train_items 224928.
I0304 19:31:25.967531 22849303695488 run.py:483] Algo bellman_ford step 7029 current loss 0.038863, current_train_items 224960.
I0304 19:31:25.987839 22849303695488 run.py:483] Algo bellman_ford step 7030 current loss 0.002982, current_train_items 224992.
I0304 19:31:26.004418 22849303695488 run.py:483] Algo bellman_ford step 7031 current loss 0.010390, current_train_items 225024.
I0304 19:31:26.028671 22849303695488 run.py:483] Algo bellman_ford step 7032 current loss 0.055888, current_train_items 225056.
I0304 19:31:26.060091 22849303695488 run.py:483] Algo bellman_ford step 7033 current loss 0.043353, current_train_items 225088.
I0304 19:31:26.096253 22849303695488 run.py:483] Algo bellman_ford step 7034 current loss 0.060679, current_train_items 225120.
I0304 19:31:26.116683 22849303695488 run.py:483] Algo bellman_ford step 7035 current loss 0.007526, current_train_items 225152.
I0304 19:31:26.133291 22849303695488 run.py:483] Algo bellman_ford step 7036 current loss 0.015119, current_train_items 225184.
I0304 19:31:26.157365 22849303695488 run.py:483] Algo bellman_ford step 7037 current loss 0.093637, current_train_items 225216.
I0304 19:31:26.189800 22849303695488 run.py:483] Algo bellman_ford step 7038 current loss 0.122537, current_train_items 225248.
I0304 19:31:26.223470 22849303695488 run.py:483] Algo bellman_ford step 7039 current loss 0.057311, current_train_items 225280.
I0304 19:31:26.243748 22849303695488 run.py:483] Algo bellman_ford step 7040 current loss 0.051003, current_train_items 225312.
I0304 19:31:26.260326 22849303695488 run.py:483] Algo bellman_ford step 7041 current loss 0.025435, current_train_items 225344.
I0304 19:31:26.284739 22849303695488 run.py:483] Algo bellman_ford step 7042 current loss 0.066380, current_train_items 225376.
I0304 19:31:26.316862 22849303695488 run.py:483] Algo bellman_ford step 7043 current loss 0.097016, current_train_items 225408.
I0304 19:31:26.351424 22849303695488 run.py:483] Algo bellman_ford step 7044 current loss 0.093149, current_train_items 225440.
I0304 19:31:26.371690 22849303695488 run.py:483] Algo bellman_ford step 7045 current loss 0.004448, current_train_items 225472.
I0304 19:31:26.388588 22849303695488 run.py:483] Algo bellman_ford step 7046 current loss 0.028472, current_train_items 225504.
I0304 19:31:26.412496 22849303695488 run.py:483] Algo bellman_ford step 7047 current loss 0.105881, current_train_items 225536.
I0304 19:31:26.445074 22849303695488 run.py:483] Algo bellman_ford step 7048 current loss 0.090136, current_train_items 225568.
I0304 19:31:26.478860 22849303695488 run.py:483] Algo bellman_ford step 7049 current loss 0.079136, current_train_items 225600.
I0304 19:31:26.499202 22849303695488 run.py:483] Algo bellman_ford step 7050 current loss 0.003112, current_train_items 225632.
I0304 19:31:26.507491 22849303695488 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0304 19:31:26.507599 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:31:26.524366 22849303695488 run.py:483] Algo bellman_ford step 7051 current loss 0.007917, current_train_items 225664.
I0304 19:31:26.550067 22849303695488 run.py:483] Algo bellman_ford step 7052 current loss 0.082211, current_train_items 225696.
I0304 19:31:26.583834 22849303695488 run.py:483] Algo bellman_ford step 7053 current loss 0.092833, current_train_items 225728.
I0304 19:31:26.619779 22849303695488 run.py:483] Algo bellman_ford step 7054 current loss 0.083494, current_train_items 225760.
I0304 19:31:26.640370 22849303695488 run.py:483] Algo bellman_ford step 7055 current loss 0.006013, current_train_items 225792.
I0304 19:31:26.656965 22849303695488 run.py:483] Algo bellman_ford step 7056 current loss 0.013817, current_train_items 225824.
I0304 19:31:26.682244 22849303695488 run.py:483] Algo bellman_ford step 7057 current loss 0.068670, current_train_items 225856.
I0304 19:31:26.714036 22849303695488 run.py:483] Algo bellman_ford step 7058 current loss 0.074155, current_train_items 225888.
I0304 19:31:26.750663 22849303695488 run.py:483] Algo bellman_ford step 7059 current loss 0.117964, current_train_items 225920.
I0304 19:31:26.771023 22849303695488 run.py:483] Algo bellman_ford step 7060 current loss 0.006864, current_train_items 225952.
I0304 19:31:26.787702 22849303695488 run.py:483] Algo bellman_ford step 7061 current loss 0.017963, current_train_items 225984.
I0304 19:31:26.812620 22849303695488 run.py:483] Algo bellman_ford step 7062 current loss 0.067311, current_train_items 226016.
I0304 19:31:26.842149 22849303695488 run.py:483] Algo bellman_ford step 7063 current loss 0.048245, current_train_items 226048.
I0304 19:31:26.876041 22849303695488 run.py:483] Algo bellman_ford step 7064 current loss 0.048374, current_train_items 226080.
I0304 19:31:26.896036 22849303695488 run.py:483] Algo bellman_ford step 7065 current loss 0.003096, current_train_items 226112.
I0304 19:31:26.913022 22849303695488 run.py:483] Algo bellman_ford step 7066 current loss 0.034087, current_train_items 226144.
I0304 19:31:26.937521 22849303695488 run.py:483] Algo bellman_ford step 7067 current loss 0.021744, current_train_items 226176.
I0304 19:31:26.971217 22849303695488 run.py:483] Algo bellman_ford step 7068 current loss 0.093445, current_train_items 226208.
I0304 19:31:27.007104 22849303695488 run.py:483] Algo bellman_ford step 7069 current loss 0.072932, current_train_items 226240.
I0304 19:31:27.027498 22849303695488 run.py:483] Algo bellman_ford step 7070 current loss 0.068952, current_train_items 226272.
I0304 19:31:27.043958 22849303695488 run.py:483] Algo bellman_ford step 7071 current loss 0.021462, current_train_items 226304.
I0304 19:31:27.066951 22849303695488 run.py:483] Algo bellman_ford step 7072 current loss 0.065669, current_train_items 226336.
I0304 19:31:27.097947 22849303695488 run.py:483] Algo bellman_ford step 7073 current loss 0.049496, current_train_items 226368.
I0304 19:31:27.132187 22849303695488 run.py:483] Algo bellman_ford step 7074 current loss 0.060205, current_train_items 226400.
I0304 19:31:27.152767 22849303695488 run.py:483] Algo bellman_ford step 7075 current loss 0.004735, current_train_items 226432.
I0304 19:31:27.168913 22849303695488 run.py:483] Algo bellman_ford step 7076 current loss 0.013243, current_train_items 226464.
I0304 19:31:27.192816 22849303695488 run.py:483] Algo bellman_ford step 7077 current loss 0.076753, current_train_items 226496.
I0304 19:31:27.225322 22849303695488 run.py:483] Algo bellman_ford step 7078 current loss 0.059382, current_train_items 226528.
I0304 19:31:27.260507 22849303695488 run.py:483] Algo bellman_ford step 7079 current loss 0.067637, current_train_items 226560.
I0304 19:31:27.280503 22849303695488 run.py:483] Algo bellman_ford step 7080 current loss 0.003177, current_train_items 226592.
I0304 19:31:27.296833 22849303695488 run.py:483] Algo bellman_ford step 7081 current loss 0.014713, current_train_items 226624.
I0304 19:31:27.321232 22849303695488 run.py:483] Algo bellman_ford step 7082 current loss 0.070243, current_train_items 226656.
I0304 19:31:27.353477 22849303695488 run.py:483] Algo bellman_ford step 7083 current loss 0.103665, current_train_items 226688.
I0304 19:31:27.387243 22849303695488 run.py:483] Algo bellman_ford step 7084 current loss 0.113832, current_train_items 226720.
I0304 19:31:27.408014 22849303695488 run.py:483] Algo bellman_ford step 7085 current loss 0.004535, current_train_items 226752.
I0304 19:31:27.424488 22849303695488 run.py:483] Algo bellman_ford step 7086 current loss 0.016233, current_train_items 226784.
I0304 19:31:27.447823 22849303695488 run.py:483] Algo bellman_ford step 7087 current loss 0.015952, current_train_items 226816.
I0304 19:31:27.479661 22849303695488 run.py:483] Algo bellman_ford step 7088 current loss 0.068854, current_train_items 226848.
I0304 19:31:27.513733 22849303695488 run.py:483] Algo bellman_ford step 7089 current loss 0.043316, current_train_items 226880.
I0304 19:31:27.534575 22849303695488 run.py:483] Algo bellman_ford step 7090 current loss 0.029922, current_train_items 226912.
I0304 19:31:27.550884 22849303695488 run.py:483] Algo bellman_ford step 7091 current loss 0.014325, current_train_items 226944.
I0304 19:31:27.575331 22849303695488 run.py:483] Algo bellman_ford step 7092 current loss 0.053156, current_train_items 226976.
I0304 19:31:27.608136 22849303695488 run.py:483] Algo bellman_ford step 7093 current loss 0.081324, current_train_items 227008.
I0304 19:31:27.640440 22849303695488 run.py:483] Algo bellman_ford step 7094 current loss 0.054504, current_train_items 227040.
I0304 19:31:27.660821 22849303695488 run.py:483] Algo bellman_ford step 7095 current loss 0.004400, current_train_items 227072.
I0304 19:31:27.677297 22849303695488 run.py:483] Algo bellman_ford step 7096 current loss 0.013844, current_train_items 227104.
I0304 19:31:27.700292 22849303695488 run.py:483] Algo bellman_ford step 7097 current loss 0.047501, current_train_items 227136.
I0304 19:31:27.732517 22849303695488 run.py:483] Algo bellman_ford step 7098 current loss 0.077056, current_train_items 227168.
I0304 19:31:27.765990 22849303695488 run.py:483] Algo bellman_ford step 7099 current loss 0.072853, current_train_items 227200.
I0304 19:31:27.786855 22849303695488 run.py:483] Algo bellman_ford step 7100 current loss 0.004307, current_train_items 227232.
I0304 19:31:27.794923 22849303695488 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0304 19:31:27.795044 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:31:27.812149 22849303695488 run.py:483] Algo bellman_ford step 7101 current loss 0.055855, current_train_items 227264.
I0304 19:31:27.836869 22849303695488 run.py:483] Algo bellman_ford step 7102 current loss 0.037525, current_train_items 227296.
I0304 19:31:27.869546 22849303695488 run.py:483] Algo bellman_ford step 7103 current loss 0.079323, current_train_items 227328.
I0304 19:31:27.905597 22849303695488 run.py:483] Algo bellman_ford step 7104 current loss 0.126610, current_train_items 227360.
I0304 19:31:27.926111 22849303695488 run.py:483] Algo bellman_ford step 7105 current loss 0.034023, current_train_items 227392.
I0304 19:31:27.942535 22849303695488 run.py:483] Algo bellman_ford step 7106 current loss 0.067128, current_train_items 227424.
I0304 19:31:27.967216 22849303695488 run.py:483] Algo bellman_ford step 7107 current loss 0.037295, current_train_items 227456.
I0304 19:31:27.998879 22849303695488 run.py:483] Algo bellman_ford step 7108 current loss 0.075204, current_train_items 227488.
I0304 19:31:28.035290 22849303695488 run.py:483] Algo bellman_ford step 7109 current loss 0.070608, current_train_items 227520.
I0304 19:31:28.055488 22849303695488 run.py:483] Algo bellman_ford step 7110 current loss 0.007051, current_train_items 227552.
I0304 19:31:28.072269 22849303695488 run.py:483] Algo bellman_ford step 7111 current loss 0.017562, current_train_items 227584.
I0304 19:31:28.096928 22849303695488 run.py:483] Algo bellman_ford step 7112 current loss 0.030133, current_train_items 227616.
I0304 19:31:28.129535 22849303695488 run.py:483] Algo bellman_ford step 7113 current loss 0.089361, current_train_items 227648.
I0304 19:31:28.164268 22849303695488 run.py:483] Algo bellman_ford step 7114 current loss 0.068472, current_train_items 227680.
I0304 19:31:28.184400 22849303695488 run.py:483] Algo bellman_ford step 7115 current loss 0.005087, current_train_items 227712.
I0304 19:31:28.200995 22849303695488 run.py:483] Algo bellman_ford step 7116 current loss 0.023542, current_train_items 227744.
I0304 19:31:28.225763 22849303695488 run.py:483] Algo bellman_ford step 7117 current loss 0.038371, current_train_items 227776.
I0304 19:31:28.258110 22849303695488 run.py:483] Algo bellman_ford step 7118 current loss 0.061940, current_train_items 227808.
I0304 19:31:28.292804 22849303695488 run.py:483] Algo bellman_ford step 7119 current loss 0.041173, current_train_items 227840.
I0304 19:31:28.312587 22849303695488 run.py:483] Algo bellman_ford step 7120 current loss 0.045836, current_train_items 227872.
I0304 19:31:28.329256 22849303695488 run.py:483] Algo bellman_ford step 7121 current loss 0.027289, current_train_items 227904.
I0304 19:31:28.353416 22849303695488 run.py:483] Algo bellman_ford step 7122 current loss 0.095369, current_train_items 227936.
I0304 19:31:28.384732 22849303695488 run.py:483] Algo bellman_ford step 7123 current loss 0.037509, current_train_items 227968.
I0304 19:31:28.419755 22849303695488 run.py:483] Algo bellman_ford step 7124 current loss 0.063660, current_train_items 228000.
I0304 19:31:28.439748 22849303695488 run.py:483] Algo bellman_ford step 7125 current loss 0.003269, current_train_items 228032.
I0304 19:31:28.455761 22849303695488 run.py:483] Algo bellman_ford step 7126 current loss 0.007667, current_train_items 228064.
I0304 19:31:28.479165 22849303695488 run.py:483] Algo bellman_ford step 7127 current loss 0.035716, current_train_items 228096.
I0304 19:31:28.510500 22849303695488 run.py:483] Algo bellman_ford step 7128 current loss 0.080121, current_train_items 228128.
I0304 19:31:28.546130 22849303695488 run.py:483] Algo bellman_ford step 7129 current loss 0.108923, current_train_items 228160.
I0304 19:31:28.566379 22849303695488 run.py:483] Algo bellman_ford step 7130 current loss 0.008144, current_train_items 228192.
I0304 19:31:28.583209 22849303695488 run.py:483] Algo bellman_ford step 7131 current loss 0.018824, current_train_items 228224.
I0304 19:31:28.608421 22849303695488 run.py:483] Algo bellman_ford step 7132 current loss 0.069362, current_train_items 228256.
I0304 19:31:28.640403 22849303695488 run.py:483] Algo bellman_ford step 7133 current loss 0.066575, current_train_items 228288.
I0304 19:31:28.674144 22849303695488 run.py:483] Algo bellman_ford step 7134 current loss 0.065430, current_train_items 228320.
I0304 19:31:28.694253 22849303695488 run.py:483] Algo bellman_ford step 7135 current loss 0.032894, current_train_items 228352.
I0304 19:31:28.710624 22849303695488 run.py:483] Algo bellman_ford step 7136 current loss 0.020429, current_train_items 228384.
I0304 19:31:28.734231 22849303695488 run.py:483] Algo bellman_ford step 7137 current loss 0.065711, current_train_items 228416.
I0304 19:31:28.764350 22849303695488 run.py:483] Algo bellman_ford step 7138 current loss 0.055860, current_train_items 228448.
I0304 19:31:28.801140 22849303695488 run.py:483] Algo bellman_ford step 7139 current loss 0.075746, current_train_items 228480.
I0304 19:31:28.821301 22849303695488 run.py:483] Algo bellman_ford step 7140 current loss 0.002658, current_train_items 228512.
I0304 19:31:28.837595 22849303695488 run.py:483] Algo bellman_ford step 7141 current loss 0.010802, current_train_items 228544.
I0304 19:31:28.862756 22849303695488 run.py:483] Algo bellman_ford step 7142 current loss 0.047334, current_train_items 228576.
I0304 19:31:28.892668 22849303695488 run.py:483] Algo bellman_ford step 7143 current loss 0.032209, current_train_items 228608.
I0304 19:31:28.925946 22849303695488 run.py:483] Algo bellman_ford step 7144 current loss 0.065205, current_train_items 228640.
I0304 19:31:28.946146 22849303695488 run.py:483] Algo bellman_ford step 7145 current loss 0.011898, current_train_items 228672.
I0304 19:31:28.962988 22849303695488 run.py:483] Algo bellman_ford step 7146 current loss 0.017660, current_train_items 228704.
I0304 19:31:28.988197 22849303695488 run.py:483] Algo bellman_ford step 7147 current loss 0.045662, current_train_items 228736.
I0304 19:31:29.018913 22849303695488 run.py:483] Algo bellman_ford step 7148 current loss 0.044650, current_train_items 228768.
I0304 19:31:29.050601 22849303695488 run.py:483] Algo bellman_ford step 7149 current loss 0.064337, current_train_items 228800.
I0304 19:31:29.070652 22849303695488 run.py:483] Algo bellman_ford step 7150 current loss 0.002733, current_train_items 228832.
I0304 19:31:29.078924 22849303695488 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0304 19:31:29.079044 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:31:29.096840 22849303695488 run.py:483] Algo bellman_ford step 7151 current loss 0.025511, current_train_items 228864.
I0304 19:31:29.122588 22849303695488 run.py:483] Algo bellman_ford step 7152 current loss 0.028687, current_train_items 228896.
I0304 19:31:29.154811 22849303695488 run.py:483] Algo bellman_ford step 7153 current loss 0.053167, current_train_items 228928.
I0304 19:31:29.188786 22849303695488 run.py:483] Algo bellman_ford step 7154 current loss 0.035063, current_train_items 228960.
I0304 19:31:29.209392 22849303695488 run.py:483] Algo bellman_ford step 7155 current loss 0.015751, current_train_items 228992.
I0304 19:31:29.226151 22849303695488 run.py:483] Algo bellman_ford step 7156 current loss 0.043797, current_train_items 229024.
I0304 19:31:29.250208 22849303695488 run.py:483] Algo bellman_ford step 7157 current loss 0.057227, current_train_items 229056.
I0304 19:31:29.281535 22849303695488 run.py:483] Algo bellman_ford step 7158 current loss 0.035703, current_train_items 229088.
I0304 19:31:29.316265 22849303695488 run.py:483] Algo bellman_ford step 7159 current loss 0.083008, current_train_items 229120.
I0304 19:31:29.337044 22849303695488 run.py:483] Algo bellman_ford step 7160 current loss 0.005451, current_train_items 229152.
I0304 19:31:29.354075 22849303695488 run.py:483] Algo bellman_ford step 7161 current loss 0.061939, current_train_items 229184.
W0304 19:31:29.369391 22849303695488 samplers.py:155] Increasing hint lengh from 10 to 11
I0304 19:31:36.507168 22849303695488 run.py:483] Algo bellman_ford step 7162 current loss 0.047811, current_train_items 229216.
I0304 19:31:36.540071 22849303695488 run.py:483] Algo bellman_ford step 7163 current loss 0.067968, current_train_items 229248.
I0304 19:31:36.576097 22849303695488 run.py:483] Algo bellman_ford step 7164 current loss 0.077777, current_train_items 229280.
I0304 19:31:36.596612 22849303695488 run.py:483] Algo bellman_ford step 7165 current loss 0.006435, current_train_items 229312.
I0304 19:31:36.613534 22849303695488 run.py:483] Algo bellman_ford step 7166 current loss 0.035231, current_train_items 229344.
I0304 19:31:36.638375 22849303695488 run.py:483] Algo bellman_ford step 7167 current loss 0.050419, current_train_items 229376.
I0304 19:31:36.670829 22849303695488 run.py:483] Algo bellman_ford step 7168 current loss 0.042578, current_train_items 229408.
I0304 19:31:36.702820 22849303695488 run.py:483] Algo bellman_ford step 7169 current loss 0.068760, current_train_items 229440.
I0304 19:31:36.723618 22849303695488 run.py:483] Algo bellman_ford step 7170 current loss 0.007107, current_train_items 229472.
I0304 19:31:36.740865 22849303695488 run.py:483] Algo bellman_ford step 7171 current loss 0.015887, current_train_items 229504.
I0304 19:31:36.764982 22849303695488 run.py:483] Algo bellman_ford step 7172 current loss 0.094340, current_train_items 229536.
I0304 19:31:36.797060 22849303695488 run.py:483] Algo bellman_ford step 7173 current loss 0.065167, current_train_items 229568.
I0304 19:31:36.832251 22849303695488 run.py:483] Algo bellman_ford step 7174 current loss 0.098221, current_train_items 229600.
I0304 19:31:36.853184 22849303695488 run.py:483] Algo bellman_ford step 7175 current loss 0.003909, current_train_items 229632.
I0304 19:31:36.869719 22849303695488 run.py:483] Algo bellman_ford step 7176 current loss 0.017085, current_train_items 229664.
I0304 19:31:36.894247 22849303695488 run.py:483] Algo bellman_ford step 7177 current loss 0.056290, current_train_items 229696.
I0304 19:31:36.928783 22849303695488 run.py:483] Algo bellman_ford step 7178 current loss 0.072390, current_train_items 229728.
I0304 19:31:36.964355 22849303695488 run.py:483] Algo bellman_ford step 7179 current loss 0.103193, current_train_items 229760.
I0304 19:31:36.984513 22849303695488 run.py:483] Algo bellman_ford step 7180 current loss 0.004297, current_train_items 229792.
I0304 19:31:37.001366 22849303695488 run.py:483] Algo bellman_ford step 7181 current loss 0.028987, current_train_items 229824.
I0304 19:31:37.026814 22849303695488 run.py:483] Algo bellman_ford step 7182 current loss 0.043894, current_train_items 229856.
I0304 19:31:37.058676 22849303695488 run.py:483] Algo bellman_ford step 7183 current loss 0.043587, current_train_items 229888.
I0304 19:31:37.096070 22849303695488 run.py:483] Algo bellman_ford step 7184 current loss 0.082091, current_train_items 229920.
I0304 19:31:37.116651 22849303695488 run.py:483] Algo bellman_ford step 7185 current loss 0.005319, current_train_items 229952.
I0304 19:31:37.133494 22849303695488 run.py:483] Algo bellman_ford step 7186 current loss 0.015218, current_train_items 229984.
I0304 19:31:37.158517 22849303695488 run.py:483] Algo bellman_ford step 7187 current loss 0.056469, current_train_items 230016.
I0304 19:31:37.190988 22849303695488 run.py:483] Algo bellman_ford step 7188 current loss 0.063694, current_train_items 230048.
I0304 19:31:37.227218 22849303695488 run.py:483] Algo bellman_ford step 7189 current loss 0.089420, current_train_items 230080.
I0304 19:31:37.248193 22849303695488 run.py:483] Algo bellman_ford step 7190 current loss 0.002159, current_train_items 230112.
I0304 19:31:37.265248 22849303695488 run.py:483] Algo bellman_ford step 7191 current loss 0.023557, current_train_items 230144.
I0304 19:31:37.289631 22849303695488 run.py:483] Algo bellman_ford step 7192 current loss 0.023507, current_train_items 230176.
I0304 19:31:37.322182 22849303695488 run.py:483] Algo bellman_ford step 7193 current loss 0.034085, current_train_items 230208.
I0304 19:31:37.355916 22849303695488 run.py:483] Algo bellman_ford step 7194 current loss 0.079155, current_train_items 230240.
I0304 19:31:37.376022 22849303695488 run.py:483] Algo bellman_ford step 7195 current loss 0.001729, current_train_items 230272.
I0304 19:31:37.392277 22849303695488 run.py:483] Algo bellman_ford step 7196 current loss 0.006960, current_train_items 230304.
I0304 19:31:37.416616 22849303695488 run.py:483] Algo bellman_ford step 7197 current loss 0.018272, current_train_items 230336.
I0304 19:31:37.449351 22849303695488 run.py:483] Algo bellman_ford step 7198 current loss 0.045908, current_train_items 230368.
I0304 19:31:37.483958 22849303695488 run.py:483] Algo bellman_ford step 7199 current loss 0.063380, current_train_items 230400.
I0304 19:31:37.504611 22849303695488 run.py:483] Algo bellman_ford step 7200 current loss 0.004419, current_train_items 230432.
I0304 19:31:37.514391 22849303695488 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0304 19:31:37.514535 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:37.531975 22849303695488 run.py:483] Algo bellman_ford step 7201 current loss 0.020832, current_train_items 230464.
I0304 19:31:37.556598 22849303695488 run.py:483] Algo bellman_ford step 7202 current loss 0.022789, current_train_items 230496.
I0304 19:31:37.589450 22849303695488 run.py:483] Algo bellman_ford step 7203 current loss 0.074346, current_train_items 230528.
I0304 19:31:37.623850 22849303695488 run.py:483] Algo bellman_ford step 7204 current loss 0.051719, current_train_items 230560.
I0304 19:31:37.644151 22849303695488 run.py:483] Algo bellman_ford step 7205 current loss 0.004224, current_train_items 230592.
I0304 19:31:37.660217 22849303695488 run.py:483] Algo bellman_ford step 7206 current loss 0.020523, current_train_items 230624.
I0304 19:31:37.684933 22849303695488 run.py:483] Algo bellman_ford step 7207 current loss 0.091405, current_train_items 230656.
I0304 19:31:37.717091 22849303695488 run.py:483] Algo bellman_ford step 7208 current loss 0.049700, current_train_items 230688.
I0304 19:31:37.752368 22849303695488 run.py:483] Algo bellman_ford step 7209 current loss 0.075648, current_train_items 230720.
I0304 19:31:37.772515 22849303695488 run.py:483] Algo bellman_ford step 7210 current loss 0.005290, current_train_items 230752.
I0304 19:31:37.788897 22849303695488 run.py:483] Algo bellman_ford step 7211 current loss 0.012736, current_train_items 230784.
I0304 19:31:37.812846 22849303695488 run.py:483] Algo bellman_ford step 7212 current loss 0.120425, current_train_items 230816.
I0304 19:31:37.844480 22849303695488 run.py:483] Algo bellman_ford step 7213 current loss 0.104822, current_train_items 230848.
I0304 19:31:37.876905 22849303695488 run.py:483] Algo bellman_ford step 7214 current loss 0.130933, current_train_items 230880.
I0304 19:31:37.896750 22849303695488 run.py:483] Algo bellman_ford step 7215 current loss 0.009185, current_train_items 230912.
I0304 19:31:37.913354 22849303695488 run.py:483] Algo bellman_ford step 7216 current loss 0.007199, current_train_items 230944.
I0304 19:31:37.938064 22849303695488 run.py:483] Algo bellman_ford step 7217 current loss 0.039308, current_train_items 230976.
I0304 19:31:37.970207 22849303695488 run.py:483] Algo bellman_ford step 7218 current loss 0.055683, current_train_items 231008.
I0304 19:31:38.003562 22849303695488 run.py:483] Algo bellman_ford step 7219 current loss 0.067255, current_train_items 231040.
I0304 19:31:38.023639 22849303695488 run.py:483] Algo bellman_ford step 7220 current loss 0.020038, current_train_items 231072.
I0304 19:31:38.040286 22849303695488 run.py:483] Algo bellman_ford step 7221 current loss 0.016515, current_train_items 231104.
I0304 19:31:38.065865 22849303695488 run.py:483] Algo bellman_ford step 7222 current loss 0.059211, current_train_items 231136.
I0304 19:31:38.097701 22849303695488 run.py:483] Algo bellman_ford step 7223 current loss 0.143916, current_train_items 231168.
I0304 19:31:38.132874 22849303695488 run.py:483] Algo bellman_ford step 7224 current loss 0.124646, current_train_items 231200.
I0304 19:31:38.153312 22849303695488 run.py:483] Algo bellman_ford step 7225 current loss 0.004531, current_train_items 231232.
I0304 19:31:38.169974 22849303695488 run.py:483] Algo bellman_ford step 7226 current loss 0.034136, current_train_items 231264.
I0304 19:31:38.195840 22849303695488 run.py:483] Algo bellman_ford step 7227 current loss 0.080233, current_train_items 231296.
I0304 19:31:38.227841 22849303695488 run.py:483] Algo bellman_ford step 7228 current loss 0.069904, current_train_items 231328.
I0304 19:31:38.260108 22849303695488 run.py:483] Algo bellman_ford step 7229 current loss 0.072728, current_train_items 231360.
I0304 19:31:38.280401 22849303695488 run.py:483] Algo bellman_ford step 7230 current loss 0.004447, current_train_items 231392.
I0304 19:31:38.297170 22849303695488 run.py:483] Algo bellman_ford step 7231 current loss 0.012410, current_train_items 231424.
I0304 19:31:38.319404 22849303695488 run.py:483] Algo bellman_ford step 7232 current loss 0.018512, current_train_items 231456.
I0304 19:31:38.352064 22849303695488 run.py:483] Algo bellman_ford step 7233 current loss 0.076544, current_train_items 231488.
I0304 19:31:38.385254 22849303695488 run.py:483] Algo bellman_ford step 7234 current loss 0.081569, current_train_items 231520.
I0304 19:31:38.405394 22849303695488 run.py:483] Algo bellman_ford step 7235 current loss 0.002245, current_train_items 231552.
I0304 19:31:38.422269 22849303695488 run.py:483] Algo bellman_ford step 7236 current loss 0.019054, current_train_items 231584.
I0304 19:31:38.447517 22849303695488 run.py:483] Algo bellman_ford step 7237 current loss 0.027716, current_train_items 231616.
I0304 19:31:38.481316 22849303695488 run.py:483] Algo bellman_ford step 7238 current loss 0.109818, current_train_items 231648.
I0304 19:31:38.515244 22849303695488 run.py:483] Algo bellman_ford step 7239 current loss 0.053015, current_train_items 231680.
I0304 19:31:38.535215 22849303695488 run.py:483] Algo bellman_ford step 7240 current loss 0.009806, current_train_items 231712.
I0304 19:31:38.551928 22849303695488 run.py:483] Algo bellman_ford step 7241 current loss 0.009085, current_train_items 231744.
I0304 19:31:38.577296 22849303695488 run.py:483] Algo bellman_ford step 7242 current loss 0.068091, current_train_items 231776.
I0304 19:31:38.609492 22849303695488 run.py:483] Algo bellman_ford step 7243 current loss 0.062942, current_train_items 231808.
I0304 19:31:38.645423 22849303695488 run.py:483] Algo bellman_ford step 7244 current loss 0.055436, current_train_items 231840.
I0304 19:31:38.665247 22849303695488 run.py:483] Algo bellman_ford step 7245 current loss 0.002294, current_train_items 231872.
I0304 19:31:38.681809 22849303695488 run.py:483] Algo bellman_ford step 7246 current loss 0.009294, current_train_items 231904.
I0304 19:31:38.706790 22849303695488 run.py:483] Algo bellman_ford step 7247 current loss 0.032195, current_train_items 231936.
I0304 19:31:38.740267 22849303695488 run.py:483] Algo bellman_ford step 7248 current loss 0.088397, current_train_items 231968.
I0304 19:31:38.774856 22849303695488 run.py:483] Algo bellman_ford step 7249 current loss 0.069105, current_train_items 232000.
I0304 19:31:38.795104 22849303695488 run.py:483] Algo bellman_ford step 7250 current loss 0.029248, current_train_items 232032.
I0304 19:31:38.803853 22849303695488 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0304 19:31:38.803963 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:38.821300 22849303695488 run.py:483] Algo bellman_ford step 7251 current loss 0.048298, current_train_items 232064.
I0304 19:31:38.846180 22849303695488 run.py:483] Algo bellman_ford step 7252 current loss 0.041966, current_train_items 232096.
I0304 19:31:38.877753 22849303695488 run.py:483] Algo bellman_ford step 7253 current loss 0.041538, current_train_items 232128.
I0304 19:31:38.912628 22849303695488 run.py:483] Algo bellman_ford step 7254 current loss 0.071776, current_train_items 232160.
I0304 19:31:38.932914 22849303695488 run.py:483] Algo bellman_ford step 7255 current loss 0.004512, current_train_items 232192.
I0304 19:31:38.948876 22849303695488 run.py:483] Algo bellman_ford step 7256 current loss 0.033390, current_train_items 232224.
I0304 19:31:38.973568 22849303695488 run.py:483] Algo bellman_ford step 7257 current loss 0.074238, current_train_items 232256.
I0304 19:31:39.008270 22849303695488 run.py:483] Algo bellman_ford step 7258 current loss 0.092654, current_train_items 232288.
I0304 19:31:39.042931 22849303695488 run.py:483] Algo bellman_ford step 7259 current loss 0.124676, current_train_items 232320.
I0304 19:31:39.063524 22849303695488 run.py:483] Algo bellman_ford step 7260 current loss 0.004816, current_train_items 232352.
I0304 19:31:39.080184 22849303695488 run.py:483] Algo bellman_ford step 7261 current loss 0.015979, current_train_items 232384.
I0304 19:31:39.104646 22849303695488 run.py:483] Algo bellman_ford step 7262 current loss 0.038845, current_train_items 232416.
I0304 19:31:39.137498 22849303695488 run.py:483] Algo bellman_ford step 7263 current loss 0.077275, current_train_items 232448.
I0304 19:31:39.171877 22849303695488 run.py:483] Algo bellman_ford step 7264 current loss 0.085416, current_train_items 232480.
I0304 19:31:39.192086 22849303695488 run.py:483] Algo bellman_ford step 7265 current loss 0.006491, current_train_items 232512.
I0304 19:31:39.208594 22849303695488 run.py:483] Algo bellman_ford step 7266 current loss 0.038725, current_train_items 232544.
I0304 19:31:39.233039 22849303695488 run.py:483] Algo bellman_ford step 7267 current loss 0.040724, current_train_items 232576.
I0304 19:31:39.265557 22849303695488 run.py:483] Algo bellman_ford step 7268 current loss 0.051333, current_train_items 232608.
I0304 19:31:39.299064 22849303695488 run.py:483] Algo bellman_ford step 7269 current loss 0.058761, current_train_items 232640.
I0304 19:31:39.319691 22849303695488 run.py:483] Algo bellman_ford step 7270 current loss 0.016865, current_train_items 232672.
I0304 19:31:39.336760 22849303695488 run.py:483] Algo bellman_ford step 7271 current loss 0.041548, current_train_items 232704.
I0304 19:31:39.360926 22849303695488 run.py:483] Algo bellman_ford step 7272 current loss 0.025023, current_train_items 232736.
I0304 19:31:39.393365 22849303695488 run.py:483] Algo bellman_ford step 7273 current loss 0.049116, current_train_items 232768.
I0304 19:31:39.427583 22849303695488 run.py:483] Algo bellman_ford step 7274 current loss 0.102664, current_train_items 232800.
I0304 19:31:39.448253 22849303695488 run.py:483] Algo bellman_ford step 7275 current loss 0.003356, current_train_items 232832.
I0304 19:31:39.464864 22849303695488 run.py:483] Algo bellman_ford step 7276 current loss 0.016930, current_train_items 232864.
I0304 19:31:39.489309 22849303695488 run.py:483] Algo bellman_ford step 7277 current loss 0.056051, current_train_items 232896.
I0304 19:31:39.521839 22849303695488 run.py:483] Algo bellman_ford step 7278 current loss 0.088434, current_train_items 232928.
I0304 19:31:39.556446 22849303695488 run.py:483] Algo bellman_ford step 7279 current loss 0.086419, current_train_items 232960.
I0304 19:31:39.576786 22849303695488 run.py:483] Algo bellman_ford step 7280 current loss 0.033319, current_train_items 232992.
I0304 19:31:39.593851 22849303695488 run.py:483] Algo bellman_ford step 7281 current loss 0.020276, current_train_items 233024.
I0304 19:31:39.617881 22849303695488 run.py:483] Algo bellman_ford step 7282 current loss 0.046136, current_train_items 233056.
I0304 19:31:39.650562 22849303695488 run.py:483] Algo bellman_ford step 7283 current loss 0.081316, current_train_items 233088.
I0304 19:31:39.683963 22849303695488 run.py:483] Algo bellman_ford step 7284 current loss 0.076867, current_train_items 233120.
I0304 19:31:39.704620 22849303695488 run.py:483] Algo bellman_ford step 7285 current loss 0.003648, current_train_items 233152.
I0304 19:31:39.722399 22849303695488 run.py:483] Algo bellman_ford step 7286 current loss 0.057949, current_train_items 233184.
I0304 19:31:39.746276 22849303695488 run.py:483] Algo bellman_ford step 7287 current loss 0.027198, current_train_items 233216.
I0304 19:31:39.778910 22849303695488 run.py:483] Algo bellman_ford step 7288 current loss 0.042383, current_train_items 233248.
I0304 19:31:39.812544 22849303695488 run.py:483] Algo bellman_ford step 7289 current loss 0.047489, current_train_items 233280.
I0304 19:31:39.833145 22849303695488 run.py:483] Algo bellman_ford step 7290 current loss 0.027932, current_train_items 233312.
I0304 19:31:39.850019 22849303695488 run.py:483] Algo bellman_ford step 7291 current loss 0.028982, current_train_items 233344.
I0304 19:31:39.873775 22849303695488 run.py:483] Algo bellman_ford step 7292 current loss 0.014568, current_train_items 233376.
I0304 19:31:39.906767 22849303695488 run.py:483] Algo bellman_ford step 7293 current loss 0.046042, current_train_items 233408.
I0304 19:31:39.939512 22849303695488 run.py:483] Algo bellman_ford step 7294 current loss 0.048410, current_train_items 233440.
I0304 19:31:39.959609 22849303695488 run.py:483] Algo bellman_ford step 7295 current loss 0.003545, current_train_items 233472.
I0304 19:31:39.976380 22849303695488 run.py:483] Algo bellman_ford step 7296 current loss 0.008969, current_train_items 233504.
I0304 19:31:40.001018 22849303695488 run.py:483] Algo bellman_ford step 7297 current loss 0.066012, current_train_items 233536.
I0304 19:31:40.033698 22849303695488 run.py:483] Algo bellman_ford step 7298 current loss 0.042273, current_train_items 233568.
I0304 19:31:40.067886 22849303695488 run.py:483] Algo bellman_ford step 7299 current loss 0.109730, current_train_items 233600.
I0304 19:31:40.088404 22849303695488 run.py:483] Algo bellman_ford step 7300 current loss 0.005050, current_train_items 233632.
I0304 19:31:40.096483 22849303695488 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0304 19:31:40.096590 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:31:40.114034 22849303695488 run.py:483] Algo bellman_ford step 7301 current loss 0.017866, current_train_items 233664.
I0304 19:31:40.138568 22849303695488 run.py:483] Algo bellman_ford step 7302 current loss 0.033300, current_train_items 233696.
I0304 19:31:40.170643 22849303695488 run.py:483] Algo bellman_ford step 7303 current loss 0.061853, current_train_items 233728.
I0304 19:31:40.205735 22849303695488 run.py:483] Algo bellman_ford step 7304 current loss 0.059777, current_train_items 233760.
I0304 19:31:40.225910 22849303695488 run.py:483] Algo bellman_ford step 7305 current loss 0.001844, current_train_items 233792.
I0304 19:31:40.241999 22849303695488 run.py:483] Algo bellman_ford step 7306 current loss 0.016671, current_train_items 233824.
I0304 19:31:40.265850 22849303695488 run.py:483] Algo bellman_ford step 7307 current loss 0.062956, current_train_items 233856.
I0304 19:31:40.296978 22849303695488 run.py:483] Algo bellman_ford step 7308 current loss 0.040006, current_train_items 233888.
I0304 19:31:40.332521 22849303695488 run.py:483] Algo bellman_ford step 7309 current loss 0.063418, current_train_items 233920.
I0304 19:31:40.352644 22849303695488 run.py:483] Algo bellman_ford step 7310 current loss 0.001714, current_train_items 233952.
I0304 19:31:40.368945 22849303695488 run.py:483] Algo bellman_ford step 7311 current loss 0.008283, current_train_items 233984.
I0304 19:31:40.394854 22849303695488 run.py:483] Algo bellman_ford step 7312 current loss 0.071930, current_train_items 234016.
I0304 19:31:40.426991 22849303695488 run.py:483] Algo bellman_ford step 7313 current loss 0.123211, current_train_items 234048.
I0304 19:31:40.461965 22849303695488 run.py:483] Algo bellman_ford step 7314 current loss 0.048024, current_train_items 234080.
I0304 19:31:40.482168 22849303695488 run.py:483] Algo bellman_ford step 7315 current loss 0.005908, current_train_items 234112.
I0304 19:31:40.498583 22849303695488 run.py:483] Algo bellman_ford step 7316 current loss 0.024091, current_train_items 234144.
I0304 19:31:40.523694 22849303695488 run.py:483] Algo bellman_ford step 7317 current loss 0.082044, current_train_items 234176.
I0304 19:31:40.555713 22849303695488 run.py:483] Algo bellman_ford step 7318 current loss 0.106884, current_train_items 234208.
I0304 19:31:40.591484 22849303695488 run.py:483] Algo bellman_ford step 7319 current loss 0.107264, current_train_items 234240.
I0304 19:31:40.611585 22849303695488 run.py:483] Algo bellman_ford step 7320 current loss 0.002221, current_train_items 234272.
I0304 19:31:40.628654 22849303695488 run.py:483] Algo bellman_ford step 7321 current loss 0.013305, current_train_items 234304.
I0304 19:31:40.654620 22849303695488 run.py:483] Algo bellman_ford step 7322 current loss 0.058282, current_train_items 234336.
I0304 19:31:40.686195 22849303695488 run.py:483] Algo bellman_ford step 7323 current loss 0.028426, current_train_items 234368.
I0304 19:31:40.721666 22849303695488 run.py:483] Algo bellman_ford step 7324 current loss 0.119755, current_train_items 234400.
I0304 19:31:40.741970 22849303695488 run.py:483] Algo bellman_ford step 7325 current loss 0.001485, current_train_items 234432.
I0304 19:31:40.759111 22849303695488 run.py:483] Algo bellman_ford step 7326 current loss 0.017773, current_train_items 234464.
I0304 19:31:40.783535 22849303695488 run.py:483] Algo bellman_ford step 7327 current loss 0.026869, current_train_items 234496.
I0304 19:31:40.814948 22849303695488 run.py:483] Algo bellman_ford step 7328 current loss 0.044599, current_train_items 234528.
I0304 19:31:40.850378 22849303695488 run.py:483] Algo bellman_ford step 7329 current loss 0.065053, current_train_items 234560.
I0304 19:31:40.870091 22849303695488 run.py:483] Algo bellman_ford step 7330 current loss 0.001326, current_train_items 234592.
I0304 19:31:40.886866 22849303695488 run.py:483] Algo bellman_ford step 7331 current loss 0.030140, current_train_items 234624.
I0304 19:31:40.912580 22849303695488 run.py:483] Algo bellman_ford step 7332 current loss 0.050334, current_train_items 234656.
I0304 19:31:40.945255 22849303695488 run.py:483] Algo bellman_ford step 7333 current loss 0.038189, current_train_items 234688.
I0304 19:31:40.978069 22849303695488 run.py:483] Algo bellman_ford step 7334 current loss 0.060988, current_train_items 234720.
I0304 19:31:40.998133 22849303695488 run.py:483] Algo bellman_ford step 7335 current loss 0.002175, current_train_items 234752.
I0304 19:31:41.014344 22849303695488 run.py:483] Algo bellman_ford step 7336 current loss 0.012621, current_train_items 234784.
I0304 19:31:41.038731 22849303695488 run.py:483] Algo bellman_ford step 7337 current loss 0.052827, current_train_items 234816.
I0304 19:31:41.071505 22849303695488 run.py:483] Algo bellman_ford step 7338 current loss 0.060727, current_train_items 234848.
I0304 19:31:41.107976 22849303695488 run.py:483] Algo bellman_ford step 7339 current loss 0.048773, current_train_items 234880.
I0304 19:31:41.127837 22849303695488 run.py:483] Algo bellman_ford step 7340 current loss 0.001924, current_train_items 234912.
I0304 19:31:41.144200 22849303695488 run.py:483] Algo bellman_ford step 7341 current loss 0.013852, current_train_items 234944.
I0304 19:31:41.170013 22849303695488 run.py:483] Algo bellman_ford step 7342 current loss 0.045877, current_train_items 234976.
I0304 19:31:41.203037 22849303695488 run.py:483] Algo bellman_ford step 7343 current loss 0.041003, current_train_items 235008.
I0304 19:31:41.236935 22849303695488 run.py:483] Algo bellman_ford step 7344 current loss 0.087868, current_train_items 235040.
I0304 19:31:41.257368 22849303695488 run.py:483] Algo bellman_ford step 7345 current loss 0.007793, current_train_items 235072.
I0304 19:31:41.273685 22849303695488 run.py:483] Algo bellman_ford step 7346 current loss 0.012112, current_train_items 235104.
I0304 19:31:41.298991 22849303695488 run.py:483] Algo bellman_ford step 7347 current loss 0.077486, current_train_items 235136.
I0304 19:31:41.331625 22849303695488 run.py:483] Algo bellman_ford step 7348 current loss 0.050280, current_train_items 235168.
I0304 19:31:41.362435 22849303695488 run.py:483] Algo bellman_ford step 7349 current loss 0.047441, current_train_items 235200.
I0304 19:31:41.382278 22849303695488 run.py:483] Algo bellman_ford step 7350 current loss 0.003423, current_train_items 235232.
I0304 19:31:41.390717 22849303695488 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0304 19:31:41.390828 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:41.408747 22849303695488 run.py:483] Algo bellman_ford step 7351 current loss 0.032189, current_train_items 235264.
I0304 19:31:41.434770 22849303695488 run.py:483] Algo bellman_ford step 7352 current loss 0.051577, current_train_items 235296.
I0304 19:31:41.468488 22849303695488 run.py:483] Algo bellman_ford step 7353 current loss 0.051250, current_train_items 235328.
I0304 19:31:41.502670 22849303695488 run.py:483] Algo bellman_ford step 7354 current loss 0.085392, current_train_items 235360.
I0304 19:31:41.523110 22849303695488 run.py:483] Algo bellman_ford step 7355 current loss 0.003198, current_train_items 235392.
I0304 19:31:41.539364 22849303695488 run.py:483] Algo bellman_ford step 7356 current loss 0.029700, current_train_items 235424.
I0304 19:31:41.565206 22849303695488 run.py:483] Algo bellman_ford step 7357 current loss 0.036271, current_train_items 235456.
I0304 19:31:41.597609 22849303695488 run.py:483] Algo bellman_ford step 7358 current loss 0.037907, current_train_items 235488.
I0304 19:31:41.632145 22849303695488 run.py:483] Algo bellman_ford step 7359 current loss 0.082603, current_train_items 235520.
I0304 19:31:41.652805 22849303695488 run.py:483] Algo bellman_ford step 7360 current loss 0.002805, current_train_items 235552.
I0304 19:31:41.669952 22849303695488 run.py:483] Algo bellman_ford step 7361 current loss 0.052113, current_train_items 235584.
I0304 19:31:41.693468 22849303695488 run.py:483] Algo bellman_ford step 7362 current loss 0.028702, current_train_items 235616.
I0304 19:31:41.727935 22849303695488 run.py:483] Algo bellman_ford step 7363 current loss 0.103403, current_train_items 235648.
I0304 19:31:41.760961 22849303695488 run.py:483] Algo bellman_ford step 7364 current loss 0.072035, current_train_items 235680.
I0304 19:31:41.781313 22849303695488 run.py:483] Algo bellman_ford step 7365 current loss 0.030495, current_train_items 235712.
I0304 19:31:41.797978 22849303695488 run.py:483] Algo bellman_ford step 7366 current loss 0.025964, current_train_items 235744.
I0304 19:31:41.823205 22849303695488 run.py:483] Algo bellman_ford step 7367 current loss 0.094999, current_train_items 235776.
I0304 19:31:41.857625 22849303695488 run.py:483] Algo bellman_ford step 7368 current loss 0.193206, current_train_items 235808.
I0304 19:31:41.890594 22849303695488 run.py:483] Algo bellman_ford step 7369 current loss 0.084176, current_train_items 235840.
I0304 19:31:41.910996 22849303695488 run.py:483] Algo bellman_ford step 7370 current loss 0.008164, current_train_items 235872.
I0304 19:31:41.927824 22849303695488 run.py:483] Algo bellman_ford step 7371 current loss 0.039417, current_train_items 235904.
I0304 19:31:41.950991 22849303695488 run.py:483] Algo bellman_ford step 7372 current loss 0.027462, current_train_items 235936.
I0304 19:31:41.983308 22849303695488 run.py:483] Algo bellman_ford step 7373 current loss 0.037974, current_train_items 235968.
I0304 19:31:42.017455 22849303695488 run.py:483] Algo bellman_ford step 7374 current loss 0.115726, current_train_items 236000.
I0304 19:31:42.038139 22849303695488 run.py:483] Algo bellman_ford step 7375 current loss 0.004125, current_train_items 236032.
I0304 19:31:42.055585 22849303695488 run.py:483] Algo bellman_ford step 7376 current loss 0.079413, current_train_items 236064.
I0304 19:31:42.080449 22849303695488 run.py:483] Algo bellman_ford step 7377 current loss 0.063532, current_train_items 236096.
I0304 19:31:42.112899 22849303695488 run.py:483] Algo bellman_ford step 7378 current loss 0.058557, current_train_items 236128.
I0304 19:31:42.145892 22849303695488 run.py:483] Algo bellman_ford step 7379 current loss 0.098969, current_train_items 236160.
I0304 19:31:42.165822 22849303695488 run.py:483] Algo bellman_ford step 7380 current loss 0.005233, current_train_items 236192.
I0304 19:31:42.182020 22849303695488 run.py:483] Algo bellman_ford step 7381 current loss 0.028343, current_train_items 236224.
I0304 19:31:42.207946 22849303695488 run.py:483] Algo bellman_ford step 7382 current loss 0.092045, current_train_items 236256.
I0304 19:31:42.239943 22849303695488 run.py:483] Algo bellman_ford step 7383 current loss 0.054903, current_train_items 236288.
I0304 19:31:42.273255 22849303695488 run.py:483] Algo bellman_ford step 7384 current loss 0.078999, current_train_items 236320.
I0304 19:31:42.293572 22849303695488 run.py:483] Algo bellman_ford step 7385 current loss 0.004500, current_train_items 236352.
I0304 19:31:42.310240 22849303695488 run.py:483] Algo bellman_ford step 7386 current loss 0.030977, current_train_items 236384.
I0304 19:31:42.335083 22849303695488 run.py:483] Algo bellman_ford step 7387 current loss 0.047354, current_train_items 236416.
I0304 19:31:42.368225 22849303695488 run.py:483] Algo bellman_ford step 7388 current loss 0.094034, current_train_items 236448.
I0304 19:31:42.401473 22849303695488 run.py:483] Algo bellman_ford step 7389 current loss 0.180021, current_train_items 236480.
I0304 19:31:42.422029 22849303695488 run.py:483] Algo bellman_ford step 7390 current loss 0.005749, current_train_items 236512.
I0304 19:31:42.439015 22849303695488 run.py:483] Algo bellman_ford step 7391 current loss 0.027528, current_train_items 236544.
I0304 19:31:42.463405 22849303695488 run.py:483] Algo bellman_ford step 7392 current loss 0.037494, current_train_items 236576.
I0304 19:31:42.495028 22849303695488 run.py:483] Algo bellman_ford step 7393 current loss 0.072943, current_train_items 236608.
I0304 19:31:42.529359 22849303695488 run.py:483] Algo bellman_ford step 7394 current loss 0.116634, current_train_items 236640.
I0304 19:31:42.549389 22849303695488 run.py:483] Algo bellman_ford step 7395 current loss 0.002931, current_train_items 236672.
I0304 19:31:42.566255 22849303695488 run.py:483] Algo bellman_ford step 7396 current loss 0.029421, current_train_items 236704.
I0304 19:31:42.591682 22849303695488 run.py:483] Algo bellman_ford step 7397 current loss 0.041431, current_train_items 236736.
I0304 19:31:42.623494 22849303695488 run.py:483] Algo bellman_ford step 7398 current loss 0.083785, current_train_items 236768.
I0304 19:31:42.656573 22849303695488 run.py:483] Algo bellman_ford step 7399 current loss 0.062472, current_train_items 236800.
I0304 19:31:42.676918 22849303695488 run.py:483] Algo bellman_ford step 7400 current loss 0.002871, current_train_items 236832.
I0304 19:31:42.685229 22849303695488 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0304 19:31:42.685340 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:31:42.702829 22849303695488 run.py:483] Algo bellman_ford step 7401 current loss 0.017459, current_train_items 236864.
I0304 19:31:42.728812 22849303695488 run.py:483] Algo bellman_ford step 7402 current loss 0.042294, current_train_items 236896.
I0304 19:31:42.762045 22849303695488 run.py:483] Algo bellman_ford step 7403 current loss 0.045305, current_train_items 236928.
I0304 19:31:42.797087 22849303695488 run.py:483] Algo bellman_ford step 7404 current loss 0.072132, current_train_items 236960.
I0304 19:31:42.817396 22849303695488 run.py:483] Algo bellman_ford step 7405 current loss 0.005822, current_train_items 236992.
I0304 19:31:42.833529 22849303695488 run.py:483] Algo bellman_ford step 7406 current loss 0.009582, current_train_items 237024.
I0304 19:31:42.858235 22849303695488 run.py:483] Algo bellman_ford step 7407 current loss 0.043079, current_train_items 237056.
I0304 19:31:42.891159 22849303695488 run.py:483] Algo bellman_ford step 7408 current loss 0.073319, current_train_items 237088.
I0304 19:31:42.923636 22849303695488 run.py:483] Algo bellman_ford step 7409 current loss 0.075280, current_train_items 237120.
I0304 19:31:42.943761 22849303695488 run.py:483] Algo bellman_ford step 7410 current loss 0.003179, current_train_items 237152.
I0304 19:31:42.960802 22849303695488 run.py:483] Algo bellman_ford step 7411 current loss 0.014551, current_train_items 237184.
I0304 19:31:42.986673 22849303695488 run.py:483] Algo bellman_ford step 7412 current loss 0.041222, current_train_items 237216.
I0304 19:31:43.019503 22849303695488 run.py:483] Algo bellman_ford step 7413 current loss 0.070660, current_train_items 237248.
I0304 19:31:43.052083 22849303695488 run.py:483] Algo bellman_ford step 7414 current loss 0.055108, current_train_items 237280.
I0304 19:31:43.072171 22849303695488 run.py:483] Algo bellman_ford step 7415 current loss 0.004019, current_train_items 237312.
I0304 19:31:43.088457 22849303695488 run.py:483] Algo bellman_ford step 7416 current loss 0.008690, current_train_items 237344.
I0304 19:31:43.113064 22849303695488 run.py:483] Algo bellman_ford step 7417 current loss 0.073503, current_train_items 237376.
I0304 19:31:43.145221 22849303695488 run.py:483] Algo bellman_ford step 7418 current loss 0.099276, current_train_items 237408.
I0304 19:31:43.178195 22849303695488 run.py:483] Algo bellman_ford step 7419 current loss 0.068261, current_train_items 237440.
I0304 19:31:43.198089 22849303695488 run.py:483] Algo bellman_ford step 7420 current loss 0.007422, current_train_items 237472.
I0304 19:31:43.214452 22849303695488 run.py:483] Algo bellman_ford step 7421 current loss 0.011812, current_train_items 237504.
I0304 19:31:43.238892 22849303695488 run.py:483] Algo bellman_ford step 7422 current loss 0.090345, current_train_items 237536.
I0304 19:31:43.270890 22849303695488 run.py:483] Algo bellman_ford step 7423 current loss 0.103328, current_train_items 237568.
I0304 19:31:43.304568 22849303695488 run.py:483] Algo bellman_ford step 7424 current loss 0.067552, current_train_items 237600.
I0304 19:31:43.324652 22849303695488 run.py:483] Algo bellman_ford step 7425 current loss 0.004100, current_train_items 237632.
I0304 19:31:43.341074 22849303695488 run.py:483] Algo bellman_ford step 7426 current loss 0.014590, current_train_items 237664.
I0304 19:31:43.366217 22849303695488 run.py:483] Algo bellman_ford step 7427 current loss 0.083196, current_train_items 237696.
I0304 19:31:43.397966 22849303695488 run.py:483] Algo bellman_ford step 7428 current loss 0.060676, current_train_items 237728.
I0304 19:31:43.432644 22849303695488 run.py:483] Algo bellman_ford step 7429 current loss 0.121872, current_train_items 237760.
I0304 19:31:43.452520 22849303695488 run.py:483] Algo bellman_ford step 7430 current loss 0.002362, current_train_items 237792.
I0304 19:31:43.468741 22849303695488 run.py:483] Algo bellman_ford step 7431 current loss 0.007562, current_train_items 237824.
I0304 19:31:43.493811 22849303695488 run.py:483] Algo bellman_ford step 7432 current loss 0.051467, current_train_items 237856.
I0304 19:31:43.526576 22849303695488 run.py:483] Algo bellman_ford step 7433 current loss 0.049882, current_train_items 237888.
I0304 19:31:43.558297 22849303695488 run.py:483] Algo bellman_ford step 7434 current loss 0.043105, current_train_items 237920.
I0304 19:31:43.578422 22849303695488 run.py:483] Algo bellman_ford step 7435 current loss 0.003108, current_train_items 237952.
I0304 19:31:43.594961 22849303695488 run.py:483] Algo bellman_ford step 7436 current loss 0.006930, current_train_items 237984.
I0304 19:31:43.620492 22849303695488 run.py:483] Algo bellman_ford step 7437 current loss 0.072526, current_train_items 238016.
I0304 19:31:43.652148 22849303695488 run.py:483] Algo bellman_ford step 7438 current loss 0.085668, current_train_items 238048.
I0304 19:31:43.687193 22849303695488 run.py:483] Algo bellman_ford step 7439 current loss 0.074377, current_train_items 238080.
I0304 19:31:43.707364 22849303695488 run.py:483] Algo bellman_ford step 7440 current loss 0.005320, current_train_items 238112.
I0304 19:31:43.724215 22849303695488 run.py:483] Algo bellman_ford step 7441 current loss 0.024525, current_train_items 238144.
I0304 19:31:43.748310 22849303695488 run.py:483] Algo bellman_ford step 7442 current loss 0.028683, current_train_items 238176.
I0304 19:31:43.781811 22849303695488 run.py:483] Algo bellman_ford step 7443 current loss 0.074761, current_train_items 238208.
I0304 19:31:43.816447 22849303695488 run.py:483] Algo bellman_ford step 7444 current loss 0.081752, current_train_items 238240.
I0304 19:31:43.836342 22849303695488 run.py:483] Algo bellman_ford step 7445 current loss 0.005433, current_train_items 238272.
I0304 19:31:43.852878 22849303695488 run.py:483] Algo bellman_ford step 7446 current loss 0.018868, current_train_items 238304.
I0304 19:31:43.877270 22849303695488 run.py:483] Algo bellman_ford step 7447 current loss 0.028324, current_train_items 238336.
I0304 19:31:43.909139 22849303695488 run.py:483] Algo bellman_ford step 7448 current loss 0.071567, current_train_items 238368.
I0304 19:31:43.944815 22849303695488 run.py:483] Algo bellman_ford step 7449 current loss 0.084475, current_train_items 238400.
I0304 19:31:43.964932 22849303695488 run.py:483] Algo bellman_ford step 7450 current loss 0.004061, current_train_items 238432.
I0304 19:31:43.973029 22849303695488 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0304 19:31:43.973137 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:43.990384 22849303695488 run.py:483] Algo bellman_ford step 7451 current loss 0.019705, current_train_items 238464.
I0304 19:31:44.016729 22849303695488 run.py:483] Algo bellman_ford step 7452 current loss 0.044178, current_train_items 238496.
I0304 19:31:44.049255 22849303695488 run.py:483] Algo bellman_ford step 7453 current loss 0.043534, current_train_items 238528.
I0304 19:31:44.083041 22849303695488 run.py:483] Algo bellman_ford step 7454 current loss 0.061774, current_train_items 238560.
I0304 19:31:44.103542 22849303695488 run.py:483] Algo bellman_ford step 7455 current loss 0.018482, current_train_items 238592.
I0304 19:31:44.120372 22849303695488 run.py:483] Algo bellman_ford step 7456 current loss 0.020865, current_train_items 238624.
I0304 19:31:44.144900 22849303695488 run.py:483] Algo bellman_ford step 7457 current loss 0.034565, current_train_items 238656.
I0304 19:31:44.177963 22849303695488 run.py:483] Algo bellman_ford step 7458 current loss 0.089194, current_train_items 238688.
I0304 19:31:44.211704 22849303695488 run.py:483] Algo bellman_ford step 7459 current loss 0.058160, current_train_items 238720.
I0304 19:31:44.232284 22849303695488 run.py:483] Algo bellman_ford step 7460 current loss 0.003623, current_train_items 238752.
I0304 19:31:44.249500 22849303695488 run.py:483] Algo bellman_ford step 7461 current loss 0.016435, current_train_items 238784.
I0304 19:31:44.273673 22849303695488 run.py:483] Algo bellman_ford step 7462 current loss 0.043692, current_train_items 238816.
I0304 19:31:44.305670 22849303695488 run.py:483] Algo bellman_ford step 7463 current loss 0.045784, current_train_items 238848.
I0304 19:31:44.339988 22849303695488 run.py:483] Algo bellman_ford step 7464 current loss 0.085237, current_train_items 238880.
I0304 19:31:44.360414 22849303695488 run.py:483] Algo bellman_ford step 7465 current loss 0.002550, current_train_items 238912.
I0304 19:31:44.376729 22849303695488 run.py:483] Algo bellman_ford step 7466 current loss 0.009234, current_train_items 238944.
I0304 19:31:44.401192 22849303695488 run.py:483] Algo bellman_ford step 7467 current loss 0.044585, current_train_items 238976.
I0304 19:31:44.434623 22849303695488 run.py:483] Algo bellman_ford step 7468 current loss 0.073038, current_train_items 239008.
I0304 19:31:44.467147 22849303695488 run.py:483] Algo bellman_ford step 7469 current loss 0.046393, current_train_items 239040.
I0304 19:31:44.487905 22849303695488 run.py:483] Algo bellman_ford step 7470 current loss 0.005847, current_train_items 239072.
I0304 19:31:44.504591 22849303695488 run.py:483] Algo bellman_ford step 7471 current loss 0.013991, current_train_items 239104.
I0304 19:31:44.528371 22849303695488 run.py:483] Algo bellman_ford step 7472 current loss 0.097928, current_train_items 239136.
I0304 19:31:44.561022 22849303695488 run.py:483] Algo bellman_ford step 7473 current loss 0.147098, current_train_items 239168.
I0304 19:31:44.596635 22849303695488 run.py:483] Algo bellman_ford step 7474 current loss 0.068974, current_train_items 239200.
I0304 19:31:44.617157 22849303695488 run.py:483] Algo bellman_ford step 7475 current loss 0.002664, current_train_items 239232.
I0304 19:31:44.634144 22849303695488 run.py:483] Algo bellman_ford step 7476 current loss 0.007124, current_train_items 239264.
I0304 19:31:44.657966 22849303695488 run.py:483] Algo bellman_ford step 7477 current loss 0.051204, current_train_items 239296.
I0304 19:31:44.690642 22849303695488 run.py:483] Algo bellman_ford step 7478 current loss 0.031465, current_train_items 239328.
I0304 19:31:44.727887 22849303695488 run.py:483] Algo bellman_ford step 7479 current loss 0.089457, current_train_items 239360.
I0304 19:31:44.748013 22849303695488 run.py:483] Algo bellman_ford step 7480 current loss 0.003307, current_train_items 239392.
I0304 19:31:44.764821 22849303695488 run.py:483] Algo bellman_ford step 7481 current loss 0.008974, current_train_items 239424.
I0304 19:31:44.790354 22849303695488 run.py:483] Algo bellman_ford step 7482 current loss 0.057531, current_train_items 239456.
I0304 19:31:44.824087 22849303695488 run.py:483] Algo bellman_ford step 7483 current loss 0.054262, current_train_items 239488.
I0304 19:31:44.859069 22849303695488 run.py:483] Algo bellman_ford step 7484 current loss 0.102318, current_train_items 239520.
I0304 19:31:44.879697 22849303695488 run.py:483] Algo bellman_ford step 7485 current loss 0.003324, current_train_items 239552.
I0304 19:31:44.896334 22849303695488 run.py:483] Algo bellman_ford step 7486 current loss 0.014363, current_train_items 239584.
I0304 19:31:44.919803 22849303695488 run.py:483] Algo bellman_ford step 7487 current loss 0.037960, current_train_items 239616.
I0304 19:31:44.952774 22849303695488 run.py:483] Algo bellman_ford step 7488 current loss 0.070450, current_train_items 239648.
I0304 19:31:44.988933 22849303695488 run.py:483] Algo bellman_ford step 7489 current loss 0.053670, current_train_items 239680.
I0304 19:31:45.009415 22849303695488 run.py:483] Algo bellman_ford step 7490 current loss 0.001252, current_train_items 239712.
I0304 19:31:45.025782 22849303695488 run.py:483] Algo bellman_ford step 7491 current loss 0.045441, current_train_items 239744.
I0304 19:31:45.049480 22849303695488 run.py:483] Algo bellman_ford step 7492 current loss 0.012454, current_train_items 239776.
I0304 19:31:45.081584 22849303695488 run.py:483] Algo bellman_ford step 7493 current loss 0.028913, current_train_items 239808.
I0304 19:31:45.115422 22849303695488 run.py:483] Algo bellman_ford step 7494 current loss 0.086643, current_train_items 239840.
I0304 19:31:45.135694 22849303695488 run.py:483] Algo bellman_ford step 7495 current loss 0.002772, current_train_items 239872.
I0304 19:31:45.152496 22849303695488 run.py:483] Algo bellman_ford step 7496 current loss 0.019198, current_train_items 239904.
I0304 19:31:45.176903 22849303695488 run.py:483] Algo bellman_ford step 7497 current loss 0.060096, current_train_items 239936.
I0304 19:31:45.208634 22849303695488 run.py:483] Algo bellman_ford step 7498 current loss 0.047082, current_train_items 239968.
I0304 19:31:45.242196 22849303695488 run.py:483] Algo bellman_ford step 7499 current loss 0.047624, current_train_items 240000.
I0304 19:31:45.262417 22849303695488 run.py:483] Algo bellman_ford step 7500 current loss 0.006848, current_train_items 240032.
I0304 19:31:45.270641 22849303695488 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0304 19:31:45.270749 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:45.287775 22849303695488 run.py:483] Algo bellman_ford step 7501 current loss 0.024959, current_train_items 240064.
I0304 19:31:45.312622 22849303695488 run.py:483] Algo bellman_ford step 7502 current loss 0.069906, current_train_items 240096.
I0304 19:31:45.345174 22849303695488 run.py:483] Algo bellman_ford step 7503 current loss 0.034673, current_train_items 240128.
I0304 19:31:45.379327 22849303695488 run.py:483] Algo bellman_ford step 7504 current loss 0.048784, current_train_items 240160.
I0304 19:31:45.399680 22849303695488 run.py:483] Algo bellman_ford step 7505 current loss 0.009994, current_train_items 240192.
I0304 19:31:45.416036 22849303695488 run.py:483] Algo bellman_ford step 7506 current loss 0.007322, current_train_items 240224.
I0304 19:31:45.440016 22849303695488 run.py:483] Algo bellman_ford step 7507 current loss 0.027381, current_train_items 240256.
I0304 19:31:45.471935 22849303695488 run.py:483] Algo bellman_ford step 7508 current loss 0.023401, current_train_items 240288.
I0304 19:31:45.505068 22849303695488 run.py:483] Algo bellman_ford step 7509 current loss 0.036579, current_train_items 240320.
I0304 19:31:45.525147 22849303695488 run.py:483] Algo bellman_ford step 7510 current loss 0.009962, current_train_items 240352.
I0304 19:31:45.542025 22849303695488 run.py:483] Algo bellman_ford step 7511 current loss 0.024868, current_train_items 240384.
I0304 19:31:45.565730 22849303695488 run.py:483] Algo bellman_ford step 7512 current loss 0.033289, current_train_items 240416.
I0304 19:31:45.597592 22849303695488 run.py:483] Algo bellman_ford step 7513 current loss 0.076268, current_train_items 240448.
I0304 19:31:45.630189 22849303695488 run.py:483] Algo bellman_ford step 7514 current loss 0.078568, current_train_items 240480.
I0304 19:31:45.650145 22849303695488 run.py:483] Algo bellman_ford step 7515 current loss 0.006990, current_train_items 240512.
I0304 19:31:45.666021 22849303695488 run.py:483] Algo bellman_ford step 7516 current loss 0.023293, current_train_items 240544.
I0304 19:31:45.691126 22849303695488 run.py:483] Algo bellman_ford step 7517 current loss 0.052760, current_train_items 240576.
I0304 19:31:45.723546 22849303695488 run.py:483] Algo bellman_ford step 7518 current loss 0.077644, current_train_items 240608.
I0304 19:31:45.758077 22849303695488 run.py:483] Algo bellman_ford step 7519 current loss 0.078342, current_train_items 240640.
I0304 19:31:45.777886 22849303695488 run.py:483] Algo bellman_ford step 7520 current loss 0.004695, current_train_items 240672.
I0304 19:31:45.794690 22849303695488 run.py:483] Algo bellman_ford step 7521 current loss 0.030729, current_train_items 240704.
I0304 19:31:45.818881 22849303695488 run.py:483] Algo bellman_ford step 7522 current loss 0.048129, current_train_items 240736.
I0304 19:31:45.851132 22849303695488 run.py:483] Algo bellman_ford step 7523 current loss 0.057216, current_train_items 240768.
I0304 19:31:45.882711 22849303695488 run.py:483] Algo bellman_ford step 7524 current loss 0.028298, current_train_items 240800.
I0304 19:31:45.902577 22849303695488 run.py:483] Algo bellman_ford step 7525 current loss 0.031162, current_train_items 240832.
I0304 19:31:45.918989 22849303695488 run.py:483] Algo bellman_ford step 7526 current loss 0.025651, current_train_items 240864.
I0304 19:31:45.944265 22849303695488 run.py:483] Algo bellman_ford step 7527 current loss 0.035056, current_train_items 240896.
I0304 19:31:45.975407 22849303695488 run.py:483] Algo bellman_ford step 7528 current loss 0.049529, current_train_items 240928.
I0304 19:31:46.009310 22849303695488 run.py:483] Algo bellman_ford step 7529 current loss 0.076962, current_train_items 240960.
I0304 19:31:46.029296 22849303695488 run.py:483] Algo bellman_ford step 7530 current loss 0.006586, current_train_items 240992.
I0304 19:31:46.045774 22849303695488 run.py:483] Algo bellman_ford step 7531 current loss 0.006689, current_train_items 241024.
I0304 19:31:46.069604 22849303695488 run.py:483] Algo bellman_ford step 7532 current loss 0.056064, current_train_items 241056.
I0304 19:31:46.102753 22849303695488 run.py:483] Algo bellman_ford step 7533 current loss 0.088719, current_train_items 241088.
I0304 19:31:46.137462 22849303695488 run.py:483] Algo bellman_ford step 7534 current loss 0.075791, current_train_items 241120.
I0304 19:31:46.157371 22849303695488 run.py:483] Algo bellman_ford step 7535 current loss 0.003281, current_train_items 241152.
I0304 19:31:46.174182 22849303695488 run.py:483] Algo bellman_ford step 7536 current loss 0.018504, current_train_items 241184.
I0304 19:31:46.198381 22849303695488 run.py:483] Algo bellman_ford step 7537 current loss 0.065139, current_train_items 241216.
I0304 19:31:46.231109 22849303695488 run.py:483] Algo bellman_ford step 7538 current loss 0.046974, current_train_items 241248.
I0304 19:31:46.264605 22849303695488 run.py:483] Algo bellman_ford step 7539 current loss 0.029931, current_train_items 241280.
I0304 19:31:46.284791 22849303695488 run.py:483] Algo bellman_ford step 7540 current loss 0.005182, current_train_items 241312.
I0304 19:31:46.301736 22849303695488 run.py:483] Algo bellman_ford step 7541 current loss 0.016769, current_train_items 241344.
I0304 19:31:46.327196 22849303695488 run.py:483] Algo bellman_ford step 7542 current loss 0.154411, current_train_items 241376.
I0304 19:31:46.360574 22849303695488 run.py:483] Algo bellman_ford step 7543 current loss 0.126507, current_train_items 241408.
I0304 19:31:46.394592 22849303695488 run.py:483] Algo bellman_ford step 7544 current loss 0.077377, current_train_items 241440.
I0304 19:31:46.414182 22849303695488 run.py:483] Algo bellman_ford step 7545 current loss 0.002257, current_train_items 241472.
I0304 19:31:46.430848 22849303695488 run.py:483] Algo bellman_ford step 7546 current loss 0.027435, current_train_items 241504.
I0304 19:31:46.455932 22849303695488 run.py:483] Algo bellman_ford step 7547 current loss 0.071928, current_train_items 241536.
I0304 19:31:46.489001 22849303695488 run.py:483] Algo bellman_ford step 7548 current loss 0.080188, current_train_items 241568.
I0304 19:31:46.523561 22849303695488 run.py:483] Algo bellman_ford step 7549 current loss 0.103733, current_train_items 241600.
I0304 19:31:46.543584 22849303695488 run.py:483] Algo bellman_ford step 7550 current loss 0.003654, current_train_items 241632.
I0304 19:31:46.551859 22849303695488 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0304 19:31:46.551967 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:46.569637 22849303695488 run.py:483] Algo bellman_ford step 7551 current loss 0.055185, current_train_items 241664.
I0304 19:31:46.594530 22849303695488 run.py:483] Algo bellman_ford step 7552 current loss 0.013636, current_train_items 241696.
I0304 19:31:46.627567 22849303695488 run.py:483] Algo bellman_ford step 7553 current loss 0.074707, current_train_items 241728.
I0304 19:31:46.661974 22849303695488 run.py:483] Algo bellman_ford step 7554 current loss 0.080952, current_train_items 241760.
I0304 19:31:46.682449 22849303695488 run.py:483] Algo bellman_ford step 7555 current loss 0.002587, current_train_items 241792.
I0304 19:31:46.698884 22849303695488 run.py:483] Algo bellman_ford step 7556 current loss 0.028109, current_train_items 241824.
I0304 19:31:46.723607 22849303695488 run.py:483] Algo bellman_ford step 7557 current loss 0.037431, current_train_items 241856.
I0304 19:31:46.757251 22849303695488 run.py:483] Algo bellman_ford step 7558 current loss 0.051292, current_train_items 241888.
I0304 19:31:46.790112 22849303695488 run.py:483] Algo bellman_ford step 7559 current loss 0.058441, current_train_items 241920.
I0304 19:31:46.810266 22849303695488 run.py:483] Algo bellman_ford step 7560 current loss 0.001921, current_train_items 241952.
I0304 19:31:46.827907 22849303695488 run.py:483] Algo bellman_ford step 7561 current loss 0.069237, current_train_items 241984.
I0304 19:31:46.851565 22849303695488 run.py:483] Algo bellman_ford step 7562 current loss 0.051721, current_train_items 242016.
I0304 19:31:46.884039 22849303695488 run.py:483] Algo bellman_ford step 7563 current loss 0.058007, current_train_items 242048.
I0304 19:31:46.918125 22849303695488 run.py:483] Algo bellman_ford step 7564 current loss 0.060136, current_train_items 242080.
I0304 19:31:46.937940 22849303695488 run.py:483] Algo bellman_ford step 7565 current loss 0.006686, current_train_items 242112.
I0304 19:31:46.954486 22849303695488 run.py:483] Algo bellman_ford step 7566 current loss 0.021493, current_train_items 242144.
I0304 19:31:46.979026 22849303695488 run.py:483] Algo bellman_ford step 7567 current loss 0.060608, current_train_items 242176.
I0304 19:31:47.010849 22849303695488 run.py:483] Algo bellman_ford step 7568 current loss 0.123468, current_train_items 242208.
I0304 19:31:47.044082 22849303695488 run.py:483] Algo bellman_ford step 7569 current loss 0.152078, current_train_items 242240.
I0304 19:31:47.064511 22849303695488 run.py:483] Algo bellman_ford step 7570 current loss 0.004197, current_train_items 242272.
I0304 19:31:47.081350 22849303695488 run.py:483] Algo bellman_ford step 7571 current loss 0.022466, current_train_items 242304.
I0304 19:31:47.105054 22849303695488 run.py:483] Algo bellman_ford step 7572 current loss 0.019755, current_train_items 242336.
I0304 19:31:47.136255 22849303695488 run.py:483] Algo bellman_ford step 7573 current loss 0.044411, current_train_items 242368.
I0304 19:31:47.167929 22849303695488 run.py:483] Algo bellman_ford step 7574 current loss 0.032627, current_train_items 242400.
I0304 19:31:47.188179 22849303695488 run.py:483] Algo bellman_ford step 7575 current loss 0.002568, current_train_items 242432.
I0304 19:31:47.205366 22849303695488 run.py:483] Algo bellman_ford step 7576 current loss 0.016588, current_train_items 242464.
I0304 19:31:47.228845 22849303695488 run.py:483] Algo bellman_ford step 7577 current loss 0.055196, current_train_items 242496.
I0304 19:31:47.261246 22849303695488 run.py:483] Algo bellman_ford step 7578 current loss 0.059725, current_train_items 242528.
I0304 19:31:47.296774 22849303695488 run.py:483] Algo bellman_ford step 7579 current loss 0.086246, current_train_items 242560.
I0304 19:31:47.316685 22849303695488 run.py:483] Algo bellman_ford step 7580 current loss 0.003094, current_train_items 242592.
I0304 19:31:47.333414 22849303695488 run.py:483] Algo bellman_ford step 7581 current loss 0.076139, current_train_items 242624.
I0304 19:31:47.358057 22849303695488 run.py:483] Algo bellman_ford step 7582 current loss 0.081081, current_train_items 242656.
I0304 19:31:47.390060 22849303695488 run.py:483] Algo bellman_ford step 7583 current loss 0.115029, current_train_items 242688.
I0304 19:31:47.423153 22849303695488 run.py:483] Algo bellman_ford step 7584 current loss 0.068552, current_train_items 242720.
I0304 19:31:47.443411 22849303695488 run.py:483] Algo bellman_ford step 7585 current loss 0.002779, current_train_items 242752.
I0304 19:31:47.459776 22849303695488 run.py:483] Algo bellman_ford step 7586 current loss 0.031546, current_train_items 242784.
I0304 19:31:47.483827 22849303695488 run.py:483] Algo bellman_ford step 7587 current loss 0.076198, current_train_items 242816.
I0304 19:31:47.514158 22849303695488 run.py:483] Algo bellman_ford step 7588 current loss 0.091390, current_train_items 242848.
I0304 19:31:47.548769 22849303695488 run.py:483] Algo bellman_ford step 7589 current loss 0.176099, current_train_items 242880.
I0304 19:31:47.569000 22849303695488 run.py:483] Algo bellman_ford step 7590 current loss 0.018054, current_train_items 242912.
I0304 19:31:47.585414 22849303695488 run.py:483] Algo bellman_ford step 7591 current loss 0.019180, current_train_items 242944.
I0304 19:31:47.610481 22849303695488 run.py:483] Algo bellman_ford step 7592 current loss 0.054664, current_train_items 242976.
I0304 19:31:47.641218 22849303695488 run.py:483] Algo bellman_ford step 7593 current loss 0.026358, current_train_items 243008.
I0304 19:31:47.675785 22849303695488 run.py:483] Algo bellman_ford step 7594 current loss 0.087638, current_train_items 243040.
I0304 19:31:47.695581 22849303695488 run.py:483] Algo bellman_ford step 7595 current loss 0.011489, current_train_items 243072.
I0304 19:31:47.712169 22849303695488 run.py:483] Algo bellman_ford step 7596 current loss 0.020890, current_train_items 243104.
I0304 19:31:47.736813 22849303695488 run.py:483] Algo bellman_ford step 7597 current loss 0.041717, current_train_items 243136.
I0304 19:31:47.769590 22849303695488 run.py:483] Algo bellman_ford step 7598 current loss 0.063613, current_train_items 243168.
I0304 19:31:47.800654 22849303695488 run.py:483] Algo bellman_ford step 7599 current loss 0.064074, current_train_items 243200.
I0304 19:31:47.821161 22849303695488 run.py:483] Algo bellman_ford step 7600 current loss 0.006248, current_train_items 243232.
I0304 19:31:47.829460 22849303695488 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0304 19:31:47.829568 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:31:47.846729 22849303695488 run.py:483] Algo bellman_ford step 7601 current loss 0.028822, current_train_items 243264.
I0304 19:31:47.873150 22849303695488 run.py:483] Algo bellman_ford step 7602 current loss 0.162949, current_train_items 243296.
I0304 19:31:47.905285 22849303695488 run.py:483] Algo bellman_ford step 7603 current loss 0.051448, current_train_items 243328.
I0304 19:31:47.940624 22849303695488 run.py:483] Algo bellman_ford step 7604 current loss 0.111710, current_train_items 243360.
I0304 19:31:47.960816 22849303695488 run.py:483] Algo bellman_ford step 7605 current loss 0.054863, current_train_items 243392.
I0304 19:31:47.976517 22849303695488 run.py:483] Algo bellman_ford step 7606 current loss 0.009383, current_train_items 243424.
I0304 19:31:48.000645 22849303695488 run.py:483] Algo bellman_ford step 7607 current loss 0.031804, current_train_items 243456.
I0304 19:31:48.033029 22849303695488 run.py:483] Algo bellman_ford step 7608 current loss 0.074995, current_train_items 243488.
I0304 19:31:48.067999 22849303695488 run.py:483] Algo bellman_ford step 7609 current loss 0.070538, current_train_items 243520.
I0304 19:31:48.087856 22849303695488 run.py:483] Algo bellman_ford step 7610 current loss 0.002983, current_train_items 243552.
I0304 19:31:48.104845 22849303695488 run.py:483] Algo bellman_ford step 7611 current loss 0.009873, current_train_items 243584.
I0304 19:31:48.129319 22849303695488 run.py:483] Algo bellman_ford step 7612 current loss 0.026266, current_train_items 243616.
I0304 19:31:48.159452 22849303695488 run.py:483] Algo bellman_ford step 7613 current loss 0.061000, current_train_items 243648.
I0304 19:31:48.192637 22849303695488 run.py:483] Algo bellman_ford step 7614 current loss 0.123090, current_train_items 243680.
I0304 19:31:48.212424 22849303695488 run.py:483] Algo bellman_ford step 7615 current loss 0.002280, current_train_items 243712.
I0304 19:31:48.229123 22849303695488 run.py:483] Algo bellman_ford step 7616 current loss 0.012945, current_train_items 243744.
I0304 19:31:48.253432 22849303695488 run.py:483] Algo bellman_ford step 7617 current loss 0.036040, current_train_items 243776.
I0304 19:31:48.285124 22849303695488 run.py:483] Algo bellman_ford step 7618 current loss 0.058797, current_train_items 243808.
I0304 19:31:48.319940 22849303695488 run.py:483] Algo bellman_ford step 7619 current loss 0.129729, current_train_items 243840.
I0304 19:31:48.339807 22849303695488 run.py:483] Algo bellman_ford step 7620 current loss 0.005528, current_train_items 243872.
I0304 19:31:48.356543 22849303695488 run.py:483] Algo bellman_ford step 7621 current loss 0.015973, current_train_items 243904.
I0304 19:31:48.381331 22849303695488 run.py:483] Algo bellman_ford step 7622 current loss 0.031763, current_train_items 243936.
I0304 19:31:48.414832 22849303695488 run.py:483] Algo bellman_ford step 7623 current loss 0.082126, current_train_items 243968.
I0304 19:31:48.448248 22849303695488 run.py:483] Algo bellman_ford step 7624 current loss 0.075640, current_train_items 244000.
I0304 19:31:48.468407 22849303695488 run.py:483] Algo bellman_ford step 7625 current loss 0.004463, current_train_items 244032.
I0304 19:31:48.484297 22849303695488 run.py:483] Algo bellman_ford step 7626 current loss 0.017005, current_train_items 244064.
I0304 19:31:48.509518 22849303695488 run.py:483] Algo bellman_ford step 7627 current loss 0.067351, current_train_items 244096.
I0304 19:31:48.542726 22849303695488 run.py:483] Algo bellman_ford step 7628 current loss 0.050962, current_train_items 244128.
I0304 19:31:48.575963 22849303695488 run.py:483] Algo bellman_ford step 7629 current loss 0.048094, current_train_items 244160.
I0304 19:31:48.595888 22849303695488 run.py:483] Algo bellman_ford step 7630 current loss 0.019151, current_train_items 244192.
I0304 19:31:48.612658 22849303695488 run.py:483] Algo bellman_ford step 7631 current loss 0.032028, current_train_items 244224.
I0304 19:31:48.636976 22849303695488 run.py:483] Algo bellman_ford step 7632 current loss 0.045173, current_train_items 244256.
I0304 19:31:48.669884 22849303695488 run.py:483] Algo bellman_ford step 7633 current loss 0.025028, current_train_items 244288.
I0304 19:31:48.704279 22849303695488 run.py:483] Algo bellman_ford step 7634 current loss 0.088163, current_train_items 244320.
I0304 19:31:48.724306 22849303695488 run.py:483] Algo bellman_ford step 7635 current loss 0.008807, current_train_items 244352.
I0304 19:31:48.740701 22849303695488 run.py:483] Algo bellman_ford step 7636 current loss 0.019148, current_train_items 244384.
I0304 19:31:48.765164 22849303695488 run.py:483] Algo bellman_ford step 7637 current loss 0.044977, current_train_items 244416.
I0304 19:31:48.796368 22849303695488 run.py:483] Algo bellman_ford step 7638 current loss 0.083370, current_train_items 244448.
I0304 19:31:48.832809 22849303695488 run.py:483] Algo bellman_ford step 7639 current loss 0.069550, current_train_items 244480.
I0304 19:31:48.852705 22849303695488 run.py:483] Algo bellman_ford step 7640 current loss 0.002546, current_train_items 244512.
I0304 19:31:48.869087 22849303695488 run.py:483] Algo bellman_ford step 7641 current loss 0.021451, current_train_items 244544.
I0304 19:31:48.892563 22849303695488 run.py:483] Algo bellman_ford step 7642 current loss 0.048432, current_train_items 244576.
I0304 19:31:48.927108 22849303695488 run.py:483] Algo bellman_ford step 7643 current loss 0.077640, current_train_items 244608.
I0304 19:31:48.962990 22849303695488 run.py:483] Algo bellman_ford step 7644 current loss 0.063247, current_train_items 244640.
I0304 19:31:48.982784 22849303695488 run.py:483] Algo bellman_ford step 7645 current loss 0.009071, current_train_items 244672.
I0304 19:31:48.999313 22849303695488 run.py:483] Algo bellman_ford step 7646 current loss 0.017055, current_train_items 244704.
I0304 19:31:49.024987 22849303695488 run.py:483] Algo bellman_ford step 7647 current loss 0.057048, current_train_items 244736.
I0304 19:31:49.058637 22849303695488 run.py:483] Algo bellman_ford step 7648 current loss 0.072763, current_train_items 244768.
I0304 19:31:49.094864 22849303695488 run.py:483] Algo bellman_ford step 7649 current loss 0.091525, current_train_items 244800.
I0304 19:31:49.114887 22849303695488 run.py:483] Algo bellman_ford step 7650 current loss 0.018758, current_train_items 244832.
I0304 19:31:49.123401 22849303695488 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0304 19:31:49.123508 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:31:49.140245 22849303695488 run.py:483] Algo bellman_ford step 7651 current loss 0.024486, current_train_items 244864.
I0304 19:31:49.166127 22849303695488 run.py:483] Algo bellman_ford step 7652 current loss 0.089029, current_train_items 244896.
I0304 19:31:49.198433 22849303695488 run.py:483] Algo bellman_ford step 7653 current loss 0.040094, current_train_items 244928.
I0304 19:31:49.232759 22849303695488 run.py:483] Algo bellman_ford step 7654 current loss 0.077659, current_train_items 244960.
I0304 19:31:49.253401 22849303695488 run.py:483] Algo bellman_ford step 7655 current loss 0.018232, current_train_items 244992.
I0304 19:31:49.269655 22849303695488 run.py:483] Algo bellman_ford step 7656 current loss 0.015506, current_train_items 245024.
I0304 19:31:49.294941 22849303695488 run.py:483] Algo bellman_ford step 7657 current loss 0.040981, current_train_items 245056.
I0304 19:31:49.328276 22849303695488 run.py:483] Algo bellman_ford step 7658 current loss 0.058597, current_train_items 245088.
I0304 19:31:49.363843 22849303695488 run.py:483] Algo bellman_ford step 7659 current loss 0.107728, current_train_items 245120.
I0304 19:31:49.384330 22849303695488 run.py:483] Algo bellman_ford step 7660 current loss 0.005867, current_train_items 245152.
I0304 19:31:49.400984 22849303695488 run.py:483] Algo bellman_ford step 7661 current loss 0.006972, current_train_items 245184.
I0304 19:31:49.425163 22849303695488 run.py:483] Algo bellman_ford step 7662 current loss 0.043400, current_train_items 245216.
I0304 19:31:49.457146 22849303695488 run.py:483] Algo bellman_ford step 7663 current loss 0.040855, current_train_items 245248.
I0304 19:31:49.491384 22849303695488 run.py:483] Algo bellman_ford step 7664 current loss 0.065442, current_train_items 245280.
I0304 19:31:49.511667 22849303695488 run.py:483] Algo bellman_ford step 7665 current loss 0.019902, current_train_items 245312.
I0304 19:31:49.528299 22849303695488 run.py:483] Algo bellman_ford step 7666 current loss 0.036203, current_train_items 245344.
I0304 19:31:49.552634 22849303695488 run.py:483] Algo bellman_ford step 7667 current loss 0.047091, current_train_items 245376.
I0304 19:31:49.587340 22849303695488 run.py:483] Algo bellman_ford step 7668 current loss 0.090709, current_train_items 245408.
I0304 19:31:49.622260 22849303695488 run.py:483] Algo bellman_ford step 7669 current loss 0.080692, current_train_items 245440.
I0304 19:31:49.642553 22849303695488 run.py:483] Algo bellman_ford step 7670 current loss 0.014709, current_train_items 245472.
I0304 19:31:49.659717 22849303695488 run.py:483] Algo bellman_ford step 7671 current loss 0.020543, current_train_items 245504.
I0304 19:31:49.684386 22849303695488 run.py:483] Algo bellman_ford step 7672 current loss 0.041869, current_train_items 245536.
I0304 19:31:49.717099 22849303695488 run.py:483] Algo bellman_ford step 7673 current loss 0.052710, current_train_items 245568.
I0304 19:31:49.752047 22849303695488 run.py:483] Algo bellman_ford step 7674 current loss 0.098699, current_train_items 245600.
I0304 19:31:49.772546 22849303695488 run.py:483] Algo bellman_ford step 7675 current loss 0.008175, current_train_items 245632.
I0304 19:31:49.790077 22849303695488 run.py:483] Algo bellman_ford step 7676 current loss 0.022692, current_train_items 245664.
I0304 19:31:49.815342 22849303695488 run.py:483] Algo bellman_ford step 7677 current loss 0.041195, current_train_items 245696.
I0304 19:31:49.847463 22849303695488 run.py:483] Algo bellman_ford step 7678 current loss 0.055203, current_train_items 245728.
I0304 19:31:49.882893 22849303695488 run.py:483] Algo bellman_ford step 7679 current loss 0.057016, current_train_items 245760.
I0304 19:31:49.903460 22849303695488 run.py:483] Algo bellman_ford step 7680 current loss 0.003263, current_train_items 245792.
I0304 19:31:49.920247 22849303695488 run.py:483] Algo bellman_ford step 7681 current loss 0.024916, current_train_items 245824.
I0304 19:31:49.944531 22849303695488 run.py:483] Algo bellman_ford step 7682 current loss 0.032935, current_train_items 245856.
I0304 19:31:49.977276 22849303695488 run.py:483] Algo bellman_ford step 7683 current loss 0.048611, current_train_items 245888.
I0304 19:31:50.012615 22849303695488 run.py:483] Algo bellman_ford step 7684 current loss 0.078786, current_train_items 245920.
I0304 19:31:50.033377 22849303695488 run.py:483] Algo bellman_ford step 7685 current loss 0.024448, current_train_items 245952.
I0304 19:31:50.050217 22849303695488 run.py:483] Algo bellman_ford step 7686 current loss 0.019646, current_train_items 245984.
I0304 19:31:50.073611 22849303695488 run.py:483] Algo bellman_ford step 7687 current loss 0.049860, current_train_items 246016.
I0304 19:31:50.107029 22849303695488 run.py:483] Algo bellman_ford step 7688 current loss 0.051839, current_train_items 246048.
I0304 19:31:50.141816 22849303695488 run.py:483] Algo bellman_ford step 7689 current loss 0.124382, current_train_items 246080.
I0304 19:31:50.162251 22849303695488 run.py:483] Algo bellman_ford step 7690 current loss 0.004979, current_train_items 246112.
I0304 19:31:50.179018 22849303695488 run.py:483] Algo bellman_ford step 7691 current loss 0.014534, current_train_items 246144.
I0304 19:31:50.203043 22849303695488 run.py:483] Algo bellman_ford step 7692 current loss 0.060475, current_train_items 246176.
I0304 19:31:50.237335 22849303695488 run.py:483] Algo bellman_ford step 7693 current loss 0.059125, current_train_items 246208.
I0304 19:31:50.271138 22849303695488 run.py:483] Algo bellman_ford step 7694 current loss 0.062478, current_train_items 246240.
I0304 19:31:50.291302 22849303695488 run.py:483] Algo bellman_ford step 7695 current loss 0.004978, current_train_items 246272.
I0304 19:31:50.308097 22849303695488 run.py:483] Algo bellman_ford step 7696 current loss 0.026609, current_train_items 246304.
I0304 19:31:50.333373 22849303695488 run.py:483] Algo bellman_ford step 7697 current loss 0.071923, current_train_items 246336.
I0304 19:31:50.366683 22849303695488 run.py:483] Algo bellman_ford step 7698 current loss 0.080648, current_train_items 246368.
I0304 19:31:50.402852 22849303695488 run.py:483] Algo bellman_ford step 7699 current loss 0.074338, current_train_items 246400.
I0304 19:31:50.423646 22849303695488 run.py:483] Algo bellman_ford step 7700 current loss 0.014833, current_train_items 246432.
I0304 19:31:50.431831 22849303695488 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0304 19:31:50.431941 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:50.449449 22849303695488 run.py:483] Algo bellman_ford step 7701 current loss 0.033455, current_train_items 246464.
I0304 19:31:50.474813 22849303695488 run.py:483] Algo bellman_ford step 7702 current loss 0.030307, current_train_items 246496.
I0304 19:31:50.507283 22849303695488 run.py:483] Algo bellman_ford step 7703 current loss 0.028560, current_train_items 246528.
I0304 19:31:50.541546 22849303695488 run.py:483] Algo bellman_ford step 7704 current loss 0.069683, current_train_items 246560.
I0304 19:31:50.562075 22849303695488 run.py:483] Algo bellman_ford step 7705 current loss 0.016887, current_train_items 246592.
I0304 19:31:50.578377 22849303695488 run.py:483] Algo bellman_ford step 7706 current loss 0.015334, current_train_items 246624.
I0304 19:31:50.604581 22849303695488 run.py:483] Algo bellman_ford step 7707 current loss 0.042649, current_train_items 246656.
I0304 19:31:50.636551 22849303695488 run.py:483] Algo bellman_ford step 7708 current loss 0.062461, current_train_items 246688.
I0304 19:31:50.669848 22849303695488 run.py:483] Algo bellman_ford step 7709 current loss 0.052471, current_train_items 246720.
I0304 19:31:50.690398 22849303695488 run.py:483] Algo bellman_ford step 7710 current loss 0.007394, current_train_items 246752.
I0304 19:31:50.707250 22849303695488 run.py:483] Algo bellman_ford step 7711 current loss 0.006452, current_train_items 246784.
I0304 19:31:50.732109 22849303695488 run.py:483] Algo bellman_ford step 7712 current loss 0.064835, current_train_items 246816.
I0304 19:31:50.763932 22849303695488 run.py:483] Algo bellman_ford step 7713 current loss 0.046167, current_train_items 246848.
I0304 19:31:50.798202 22849303695488 run.py:483] Algo bellman_ford step 7714 current loss 0.045721, current_train_items 246880.
I0304 19:31:50.818524 22849303695488 run.py:483] Algo bellman_ford step 7715 current loss 0.027106, current_train_items 246912.
I0304 19:31:50.835015 22849303695488 run.py:483] Algo bellman_ford step 7716 current loss 0.006684, current_train_items 246944.
I0304 19:31:50.859995 22849303695488 run.py:483] Algo bellman_ford step 7717 current loss 0.075718, current_train_items 246976.
I0304 19:31:50.892441 22849303695488 run.py:483] Algo bellman_ford step 7718 current loss 0.080255, current_train_items 247008.
I0304 19:31:50.925544 22849303695488 run.py:483] Algo bellman_ford step 7719 current loss 0.096531, current_train_items 247040.
I0304 19:31:50.945932 22849303695488 run.py:483] Algo bellman_ford step 7720 current loss 0.003267, current_train_items 247072.
I0304 19:31:50.962343 22849303695488 run.py:483] Algo bellman_ford step 7721 current loss 0.024887, current_train_items 247104.
I0304 19:31:50.987643 22849303695488 run.py:483] Algo bellman_ford step 7722 current loss 0.046317, current_train_items 247136.
I0304 19:31:51.021269 22849303695488 run.py:483] Algo bellman_ford step 7723 current loss 0.089064, current_train_items 247168.
I0304 19:31:51.056165 22849303695488 run.py:483] Algo bellman_ford step 7724 current loss 0.069308, current_train_items 247200.
I0304 19:31:51.076246 22849303695488 run.py:483] Algo bellman_ford step 7725 current loss 0.007896, current_train_items 247232.
I0304 19:31:51.092783 22849303695488 run.py:483] Algo bellman_ford step 7726 current loss 0.016963, current_train_items 247264.
I0304 19:31:51.116443 22849303695488 run.py:483] Algo bellman_ford step 7727 current loss 0.031309, current_train_items 247296.
I0304 19:31:51.149031 22849303695488 run.py:483] Algo bellman_ford step 7728 current loss 0.052408, current_train_items 247328.
I0304 19:31:51.182565 22849303695488 run.py:483] Algo bellman_ford step 7729 current loss 0.108066, current_train_items 247360.
I0304 19:31:51.203023 22849303695488 run.py:483] Algo bellman_ford step 7730 current loss 0.015654, current_train_items 247392.
I0304 19:31:51.219383 22849303695488 run.py:483] Algo bellman_ford step 7731 current loss 0.014055, current_train_items 247424.
I0304 19:31:51.243680 22849303695488 run.py:483] Algo bellman_ford step 7732 current loss 0.047518, current_train_items 247456.
I0304 19:31:51.276344 22849303695488 run.py:483] Algo bellman_ford step 7733 current loss 0.096646, current_train_items 247488.
I0304 19:31:51.309114 22849303695488 run.py:483] Algo bellman_ford step 7734 current loss 0.094850, current_train_items 247520.
I0304 19:31:51.329226 22849303695488 run.py:483] Algo bellman_ford step 7735 current loss 0.002735, current_train_items 247552.
I0304 19:31:51.345824 22849303695488 run.py:483] Algo bellman_ford step 7736 current loss 0.012758, current_train_items 247584.
I0304 19:31:51.370717 22849303695488 run.py:483] Algo bellman_ford step 7737 current loss 0.021140, current_train_items 247616.
I0304 19:31:51.402483 22849303695488 run.py:483] Algo bellman_ford step 7738 current loss 0.089053, current_train_items 247648.
I0304 19:31:51.437257 22849303695488 run.py:483] Algo bellman_ford step 7739 current loss 0.080917, current_train_items 247680.
I0304 19:31:51.457501 22849303695488 run.py:483] Algo bellman_ford step 7740 current loss 0.006948, current_train_items 247712.
I0304 19:31:51.474163 22849303695488 run.py:483] Algo bellman_ford step 7741 current loss 0.019423, current_train_items 247744.
I0304 19:31:51.500231 22849303695488 run.py:483] Algo bellman_ford step 7742 current loss 0.062839, current_train_items 247776.
I0304 19:31:51.533535 22849303695488 run.py:483] Algo bellman_ford step 7743 current loss 0.062000, current_train_items 247808.
I0304 19:31:51.568311 22849303695488 run.py:483] Algo bellman_ford step 7744 current loss 0.067389, current_train_items 247840.
I0304 19:31:51.588792 22849303695488 run.py:483] Algo bellman_ford step 7745 current loss 0.050158, current_train_items 247872.
I0304 19:31:51.605265 22849303695488 run.py:483] Algo bellman_ford step 7746 current loss 0.004732, current_train_items 247904.
I0304 19:31:51.629840 22849303695488 run.py:483] Algo bellman_ford step 7747 current loss 0.057564, current_train_items 247936.
I0304 19:31:51.663684 22849303695488 run.py:483] Algo bellman_ford step 7748 current loss 0.133239, current_train_items 247968.
I0304 19:31:51.698826 22849303695488 run.py:483] Algo bellman_ford step 7749 current loss 0.075376, current_train_items 248000.
I0304 19:31:51.719424 22849303695488 run.py:483] Algo bellman_ford step 7750 current loss 0.006061, current_train_items 248032.
I0304 19:31:51.727897 22849303695488 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0304 19:31:51.728024 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:51.745664 22849303695488 run.py:483] Algo bellman_ford step 7751 current loss 0.023132, current_train_items 248064.
I0304 19:31:51.770766 22849303695488 run.py:483] Algo bellman_ford step 7752 current loss 0.032089, current_train_items 248096.
I0304 19:31:51.803330 22849303695488 run.py:483] Algo bellman_ford step 7753 current loss 0.066891, current_train_items 248128.
I0304 19:31:51.839749 22849303695488 run.py:483] Algo bellman_ford step 7754 current loss 0.068197, current_train_items 248160.
I0304 19:31:51.860517 22849303695488 run.py:483] Algo bellman_ford step 7755 current loss 0.002765, current_train_items 248192.
I0304 19:31:51.876444 22849303695488 run.py:483] Algo bellman_ford step 7756 current loss 0.019921, current_train_items 248224.
I0304 19:31:51.900990 22849303695488 run.py:483] Algo bellman_ford step 7757 current loss 0.027781, current_train_items 248256.
I0304 19:31:51.932817 22849303695488 run.py:483] Algo bellman_ford step 7758 current loss 0.064540, current_train_items 248288.
I0304 19:31:51.966902 22849303695488 run.py:483] Algo bellman_ford step 7759 current loss 0.067417, current_train_items 248320.
I0304 19:31:51.987361 22849303695488 run.py:483] Algo bellman_ford step 7760 current loss 0.002507, current_train_items 248352.
I0304 19:31:52.003966 22849303695488 run.py:483] Algo bellman_ford step 7761 current loss 0.034485, current_train_items 248384.
I0304 19:31:52.028692 22849303695488 run.py:483] Algo bellman_ford step 7762 current loss 0.036905, current_train_items 248416.
I0304 19:31:52.061198 22849303695488 run.py:483] Algo bellman_ford step 7763 current loss 0.032618, current_train_items 248448.
I0304 19:31:52.094045 22849303695488 run.py:483] Algo bellman_ford step 7764 current loss 0.059668, current_train_items 248480.
I0304 19:31:52.114099 22849303695488 run.py:483] Algo bellman_ford step 7765 current loss 0.001950, current_train_items 248512.
I0304 19:31:52.130796 22849303695488 run.py:483] Algo bellman_ford step 7766 current loss 0.018570, current_train_items 248544.
I0304 19:31:52.154994 22849303695488 run.py:483] Algo bellman_ford step 7767 current loss 0.031523, current_train_items 248576.
I0304 19:31:52.187575 22849303695488 run.py:483] Algo bellman_ford step 7768 current loss 0.037753, current_train_items 248608.
I0304 19:31:52.221829 22849303695488 run.py:483] Algo bellman_ford step 7769 current loss 0.062241, current_train_items 248640.
I0304 19:31:52.242363 22849303695488 run.py:483] Algo bellman_ford step 7770 current loss 0.001641, current_train_items 248672.
I0304 19:31:52.259225 22849303695488 run.py:483] Algo bellman_ford step 7771 current loss 0.030591, current_train_items 248704.
I0304 19:31:52.283574 22849303695488 run.py:483] Algo bellman_ford step 7772 current loss 0.035289, current_train_items 248736.
I0304 19:31:52.316797 22849303695488 run.py:483] Algo bellman_ford step 7773 current loss 0.042903, current_train_items 248768.
I0304 19:31:52.350930 22849303695488 run.py:483] Algo bellman_ford step 7774 current loss 0.068127, current_train_items 248800.
I0304 19:31:52.371845 22849303695488 run.py:483] Algo bellman_ford step 7775 current loss 0.002511, current_train_items 248832.
I0304 19:31:52.388621 22849303695488 run.py:483] Algo bellman_ford step 7776 current loss 0.048158, current_train_items 248864.
I0304 19:31:52.413883 22849303695488 run.py:483] Algo bellman_ford step 7777 current loss 0.031447, current_train_items 248896.
I0304 19:31:52.446831 22849303695488 run.py:483] Algo bellman_ford step 7778 current loss 0.059688, current_train_items 248928.
I0304 19:31:52.482852 22849303695488 run.py:483] Algo bellman_ford step 7779 current loss 0.105954, current_train_items 248960.
I0304 19:31:52.502683 22849303695488 run.py:483] Algo bellman_ford step 7780 current loss 0.001852, current_train_items 248992.
I0304 19:31:52.519549 22849303695488 run.py:483] Algo bellman_ford step 7781 current loss 0.015267, current_train_items 249024.
I0304 19:31:52.544149 22849303695488 run.py:483] Algo bellman_ford step 7782 current loss 0.053248, current_train_items 249056.
I0304 19:31:52.577686 22849303695488 run.py:483] Algo bellman_ford step 7783 current loss 0.049955, current_train_items 249088.
I0304 19:31:52.612243 22849303695488 run.py:483] Algo bellman_ford step 7784 current loss 0.036596, current_train_items 249120.
I0304 19:31:52.632996 22849303695488 run.py:483] Algo bellman_ford step 7785 current loss 0.006237, current_train_items 249152.
I0304 19:31:52.649708 22849303695488 run.py:483] Algo bellman_ford step 7786 current loss 0.042313, current_train_items 249184.
I0304 19:31:52.673660 22849303695488 run.py:483] Algo bellman_ford step 7787 current loss 0.051332, current_train_items 249216.
I0304 19:31:52.706171 22849303695488 run.py:483] Algo bellman_ford step 7788 current loss 0.055274, current_train_items 249248.
I0304 19:31:52.738404 22849303695488 run.py:483] Algo bellman_ford step 7789 current loss 0.062625, current_train_items 249280.
I0304 19:31:52.758816 22849303695488 run.py:483] Algo bellman_ford step 7790 current loss 0.003786, current_train_items 249312.
I0304 19:31:52.775087 22849303695488 run.py:483] Algo bellman_ford step 7791 current loss 0.014067, current_train_items 249344.
I0304 19:31:52.799824 22849303695488 run.py:483] Algo bellman_ford step 7792 current loss 0.059772, current_train_items 249376.
I0304 19:31:52.831713 22849303695488 run.py:483] Algo bellman_ford step 7793 current loss 0.065382, current_train_items 249408.
I0304 19:31:52.865418 22849303695488 run.py:483] Algo bellman_ford step 7794 current loss 0.057166, current_train_items 249440.
I0304 19:31:52.885470 22849303695488 run.py:483] Algo bellman_ford step 7795 current loss 0.011594, current_train_items 249472.
I0304 19:31:52.901689 22849303695488 run.py:483] Algo bellman_ford step 7796 current loss 0.007605, current_train_items 249504.
I0304 19:31:52.925830 22849303695488 run.py:483] Algo bellman_ford step 7797 current loss 0.033837, current_train_items 249536.
I0304 19:31:52.957621 22849303695488 run.py:483] Algo bellman_ford step 7798 current loss 0.047213, current_train_items 249568.
I0304 19:31:52.993180 22849303695488 run.py:483] Algo bellman_ford step 7799 current loss 0.129159, current_train_items 249600.
I0304 19:31:53.013926 22849303695488 run.py:483] Algo bellman_ford step 7800 current loss 0.004518, current_train_items 249632.
I0304 19:31:53.022070 22849303695488 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0304 19:31:53.022180 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:53.039289 22849303695488 run.py:483] Algo bellman_ford step 7801 current loss 0.046335, current_train_items 249664.
I0304 19:31:53.064399 22849303695488 run.py:483] Algo bellman_ford step 7802 current loss 0.025673, current_train_items 249696.
I0304 19:31:53.098477 22849303695488 run.py:483] Algo bellman_ford step 7803 current loss 0.054973, current_train_items 249728.
I0304 19:31:53.135023 22849303695488 run.py:483] Algo bellman_ford step 7804 current loss 0.077423, current_train_items 249760.
I0304 19:31:53.155361 22849303695488 run.py:483] Algo bellman_ford step 7805 current loss 0.016111, current_train_items 249792.
I0304 19:31:53.171787 22849303695488 run.py:483] Algo bellman_ford step 7806 current loss 0.004469, current_train_items 249824.
I0304 19:31:53.195455 22849303695488 run.py:483] Algo bellman_ford step 7807 current loss 0.017384, current_train_items 249856.
I0304 19:31:53.227079 22849303695488 run.py:483] Algo bellman_ford step 7808 current loss 0.049039, current_train_items 249888.
I0304 19:31:53.261487 22849303695488 run.py:483] Algo bellman_ford step 7809 current loss 0.165442, current_train_items 249920.
I0304 19:31:53.282281 22849303695488 run.py:483] Algo bellman_ford step 7810 current loss 0.002328, current_train_items 249952.
I0304 19:31:53.299165 22849303695488 run.py:483] Algo bellman_ford step 7811 current loss 0.006512, current_train_items 249984.
I0304 19:31:53.323996 22849303695488 run.py:483] Algo bellman_ford step 7812 current loss 0.078458, current_train_items 250016.
I0304 19:31:53.357370 22849303695488 run.py:483] Algo bellman_ford step 7813 current loss 0.075943, current_train_items 250048.
I0304 19:31:53.391401 22849303695488 run.py:483] Algo bellman_ford step 7814 current loss 0.072088, current_train_items 250080.
I0304 19:31:53.411109 22849303695488 run.py:483] Algo bellman_ford step 7815 current loss 0.006945, current_train_items 250112.
I0304 19:31:53.427550 22849303695488 run.py:483] Algo bellman_ford step 7816 current loss 0.021626, current_train_items 250144.
I0304 19:31:53.452888 22849303695488 run.py:483] Algo bellman_ford step 7817 current loss 0.033057, current_train_items 250176.
I0304 19:31:53.483899 22849303695488 run.py:483] Algo bellman_ford step 7818 current loss 0.085600, current_train_items 250208.
I0304 19:31:53.516771 22849303695488 run.py:483] Algo bellman_ford step 7819 current loss 0.060248, current_train_items 250240.
I0304 19:31:53.536598 22849303695488 run.py:483] Algo bellman_ford step 7820 current loss 0.004067, current_train_items 250272.
I0304 19:31:53.553436 22849303695488 run.py:483] Algo bellman_ford step 7821 current loss 0.028052, current_train_items 250304.
I0304 19:31:53.579099 22849303695488 run.py:483] Algo bellman_ford step 7822 current loss 0.074226, current_train_items 250336.
I0304 19:31:53.613183 22849303695488 run.py:483] Algo bellman_ford step 7823 current loss 0.270604, current_train_items 250368.
I0304 19:31:53.646559 22849303695488 run.py:483] Algo bellman_ford step 7824 current loss 0.257688, current_train_items 250400.
I0304 19:31:53.666504 22849303695488 run.py:483] Algo bellman_ford step 7825 current loss 0.003601, current_train_items 250432.
I0304 19:31:53.683542 22849303695488 run.py:483] Algo bellman_ford step 7826 current loss 0.065820, current_train_items 250464.
I0304 19:31:53.708479 22849303695488 run.py:483] Algo bellman_ford step 7827 current loss 0.032643, current_train_items 250496.
I0304 19:31:53.740898 22849303695488 run.py:483] Algo bellman_ford step 7828 current loss 0.092001, current_train_items 250528.
I0304 19:31:53.775211 22849303695488 run.py:483] Algo bellman_ford step 7829 current loss 0.148612, current_train_items 250560.
I0304 19:31:53.795189 22849303695488 run.py:483] Algo bellman_ford step 7830 current loss 0.006613, current_train_items 250592.
I0304 19:31:53.811447 22849303695488 run.py:483] Algo bellman_ford step 7831 current loss 0.026093, current_train_items 250624.
I0304 19:31:53.836046 22849303695488 run.py:483] Algo bellman_ford step 7832 current loss 0.020299, current_train_items 250656.
I0304 19:31:53.869507 22849303695488 run.py:483] Algo bellman_ford step 7833 current loss 0.071384, current_train_items 250688.
I0304 19:31:53.901732 22849303695488 run.py:483] Algo bellman_ford step 7834 current loss 0.068272, current_train_items 250720.
I0304 19:31:53.921760 22849303695488 run.py:483] Algo bellman_ford step 7835 current loss 0.017405, current_train_items 250752.
I0304 19:31:53.938635 22849303695488 run.py:483] Algo bellman_ford step 7836 current loss 0.023312, current_train_items 250784.
I0304 19:31:53.963156 22849303695488 run.py:483] Algo bellman_ford step 7837 current loss 0.049627, current_train_items 250816.
I0304 19:31:53.996784 22849303695488 run.py:483] Algo bellman_ford step 7838 current loss 0.027001, current_train_items 250848.
I0304 19:31:54.032140 22849303695488 run.py:483] Algo bellman_ford step 7839 current loss 0.106056, current_train_items 250880.
I0304 19:31:54.052067 22849303695488 run.py:483] Algo bellman_ford step 7840 current loss 0.004374, current_train_items 250912.
I0304 19:31:54.068723 22849303695488 run.py:483] Algo bellman_ford step 7841 current loss 0.008581, current_train_items 250944.
I0304 19:31:54.093436 22849303695488 run.py:483] Algo bellman_ford step 7842 current loss 0.026608, current_train_items 250976.
I0304 19:31:54.127196 22849303695488 run.py:483] Algo bellman_ford step 7843 current loss 0.084907, current_train_items 251008.
I0304 19:31:54.162400 22849303695488 run.py:483] Algo bellman_ford step 7844 current loss 0.118369, current_train_items 251040.
I0304 19:31:54.182332 22849303695488 run.py:483] Algo bellman_ford step 7845 current loss 0.004322, current_train_items 251072.
I0304 19:31:54.198850 22849303695488 run.py:483] Algo bellman_ford step 7846 current loss 0.011348, current_train_items 251104.
I0304 19:31:54.223761 22849303695488 run.py:483] Algo bellman_ford step 7847 current loss 0.044223, current_train_items 251136.
I0304 19:31:54.256759 22849303695488 run.py:483] Algo bellman_ford step 7848 current loss 0.051305, current_train_items 251168.
I0304 19:31:54.291379 22849303695488 run.py:483] Algo bellman_ford step 7849 current loss 0.116174, current_train_items 251200.
I0304 19:31:54.311122 22849303695488 run.py:483] Algo bellman_ford step 7850 current loss 0.005390, current_train_items 251232.
I0304 19:31:54.319548 22849303695488 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0304 19:31:54.319656 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:54.336868 22849303695488 run.py:483] Algo bellman_ford step 7851 current loss 0.054489, current_train_items 251264.
I0304 19:31:54.361810 22849303695488 run.py:483] Algo bellman_ford step 7852 current loss 0.020115, current_train_items 251296.
I0304 19:31:54.395676 22849303695488 run.py:483] Algo bellman_ford step 7853 current loss 0.067529, current_train_items 251328.
I0304 19:31:54.432759 22849303695488 run.py:483] Algo bellman_ford step 7854 current loss 0.072984, current_train_items 251360.
I0304 19:31:54.453252 22849303695488 run.py:483] Algo bellman_ford step 7855 current loss 0.004472, current_train_items 251392.
I0304 19:31:54.470077 22849303695488 run.py:483] Algo bellman_ford step 7856 current loss 0.025906, current_train_items 251424.
I0304 19:31:54.495231 22849303695488 run.py:483] Algo bellman_ford step 7857 current loss 0.045719, current_train_items 251456.
I0304 19:31:54.527871 22849303695488 run.py:483] Algo bellman_ford step 7858 current loss 0.024057, current_train_items 251488.
I0304 19:31:54.563349 22849303695488 run.py:483] Algo bellman_ford step 7859 current loss 0.075792, current_train_items 251520.
I0304 19:31:54.583935 22849303695488 run.py:483] Algo bellman_ford step 7860 current loss 0.009685, current_train_items 251552.
I0304 19:31:54.600394 22849303695488 run.py:483] Algo bellman_ford step 7861 current loss 0.012514, current_train_items 251584.
I0304 19:31:54.624843 22849303695488 run.py:483] Algo bellman_ford step 7862 current loss 0.028057, current_train_items 251616.
I0304 19:31:54.657375 22849303695488 run.py:483] Algo bellman_ford step 7863 current loss 0.078295, current_train_items 251648.
I0304 19:31:54.690873 22849303695488 run.py:483] Algo bellman_ford step 7864 current loss 0.042233, current_train_items 251680.
I0304 19:31:54.711091 22849303695488 run.py:483] Algo bellman_ford step 7865 current loss 0.003586, current_train_items 251712.
I0304 19:31:54.728050 22849303695488 run.py:483] Algo bellman_ford step 7866 current loss 0.018076, current_train_items 251744.
I0304 19:31:54.753631 22849303695488 run.py:483] Algo bellman_ford step 7867 current loss 0.062963, current_train_items 251776.
I0304 19:31:54.785668 22849303695488 run.py:483] Algo bellman_ford step 7868 current loss 0.056229, current_train_items 251808.
I0304 19:31:54.819431 22849303695488 run.py:483] Algo bellman_ford step 7869 current loss 0.068385, current_train_items 251840.
I0304 19:31:54.840241 22849303695488 run.py:483] Algo bellman_ford step 7870 current loss 0.003817, current_train_items 251872.
I0304 19:31:54.856954 22849303695488 run.py:483] Algo bellman_ford step 7871 current loss 0.018182, current_train_items 251904.
I0304 19:31:54.881694 22849303695488 run.py:483] Algo bellman_ford step 7872 current loss 0.029958, current_train_items 251936.
I0304 19:31:54.913916 22849303695488 run.py:483] Algo bellman_ford step 7873 current loss 0.051009, current_train_items 251968.
I0304 19:31:54.948118 22849303695488 run.py:483] Algo bellman_ford step 7874 current loss 0.062553, current_train_items 252000.
I0304 19:31:54.968720 22849303695488 run.py:483] Algo bellman_ford step 7875 current loss 0.010497, current_train_items 252032.
I0304 19:31:54.985000 22849303695488 run.py:483] Algo bellman_ford step 7876 current loss 0.050976, current_train_items 252064.
I0304 19:31:55.009356 22849303695488 run.py:483] Algo bellman_ford step 7877 current loss 0.092798, current_train_items 252096.
I0304 19:31:55.041982 22849303695488 run.py:483] Algo bellman_ford step 7878 current loss 0.060863, current_train_items 252128.
I0304 19:31:55.076957 22849303695488 run.py:483] Algo bellman_ford step 7879 current loss 0.117703, current_train_items 252160.
I0304 19:31:55.097242 22849303695488 run.py:483] Algo bellman_ford step 7880 current loss 0.002294, current_train_items 252192.
I0304 19:31:55.113515 22849303695488 run.py:483] Algo bellman_ford step 7881 current loss 0.009008, current_train_items 252224.
I0304 19:31:55.137681 22849303695488 run.py:483] Algo bellman_ford step 7882 current loss 0.060573, current_train_items 252256.
I0304 19:31:55.170541 22849303695488 run.py:483] Algo bellman_ford step 7883 current loss 0.052203, current_train_items 252288.
I0304 19:31:55.205533 22849303695488 run.py:483] Algo bellman_ford step 7884 current loss 0.101116, current_train_items 252320.
I0304 19:31:55.226342 22849303695488 run.py:483] Algo bellman_ford step 7885 current loss 0.004514, current_train_items 252352.
I0304 19:31:55.243073 22849303695488 run.py:483] Algo bellman_ford step 7886 current loss 0.029362, current_train_items 252384.
I0304 19:31:55.266315 22849303695488 run.py:483] Algo bellman_ford step 7887 current loss 0.021124, current_train_items 252416.
I0304 19:31:55.297206 22849303695488 run.py:483] Algo bellman_ford step 7888 current loss 0.028107, current_train_items 252448.
I0304 19:31:55.331368 22849303695488 run.py:483] Algo bellman_ford step 7889 current loss 0.040183, current_train_items 252480.
I0304 19:31:55.352251 22849303695488 run.py:483] Algo bellman_ford step 7890 current loss 0.002446, current_train_items 252512.
I0304 19:31:55.368749 22849303695488 run.py:483] Algo bellman_ford step 7891 current loss 0.004298, current_train_items 252544.
I0304 19:31:55.393479 22849303695488 run.py:483] Algo bellman_ford step 7892 current loss 0.045070, current_train_items 252576.
I0304 19:31:55.426882 22849303695488 run.py:483] Algo bellman_ford step 7893 current loss 0.054436, current_train_items 252608.
I0304 19:31:55.460795 22849303695488 run.py:483] Algo bellman_ford step 7894 current loss 0.073408, current_train_items 252640.
I0304 19:31:55.481129 22849303695488 run.py:483] Algo bellman_ford step 7895 current loss 0.003066, current_train_items 252672.
I0304 19:31:55.497431 22849303695488 run.py:483] Algo bellman_ford step 7896 current loss 0.021523, current_train_items 252704.
I0304 19:31:55.522880 22849303695488 run.py:483] Algo bellman_ford step 7897 current loss 0.059319, current_train_items 252736.
I0304 19:31:55.556414 22849303695488 run.py:483] Algo bellman_ford step 7898 current loss 0.043433, current_train_items 252768.
I0304 19:31:55.591113 22849303695488 run.py:483] Algo bellman_ford step 7899 current loss 0.083178, current_train_items 252800.
I0304 19:31:55.611812 22849303695488 run.py:483] Algo bellman_ford step 7900 current loss 0.003022, current_train_items 252832.
I0304 19:31:55.620222 22849303695488 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0304 19:31:55.620331 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:55.637458 22849303695488 run.py:483] Algo bellman_ford step 7901 current loss 0.029990, current_train_items 252864.
I0304 19:31:55.663031 22849303695488 run.py:483] Algo bellman_ford step 7902 current loss 0.042982, current_train_items 252896.
I0304 19:31:55.696801 22849303695488 run.py:483] Algo bellman_ford step 7903 current loss 0.037915, current_train_items 252928.
I0304 19:31:55.734074 22849303695488 run.py:483] Algo bellman_ford step 7904 current loss 0.075530, current_train_items 252960.
I0304 19:31:55.754577 22849303695488 run.py:483] Algo bellman_ford step 7905 current loss 0.003140, current_train_items 252992.
I0304 19:31:55.771501 22849303695488 run.py:483] Algo bellman_ford step 7906 current loss 0.028520, current_train_items 253024.
I0304 19:31:55.796913 22849303695488 run.py:483] Algo bellman_ford step 7907 current loss 0.040531, current_train_items 253056.
I0304 19:31:55.829256 22849303695488 run.py:483] Algo bellman_ford step 7908 current loss 0.024142, current_train_items 253088.
I0304 19:31:55.861034 22849303695488 run.py:483] Algo bellman_ford step 7909 current loss 0.046490, current_train_items 253120.
I0304 19:31:55.881178 22849303695488 run.py:483] Algo bellman_ford step 7910 current loss 0.008949, current_train_items 253152.
I0304 19:31:55.898357 22849303695488 run.py:483] Algo bellman_ford step 7911 current loss 0.015157, current_train_items 253184.
I0304 19:31:55.923659 22849303695488 run.py:483] Algo bellman_ford step 7912 current loss 0.045225, current_train_items 253216.
I0304 19:31:55.956687 22849303695488 run.py:483] Algo bellman_ford step 7913 current loss 0.028765, current_train_items 253248.
I0304 19:31:55.991398 22849303695488 run.py:483] Algo bellman_ford step 7914 current loss 0.075278, current_train_items 253280.
I0304 19:31:56.011658 22849303695488 run.py:483] Algo bellman_ford step 7915 current loss 0.025194, current_train_items 253312.
I0304 19:31:56.028535 22849303695488 run.py:483] Algo bellman_ford step 7916 current loss 0.054449, current_train_items 253344.
I0304 19:31:56.053518 22849303695488 run.py:483] Algo bellman_ford step 7917 current loss 0.024040, current_train_items 253376.
I0304 19:31:56.086134 22849303695488 run.py:483] Algo bellman_ford step 7918 current loss 0.041255, current_train_items 253408.
I0304 19:31:56.122460 22849303695488 run.py:483] Algo bellman_ford step 7919 current loss 0.058585, current_train_items 253440.
I0304 19:31:56.142732 22849303695488 run.py:483] Algo bellman_ford step 7920 current loss 0.001845, current_train_items 253472.
I0304 19:31:56.159608 22849303695488 run.py:483] Algo bellman_ford step 7921 current loss 0.008494, current_train_items 253504.
I0304 19:31:56.184899 22849303695488 run.py:483] Algo bellman_ford step 7922 current loss 0.034306, current_train_items 253536.
I0304 19:31:56.216788 22849303695488 run.py:483] Algo bellman_ford step 7923 current loss 0.041930, current_train_items 253568.
I0304 19:31:56.251428 22849303695488 run.py:483] Algo bellman_ford step 7924 current loss 0.058480, current_train_items 253600.
I0304 19:31:56.271509 22849303695488 run.py:483] Algo bellman_ford step 7925 current loss 0.035645, current_train_items 253632.
I0304 19:31:56.288421 22849303695488 run.py:483] Algo bellman_ford step 7926 current loss 0.012020, current_train_items 253664.
I0304 19:31:56.312993 22849303695488 run.py:483] Algo bellman_ford step 7927 current loss 0.101780, current_train_items 253696.
I0304 19:31:56.346704 22849303695488 run.py:483] Algo bellman_ford step 7928 current loss 0.099984, current_train_items 253728.
I0304 19:31:56.380871 22849303695488 run.py:483] Algo bellman_ford step 7929 current loss 0.064996, current_train_items 253760.
I0304 19:31:56.401149 22849303695488 run.py:483] Algo bellman_ford step 7930 current loss 0.009192, current_train_items 253792.
I0304 19:31:56.417784 22849303695488 run.py:483] Algo bellman_ford step 7931 current loss 0.029351, current_train_items 253824.
I0304 19:31:56.441620 22849303695488 run.py:483] Algo bellman_ford step 7932 current loss 0.047911, current_train_items 253856.
I0304 19:31:56.474537 22849303695488 run.py:483] Algo bellman_ford step 7933 current loss 0.050549, current_train_items 253888.
I0304 19:31:56.509323 22849303695488 run.py:483] Algo bellman_ford step 7934 current loss 0.077671, current_train_items 253920.
I0304 19:31:56.529354 22849303695488 run.py:483] Algo bellman_ford step 7935 current loss 0.005309, current_train_items 253952.
I0304 19:31:56.545628 22849303695488 run.py:483] Algo bellman_ford step 7936 current loss 0.015392, current_train_items 253984.
I0304 19:31:56.570904 22849303695488 run.py:483] Algo bellman_ford step 7937 current loss 0.042357, current_train_items 254016.
I0304 19:31:56.603120 22849303695488 run.py:483] Algo bellman_ford step 7938 current loss 0.049740, current_train_items 254048.
I0304 19:31:56.638794 22849303695488 run.py:483] Algo bellman_ford step 7939 current loss 0.095622, current_train_items 254080.
I0304 19:31:56.658765 22849303695488 run.py:483] Algo bellman_ford step 7940 current loss 0.005990, current_train_items 254112.
I0304 19:31:56.674837 22849303695488 run.py:483] Algo bellman_ford step 7941 current loss 0.006057, current_train_items 254144.
I0304 19:31:56.700475 22849303695488 run.py:483] Algo bellman_ford step 7942 current loss 0.076416, current_train_items 254176.
I0304 19:31:56.734597 22849303695488 run.py:483] Algo bellman_ford step 7943 current loss 0.174475, current_train_items 254208.
I0304 19:31:56.768516 22849303695488 run.py:483] Algo bellman_ford step 7944 current loss 0.145136, current_train_items 254240.
I0304 19:31:56.788650 22849303695488 run.py:483] Algo bellman_ford step 7945 current loss 0.005063, current_train_items 254272.
I0304 19:31:56.804851 22849303695488 run.py:483] Algo bellman_ford step 7946 current loss 0.017596, current_train_items 254304.
I0304 19:31:56.828867 22849303695488 run.py:483] Algo bellman_ford step 7947 current loss 0.067604, current_train_items 254336.
I0304 19:31:56.860094 22849303695488 run.py:483] Algo bellman_ford step 7948 current loss 0.030395, current_train_items 254368.
I0304 19:31:56.894865 22849303695488 run.py:483] Algo bellman_ford step 7949 current loss 0.091706, current_train_items 254400.
I0304 19:31:56.914936 22849303695488 run.py:483] Algo bellman_ford step 7950 current loss 0.003591, current_train_items 254432.
I0304 19:31:56.923319 22849303695488 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0304 19:31:56.923429 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:56.940639 22849303695488 run.py:483] Algo bellman_ford step 7951 current loss 0.007758, current_train_items 254464.
I0304 19:31:56.966458 22849303695488 run.py:483] Algo bellman_ford step 7952 current loss 0.050513, current_train_items 254496.
I0304 19:31:57.000564 22849303695488 run.py:483] Algo bellman_ford step 7953 current loss 0.032271, current_train_items 254528.
I0304 19:31:57.034921 22849303695488 run.py:483] Algo bellman_ford step 7954 current loss 0.051080, current_train_items 254560.
I0304 19:31:57.055623 22849303695488 run.py:483] Algo bellman_ford step 7955 current loss 0.005952, current_train_items 254592.
I0304 19:31:57.072244 22849303695488 run.py:483] Algo bellman_ford step 7956 current loss 0.026011, current_train_items 254624.
I0304 19:31:57.096898 22849303695488 run.py:483] Algo bellman_ford step 7957 current loss 0.028341, current_train_items 254656.
I0304 19:31:57.129722 22849303695488 run.py:483] Algo bellman_ford step 7958 current loss 0.100615, current_train_items 254688.
I0304 19:31:57.163690 22849303695488 run.py:483] Algo bellman_ford step 7959 current loss 0.038537, current_train_items 254720.
I0304 19:31:57.184204 22849303695488 run.py:483] Algo bellman_ford step 7960 current loss 0.002088, current_train_items 254752.
I0304 19:31:57.201254 22849303695488 run.py:483] Algo bellman_ford step 7961 current loss 0.019854, current_train_items 254784.
I0304 19:31:57.225898 22849303695488 run.py:483] Algo bellman_ford step 7962 current loss 0.041321, current_train_items 254816.
I0304 19:31:57.257378 22849303695488 run.py:483] Algo bellman_ford step 7963 current loss 0.039023, current_train_items 254848.
I0304 19:31:57.292599 22849303695488 run.py:483] Algo bellman_ford step 7964 current loss 0.035811, current_train_items 254880.
I0304 19:31:57.312717 22849303695488 run.py:483] Algo bellman_ford step 7965 current loss 0.003659, current_train_items 254912.
I0304 19:31:57.329610 22849303695488 run.py:483] Algo bellman_ford step 7966 current loss 0.020008, current_train_items 254944.
I0304 19:31:57.355202 22849303695488 run.py:483] Algo bellman_ford step 7967 current loss 0.079635, current_train_items 254976.
I0304 19:31:57.387855 22849303695488 run.py:483] Algo bellman_ford step 7968 current loss 0.044311, current_train_items 255008.
I0304 19:31:57.420849 22849303695488 run.py:483] Algo bellman_ford step 7969 current loss 0.045619, current_train_items 255040.
I0304 19:31:57.441384 22849303695488 run.py:483] Algo bellman_ford step 7970 current loss 0.005765, current_train_items 255072.
I0304 19:31:57.457996 22849303695488 run.py:483] Algo bellman_ford step 7971 current loss 0.033115, current_train_items 255104.
I0304 19:31:57.483025 22849303695488 run.py:483] Algo bellman_ford step 7972 current loss 0.055098, current_train_items 255136.
I0304 19:31:57.514944 22849303695488 run.py:483] Algo bellman_ford step 7973 current loss 0.038476, current_train_items 255168.
I0304 19:31:57.550838 22849303695488 run.py:483] Algo bellman_ford step 7974 current loss 0.072000, current_train_items 255200.
I0304 19:31:57.571503 22849303695488 run.py:483] Algo bellman_ford step 7975 current loss 0.008150, current_train_items 255232.
I0304 19:31:57.588277 22849303695488 run.py:483] Algo bellman_ford step 7976 current loss 0.023169, current_train_items 255264.
I0304 19:31:57.613027 22849303695488 run.py:483] Algo bellman_ford step 7977 current loss 0.044302, current_train_items 255296.
I0304 19:31:57.645709 22849303695488 run.py:483] Algo bellman_ford step 7978 current loss 0.050219, current_train_items 255328.
I0304 19:31:57.678154 22849303695488 run.py:483] Algo bellman_ford step 7979 current loss 0.038746, current_train_items 255360.
I0304 19:31:57.698267 22849303695488 run.py:483] Algo bellman_ford step 7980 current loss 0.002507, current_train_items 255392.
I0304 19:31:57.714689 22849303695488 run.py:483] Algo bellman_ford step 7981 current loss 0.044980, current_train_items 255424.
I0304 19:31:57.738826 22849303695488 run.py:483] Algo bellman_ford step 7982 current loss 0.028972, current_train_items 255456.
I0304 19:31:57.770618 22849303695488 run.py:483] Algo bellman_ford step 7983 current loss 0.035236, current_train_items 255488.
I0304 19:31:57.805515 22849303695488 run.py:483] Algo bellman_ford step 7984 current loss 0.048168, current_train_items 255520.
I0304 19:31:57.826129 22849303695488 run.py:483] Algo bellman_ford step 7985 current loss 0.005422, current_train_items 255552.
I0304 19:31:57.842995 22849303695488 run.py:483] Algo bellman_ford step 7986 current loss 0.023406, current_train_items 255584.
I0304 19:31:57.866665 22849303695488 run.py:483] Algo bellman_ford step 7987 current loss 0.054081, current_train_items 255616.
I0304 19:31:57.899612 22849303695488 run.py:483] Algo bellman_ford step 7988 current loss 0.036372, current_train_items 255648.
I0304 19:31:57.933459 22849303695488 run.py:483] Algo bellman_ford step 7989 current loss 0.060649, current_train_items 255680.
I0304 19:31:57.954151 22849303695488 run.py:483] Algo bellman_ford step 7990 current loss 0.001986, current_train_items 255712.
I0304 19:31:57.970988 22849303695488 run.py:483] Algo bellman_ford step 7991 current loss 0.015193, current_train_items 255744.
I0304 19:31:57.995609 22849303695488 run.py:483] Algo bellman_ford step 7992 current loss 0.073988, current_train_items 255776.
I0304 19:31:58.030128 22849303695488 run.py:483] Algo bellman_ford step 7993 current loss 0.063915, current_train_items 255808.
I0304 19:31:58.065804 22849303695488 run.py:483] Algo bellman_ford step 7994 current loss 0.048673, current_train_items 255840.
I0304 19:31:58.086028 22849303695488 run.py:483] Algo bellman_ford step 7995 current loss 0.003172, current_train_items 255872.
I0304 19:31:58.102160 22849303695488 run.py:483] Algo bellman_ford step 7996 current loss 0.008741, current_train_items 255904.
I0304 19:31:58.126521 22849303695488 run.py:483] Algo bellman_ford step 7997 current loss 0.047149, current_train_items 255936.
I0304 19:31:58.158672 22849303695488 run.py:483] Algo bellman_ford step 7998 current loss 0.030891, current_train_items 255968.
I0304 19:31:58.191168 22849303695488 run.py:483] Algo bellman_ford step 7999 current loss 0.058139, current_train_items 256000.
I0304 19:31:58.211657 22849303695488 run.py:483] Algo bellman_ford step 8000 current loss 0.001785, current_train_items 256032.
I0304 19:31:58.219868 22849303695488 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0304 19:31:58.219976 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:31:58.237399 22849303695488 run.py:483] Algo bellman_ford step 8001 current loss 0.009208, current_train_items 256064.
I0304 19:31:58.262269 22849303695488 run.py:483] Algo bellman_ford step 8002 current loss 0.052971, current_train_items 256096.
I0304 19:31:58.295884 22849303695488 run.py:483] Algo bellman_ford step 8003 current loss 0.053996, current_train_items 256128.
I0304 19:31:58.329140 22849303695488 run.py:483] Algo bellman_ford step 8004 current loss 0.060510, current_train_items 256160.
I0304 19:31:58.349677 22849303695488 run.py:483] Algo bellman_ford step 8005 current loss 0.004302, current_train_items 256192.
I0304 19:31:58.366325 22849303695488 run.py:483] Algo bellman_ford step 8006 current loss 0.014632, current_train_items 256224.
I0304 19:31:58.391286 22849303695488 run.py:483] Algo bellman_ford step 8007 current loss 0.027010, current_train_items 256256.
I0304 19:31:58.425960 22849303695488 run.py:483] Algo bellman_ford step 8008 current loss 0.058553, current_train_items 256288.
I0304 19:31:58.460434 22849303695488 run.py:483] Algo bellman_ford step 8009 current loss 0.063306, current_train_items 256320.
I0304 19:31:58.480721 22849303695488 run.py:483] Algo bellman_ford step 8010 current loss 0.003334, current_train_items 256352.
I0304 19:31:58.497143 22849303695488 run.py:483] Algo bellman_ford step 8011 current loss 0.031885, current_train_items 256384.
I0304 19:31:58.522903 22849303695488 run.py:483] Algo bellman_ford step 8012 current loss 0.039476, current_train_items 256416.
I0304 19:31:58.555153 22849303695488 run.py:483] Algo bellman_ford step 8013 current loss 0.067966, current_train_items 256448.
I0304 19:31:58.587733 22849303695488 run.py:483] Algo bellman_ford step 8014 current loss 0.060039, current_train_items 256480.
I0304 19:31:58.608088 22849303695488 run.py:483] Algo bellman_ford step 8015 current loss 0.039426, current_train_items 256512.
I0304 19:31:58.625407 22849303695488 run.py:483] Algo bellman_ford step 8016 current loss 0.022474, current_train_items 256544.
I0304 19:31:58.649825 22849303695488 run.py:483] Algo bellman_ford step 8017 current loss 0.087742, current_train_items 256576.
I0304 19:31:58.682242 22849303695488 run.py:483] Algo bellman_ford step 8018 current loss 0.091601, current_train_items 256608.
I0304 19:31:58.715260 22849303695488 run.py:483] Algo bellman_ford step 8019 current loss 0.063581, current_train_items 256640.
I0304 19:31:58.735302 22849303695488 run.py:483] Algo bellman_ford step 8020 current loss 0.005602, current_train_items 256672.
I0304 19:31:58.751993 22849303695488 run.py:483] Algo bellman_ford step 8021 current loss 0.008686, current_train_items 256704.
I0304 19:31:58.776265 22849303695488 run.py:483] Algo bellman_ford step 8022 current loss 0.023097, current_train_items 256736.
I0304 19:31:58.807957 22849303695488 run.py:483] Algo bellman_ford step 8023 current loss 0.064712, current_train_items 256768.
I0304 19:31:58.843748 22849303695488 run.py:483] Algo bellman_ford step 8024 current loss 0.099060, current_train_items 256800.
I0304 19:31:58.863736 22849303695488 run.py:483] Algo bellman_ford step 8025 current loss 0.011562, current_train_items 256832.
I0304 19:31:58.880542 22849303695488 run.py:483] Algo bellman_ford step 8026 current loss 0.018265, current_train_items 256864.
I0304 19:31:58.905188 22849303695488 run.py:483] Algo bellman_ford step 8027 current loss 0.033582, current_train_items 256896.
I0304 19:31:58.936885 22849303695488 run.py:483] Algo bellman_ford step 8028 current loss 0.040934, current_train_items 256928.
I0304 19:31:58.971431 22849303695488 run.py:483] Algo bellman_ford step 8029 current loss 0.130137, current_train_items 256960.
I0304 19:31:58.991708 22849303695488 run.py:483] Algo bellman_ford step 8030 current loss 0.004376, current_train_items 256992.
I0304 19:31:59.008131 22849303695488 run.py:483] Algo bellman_ford step 8031 current loss 0.019517, current_train_items 257024.
I0304 19:31:59.033592 22849303695488 run.py:483] Algo bellman_ford step 8032 current loss 0.066427, current_train_items 257056.
I0304 19:31:59.066880 22849303695488 run.py:483] Algo bellman_ford step 8033 current loss 0.041510, current_train_items 257088.
I0304 19:31:59.102362 22849303695488 run.py:483] Algo bellman_ford step 8034 current loss 0.054256, current_train_items 257120.
I0304 19:31:59.122497 22849303695488 run.py:483] Algo bellman_ford step 8035 current loss 0.003995, current_train_items 257152.
I0304 19:31:59.139566 22849303695488 run.py:483] Algo bellman_ford step 8036 current loss 0.027417, current_train_items 257184.
I0304 19:31:59.164568 22849303695488 run.py:483] Algo bellman_ford step 8037 current loss 0.094313, current_train_items 257216.
I0304 19:31:59.196613 22849303695488 run.py:483] Algo bellman_ford step 8038 current loss 0.047870, current_train_items 257248.
I0304 19:31:59.232143 22849303695488 run.py:483] Algo bellman_ford step 8039 current loss 0.097223, current_train_items 257280.
I0304 19:31:59.252367 22849303695488 run.py:483] Algo bellman_ford step 8040 current loss 0.002359, current_train_items 257312.
I0304 19:31:59.269176 22849303695488 run.py:483] Algo bellman_ford step 8041 current loss 0.029381, current_train_items 257344.
I0304 19:31:59.293995 22849303695488 run.py:483] Algo bellman_ford step 8042 current loss 0.084434, current_train_items 257376.
I0304 19:31:59.326936 22849303695488 run.py:483] Algo bellman_ford step 8043 current loss 0.072003, current_train_items 257408.
I0304 19:31:59.361635 22849303695488 run.py:483] Algo bellman_ford step 8044 current loss 0.125654, current_train_items 257440.
I0304 19:31:59.382113 22849303695488 run.py:483] Algo bellman_ford step 8045 current loss 0.003813, current_train_items 257472.
I0304 19:31:59.399338 22849303695488 run.py:483] Algo bellman_ford step 8046 current loss 0.085840, current_train_items 257504.
I0304 19:31:59.423630 22849303695488 run.py:483] Algo bellman_ford step 8047 current loss 0.020909, current_train_items 257536.
I0304 19:31:59.456426 22849303695488 run.py:483] Algo bellman_ford step 8048 current loss 0.049415, current_train_items 257568.
I0304 19:31:59.489700 22849303695488 run.py:483] Algo bellman_ford step 8049 current loss 0.044096, current_train_items 257600.
I0304 19:31:59.509884 22849303695488 run.py:483] Algo bellman_ford step 8050 current loss 0.002284, current_train_items 257632.
I0304 19:31:59.518389 22849303695488 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0304 19:31:59.518498 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:31:59.535933 22849303695488 run.py:483] Algo bellman_ford step 8051 current loss 0.033208, current_train_items 257664.
I0304 19:31:59.560617 22849303695488 run.py:483] Algo bellman_ford step 8052 current loss 0.020531, current_train_items 257696.
I0304 19:31:59.593148 22849303695488 run.py:483] Algo bellman_ford step 8053 current loss 0.033878, current_train_items 257728.
I0304 19:31:59.628910 22849303695488 run.py:483] Algo bellman_ford step 8054 current loss 0.125463, current_train_items 257760.
I0304 19:31:59.649049 22849303695488 run.py:483] Algo bellman_ford step 8055 current loss 0.006262, current_train_items 257792.
I0304 19:31:59.664592 22849303695488 run.py:483] Algo bellman_ford step 8056 current loss 0.002744, current_train_items 257824.
I0304 19:31:59.688149 22849303695488 run.py:483] Algo bellman_ford step 8057 current loss 0.052244, current_train_items 257856.
I0304 19:31:59.721711 22849303695488 run.py:483] Algo bellman_ford step 8058 current loss 0.034602, current_train_items 257888.
I0304 19:31:59.757913 22849303695488 run.py:483] Algo bellman_ford step 8059 current loss 0.058182, current_train_items 257920.
I0304 19:31:59.778218 22849303695488 run.py:483] Algo bellman_ford step 8060 current loss 0.022052, current_train_items 257952.
I0304 19:31:59.794794 22849303695488 run.py:483] Algo bellman_ford step 8061 current loss 0.023265, current_train_items 257984.
I0304 19:31:59.820222 22849303695488 run.py:483] Algo bellman_ford step 8062 current loss 0.050866, current_train_items 258016.
I0304 19:31:59.852785 22849303695488 run.py:483] Algo bellman_ford step 8063 current loss 0.047884, current_train_items 258048.
I0304 19:31:59.885883 22849303695488 run.py:483] Algo bellman_ford step 8064 current loss 0.047259, current_train_items 258080.
I0304 19:31:59.905807 22849303695488 run.py:483] Algo bellman_ford step 8065 current loss 0.001506, current_train_items 258112.
I0304 19:31:59.922234 22849303695488 run.py:483] Algo bellman_ford step 8066 current loss 0.039488, current_train_items 258144.
I0304 19:31:59.946701 22849303695488 run.py:483] Algo bellman_ford step 8067 current loss 0.050709, current_train_items 258176.
I0304 19:31:59.979295 22849303695488 run.py:483] Algo bellman_ford step 8068 current loss 0.041751, current_train_items 258208.
I0304 19:32:00.012102 22849303695488 run.py:483] Algo bellman_ford step 8069 current loss 0.046584, current_train_items 258240.
I0304 19:32:00.032423 22849303695488 run.py:483] Algo bellman_ford step 8070 current loss 0.004718, current_train_items 258272.
I0304 19:32:00.049785 22849303695488 run.py:483] Algo bellman_ford step 8071 current loss 0.038925, current_train_items 258304.
I0304 19:32:00.074477 22849303695488 run.py:483] Algo bellman_ford step 8072 current loss 0.026032, current_train_items 258336.
I0304 19:32:00.106861 22849303695488 run.py:483] Algo bellman_ford step 8073 current loss 0.057930, current_train_items 258368.
I0304 19:32:00.142301 22849303695488 run.py:483] Algo bellman_ford step 8074 current loss 0.051075, current_train_items 258400.
I0304 19:32:00.162738 22849303695488 run.py:483] Algo bellman_ford step 8075 current loss 0.002517, current_train_items 258432.
I0304 19:32:00.179386 22849303695488 run.py:483] Algo bellman_ford step 8076 current loss 0.043366, current_train_items 258464.
I0304 19:32:00.203235 22849303695488 run.py:483] Algo bellman_ford step 8077 current loss 0.031536, current_train_items 258496.
I0304 19:32:00.235320 22849303695488 run.py:483] Algo bellman_ford step 8078 current loss 0.059619, current_train_items 258528.
I0304 19:32:00.267559 22849303695488 run.py:483] Algo bellman_ford step 8079 current loss 0.076395, current_train_items 258560.
I0304 19:32:00.287710 22849303695488 run.py:483] Algo bellman_ford step 8080 current loss 0.020227, current_train_items 258592.
I0304 19:32:00.304510 22849303695488 run.py:483] Algo bellman_ford step 8081 current loss 0.011568, current_train_items 258624.
I0304 19:32:00.329198 22849303695488 run.py:483] Algo bellman_ford step 8082 current loss 0.039192, current_train_items 258656.
I0304 19:32:00.361824 22849303695488 run.py:483] Algo bellman_ford step 8083 current loss 0.044452, current_train_items 258688.
I0304 19:32:00.399358 22849303695488 run.py:483] Algo bellman_ford step 8084 current loss 0.057463, current_train_items 258720.
I0304 19:32:00.419936 22849303695488 run.py:483] Algo bellman_ford step 8085 current loss 0.002199, current_train_items 258752.
I0304 19:32:00.436999 22849303695488 run.py:483] Algo bellman_ford step 8086 current loss 0.021138, current_train_items 258784.
I0304 19:32:00.460725 22849303695488 run.py:483] Algo bellman_ford step 8087 current loss 0.033361, current_train_items 258816.
I0304 19:32:00.494448 22849303695488 run.py:483] Algo bellman_ford step 8088 current loss 0.057798, current_train_items 258848.
I0304 19:32:00.527184 22849303695488 run.py:483] Algo bellman_ford step 8089 current loss 0.054400, current_train_items 258880.
I0304 19:32:00.547493 22849303695488 run.py:483] Algo bellman_ford step 8090 current loss 0.002705, current_train_items 258912.
I0304 19:32:00.563662 22849303695488 run.py:483] Algo bellman_ford step 8091 current loss 0.017709, current_train_items 258944.
I0304 19:32:00.588379 22849303695488 run.py:483] Algo bellman_ford step 8092 current loss 0.023682, current_train_items 258976.
I0304 19:32:00.620084 22849303695488 run.py:483] Algo bellman_ford step 8093 current loss 0.031414, current_train_items 259008.
I0304 19:32:00.654927 22849303695488 run.py:483] Algo bellman_ford step 8094 current loss 0.059364, current_train_items 259040.
I0304 19:32:00.675295 22849303695488 run.py:483] Algo bellman_ford step 8095 current loss 0.009692, current_train_items 259072.
I0304 19:32:00.691684 22849303695488 run.py:483] Algo bellman_ford step 8096 current loss 0.017222, current_train_items 259104.
I0304 19:32:00.717213 22849303695488 run.py:483] Algo bellman_ford step 8097 current loss 0.030789, current_train_items 259136.
I0304 19:32:00.750456 22849303695488 run.py:483] Algo bellman_ford step 8098 current loss 0.051816, current_train_items 259168.
I0304 19:32:00.784239 22849303695488 run.py:483] Algo bellman_ford step 8099 current loss 0.039847, current_train_items 259200.
I0304 19:32:00.804333 22849303695488 run.py:483] Algo bellman_ford step 8100 current loss 0.001946, current_train_items 259232.
I0304 19:32:00.812744 22849303695488 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0304 19:32:00.812852 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:00.830279 22849303695488 run.py:483] Algo bellman_ford step 8101 current loss 0.009084, current_train_items 259264.
I0304 19:32:00.856984 22849303695488 run.py:483] Algo bellman_ford step 8102 current loss 0.044654, current_train_items 259296.
I0304 19:32:00.889939 22849303695488 run.py:483] Algo bellman_ford step 8103 current loss 0.045797, current_train_items 259328.
I0304 19:32:00.924490 22849303695488 run.py:483] Algo bellman_ford step 8104 current loss 0.073322, current_train_items 259360.
I0304 19:32:00.944965 22849303695488 run.py:483] Algo bellman_ford step 8105 current loss 0.002394, current_train_items 259392.
I0304 19:32:00.961489 22849303695488 run.py:483] Algo bellman_ford step 8106 current loss 0.034076, current_train_items 259424.
I0304 19:32:00.985948 22849303695488 run.py:483] Algo bellman_ford step 8107 current loss 0.092711, current_train_items 259456.
I0304 19:32:01.017974 22849303695488 run.py:483] Algo bellman_ford step 8108 current loss 0.141004, current_train_items 259488.
I0304 19:32:01.053051 22849303695488 run.py:483] Algo bellman_ford step 8109 current loss 0.115304, current_train_items 259520.
I0304 19:32:01.073349 22849303695488 run.py:483] Algo bellman_ford step 8110 current loss 0.016097, current_train_items 259552.
I0304 19:32:01.089506 22849303695488 run.py:483] Algo bellman_ford step 8111 current loss 0.032522, current_train_items 259584.
I0304 19:32:01.113780 22849303695488 run.py:483] Algo bellman_ford step 8112 current loss 0.065554, current_train_items 259616.
I0304 19:32:01.144950 22849303695488 run.py:483] Algo bellman_ford step 8113 current loss 0.046736, current_train_items 259648.
I0304 19:32:01.181419 22849303695488 run.py:483] Algo bellman_ford step 8114 current loss 0.090908, current_train_items 259680.
I0304 19:32:01.201542 22849303695488 run.py:483] Algo bellman_ford step 8115 current loss 0.005558, current_train_items 259712.
I0304 19:32:01.218235 22849303695488 run.py:483] Algo bellman_ford step 8116 current loss 0.005740, current_train_items 259744.
I0304 19:32:01.243642 22849303695488 run.py:483] Algo bellman_ford step 8117 current loss 0.044624, current_train_items 259776.
I0304 19:32:01.276861 22849303695488 run.py:483] Algo bellman_ford step 8118 current loss 0.046868, current_train_items 259808.
I0304 19:32:01.310956 22849303695488 run.py:483] Algo bellman_ford step 8119 current loss 0.079227, current_train_items 259840.
I0304 19:32:01.331138 22849303695488 run.py:483] Algo bellman_ford step 8120 current loss 0.005274, current_train_items 259872.
I0304 19:32:01.347817 22849303695488 run.py:483] Algo bellman_ford step 8121 current loss 0.006546, current_train_items 259904.
I0304 19:32:01.372603 22849303695488 run.py:483] Algo bellman_ford step 8122 current loss 0.086300, current_train_items 259936.
I0304 19:32:01.406085 22849303695488 run.py:483] Algo bellman_ford step 8123 current loss 0.069774, current_train_items 259968.
I0304 19:32:01.439093 22849303695488 run.py:483] Algo bellman_ford step 8124 current loss 0.054969, current_train_items 260000.
I0304 19:32:01.459336 22849303695488 run.py:483] Algo bellman_ford step 8125 current loss 0.002708, current_train_items 260032.
I0304 19:32:01.476230 22849303695488 run.py:483] Algo bellman_ford step 8126 current loss 0.018395, current_train_items 260064.
I0304 19:32:01.501800 22849303695488 run.py:483] Algo bellman_ford step 8127 current loss 0.050187, current_train_items 260096.
I0304 19:32:01.535010 22849303695488 run.py:483] Algo bellman_ford step 8128 current loss 0.082965, current_train_items 260128.
I0304 19:32:01.571513 22849303695488 run.py:483] Algo bellman_ford step 8129 current loss 0.156510, current_train_items 260160.
I0304 19:32:01.592124 22849303695488 run.py:483] Algo bellman_ford step 8130 current loss 0.018789, current_train_items 260192.
I0304 19:32:01.608466 22849303695488 run.py:483] Algo bellman_ford step 8131 current loss 0.017808, current_train_items 260224.
I0304 19:32:01.633332 22849303695488 run.py:483] Algo bellman_ford step 8132 current loss 0.042854, current_train_items 260256.
I0304 19:32:01.666138 22849303695488 run.py:483] Algo bellman_ford step 8133 current loss 0.043384, current_train_items 260288.
I0304 19:32:01.700856 22849303695488 run.py:483] Algo bellman_ford step 8134 current loss 0.073826, current_train_items 260320.
I0304 19:32:01.721199 22849303695488 run.py:483] Algo bellman_ford step 8135 current loss 0.005130, current_train_items 260352.
I0304 19:32:01.737510 22849303695488 run.py:483] Algo bellman_ford step 8136 current loss 0.017179, current_train_items 260384.
I0304 19:32:01.761934 22849303695488 run.py:483] Algo bellman_ford step 8137 current loss 0.097349, current_train_items 260416.
I0304 19:32:01.794869 22849303695488 run.py:483] Algo bellman_ford step 8138 current loss 0.054091, current_train_items 260448.
I0304 19:32:01.831525 22849303695488 run.py:483] Algo bellman_ford step 8139 current loss 0.089104, current_train_items 260480.
I0304 19:32:01.851384 22849303695488 run.py:483] Algo bellman_ford step 8140 current loss 0.004991, current_train_items 260512.
I0304 19:32:01.867706 22849303695488 run.py:483] Algo bellman_ford step 8141 current loss 0.030806, current_train_items 260544.
I0304 19:32:01.893262 22849303695488 run.py:483] Algo bellman_ford step 8142 current loss 0.139166, current_train_items 260576.
I0304 19:32:01.926305 22849303695488 run.py:483] Algo bellman_ford step 8143 current loss 0.131996, current_train_items 260608.
I0304 19:32:01.961685 22849303695488 run.py:483] Algo bellman_ford step 8144 current loss 0.124541, current_train_items 260640.
I0304 19:32:01.981672 22849303695488 run.py:483] Algo bellman_ford step 8145 current loss 0.005089, current_train_items 260672.
I0304 19:32:01.998483 22849303695488 run.py:483] Algo bellman_ford step 8146 current loss 0.022362, current_train_items 260704.
I0304 19:32:02.022984 22849303695488 run.py:483] Algo bellman_ford step 8147 current loss 0.070878, current_train_items 260736.
I0304 19:32:02.055587 22849303695488 run.py:483] Algo bellman_ford step 8148 current loss 0.101263, current_train_items 260768.
I0304 19:32:02.089350 22849303695488 run.py:483] Algo bellman_ford step 8149 current loss 0.071228, current_train_items 260800.
I0304 19:32:02.109776 22849303695488 run.py:483] Algo bellman_ford step 8150 current loss 0.004527, current_train_items 260832.
I0304 19:32:02.118223 22849303695488 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0304 19:32:02.118333 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:02.135647 22849303695488 run.py:483] Algo bellman_ford step 8151 current loss 0.011880, current_train_items 260864.
I0304 19:32:02.161285 22849303695488 run.py:483] Algo bellman_ford step 8152 current loss 0.075631, current_train_items 260896.
I0304 19:32:02.193073 22849303695488 run.py:483] Algo bellman_ford step 8153 current loss 0.017468, current_train_items 260928.
I0304 19:32:02.229053 22849303695488 run.py:483] Algo bellman_ford step 8154 current loss 0.091771, current_train_items 260960.
I0304 19:32:02.249612 22849303695488 run.py:483] Algo bellman_ford step 8155 current loss 0.004961, current_train_items 260992.
I0304 19:32:02.265550 22849303695488 run.py:483] Algo bellman_ford step 8156 current loss 0.034126, current_train_items 261024.
I0304 19:32:02.290821 22849303695488 run.py:483] Algo bellman_ford step 8157 current loss 0.022451, current_train_items 261056.
I0304 19:32:02.323274 22849303695488 run.py:483] Algo bellman_ford step 8158 current loss 0.053903, current_train_items 261088.
I0304 19:32:02.357203 22849303695488 run.py:483] Algo bellman_ford step 8159 current loss 0.070574, current_train_items 261120.
I0304 19:32:02.377670 22849303695488 run.py:483] Algo bellman_ford step 8160 current loss 0.003563, current_train_items 261152.
I0304 19:32:02.394583 22849303695488 run.py:483] Algo bellman_ford step 8161 current loss 0.017323, current_train_items 261184.
I0304 19:32:02.418771 22849303695488 run.py:483] Algo bellman_ford step 8162 current loss 0.060737, current_train_items 261216.
I0304 19:32:02.451418 22849303695488 run.py:483] Algo bellman_ford step 8163 current loss 0.047467, current_train_items 261248.
I0304 19:32:02.482829 22849303695488 run.py:483] Algo bellman_ford step 8164 current loss 0.023614, current_train_items 261280.
I0304 19:32:02.502696 22849303695488 run.py:483] Algo bellman_ford step 8165 current loss 0.001715, current_train_items 261312.
I0304 19:32:02.519299 22849303695488 run.py:483] Algo bellman_ford step 8166 current loss 0.023646, current_train_items 261344.
I0304 19:32:02.543792 22849303695488 run.py:483] Algo bellman_ford step 8167 current loss 0.063125, current_train_items 261376.
I0304 19:32:02.575669 22849303695488 run.py:483] Algo bellman_ford step 8168 current loss 0.037944, current_train_items 261408.
I0304 19:32:02.611499 22849303695488 run.py:483] Algo bellman_ford step 8169 current loss 0.072353, current_train_items 261440.
I0304 19:32:02.632016 22849303695488 run.py:483] Algo bellman_ford step 8170 current loss 0.011479, current_train_items 261472.
I0304 19:32:02.648734 22849303695488 run.py:483] Algo bellman_ford step 8171 current loss 0.009392, current_train_items 261504.
I0304 19:32:02.673481 22849303695488 run.py:483] Algo bellman_ford step 8172 current loss 0.036795, current_train_items 261536.
I0304 19:32:02.706579 22849303695488 run.py:483] Algo bellman_ford step 8173 current loss 0.034335, current_train_items 261568.
I0304 19:32:02.740209 22849303695488 run.py:483] Algo bellman_ford step 8174 current loss 0.056536, current_train_items 261600.
I0304 19:32:02.760679 22849303695488 run.py:483] Algo bellman_ford step 8175 current loss 0.004243, current_train_items 261632.
I0304 19:32:02.777716 22849303695488 run.py:483] Algo bellman_ford step 8176 current loss 0.033031, current_train_items 261664.
I0304 19:32:02.802246 22849303695488 run.py:483] Algo bellman_ford step 8177 current loss 0.022625, current_train_items 261696.
I0304 19:32:02.835610 22849303695488 run.py:483] Algo bellman_ford step 8178 current loss 0.031865, current_train_items 261728.
I0304 19:32:02.867471 22849303695488 run.py:483] Algo bellman_ford step 8179 current loss 0.038880, current_train_items 261760.
I0304 19:32:02.887558 22849303695488 run.py:483] Algo bellman_ford step 8180 current loss 0.001510, current_train_items 261792.
I0304 19:32:02.904061 22849303695488 run.py:483] Algo bellman_ford step 8181 current loss 0.018646, current_train_items 261824.
I0304 19:32:02.928401 22849303695488 run.py:483] Algo bellman_ford step 8182 current loss 0.025701, current_train_items 261856.
I0304 19:32:02.961523 22849303695488 run.py:483] Algo bellman_ford step 8183 current loss 0.055523, current_train_items 261888.
I0304 19:32:02.998384 22849303695488 run.py:483] Algo bellman_ford step 8184 current loss 0.072119, current_train_items 261920.
I0304 19:32:03.018901 22849303695488 run.py:483] Algo bellman_ford step 8185 current loss 0.004806, current_train_items 261952.
I0304 19:32:03.035621 22849303695488 run.py:483] Algo bellman_ford step 8186 current loss 0.010754, current_train_items 261984.
I0304 19:32:03.060249 22849303695488 run.py:483] Algo bellman_ford step 8187 current loss 0.112719, current_train_items 262016.
I0304 19:32:03.092982 22849303695488 run.py:483] Algo bellman_ford step 8188 current loss 0.189707, current_train_items 262048.
I0304 19:32:03.126855 22849303695488 run.py:483] Algo bellman_ford step 8189 current loss 0.091706, current_train_items 262080.
I0304 19:32:03.147662 22849303695488 run.py:483] Algo bellman_ford step 8190 current loss 0.011767, current_train_items 262112.
I0304 19:32:03.164488 22849303695488 run.py:483] Algo bellman_ford step 8191 current loss 0.017978, current_train_items 262144.
I0304 19:32:03.189107 22849303695488 run.py:483] Algo bellman_ford step 8192 current loss 0.027286, current_train_items 262176.
I0304 19:32:03.221080 22849303695488 run.py:483] Algo bellman_ford step 8193 current loss 0.039651, current_train_items 262208.
I0304 19:32:03.255465 22849303695488 run.py:483] Algo bellman_ford step 8194 current loss 0.063548, current_train_items 262240.
I0304 19:32:03.275263 22849303695488 run.py:483] Algo bellman_ford step 8195 current loss 0.002996, current_train_items 262272.
I0304 19:32:03.291897 22849303695488 run.py:483] Algo bellman_ford step 8196 current loss 0.019597, current_train_items 262304.
I0304 19:32:03.315860 22849303695488 run.py:483] Algo bellman_ford step 8197 current loss 0.044581, current_train_items 262336.
I0304 19:32:03.348236 22849303695488 run.py:483] Algo bellman_ford step 8198 current loss 0.025961, current_train_items 262368.
I0304 19:32:03.382481 22849303695488 run.py:483] Algo bellman_ford step 8199 current loss 0.039779, current_train_items 262400.
I0304 19:32:03.402965 22849303695488 run.py:483] Algo bellman_ford step 8200 current loss 0.002542, current_train_items 262432.
I0304 19:32:03.410533 22849303695488 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0304 19:32:03.410640 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:32:03.427653 22849303695488 run.py:483] Algo bellman_ford step 8201 current loss 0.007288, current_train_items 262464.
I0304 19:32:03.452846 22849303695488 run.py:483] Algo bellman_ford step 8202 current loss 0.039057, current_train_items 262496.
I0304 19:32:03.485247 22849303695488 run.py:483] Algo bellman_ford step 8203 current loss 0.030510, current_train_items 262528.
I0304 19:32:03.520611 22849303695488 run.py:483] Algo bellman_ford step 8204 current loss 0.053377, current_train_items 262560.
I0304 19:32:03.540590 22849303695488 run.py:483] Algo bellman_ford step 8205 current loss 0.001422, current_train_items 262592.
I0304 19:32:03.557246 22849303695488 run.py:483] Algo bellman_ford step 8206 current loss 0.007651, current_train_items 262624.
I0304 19:32:03.582572 22849303695488 run.py:483] Algo bellman_ford step 8207 current loss 0.066649, current_train_items 262656.
I0304 19:32:03.614850 22849303695488 run.py:483] Algo bellman_ford step 8208 current loss 0.126861, current_train_items 262688.
I0304 19:32:03.646241 22849303695488 run.py:483] Algo bellman_ford step 8209 current loss 0.095424, current_train_items 262720.
I0304 19:32:03.666180 22849303695488 run.py:483] Algo bellman_ford step 8210 current loss 0.002335, current_train_items 262752.
I0304 19:32:03.682588 22849303695488 run.py:483] Algo bellman_ford step 8211 current loss 0.038776, current_train_items 262784.
I0304 19:32:03.706644 22849303695488 run.py:483] Algo bellman_ford step 8212 current loss 0.041718, current_train_items 262816.
I0304 19:32:03.738699 22849303695488 run.py:483] Algo bellman_ford step 8213 current loss 0.051885, current_train_items 262848.
I0304 19:32:03.772642 22849303695488 run.py:483] Algo bellman_ford step 8214 current loss 0.097483, current_train_items 262880.
I0304 19:32:03.792493 22849303695488 run.py:483] Algo bellman_ford step 8215 current loss 0.002115, current_train_items 262912.
I0304 19:32:03.808414 22849303695488 run.py:483] Algo bellman_ford step 8216 current loss 0.006053, current_train_items 262944.
I0304 19:32:03.832142 22849303695488 run.py:483] Algo bellman_ford step 8217 current loss 0.036596, current_train_items 262976.
I0304 19:32:03.865632 22849303695488 run.py:483] Algo bellman_ford step 8218 current loss 0.040164, current_train_items 263008.
I0304 19:32:03.900486 22849303695488 run.py:483] Algo bellman_ford step 8219 current loss 0.038025, current_train_items 263040.
I0304 19:32:03.920102 22849303695488 run.py:483] Algo bellman_ford step 8220 current loss 0.001925, current_train_items 263072.
I0304 19:32:03.936398 22849303695488 run.py:483] Algo bellman_ford step 8221 current loss 0.037030, current_train_items 263104.
I0304 19:32:03.961561 22849303695488 run.py:483] Algo bellman_ford step 8222 current loss 0.059890, current_train_items 263136.
I0304 19:32:03.994213 22849303695488 run.py:483] Algo bellman_ford step 8223 current loss 0.069694, current_train_items 263168.
I0304 19:32:04.026074 22849303695488 run.py:483] Algo bellman_ford step 8224 current loss 0.049578, current_train_items 263200.
I0304 19:32:04.046200 22849303695488 run.py:483] Algo bellman_ford step 8225 current loss 0.012511, current_train_items 263232.
I0304 19:32:04.062915 22849303695488 run.py:483] Algo bellman_ford step 8226 current loss 0.011133, current_train_items 263264.
I0304 19:32:04.088288 22849303695488 run.py:483] Algo bellman_ford step 8227 current loss 0.012482, current_train_items 263296.
I0304 19:32:04.119570 22849303695488 run.py:483] Algo bellman_ford step 8228 current loss 0.063167, current_train_items 263328.
I0304 19:32:04.153724 22849303695488 run.py:483] Algo bellman_ford step 8229 current loss 0.062069, current_train_items 263360.
I0304 19:32:04.173635 22849303695488 run.py:483] Algo bellman_ford step 8230 current loss 0.002355, current_train_items 263392.
I0304 19:32:04.190152 22849303695488 run.py:483] Algo bellman_ford step 8231 current loss 0.017811, current_train_items 263424.
I0304 19:32:04.215304 22849303695488 run.py:483] Algo bellman_ford step 8232 current loss 0.038773, current_train_items 263456.
I0304 19:32:04.247352 22849303695488 run.py:483] Algo bellman_ford step 8233 current loss 0.047207, current_train_items 263488.
I0304 19:32:04.280808 22849303695488 run.py:483] Algo bellman_ford step 8234 current loss 0.068394, current_train_items 263520.
I0304 19:32:04.300503 22849303695488 run.py:483] Algo bellman_ford step 8235 current loss 0.007058, current_train_items 263552.
I0304 19:32:04.317709 22849303695488 run.py:483] Algo bellman_ford step 8236 current loss 0.017938, current_train_items 263584.
I0304 19:32:04.342813 22849303695488 run.py:483] Algo bellman_ford step 8237 current loss 0.032220, current_train_items 263616.
I0304 19:32:04.375521 22849303695488 run.py:483] Algo bellman_ford step 8238 current loss 0.033640, current_train_items 263648.
I0304 19:32:04.408954 22849303695488 run.py:483] Algo bellman_ford step 8239 current loss 0.038058, current_train_items 263680.
I0304 19:32:04.428944 22849303695488 run.py:483] Algo bellman_ford step 8240 current loss 0.003078, current_train_items 263712.
I0304 19:32:04.445453 22849303695488 run.py:483] Algo bellman_ford step 8241 current loss 0.008023, current_train_items 263744.
I0304 19:32:04.469535 22849303695488 run.py:483] Algo bellman_ford step 8242 current loss 0.029110, current_train_items 263776.
I0304 19:32:04.501559 22849303695488 run.py:483] Algo bellman_ford step 8243 current loss 0.032728, current_train_items 263808.
I0304 19:32:04.537594 22849303695488 run.py:483] Algo bellman_ford step 8244 current loss 0.061985, current_train_items 263840.
I0304 19:32:04.557564 22849303695488 run.py:483] Algo bellman_ford step 8245 current loss 0.018610, current_train_items 263872.
I0304 19:32:04.574054 22849303695488 run.py:483] Algo bellman_ford step 8246 current loss 0.022609, current_train_items 263904.
I0304 19:32:04.597646 22849303695488 run.py:483] Algo bellman_ford step 8247 current loss 0.034622, current_train_items 263936.
I0304 19:32:04.629623 22849303695488 run.py:483] Algo bellman_ford step 8248 current loss 0.041093, current_train_items 263968.
I0304 19:32:04.663765 22849303695488 run.py:483] Algo bellman_ford step 8249 current loss 0.044187, current_train_items 264000.
I0304 19:32:04.683640 22849303695488 run.py:483] Algo bellman_ford step 8250 current loss 0.001272, current_train_items 264032.
I0304 19:32:04.691983 22849303695488 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0304 19:32:04.692101 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:04.709268 22849303695488 run.py:483] Algo bellman_ford step 8251 current loss 0.014018, current_train_items 264064.
I0304 19:32:04.734694 22849303695488 run.py:483] Algo bellman_ford step 8252 current loss 0.044713, current_train_items 264096.
I0304 19:32:04.768581 22849303695488 run.py:483] Algo bellman_ford step 8253 current loss 0.044885, current_train_items 264128.
I0304 19:32:04.802607 22849303695488 run.py:483] Algo bellman_ford step 8254 current loss 0.049474, current_train_items 264160.
I0304 19:32:04.823061 22849303695488 run.py:483] Algo bellman_ford step 8255 current loss 0.017058, current_train_items 264192.
I0304 19:32:04.840016 22849303695488 run.py:483] Algo bellman_ford step 8256 current loss 0.029901, current_train_items 264224.
I0304 19:32:04.865063 22849303695488 run.py:483] Algo bellman_ford step 8257 current loss 0.034271, current_train_items 264256.
I0304 19:32:04.898301 22849303695488 run.py:483] Algo bellman_ford step 8258 current loss 0.052989, current_train_items 264288.
I0304 19:32:04.933892 22849303695488 run.py:483] Algo bellman_ford step 8259 current loss 0.078186, current_train_items 264320.
I0304 19:32:04.954479 22849303695488 run.py:483] Algo bellman_ford step 8260 current loss 0.003979, current_train_items 264352.
I0304 19:32:04.971329 22849303695488 run.py:483] Algo bellman_ford step 8261 current loss 0.005500, current_train_items 264384.
I0304 19:32:04.996612 22849303695488 run.py:483] Algo bellman_ford step 8262 current loss 0.048447, current_train_items 264416.
I0304 19:32:05.029583 22849303695488 run.py:483] Algo bellman_ford step 8263 current loss 0.057455, current_train_items 264448.
I0304 19:32:05.065824 22849303695488 run.py:483] Algo bellman_ford step 8264 current loss 0.047346, current_train_items 264480.
I0304 19:32:05.085691 22849303695488 run.py:483] Algo bellman_ford step 8265 current loss 0.006750, current_train_items 264512.
I0304 19:32:05.101957 22849303695488 run.py:483] Algo bellman_ford step 8266 current loss 0.025258, current_train_items 264544.
I0304 19:32:05.126795 22849303695488 run.py:483] Algo bellman_ford step 8267 current loss 0.059954, current_train_items 264576.
I0304 19:32:05.160126 22849303695488 run.py:483] Algo bellman_ford step 8268 current loss 0.045126, current_train_items 264608.
I0304 19:32:05.192957 22849303695488 run.py:483] Algo bellman_ford step 8269 current loss 0.073617, current_train_items 264640.
I0304 19:32:05.213629 22849303695488 run.py:483] Algo bellman_ford step 8270 current loss 0.006497, current_train_items 264672.
I0304 19:32:05.229714 22849303695488 run.py:483] Algo bellman_ford step 8271 current loss 0.009017, current_train_items 264704.
I0304 19:32:05.253974 22849303695488 run.py:483] Algo bellman_ford step 8272 current loss 0.035125, current_train_items 264736.
I0304 19:32:05.285486 22849303695488 run.py:483] Algo bellman_ford step 8273 current loss 0.026600, current_train_items 264768.
I0304 19:32:05.321289 22849303695488 run.py:483] Algo bellman_ford step 8274 current loss 0.076572, current_train_items 264800.
I0304 19:32:05.341760 22849303695488 run.py:483] Algo bellman_ford step 8275 current loss 0.002011, current_train_items 264832.
I0304 19:32:05.358954 22849303695488 run.py:483] Algo bellman_ford step 8276 current loss 0.026111, current_train_items 264864.
I0304 19:32:05.383986 22849303695488 run.py:483] Algo bellman_ford step 8277 current loss 0.024450, current_train_items 264896.
I0304 19:32:05.416278 22849303695488 run.py:483] Algo bellman_ford step 8278 current loss 0.075325, current_train_items 264928.
I0304 19:32:05.450170 22849303695488 run.py:483] Algo bellman_ford step 8279 current loss 0.055948, current_train_items 264960.
I0304 19:32:05.470161 22849303695488 run.py:483] Algo bellman_ford step 8280 current loss 0.007375, current_train_items 264992.
I0304 19:32:05.487121 22849303695488 run.py:483] Algo bellman_ford step 8281 current loss 0.013858, current_train_items 265024.
I0304 19:32:05.511372 22849303695488 run.py:483] Algo bellman_ford step 8282 current loss 0.015650, current_train_items 265056.
I0304 19:32:05.543284 22849303695488 run.py:483] Algo bellman_ford step 8283 current loss 0.040782, current_train_items 265088.
I0304 19:32:05.576761 22849303695488 run.py:483] Algo bellman_ford step 8284 current loss 0.068609, current_train_items 265120.
I0304 19:32:05.597189 22849303695488 run.py:483] Algo bellman_ford step 8285 current loss 0.002884, current_train_items 265152.
I0304 19:32:05.614149 22849303695488 run.py:483] Algo bellman_ford step 8286 current loss 0.024224, current_train_items 265184.
I0304 19:32:05.638792 22849303695488 run.py:483] Algo bellman_ford step 8287 current loss 0.027508, current_train_items 265216.
I0304 19:32:05.672398 22849303695488 run.py:483] Algo bellman_ford step 8288 current loss 0.059018, current_train_items 265248.
I0304 19:32:05.707348 22849303695488 run.py:483] Algo bellman_ford step 8289 current loss 0.052792, current_train_items 265280.
I0304 19:32:05.727898 22849303695488 run.py:483] Algo bellman_ford step 8290 current loss 0.001727, current_train_items 265312.
I0304 19:32:05.744676 22849303695488 run.py:483] Algo bellman_ford step 8291 current loss 0.018950, current_train_items 265344.
I0304 19:32:05.769834 22849303695488 run.py:483] Algo bellman_ford step 8292 current loss 0.043708, current_train_items 265376.
I0304 19:32:05.801135 22849303695488 run.py:483] Algo bellman_ford step 8293 current loss 0.061630, current_train_items 265408.
I0304 19:32:05.837029 22849303695488 run.py:483] Algo bellman_ford step 8294 current loss 0.054651, current_train_items 265440.
I0304 19:32:05.857332 22849303695488 run.py:483] Algo bellman_ford step 8295 current loss 0.021725, current_train_items 265472.
I0304 19:32:05.874096 22849303695488 run.py:483] Algo bellman_ford step 8296 current loss 0.032139, current_train_items 265504.
I0304 19:32:05.900111 22849303695488 run.py:483] Algo bellman_ford step 8297 current loss 0.058452, current_train_items 265536.
I0304 19:32:05.932911 22849303695488 run.py:483] Algo bellman_ford step 8298 current loss 0.059899, current_train_items 265568.
I0304 19:32:05.966798 22849303695488 run.py:483] Algo bellman_ford step 8299 current loss 0.061039, current_train_items 265600.
I0304 19:32:05.987224 22849303695488 run.py:483] Algo bellman_ford step 8300 current loss 0.007627, current_train_items 265632.
I0304 19:32:05.995121 22849303695488 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0304 19:32:05.995230 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:32:06.012194 22849303695488 run.py:483] Algo bellman_ford step 8301 current loss 0.007164, current_train_items 265664.
I0304 19:32:06.039372 22849303695488 run.py:483] Algo bellman_ford step 8302 current loss 0.064194, current_train_items 265696.
I0304 19:32:06.073092 22849303695488 run.py:483] Algo bellman_ford step 8303 current loss 0.124662, current_train_items 265728.
I0304 19:32:06.107199 22849303695488 run.py:483] Algo bellman_ford step 8304 current loss 0.118949, current_train_items 265760.
I0304 19:32:06.127641 22849303695488 run.py:483] Algo bellman_ford step 8305 current loss 0.010658, current_train_items 265792.
I0304 19:32:06.144152 22849303695488 run.py:483] Algo bellman_ford step 8306 current loss 0.028499, current_train_items 265824.
I0304 19:32:06.168756 22849303695488 run.py:483] Algo bellman_ford step 8307 current loss 0.072064, current_train_items 265856.
I0304 19:32:06.199038 22849303695488 run.py:483] Algo bellman_ford step 8308 current loss 0.048127, current_train_items 265888.
I0304 19:32:06.235080 22849303695488 run.py:483] Algo bellman_ford step 8309 current loss 0.166249, current_train_items 265920.
I0304 19:32:06.255422 22849303695488 run.py:483] Algo bellman_ford step 8310 current loss 0.005157, current_train_items 265952.
I0304 19:32:06.272102 22849303695488 run.py:483] Algo bellman_ford step 8311 current loss 0.010268, current_train_items 265984.
I0304 19:32:06.297068 22849303695488 run.py:483] Algo bellman_ford step 8312 current loss 0.055710, current_train_items 266016.
I0304 19:32:06.328782 22849303695488 run.py:483] Algo bellman_ford step 8313 current loss 0.032509, current_train_items 266048.
I0304 19:32:06.361671 22849303695488 run.py:483] Algo bellman_ford step 8314 current loss 0.061536, current_train_items 266080.
I0304 19:32:06.381864 22849303695488 run.py:483] Algo bellman_ford step 8315 current loss 0.004712, current_train_items 266112.
I0304 19:32:06.398859 22849303695488 run.py:483] Algo bellman_ford step 8316 current loss 0.019582, current_train_items 266144.
I0304 19:32:06.423318 22849303695488 run.py:483] Algo bellman_ford step 8317 current loss 0.037704, current_train_items 266176.
I0304 19:32:06.454031 22849303695488 run.py:483] Algo bellman_ford step 8318 current loss 0.021587, current_train_items 266208.
I0304 19:32:06.488480 22849303695488 run.py:483] Algo bellman_ford step 8319 current loss 0.067924, current_train_items 266240.
I0304 19:32:06.508310 22849303695488 run.py:483] Algo bellman_ford step 8320 current loss 0.006824, current_train_items 266272.
I0304 19:32:06.524763 22849303695488 run.py:483] Algo bellman_ford step 8321 current loss 0.012009, current_train_items 266304.
I0304 19:32:06.550277 22849303695488 run.py:483] Algo bellman_ford step 8322 current loss 0.050905, current_train_items 266336.
I0304 19:32:06.582323 22849303695488 run.py:483] Algo bellman_ford step 8323 current loss 0.040876, current_train_items 266368.
I0304 19:32:06.618319 22849303695488 run.py:483] Algo bellman_ford step 8324 current loss 0.052002, current_train_items 266400.
I0304 19:32:06.638458 22849303695488 run.py:483] Algo bellman_ford step 8325 current loss 0.001922, current_train_items 266432.
I0304 19:32:06.654518 22849303695488 run.py:483] Algo bellman_ford step 8326 current loss 0.006704, current_train_items 266464.
I0304 19:32:06.679543 22849303695488 run.py:483] Algo bellman_ford step 8327 current loss 0.023003, current_train_items 266496.
I0304 19:32:06.711339 22849303695488 run.py:483] Algo bellman_ford step 8328 current loss 0.066457, current_train_items 266528.
I0304 19:32:06.746497 22849303695488 run.py:483] Algo bellman_ford step 8329 current loss 0.047316, current_train_items 266560.
I0304 19:32:06.766689 22849303695488 run.py:483] Algo bellman_ford step 8330 current loss 0.003300, current_train_items 266592.
I0304 19:32:06.783106 22849303695488 run.py:483] Algo bellman_ford step 8331 current loss 0.053552, current_train_items 266624.
I0304 19:32:06.807307 22849303695488 run.py:483] Algo bellman_ford step 8332 current loss 0.012846, current_train_items 266656.
I0304 19:32:06.840565 22849303695488 run.py:483] Algo bellman_ford step 8333 current loss 0.117628, current_train_items 266688.
I0304 19:32:06.874403 22849303695488 run.py:483] Algo bellman_ford step 8334 current loss 0.070868, current_train_items 266720.
I0304 19:32:06.894753 22849303695488 run.py:483] Algo bellman_ford step 8335 current loss 0.004453, current_train_items 266752.
I0304 19:32:06.911321 22849303695488 run.py:483] Algo bellman_ford step 8336 current loss 0.021064, current_train_items 266784.
I0304 19:32:06.937288 22849303695488 run.py:483] Algo bellman_ford step 8337 current loss 0.022919, current_train_items 266816.
I0304 19:32:06.969339 22849303695488 run.py:483] Algo bellman_ford step 8338 current loss 0.055926, current_train_items 266848.
I0304 19:32:07.002739 22849303695488 run.py:483] Algo bellman_ford step 8339 current loss 0.060240, current_train_items 266880.
I0304 19:32:07.023209 22849303695488 run.py:483] Algo bellman_ford step 8340 current loss 0.002660, current_train_items 266912.
I0304 19:32:07.039641 22849303695488 run.py:483] Algo bellman_ford step 8341 current loss 0.023179, current_train_items 266944.
I0304 19:32:07.063840 22849303695488 run.py:483] Algo bellman_ford step 8342 current loss 0.033065, current_train_items 266976.
I0304 19:32:07.095690 22849303695488 run.py:483] Algo bellman_ford step 8343 current loss 0.045838, current_train_items 267008.
I0304 19:32:07.131191 22849303695488 run.py:483] Algo bellman_ford step 8344 current loss 0.059537, current_train_items 267040.
I0304 19:32:07.151382 22849303695488 run.py:483] Algo bellman_ford step 8345 current loss 0.002268, current_train_items 267072.
I0304 19:32:07.167726 22849303695488 run.py:483] Algo bellman_ford step 8346 current loss 0.014443, current_train_items 267104.
I0304 19:32:07.193619 22849303695488 run.py:483] Algo bellman_ford step 8347 current loss 0.049748, current_train_items 267136.
I0304 19:32:07.226503 22849303695488 run.py:483] Algo bellman_ford step 8348 current loss 0.082443, current_train_items 267168.
I0304 19:32:07.261789 22849303695488 run.py:483] Algo bellman_ford step 8349 current loss 0.129254, current_train_items 267200.
I0304 19:32:07.281932 22849303695488 run.py:483] Algo bellman_ford step 8350 current loss 0.002997, current_train_items 267232.
I0304 19:32:07.290459 22849303695488 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0304 19:32:07.290598 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:32:07.307843 22849303695488 run.py:483] Algo bellman_ford step 8351 current loss 0.021391, current_train_items 267264.
I0304 19:32:07.333370 22849303695488 run.py:483] Algo bellman_ford step 8352 current loss 0.050006, current_train_items 267296.
I0304 19:32:07.366301 22849303695488 run.py:483] Algo bellman_ford step 8353 current loss 0.040957, current_train_items 267328.
I0304 19:32:07.403222 22849303695488 run.py:483] Algo bellman_ford step 8354 current loss 0.072964, current_train_items 267360.
I0304 19:32:07.423841 22849303695488 run.py:483] Algo bellman_ford step 8355 current loss 0.006958, current_train_items 267392.
I0304 19:32:07.440215 22849303695488 run.py:483] Algo bellman_ford step 8356 current loss 0.043819, current_train_items 267424.
I0304 19:32:07.465089 22849303695488 run.py:483] Algo bellman_ford step 8357 current loss 0.044689, current_train_items 267456.
I0304 19:32:07.497918 22849303695488 run.py:483] Algo bellman_ford step 8358 current loss 0.038390, current_train_items 267488.
I0304 19:32:07.531490 22849303695488 run.py:483] Algo bellman_ford step 8359 current loss 0.069857, current_train_items 267520.
I0304 19:32:07.551929 22849303695488 run.py:483] Algo bellman_ford step 8360 current loss 0.002570, current_train_items 267552.
I0304 19:32:07.568375 22849303695488 run.py:483] Algo bellman_ford step 8361 current loss 0.006028, current_train_items 267584.
I0304 19:32:07.593040 22849303695488 run.py:483] Algo bellman_ford step 8362 current loss 0.027771, current_train_items 267616.
I0304 19:32:07.624938 22849303695488 run.py:483] Algo bellman_ford step 8363 current loss 0.044066, current_train_items 267648.
I0304 19:32:07.659892 22849303695488 run.py:483] Algo bellman_ford step 8364 current loss 0.052217, current_train_items 267680.
I0304 19:32:07.680337 22849303695488 run.py:483] Algo bellman_ford step 8365 current loss 0.002623, current_train_items 267712.
I0304 19:32:07.697322 22849303695488 run.py:483] Algo bellman_ford step 8366 current loss 0.014906, current_train_items 267744.
I0304 19:32:07.721373 22849303695488 run.py:483] Algo bellman_ford step 8367 current loss 0.027560, current_train_items 267776.
I0304 19:32:07.754097 22849303695488 run.py:483] Algo bellman_ford step 8368 current loss 0.054456, current_train_items 267808.
I0304 19:32:07.791077 22849303695488 run.py:483] Algo bellman_ford step 8369 current loss 0.067262, current_train_items 267840.
I0304 19:32:07.811544 22849303695488 run.py:483] Algo bellman_ford step 8370 current loss 0.002416, current_train_items 267872.
I0304 19:32:07.828518 22849303695488 run.py:483] Algo bellman_ford step 8371 current loss 0.034107, current_train_items 267904.
I0304 19:32:07.852508 22849303695488 run.py:483] Algo bellman_ford step 8372 current loss 0.116266, current_train_items 267936.
I0304 19:32:07.885057 22849303695488 run.py:483] Algo bellman_ford step 8373 current loss 0.110214, current_train_items 267968.
I0304 19:32:07.920338 22849303695488 run.py:483] Algo bellman_ford step 8374 current loss 0.055616, current_train_items 268000.
I0304 19:32:07.941351 22849303695488 run.py:483] Algo bellman_ford step 8375 current loss 0.002550, current_train_items 268032.
I0304 19:32:07.957767 22849303695488 run.py:483] Algo bellman_ford step 8376 current loss 0.009291, current_train_items 268064.
I0304 19:32:07.981650 22849303695488 run.py:483] Algo bellman_ford step 8377 current loss 0.046269, current_train_items 268096.
I0304 19:32:08.014440 22849303695488 run.py:483] Algo bellman_ford step 8378 current loss 0.045368, current_train_items 268128.
I0304 19:32:08.049189 22849303695488 run.py:483] Algo bellman_ford step 8379 current loss 0.076415, current_train_items 268160.
I0304 19:32:08.069562 22849303695488 run.py:483] Algo bellman_ford step 8380 current loss 0.017109, current_train_items 268192.
I0304 19:32:08.086294 22849303695488 run.py:483] Algo bellman_ford step 8381 current loss 0.006652, current_train_items 268224.
I0304 19:32:08.110204 22849303695488 run.py:483] Algo bellman_ford step 8382 current loss 0.039513, current_train_items 268256.
I0304 19:32:08.143229 22849303695488 run.py:483] Algo bellman_ford step 8383 current loss 0.061801, current_train_items 268288.
I0304 19:32:08.176572 22849303695488 run.py:483] Algo bellman_ford step 8384 current loss 0.048559, current_train_items 268320.
I0304 19:32:08.196886 22849303695488 run.py:483] Algo bellman_ford step 8385 current loss 0.008063, current_train_items 268352.
I0304 19:32:08.213319 22849303695488 run.py:483] Algo bellman_ford step 8386 current loss 0.011108, current_train_items 268384.
I0304 19:32:08.237414 22849303695488 run.py:483] Algo bellman_ford step 8387 current loss 0.073771, current_train_items 268416.
I0304 19:32:08.270042 22849303695488 run.py:483] Algo bellman_ford step 8388 current loss 0.038359, current_train_items 268448.
I0304 19:32:08.304159 22849303695488 run.py:483] Algo bellman_ford step 8389 current loss 0.063446, current_train_items 268480.
I0304 19:32:08.325127 22849303695488 run.py:483] Algo bellman_ford step 8390 current loss 0.002601, current_train_items 268512.
I0304 19:32:08.341947 22849303695488 run.py:483] Algo bellman_ford step 8391 current loss 0.004181, current_train_items 268544.
I0304 19:32:08.365489 22849303695488 run.py:483] Algo bellman_ford step 8392 current loss 0.049071, current_train_items 268576.
I0304 19:32:08.399300 22849303695488 run.py:483] Algo bellman_ford step 8393 current loss 0.099113, current_train_items 268608.
I0304 19:32:08.432500 22849303695488 run.py:483] Algo bellman_ford step 8394 current loss 0.053926, current_train_items 268640.
I0304 19:32:08.452979 22849303695488 run.py:483] Algo bellman_ford step 8395 current loss 0.004730, current_train_items 268672.
I0304 19:32:08.469702 22849303695488 run.py:483] Algo bellman_ford step 8396 current loss 0.027041, current_train_items 268704.
I0304 19:32:08.494142 22849303695488 run.py:483] Algo bellman_ford step 8397 current loss 0.051066, current_train_items 268736.
I0304 19:32:08.526997 22849303695488 run.py:483] Algo bellman_ford step 8398 current loss 0.087033, current_train_items 268768.
I0304 19:32:08.562376 22849303695488 run.py:483] Algo bellman_ford step 8399 current loss 0.087138, current_train_items 268800.
I0304 19:32:08.582795 22849303695488 run.py:483] Algo bellman_ford step 8400 current loss 0.006064, current_train_items 268832.
I0304 19:32:08.590703 22849303695488 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0304 19:32:08.590813 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:08.608196 22849303695488 run.py:483] Algo bellman_ford step 8401 current loss 0.017593, current_train_items 268864.
I0304 19:32:08.633779 22849303695488 run.py:483] Algo bellman_ford step 8402 current loss 0.036962, current_train_items 268896.
I0304 19:32:08.667364 22849303695488 run.py:483] Algo bellman_ford step 8403 current loss 0.054896, current_train_items 268928.
I0304 19:32:08.704434 22849303695488 run.py:483] Algo bellman_ford step 8404 current loss 0.130550, current_train_items 268960.
I0304 19:32:08.724517 22849303695488 run.py:483] Algo bellman_ford step 8405 current loss 0.003584, current_train_items 268992.
I0304 19:32:08.740879 22849303695488 run.py:483] Algo bellman_ford step 8406 current loss 0.018344, current_train_items 269024.
I0304 19:32:08.764821 22849303695488 run.py:483] Algo bellman_ford step 8407 current loss 0.030288, current_train_items 269056.
I0304 19:32:08.796365 22849303695488 run.py:483] Algo bellman_ford step 8408 current loss 0.048528, current_train_items 269088.
I0304 19:32:08.832339 22849303695488 run.py:483] Algo bellman_ford step 8409 current loss 0.063962, current_train_items 269120.
I0304 19:32:08.852369 22849303695488 run.py:483] Algo bellman_ford step 8410 current loss 0.003919, current_train_items 269152.
I0304 19:32:08.868981 22849303695488 run.py:483] Algo bellman_ford step 8411 current loss 0.011564, current_train_items 269184.
I0304 19:32:08.893858 22849303695488 run.py:483] Algo bellman_ford step 8412 current loss 0.054968, current_train_items 269216.
I0304 19:32:08.927484 22849303695488 run.py:483] Algo bellman_ford step 8413 current loss 0.071815, current_train_items 269248.
I0304 19:32:08.962627 22849303695488 run.py:483] Algo bellman_ford step 8414 current loss 0.074384, current_train_items 269280.
I0304 19:32:08.982836 22849303695488 run.py:483] Algo bellman_ford step 8415 current loss 0.008739, current_train_items 269312.
I0304 19:32:08.999667 22849303695488 run.py:483] Algo bellman_ford step 8416 current loss 0.010454, current_train_items 269344.
I0304 19:32:09.024378 22849303695488 run.py:483] Algo bellman_ford step 8417 current loss 0.074244, current_train_items 269376.
I0304 19:32:09.057544 22849303695488 run.py:483] Algo bellman_ford step 8418 current loss 0.030115, current_train_items 269408.
I0304 19:32:09.095057 22849303695488 run.py:483] Algo bellman_ford step 8419 current loss 0.099907, current_train_items 269440.
I0304 19:32:09.114934 22849303695488 run.py:483] Algo bellman_ford step 8420 current loss 0.007765, current_train_items 269472.
I0304 19:32:09.131586 22849303695488 run.py:483] Algo bellman_ford step 8421 current loss 0.032723, current_train_items 269504.
I0304 19:32:09.157136 22849303695488 run.py:483] Algo bellman_ford step 8422 current loss 0.047506, current_train_items 269536.
I0304 19:32:09.189843 22849303695488 run.py:483] Algo bellman_ford step 8423 current loss 0.034959, current_train_items 269568.
I0304 19:32:09.225084 22849303695488 run.py:483] Algo bellman_ford step 8424 current loss 0.036175, current_train_items 269600.
I0304 19:32:09.245019 22849303695488 run.py:483] Algo bellman_ford step 8425 current loss 0.008619, current_train_items 269632.
I0304 19:32:09.261206 22849303695488 run.py:483] Algo bellman_ford step 8426 current loss 0.066013, current_train_items 269664.
I0304 19:32:09.285749 22849303695488 run.py:483] Algo bellman_ford step 8427 current loss 0.069094, current_train_items 269696.
I0304 19:32:09.317972 22849303695488 run.py:483] Algo bellman_ford step 8428 current loss 0.057962, current_train_items 269728.
I0304 19:32:09.351871 22849303695488 run.py:483] Algo bellman_ford step 8429 current loss 0.037436, current_train_items 269760.
I0304 19:32:09.371918 22849303695488 run.py:483] Algo bellman_ford step 8430 current loss 0.010953, current_train_items 269792.
I0304 19:32:09.388926 22849303695488 run.py:483] Algo bellman_ford step 8431 current loss 0.050879, current_train_items 269824.
I0304 19:32:09.413319 22849303695488 run.py:483] Algo bellman_ford step 8432 current loss 0.076164, current_train_items 269856.
I0304 19:32:09.447160 22849303695488 run.py:483] Algo bellman_ford step 8433 current loss 0.082367, current_train_items 269888.
I0304 19:32:09.480163 22849303695488 run.py:483] Algo bellman_ford step 8434 current loss 0.040359, current_train_items 269920.
I0304 19:32:09.499653 22849303695488 run.py:483] Algo bellman_ford step 8435 current loss 0.003669, current_train_items 269952.
I0304 19:32:09.515889 22849303695488 run.py:483] Algo bellman_ford step 8436 current loss 0.019917, current_train_items 269984.
I0304 19:32:09.540037 22849303695488 run.py:483] Algo bellman_ford step 8437 current loss 0.056436, current_train_items 270016.
I0304 19:32:09.572572 22849303695488 run.py:483] Algo bellman_ford step 8438 current loss 0.117435, current_train_items 270048.
I0304 19:32:09.605510 22849303695488 run.py:483] Algo bellman_ford step 8439 current loss 0.101270, current_train_items 270080.
I0304 19:32:09.625599 22849303695488 run.py:483] Algo bellman_ford step 8440 current loss 0.003621, current_train_items 270112.
I0304 19:32:09.642197 22849303695488 run.py:483] Algo bellman_ford step 8441 current loss 0.013892, current_train_items 270144.
I0304 19:32:09.666854 22849303695488 run.py:483] Algo bellman_ford step 8442 current loss 0.023607, current_train_items 270176.
I0304 19:32:09.699579 22849303695488 run.py:483] Algo bellman_ford step 8443 current loss 0.035803, current_train_items 270208.
I0304 19:32:09.734342 22849303695488 run.py:483] Algo bellman_ford step 8444 current loss 0.039535, current_train_items 270240.
I0304 19:32:09.754143 22849303695488 run.py:483] Algo bellman_ford step 8445 current loss 0.006965, current_train_items 270272.
I0304 19:32:09.770539 22849303695488 run.py:483] Algo bellman_ford step 8446 current loss 0.017897, current_train_items 270304.
I0304 19:32:09.794902 22849303695488 run.py:483] Algo bellman_ford step 8447 current loss 0.042583, current_train_items 270336.
I0304 19:32:09.827574 22849303695488 run.py:483] Algo bellman_ford step 8448 current loss 0.037097, current_train_items 270368.
I0304 19:32:09.861962 22849303695488 run.py:483] Algo bellman_ford step 8449 current loss 0.072364, current_train_items 270400.
I0304 19:32:09.881750 22849303695488 run.py:483] Algo bellman_ford step 8450 current loss 0.005390, current_train_items 270432.
I0304 19:32:09.890049 22849303695488 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0304 19:32:09.890157 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:32:09.907033 22849303695488 run.py:483] Algo bellman_ford step 8451 current loss 0.014639, current_train_items 270464.
I0304 19:32:09.932071 22849303695488 run.py:483] Algo bellman_ford step 8452 current loss 0.088228, current_train_items 270496.
I0304 19:32:09.965854 22849303695488 run.py:483] Algo bellman_ford step 8453 current loss 0.134708, current_train_items 270528.
I0304 19:32:10.001405 22849303695488 run.py:483] Algo bellman_ford step 8454 current loss 0.129280, current_train_items 270560.
I0304 19:32:10.022207 22849303695488 run.py:483] Algo bellman_ford step 8455 current loss 0.010481, current_train_items 270592.
I0304 19:32:10.038891 22849303695488 run.py:483] Algo bellman_ford step 8456 current loss 0.013380, current_train_items 270624.
I0304 19:32:10.063857 22849303695488 run.py:483] Algo bellman_ford step 8457 current loss 0.065694, current_train_items 270656.
I0304 19:32:10.095165 22849303695488 run.py:483] Algo bellman_ford step 8458 current loss 0.050202, current_train_items 270688.
I0304 19:32:10.127000 22849303695488 run.py:483] Algo bellman_ford step 8459 current loss 0.051114, current_train_items 270720.
I0304 19:32:10.147408 22849303695488 run.py:483] Algo bellman_ford step 8460 current loss 0.009443, current_train_items 270752.
I0304 19:32:10.163970 22849303695488 run.py:483] Algo bellman_ford step 8461 current loss 0.009873, current_train_items 270784.
I0304 19:32:10.187210 22849303695488 run.py:483] Algo bellman_ford step 8462 current loss 0.035276, current_train_items 270816.
I0304 19:32:10.218631 22849303695488 run.py:483] Algo bellman_ford step 8463 current loss 0.043545, current_train_items 270848.
I0304 19:32:10.253582 22849303695488 run.py:483] Algo bellman_ford step 8464 current loss 0.066632, current_train_items 270880.
I0304 19:32:10.273418 22849303695488 run.py:483] Algo bellman_ford step 8465 current loss 0.003195, current_train_items 270912.
I0304 19:32:10.290371 22849303695488 run.py:483] Algo bellman_ford step 8466 current loss 0.071946, current_train_items 270944.
I0304 19:32:10.314286 22849303695488 run.py:483] Algo bellman_ford step 8467 current loss 0.027168, current_train_items 270976.
I0304 19:32:10.345973 22849303695488 run.py:483] Algo bellman_ford step 8468 current loss 0.048306, current_train_items 271008.
I0304 19:32:10.379712 22849303695488 run.py:483] Algo bellman_ford step 8469 current loss 0.047480, current_train_items 271040.
I0304 19:32:10.399969 22849303695488 run.py:483] Algo bellman_ford step 8470 current loss 0.004460, current_train_items 271072.
I0304 19:32:10.417178 22849303695488 run.py:483] Algo bellman_ford step 8471 current loss 0.059728, current_train_items 271104.
I0304 19:32:10.441320 22849303695488 run.py:483] Algo bellman_ford step 8472 current loss 0.042478, current_train_items 271136.
I0304 19:32:10.471302 22849303695488 run.py:483] Algo bellman_ford step 8473 current loss 0.043853, current_train_items 271168.
I0304 19:32:10.506055 22849303695488 run.py:483] Algo bellman_ford step 8474 current loss 0.080624, current_train_items 271200.
I0304 19:32:10.526473 22849303695488 run.py:483] Algo bellman_ford step 8475 current loss 0.002988, current_train_items 271232.
I0304 19:32:10.543449 22849303695488 run.py:483] Algo bellman_ford step 8476 current loss 0.031117, current_train_items 271264.
I0304 19:32:10.568938 22849303695488 run.py:483] Algo bellman_ford step 8477 current loss 0.075058, current_train_items 271296.
I0304 19:32:10.601636 22849303695488 run.py:483] Algo bellman_ford step 8478 current loss 0.053597, current_train_items 271328.
I0304 19:32:10.637118 22849303695488 run.py:483] Algo bellman_ford step 8479 current loss 0.063283, current_train_items 271360.
I0304 19:32:10.656994 22849303695488 run.py:483] Algo bellman_ford step 8480 current loss 0.005417, current_train_items 271392.
I0304 19:32:10.673748 22849303695488 run.py:483] Algo bellman_ford step 8481 current loss 0.109663, current_train_items 271424.
I0304 19:32:10.698553 22849303695488 run.py:483] Algo bellman_ford step 8482 current loss 0.087481, current_train_items 271456.
I0304 19:32:10.731319 22849303695488 run.py:483] Algo bellman_ford step 8483 current loss 0.059077, current_train_items 271488.
I0304 19:32:10.764192 22849303695488 run.py:483] Algo bellman_ford step 8484 current loss 0.056250, current_train_items 271520.
I0304 19:32:10.784490 22849303695488 run.py:483] Algo bellman_ford step 8485 current loss 0.007849, current_train_items 271552.
I0304 19:32:10.800857 22849303695488 run.py:483] Algo bellman_ford step 8486 current loss 0.007196, current_train_items 271584.
I0304 19:32:10.825259 22849303695488 run.py:483] Algo bellman_ford step 8487 current loss 0.035186, current_train_items 271616.
I0304 19:32:10.858165 22849303695488 run.py:483] Algo bellman_ford step 8488 current loss 0.120111, current_train_items 271648.
I0304 19:32:10.892698 22849303695488 run.py:483] Algo bellman_ford step 8489 current loss 0.149567, current_train_items 271680.
I0304 19:32:10.912975 22849303695488 run.py:483] Algo bellman_ford step 8490 current loss 0.017402, current_train_items 271712.
I0304 19:32:10.929336 22849303695488 run.py:483] Algo bellman_ford step 8491 current loss 0.031067, current_train_items 271744.
I0304 19:32:10.954514 22849303695488 run.py:483] Algo bellman_ford step 8492 current loss 0.043210, current_train_items 271776.
I0304 19:32:10.986220 22849303695488 run.py:483] Algo bellman_ford step 8493 current loss 0.041437, current_train_items 271808.
I0304 19:32:11.019209 22849303695488 run.py:483] Algo bellman_ford step 8494 current loss 0.119037, current_train_items 271840.
I0304 19:32:11.039266 22849303695488 run.py:483] Algo bellman_ford step 8495 current loss 0.008708, current_train_items 271872.
I0304 19:32:11.056166 22849303695488 run.py:483] Algo bellman_ford step 8496 current loss 0.042255, current_train_items 271904.
I0304 19:32:11.080988 22849303695488 run.py:483] Algo bellman_ford step 8497 current loss 0.018059, current_train_items 271936.
I0304 19:32:11.113813 22849303695488 run.py:483] Algo bellman_ford step 8498 current loss 0.045314, current_train_items 271968.
I0304 19:32:11.146585 22849303695488 run.py:483] Algo bellman_ford step 8499 current loss 0.041290, current_train_items 272000.
I0304 19:32:11.166919 22849303695488 run.py:483] Algo bellman_ford step 8500 current loss 0.003741, current_train_items 272032.
I0304 19:32:11.174944 22849303695488 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0304 19:32:11.175094 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:32:11.192650 22849303695488 run.py:483] Algo bellman_ford step 8501 current loss 0.015752, current_train_items 272064.
I0304 19:32:11.218406 22849303695488 run.py:483] Algo bellman_ford step 8502 current loss 0.047668, current_train_items 272096.
I0304 19:32:11.250509 22849303695488 run.py:483] Algo bellman_ford step 8503 current loss 0.041502, current_train_items 272128.
I0304 19:32:11.284287 22849303695488 run.py:483] Algo bellman_ford step 8504 current loss 0.059113, current_train_items 272160.
I0304 19:32:11.304771 22849303695488 run.py:483] Algo bellman_ford step 8505 current loss 0.002860, current_train_items 272192.
I0304 19:32:11.320494 22849303695488 run.py:483] Algo bellman_ford step 8506 current loss 0.003073, current_train_items 272224.
I0304 19:32:11.343833 22849303695488 run.py:483] Algo bellman_ford step 8507 current loss 0.028389, current_train_items 272256.
I0304 19:32:11.376894 22849303695488 run.py:483] Algo bellman_ford step 8508 current loss 0.042935, current_train_items 272288.
I0304 19:32:11.412530 22849303695488 run.py:483] Algo bellman_ford step 8509 current loss 0.090324, current_train_items 272320.
I0304 19:32:11.432662 22849303695488 run.py:483] Algo bellman_ford step 8510 current loss 0.004196, current_train_items 272352.
I0304 19:32:11.448767 22849303695488 run.py:483] Algo bellman_ford step 8511 current loss 0.040050, current_train_items 272384.
I0304 19:32:11.474520 22849303695488 run.py:483] Algo bellman_ford step 8512 current loss 0.028673, current_train_items 272416.
I0304 19:32:11.506737 22849303695488 run.py:483] Algo bellman_ford step 8513 current loss 0.044688, current_train_items 272448.
I0304 19:32:11.541843 22849303695488 run.py:483] Algo bellman_ford step 8514 current loss 0.054353, current_train_items 272480.
I0304 19:32:11.561913 22849303695488 run.py:483] Algo bellman_ford step 8515 current loss 0.002573, current_train_items 272512.
I0304 19:32:11.578602 22849303695488 run.py:483] Algo bellman_ford step 8516 current loss 0.020854, current_train_items 272544.
I0304 19:32:11.603235 22849303695488 run.py:483] Algo bellman_ford step 8517 current loss 0.093651, current_train_items 272576.
I0304 19:32:11.636994 22849303695488 run.py:483] Algo bellman_ford step 8518 current loss 0.085459, current_train_items 272608.
I0304 19:32:11.670590 22849303695488 run.py:483] Algo bellman_ford step 8519 current loss 0.075432, current_train_items 272640.
I0304 19:32:11.690947 22849303695488 run.py:483] Algo bellman_ford step 8520 current loss 0.002299, current_train_items 272672.
I0304 19:32:11.707317 22849303695488 run.py:483] Algo bellman_ford step 8521 current loss 0.004687, current_train_items 272704.
I0304 19:32:11.731950 22849303695488 run.py:483] Algo bellman_ford step 8522 current loss 0.063687, current_train_items 272736.
I0304 19:32:11.764395 22849303695488 run.py:483] Algo bellman_ford step 8523 current loss 0.064524, current_train_items 272768.
I0304 19:32:11.799629 22849303695488 run.py:483] Algo bellman_ford step 8524 current loss 0.143246, current_train_items 272800.
I0304 19:32:11.819955 22849303695488 run.py:483] Algo bellman_ford step 8525 current loss 0.003043, current_train_items 272832.
I0304 19:32:11.836601 22849303695488 run.py:483] Algo bellman_ford step 8526 current loss 0.016671, current_train_items 272864.
I0304 19:32:11.861381 22849303695488 run.py:483] Algo bellman_ford step 8527 current loss 0.080906, current_train_items 272896.
I0304 19:32:11.893441 22849303695488 run.py:483] Algo bellman_ford step 8528 current loss 0.027376, current_train_items 272928.
I0304 19:32:11.928370 22849303695488 run.py:483] Algo bellman_ford step 8529 current loss 0.129335, current_train_items 272960.
I0304 19:32:11.948724 22849303695488 run.py:483] Algo bellman_ford step 8530 current loss 0.001638, current_train_items 272992.
I0304 19:32:11.965367 22849303695488 run.py:483] Algo bellman_ford step 8531 current loss 0.011455, current_train_items 273024.
I0304 19:32:11.988837 22849303695488 run.py:483] Algo bellman_ford step 8532 current loss 0.035450, current_train_items 273056.
I0304 19:32:12.020520 22849303695488 run.py:483] Algo bellman_ford step 8533 current loss 0.050069, current_train_items 273088.
I0304 19:32:12.054205 22849303695488 run.py:483] Algo bellman_ford step 8534 current loss 0.069848, current_train_items 273120.
I0304 19:32:12.074537 22849303695488 run.py:483] Algo bellman_ford step 8535 current loss 0.001857, current_train_items 273152.
I0304 19:32:12.090995 22849303695488 run.py:483] Algo bellman_ford step 8536 current loss 0.028757, current_train_items 273184.
I0304 19:32:12.115806 22849303695488 run.py:483] Algo bellman_ford step 8537 current loss 0.050399, current_train_items 273216.
I0304 19:32:12.148433 22849303695488 run.py:483] Algo bellman_ford step 8538 current loss 0.029518, current_train_items 273248.
I0304 19:32:12.185158 22849303695488 run.py:483] Algo bellman_ford step 8539 current loss 0.073214, current_train_items 273280.
I0304 19:32:12.205382 22849303695488 run.py:483] Algo bellman_ford step 8540 current loss 0.015469, current_train_items 273312.
I0304 19:32:12.221865 22849303695488 run.py:483] Algo bellman_ford step 8541 current loss 0.031521, current_train_items 273344.
I0304 19:32:12.246848 22849303695488 run.py:483] Algo bellman_ford step 8542 current loss 0.021558, current_train_items 273376.
I0304 19:32:12.280184 22849303695488 run.py:483] Algo bellman_ford step 8543 current loss 0.046675, current_train_items 273408.
I0304 19:32:12.314595 22849303695488 run.py:483] Algo bellman_ford step 8544 current loss 0.056026, current_train_items 273440.
I0304 19:32:12.334501 22849303695488 run.py:483] Algo bellman_ford step 8545 current loss 0.003277, current_train_items 273472.
I0304 19:32:12.350595 22849303695488 run.py:483] Algo bellman_ford step 8546 current loss 0.025315, current_train_items 273504.
I0304 19:32:12.375813 22849303695488 run.py:483] Algo bellman_ford step 8547 current loss 0.073176, current_train_items 273536.
I0304 19:32:12.408417 22849303695488 run.py:483] Algo bellman_ford step 8548 current loss 0.057807, current_train_items 273568.
I0304 19:32:12.443350 22849303695488 run.py:483] Algo bellman_ford step 8549 current loss 0.106383, current_train_items 273600.
I0304 19:32:12.463275 22849303695488 run.py:483] Algo bellman_ford step 8550 current loss 0.002857, current_train_items 273632.
I0304 19:32:12.471448 22849303695488 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0304 19:32:12.471555 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:32:12.488962 22849303695488 run.py:483] Algo bellman_ford step 8551 current loss 0.014639, current_train_items 273664.
I0304 19:32:12.514528 22849303695488 run.py:483] Algo bellman_ford step 8552 current loss 0.036475, current_train_items 273696.
I0304 19:32:12.547580 22849303695488 run.py:483] Algo bellman_ford step 8553 current loss 0.053272, current_train_items 273728.
I0304 19:32:12.579790 22849303695488 run.py:483] Algo bellman_ford step 8554 current loss 0.024237, current_train_items 273760.
I0304 19:32:12.599886 22849303695488 run.py:483] Algo bellman_ford step 8555 current loss 0.003548, current_train_items 273792.
I0304 19:32:12.616069 22849303695488 run.py:483] Algo bellman_ford step 8556 current loss 0.032068, current_train_items 273824.
I0304 19:32:12.640320 22849303695488 run.py:483] Algo bellman_ford step 8557 current loss 0.033341, current_train_items 273856.
I0304 19:32:12.672992 22849303695488 run.py:483] Algo bellman_ford step 8558 current loss 0.074869, current_train_items 273888.
I0304 19:32:12.708056 22849303695488 run.py:483] Algo bellman_ford step 8559 current loss 0.091546, current_train_items 273920.
I0304 19:32:12.728292 22849303695488 run.py:483] Algo bellman_ford step 8560 current loss 0.001920, current_train_items 273952.
I0304 19:32:12.745242 22849303695488 run.py:483] Algo bellman_ford step 8561 current loss 0.015038, current_train_items 273984.
I0304 19:32:12.769527 22849303695488 run.py:483] Algo bellman_ford step 8562 current loss 0.047683, current_train_items 274016.
I0304 19:32:12.803453 22849303695488 run.py:483] Algo bellman_ford step 8563 current loss 0.046814, current_train_items 274048.
I0304 19:32:12.836495 22849303695488 run.py:483] Algo bellman_ford step 8564 current loss 0.067303, current_train_items 274080.
I0304 19:32:12.856438 22849303695488 run.py:483] Algo bellman_ford step 8565 current loss 0.002055, current_train_items 274112.
I0304 19:32:12.873750 22849303695488 run.py:483] Algo bellman_ford step 8566 current loss 0.035141, current_train_items 274144.
I0304 19:32:12.899484 22849303695488 run.py:483] Algo bellman_ford step 8567 current loss 0.030866, current_train_items 274176.
I0304 19:32:12.931762 22849303695488 run.py:483] Algo bellman_ford step 8568 current loss 0.044801, current_train_items 274208.
I0304 19:32:12.966443 22849303695488 run.py:483] Algo bellman_ford step 8569 current loss 0.073345, current_train_items 274240.
I0304 19:32:12.987101 22849303695488 run.py:483] Algo bellman_ford step 8570 current loss 0.002314, current_train_items 274272.
I0304 19:32:13.003701 22849303695488 run.py:483] Algo bellman_ford step 8571 current loss 0.025836, current_train_items 274304.
I0304 19:32:13.027348 22849303695488 run.py:483] Algo bellman_ford step 8572 current loss 0.047418, current_train_items 274336.
I0304 19:32:13.059475 22849303695488 run.py:483] Algo bellman_ford step 8573 current loss 0.046565, current_train_items 274368.
I0304 19:32:13.092337 22849303695488 run.py:483] Algo bellman_ford step 8574 current loss 0.061145, current_train_items 274400.
I0304 19:32:13.112725 22849303695488 run.py:483] Algo bellman_ford step 8575 current loss 0.004722, current_train_items 274432.
I0304 19:32:13.129320 22849303695488 run.py:483] Algo bellman_ford step 8576 current loss 0.014865, current_train_items 274464.
I0304 19:32:13.153564 22849303695488 run.py:483] Algo bellman_ford step 8577 current loss 0.030494, current_train_items 274496.
I0304 19:32:13.186442 22849303695488 run.py:483] Algo bellman_ford step 8578 current loss 0.043056, current_train_items 274528.
I0304 19:32:13.220921 22849303695488 run.py:483] Algo bellman_ford step 8579 current loss 0.068506, current_train_items 274560.
I0304 19:32:13.240842 22849303695488 run.py:483] Algo bellman_ford step 8580 current loss 0.008061, current_train_items 274592.
I0304 19:32:13.257495 22849303695488 run.py:483] Algo bellman_ford step 8581 current loss 0.006152, current_train_items 274624.
I0304 19:32:13.280936 22849303695488 run.py:483] Algo bellman_ford step 8582 current loss 0.029934, current_train_items 274656.
I0304 19:32:13.313239 22849303695488 run.py:483] Algo bellman_ford step 8583 current loss 0.058591, current_train_items 274688.
I0304 19:32:13.348611 22849303695488 run.py:483] Algo bellman_ford step 8584 current loss 0.084851, current_train_items 274720.
I0304 19:32:13.368695 22849303695488 run.py:483] Algo bellman_ford step 8585 current loss 0.002766, current_train_items 274752.
I0304 19:32:13.385250 22849303695488 run.py:483] Algo bellman_ford step 8586 current loss 0.006550, current_train_items 274784.
I0304 19:32:13.410669 22849303695488 run.py:483] Algo bellman_ford step 8587 current loss 0.062914, current_train_items 274816.
I0304 19:32:13.442859 22849303695488 run.py:483] Algo bellman_ford step 8588 current loss 0.030563, current_train_items 274848.
I0304 19:32:13.475905 22849303695488 run.py:483] Algo bellman_ford step 8589 current loss 0.040368, current_train_items 274880.
I0304 19:32:13.496248 22849303695488 run.py:483] Algo bellman_ford step 8590 current loss 0.100178, current_train_items 274912.
I0304 19:32:13.512193 22849303695488 run.py:483] Algo bellman_ford step 8591 current loss 0.022769, current_train_items 274944.
I0304 19:32:13.536484 22849303695488 run.py:483] Algo bellman_ford step 8592 current loss 0.040262, current_train_items 274976.
I0304 19:32:13.569639 22849303695488 run.py:483] Algo bellman_ford step 8593 current loss 0.067274, current_train_items 275008.
I0304 19:32:13.604086 22849303695488 run.py:483] Algo bellman_ford step 8594 current loss 0.119542, current_train_items 275040.
I0304 19:32:13.623831 22849303695488 run.py:483] Algo bellman_ford step 8595 current loss 0.002816, current_train_items 275072.
I0304 19:32:13.640054 22849303695488 run.py:483] Algo bellman_ford step 8596 current loss 0.007424, current_train_items 275104.
I0304 19:32:13.664620 22849303695488 run.py:483] Algo bellman_ford step 8597 current loss 0.079833, current_train_items 275136.
I0304 19:32:13.695800 22849303695488 run.py:483] Algo bellman_ford step 8598 current loss 0.029589, current_train_items 275168.
I0304 19:32:13.730852 22849303695488 run.py:483] Algo bellman_ford step 8599 current loss 0.104629, current_train_items 275200.
I0304 19:32:13.751115 22849303695488 run.py:483] Algo bellman_ford step 8600 current loss 0.012356, current_train_items 275232.
I0304 19:32:13.759146 22849303695488 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0304 19:32:13.759256 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:13.776946 22849303695488 run.py:483] Algo bellman_ford step 8601 current loss 0.021933, current_train_items 275264.
I0304 19:32:13.802488 22849303695488 run.py:483] Algo bellman_ford step 8602 current loss 0.035718, current_train_items 275296.
I0304 19:32:13.836173 22849303695488 run.py:483] Algo bellman_ford step 8603 current loss 0.053240, current_train_items 275328.
I0304 19:32:13.871145 22849303695488 run.py:483] Algo bellman_ford step 8604 current loss 0.081872, current_train_items 275360.
I0304 19:32:13.891341 22849303695488 run.py:483] Algo bellman_ford step 8605 current loss 0.003355, current_train_items 275392.
I0304 19:32:13.907140 22849303695488 run.py:483] Algo bellman_ford step 8606 current loss 0.006816, current_train_items 275424.
I0304 19:32:13.931214 22849303695488 run.py:483] Algo bellman_ford step 8607 current loss 0.036706, current_train_items 275456.
I0304 19:32:13.963473 22849303695488 run.py:483] Algo bellman_ford step 8608 current loss 0.042185, current_train_items 275488.
I0304 19:32:13.999626 22849303695488 run.py:483] Algo bellman_ford step 8609 current loss 0.069976, current_train_items 275520.
I0304 19:32:14.019657 22849303695488 run.py:483] Algo bellman_ford step 8610 current loss 0.030473, current_train_items 275552.
I0304 19:32:14.036275 22849303695488 run.py:483] Algo bellman_ford step 8611 current loss 0.006439, current_train_items 275584.
I0304 19:32:14.061150 22849303695488 run.py:483] Algo bellman_ford step 8612 current loss 0.059787, current_train_items 275616.
I0304 19:32:14.093675 22849303695488 run.py:483] Algo bellman_ford step 8613 current loss 0.070769, current_train_items 275648.
I0304 19:32:14.126109 22849303695488 run.py:483] Algo bellman_ford step 8614 current loss 0.033109, current_train_items 275680.
I0304 19:32:14.146256 22849303695488 run.py:483] Algo bellman_ford step 8615 current loss 0.003105, current_train_items 275712.
I0304 19:32:14.162823 22849303695488 run.py:483] Algo bellman_ford step 8616 current loss 0.043328, current_train_items 275744.
I0304 19:32:14.187467 22849303695488 run.py:483] Algo bellman_ford step 8617 current loss 0.049486, current_train_items 275776.
I0304 19:32:14.220553 22849303695488 run.py:483] Algo bellman_ford step 8618 current loss 0.044605, current_train_items 275808.
I0304 19:32:14.254811 22849303695488 run.py:483] Algo bellman_ford step 8619 current loss 0.090263, current_train_items 275840.
I0304 19:32:14.274720 22849303695488 run.py:483] Algo bellman_ford step 8620 current loss 0.002833, current_train_items 275872.
I0304 19:32:14.291518 22849303695488 run.py:483] Algo bellman_ford step 8621 current loss 0.032949, current_train_items 275904.
I0304 19:32:14.316745 22849303695488 run.py:483] Algo bellman_ford step 8622 current loss 0.061999, current_train_items 275936.
I0304 19:32:14.349807 22849303695488 run.py:483] Algo bellman_ford step 8623 current loss 0.026845, current_train_items 275968.
I0304 19:32:14.383962 22849303695488 run.py:483] Algo bellman_ford step 8624 current loss 0.036847, current_train_items 276000.
I0304 19:32:14.403966 22849303695488 run.py:483] Algo bellman_ford step 8625 current loss 0.004544, current_train_items 276032.
I0304 19:32:14.420550 22849303695488 run.py:483] Algo bellman_ford step 8626 current loss 0.031127, current_train_items 276064.
I0304 19:32:14.445424 22849303695488 run.py:483] Algo bellman_ford step 8627 current loss 0.035262, current_train_items 276096.
I0304 19:32:14.478711 22849303695488 run.py:483] Algo bellman_ford step 8628 current loss 0.042689, current_train_items 276128.
I0304 19:32:14.512223 22849303695488 run.py:483] Algo bellman_ford step 8629 current loss 0.079750, current_train_items 276160.
I0304 19:32:14.532241 22849303695488 run.py:483] Algo bellman_ford step 8630 current loss 0.002021, current_train_items 276192.
I0304 19:32:14.548909 22849303695488 run.py:483] Algo bellman_ford step 8631 current loss 0.017136, current_train_items 276224.
I0304 19:32:14.573164 22849303695488 run.py:483] Algo bellman_ford step 8632 current loss 0.042506, current_train_items 276256.
I0304 19:32:14.605359 22849303695488 run.py:483] Algo bellman_ford step 8633 current loss 0.039073, current_train_items 276288.
I0304 19:32:14.639438 22849303695488 run.py:483] Algo bellman_ford step 8634 current loss 0.051997, current_train_items 276320.
I0304 19:32:14.659726 22849303695488 run.py:483] Algo bellman_ford step 8635 current loss 0.010280, current_train_items 276352.
I0304 19:32:14.676280 22849303695488 run.py:483] Algo bellman_ford step 8636 current loss 0.013191, current_train_items 276384.
I0304 19:32:14.701391 22849303695488 run.py:483] Algo bellman_ford step 8637 current loss 0.042165, current_train_items 276416.
I0304 19:32:14.732889 22849303695488 run.py:483] Algo bellman_ford step 8638 current loss 0.086627, current_train_items 276448.
I0304 19:32:14.767835 22849303695488 run.py:483] Algo bellman_ford step 8639 current loss 0.075911, current_train_items 276480.
I0304 19:32:14.787900 22849303695488 run.py:483] Algo bellman_ford step 8640 current loss 0.006214, current_train_items 276512.
I0304 19:32:14.804743 22849303695488 run.py:483] Algo bellman_ford step 8641 current loss 0.041529, current_train_items 276544.
I0304 19:32:14.829334 22849303695488 run.py:483] Algo bellman_ford step 8642 current loss 0.054845, current_train_items 276576.
I0304 19:32:14.862109 22849303695488 run.py:483] Algo bellman_ford step 8643 current loss 0.055543, current_train_items 276608.
I0304 19:32:14.896473 22849303695488 run.py:483] Algo bellman_ford step 8644 current loss 0.067757, current_train_items 276640.
I0304 19:32:14.916429 22849303695488 run.py:483] Algo bellman_ford step 8645 current loss 0.003056, current_train_items 276672.
I0304 19:32:14.932953 22849303695488 run.py:483] Algo bellman_ford step 8646 current loss 0.038905, current_train_items 276704.
I0304 19:32:14.957495 22849303695488 run.py:483] Algo bellman_ford step 8647 current loss 0.080937, current_train_items 276736.
I0304 19:32:14.990693 22849303695488 run.py:483] Algo bellman_ford step 8648 current loss 0.079405, current_train_items 276768.
I0304 19:32:15.024434 22849303695488 run.py:483] Algo bellman_ford step 8649 current loss 0.059053, current_train_items 276800.
I0304 19:32:15.044255 22849303695488 run.py:483] Algo bellman_ford step 8650 current loss 0.002464, current_train_items 276832.
I0304 19:32:15.052541 22849303695488 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0304 19:32:15.052650 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:32:15.070023 22849303695488 run.py:483] Algo bellman_ford step 8651 current loss 0.030749, current_train_items 276864.
I0304 19:32:15.095314 22849303695488 run.py:483] Algo bellman_ford step 8652 current loss 0.091045, current_train_items 276896.
I0304 19:32:15.128624 22849303695488 run.py:483] Algo bellman_ford step 8653 current loss 0.088376, current_train_items 276928.
I0304 19:32:15.164297 22849303695488 run.py:483] Algo bellman_ford step 8654 current loss 0.131505, current_train_items 276960.
I0304 19:32:15.184865 22849303695488 run.py:483] Algo bellman_ford step 8655 current loss 0.002194, current_train_items 276992.
I0304 19:32:15.201376 22849303695488 run.py:483] Algo bellman_ford step 8656 current loss 0.026295, current_train_items 277024.
I0304 19:32:15.227493 22849303695488 run.py:483] Algo bellman_ford step 8657 current loss 0.079336, current_train_items 277056.
I0304 19:32:15.260711 22849303695488 run.py:483] Algo bellman_ford step 8658 current loss 0.085321, current_train_items 277088.
I0304 19:32:15.295320 22849303695488 run.py:483] Algo bellman_ford step 8659 current loss 0.166326, current_train_items 277120.
I0304 19:32:15.315769 22849303695488 run.py:483] Algo bellman_ford step 8660 current loss 0.006102, current_train_items 277152.
I0304 19:32:15.332226 22849303695488 run.py:483] Algo bellman_ford step 8661 current loss 0.041268, current_train_items 277184.
I0304 19:32:15.356450 22849303695488 run.py:483] Algo bellman_ford step 8662 current loss 0.075427, current_train_items 277216.
I0304 19:32:15.388787 22849303695488 run.py:483] Algo bellman_ford step 8663 current loss 0.124186, current_train_items 277248.
I0304 19:32:15.424480 22849303695488 run.py:483] Algo bellman_ford step 8664 current loss 0.071806, current_train_items 277280.
I0304 19:32:15.444536 22849303695488 run.py:483] Algo bellman_ford step 8665 current loss 0.004080, current_train_items 277312.
I0304 19:32:15.460724 22849303695488 run.py:483] Algo bellman_ford step 8666 current loss 0.024919, current_train_items 277344.
I0304 19:32:15.485525 22849303695488 run.py:483] Algo bellman_ford step 8667 current loss 0.049769, current_train_items 277376.
I0304 19:32:15.517571 22849303695488 run.py:483] Algo bellman_ford step 8668 current loss 0.074811, current_train_items 277408.
I0304 19:32:15.551054 22849303695488 run.py:483] Algo bellman_ford step 8669 current loss 0.054325, current_train_items 277440.
I0304 19:32:15.571671 22849303695488 run.py:483] Algo bellman_ford step 8670 current loss 0.002809, current_train_items 277472.
I0304 19:32:15.588470 22849303695488 run.py:483] Algo bellman_ford step 8671 current loss 0.020524, current_train_items 277504.
I0304 19:32:15.612518 22849303695488 run.py:483] Algo bellman_ford step 8672 current loss 0.035917, current_train_items 277536.
I0304 19:32:15.644947 22849303695488 run.py:483] Algo bellman_ford step 8673 current loss 0.053918, current_train_items 277568.
I0304 19:32:15.677444 22849303695488 run.py:483] Algo bellman_ford step 8674 current loss 0.117371, current_train_items 277600.
I0304 19:32:15.698027 22849303695488 run.py:483] Algo bellman_ford step 8675 current loss 0.015657, current_train_items 277632.
I0304 19:32:15.714524 22849303695488 run.py:483] Algo bellman_ford step 8676 current loss 0.022735, current_train_items 277664.
I0304 19:32:15.739500 22849303695488 run.py:483] Algo bellman_ford step 8677 current loss 0.051684, current_train_items 277696.
I0304 19:32:15.772510 22849303695488 run.py:483] Algo bellman_ford step 8678 current loss 0.048548, current_train_items 277728.
I0304 19:32:15.806474 22849303695488 run.py:483] Algo bellman_ford step 8679 current loss 0.061011, current_train_items 277760.
I0304 19:32:15.826484 22849303695488 run.py:483] Algo bellman_ford step 8680 current loss 0.004072, current_train_items 277792.
I0304 19:32:15.843252 22849303695488 run.py:483] Algo bellman_ford step 8681 current loss 0.009828, current_train_items 277824.
I0304 19:32:15.868401 22849303695488 run.py:483] Algo bellman_ford step 8682 current loss 0.038746, current_train_items 277856.
I0304 19:32:15.901083 22849303695488 run.py:483] Algo bellman_ford step 8683 current loss 0.045894, current_train_items 277888.
I0304 19:32:15.934826 22849303695488 run.py:483] Algo bellman_ford step 8684 current loss 0.103580, current_train_items 277920.
I0304 19:32:15.955588 22849303695488 run.py:483] Algo bellman_ford step 8685 current loss 0.004223, current_train_items 277952.
I0304 19:32:15.972203 22849303695488 run.py:483] Algo bellman_ford step 8686 current loss 0.020718, current_train_items 277984.
I0304 19:32:15.997216 22849303695488 run.py:483] Algo bellman_ford step 8687 current loss 0.050224, current_train_items 278016.
I0304 19:32:16.030298 22849303695488 run.py:483] Algo bellman_ford step 8688 current loss 0.069508, current_train_items 278048.
I0304 19:32:16.064124 22849303695488 run.py:483] Algo bellman_ford step 8689 current loss 0.094989, current_train_items 278080.
I0304 19:32:16.084474 22849303695488 run.py:483] Algo bellman_ford step 8690 current loss 0.003104, current_train_items 278112.
I0304 19:32:16.101215 22849303695488 run.py:483] Algo bellman_ford step 8691 current loss 0.054295, current_train_items 278144.
I0304 19:32:16.125389 22849303695488 run.py:483] Algo bellman_ford step 8692 current loss 0.061852, current_train_items 278176.
I0304 19:32:16.158126 22849303695488 run.py:483] Algo bellman_ford step 8693 current loss 0.042307, current_train_items 278208.
I0304 19:32:16.191984 22849303695488 run.py:483] Algo bellman_ford step 8694 current loss 0.056683, current_train_items 278240.
I0304 19:32:16.211973 22849303695488 run.py:483] Algo bellman_ford step 8695 current loss 0.003263, current_train_items 278272.
I0304 19:32:16.228354 22849303695488 run.py:483] Algo bellman_ford step 8696 current loss 0.008622, current_train_items 278304.
I0304 19:32:16.253337 22849303695488 run.py:483] Algo bellman_ford step 8697 current loss 0.055619, current_train_items 278336.
I0304 19:32:16.286427 22849303695488 run.py:483] Algo bellman_ford step 8698 current loss 0.036475, current_train_items 278368.
I0304 19:32:16.322978 22849303695488 run.py:483] Algo bellman_ford step 8699 current loss 0.182568, current_train_items 278400.
I0304 19:32:16.343476 22849303695488 run.py:483] Algo bellman_ford step 8700 current loss 0.003272, current_train_items 278432.
I0304 19:32:16.351686 22849303695488 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0304 19:32:16.351796 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:16.369096 22849303695488 run.py:483] Algo bellman_ford step 8701 current loss 0.008383, current_train_items 278464.
I0304 19:32:16.394296 22849303695488 run.py:483] Algo bellman_ford step 8702 current loss 0.028818, current_train_items 278496.
I0304 19:32:16.427912 22849303695488 run.py:483] Algo bellman_ford step 8703 current loss 0.051144, current_train_items 278528.
I0304 19:32:16.464274 22849303695488 run.py:483] Algo bellman_ford step 8704 current loss 0.073576, current_train_items 278560.
I0304 19:32:16.484714 22849303695488 run.py:483] Algo bellman_ford step 8705 current loss 0.006611, current_train_items 278592.
I0304 19:32:16.501046 22849303695488 run.py:483] Algo bellman_ford step 8706 current loss 0.044191, current_train_items 278624.
I0304 19:32:16.525784 22849303695488 run.py:483] Algo bellman_ford step 8707 current loss 0.027970, current_train_items 278656.
I0304 19:32:16.559196 22849303695488 run.py:483] Algo bellman_ford step 8708 current loss 0.056334, current_train_items 278688.
I0304 19:32:16.593877 22849303695488 run.py:483] Algo bellman_ford step 8709 current loss 0.073472, current_train_items 278720.
I0304 19:32:16.614316 22849303695488 run.py:483] Algo bellman_ford step 8710 current loss 0.007185, current_train_items 278752.
I0304 19:32:16.631062 22849303695488 run.py:483] Algo bellman_ford step 8711 current loss 0.023112, current_train_items 278784.
I0304 19:32:16.655676 22849303695488 run.py:483] Algo bellman_ford step 8712 current loss 0.036261, current_train_items 278816.
I0304 19:32:16.690493 22849303695488 run.py:483] Algo bellman_ford step 8713 current loss 0.092558, current_train_items 278848.
I0304 19:32:16.723765 22849303695488 run.py:483] Algo bellman_ford step 8714 current loss 0.039532, current_train_items 278880.
I0304 19:32:16.743778 22849303695488 run.py:483] Algo bellman_ford step 8715 current loss 0.006509, current_train_items 278912.
I0304 19:32:16.760636 22849303695488 run.py:483] Algo bellman_ford step 8716 current loss 0.034264, current_train_items 278944.
I0304 19:32:16.784825 22849303695488 run.py:483] Algo bellman_ford step 8717 current loss 0.042687, current_train_items 278976.
I0304 19:32:16.816524 22849303695488 run.py:483] Algo bellman_ford step 8718 current loss 0.059110, current_train_items 279008.
I0304 19:32:16.850189 22849303695488 run.py:483] Algo bellman_ford step 8719 current loss 0.070497, current_train_items 279040.
I0304 19:32:16.870248 22849303695488 run.py:483] Algo bellman_ford step 8720 current loss 0.002749, current_train_items 279072.
I0304 19:32:16.887219 22849303695488 run.py:483] Algo bellman_ford step 8721 current loss 0.029941, current_train_items 279104.
I0304 19:32:16.912863 22849303695488 run.py:483] Algo bellman_ford step 8722 current loss 0.053219, current_train_items 279136.
I0304 19:32:16.944691 22849303695488 run.py:483] Algo bellman_ford step 8723 current loss 0.076492, current_train_items 279168.
I0304 19:32:16.978629 22849303695488 run.py:483] Algo bellman_ford step 8724 current loss 0.074259, current_train_items 279200.
I0304 19:32:16.999047 22849303695488 run.py:483] Algo bellman_ford step 8725 current loss 0.011892, current_train_items 279232.
I0304 19:32:17.015778 22849303695488 run.py:483] Algo bellman_ford step 8726 current loss 0.028174, current_train_items 279264.
I0304 19:32:17.040678 22849303695488 run.py:483] Algo bellman_ford step 8727 current loss 0.080049, current_train_items 279296.
I0304 19:32:17.073307 22849303695488 run.py:483] Algo bellman_ford step 8728 current loss 0.068171, current_train_items 279328.
I0304 19:32:17.107641 22849303695488 run.py:483] Algo bellman_ford step 8729 current loss 0.064765, current_train_items 279360.
I0304 19:32:17.127671 22849303695488 run.py:483] Algo bellman_ford step 8730 current loss 0.017044, current_train_items 279392.
I0304 19:32:17.143707 22849303695488 run.py:483] Algo bellman_ford step 8731 current loss 0.005065, current_train_items 279424.
I0304 19:32:17.168721 22849303695488 run.py:483] Algo bellman_ford step 8732 current loss 0.062942, current_train_items 279456.
I0304 19:32:17.201466 22849303695488 run.py:483] Algo bellman_ford step 8733 current loss 0.036734, current_train_items 279488.
I0304 19:32:17.237760 22849303695488 run.py:483] Algo bellman_ford step 8734 current loss 0.071406, current_train_items 279520.
I0304 19:32:17.257624 22849303695488 run.py:483] Algo bellman_ford step 8735 current loss 0.003145, current_train_items 279552.
I0304 19:32:17.274080 22849303695488 run.py:483] Algo bellman_ford step 8736 current loss 0.026995, current_train_items 279584.
I0304 19:32:17.299945 22849303695488 run.py:483] Algo bellman_ford step 8737 current loss 0.051364, current_train_items 279616.
I0304 19:32:17.332385 22849303695488 run.py:483] Algo bellman_ford step 8738 current loss 0.036216, current_train_items 279648.
I0304 19:32:17.367788 22849303695488 run.py:483] Algo bellman_ford step 8739 current loss 0.071205, current_train_items 279680.
I0304 19:32:17.388041 22849303695488 run.py:483] Algo bellman_ford step 8740 current loss 0.004905, current_train_items 279712.
I0304 19:32:17.404375 22849303695488 run.py:483] Algo bellman_ford step 8741 current loss 0.020314, current_train_items 279744.
I0304 19:32:17.429416 22849303695488 run.py:483] Algo bellman_ford step 8742 current loss 0.036153, current_train_items 279776.
I0304 19:32:17.462935 22849303695488 run.py:483] Algo bellman_ford step 8743 current loss 0.049608, current_train_items 279808.
I0304 19:32:17.497742 22849303695488 run.py:483] Algo bellman_ford step 8744 current loss 0.038102, current_train_items 279840.
I0304 19:32:17.518082 22849303695488 run.py:483] Algo bellman_ford step 8745 current loss 0.020713, current_train_items 279872.
I0304 19:32:17.535027 22849303695488 run.py:483] Algo bellman_ford step 8746 current loss 0.055089, current_train_items 279904.
I0304 19:32:17.559396 22849303695488 run.py:483] Algo bellman_ford step 8747 current loss 0.056493, current_train_items 279936.
I0304 19:32:17.593186 22849303695488 run.py:483] Algo bellman_ford step 8748 current loss 0.054743, current_train_items 279968.
I0304 19:32:17.628978 22849303695488 run.py:483] Algo bellman_ford step 8749 current loss 0.061770, current_train_items 280000.
I0304 19:32:17.649265 22849303695488 run.py:483] Algo bellman_ford step 8750 current loss 0.004830, current_train_items 280032.
I0304 19:32:17.657654 22849303695488 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0304 19:32:17.657761 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:17.674925 22849303695488 run.py:483] Algo bellman_ford step 8751 current loss 0.015656, current_train_items 280064.
I0304 19:32:17.699804 22849303695488 run.py:483] Algo bellman_ford step 8752 current loss 0.055877, current_train_items 280096.
I0304 19:32:17.733505 22849303695488 run.py:483] Algo bellman_ford step 8753 current loss 0.032367, current_train_items 280128.
I0304 19:32:17.768542 22849303695488 run.py:483] Algo bellman_ford step 8754 current loss 0.068168, current_train_items 280160.
I0304 19:32:17.789057 22849303695488 run.py:483] Algo bellman_ford step 8755 current loss 0.005002, current_train_items 280192.
I0304 19:32:17.805474 22849303695488 run.py:483] Algo bellman_ford step 8756 current loss 0.014243, current_train_items 280224.
I0304 19:32:17.831150 22849303695488 run.py:483] Algo bellman_ford step 8757 current loss 0.072472, current_train_items 280256.
I0304 19:32:17.863832 22849303695488 run.py:483] Algo bellman_ford step 8758 current loss 0.050803, current_train_items 280288.
I0304 19:32:17.898401 22849303695488 run.py:483] Algo bellman_ford step 8759 current loss 0.085743, current_train_items 280320.
I0304 19:32:17.919230 22849303695488 run.py:483] Algo bellman_ford step 8760 current loss 0.005442, current_train_items 280352.
I0304 19:32:17.936099 22849303695488 run.py:483] Algo bellman_ford step 8761 current loss 0.014930, current_train_items 280384.
I0304 19:32:17.961276 22849303695488 run.py:483] Algo bellman_ford step 8762 current loss 0.042349, current_train_items 280416.
I0304 19:32:17.993019 22849303695488 run.py:483] Algo bellman_ford step 8763 current loss 0.064756, current_train_items 280448.
I0304 19:32:18.029358 22849303695488 run.py:483] Algo bellman_ford step 8764 current loss 0.086824, current_train_items 280480.
I0304 19:32:18.049235 22849303695488 run.py:483] Algo bellman_ford step 8765 current loss 0.004126, current_train_items 280512.
I0304 19:32:18.066360 22849303695488 run.py:483] Algo bellman_ford step 8766 current loss 0.022581, current_train_items 280544.
I0304 19:32:18.090121 22849303695488 run.py:483] Algo bellman_ford step 8767 current loss 0.024937, current_train_items 280576.
I0304 19:32:18.122113 22849303695488 run.py:483] Algo bellman_ford step 8768 current loss 0.043315, current_train_items 280608.
I0304 19:32:18.155015 22849303695488 run.py:483] Algo bellman_ford step 8769 current loss 0.041928, current_train_items 280640.
I0304 19:32:18.175765 22849303695488 run.py:483] Algo bellman_ford step 8770 current loss 0.002155, current_train_items 280672.
I0304 19:32:18.192754 22849303695488 run.py:483] Algo bellman_ford step 8771 current loss 0.008508, current_train_items 280704.
I0304 19:32:18.217441 22849303695488 run.py:483] Algo bellman_ford step 8772 current loss 0.029664, current_train_items 280736.
I0304 19:32:18.250614 22849303695488 run.py:483] Algo bellman_ford step 8773 current loss 0.055020, current_train_items 280768.
I0304 19:32:18.284214 22849303695488 run.py:483] Algo bellman_ford step 8774 current loss 0.056408, current_train_items 280800.
I0304 19:32:18.304737 22849303695488 run.py:483] Algo bellman_ford step 8775 current loss 0.004339, current_train_items 280832.
I0304 19:32:18.321489 22849303695488 run.py:483] Algo bellman_ford step 8776 current loss 0.010005, current_train_items 280864.
I0304 19:32:18.345282 22849303695488 run.py:483] Algo bellman_ford step 8777 current loss 0.064997, current_train_items 280896.
I0304 19:32:18.377782 22849303695488 run.py:483] Algo bellman_ford step 8778 current loss 0.048252, current_train_items 280928.
I0304 19:32:18.412875 22849303695488 run.py:483] Algo bellman_ford step 8779 current loss 0.035736, current_train_items 280960.
I0304 19:32:18.432763 22849303695488 run.py:483] Algo bellman_ford step 8780 current loss 0.001360, current_train_items 280992.
I0304 19:32:18.449308 22849303695488 run.py:483] Algo bellman_ford step 8781 current loss 0.045144, current_train_items 281024.
I0304 19:32:18.474165 22849303695488 run.py:483] Algo bellman_ford step 8782 current loss 0.035084, current_train_items 281056.
I0304 19:32:18.507552 22849303695488 run.py:483] Algo bellman_ford step 8783 current loss 0.077835, current_train_items 281088.
I0304 19:32:18.542241 22849303695488 run.py:483] Algo bellman_ford step 8784 current loss 0.072305, current_train_items 281120.
I0304 19:32:18.562658 22849303695488 run.py:483] Algo bellman_ford step 8785 current loss 0.002132, current_train_items 281152.
I0304 19:32:18.579230 22849303695488 run.py:483] Algo bellman_ford step 8786 current loss 0.032558, current_train_items 281184.
I0304 19:32:18.604125 22849303695488 run.py:483] Algo bellman_ford step 8787 current loss 0.035833, current_train_items 281216.
I0304 19:32:18.636416 22849303695488 run.py:483] Algo bellman_ford step 8788 current loss 0.046337, current_train_items 281248.
I0304 19:32:18.671239 22849303695488 run.py:483] Algo bellman_ford step 8789 current loss 0.045903, current_train_items 281280.
I0304 19:32:18.691708 22849303695488 run.py:483] Algo bellman_ford step 8790 current loss 0.014705, current_train_items 281312.
I0304 19:32:18.708791 22849303695488 run.py:483] Algo bellman_ford step 8791 current loss 0.015038, current_train_items 281344.
I0304 19:32:18.732897 22849303695488 run.py:483] Algo bellman_ford step 8792 current loss 0.025978, current_train_items 281376.
I0304 19:32:18.765794 22849303695488 run.py:483] Algo bellman_ford step 8793 current loss 0.027124, current_train_items 281408.
I0304 19:32:18.801471 22849303695488 run.py:483] Algo bellman_ford step 8794 current loss 0.066340, current_train_items 281440.
I0304 19:32:18.821667 22849303695488 run.py:483] Algo bellman_ford step 8795 current loss 0.003877, current_train_items 281472.
I0304 19:32:18.838222 22849303695488 run.py:483] Algo bellman_ford step 8796 current loss 0.023351, current_train_items 281504.
I0304 19:32:18.862696 22849303695488 run.py:483] Algo bellman_ford step 8797 current loss 0.051183, current_train_items 281536.
I0304 19:32:18.895942 22849303695488 run.py:483] Algo bellman_ford step 8798 current loss 0.065240, current_train_items 281568.
I0304 19:32:18.932167 22849303695488 run.py:483] Algo bellman_ford step 8799 current loss 0.090262, current_train_items 281600.
I0304 19:32:18.952391 22849303695488 run.py:483] Algo bellman_ford step 8800 current loss 0.003343, current_train_items 281632.
I0304 19:32:18.960474 22849303695488 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0304 19:32:18.960583 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:18.977391 22849303695488 run.py:483] Algo bellman_ford step 8801 current loss 0.017765, current_train_items 281664.
I0304 19:32:19.002754 22849303695488 run.py:483] Algo bellman_ford step 8802 current loss 0.047940, current_train_items 281696.
I0304 19:32:19.036761 22849303695488 run.py:483] Algo bellman_ford step 8803 current loss 0.063954, current_train_items 281728.
I0304 19:32:19.071558 22849303695488 run.py:483] Algo bellman_ford step 8804 current loss 0.040807, current_train_items 281760.
I0304 19:32:19.092295 22849303695488 run.py:483] Algo bellman_ford step 8805 current loss 0.003639, current_train_items 281792.
I0304 19:32:19.108596 22849303695488 run.py:483] Algo bellman_ford step 8806 current loss 0.023285, current_train_items 281824.
I0304 19:32:19.133630 22849303695488 run.py:483] Algo bellman_ford step 8807 current loss 0.038467, current_train_items 281856.
I0304 19:32:19.166132 22849303695488 run.py:483] Algo bellman_ford step 8808 current loss 0.043498, current_train_items 281888.
I0304 19:32:19.203027 22849303695488 run.py:483] Algo bellman_ford step 8809 current loss 0.069139, current_train_items 281920.
I0304 19:32:19.223343 22849303695488 run.py:483] Algo bellman_ford step 8810 current loss 0.001689, current_train_items 281952.
I0304 19:32:19.240107 22849303695488 run.py:483] Algo bellman_ford step 8811 current loss 0.018615, current_train_items 281984.
I0304 19:32:19.264753 22849303695488 run.py:483] Algo bellman_ford step 8812 current loss 0.047816, current_train_items 282016.
I0304 19:32:19.297670 22849303695488 run.py:483] Algo bellman_ford step 8813 current loss 0.041354, current_train_items 282048.
I0304 19:32:19.332236 22849303695488 run.py:483] Algo bellman_ford step 8814 current loss 0.074254, current_train_items 282080.
I0304 19:32:19.352344 22849303695488 run.py:483] Algo bellman_ford step 8815 current loss 0.001620, current_train_items 282112.
I0304 19:32:19.368958 22849303695488 run.py:483] Algo bellman_ford step 8816 current loss 0.091475, current_train_items 282144.
I0304 19:32:19.393652 22849303695488 run.py:483] Algo bellman_ford step 8817 current loss 0.088526, current_train_items 282176.
I0304 19:32:19.425340 22849303695488 run.py:483] Algo bellman_ford step 8818 current loss 0.083665, current_train_items 282208.
I0304 19:32:19.462998 22849303695488 run.py:483] Algo bellman_ford step 8819 current loss 0.109724, current_train_items 282240.
I0304 19:32:19.483116 22849303695488 run.py:483] Algo bellman_ford step 8820 current loss 0.010126, current_train_items 282272.
I0304 19:32:19.499500 22849303695488 run.py:483] Algo bellman_ford step 8821 current loss 0.041589, current_train_items 282304.
I0304 19:32:19.524098 22849303695488 run.py:483] Algo bellman_ford step 8822 current loss 0.062699, current_train_items 282336.
I0304 19:32:19.557273 22849303695488 run.py:483] Algo bellman_ford step 8823 current loss 0.065888, current_train_items 282368.
I0304 19:32:19.590726 22849303695488 run.py:483] Algo bellman_ford step 8824 current loss 0.095671, current_train_items 282400.
I0304 19:32:19.610980 22849303695488 run.py:483] Algo bellman_ford step 8825 current loss 0.003664, current_train_items 282432.
I0304 19:32:19.627852 22849303695488 run.py:483] Algo bellman_ford step 8826 current loss 0.020200, current_train_items 282464.
I0304 19:32:19.652785 22849303695488 run.py:483] Algo bellman_ford step 8827 current loss 0.068715, current_train_items 282496.
I0304 19:32:19.685343 22849303695488 run.py:483] Algo bellman_ford step 8828 current loss 0.046139, current_train_items 282528.
I0304 19:32:19.719370 22849303695488 run.py:483] Algo bellman_ford step 8829 current loss 0.034525, current_train_items 282560.
I0304 19:32:19.739435 22849303695488 run.py:483] Algo bellman_ford step 8830 current loss 0.003563, current_train_items 282592.
I0304 19:32:19.755765 22849303695488 run.py:483] Algo bellman_ford step 8831 current loss 0.005410, current_train_items 282624.
I0304 19:32:19.781312 22849303695488 run.py:483] Algo bellman_ford step 8832 current loss 0.067302, current_train_items 282656.
I0304 19:32:19.812483 22849303695488 run.py:483] Algo bellman_ford step 8833 current loss 0.057260, current_train_items 282688.
I0304 19:32:19.845829 22849303695488 run.py:483] Algo bellman_ford step 8834 current loss 0.069197, current_train_items 282720.
I0304 19:32:19.866165 22849303695488 run.py:483] Algo bellman_ford step 8835 current loss 0.013366, current_train_items 282752.
I0304 19:32:19.882475 22849303695488 run.py:483] Algo bellman_ford step 8836 current loss 0.035065, current_train_items 282784.
I0304 19:32:19.907793 22849303695488 run.py:483] Algo bellman_ford step 8837 current loss 0.086310, current_train_items 282816.
I0304 19:32:19.940387 22849303695488 run.py:483] Algo bellman_ford step 8838 current loss 0.088153, current_train_items 282848.
I0304 19:32:19.974998 22849303695488 run.py:483] Algo bellman_ford step 8839 current loss 0.066882, current_train_items 282880.
I0304 19:32:19.995124 22849303695488 run.py:483] Algo bellman_ford step 8840 current loss 0.004212, current_train_items 282912.
I0304 19:32:20.011941 22849303695488 run.py:483] Algo bellman_ford step 8841 current loss 0.010774, current_train_items 282944.
I0304 19:32:20.037305 22849303695488 run.py:483] Algo bellman_ford step 8842 current loss 0.072060, current_train_items 282976.
I0304 19:32:20.069781 22849303695488 run.py:483] Algo bellman_ford step 8843 current loss 0.053346, current_train_items 283008.
I0304 19:32:20.105544 22849303695488 run.py:483] Algo bellman_ford step 8844 current loss 0.080050, current_train_items 283040.
I0304 19:32:20.125910 22849303695488 run.py:483] Algo bellman_ford step 8845 current loss 0.007817, current_train_items 283072.
I0304 19:32:20.142295 22849303695488 run.py:483] Algo bellman_ford step 8846 current loss 0.017934, current_train_items 283104.
I0304 19:32:20.167469 22849303695488 run.py:483] Algo bellman_ford step 8847 current loss 0.066548, current_train_items 283136.
I0304 19:32:20.198470 22849303695488 run.py:483] Algo bellman_ford step 8848 current loss 0.072489, current_train_items 283168.
I0304 19:32:20.232913 22849303695488 run.py:483] Algo bellman_ford step 8849 current loss 0.092068, current_train_items 283200.
I0304 19:32:20.253066 22849303695488 run.py:483] Algo bellman_ford step 8850 current loss 0.001867, current_train_items 283232.
I0304 19:32:20.261409 22849303695488 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0304 19:32:20.261517 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:32:20.279024 22849303695488 run.py:483] Algo bellman_ford step 8851 current loss 0.018967, current_train_items 283264.
I0304 19:32:20.304641 22849303695488 run.py:483] Algo bellman_ford step 8852 current loss 0.045059, current_train_items 283296.
I0304 19:32:20.338215 22849303695488 run.py:483] Algo bellman_ford step 8853 current loss 0.092530, current_train_items 283328.
I0304 19:32:20.373205 22849303695488 run.py:483] Algo bellman_ford step 8854 current loss 0.102175, current_train_items 283360.
I0304 19:32:20.393798 22849303695488 run.py:483] Algo bellman_ford step 8855 current loss 0.002239, current_train_items 283392.
I0304 19:32:20.410289 22849303695488 run.py:483] Algo bellman_ford step 8856 current loss 0.048563, current_train_items 283424.
I0304 19:32:20.434776 22849303695488 run.py:483] Algo bellman_ford step 8857 current loss 0.039969, current_train_items 283456.
I0304 19:32:20.468023 22849303695488 run.py:483] Algo bellman_ford step 8858 current loss 0.033741, current_train_items 283488.
I0304 19:32:20.500922 22849303695488 run.py:483] Algo bellman_ford step 8859 current loss 0.046346, current_train_items 283520.
I0304 19:32:20.521470 22849303695488 run.py:483] Algo bellman_ford step 8860 current loss 0.002202, current_train_items 283552.
I0304 19:32:20.538371 22849303695488 run.py:483] Algo bellman_ford step 8861 current loss 0.006346, current_train_items 283584.
I0304 19:32:20.563599 22849303695488 run.py:483] Algo bellman_ford step 8862 current loss 0.043520, current_train_items 283616.
I0304 19:32:20.595475 22849303695488 run.py:483] Algo bellman_ford step 8863 current loss 0.024264, current_train_items 283648.
I0304 19:32:20.628726 22849303695488 run.py:483] Algo bellman_ford step 8864 current loss 0.057969, current_train_items 283680.
I0304 19:32:20.649225 22849303695488 run.py:483] Algo bellman_ford step 8865 current loss 0.013825, current_train_items 283712.
I0304 19:32:20.665850 22849303695488 run.py:483] Algo bellman_ford step 8866 current loss 0.017909, current_train_items 283744.
I0304 19:32:20.689624 22849303695488 run.py:483] Algo bellman_ford step 8867 current loss 0.077023, current_train_items 283776.
I0304 19:32:20.722893 22849303695488 run.py:483] Algo bellman_ford step 8868 current loss 0.129856, current_train_items 283808.
I0304 19:32:20.756711 22849303695488 run.py:483] Algo bellman_ford step 8869 current loss 0.075709, current_train_items 283840.
I0304 19:32:20.777233 22849303695488 run.py:483] Algo bellman_ford step 8870 current loss 0.007368, current_train_items 283872.
I0304 19:32:20.793826 22849303695488 run.py:483] Algo bellman_ford step 8871 current loss 0.003687, current_train_items 283904.
I0304 19:32:20.819064 22849303695488 run.py:483] Algo bellman_ford step 8872 current loss 0.035143, current_train_items 283936.
I0304 19:32:20.851281 22849303695488 run.py:483] Algo bellman_ford step 8873 current loss 0.032769, current_train_items 283968.
I0304 19:32:20.886873 22849303695488 run.py:483] Algo bellman_ford step 8874 current loss 0.189846, current_train_items 284000.
I0304 19:32:20.907436 22849303695488 run.py:483] Algo bellman_ford step 8875 current loss 0.002887, current_train_items 284032.
I0304 19:32:20.924283 22849303695488 run.py:483] Algo bellman_ford step 8876 current loss 0.034050, current_train_items 284064.
I0304 19:32:20.948279 22849303695488 run.py:483] Algo bellman_ford step 8877 current loss 0.038997, current_train_items 284096.
I0304 19:32:20.980564 22849303695488 run.py:483] Algo bellman_ford step 8878 current loss 0.038225, current_train_items 284128.
I0304 19:32:21.015686 22849303695488 run.py:483] Algo bellman_ford step 8879 current loss 0.105089, current_train_items 284160.
I0304 19:32:21.036031 22849303695488 run.py:483] Algo bellman_ford step 8880 current loss 0.004161, current_train_items 284192.
I0304 19:32:21.052569 22849303695488 run.py:483] Algo bellman_ford step 8881 current loss 0.036377, current_train_items 284224.
I0304 19:32:21.076479 22849303695488 run.py:483] Algo bellman_ford step 8882 current loss 0.044669, current_train_items 284256.
I0304 19:32:21.110330 22849303695488 run.py:483] Algo bellman_ford step 8883 current loss 0.042875, current_train_items 284288.
I0304 19:32:21.144918 22849303695488 run.py:483] Algo bellman_ford step 8884 current loss 0.053935, current_train_items 284320.
I0304 19:32:21.165457 22849303695488 run.py:483] Algo bellman_ford step 8885 current loss 0.006999, current_train_items 284352.
I0304 19:32:21.182576 22849303695488 run.py:483] Algo bellman_ford step 8886 current loss 0.050054, current_train_items 284384.
I0304 19:32:21.207555 22849303695488 run.py:483] Algo bellman_ford step 8887 current loss 0.038947, current_train_items 284416.
I0304 19:32:21.239616 22849303695488 run.py:483] Algo bellman_ford step 8888 current loss 0.044797, current_train_items 284448.
I0304 19:32:21.274869 22849303695488 run.py:483] Algo bellman_ford step 8889 current loss 0.051052, current_train_items 284480.
I0304 19:32:21.295405 22849303695488 run.py:483] Algo bellman_ford step 8890 current loss 0.001446, current_train_items 284512.
I0304 19:32:21.311690 22849303695488 run.py:483] Algo bellman_ford step 8891 current loss 0.062116, current_train_items 284544.
I0304 19:32:21.336073 22849303695488 run.py:483] Algo bellman_ford step 8892 current loss 0.039814, current_train_items 284576.
I0304 19:32:21.370435 22849303695488 run.py:483] Algo bellman_ford step 8893 current loss 0.057981, current_train_items 284608.
I0304 19:32:21.404921 22849303695488 run.py:483] Algo bellman_ford step 8894 current loss 0.096588, current_train_items 284640.
I0304 19:32:21.424947 22849303695488 run.py:483] Algo bellman_ford step 8895 current loss 0.002635, current_train_items 284672.
I0304 19:32:21.441423 22849303695488 run.py:483] Algo bellman_ford step 8896 current loss 0.012309, current_train_items 284704.
I0304 19:32:21.465173 22849303695488 run.py:483] Algo bellman_ford step 8897 current loss 0.048587, current_train_items 284736.
I0304 19:32:21.499000 22849303695488 run.py:483] Algo bellman_ford step 8898 current loss 0.098344, current_train_items 284768.
I0304 19:32:21.535166 22849303695488 run.py:483] Algo bellman_ford step 8899 current loss 0.059196, current_train_items 284800.
I0304 19:32:21.555761 22849303695488 run.py:483] Algo bellman_ford step 8900 current loss 0.001930, current_train_items 284832.
I0304 19:32:21.563596 22849303695488 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0304 19:32:21.563706 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:32:21.580673 22849303695488 run.py:483] Algo bellman_ford step 8901 current loss 0.019591, current_train_items 284864.
I0304 19:32:21.606316 22849303695488 run.py:483] Algo bellman_ford step 8902 current loss 0.021085, current_train_items 284896.
I0304 19:32:21.638100 22849303695488 run.py:483] Algo bellman_ford step 8903 current loss 0.072589, current_train_items 284928.
I0304 19:32:21.674440 22849303695488 run.py:483] Algo bellman_ford step 8904 current loss 0.062630, current_train_items 284960.
I0304 19:32:21.694986 22849303695488 run.py:483] Algo bellman_ford step 8905 current loss 0.005305, current_train_items 284992.
I0304 19:32:21.711445 22849303695488 run.py:483] Algo bellman_ford step 8906 current loss 0.026770, current_train_items 285024.
I0304 19:32:21.736351 22849303695488 run.py:483] Algo bellman_ford step 8907 current loss 0.028925, current_train_items 285056.
I0304 19:32:21.768575 22849303695488 run.py:483] Algo bellman_ford step 8908 current loss 0.050862, current_train_items 285088.
I0304 19:32:21.801350 22849303695488 run.py:483] Algo bellman_ford step 8909 current loss 0.055129, current_train_items 285120.
I0304 19:32:21.821688 22849303695488 run.py:483] Algo bellman_ford step 8910 current loss 0.004498, current_train_items 285152.
I0304 19:32:21.838079 22849303695488 run.py:483] Algo bellman_ford step 8911 current loss 0.025291, current_train_items 285184.
I0304 19:32:21.863372 22849303695488 run.py:483] Algo bellman_ford step 8912 current loss 0.044447, current_train_items 285216.
I0304 19:32:21.895055 22849303695488 run.py:483] Algo bellman_ford step 8913 current loss 0.047443, current_train_items 285248.
I0304 19:32:21.931116 22849303695488 run.py:483] Algo bellman_ford step 8914 current loss 0.155861, current_train_items 285280.
I0304 19:32:21.951579 22849303695488 run.py:483] Algo bellman_ford step 8915 current loss 0.002325, current_train_items 285312.
I0304 19:32:21.967765 22849303695488 run.py:483] Algo bellman_ford step 8916 current loss 0.010389, current_train_items 285344.
I0304 19:32:21.993122 22849303695488 run.py:483] Algo bellman_ford step 8917 current loss 0.053215, current_train_items 285376.
I0304 19:32:22.025766 22849303695488 run.py:483] Algo bellman_ford step 8918 current loss 0.022279, current_train_items 285408.
I0304 19:32:22.059639 22849303695488 run.py:483] Algo bellman_ford step 8919 current loss 0.045642, current_train_items 285440.
I0304 19:32:22.079906 22849303695488 run.py:483] Algo bellman_ford step 8920 current loss 0.001933, current_train_items 285472.
I0304 19:32:22.096647 22849303695488 run.py:483] Algo bellman_ford step 8921 current loss 0.074372, current_train_items 285504.
I0304 19:32:22.122053 22849303695488 run.py:483] Algo bellman_ford step 8922 current loss 0.078853, current_train_items 285536.
I0304 19:32:22.154205 22849303695488 run.py:483] Algo bellman_ford step 8923 current loss 0.084803, current_train_items 285568.
I0304 19:32:22.189073 22849303695488 run.py:483] Algo bellman_ford step 8924 current loss 0.106052, current_train_items 285600.
I0304 19:32:22.209253 22849303695488 run.py:483] Algo bellman_ford step 8925 current loss 0.001468, current_train_items 285632.
I0304 19:32:22.225424 22849303695488 run.py:483] Algo bellman_ford step 8926 current loss 0.023747, current_train_items 285664.
I0304 19:32:22.250903 22849303695488 run.py:483] Algo bellman_ford step 8927 current loss 0.060535, current_train_items 285696.
I0304 19:32:22.282227 22849303695488 run.py:483] Algo bellman_ford step 8928 current loss 0.027006, current_train_items 285728.
I0304 19:32:22.316979 22849303695488 run.py:483] Algo bellman_ford step 8929 current loss 0.083674, current_train_items 285760.
I0304 19:32:22.337324 22849303695488 run.py:483] Algo bellman_ford step 8930 current loss 0.001997, current_train_items 285792.
I0304 19:32:22.354528 22849303695488 run.py:483] Algo bellman_ford step 8931 current loss 0.018302, current_train_items 285824.
I0304 19:32:22.379292 22849303695488 run.py:483] Algo bellman_ford step 8932 current loss 0.034884, current_train_items 285856.
I0304 19:32:22.413336 22849303695488 run.py:483] Algo bellman_ford step 8933 current loss 0.104384, current_train_items 285888.
I0304 19:32:22.448562 22849303695488 run.py:483] Algo bellman_ford step 8934 current loss 0.070170, current_train_items 285920.
I0304 19:32:22.468768 22849303695488 run.py:483] Algo bellman_ford step 8935 current loss 0.002701, current_train_items 285952.
I0304 19:32:22.485070 22849303695488 run.py:483] Algo bellman_ford step 8936 current loss 0.025920, current_train_items 285984.
I0304 19:32:22.509916 22849303695488 run.py:483] Algo bellman_ford step 8937 current loss 0.073598, current_train_items 286016.
I0304 19:32:22.541705 22849303695488 run.py:483] Algo bellman_ford step 8938 current loss 0.050742, current_train_items 286048.
I0304 19:32:22.575010 22849303695488 run.py:483] Algo bellman_ford step 8939 current loss 0.071056, current_train_items 286080.
I0304 19:32:22.595199 22849303695488 run.py:483] Algo bellman_ford step 8940 current loss 0.009119, current_train_items 286112.
I0304 19:32:22.611657 22849303695488 run.py:483] Algo bellman_ford step 8941 current loss 0.004589, current_train_items 286144.
I0304 19:32:22.636641 22849303695488 run.py:483] Algo bellman_ford step 8942 current loss 0.027415, current_train_items 286176.
I0304 19:32:22.670158 22849303695488 run.py:483] Algo bellman_ford step 8943 current loss 0.050982, current_train_items 286208.
I0304 19:32:22.704936 22849303695488 run.py:483] Algo bellman_ford step 8944 current loss 0.051788, current_train_items 286240.
I0304 19:32:22.725050 22849303695488 run.py:483] Algo bellman_ford step 8945 current loss 0.001692, current_train_items 286272.
I0304 19:32:22.741598 22849303695488 run.py:483] Algo bellman_ford step 8946 current loss 0.038085, current_train_items 286304.
I0304 19:32:22.765793 22849303695488 run.py:483] Algo bellman_ford step 8947 current loss 0.030693, current_train_items 286336.
I0304 19:32:22.798995 22849303695488 run.py:483] Algo bellman_ford step 8948 current loss 0.063770, current_train_items 286368.
I0304 19:32:22.833140 22849303695488 run.py:483] Algo bellman_ford step 8949 current loss 0.043135, current_train_items 286400.
I0304 19:32:22.853210 22849303695488 run.py:483] Algo bellman_ford step 8950 current loss 0.002040, current_train_items 286432.
I0304 19:32:22.861450 22849303695488 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0304 19:32:22.861559 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:32:22.879358 22849303695488 run.py:483] Algo bellman_ford step 8951 current loss 0.029622, current_train_items 286464.
I0304 19:32:22.904047 22849303695488 run.py:483] Algo bellman_ford step 8952 current loss 0.037181, current_train_items 286496.
I0304 19:32:22.939355 22849303695488 run.py:483] Algo bellman_ford step 8953 current loss 0.054225, current_train_items 286528.
I0304 19:32:22.975084 22849303695488 run.py:483] Algo bellman_ford step 8954 current loss 0.044393, current_train_items 286560.
I0304 19:32:22.995390 22849303695488 run.py:483] Algo bellman_ford step 8955 current loss 0.002146, current_train_items 286592.
I0304 19:32:23.012183 22849303695488 run.py:483] Algo bellman_ford step 8956 current loss 0.016141, current_train_items 286624.
I0304 19:32:23.037240 22849303695488 run.py:483] Algo bellman_ford step 8957 current loss 0.070161, current_train_items 286656.
I0304 19:32:23.069229 22849303695488 run.py:483] Algo bellman_ford step 8958 current loss 0.070546, current_train_items 286688.
I0304 19:32:23.104471 22849303695488 run.py:483] Algo bellman_ford step 8959 current loss 0.121066, current_train_items 286720.
I0304 19:32:23.124964 22849303695488 run.py:483] Algo bellman_ford step 8960 current loss 0.002951, current_train_items 286752.
I0304 19:32:23.141196 22849303695488 run.py:483] Algo bellman_ford step 8961 current loss 0.014215, current_train_items 286784.
I0304 19:32:23.166313 22849303695488 run.py:483] Algo bellman_ford step 8962 current loss 0.038707, current_train_items 286816.
I0304 19:32:23.198946 22849303695488 run.py:483] Algo bellman_ford step 8963 current loss 0.036774, current_train_items 286848.
I0304 19:32:23.231792 22849303695488 run.py:483] Algo bellman_ford step 8964 current loss 0.057759, current_train_items 286880.
I0304 19:32:23.251744 22849303695488 run.py:483] Algo bellman_ford step 8965 current loss 0.002979, current_train_items 286912.
I0304 19:32:23.268494 22849303695488 run.py:483] Algo bellman_ford step 8966 current loss 0.011099, current_train_items 286944.
I0304 19:32:23.293170 22849303695488 run.py:483] Algo bellman_ford step 8967 current loss 0.027637, current_train_items 286976.
I0304 19:32:23.325778 22849303695488 run.py:483] Algo bellman_ford step 8968 current loss 0.082146, current_train_items 287008.
I0304 19:32:23.359952 22849303695488 run.py:483] Algo bellman_ford step 8969 current loss 0.059650, current_train_items 287040.
I0304 19:32:23.380092 22849303695488 run.py:483] Algo bellman_ford step 8970 current loss 0.003659, current_train_items 287072.
I0304 19:32:23.396723 22849303695488 run.py:483] Algo bellman_ford step 8971 current loss 0.004626, current_train_items 287104.
I0304 19:32:23.420860 22849303695488 run.py:483] Algo bellman_ford step 8972 current loss 0.024120, current_train_items 287136.
I0304 19:32:23.452970 22849303695488 run.py:483] Algo bellman_ford step 8973 current loss 0.023110, current_train_items 287168.
I0304 19:32:23.486680 22849303695488 run.py:483] Algo bellman_ford step 8974 current loss 0.055119, current_train_items 287200.
I0304 19:32:23.507035 22849303695488 run.py:483] Algo bellman_ford step 8975 current loss 0.011493, current_train_items 287232.
I0304 19:32:23.523406 22849303695488 run.py:483] Algo bellman_ford step 8976 current loss 0.006265, current_train_items 287264.
I0304 19:32:23.546743 22849303695488 run.py:483] Algo bellman_ford step 8977 current loss 0.021642, current_train_items 287296.
I0304 19:32:23.579271 22849303695488 run.py:483] Algo bellman_ford step 8978 current loss 0.034112, current_train_items 287328.
I0304 19:32:23.612918 22849303695488 run.py:483] Algo bellman_ford step 8979 current loss 0.046749, current_train_items 287360.
I0304 19:32:23.632768 22849303695488 run.py:483] Algo bellman_ford step 8980 current loss 0.002456, current_train_items 287392.
I0304 19:32:23.649547 22849303695488 run.py:483] Algo bellman_ford step 8981 current loss 0.013482, current_train_items 287424.
I0304 19:32:23.674035 22849303695488 run.py:483] Algo bellman_ford step 8982 current loss 0.018619, current_train_items 287456.
I0304 19:32:23.706806 22849303695488 run.py:483] Algo bellman_ford step 8983 current loss 0.034614, current_train_items 287488.
I0304 19:32:23.741155 22849303695488 run.py:483] Algo bellman_ford step 8984 current loss 0.047826, current_train_items 287520.
I0304 19:32:23.761387 22849303695488 run.py:483] Algo bellman_ford step 8985 current loss 0.035886, current_train_items 287552.
I0304 19:32:23.778024 22849303695488 run.py:483] Algo bellman_ford step 8986 current loss 0.010863, current_train_items 287584.
I0304 19:32:23.801564 22849303695488 run.py:483] Algo bellman_ford step 8987 current loss 0.027183, current_train_items 287616.
I0304 19:32:23.832485 22849303695488 run.py:483] Algo bellman_ford step 8988 current loss 0.034171, current_train_items 287648.
I0304 19:32:23.868391 22849303695488 run.py:483] Algo bellman_ford step 8989 current loss 0.060724, current_train_items 287680.
I0304 19:32:23.888406 22849303695488 run.py:483] Algo bellman_ford step 8990 current loss 0.014557, current_train_items 287712.
I0304 19:32:23.904964 22849303695488 run.py:483] Algo bellman_ford step 8991 current loss 0.008879, current_train_items 287744.
I0304 19:32:23.928974 22849303695488 run.py:483] Algo bellman_ford step 8992 current loss 0.046865, current_train_items 287776.
I0304 19:32:23.961016 22849303695488 run.py:483] Algo bellman_ford step 8993 current loss 0.074198, current_train_items 287808.
I0304 19:32:23.994844 22849303695488 run.py:483] Algo bellman_ford step 8994 current loss 0.085660, current_train_items 287840.
I0304 19:32:24.014370 22849303695488 run.py:483] Algo bellman_ford step 8995 current loss 0.038963, current_train_items 287872.
I0304 19:32:24.030955 22849303695488 run.py:483] Algo bellman_ford step 8996 current loss 0.038591, current_train_items 287904.
I0304 19:32:24.056044 22849303695488 run.py:483] Algo bellman_ford step 8997 current loss 0.076364, current_train_items 287936.
I0304 19:32:24.089962 22849303695488 run.py:483] Algo bellman_ford step 8998 current loss 0.092526, current_train_items 287968.
I0304 19:32:24.122869 22849303695488 run.py:483] Algo bellman_ford step 8999 current loss 0.052901, current_train_items 288000.
I0304 19:32:24.143265 22849303695488 run.py:483] Algo bellman_ford step 9000 current loss 0.003523, current_train_items 288032.
I0304 19:32:24.151064 22849303695488 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0304 19:32:24.151175 22849303695488 run.py:519] Checkpointing best model, best avg val score was 0.992, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0304 19:32:24.181773 22849303695488 run.py:483] Algo bellman_ford step 9001 current loss 0.021623, current_train_items 288064.
I0304 19:32:24.205970 22849303695488 run.py:483] Algo bellman_ford step 9002 current loss 0.031393, current_train_items 288096.
I0304 19:32:24.238021 22849303695488 run.py:483] Algo bellman_ford step 9003 current loss 0.059017, current_train_items 288128.
I0304 19:32:24.273364 22849303695488 run.py:483] Algo bellman_ford step 9004 current loss 0.058092, current_train_items 288160.
I0304 19:32:24.294035 22849303695488 run.py:483] Algo bellman_ford step 9005 current loss 0.002192, current_train_items 288192.
I0304 19:32:24.310662 22849303695488 run.py:483] Algo bellman_ford step 9006 current loss 0.005649, current_train_items 288224.
I0304 19:32:24.334765 22849303695488 run.py:483] Algo bellman_ford step 9007 current loss 0.035959, current_train_items 288256.
I0304 19:32:24.368028 22849303695488 run.py:483] Algo bellman_ford step 9008 current loss 0.047830, current_train_items 288288.
I0304 19:32:24.402900 22849303695488 run.py:483] Algo bellman_ford step 9009 current loss 0.069662, current_train_items 288320.
I0304 19:32:24.422951 22849303695488 run.py:483] Algo bellman_ford step 9010 current loss 0.044772, current_train_items 288352.
I0304 19:32:24.439897 22849303695488 run.py:483] Algo bellman_ford step 9011 current loss 0.020797, current_train_items 288384.
I0304 19:32:24.463879 22849303695488 run.py:483] Algo bellman_ford step 9012 current loss 0.018168, current_train_items 288416.
I0304 19:32:24.495581 22849303695488 run.py:483] Algo bellman_ford step 9013 current loss 0.051708, current_train_items 288448.
I0304 19:32:24.531804 22849303695488 run.py:483] Algo bellman_ford step 9014 current loss 0.077717, current_train_items 288480.
I0304 19:32:24.551993 22849303695488 run.py:483] Algo bellman_ford step 9015 current loss 0.001680, current_train_items 288512.
I0304 19:32:24.568715 22849303695488 run.py:483] Algo bellman_ford step 9016 current loss 0.049178, current_train_items 288544.
I0304 19:32:24.593973 22849303695488 run.py:483] Algo bellman_ford step 9017 current loss 0.044689, current_train_items 288576.
I0304 19:32:24.626407 22849303695488 run.py:483] Algo bellman_ford step 9018 current loss 0.063991, current_train_items 288608.
I0304 19:32:24.662070 22849303695488 run.py:483] Algo bellman_ford step 9019 current loss 0.075566, current_train_items 288640.
I0304 19:32:24.681788 22849303695488 run.py:483] Algo bellman_ford step 9020 current loss 0.002475, current_train_items 288672.
I0304 19:32:24.698396 22849303695488 run.py:483] Algo bellman_ford step 9021 current loss 0.019465, current_train_items 288704.
I0304 19:32:24.724111 22849303695488 run.py:483] Algo bellman_ford step 9022 current loss 0.051258, current_train_items 288736.
I0304 19:32:24.757518 22849303695488 run.py:483] Algo bellman_ford step 9023 current loss 0.072495, current_train_items 288768.
I0304 19:32:24.792303 22849303695488 run.py:483] Algo bellman_ford step 9024 current loss 0.080593, current_train_items 288800.
I0304 19:32:24.812254 22849303695488 run.py:483] Algo bellman_ford step 9025 current loss 0.004080, current_train_items 288832.
I0304 19:32:24.828511 22849303695488 run.py:483] Algo bellman_ford step 9026 current loss 0.049153, current_train_items 288864.
I0304 19:32:24.853518 22849303695488 run.py:483] Algo bellman_ford step 9027 current loss 0.069281, current_train_items 288896.
I0304 19:32:24.886493 22849303695488 run.py:483] Algo bellman_ford step 9028 current loss 0.052864, current_train_items 288928.
I0304 19:32:24.920876 22849303695488 run.py:483] Algo bellman_ford step 9029 current loss 0.090919, current_train_items 288960.
I0304 19:32:24.941594 22849303695488 run.py:483] Algo bellman_ford step 9030 current loss 0.002717, current_train_items 288992.
I0304 19:32:24.958075 22849303695488 run.py:483] Algo bellman_ford step 9031 current loss 0.063836, current_train_items 289024.
I0304 19:32:24.983354 22849303695488 run.py:483] Algo bellman_ford step 9032 current loss 0.116814, current_train_items 289056.
I0304 19:32:25.016176 22849303695488 run.py:483] Algo bellman_ford step 9033 current loss 0.119643, current_train_items 289088.
I0304 19:32:25.053013 22849303695488 run.py:483] Algo bellman_ford step 9034 current loss 0.241701, current_train_items 289120.
I0304 19:32:25.072918 22849303695488 run.py:483] Algo bellman_ford step 9035 current loss 0.006411, current_train_items 289152.
I0304 19:32:25.089143 22849303695488 run.py:483] Algo bellman_ford step 9036 current loss 0.015195, current_train_items 289184.
I0304 19:32:25.114635 22849303695488 run.py:483] Algo bellman_ford step 9037 current loss 0.037684, current_train_items 289216.
I0304 19:32:25.147385 22849303695488 run.py:483] Algo bellman_ford step 9038 current loss 0.060694, current_train_items 289248.
I0304 19:32:25.181535 22849303695488 run.py:483] Algo bellman_ford step 9039 current loss 0.132808, current_train_items 289280.
I0304 19:32:25.201591 22849303695488 run.py:483] Algo bellman_ford step 9040 current loss 0.011500, current_train_items 289312.
I0304 19:32:25.218425 22849303695488 run.py:483] Algo bellman_ford step 9041 current loss 0.025892, current_train_items 289344.
I0304 19:32:25.242399 22849303695488 run.py:483] Algo bellman_ford step 9042 current loss 0.060159, current_train_items 289376.
I0304 19:32:25.274743 22849303695488 run.py:483] Algo bellman_ford step 9043 current loss 0.055586, current_train_items 289408.
I0304 19:32:25.309815 22849303695488 run.py:483] Algo bellman_ford step 9044 current loss 0.050070, current_train_items 289440.
I0304 19:32:25.329826 22849303695488 run.py:483] Algo bellman_ford step 9045 current loss 0.005649, current_train_items 289472.
I0304 19:32:25.346624 22849303695488 run.py:483] Algo bellman_ford step 9046 current loss 0.028139, current_train_items 289504.
I0304 19:32:25.370086 22849303695488 run.py:483] Algo bellman_ford step 9047 current loss 0.028259, current_train_items 289536.
I0304 19:32:25.403021 22849303695488 run.py:483] Algo bellman_ford step 9048 current loss 0.075489, current_train_items 289568.
I0304 19:32:25.437183 22849303695488 run.py:483] Algo bellman_ford step 9049 current loss 0.075831, current_train_items 289600.
I0304 19:32:25.456918 22849303695488 run.py:483] Algo bellman_ford step 9050 current loss 0.005621, current_train_items 289632.
I0304 19:32:25.465357 22849303695488 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0304 19:32:25.465466 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:32:25.482597 22849303695488 run.py:483] Algo bellman_ford step 9051 current loss 0.039083, current_train_items 289664.
I0304 19:32:25.508209 22849303695488 run.py:483] Algo bellman_ford step 9052 current loss 0.101351, current_train_items 289696.
I0304 19:32:25.541380 22849303695488 run.py:483] Algo bellman_ford step 9053 current loss 0.151164, current_train_items 289728.
I0304 19:32:25.574967 22849303695488 run.py:483] Algo bellman_ford step 9054 current loss 0.029804, current_train_items 289760.
I0304 19:32:25.595309 22849303695488 run.py:483] Algo bellman_ford step 9055 current loss 0.002615, current_train_items 289792.
I0304 19:32:25.611499 22849303695488 run.py:483] Algo bellman_ford step 9056 current loss 0.021944, current_train_items 289824.
I0304 19:32:25.635645 22849303695488 run.py:483] Algo bellman_ford step 9057 current loss 0.030907, current_train_items 289856.
I0304 19:32:25.666819 22849303695488 run.py:483] Algo bellman_ford step 9058 current loss 0.034611, current_train_items 289888.
I0304 19:32:25.699312 22849303695488 run.py:483] Algo bellman_ford step 9059 current loss 0.045535, current_train_items 289920.
I0304 19:32:25.719793 22849303695488 run.py:483] Algo bellman_ford step 9060 current loss 0.010410, current_train_items 289952.
I0304 19:32:25.736968 22849303695488 run.py:483] Algo bellman_ford step 9061 current loss 0.015628, current_train_items 289984.
I0304 19:32:25.762342 22849303695488 run.py:483] Algo bellman_ford step 9062 current loss 0.039846, current_train_items 290016.
I0304 19:32:25.793510 22849303695488 run.py:483] Algo bellman_ford step 9063 current loss 0.036077, current_train_items 290048.
I0304 19:32:25.828967 22849303695488 run.py:483] Algo bellman_ford step 9064 current loss 0.058990, current_train_items 290080.
I0304 19:32:25.849102 22849303695488 run.py:483] Algo bellman_ford step 9065 current loss 0.001612, current_train_items 290112.
I0304 19:32:25.865765 22849303695488 run.py:483] Algo bellman_ford step 9066 current loss 0.032327, current_train_items 290144.
I0304 19:32:25.888626 22849303695488 run.py:483] Algo bellman_ford step 9067 current loss 0.034021, current_train_items 290176.
I0304 19:32:25.921163 22849303695488 run.py:483] Algo bellman_ford step 9068 current loss 0.032443, current_train_items 290208.
I0304 19:32:25.956403 22849303695488 run.py:483] Algo bellman_ford step 9069 current loss 0.075902, current_train_items 290240.
I0304 19:32:25.976576 22849303695488 run.py:483] Algo bellman_ford step 9070 current loss 0.001579, current_train_items 290272.
I0304 19:32:25.993428 22849303695488 run.py:483] Algo bellman_ford step 9071 current loss 0.045286, current_train_items 290304.
I0304 19:32:26.018699 22849303695488 run.py:483] Algo bellman_ford step 9072 current loss 0.035500, current_train_items 290336.
I0304 19:32:26.051589 22849303695488 run.py:483] Algo bellman_ford step 9073 current loss 0.047124, current_train_items 290368.
I0304 19:32:26.085599 22849303695488 run.py:483] Algo bellman_ford step 9074 current loss 0.057680, current_train_items 290400.
I0304 19:32:26.105600 22849303695488 run.py:483] Algo bellman_ford step 9075 current loss 0.007694, current_train_items 290432.
I0304 19:32:26.122393 22849303695488 run.py:483] Algo bellman_ford step 9076 current loss 0.012011, current_train_items 290464.
I0304 19:32:26.146955 22849303695488 run.py:483] Algo bellman_ford step 9077 current loss 0.037390, current_train_items 290496.
I0304 19:32:26.179150 22849303695488 run.py:483] Algo bellman_ford step 9078 current loss 0.034898, current_train_items 290528.
I0304 19:32:26.214137 22849303695488 run.py:483] Algo bellman_ford step 9079 current loss 0.061153, current_train_items 290560.
I0304 19:32:26.234499 22849303695488 run.py:483] Algo bellman_ford step 9080 current loss 0.001866, current_train_items 290592.
I0304 19:32:26.250987 22849303695488 run.py:483] Algo bellman_ford step 9081 current loss 0.008757, current_train_items 290624.
I0304 19:32:26.275347 22849303695488 run.py:483] Algo bellman_ford step 9082 current loss 0.015103, current_train_items 290656.
I0304 19:32:26.306955 22849303695488 run.py:483] Algo bellman_ford step 9083 current loss 0.043141, current_train_items 290688.
I0304 19:32:26.340293 22849303695488 run.py:483] Algo bellman_ford step 9084 current loss 0.053601, current_train_items 290720.
I0304 19:32:26.360564 22849303695488 run.py:483] Algo bellman_ford step 9085 current loss 0.002734, current_train_items 290752.
I0304 19:32:26.377240 22849303695488 run.py:483] Algo bellman_ford step 9086 current loss 0.010206, current_train_items 290784.
I0304 19:32:26.401878 22849303695488 run.py:483] Algo bellman_ford step 9087 current loss 0.066890, current_train_items 290816.
I0304 19:32:26.433628 22849303695488 run.py:483] Algo bellman_ford step 9088 current loss 0.022452, current_train_items 290848.
I0304 19:32:26.469393 22849303695488 run.py:483] Algo bellman_ford step 9089 current loss 0.088526, current_train_items 290880.
I0304 19:32:26.489470 22849303695488 run.py:483] Algo bellman_ford step 9090 current loss 0.001858, current_train_items 290912.
I0304 19:32:26.505954 22849303695488 run.py:483] Algo bellman_ford step 9091 current loss 0.049072, current_train_items 290944.
I0304 19:32:26.529920 22849303695488 run.py:483] Algo bellman_ford step 9092 current loss 0.105680, current_train_items 290976.
I0304 19:32:26.560839 22849303695488 run.py:483] Algo bellman_ford step 9093 current loss 0.086333, current_train_items 291008.
I0304 19:32:26.597332 22849303695488 run.py:483] Algo bellman_ford step 9094 current loss 0.105678, current_train_items 291040.
I0304 19:32:26.617520 22849303695488 run.py:483] Algo bellman_ford step 9095 current loss 0.008090, current_train_items 291072.
I0304 19:32:26.633968 22849303695488 run.py:483] Algo bellman_ford step 9096 current loss 0.014056, current_train_items 291104.
I0304 19:32:26.658169 22849303695488 run.py:483] Algo bellman_ford step 9097 current loss 0.075051, current_train_items 291136.
I0304 19:32:26.690916 22849303695488 run.py:483] Algo bellman_ford step 9098 current loss 0.106315, current_train_items 291168.
I0304 19:32:26.724293 22849303695488 run.py:483] Algo bellman_ford step 9099 current loss 0.061337, current_train_items 291200.
I0304 19:32:26.744474 22849303695488 run.py:483] Algo bellman_ford step 9100 current loss 0.002082, current_train_items 291232.
I0304 19:32:26.752507 22849303695488 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0304 19:32:26.752615 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:26.769716 22849303695488 run.py:483] Algo bellman_ford step 9101 current loss 0.005259, current_train_items 291264.
I0304 19:32:26.795975 22849303695488 run.py:483] Algo bellman_ford step 9102 current loss 0.059109, current_train_items 291296.
I0304 19:32:26.829319 22849303695488 run.py:483] Algo bellman_ford step 9103 current loss 0.069615, current_train_items 291328.
I0304 19:32:26.865680 22849303695488 run.py:483] Algo bellman_ford step 9104 current loss 0.085142, current_train_items 291360.
I0304 19:32:26.886255 22849303695488 run.py:483] Algo bellman_ford step 9105 current loss 0.027501, current_train_items 291392.
I0304 19:32:26.902865 22849303695488 run.py:483] Algo bellman_ford step 9106 current loss 0.014721, current_train_items 291424.
I0304 19:32:26.927941 22849303695488 run.py:483] Algo bellman_ford step 9107 current loss 0.055268, current_train_items 291456.
I0304 19:32:26.961128 22849303695488 run.py:483] Algo bellman_ford step 9108 current loss 0.070429, current_train_items 291488.
I0304 19:32:26.996132 22849303695488 run.py:483] Algo bellman_ford step 9109 current loss 0.088801, current_train_items 291520.
I0304 19:32:27.016274 22849303695488 run.py:483] Algo bellman_ford step 9110 current loss 0.003649, current_train_items 291552.
I0304 19:32:27.032735 22849303695488 run.py:483] Algo bellman_ford step 9111 current loss 0.044792, current_train_items 291584.
I0304 19:32:27.057317 22849303695488 run.py:483] Algo bellman_ford step 9112 current loss 0.044905, current_train_items 291616.
I0304 19:32:27.091232 22849303695488 run.py:483] Algo bellman_ford step 9113 current loss 0.104167, current_train_items 291648.
I0304 19:32:27.127134 22849303695488 run.py:483] Algo bellman_ford step 9114 current loss 0.168887, current_train_items 291680.
I0304 19:32:27.147597 22849303695488 run.py:483] Algo bellman_ford step 9115 current loss 0.063302, current_train_items 291712.
I0304 19:32:27.164412 22849303695488 run.py:483] Algo bellman_ford step 9116 current loss 0.009601, current_train_items 291744.
I0304 19:32:27.188852 22849303695488 run.py:483] Algo bellman_ford step 9117 current loss 0.048998, current_train_items 291776.
I0304 19:32:27.220636 22849303695488 run.py:483] Algo bellman_ford step 9118 current loss 0.049871, current_train_items 291808.
I0304 19:32:27.256816 22849303695488 run.py:483] Algo bellman_ford step 9119 current loss 0.084337, current_train_items 291840.
I0304 19:32:27.276878 22849303695488 run.py:483] Algo bellman_ford step 9120 current loss 0.004755, current_train_items 291872.
I0304 19:32:27.293374 22849303695488 run.py:483] Algo bellman_ford step 9121 current loss 0.033473, current_train_items 291904.
I0304 19:32:27.318028 22849303695488 run.py:483] Algo bellman_ford step 9122 current loss 0.042811, current_train_items 291936.
I0304 19:32:27.351869 22849303695488 run.py:483] Algo bellman_ford step 9123 current loss 0.122886, current_train_items 291968.
I0304 19:32:27.388304 22849303695488 run.py:483] Algo bellman_ford step 9124 current loss 0.107203, current_train_items 292000.
I0304 19:32:27.408926 22849303695488 run.py:483] Algo bellman_ford step 9125 current loss 0.005386, current_train_items 292032.
I0304 19:32:27.425219 22849303695488 run.py:483] Algo bellman_ford step 9126 current loss 0.033279, current_train_items 292064.
I0304 19:32:27.449162 22849303695488 run.py:483] Algo bellman_ford step 9127 current loss 0.057641, current_train_items 292096.
I0304 19:32:27.482717 22849303695488 run.py:483] Algo bellman_ford step 9128 current loss 0.082454, current_train_items 292128.
I0304 19:32:27.517306 22849303695488 run.py:483] Algo bellman_ford step 9129 current loss 0.102106, current_train_items 292160.
I0304 19:32:27.537543 22849303695488 run.py:483] Algo bellman_ford step 9130 current loss 0.004091, current_train_items 292192.
I0304 19:32:27.554075 22849303695488 run.py:483] Algo bellman_ford step 9131 current loss 0.008309, current_train_items 292224.
I0304 19:32:27.579463 22849303695488 run.py:483] Algo bellman_ford step 9132 current loss 0.105324, current_train_items 292256.
I0304 19:32:27.612536 22849303695488 run.py:483] Algo bellman_ford step 9133 current loss 0.148496, current_train_items 292288.
I0304 19:32:27.645938 22849303695488 run.py:483] Algo bellman_ford step 9134 current loss 0.079102, current_train_items 292320.
I0304 19:32:27.666239 22849303695488 run.py:483] Algo bellman_ford step 9135 current loss 0.003637, current_train_items 292352.
I0304 19:32:27.682275 22849303695488 run.py:483] Algo bellman_ford step 9136 current loss 0.006932, current_train_items 292384.
I0304 19:32:27.706352 22849303695488 run.py:483] Algo bellman_ford step 9137 current loss 0.028961, current_train_items 292416.
I0304 19:32:27.738761 22849303695488 run.py:483] Algo bellman_ford step 9138 current loss 0.057172, current_train_items 292448.
I0304 19:32:27.771954 22849303695488 run.py:483] Algo bellman_ford step 9139 current loss 0.057462, current_train_items 292480.
I0304 19:32:27.792013 22849303695488 run.py:483] Algo bellman_ford step 9140 current loss 0.002307, current_train_items 292512.
I0304 19:32:27.808479 22849303695488 run.py:483] Algo bellman_ford step 9141 current loss 0.009104, current_train_items 292544.
I0304 19:32:27.832963 22849303695488 run.py:483] Algo bellman_ford step 9142 current loss 0.038995, current_train_items 292576.
I0304 19:32:27.864861 22849303695488 run.py:483] Algo bellman_ford step 9143 current loss 0.068286, current_train_items 292608.
I0304 19:32:27.900330 22849303695488 run.py:483] Algo bellman_ford step 9144 current loss 0.092190, current_train_items 292640.
I0304 19:32:27.920522 22849303695488 run.py:483] Algo bellman_ford step 9145 current loss 0.005170, current_train_items 292672.
I0304 19:32:27.937085 22849303695488 run.py:483] Algo bellman_ford step 9146 current loss 0.059973, current_train_items 292704.
I0304 19:32:27.962179 22849303695488 run.py:483] Algo bellman_ford step 9147 current loss 0.041584, current_train_items 292736.
I0304 19:32:27.994642 22849303695488 run.py:483] Algo bellman_ford step 9148 current loss 0.037732, current_train_items 292768.
I0304 19:32:28.025583 22849303695488 run.py:483] Algo bellman_ford step 9149 current loss 0.057119, current_train_items 292800.
I0304 19:32:28.045814 22849303695488 run.py:483] Algo bellman_ford step 9150 current loss 0.003229, current_train_items 292832.
I0304 19:32:28.054044 22849303695488 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0304 19:32:28.054152 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:32:28.071338 22849303695488 run.py:483] Algo bellman_ford step 9151 current loss 0.022618, current_train_items 292864.
I0304 19:32:28.096113 22849303695488 run.py:483] Algo bellman_ford step 9152 current loss 0.032566, current_train_items 292896.
I0304 19:32:28.129226 22849303695488 run.py:483] Algo bellman_ford step 9153 current loss 0.053402, current_train_items 292928.
I0304 19:32:28.161539 22849303695488 run.py:483] Algo bellman_ford step 9154 current loss 0.036917, current_train_items 292960.
I0304 19:32:28.181498 22849303695488 run.py:483] Algo bellman_ford step 9155 current loss 0.003869, current_train_items 292992.
I0304 19:32:28.198418 22849303695488 run.py:483] Algo bellman_ford step 9156 current loss 0.022962, current_train_items 293024.
I0304 19:32:28.223849 22849303695488 run.py:483] Algo bellman_ford step 9157 current loss 0.064419, current_train_items 293056.
I0304 19:32:28.255661 22849303695488 run.py:483] Algo bellman_ford step 9158 current loss 0.046049, current_train_items 293088.
I0304 19:32:28.291084 22849303695488 run.py:483] Algo bellman_ford step 9159 current loss 0.056448, current_train_items 293120.
I0304 19:32:28.311218 22849303695488 run.py:483] Algo bellman_ford step 9160 current loss 0.003809, current_train_items 293152.
I0304 19:32:28.327946 22849303695488 run.py:483] Algo bellman_ford step 9161 current loss 0.031317, current_train_items 293184.
I0304 19:32:28.352248 22849303695488 run.py:483] Algo bellman_ford step 9162 current loss 0.111955, current_train_items 293216.
I0304 19:32:28.384406 22849303695488 run.py:483] Algo bellman_ford step 9163 current loss 0.087895, current_train_items 293248.
I0304 19:32:28.420445 22849303695488 run.py:483] Algo bellman_ford step 9164 current loss 0.090679, current_train_items 293280.
I0304 19:32:28.440493 22849303695488 run.py:483] Algo bellman_ford step 9165 current loss 0.006385, current_train_items 293312.
I0304 19:32:28.457195 22849303695488 run.py:483] Algo bellman_ford step 9166 current loss 0.029736, current_train_items 293344.
I0304 19:32:28.481872 22849303695488 run.py:483] Algo bellman_ford step 9167 current loss 0.026631, current_train_items 293376.
I0304 19:32:28.514803 22849303695488 run.py:483] Algo bellman_ford step 9168 current loss 0.110747, current_train_items 293408.
I0304 19:32:28.548604 22849303695488 run.py:483] Algo bellman_ford step 9169 current loss 0.099728, current_train_items 293440.
I0304 19:32:28.568888 22849303695488 run.py:483] Algo bellman_ford step 9170 current loss 0.001997, current_train_items 293472.
I0304 19:32:28.585390 22849303695488 run.py:483] Algo bellman_ford step 9171 current loss 0.007839, current_train_items 293504.
I0304 19:32:28.611104 22849303695488 run.py:483] Algo bellman_ford step 9172 current loss 0.052225, current_train_items 293536.
I0304 19:32:28.643843 22849303695488 run.py:483] Algo bellman_ford step 9173 current loss 0.041536, current_train_items 293568.
I0304 19:32:28.678837 22849303695488 run.py:483] Algo bellman_ford step 9174 current loss 0.065212, current_train_items 293600.
I0304 19:32:28.699168 22849303695488 run.py:483] Algo bellman_ford step 9175 current loss 0.015037, current_train_items 293632.
I0304 19:32:28.716159 22849303695488 run.py:483] Algo bellman_ford step 9176 current loss 0.006309, current_train_items 293664.
I0304 19:32:28.739541 22849303695488 run.py:483] Algo bellman_ford step 9177 current loss 0.028792, current_train_items 293696.
I0304 19:32:28.770461 22849303695488 run.py:483] Algo bellman_ford step 9178 current loss 0.054361, current_train_items 293728.
I0304 19:32:28.806110 22849303695488 run.py:483] Algo bellman_ford step 9179 current loss 0.070214, current_train_items 293760.
I0304 19:32:28.826150 22849303695488 run.py:483] Algo bellman_ford step 9180 current loss 0.002104, current_train_items 293792.
I0304 19:32:28.842699 22849303695488 run.py:483] Algo bellman_ford step 9181 current loss 0.040301, current_train_items 293824.
I0304 19:32:28.867924 22849303695488 run.py:483] Algo bellman_ford step 9182 current loss 0.126508, current_train_items 293856.
I0304 19:32:28.899199 22849303695488 run.py:483] Algo bellman_ford step 9183 current loss 0.081699, current_train_items 293888.
I0304 19:32:28.932543 22849303695488 run.py:483] Algo bellman_ford step 9184 current loss 0.092112, current_train_items 293920.
I0304 19:32:28.952778 22849303695488 run.py:483] Algo bellman_ford step 9185 current loss 0.017957, current_train_items 293952.
I0304 19:32:28.969664 22849303695488 run.py:483] Algo bellman_ford step 9186 current loss 0.020471, current_train_items 293984.
I0304 19:32:28.994580 22849303695488 run.py:483] Algo bellman_ford step 9187 current loss 0.053593, current_train_items 294016.
I0304 19:32:29.028280 22849303695488 run.py:483] Algo bellman_ford step 9188 current loss 0.070841, current_train_items 294048.
I0304 19:32:29.064749 22849303695488 run.py:483] Algo bellman_ford step 9189 current loss 0.046284, current_train_items 294080.
I0304 19:32:29.085194 22849303695488 run.py:483] Algo bellman_ford step 9190 current loss 0.002902, current_train_items 294112.
I0304 19:32:29.101675 22849303695488 run.py:483] Algo bellman_ford step 9191 current loss 0.046222, current_train_items 294144.
I0304 19:32:29.126313 22849303695488 run.py:483] Algo bellman_ford step 9192 current loss 0.074480, current_train_items 294176.
I0304 19:32:29.158955 22849303695488 run.py:483] Algo bellman_ford step 9193 current loss 0.045955, current_train_items 294208.
I0304 19:32:29.192367 22849303695488 run.py:483] Algo bellman_ford step 9194 current loss 0.052391, current_train_items 294240.
I0304 19:32:29.212330 22849303695488 run.py:483] Algo bellman_ford step 9195 current loss 0.003995, current_train_items 294272.
I0304 19:32:29.229575 22849303695488 run.py:483] Algo bellman_ford step 9196 current loss 0.031720, current_train_items 294304.
I0304 19:32:29.254756 22849303695488 run.py:483] Algo bellman_ford step 9197 current loss 0.079573, current_train_items 294336.
I0304 19:32:29.286394 22849303695488 run.py:483] Algo bellman_ford step 9198 current loss 0.026253, current_train_items 294368.
I0304 19:32:29.319400 22849303695488 run.py:483] Algo bellman_ford step 9199 current loss 0.088697, current_train_items 294400.
I0304 19:32:29.339898 22849303695488 run.py:483] Algo bellman_ford step 9200 current loss 0.001794, current_train_items 294432.
I0304 19:32:29.347937 22849303695488 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0304 19:32:29.348053 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:32:29.365361 22849303695488 run.py:483] Algo bellman_ford step 9201 current loss 0.016313, current_train_items 294464.
I0304 19:32:29.390492 22849303695488 run.py:483] Algo bellman_ford step 9202 current loss 0.065690, current_train_items 294496.
I0304 19:32:29.424838 22849303695488 run.py:483] Algo bellman_ford step 9203 current loss 0.067181, current_train_items 294528.
I0304 19:32:29.460388 22849303695488 run.py:483] Algo bellman_ford step 9204 current loss 0.051227, current_train_items 294560.
I0304 19:32:29.481086 22849303695488 run.py:483] Algo bellman_ford step 9205 current loss 0.006402, current_train_items 294592.
I0304 19:32:29.497574 22849303695488 run.py:483] Algo bellman_ford step 9206 current loss 0.025663, current_train_items 294624.
I0304 19:32:29.522492 22849303695488 run.py:483] Algo bellman_ford step 9207 current loss 0.044909, current_train_items 294656.
I0304 19:32:29.555523 22849303695488 run.py:483] Algo bellman_ford step 9208 current loss 0.092343, current_train_items 294688.
I0304 19:32:29.587852 22849303695488 run.py:483] Algo bellman_ford step 9209 current loss 0.058637, current_train_items 294720.
I0304 19:32:29.608173 22849303695488 run.py:483] Algo bellman_ford step 9210 current loss 0.003622, current_train_items 294752.
I0304 19:32:29.625183 22849303695488 run.py:483] Algo bellman_ford step 9211 current loss 0.016625, current_train_items 294784.
I0304 19:32:29.649595 22849303695488 run.py:483] Algo bellman_ford step 9212 current loss 0.033812, current_train_items 294816.
I0304 19:32:29.682693 22849303695488 run.py:483] Algo bellman_ford step 9213 current loss 0.070224, current_train_items 294848.
I0304 19:32:29.715896 22849303695488 run.py:483] Algo bellman_ford step 9214 current loss 0.130124, current_train_items 294880.
I0304 19:32:29.736159 22849303695488 run.py:483] Algo bellman_ford step 9215 current loss 0.025772, current_train_items 294912.
I0304 19:32:29.752874 22849303695488 run.py:483] Algo bellman_ford step 9216 current loss 0.022011, current_train_items 294944.
I0304 19:32:29.777358 22849303695488 run.py:483] Algo bellman_ford step 9217 current loss 0.025921, current_train_items 294976.
I0304 19:32:29.809558 22849303695488 run.py:483] Algo bellman_ford step 9218 current loss 0.103949, current_train_items 295008.
I0304 19:32:29.843854 22849303695488 run.py:483] Algo bellman_ford step 9219 current loss 0.077024, current_train_items 295040.
I0304 19:32:29.863990 22849303695488 run.py:483] Algo bellman_ford step 9220 current loss 0.027583, current_train_items 295072.
I0304 19:32:29.880389 22849303695488 run.py:483] Algo bellman_ford step 9221 current loss 0.023902, current_train_items 295104.
I0304 19:32:29.905585 22849303695488 run.py:483] Algo bellman_ford step 9222 current loss 0.059413, current_train_items 295136.
I0304 19:32:29.937636 22849303695488 run.py:483] Algo bellman_ford step 9223 current loss 0.041913, current_train_items 295168.
I0304 19:32:29.972300 22849303695488 run.py:483] Algo bellman_ford step 9224 current loss 0.050630, current_train_items 295200.
I0304 19:32:29.992695 22849303695488 run.py:483] Algo bellman_ford step 9225 current loss 0.003598, current_train_items 295232.
I0304 19:32:30.009183 22849303695488 run.py:483] Algo bellman_ford step 9226 current loss 0.009528, current_train_items 295264.
I0304 19:32:30.034512 22849303695488 run.py:483] Algo bellman_ford step 9227 current loss 0.039311, current_train_items 295296.
I0304 19:32:30.067473 22849303695488 run.py:483] Algo bellman_ford step 9228 current loss 0.055348, current_train_items 295328.
I0304 19:32:30.099449 22849303695488 run.py:483] Algo bellman_ford step 9229 current loss 0.035526, current_train_items 295360.
I0304 19:32:30.119422 22849303695488 run.py:483] Algo bellman_ford step 9230 current loss 0.003916, current_train_items 295392.
I0304 19:32:30.136361 22849303695488 run.py:483] Algo bellman_ford step 9231 current loss 0.045309, current_train_items 295424.
I0304 19:32:30.161619 22849303695488 run.py:483] Algo bellman_ford step 9232 current loss 0.042000, current_train_items 295456.
I0304 19:32:30.196785 22849303695488 run.py:483] Algo bellman_ford step 9233 current loss 0.089141, current_train_items 295488.
I0304 19:32:30.231153 22849303695488 run.py:483] Algo bellman_ford step 9234 current loss 0.042299, current_train_items 295520.
I0304 19:32:30.251060 22849303695488 run.py:483] Algo bellman_ford step 9235 current loss 0.001357, current_train_items 295552.
I0304 19:32:30.267613 22849303695488 run.py:483] Algo bellman_ford step 9236 current loss 0.033399, current_train_items 295584.
I0304 19:32:30.292756 22849303695488 run.py:483] Algo bellman_ford step 9237 current loss 0.027480, current_train_items 295616.
I0304 19:32:30.326290 22849303695488 run.py:483] Algo bellman_ford step 9238 current loss 0.036515, current_train_items 295648.
I0304 19:32:30.361357 22849303695488 run.py:483] Algo bellman_ford step 9239 current loss 0.047348, current_train_items 295680.
I0304 19:32:30.381664 22849303695488 run.py:483] Algo bellman_ford step 9240 current loss 0.002415, current_train_items 295712.
I0304 19:32:30.398778 22849303695488 run.py:483] Algo bellman_ford step 9241 current loss 0.018926, current_train_items 295744.
I0304 19:32:30.424480 22849303695488 run.py:483] Algo bellman_ford step 9242 current loss 0.053195, current_train_items 295776.
I0304 19:32:30.457291 22849303695488 run.py:483] Algo bellman_ford step 9243 current loss 0.044779, current_train_items 295808.
I0304 19:32:30.490320 22849303695488 run.py:483] Algo bellman_ford step 9244 current loss 0.055533, current_train_items 295840.
I0304 19:32:30.510807 22849303695488 run.py:483] Algo bellman_ford step 9245 current loss 0.003061, current_train_items 295872.
I0304 19:32:30.527653 22849303695488 run.py:483] Algo bellman_ford step 9246 current loss 0.017549, current_train_items 295904.
I0304 19:32:30.551719 22849303695488 run.py:483] Algo bellman_ford step 9247 current loss 0.014770, current_train_items 295936.
I0304 19:32:30.584401 22849303695488 run.py:483] Algo bellman_ford step 9248 current loss 0.039555, current_train_items 295968.
I0304 19:32:30.619046 22849303695488 run.py:483] Algo bellman_ford step 9249 current loss 0.061603, current_train_items 296000.
I0304 19:32:30.639351 22849303695488 run.py:483] Algo bellman_ford step 9250 current loss 0.004676, current_train_items 296032.
I0304 19:32:30.647728 22849303695488 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0304 19:32:30.647836 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:30.664769 22849303695488 run.py:483] Algo bellman_ford step 9251 current loss 0.012959, current_train_items 296064.
I0304 19:32:30.690071 22849303695488 run.py:483] Algo bellman_ford step 9252 current loss 0.038709, current_train_items 296096.
I0304 19:32:30.722066 22849303695488 run.py:483] Algo bellman_ford step 9253 current loss 0.038651, current_train_items 296128.
I0304 19:32:30.754861 22849303695488 run.py:483] Algo bellman_ford step 9254 current loss 0.033791, current_train_items 296160.
I0304 19:32:30.774900 22849303695488 run.py:483] Algo bellman_ford step 9255 current loss 0.001686, current_train_items 296192.
I0304 19:32:30.791179 22849303695488 run.py:483] Algo bellman_ford step 9256 current loss 0.031226, current_train_items 296224.
I0304 19:32:30.816023 22849303695488 run.py:483] Algo bellman_ford step 9257 current loss 0.028387, current_train_items 296256.
I0304 19:32:30.848701 22849303695488 run.py:483] Algo bellman_ford step 9258 current loss 0.070922, current_train_items 296288.
I0304 19:32:30.882859 22849303695488 run.py:483] Algo bellman_ford step 9259 current loss 0.055715, current_train_items 296320.
I0304 19:32:30.903337 22849303695488 run.py:483] Algo bellman_ford step 9260 current loss 0.002453, current_train_items 296352.
I0304 19:32:30.920739 22849303695488 run.py:483] Algo bellman_ford step 9261 current loss 0.015735, current_train_items 296384.
I0304 19:32:30.944143 22849303695488 run.py:483] Algo bellman_ford step 9262 current loss 0.029670, current_train_items 296416.
I0304 19:32:30.976642 22849303695488 run.py:483] Algo bellman_ford step 9263 current loss 0.041656, current_train_items 296448.
I0304 19:32:31.010819 22849303695488 run.py:483] Algo bellman_ford step 9264 current loss 0.046349, current_train_items 296480.
I0304 19:32:31.030740 22849303695488 run.py:483] Algo bellman_ford step 9265 current loss 0.004083, current_train_items 296512.
I0304 19:32:31.047486 22849303695488 run.py:483] Algo bellman_ford step 9266 current loss 0.015163, current_train_items 296544.
I0304 19:32:31.070678 22849303695488 run.py:483] Algo bellman_ford step 9267 current loss 0.019630, current_train_items 296576.
I0304 19:32:31.103061 22849303695488 run.py:483] Algo bellman_ford step 9268 current loss 0.043590, current_train_items 296608.
I0304 19:32:31.137012 22849303695488 run.py:483] Algo bellman_ford step 9269 current loss 0.068878, current_train_items 296640.
I0304 19:32:31.157208 22849303695488 run.py:483] Algo bellman_ford step 9270 current loss 0.002591, current_train_items 296672.
I0304 19:32:31.173931 22849303695488 run.py:483] Algo bellman_ford step 9271 current loss 0.009868, current_train_items 296704.
I0304 19:32:31.197858 22849303695488 run.py:483] Algo bellman_ford step 9272 current loss 0.019071, current_train_items 296736.
I0304 19:32:31.231867 22849303695488 run.py:483] Algo bellman_ford step 9273 current loss 0.064640, current_train_items 296768.
I0304 19:32:31.266910 22849303695488 run.py:483] Algo bellman_ford step 9274 current loss 0.054080, current_train_items 296800.
I0304 19:32:31.287652 22849303695488 run.py:483] Algo bellman_ford step 9275 current loss 0.014394, current_train_items 296832.
I0304 19:32:31.304345 22849303695488 run.py:483] Algo bellman_ford step 9276 current loss 0.028583, current_train_items 296864.
I0304 19:32:31.328392 22849303695488 run.py:483] Algo bellman_ford step 9277 current loss 0.038017, current_train_items 296896.
I0304 19:32:31.361022 22849303695488 run.py:483] Algo bellman_ford step 9278 current loss 0.042258, current_train_items 296928.
I0304 19:32:31.395240 22849303695488 run.py:483] Algo bellman_ford step 9279 current loss 0.080382, current_train_items 296960.
I0304 19:32:31.415270 22849303695488 run.py:483] Algo bellman_ford step 9280 current loss 0.002105, current_train_items 296992.
I0304 19:32:31.431971 22849303695488 run.py:483] Algo bellman_ford step 9281 current loss 0.027503, current_train_items 297024.
I0304 19:32:31.456305 22849303695488 run.py:483] Algo bellman_ford step 9282 current loss 0.024582, current_train_items 297056.
I0304 19:32:31.486980 22849303695488 run.py:483] Algo bellman_ford step 9283 current loss 0.026594, current_train_items 297088.
I0304 19:32:31.520241 22849303695488 run.py:483] Algo bellman_ford step 9284 current loss 0.072428, current_train_items 297120.
I0304 19:32:31.540661 22849303695488 run.py:483] Algo bellman_ford step 9285 current loss 0.005015, current_train_items 297152.
I0304 19:32:31.556683 22849303695488 run.py:483] Algo bellman_ford step 9286 current loss 0.012313, current_train_items 297184.
I0304 19:32:31.582222 22849303695488 run.py:483] Algo bellman_ford step 9287 current loss 0.054049, current_train_items 297216.
I0304 19:32:31.613677 22849303695488 run.py:483] Algo bellman_ford step 9288 current loss 0.048406, current_train_items 297248.
I0304 19:32:31.649837 22849303695488 run.py:483] Algo bellman_ford step 9289 current loss 0.038812, current_train_items 297280.
I0304 19:32:31.670174 22849303695488 run.py:483] Algo bellman_ford step 9290 current loss 0.003768, current_train_items 297312.
I0304 19:32:31.686792 22849303695488 run.py:483] Algo bellman_ford step 9291 current loss 0.024004, current_train_items 297344.
I0304 19:32:31.711621 22849303695488 run.py:483] Algo bellman_ford step 9292 current loss 0.073844, current_train_items 297376.
I0304 19:32:31.743018 22849303695488 run.py:483] Algo bellman_ford step 9293 current loss 0.063207, current_train_items 297408.
I0304 19:32:31.778268 22849303695488 run.py:483] Algo bellman_ford step 9294 current loss 0.062093, current_train_items 297440.
I0304 19:32:31.798218 22849303695488 run.py:483] Algo bellman_ford step 9295 current loss 0.003924, current_train_items 297472.
I0304 19:32:31.814756 22849303695488 run.py:483] Algo bellman_ford step 9296 current loss 0.009724, current_train_items 297504.
I0304 19:32:31.838996 22849303695488 run.py:483] Algo bellman_ford step 9297 current loss 0.038282, current_train_items 297536.
I0304 19:32:31.872027 22849303695488 run.py:483] Algo bellman_ford step 9298 current loss 0.063132, current_train_items 297568.
I0304 19:32:31.906864 22849303695488 run.py:483] Algo bellman_ford step 9299 current loss 0.040363, current_train_items 297600.
I0304 19:32:31.927292 22849303695488 run.py:483] Algo bellman_ford step 9300 current loss 0.001628, current_train_items 297632.
I0304 19:32:31.935312 22849303695488 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0304 19:32:31.935422 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:32:31.952695 22849303695488 run.py:483] Algo bellman_ford step 9301 current loss 0.021431, current_train_items 297664.
I0304 19:32:31.977992 22849303695488 run.py:483] Algo bellman_ford step 9302 current loss 0.020621, current_train_items 297696.
I0304 19:32:32.010844 22849303695488 run.py:483] Algo bellman_ford step 9303 current loss 0.030512, current_train_items 297728.
I0304 19:32:32.046755 22849303695488 run.py:483] Algo bellman_ford step 9304 current loss 0.081658, current_train_items 297760.
I0304 19:32:32.066887 22849303695488 run.py:483] Algo bellman_ford step 9305 current loss 0.001728, current_train_items 297792.
I0304 19:32:32.083073 22849303695488 run.py:483] Algo bellman_ford step 9306 current loss 0.028077, current_train_items 297824.
I0304 19:32:32.108400 22849303695488 run.py:483] Algo bellman_ford step 9307 current loss 0.032484, current_train_items 297856.
I0304 19:32:32.141162 22849303695488 run.py:483] Algo bellman_ford step 9308 current loss 0.088528, current_train_items 297888.
I0304 19:32:32.175055 22849303695488 run.py:483] Algo bellman_ford step 9309 current loss 0.057680, current_train_items 297920.
I0304 19:32:32.194947 22849303695488 run.py:483] Algo bellman_ford step 9310 current loss 0.002332, current_train_items 297952.
I0304 19:32:32.211436 22849303695488 run.py:483] Algo bellman_ford step 9311 current loss 0.019333, current_train_items 297984.
I0304 19:32:32.235852 22849303695488 run.py:483] Algo bellman_ford step 9312 current loss 0.052672, current_train_items 298016.
I0304 19:32:32.267282 22849303695488 run.py:483] Algo bellman_ford step 9313 current loss 0.047016, current_train_items 298048.
I0304 19:32:32.301910 22849303695488 run.py:483] Algo bellman_ford step 9314 current loss 0.083006, current_train_items 298080.
I0304 19:32:32.321933 22849303695488 run.py:483] Algo bellman_ford step 9315 current loss 0.008252, current_train_items 298112.
I0304 19:32:32.338190 22849303695488 run.py:483] Algo bellman_ford step 9316 current loss 0.003195, current_train_items 298144.
I0304 19:32:32.362606 22849303695488 run.py:483] Algo bellman_ford step 9317 current loss 0.041538, current_train_items 298176.
I0304 19:32:32.395805 22849303695488 run.py:483] Algo bellman_ford step 9318 current loss 0.066723, current_train_items 298208.
I0304 19:32:32.430162 22849303695488 run.py:483] Algo bellman_ford step 9319 current loss 0.043309, current_train_items 298240.
I0304 19:32:32.450172 22849303695488 run.py:483] Algo bellman_ford step 9320 current loss 0.003238, current_train_items 298272.
I0304 19:32:32.466845 22849303695488 run.py:483] Algo bellman_ford step 9321 current loss 0.015663, current_train_items 298304.
I0304 19:32:32.490234 22849303695488 run.py:483] Algo bellman_ford step 9322 current loss 0.029874, current_train_items 298336.
I0304 19:32:32.521728 22849303695488 run.py:483] Algo bellman_ford step 9323 current loss 0.056219, current_train_items 298368.
I0304 19:32:32.558546 22849303695488 run.py:483] Algo bellman_ford step 9324 current loss 0.096179, current_train_items 298400.
I0304 19:32:32.578500 22849303695488 run.py:483] Algo bellman_ford step 9325 current loss 0.001305, current_train_items 298432.
I0304 19:32:32.594864 22849303695488 run.py:483] Algo bellman_ford step 9326 current loss 0.010412, current_train_items 298464.
I0304 19:32:32.619868 22849303695488 run.py:483] Algo bellman_ford step 9327 current loss 0.058817, current_train_items 298496.
I0304 19:32:32.653017 22849303695488 run.py:483] Algo bellman_ford step 9328 current loss 0.071826, current_train_items 298528.
I0304 19:32:32.684514 22849303695488 run.py:483] Algo bellman_ford step 9329 current loss 0.044420, current_train_items 298560.
I0304 19:32:32.704667 22849303695488 run.py:483] Algo bellman_ford step 9330 current loss 0.002280, current_train_items 298592.
I0304 19:32:32.721557 22849303695488 run.py:483] Algo bellman_ford step 9331 current loss 0.027620, current_train_items 298624.
I0304 19:32:32.745475 22849303695488 run.py:483] Algo bellman_ford step 9332 current loss 0.045459, current_train_items 298656.
I0304 19:32:32.777709 22849303695488 run.py:483] Algo bellman_ford step 9333 current loss 0.057446, current_train_items 298688.
I0304 19:32:32.811032 22849303695488 run.py:483] Algo bellman_ford step 9334 current loss 0.102509, current_train_items 298720.
I0304 19:32:32.831044 22849303695488 run.py:483] Algo bellman_ford step 9335 current loss 0.002525, current_train_items 298752.
I0304 19:32:32.847717 22849303695488 run.py:483] Algo bellman_ford step 9336 current loss 0.057660, current_train_items 298784.
I0304 19:32:32.873323 22849303695488 run.py:483] Algo bellman_ford step 9337 current loss 0.052026, current_train_items 298816.
I0304 19:32:32.905948 22849303695488 run.py:483] Algo bellman_ford step 9338 current loss 0.046131, current_train_items 298848.
I0304 19:32:32.940323 22849303695488 run.py:483] Algo bellman_ford step 9339 current loss 0.068734, current_train_items 298880.
I0304 19:32:32.960438 22849303695488 run.py:483] Algo bellman_ford step 9340 current loss 0.006607, current_train_items 298912.
I0304 19:32:32.976672 22849303695488 run.py:483] Algo bellman_ford step 9341 current loss 0.009260, current_train_items 298944.
I0304 19:32:33.001817 22849303695488 run.py:483] Algo bellman_ford step 9342 current loss 0.043254, current_train_items 298976.
I0304 19:32:33.034369 22849303695488 run.py:483] Algo bellman_ford step 9343 current loss 0.053875, current_train_items 299008.
I0304 19:32:33.069331 22849303695488 run.py:483] Algo bellman_ford step 9344 current loss 0.037198, current_train_items 299040.
I0304 19:32:33.089559 22849303695488 run.py:483] Algo bellman_ford step 9345 current loss 0.002864, current_train_items 299072.
I0304 19:32:33.106127 22849303695488 run.py:483] Algo bellman_ford step 9346 current loss 0.011058, current_train_items 299104.
I0304 19:32:33.131111 22849303695488 run.py:483] Algo bellman_ford step 9347 current loss 0.016913, current_train_items 299136.
I0304 19:32:33.163777 22849303695488 run.py:483] Algo bellman_ford step 9348 current loss 0.051897, current_train_items 299168.
I0304 19:32:33.197069 22849303695488 run.py:483] Algo bellman_ford step 9349 current loss 0.083225, current_train_items 299200.
I0304 19:32:33.217069 22849303695488 run.py:483] Algo bellman_ford step 9350 current loss 0.043225, current_train_items 299232.
I0304 19:32:33.225475 22849303695488 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0304 19:32:33.225582 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:33.242491 22849303695488 run.py:483] Algo bellman_ford step 9351 current loss 0.002083, current_train_items 299264.
I0304 19:32:33.268376 22849303695488 run.py:483] Algo bellman_ford step 9352 current loss 0.050524, current_train_items 299296.
I0304 19:32:33.302082 22849303695488 run.py:483] Algo bellman_ford step 9353 current loss 0.044362, current_train_items 299328.
I0304 19:32:33.338788 22849303695488 run.py:483] Algo bellman_ford step 9354 current loss 0.093172, current_train_items 299360.
I0304 19:32:33.359354 22849303695488 run.py:483] Algo bellman_ford step 9355 current loss 0.059116, current_train_items 299392.
I0304 19:32:33.375362 22849303695488 run.py:483] Algo bellman_ford step 9356 current loss 0.029657, current_train_items 299424.
I0304 19:32:33.400152 22849303695488 run.py:483] Algo bellman_ford step 9357 current loss 0.050630, current_train_items 299456.
I0304 19:32:33.433216 22849303695488 run.py:483] Algo bellman_ford step 9358 current loss 0.038624, current_train_items 299488.
I0304 19:32:33.467587 22849303695488 run.py:483] Algo bellman_ford step 9359 current loss 0.053518, current_train_items 299520.
I0304 19:32:33.488350 22849303695488 run.py:483] Algo bellman_ford step 9360 current loss 0.023446, current_train_items 299552.
I0304 19:32:33.504920 22849303695488 run.py:483] Algo bellman_ford step 9361 current loss 0.024061, current_train_items 299584.
I0304 19:32:33.528899 22849303695488 run.py:483] Algo bellman_ford step 9362 current loss 0.051161, current_train_items 299616.
I0304 19:32:33.561847 22849303695488 run.py:483] Algo bellman_ford step 9363 current loss 0.036755, current_train_items 299648.
I0304 19:32:33.597166 22849303695488 run.py:483] Algo bellman_ford step 9364 current loss 0.045899, current_train_items 299680.
I0304 19:32:33.617053 22849303695488 run.py:483] Algo bellman_ford step 9365 current loss 0.008668, current_train_items 299712.
I0304 19:32:33.634037 22849303695488 run.py:483] Algo bellman_ford step 9366 current loss 0.047749, current_train_items 299744.
I0304 19:32:33.660545 22849303695488 run.py:483] Algo bellman_ford step 9367 current loss 0.055111, current_train_items 299776.
I0304 19:32:33.694361 22849303695488 run.py:483] Algo bellman_ford step 9368 current loss 0.065622, current_train_items 299808.
I0304 19:32:33.728034 22849303695488 run.py:483] Algo bellman_ford step 9369 current loss 0.050849, current_train_items 299840.
I0304 19:32:33.748795 22849303695488 run.py:483] Algo bellman_ford step 9370 current loss 0.019776, current_train_items 299872.
I0304 19:32:33.765744 22849303695488 run.py:483] Algo bellman_ford step 9371 current loss 0.035993, current_train_items 299904.
I0304 19:32:33.790748 22849303695488 run.py:483] Algo bellman_ford step 9372 current loss 0.019177, current_train_items 299936.
I0304 19:32:33.824376 22849303695488 run.py:483] Algo bellman_ford step 9373 current loss 0.076281, current_train_items 299968.
I0304 19:32:33.859013 22849303695488 run.py:483] Algo bellman_ford step 9374 current loss 0.037481, current_train_items 300000.
I0304 19:32:33.879594 22849303695488 run.py:483] Algo bellman_ford step 9375 current loss 0.011547, current_train_items 300032.
I0304 19:32:33.895807 22849303695488 run.py:483] Algo bellman_ford step 9376 current loss 0.024005, current_train_items 300064.
I0304 19:32:33.920325 22849303695488 run.py:483] Algo bellman_ford step 9377 current loss 0.057819, current_train_items 300096.
I0304 19:32:33.952178 22849303695488 run.py:483] Algo bellman_ford step 9378 current loss 0.047358, current_train_items 300128.
I0304 19:32:33.986103 22849303695488 run.py:483] Algo bellman_ford step 9379 current loss 0.083714, current_train_items 300160.
I0304 19:32:34.006317 22849303695488 run.py:483] Algo bellman_ford step 9380 current loss 0.002677, current_train_items 300192.
I0304 19:32:34.022433 22849303695488 run.py:483] Algo bellman_ford step 9381 current loss 0.056064, current_train_items 300224.
I0304 19:32:34.047421 22849303695488 run.py:483] Algo bellman_ford step 9382 current loss 0.023676, current_train_items 300256.
I0304 19:32:34.079235 22849303695488 run.py:483] Algo bellman_ford step 9383 current loss 0.034590, current_train_items 300288.
I0304 19:32:34.113883 22849303695488 run.py:483] Algo bellman_ford step 9384 current loss 0.048779, current_train_items 300320.
I0304 19:32:34.134387 22849303695488 run.py:483] Algo bellman_ford step 9385 current loss 0.003226, current_train_items 300352.
I0304 19:32:34.151576 22849303695488 run.py:483] Algo bellman_ford step 9386 current loss 0.030754, current_train_items 300384.
I0304 19:32:34.176013 22849303695488 run.py:483] Algo bellman_ford step 9387 current loss 0.060555, current_train_items 300416.
I0304 19:32:34.208322 22849303695488 run.py:483] Algo bellman_ford step 9388 current loss 0.064279, current_train_items 300448.
I0304 19:32:34.244710 22849303695488 run.py:483] Algo bellman_ford step 9389 current loss 0.052949, current_train_items 300480.
I0304 19:32:34.265396 22849303695488 run.py:483] Algo bellman_ford step 9390 current loss 0.011700, current_train_items 300512.
I0304 19:32:34.282286 22849303695488 run.py:483] Algo bellman_ford step 9391 current loss 0.007546, current_train_items 300544.
I0304 19:32:34.306609 22849303695488 run.py:483] Algo bellman_ford step 9392 current loss 0.018903, current_train_items 300576.
I0304 19:32:34.339238 22849303695488 run.py:483] Algo bellman_ford step 9393 current loss 0.058035, current_train_items 300608.
I0304 19:32:34.373108 22849303695488 run.py:483] Algo bellman_ford step 9394 current loss 0.117616, current_train_items 300640.
I0304 19:32:34.393644 22849303695488 run.py:483] Algo bellman_ford step 9395 current loss 0.019436, current_train_items 300672.
I0304 19:32:34.410376 22849303695488 run.py:483] Algo bellman_ford step 9396 current loss 0.028435, current_train_items 300704.
I0304 19:32:34.436298 22849303695488 run.py:483] Algo bellman_ford step 9397 current loss 0.088163, current_train_items 300736.
I0304 19:32:34.468544 22849303695488 run.py:483] Algo bellman_ford step 9398 current loss 0.089879, current_train_items 300768.
I0304 19:32:34.503072 22849303695488 run.py:483] Algo bellman_ford step 9399 current loss 0.125546, current_train_items 300800.
I0304 19:32:34.523810 22849303695488 run.py:483] Algo bellman_ford step 9400 current loss 0.008482, current_train_items 300832.
I0304 19:32:34.532042 22849303695488 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0304 19:32:34.532153 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:32:34.549603 22849303695488 run.py:483] Algo bellman_ford step 9401 current loss 0.016403, current_train_items 300864.
I0304 19:32:34.575876 22849303695488 run.py:483] Algo bellman_ford step 9402 current loss 0.054772, current_train_items 300896.
I0304 19:32:34.608758 22849303695488 run.py:483] Algo bellman_ford step 9403 current loss 0.022933, current_train_items 300928.
I0304 19:32:34.643919 22849303695488 run.py:483] Algo bellman_ford step 9404 current loss 0.060819, current_train_items 300960.
I0304 19:32:34.664322 22849303695488 run.py:483] Algo bellman_ford step 9405 current loss 0.003977, current_train_items 300992.
I0304 19:32:34.680660 22849303695488 run.py:483] Algo bellman_ford step 9406 current loss 0.019873, current_train_items 301024.
I0304 19:32:34.706710 22849303695488 run.py:483] Algo bellman_ford step 9407 current loss 0.058058, current_train_items 301056.
I0304 19:32:34.739143 22849303695488 run.py:483] Algo bellman_ford step 9408 current loss 0.035920, current_train_items 301088.
I0304 19:32:34.774785 22849303695488 run.py:483] Algo bellman_ford step 9409 current loss 0.063857, current_train_items 301120.
I0304 19:32:34.795051 22849303695488 run.py:483] Algo bellman_ford step 9410 current loss 0.004459, current_train_items 301152.
I0304 19:32:34.811590 22849303695488 run.py:483] Algo bellman_ford step 9411 current loss 0.007045, current_train_items 301184.
I0304 19:32:34.836872 22849303695488 run.py:483] Algo bellman_ford step 9412 current loss 0.018409, current_train_items 301216.
I0304 19:32:34.869464 22849303695488 run.py:483] Algo bellman_ford step 9413 current loss 0.028546, current_train_items 301248.
I0304 19:32:34.903201 22849303695488 run.py:483] Algo bellman_ford step 9414 current loss 0.051905, current_train_items 301280.
I0304 19:32:34.923329 22849303695488 run.py:483] Algo bellman_ford step 9415 current loss 0.011347, current_train_items 301312.
I0304 19:32:34.940064 22849303695488 run.py:483] Algo bellman_ford step 9416 current loss 0.013013, current_train_items 301344.
I0304 19:32:34.965363 22849303695488 run.py:483] Algo bellman_ford step 9417 current loss 0.062361, current_train_items 301376.
I0304 19:32:34.997472 22849303695488 run.py:483] Algo bellman_ford step 9418 current loss 0.046514, current_train_items 301408.
I0304 19:32:35.030024 22849303695488 run.py:483] Algo bellman_ford step 9419 current loss 0.025119, current_train_items 301440.
I0304 19:32:35.050241 22849303695488 run.py:483] Algo bellman_ford step 9420 current loss 0.003089, current_train_items 301472.
I0304 19:32:35.067052 22849303695488 run.py:483] Algo bellman_ford step 9421 current loss 0.014601, current_train_items 301504.
I0304 19:32:35.092329 22849303695488 run.py:483] Algo bellman_ford step 9422 current loss 0.070518, current_train_items 301536.
I0304 19:32:35.124802 22849303695488 run.py:483] Algo bellman_ford step 9423 current loss 0.035185, current_train_items 301568.
I0304 19:32:35.159186 22849303695488 run.py:483] Algo bellman_ford step 9424 current loss 0.044671, current_train_items 301600.
I0304 19:32:35.179777 22849303695488 run.py:483] Algo bellman_ford step 9425 current loss 0.023008, current_train_items 301632.
I0304 19:32:35.195925 22849303695488 run.py:483] Algo bellman_ford step 9426 current loss 0.005835, current_train_items 301664.
I0304 19:32:35.220463 22849303695488 run.py:483] Algo bellman_ford step 9427 current loss 0.037926, current_train_items 301696.
I0304 19:32:35.253983 22849303695488 run.py:483] Algo bellman_ford step 9428 current loss 0.060323, current_train_items 301728.
I0304 19:32:35.285624 22849303695488 run.py:483] Algo bellman_ford step 9429 current loss 0.141276, current_train_items 301760.
I0304 19:32:35.305804 22849303695488 run.py:483] Algo bellman_ford step 9430 current loss 0.004680, current_train_items 301792.
I0304 19:32:35.322387 22849303695488 run.py:483] Algo bellman_ford step 9431 current loss 0.041855, current_train_items 301824.
I0304 19:32:35.347001 22849303695488 run.py:483] Algo bellman_ford step 9432 current loss 0.045841, current_train_items 301856.
I0304 19:32:35.379045 22849303695488 run.py:483] Algo bellman_ford step 9433 current loss 0.062604, current_train_items 301888.
I0304 19:32:35.414197 22849303695488 run.py:483] Algo bellman_ford step 9434 current loss 0.085385, current_train_items 301920.
I0304 19:32:35.434288 22849303695488 run.py:483] Algo bellman_ford step 9435 current loss 0.002480, current_train_items 301952.
I0304 19:32:35.451021 22849303695488 run.py:483] Algo bellman_ford step 9436 current loss 0.031365, current_train_items 301984.
I0304 19:32:35.475794 22849303695488 run.py:483] Algo bellman_ford step 9437 current loss 0.033949, current_train_items 302016.
I0304 19:32:35.508815 22849303695488 run.py:483] Algo bellman_ford step 9438 current loss 0.058461, current_train_items 302048.
I0304 19:32:35.542186 22849303695488 run.py:483] Algo bellman_ford step 9439 current loss 0.072922, current_train_items 302080.
I0304 19:32:35.562544 22849303695488 run.py:483] Algo bellman_ford step 9440 current loss 0.006309, current_train_items 302112.
I0304 19:32:35.579495 22849303695488 run.py:483] Algo bellman_ford step 9441 current loss 0.037371, current_train_items 302144.
I0304 19:32:35.604123 22849303695488 run.py:483] Algo bellman_ford step 9442 current loss 0.054972, current_train_items 302176.
I0304 19:32:35.637575 22849303695488 run.py:483] Algo bellman_ford step 9443 current loss 0.048947, current_train_items 302208.
I0304 19:32:35.674530 22849303695488 run.py:483] Algo bellman_ford step 9444 current loss 0.135069, current_train_items 302240.
I0304 19:32:35.694622 22849303695488 run.py:483] Algo bellman_ford step 9445 current loss 0.069457, current_train_items 302272.
I0304 19:32:35.710713 22849303695488 run.py:483] Algo bellman_ford step 9446 current loss 0.010005, current_train_items 302304.
I0304 19:32:35.734994 22849303695488 run.py:483] Algo bellman_ford step 9447 current loss 0.024756, current_train_items 302336.
I0304 19:32:35.767096 22849303695488 run.py:483] Algo bellman_ford step 9448 current loss 0.102879, current_train_items 302368.
I0304 19:32:35.800217 22849303695488 run.py:483] Algo bellman_ford step 9449 current loss 0.126177, current_train_items 302400.
I0304 19:32:35.820355 22849303695488 run.py:483] Algo bellman_ford step 9450 current loss 0.007539, current_train_items 302432.
I0304 19:32:35.828593 22849303695488 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0304 19:32:35.828702 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:32:35.846301 22849303695488 run.py:483] Algo bellman_ford step 9451 current loss 0.008407, current_train_items 302464.
I0304 19:32:35.870581 22849303695488 run.py:483] Algo bellman_ford step 9452 current loss 0.028447, current_train_items 302496.
I0304 19:32:35.904036 22849303695488 run.py:483] Algo bellman_ford step 9453 current loss 0.052586, current_train_items 302528.
I0304 19:32:35.939807 22849303695488 run.py:483] Algo bellman_ford step 9454 current loss 0.060331, current_train_items 302560.
I0304 19:32:35.960076 22849303695488 run.py:483] Algo bellman_ford step 9455 current loss 0.002497, current_train_items 302592.
I0304 19:32:35.976405 22849303695488 run.py:483] Algo bellman_ford step 9456 current loss 0.003675, current_train_items 302624.
I0304 19:32:36.001251 22849303695488 run.py:483] Algo bellman_ford step 9457 current loss 0.038103, current_train_items 302656.
I0304 19:32:36.033892 22849303695488 run.py:483] Algo bellman_ford step 9458 current loss 0.112047, current_train_items 302688.
I0304 19:32:36.067556 22849303695488 run.py:483] Algo bellman_ford step 9459 current loss 0.065298, current_train_items 302720.
I0304 19:32:36.088280 22849303695488 run.py:483] Algo bellman_ford step 9460 current loss 0.003777, current_train_items 302752.
I0304 19:32:36.105093 22849303695488 run.py:483] Algo bellman_ford step 9461 current loss 0.030801, current_train_items 302784.
I0304 19:32:36.129202 22849303695488 run.py:483] Algo bellman_ford step 9462 current loss 0.018770, current_train_items 302816.
I0304 19:32:36.162685 22849303695488 run.py:483] Algo bellman_ford step 9463 current loss 0.046336, current_train_items 302848.
I0304 19:32:36.196982 22849303695488 run.py:483] Algo bellman_ford step 9464 current loss 0.042506, current_train_items 302880.
I0304 19:32:36.216984 22849303695488 run.py:483] Algo bellman_ford step 9465 current loss 0.003999, current_train_items 302912.
I0304 19:32:36.233949 22849303695488 run.py:483] Algo bellman_ford step 9466 current loss 0.043596, current_train_items 302944.
I0304 19:32:36.260394 22849303695488 run.py:483] Algo bellman_ford step 9467 current loss 0.119293, current_train_items 302976.
I0304 19:32:36.294150 22849303695488 run.py:483] Algo bellman_ford step 9468 current loss 0.057290, current_train_items 303008.
I0304 19:32:36.329478 22849303695488 run.py:483] Algo bellman_ford step 9469 current loss 0.080761, current_train_items 303040.
I0304 19:32:36.350265 22849303695488 run.py:483] Algo bellman_ford step 9470 current loss 0.006694, current_train_items 303072.
I0304 19:32:36.366563 22849303695488 run.py:483] Algo bellman_ford step 9471 current loss 0.021940, current_train_items 303104.
I0304 19:32:36.391101 22849303695488 run.py:483] Algo bellman_ford step 9472 current loss 0.041849, current_train_items 303136.
I0304 19:32:36.424149 22849303695488 run.py:483] Algo bellman_ford step 9473 current loss 0.045395, current_train_items 303168.
I0304 19:32:36.457882 22849303695488 run.py:483] Algo bellman_ford step 9474 current loss 0.073400, current_train_items 303200.
I0304 19:32:36.478518 22849303695488 run.py:483] Algo bellman_ford step 9475 current loss 0.005150, current_train_items 303232.
I0304 19:32:36.495700 22849303695488 run.py:483] Algo bellman_ford step 9476 current loss 0.045338, current_train_items 303264.
I0304 19:32:36.520145 22849303695488 run.py:483] Algo bellman_ford step 9477 current loss 0.025515, current_train_items 303296.
I0304 19:32:36.552103 22849303695488 run.py:483] Algo bellman_ford step 9478 current loss 0.062150, current_train_items 303328.
I0304 19:32:36.585998 22849303695488 run.py:483] Algo bellman_ford step 9479 current loss 0.111843, current_train_items 303360.
I0304 19:32:36.606338 22849303695488 run.py:483] Algo bellman_ford step 9480 current loss 0.005447, current_train_items 303392.
I0304 19:32:36.622831 22849303695488 run.py:483] Algo bellman_ford step 9481 current loss 0.014086, current_train_items 303424.
I0304 19:32:36.649123 22849303695488 run.py:483] Algo bellman_ford step 9482 current loss 0.067198, current_train_items 303456.
I0304 19:32:36.680684 22849303695488 run.py:483] Algo bellman_ford step 9483 current loss 0.057698, current_train_items 303488.
I0304 19:32:36.713991 22849303695488 run.py:483] Algo bellman_ford step 9484 current loss 0.069632, current_train_items 303520.
I0304 19:32:36.734135 22849303695488 run.py:483] Algo bellman_ford step 9485 current loss 0.003697, current_train_items 303552.
I0304 19:32:36.751123 22849303695488 run.py:483] Algo bellman_ford step 9486 current loss 0.023920, current_train_items 303584.
I0304 19:32:36.776470 22849303695488 run.py:483] Algo bellman_ford step 9487 current loss 0.045389, current_train_items 303616.
I0304 19:32:36.810149 22849303695488 run.py:483] Algo bellman_ford step 9488 current loss 0.046755, current_train_items 303648.
I0304 19:32:36.843650 22849303695488 run.py:483] Algo bellman_ford step 9489 current loss 0.065499, current_train_items 303680.
I0304 19:32:36.864266 22849303695488 run.py:483] Algo bellman_ford step 9490 current loss 0.002335, current_train_items 303712.
I0304 19:32:36.880734 22849303695488 run.py:483] Algo bellman_ford step 9491 current loss 0.021977, current_train_items 303744.
I0304 19:32:36.906018 22849303695488 run.py:483] Algo bellman_ford step 9492 current loss 0.029519, current_train_items 303776.
I0304 19:32:36.937612 22849303695488 run.py:483] Algo bellman_ford step 9493 current loss 0.035691, current_train_items 303808.
I0304 19:32:36.972432 22849303695488 run.py:483] Algo bellman_ford step 9494 current loss 0.072124, current_train_items 303840.
I0304 19:32:36.992537 22849303695488 run.py:483] Algo bellman_ford step 9495 current loss 0.011983, current_train_items 303872.
I0304 19:32:37.009614 22849303695488 run.py:483] Algo bellman_ford step 9496 current loss 0.015510, current_train_items 303904.
I0304 19:32:37.034447 22849303695488 run.py:483] Algo bellman_ford step 9497 current loss 0.042279, current_train_items 303936.
I0304 19:32:37.065997 22849303695488 run.py:483] Algo bellman_ford step 9498 current loss 0.050816, current_train_items 303968.
I0304 19:32:37.101194 22849303695488 run.py:483] Algo bellman_ford step 9499 current loss 0.058184, current_train_items 304000.
I0304 19:32:37.122105 22849303695488 run.py:483] Algo bellman_ford step 9500 current loss 0.004919, current_train_items 304032.
I0304 19:32:37.130097 22849303695488 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0304 19:32:37.130208 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:32:37.147163 22849303695488 run.py:483] Algo bellman_ford step 9501 current loss 0.023252, current_train_items 304064.
I0304 19:32:37.172647 22849303695488 run.py:483] Algo bellman_ford step 9502 current loss 0.025364, current_train_items 304096.
I0304 19:32:37.205268 22849303695488 run.py:483] Algo bellman_ford step 9503 current loss 0.036504, current_train_items 304128.
I0304 19:32:37.238862 22849303695488 run.py:483] Algo bellman_ford step 9504 current loss 0.041490, current_train_items 304160.
I0304 19:32:37.259061 22849303695488 run.py:483] Algo bellman_ford step 9505 current loss 0.004482, current_train_items 304192.
I0304 19:32:37.275602 22849303695488 run.py:483] Algo bellman_ford step 9506 current loss 0.012875, current_train_items 304224.
I0304 19:32:37.300799 22849303695488 run.py:483] Algo bellman_ford step 9507 current loss 0.060175, current_train_items 304256.
I0304 19:32:37.331789 22849303695488 run.py:483] Algo bellman_ford step 9508 current loss 0.072189, current_train_items 304288.
I0304 19:32:37.368534 22849303695488 run.py:483] Algo bellman_ford step 9509 current loss 0.138104, current_train_items 304320.
I0304 19:32:37.388826 22849303695488 run.py:483] Algo bellman_ford step 9510 current loss 0.003080, current_train_items 304352.
I0304 19:32:37.405940 22849303695488 run.py:483] Algo bellman_ford step 9511 current loss 0.012822, current_train_items 304384.
I0304 19:32:37.430748 22849303695488 run.py:483] Algo bellman_ford step 9512 current loss 0.050351, current_train_items 304416.
I0304 19:32:37.463254 22849303695488 run.py:483] Algo bellman_ford step 9513 current loss 0.048841, current_train_items 304448.
I0304 19:32:37.496404 22849303695488 run.py:483] Algo bellman_ford step 9514 current loss 0.050941, current_train_items 304480.
I0304 19:32:37.516280 22849303695488 run.py:483] Algo bellman_ford step 9515 current loss 0.051001, current_train_items 304512.
I0304 19:32:37.532689 22849303695488 run.py:483] Algo bellman_ford step 9516 current loss 0.014163, current_train_items 304544.
I0304 19:32:37.557379 22849303695488 run.py:483] Algo bellman_ford step 9517 current loss 0.024731, current_train_items 304576.
I0304 19:32:37.589729 22849303695488 run.py:483] Algo bellman_ford step 9518 current loss 0.041283, current_train_items 304608.
I0304 19:32:37.622403 22849303695488 run.py:483] Algo bellman_ford step 9519 current loss 0.048272, current_train_items 304640.
I0304 19:32:37.642353 22849303695488 run.py:483] Algo bellman_ford step 9520 current loss 0.002388, current_train_items 304672.
I0304 19:32:37.658263 22849303695488 run.py:483] Algo bellman_ford step 9521 current loss 0.007463, current_train_items 304704.
I0304 19:32:37.682250 22849303695488 run.py:483] Algo bellman_ford step 9522 current loss 0.040020, current_train_items 304736.
I0304 19:32:37.713028 22849303695488 run.py:483] Algo bellman_ford step 9523 current loss 0.026198, current_train_items 304768.
I0304 19:32:37.746383 22849303695488 run.py:483] Algo bellman_ford step 9524 current loss 0.031558, current_train_items 304800.
I0304 19:32:37.766088 22849303695488 run.py:483] Algo bellman_ford step 9525 current loss 0.002727, current_train_items 304832.
I0304 19:32:37.782749 22849303695488 run.py:483] Algo bellman_ford step 9526 current loss 0.012815, current_train_items 304864.
I0304 19:32:37.807799 22849303695488 run.py:483] Algo bellman_ford step 9527 current loss 0.042922, current_train_items 304896.
I0304 19:32:37.840720 22849303695488 run.py:483] Algo bellman_ford step 9528 current loss 0.036422, current_train_items 304928.
I0304 19:32:37.875882 22849303695488 run.py:483] Algo bellman_ford step 9529 current loss 0.061164, current_train_items 304960.
I0304 19:32:37.895717 22849303695488 run.py:483] Algo bellman_ford step 9530 current loss 0.003447, current_train_items 304992.
I0304 19:32:37.912385 22849303695488 run.py:483] Algo bellman_ford step 9531 current loss 0.020243, current_train_items 305024.
I0304 19:32:37.936605 22849303695488 run.py:483] Algo bellman_ford step 9532 current loss 0.026309, current_train_items 305056.
I0304 19:32:37.970591 22849303695488 run.py:483] Algo bellman_ford step 9533 current loss 0.161268, current_train_items 305088.
I0304 19:32:38.004319 22849303695488 run.py:483] Algo bellman_ford step 9534 current loss 0.081064, current_train_items 305120.
I0304 19:32:38.024470 22849303695488 run.py:483] Algo bellman_ford step 9535 current loss 0.018970, current_train_items 305152.
I0304 19:32:38.041089 22849303695488 run.py:483] Algo bellman_ford step 9536 current loss 0.033362, current_train_items 305184.
I0304 19:32:38.065994 22849303695488 run.py:483] Algo bellman_ford step 9537 current loss 0.068522, current_train_items 305216.
I0304 19:32:38.099430 22849303695488 run.py:483] Algo bellman_ford step 9538 current loss 0.055665, current_train_items 305248.
I0304 19:32:38.131380 22849303695488 run.py:483] Algo bellman_ford step 9539 current loss 0.048946, current_train_items 305280.
I0304 19:32:38.151350 22849303695488 run.py:483] Algo bellman_ford step 9540 current loss 0.003586, current_train_items 305312.
I0304 19:32:38.167742 22849303695488 run.py:483] Algo bellman_ford step 9541 current loss 0.035515, current_train_items 305344.
I0304 19:32:38.191945 22849303695488 run.py:483] Algo bellman_ford step 9542 current loss 0.029340, current_train_items 305376.
I0304 19:32:38.225986 22849303695488 run.py:483] Algo bellman_ford step 9543 current loss 0.069363, current_train_items 305408.
I0304 19:32:38.260120 22849303695488 run.py:483] Algo bellman_ford step 9544 current loss 0.075404, current_train_items 305440.
I0304 19:32:38.280319 22849303695488 run.py:483] Algo bellman_ford step 9545 current loss 0.003595, current_train_items 305472.
I0304 19:32:38.296281 22849303695488 run.py:483] Algo bellman_ford step 9546 current loss 0.025383, current_train_items 305504.
I0304 19:32:38.318791 22849303695488 run.py:483] Algo bellman_ford step 9547 current loss 0.017438, current_train_items 305536.
I0304 19:32:38.350687 22849303695488 run.py:483] Algo bellman_ford step 9548 current loss 0.082452, current_train_items 305568.
I0304 19:32:38.385185 22849303695488 run.py:483] Algo bellman_ford step 9549 current loss 0.108097, current_train_items 305600.
I0304 19:32:38.405054 22849303695488 run.py:483] Algo bellman_ford step 9550 current loss 0.004540, current_train_items 305632.
I0304 19:32:38.413549 22849303695488 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0304 19:32:38.413658 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:32:38.431171 22849303695488 run.py:483] Algo bellman_ford step 9551 current loss 0.021899, current_train_items 305664.
I0304 19:32:38.456721 22849303695488 run.py:483] Algo bellman_ford step 9552 current loss 0.070186, current_train_items 305696.
I0304 19:32:38.491084 22849303695488 run.py:483] Algo bellman_ford step 9553 current loss 0.054415, current_train_items 305728.
I0304 19:32:38.524733 22849303695488 run.py:483] Algo bellman_ford step 9554 current loss 0.041441, current_train_items 305760.
I0304 19:32:38.545120 22849303695488 run.py:483] Algo bellman_ford step 9555 current loss 0.002567, current_train_items 305792.
I0304 19:32:38.560955 22849303695488 run.py:483] Algo bellman_ford step 9556 current loss 0.009755, current_train_items 305824.
I0304 19:32:38.585963 22849303695488 run.py:483] Algo bellman_ford step 9557 current loss 0.060603, current_train_items 305856.
I0304 19:32:38.618043 22849303695488 run.py:483] Algo bellman_ford step 9558 current loss 0.058846, current_train_items 305888.
I0304 19:32:38.651475 22849303695488 run.py:483] Algo bellman_ford step 9559 current loss 0.060483, current_train_items 305920.
I0304 19:32:38.671890 22849303695488 run.py:483] Algo bellman_ford step 9560 current loss 0.010377, current_train_items 305952.
I0304 19:32:38.688525 22849303695488 run.py:483] Algo bellman_ford step 9561 current loss 0.006501, current_train_items 305984.
I0304 19:32:38.712428 22849303695488 run.py:483] Algo bellman_ford step 9562 current loss 0.016364, current_train_items 306016.
I0304 19:32:38.744508 22849303695488 run.py:483] Algo bellman_ford step 9563 current loss 0.036453, current_train_items 306048.
I0304 19:32:38.778052 22849303695488 run.py:483] Algo bellman_ford step 9564 current loss 0.054984, current_train_items 306080.
I0304 19:32:38.798118 22849303695488 run.py:483] Algo bellman_ford step 9565 current loss 0.044227, current_train_items 306112.
I0304 19:32:38.815171 22849303695488 run.py:483] Algo bellman_ford step 9566 current loss 0.014570, current_train_items 306144.
I0304 19:32:38.840633 22849303695488 run.py:483] Algo bellman_ford step 9567 current loss 0.071521, current_train_items 306176.
I0304 19:32:38.873340 22849303695488 run.py:483] Algo bellman_ford step 9568 current loss 0.077521, current_train_items 306208.
I0304 19:32:38.907166 22849303695488 run.py:483] Algo bellman_ford step 9569 current loss 0.049655, current_train_items 306240.
I0304 19:32:38.927378 22849303695488 run.py:483] Algo bellman_ford step 9570 current loss 0.002932, current_train_items 306272.
I0304 19:32:38.944195 22849303695488 run.py:483] Algo bellman_ford step 9571 current loss 0.010324, current_train_items 306304.
I0304 19:32:38.969378 22849303695488 run.py:483] Algo bellman_ford step 9572 current loss 0.063036, current_train_items 306336.
I0304 19:32:39.001070 22849303695488 run.py:483] Algo bellman_ford step 9573 current loss 0.042234, current_train_items 306368.
I0304 19:32:39.036061 22849303695488 run.py:483] Algo bellman_ford step 9574 current loss 0.075820, current_train_items 306400.
I0304 19:32:39.056736 22849303695488 run.py:483] Algo bellman_ford step 9575 current loss 0.005552, current_train_items 306432.
I0304 19:32:39.074161 22849303695488 run.py:483] Algo bellman_ford step 9576 current loss 0.023976, current_train_items 306464.
I0304 19:32:39.099318 22849303695488 run.py:483] Algo bellman_ford step 9577 current loss 0.051780, current_train_items 306496.
I0304 19:32:39.131232 22849303695488 run.py:483] Algo bellman_ford step 9578 current loss 0.031499, current_train_items 306528.
I0304 19:32:39.165517 22849303695488 run.py:483] Algo bellman_ford step 9579 current loss 0.115698, current_train_items 306560.
I0304 19:32:39.185588 22849303695488 run.py:483] Algo bellman_ford step 9580 current loss 0.007270, current_train_items 306592.
I0304 19:32:39.202030 22849303695488 run.py:483] Algo bellman_ford step 9581 current loss 0.020367, current_train_items 306624.
I0304 19:32:39.227804 22849303695488 run.py:483] Algo bellman_ford step 9582 current loss 0.027766, current_train_items 306656.
I0304 19:32:39.261554 22849303695488 run.py:483] Algo bellman_ford step 9583 current loss 0.033490, current_train_items 306688.
I0304 19:32:39.296778 22849303695488 run.py:483] Algo bellman_ford step 9584 current loss 0.048667, current_train_items 306720.
I0304 19:32:39.317184 22849303695488 run.py:483] Algo bellman_ford step 9585 current loss 0.015608, current_train_items 306752.
I0304 19:32:39.333941 22849303695488 run.py:483] Algo bellman_ford step 9586 current loss 0.053443, current_train_items 306784.
I0304 19:32:39.358544 22849303695488 run.py:483] Algo bellman_ford step 9587 current loss 0.053705, current_train_items 306816.
I0304 19:32:39.391459 22849303695488 run.py:483] Algo bellman_ford step 9588 current loss 0.128011, current_train_items 306848.
I0304 19:32:39.427016 22849303695488 run.py:483] Algo bellman_ford step 9589 current loss 0.197210, current_train_items 306880.
I0304 19:32:39.447241 22849303695488 run.py:483] Algo bellman_ford step 9590 current loss 0.005907, current_train_items 306912.
I0304 19:32:39.463837 22849303695488 run.py:483] Algo bellman_ford step 9591 current loss 0.016097, current_train_items 306944.
I0304 19:32:39.488091 22849303695488 run.py:483] Algo bellman_ford step 9592 current loss 0.060724, current_train_items 306976.
I0304 19:32:39.520798 22849303695488 run.py:483] Algo bellman_ford step 9593 current loss 0.097408, current_train_items 307008.
I0304 19:32:39.554380 22849303695488 run.py:483] Algo bellman_ford step 9594 current loss 0.041478, current_train_items 307040.
I0304 19:32:39.574537 22849303695488 run.py:483] Algo bellman_ford step 9595 current loss 0.004123, current_train_items 307072.
I0304 19:32:39.591368 22849303695488 run.py:483] Algo bellman_ford step 9596 current loss 0.010431, current_train_items 307104.
I0304 19:32:39.616817 22849303695488 run.py:483] Algo bellman_ford step 9597 current loss 0.068744, current_train_items 307136.
I0304 19:32:39.648358 22849303695488 run.py:483] Algo bellman_ford step 9598 current loss 0.039108, current_train_items 307168.
I0304 19:32:39.682287 22849303695488 run.py:483] Algo bellman_ford step 9599 current loss 0.050010, current_train_items 307200.
I0304 19:32:39.702634 22849303695488 run.py:483] Algo bellman_ford step 9600 current loss 0.002859, current_train_items 307232.
I0304 19:32:39.710562 22849303695488 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0304 19:32:39.710671 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:32:39.728226 22849303695488 run.py:483] Algo bellman_ford step 9601 current loss 0.018541, current_train_items 307264.
I0304 19:32:39.753874 22849303695488 run.py:483] Algo bellman_ford step 9602 current loss 0.041431, current_train_items 307296.
I0304 19:32:39.786055 22849303695488 run.py:483] Algo bellman_ford step 9603 current loss 0.032599, current_train_items 307328.
I0304 19:32:39.822192 22849303695488 run.py:483] Algo bellman_ford step 9604 current loss 0.055196, current_train_items 307360.
I0304 19:32:39.842385 22849303695488 run.py:483] Algo bellman_ford step 9605 current loss 0.004317, current_train_items 307392.
I0304 19:32:39.858575 22849303695488 run.py:483] Algo bellman_ford step 9606 current loss 0.010720, current_train_items 307424.
I0304 19:32:39.883176 22849303695488 run.py:483] Algo bellman_ford step 9607 current loss 0.040277, current_train_items 307456.
I0304 19:32:39.915203 22849303695488 run.py:483] Algo bellman_ford step 9608 current loss 0.047025, current_train_items 307488.
I0304 19:32:39.949398 22849303695488 run.py:483] Algo bellman_ford step 9609 current loss 0.065821, current_train_items 307520.
I0304 19:32:39.969757 22849303695488 run.py:483] Algo bellman_ford step 9610 current loss 0.033363, current_train_items 307552.
I0304 19:32:39.986428 22849303695488 run.py:483] Algo bellman_ford step 9611 current loss 0.034930, current_train_items 307584.
I0304 19:32:40.011822 22849303695488 run.py:483] Algo bellman_ford step 9612 current loss 0.050676, current_train_items 307616.
I0304 19:32:40.044381 22849303695488 run.py:483] Algo bellman_ford step 9613 current loss 0.051457, current_train_items 307648.
I0304 19:32:40.078496 22849303695488 run.py:483] Algo bellman_ford step 9614 current loss 0.046103, current_train_items 307680.
I0304 19:32:40.098998 22849303695488 run.py:483] Algo bellman_ford step 9615 current loss 0.003491, current_train_items 307712.
I0304 19:32:40.115767 22849303695488 run.py:483] Algo bellman_ford step 9616 current loss 0.042654, current_train_items 307744.
I0304 19:32:40.140354 22849303695488 run.py:483] Algo bellman_ford step 9617 current loss 0.081356, current_train_items 307776.
I0304 19:32:40.172394 22849303695488 run.py:483] Algo bellman_ford step 9618 current loss 0.057967, current_train_items 307808.
I0304 19:32:40.206615 22849303695488 run.py:483] Algo bellman_ford step 9619 current loss 0.030801, current_train_items 307840.
I0304 19:32:40.226763 22849303695488 run.py:483] Algo bellman_ford step 9620 current loss 0.002376, current_train_items 307872.
I0304 19:32:40.243084 22849303695488 run.py:483] Algo bellman_ford step 9621 current loss 0.010075, current_train_items 307904.
I0304 19:32:40.267978 22849303695488 run.py:483] Algo bellman_ford step 9622 current loss 0.053004, current_train_items 307936.
I0304 19:32:40.300384 22849303695488 run.py:483] Algo bellman_ford step 9623 current loss 0.059303, current_train_items 307968.
I0304 19:32:40.335520 22849303695488 run.py:483] Algo bellman_ford step 9624 current loss 0.105547, current_train_items 308000.
I0304 19:32:40.355456 22849303695488 run.py:483] Algo bellman_ford step 9625 current loss 0.007024, current_train_items 308032.
I0304 19:32:40.371982 22849303695488 run.py:483] Algo bellman_ford step 9626 current loss 0.009133, current_train_items 308064.
I0304 19:32:40.396483 22849303695488 run.py:483] Algo bellman_ford step 9627 current loss 0.040630, current_train_items 308096.
I0304 19:32:40.428731 22849303695488 run.py:483] Algo bellman_ford step 9628 current loss 0.030011, current_train_items 308128.
I0304 19:32:40.463813 22849303695488 run.py:483] Algo bellman_ford step 9629 current loss 0.073253, current_train_items 308160.
I0304 19:32:40.483789 22849303695488 run.py:483] Algo bellman_ford step 9630 current loss 0.031604, current_train_items 308192.
I0304 19:32:40.500703 22849303695488 run.py:483] Algo bellman_ford step 9631 current loss 0.012592, current_train_items 308224.
I0304 19:32:40.526263 22849303695488 run.py:483] Algo bellman_ford step 9632 current loss 0.143725, current_train_items 308256.
I0304 19:32:40.557933 22849303695488 run.py:483] Algo bellman_ford step 9633 current loss 0.064342, current_train_items 308288.
I0304 19:32:40.592908 22849303695488 run.py:483] Algo bellman_ford step 9634 current loss 0.106101, current_train_items 308320.
I0304 19:32:40.613078 22849303695488 run.py:483] Algo bellman_ford step 9635 current loss 0.002382, current_train_items 308352.
I0304 19:32:40.629581 22849303695488 run.py:483] Algo bellman_ford step 9636 current loss 0.021788, current_train_items 308384.
I0304 19:32:40.654637 22849303695488 run.py:483] Algo bellman_ford step 9637 current loss 0.024396, current_train_items 308416.
I0304 19:32:40.687631 22849303695488 run.py:483] Algo bellman_ford step 9638 current loss 0.044824, current_train_items 308448.
I0304 19:32:40.721346 22849303695488 run.py:483] Algo bellman_ford step 9639 current loss 0.135732, current_train_items 308480.
I0304 19:32:40.741413 22849303695488 run.py:483] Algo bellman_ford step 9640 current loss 0.002538, current_train_items 308512.
I0304 19:32:40.757762 22849303695488 run.py:483] Algo bellman_ford step 9641 current loss 0.008124, current_train_items 308544.
I0304 19:32:40.782819 22849303695488 run.py:483] Algo bellman_ford step 9642 current loss 0.072716, current_train_items 308576.
I0304 19:32:40.817189 22849303695488 run.py:483] Algo bellman_ford step 9643 current loss 0.031904, current_train_items 308608.
I0304 19:32:40.849303 22849303695488 run.py:483] Algo bellman_ford step 9644 current loss 0.056702, current_train_items 308640.
I0304 19:32:40.869040 22849303695488 run.py:483] Algo bellman_ford step 9645 current loss 0.020513, current_train_items 308672.
I0304 19:32:40.885708 22849303695488 run.py:483] Algo bellman_ford step 9646 current loss 0.023131, current_train_items 308704.
I0304 19:32:40.910052 22849303695488 run.py:483] Algo bellman_ford step 9647 current loss 0.023059, current_train_items 308736.
I0304 19:32:40.943131 22849303695488 run.py:483] Algo bellman_ford step 9648 current loss 0.042819, current_train_items 308768.
I0304 19:32:40.977506 22849303695488 run.py:483] Algo bellman_ford step 9649 current loss 0.053136, current_train_items 308800.
I0304 19:32:40.997458 22849303695488 run.py:483] Algo bellman_ford step 9650 current loss 0.003471, current_train_items 308832.
I0304 19:32:41.006175 22849303695488 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0304 19:32:41.006317 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:32:41.023760 22849303695488 run.py:483] Algo bellman_ford step 9651 current loss 0.017663, current_train_items 308864.
I0304 19:32:41.050274 22849303695488 run.py:483] Algo bellman_ford step 9652 current loss 0.082031, current_train_items 308896.
I0304 19:32:41.083245 22849303695488 run.py:483] Algo bellman_ford step 9653 current loss 0.161300, current_train_items 308928.
I0304 19:32:41.119281 22849303695488 run.py:483] Algo bellman_ford step 9654 current loss 0.062701, current_train_items 308960.
I0304 19:32:41.139664 22849303695488 run.py:483] Algo bellman_ford step 9655 current loss 0.010319, current_train_items 308992.
I0304 19:32:41.156125 22849303695488 run.py:483] Algo bellman_ford step 9656 current loss 0.016711, current_train_items 309024.
I0304 19:32:41.180633 22849303695488 run.py:483] Algo bellman_ford step 9657 current loss 0.071625, current_train_items 309056.
I0304 19:32:41.214133 22849303695488 run.py:483] Algo bellman_ford step 9658 current loss 0.108269, current_train_items 309088.
I0304 19:32:41.247494 22849303695488 run.py:483] Algo bellman_ford step 9659 current loss 0.060327, current_train_items 309120.
I0304 19:32:41.267710 22849303695488 run.py:483] Algo bellman_ford step 9660 current loss 0.007776, current_train_items 309152.
I0304 19:32:41.284745 22849303695488 run.py:483] Algo bellman_ford step 9661 current loss 0.018038, current_train_items 309184.
I0304 19:32:41.309085 22849303695488 run.py:483] Algo bellman_ford step 9662 current loss 0.067906, current_train_items 309216.
I0304 19:32:41.341655 22849303695488 run.py:483] Algo bellman_ford step 9663 current loss 0.038014, current_train_items 309248.
I0304 19:32:41.376209 22849303695488 run.py:483] Algo bellman_ford step 9664 current loss 0.078355, current_train_items 309280.
I0304 19:32:41.396270 22849303695488 run.py:483] Algo bellman_ford step 9665 current loss 0.012922, current_train_items 309312.
I0304 19:32:41.413001 22849303695488 run.py:483] Algo bellman_ford step 9666 current loss 0.012378, current_train_items 309344.
I0304 19:32:41.437059 22849303695488 run.py:483] Algo bellman_ford step 9667 current loss 0.058843, current_train_items 309376.
I0304 19:32:41.469154 22849303695488 run.py:483] Algo bellman_ford step 9668 current loss 0.046820, current_train_items 309408.
I0304 19:32:41.503152 22849303695488 run.py:483] Algo bellman_ford step 9669 current loss 0.074748, current_train_items 309440.
I0304 19:32:41.523817 22849303695488 run.py:483] Algo bellman_ford step 9670 current loss 0.003321, current_train_items 309472.
I0304 19:32:41.540431 22849303695488 run.py:483] Algo bellman_ford step 9671 current loss 0.012332, current_train_items 309504.
I0304 19:32:41.565030 22849303695488 run.py:483] Algo bellman_ford step 9672 current loss 0.045760, current_train_items 309536.
I0304 19:32:41.596749 22849303695488 run.py:483] Algo bellman_ford step 9673 current loss 0.034607, current_train_items 309568.
I0304 19:32:41.630815 22849303695488 run.py:483] Algo bellman_ford step 9674 current loss 0.059204, current_train_items 309600.
I0304 19:32:41.651067 22849303695488 run.py:483] Algo bellman_ford step 9675 current loss 0.001586, current_train_items 309632.
I0304 19:32:41.668234 22849303695488 run.py:483] Algo bellman_ford step 9676 current loss 0.035223, current_train_items 309664.
I0304 19:32:41.692762 22849303695488 run.py:483] Algo bellman_ford step 9677 current loss 0.038654, current_train_items 309696.
I0304 19:32:41.724183 22849303695488 run.py:483] Algo bellman_ford step 9678 current loss 0.061545, current_train_items 309728.
I0304 19:32:41.758874 22849303695488 run.py:483] Algo bellman_ford step 9679 current loss 0.059341, current_train_items 309760.
I0304 19:32:41.778917 22849303695488 run.py:483] Algo bellman_ford step 9680 current loss 0.007856, current_train_items 309792.
I0304 19:32:41.795706 22849303695488 run.py:483] Algo bellman_ford step 9681 current loss 0.011411, current_train_items 309824.
I0304 19:32:41.820122 22849303695488 run.py:483] Algo bellman_ford step 9682 current loss 0.023585, current_train_items 309856.
I0304 19:32:41.851847 22849303695488 run.py:483] Algo bellman_ford step 9683 current loss 0.044895, current_train_items 309888.
I0304 19:32:41.884872 22849303695488 run.py:483] Algo bellman_ford step 9684 current loss 0.047189, current_train_items 309920.
I0304 19:32:41.905014 22849303695488 run.py:483] Algo bellman_ford step 9685 current loss 0.003467, current_train_items 309952.
I0304 19:32:41.921573 22849303695488 run.py:483] Algo bellman_ford step 9686 current loss 0.014107, current_train_items 309984.
I0304 19:32:41.946269 22849303695488 run.py:483] Algo bellman_ford step 9687 current loss 0.108052, current_train_items 310016.
I0304 19:32:41.978540 22849303695488 run.py:483] Algo bellman_ford step 9688 current loss 0.083961, current_train_items 310048.
I0304 19:32:42.012853 22849303695488 run.py:483] Algo bellman_ford step 9689 current loss 0.092317, current_train_items 310080.
I0304 19:32:42.033240 22849303695488 run.py:483] Algo bellman_ford step 9690 current loss 0.004328, current_train_items 310112.
I0304 19:32:42.049653 22849303695488 run.py:483] Algo bellman_ford step 9691 current loss 0.006907, current_train_items 310144.
I0304 19:32:42.074109 22849303695488 run.py:483] Algo bellman_ford step 9692 current loss 0.024810, current_train_items 310176.
I0304 19:32:42.106012 22849303695488 run.py:483] Algo bellman_ford step 9693 current loss 0.046153, current_train_items 310208.
I0304 19:32:42.140326 22849303695488 run.py:483] Algo bellman_ford step 9694 current loss 0.034689, current_train_items 310240.
I0304 19:32:42.159981 22849303695488 run.py:483] Algo bellman_ford step 9695 current loss 0.008245, current_train_items 310272.
I0304 19:32:42.176566 22849303695488 run.py:483] Algo bellman_ford step 9696 current loss 0.068142, current_train_items 310304.
I0304 19:32:42.200781 22849303695488 run.py:483] Algo bellman_ford step 9697 current loss 0.019079, current_train_items 310336.
I0304 19:32:42.235771 22849303695488 run.py:483] Algo bellman_ford step 9698 current loss 0.126188, current_train_items 310368.
I0304 19:32:42.271379 22849303695488 run.py:483] Algo bellman_ford step 9699 current loss 0.079430, current_train_items 310400.
I0304 19:32:42.291626 22849303695488 run.py:483] Algo bellman_ford step 9700 current loss 0.006140, current_train_items 310432.
I0304 19:32:42.299606 22849303695488 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0304 19:32:42.299715 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:42.316833 22849303695488 run.py:483] Algo bellman_ford step 9701 current loss 0.010881, current_train_items 310464.
I0304 19:32:42.342901 22849303695488 run.py:483] Algo bellman_ford step 9702 current loss 0.034029, current_train_items 310496.
I0304 19:32:42.376696 22849303695488 run.py:483] Algo bellman_ford step 9703 current loss 0.037140, current_train_items 310528.
I0304 19:32:42.410647 22849303695488 run.py:483] Algo bellman_ford step 9704 current loss 0.041249, current_train_items 310560.
I0304 19:32:42.430984 22849303695488 run.py:483] Algo bellman_ford step 9705 current loss 0.002233, current_train_items 310592.
I0304 19:32:42.447467 22849303695488 run.py:483] Algo bellman_ford step 9706 current loss 0.016231, current_train_items 310624.
I0304 19:32:42.473189 22849303695488 run.py:483] Algo bellman_ford step 9707 current loss 0.027480, current_train_items 310656.
I0304 19:32:42.506342 22849303695488 run.py:483] Algo bellman_ford step 9708 current loss 0.055235, current_train_items 310688.
I0304 19:32:42.538912 22849303695488 run.py:483] Algo bellman_ford step 9709 current loss 0.064624, current_train_items 310720.
I0304 19:32:42.558997 22849303695488 run.py:483] Algo bellman_ford step 9710 current loss 0.001608, current_train_items 310752.
I0304 19:32:42.575489 22849303695488 run.py:483] Algo bellman_ford step 9711 current loss 0.010987, current_train_items 310784.
I0304 19:32:42.600238 22849303695488 run.py:483] Algo bellman_ford step 9712 current loss 0.025391, current_train_items 310816.
I0304 19:32:42.633148 22849303695488 run.py:483] Algo bellman_ford step 9713 current loss 0.032271, current_train_items 310848.
I0304 19:32:42.668108 22849303695488 run.py:483] Algo bellman_ford step 9714 current loss 0.041025, current_train_items 310880.
I0304 19:32:42.687930 22849303695488 run.py:483] Algo bellman_ford step 9715 current loss 0.002330, current_train_items 310912.
I0304 19:32:42.705172 22849303695488 run.py:483] Algo bellman_ford step 9716 current loss 0.036921, current_train_items 310944.
I0304 19:32:42.729021 22849303695488 run.py:483] Algo bellman_ford step 9717 current loss 0.035801, current_train_items 310976.
I0304 19:32:42.761737 22849303695488 run.py:483] Algo bellman_ford step 9718 current loss 0.097356, current_train_items 311008.
I0304 19:32:42.793427 22849303695488 run.py:483] Algo bellman_ford step 9719 current loss 0.088397, current_train_items 311040.
I0304 19:32:42.813219 22849303695488 run.py:483] Algo bellman_ford step 9720 current loss 0.006103, current_train_items 311072.
I0304 19:32:42.829345 22849303695488 run.py:483] Algo bellman_ford step 9721 current loss 0.005202, current_train_items 311104.
I0304 19:32:42.853441 22849303695488 run.py:483] Algo bellman_ford step 9722 current loss 0.027981, current_train_items 311136.
I0304 19:32:42.885975 22849303695488 run.py:483] Algo bellman_ford step 9723 current loss 0.035032, current_train_items 311168.
I0304 19:32:42.920206 22849303695488 run.py:483] Algo bellman_ford step 9724 current loss 0.045601, current_train_items 311200.
I0304 19:32:42.940288 22849303695488 run.py:483] Algo bellman_ford step 9725 current loss 0.002182, current_train_items 311232.
I0304 19:32:42.956624 22849303695488 run.py:483] Algo bellman_ford step 9726 current loss 0.009120, current_train_items 311264.
I0304 19:32:42.980387 22849303695488 run.py:483] Algo bellman_ford step 9727 current loss 0.010908, current_train_items 311296.
I0304 19:32:43.013204 22849303695488 run.py:483] Algo bellman_ford step 9728 current loss 0.025820, current_train_items 311328.
I0304 19:32:43.047805 22849303695488 run.py:483] Algo bellman_ford step 9729 current loss 0.045889, current_train_items 311360.
I0304 19:32:43.068119 22849303695488 run.py:483] Algo bellman_ford step 9730 current loss 0.009292, current_train_items 311392.
I0304 19:32:43.084896 22849303695488 run.py:483] Algo bellman_ford step 9731 current loss 0.011214, current_train_items 311424.
I0304 19:32:43.109328 22849303695488 run.py:483] Algo bellman_ford step 9732 current loss 0.035053, current_train_items 311456.
I0304 19:32:43.141538 22849303695488 run.py:483] Algo bellman_ford step 9733 current loss 0.108246, current_train_items 311488.
I0304 19:32:43.174765 22849303695488 run.py:483] Algo bellman_ford step 9734 current loss 0.064710, current_train_items 311520.
I0304 19:32:43.194744 22849303695488 run.py:483] Algo bellman_ford step 9735 current loss 0.005436, current_train_items 311552.
I0304 19:32:43.211329 22849303695488 run.py:483] Algo bellman_ford step 9736 current loss 0.011377, current_train_items 311584.
I0304 19:32:43.235615 22849303695488 run.py:483] Algo bellman_ford step 9737 current loss 0.034793, current_train_items 311616.
I0304 19:32:43.269586 22849303695488 run.py:483] Algo bellman_ford step 9738 current loss 0.075269, current_train_items 311648.
I0304 19:32:43.304582 22849303695488 run.py:483] Algo bellman_ford step 9739 current loss 0.059716, current_train_items 311680.
I0304 19:32:43.324307 22849303695488 run.py:483] Algo bellman_ford step 9740 current loss 0.002159, current_train_items 311712.
I0304 19:32:43.341604 22849303695488 run.py:483] Algo bellman_ford step 9741 current loss 0.021562, current_train_items 311744.
I0304 19:32:43.368017 22849303695488 run.py:483] Algo bellman_ford step 9742 current loss 0.074769, current_train_items 311776.
I0304 19:32:43.401195 22849303695488 run.py:483] Algo bellman_ford step 9743 current loss 0.048733, current_train_items 311808.
I0304 19:32:43.435573 22849303695488 run.py:483] Algo bellman_ford step 9744 current loss 0.054439, current_train_items 311840.
I0304 19:32:43.455675 22849303695488 run.py:483] Algo bellman_ford step 9745 current loss 0.059186, current_train_items 311872.
I0304 19:32:43.472998 22849303695488 run.py:483] Algo bellman_ford step 9746 current loss 0.016255, current_train_items 311904.
I0304 19:32:43.497219 22849303695488 run.py:483] Algo bellman_ford step 9747 current loss 0.016995, current_train_items 311936.
I0304 19:32:43.527872 22849303695488 run.py:483] Algo bellman_ford step 9748 current loss 0.034447, current_train_items 311968.
I0304 19:32:43.560670 22849303695488 run.py:483] Algo bellman_ford step 9749 current loss 0.047038, current_train_items 312000.
I0304 19:32:43.580895 22849303695488 run.py:483] Algo bellman_ford step 9750 current loss 0.001750, current_train_items 312032.
I0304 19:32:43.589315 22849303695488 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0304 19:32:43.589426 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:32:43.606718 22849303695488 run.py:483] Algo bellman_ford step 9751 current loss 0.013367, current_train_items 312064.
I0304 19:32:43.633117 22849303695488 run.py:483] Algo bellman_ford step 9752 current loss 0.032892, current_train_items 312096.
I0304 19:32:43.665051 22849303695488 run.py:483] Algo bellman_ford step 9753 current loss 0.037784, current_train_items 312128.
I0304 19:32:43.698629 22849303695488 run.py:483] Algo bellman_ford step 9754 current loss 0.049803, current_train_items 312160.
I0304 19:32:43.718781 22849303695488 run.py:483] Algo bellman_ford step 9755 current loss 0.020208, current_train_items 312192.
I0304 19:32:43.735868 22849303695488 run.py:483] Algo bellman_ford step 9756 current loss 0.026167, current_train_items 312224.
I0304 19:32:43.759392 22849303695488 run.py:483] Algo bellman_ford step 9757 current loss 0.045523, current_train_items 312256.
I0304 19:32:43.792199 22849303695488 run.py:483] Algo bellman_ford step 9758 current loss 0.080641, current_train_items 312288.
I0304 19:32:43.826156 22849303695488 run.py:483] Algo bellman_ford step 9759 current loss 0.060254, current_train_items 312320.
I0304 19:32:43.846664 22849303695488 run.py:483] Algo bellman_ford step 9760 current loss 0.005524, current_train_items 312352.
I0304 19:32:43.863724 22849303695488 run.py:483] Algo bellman_ford step 9761 current loss 0.025646, current_train_items 312384.
I0304 19:32:43.888682 22849303695488 run.py:483] Algo bellman_ford step 9762 current loss 0.055672, current_train_items 312416.
I0304 19:32:43.921520 22849303695488 run.py:483] Algo bellman_ford step 9763 current loss 0.112572, current_train_items 312448.
I0304 19:32:43.952869 22849303695488 run.py:483] Algo bellman_ford step 9764 current loss 0.043992, current_train_items 312480.
I0304 19:32:43.972856 22849303695488 run.py:483] Algo bellman_ford step 9765 current loss 0.009798, current_train_items 312512.
I0304 19:32:43.989452 22849303695488 run.py:483] Algo bellman_ford step 9766 current loss 0.019464, current_train_items 312544.
I0304 19:32:44.014289 22849303695488 run.py:483] Algo bellman_ford step 9767 current loss 0.084855, current_train_items 312576.
I0304 19:32:44.046336 22849303695488 run.py:483] Algo bellman_ford step 9768 current loss 0.085391, current_train_items 312608.
I0304 19:32:44.081129 22849303695488 run.py:483] Algo bellman_ford step 9769 current loss 0.104588, current_train_items 312640.
I0304 19:32:44.101583 22849303695488 run.py:483] Algo bellman_ford step 9770 current loss 0.002308, current_train_items 312672.
I0304 19:32:44.117940 22849303695488 run.py:483] Algo bellman_ford step 9771 current loss 0.071870, current_train_items 312704.
I0304 19:32:44.141539 22849303695488 run.py:483] Algo bellman_ford step 9772 current loss 0.065247, current_train_items 312736.
I0304 19:32:44.175101 22849303695488 run.py:483] Algo bellman_ford step 9773 current loss 0.081345, current_train_items 312768.
I0304 19:32:44.209234 22849303695488 run.py:483] Algo bellman_ford step 9774 current loss 0.073185, current_train_items 312800.
I0304 19:32:44.229341 22849303695488 run.py:483] Algo bellman_ford step 9775 current loss 0.003512, current_train_items 312832.
I0304 19:32:44.246132 22849303695488 run.py:483] Algo bellman_ford step 9776 current loss 0.127799, current_train_items 312864.
I0304 19:32:44.271021 22849303695488 run.py:483] Algo bellman_ford step 9777 current loss 0.036868, current_train_items 312896.
I0304 19:32:44.303934 22849303695488 run.py:483] Algo bellman_ford step 9778 current loss 0.338756, current_train_items 312928.
I0304 19:32:44.338175 22849303695488 run.py:483] Algo bellman_ford step 9779 current loss 0.050467, current_train_items 312960.
I0304 19:32:44.358108 22849303695488 run.py:483] Algo bellman_ford step 9780 current loss 0.004459, current_train_items 312992.
I0304 19:32:44.374833 22849303695488 run.py:483] Algo bellman_ford step 9781 current loss 0.008666, current_train_items 313024.
I0304 19:32:44.398990 22849303695488 run.py:483] Algo bellman_ford step 9782 current loss 0.043126, current_train_items 313056.
I0304 19:32:44.433309 22849303695488 run.py:483] Algo bellman_ford step 9783 current loss 0.096322, current_train_items 313088.
I0304 19:32:44.465852 22849303695488 run.py:483] Algo bellman_ford step 9784 current loss 0.049405, current_train_items 313120.
I0304 19:32:44.486353 22849303695488 run.py:483] Algo bellman_ford step 9785 current loss 0.004178, current_train_items 313152.
I0304 19:32:44.503495 22849303695488 run.py:483] Algo bellman_ford step 9786 current loss 0.019155, current_train_items 313184.
I0304 19:32:44.527482 22849303695488 run.py:483] Algo bellman_ford step 9787 current loss 0.029084, current_train_items 313216.
I0304 19:32:44.559374 22849303695488 run.py:483] Algo bellman_ford step 9788 current loss 0.054017, current_train_items 313248.
I0304 19:32:44.593687 22849303695488 run.py:483] Algo bellman_ford step 9789 current loss 0.074526, current_train_items 313280.
I0304 19:32:44.614065 22849303695488 run.py:483] Algo bellman_ford step 9790 current loss 0.017033, current_train_items 313312.
I0304 19:32:44.630567 22849303695488 run.py:483] Algo bellman_ford step 9791 current loss 0.021774, current_train_items 313344.
I0304 19:32:44.654398 22849303695488 run.py:483] Algo bellman_ford step 9792 current loss 0.063778, current_train_items 313376.
I0304 19:32:44.686702 22849303695488 run.py:483] Algo bellman_ford step 9793 current loss 0.060059, current_train_items 313408.
I0304 19:32:44.719036 22849303695488 run.py:483] Algo bellman_ford step 9794 current loss 0.035391, current_train_items 313440.
I0304 19:32:44.739170 22849303695488 run.py:483] Algo bellman_ford step 9795 current loss 0.006417, current_train_items 313472.
I0304 19:32:44.756028 22849303695488 run.py:483] Algo bellman_ford step 9796 current loss 0.011449, current_train_items 313504.
I0304 19:32:44.780491 22849303695488 run.py:483] Algo bellman_ford step 9797 current loss 0.049783, current_train_items 313536.
I0304 19:32:44.812326 22849303695488 run.py:483] Algo bellman_ford step 9798 current loss 0.035930, current_train_items 313568.
I0304 19:32:44.845633 22849303695488 run.py:483] Algo bellman_ford step 9799 current loss 0.069037, current_train_items 313600.
I0304 19:32:44.865955 22849303695488 run.py:483] Algo bellman_ford step 9800 current loss 0.058543, current_train_items 313632.
I0304 19:32:44.874086 22849303695488 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0304 19:32:44.874193 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:44.891084 22849303695488 run.py:483] Algo bellman_ford step 9801 current loss 0.026658, current_train_items 313664.
I0304 19:32:44.916638 22849303695488 run.py:483] Algo bellman_ford step 9802 current loss 0.048578, current_train_items 313696.
I0304 19:32:44.950380 22849303695488 run.py:483] Algo bellman_ford step 9803 current loss 0.061394, current_train_items 313728.
I0304 19:32:44.983887 22849303695488 run.py:483] Algo bellman_ford step 9804 current loss 0.049913, current_train_items 313760.
I0304 19:32:45.004506 22849303695488 run.py:483] Algo bellman_ford step 9805 current loss 0.005981, current_train_items 313792.
I0304 19:32:45.021071 22849303695488 run.py:483] Algo bellman_ford step 9806 current loss 0.012869, current_train_items 313824.
I0304 19:32:45.045128 22849303695488 run.py:483] Algo bellman_ford step 9807 current loss 0.032396, current_train_items 313856.
I0304 19:32:45.077973 22849303695488 run.py:483] Algo bellman_ford step 9808 current loss 0.044622, current_train_items 313888.
I0304 19:32:45.114361 22849303695488 run.py:483] Algo bellman_ford step 9809 current loss 0.064883, current_train_items 313920.
I0304 19:32:45.134839 22849303695488 run.py:483] Algo bellman_ford step 9810 current loss 0.003886, current_train_items 313952.
I0304 19:32:45.151241 22849303695488 run.py:483] Algo bellman_ford step 9811 current loss 0.007900, current_train_items 313984.
I0304 19:32:45.176954 22849303695488 run.py:483] Algo bellman_ford step 9812 current loss 0.043085, current_train_items 314016.
I0304 19:32:45.207283 22849303695488 run.py:483] Algo bellman_ford step 9813 current loss 0.051431, current_train_items 314048.
I0304 19:32:45.241883 22849303695488 run.py:483] Algo bellman_ford step 9814 current loss 0.059300, current_train_items 314080.
I0304 19:32:45.262302 22849303695488 run.py:483] Algo bellman_ford step 9815 current loss 0.002893, current_train_items 314112.
I0304 19:32:45.279145 22849303695488 run.py:483] Algo bellman_ford step 9816 current loss 0.018658, current_train_items 314144.
I0304 19:32:45.303660 22849303695488 run.py:483] Algo bellman_ford step 9817 current loss 0.014886, current_train_items 314176.
I0304 19:32:45.337609 22849303695488 run.py:483] Algo bellman_ford step 9818 current loss 0.050049, current_train_items 314208.
I0304 19:32:45.371356 22849303695488 run.py:483] Algo bellman_ford step 9819 current loss 0.067901, current_train_items 314240.
I0304 19:32:45.391378 22849303695488 run.py:483] Algo bellman_ford step 9820 current loss 0.001774, current_train_items 314272.
I0304 19:32:45.407966 22849303695488 run.py:483] Algo bellman_ford step 9821 current loss 0.026039, current_train_items 314304.
I0304 19:32:45.433081 22849303695488 run.py:483] Algo bellman_ford step 9822 current loss 0.066070, current_train_items 314336.
I0304 19:32:45.465025 22849303695488 run.py:483] Algo bellman_ford step 9823 current loss 0.049204, current_train_items 314368.
I0304 19:32:45.499128 22849303695488 run.py:483] Algo bellman_ford step 9824 current loss 0.065291, current_train_items 314400.
I0304 19:32:45.519615 22849303695488 run.py:483] Algo bellman_ford step 9825 current loss 0.001830, current_train_items 314432.
I0304 19:32:45.535954 22849303695488 run.py:483] Algo bellman_ford step 9826 current loss 0.026887, current_train_items 314464.
I0304 19:32:45.560468 22849303695488 run.py:483] Algo bellman_ford step 9827 current loss 0.054217, current_train_items 314496.
I0304 19:32:45.592057 22849303695488 run.py:483] Algo bellman_ford step 9828 current loss 0.054192, current_train_items 314528.
I0304 19:32:45.627177 22849303695488 run.py:483] Algo bellman_ford step 9829 current loss 0.048411, current_train_items 314560.
I0304 19:32:45.647317 22849303695488 run.py:483] Algo bellman_ford step 9830 current loss 0.002309, current_train_items 314592.
I0304 19:32:45.664570 22849303695488 run.py:483] Algo bellman_ford step 9831 current loss 0.020296, current_train_items 314624.
I0304 19:32:45.689243 22849303695488 run.py:483] Algo bellman_ford step 9832 current loss 0.036612, current_train_items 314656.
I0304 19:32:45.722503 22849303695488 run.py:483] Algo bellman_ford step 9833 current loss 0.059506, current_train_items 314688.
I0304 19:32:45.755355 22849303695488 run.py:483] Algo bellman_ford step 9834 current loss 0.036621, current_train_items 314720.
I0304 19:32:45.775700 22849303695488 run.py:483] Algo bellman_ford step 9835 current loss 0.015383, current_train_items 314752.
I0304 19:32:45.792207 22849303695488 run.py:483] Algo bellman_ford step 9836 current loss 0.011274, current_train_items 314784.
I0304 19:32:45.816923 22849303695488 run.py:483] Algo bellman_ford step 9837 current loss 0.026351, current_train_items 314816.
I0304 19:32:45.850825 22849303695488 run.py:483] Algo bellman_ford step 9838 current loss 0.056277, current_train_items 314848.
I0304 19:32:45.886742 22849303695488 run.py:483] Algo bellman_ford step 9839 current loss 0.067622, current_train_items 314880.
I0304 19:32:45.906886 22849303695488 run.py:483] Algo bellman_ford step 9840 current loss 0.003529, current_train_items 314912.
I0304 19:32:45.923475 22849303695488 run.py:483] Algo bellman_ford step 9841 current loss 0.023025, current_train_items 314944.
I0304 19:32:45.948853 22849303695488 run.py:483] Algo bellman_ford step 9842 current loss 0.028795, current_train_items 314976.
I0304 19:32:45.980981 22849303695488 run.py:483] Algo bellman_ford step 9843 current loss 0.087608, current_train_items 315008.
I0304 19:32:46.013231 22849303695488 run.py:483] Algo bellman_ford step 9844 current loss 0.127896, current_train_items 315040.
I0304 19:32:46.033520 22849303695488 run.py:483] Algo bellman_ford step 9845 current loss 0.002279, current_train_items 315072.
I0304 19:32:46.050489 22849303695488 run.py:483] Algo bellman_ford step 9846 current loss 0.013765, current_train_items 315104.
I0304 19:32:46.076115 22849303695488 run.py:483] Algo bellman_ford step 9847 current loss 0.032529, current_train_items 315136.
I0304 19:32:46.108510 22849303695488 run.py:483] Algo bellman_ford step 9848 current loss 0.057750, current_train_items 315168.
I0304 19:32:46.142855 22849303695488 run.py:483] Algo bellman_ford step 9849 current loss 0.041679, current_train_items 315200.
I0304 19:32:46.163053 22849303695488 run.py:483] Algo bellman_ford step 9850 current loss 0.006250, current_train_items 315232.
I0304 19:32:46.171356 22849303695488 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0304 19:32:46.171465 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:32:46.189275 22849303695488 run.py:483] Algo bellman_ford step 9851 current loss 0.039671, current_train_items 315264.
I0304 19:32:46.216057 22849303695488 run.py:483] Algo bellman_ford step 9852 current loss 0.060089, current_train_items 315296.
I0304 19:32:46.248644 22849303695488 run.py:483] Algo bellman_ford step 9853 current loss 0.061486, current_train_items 315328.
I0304 19:32:46.283539 22849303695488 run.py:483] Algo bellman_ford step 9854 current loss 0.051625, current_train_items 315360.
I0304 19:32:46.304138 22849303695488 run.py:483] Algo bellman_ford step 9855 current loss 0.004129, current_train_items 315392.
I0304 19:32:46.320597 22849303695488 run.py:483] Algo bellman_ford step 9856 current loss 0.025823, current_train_items 315424.
I0304 19:32:46.345957 22849303695488 run.py:483] Algo bellman_ford step 9857 current loss 0.062903, current_train_items 315456.
I0304 19:32:46.378130 22849303695488 run.py:483] Algo bellman_ford step 9858 current loss 0.044240, current_train_items 315488.
I0304 19:32:46.412857 22849303695488 run.py:483] Algo bellman_ford step 9859 current loss 0.088522, current_train_items 315520.
I0304 19:32:46.433960 22849303695488 run.py:483] Algo bellman_ford step 9860 current loss 0.004158, current_train_items 315552.
I0304 19:32:46.450660 22849303695488 run.py:483] Algo bellman_ford step 9861 current loss 0.030070, current_train_items 315584.
I0304 19:32:46.475866 22849303695488 run.py:483] Algo bellman_ford step 9862 current loss 0.035001, current_train_items 315616.
I0304 19:32:46.508678 22849303695488 run.py:483] Algo bellman_ford step 9863 current loss 0.130439, current_train_items 315648.
I0304 19:32:46.542851 22849303695488 run.py:483] Algo bellman_ford step 9864 current loss 0.064389, current_train_items 315680.
I0304 19:32:46.563506 22849303695488 run.py:483] Algo bellman_ford step 9865 current loss 0.003515, current_train_items 315712.
I0304 19:32:46.579860 22849303695488 run.py:483] Algo bellman_ford step 9866 current loss 0.010601, current_train_items 315744.
I0304 19:32:46.604752 22849303695488 run.py:483] Algo bellman_ford step 9867 current loss 0.045305, current_train_items 315776.
I0304 19:32:46.637828 22849303695488 run.py:483] Algo bellman_ford step 9868 current loss 0.064336, current_train_items 315808.
I0304 19:32:46.672163 22849303695488 run.py:483] Algo bellman_ford step 9869 current loss 0.049620, current_train_items 315840.
I0304 19:32:46.693092 22849303695488 run.py:483] Algo bellman_ford step 9870 current loss 0.004614, current_train_items 315872.
I0304 19:32:46.710335 22849303695488 run.py:483] Algo bellman_ford step 9871 current loss 0.045270, current_train_items 315904.
I0304 19:32:46.735536 22849303695488 run.py:483] Algo bellman_ford step 9872 current loss 0.050006, current_train_items 315936.
I0304 19:32:46.768842 22849303695488 run.py:483] Algo bellman_ford step 9873 current loss 0.031566, current_train_items 315968.
I0304 19:32:46.802176 22849303695488 run.py:483] Algo bellman_ford step 9874 current loss 0.074343, current_train_items 316000.
I0304 19:32:46.822482 22849303695488 run.py:483] Algo bellman_ford step 9875 current loss 0.001746, current_train_items 316032.
I0304 19:32:46.839132 22849303695488 run.py:483] Algo bellman_ford step 9876 current loss 0.022114, current_train_items 316064.
I0304 19:32:46.862090 22849303695488 run.py:483] Algo bellman_ford step 9877 current loss 0.029432, current_train_items 316096.
I0304 19:32:46.894301 22849303695488 run.py:483] Algo bellman_ford step 9878 current loss 0.040930, current_train_items 316128.
I0304 19:32:46.928042 22849303695488 run.py:483] Algo bellman_ford step 9879 current loss 0.057928, current_train_items 316160.
I0304 19:32:46.948199 22849303695488 run.py:483] Algo bellman_ford step 9880 current loss 0.009571, current_train_items 316192.
I0304 19:32:46.964908 22849303695488 run.py:483] Algo bellman_ford step 9881 current loss 0.050286, current_train_items 316224.
I0304 19:32:46.989438 22849303695488 run.py:483] Algo bellman_ford step 9882 current loss 0.017580, current_train_items 316256.
I0304 19:32:47.022088 22849303695488 run.py:483] Algo bellman_ford step 9883 current loss 0.043107, current_train_items 316288.
I0304 19:32:47.056757 22849303695488 run.py:483] Algo bellman_ford step 9884 current loss 0.068196, current_train_items 316320.
I0304 19:32:47.077195 22849303695488 run.py:483] Algo bellman_ford step 9885 current loss 0.002947, current_train_items 316352.
I0304 19:32:47.093758 22849303695488 run.py:483] Algo bellman_ford step 9886 current loss 0.007027, current_train_items 316384.
I0304 19:32:47.118089 22849303695488 run.py:483] Algo bellman_ford step 9887 current loss 0.019789, current_train_items 316416.
I0304 19:32:47.150557 22849303695488 run.py:483] Algo bellman_ford step 9888 current loss 0.043224, current_train_items 316448.
I0304 19:32:47.183829 22849303695488 run.py:483] Algo bellman_ford step 9889 current loss 0.050649, current_train_items 316480.
I0304 19:32:47.204210 22849303695488 run.py:483] Algo bellman_ford step 9890 current loss 0.007664, current_train_items 316512.
I0304 19:32:47.220850 22849303695488 run.py:483] Algo bellman_ford step 9891 current loss 0.039203, current_train_items 316544.
I0304 19:32:47.244620 22849303695488 run.py:483] Algo bellman_ford step 9892 current loss 0.044961, current_train_items 316576.
I0304 19:32:47.277954 22849303695488 run.py:483] Algo bellman_ford step 9893 current loss 0.038151, current_train_items 316608.
I0304 19:32:47.313342 22849303695488 run.py:483] Algo bellman_ford step 9894 current loss 0.057644, current_train_items 316640.
I0304 19:32:47.333937 22849303695488 run.py:483] Algo bellman_ford step 9895 current loss 0.005085, current_train_items 316672.
I0304 19:32:47.350325 22849303695488 run.py:483] Algo bellman_ford step 9896 current loss 0.025150, current_train_items 316704.
I0304 19:32:47.375618 22849303695488 run.py:483] Algo bellman_ford step 9897 current loss 0.066497, current_train_items 316736.
I0304 19:32:47.408516 22849303695488 run.py:483] Algo bellman_ford step 9898 current loss 0.103447, current_train_items 316768.
I0304 19:32:47.444492 22849303695488 run.py:483] Algo bellman_ford step 9899 current loss 0.043564, current_train_items 316800.
I0304 19:32:47.464786 22849303695488 run.py:483] Algo bellman_ford step 9900 current loss 0.003489, current_train_items 316832.
I0304 19:32:47.472997 22849303695488 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0304 19:32:47.473116 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:32:47.490327 22849303695488 run.py:483] Algo bellman_ford step 9901 current loss 0.020951, current_train_items 316864.
I0304 19:32:47.515407 22849303695488 run.py:483] Algo bellman_ford step 9902 current loss 0.039321, current_train_items 316896.
I0304 19:32:47.547980 22849303695488 run.py:483] Algo bellman_ford step 9903 current loss 0.025540, current_train_items 316928.
I0304 19:32:47.583333 22849303695488 run.py:483] Algo bellman_ford step 9904 current loss 0.045161, current_train_items 316960.
I0304 19:32:47.603549 22849303695488 run.py:483] Algo bellman_ford step 9905 current loss 0.003061, current_train_items 316992.
I0304 19:32:47.620200 22849303695488 run.py:483] Algo bellman_ford step 9906 current loss 0.062418, current_train_items 317024.
I0304 19:32:47.645412 22849303695488 run.py:483] Algo bellman_ford step 9907 current loss 0.079520, current_train_items 317056.
I0304 19:32:47.678135 22849303695488 run.py:483] Algo bellman_ford step 9908 current loss 0.043135, current_train_items 317088.
I0304 19:32:47.711085 22849303695488 run.py:483] Algo bellman_ford step 9909 current loss 0.043055, current_train_items 317120.
I0304 19:32:47.731535 22849303695488 run.py:483] Algo bellman_ford step 9910 current loss 0.010535, current_train_items 317152.
I0304 19:32:47.748473 22849303695488 run.py:483] Algo bellman_ford step 9911 current loss 0.008041, current_train_items 317184.
I0304 19:32:47.773386 22849303695488 run.py:483] Algo bellman_ford step 9912 current loss 0.063793, current_train_items 317216.
I0304 19:32:47.806847 22849303695488 run.py:483] Algo bellman_ford step 9913 current loss 0.039516, current_train_items 317248.
I0304 19:32:47.840986 22849303695488 run.py:483] Algo bellman_ford step 9914 current loss 0.060367, current_train_items 317280.
I0304 19:32:47.861263 22849303695488 run.py:483] Algo bellman_ford step 9915 current loss 0.018997, current_train_items 317312.
I0304 19:32:47.878125 22849303695488 run.py:483] Algo bellman_ford step 9916 current loss 0.016114, current_train_items 317344.
I0304 19:32:47.903445 22849303695488 run.py:483] Algo bellman_ford step 9917 current loss 0.043585, current_train_items 317376.
I0304 19:32:47.936553 22849303695488 run.py:483] Algo bellman_ford step 9918 current loss 0.059988, current_train_items 317408.
I0304 19:32:47.970125 22849303695488 run.py:483] Algo bellman_ford step 9919 current loss 0.047045, current_train_items 317440.
I0304 19:32:47.990293 22849303695488 run.py:483] Algo bellman_ford step 9920 current loss 0.002012, current_train_items 317472.
I0304 19:32:48.007280 22849303695488 run.py:483] Algo bellman_ford step 9921 current loss 0.009426, current_train_items 317504.
I0304 19:32:48.032544 22849303695488 run.py:483] Algo bellman_ford step 9922 current loss 0.049781, current_train_items 317536.
I0304 19:32:48.063457 22849303695488 run.py:483] Algo bellman_ford step 9923 current loss 0.065882, current_train_items 317568.
I0304 19:32:48.099925 22849303695488 run.py:483] Algo bellman_ford step 9924 current loss 0.062365, current_train_items 317600.
I0304 19:32:48.120093 22849303695488 run.py:483] Algo bellman_ford step 9925 current loss 0.026489, current_train_items 317632.
I0304 19:32:48.136466 22849303695488 run.py:483] Algo bellman_ford step 9926 current loss 0.016789, current_train_items 317664.
I0304 19:32:48.162105 22849303695488 run.py:483] Algo bellman_ford step 9927 current loss 0.054536, current_train_items 317696.
I0304 19:32:48.195929 22849303695488 run.py:483] Algo bellman_ford step 9928 current loss 0.055428, current_train_items 317728.
I0304 19:32:48.228074 22849303695488 run.py:483] Algo bellman_ford step 9929 current loss 0.106579, current_train_items 317760.
I0304 19:32:48.248340 22849303695488 run.py:483] Algo bellman_ford step 9930 current loss 0.001300, current_train_items 317792.
I0304 19:32:48.264298 22849303695488 run.py:483] Algo bellman_ford step 9931 current loss 0.014734, current_train_items 317824.
I0304 19:32:48.289292 22849303695488 run.py:483] Algo bellman_ford step 9932 current loss 0.030652, current_train_items 317856.
I0304 19:32:48.321420 22849303695488 run.py:483] Algo bellman_ford step 9933 current loss 0.092507, current_train_items 317888.
I0304 19:32:48.354521 22849303695488 run.py:483] Algo bellman_ford step 9934 current loss 0.087695, current_train_items 317920.
I0304 19:32:48.374713 22849303695488 run.py:483] Algo bellman_ford step 9935 current loss 0.002065, current_train_items 317952.
I0304 19:32:48.390976 22849303695488 run.py:483] Algo bellman_ford step 9936 current loss 0.007199, current_train_items 317984.
I0304 19:32:48.415774 22849303695488 run.py:483] Algo bellman_ford step 9937 current loss 0.047431, current_train_items 318016.
I0304 19:32:48.447911 22849303695488 run.py:483] Algo bellman_ford step 9938 current loss 0.028655, current_train_items 318048.
I0304 19:32:48.482573 22849303695488 run.py:483] Algo bellman_ford step 9939 current loss 0.107541, current_train_items 318080.
I0304 19:32:48.502978 22849303695488 run.py:483] Algo bellman_ford step 9940 current loss 0.005980, current_train_items 318112.
I0304 19:32:48.519244 22849303695488 run.py:483] Algo bellman_ford step 9941 current loss 0.005817, current_train_items 318144.
I0304 19:32:48.543743 22849303695488 run.py:483] Algo bellman_ford step 9942 current loss 0.082009, current_train_items 318176.
I0304 19:32:48.576781 22849303695488 run.py:483] Algo bellman_ford step 9943 current loss 0.076465, current_train_items 318208.
I0304 19:32:48.609670 22849303695488 run.py:483] Algo bellman_ford step 9944 current loss 0.110389, current_train_items 318240.
I0304 19:32:48.629499 22849303695488 run.py:483] Algo bellman_ford step 9945 current loss 0.004993, current_train_items 318272.
I0304 19:32:48.645646 22849303695488 run.py:483] Algo bellman_ford step 9946 current loss 0.063723, current_train_items 318304.
I0304 19:32:48.670766 22849303695488 run.py:483] Algo bellman_ford step 9947 current loss 0.041694, current_train_items 318336.
I0304 19:32:48.703641 22849303695488 run.py:483] Algo bellman_ford step 9948 current loss 0.072132, current_train_items 318368.
I0304 19:32:48.739230 22849303695488 run.py:483] Algo bellman_ford step 9949 current loss 0.063337, current_train_items 318400.
I0304 19:32:48.759427 22849303695488 run.py:483] Algo bellman_ford step 9950 current loss 0.001400, current_train_items 318432.
I0304 19:32:48.767525 22849303695488 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0304 19:32:48.767636 22849303695488 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:48.785116 22849303695488 run.py:483] Algo bellman_ford step 9951 current loss 0.018719, current_train_items 318464.
I0304 19:32:48.810210 22849303695488 run.py:483] Algo bellman_ford step 9952 current loss 0.023977, current_train_items 318496.
I0304 19:32:48.844084 22849303695488 run.py:483] Algo bellman_ford step 9953 current loss 0.065783, current_train_items 318528.
I0304 19:32:48.880250 22849303695488 run.py:483] Algo bellman_ford step 9954 current loss 0.068584, current_train_items 318560.
I0304 19:32:48.900626 22849303695488 run.py:483] Algo bellman_ford step 9955 current loss 0.012081, current_train_items 318592.
I0304 19:32:48.917348 22849303695488 run.py:483] Algo bellman_ford step 9956 current loss 0.031389, current_train_items 318624.
I0304 19:32:48.942319 22849303695488 run.py:483] Algo bellman_ford step 9957 current loss 0.019508, current_train_items 318656.
I0304 19:32:48.975706 22849303695488 run.py:483] Algo bellman_ford step 9958 current loss 0.095932, current_train_items 318688.
I0304 19:32:49.011438 22849303695488 run.py:483] Algo bellman_ford step 9959 current loss 0.064576, current_train_items 318720.
I0304 19:32:49.032289 22849303695488 run.py:483] Algo bellman_ford step 9960 current loss 0.001853, current_train_items 318752.
I0304 19:32:49.048446 22849303695488 run.py:483] Algo bellman_ford step 9961 current loss 0.023803, current_train_items 318784.
I0304 19:32:49.073583 22849303695488 run.py:483] Algo bellman_ford step 9962 current loss 0.027635, current_train_items 318816.
I0304 19:32:49.106607 22849303695488 run.py:483] Algo bellman_ford step 9963 current loss 0.055540, current_train_items 318848.
I0304 19:32:49.140693 22849303695488 run.py:483] Algo bellman_ford step 9964 current loss 0.088337, current_train_items 318880.
I0304 19:32:49.160547 22849303695488 run.py:483] Algo bellman_ford step 9965 current loss 0.003387, current_train_items 318912.
I0304 19:32:49.177483 22849303695488 run.py:483] Algo bellman_ford step 9966 current loss 0.028345, current_train_items 318944.
I0304 19:32:49.202596 22849303695488 run.py:483] Algo bellman_ford step 9967 current loss 0.048098, current_train_items 318976.
I0304 19:32:49.234973 22849303695488 run.py:483] Algo bellman_ford step 9968 current loss 0.041811, current_train_items 319008.
I0304 19:32:49.269234 22849303695488 run.py:483] Algo bellman_ford step 9969 current loss 0.045789, current_train_items 319040.
I0304 19:32:49.290143 22849303695488 run.py:483] Algo bellman_ford step 9970 current loss 0.004096, current_train_items 319072.
I0304 19:32:49.306585 22849303695488 run.py:483] Algo bellman_ford step 9971 current loss 0.020556, current_train_items 319104.
I0304 19:32:49.330862 22849303695488 run.py:483] Algo bellman_ford step 9972 current loss 0.044915, current_train_items 319136.
I0304 19:32:49.362349 22849303695488 run.py:483] Algo bellman_ford step 9973 current loss 0.064276, current_train_items 319168.
I0304 19:32:49.397155 22849303695488 run.py:483] Algo bellman_ford step 9974 current loss 0.054961, current_train_items 319200.
I0304 19:32:49.417866 22849303695488 run.py:483] Algo bellman_ford step 9975 current loss 0.002679, current_train_items 319232.
I0304 19:32:49.434811 22849303695488 run.py:483] Algo bellman_ford step 9976 current loss 0.027896, current_train_items 319264.
I0304 19:32:49.459275 22849303695488 run.py:483] Algo bellman_ford step 9977 current loss 0.053190, current_train_items 319296.
I0304 19:32:49.491931 22849303695488 run.py:483] Algo bellman_ford step 9978 current loss 0.021738, current_train_items 319328.
I0304 19:32:49.525383 22849303695488 run.py:483] Algo bellman_ford step 9979 current loss 0.029692, current_train_items 319360.
I0304 19:32:49.545295 22849303695488 run.py:483] Algo bellman_ford step 9980 current loss 0.004350, current_train_items 319392.
I0304 19:32:49.561764 22849303695488 run.py:483] Algo bellman_ford step 9981 current loss 0.023663, current_train_items 319424.
I0304 19:32:49.587372 22849303695488 run.py:483] Algo bellman_ford step 9982 current loss 0.037156, current_train_items 319456.
I0304 19:32:49.621447 22849303695488 run.py:483] Algo bellman_ford step 9983 current loss 0.051426, current_train_items 319488.
I0304 19:32:49.656362 22849303695488 run.py:483] Algo bellman_ford step 9984 current loss 0.061352, current_train_items 319520.
I0304 19:32:49.676751 22849303695488 run.py:483] Algo bellman_ford step 9985 current loss 0.003477, current_train_items 319552.
I0304 19:32:49.693051 22849303695488 run.py:483] Algo bellman_ford step 9986 current loss 0.004944, current_train_items 319584.
I0304 19:32:49.717746 22849303695488 run.py:483] Algo bellman_ford step 9987 current loss 0.020598, current_train_items 319616.
I0304 19:32:49.750335 22849303695488 run.py:483] Algo bellman_ford step 9988 current loss 0.046927, current_train_items 319648.
I0304 19:32:49.784329 22849303695488 run.py:483] Algo bellman_ford step 9989 current loss 0.039439, current_train_items 319680.
I0304 19:32:49.804956 22849303695488 run.py:483] Algo bellman_ford step 9990 current loss 0.111787, current_train_items 319712.
I0304 19:32:49.821890 22849303695488 run.py:483] Algo bellman_ford step 9991 current loss 0.024739, current_train_items 319744.
I0304 19:32:49.846101 22849303695488 run.py:483] Algo bellman_ford step 9992 current loss 0.028822, current_train_items 319776.
I0304 19:32:49.878242 22849303695488 run.py:483] Algo bellman_ford step 9993 current loss 0.020638, current_train_items 319808.
I0304 19:32:49.911416 22849303695488 run.py:483] Algo bellman_ford step 9994 current loss 0.038016, current_train_items 319840.
I0304 19:32:49.931947 22849303695488 run.py:483] Algo bellman_ford step 9995 current loss 0.006555, current_train_items 319872.
I0304 19:32:49.948635 22849303695488 run.py:483] Algo bellman_ford step 9996 current loss 0.049566, current_train_items 319904.
I0304 19:32:49.973503 22849303695488 run.py:483] Algo bellman_ford step 9997 current loss 0.019203, current_train_items 319936.
I0304 19:32:50.006221 22849303695488 run.py:483] Algo bellman_ford step 9998 current loss 0.056256, current_train_items 319968.
I0304 19:32:50.038174 22849303695488 run.py:483] Algo bellman_ford step 9999 current loss 0.071812, current_train_items 320000.
I0304 19:32:50.044293 22849303695488 run.py:527] Restoring best model from checkpoint...
I0304 19:32:52.801109 22849303695488 run.py:542] (test) algo bellman_ford : {'pi': 0.94677734375, 'score': 0.94677734375, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0304 19:32:52.801388 22849303695488 run.py:544] Done!

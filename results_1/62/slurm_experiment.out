Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-04 19:24:58.438423: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-04 19:24:58.438757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-04 19:24:58.481926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-04 19:25:19.187146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0304 19:26:25.666201 22579586809984 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0304 19:26:25.670390 22579586809984 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0304 19:26:27.027740 22579586809984 run.py:307] Creating samplers for algo bellman_ford
W0304 19:26:27.028139 22579586809984 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.028409 22579586809984 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.242089 22579586809984 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.242336 22579586809984 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.488820 22579586809984 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.489069 22579586809984 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.829761 22579586809984 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.829988 22579586809984 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:28.249711 22579586809984 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:28.249948 22579586809984 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:28.782164 22579586809984 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0304 19:26:28.782406 22579586809984 samplers.py:112] Creating a dataset with 64 samples.
I0304 19:26:28.821053 22579586809984 run.py:166] Dataset not found in ./datasets_1/62/CLRS30_v1.0.0. Downloading...
I0304 19:26:44.618314 22579586809984 dataset_info.py:482] Load dataset info from ./datasets_1/62/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:26:44.620775 22579586809984 dataset_info.py:482] Load dataset info from ./datasets_1/62/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:26:44.621554 22579586809984 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/62/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0304 19:26:44.621635 22579586809984 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/62/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:27:00.646752 22579586809984 run.py:483] Algo bellman_ford step 0 current loss 3.705739, current_train_items 32.
I0304 19:27:03.537847 22579586809984 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.5322265625, 'score': 0.5322265625, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0304 19:27:03.538025 22579586809984 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.532, val scores are: bellman_ford: 0.532
I0304 19:27:13.485252 22579586809984 run.py:483] Algo bellman_ford step 1 current loss 3.771457, current_train_items 64.
I0304 19:27:24.612400 22579586809984 run.py:483] Algo bellman_ford step 2 current loss 3.582578, current_train_items 96.
I0304 19:27:35.714319 22579586809984 run.py:483] Algo bellman_ford step 3 current loss 3.378566, current_train_items 128.
I0304 19:27:45.524793 22579586809984 run.py:483] Algo bellman_ford step 4 current loss 3.581559, current_train_items 160.
I0304 19:27:45.543538 22579586809984 run.py:483] Algo bellman_ford step 5 current loss 1.071307, current_train_items 192.
I0304 19:27:45.560456 22579586809984 run.py:483] Algo bellman_ford step 6 current loss 1.928557, current_train_items 224.
I0304 19:27:45.583369 22579586809984 run.py:483] Algo bellman_ford step 7 current loss 2.216949, current_train_items 256.
I0304 19:27:45.612749 22579586809984 run.py:483] Algo bellman_ford step 8 current loss 2.592420, current_train_items 288.
I0304 19:27:45.644923 22579586809984 run.py:483] Algo bellman_ford step 9 current loss 2.782382, current_train_items 320.
I0304 19:27:45.662701 22579586809984 run.py:483] Algo bellman_ford step 10 current loss 1.071902, current_train_items 352.
I0304 19:27:45.679474 22579586809984 run.py:483] Algo bellman_ford step 11 current loss 1.567022, current_train_items 384.
I0304 19:27:45.701956 22579586809984 run.py:483] Algo bellman_ford step 12 current loss 1.639207, current_train_items 416.
I0304 19:27:45.733192 22579586809984 run.py:483] Algo bellman_ford step 13 current loss 2.115820, current_train_items 448.
I0304 19:27:45.761150 22579586809984 run.py:483] Algo bellman_ford step 14 current loss 1.905356, current_train_items 480.
I0304 19:27:45.779485 22579586809984 run.py:483] Algo bellman_ford step 15 current loss 0.738775, current_train_items 512.
I0304 19:27:45.795562 22579586809984 run.py:483] Algo bellman_ford step 16 current loss 1.106399, current_train_items 544.
I0304 19:27:45.820593 22579586809984 run.py:483] Algo bellman_ford step 17 current loss 1.929381, current_train_items 576.
I0304 19:27:45.849723 22579586809984 run.py:483] Algo bellman_ford step 18 current loss 1.700549, current_train_items 608.
I0304 19:27:45.882550 22579586809984 run.py:483] Algo bellman_ford step 19 current loss 1.942826, current_train_items 640.
I0304 19:27:45.900059 22579586809984 run.py:483] Algo bellman_ford step 20 current loss 0.563136, current_train_items 672.
I0304 19:27:45.915856 22579586809984 run.py:483] Algo bellman_ford step 21 current loss 0.856531, current_train_items 704.
I0304 19:27:45.939218 22579586809984 run.py:483] Algo bellman_ford step 22 current loss 1.468099, current_train_items 736.
I0304 19:27:45.968718 22579586809984 run.py:483] Algo bellman_ford step 23 current loss 1.529664, current_train_items 768.
I0304 19:27:45.999994 22579586809984 run.py:483] Algo bellman_ford step 24 current loss 1.807874, current_train_items 800.
I0304 19:27:46.018067 22579586809984 run.py:483] Algo bellman_ford step 25 current loss 0.523866, current_train_items 832.
I0304 19:27:46.034428 22579586809984 run.py:483] Algo bellman_ford step 26 current loss 0.940680, current_train_items 864.
I0304 19:27:46.058384 22579586809984 run.py:483] Algo bellman_ford step 27 current loss 1.343545, current_train_items 896.
I0304 19:27:46.088783 22579586809984 run.py:483] Algo bellman_ford step 28 current loss 1.317617, current_train_items 928.
I0304 19:27:46.121062 22579586809984 run.py:483] Algo bellman_ford step 29 current loss 1.596864, current_train_items 960.
I0304 19:27:46.139036 22579586809984 run.py:483] Algo bellman_ford step 30 current loss 0.414104, current_train_items 992.
I0304 19:27:46.154462 22579586809984 run.py:483] Algo bellman_ford step 31 current loss 0.677546, current_train_items 1024.
I0304 19:27:46.177558 22579586809984 run.py:483] Algo bellman_ford step 32 current loss 1.107561, current_train_items 1056.
I0304 19:27:46.208074 22579586809984 run.py:483] Algo bellman_ford step 33 current loss 1.280867, current_train_items 1088.
I0304 19:27:46.239209 22579586809984 run.py:483] Algo bellman_ford step 34 current loss 1.353985, current_train_items 1120.
I0304 19:27:46.256726 22579586809984 run.py:483] Algo bellman_ford step 35 current loss 0.431311, current_train_items 1152.
I0304 19:27:46.272421 22579586809984 run.py:483] Algo bellman_ford step 36 current loss 0.583396, current_train_items 1184.
I0304 19:27:46.296132 22579586809984 run.py:483] Algo bellman_ford step 37 current loss 1.060390, current_train_items 1216.
I0304 19:27:46.326114 22579586809984 run.py:483] Algo bellman_ford step 38 current loss 1.156687, current_train_items 1248.
W0304 19:27:46.349209 22579586809984 samplers.py:155] Increasing hint lengh from 9 to 11
I0304 19:27:52.972888 22579586809984 run.py:483] Algo bellman_ford step 39 current loss 1.578027, current_train_items 1280.
I0304 19:27:52.992850 22579586809984 run.py:483] Algo bellman_ford step 40 current loss 0.395683, current_train_items 1312.
I0304 19:27:53.009383 22579586809984 run.py:483] Algo bellman_ford step 41 current loss 0.609301, current_train_items 1344.
I0304 19:27:53.032399 22579586809984 run.py:483] Algo bellman_ford step 42 current loss 0.850920, current_train_items 1376.
I0304 19:27:53.063341 22579586809984 run.py:483] Algo bellman_ford step 43 current loss 1.068939, current_train_items 1408.
I0304 19:27:53.095904 22579586809984 run.py:483] Algo bellman_ford step 44 current loss 1.227520, current_train_items 1440.
I0304 19:27:53.115179 22579586809984 run.py:483] Algo bellman_ford step 45 current loss 0.286101, current_train_items 1472.
I0304 19:27:53.131637 22579586809984 run.py:483] Algo bellman_ford step 46 current loss 0.583042, current_train_items 1504.
I0304 19:27:53.154139 22579586809984 run.py:483] Algo bellman_ford step 47 current loss 0.797464, current_train_items 1536.
I0304 19:27:53.181400 22579586809984 run.py:483] Algo bellman_ford step 48 current loss 0.694160, current_train_items 1568.
I0304 19:27:53.210731 22579586809984 run.py:483] Algo bellman_ford step 49 current loss 0.964563, current_train_items 1600.
I0304 19:27:53.229328 22579586809984 run.py:483] Algo bellman_ford step 50 current loss 0.286094, current_train_items 1632.
I0304 19:27:53.238848 22579586809984 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.8291015625, 'score': 0.8291015625, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0304 19:27:53.238958 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.532, current avg val score is 0.829, val scores are: bellman_ford: 0.829
I0304 19:27:53.268027 22579586809984 run.py:483] Algo bellman_ford step 51 current loss 0.533644, current_train_items 1664.
I0304 19:27:53.291478 22579586809984 run.py:483] Algo bellman_ford step 52 current loss 0.976276, current_train_items 1696.
I0304 19:27:53.321013 22579586809984 run.py:483] Algo bellman_ford step 53 current loss 0.835630, current_train_items 1728.
I0304 19:27:53.353615 22579586809984 run.py:483] Algo bellman_ford step 54 current loss 1.089599, current_train_items 1760.
I0304 19:27:53.372724 22579586809984 run.py:483] Algo bellman_ford step 55 current loss 0.274225, current_train_items 1792.
I0304 19:27:53.388906 22579586809984 run.py:483] Algo bellman_ford step 56 current loss 0.396754, current_train_items 1824.
I0304 19:27:53.412242 22579586809984 run.py:483] Algo bellman_ford step 57 current loss 0.792244, current_train_items 1856.
I0304 19:27:53.440494 22579586809984 run.py:483] Algo bellman_ford step 58 current loss 0.646001, current_train_items 1888.
I0304 19:27:53.473230 22579586809984 run.py:483] Algo bellman_ford step 59 current loss 1.099978, current_train_items 1920.
I0304 19:27:53.492131 22579586809984 run.py:483] Algo bellman_ford step 60 current loss 0.187166, current_train_items 1952.
W0304 19:27:53.501384 22579586809984 samplers.py:155] Increasing hint lengh from 6 to 7
I0304 19:28:00.023022 22579586809984 run.py:483] Algo bellman_ford step 61 current loss 0.490742, current_train_items 1984.
I0304 19:28:00.048437 22579586809984 run.py:483] Algo bellman_ford step 62 current loss 0.700879, current_train_items 2016.
I0304 19:28:00.078029 22579586809984 run.py:483] Algo bellman_ford step 63 current loss 0.861294, current_train_items 2048.
I0304 19:28:00.112034 22579586809984 run.py:483] Algo bellman_ford step 64 current loss 1.161190, current_train_items 2080.
I0304 19:28:00.131776 22579586809984 run.py:483] Algo bellman_ford step 65 current loss 0.180079, current_train_items 2112.
I0304 19:28:00.148080 22579586809984 run.py:483] Algo bellman_ford step 66 current loss 0.309351, current_train_items 2144.
I0304 19:28:00.172937 22579586809984 run.py:483] Algo bellman_ford step 67 current loss 0.677884, current_train_items 2176.
I0304 19:28:00.201346 22579586809984 run.py:483] Algo bellman_ford step 68 current loss 0.647632, current_train_items 2208.
I0304 19:28:00.233372 22579586809984 run.py:483] Algo bellman_ford step 69 current loss 0.851003, current_train_items 2240.
I0304 19:28:00.251794 22579586809984 run.py:483] Algo bellman_ford step 70 current loss 0.148256, current_train_items 2272.
I0304 19:28:00.268198 22579586809984 run.py:483] Algo bellman_ford step 71 current loss 0.320209, current_train_items 2304.
I0304 19:28:00.292064 22579586809984 run.py:483] Algo bellman_ford step 72 current loss 0.716757, current_train_items 2336.
I0304 19:28:00.322064 22579586809984 run.py:483] Algo bellman_ford step 73 current loss 0.696629, current_train_items 2368.
I0304 19:28:00.354496 22579586809984 run.py:483] Algo bellman_ford step 74 current loss 0.853091, current_train_items 2400.
I0304 19:28:00.372998 22579586809984 run.py:483] Algo bellman_ford step 75 current loss 0.127962, current_train_items 2432.
I0304 19:28:00.389767 22579586809984 run.py:483] Algo bellman_ford step 76 current loss 0.456772, current_train_items 2464.
I0304 19:28:00.412876 22579586809984 run.py:483] Algo bellman_ford step 77 current loss 0.653182, current_train_items 2496.
I0304 19:28:00.441571 22579586809984 run.py:483] Algo bellman_ford step 78 current loss 0.530010, current_train_items 2528.
I0304 19:28:00.471231 22579586809984 run.py:483] Algo bellman_ford step 79 current loss 0.787109, current_train_items 2560.
I0304 19:28:00.490234 22579586809984 run.py:483] Algo bellman_ford step 80 current loss 0.106272, current_train_items 2592.
I0304 19:28:00.506511 22579586809984 run.py:483] Algo bellman_ford step 81 current loss 0.307089, current_train_items 2624.
I0304 19:28:00.529931 22579586809984 run.py:483] Algo bellman_ford step 82 current loss 0.563440, current_train_items 2656.
I0304 19:28:00.558891 22579586809984 run.py:483] Algo bellman_ford step 83 current loss 0.634072, current_train_items 2688.
I0304 19:28:00.588973 22579586809984 run.py:483] Algo bellman_ford step 84 current loss 0.698983, current_train_items 2720.
I0304 19:28:00.607329 22579586809984 run.py:483] Algo bellman_ford step 85 current loss 0.134876, current_train_items 2752.
I0304 19:28:00.623789 22579586809984 run.py:483] Algo bellman_ford step 86 current loss 0.310694, current_train_items 2784.
I0304 19:28:00.648281 22579586809984 run.py:483] Algo bellman_ford step 87 current loss 0.586601, current_train_items 2816.
I0304 19:28:00.678223 22579586809984 run.py:483] Algo bellman_ford step 88 current loss 0.533548, current_train_items 2848.
I0304 19:28:00.709969 22579586809984 run.py:483] Algo bellman_ford step 89 current loss 0.705257, current_train_items 2880.
I0304 19:28:00.728674 22579586809984 run.py:483] Algo bellman_ford step 90 current loss 0.097825, current_train_items 2912.
I0304 19:28:00.744956 22579586809984 run.py:483] Algo bellman_ford step 91 current loss 0.340326, current_train_items 2944.
I0304 19:28:00.767998 22579586809984 run.py:483] Algo bellman_ford step 92 current loss 0.506809, current_train_items 2976.
I0304 19:28:00.798365 22579586809984 run.py:483] Algo bellman_ford step 93 current loss 0.614449, current_train_items 3008.
I0304 19:28:00.829419 22579586809984 run.py:483] Algo bellman_ford step 94 current loss 0.586386, current_train_items 3040.
I0304 19:28:00.848501 22579586809984 run.py:483] Algo bellman_ford step 95 current loss 0.081006, current_train_items 3072.
I0304 19:28:00.864547 22579586809984 run.py:483] Algo bellman_ford step 96 current loss 0.208004, current_train_items 3104.
I0304 19:28:00.888089 22579586809984 run.py:483] Algo bellman_ford step 97 current loss 0.396990, current_train_items 3136.
I0304 19:28:00.917490 22579586809984 run.py:483] Algo bellman_ford step 98 current loss 0.441942, current_train_items 3168.
I0304 19:28:00.949213 22579586809984 run.py:483] Algo bellman_ford step 99 current loss 0.723060, current_train_items 3200.
I0304 19:28:00.967699 22579586809984 run.py:483] Algo bellman_ford step 100 current loss 0.126949, current_train_items 3232.
I0304 19:28:00.977383 22579586809984 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0304 19:28:00.977493 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.829, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0304 19:28:01.007565 22579586809984 run.py:483] Algo bellman_ford step 101 current loss 0.327106, current_train_items 3264.
I0304 19:28:01.031389 22579586809984 run.py:483] Algo bellman_ford step 102 current loss 0.369218, current_train_items 3296.
I0304 19:28:01.060696 22579586809984 run.py:483] Algo bellman_ford step 103 current loss 0.561841, current_train_items 3328.
I0304 19:28:01.095357 22579586809984 run.py:483] Algo bellman_ford step 104 current loss 0.809811, current_train_items 3360.
I0304 19:28:01.114375 22579586809984 run.py:483] Algo bellman_ford step 105 current loss 0.059296, current_train_items 3392.
I0304 19:28:01.130813 22579586809984 run.py:483] Algo bellman_ford step 106 current loss 0.274450, current_train_items 3424.
I0304 19:28:01.154669 22579586809984 run.py:483] Algo bellman_ford step 107 current loss 0.675914, current_train_items 3456.
I0304 19:28:01.183843 22579586809984 run.py:483] Algo bellman_ford step 108 current loss 0.710469, current_train_items 3488.
I0304 19:28:01.214452 22579586809984 run.py:483] Algo bellman_ford step 109 current loss 0.613973, current_train_items 3520.
I0304 19:28:01.233260 22579586809984 run.py:483] Algo bellman_ford step 110 current loss 0.089022, current_train_items 3552.
I0304 19:28:01.249326 22579586809984 run.py:483] Algo bellman_ford step 111 current loss 0.229075, current_train_items 3584.
I0304 19:28:01.272261 22579586809984 run.py:483] Algo bellman_ford step 112 current loss 0.398054, current_train_items 3616.
I0304 19:28:01.302065 22579586809984 run.py:483] Algo bellman_ford step 113 current loss 0.661288, current_train_items 3648.
I0304 19:28:01.332933 22579586809984 run.py:483] Algo bellman_ford step 114 current loss 0.640758, current_train_items 3680.
I0304 19:28:01.351467 22579586809984 run.py:483] Algo bellman_ford step 115 current loss 0.051379, current_train_items 3712.
I0304 19:28:01.367925 22579586809984 run.py:483] Algo bellman_ford step 116 current loss 0.211351, current_train_items 3744.
I0304 19:28:01.392829 22579586809984 run.py:483] Algo bellman_ford step 117 current loss 0.531262, current_train_items 3776.
I0304 19:28:01.421892 22579586809984 run.py:483] Algo bellman_ford step 118 current loss 0.437354, current_train_items 3808.
I0304 19:28:01.451706 22579586809984 run.py:483] Algo bellman_ford step 119 current loss 0.447236, current_train_items 3840.
I0304 19:28:01.470622 22579586809984 run.py:483] Algo bellman_ford step 120 current loss 0.070949, current_train_items 3872.
I0304 19:28:01.487428 22579586809984 run.py:483] Algo bellman_ford step 121 current loss 0.229415, current_train_items 3904.
I0304 19:28:01.511587 22579586809984 run.py:483] Algo bellman_ford step 122 current loss 0.422503, current_train_items 3936.
I0304 19:28:01.541943 22579586809984 run.py:483] Algo bellman_ford step 123 current loss 0.462433, current_train_items 3968.
I0304 19:28:01.577357 22579586809984 run.py:483] Algo bellman_ford step 124 current loss 0.727648, current_train_items 4000.
I0304 19:28:01.596195 22579586809984 run.py:483] Algo bellman_ford step 125 current loss 0.190303, current_train_items 4032.
I0304 19:28:01.613035 22579586809984 run.py:483] Algo bellman_ford step 126 current loss 0.364094, current_train_items 4064.
I0304 19:28:01.636775 22579586809984 run.py:483] Algo bellman_ford step 127 current loss 0.571802, current_train_items 4096.
I0304 19:28:01.666263 22579586809984 run.py:483] Algo bellman_ford step 128 current loss 0.434681, current_train_items 4128.
I0304 19:28:01.698374 22579586809984 run.py:483] Algo bellman_ford step 129 current loss 0.406982, current_train_items 4160.
I0304 19:28:01.717244 22579586809984 run.py:483] Algo bellman_ford step 130 current loss 0.134588, current_train_items 4192.
I0304 19:28:01.733290 22579586809984 run.py:483] Algo bellman_ford step 131 current loss 0.210219, current_train_items 4224.
I0304 19:28:01.757306 22579586809984 run.py:483] Algo bellman_ford step 132 current loss 0.509112, current_train_items 4256.
I0304 19:28:01.786792 22579586809984 run.py:483] Algo bellman_ford step 133 current loss 0.494480, current_train_items 4288.
I0304 19:28:01.818051 22579586809984 run.py:483] Algo bellman_ford step 134 current loss 0.497138, current_train_items 4320.
I0304 19:28:01.836309 22579586809984 run.py:483] Algo bellman_ford step 135 current loss 0.094773, current_train_items 4352.
I0304 19:28:01.852546 22579586809984 run.py:483] Algo bellman_ford step 136 current loss 0.258333, current_train_items 4384.
I0304 19:28:01.876364 22579586809984 run.py:483] Algo bellman_ford step 137 current loss 0.433354, current_train_items 4416.
I0304 19:28:01.905819 22579586809984 run.py:483] Algo bellman_ford step 138 current loss 0.487389, current_train_items 4448.
I0304 19:28:01.938366 22579586809984 run.py:483] Algo bellman_ford step 139 current loss 0.561257, current_train_items 4480.
I0304 19:28:01.956973 22579586809984 run.py:483] Algo bellman_ford step 140 current loss 0.066890, current_train_items 4512.
I0304 19:28:01.973596 22579586809984 run.py:483] Algo bellman_ford step 141 current loss 0.309179, current_train_items 4544.
I0304 19:28:01.997005 22579586809984 run.py:483] Algo bellman_ford step 142 current loss 0.511841, current_train_items 4576.
I0304 19:28:02.027183 22579586809984 run.py:483] Algo bellman_ford step 143 current loss 0.669112, current_train_items 4608.
I0304 19:28:02.058689 22579586809984 run.py:483] Algo bellman_ford step 144 current loss 0.460182, current_train_items 4640.
I0304 19:28:02.077582 22579586809984 run.py:483] Algo bellman_ford step 145 current loss 0.053248, current_train_items 4672.
I0304 19:28:02.093847 22579586809984 run.py:483] Algo bellman_ford step 146 current loss 0.185028, current_train_items 4704.
I0304 19:28:02.116872 22579586809984 run.py:483] Algo bellman_ford step 147 current loss 0.296544, current_train_items 4736.
I0304 19:28:02.146552 22579586809984 run.py:483] Algo bellman_ford step 148 current loss 0.456713, current_train_items 4768.
I0304 19:28:02.176699 22579586809984 run.py:483] Algo bellman_ford step 149 current loss 0.604660, current_train_items 4800.
I0304 19:28:02.195376 22579586809984 run.py:483] Algo bellman_ford step 150 current loss 0.067760, current_train_items 4832.
I0304 19:28:02.203289 22579586809984 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0304 19:28:02.203403 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.911, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0304 19:28:02.231312 22579586809984 run.py:483] Algo bellman_ford step 151 current loss 0.145428, current_train_items 4864.
I0304 19:28:02.255178 22579586809984 run.py:483] Algo bellman_ford step 152 current loss 0.286147, current_train_items 4896.
I0304 19:28:02.284541 22579586809984 run.py:483] Algo bellman_ford step 153 current loss 0.311476, current_train_items 4928.
I0304 19:28:02.320082 22579586809984 run.py:483] Algo bellman_ford step 154 current loss 0.573740, current_train_items 4960.
I0304 19:28:02.339600 22579586809984 run.py:483] Algo bellman_ford step 155 current loss 0.076700, current_train_items 4992.
I0304 19:28:02.356085 22579586809984 run.py:483] Algo bellman_ford step 156 current loss 0.209103, current_train_items 5024.
I0304 19:28:02.379348 22579586809984 run.py:483] Algo bellman_ford step 157 current loss 0.299437, current_train_items 5056.
I0304 19:28:02.407869 22579586809984 run.py:483] Algo bellman_ford step 158 current loss 0.301270, current_train_items 5088.
I0304 19:28:02.439945 22579586809984 run.py:483] Algo bellman_ford step 159 current loss 0.343540, current_train_items 5120.
I0304 19:28:02.458969 22579586809984 run.py:483] Algo bellman_ford step 160 current loss 0.105226, current_train_items 5152.
I0304 19:28:02.475415 22579586809984 run.py:483] Algo bellman_ford step 161 current loss 0.157743, current_train_items 5184.
I0304 19:28:02.498255 22579586809984 run.py:483] Algo bellman_ford step 162 current loss 0.207765, current_train_items 5216.
I0304 19:28:02.527712 22579586809984 run.py:483] Algo bellman_ford step 163 current loss 0.312917, current_train_items 5248.
I0304 19:28:02.558887 22579586809984 run.py:483] Algo bellman_ford step 164 current loss 0.478689, current_train_items 5280.
I0304 19:28:02.578005 22579586809984 run.py:483] Algo bellman_ford step 165 current loss 0.055623, current_train_items 5312.
I0304 19:28:02.594624 22579586809984 run.py:483] Algo bellman_ford step 166 current loss 0.229218, current_train_items 5344.
I0304 19:28:02.617789 22579586809984 run.py:483] Algo bellman_ford step 167 current loss 0.257167, current_train_items 5376.
I0304 19:28:02.647698 22579586809984 run.py:483] Algo bellman_ford step 168 current loss 0.430057, current_train_items 5408.
I0304 19:28:02.679442 22579586809984 run.py:483] Algo bellman_ford step 169 current loss 0.570899, current_train_items 5440.
I0304 19:28:02.698603 22579586809984 run.py:483] Algo bellman_ford step 170 current loss 0.072024, current_train_items 5472.
I0304 19:28:02.715058 22579586809984 run.py:483] Algo bellman_ford step 171 current loss 0.106473, current_train_items 5504.
I0304 19:28:02.737731 22579586809984 run.py:483] Algo bellman_ford step 172 current loss 0.252305, current_train_items 5536.
I0304 19:28:02.767509 22579586809984 run.py:483] Algo bellman_ford step 173 current loss 0.468040, current_train_items 5568.
I0304 19:28:02.802530 22579586809984 run.py:483] Algo bellman_ford step 174 current loss 0.523375, current_train_items 5600.
I0304 19:28:02.821253 22579586809984 run.py:483] Algo bellman_ford step 175 current loss 0.101553, current_train_items 5632.
I0304 19:28:02.837586 22579586809984 run.py:483] Algo bellman_ford step 176 current loss 0.302242, current_train_items 5664.
I0304 19:28:02.861571 22579586809984 run.py:483] Algo bellman_ford step 177 current loss 0.525277, current_train_items 5696.
I0304 19:28:02.889655 22579586809984 run.py:483] Algo bellman_ford step 178 current loss 0.324198, current_train_items 5728.
I0304 19:28:02.920505 22579586809984 run.py:483] Algo bellman_ford step 179 current loss 0.390995, current_train_items 5760.
I0304 19:28:02.939865 22579586809984 run.py:483] Algo bellman_ford step 180 current loss 0.067235, current_train_items 5792.
I0304 19:28:02.956383 22579586809984 run.py:483] Algo bellman_ford step 181 current loss 0.223695, current_train_items 5824.
I0304 19:28:02.981031 22579586809984 run.py:483] Algo bellman_ford step 182 current loss 0.592344, current_train_items 5856.
I0304 19:28:03.010391 22579586809984 run.py:483] Algo bellman_ford step 183 current loss 0.498930, current_train_items 5888.
I0304 19:28:03.040416 22579586809984 run.py:483] Algo bellman_ford step 184 current loss 0.404436, current_train_items 5920.
I0304 19:28:03.059211 22579586809984 run.py:483] Algo bellman_ford step 185 current loss 0.037695, current_train_items 5952.
I0304 19:28:03.075876 22579586809984 run.py:483] Algo bellman_ford step 186 current loss 0.334616, current_train_items 5984.
I0304 19:28:03.098884 22579586809984 run.py:483] Algo bellman_ford step 187 current loss 0.332840, current_train_items 6016.
I0304 19:28:03.128174 22579586809984 run.py:483] Algo bellman_ford step 188 current loss 0.457719, current_train_items 6048.
I0304 19:28:03.161624 22579586809984 run.py:483] Algo bellman_ford step 189 current loss 0.489064, current_train_items 6080.
I0304 19:28:03.180382 22579586809984 run.py:483] Algo bellman_ford step 190 current loss 0.052623, current_train_items 6112.
I0304 19:28:03.197000 22579586809984 run.py:483] Algo bellman_ford step 191 current loss 0.301948, current_train_items 6144.
I0304 19:28:03.221481 22579586809984 run.py:483] Algo bellman_ford step 192 current loss 0.543629, current_train_items 6176.
I0304 19:28:03.250739 22579586809984 run.py:483] Algo bellman_ford step 193 current loss 0.300765, current_train_items 6208.
I0304 19:28:03.282315 22579586809984 run.py:483] Algo bellman_ford step 194 current loss 0.402712, current_train_items 6240.
I0304 19:28:03.301674 22579586809984 run.py:483] Algo bellman_ford step 195 current loss 0.089977, current_train_items 6272.
I0304 19:28:03.318174 22579586809984 run.py:483] Algo bellman_ford step 196 current loss 0.386240, current_train_items 6304.
I0304 19:28:03.341461 22579586809984 run.py:483] Algo bellman_ford step 197 current loss 0.434625, current_train_items 6336.
I0304 19:28:03.371833 22579586809984 run.py:483] Algo bellman_ford step 198 current loss 0.587563, current_train_items 6368.
I0304 19:28:03.404433 22579586809984 run.py:483] Algo bellman_ford step 199 current loss 0.495084, current_train_items 6400.
I0304 19:28:03.423140 22579586809984 run.py:483] Algo bellman_ford step 200 current loss 0.051910, current_train_items 6432.
I0304 19:28:03.431032 22579586809984 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.86328125, 'score': 0.86328125, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0304 19:28:03.431138 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.932, current avg val score is 0.863, val scores are: bellman_ford: 0.863
I0304 19:28:03.448077 22579586809984 run.py:483] Algo bellman_ford step 201 current loss 0.304641, current_train_items 6464.
I0304 19:28:03.472257 22579586809984 run.py:483] Algo bellman_ford step 202 current loss 0.458027, current_train_items 6496.
I0304 19:28:03.504223 22579586809984 run.py:483] Algo bellman_ford step 203 current loss 0.543775, current_train_items 6528.
I0304 19:28:03.538918 22579586809984 run.py:483] Algo bellman_ford step 204 current loss 0.622098, current_train_items 6560.
I0304 19:28:03.557864 22579586809984 run.py:483] Algo bellman_ford step 205 current loss 0.115557, current_train_items 6592.
I0304 19:28:03.573330 22579586809984 run.py:483] Algo bellman_ford step 206 current loss 0.225902, current_train_items 6624.
I0304 19:28:03.597572 22579586809984 run.py:483] Algo bellman_ford step 207 current loss 0.532921, current_train_items 6656.
I0304 19:28:03.627350 22579586809984 run.py:483] Algo bellman_ford step 208 current loss 0.561066, current_train_items 6688.
I0304 19:28:03.658030 22579586809984 run.py:483] Algo bellman_ford step 209 current loss 0.489031, current_train_items 6720.
I0304 19:28:03.676542 22579586809984 run.py:483] Algo bellman_ford step 210 current loss 0.179628, current_train_items 6752.
I0304 19:28:03.692861 22579586809984 run.py:483] Algo bellman_ford step 211 current loss 0.207522, current_train_items 6784.
I0304 19:28:03.716951 22579586809984 run.py:483] Algo bellman_ford step 212 current loss 0.347602, current_train_items 6816.
I0304 19:28:03.746808 22579586809984 run.py:483] Algo bellman_ford step 213 current loss 0.504855, current_train_items 6848.
I0304 19:28:03.774732 22579586809984 run.py:483] Algo bellman_ford step 214 current loss 0.325488, current_train_items 6880.
I0304 19:28:03.793313 22579586809984 run.py:483] Algo bellman_ford step 215 current loss 0.051523, current_train_items 6912.
I0304 19:28:03.809293 22579586809984 run.py:483] Algo bellman_ford step 216 current loss 0.260002, current_train_items 6944.
I0304 19:28:03.832330 22579586809984 run.py:483] Algo bellman_ford step 217 current loss 0.385582, current_train_items 6976.
I0304 19:28:03.861449 22579586809984 run.py:483] Algo bellman_ford step 218 current loss 0.423352, current_train_items 7008.
I0304 19:28:03.892000 22579586809984 run.py:483] Algo bellman_ford step 219 current loss 0.414566, current_train_items 7040.
I0304 19:28:03.910347 22579586809984 run.py:483] Algo bellman_ford step 220 current loss 0.054352, current_train_items 7072.
I0304 19:28:03.926709 22579586809984 run.py:483] Algo bellman_ford step 221 current loss 0.179021, current_train_items 7104.
I0304 19:28:03.950815 22579586809984 run.py:483] Algo bellman_ford step 222 current loss 0.423048, current_train_items 7136.
I0304 19:28:03.979707 22579586809984 run.py:483] Algo bellman_ford step 223 current loss 0.524255, current_train_items 7168.
I0304 19:28:04.011420 22579586809984 run.py:483] Algo bellman_ford step 224 current loss 0.495211, current_train_items 7200.
I0304 19:28:04.030042 22579586809984 run.py:483] Algo bellman_ford step 225 current loss 0.066201, current_train_items 7232.
I0304 19:28:04.046494 22579586809984 run.py:483] Algo bellman_ford step 226 current loss 0.161156, current_train_items 7264.
I0304 19:28:04.070055 22579586809984 run.py:483] Algo bellman_ford step 227 current loss 0.396007, current_train_items 7296.
I0304 19:28:04.099964 22579586809984 run.py:483] Algo bellman_ford step 228 current loss 0.368968, current_train_items 7328.
I0304 19:28:04.132233 22579586809984 run.py:483] Algo bellman_ford step 229 current loss 0.419151, current_train_items 7360.
I0304 19:28:04.150487 22579586809984 run.py:483] Algo bellman_ford step 230 current loss 0.038124, current_train_items 7392.
I0304 19:28:04.166672 22579586809984 run.py:483] Algo bellman_ford step 231 current loss 0.131095, current_train_items 7424.
I0304 19:28:04.190952 22579586809984 run.py:483] Algo bellman_ford step 232 current loss 0.330406, current_train_items 7456.
I0304 19:28:04.220863 22579586809984 run.py:483] Algo bellman_ford step 233 current loss 0.323688, current_train_items 7488.
I0304 19:28:04.253870 22579586809984 run.py:483] Algo bellman_ford step 234 current loss 0.411221, current_train_items 7520.
I0304 19:28:04.272617 22579586809984 run.py:483] Algo bellman_ford step 235 current loss 0.069933, current_train_items 7552.
I0304 19:28:04.288920 22579586809984 run.py:483] Algo bellman_ford step 236 current loss 0.161609, current_train_items 7584.
I0304 19:28:04.311966 22579586809984 run.py:483] Algo bellman_ford step 237 current loss 0.354072, current_train_items 7616.
I0304 19:28:04.341310 22579586809984 run.py:483] Algo bellman_ford step 238 current loss 0.302213, current_train_items 7648.
I0304 19:28:04.372687 22579586809984 run.py:483] Algo bellman_ford step 239 current loss 0.370364, current_train_items 7680.
I0304 19:28:04.391016 22579586809984 run.py:483] Algo bellman_ford step 240 current loss 0.035398, current_train_items 7712.
I0304 19:28:04.407329 22579586809984 run.py:483] Algo bellman_ford step 241 current loss 0.208549, current_train_items 7744.
I0304 19:28:04.430687 22579586809984 run.py:483] Algo bellman_ford step 242 current loss 0.261400, current_train_items 7776.
I0304 19:28:04.459813 22579586809984 run.py:483] Algo bellman_ford step 243 current loss 0.281567, current_train_items 7808.
I0304 19:28:04.488762 22579586809984 run.py:483] Algo bellman_ford step 244 current loss 0.348237, current_train_items 7840.
I0304 19:28:04.507397 22579586809984 run.py:483] Algo bellman_ford step 245 current loss 0.091443, current_train_items 7872.
I0304 19:28:04.523485 22579586809984 run.py:483] Algo bellman_ford step 246 current loss 0.154959, current_train_items 7904.
I0304 19:28:04.545957 22579586809984 run.py:483] Algo bellman_ford step 247 current loss 0.275635, current_train_items 7936.
I0304 19:28:04.576300 22579586809984 run.py:483] Algo bellman_ford step 248 current loss 0.326754, current_train_items 7968.
I0304 19:28:04.608012 22579586809984 run.py:483] Algo bellman_ford step 249 current loss 0.340763, current_train_items 8000.
I0304 19:28:04.626411 22579586809984 run.py:483] Algo bellman_ford step 250 current loss 0.095503, current_train_items 8032.
I0304 19:28:04.634243 22579586809984 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0304 19:28:04.634350 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.932, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0304 19:28:04.651602 22579586809984 run.py:483] Algo bellman_ford step 251 current loss 0.257594, current_train_items 8064.
I0304 19:28:04.676060 22579586809984 run.py:483] Algo bellman_ford step 252 current loss 0.296800, current_train_items 8096.
I0304 19:28:04.707001 22579586809984 run.py:483] Algo bellman_ford step 253 current loss 0.383950, current_train_items 8128.
I0304 19:28:04.740196 22579586809984 run.py:483] Algo bellman_ford step 254 current loss 0.425116, current_train_items 8160.
I0304 19:28:04.759401 22579586809984 run.py:483] Algo bellman_ford step 255 current loss 0.119084, current_train_items 8192.
I0304 19:28:04.775138 22579586809984 run.py:483] Algo bellman_ford step 256 current loss 0.159080, current_train_items 8224.
I0304 19:28:04.797853 22579586809984 run.py:483] Algo bellman_ford step 257 current loss 0.181022, current_train_items 8256.
I0304 19:28:04.827390 22579586809984 run.py:483] Algo bellman_ford step 258 current loss 0.274572, current_train_items 8288.
I0304 19:28:04.857937 22579586809984 run.py:483] Algo bellman_ford step 259 current loss 0.301929, current_train_items 8320.
I0304 19:28:04.877397 22579586809984 run.py:483] Algo bellman_ford step 260 current loss 0.037397, current_train_items 8352.
I0304 19:28:04.893594 22579586809984 run.py:483] Algo bellman_ford step 261 current loss 0.105303, current_train_items 8384.
I0304 19:28:04.916445 22579586809984 run.py:483] Algo bellman_ford step 262 current loss 0.267986, current_train_items 8416.
I0304 19:28:04.945194 22579586809984 run.py:483] Algo bellman_ford step 263 current loss 0.301861, current_train_items 8448.
I0304 19:28:04.975910 22579586809984 run.py:483] Algo bellman_ford step 264 current loss 0.262344, current_train_items 8480.
I0304 19:28:04.994588 22579586809984 run.py:483] Algo bellman_ford step 265 current loss 0.041781, current_train_items 8512.
I0304 19:28:05.011073 22579586809984 run.py:483] Algo bellman_ford step 266 current loss 0.192361, current_train_items 8544.
I0304 19:28:05.035590 22579586809984 run.py:483] Algo bellman_ford step 267 current loss 0.354720, current_train_items 8576.
I0304 19:28:05.066059 22579586809984 run.py:483] Algo bellman_ford step 268 current loss 0.342398, current_train_items 8608.
I0304 19:28:05.096068 22579586809984 run.py:483] Algo bellman_ford step 269 current loss 0.279641, current_train_items 8640.
I0304 19:28:05.115135 22579586809984 run.py:483] Algo bellman_ford step 270 current loss 0.042767, current_train_items 8672.
I0304 19:28:05.131000 22579586809984 run.py:483] Algo bellman_ford step 271 current loss 0.088319, current_train_items 8704.
I0304 19:28:05.154971 22579586809984 run.py:483] Algo bellman_ford step 272 current loss 0.199395, current_train_items 8736.
I0304 19:28:05.184229 22579586809984 run.py:483] Algo bellman_ford step 273 current loss 0.285462, current_train_items 8768.
I0304 19:28:05.214851 22579586809984 run.py:483] Algo bellman_ford step 274 current loss 0.345643, current_train_items 8800.
I0304 19:28:05.234169 22579586809984 run.py:483] Algo bellman_ford step 275 current loss 0.041079, current_train_items 8832.
I0304 19:28:05.250547 22579586809984 run.py:483] Algo bellman_ford step 276 current loss 0.111942, current_train_items 8864.
I0304 19:28:05.275243 22579586809984 run.py:483] Algo bellman_ford step 277 current loss 0.284270, current_train_items 8896.
I0304 19:28:05.305694 22579586809984 run.py:483] Algo bellman_ford step 278 current loss 0.347877, current_train_items 8928.
I0304 19:28:05.337034 22579586809984 run.py:483] Algo bellman_ford step 279 current loss 0.277353, current_train_items 8960.
I0304 19:28:05.355921 22579586809984 run.py:483] Algo bellman_ford step 280 current loss 0.056614, current_train_items 8992.
I0304 19:28:05.372565 22579586809984 run.py:483] Algo bellman_ford step 281 current loss 0.150566, current_train_items 9024.
I0304 19:28:05.396614 22579586809984 run.py:483] Algo bellman_ford step 282 current loss 0.240319, current_train_items 9056.
I0304 19:28:05.426181 22579586809984 run.py:483] Algo bellman_ford step 283 current loss 0.285913, current_train_items 9088.
I0304 19:28:05.459457 22579586809984 run.py:483] Algo bellman_ford step 284 current loss 0.388561, current_train_items 9120.
I0304 19:28:05.478617 22579586809984 run.py:483] Algo bellman_ford step 285 current loss 0.049594, current_train_items 9152.
I0304 19:28:05.495130 22579586809984 run.py:483] Algo bellman_ford step 286 current loss 0.118708, current_train_items 9184.
I0304 19:28:05.518430 22579586809984 run.py:483] Algo bellman_ford step 287 current loss 0.209524, current_train_items 9216.
I0304 19:28:05.548751 22579586809984 run.py:483] Algo bellman_ford step 288 current loss 0.286130, current_train_items 9248.
I0304 19:28:05.581249 22579586809984 run.py:483] Algo bellman_ford step 289 current loss 0.353093, current_train_items 9280.
I0304 19:28:05.600642 22579586809984 run.py:483] Algo bellman_ford step 290 current loss 0.016524, current_train_items 9312.
I0304 19:28:05.616940 22579586809984 run.py:483] Algo bellman_ford step 291 current loss 0.142498, current_train_items 9344.
I0304 19:28:05.640102 22579586809984 run.py:483] Algo bellman_ford step 292 current loss 0.198737, current_train_items 9376.
I0304 19:28:05.669416 22579586809984 run.py:483] Algo bellman_ford step 293 current loss 0.240549, current_train_items 9408.
I0304 19:28:05.701062 22579586809984 run.py:483] Algo bellman_ford step 294 current loss 0.262162, current_train_items 9440.
I0304 19:28:05.719648 22579586809984 run.py:483] Algo bellman_ford step 295 current loss 0.064588, current_train_items 9472.
I0304 19:28:05.736007 22579586809984 run.py:483] Algo bellman_ford step 296 current loss 0.098986, current_train_items 9504.
I0304 19:28:05.760015 22579586809984 run.py:483] Algo bellman_ford step 297 current loss 0.251974, current_train_items 9536.
I0304 19:28:05.790613 22579586809984 run.py:483] Algo bellman_ford step 298 current loss 0.354369, current_train_items 9568.
I0304 19:28:05.821771 22579586809984 run.py:483] Algo bellman_ford step 299 current loss 0.353499, current_train_items 9600.
I0304 19:28:05.841325 22579586809984 run.py:483] Algo bellman_ford step 300 current loss 0.063424, current_train_items 9632.
I0304 19:28:05.849085 22579586809984 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.9404296875, 'score': 0.9404296875, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0304 19:28:05.849194 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.932, current avg val score is 0.940, val scores are: bellman_ford: 0.940
I0304 19:28:05.878909 22579586809984 run.py:483] Algo bellman_ford step 301 current loss 0.102859, current_train_items 9664.
I0304 19:28:05.901919 22579586809984 run.py:483] Algo bellman_ford step 302 current loss 0.198186, current_train_items 9696.
I0304 19:28:05.931180 22579586809984 run.py:483] Algo bellman_ford step 303 current loss 0.171623, current_train_items 9728.
I0304 19:28:05.963839 22579586809984 run.py:483] Algo bellman_ford step 304 current loss 0.417428, current_train_items 9760.
I0304 19:28:05.982931 22579586809984 run.py:483] Algo bellman_ford step 305 current loss 0.025535, current_train_items 9792.
I0304 19:28:05.999541 22579586809984 run.py:483] Algo bellman_ford step 306 current loss 0.229231, current_train_items 9824.
I0304 19:28:06.023966 22579586809984 run.py:483] Algo bellman_ford step 307 current loss 0.420778, current_train_items 9856.
I0304 19:28:06.054128 22579586809984 run.py:483] Algo bellman_ford step 308 current loss 0.413407, current_train_items 9888.
I0304 19:28:06.085755 22579586809984 run.py:483] Algo bellman_ford step 309 current loss 0.329123, current_train_items 9920.
I0304 19:28:06.104478 22579586809984 run.py:483] Algo bellman_ford step 310 current loss 0.107799, current_train_items 9952.
I0304 19:28:06.120645 22579586809984 run.py:483] Algo bellman_ford step 311 current loss 0.149300, current_train_items 9984.
I0304 19:28:06.144121 22579586809984 run.py:483] Algo bellman_ford step 312 current loss 0.341742, current_train_items 10016.
I0304 19:28:06.174541 22579586809984 run.py:483] Algo bellman_ford step 313 current loss 0.461903, current_train_items 10048.
I0304 19:28:06.206117 22579586809984 run.py:483] Algo bellman_ford step 314 current loss 0.321514, current_train_items 10080.
I0304 19:28:06.224569 22579586809984 run.py:483] Algo bellman_ford step 315 current loss 0.075989, current_train_items 10112.
I0304 19:28:06.240899 22579586809984 run.py:483] Algo bellman_ford step 316 current loss 0.173611, current_train_items 10144.
I0304 19:28:06.263657 22579586809984 run.py:483] Algo bellman_ford step 317 current loss 0.233572, current_train_items 10176.
I0304 19:28:06.292248 22579586809984 run.py:483] Algo bellman_ford step 318 current loss 0.209962, current_train_items 10208.
I0304 19:28:06.322881 22579586809984 run.py:483] Algo bellman_ford step 319 current loss 0.345721, current_train_items 10240.
I0304 19:28:06.341356 22579586809984 run.py:483] Algo bellman_ford step 320 current loss 0.032867, current_train_items 10272.
I0304 19:28:06.358152 22579586809984 run.py:483] Algo bellman_ford step 321 current loss 0.130076, current_train_items 10304.
I0304 19:28:06.381481 22579586809984 run.py:483] Algo bellman_ford step 322 current loss 0.348511, current_train_items 10336.
I0304 19:28:06.409875 22579586809984 run.py:483] Algo bellman_ford step 323 current loss 0.194481, current_train_items 10368.
I0304 19:28:06.441478 22579586809984 run.py:483] Algo bellman_ford step 324 current loss 0.316154, current_train_items 10400.
I0304 19:28:06.460297 22579586809984 run.py:483] Algo bellman_ford step 325 current loss 0.114128, current_train_items 10432.
I0304 19:28:06.476123 22579586809984 run.py:483] Algo bellman_ford step 326 current loss 0.108856, current_train_items 10464.
I0304 19:28:06.499351 22579586809984 run.py:483] Algo bellman_ford step 327 current loss 0.354661, current_train_items 10496.
I0304 19:28:06.529194 22579586809984 run.py:483] Algo bellman_ford step 328 current loss 0.428642, current_train_items 10528.
I0304 19:28:06.561300 22579586809984 run.py:483] Algo bellman_ford step 329 current loss 0.396445, current_train_items 10560.
I0304 19:28:06.579863 22579586809984 run.py:483] Algo bellman_ford step 330 current loss 0.069745, current_train_items 10592.
I0304 19:28:06.595938 22579586809984 run.py:483] Algo bellman_ford step 331 current loss 0.192339, current_train_items 10624.
I0304 19:28:06.619349 22579586809984 run.py:483] Algo bellman_ford step 332 current loss 0.524767, current_train_items 10656.
I0304 19:28:06.649258 22579586809984 run.py:483] Algo bellman_ford step 333 current loss 0.587985, current_train_items 10688.
I0304 19:28:06.682142 22579586809984 run.py:483] Algo bellman_ford step 334 current loss 0.626986, current_train_items 10720.
I0304 19:28:06.700636 22579586809984 run.py:483] Algo bellman_ford step 335 current loss 0.048435, current_train_items 10752.
I0304 19:28:06.717328 22579586809984 run.py:483] Algo bellman_ford step 336 current loss 0.172728, current_train_items 10784.
I0304 19:28:06.741968 22579586809984 run.py:483] Algo bellman_ford step 337 current loss 0.301903, current_train_items 10816.
I0304 19:28:06.771270 22579586809984 run.py:483] Algo bellman_ford step 338 current loss 0.321739, current_train_items 10848.
I0304 19:28:06.803626 22579586809984 run.py:483] Algo bellman_ford step 339 current loss 0.416119, current_train_items 10880.
I0304 19:28:06.822667 22579586809984 run.py:483] Algo bellman_ford step 340 current loss 0.054035, current_train_items 10912.
I0304 19:28:06.838649 22579586809984 run.py:483] Algo bellman_ford step 341 current loss 0.157398, current_train_items 10944.
I0304 19:28:06.861783 22579586809984 run.py:483] Algo bellman_ford step 342 current loss 0.376132, current_train_items 10976.
I0304 19:28:06.890220 22579586809984 run.py:483] Algo bellman_ford step 343 current loss 0.419785, current_train_items 11008.
I0304 19:28:06.922486 22579586809984 run.py:483] Algo bellman_ford step 344 current loss 0.491888, current_train_items 11040.
I0304 19:28:06.941213 22579586809984 run.py:483] Algo bellman_ford step 345 current loss 0.031272, current_train_items 11072.
I0304 19:28:06.957475 22579586809984 run.py:483] Algo bellman_ford step 346 current loss 0.108392, current_train_items 11104.
I0304 19:28:06.982043 22579586809984 run.py:483] Algo bellman_ford step 347 current loss 0.231745, current_train_items 11136.
I0304 19:28:07.011102 22579586809984 run.py:483] Algo bellman_ford step 348 current loss 0.272775, current_train_items 11168.
I0304 19:28:07.043726 22579586809984 run.py:483] Algo bellman_ford step 349 current loss 0.260685, current_train_items 11200.
I0304 19:28:07.062559 22579586809984 run.py:483] Algo bellman_ford step 350 current loss 0.049531, current_train_items 11232.
I0304 19:28:07.070403 22579586809984 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0304 19:28:07.070509 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.940, current avg val score is 0.955, val scores are: bellman_ford: 0.955
I0304 19:28:07.098831 22579586809984 run.py:483] Algo bellman_ford step 351 current loss 0.117136, current_train_items 11264.
I0304 19:28:07.123286 22579586809984 run.py:483] Algo bellman_ford step 352 current loss 0.233559, current_train_items 11296.
I0304 19:28:07.153172 22579586809984 run.py:483] Algo bellman_ford step 353 current loss 0.211319, current_train_items 11328.
I0304 19:28:07.187265 22579586809984 run.py:483] Algo bellman_ford step 354 current loss 0.299787, current_train_items 11360.
I0304 19:28:07.206252 22579586809984 run.py:483] Algo bellman_ford step 355 current loss 0.056090, current_train_items 11392.
I0304 19:28:07.222145 22579586809984 run.py:483] Algo bellman_ford step 356 current loss 0.101242, current_train_items 11424.
I0304 19:28:07.245317 22579586809984 run.py:483] Algo bellman_ford step 357 current loss 0.165702, current_train_items 11456.
I0304 19:28:07.276340 22579586809984 run.py:483] Algo bellman_ford step 358 current loss 0.201683, current_train_items 11488.
I0304 19:28:07.309401 22579586809984 run.py:483] Algo bellman_ford step 359 current loss 0.263688, current_train_items 11520.
I0304 19:28:07.328254 22579586809984 run.py:483] Algo bellman_ford step 360 current loss 0.031602, current_train_items 11552.
I0304 19:28:07.344897 22579586809984 run.py:483] Algo bellman_ford step 361 current loss 0.155859, current_train_items 11584.
I0304 19:28:07.367188 22579586809984 run.py:483] Algo bellman_ford step 362 current loss 0.175919, current_train_items 11616.
I0304 19:28:07.396617 22579586809984 run.py:483] Algo bellman_ford step 363 current loss 0.198534, current_train_items 11648.
I0304 19:28:07.426807 22579586809984 run.py:483] Algo bellman_ford step 364 current loss 0.226251, current_train_items 11680.
I0304 19:28:07.445508 22579586809984 run.py:483] Algo bellman_ford step 365 current loss 0.023320, current_train_items 11712.
I0304 19:28:07.461834 22579586809984 run.py:483] Algo bellman_ford step 366 current loss 0.134299, current_train_items 11744.
I0304 19:28:07.484800 22579586809984 run.py:483] Algo bellman_ford step 367 current loss 0.172141, current_train_items 11776.
I0304 19:28:07.515084 22579586809984 run.py:483] Algo bellman_ford step 368 current loss 0.306803, current_train_items 11808.
I0304 19:28:07.544894 22579586809984 run.py:483] Algo bellman_ford step 369 current loss 0.173670, current_train_items 11840.
I0304 19:28:07.563574 22579586809984 run.py:483] Algo bellman_ford step 370 current loss 0.034931, current_train_items 11872.
I0304 19:28:07.579716 22579586809984 run.py:483] Algo bellman_ford step 371 current loss 0.147509, current_train_items 11904.
I0304 19:28:07.602507 22579586809984 run.py:483] Algo bellman_ford step 372 current loss 0.220006, current_train_items 11936.
I0304 19:28:07.632741 22579586809984 run.py:483] Algo bellman_ford step 373 current loss 0.533144, current_train_items 11968.
I0304 19:28:07.666157 22579586809984 run.py:483] Algo bellman_ford step 374 current loss 0.570022, current_train_items 12000.
I0304 19:28:07.685261 22579586809984 run.py:483] Algo bellman_ford step 375 current loss 0.018387, current_train_items 12032.
I0304 19:28:07.701076 22579586809984 run.py:483] Algo bellman_ford step 376 current loss 0.051476, current_train_items 12064.
I0304 19:28:07.725076 22579586809984 run.py:483] Algo bellman_ford step 377 current loss 0.184647, current_train_items 12096.
I0304 19:28:07.754672 22579586809984 run.py:483] Algo bellman_ford step 378 current loss 0.175911, current_train_items 12128.
I0304 19:28:07.787517 22579586809984 run.py:483] Algo bellman_ford step 379 current loss 0.323532, current_train_items 12160.
I0304 19:28:07.806143 22579586809984 run.py:483] Algo bellman_ford step 380 current loss 0.032905, current_train_items 12192.
I0304 19:28:07.822581 22579586809984 run.py:483] Algo bellman_ford step 381 current loss 0.113866, current_train_items 12224.
I0304 19:28:07.846206 22579586809984 run.py:483] Algo bellman_ford step 382 current loss 0.125744, current_train_items 12256.
I0304 19:28:07.876054 22579586809984 run.py:483] Algo bellman_ford step 383 current loss 0.228569, current_train_items 12288.
I0304 19:28:07.909069 22579586809984 run.py:483] Algo bellman_ford step 384 current loss 0.237680, current_train_items 12320.
I0304 19:28:07.928252 22579586809984 run.py:483] Algo bellman_ford step 385 current loss 0.035131, current_train_items 12352.
I0304 19:28:07.944938 22579586809984 run.py:483] Algo bellman_ford step 386 current loss 0.049882, current_train_items 12384.
I0304 19:28:07.967848 22579586809984 run.py:483] Algo bellman_ford step 387 current loss 0.140245, current_train_items 12416.
I0304 19:28:07.996939 22579586809984 run.py:483] Algo bellman_ford step 388 current loss 0.213100, current_train_items 12448.
I0304 19:28:08.029546 22579586809984 run.py:483] Algo bellman_ford step 389 current loss 0.262353, current_train_items 12480.
I0304 19:28:08.048345 22579586809984 run.py:483] Algo bellman_ford step 390 current loss 0.021477, current_train_items 12512.
I0304 19:28:08.064709 22579586809984 run.py:483] Algo bellman_ford step 391 current loss 0.066064, current_train_items 12544.
I0304 19:28:08.087930 22579586809984 run.py:483] Algo bellman_ford step 392 current loss 0.198856, current_train_items 12576.
I0304 19:28:08.119284 22579586809984 run.py:483] Algo bellman_ford step 393 current loss 0.297204, current_train_items 12608.
I0304 19:28:08.150689 22579586809984 run.py:483] Algo bellman_ford step 394 current loss 0.290954, current_train_items 12640.
I0304 19:28:08.169269 22579586809984 run.py:483] Algo bellman_ford step 395 current loss 0.016721, current_train_items 12672.
I0304 19:28:08.185431 22579586809984 run.py:483] Algo bellman_ford step 396 current loss 0.145260, current_train_items 12704.
I0304 19:28:08.209375 22579586809984 run.py:483] Algo bellman_ford step 397 current loss 0.253510, current_train_items 12736.
I0304 19:28:08.239886 22579586809984 run.py:483] Algo bellman_ford step 398 current loss 0.215933, current_train_items 12768.
I0304 19:28:08.269639 22579586809984 run.py:483] Algo bellman_ford step 399 current loss 0.257300, current_train_items 12800.
I0304 19:28:08.288438 22579586809984 run.py:483] Algo bellman_ford step 400 current loss 0.016635, current_train_items 12832.
I0304 19:28:08.296438 22579586809984 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.9462890625, 'score': 0.9462890625, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0304 19:28:08.296562 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.946, val scores are: bellman_ford: 0.946
I0304 19:28:08.313688 22579586809984 run.py:483] Algo bellman_ford step 401 current loss 0.124711, current_train_items 12864.
I0304 19:28:08.338396 22579586809984 run.py:483] Algo bellman_ford step 402 current loss 0.273568, current_train_items 12896.
I0304 19:28:08.368959 22579586809984 run.py:483] Algo bellman_ford step 403 current loss 0.368584, current_train_items 12928.
I0304 19:28:08.400226 22579586809984 run.py:483] Algo bellman_ford step 404 current loss 0.393086, current_train_items 12960.
I0304 19:28:08.419314 22579586809984 run.py:483] Algo bellman_ford step 405 current loss 0.093823, current_train_items 12992.
I0304 19:28:08.434773 22579586809984 run.py:483] Algo bellman_ford step 406 current loss 0.121275, current_train_items 13024.
I0304 19:28:08.458697 22579586809984 run.py:483] Algo bellman_ford step 407 current loss 0.224776, current_train_items 13056.
I0304 19:28:08.489669 22579586809984 run.py:483] Algo bellman_ford step 408 current loss 0.279431, current_train_items 13088.
I0304 19:28:08.521405 22579586809984 run.py:483] Algo bellman_ford step 409 current loss 0.283925, current_train_items 13120.
I0304 19:28:08.540193 22579586809984 run.py:483] Algo bellman_ford step 410 current loss 0.035056, current_train_items 13152.
I0304 19:28:08.556217 22579586809984 run.py:483] Algo bellman_ford step 411 current loss 0.088097, current_train_items 13184.
I0304 19:28:08.579687 22579586809984 run.py:483] Algo bellman_ford step 412 current loss 0.181088, current_train_items 13216.
I0304 19:28:08.609930 22579586809984 run.py:483] Algo bellman_ford step 413 current loss 0.209452, current_train_items 13248.
I0304 19:28:08.642113 22579586809984 run.py:483] Algo bellman_ford step 414 current loss 0.246262, current_train_items 13280.
I0304 19:28:08.661138 22579586809984 run.py:483] Algo bellman_ford step 415 current loss 0.042949, current_train_items 13312.
I0304 19:28:08.677108 22579586809984 run.py:483] Algo bellman_ford step 416 current loss 0.086365, current_train_items 13344.
I0304 19:28:08.701287 22579586809984 run.py:483] Algo bellman_ford step 417 current loss 0.161643, current_train_items 13376.
I0304 19:28:08.732729 22579586809984 run.py:483] Algo bellman_ford step 418 current loss 0.335489, current_train_items 13408.
I0304 19:28:08.764482 22579586809984 run.py:483] Algo bellman_ford step 419 current loss 0.346484, current_train_items 13440.
I0304 19:28:08.783312 22579586809984 run.py:483] Algo bellman_ford step 420 current loss 0.124087, current_train_items 13472.
I0304 19:28:08.799242 22579586809984 run.py:483] Algo bellman_ford step 421 current loss 0.104663, current_train_items 13504.
I0304 19:28:08.823197 22579586809984 run.py:483] Algo bellman_ford step 422 current loss 0.192941, current_train_items 13536.
I0304 19:28:08.852014 22579586809984 run.py:483] Algo bellman_ford step 423 current loss 0.279764, current_train_items 13568.
I0304 19:28:08.883490 22579586809984 run.py:483] Algo bellman_ford step 424 current loss 0.379395, current_train_items 13600.
I0304 19:28:08.902130 22579586809984 run.py:483] Algo bellman_ford step 425 current loss 0.151457, current_train_items 13632.
I0304 19:28:08.917974 22579586809984 run.py:483] Algo bellman_ford step 426 current loss 0.093382, current_train_items 13664.
I0304 19:28:08.940733 22579586809984 run.py:483] Algo bellman_ford step 427 current loss 0.200222, current_train_items 13696.
I0304 19:28:08.969863 22579586809984 run.py:483] Algo bellman_ford step 428 current loss 0.208830, current_train_items 13728.
I0304 19:28:09.001532 22579586809984 run.py:483] Algo bellman_ford step 429 current loss 0.242775, current_train_items 13760.
I0304 19:28:09.019962 22579586809984 run.py:483] Algo bellman_ford step 430 current loss 0.044879, current_train_items 13792.
I0304 19:28:09.036103 22579586809984 run.py:483] Algo bellman_ford step 431 current loss 0.081487, current_train_items 13824.
I0304 19:28:09.059886 22579586809984 run.py:483] Algo bellman_ford step 432 current loss 0.211356, current_train_items 13856.
I0304 19:28:09.088384 22579586809984 run.py:483] Algo bellman_ford step 433 current loss 0.157636, current_train_items 13888.
I0304 19:28:09.119863 22579586809984 run.py:483] Algo bellman_ford step 434 current loss 0.239782, current_train_items 13920.
I0304 19:28:09.137969 22579586809984 run.py:483] Algo bellman_ford step 435 current loss 0.018562, current_train_items 13952.
I0304 19:28:09.154369 22579586809984 run.py:483] Algo bellman_ford step 436 current loss 0.087361, current_train_items 13984.
I0304 19:28:09.177255 22579586809984 run.py:483] Algo bellman_ford step 437 current loss 0.196099, current_train_items 14016.
I0304 19:28:09.205999 22579586809984 run.py:483] Algo bellman_ford step 438 current loss 0.193030, current_train_items 14048.
I0304 19:28:09.237639 22579586809984 run.py:483] Algo bellman_ford step 439 current loss 0.248132, current_train_items 14080.
I0304 19:28:09.256063 22579586809984 run.py:483] Algo bellman_ford step 440 current loss 0.031559, current_train_items 14112.
I0304 19:28:09.271918 22579586809984 run.py:483] Algo bellman_ford step 441 current loss 0.087016, current_train_items 14144.
I0304 19:28:09.294972 22579586809984 run.py:483] Algo bellman_ford step 442 current loss 0.138708, current_train_items 14176.
I0304 19:28:09.324395 22579586809984 run.py:483] Algo bellman_ford step 443 current loss 0.217974, current_train_items 14208.
I0304 19:28:09.355277 22579586809984 run.py:483] Algo bellman_ford step 444 current loss 0.222249, current_train_items 14240.
I0304 19:28:09.373656 22579586809984 run.py:483] Algo bellman_ford step 445 current loss 0.033911, current_train_items 14272.
I0304 19:28:09.390663 22579586809984 run.py:483] Algo bellman_ford step 446 current loss 0.143168, current_train_items 14304.
I0304 19:28:09.414552 22579586809984 run.py:483] Algo bellman_ford step 447 current loss 0.208065, current_train_items 14336.
I0304 19:28:09.442511 22579586809984 run.py:483] Algo bellman_ford step 448 current loss 0.159480, current_train_items 14368.
I0304 19:28:09.472497 22579586809984 run.py:483] Algo bellman_ford step 449 current loss 0.224662, current_train_items 14400.
I0304 19:28:09.491162 22579586809984 run.py:483] Algo bellman_ford step 450 current loss 0.044312, current_train_items 14432.
I0304 19:28:09.499310 22579586809984 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.9541015625, 'score': 0.9541015625, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0304 19:28:09.499414 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.954, val scores are: bellman_ford: 0.954
I0304 19:28:09.516747 22579586809984 run.py:483] Algo bellman_ford step 451 current loss 0.092607, current_train_items 14464.
I0304 19:28:09.540230 22579586809984 run.py:483] Algo bellman_ford step 452 current loss 0.102282, current_train_items 14496.
I0304 19:28:09.571102 22579586809984 run.py:483] Algo bellman_ford step 453 current loss 0.203576, current_train_items 14528.
I0304 19:28:09.603255 22579586809984 run.py:483] Algo bellman_ford step 454 current loss 0.250122, current_train_items 14560.
I0304 19:28:09.622106 22579586809984 run.py:483] Algo bellman_ford step 455 current loss 0.019750, current_train_items 14592.
I0304 19:28:09.638319 22579586809984 run.py:483] Algo bellman_ford step 456 current loss 0.111826, current_train_items 14624.
I0304 19:28:09.661206 22579586809984 run.py:483] Algo bellman_ford step 457 current loss 0.199347, current_train_items 14656.
I0304 19:28:09.690374 22579586809984 run.py:483] Algo bellman_ford step 458 current loss 0.130863, current_train_items 14688.
I0304 19:28:09.720719 22579586809984 run.py:483] Algo bellman_ford step 459 current loss 0.218415, current_train_items 14720.
I0304 19:28:09.739391 22579586809984 run.py:483] Algo bellman_ford step 460 current loss 0.020250, current_train_items 14752.
I0304 19:28:09.756069 22579586809984 run.py:483] Algo bellman_ford step 461 current loss 0.091644, current_train_items 14784.
I0304 19:28:09.779592 22579586809984 run.py:483] Algo bellman_ford step 462 current loss 0.136066, current_train_items 14816.
I0304 19:28:09.809351 22579586809984 run.py:483] Algo bellman_ford step 463 current loss 0.161626, current_train_items 14848.
I0304 19:28:09.842446 22579586809984 run.py:483] Algo bellman_ford step 464 current loss 0.236973, current_train_items 14880.
I0304 19:28:09.861093 22579586809984 run.py:483] Algo bellman_ford step 465 current loss 0.034522, current_train_items 14912.
I0304 19:28:09.876951 22579586809984 run.py:483] Algo bellman_ford step 466 current loss 0.065865, current_train_items 14944.
I0304 19:28:09.901068 22579586809984 run.py:483] Algo bellman_ford step 467 current loss 0.252735, current_train_items 14976.
I0304 19:28:09.929711 22579586809984 run.py:483] Algo bellman_ford step 468 current loss 0.157302, current_train_items 15008.
I0304 19:28:09.962823 22579586809984 run.py:483] Algo bellman_ford step 469 current loss 0.299163, current_train_items 15040.
I0304 19:28:09.981600 22579586809984 run.py:483] Algo bellman_ford step 470 current loss 0.022496, current_train_items 15072.
I0304 19:28:09.997819 22579586809984 run.py:483] Algo bellman_ford step 471 current loss 0.083509, current_train_items 15104.
I0304 19:28:10.021464 22579586809984 run.py:483] Algo bellman_ford step 472 current loss 0.179810, current_train_items 15136.
I0304 19:28:10.051460 22579586809984 run.py:483] Algo bellman_ford step 473 current loss 0.199887, current_train_items 15168.
I0304 19:28:10.083962 22579586809984 run.py:483] Algo bellman_ford step 474 current loss 0.305842, current_train_items 15200.
I0304 19:28:10.103165 22579586809984 run.py:483] Algo bellman_ford step 475 current loss 0.025113, current_train_items 15232.
I0304 19:28:10.119170 22579586809984 run.py:483] Algo bellman_ford step 476 current loss 0.136125, current_train_items 15264.
I0304 19:28:10.142648 22579586809984 run.py:483] Algo bellman_ford step 477 current loss 0.153255, current_train_items 15296.
I0304 19:28:10.171094 22579586809984 run.py:483] Algo bellman_ford step 478 current loss 0.124873, current_train_items 15328.
I0304 19:28:10.203595 22579586809984 run.py:483] Algo bellman_ford step 479 current loss 0.483128, current_train_items 15360.
I0304 19:28:10.221873 22579586809984 run.py:483] Algo bellman_ford step 480 current loss 0.096368, current_train_items 15392.
I0304 19:28:10.237961 22579586809984 run.py:483] Algo bellman_ford step 481 current loss 0.080520, current_train_items 15424.
I0304 19:28:10.260979 22579586809984 run.py:483] Algo bellman_ford step 482 current loss 0.124730, current_train_items 15456.
I0304 19:28:10.290644 22579586809984 run.py:483] Algo bellman_ford step 483 current loss 0.170636, current_train_items 15488.
I0304 19:28:10.322903 22579586809984 run.py:483] Algo bellman_ford step 484 current loss 0.295787, current_train_items 15520.
I0304 19:28:10.341543 22579586809984 run.py:483] Algo bellman_ford step 485 current loss 0.048678, current_train_items 15552.
I0304 19:28:10.357853 22579586809984 run.py:483] Algo bellman_ford step 486 current loss 0.069425, current_train_items 15584.
I0304 19:28:10.381472 22579586809984 run.py:483] Algo bellman_ford step 487 current loss 0.143961, current_train_items 15616.
I0304 19:28:10.410537 22579586809984 run.py:483] Algo bellman_ford step 488 current loss 0.153021, current_train_items 15648.
I0304 19:28:10.441976 22579586809984 run.py:483] Algo bellman_ford step 489 current loss 0.226166, current_train_items 15680.
I0304 19:28:10.460711 22579586809984 run.py:483] Algo bellman_ford step 490 current loss 0.014574, current_train_items 15712.
I0304 19:28:10.477119 22579586809984 run.py:483] Algo bellman_ford step 491 current loss 0.061879, current_train_items 15744.
I0304 19:28:10.500085 22579586809984 run.py:483] Algo bellman_ford step 492 current loss 0.109588, current_train_items 15776.
I0304 19:28:10.529141 22579586809984 run.py:483] Algo bellman_ford step 493 current loss 0.147487, current_train_items 15808.
I0304 19:28:10.562340 22579586809984 run.py:483] Algo bellman_ford step 494 current loss 0.239114, current_train_items 15840.
I0304 19:28:10.580840 22579586809984 run.py:483] Algo bellman_ford step 495 current loss 0.011033, current_train_items 15872.
I0304 19:28:10.596812 22579586809984 run.py:483] Algo bellman_ford step 496 current loss 0.066892, current_train_items 15904.
I0304 19:28:10.621324 22579586809984 run.py:483] Algo bellman_ford step 497 current loss 0.222742, current_train_items 15936.
I0304 19:28:10.651585 22579586809984 run.py:483] Algo bellman_ford step 498 current loss 0.168224, current_train_items 15968.
I0304 19:28:10.683065 22579586809984 run.py:483] Algo bellman_ford step 499 current loss 0.183122, current_train_items 16000.
I0304 19:28:10.701848 22579586809984 run.py:483] Algo bellman_ford step 500 current loss 0.017540, current_train_items 16032.
I0304 19:28:10.709599 22579586809984 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0304 19:28:10.709713 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.955, val scores are: bellman_ford: 0.955
I0304 19:28:10.725966 22579586809984 run.py:483] Algo bellman_ford step 501 current loss 0.031111, current_train_items 16064.
I0304 19:28:10.750101 22579586809984 run.py:483] Algo bellman_ford step 502 current loss 0.182149, current_train_items 16096.
I0304 19:28:10.781283 22579586809984 run.py:483] Algo bellman_ford step 503 current loss 0.313784, current_train_items 16128.
I0304 19:28:10.815868 22579586809984 run.py:483] Algo bellman_ford step 504 current loss 0.391559, current_train_items 16160.
I0304 19:28:10.835081 22579586809984 run.py:483] Algo bellman_ford step 505 current loss 0.042245, current_train_items 16192.
I0304 19:28:10.850463 22579586809984 run.py:483] Algo bellman_ford step 506 current loss 0.078382, current_train_items 16224.
I0304 19:28:10.873268 22579586809984 run.py:483] Algo bellman_ford step 507 current loss 0.099249, current_train_items 16256.
I0304 19:28:10.903695 22579586809984 run.py:483] Algo bellman_ford step 508 current loss 0.160428, current_train_items 16288.
I0304 19:28:10.936369 22579586809984 run.py:483] Algo bellman_ford step 509 current loss 0.302416, current_train_items 16320.
I0304 19:28:10.955482 22579586809984 run.py:483] Algo bellman_ford step 510 current loss 0.029056, current_train_items 16352.
I0304 19:28:10.971766 22579586809984 run.py:483] Algo bellman_ford step 511 current loss 0.125267, current_train_items 16384.
I0304 19:28:10.995398 22579586809984 run.py:483] Algo bellman_ford step 512 current loss 0.203606, current_train_items 16416.
I0304 19:28:11.025792 22579586809984 run.py:483] Algo bellman_ford step 513 current loss 0.227157, current_train_items 16448.
I0304 19:28:11.058305 22579586809984 run.py:483] Algo bellman_ford step 514 current loss 0.175535, current_train_items 16480.
I0304 19:28:11.076735 22579586809984 run.py:483] Algo bellman_ford step 515 current loss 0.014503, current_train_items 16512.
I0304 19:28:11.092581 22579586809984 run.py:483] Algo bellman_ford step 516 current loss 0.094670, current_train_items 16544.
I0304 19:28:11.116764 22579586809984 run.py:483] Algo bellman_ford step 517 current loss 0.142576, current_train_items 16576.
I0304 19:28:11.147799 22579586809984 run.py:483] Algo bellman_ford step 518 current loss 0.177289, current_train_items 16608.
I0304 19:28:11.180269 22579586809984 run.py:483] Algo bellman_ford step 519 current loss 0.187437, current_train_items 16640.
I0304 19:28:11.198953 22579586809984 run.py:483] Algo bellman_ford step 520 current loss 0.033685, current_train_items 16672.
I0304 19:28:11.215521 22579586809984 run.py:483] Algo bellman_ford step 521 current loss 0.066502, current_train_items 16704.
I0304 19:28:11.239171 22579586809984 run.py:483] Algo bellman_ford step 522 current loss 0.098530, current_train_items 16736.
I0304 19:28:11.269578 22579586809984 run.py:483] Algo bellman_ford step 523 current loss 0.174532, current_train_items 16768.
I0304 19:28:11.300857 22579586809984 run.py:483] Algo bellman_ford step 524 current loss 0.257510, current_train_items 16800.
I0304 19:28:11.319349 22579586809984 run.py:483] Algo bellman_ford step 525 current loss 0.028321, current_train_items 16832.
I0304 19:28:11.335892 22579586809984 run.py:483] Algo bellman_ford step 526 current loss 0.094894, current_train_items 16864.
I0304 19:28:11.359669 22579586809984 run.py:483] Algo bellman_ford step 527 current loss 0.112903, current_train_items 16896.
I0304 19:28:11.388319 22579586809984 run.py:483] Algo bellman_ford step 528 current loss 0.153268, current_train_items 16928.
I0304 19:28:11.420872 22579586809984 run.py:483] Algo bellman_ford step 529 current loss 0.202753, current_train_items 16960.
I0304 19:28:11.439726 22579586809984 run.py:483] Algo bellman_ford step 530 current loss 0.019909, current_train_items 16992.
I0304 19:28:11.455985 22579586809984 run.py:483] Algo bellman_ford step 531 current loss 0.056267, current_train_items 17024.
I0304 19:28:11.479161 22579586809984 run.py:483] Algo bellman_ford step 532 current loss 0.112605, current_train_items 17056.
I0304 19:28:11.509519 22579586809984 run.py:483] Algo bellman_ford step 533 current loss 0.186011, current_train_items 17088.
I0304 19:28:11.542159 22579586809984 run.py:483] Algo bellman_ford step 534 current loss 0.211537, current_train_items 17120.
I0304 19:28:11.560786 22579586809984 run.py:483] Algo bellman_ford step 535 current loss 0.020317, current_train_items 17152.
I0304 19:28:11.576566 22579586809984 run.py:483] Algo bellman_ford step 536 current loss 0.067346, current_train_items 17184.
I0304 19:28:11.599434 22579586809984 run.py:483] Algo bellman_ford step 537 current loss 0.103310, current_train_items 17216.
I0304 19:28:11.628754 22579586809984 run.py:483] Algo bellman_ford step 538 current loss 0.113141, current_train_items 17248.
I0304 19:28:11.659840 22579586809984 run.py:483] Algo bellman_ford step 539 current loss 0.211573, current_train_items 17280.
I0304 19:28:11.678565 22579586809984 run.py:483] Algo bellman_ford step 540 current loss 0.044818, current_train_items 17312.
I0304 19:28:11.694264 22579586809984 run.py:483] Algo bellman_ford step 541 current loss 0.039513, current_train_items 17344.
I0304 19:28:11.718605 22579586809984 run.py:483] Algo bellman_ford step 542 current loss 0.196230, current_train_items 17376.
I0304 19:28:11.749143 22579586809984 run.py:483] Algo bellman_ford step 543 current loss 0.171600, current_train_items 17408.
I0304 19:28:11.781663 22579586809984 run.py:483] Algo bellman_ford step 544 current loss 0.205592, current_train_items 17440.
I0304 19:28:11.800370 22579586809984 run.py:483] Algo bellman_ford step 545 current loss 0.016490, current_train_items 17472.
I0304 19:28:11.816640 22579586809984 run.py:483] Algo bellman_ford step 546 current loss 0.088774, current_train_items 17504.
I0304 19:28:11.840365 22579586809984 run.py:483] Algo bellman_ford step 547 current loss 0.223689, current_train_items 17536.
I0304 19:28:11.870896 22579586809984 run.py:483] Algo bellman_ford step 548 current loss 0.299171, current_train_items 17568.
I0304 19:28:11.900856 22579586809984 run.py:483] Algo bellman_ford step 549 current loss 0.192193, current_train_items 17600.
I0304 19:28:11.919662 22579586809984 run.py:483] Algo bellman_ford step 550 current loss 0.046692, current_train_items 17632.
I0304 19:28:11.927733 22579586809984 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0304 19:28:11.927838 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0304 19:28:11.944501 22579586809984 run.py:483] Algo bellman_ford step 551 current loss 0.077792, current_train_items 17664.
I0304 19:28:11.968272 22579586809984 run.py:483] Algo bellman_ford step 552 current loss 0.290136, current_train_items 17696.
I0304 19:28:12.000136 22579586809984 run.py:483] Algo bellman_ford step 553 current loss 0.639335, current_train_items 17728.
I0304 19:28:12.032575 22579586809984 run.py:483] Algo bellman_ford step 554 current loss 0.350547, current_train_items 17760.
I0304 19:28:12.051928 22579586809984 run.py:483] Algo bellman_ford step 555 current loss 0.054429, current_train_items 17792.
I0304 19:28:12.067869 22579586809984 run.py:483] Algo bellman_ford step 556 current loss 0.121702, current_train_items 17824.
I0304 19:28:12.092267 22579586809984 run.py:483] Algo bellman_ford step 557 current loss 0.445658, current_train_items 17856.
I0304 19:28:12.121967 22579586809984 run.py:483] Algo bellman_ford step 558 current loss 0.347889, current_train_items 17888.
I0304 19:28:12.155925 22579586809984 run.py:483] Algo bellman_ford step 559 current loss 0.318819, current_train_items 17920.
I0304 19:28:12.175282 22579586809984 run.py:483] Algo bellman_ford step 560 current loss 0.036082, current_train_items 17952.
I0304 19:28:12.191732 22579586809984 run.py:483] Algo bellman_ford step 561 current loss 0.089643, current_train_items 17984.
I0304 19:28:12.215569 22579586809984 run.py:483] Algo bellman_ford step 562 current loss 0.185319, current_train_items 18016.
I0304 19:28:12.245187 22579586809984 run.py:483] Algo bellman_ford step 563 current loss 0.226484, current_train_items 18048.
I0304 19:28:12.275160 22579586809984 run.py:483] Algo bellman_ford step 564 current loss 0.289015, current_train_items 18080.
I0304 19:28:12.293933 22579586809984 run.py:483] Algo bellman_ford step 565 current loss 0.016344, current_train_items 18112.
I0304 19:28:12.310353 22579586809984 run.py:483] Algo bellman_ford step 566 current loss 0.056967, current_train_items 18144.
I0304 19:28:12.334760 22579586809984 run.py:483] Algo bellman_ford step 567 current loss 0.210734, current_train_items 18176.
I0304 19:28:12.364671 22579586809984 run.py:483] Algo bellman_ford step 568 current loss 0.294293, current_train_items 18208.
I0304 19:28:12.396022 22579586809984 run.py:483] Algo bellman_ford step 569 current loss 0.182427, current_train_items 18240.
I0304 19:28:12.414886 22579586809984 run.py:483] Algo bellman_ford step 570 current loss 0.083279, current_train_items 18272.
I0304 19:28:12.431060 22579586809984 run.py:483] Algo bellman_ford step 571 current loss 0.103483, current_train_items 18304.
I0304 19:28:12.455409 22579586809984 run.py:483] Algo bellman_ford step 572 current loss 0.192325, current_train_items 18336.
I0304 19:28:12.486750 22579586809984 run.py:483] Algo bellman_ford step 573 current loss 0.225168, current_train_items 18368.
I0304 19:28:12.517776 22579586809984 run.py:483] Algo bellman_ford step 574 current loss 0.227071, current_train_items 18400.
I0304 19:28:12.537145 22579586809984 run.py:483] Algo bellman_ford step 575 current loss 0.068562, current_train_items 18432.
I0304 19:28:12.553181 22579586809984 run.py:483] Algo bellman_ford step 576 current loss 0.076462, current_train_items 18464.
I0304 19:28:12.576619 22579586809984 run.py:483] Algo bellman_ford step 577 current loss 0.203143, current_train_items 18496.
I0304 19:28:12.606848 22579586809984 run.py:483] Algo bellman_ford step 578 current loss 0.208041, current_train_items 18528.
I0304 19:28:12.639967 22579586809984 run.py:483] Algo bellman_ford step 579 current loss 0.378857, current_train_items 18560.
I0304 19:28:12.658964 22579586809984 run.py:483] Algo bellman_ford step 580 current loss 0.084542, current_train_items 18592.
I0304 19:28:12.675131 22579586809984 run.py:483] Algo bellman_ford step 581 current loss 0.110390, current_train_items 18624.
I0304 19:28:12.697450 22579586809984 run.py:483] Algo bellman_ford step 582 current loss 0.198782, current_train_items 18656.
I0304 19:28:12.726250 22579586809984 run.py:483] Algo bellman_ford step 583 current loss 0.113768, current_train_items 18688.
I0304 19:28:12.758211 22579586809984 run.py:483] Algo bellman_ford step 584 current loss 0.205644, current_train_items 18720.
I0304 19:28:12.777227 22579586809984 run.py:483] Algo bellman_ford step 585 current loss 0.065829, current_train_items 18752.
I0304 19:28:12.793493 22579586809984 run.py:483] Algo bellman_ford step 586 current loss 0.111211, current_train_items 18784.
I0304 19:28:12.815774 22579586809984 run.py:483] Algo bellman_ford step 587 current loss 0.064351, current_train_items 18816.
I0304 19:28:12.845061 22579586809984 run.py:483] Algo bellman_ford step 588 current loss 0.163862, current_train_items 18848.
I0304 19:28:12.878554 22579586809984 run.py:483] Algo bellman_ford step 589 current loss 0.276888, current_train_items 18880.
I0304 19:28:12.897742 22579586809984 run.py:483] Algo bellman_ford step 590 current loss 0.021559, current_train_items 18912.
I0304 19:28:12.914077 22579586809984 run.py:483] Algo bellman_ford step 591 current loss 0.066075, current_train_items 18944.
I0304 19:28:12.938274 22579586809984 run.py:483] Algo bellman_ford step 592 current loss 0.170592, current_train_items 18976.
I0304 19:28:12.967887 22579586809984 run.py:483] Algo bellman_ford step 593 current loss 0.150021, current_train_items 19008.
I0304 19:28:12.999346 22579586809984 run.py:483] Algo bellman_ford step 594 current loss 0.265639, current_train_items 19040.
I0304 19:28:13.018318 22579586809984 run.py:483] Algo bellman_ford step 595 current loss 0.031456, current_train_items 19072.
I0304 19:28:13.034548 22579586809984 run.py:483] Algo bellman_ford step 596 current loss 0.050947, current_train_items 19104.
I0304 19:28:13.058791 22579586809984 run.py:483] Algo bellman_ford step 597 current loss 0.165411, current_train_items 19136.
I0304 19:28:13.088454 22579586809984 run.py:483] Algo bellman_ford step 598 current loss 0.146070, current_train_items 19168.
I0304 19:28:13.120178 22579586809984 run.py:483] Algo bellman_ford step 599 current loss 0.356929, current_train_items 19200.
I0304 19:28:13.139251 22579586809984 run.py:483] Algo bellman_ford step 600 current loss 0.052461, current_train_items 19232.
I0304 19:28:13.147112 22579586809984 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.9521484375, 'score': 0.9521484375, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0304 19:28:13.147218 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.952, val scores are: bellman_ford: 0.952
I0304 19:28:13.163791 22579586809984 run.py:483] Algo bellman_ford step 601 current loss 0.115462, current_train_items 19264.
I0304 19:28:13.187646 22579586809984 run.py:483] Algo bellman_ford step 602 current loss 0.128545, current_train_items 19296.
I0304 19:28:13.216853 22579586809984 run.py:483] Algo bellman_ford step 603 current loss 0.205536, current_train_items 19328.
I0304 19:28:13.247850 22579586809984 run.py:483] Algo bellman_ford step 604 current loss 0.206116, current_train_items 19360.
I0304 19:28:13.266793 22579586809984 run.py:483] Algo bellman_ford step 605 current loss 0.036110, current_train_items 19392.
I0304 19:28:13.282302 22579586809984 run.py:483] Algo bellman_ford step 606 current loss 0.048509, current_train_items 19424.
I0304 19:28:13.306442 22579586809984 run.py:483] Algo bellman_ford step 607 current loss 0.134825, current_train_items 19456.
I0304 19:28:13.335170 22579586809984 run.py:483] Algo bellman_ford step 608 current loss 0.185073, current_train_items 19488.
I0304 19:28:13.366986 22579586809984 run.py:483] Algo bellman_ford step 609 current loss 0.176701, current_train_items 19520.
I0304 19:28:13.385647 22579586809984 run.py:483] Algo bellman_ford step 610 current loss 0.041012, current_train_items 19552.
I0304 19:28:13.402062 22579586809984 run.py:483] Algo bellman_ford step 611 current loss 0.038070, current_train_items 19584.
I0304 19:28:13.425693 22579586809984 run.py:483] Algo bellman_ford step 612 current loss 0.101943, current_train_items 19616.
I0304 19:28:13.453655 22579586809984 run.py:483] Algo bellman_ford step 613 current loss 0.108096, current_train_items 19648.
I0304 19:28:13.485199 22579586809984 run.py:483] Algo bellman_ford step 614 current loss 0.180622, current_train_items 19680.
I0304 19:28:13.503580 22579586809984 run.py:483] Algo bellman_ford step 615 current loss 0.009427, current_train_items 19712.
I0304 19:28:13.520128 22579586809984 run.py:483] Algo bellman_ford step 616 current loss 0.042869, current_train_items 19744.
I0304 19:28:13.543074 22579586809984 run.py:483] Algo bellman_ford step 617 current loss 0.107122, current_train_items 19776.
I0304 19:28:13.571803 22579586809984 run.py:483] Algo bellman_ford step 618 current loss 0.122658, current_train_items 19808.
I0304 19:28:13.603393 22579586809984 run.py:483] Algo bellman_ford step 619 current loss 0.158659, current_train_items 19840.
I0304 19:28:13.622101 22579586809984 run.py:483] Algo bellman_ford step 620 current loss 0.041814, current_train_items 19872.
I0304 19:28:13.638734 22579586809984 run.py:483] Algo bellman_ford step 621 current loss 0.124267, current_train_items 19904.
I0304 19:28:13.662350 22579586809984 run.py:483] Algo bellman_ford step 622 current loss 0.152502, current_train_items 19936.
I0304 19:28:13.691500 22579586809984 run.py:483] Algo bellman_ford step 623 current loss 0.148355, current_train_items 19968.
I0304 19:28:13.722956 22579586809984 run.py:483] Algo bellman_ford step 624 current loss 0.236035, current_train_items 20000.
I0304 19:28:13.741451 22579586809984 run.py:483] Algo bellman_ford step 625 current loss 0.024677, current_train_items 20032.
I0304 19:28:13.757423 22579586809984 run.py:483] Algo bellman_ford step 626 current loss 0.104539, current_train_items 20064.
I0304 19:28:13.781603 22579586809984 run.py:483] Algo bellman_ford step 627 current loss 0.159293, current_train_items 20096.
I0304 19:28:13.811651 22579586809984 run.py:483] Algo bellman_ford step 628 current loss 0.216933, current_train_items 20128.
I0304 19:28:13.843562 22579586809984 run.py:483] Algo bellman_ford step 629 current loss 0.360795, current_train_items 20160.
I0304 19:28:13.862151 22579586809984 run.py:483] Algo bellman_ford step 630 current loss 0.031026, current_train_items 20192.
I0304 19:28:13.878134 22579586809984 run.py:483] Algo bellman_ford step 631 current loss 0.123274, current_train_items 20224.
I0304 19:28:13.901388 22579586809984 run.py:483] Algo bellman_ford step 632 current loss 0.226182, current_train_items 20256.
I0304 19:28:13.930615 22579586809984 run.py:483] Algo bellman_ford step 633 current loss 0.207985, current_train_items 20288.
I0304 19:28:13.961699 22579586809984 run.py:483] Algo bellman_ford step 634 current loss 0.171865, current_train_items 20320.
I0304 19:28:13.980165 22579586809984 run.py:483] Algo bellman_ford step 635 current loss 0.048908, current_train_items 20352.
I0304 19:28:13.996545 22579586809984 run.py:483] Algo bellman_ford step 636 current loss 0.198649, current_train_items 20384.
I0304 19:28:14.020804 22579586809984 run.py:483] Algo bellman_ford step 637 current loss 0.206406, current_train_items 20416.
I0304 19:28:14.051191 22579586809984 run.py:483] Algo bellman_ford step 638 current loss 0.230804, current_train_items 20448.
I0304 19:28:14.080040 22579586809984 run.py:483] Algo bellman_ford step 639 current loss 0.266680, current_train_items 20480.
I0304 19:28:14.098539 22579586809984 run.py:483] Algo bellman_ford step 640 current loss 0.039950, current_train_items 20512.
I0304 19:28:14.114705 22579586809984 run.py:483] Algo bellman_ford step 641 current loss 0.147791, current_train_items 20544.
I0304 19:28:14.137049 22579586809984 run.py:483] Algo bellman_ford step 642 current loss 0.424371, current_train_items 20576.
I0304 19:28:14.166157 22579586809984 run.py:483] Algo bellman_ford step 643 current loss 0.276199, current_train_items 20608.
I0304 19:28:14.198173 22579586809984 run.py:483] Algo bellman_ford step 644 current loss 0.274389, current_train_items 20640.
I0304 19:28:14.216636 22579586809984 run.py:483] Algo bellman_ford step 645 current loss 0.025411, current_train_items 20672.
I0304 19:28:14.233741 22579586809984 run.py:483] Algo bellman_ford step 646 current loss 0.226186, current_train_items 20704.
I0304 19:28:14.256631 22579586809984 run.py:483] Algo bellman_ford step 647 current loss 0.291488, current_train_items 20736.
I0304 19:28:14.286883 22579586809984 run.py:483] Algo bellman_ford step 648 current loss 0.655195, current_train_items 20768.
I0304 19:28:14.319248 22579586809984 run.py:483] Algo bellman_ford step 649 current loss 0.448326, current_train_items 20800.
I0304 19:28:14.338161 22579586809984 run.py:483] Algo bellman_ford step 650 current loss 0.047786, current_train_items 20832.
I0304 19:28:14.346546 22579586809984 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.9482421875, 'score': 0.9482421875, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0304 19:28:14.346652 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.948, val scores are: bellman_ford: 0.948
I0304 19:28:14.364060 22579586809984 run.py:483] Algo bellman_ford step 651 current loss 0.081877, current_train_items 20864.
I0304 19:28:14.387728 22579586809984 run.py:483] Algo bellman_ford step 652 current loss 0.211276, current_train_items 20896.
I0304 19:28:14.416769 22579586809984 run.py:483] Algo bellman_ford step 653 current loss 0.212458, current_train_items 20928.
I0304 19:28:14.448746 22579586809984 run.py:483] Algo bellman_ford step 654 current loss 0.204617, current_train_items 20960.
I0304 19:28:14.468056 22579586809984 run.py:483] Algo bellman_ford step 655 current loss 0.014911, current_train_items 20992.
I0304 19:28:14.483864 22579586809984 run.py:483] Algo bellman_ford step 656 current loss 0.093193, current_train_items 21024.
I0304 19:28:14.507999 22579586809984 run.py:483] Algo bellman_ford step 657 current loss 0.186683, current_train_items 21056.
I0304 19:28:14.538461 22579586809984 run.py:483] Algo bellman_ford step 658 current loss 0.172794, current_train_items 21088.
I0304 19:28:14.569553 22579586809984 run.py:483] Algo bellman_ford step 659 current loss 0.182943, current_train_items 21120.
I0304 19:28:14.588783 22579586809984 run.py:483] Algo bellman_ford step 660 current loss 0.019124, current_train_items 21152.
I0304 19:28:14.605004 22579586809984 run.py:483] Algo bellman_ford step 661 current loss 0.074932, current_train_items 21184.
I0304 19:28:14.628044 22579586809984 run.py:483] Algo bellman_ford step 662 current loss 0.087979, current_train_items 21216.
I0304 19:28:14.657740 22579586809984 run.py:483] Algo bellman_ford step 663 current loss 0.178970, current_train_items 21248.
I0304 19:28:14.692028 22579586809984 run.py:483] Algo bellman_ford step 664 current loss 0.265805, current_train_items 21280.
I0304 19:28:14.711121 22579586809984 run.py:483] Algo bellman_ford step 665 current loss 0.030666, current_train_items 21312.
I0304 19:28:14.727283 22579586809984 run.py:483] Algo bellman_ford step 666 current loss 0.085122, current_train_items 21344.
I0304 19:28:14.751760 22579586809984 run.py:483] Algo bellman_ford step 667 current loss 0.230301, current_train_items 21376.
I0304 19:28:14.781323 22579586809984 run.py:483] Algo bellman_ford step 668 current loss 0.226242, current_train_items 21408.
I0304 19:28:14.812515 22579586809984 run.py:483] Algo bellman_ford step 669 current loss 0.191075, current_train_items 21440.
I0304 19:28:14.831465 22579586809984 run.py:483] Algo bellman_ford step 670 current loss 0.012545, current_train_items 21472.
I0304 19:28:14.847539 22579586809984 run.py:483] Algo bellman_ford step 671 current loss 0.070760, current_train_items 21504.
I0304 19:28:14.871461 22579586809984 run.py:483] Algo bellman_ford step 672 current loss 0.312595, current_train_items 21536.
I0304 19:28:14.901000 22579586809984 run.py:483] Algo bellman_ford step 673 current loss 0.264162, current_train_items 21568.
I0304 19:28:14.934125 22579586809984 run.py:483] Algo bellman_ford step 674 current loss 0.248276, current_train_items 21600.
I0304 19:28:14.953573 22579586809984 run.py:483] Algo bellman_ford step 675 current loss 0.030635, current_train_items 21632.
I0304 19:28:14.969421 22579586809984 run.py:483] Algo bellman_ford step 676 current loss 0.082006, current_train_items 21664.
I0304 19:28:14.992534 22579586809984 run.py:483] Algo bellman_ford step 677 current loss 0.226449, current_train_items 21696.
I0304 19:28:15.022686 22579586809984 run.py:483] Algo bellman_ford step 678 current loss 0.276951, current_train_items 21728.
I0304 19:28:15.055450 22579586809984 run.py:483] Algo bellman_ford step 679 current loss 0.302579, current_train_items 21760.
I0304 19:28:15.074350 22579586809984 run.py:483] Algo bellman_ford step 680 current loss 0.032092, current_train_items 21792.
I0304 19:28:15.090637 22579586809984 run.py:483] Algo bellman_ford step 681 current loss 0.070626, current_train_items 21824.
I0304 19:28:15.114182 22579586809984 run.py:483] Algo bellman_ford step 682 current loss 0.220170, current_train_items 21856.
I0304 19:28:15.143230 22579586809984 run.py:483] Algo bellman_ford step 683 current loss 0.232783, current_train_items 21888.
I0304 19:28:15.176161 22579586809984 run.py:483] Algo bellman_ford step 684 current loss 0.249247, current_train_items 21920.
I0304 19:28:15.195098 22579586809984 run.py:483] Algo bellman_ford step 685 current loss 0.026587, current_train_items 21952.
I0304 19:28:15.211192 22579586809984 run.py:483] Algo bellman_ford step 686 current loss 0.109152, current_train_items 21984.
I0304 19:28:15.234740 22579586809984 run.py:483] Algo bellman_ford step 687 current loss 0.250992, current_train_items 22016.
I0304 19:28:15.262733 22579586809984 run.py:483] Algo bellman_ford step 688 current loss 0.222126, current_train_items 22048.
I0304 19:28:15.296627 22579586809984 run.py:483] Algo bellman_ford step 689 current loss 0.285008, current_train_items 22080.
I0304 19:28:15.315753 22579586809984 run.py:483] Algo bellman_ford step 690 current loss 0.034017, current_train_items 22112.
I0304 19:28:15.332492 22579586809984 run.py:483] Algo bellman_ford step 691 current loss 0.191829, current_train_items 22144.
I0304 19:28:15.356925 22579586809984 run.py:483] Algo bellman_ford step 692 current loss 0.268785, current_train_items 22176.
I0304 19:28:15.385921 22579586809984 run.py:483] Algo bellman_ford step 693 current loss 0.313588, current_train_items 22208.
I0304 19:28:15.419496 22579586809984 run.py:483] Algo bellman_ford step 694 current loss 0.455099, current_train_items 22240.
I0304 19:28:15.438252 22579586809984 run.py:483] Algo bellman_ford step 695 current loss 0.020775, current_train_items 22272.
I0304 19:28:15.454189 22579586809984 run.py:483] Algo bellman_ford step 696 current loss 0.059782, current_train_items 22304.
I0304 19:28:15.476362 22579586809984 run.py:483] Algo bellman_ford step 697 current loss 0.191165, current_train_items 22336.
I0304 19:28:15.505938 22579586809984 run.py:483] Algo bellman_ford step 698 current loss 0.197023, current_train_items 22368.
I0304 19:28:15.539142 22579586809984 run.py:483] Algo bellman_ford step 699 current loss 0.231107, current_train_items 22400.
I0304 19:28:15.558468 22579586809984 run.py:483] Algo bellman_ford step 700 current loss 0.014268, current_train_items 22432.
I0304 19:28:15.566048 22579586809984 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.9580078125, 'score': 0.9580078125, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0304 19:28:15.566154 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.955, current avg val score is 0.958, val scores are: bellman_ford: 0.958
I0304 19:28:15.595194 22579586809984 run.py:483] Algo bellman_ford step 701 current loss 0.038437, current_train_items 22464.
I0304 19:28:15.618801 22579586809984 run.py:483] Algo bellman_ford step 702 current loss 0.145068, current_train_items 22496.
I0304 19:28:15.647273 22579586809984 run.py:483] Algo bellman_ford step 703 current loss 0.186893, current_train_items 22528.
I0304 19:28:15.678337 22579586809984 run.py:483] Algo bellman_ford step 704 current loss 0.209527, current_train_items 22560.
I0304 19:28:15.697352 22579586809984 run.py:483] Algo bellman_ford step 705 current loss 0.022173, current_train_items 22592.
I0304 19:28:15.713277 22579586809984 run.py:483] Algo bellman_ford step 706 current loss 0.049544, current_train_items 22624.
I0304 19:28:15.736915 22579586809984 run.py:483] Algo bellman_ford step 707 current loss 0.129105, current_train_items 22656.
I0304 19:28:15.766230 22579586809984 run.py:483] Algo bellman_ford step 708 current loss 0.164270, current_train_items 22688.
I0304 19:28:15.798821 22579586809984 run.py:483] Algo bellman_ford step 709 current loss 0.161202, current_train_items 22720.
I0304 19:28:15.817476 22579586809984 run.py:483] Algo bellman_ford step 710 current loss 0.010308, current_train_items 22752.
I0304 19:28:15.833907 22579586809984 run.py:483] Algo bellman_ford step 711 current loss 0.059277, current_train_items 22784.
I0304 19:28:15.857812 22579586809984 run.py:483] Algo bellman_ford step 712 current loss 0.139363, current_train_items 22816.
I0304 19:28:15.888149 22579586809984 run.py:483] Algo bellman_ford step 713 current loss 0.143496, current_train_items 22848.
I0304 19:28:15.918102 22579586809984 run.py:483] Algo bellman_ford step 714 current loss 0.148814, current_train_items 22880.
I0304 19:28:15.936672 22579586809984 run.py:483] Algo bellman_ford step 715 current loss 0.010343, current_train_items 22912.
I0304 19:28:15.953193 22579586809984 run.py:483] Algo bellman_ford step 716 current loss 0.105066, current_train_items 22944.
I0304 19:28:15.977757 22579586809984 run.py:483] Algo bellman_ford step 717 current loss 0.162830, current_train_items 22976.
I0304 19:28:16.006404 22579586809984 run.py:483] Algo bellman_ford step 718 current loss 0.093559, current_train_items 23008.
I0304 19:28:16.038470 22579586809984 run.py:483] Algo bellman_ford step 719 current loss 0.162128, current_train_items 23040.
I0304 19:28:16.057289 22579586809984 run.py:483] Algo bellman_ford step 720 current loss 0.028438, current_train_items 23072.
I0304 19:28:16.073834 22579586809984 run.py:483] Algo bellman_ford step 721 current loss 0.086446, current_train_items 23104.
I0304 19:28:16.097026 22579586809984 run.py:483] Algo bellman_ford step 722 current loss 0.132833, current_train_items 23136.
I0304 19:28:16.125987 22579586809984 run.py:483] Algo bellman_ford step 723 current loss 0.198861, current_train_items 23168.
I0304 19:28:16.156949 22579586809984 run.py:483] Algo bellman_ford step 724 current loss 0.226141, current_train_items 23200.
I0304 19:28:16.175333 22579586809984 run.py:483] Algo bellman_ford step 725 current loss 0.023454, current_train_items 23232.
I0304 19:28:16.191991 22579586809984 run.py:483] Algo bellman_ford step 726 current loss 0.088234, current_train_items 23264.
I0304 19:28:16.216445 22579586809984 run.py:483] Algo bellman_ford step 727 current loss 0.229968, current_train_items 23296.
I0304 19:28:16.245527 22579586809984 run.py:483] Algo bellman_ford step 728 current loss 0.203190, current_train_items 23328.
I0304 19:28:16.277208 22579586809984 run.py:483] Algo bellman_ford step 729 current loss 0.195158, current_train_items 23360.
I0304 19:28:16.295574 22579586809984 run.py:483] Algo bellman_ford step 730 current loss 0.014645, current_train_items 23392.
I0304 19:28:16.311853 22579586809984 run.py:483] Algo bellman_ford step 731 current loss 0.114453, current_train_items 23424.
I0304 19:28:16.335257 22579586809984 run.py:483] Algo bellman_ford step 732 current loss 0.205714, current_train_items 23456.
I0304 19:28:16.365125 22579586809984 run.py:483] Algo bellman_ford step 733 current loss 0.313526, current_train_items 23488.
I0304 19:28:16.397469 22579586809984 run.py:483] Algo bellman_ford step 734 current loss 0.269622, current_train_items 23520.
I0304 19:28:16.415997 22579586809984 run.py:483] Algo bellman_ford step 735 current loss 0.022747, current_train_items 23552.
I0304 19:28:16.432421 22579586809984 run.py:483] Algo bellman_ford step 736 current loss 0.077510, current_train_items 23584.
I0304 19:28:16.455788 22579586809984 run.py:483] Algo bellman_ford step 737 current loss 0.161089, current_train_items 23616.
I0304 19:28:16.485431 22579586809984 run.py:483] Algo bellman_ford step 738 current loss 0.172460, current_train_items 23648.
I0304 19:28:16.515187 22579586809984 run.py:483] Algo bellman_ford step 739 current loss 0.151314, current_train_items 23680.
I0304 19:28:16.533718 22579586809984 run.py:483] Algo bellman_ford step 740 current loss 0.015990, current_train_items 23712.
I0304 19:28:16.549998 22579586809984 run.py:483] Algo bellman_ford step 741 current loss 0.120432, current_train_items 23744.
I0304 19:28:16.573293 22579586809984 run.py:483] Algo bellman_ford step 742 current loss 0.201799, current_train_items 23776.
I0304 19:28:16.601659 22579586809984 run.py:483] Algo bellman_ford step 743 current loss 0.137341, current_train_items 23808.
I0304 19:28:16.635229 22579586809984 run.py:483] Algo bellman_ford step 744 current loss 0.240539, current_train_items 23840.
I0304 19:28:16.653807 22579586809984 run.py:483] Algo bellman_ford step 745 current loss 0.021575, current_train_items 23872.
I0304 19:28:16.669911 22579586809984 run.py:483] Algo bellman_ford step 746 current loss 0.095494, current_train_items 23904.
I0304 19:28:16.693309 22579586809984 run.py:483] Algo bellman_ford step 747 current loss 0.186159, current_train_items 23936.
I0304 19:28:16.723537 22579586809984 run.py:483] Algo bellman_ford step 748 current loss 0.162720, current_train_items 23968.
I0304 19:28:16.755251 22579586809984 run.py:483] Algo bellman_ford step 749 current loss 0.264366, current_train_items 24000.
I0304 19:28:16.773486 22579586809984 run.py:483] Algo bellman_ford step 750 current loss 0.028433, current_train_items 24032.
I0304 19:28:16.781822 22579586809984 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.935546875, 'score': 0.935546875, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0304 19:28:16.781927 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.936, val scores are: bellman_ford: 0.936
I0304 19:28:16.799156 22579586809984 run.py:483] Algo bellman_ford step 751 current loss 0.108105, current_train_items 24064.
I0304 19:28:16.822570 22579586809984 run.py:483] Algo bellman_ford step 752 current loss 0.145171, current_train_items 24096.
I0304 19:28:16.851871 22579586809984 run.py:483] Algo bellman_ford step 753 current loss 0.156583, current_train_items 24128.
I0304 19:28:16.885082 22579586809984 run.py:483] Algo bellman_ford step 754 current loss 0.176154, current_train_items 24160.
I0304 19:28:16.904209 22579586809984 run.py:483] Algo bellman_ford step 755 current loss 0.093149, current_train_items 24192.
I0304 19:28:16.919847 22579586809984 run.py:483] Algo bellman_ford step 756 current loss 0.083308, current_train_items 24224.
I0304 19:28:16.944331 22579586809984 run.py:483] Algo bellman_ford step 757 current loss 0.170946, current_train_items 24256.
I0304 19:28:16.973886 22579586809984 run.py:483] Algo bellman_ford step 758 current loss 0.183645, current_train_items 24288.
I0304 19:28:17.005512 22579586809984 run.py:483] Algo bellman_ford step 759 current loss 0.240278, current_train_items 24320.
I0304 19:28:17.024474 22579586809984 run.py:483] Algo bellman_ford step 760 current loss 0.018074, current_train_items 24352.
I0304 19:28:17.040651 22579586809984 run.py:483] Algo bellman_ford step 761 current loss 0.102430, current_train_items 24384.
I0304 19:28:17.063928 22579586809984 run.py:483] Algo bellman_ford step 762 current loss 0.211839, current_train_items 24416.
I0304 19:28:17.092863 22579586809984 run.py:483] Algo bellman_ford step 763 current loss 0.136948, current_train_items 24448.
I0304 19:28:17.125198 22579586809984 run.py:483] Algo bellman_ford step 764 current loss 0.165254, current_train_items 24480.
I0304 19:28:17.144253 22579586809984 run.py:483] Algo bellman_ford step 765 current loss 0.027054, current_train_items 24512.
I0304 19:28:17.160799 22579586809984 run.py:483] Algo bellman_ford step 766 current loss 0.130157, current_train_items 24544.
I0304 19:28:17.184643 22579586809984 run.py:483] Algo bellman_ford step 767 current loss 0.188085, current_train_items 24576.
I0304 19:28:17.214442 22579586809984 run.py:483] Algo bellman_ford step 768 current loss 0.165671, current_train_items 24608.
I0304 19:28:17.247400 22579586809984 run.py:483] Algo bellman_ford step 769 current loss 0.183219, current_train_items 24640.
I0304 19:28:17.266735 22579586809984 run.py:483] Algo bellman_ford step 770 current loss 0.011691, current_train_items 24672.
I0304 19:28:17.283180 22579586809984 run.py:483] Algo bellman_ford step 771 current loss 0.143450, current_train_items 24704.
I0304 19:28:17.306562 22579586809984 run.py:483] Algo bellman_ford step 772 current loss 0.212840, current_train_items 24736.
I0304 19:28:17.335784 22579586809984 run.py:483] Algo bellman_ford step 773 current loss 0.122337, current_train_items 24768.
I0304 19:28:17.368655 22579586809984 run.py:483] Algo bellman_ford step 774 current loss 0.220890, current_train_items 24800.
I0304 19:28:17.388084 22579586809984 run.py:483] Algo bellman_ford step 775 current loss 0.021408, current_train_items 24832.
I0304 19:28:17.404997 22579586809984 run.py:483] Algo bellman_ford step 776 current loss 0.053544, current_train_items 24864.
I0304 19:28:17.428858 22579586809984 run.py:483] Algo bellman_ford step 777 current loss 0.122662, current_train_items 24896.
I0304 19:28:17.457497 22579586809984 run.py:483] Algo bellman_ford step 778 current loss 0.146612, current_train_items 24928.
I0304 19:28:17.488827 22579586809984 run.py:483] Algo bellman_ford step 779 current loss 0.141681, current_train_items 24960.
I0304 19:28:17.507856 22579586809984 run.py:483] Algo bellman_ford step 780 current loss 0.019948, current_train_items 24992.
I0304 19:28:17.524200 22579586809984 run.py:483] Algo bellman_ford step 781 current loss 0.069924, current_train_items 25024.
I0304 19:28:17.547210 22579586809984 run.py:483] Algo bellman_ford step 782 current loss 0.051028, current_train_items 25056.
I0304 19:28:17.576571 22579586809984 run.py:483] Algo bellman_ford step 783 current loss 0.177213, current_train_items 25088.
I0304 19:28:17.609305 22579586809984 run.py:483] Algo bellman_ford step 784 current loss 0.179397, current_train_items 25120.
I0304 19:28:17.628221 22579586809984 run.py:483] Algo bellman_ford step 785 current loss 0.019581, current_train_items 25152.
I0304 19:28:17.644753 22579586809984 run.py:483] Algo bellman_ford step 786 current loss 0.045752, current_train_items 25184.
I0304 19:28:17.667823 22579586809984 run.py:483] Algo bellman_ford step 787 current loss 0.085296, current_train_items 25216.
I0304 19:28:17.697857 22579586809984 run.py:483] Algo bellman_ford step 788 current loss 0.160727, current_train_items 25248.
I0304 19:28:17.727501 22579586809984 run.py:483] Algo bellman_ford step 789 current loss 0.198430, current_train_items 25280.
I0304 19:28:17.746724 22579586809984 run.py:483] Algo bellman_ford step 790 current loss 0.015531, current_train_items 25312.
I0304 19:28:17.763313 22579586809984 run.py:483] Algo bellman_ford step 791 current loss 0.082062, current_train_items 25344.
I0304 19:28:17.785342 22579586809984 run.py:483] Algo bellman_ford step 792 current loss 0.080131, current_train_items 25376.
I0304 19:28:17.814957 22579586809984 run.py:483] Algo bellman_ford step 793 current loss 0.146695, current_train_items 25408.
I0304 19:28:17.845753 22579586809984 run.py:483] Algo bellman_ford step 794 current loss 0.172508, current_train_items 25440.
I0304 19:28:17.864543 22579586809984 run.py:483] Algo bellman_ford step 795 current loss 0.023847, current_train_items 25472.
I0304 19:28:17.880400 22579586809984 run.py:483] Algo bellman_ford step 796 current loss 0.044944, current_train_items 25504.
I0304 19:28:17.904352 22579586809984 run.py:483] Algo bellman_ford step 797 current loss 0.094015, current_train_items 25536.
I0304 19:28:17.933305 22579586809984 run.py:483] Algo bellman_ford step 798 current loss 0.110895, current_train_items 25568.
I0304 19:28:17.964426 22579586809984 run.py:483] Algo bellman_ford step 799 current loss 0.133558, current_train_items 25600.
I0304 19:28:17.983249 22579586809984 run.py:483] Algo bellman_ford step 800 current loss 0.007441, current_train_items 25632.
I0304 19:28:17.991290 22579586809984 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0304 19:28:17.991399 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.958, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:28:18.021440 22579586809984 run.py:483] Algo bellman_ford step 801 current loss 0.055770, current_train_items 25664.
I0304 19:28:18.046047 22579586809984 run.py:483] Algo bellman_ford step 802 current loss 0.117273, current_train_items 25696.
I0304 19:28:18.076140 22579586809984 run.py:483] Algo bellman_ford step 803 current loss 0.161750, current_train_items 25728.
I0304 19:28:18.107785 22579586809984 run.py:483] Algo bellman_ford step 804 current loss 0.169734, current_train_items 25760.
I0304 19:28:18.127292 22579586809984 run.py:483] Algo bellman_ford step 805 current loss 0.010501, current_train_items 25792.
I0304 19:28:18.143135 22579586809984 run.py:483] Algo bellman_ford step 806 current loss 0.043601, current_train_items 25824.
I0304 19:28:18.167397 22579586809984 run.py:483] Algo bellman_ford step 807 current loss 0.099857, current_train_items 25856.
I0304 19:28:18.196814 22579586809984 run.py:483] Algo bellman_ford step 808 current loss 0.148331, current_train_items 25888.
I0304 19:28:18.227651 22579586809984 run.py:483] Algo bellman_ford step 809 current loss 0.194128, current_train_items 25920.
I0304 19:28:18.246461 22579586809984 run.py:483] Algo bellman_ford step 810 current loss 0.024057, current_train_items 25952.
I0304 19:28:18.263258 22579586809984 run.py:483] Algo bellman_ford step 811 current loss 0.090769, current_train_items 25984.
I0304 19:28:18.286024 22579586809984 run.py:483] Algo bellman_ford step 812 current loss 0.086565, current_train_items 26016.
I0304 19:28:18.314826 22579586809984 run.py:483] Algo bellman_ford step 813 current loss 0.120308, current_train_items 26048.
I0304 19:28:18.346307 22579586809984 run.py:483] Algo bellman_ford step 814 current loss 0.178595, current_train_items 26080.
I0304 19:28:18.365202 22579586809984 run.py:483] Algo bellman_ford step 815 current loss 0.041363, current_train_items 26112.
I0304 19:28:18.381831 22579586809984 run.py:483] Algo bellman_ford step 816 current loss 0.090941, current_train_items 26144.
I0304 19:28:18.405316 22579586809984 run.py:483] Algo bellman_ford step 817 current loss 0.063700, current_train_items 26176.
I0304 19:28:18.435761 22579586809984 run.py:483] Algo bellman_ford step 818 current loss 0.179133, current_train_items 26208.
I0304 19:28:18.469379 22579586809984 run.py:483] Algo bellman_ford step 819 current loss 0.195300, current_train_items 26240.
I0304 19:28:18.488081 22579586809984 run.py:483] Algo bellman_ford step 820 current loss 0.023823, current_train_items 26272.
I0304 19:28:18.503922 22579586809984 run.py:483] Algo bellman_ford step 821 current loss 0.083793, current_train_items 26304.
I0304 19:28:18.527053 22579586809984 run.py:483] Algo bellman_ford step 822 current loss 0.130560, current_train_items 26336.
I0304 19:28:18.556493 22579586809984 run.py:483] Algo bellman_ford step 823 current loss 0.152873, current_train_items 26368.
I0304 19:28:18.587096 22579586809984 run.py:483] Algo bellman_ford step 824 current loss 0.326518, current_train_items 26400.
I0304 19:28:18.605715 22579586809984 run.py:483] Algo bellman_ford step 825 current loss 0.021803, current_train_items 26432.
I0304 19:28:18.622355 22579586809984 run.py:483] Algo bellman_ford step 826 current loss 0.057362, current_train_items 26464.
I0304 19:28:18.646817 22579586809984 run.py:483] Algo bellman_ford step 827 current loss 0.109175, current_train_items 26496.
I0304 19:28:18.676459 22579586809984 run.py:483] Algo bellman_ford step 828 current loss 0.135109, current_train_items 26528.
I0304 19:28:18.708030 22579586809984 run.py:483] Algo bellman_ford step 829 current loss 0.142661, current_train_items 26560.
I0304 19:28:18.726613 22579586809984 run.py:483] Algo bellman_ford step 830 current loss 0.010421, current_train_items 26592.
I0304 19:28:18.742877 22579586809984 run.py:483] Algo bellman_ford step 831 current loss 0.066010, current_train_items 26624.
I0304 19:28:18.766460 22579586809984 run.py:483] Algo bellman_ford step 832 current loss 0.141011, current_train_items 26656.
I0304 19:28:18.795701 22579586809984 run.py:483] Algo bellman_ford step 833 current loss 0.158196, current_train_items 26688.
I0304 19:28:18.826098 22579586809984 run.py:483] Algo bellman_ford step 834 current loss 0.117738, current_train_items 26720.
I0304 19:28:18.844642 22579586809984 run.py:483] Algo bellman_ford step 835 current loss 0.011028, current_train_items 26752.
I0304 19:28:18.860673 22579586809984 run.py:483] Algo bellman_ford step 836 current loss 0.047720, current_train_items 26784.
I0304 19:28:18.884346 22579586809984 run.py:483] Algo bellman_ford step 837 current loss 0.216622, current_train_items 26816.
I0304 19:28:18.913805 22579586809984 run.py:483] Algo bellman_ford step 838 current loss 0.134903, current_train_items 26848.
I0304 19:28:18.946423 22579586809984 run.py:483] Algo bellman_ford step 839 current loss 0.227685, current_train_items 26880.
I0304 19:28:18.965121 22579586809984 run.py:483] Algo bellman_ford step 840 current loss 0.035669, current_train_items 26912.
I0304 19:28:18.981824 22579586809984 run.py:483] Algo bellman_ford step 841 current loss 0.060850, current_train_items 26944.
I0304 19:28:19.005387 22579586809984 run.py:483] Algo bellman_ford step 842 current loss 0.165466, current_train_items 26976.
I0304 19:28:19.033545 22579586809984 run.py:483] Algo bellman_ford step 843 current loss 0.154079, current_train_items 27008.
I0304 19:28:19.064098 22579586809984 run.py:483] Algo bellman_ford step 844 current loss 0.147477, current_train_items 27040.
I0304 19:28:19.082793 22579586809984 run.py:483] Algo bellman_ford step 845 current loss 0.040094, current_train_items 27072.
I0304 19:28:19.099188 22579586809984 run.py:483] Algo bellman_ford step 846 current loss 0.058299, current_train_items 27104.
I0304 19:28:19.123949 22579586809984 run.py:483] Algo bellman_ford step 847 current loss 0.125500, current_train_items 27136.
I0304 19:28:19.153048 22579586809984 run.py:483] Algo bellman_ford step 848 current loss 0.169700, current_train_items 27168.
I0304 19:28:19.185300 22579586809984 run.py:483] Algo bellman_ford step 849 current loss 0.179038, current_train_items 27200.
I0304 19:28:19.204101 22579586809984 run.py:483] Algo bellman_ford step 850 current loss 0.009884, current_train_items 27232.
I0304 19:28:19.211889 22579586809984 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.9599609375, 'score': 0.9599609375, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0304 19:28:19.211995 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.971, current avg val score is 0.960, val scores are: bellman_ford: 0.960
I0304 19:28:19.228470 22579586809984 run.py:483] Algo bellman_ford step 851 current loss 0.030248, current_train_items 27264.
I0304 19:28:19.252410 22579586809984 run.py:483] Algo bellman_ford step 852 current loss 0.161558, current_train_items 27296.
I0304 19:28:19.283706 22579586809984 run.py:483] Algo bellman_ford step 853 current loss 0.262961, current_train_items 27328.
I0304 19:28:19.314842 22579586809984 run.py:483] Algo bellman_ford step 854 current loss 0.217800, current_train_items 27360.
I0304 19:28:19.333690 22579586809984 run.py:483] Algo bellman_ford step 855 current loss 0.047468, current_train_items 27392.
I0304 19:28:19.349863 22579586809984 run.py:483] Algo bellman_ford step 856 current loss 0.045563, current_train_items 27424.
I0304 19:28:19.372706 22579586809984 run.py:483] Algo bellman_ford step 857 current loss 0.211697, current_train_items 27456.
I0304 19:28:19.403107 22579586809984 run.py:483] Algo bellman_ford step 858 current loss 0.270031, current_train_items 27488.
I0304 19:28:19.433990 22579586809984 run.py:483] Algo bellman_ford step 859 current loss 0.224658, current_train_items 27520.
I0304 19:28:19.453122 22579586809984 run.py:483] Algo bellman_ford step 860 current loss 0.012942, current_train_items 27552.
I0304 19:28:19.469663 22579586809984 run.py:483] Algo bellman_ford step 861 current loss 0.075764, current_train_items 27584.
I0304 19:28:19.492890 22579586809984 run.py:483] Algo bellman_ford step 862 current loss 0.171819, current_train_items 27616.
I0304 19:28:19.522107 22579586809984 run.py:483] Algo bellman_ford step 863 current loss 0.262912, current_train_items 27648.
I0304 19:28:19.553020 22579586809984 run.py:483] Algo bellman_ford step 864 current loss 0.297840, current_train_items 27680.
I0304 19:28:19.571657 22579586809984 run.py:483] Algo bellman_ford step 865 current loss 0.018761, current_train_items 27712.
I0304 19:28:19.587541 22579586809984 run.py:483] Algo bellman_ford step 866 current loss 0.066031, current_train_items 27744.
I0304 19:28:19.611664 22579586809984 run.py:483] Algo bellman_ford step 867 current loss 0.147163, current_train_items 27776.
I0304 19:28:19.641244 22579586809984 run.py:483] Algo bellman_ford step 868 current loss 0.096363, current_train_items 27808.
I0304 19:28:19.674284 22579586809984 run.py:483] Algo bellman_ford step 869 current loss 0.167050, current_train_items 27840.
I0304 19:28:19.693070 22579586809984 run.py:483] Algo bellman_ford step 870 current loss 0.052414, current_train_items 27872.
I0304 19:28:19.709578 22579586809984 run.py:483] Algo bellman_ford step 871 current loss 0.053561, current_train_items 27904.
I0304 19:28:19.731819 22579586809984 run.py:483] Algo bellman_ford step 872 current loss 0.122226, current_train_items 27936.
I0304 19:28:19.761774 22579586809984 run.py:483] Algo bellman_ford step 873 current loss 0.181451, current_train_items 27968.
I0304 19:28:19.792593 22579586809984 run.py:483] Algo bellman_ford step 874 current loss 0.192425, current_train_items 28000.
I0304 19:28:19.811355 22579586809984 run.py:483] Algo bellman_ford step 875 current loss 0.009461, current_train_items 28032.
I0304 19:28:19.827862 22579586809984 run.py:483] Algo bellman_ford step 876 current loss 0.108733, current_train_items 28064.
I0304 19:28:19.852883 22579586809984 run.py:483] Algo bellman_ford step 877 current loss 0.313058, current_train_items 28096.
I0304 19:28:19.881386 22579586809984 run.py:483] Algo bellman_ford step 878 current loss 0.178664, current_train_items 28128.
I0304 19:28:19.913866 22579586809984 run.py:483] Algo bellman_ford step 879 current loss 0.159104, current_train_items 28160.
I0304 19:28:19.932222 22579586809984 run.py:483] Algo bellman_ford step 880 current loss 0.009314, current_train_items 28192.
I0304 19:28:19.948138 22579586809984 run.py:483] Algo bellman_ford step 881 current loss 0.026716, current_train_items 28224.
I0304 19:28:19.971100 22579586809984 run.py:483] Algo bellman_ford step 882 current loss 0.294288, current_train_items 28256.
I0304 19:28:20.000669 22579586809984 run.py:483] Algo bellman_ford step 883 current loss 0.555056, current_train_items 28288.
I0304 19:28:20.033739 22579586809984 run.py:483] Algo bellman_ford step 884 current loss 0.629119, current_train_items 28320.
I0304 19:28:20.053182 22579586809984 run.py:483] Algo bellman_ford step 885 current loss 0.017139, current_train_items 28352.
I0304 19:28:20.069368 22579586809984 run.py:483] Algo bellman_ford step 886 current loss 0.075383, current_train_items 28384.
I0304 19:28:20.092774 22579586809984 run.py:483] Algo bellman_ford step 887 current loss 0.116445, current_train_items 28416.
I0304 19:28:20.121761 22579586809984 run.py:483] Algo bellman_ford step 888 current loss 0.136521, current_train_items 28448.
I0304 19:28:20.154760 22579586809984 run.py:483] Algo bellman_ford step 889 current loss 0.250206, current_train_items 28480.
I0304 19:28:20.173832 22579586809984 run.py:483] Algo bellman_ford step 890 current loss 0.033060, current_train_items 28512.
I0304 19:28:20.190272 22579586809984 run.py:483] Algo bellman_ford step 891 current loss 0.074383, current_train_items 28544.
I0304 19:28:20.213766 22579586809984 run.py:483] Algo bellman_ford step 892 current loss 0.133474, current_train_items 28576.
I0304 19:28:20.244266 22579586809984 run.py:483] Algo bellman_ford step 893 current loss 0.190611, current_train_items 28608.
I0304 19:28:20.275649 22579586809984 run.py:483] Algo bellman_ford step 894 current loss 0.135451, current_train_items 28640.
I0304 19:28:20.294274 22579586809984 run.py:483] Algo bellman_ford step 895 current loss 0.051250, current_train_items 28672.
I0304 19:28:20.310353 22579586809984 run.py:483] Algo bellman_ford step 896 current loss 0.034260, current_train_items 28704.
I0304 19:28:20.334187 22579586809984 run.py:483] Algo bellman_ford step 897 current loss 0.148370, current_train_items 28736.
I0304 19:28:20.364201 22579586809984 run.py:483] Algo bellman_ford step 898 current loss 0.125623, current_train_items 28768.
I0304 19:28:20.397716 22579586809984 run.py:483] Algo bellman_ford step 899 current loss 0.206272, current_train_items 28800.
I0304 19:28:20.416667 22579586809984 run.py:483] Algo bellman_ford step 900 current loss 0.014984, current_train_items 28832.
I0304 19:28:20.424550 22579586809984 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0304 19:28:20.424655 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.971, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:28:20.455401 22579586809984 run.py:483] Algo bellman_ford step 901 current loss 0.078290, current_train_items 28864.
I0304 19:28:20.479917 22579586809984 run.py:483] Algo bellman_ford step 902 current loss 0.103362, current_train_items 28896.
I0304 19:28:20.510783 22579586809984 run.py:483] Algo bellman_ford step 903 current loss 0.102623, current_train_items 28928.
I0304 19:28:20.543201 22579586809984 run.py:483] Algo bellman_ford step 904 current loss 0.149686, current_train_items 28960.
I0304 19:28:20.562509 22579586809984 run.py:483] Algo bellman_ford step 905 current loss 0.008858, current_train_items 28992.
I0304 19:28:20.579011 22579586809984 run.py:483] Algo bellman_ford step 906 current loss 0.067977, current_train_items 29024.
I0304 19:28:20.602719 22579586809984 run.py:483] Algo bellman_ford step 907 current loss 0.074630, current_train_items 29056.
I0304 19:28:20.633122 22579586809984 run.py:483] Algo bellman_ford step 908 current loss 0.161860, current_train_items 29088.
I0304 19:28:20.665880 22579586809984 run.py:483] Algo bellman_ford step 909 current loss 0.141430, current_train_items 29120.
I0304 19:28:20.684785 22579586809984 run.py:483] Algo bellman_ford step 910 current loss 0.029876, current_train_items 29152.
I0304 19:28:20.701743 22579586809984 run.py:483] Algo bellman_ford step 911 current loss 0.068410, current_train_items 29184.
I0304 19:28:20.726050 22579586809984 run.py:483] Algo bellman_ford step 912 current loss 0.116433, current_train_items 29216.
I0304 19:28:20.753872 22579586809984 run.py:483] Algo bellman_ford step 913 current loss 0.065785, current_train_items 29248.
I0304 19:28:20.786380 22579586809984 run.py:483] Algo bellman_ford step 914 current loss 0.222873, current_train_items 29280.
I0304 19:28:20.804596 22579586809984 run.py:483] Algo bellman_ford step 915 current loss 0.008353, current_train_items 29312.
I0304 19:28:20.821056 22579586809984 run.py:483] Algo bellman_ford step 916 current loss 0.045283, current_train_items 29344.
I0304 19:28:20.845225 22579586809984 run.py:483] Algo bellman_ford step 917 current loss 0.119108, current_train_items 29376.
I0304 19:28:20.874846 22579586809984 run.py:483] Algo bellman_ford step 918 current loss 0.112417, current_train_items 29408.
I0304 19:28:20.907857 22579586809984 run.py:483] Algo bellman_ford step 919 current loss 0.165286, current_train_items 29440.
I0304 19:28:20.926695 22579586809984 run.py:483] Algo bellman_ford step 920 current loss 0.021641, current_train_items 29472.
I0304 19:28:20.943174 22579586809984 run.py:483] Algo bellman_ford step 921 current loss 0.088274, current_train_items 29504.
I0304 19:28:20.967166 22579586809984 run.py:483] Algo bellman_ford step 922 current loss 0.101062, current_train_items 29536.
I0304 19:28:20.996452 22579586809984 run.py:483] Algo bellman_ford step 923 current loss 0.101963, current_train_items 29568.
I0304 19:28:21.028227 22579586809984 run.py:483] Algo bellman_ford step 924 current loss 0.190841, current_train_items 29600.
I0304 19:28:21.046665 22579586809984 run.py:483] Algo bellman_ford step 925 current loss 0.012899, current_train_items 29632.
I0304 19:28:21.062589 22579586809984 run.py:483] Algo bellman_ford step 926 current loss 0.055811, current_train_items 29664.
I0304 19:28:21.086583 22579586809984 run.py:483] Algo bellman_ford step 927 current loss 0.107332, current_train_items 29696.
I0304 19:28:21.117016 22579586809984 run.py:483] Algo bellman_ford step 928 current loss 0.176216, current_train_items 29728.
I0304 19:28:21.147485 22579586809984 run.py:483] Algo bellman_ford step 929 current loss 0.163264, current_train_items 29760.
I0304 19:28:21.166017 22579586809984 run.py:483] Algo bellman_ford step 930 current loss 0.021854, current_train_items 29792.
I0304 19:28:21.182163 22579586809984 run.py:483] Algo bellman_ford step 931 current loss 0.041096, current_train_items 29824.
I0304 19:28:21.206238 22579586809984 run.py:483] Algo bellman_ford step 932 current loss 0.134541, current_train_items 29856.
I0304 19:28:21.234792 22579586809984 run.py:483] Algo bellman_ford step 933 current loss 0.097563, current_train_items 29888.
I0304 19:28:21.267202 22579586809984 run.py:483] Algo bellman_ford step 934 current loss 0.180786, current_train_items 29920.
I0304 19:28:21.285523 22579586809984 run.py:483] Algo bellman_ford step 935 current loss 0.016826, current_train_items 29952.
I0304 19:28:21.301728 22579586809984 run.py:483] Algo bellman_ford step 936 current loss 0.047162, current_train_items 29984.
I0304 19:28:21.324540 22579586809984 run.py:483] Algo bellman_ford step 937 current loss 0.074474, current_train_items 30016.
I0304 19:28:21.353676 22579586809984 run.py:483] Algo bellman_ford step 938 current loss 0.127388, current_train_items 30048.
I0304 19:28:21.385488 22579586809984 run.py:483] Algo bellman_ford step 939 current loss 0.179613, current_train_items 30080.
I0304 19:28:21.403859 22579586809984 run.py:483] Algo bellman_ford step 940 current loss 0.018997, current_train_items 30112.
I0304 19:28:21.420116 22579586809984 run.py:483] Algo bellman_ford step 941 current loss 0.070895, current_train_items 30144.
I0304 19:28:21.444034 22579586809984 run.py:483] Algo bellman_ford step 942 current loss 0.206745, current_train_items 30176.
I0304 19:28:21.473761 22579586809984 run.py:483] Algo bellman_ford step 943 current loss 0.098909, current_train_items 30208.
I0304 19:28:21.506179 22579586809984 run.py:483] Algo bellman_ford step 944 current loss 0.150992, current_train_items 30240.
I0304 19:28:21.524510 22579586809984 run.py:483] Algo bellman_ford step 945 current loss 0.013629, current_train_items 30272.
I0304 19:28:21.540208 22579586809984 run.py:483] Algo bellman_ford step 946 current loss 0.084706, current_train_items 30304.
I0304 19:28:21.564827 22579586809984 run.py:483] Algo bellman_ford step 947 current loss 0.201898, current_train_items 30336.
I0304 19:28:21.593749 22579586809984 run.py:483] Algo bellman_ford step 948 current loss 0.098960, current_train_items 30368.
I0304 19:28:21.625036 22579586809984 run.py:483] Algo bellman_ford step 949 current loss 0.130562, current_train_items 30400.
I0304 19:28:21.643595 22579586809984 run.py:483] Algo bellman_ford step 950 current loss 0.039312, current_train_items 30432.
I0304 19:28:21.651398 22579586809984 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0304 19:28:21.651504 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.972, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:28:21.668364 22579586809984 run.py:483] Algo bellman_ford step 951 current loss 0.043949, current_train_items 30464.
I0304 19:28:21.692182 22579586809984 run.py:483] Algo bellman_ford step 952 current loss 0.082253, current_train_items 30496.
I0304 19:28:21.722157 22579586809984 run.py:483] Algo bellman_ford step 953 current loss 0.142598, current_train_items 30528.
I0304 19:28:21.751677 22579586809984 run.py:483] Algo bellman_ford step 954 current loss 0.103223, current_train_items 30560.
I0304 19:28:21.770780 22579586809984 run.py:483] Algo bellman_ford step 955 current loss 0.022556, current_train_items 30592.
I0304 19:28:21.786832 22579586809984 run.py:483] Algo bellman_ford step 956 current loss 0.032277, current_train_items 30624.
I0304 19:28:21.810749 22579586809984 run.py:483] Algo bellman_ford step 957 current loss 0.116665, current_train_items 30656.
I0304 19:28:21.839234 22579586809984 run.py:483] Algo bellman_ford step 958 current loss 0.145922, current_train_items 30688.
I0304 19:28:21.871245 22579586809984 run.py:483] Algo bellman_ford step 959 current loss 0.154110, current_train_items 30720.
I0304 19:28:21.890219 22579586809984 run.py:483] Algo bellman_ford step 960 current loss 0.035928, current_train_items 30752.
I0304 19:28:21.907132 22579586809984 run.py:483] Algo bellman_ford step 961 current loss 0.092238, current_train_items 30784.
I0304 19:28:21.929331 22579586809984 run.py:483] Algo bellman_ford step 962 current loss 0.055952, current_train_items 30816.
I0304 19:28:21.957241 22579586809984 run.py:483] Algo bellman_ford step 963 current loss 0.093727, current_train_items 30848.
I0304 19:28:21.989538 22579586809984 run.py:483] Algo bellman_ford step 964 current loss 0.193693, current_train_items 30880.
I0304 19:28:22.008183 22579586809984 run.py:483] Algo bellman_ford step 965 current loss 0.029923, current_train_items 30912.
I0304 19:28:22.024272 22579586809984 run.py:483] Algo bellman_ford step 966 current loss 0.031570, current_train_items 30944.
I0304 19:28:22.048418 22579586809984 run.py:483] Algo bellman_ford step 967 current loss 0.105716, current_train_items 30976.
I0304 19:28:22.078482 22579586809984 run.py:483] Algo bellman_ford step 968 current loss 0.107038, current_train_items 31008.
I0304 19:28:22.109879 22579586809984 run.py:483] Algo bellman_ford step 969 current loss 0.138789, current_train_items 31040.
I0304 19:28:22.128645 22579586809984 run.py:483] Algo bellman_ford step 970 current loss 0.007424, current_train_items 31072.
I0304 19:28:22.145441 22579586809984 run.py:483] Algo bellman_ford step 971 current loss 0.021224, current_train_items 31104.
I0304 19:28:22.169228 22579586809984 run.py:483] Algo bellman_ford step 972 current loss 0.117101, current_train_items 31136.
I0304 19:28:22.199395 22579586809984 run.py:483] Algo bellman_ford step 973 current loss 0.138852, current_train_items 31168.
I0304 19:28:22.232605 22579586809984 run.py:483] Algo bellman_ford step 974 current loss 0.152630, current_train_items 31200.
I0304 19:28:22.251443 22579586809984 run.py:483] Algo bellman_ford step 975 current loss 0.022286, current_train_items 31232.
I0304 19:28:22.267612 22579586809984 run.py:483] Algo bellman_ford step 976 current loss 0.026077, current_train_items 31264.
I0304 19:28:22.290862 22579586809984 run.py:483] Algo bellman_ford step 977 current loss 0.146276, current_train_items 31296.
I0304 19:28:22.319744 22579586809984 run.py:483] Algo bellman_ford step 978 current loss 0.260528, current_train_items 31328.
I0304 19:28:22.350929 22579586809984 run.py:483] Algo bellman_ford step 979 current loss 0.231139, current_train_items 31360.
I0304 19:28:22.369365 22579586809984 run.py:483] Algo bellman_ford step 980 current loss 0.042368, current_train_items 31392.
I0304 19:28:22.385627 22579586809984 run.py:483] Algo bellman_ford step 981 current loss 0.084424, current_train_items 31424.
I0304 19:28:22.409019 22579586809984 run.py:483] Algo bellman_ford step 982 current loss 0.169181, current_train_items 31456.
I0304 19:28:22.437950 22579586809984 run.py:483] Algo bellman_ford step 983 current loss 0.150660, current_train_items 31488.
I0304 19:28:22.470063 22579586809984 run.py:483] Algo bellman_ford step 984 current loss 0.166489, current_train_items 31520.
I0304 19:28:22.489193 22579586809984 run.py:483] Algo bellman_ford step 985 current loss 0.017079, current_train_items 31552.
I0304 19:28:22.505910 22579586809984 run.py:483] Algo bellman_ford step 986 current loss 0.116223, current_train_items 31584.
I0304 19:28:22.528814 22579586809984 run.py:483] Algo bellman_ford step 987 current loss 0.094894, current_train_items 31616.
I0304 19:28:22.558544 22579586809984 run.py:483] Algo bellman_ford step 988 current loss 0.158794, current_train_items 31648.
I0304 19:28:22.589643 22579586809984 run.py:483] Algo bellman_ford step 989 current loss 0.111228, current_train_items 31680.
I0304 19:28:22.608419 22579586809984 run.py:483] Algo bellman_ford step 990 current loss 0.010109, current_train_items 31712.
I0304 19:28:22.624582 22579586809984 run.py:483] Algo bellman_ford step 991 current loss 0.055208, current_train_items 31744.
I0304 19:28:22.648197 22579586809984 run.py:483] Algo bellman_ford step 992 current loss 0.163752, current_train_items 31776.
I0304 19:28:22.676505 22579586809984 run.py:483] Algo bellman_ford step 993 current loss 0.137365, current_train_items 31808.
I0304 19:28:22.710393 22579586809984 run.py:483] Algo bellman_ford step 994 current loss 0.189825, current_train_items 31840.
I0304 19:28:22.728805 22579586809984 run.py:483] Algo bellman_ford step 995 current loss 0.017737, current_train_items 31872.
I0304 19:28:22.744867 22579586809984 run.py:483] Algo bellman_ford step 996 current loss 0.035895, current_train_items 31904.
I0304 19:28:22.768337 22579586809984 run.py:483] Algo bellman_ford step 997 current loss 0.181835, current_train_items 31936.
I0304 19:28:22.798732 22579586809984 run.py:483] Algo bellman_ford step 998 current loss 0.144532, current_train_items 31968.
I0304 19:28:22.829885 22579586809984 run.py:483] Algo bellman_ford step 999 current loss 0.136413, current_train_items 32000.
I0304 19:28:22.848812 22579586809984 run.py:483] Algo bellman_ford step 1000 current loss 0.019184, current_train_items 32032.
I0304 19:28:22.856520 22579586809984 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0304 19:28:22.856626 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.972, current avg val score is 0.955, val scores are: bellman_ford: 0.955
I0304 19:28:22.873521 22579586809984 run.py:483] Algo bellman_ford step 1001 current loss 0.111743, current_train_items 32064.
I0304 19:28:22.897583 22579586809984 run.py:483] Algo bellman_ford step 1002 current loss 0.147435, current_train_items 32096.
I0304 19:28:22.925923 22579586809984 run.py:483] Algo bellman_ford step 1003 current loss 0.164586, current_train_items 32128.
I0304 19:28:22.960381 22579586809984 run.py:483] Algo bellman_ford step 1004 current loss 0.188059, current_train_items 32160.
I0304 19:28:22.979420 22579586809984 run.py:483] Algo bellman_ford step 1005 current loss 0.044390, current_train_items 32192.
I0304 19:28:22.995205 22579586809984 run.py:483] Algo bellman_ford step 1006 current loss 0.107379, current_train_items 32224.
I0304 19:28:23.019815 22579586809984 run.py:483] Algo bellman_ford step 1007 current loss 0.254497, current_train_items 32256.
I0304 19:28:23.048938 22579586809984 run.py:483] Algo bellman_ford step 1008 current loss 0.171526, current_train_items 32288.
I0304 19:28:23.081703 22579586809984 run.py:483] Algo bellman_ford step 1009 current loss 0.158127, current_train_items 32320.
I0304 19:28:23.100404 22579586809984 run.py:483] Algo bellman_ford step 1010 current loss 0.038606, current_train_items 32352.
I0304 19:28:23.116674 22579586809984 run.py:483] Algo bellman_ford step 1011 current loss 0.143380, current_train_items 32384.
I0304 19:28:23.141153 22579586809984 run.py:483] Algo bellman_ford step 1012 current loss 0.319675, current_train_items 32416.
I0304 19:28:23.171103 22579586809984 run.py:483] Algo bellman_ford step 1013 current loss 0.163115, current_train_items 32448.
I0304 19:28:23.203697 22579586809984 run.py:483] Algo bellman_ford step 1014 current loss 0.134095, current_train_items 32480.
I0304 19:28:23.222433 22579586809984 run.py:483] Algo bellman_ford step 1015 current loss 0.019608, current_train_items 32512.
I0304 19:28:23.238898 22579586809984 run.py:483] Algo bellman_ford step 1016 current loss 0.124438, current_train_items 32544.
I0304 19:28:23.262630 22579586809984 run.py:483] Algo bellman_ford step 1017 current loss 0.182223, current_train_items 32576.
I0304 19:28:23.292474 22579586809984 run.py:483] Algo bellman_ford step 1018 current loss 0.132259, current_train_items 32608.
I0304 19:28:23.326926 22579586809984 run.py:483] Algo bellman_ford step 1019 current loss 0.180975, current_train_items 32640.
I0304 19:28:23.345368 22579586809984 run.py:483] Algo bellman_ford step 1020 current loss 0.010086, current_train_items 32672.
I0304 19:28:23.361324 22579586809984 run.py:483] Algo bellman_ford step 1021 current loss 0.057424, current_train_items 32704.
I0304 19:28:23.384961 22579586809984 run.py:483] Algo bellman_ford step 1022 current loss 0.168738, current_train_items 32736.
I0304 19:28:23.413488 22579586809984 run.py:483] Algo bellman_ford step 1023 current loss 0.258854, current_train_items 32768.
I0304 19:28:23.443950 22579586809984 run.py:483] Algo bellman_ford step 1024 current loss 0.128765, current_train_items 32800.
I0304 19:28:23.462628 22579586809984 run.py:483] Algo bellman_ford step 1025 current loss 0.026152, current_train_items 32832.
I0304 19:28:23.479347 22579586809984 run.py:483] Algo bellman_ford step 1026 current loss 0.082710, current_train_items 32864.
I0304 19:28:23.502716 22579586809984 run.py:483] Algo bellman_ford step 1027 current loss 0.127838, current_train_items 32896.
I0304 19:28:23.532998 22579586809984 run.py:483] Algo bellman_ford step 1028 current loss 0.202059, current_train_items 32928.
I0304 19:28:23.564193 22579586809984 run.py:483] Algo bellman_ford step 1029 current loss 0.313938, current_train_items 32960.
I0304 19:28:23.582656 22579586809984 run.py:483] Algo bellman_ford step 1030 current loss 0.017273, current_train_items 32992.
I0304 19:28:23.598513 22579586809984 run.py:483] Algo bellman_ford step 1031 current loss 0.023185, current_train_items 33024.
I0304 19:28:23.621323 22579586809984 run.py:483] Algo bellman_ford step 1032 current loss 0.076072, current_train_items 33056.
I0304 19:28:23.649987 22579586809984 run.py:483] Algo bellman_ford step 1033 current loss 0.180069, current_train_items 33088.
I0304 19:28:23.682386 22579586809984 run.py:483] Algo bellman_ford step 1034 current loss 0.197658, current_train_items 33120.
I0304 19:28:23.701308 22579586809984 run.py:483] Algo bellman_ford step 1035 current loss 0.032754, current_train_items 33152.
I0304 19:28:23.717447 22579586809984 run.py:483] Algo bellman_ford step 1036 current loss 0.035510, current_train_items 33184.
I0304 19:28:23.741599 22579586809984 run.py:483] Algo bellman_ford step 1037 current loss 0.088142, current_train_items 33216.
I0304 19:28:23.771534 22579586809984 run.py:483] Algo bellman_ford step 1038 current loss 0.145991, current_train_items 33248.
I0304 19:28:23.804862 22579586809984 run.py:483] Algo bellman_ford step 1039 current loss 0.148037, current_train_items 33280.
I0304 19:28:23.823367 22579586809984 run.py:483] Algo bellman_ford step 1040 current loss 0.010002, current_train_items 33312.
I0304 19:28:23.840342 22579586809984 run.py:483] Algo bellman_ford step 1041 current loss 0.081967, current_train_items 33344.
I0304 19:28:23.864378 22579586809984 run.py:483] Algo bellman_ford step 1042 current loss 0.211254, current_train_items 33376.
I0304 19:28:23.894181 22579586809984 run.py:483] Algo bellman_ford step 1043 current loss 0.157812, current_train_items 33408.
I0304 19:28:23.925770 22579586809984 run.py:483] Algo bellman_ford step 1044 current loss 0.116217, current_train_items 33440.
I0304 19:28:23.944016 22579586809984 run.py:483] Algo bellman_ford step 1045 current loss 0.009848, current_train_items 33472.
I0304 19:28:23.959890 22579586809984 run.py:483] Algo bellman_ford step 1046 current loss 0.046111, current_train_items 33504.
I0304 19:28:23.984003 22579586809984 run.py:483] Algo bellman_ford step 1047 current loss 0.146487, current_train_items 33536.
I0304 19:28:24.014242 22579586809984 run.py:483] Algo bellman_ford step 1048 current loss 0.171703, current_train_items 33568.
I0304 19:28:24.047101 22579586809984 run.py:483] Algo bellman_ford step 1049 current loss 0.188578, current_train_items 33600.
I0304 19:28:24.065862 22579586809984 run.py:483] Algo bellman_ford step 1050 current loss 0.069992, current_train_items 33632.
I0304 19:28:24.073947 22579586809984 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0304 19:28:24.074054 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.972, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:28:24.090528 22579586809984 run.py:483] Algo bellman_ford step 1051 current loss 0.059103, current_train_items 33664.
I0304 19:28:24.115317 22579586809984 run.py:483] Algo bellman_ford step 1052 current loss 0.131662, current_train_items 33696.
I0304 19:28:24.144530 22579586809984 run.py:483] Algo bellman_ford step 1053 current loss 0.136353, current_train_items 33728.
I0304 19:28:24.177292 22579586809984 run.py:483] Algo bellman_ford step 1054 current loss 0.192415, current_train_items 33760.
I0304 19:28:24.196671 22579586809984 run.py:483] Algo bellman_ford step 1055 current loss 0.023062, current_train_items 33792.
I0304 19:28:24.213006 22579586809984 run.py:483] Algo bellman_ford step 1056 current loss 0.065478, current_train_items 33824.
I0304 19:28:24.237317 22579586809984 run.py:483] Algo bellman_ford step 1057 current loss 0.131226, current_train_items 33856.
I0304 19:28:24.268169 22579586809984 run.py:483] Algo bellman_ford step 1058 current loss 0.120041, current_train_items 33888.
I0304 19:28:24.298208 22579586809984 run.py:483] Algo bellman_ford step 1059 current loss 0.108127, current_train_items 33920.
I0304 19:28:24.317080 22579586809984 run.py:483] Algo bellman_ford step 1060 current loss 0.009759, current_train_items 33952.
I0304 19:28:24.333446 22579586809984 run.py:483] Algo bellman_ford step 1061 current loss 0.044733, current_train_items 33984.
I0304 19:28:24.355506 22579586809984 run.py:483] Algo bellman_ford step 1062 current loss 0.090203, current_train_items 34016.
I0304 19:28:24.384754 22579586809984 run.py:483] Algo bellman_ford step 1063 current loss 0.178109, current_train_items 34048.
I0304 19:28:24.417304 22579586809984 run.py:483] Algo bellman_ford step 1064 current loss 0.199385, current_train_items 34080.
I0304 19:28:24.436095 22579586809984 run.py:483] Algo bellman_ford step 1065 current loss 0.012565, current_train_items 34112.
I0304 19:28:24.452308 22579586809984 run.py:483] Algo bellman_ford step 1066 current loss 0.127118, current_train_items 34144.
I0304 19:28:24.476147 22579586809984 run.py:483] Algo bellman_ford step 1067 current loss 0.162826, current_train_items 34176.
I0304 19:28:24.506100 22579586809984 run.py:483] Algo bellman_ford step 1068 current loss 0.097913, current_train_items 34208.
I0304 19:28:24.539869 22579586809984 run.py:483] Algo bellman_ford step 1069 current loss 0.173640, current_train_items 34240.
I0304 19:28:24.558385 22579586809984 run.py:483] Algo bellman_ford step 1070 current loss 0.010255, current_train_items 34272.
I0304 19:28:24.575036 22579586809984 run.py:483] Algo bellman_ford step 1071 current loss 0.073209, current_train_items 34304.
I0304 19:28:24.598894 22579586809984 run.py:483] Algo bellman_ford step 1072 current loss 0.126045, current_train_items 34336.
I0304 19:28:24.629015 22579586809984 run.py:483] Algo bellman_ford step 1073 current loss 0.134647, current_train_items 34368.
I0304 19:28:24.660071 22579586809984 run.py:483] Algo bellman_ford step 1074 current loss 0.105083, current_train_items 34400.
I0304 19:28:24.678636 22579586809984 run.py:483] Algo bellman_ford step 1075 current loss 0.010981, current_train_items 34432.
I0304 19:28:24.695281 22579586809984 run.py:483] Algo bellman_ford step 1076 current loss 0.210353, current_train_items 34464.
I0304 19:28:24.719294 22579586809984 run.py:483] Algo bellman_ford step 1077 current loss 0.187931, current_train_items 34496.
I0304 19:28:24.749523 22579586809984 run.py:483] Algo bellman_ford step 1078 current loss 0.161214, current_train_items 34528.
I0304 19:28:24.781590 22579586809984 run.py:483] Algo bellman_ford step 1079 current loss 0.129589, current_train_items 34560.
I0304 19:28:24.800398 22579586809984 run.py:483] Algo bellman_ford step 1080 current loss 0.031569, current_train_items 34592.
I0304 19:28:24.817126 22579586809984 run.py:483] Algo bellman_ford step 1081 current loss 0.133672, current_train_items 34624.
I0304 19:28:24.840602 22579586809984 run.py:483] Algo bellman_ford step 1082 current loss 0.137876, current_train_items 34656.
I0304 19:28:24.869956 22579586809984 run.py:483] Algo bellman_ford step 1083 current loss 0.139614, current_train_items 34688.
I0304 19:28:24.903156 22579586809984 run.py:483] Algo bellman_ford step 1084 current loss 0.119777, current_train_items 34720.
I0304 19:28:24.922184 22579586809984 run.py:483] Algo bellman_ford step 1085 current loss 0.008683, current_train_items 34752.
I0304 19:28:24.938640 22579586809984 run.py:483] Algo bellman_ford step 1086 current loss 0.096358, current_train_items 34784.
I0304 19:28:24.962607 22579586809984 run.py:483] Algo bellman_ford step 1087 current loss 0.206482, current_train_items 34816.
I0304 19:28:24.991141 22579586809984 run.py:483] Algo bellman_ford step 1088 current loss 0.174328, current_train_items 34848.
I0304 19:28:25.021698 22579586809984 run.py:483] Algo bellman_ford step 1089 current loss 0.171964, current_train_items 34880.
I0304 19:28:25.040572 22579586809984 run.py:483] Algo bellman_ford step 1090 current loss 0.014286, current_train_items 34912.
I0304 19:28:25.057288 22579586809984 run.py:483] Algo bellman_ford step 1091 current loss 0.106398, current_train_items 34944.
I0304 19:28:25.080157 22579586809984 run.py:483] Algo bellman_ford step 1092 current loss 0.176994, current_train_items 34976.
I0304 19:28:25.110664 22579586809984 run.py:483] Algo bellman_ford step 1093 current loss 0.221491, current_train_items 35008.
I0304 19:28:25.143697 22579586809984 run.py:483] Algo bellman_ford step 1094 current loss 0.126318, current_train_items 35040.
I0304 19:28:25.162314 22579586809984 run.py:483] Algo bellman_ford step 1095 current loss 0.010755, current_train_items 35072.
I0304 19:28:25.178530 22579586809984 run.py:483] Algo bellman_ford step 1096 current loss 0.072539, current_train_items 35104.
I0304 19:28:25.202097 22579586809984 run.py:483] Algo bellman_ford step 1097 current loss 0.078422, current_train_items 35136.
I0304 19:28:25.232228 22579586809984 run.py:483] Algo bellman_ford step 1098 current loss 0.168884, current_train_items 35168.
I0304 19:28:25.263881 22579586809984 run.py:483] Algo bellman_ford step 1099 current loss 0.131813, current_train_items 35200.
I0304 19:28:25.282904 22579586809984 run.py:483] Algo bellman_ford step 1100 current loss 0.009899, current_train_items 35232.
I0304 19:28:25.291242 22579586809984 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0304 19:28:25.291347 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.972, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:28:25.321213 22579586809984 run.py:483] Algo bellman_ford step 1101 current loss 0.052571, current_train_items 35264.
I0304 19:28:25.344539 22579586809984 run.py:483] Algo bellman_ford step 1102 current loss 0.069251, current_train_items 35296.
I0304 19:28:25.375507 22579586809984 run.py:483] Algo bellman_ford step 1103 current loss 0.109248, current_train_items 35328.
I0304 19:28:25.409893 22579586809984 run.py:483] Algo bellman_ford step 1104 current loss 0.159536, current_train_items 35360.
I0304 19:28:25.428705 22579586809984 run.py:483] Algo bellman_ford step 1105 current loss 0.008455, current_train_items 35392.
I0304 19:28:25.444635 22579586809984 run.py:483] Algo bellman_ford step 1106 current loss 0.049293, current_train_items 35424.
I0304 19:28:25.468353 22579586809984 run.py:483] Algo bellman_ford step 1107 current loss 0.132539, current_train_items 35456.
I0304 19:28:25.497196 22579586809984 run.py:483] Algo bellman_ford step 1108 current loss 0.092958, current_train_items 35488.
I0304 19:28:25.527385 22579586809984 run.py:483] Algo bellman_ford step 1109 current loss 0.096395, current_train_items 35520.
I0304 19:28:25.545994 22579586809984 run.py:483] Algo bellman_ford step 1110 current loss 0.018186, current_train_items 35552.
I0304 19:28:25.562294 22579586809984 run.py:483] Algo bellman_ford step 1111 current loss 0.041302, current_train_items 35584.
I0304 19:28:25.585862 22579586809984 run.py:483] Algo bellman_ford step 1112 current loss 0.071825, current_train_items 35616.
I0304 19:28:25.614231 22579586809984 run.py:483] Algo bellman_ford step 1113 current loss 0.104880, current_train_items 35648.
I0304 19:28:25.647223 22579586809984 run.py:483] Algo bellman_ford step 1114 current loss 0.170777, current_train_items 35680.
I0304 19:28:25.665777 22579586809984 run.py:483] Algo bellman_ford step 1115 current loss 0.012070, current_train_items 35712.
I0304 19:28:25.681725 22579586809984 run.py:483] Algo bellman_ford step 1116 current loss 0.054385, current_train_items 35744.
I0304 19:28:25.706341 22579586809984 run.py:483] Algo bellman_ford step 1117 current loss 0.127613, current_train_items 35776.
I0304 19:28:25.736153 22579586809984 run.py:483] Algo bellman_ford step 1118 current loss 0.079423, current_train_items 35808.
I0304 19:28:25.768529 22579586809984 run.py:483] Algo bellman_ford step 1119 current loss 0.251536, current_train_items 35840.
I0304 19:28:25.787380 22579586809984 run.py:483] Algo bellman_ford step 1120 current loss 0.013294, current_train_items 35872.
I0304 19:28:25.803205 22579586809984 run.py:483] Algo bellman_ford step 1121 current loss 0.066872, current_train_items 35904.
I0304 19:28:25.827040 22579586809984 run.py:483] Algo bellman_ford step 1122 current loss 0.078804, current_train_items 35936.
I0304 19:28:25.856014 22579586809984 run.py:483] Algo bellman_ford step 1123 current loss 0.088899, current_train_items 35968.
I0304 19:28:25.889329 22579586809984 run.py:483] Algo bellman_ford step 1124 current loss 0.178638, current_train_items 36000.
I0304 19:28:25.907670 22579586809984 run.py:483] Algo bellman_ford step 1125 current loss 0.011495, current_train_items 36032.
I0304 19:28:25.923561 22579586809984 run.py:483] Algo bellman_ford step 1126 current loss 0.047337, current_train_items 36064.
I0304 19:28:25.946947 22579586809984 run.py:483] Algo bellman_ford step 1127 current loss 0.111084, current_train_items 36096.
I0304 19:28:25.976376 22579586809984 run.py:483] Algo bellman_ford step 1128 current loss 0.200079, current_train_items 36128.
I0304 19:28:26.005923 22579586809984 run.py:483] Algo bellman_ford step 1129 current loss 0.118065, current_train_items 36160.
I0304 19:28:26.024863 22579586809984 run.py:483] Algo bellman_ford step 1130 current loss 0.008965, current_train_items 36192.
I0304 19:28:26.041015 22579586809984 run.py:483] Algo bellman_ford step 1131 current loss 0.022481, current_train_items 36224.
I0304 19:28:26.064769 22579586809984 run.py:483] Algo bellman_ford step 1132 current loss 0.120198, current_train_items 36256.
I0304 19:28:26.094200 22579586809984 run.py:483] Algo bellman_ford step 1133 current loss 0.153073, current_train_items 36288.
I0304 19:28:26.125141 22579586809984 run.py:483] Algo bellman_ford step 1134 current loss 0.144911, current_train_items 36320.
I0304 19:28:26.143880 22579586809984 run.py:483] Algo bellman_ford step 1135 current loss 0.011630, current_train_items 36352.
I0304 19:28:26.160106 22579586809984 run.py:483] Algo bellman_ford step 1136 current loss 0.029876, current_train_items 36384.
I0304 19:28:26.182956 22579586809984 run.py:483] Algo bellman_ford step 1137 current loss 0.099155, current_train_items 36416.
I0304 19:28:26.213458 22579586809984 run.py:483] Algo bellman_ford step 1138 current loss 0.098915, current_train_items 36448.
I0304 19:28:26.245085 22579586809984 run.py:483] Algo bellman_ford step 1139 current loss 0.119938, current_train_items 36480.
I0304 19:28:26.263570 22579586809984 run.py:483] Algo bellman_ford step 1140 current loss 0.009145, current_train_items 36512.
I0304 19:28:26.279951 22579586809984 run.py:483] Algo bellman_ford step 1141 current loss 0.029657, current_train_items 36544.
I0304 19:28:26.302831 22579586809984 run.py:483] Algo bellman_ford step 1142 current loss 0.099888, current_train_items 36576.
I0304 19:28:26.331337 22579586809984 run.py:483] Algo bellman_ford step 1143 current loss 0.147206, current_train_items 36608.
I0304 19:28:26.362872 22579586809984 run.py:483] Algo bellman_ford step 1144 current loss 0.077171, current_train_items 36640.
I0304 19:28:26.381994 22579586809984 run.py:483] Algo bellman_ford step 1145 current loss 0.035704, current_train_items 36672.
I0304 19:28:26.398260 22579586809984 run.py:483] Algo bellman_ford step 1146 current loss 0.047340, current_train_items 36704.
I0304 19:28:26.422330 22579586809984 run.py:483] Algo bellman_ford step 1147 current loss 0.086688, current_train_items 36736.
I0304 19:28:26.451892 22579586809984 run.py:483] Algo bellman_ford step 1148 current loss 0.098334, current_train_items 36768.
I0304 19:28:26.481892 22579586809984 run.py:483] Algo bellman_ford step 1149 current loss 0.093854, current_train_items 36800.
I0304 19:28:26.500517 22579586809984 run.py:483] Algo bellman_ford step 1150 current loss 0.029178, current_train_items 36832.
I0304 19:28:26.508737 22579586809984 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0304 19:28:26.508842 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:28:26.525492 22579586809984 run.py:483] Algo bellman_ford step 1151 current loss 0.020650, current_train_items 36864.
I0304 19:28:26.549865 22579586809984 run.py:483] Algo bellman_ford step 1152 current loss 0.121255, current_train_items 36896.
I0304 19:28:26.580962 22579586809984 run.py:483] Algo bellman_ford step 1153 current loss 0.131247, current_train_items 36928.
W0304 19:28:26.601581 22579586809984 samplers.py:155] Increasing hint lengh from 11 to 12
I0304 19:28:33.263762 22579586809984 run.py:483] Algo bellman_ford step 1154 current loss 0.152430, current_train_items 36960.
I0304 19:28:33.285427 22579586809984 run.py:483] Algo bellman_ford step 1155 current loss 0.014363, current_train_items 36992.
I0304 19:28:33.302018 22579586809984 run.py:483] Algo bellman_ford step 1156 current loss 0.054965, current_train_items 37024.
I0304 19:28:33.325670 22579586809984 run.py:483] Algo bellman_ford step 1157 current loss 0.083264, current_train_items 37056.
I0304 19:28:33.355187 22579586809984 run.py:483] Algo bellman_ford step 1158 current loss 0.112453, current_train_items 37088.
I0304 19:28:33.385958 22579586809984 run.py:483] Algo bellman_ford step 1159 current loss 0.230480, current_train_items 37120.
I0304 19:28:33.405768 22579586809984 run.py:483] Algo bellman_ford step 1160 current loss 0.016033, current_train_items 37152.
I0304 19:28:33.422490 22579586809984 run.py:483] Algo bellman_ford step 1161 current loss 0.035126, current_train_items 37184.
I0304 19:28:33.445301 22579586809984 run.py:483] Algo bellman_ford step 1162 current loss 0.055765, current_train_items 37216.
I0304 19:28:33.474572 22579586809984 run.py:483] Algo bellman_ford step 1163 current loss 0.106993, current_train_items 37248.
I0304 19:28:33.506929 22579586809984 run.py:483] Algo bellman_ford step 1164 current loss 0.150260, current_train_items 37280.
I0304 19:28:33.525964 22579586809984 run.py:483] Algo bellman_ford step 1165 current loss 0.008177, current_train_items 37312.
I0304 19:28:33.542420 22579586809984 run.py:483] Algo bellman_ford step 1166 current loss 0.042148, current_train_items 37344.
I0304 19:28:33.566190 22579586809984 run.py:483] Algo bellman_ford step 1167 current loss 0.171377, current_train_items 37376.
I0304 19:28:33.596153 22579586809984 run.py:483] Algo bellman_ford step 1168 current loss 0.153788, current_train_items 37408.
I0304 19:28:33.629740 22579586809984 run.py:483] Algo bellman_ford step 1169 current loss 0.219540, current_train_items 37440.
I0304 19:28:33.649568 22579586809984 run.py:483] Algo bellman_ford step 1170 current loss 0.019431, current_train_items 37472.
I0304 19:28:33.666394 22579586809984 run.py:483] Algo bellman_ford step 1171 current loss 0.058542, current_train_items 37504.
I0304 19:28:33.689977 22579586809984 run.py:483] Algo bellman_ford step 1172 current loss 0.107669, current_train_items 37536.
I0304 19:28:33.721080 22579586809984 run.py:483] Algo bellman_ford step 1173 current loss 0.135842, current_train_items 37568.
I0304 19:28:33.753467 22579586809984 run.py:483] Algo bellman_ford step 1174 current loss 0.155009, current_train_items 37600.
I0304 19:28:33.772925 22579586809984 run.py:483] Algo bellman_ford step 1175 current loss 0.006483, current_train_items 37632.
I0304 19:28:33.789269 22579586809984 run.py:483] Algo bellman_ford step 1176 current loss 0.045816, current_train_items 37664.
I0304 19:28:33.811915 22579586809984 run.py:483] Algo bellman_ford step 1177 current loss 0.146021, current_train_items 37696.
I0304 19:28:33.841142 22579586809984 run.py:483] Algo bellman_ford step 1178 current loss 0.095660, current_train_items 37728.
I0304 19:28:33.875130 22579586809984 run.py:483] Algo bellman_ford step 1179 current loss 0.172040, current_train_items 37760.
I0304 19:28:33.894532 22579586809984 run.py:483] Algo bellman_ford step 1180 current loss 0.007256, current_train_items 37792.
I0304 19:28:33.911169 22579586809984 run.py:483] Algo bellman_ford step 1181 current loss 0.079248, current_train_items 37824.
I0304 19:28:33.935090 22579586809984 run.py:483] Algo bellman_ford step 1182 current loss 0.074485, current_train_items 37856.
I0304 19:28:33.963701 22579586809984 run.py:483] Algo bellman_ford step 1183 current loss 0.120116, current_train_items 37888.
I0304 19:28:33.996071 22579586809984 run.py:483] Algo bellman_ford step 1184 current loss 0.181312, current_train_items 37920.
I0304 19:28:34.015664 22579586809984 run.py:483] Algo bellman_ford step 1185 current loss 0.039988, current_train_items 37952.
I0304 19:28:34.031750 22579586809984 run.py:483] Algo bellman_ford step 1186 current loss 0.046473, current_train_items 37984.
I0304 19:28:34.055363 22579586809984 run.py:483] Algo bellman_ford step 1187 current loss 0.087873, current_train_items 38016.
I0304 19:28:34.084610 22579586809984 run.py:483] Algo bellman_ford step 1188 current loss 0.078118, current_train_items 38048.
I0304 19:28:34.115768 22579586809984 run.py:483] Algo bellman_ford step 1189 current loss 0.127191, current_train_items 38080.
I0304 19:28:34.135502 22579586809984 run.py:483] Algo bellman_ford step 1190 current loss 0.046487, current_train_items 38112.
I0304 19:28:34.152040 22579586809984 run.py:483] Algo bellman_ford step 1191 current loss 0.048949, current_train_items 38144.
I0304 19:28:34.176694 22579586809984 run.py:483] Algo bellman_ford step 1192 current loss 0.098194, current_train_items 38176.
I0304 19:28:34.206330 22579586809984 run.py:483] Algo bellman_ford step 1193 current loss 0.106722, current_train_items 38208.
I0304 19:28:34.238802 22579586809984 run.py:483] Algo bellman_ford step 1194 current loss 0.106025, current_train_items 38240.
I0304 19:28:34.257813 22579586809984 run.py:483] Algo bellman_ford step 1195 current loss 0.013858, current_train_items 38272.
I0304 19:28:34.274229 22579586809984 run.py:483] Algo bellman_ford step 1196 current loss 0.049458, current_train_items 38304.
I0304 19:28:34.297601 22579586809984 run.py:483] Algo bellman_ford step 1197 current loss 0.126773, current_train_items 38336.
I0304 19:28:34.328491 22579586809984 run.py:483] Algo bellman_ford step 1198 current loss 0.125390, current_train_items 38368.
I0304 19:28:34.361737 22579586809984 run.py:483] Algo bellman_ford step 1199 current loss 0.178358, current_train_items 38400.
I0304 19:28:34.381356 22579586809984 run.py:483] Algo bellman_ford step 1200 current loss 0.021794, current_train_items 38432.
I0304 19:28:34.390712 22579586809984 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0304 19:28:34.390819 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:28:34.407976 22579586809984 run.py:483] Algo bellman_ford step 1201 current loss 0.099990, current_train_items 38464.
I0304 19:28:34.432221 22579586809984 run.py:483] Algo bellman_ford step 1202 current loss 0.065586, current_train_items 38496.
I0304 19:28:34.463363 22579586809984 run.py:483] Algo bellman_ford step 1203 current loss 0.136869, current_train_items 38528.
I0304 19:28:34.496653 22579586809984 run.py:483] Algo bellman_ford step 1204 current loss 0.143504, current_train_items 38560.
I0304 19:28:34.516109 22579586809984 run.py:483] Algo bellman_ford step 1205 current loss 0.009689, current_train_items 38592.
I0304 19:28:34.532196 22579586809984 run.py:483] Algo bellman_ford step 1206 current loss 0.022198, current_train_items 38624.
I0304 19:28:34.555364 22579586809984 run.py:483] Algo bellman_ford step 1207 current loss 0.124594, current_train_items 38656.
I0304 19:28:34.585372 22579586809984 run.py:483] Algo bellman_ford step 1208 current loss 0.100057, current_train_items 38688.
I0304 19:28:34.619093 22579586809984 run.py:483] Algo bellman_ford step 1209 current loss 0.095385, current_train_items 38720.
I0304 19:28:34.638182 22579586809984 run.py:483] Algo bellman_ford step 1210 current loss 0.010552, current_train_items 38752.
I0304 19:28:34.654384 22579586809984 run.py:483] Algo bellman_ford step 1211 current loss 0.108166, current_train_items 38784.
I0304 19:28:34.677350 22579586809984 run.py:483] Algo bellman_ford step 1212 current loss 0.153054, current_train_items 38816.
I0304 19:28:34.705974 22579586809984 run.py:483] Algo bellman_ford step 1213 current loss 0.122230, current_train_items 38848.
I0304 19:28:34.738246 22579586809984 run.py:483] Algo bellman_ford step 1214 current loss 0.193472, current_train_items 38880.
I0304 19:28:34.757469 22579586809984 run.py:483] Algo bellman_ford step 1215 current loss 0.017884, current_train_items 38912.
I0304 19:28:34.773634 22579586809984 run.py:483] Algo bellman_ford step 1216 current loss 0.045244, current_train_items 38944.
I0304 19:28:34.796999 22579586809984 run.py:483] Algo bellman_ford step 1217 current loss 0.211378, current_train_items 38976.
I0304 19:28:34.827924 22579586809984 run.py:483] Algo bellman_ford step 1218 current loss 0.296135, current_train_items 39008.
I0304 19:28:34.860516 22579586809984 run.py:483] Algo bellman_ford step 1219 current loss 0.175812, current_train_items 39040.
I0304 19:28:34.879783 22579586809984 run.py:483] Algo bellman_ford step 1220 current loss 0.013215, current_train_items 39072.
I0304 19:28:34.895901 22579586809984 run.py:483] Algo bellman_ford step 1221 current loss 0.040273, current_train_items 39104.
I0304 19:28:34.920196 22579586809984 run.py:483] Algo bellman_ford step 1222 current loss 0.172215, current_train_items 39136.
I0304 19:28:34.950456 22579586809984 run.py:483] Algo bellman_ford step 1223 current loss 0.241651, current_train_items 39168.
I0304 19:28:34.982442 22579586809984 run.py:483] Algo bellman_ford step 1224 current loss 0.171467, current_train_items 39200.
I0304 19:28:35.001770 22579586809984 run.py:483] Algo bellman_ford step 1225 current loss 0.008078, current_train_items 39232.
I0304 19:28:35.018176 22579586809984 run.py:483] Algo bellman_ford step 1226 current loss 0.036984, current_train_items 39264.
I0304 19:28:35.042762 22579586809984 run.py:483] Algo bellman_ford step 1227 current loss 0.076564, current_train_items 39296.
I0304 19:28:35.072444 22579586809984 run.py:483] Algo bellman_ford step 1228 current loss 0.099692, current_train_items 39328.
I0304 19:28:35.106435 22579586809984 run.py:483] Algo bellman_ford step 1229 current loss 0.153643, current_train_items 39360.
I0304 19:28:35.125877 22579586809984 run.py:483] Algo bellman_ford step 1230 current loss 0.008954, current_train_items 39392.
I0304 19:28:35.142536 22579586809984 run.py:483] Algo bellman_ford step 1231 current loss 0.048446, current_train_items 39424.
I0304 19:28:35.167156 22579586809984 run.py:483] Algo bellman_ford step 1232 current loss 0.133570, current_train_items 39456.
I0304 19:28:35.196878 22579586809984 run.py:483] Algo bellman_ford step 1233 current loss 0.098904, current_train_items 39488.
I0304 19:28:35.227823 22579586809984 run.py:483] Algo bellman_ford step 1234 current loss 0.122698, current_train_items 39520.
I0304 19:28:35.247077 22579586809984 run.py:483] Algo bellman_ford step 1235 current loss 0.007933, current_train_items 39552.
I0304 19:28:35.262973 22579586809984 run.py:483] Algo bellman_ford step 1236 current loss 0.022188, current_train_items 39584.
I0304 19:28:35.286816 22579586809984 run.py:483] Algo bellman_ford step 1237 current loss 0.079438, current_train_items 39616.
I0304 19:28:35.316979 22579586809984 run.py:483] Algo bellman_ford step 1238 current loss 0.089119, current_train_items 39648.
I0304 19:28:35.350453 22579586809984 run.py:483] Algo bellman_ford step 1239 current loss 0.107902, current_train_items 39680.
I0304 19:28:35.369605 22579586809984 run.py:483] Algo bellman_ford step 1240 current loss 0.014396, current_train_items 39712.
I0304 19:28:35.386068 22579586809984 run.py:483] Algo bellman_ford step 1241 current loss 0.031699, current_train_items 39744.
I0304 19:28:35.410789 22579586809984 run.py:483] Algo bellman_ford step 1242 current loss 0.051348, current_train_items 39776.
I0304 19:28:35.440519 22579586809984 run.py:483] Algo bellman_ford step 1243 current loss 0.101659, current_train_items 39808.
I0304 19:28:35.473140 22579586809984 run.py:483] Algo bellman_ford step 1244 current loss 0.162358, current_train_items 39840.
I0304 19:28:35.492327 22579586809984 run.py:483] Algo bellman_ford step 1245 current loss 0.009074, current_train_items 39872.
I0304 19:28:35.508569 22579586809984 run.py:483] Algo bellman_ford step 1246 current loss 0.025700, current_train_items 39904.
I0304 19:28:35.531745 22579586809984 run.py:483] Algo bellman_ford step 1247 current loss 0.069969, current_train_items 39936.
I0304 19:28:35.561944 22579586809984 run.py:483] Algo bellman_ford step 1248 current loss 0.118711, current_train_items 39968.
I0304 19:28:35.593995 22579586809984 run.py:483] Algo bellman_ford step 1249 current loss 0.164384, current_train_items 40000.
I0304 19:28:35.613230 22579586809984 run.py:483] Algo bellman_ford step 1250 current loss 0.008705, current_train_items 40032.
I0304 19:28:35.621695 22579586809984 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.96484375, 'score': 0.96484375, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0304 19:28:35.621803 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.965, val scores are: bellman_ford: 0.965
I0304 19:28:35.638707 22579586809984 run.py:483] Algo bellman_ford step 1251 current loss 0.069359, current_train_items 40064.
I0304 19:28:35.662635 22579586809984 run.py:483] Algo bellman_ford step 1252 current loss 0.064680, current_train_items 40096.
I0304 19:28:35.691078 22579586809984 run.py:483] Algo bellman_ford step 1253 current loss 0.079529, current_train_items 40128.
I0304 19:28:35.721007 22579586809984 run.py:483] Algo bellman_ford step 1254 current loss 0.114931, current_train_items 40160.
I0304 19:28:35.740174 22579586809984 run.py:483] Algo bellman_ford step 1255 current loss 0.010632, current_train_items 40192.
I0304 19:28:35.756449 22579586809984 run.py:483] Algo bellman_ford step 1256 current loss 0.072400, current_train_items 40224.
I0304 19:28:35.780178 22579586809984 run.py:483] Algo bellman_ford step 1257 current loss 0.097437, current_train_items 40256.
I0304 19:28:35.809264 22579586809984 run.py:483] Algo bellman_ford step 1258 current loss 0.082325, current_train_items 40288.
I0304 19:28:35.841068 22579586809984 run.py:483] Algo bellman_ford step 1259 current loss 0.105151, current_train_items 40320.
I0304 19:28:35.860369 22579586809984 run.py:483] Algo bellman_ford step 1260 current loss 0.018782, current_train_items 40352.
I0304 19:28:35.877251 22579586809984 run.py:483] Algo bellman_ford step 1261 current loss 0.029580, current_train_items 40384.
I0304 19:28:35.900828 22579586809984 run.py:483] Algo bellman_ford step 1262 current loss 0.069699, current_train_items 40416.
I0304 19:28:35.927841 22579586809984 run.py:483] Algo bellman_ford step 1263 current loss 0.074380, current_train_items 40448.
I0304 19:28:35.961976 22579586809984 run.py:483] Algo bellman_ford step 1264 current loss 0.142092, current_train_items 40480.
I0304 19:28:35.980662 22579586809984 run.py:483] Algo bellman_ford step 1265 current loss 0.006376, current_train_items 40512.
I0304 19:28:35.996886 22579586809984 run.py:483] Algo bellman_ford step 1266 current loss 0.036596, current_train_items 40544.
I0304 19:28:36.019628 22579586809984 run.py:483] Algo bellman_ford step 1267 current loss 0.080050, current_train_items 40576.
I0304 19:28:36.048594 22579586809984 run.py:483] Algo bellman_ford step 1268 current loss 0.070560, current_train_items 40608.
I0304 19:28:36.081159 22579586809984 run.py:483] Algo bellman_ford step 1269 current loss 0.119175, current_train_items 40640.
I0304 19:28:36.100433 22579586809984 run.py:483] Algo bellman_ford step 1270 current loss 0.008811, current_train_items 40672.
I0304 19:28:36.116572 22579586809984 run.py:483] Algo bellman_ford step 1271 current loss 0.036924, current_train_items 40704.
I0304 19:28:36.138593 22579586809984 run.py:483] Algo bellman_ford step 1272 current loss 0.087493, current_train_items 40736.
I0304 19:28:36.166846 22579586809984 run.py:483] Algo bellman_ford step 1273 current loss 0.083021, current_train_items 40768.
I0304 19:28:36.199383 22579586809984 run.py:483] Algo bellman_ford step 1274 current loss 0.157939, current_train_items 40800.
I0304 19:28:36.218567 22579586809984 run.py:483] Algo bellman_ford step 1275 current loss 0.005674, current_train_items 40832.
I0304 19:28:36.234896 22579586809984 run.py:483] Algo bellman_ford step 1276 current loss 0.065339, current_train_items 40864.
I0304 19:28:36.257087 22579586809984 run.py:483] Algo bellman_ford step 1277 current loss 0.111676, current_train_items 40896.
I0304 19:28:36.286282 22579586809984 run.py:483] Algo bellman_ford step 1278 current loss 0.098566, current_train_items 40928.
I0304 19:28:36.318127 22579586809984 run.py:483] Algo bellman_ford step 1279 current loss 0.146779, current_train_items 40960.
I0304 19:28:36.337366 22579586809984 run.py:483] Algo bellman_ford step 1280 current loss 0.007051, current_train_items 40992.
I0304 19:28:36.353453 22579586809984 run.py:483] Algo bellman_ford step 1281 current loss 0.030506, current_train_items 41024.
I0304 19:28:36.377046 22579586809984 run.py:483] Algo bellman_ford step 1282 current loss 0.089347, current_train_items 41056.
I0304 19:28:36.407337 22579586809984 run.py:483] Algo bellman_ford step 1283 current loss 0.111004, current_train_items 41088.
I0304 19:28:36.438614 22579586809984 run.py:483] Algo bellman_ford step 1284 current loss 0.176749, current_train_items 41120.
I0304 19:28:36.458172 22579586809984 run.py:483] Algo bellman_ford step 1285 current loss 0.055166, current_train_items 41152.
I0304 19:28:36.474571 22579586809984 run.py:483] Algo bellman_ford step 1286 current loss 0.033397, current_train_items 41184.
I0304 19:28:36.499024 22579586809984 run.py:483] Algo bellman_ford step 1287 current loss 0.108412, current_train_items 41216.
I0304 19:28:36.528594 22579586809984 run.py:483] Algo bellman_ford step 1288 current loss 0.106081, current_train_items 41248.
I0304 19:28:36.561055 22579586809984 run.py:483] Algo bellman_ford step 1289 current loss 0.125655, current_train_items 41280.
I0304 19:28:36.580332 22579586809984 run.py:483] Algo bellman_ford step 1290 current loss 0.014190, current_train_items 41312.
I0304 19:28:36.596748 22579586809984 run.py:483] Algo bellman_ford step 1291 current loss 0.045123, current_train_items 41344.
I0304 19:28:36.619466 22579586809984 run.py:483] Algo bellman_ford step 1292 current loss 0.073071, current_train_items 41376.
I0304 19:28:36.649166 22579586809984 run.py:483] Algo bellman_ford step 1293 current loss 0.093331, current_train_items 41408.
I0304 19:28:36.682281 22579586809984 run.py:483] Algo bellman_ford step 1294 current loss 0.135984, current_train_items 41440.
I0304 19:28:36.701206 22579586809984 run.py:483] Algo bellman_ford step 1295 current loss 0.017883, current_train_items 41472.
I0304 19:28:36.717417 22579586809984 run.py:483] Algo bellman_ford step 1296 current loss 0.035428, current_train_items 41504.
I0304 19:28:36.741035 22579586809984 run.py:483] Algo bellman_ford step 1297 current loss 0.081672, current_train_items 41536.
I0304 19:28:36.769179 22579586809984 run.py:483] Algo bellman_ford step 1298 current loss 0.085206, current_train_items 41568.
I0304 19:28:36.799171 22579586809984 run.py:483] Algo bellman_ford step 1299 current loss 0.104710, current_train_items 41600.
I0304 19:28:36.818318 22579586809984 run.py:483] Algo bellman_ford step 1300 current loss 0.012175, current_train_items 41632.
I0304 19:28:36.826202 22579586809984 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0304 19:28:36.826306 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.979, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:28:36.856089 22579586809984 run.py:483] Algo bellman_ford step 1301 current loss 0.057074, current_train_items 41664.
I0304 19:28:36.880962 22579586809984 run.py:483] Algo bellman_ford step 1302 current loss 0.103521, current_train_items 41696.
I0304 19:28:36.911770 22579586809984 run.py:483] Algo bellman_ford step 1303 current loss 0.119343, current_train_items 41728.
I0304 19:28:36.943268 22579586809984 run.py:483] Algo bellman_ford step 1304 current loss 0.132028, current_train_items 41760.
I0304 19:28:36.962735 22579586809984 run.py:483] Algo bellman_ford step 1305 current loss 0.006128, current_train_items 41792.
I0304 19:28:36.979087 22579586809984 run.py:483] Algo bellman_ford step 1306 current loss 0.068794, current_train_items 41824.
I0304 19:28:37.002402 22579586809984 run.py:483] Algo bellman_ford step 1307 current loss 0.093473, current_train_items 41856.
I0304 19:28:37.031814 22579586809984 run.py:483] Algo bellman_ford step 1308 current loss 0.092584, current_train_items 41888.
I0304 19:28:37.063884 22579586809984 run.py:483] Algo bellman_ford step 1309 current loss 0.117066, current_train_items 41920.
I0304 19:28:37.083096 22579586809984 run.py:483] Algo bellman_ford step 1310 current loss 0.007727, current_train_items 41952.
I0304 19:28:37.099368 22579586809984 run.py:483] Algo bellman_ford step 1311 current loss 0.057507, current_train_items 41984.
I0304 19:28:37.122545 22579586809984 run.py:483] Algo bellman_ford step 1312 current loss 0.126973, current_train_items 42016.
I0304 19:28:37.153131 22579586809984 run.py:483] Algo bellman_ford step 1313 current loss 0.160251, current_train_items 42048.
I0304 19:28:37.185752 22579586809984 run.py:483] Algo bellman_ford step 1314 current loss 0.121442, current_train_items 42080.
I0304 19:28:37.204827 22579586809984 run.py:483] Algo bellman_ford step 1315 current loss 0.008876, current_train_items 42112.
I0304 19:28:37.220840 22579586809984 run.py:483] Algo bellman_ford step 1316 current loss 0.028142, current_train_items 42144.
I0304 19:28:37.244636 22579586809984 run.py:483] Algo bellman_ford step 1317 current loss 0.200445, current_train_items 42176.
I0304 19:28:37.274693 22579586809984 run.py:483] Algo bellman_ford step 1318 current loss 0.099717, current_train_items 42208.
I0304 19:28:37.306753 22579586809984 run.py:483] Algo bellman_ford step 1319 current loss 0.222666, current_train_items 42240.
I0304 19:28:37.325951 22579586809984 run.py:483] Algo bellman_ford step 1320 current loss 0.024366, current_train_items 42272.
I0304 19:28:37.342244 22579586809984 run.py:483] Algo bellman_ford step 1321 current loss 0.039376, current_train_items 42304.
I0304 19:28:37.366314 22579586809984 run.py:483] Algo bellman_ford step 1322 current loss 0.098094, current_train_items 42336.
I0304 19:28:37.397160 22579586809984 run.py:483] Algo bellman_ford step 1323 current loss 0.093055, current_train_items 42368.
I0304 19:28:37.428770 22579586809984 run.py:483] Algo bellman_ford step 1324 current loss 0.203481, current_train_items 42400.
I0304 19:28:37.447885 22579586809984 run.py:483] Algo bellman_ford step 1325 current loss 0.045058, current_train_items 42432.
I0304 19:28:37.463958 22579586809984 run.py:483] Algo bellman_ford step 1326 current loss 0.038977, current_train_items 42464.
I0304 19:28:37.487045 22579586809984 run.py:483] Algo bellman_ford step 1327 current loss 0.059970, current_train_items 42496.
I0304 19:28:37.517194 22579586809984 run.py:483] Algo bellman_ford step 1328 current loss 0.126909, current_train_items 42528.
I0304 19:28:37.549654 22579586809984 run.py:483] Algo bellman_ford step 1329 current loss 0.133468, current_train_items 42560.
I0304 19:28:37.568632 22579586809984 run.py:483] Algo bellman_ford step 1330 current loss 0.013877, current_train_items 42592.
I0304 19:28:37.584738 22579586809984 run.py:483] Algo bellman_ford step 1331 current loss 0.032261, current_train_items 42624.
I0304 19:28:37.607480 22579586809984 run.py:483] Algo bellman_ford step 1332 current loss 0.108605, current_train_items 42656.
I0304 19:28:37.637237 22579586809984 run.py:483] Algo bellman_ford step 1333 current loss 0.129686, current_train_items 42688.
I0304 19:28:37.668626 22579586809984 run.py:483] Algo bellman_ford step 1334 current loss 0.106224, current_train_items 42720.
I0304 19:28:37.687860 22579586809984 run.py:483] Algo bellman_ford step 1335 current loss 0.013729, current_train_items 42752.
I0304 19:28:37.703717 22579586809984 run.py:483] Algo bellman_ford step 1336 current loss 0.056940, current_train_items 42784.
I0304 19:28:37.728508 22579586809984 run.py:483] Algo bellman_ford step 1337 current loss 0.088831, current_train_items 42816.
I0304 19:28:37.758148 22579586809984 run.py:483] Algo bellman_ford step 1338 current loss 0.150334, current_train_items 42848.
I0304 19:28:37.792638 22579586809984 run.py:483] Algo bellman_ford step 1339 current loss 0.135897, current_train_items 42880.
I0304 19:28:37.811718 22579586809984 run.py:483] Algo bellman_ford step 1340 current loss 0.056946, current_train_items 42912.
I0304 19:28:37.827994 22579586809984 run.py:483] Algo bellman_ford step 1341 current loss 0.028032, current_train_items 42944.
I0304 19:28:37.851528 22579586809984 run.py:483] Algo bellman_ford step 1342 current loss 0.121162, current_train_items 42976.
I0304 19:28:37.880831 22579586809984 run.py:483] Algo bellman_ford step 1343 current loss 0.140161, current_train_items 43008.
I0304 19:28:37.912804 22579586809984 run.py:483] Algo bellman_ford step 1344 current loss 0.148915, current_train_items 43040.
I0304 19:28:37.932413 22579586809984 run.py:483] Algo bellman_ford step 1345 current loss 0.017652, current_train_items 43072.
I0304 19:28:37.948579 22579586809984 run.py:483] Algo bellman_ford step 1346 current loss 0.125853, current_train_items 43104.
I0304 19:28:37.973102 22579586809984 run.py:483] Algo bellman_ford step 1347 current loss 0.285255, current_train_items 43136.
I0304 19:28:38.001653 22579586809984 run.py:483] Algo bellman_ford step 1348 current loss 0.209656, current_train_items 43168.
I0304 19:28:38.032836 22579586809984 run.py:483] Algo bellman_ford step 1349 current loss 0.123967, current_train_items 43200.
I0304 19:28:38.052004 22579586809984 run.py:483] Algo bellman_ford step 1350 current loss 0.074426, current_train_items 43232.
I0304 19:28:38.059938 22579586809984 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0304 19:28:38.060043 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:28:38.077063 22579586809984 run.py:483] Algo bellman_ford step 1351 current loss 0.062820, current_train_items 43264.
I0304 19:28:38.101835 22579586809984 run.py:483] Algo bellman_ford step 1352 current loss 0.107374, current_train_items 43296.
I0304 19:28:38.130224 22579586809984 run.py:483] Algo bellman_ford step 1353 current loss 0.106460, current_train_items 43328.
I0304 19:28:38.164214 22579586809984 run.py:483] Algo bellman_ford step 1354 current loss 0.123002, current_train_items 43360.
I0304 19:28:38.183645 22579586809984 run.py:483] Algo bellman_ford step 1355 current loss 0.013105, current_train_items 43392.
I0304 19:28:38.199497 22579586809984 run.py:483] Algo bellman_ford step 1356 current loss 0.108820, current_train_items 43424.
I0304 19:28:38.222798 22579586809984 run.py:483] Algo bellman_ford step 1357 current loss 0.126726, current_train_items 43456.
I0304 19:28:38.252042 22579586809984 run.py:483] Algo bellman_ford step 1358 current loss 0.141547, current_train_items 43488.
I0304 19:28:38.285392 22579586809984 run.py:483] Algo bellman_ford step 1359 current loss 0.134911, current_train_items 43520.
I0304 19:28:38.304688 22579586809984 run.py:483] Algo bellman_ford step 1360 current loss 0.007190, current_train_items 43552.
I0304 19:28:38.321718 22579586809984 run.py:483] Algo bellman_ford step 1361 current loss 0.164007, current_train_items 43584.
I0304 19:28:38.345399 22579586809984 run.py:483] Algo bellman_ford step 1362 current loss 0.257450, current_train_items 43616.
I0304 19:28:38.374948 22579586809984 run.py:483] Algo bellman_ford step 1363 current loss 0.202766, current_train_items 43648.
I0304 19:28:38.408212 22579586809984 run.py:483] Algo bellman_ford step 1364 current loss 0.171117, current_train_items 43680.
I0304 19:28:38.427362 22579586809984 run.py:483] Algo bellman_ford step 1365 current loss 0.006381, current_train_items 43712.
I0304 19:28:38.443318 22579586809984 run.py:483] Algo bellman_ford step 1366 current loss 0.070344, current_train_items 43744.
I0304 19:28:38.466122 22579586809984 run.py:483] Algo bellman_ford step 1367 current loss 0.104087, current_train_items 43776.
I0304 19:28:38.493991 22579586809984 run.py:483] Algo bellman_ford step 1368 current loss 0.110191, current_train_items 43808.
I0304 19:28:38.526320 22579586809984 run.py:483] Algo bellman_ford step 1369 current loss 0.274933, current_train_items 43840.
I0304 19:28:38.545503 22579586809984 run.py:483] Algo bellman_ford step 1370 current loss 0.006524, current_train_items 43872.
I0304 19:28:38.562165 22579586809984 run.py:483] Algo bellman_ford step 1371 current loss 0.082109, current_train_items 43904.
I0304 19:28:38.585757 22579586809984 run.py:483] Algo bellman_ford step 1372 current loss 0.111308, current_train_items 43936.
I0304 19:28:38.616208 22579586809984 run.py:483] Algo bellman_ford step 1373 current loss 0.123774, current_train_items 43968.
I0304 19:28:38.649083 22579586809984 run.py:483] Algo bellman_ford step 1374 current loss 0.120460, current_train_items 44000.
I0304 19:28:38.668348 22579586809984 run.py:483] Algo bellman_ford step 1375 current loss 0.011939, current_train_items 44032.
I0304 19:28:38.684251 22579586809984 run.py:483] Algo bellman_ford step 1376 current loss 0.061329, current_train_items 44064.
I0304 19:28:38.707247 22579586809984 run.py:483] Algo bellman_ford step 1377 current loss 0.168640, current_train_items 44096.
I0304 19:28:38.737079 22579586809984 run.py:483] Algo bellman_ford step 1378 current loss 0.227483, current_train_items 44128.
I0304 19:28:38.769557 22579586809984 run.py:483] Algo bellman_ford step 1379 current loss 0.143588, current_train_items 44160.
I0304 19:28:38.788760 22579586809984 run.py:483] Algo bellman_ford step 1380 current loss 0.021182, current_train_items 44192.
I0304 19:28:38.805296 22579586809984 run.py:483] Algo bellman_ford step 1381 current loss 0.041346, current_train_items 44224.
I0304 19:28:38.828251 22579586809984 run.py:483] Algo bellman_ford step 1382 current loss 0.081834, current_train_items 44256.
I0304 19:28:38.857649 22579586809984 run.py:483] Algo bellman_ford step 1383 current loss 0.128733, current_train_items 44288.
I0304 19:28:38.887754 22579586809984 run.py:483] Algo bellman_ford step 1384 current loss 0.141750, current_train_items 44320.
I0304 19:28:38.907396 22579586809984 run.py:483] Algo bellman_ford step 1385 current loss 0.006681, current_train_items 44352.
I0304 19:28:38.923287 22579586809984 run.py:483] Algo bellman_ford step 1386 current loss 0.039111, current_train_items 44384.
I0304 19:28:38.945536 22579586809984 run.py:483] Algo bellman_ford step 1387 current loss 0.053228, current_train_items 44416.
I0304 19:28:38.975534 22579586809984 run.py:483] Algo bellman_ford step 1388 current loss 0.114192, current_train_items 44448.
I0304 19:28:39.007627 22579586809984 run.py:483] Algo bellman_ford step 1389 current loss 0.139030, current_train_items 44480.
I0304 19:28:39.026855 22579586809984 run.py:483] Algo bellman_ford step 1390 current loss 0.012079, current_train_items 44512.
I0304 19:28:39.042944 22579586809984 run.py:483] Algo bellman_ford step 1391 current loss 0.032050, current_train_items 44544.
I0304 19:28:39.066856 22579586809984 run.py:483] Algo bellman_ford step 1392 current loss 0.070044, current_train_items 44576.
I0304 19:28:39.096147 22579586809984 run.py:483] Algo bellman_ford step 1393 current loss 0.123254, current_train_items 44608.
I0304 19:28:39.128221 22579586809984 run.py:483] Algo bellman_ford step 1394 current loss 0.109395, current_train_items 44640.
I0304 19:28:39.146935 22579586809984 run.py:483] Algo bellman_ford step 1395 current loss 0.005637, current_train_items 44672.
I0304 19:28:39.162996 22579586809984 run.py:483] Algo bellman_ford step 1396 current loss 0.031067, current_train_items 44704.
I0304 19:28:39.186402 22579586809984 run.py:483] Algo bellman_ford step 1397 current loss 0.096671, current_train_items 44736.
I0304 19:28:39.216882 22579586809984 run.py:483] Algo bellman_ford step 1398 current loss 0.100701, current_train_items 44768.
I0304 19:28:39.248241 22579586809984 run.py:483] Algo bellman_ford step 1399 current loss 0.116794, current_train_items 44800.
I0304 19:28:39.267781 22579586809984 run.py:483] Algo bellman_ford step 1400 current loss 0.006869, current_train_items 44832.
I0304 19:28:39.275538 22579586809984 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0304 19:28:39.275642 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:28:39.292147 22579586809984 run.py:483] Algo bellman_ford step 1401 current loss 0.059274, current_train_items 44864.
I0304 19:28:39.316608 22579586809984 run.py:483] Algo bellman_ford step 1402 current loss 0.145765, current_train_items 44896.
I0304 19:28:39.346895 22579586809984 run.py:483] Algo bellman_ford step 1403 current loss 0.136264, current_train_items 44928.
I0304 19:28:39.380445 22579586809984 run.py:483] Algo bellman_ford step 1404 current loss 0.120948, current_train_items 44960.
I0304 19:28:39.399843 22579586809984 run.py:483] Algo bellman_ford step 1405 current loss 0.008584, current_train_items 44992.
I0304 19:28:39.416251 22579586809984 run.py:483] Algo bellman_ford step 1406 current loss 0.051879, current_train_items 45024.
I0304 19:28:39.439710 22579586809984 run.py:483] Algo bellman_ford step 1407 current loss 0.091719, current_train_items 45056.
I0304 19:28:39.470058 22579586809984 run.py:483] Algo bellman_ford step 1408 current loss 0.142964, current_train_items 45088.
I0304 19:28:39.502484 22579586809984 run.py:483] Algo bellman_ford step 1409 current loss 0.141108, current_train_items 45120.
I0304 19:28:39.521373 22579586809984 run.py:483] Algo bellman_ford step 1410 current loss 0.010134, current_train_items 45152.
I0304 19:28:39.537893 22579586809984 run.py:483] Algo bellman_ford step 1411 current loss 0.028002, current_train_items 45184.
I0304 19:28:39.561193 22579586809984 run.py:483] Algo bellman_ford step 1412 current loss 0.046091, current_train_items 45216.
I0304 19:28:39.588875 22579586809984 run.py:483] Algo bellman_ford step 1413 current loss 0.061198, current_train_items 45248.
I0304 19:28:39.620586 22579586809984 run.py:483] Algo bellman_ford step 1414 current loss 0.121430, current_train_items 45280.
I0304 19:28:39.639820 22579586809984 run.py:483] Algo bellman_ford step 1415 current loss 0.013105, current_train_items 45312.
I0304 19:28:39.656433 22579586809984 run.py:483] Algo bellman_ford step 1416 current loss 0.047491, current_train_items 45344.
I0304 19:28:39.679632 22579586809984 run.py:483] Algo bellman_ford step 1417 current loss 0.030000, current_train_items 45376.
I0304 19:28:39.708168 22579586809984 run.py:483] Algo bellman_ford step 1418 current loss 0.125363, current_train_items 45408.
I0304 19:28:39.740880 22579586809984 run.py:483] Algo bellman_ford step 1419 current loss 0.117771, current_train_items 45440.
I0304 19:28:39.760211 22579586809984 run.py:483] Algo bellman_ford step 1420 current loss 0.031808, current_train_items 45472.
I0304 19:28:39.776579 22579586809984 run.py:483] Algo bellman_ford step 1421 current loss 0.045379, current_train_items 45504.
I0304 19:28:39.800586 22579586809984 run.py:483] Algo bellman_ford step 1422 current loss 0.103192, current_train_items 45536.
I0304 19:28:39.830961 22579586809984 run.py:483] Algo bellman_ford step 1423 current loss 0.139750, current_train_items 45568.
I0304 19:28:39.862286 22579586809984 run.py:483] Algo bellman_ford step 1424 current loss 0.103846, current_train_items 45600.
I0304 19:28:39.881244 22579586809984 run.py:483] Algo bellman_ford step 1425 current loss 0.006843, current_train_items 45632.
I0304 19:28:39.897918 22579586809984 run.py:483] Algo bellman_ford step 1426 current loss 0.103747, current_train_items 45664.
I0304 19:28:39.922175 22579586809984 run.py:483] Algo bellman_ford step 1427 current loss 0.067461, current_train_items 45696.
I0304 19:28:39.951812 22579586809984 run.py:483] Algo bellman_ford step 1428 current loss 0.069523, current_train_items 45728.
I0304 19:28:39.982536 22579586809984 run.py:483] Algo bellman_ford step 1429 current loss 0.102082, current_train_items 45760.
I0304 19:28:40.001228 22579586809984 run.py:483] Algo bellman_ford step 1430 current loss 0.007551, current_train_items 45792.
I0304 19:28:40.017246 22579586809984 run.py:483] Algo bellman_ford step 1431 current loss 0.020364, current_train_items 45824.
I0304 19:28:40.042551 22579586809984 run.py:483] Algo bellman_ford step 1432 current loss 0.104142, current_train_items 45856.
I0304 19:28:40.070500 22579586809984 run.py:483] Algo bellman_ford step 1433 current loss 0.088487, current_train_items 45888.
I0304 19:28:40.101580 22579586809984 run.py:483] Algo bellman_ford step 1434 current loss 0.098182, current_train_items 45920.
I0304 19:28:40.120235 22579586809984 run.py:483] Algo bellman_ford step 1435 current loss 0.008175, current_train_items 45952.
I0304 19:28:40.136418 22579586809984 run.py:483] Algo bellman_ford step 1436 current loss 0.052497, current_train_items 45984.
I0304 19:28:40.159379 22579586809984 run.py:483] Algo bellman_ford step 1437 current loss 0.084841, current_train_items 46016.
I0304 19:28:40.188211 22579586809984 run.py:483] Algo bellman_ford step 1438 current loss 0.162089, current_train_items 46048.
I0304 19:28:40.221316 22579586809984 run.py:483] Algo bellman_ford step 1439 current loss 0.193447, current_train_items 46080.
I0304 19:28:40.240363 22579586809984 run.py:483] Algo bellman_ford step 1440 current loss 0.008044, current_train_items 46112.
I0304 19:28:40.256636 22579586809984 run.py:483] Algo bellman_ford step 1441 current loss 0.039663, current_train_items 46144.
I0304 19:28:40.279595 22579586809984 run.py:483] Algo bellman_ford step 1442 current loss 0.049477, current_train_items 46176.
I0304 19:28:40.309243 22579586809984 run.py:483] Algo bellman_ford step 1443 current loss 0.113533, current_train_items 46208.
I0304 19:28:40.345194 22579586809984 run.py:483] Algo bellman_ford step 1444 current loss 0.135514, current_train_items 46240.
I0304 19:28:40.364170 22579586809984 run.py:483] Algo bellman_ford step 1445 current loss 0.008597, current_train_items 46272.
I0304 19:28:40.379826 22579586809984 run.py:483] Algo bellman_ford step 1446 current loss 0.064410, current_train_items 46304.
I0304 19:28:40.403535 22579586809984 run.py:483] Algo bellman_ford step 1447 current loss 0.078023, current_train_items 46336.
I0304 19:28:40.433202 22579586809984 run.py:483] Algo bellman_ford step 1448 current loss 0.145338, current_train_items 46368.
I0304 19:28:40.464440 22579586809984 run.py:483] Algo bellman_ford step 1449 current loss 0.114735, current_train_items 46400.
I0304 19:28:40.483415 22579586809984 run.py:483] Algo bellman_ford step 1450 current loss 0.007101, current_train_items 46432.
I0304 19:28:40.491495 22579586809984 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0304 19:28:40.491601 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:28:40.508037 22579586809984 run.py:483] Algo bellman_ford step 1451 current loss 0.048503, current_train_items 46464.
I0304 19:28:40.531665 22579586809984 run.py:483] Algo bellman_ford step 1452 current loss 0.079204, current_train_items 46496.
I0304 19:28:40.563015 22579586809984 run.py:483] Algo bellman_ford step 1453 current loss 0.132513, current_train_items 46528.
I0304 19:28:40.596799 22579586809984 run.py:483] Algo bellman_ford step 1454 current loss 0.128055, current_train_items 46560.
I0304 19:28:40.616436 22579586809984 run.py:483] Algo bellman_ford step 1455 current loss 0.012364, current_train_items 46592.
I0304 19:28:40.632532 22579586809984 run.py:483] Algo bellman_ford step 1456 current loss 0.059335, current_train_items 46624.
I0304 19:28:40.655190 22579586809984 run.py:483] Algo bellman_ford step 1457 current loss 0.103926, current_train_items 46656.
I0304 19:28:40.685735 22579586809984 run.py:483] Algo bellman_ford step 1458 current loss 0.183155, current_train_items 46688.
I0304 19:28:40.717774 22579586809984 run.py:483] Algo bellman_ford step 1459 current loss 0.146572, current_train_items 46720.
I0304 19:28:40.737143 22579586809984 run.py:483] Algo bellman_ford step 1460 current loss 0.014909, current_train_items 46752.
I0304 19:28:40.753063 22579586809984 run.py:483] Algo bellman_ford step 1461 current loss 0.085540, current_train_items 46784.
I0304 19:28:40.775816 22579586809984 run.py:483] Algo bellman_ford step 1462 current loss 0.239012, current_train_items 46816.
I0304 19:28:40.805252 22579586809984 run.py:483] Algo bellman_ford step 1463 current loss 0.286962, current_train_items 46848.
I0304 19:28:40.836536 22579586809984 run.py:483] Algo bellman_ford step 1464 current loss 0.162214, current_train_items 46880.
I0304 19:28:40.855834 22579586809984 run.py:483] Algo bellman_ford step 1465 current loss 0.032686, current_train_items 46912.
I0304 19:28:40.872340 22579586809984 run.py:483] Algo bellman_ford step 1466 current loss 0.050455, current_train_items 46944.
I0304 19:28:40.896050 22579586809984 run.py:483] Algo bellman_ford step 1467 current loss 0.134880, current_train_items 46976.
I0304 19:28:40.923940 22579586809984 run.py:483] Algo bellman_ford step 1468 current loss 0.158808, current_train_items 47008.
I0304 19:28:40.954589 22579586809984 run.py:483] Algo bellman_ford step 1469 current loss 0.145319, current_train_items 47040.
I0304 19:28:40.974408 22579586809984 run.py:483] Algo bellman_ford step 1470 current loss 0.007724, current_train_items 47072.
I0304 19:28:40.991189 22579586809984 run.py:483] Algo bellman_ford step 1471 current loss 0.060384, current_train_items 47104.
I0304 19:28:41.014254 22579586809984 run.py:483] Algo bellman_ford step 1472 current loss 0.057846, current_train_items 47136.
I0304 19:28:41.043761 22579586809984 run.py:483] Algo bellman_ford step 1473 current loss 0.187243, current_train_items 47168.
I0304 19:28:41.076738 22579586809984 run.py:483] Algo bellman_ford step 1474 current loss 0.140073, current_train_items 47200.
I0304 19:28:41.096853 22579586809984 run.py:483] Algo bellman_ford step 1475 current loss 0.019447, current_train_items 47232.
I0304 19:28:41.113069 22579586809984 run.py:483] Algo bellman_ford step 1476 current loss 0.018694, current_train_items 47264.
I0304 19:28:41.136209 22579586809984 run.py:483] Algo bellman_ford step 1477 current loss 0.080401, current_train_items 47296.
I0304 19:28:41.165833 22579586809984 run.py:483] Algo bellman_ford step 1478 current loss 0.199054, current_train_items 47328.
I0304 19:28:41.199692 22579586809984 run.py:483] Algo bellman_ford step 1479 current loss 0.157395, current_train_items 47360.
I0304 19:28:41.218544 22579586809984 run.py:483] Algo bellman_ford step 1480 current loss 0.010530, current_train_items 47392.
I0304 19:28:41.234470 22579586809984 run.py:483] Algo bellman_ford step 1481 current loss 0.028448, current_train_items 47424.
I0304 19:28:41.257428 22579586809984 run.py:483] Algo bellman_ford step 1482 current loss 0.159931, current_train_items 47456.
I0304 19:28:41.287446 22579586809984 run.py:483] Algo bellman_ford step 1483 current loss 0.267429, current_train_items 47488.
I0304 19:28:41.317899 22579586809984 run.py:483] Algo bellman_ford step 1484 current loss 0.161905, current_train_items 47520.
I0304 19:28:41.337294 22579586809984 run.py:483] Algo bellman_ford step 1485 current loss 0.018343, current_train_items 47552.
I0304 19:28:41.353196 22579586809984 run.py:483] Algo bellman_ford step 1486 current loss 0.041882, current_train_items 47584.
I0304 19:28:41.377271 22579586809984 run.py:483] Algo bellman_ford step 1487 current loss 0.126419, current_train_items 47616.
I0304 19:28:41.405027 22579586809984 run.py:483] Algo bellman_ford step 1488 current loss 0.160165, current_train_items 47648.
I0304 19:28:41.438265 22579586809984 run.py:483] Algo bellman_ford step 1489 current loss 0.161905, current_train_items 47680.
I0304 19:28:41.457463 22579586809984 run.py:483] Algo bellman_ford step 1490 current loss 0.006968, current_train_items 47712.
I0304 19:28:41.473935 22579586809984 run.py:483] Algo bellman_ford step 1491 current loss 0.044654, current_train_items 47744.
I0304 19:28:41.497698 22579586809984 run.py:483] Algo bellman_ford step 1492 current loss 0.060216, current_train_items 47776.
I0304 19:28:41.527518 22579586809984 run.py:483] Algo bellman_ford step 1493 current loss 0.175155, current_train_items 47808.
I0304 19:28:41.561200 22579586809984 run.py:483] Algo bellman_ford step 1494 current loss 0.169890, current_train_items 47840.
I0304 19:28:41.580387 22579586809984 run.py:483] Algo bellman_ford step 1495 current loss 0.009727, current_train_items 47872.
I0304 19:28:41.597009 22579586809984 run.py:483] Algo bellman_ford step 1496 current loss 0.078086, current_train_items 47904.
I0304 19:28:41.621244 22579586809984 run.py:483] Algo bellman_ford step 1497 current loss 0.135683, current_train_items 47936.
I0304 19:28:41.651602 22579586809984 run.py:483] Algo bellman_ford step 1498 current loss 0.173178, current_train_items 47968.
I0304 19:28:41.685715 22579586809984 run.py:483] Algo bellman_ford step 1499 current loss 0.137542, current_train_items 48000.
I0304 19:28:41.705221 22579586809984 run.py:483] Algo bellman_ford step 1500 current loss 0.005467, current_train_items 48032.
I0304 19:28:41.713318 22579586809984 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0304 19:28:41.713423 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:28:41.731020 22579586809984 run.py:483] Algo bellman_ford step 1501 current loss 0.040041, current_train_items 48064.
I0304 19:28:41.755692 22579586809984 run.py:483] Algo bellman_ford step 1502 current loss 0.111279, current_train_items 48096.
I0304 19:28:41.785379 22579586809984 run.py:483] Algo bellman_ford step 1503 current loss 0.112102, current_train_items 48128.
I0304 19:28:41.817978 22579586809984 run.py:483] Algo bellman_ford step 1504 current loss 0.107599, current_train_items 48160.
I0304 19:28:41.837842 22579586809984 run.py:483] Algo bellman_ford step 1505 current loss 0.010538, current_train_items 48192.
I0304 19:28:41.853775 22579586809984 run.py:483] Algo bellman_ford step 1506 current loss 0.052309, current_train_items 48224.
I0304 19:28:41.876778 22579586809984 run.py:483] Algo bellman_ford step 1507 current loss 0.070289, current_train_items 48256.
I0304 19:28:41.906419 22579586809984 run.py:483] Algo bellman_ford step 1508 current loss 0.149474, current_train_items 48288.
I0304 19:28:41.938771 22579586809984 run.py:483] Algo bellman_ford step 1509 current loss 0.116181, current_train_items 48320.
I0304 19:28:41.958104 22579586809984 run.py:483] Algo bellman_ford step 1510 current loss 0.016156, current_train_items 48352.
I0304 19:28:41.974534 22579586809984 run.py:483] Algo bellman_ford step 1511 current loss 0.051471, current_train_items 48384.
I0304 19:28:41.998643 22579586809984 run.py:483] Algo bellman_ford step 1512 current loss 0.135064, current_train_items 48416.
I0304 19:28:42.027072 22579586809984 run.py:483] Algo bellman_ford step 1513 current loss 0.096287, current_train_items 48448.
I0304 19:28:42.060921 22579586809984 run.py:483] Algo bellman_ford step 1514 current loss 0.267085, current_train_items 48480.
I0304 19:28:42.080175 22579586809984 run.py:483] Algo bellman_ford step 1515 current loss 0.007326, current_train_items 48512.
I0304 19:28:42.096055 22579586809984 run.py:483] Algo bellman_ford step 1516 current loss 0.029635, current_train_items 48544.
I0304 19:28:42.119570 22579586809984 run.py:483] Algo bellman_ford step 1517 current loss 0.148137, current_train_items 48576.
I0304 19:28:42.149891 22579586809984 run.py:483] Algo bellman_ford step 1518 current loss 0.202736, current_train_items 48608.
I0304 19:28:42.181910 22579586809984 run.py:483] Algo bellman_ford step 1519 current loss 0.144191, current_train_items 48640.
I0304 19:28:42.201166 22579586809984 run.py:483] Algo bellman_ford step 1520 current loss 0.008161, current_train_items 48672.
I0304 19:28:42.217063 22579586809984 run.py:483] Algo bellman_ford step 1521 current loss 0.029760, current_train_items 48704.
I0304 19:28:42.241296 22579586809984 run.py:483] Algo bellman_ford step 1522 current loss 0.102749, current_train_items 48736.
I0304 19:28:42.270942 22579586809984 run.py:483] Algo bellman_ford step 1523 current loss 0.183661, current_train_items 48768.
I0304 19:28:42.303135 22579586809984 run.py:483] Algo bellman_ford step 1524 current loss 0.122819, current_train_items 48800.
I0304 19:28:42.322546 22579586809984 run.py:483] Algo bellman_ford step 1525 current loss 0.010317, current_train_items 48832.
I0304 19:28:42.338380 22579586809984 run.py:483] Algo bellman_ford step 1526 current loss 0.030245, current_train_items 48864.
I0304 19:28:42.363011 22579586809984 run.py:483] Algo bellman_ford step 1527 current loss 0.146125, current_train_items 48896.
I0304 19:28:42.392400 22579586809984 run.py:483] Algo bellman_ford step 1528 current loss 0.179225, current_train_items 48928.
I0304 19:28:42.425215 22579586809984 run.py:483] Algo bellman_ford step 1529 current loss 0.179113, current_train_items 48960.
I0304 19:28:42.444193 22579586809984 run.py:483] Algo bellman_ford step 1530 current loss 0.011693, current_train_items 48992.
I0304 19:28:42.460209 22579586809984 run.py:483] Algo bellman_ford step 1531 current loss 0.049624, current_train_items 49024.
I0304 19:28:42.483331 22579586809984 run.py:483] Algo bellman_ford step 1532 current loss 0.071272, current_train_items 49056.
I0304 19:28:42.513198 22579586809984 run.py:483] Algo bellman_ford step 1533 current loss 0.189743, current_train_items 49088.
I0304 19:28:42.546675 22579586809984 run.py:483] Algo bellman_ford step 1534 current loss 0.195706, current_train_items 49120.
I0304 19:28:42.566102 22579586809984 run.py:483] Algo bellman_ford step 1535 current loss 0.024734, current_train_items 49152.
I0304 19:28:42.582627 22579586809984 run.py:483] Algo bellman_ford step 1536 current loss 0.031310, current_train_items 49184.
I0304 19:28:42.606655 22579586809984 run.py:483] Algo bellman_ford step 1537 current loss 0.102540, current_train_items 49216.
I0304 19:28:42.636893 22579586809984 run.py:483] Algo bellman_ford step 1538 current loss 0.134514, current_train_items 49248.
I0304 19:28:42.671751 22579586809984 run.py:483] Algo bellman_ford step 1539 current loss 0.269327, current_train_items 49280.
I0304 19:28:42.691158 22579586809984 run.py:483] Algo bellman_ford step 1540 current loss 0.012949, current_train_items 49312.
I0304 19:28:42.706982 22579586809984 run.py:483] Algo bellman_ford step 1541 current loss 0.051171, current_train_items 49344.
I0304 19:28:42.730673 22579586809984 run.py:483] Algo bellman_ford step 1542 current loss 0.100488, current_train_items 49376.
I0304 19:28:42.759713 22579586809984 run.py:483] Algo bellman_ford step 1543 current loss 0.129465, current_train_items 49408.
I0304 19:28:42.792045 22579586809984 run.py:483] Algo bellman_ford step 1544 current loss 0.211417, current_train_items 49440.
I0304 19:28:42.811458 22579586809984 run.py:483] Algo bellman_ford step 1545 current loss 0.018313, current_train_items 49472.
I0304 19:28:42.827606 22579586809984 run.py:483] Algo bellman_ford step 1546 current loss 0.039167, current_train_items 49504.
I0304 19:28:42.851220 22579586809984 run.py:483] Algo bellman_ford step 1547 current loss 0.094392, current_train_items 49536.
I0304 19:28:42.882066 22579586809984 run.py:483] Algo bellman_ford step 1548 current loss 0.081348, current_train_items 49568.
I0304 19:28:42.916123 22579586809984 run.py:483] Algo bellman_ford step 1549 current loss 0.199285, current_train_items 49600.
I0304 19:28:42.935044 22579586809984 run.py:483] Algo bellman_ford step 1550 current loss 0.029933, current_train_items 49632.
I0304 19:28:42.943025 22579586809984 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0304 19:28:42.943130 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:28:42.960473 22579586809984 run.py:483] Algo bellman_ford step 1551 current loss 0.075376, current_train_items 49664.
I0304 19:28:42.983826 22579586809984 run.py:483] Algo bellman_ford step 1552 current loss 0.092913, current_train_items 49696.
I0304 19:28:43.013891 22579586809984 run.py:483] Algo bellman_ford step 1553 current loss 0.087573, current_train_items 49728.
I0304 19:28:43.044932 22579586809984 run.py:483] Algo bellman_ford step 1554 current loss 0.084061, current_train_items 49760.
I0304 19:28:43.064119 22579586809984 run.py:483] Algo bellman_ford step 1555 current loss 0.006484, current_train_items 49792.
I0304 19:28:43.080051 22579586809984 run.py:483] Algo bellman_ford step 1556 current loss 0.027340, current_train_items 49824.
I0304 19:28:43.103987 22579586809984 run.py:483] Algo bellman_ford step 1557 current loss 0.179340, current_train_items 49856.
I0304 19:28:43.134046 22579586809984 run.py:483] Algo bellman_ford step 1558 current loss 0.112231, current_train_items 49888.
I0304 19:28:43.166707 22579586809984 run.py:483] Algo bellman_ford step 1559 current loss 0.134992, current_train_items 49920.
I0304 19:28:43.186468 22579586809984 run.py:483] Algo bellman_ford step 1560 current loss 0.011741, current_train_items 49952.
I0304 19:28:43.202985 22579586809984 run.py:483] Algo bellman_ford step 1561 current loss 0.026071, current_train_items 49984.
I0304 19:28:43.226104 22579586809984 run.py:483] Algo bellman_ford step 1562 current loss 0.078511, current_train_items 50016.
I0304 19:28:43.256198 22579586809984 run.py:483] Algo bellman_ford step 1563 current loss 0.093753, current_train_items 50048.
I0304 19:28:43.289495 22579586809984 run.py:483] Algo bellman_ford step 1564 current loss 0.114067, current_train_items 50080.
I0304 19:28:43.308291 22579586809984 run.py:483] Algo bellman_ford step 1565 current loss 0.007922, current_train_items 50112.
I0304 19:28:43.324594 22579586809984 run.py:483] Algo bellman_ford step 1566 current loss 0.080884, current_train_items 50144.
I0304 19:28:43.348230 22579586809984 run.py:483] Algo bellman_ford step 1567 current loss 0.229404, current_train_items 50176.
I0304 19:28:43.377422 22579586809984 run.py:483] Algo bellman_ford step 1568 current loss 0.180184, current_train_items 50208.
I0304 19:28:43.411013 22579586809984 run.py:483] Algo bellman_ford step 1569 current loss 0.184287, current_train_items 50240.
I0304 19:28:43.430363 22579586809984 run.py:483] Algo bellman_ford step 1570 current loss 0.007177, current_train_items 50272.
I0304 19:28:43.446888 22579586809984 run.py:483] Algo bellman_ford step 1571 current loss 0.024315, current_train_items 50304.
I0304 19:28:43.471165 22579586809984 run.py:483] Algo bellman_ford step 1572 current loss 0.186039, current_train_items 50336.
I0304 19:28:43.500757 22579586809984 run.py:483] Algo bellman_ford step 1573 current loss 0.196469, current_train_items 50368.
I0304 19:28:43.530860 22579586809984 run.py:483] Algo bellman_ford step 1574 current loss 0.200940, current_train_items 50400.
I0304 19:28:43.550311 22579586809984 run.py:483] Algo bellman_ford step 1575 current loss 0.008638, current_train_items 50432.
I0304 19:28:43.566715 22579586809984 run.py:483] Algo bellman_ford step 1576 current loss 0.030983, current_train_items 50464.
I0304 19:28:43.588899 22579586809984 run.py:483] Algo bellman_ford step 1577 current loss 0.047893, current_train_items 50496.
I0304 19:28:43.619296 22579586809984 run.py:483] Algo bellman_ford step 1578 current loss 0.177790, current_train_items 50528.
I0304 19:28:43.651545 22579586809984 run.py:483] Algo bellman_ford step 1579 current loss 0.081805, current_train_items 50560.
I0304 19:28:43.670649 22579586809984 run.py:483] Algo bellman_ford step 1580 current loss 0.024907, current_train_items 50592.
I0304 19:28:43.686573 22579586809984 run.py:483] Algo bellman_ford step 1581 current loss 0.056359, current_train_items 50624.
I0304 19:28:43.709824 22579586809984 run.py:483] Algo bellman_ford step 1582 current loss 0.060548, current_train_items 50656.
I0304 19:28:43.739316 22579586809984 run.py:483] Algo bellman_ford step 1583 current loss 0.084288, current_train_items 50688.
I0304 19:28:43.772300 22579586809984 run.py:483] Algo bellman_ford step 1584 current loss 0.173577, current_train_items 50720.
I0304 19:28:43.791485 22579586809984 run.py:483] Algo bellman_ford step 1585 current loss 0.006842, current_train_items 50752.
I0304 19:28:43.807471 22579586809984 run.py:483] Algo bellman_ford step 1586 current loss 0.036871, current_train_items 50784.
I0304 19:28:43.831347 22579586809984 run.py:483] Algo bellman_ford step 1587 current loss 0.074320, current_train_items 50816.
I0304 19:28:43.860549 22579586809984 run.py:483] Algo bellman_ford step 1588 current loss 0.072972, current_train_items 50848.
I0304 19:28:43.892576 22579586809984 run.py:483] Algo bellman_ford step 1589 current loss 0.112916, current_train_items 50880.
I0304 19:28:43.911814 22579586809984 run.py:483] Algo bellman_ford step 1590 current loss 0.023512, current_train_items 50912.
I0304 19:28:43.928086 22579586809984 run.py:483] Algo bellman_ford step 1591 current loss 0.039144, current_train_items 50944.
I0304 19:28:43.950736 22579586809984 run.py:483] Algo bellman_ford step 1592 current loss 0.076516, current_train_items 50976.
I0304 19:28:43.981145 22579586809984 run.py:483] Algo bellman_ford step 1593 current loss 0.094716, current_train_items 51008.
I0304 19:28:44.014468 22579586809984 run.py:483] Algo bellman_ford step 1594 current loss 0.101492, current_train_items 51040.
I0304 19:28:44.033658 22579586809984 run.py:483] Algo bellman_ford step 1595 current loss 0.008351, current_train_items 51072.
I0304 19:28:44.049473 22579586809984 run.py:483] Algo bellman_ford step 1596 current loss 0.037350, current_train_items 51104.
I0304 19:28:44.073504 22579586809984 run.py:483] Algo bellman_ford step 1597 current loss 0.066440, current_train_items 51136.
I0304 19:28:44.101990 22579586809984 run.py:483] Algo bellman_ford step 1598 current loss 0.082904, current_train_items 51168.
I0304 19:28:44.134657 22579586809984 run.py:483] Algo bellman_ford step 1599 current loss 0.101848, current_train_items 51200.
I0304 19:28:44.153984 22579586809984 run.py:483] Algo bellman_ford step 1600 current loss 0.018347, current_train_items 51232.
I0304 19:28:44.161812 22579586809984 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.9599609375, 'score': 0.9599609375, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0304 19:28:44.161918 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.960, val scores are: bellman_ford: 0.960
I0304 19:28:44.178648 22579586809984 run.py:483] Algo bellman_ford step 1601 current loss 0.023609, current_train_items 51264.
I0304 19:28:44.203395 22579586809984 run.py:483] Algo bellman_ford step 1602 current loss 0.039379, current_train_items 51296.
I0304 19:28:44.233801 22579586809984 run.py:483] Algo bellman_ford step 1603 current loss 0.092635, current_train_items 51328.
I0304 19:28:44.266896 22579586809984 run.py:483] Algo bellman_ford step 1604 current loss 0.120601, current_train_items 51360.
I0304 19:28:44.286576 22579586809984 run.py:483] Algo bellman_ford step 1605 current loss 0.017363, current_train_items 51392.
I0304 19:28:44.302116 22579586809984 run.py:483] Algo bellman_ford step 1606 current loss 0.039791, current_train_items 51424.
I0304 19:28:44.325261 22579586809984 run.py:483] Algo bellman_ford step 1607 current loss 0.041670, current_train_items 51456.
I0304 19:28:44.355468 22579586809984 run.py:483] Algo bellman_ford step 1608 current loss 0.130974, current_train_items 51488.
I0304 19:28:44.389080 22579586809984 run.py:483] Algo bellman_ford step 1609 current loss 0.178962, current_train_items 51520.
I0304 19:28:44.408124 22579586809984 run.py:483] Algo bellman_ford step 1610 current loss 0.021475, current_train_items 51552.
I0304 19:28:44.424545 22579586809984 run.py:483] Algo bellman_ford step 1611 current loss 0.047292, current_train_items 51584.
I0304 19:28:44.447859 22579586809984 run.py:483] Algo bellman_ford step 1612 current loss 0.089722, current_train_items 51616.
I0304 19:28:44.477668 22579586809984 run.py:483] Algo bellman_ford step 1613 current loss 0.280193, current_train_items 51648.
I0304 19:28:44.509240 22579586809984 run.py:483] Algo bellman_ford step 1614 current loss 0.261046, current_train_items 51680.
I0304 19:28:44.528576 22579586809984 run.py:483] Algo bellman_ford step 1615 current loss 0.008410, current_train_items 51712.
I0304 19:28:44.544931 22579586809984 run.py:483] Algo bellman_ford step 1616 current loss 0.028389, current_train_items 51744.
I0304 19:28:44.568431 22579586809984 run.py:483] Algo bellman_ford step 1617 current loss 0.146690, current_train_items 51776.
I0304 19:28:44.597614 22579586809984 run.py:483] Algo bellman_ford step 1618 current loss 0.083498, current_train_items 51808.
I0304 19:28:44.631166 22579586809984 run.py:483] Algo bellman_ford step 1619 current loss 0.177302, current_train_items 51840.
I0304 19:28:44.650049 22579586809984 run.py:483] Algo bellman_ford step 1620 current loss 0.023980, current_train_items 51872.
I0304 19:28:44.666013 22579586809984 run.py:483] Algo bellman_ford step 1621 current loss 0.043303, current_train_items 51904.
I0304 19:28:44.689841 22579586809984 run.py:483] Algo bellman_ford step 1622 current loss 0.158998, current_train_items 51936.
I0304 19:28:44.719761 22579586809984 run.py:483] Algo bellman_ford step 1623 current loss 0.141163, current_train_items 51968.
I0304 19:28:44.752275 22579586809984 run.py:483] Algo bellman_ford step 1624 current loss 0.148608, current_train_items 52000.
I0304 19:28:44.771291 22579586809984 run.py:483] Algo bellman_ford step 1625 current loss 0.010622, current_train_items 52032.
I0304 19:28:44.787598 22579586809984 run.py:483] Algo bellman_ford step 1626 current loss 0.065235, current_train_items 52064.
I0304 19:28:44.810747 22579586809984 run.py:483] Algo bellman_ford step 1627 current loss 0.132518, current_train_items 52096.
I0304 19:28:44.841560 22579586809984 run.py:483] Algo bellman_ford step 1628 current loss 0.211576, current_train_items 52128.
I0304 19:28:44.873364 22579586809984 run.py:483] Algo bellman_ford step 1629 current loss 0.162172, current_train_items 52160.
I0304 19:28:44.892628 22579586809984 run.py:483] Algo bellman_ford step 1630 current loss 0.009201, current_train_items 52192.
I0304 19:28:44.909313 22579586809984 run.py:483] Algo bellman_ford step 1631 current loss 0.073727, current_train_items 52224.
I0304 19:28:44.933246 22579586809984 run.py:483] Algo bellman_ford step 1632 current loss 0.149582, current_train_items 52256.
I0304 19:28:44.962163 22579586809984 run.py:483] Algo bellman_ford step 1633 current loss 0.094272, current_train_items 52288.
I0304 19:28:44.994656 22579586809984 run.py:483] Algo bellman_ford step 1634 current loss 0.119210, current_train_items 52320.
I0304 19:28:45.013535 22579586809984 run.py:483] Algo bellman_ford step 1635 current loss 0.013163, current_train_items 52352.
I0304 19:28:45.029673 22579586809984 run.py:483] Algo bellman_ford step 1636 current loss 0.082232, current_train_items 52384.
I0304 19:28:45.053284 22579586809984 run.py:483] Algo bellman_ford step 1637 current loss 0.146382, current_train_items 52416.
I0304 19:28:45.083558 22579586809984 run.py:483] Algo bellman_ford step 1638 current loss 0.204492, current_train_items 52448.
I0304 19:28:45.118369 22579586809984 run.py:483] Algo bellman_ford step 1639 current loss 0.143992, current_train_items 52480.
I0304 19:28:45.137168 22579586809984 run.py:483] Algo bellman_ford step 1640 current loss 0.019336, current_train_items 52512.
I0304 19:28:45.152997 22579586809984 run.py:483] Algo bellman_ford step 1641 current loss 0.049635, current_train_items 52544.
I0304 19:28:45.176563 22579586809984 run.py:483] Algo bellman_ford step 1642 current loss 0.163807, current_train_items 52576.
I0304 19:28:45.206075 22579586809984 run.py:483] Algo bellman_ford step 1643 current loss 0.133596, current_train_items 52608.
I0304 19:28:45.237805 22579586809984 run.py:483] Algo bellman_ford step 1644 current loss 0.150632, current_train_items 52640.
I0304 19:28:45.256729 22579586809984 run.py:483] Algo bellman_ford step 1645 current loss 0.024927, current_train_items 52672.
I0304 19:28:45.272840 22579586809984 run.py:483] Algo bellman_ford step 1646 current loss 0.023110, current_train_items 52704.
I0304 19:28:45.297362 22579586809984 run.py:483] Algo bellman_ford step 1647 current loss 0.117577, current_train_items 52736.
I0304 19:28:45.325844 22579586809984 run.py:483] Algo bellman_ford step 1648 current loss 0.114823, current_train_items 52768.
I0304 19:28:45.359219 22579586809984 run.py:483] Algo bellman_ford step 1649 current loss 0.262547, current_train_items 52800.
I0304 19:28:45.378114 22579586809984 run.py:483] Algo bellman_ford step 1650 current loss 0.030980, current_train_items 52832.
I0304 19:28:45.386410 22579586809984 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0304 19:28:45.386515 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:28:45.403917 22579586809984 run.py:483] Algo bellman_ford step 1651 current loss 0.065264, current_train_items 52864.
I0304 19:28:45.428678 22579586809984 run.py:483] Algo bellman_ford step 1652 current loss 0.093567, current_train_items 52896.
I0304 19:28:45.458330 22579586809984 run.py:483] Algo bellman_ford step 1653 current loss 0.151697, current_train_items 52928.
I0304 19:28:45.489827 22579586809984 run.py:483] Algo bellman_ford step 1654 current loss 0.152125, current_train_items 52960.
I0304 19:28:45.509331 22579586809984 run.py:483] Algo bellman_ford step 1655 current loss 0.008748, current_train_items 52992.
I0304 19:28:45.525395 22579586809984 run.py:483] Algo bellman_ford step 1656 current loss 0.066971, current_train_items 53024.
I0304 19:28:45.548766 22579586809984 run.py:483] Algo bellman_ford step 1657 current loss 0.132235, current_train_items 53056.
I0304 19:28:45.577292 22579586809984 run.py:483] Algo bellman_ford step 1658 current loss 0.089399, current_train_items 53088.
I0304 19:28:45.608292 22579586809984 run.py:483] Algo bellman_ford step 1659 current loss 0.157922, current_train_items 53120.
I0304 19:28:45.627698 22579586809984 run.py:483] Algo bellman_ford step 1660 current loss 0.021895, current_train_items 53152.
I0304 19:28:45.643982 22579586809984 run.py:483] Algo bellman_ford step 1661 current loss 0.086256, current_train_items 53184.
I0304 19:28:45.666604 22579586809984 run.py:483] Algo bellman_ford step 1662 current loss 0.163597, current_train_items 53216.
I0304 19:28:45.697545 22579586809984 run.py:483] Algo bellman_ford step 1663 current loss 0.168364, current_train_items 53248.
I0304 19:28:45.730889 22579586809984 run.py:483] Algo bellman_ford step 1664 current loss 0.158061, current_train_items 53280.
I0304 19:28:45.750124 22579586809984 run.py:483] Algo bellman_ford step 1665 current loss 0.019948, current_train_items 53312.
I0304 19:28:45.766400 22579586809984 run.py:483] Algo bellman_ford step 1666 current loss 0.045843, current_train_items 53344.
I0304 19:28:45.789320 22579586809984 run.py:483] Algo bellman_ford step 1667 current loss 0.063800, current_train_items 53376.
I0304 19:28:45.818159 22579586809984 run.py:483] Algo bellman_ford step 1668 current loss 0.085505, current_train_items 53408.
I0304 19:28:45.849032 22579586809984 run.py:483] Algo bellman_ford step 1669 current loss 0.150974, current_train_items 53440.
I0304 19:28:45.868259 22579586809984 run.py:483] Algo bellman_ford step 1670 current loss 0.007642, current_train_items 53472.
I0304 19:28:45.884211 22579586809984 run.py:483] Algo bellman_ford step 1671 current loss 0.034025, current_train_items 53504.
I0304 19:28:45.906927 22579586809984 run.py:483] Algo bellman_ford step 1672 current loss 0.075460, current_train_items 53536.
I0304 19:28:45.937798 22579586809984 run.py:483] Algo bellman_ford step 1673 current loss 0.078896, current_train_items 53568.
I0304 19:28:45.970494 22579586809984 run.py:483] Algo bellman_ford step 1674 current loss 0.099522, current_train_items 53600.
I0304 19:28:45.989677 22579586809984 run.py:483] Algo bellman_ford step 1675 current loss 0.010483, current_train_items 53632.
I0304 19:28:46.006064 22579586809984 run.py:483] Algo bellman_ford step 1676 current loss 0.046266, current_train_items 53664.
I0304 19:28:46.028957 22579586809984 run.py:483] Algo bellman_ford step 1677 current loss 0.085539, current_train_items 53696.
I0304 19:28:46.058308 22579586809984 run.py:483] Algo bellman_ford step 1678 current loss 0.124335, current_train_items 53728.
I0304 19:28:46.091598 22579586809984 run.py:483] Algo bellman_ford step 1679 current loss 0.095649, current_train_items 53760.
I0304 19:28:46.110766 22579586809984 run.py:483] Algo bellman_ford step 1680 current loss 0.011988, current_train_items 53792.
I0304 19:28:46.126689 22579586809984 run.py:483] Algo bellman_ford step 1681 current loss 0.019044, current_train_items 53824.
I0304 19:28:46.149321 22579586809984 run.py:483] Algo bellman_ford step 1682 current loss 0.078127, current_train_items 53856.
I0304 19:28:46.177779 22579586809984 run.py:483] Algo bellman_ford step 1683 current loss 0.071506, current_train_items 53888.
I0304 19:28:46.210032 22579586809984 run.py:483] Algo bellman_ford step 1684 current loss 0.091072, current_train_items 53920.
I0304 19:28:46.229576 22579586809984 run.py:483] Algo bellman_ford step 1685 current loss 0.014963, current_train_items 53952.
I0304 19:28:46.245858 22579586809984 run.py:483] Algo bellman_ford step 1686 current loss 0.048477, current_train_items 53984.
I0304 19:28:46.269123 22579586809984 run.py:483] Algo bellman_ford step 1687 current loss 0.090245, current_train_items 54016.
I0304 19:28:46.298566 22579586809984 run.py:483] Algo bellman_ford step 1688 current loss 0.138904, current_train_items 54048.
I0304 19:28:46.332453 22579586809984 run.py:483] Algo bellman_ford step 1689 current loss 0.124755, current_train_items 54080.
I0304 19:28:46.351747 22579586809984 run.py:483] Algo bellman_ford step 1690 current loss 0.012602, current_train_items 54112.
I0304 19:28:46.368379 22579586809984 run.py:483] Algo bellman_ford step 1691 current loss 0.037827, current_train_items 54144.
I0304 19:28:46.391695 22579586809984 run.py:483] Algo bellman_ford step 1692 current loss 0.067523, current_train_items 54176.
I0304 19:28:46.422036 22579586809984 run.py:483] Algo bellman_ford step 1693 current loss 0.105742, current_train_items 54208.
I0304 19:28:46.454644 22579586809984 run.py:483] Algo bellman_ford step 1694 current loss 0.106400, current_train_items 54240.
I0304 19:28:46.473443 22579586809984 run.py:483] Algo bellman_ford step 1695 current loss 0.016634, current_train_items 54272.
I0304 19:28:46.489855 22579586809984 run.py:483] Algo bellman_ford step 1696 current loss 0.037913, current_train_items 54304.
I0304 19:28:46.511484 22579586809984 run.py:483] Algo bellman_ford step 1697 current loss 0.112216, current_train_items 54336.
I0304 19:28:46.539769 22579586809984 run.py:483] Algo bellman_ford step 1698 current loss 0.123960, current_train_items 54368.
I0304 19:28:46.572613 22579586809984 run.py:483] Algo bellman_ford step 1699 current loss 0.192128, current_train_items 54400.
I0304 19:28:46.592225 22579586809984 run.py:483] Algo bellman_ford step 1700 current loss 0.019947, current_train_items 54432.
I0304 19:28:46.600059 22579586809984 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0304 19:28:46.600164 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:28:46.616987 22579586809984 run.py:483] Algo bellman_ford step 1701 current loss 0.065496, current_train_items 54464.
I0304 19:28:46.640532 22579586809984 run.py:483] Algo bellman_ford step 1702 current loss 0.125884, current_train_items 54496.
I0304 19:28:46.670440 22579586809984 run.py:483] Algo bellman_ford step 1703 current loss 0.113350, current_train_items 54528.
I0304 19:28:46.702537 22579586809984 run.py:483] Algo bellman_ford step 1704 current loss 0.113626, current_train_items 54560.
I0304 19:28:46.722267 22579586809984 run.py:483] Algo bellman_ford step 1705 current loss 0.009657, current_train_items 54592.
I0304 19:28:46.738042 22579586809984 run.py:483] Algo bellman_ford step 1706 current loss 0.023687, current_train_items 54624.
I0304 19:28:46.761538 22579586809984 run.py:483] Algo bellman_ford step 1707 current loss 0.149841, current_train_items 54656.
I0304 19:28:46.791494 22579586809984 run.py:483] Algo bellman_ford step 1708 current loss 0.197229, current_train_items 54688.
I0304 19:28:46.825379 22579586809984 run.py:483] Algo bellman_ford step 1709 current loss 0.179959, current_train_items 54720.
I0304 19:28:46.844399 22579586809984 run.py:483] Algo bellman_ford step 1710 current loss 0.017781, current_train_items 54752.
I0304 19:28:46.860750 22579586809984 run.py:483] Algo bellman_ford step 1711 current loss 0.032522, current_train_items 54784.
I0304 19:28:46.884656 22579586809984 run.py:483] Algo bellman_ford step 1712 current loss 0.103930, current_train_items 54816.
I0304 19:28:46.913689 22579586809984 run.py:483] Algo bellman_ford step 1713 current loss 0.096529, current_train_items 54848.
I0304 19:28:46.945712 22579586809984 run.py:483] Algo bellman_ford step 1714 current loss 0.110840, current_train_items 54880.
I0304 19:28:46.964559 22579586809984 run.py:483] Algo bellman_ford step 1715 current loss 0.008335, current_train_items 54912.
I0304 19:28:46.980392 22579586809984 run.py:483] Algo bellman_ford step 1716 current loss 0.014691, current_train_items 54944.
I0304 19:28:47.004320 22579586809984 run.py:483] Algo bellman_ford step 1717 current loss 0.085880, current_train_items 54976.
I0304 19:28:47.032668 22579586809984 run.py:483] Algo bellman_ford step 1718 current loss 0.154522, current_train_items 55008.
I0304 19:28:47.065142 22579586809984 run.py:483] Algo bellman_ford step 1719 current loss 0.151459, current_train_items 55040.
I0304 19:28:47.084480 22579586809984 run.py:483] Algo bellman_ford step 1720 current loss 0.026814, current_train_items 55072.
I0304 19:28:47.100393 22579586809984 run.py:483] Algo bellman_ford step 1721 current loss 0.060241, current_train_items 55104.
I0304 19:28:47.123712 22579586809984 run.py:483] Algo bellman_ford step 1722 current loss 0.066355, current_train_items 55136.
I0304 19:28:47.153178 22579586809984 run.py:483] Algo bellman_ford step 1723 current loss 0.088255, current_train_items 55168.
I0304 19:28:47.185329 22579586809984 run.py:483] Algo bellman_ford step 1724 current loss 0.103302, current_train_items 55200.
I0304 19:28:47.204489 22579586809984 run.py:483] Algo bellman_ford step 1725 current loss 0.003541, current_train_items 55232.
I0304 19:28:47.220860 22579586809984 run.py:483] Algo bellman_ford step 1726 current loss 0.076789, current_train_items 55264.
I0304 19:28:47.245815 22579586809984 run.py:483] Algo bellman_ford step 1727 current loss 0.095467, current_train_items 55296.
I0304 19:28:47.275345 22579586809984 run.py:483] Algo bellman_ford step 1728 current loss 0.100653, current_train_items 55328.
I0304 19:28:47.307852 22579586809984 run.py:483] Algo bellman_ford step 1729 current loss 0.099607, current_train_items 55360.
I0304 19:28:47.327072 22579586809984 run.py:483] Algo bellman_ford step 1730 current loss 0.007977, current_train_items 55392.
I0304 19:28:47.343311 22579586809984 run.py:483] Algo bellman_ford step 1731 current loss 0.022589, current_train_items 55424.
I0304 19:28:47.366763 22579586809984 run.py:483] Algo bellman_ford step 1732 current loss 0.134313, current_train_items 55456.
I0304 19:28:47.396873 22579586809984 run.py:483] Algo bellman_ford step 1733 current loss 0.163912, current_train_items 55488.
I0304 19:28:47.427185 22579586809984 run.py:483] Algo bellman_ford step 1734 current loss 0.111710, current_train_items 55520.
I0304 19:28:47.446586 22579586809984 run.py:483] Algo bellman_ford step 1735 current loss 0.007908, current_train_items 55552.
I0304 19:28:47.463023 22579586809984 run.py:483] Algo bellman_ford step 1736 current loss 0.035789, current_train_items 55584.
I0304 19:28:47.487074 22579586809984 run.py:483] Algo bellman_ford step 1737 current loss 0.134313, current_train_items 55616.
I0304 19:28:47.516898 22579586809984 run.py:483] Algo bellman_ford step 1738 current loss 0.120846, current_train_items 55648.
I0304 19:28:47.549804 22579586809984 run.py:483] Algo bellman_ford step 1739 current loss 0.130667, current_train_items 55680.
I0304 19:28:47.568694 22579586809984 run.py:483] Algo bellman_ford step 1740 current loss 0.008863, current_train_items 55712.
I0304 19:28:47.585022 22579586809984 run.py:483] Algo bellman_ford step 1741 current loss 0.030348, current_train_items 55744.
I0304 19:28:47.608746 22579586809984 run.py:483] Algo bellman_ford step 1742 current loss 0.080267, current_train_items 55776.
I0304 19:28:47.639002 22579586809984 run.py:483] Algo bellman_ford step 1743 current loss 0.072195, current_train_items 55808.
I0304 19:28:47.670384 22579586809984 run.py:483] Algo bellman_ford step 1744 current loss 0.109255, current_train_items 55840.
I0304 19:28:47.689890 22579586809984 run.py:483] Algo bellman_ford step 1745 current loss 0.008512, current_train_items 55872.
I0304 19:28:47.706349 22579586809984 run.py:483] Algo bellman_ford step 1746 current loss 0.039541, current_train_items 55904.
I0304 19:28:47.729941 22579586809984 run.py:483] Algo bellman_ford step 1747 current loss 0.081284, current_train_items 55936.
I0304 19:28:47.759712 22579586809984 run.py:483] Algo bellman_ford step 1748 current loss 0.089529, current_train_items 55968.
I0304 19:28:47.790624 22579586809984 run.py:483] Algo bellman_ford step 1749 current loss 0.086373, current_train_items 56000.
I0304 19:28:47.810080 22579586809984 run.py:483] Algo bellman_ford step 1750 current loss 0.027002, current_train_items 56032.
I0304 19:28:47.818452 22579586809984 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0304 19:28:47.818557 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:28:47.835575 22579586809984 run.py:483] Algo bellman_ford step 1751 current loss 0.068620, current_train_items 56064.
I0304 19:28:47.859388 22579586809984 run.py:483] Algo bellman_ford step 1752 current loss 0.100116, current_train_items 56096.
I0304 19:28:47.890916 22579586809984 run.py:483] Algo bellman_ford step 1753 current loss 0.121393, current_train_items 56128.
I0304 19:28:47.925790 22579586809984 run.py:483] Algo bellman_ford step 1754 current loss 0.155622, current_train_items 56160.
I0304 19:28:47.945493 22579586809984 run.py:483] Algo bellman_ford step 1755 current loss 0.004654, current_train_items 56192.
I0304 19:28:47.961732 22579586809984 run.py:483] Algo bellman_ford step 1756 current loss 0.031250, current_train_items 56224.
I0304 19:28:47.986077 22579586809984 run.py:483] Algo bellman_ford step 1757 current loss 0.223828, current_train_items 56256.
I0304 19:28:48.016556 22579586809984 run.py:483] Algo bellman_ford step 1758 current loss 0.142138, current_train_items 56288.
I0304 19:28:48.047622 22579586809984 run.py:483] Algo bellman_ford step 1759 current loss 0.117099, current_train_items 56320.
I0304 19:28:48.067008 22579586809984 run.py:483] Algo bellman_ford step 1760 current loss 0.005334, current_train_items 56352.
I0304 19:28:48.083280 22579586809984 run.py:483] Algo bellman_ford step 1761 current loss 0.024813, current_train_items 56384.
I0304 19:28:48.107512 22579586809984 run.py:483] Algo bellman_ford step 1762 current loss 0.132282, current_train_items 56416.
I0304 19:28:48.139043 22579586809984 run.py:483] Algo bellman_ford step 1763 current loss 0.164808, current_train_items 56448.
I0304 19:28:48.173591 22579586809984 run.py:483] Algo bellman_ford step 1764 current loss 0.211713, current_train_items 56480.
I0304 19:28:48.192845 22579586809984 run.py:483] Algo bellman_ford step 1765 current loss 0.006145, current_train_items 56512.
I0304 19:28:48.208841 22579586809984 run.py:483] Algo bellman_ford step 1766 current loss 0.014948, current_train_items 56544.
I0304 19:28:48.232020 22579586809984 run.py:483] Algo bellman_ford step 1767 current loss 0.050458, current_train_items 56576.
I0304 19:28:48.261770 22579586809984 run.py:483] Algo bellman_ford step 1768 current loss 0.060581, current_train_items 56608.
I0304 19:28:48.293045 22579586809984 run.py:483] Algo bellman_ford step 1769 current loss 0.121813, current_train_items 56640.
I0304 19:28:48.312631 22579586809984 run.py:483] Algo bellman_ford step 1770 current loss 0.017315, current_train_items 56672.
I0304 19:28:48.329175 22579586809984 run.py:483] Algo bellman_ford step 1771 current loss 0.044903, current_train_items 56704.
I0304 19:28:48.353316 22579586809984 run.py:483] Algo bellman_ford step 1772 current loss 0.079786, current_train_items 56736.
I0304 19:28:48.383648 22579586809984 run.py:483] Algo bellman_ford step 1773 current loss 0.091848, current_train_items 56768.
I0304 19:28:48.415208 22579586809984 run.py:483] Algo bellman_ford step 1774 current loss 0.122368, current_train_items 56800.
I0304 19:28:48.434544 22579586809984 run.py:483] Algo bellman_ford step 1775 current loss 0.005096, current_train_items 56832.
I0304 19:28:48.451543 22579586809984 run.py:483] Algo bellman_ford step 1776 current loss 0.034111, current_train_items 56864.
I0304 19:28:48.475246 22579586809984 run.py:483] Algo bellman_ford step 1777 current loss 0.108647, current_train_items 56896.
I0304 19:28:48.504833 22579586809984 run.py:483] Algo bellman_ford step 1778 current loss 0.119275, current_train_items 56928.
I0304 19:28:48.535370 22579586809984 run.py:483] Algo bellman_ford step 1779 current loss 0.106064, current_train_items 56960.
I0304 19:28:48.554480 22579586809984 run.py:483] Algo bellman_ford step 1780 current loss 0.007832, current_train_items 56992.
I0304 19:28:48.570551 22579586809984 run.py:483] Algo bellman_ford step 1781 current loss 0.031585, current_train_items 57024.
I0304 19:28:48.593834 22579586809984 run.py:483] Algo bellman_ford step 1782 current loss 0.094413, current_train_items 57056.
I0304 19:28:48.623663 22579586809984 run.py:483] Algo bellman_ford step 1783 current loss 0.148771, current_train_items 57088.
I0304 19:28:48.655557 22579586809984 run.py:483] Algo bellman_ford step 1784 current loss 0.095201, current_train_items 57120.
I0304 19:28:48.674928 22579586809984 run.py:483] Algo bellman_ford step 1785 current loss 0.004160, current_train_items 57152.
I0304 19:28:48.691465 22579586809984 run.py:483] Algo bellman_ford step 1786 current loss 0.058149, current_train_items 57184.
I0304 19:28:48.714423 22579586809984 run.py:483] Algo bellman_ford step 1787 current loss 0.074272, current_train_items 57216.
I0304 19:28:48.743418 22579586809984 run.py:483] Algo bellman_ford step 1788 current loss 0.108996, current_train_items 57248.
I0304 19:28:48.775710 22579586809984 run.py:483] Algo bellman_ford step 1789 current loss 0.170793, current_train_items 57280.
I0304 19:28:48.795140 22579586809984 run.py:483] Algo bellman_ford step 1790 current loss 0.005049, current_train_items 57312.
I0304 19:28:48.811993 22579586809984 run.py:483] Algo bellman_ford step 1791 current loss 0.061153, current_train_items 57344.
I0304 19:28:48.835687 22579586809984 run.py:483] Algo bellman_ford step 1792 current loss 0.068891, current_train_items 57376.
I0304 19:28:48.864379 22579586809984 run.py:483] Algo bellman_ford step 1793 current loss 0.089418, current_train_items 57408.
I0304 19:28:48.896581 22579586809984 run.py:483] Algo bellman_ford step 1794 current loss 0.148379, current_train_items 57440.
I0304 19:28:48.915936 22579586809984 run.py:483] Algo bellman_ford step 1795 current loss 0.009778, current_train_items 57472.
I0304 19:28:48.932077 22579586809984 run.py:483] Algo bellman_ford step 1796 current loss 0.034062, current_train_items 57504.
I0304 19:28:48.956268 22579586809984 run.py:483] Algo bellman_ford step 1797 current loss 0.125260, current_train_items 57536.
I0304 19:28:48.986408 22579586809984 run.py:483] Algo bellman_ford step 1798 current loss 0.171892, current_train_items 57568.
I0304 19:28:49.019833 22579586809984 run.py:483] Algo bellman_ford step 1799 current loss 0.150791, current_train_items 57600.
I0304 19:28:49.039633 22579586809984 run.py:483] Algo bellman_ford step 1800 current loss 0.015390, current_train_items 57632.
I0304 19:28:49.047228 22579586809984 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0304 19:28:49.047332 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:28:49.063980 22579586809984 run.py:483] Algo bellman_ford step 1801 current loss 0.015062, current_train_items 57664.
I0304 19:28:49.088418 22579586809984 run.py:483] Algo bellman_ford step 1802 current loss 0.058091, current_train_items 57696.
I0304 19:28:49.117665 22579586809984 run.py:483] Algo bellman_ford step 1803 current loss 0.077329, current_train_items 57728.
I0304 19:28:49.149257 22579586809984 run.py:483] Algo bellman_ford step 1804 current loss 0.229053, current_train_items 57760.
I0304 19:28:49.168783 22579586809984 run.py:483] Algo bellman_ford step 1805 current loss 0.006175, current_train_items 57792.
I0304 19:28:49.184565 22579586809984 run.py:483] Algo bellman_ford step 1806 current loss 0.013684, current_train_items 57824.
I0304 19:28:49.207718 22579586809984 run.py:483] Algo bellman_ford step 1807 current loss 0.051658, current_train_items 57856.
I0304 19:28:49.236225 22579586809984 run.py:483] Algo bellman_ford step 1808 current loss 0.076928, current_train_items 57888.
I0304 19:28:49.269654 22579586809984 run.py:483] Algo bellman_ford step 1809 current loss 0.104433, current_train_items 57920.
I0304 19:28:49.288625 22579586809984 run.py:483] Algo bellman_ford step 1810 current loss 0.005310, current_train_items 57952.
I0304 19:28:49.304698 22579586809984 run.py:483] Algo bellman_ford step 1811 current loss 0.035693, current_train_items 57984.
I0304 19:28:49.328475 22579586809984 run.py:483] Algo bellman_ford step 1812 current loss 0.090918, current_train_items 58016.
I0304 19:28:49.356288 22579586809984 run.py:483] Algo bellman_ford step 1813 current loss 0.074728, current_train_items 58048.
I0304 19:28:49.388557 22579586809984 run.py:483] Algo bellman_ford step 1814 current loss 0.120407, current_train_items 58080.
I0304 19:28:49.407963 22579586809984 run.py:483] Algo bellman_ford step 1815 current loss 0.005106, current_train_items 58112.
I0304 19:28:49.424570 22579586809984 run.py:483] Algo bellman_ford step 1816 current loss 0.087547, current_train_items 58144.
I0304 19:28:49.447929 22579586809984 run.py:483] Algo bellman_ford step 1817 current loss 0.092820, current_train_items 58176.
I0304 19:28:49.477599 22579586809984 run.py:483] Algo bellman_ford step 1818 current loss 0.093437, current_train_items 58208.
I0304 19:28:49.509952 22579586809984 run.py:483] Algo bellman_ford step 1819 current loss 0.115288, current_train_items 58240.
I0304 19:28:49.529229 22579586809984 run.py:483] Algo bellman_ford step 1820 current loss 0.027355, current_train_items 58272.
I0304 19:28:49.545918 22579586809984 run.py:483] Algo bellman_ford step 1821 current loss 0.089993, current_train_items 58304.
I0304 19:28:49.571506 22579586809984 run.py:483] Algo bellman_ford step 1822 current loss 0.156472, current_train_items 58336.
I0304 19:28:49.601603 22579586809984 run.py:483] Algo bellman_ford step 1823 current loss 0.131568, current_train_items 58368.
I0304 19:28:49.632971 22579586809984 run.py:483] Algo bellman_ford step 1824 current loss 0.127720, current_train_items 58400.
I0304 19:28:49.652111 22579586809984 run.py:483] Algo bellman_ford step 1825 current loss 0.006516, current_train_items 58432.
I0304 19:28:49.668533 22579586809984 run.py:483] Algo bellman_ford step 1826 current loss 0.040550, current_train_items 58464.
I0304 19:28:49.691737 22579586809984 run.py:483] Algo bellman_ford step 1827 current loss 0.099494, current_train_items 58496.
I0304 19:28:49.722413 22579586809984 run.py:483] Algo bellman_ford step 1828 current loss 0.306683, current_train_items 58528.
I0304 19:28:49.757089 22579586809984 run.py:483] Algo bellman_ford step 1829 current loss 0.327371, current_train_items 58560.
I0304 19:28:49.776518 22579586809984 run.py:483] Algo bellman_ford step 1830 current loss 0.007364, current_train_items 58592.
I0304 19:28:49.793082 22579586809984 run.py:483] Algo bellman_ford step 1831 current loss 0.146598, current_train_items 58624.
I0304 19:28:49.818043 22579586809984 run.py:483] Algo bellman_ford step 1832 current loss 0.075119, current_train_items 58656.
I0304 19:28:49.847785 22579586809984 run.py:483] Algo bellman_ford step 1833 current loss 0.148652, current_train_items 58688.
I0304 19:28:49.879805 22579586809984 run.py:483] Algo bellman_ford step 1834 current loss 0.126292, current_train_items 58720.
I0304 19:28:49.899097 22579586809984 run.py:483] Algo bellman_ford step 1835 current loss 0.009194, current_train_items 58752.
I0304 19:28:49.915641 22579586809984 run.py:483] Algo bellman_ford step 1836 current loss 0.047393, current_train_items 58784.
I0304 19:28:49.940588 22579586809984 run.py:483] Algo bellman_ford step 1837 current loss 0.083135, current_train_items 58816.
I0304 19:28:49.970107 22579586809984 run.py:483] Algo bellman_ford step 1838 current loss 0.131771, current_train_items 58848.
I0304 19:28:50.002850 22579586809984 run.py:483] Algo bellman_ford step 1839 current loss 0.123521, current_train_items 58880.
I0304 19:28:50.022325 22579586809984 run.py:483] Algo bellman_ford step 1840 current loss 0.007058, current_train_items 58912.
I0304 19:28:50.038779 22579586809984 run.py:483] Algo bellman_ford step 1841 current loss 0.032319, current_train_items 58944.
I0304 19:28:50.062752 22579586809984 run.py:483] Algo bellman_ford step 1842 current loss 0.096434, current_train_items 58976.
I0304 19:28:50.093174 22579586809984 run.py:483] Algo bellman_ford step 1843 current loss 0.095507, current_train_items 59008.
I0304 19:28:50.125462 22579586809984 run.py:483] Algo bellman_ford step 1844 current loss 0.092557, current_train_items 59040.
I0304 19:28:50.144565 22579586809984 run.py:483] Algo bellman_ford step 1845 current loss 0.006941, current_train_items 59072.
I0304 19:28:50.160699 22579586809984 run.py:483] Algo bellman_ford step 1846 current loss 0.026447, current_train_items 59104.
I0304 19:28:50.184346 22579586809984 run.py:483] Algo bellman_ford step 1847 current loss 0.129416, current_train_items 59136.
I0304 19:28:50.214345 22579586809984 run.py:483] Algo bellman_ford step 1848 current loss 0.097909, current_train_items 59168.
I0304 19:28:50.247990 22579586809984 run.py:483] Algo bellman_ford step 1849 current loss 0.141963, current_train_items 59200.
I0304 19:28:50.267098 22579586809984 run.py:483] Algo bellman_ford step 1850 current loss 0.007658, current_train_items 59232.
I0304 19:28:50.274956 22579586809984 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0304 19:28:50.275061 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0304 19:28:50.291937 22579586809984 run.py:483] Algo bellman_ford step 1851 current loss 0.019840, current_train_items 59264.
I0304 19:28:50.316415 22579586809984 run.py:483] Algo bellman_ford step 1852 current loss 0.122973, current_train_items 59296.
I0304 19:28:50.346603 22579586809984 run.py:483] Algo bellman_ford step 1853 current loss 0.097093, current_train_items 59328.
I0304 19:28:50.379090 22579586809984 run.py:483] Algo bellman_ford step 1854 current loss 0.131566, current_train_items 59360.
I0304 19:28:50.398484 22579586809984 run.py:483] Algo bellman_ford step 1855 current loss 0.033935, current_train_items 59392.
I0304 19:28:50.413991 22579586809984 run.py:483] Algo bellman_ford step 1856 current loss 0.024301, current_train_items 59424.
I0304 19:28:50.437793 22579586809984 run.py:483] Algo bellman_ford step 1857 current loss 0.093689, current_train_items 59456.
I0304 19:28:50.466423 22579586809984 run.py:483] Algo bellman_ford step 1858 current loss 0.108601, current_train_items 59488.
I0304 19:28:50.499133 22579586809984 run.py:483] Algo bellman_ford step 1859 current loss 0.126560, current_train_items 59520.
I0304 19:28:50.518387 22579586809984 run.py:483] Algo bellman_ford step 1860 current loss 0.005699, current_train_items 59552.
I0304 19:28:50.535104 22579586809984 run.py:483] Algo bellman_ford step 1861 current loss 0.028839, current_train_items 59584.
I0304 19:28:50.558820 22579586809984 run.py:483] Algo bellman_ford step 1862 current loss 0.067079, current_train_items 59616.
I0304 19:28:50.588176 22579586809984 run.py:483] Algo bellman_ford step 1863 current loss 0.080237, current_train_items 59648.
I0304 19:28:50.620962 22579586809984 run.py:483] Algo bellman_ford step 1864 current loss 0.172729, current_train_items 59680.
I0304 19:28:50.639994 22579586809984 run.py:483] Algo bellman_ford step 1865 current loss 0.006554, current_train_items 59712.
I0304 19:28:50.656220 22579586809984 run.py:483] Algo bellman_ford step 1866 current loss 0.033653, current_train_items 59744.
I0304 19:28:50.680538 22579586809984 run.py:483] Algo bellman_ford step 1867 current loss 0.094738, current_train_items 59776.
I0304 19:28:50.710329 22579586809984 run.py:483] Algo bellman_ford step 1868 current loss 0.118181, current_train_items 59808.
I0304 19:28:50.742163 22579586809984 run.py:483] Algo bellman_ford step 1869 current loss 0.111334, current_train_items 59840.
I0304 19:28:50.761293 22579586809984 run.py:483] Algo bellman_ford step 1870 current loss 0.004960, current_train_items 59872.
I0304 19:28:50.777464 22579586809984 run.py:483] Algo bellman_ford step 1871 current loss 0.020077, current_train_items 59904.
I0304 19:28:50.800312 22579586809984 run.py:483] Algo bellman_ford step 1872 current loss 0.026904, current_train_items 59936.
I0304 19:28:50.830317 22579586809984 run.py:483] Algo bellman_ford step 1873 current loss 0.108458, current_train_items 59968.
I0304 19:28:50.862081 22579586809984 run.py:483] Algo bellman_ford step 1874 current loss 0.100207, current_train_items 60000.
I0304 19:28:50.881231 22579586809984 run.py:483] Algo bellman_ford step 1875 current loss 0.032562, current_train_items 60032.
I0304 19:28:50.897543 22579586809984 run.py:483] Algo bellman_ford step 1876 current loss 0.022011, current_train_items 60064.
I0304 19:28:50.921016 22579586809984 run.py:483] Algo bellman_ford step 1877 current loss 0.121374, current_train_items 60096.
I0304 19:28:50.949355 22579586809984 run.py:483] Algo bellman_ford step 1878 current loss 0.091574, current_train_items 60128.
I0304 19:28:50.983917 22579586809984 run.py:483] Algo bellman_ford step 1879 current loss 0.207108, current_train_items 60160.
I0304 19:28:51.002938 22579586809984 run.py:483] Algo bellman_ford step 1880 current loss 0.005688, current_train_items 60192.
I0304 19:28:51.019412 22579586809984 run.py:483] Algo bellman_ford step 1881 current loss 0.033097, current_train_items 60224.
I0304 19:28:51.042439 22579586809984 run.py:483] Algo bellman_ford step 1882 current loss 0.067512, current_train_items 60256.
I0304 19:28:51.072093 22579586809984 run.py:483] Algo bellman_ford step 1883 current loss 0.092845, current_train_items 60288.
I0304 19:28:51.104475 22579586809984 run.py:483] Algo bellman_ford step 1884 current loss 0.152580, current_train_items 60320.
I0304 19:28:51.123497 22579586809984 run.py:483] Algo bellman_ford step 1885 current loss 0.033941, current_train_items 60352.
I0304 19:28:51.140173 22579586809984 run.py:483] Algo bellman_ford step 1886 current loss 0.049335, current_train_items 60384.
I0304 19:28:51.163469 22579586809984 run.py:483] Algo bellman_ford step 1887 current loss 0.070120, current_train_items 60416.
I0304 19:28:51.192043 22579586809984 run.py:483] Algo bellman_ford step 1888 current loss 0.081326, current_train_items 60448.
I0304 19:28:51.225898 22579586809984 run.py:483] Algo bellman_ford step 1889 current loss 0.117371, current_train_items 60480.
I0304 19:28:51.245381 22579586809984 run.py:483] Algo bellman_ford step 1890 current loss 0.006170, current_train_items 60512.
I0304 19:28:51.261758 22579586809984 run.py:483] Algo bellman_ford step 1891 current loss 0.056610, current_train_items 60544.
I0304 19:28:51.284423 22579586809984 run.py:483] Algo bellman_ford step 1892 current loss 0.074460, current_train_items 60576.
I0304 19:28:51.314837 22579586809984 run.py:483] Algo bellman_ford step 1893 current loss 0.115246, current_train_items 60608.
I0304 19:28:51.349836 22579586809984 run.py:483] Algo bellman_ford step 1894 current loss 0.121577, current_train_items 60640.
I0304 19:28:51.368783 22579586809984 run.py:483] Algo bellman_ford step 1895 current loss 0.012564, current_train_items 60672.
I0304 19:28:51.384938 22579586809984 run.py:483] Algo bellman_ford step 1896 current loss 0.028966, current_train_items 60704.
I0304 19:28:51.408379 22579586809984 run.py:483] Algo bellman_ford step 1897 current loss 0.124660, current_train_items 60736.
I0304 19:28:51.438482 22579586809984 run.py:483] Algo bellman_ford step 1898 current loss 0.110977, current_train_items 60768.
I0304 19:28:51.470953 22579586809984 run.py:483] Algo bellman_ford step 1899 current loss 0.144431, current_train_items 60800.
I0304 19:28:51.490434 22579586809984 run.py:483] Algo bellman_ford step 1900 current loss 0.007372, current_train_items 60832.
I0304 19:28:51.498084 22579586809984 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.9482421875, 'score': 0.9482421875, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0304 19:28:51.498190 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.948, val scores are: bellman_ford: 0.948
I0304 19:28:51.515056 22579586809984 run.py:483] Algo bellman_ford step 1901 current loss 0.032934, current_train_items 60864.
I0304 19:28:51.538646 22579586809984 run.py:483] Algo bellman_ford step 1902 current loss 0.118877, current_train_items 60896.
I0304 19:28:51.567129 22579586809984 run.py:483] Algo bellman_ford step 1903 current loss 0.121648, current_train_items 60928.
I0304 19:28:51.599323 22579586809984 run.py:483] Algo bellman_ford step 1904 current loss 0.275851, current_train_items 60960.
I0304 19:28:51.618919 22579586809984 run.py:483] Algo bellman_ford step 1905 current loss 0.006740, current_train_items 60992.
I0304 19:28:51.634828 22579586809984 run.py:483] Algo bellman_ford step 1906 current loss 0.032277, current_train_items 61024.
I0304 19:28:51.657416 22579586809984 run.py:483] Algo bellman_ford step 1907 current loss 0.107884, current_train_items 61056.
I0304 19:28:51.686770 22579586809984 run.py:483] Algo bellman_ford step 1908 current loss 0.118395, current_train_items 61088.
I0304 19:28:51.718487 22579586809984 run.py:483] Algo bellman_ford step 1909 current loss 0.111031, current_train_items 61120.
I0304 19:28:51.737357 22579586809984 run.py:483] Algo bellman_ford step 1910 current loss 0.008981, current_train_items 61152.
I0304 19:28:51.753791 22579586809984 run.py:483] Algo bellman_ford step 1911 current loss 0.031411, current_train_items 61184.
I0304 19:28:51.777669 22579586809984 run.py:483] Algo bellman_ford step 1912 current loss 0.074898, current_train_items 61216.
I0304 19:28:51.806957 22579586809984 run.py:483] Algo bellman_ford step 1913 current loss 0.068173, current_train_items 61248.
I0304 19:28:51.839562 22579586809984 run.py:483] Algo bellman_ford step 1914 current loss 0.094595, current_train_items 61280.
I0304 19:28:51.858337 22579586809984 run.py:483] Algo bellman_ford step 1915 current loss 0.004014, current_train_items 61312.
I0304 19:28:51.874795 22579586809984 run.py:483] Algo bellman_ford step 1916 current loss 0.024095, current_train_items 61344.
I0304 19:28:51.899810 22579586809984 run.py:483] Algo bellman_ford step 1917 current loss 0.066762, current_train_items 61376.
I0304 19:28:51.929877 22579586809984 run.py:483] Algo bellman_ford step 1918 current loss 0.060540, current_train_items 61408.
I0304 19:28:51.963625 22579586809984 run.py:483] Algo bellman_ford step 1919 current loss 0.121457, current_train_items 61440.
I0304 19:28:51.982506 22579586809984 run.py:483] Algo bellman_ford step 1920 current loss 0.007729, current_train_items 61472.
I0304 19:28:51.999031 22579586809984 run.py:483] Algo bellman_ford step 1921 current loss 0.069787, current_train_items 61504.
I0304 19:28:52.022051 22579586809984 run.py:483] Algo bellman_ford step 1922 current loss 0.098425, current_train_items 61536.
I0304 19:28:52.050630 22579586809984 run.py:483] Algo bellman_ford step 1923 current loss 0.113234, current_train_items 61568.
I0304 19:28:52.082928 22579586809984 run.py:483] Algo bellman_ford step 1924 current loss 0.142697, current_train_items 61600.
I0304 19:28:52.102055 22579586809984 run.py:483] Algo bellman_ford step 1925 current loss 0.005220, current_train_items 61632.
I0304 19:28:52.118341 22579586809984 run.py:483] Algo bellman_ford step 1926 current loss 0.045300, current_train_items 61664.
I0304 19:28:52.140688 22579586809984 run.py:483] Algo bellman_ford step 1927 current loss 0.038281, current_train_items 61696.
I0304 19:28:52.169474 22579586809984 run.py:483] Algo bellman_ford step 1928 current loss 0.072999, current_train_items 61728.
I0304 19:28:52.200639 22579586809984 run.py:483] Algo bellman_ford step 1929 current loss 0.152416, current_train_items 61760.
I0304 19:28:52.219749 22579586809984 run.py:483] Algo bellman_ford step 1930 current loss 0.016013, current_train_items 61792.
I0304 19:28:52.235651 22579586809984 run.py:483] Algo bellman_ford step 1931 current loss 0.048149, current_train_items 61824.
I0304 19:28:52.260385 22579586809984 run.py:483] Algo bellman_ford step 1932 current loss 0.103741, current_train_items 61856.
I0304 19:28:52.289860 22579586809984 run.py:483] Algo bellman_ford step 1933 current loss 0.154247, current_train_items 61888.
I0304 19:28:52.320584 22579586809984 run.py:483] Algo bellman_ford step 1934 current loss 0.106555, current_train_items 61920.
I0304 19:28:52.339370 22579586809984 run.py:483] Algo bellman_ford step 1935 current loss 0.024424, current_train_items 61952.
I0304 19:28:52.355858 22579586809984 run.py:483] Algo bellman_ford step 1936 current loss 0.040272, current_train_items 61984.
I0304 19:28:52.379809 22579586809984 run.py:483] Algo bellman_ford step 1937 current loss 0.131657, current_train_items 62016.
I0304 19:28:52.408670 22579586809984 run.py:483] Algo bellman_ford step 1938 current loss 0.176800, current_train_items 62048.
I0304 19:28:52.441407 22579586809984 run.py:483] Algo bellman_ford step 1939 current loss 0.219107, current_train_items 62080.
I0304 19:28:52.460595 22579586809984 run.py:483] Algo bellman_ford step 1940 current loss 0.029564, current_train_items 62112.
I0304 19:28:52.476918 22579586809984 run.py:483] Algo bellman_ford step 1941 current loss 0.018513, current_train_items 62144.
I0304 19:28:52.500062 22579586809984 run.py:483] Algo bellman_ford step 1942 current loss 0.065665, current_train_items 62176.
I0304 19:28:52.528295 22579586809984 run.py:483] Algo bellman_ford step 1943 current loss 0.082688, current_train_items 62208.
I0304 19:28:52.560754 22579586809984 run.py:483] Algo bellman_ford step 1944 current loss 0.165151, current_train_items 62240.
I0304 19:28:52.579769 22579586809984 run.py:483] Algo bellman_ford step 1945 current loss 0.013013, current_train_items 62272.
I0304 19:28:52.596040 22579586809984 run.py:483] Algo bellman_ford step 1946 current loss 0.051917, current_train_items 62304.
I0304 19:28:52.619634 22579586809984 run.py:483] Algo bellman_ford step 1947 current loss 0.031140, current_train_items 62336.
I0304 19:28:52.649983 22579586809984 run.py:483] Algo bellman_ford step 1948 current loss 0.067833, current_train_items 62368.
I0304 19:28:52.681351 22579586809984 run.py:483] Algo bellman_ford step 1949 current loss 0.079701, current_train_items 62400.
I0304 19:28:52.700359 22579586809984 run.py:483] Algo bellman_ford step 1950 current loss 0.027389, current_train_items 62432.
I0304 19:28:52.708322 22579586809984 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0304 19:28:52.708426 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:28:52.725558 22579586809984 run.py:483] Algo bellman_ford step 1951 current loss 0.056753, current_train_items 62464.
I0304 19:28:52.749644 22579586809984 run.py:483] Algo bellman_ford step 1952 current loss 0.098900, current_train_items 62496.
I0304 19:28:52.781348 22579586809984 run.py:483] Algo bellman_ford step 1953 current loss 0.143974, current_train_items 62528.
I0304 19:28:52.813543 22579586809984 run.py:483] Algo bellman_ford step 1954 current loss 0.130804, current_train_items 62560.
I0304 19:28:52.832744 22579586809984 run.py:483] Algo bellman_ford step 1955 current loss 0.016287, current_train_items 62592.
I0304 19:28:52.847914 22579586809984 run.py:483] Algo bellman_ford step 1956 current loss 0.017130, current_train_items 62624.
I0304 19:28:52.871668 22579586809984 run.py:483] Algo bellman_ford step 1957 current loss 0.107934, current_train_items 62656.
I0304 19:28:52.901726 22579586809984 run.py:483] Algo bellman_ford step 1958 current loss 0.128403, current_train_items 62688.
I0304 19:28:52.934632 22579586809984 run.py:483] Algo bellman_ford step 1959 current loss 0.100407, current_train_items 62720.
I0304 19:28:52.953872 22579586809984 run.py:483] Algo bellman_ford step 1960 current loss 0.006635, current_train_items 62752.
I0304 19:28:52.970180 22579586809984 run.py:483] Algo bellman_ford step 1961 current loss 0.066167, current_train_items 62784.
I0304 19:28:52.993061 22579586809984 run.py:483] Algo bellman_ford step 1962 current loss 0.064718, current_train_items 62816.
I0304 19:28:53.021424 22579586809984 run.py:483] Algo bellman_ford step 1963 current loss 0.118904, current_train_items 62848.
I0304 19:28:53.056351 22579586809984 run.py:483] Algo bellman_ford step 1964 current loss 0.097287, current_train_items 62880.
I0304 19:28:53.075499 22579586809984 run.py:483] Algo bellman_ford step 1965 current loss 0.007574, current_train_items 62912.
I0304 19:28:53.091552 22579586809984 run.py:483] Algo bellman_ford step 1966 current loss 0.067649, current_train_items 62944.
I0304 19:28:53.114869 22579586809984 run.py:483] Algo bellman_ford step 1967 current loss 0.181071, current_train_items 62976.
I0304 19:28:53.144105 22579586809984 run.py:483] Algo bellman_ford step 1968 current loss 0.138552, current_train_items 63008.
I0304 19:28:53.175835 22579586809984 run.py:483] Algo bellman_ford step 1969 current loss 0.098246, current_train_items 63040.
I0304 19:28:53.195034 22579586809984 run.py:483] Algo bellman_ford step 1970 current loss 0.006622, current_train_items 63072.
I0304 19:28:53.211222 22579586809984 run.py:483] Algo bellman_ford step 1971 current loss 0.035584, current_train_items 63104.
I0304 19:28:53.233772 22579586809984 run.py:483] Algo bellman_ford step 1972 current loss 0.064802, current_train_items 63136.
I0304 19:28:53.263415 22579586809984 run.py:483] Algo bellman_ford step 1973 current loss 0.132212, current_train_items 63168.
I0304 19:28:53.298744 22579586809984 run.py:483] Algo bellman_ford step 1974 current loss 0.234821, current_train_items 63200.
I0304 19:28:53.318064 22579586809984 run.py:483] Algo bellman_ford step 1975 current loss 0.007534, current_train_items 63232.
I0304 19:28:53.334006 22579586809984 run.py:483] Algo bellman_ford step 1976 current loss 0.038157, current_train_items 63264.
I0304 19:28:53.356144 22579586809984 run.py:483] Algo bellman_ford step 1977 current loss 0.062655, current_train_items 63296.
I0304 19:28:53.384942 22579586809984 run.py:483] Algo bellman_ford step 1978 current loss 0.086244, current_train_items 63328.
I0304 19:28:53.417744 22579586809984 run.py:483] Algo bellman_ford step 1979 current loss 0.260992, current_train_items 63360.
I0304 19:28:53.436721 22579586809984 run.py:483] Algo bellman_ford step 1980 current loss 0.006609, current_train_items 63392.
I0304 19:28:53.453096 22579586809984 run.py:483] Algo bellman_ford step 1981 current loss 0.019937, current_train_items 63424.
I0304 19:28:53.476442 22579586809984 run.py:483] Algo bellman_ford step 1982 current loss 0.084616, current_train_items 63456.
I0304 19:28:53.504801 22579586809984 run.py:483] Algo bellman_ford step 1983 current loss 0.053294, current_train_items 63488.
I0304 19:28:53.540181 22579586809984 run.py:483] Algo bellman_ford step 1984 current loss 0.151913, current_train_items 63520.
I0304 19:28:53.559480 22579586809984 run.py:483] Algo bellman_ford step 1985 current loss 0.005326, current_train_items 63552.
I0304 19:28:53.575703 22579586809984 run.py:483] Algo bellman_ford step 1986 current loss 0.067704, current_train_items 63584.
I0304 19:28:53.597818 22579586809984 run.py:483] Algo bellman_ford step 1987 current loss 0.061462, current_train_items 63616.
I0304 19:28:53.626597 22579586809984 run.py:483] Algo bellman_ford step 1988 current loss 0.094874, current_train_items 63648.
I0304 19:28:53.659727 22579586809984 run.py:483] Algo bellman_ford step 1989 current loss 0.104416, current_train_items 63680.
I0304 19:28:53.678856 22579586809984 run.py:483] Algo bellman_ford step 1990 current loss 0.016091, current_train_items 63712.
I0304 19:28:53.695084 22579586809984 run.py:483] Algo bellman_ford step 1991 current loss 0.042643, current_train_items 63744.
I0304 19:28:53.718640 22579586809984 run.py:483] Algo bellman_ford step 1992 current loss 0.068902, current_train_items 63776.
I0304 19:28:53.748453 22579586809984 run.py:483] Algo bellman_ford step 1993 current loss 0.104085, current_train_items 63808.
I0304 19:28:53.776822 22579586809984 run.py:483] Algo bellman_ford step 1994 current loss 0.074242, current_train_items 63840.
I0304 19:28:53.795935 22579586809984 run.py:483] Algo bellman_ford step 1995 current loss 0.017776, current_train_items 63872.
I0304 19:28:53.812590 22579586809984 run.py:483] Algo bellman_ford step 1996 current loss 0.031039, current_train_items 63904.
I0304 19:28:53.835498 22579586809984 run.py:483] Algo bellman_ford step 1997 current loss 0.048848, current_train_items 63936.
I0304 19:28:53.866257 22579586809984 run.py:483] Algo bellman_ford step 1998 current loss 0.091451, current_train_items 63968.
I0304 19:28:53.897620 22579586809984 run.py:483] Algo bellman_ford step 1999 current loss 0.109423, current_train_items 64000.
I0304 19:28:53.916701 22579586809984 run.py:483] Algo bellman_ford step 2000 current loss 0.007199, current_train_items 64032.
I0304 19:28:53.924716 22579586809984 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0304 19:28:53.924823 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:28:53.941461 22579586809984 run.py:483] Algo bellman_ford step 2001 current loss 0.037250, current_train_items 64064.
I0304 19:28:53.965805 22579586809984 run.py:483] Algo bellman_ford step 2002 current loss 0.074659, current_train_items 64096.
I0304 19:28:53.996193 22579586809984 run.py:483] Algo bellman_ford step 2003 current loss 0.077572, current_train_items 64128.
I0304 19:28:54.028301 22579586809984 run.py:483] Algo bellman_ford step 2004 current loss 0.088431, current_train_items 64160.
I0304 19:28:54.047445 22579586809984 run.py:483] Algo bellman_ford step 2005 current loss 0.010042, current_train_items 64192.
I0304 19:28:54.063394 22579586809984 run.py:483] Algo bellman_ford step 2006 current loss 0.037014, current_train_items 64224.
I0304 19:28:54.086714 22579586809984 run.py:483] Algo bellman_ford step 2007 current loss 0.052075, current_train_items 64256.
I0304 19:28:54.117116 22579586809984 run.py:483] Algo bellman_ford step 2008 current loss 0.076421, current_train_items 64288.
I0304 19:28:54.148778 22579586809984 run.py:483] Algo bellman_ford step 2009 current loss 0.087779, current_train_items 64320.
I0304 19:28:54.168048 22579586809984 run.py:483] Algo bellman_ford step 2010 current loss 0.011263, current_train_items 64352.
I0304 19:28:54.184413 22579586809984 run.py:483] Algo bellman_ford step 2011 current loss 0.027700, current_train_items 64384.
I0304 19:28:54.208035 22579586809984 run.py:483] Algo bellman_ford step 2012 current loss 0.068572, current_train_items 64416.
I0304 19:28:54.237133 22579586809984 run.py:483] Algo bellman_ford step 2013 current loss 0.091317, current_train_items 64448.
I0304 19:28:54.269027 22579586809984 run.py:483] Algo bellman_ford step 2014 current loss 0.093272, current_train_items 64480.
I0304 19:28:54.287745 22579586809984 run.py:483] Algo bellman_ford step 2015 current loss 0.015495, current_train_items 64512.
I0304 19:28:54.304390 22579586809984 run.py:483] Algo bellman_ford step 2016 current loss 0.038601, current_train_items 64544.
I0304 19:28:54.328297 22579586809984 run.py:483] Algo bellman_ford step 2017 current loss 0.113645, current_train_items 64576.
I0304 19:28:54.356286 22579586809984 run.py:483] Algo bellman_ford step 2018 current loss 0.082912, current_train_items 64608.
I0304 19:28:54.389778 22579586809984 run.py:483] Algo bellman_ford step 2019 current loss 0.110738, current_train_items 64640.
I0304 19:28:54.408586 22579586809984 run.py:483] Algo bellman_ford step 2020 current loss 0.005428, current_train_items 64672.
I0304 19:28:54.424676 22579586809984 run.py:483] Algo bellman_ford step 2021 current loss 0.016192, current_train_items 64704.
I0304 19:28:54.448237 22579586809984 run.py:483] Algo bellman_ford step 2022 current loss 0.102541, current_train_items 64736.
I0304 19:28:54.477734 22579586809984 run.py:483] Algo bellman_ford step 2023 current loss 0.080463, current_train_items 64768.
I0304 19:28:54.512729 22579586809984 run.py:483] Algo bellman_ford step 2024 current loss 0.091168, current_train_items 64800.
I0304 19:28:54.531792 22579586809984 run.py:483] Algo bellman_ford step 2025 current loss 0.018176, current_train_items 64832.
I0304 19:28:54.547737 22579586809984 run.py:483] Algo bellman_ford step 2026 current loss 0.028029, current_train_items 64864.
I0304 19:28:54.571467 22579586809984 run.py:483] Algo bellman_ford step 2027 current loss 0.070061, current_train_items 64896.
I0304 19:28:54.600812 22579586809984 run.py:483] Algo bellman_ford step 2028 current loss 0.058364, current_train_items 64928.
I0304 19:28:54.632776 22579586809984 run.py:483] Algo bellman_ford step 2029 current loss 0.089742, current_train_items 64960.
I0304 19:28:54.652114 22579586809984 run.py:483] Algo bellman_ford step 2030 current loss 0.008228, current_train_items 64992.
I0304 19:28:54.668596 22579586809984 run.py:483] Algo bellman_ford step 2031 current loss 0.049944, current_train_items 65024.
I0304 19:28:54.691388 22579586809984 run.py:483] Algo bellman_ford step 2032 current loss 0.049660, current_train_items 65056.
I0304 19:28:54.721655 22579586809984 run.py:483] Algo bellman_ford step 2033 current loss 0.127929, current_train_items 65088.
I0304 19:28:54.752962 22579586809984 run.py:483] Algo bellman_ford step 2034 current loss 0.090298, current_train_items 65120.
I0304 19:28:54.772409 22579586809984 run.py:483] Algo bellman_ford step 2035 current loss 0.010475, current_train_items 65152.
I0304 19:28:54.788340 22579586809984 run.py:483] Algo bellman_ford step 2036 current loss 0.011094, current_train_items 65184.
I0304 19:28:54.810943 22579586809984 run.py:483] Algo bellman_ford step 2037 current loss 0.077701, current_train_items 65216.
I0304 19:28:54.840486 22579586809984 run.py:483] Algo bellman_ford step 2038 current loss 0.094448, current_train_items 65248.
I0304 19:28:54.873007 22579586809984 run.py:483] Algo bellman_ford step 2039 current loss 0.090869, current_train_items 65280.
I0304 19:28:54.892021 22579586809984 run.py:483] Algo bellman_ford step 2040 current loss 0.022050, current_train_items 65312.
I0304 19:28:54.907897 22579586809984 run.py:483] Algo bellman_ford step 2041 current loss 0.038547, current_train_items 65344.
I0304 19:28:54.932037 22579586809984 run.py:483] Algo bellman_ford step 2042 current loss 0.159681, current_train_items 65376.
I0304 19:28:54.961811 22579586809984 run.py:483] Algo bellman_ford step 2043 current loss 0.100640, current_train_items 65408.
I0304 19:28:54.988945 22579586809984 run.py:483] Algo bellman_ford step 2044 current loss 0.053478, current_train_items 65440.
I0304 19:28:55.008004 22579586809984 run.py:483] Algo bellman_ford step 2045 current loss 0.006982, current_train_items 65472.
I0304 19:28:55.024725 22579586809984 run.py:483] Algo bellman_ford step 2046 current loss 0.037315, current_train_items 65504.
I0304 19:28:55.048009 22579586809984 run.py:483] Algo bellman_ford step 2047 current loss 0.100292, current_train_items 65536.
I0304 19:28:55.077275 22579586809984 run.py:483] Algo bellman_ford step 2048 current loss 0.070119, current_train_items 65568.
I0304 19:28:55.107275 22579586809984 run.py:483] Algo bellman_ford step 2049 current loss 0.073447, current_train_items 65600.
I0304 19:28:55.126361 22579586809984 run.py:483] Algo bellman_ford step 2050 current loss 0.014805, current_train_items 65632.
I0304 19:28:55.134338 22579586809984 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0304 19:28:55.134469 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:28:55.151958 22579586809984 run.py:483] Algo bellman_ford step 2051 current loss 0.031459, current_train_items 65664.
I0304 19:28:55.175771 22579586809984 run.py:483] Algo bellman_ford step 2052 current loss 0.043950, current_train_items 65696.
I0304 19:28:55.205243 22579586809984 run.py:483] Algo bellman_ford step 2053 current loss 0.102721, current_train_items 65728.
I0304 19:28:55.240797 22579586809984 run.py:483] Algo bellman_ford step 2054 current loss 0.129312, current_train_items 65760.
I0304 19:28:55.260190 22579586809984 run.py:483] Algo bellman_ford step 2055 current loss 0.004562, current_train_items 65792.
I0304 19:28:55.276061 22579586809984 run.py:483] Algo bellman_ford step 2056 current loss 0.019151, current_train_items 65824.
I0304 19:28:55.299298 22579586809984 run.py:483] Algo bellman_ford step 2057 current loss 0.055951, current_train_items 65856.
I0304 19:28:55.328665 22579586809984 run.py:483] Algo bellman_ford step 2058 current loss 0.081583, current_train_items 65888.
I0304 19:28:55.361899 22579586809984 run.py:483] Algo bellman_ford step 2059 current loss 0.157059, current_train_items 65920.
I0304 19:28:55.381619 22579586809984 run.py:483] Algo bellman_ford step 2060 current loss 0.008622, current_train_items 65952.
I0304 19:28:55.398189 22579586809984 run.py:483] Algo bellman_ford step 2061 current loss 0.031954, current_train_items 65984.
I0304 19:28:55.421786 22579586809984 run.py:483] Algo bellman_ford step 2062 current loss 0.062675, current_train_items 66016.
I0304 19:28:55.451565 22579586809984 run.py:483] Algo bellman_ford step 2063 current loss 0.110615, current_train_items 66048.
I0304 19:28:55.486103 22579586809984 run.py:483] Algo bellman_ford step 2064 current loss 0.105384, current_train_items 66080.
I0304 19:28:55.505338 22579586809984 run.py:483] Algo bellman_ford step 2065 current loss 0.004912, current_train_items 66112.
I0304 19:28:55.521581 22579586809984 run.py:483] Algo bellman_ford step 2066 current loss 0.053687, current_train_items 66144.
I0304 19:28:55.545252 22579586809984 run.py:483] Algo bellman_ford step 2067 current loss 0.058454, current_train_items 66176.
I0304 19:28:55.575762 22579586809984 run.py:483] Algo bellman_ford step 2068 current loss 0.071271, current_train_items 66208.
I0304 19:28:55.608230 22579586809984 run.py:483] Algo bellman_ford step 2069 current loss 0.075142, current_train_items 66240.
I0304 19:28:55.627928 22579586809984 run.py:483] Algo bellman_ford step 2070 current loss 0.004007, current_train_items 66272.
I0304 19:28:55.644433 22579586809984 run.py:483] Algo bellman_ford step 2071 current loss 0.021360, current_train_items 66304.
I0304 19:28:55.667814 22579586809984 run.py:483] Algo bellman_ford step 2072 current loss 0.130620, current_train_items 66336.
I0304 19:28:55.698040 22579586809984 run.py:483] Algo bellman_ford step 2073 current loss 0.162862, current_train_items 66368.
I0304 19:28:55.730887 22579586809984 run.py:483] Algo bellman_ford step 2074 current loss 0.159902, current_train_items 66400.
I0304 19:28:55.750206 22579586809984 run.py:483] Algo bellman_ford step 2075 current loss 0.017015, current_train_items 66432.
I0304 19:28:55.767085 22579586809984 run.py:483] Algo bellman_ford step 2076 current loss 0.034939, current_train_items 66464.
I0304 19:28:55.791042 22579586809984 run.py:483] Algo bellman_ford step 2077 current loss 0.059684, current_train_items 66496.
I0304 19:28:55.819008 22579586809984 run.py:483] Algo bellman_ford step 2078 current loss 0.065273, current_train_items 66528.
I0304 19:28:55.850915 22579586809984 run.py:483] Algo bellman_ford step 2079 current loss 0.102265, current_train_items 66560.
I0304 19:28:55.870249 22579586809984 run.py:483] Algo bellman_ford step 2080 current loss 0.005568, current_train_items 66592.
I0304 19:28:55.886564 22579586809984 run.py:483] Algo bellman_ford step 2081 current loss 0.043571, current_train_items 66624.
I0304 19:28:55.909963 22579586809984 run.py:483] Algo bellman_ford step 2082 current loss 0.056060, current_train_items 66656.
I0304 19:28:55.941238 22579586809984 run.py:483] Algo bellman_ford step 2083 current loss 0.086435, current_train_items 66688.
I0304 19:28:55.974017 22579586809984 run.py:483] Algo bellman_ford step 2084 current loss 0.128969, current_train_items 66720.
I0304 19:28:55.993824 22579586809984 run.py:483] Algo bellman_ford step 2085 current loss 0.006141, current_train_items 66752.
I0304 19:28:56.010146 22579586809984 run.py:483] Algo bellman_ford step 2086 current loss 0.090241, current_train_items 66784.
I0304 19:28:56.033608 22579586809984 run.py:483] Algo bellman_ford step 2087 current loss 0.073058, current_train_items 66816.
I0304 19:28:56.061435 22579586809984 run.py:483] Algo bellman_ford step 2088 current loss 0.058657, current_train_items 66848.
I0304 19:28:56.094333 22579586809984 run.py:483] Algo bellman_ford step 2089 current loss 0.159312, current_train_items 66880.
I0304 19:28:56.114043 22579586809984 run.py:483] Algo bellman_ford step 2090 current loss 0.007448, current_train_items 66912.
I0304 19:28:56.130457 22579586809984 run.py:483] Algo bellman_ford step 2091 current loss 0.032414, current_train_items 66944.
I0304 19:28:56.153876 22579586809984 run.py:483] Algo bellman_ford step 2092 current loss 0.087144, current_train_items 66976.
I0304 19:28:56.183376 22579586809984 run.py:483] Algo bellman_ford step 2093 current loss 0.080116, current_train_items 67008.
I0304 19:28:56.215918 22579586809984 run.py:483] Algo bellman_ford step 2094 current loss 0.110745, current_train_items 67040.
I0304 19:28:56.235115 22579586809984 run.py:483] Algo bellman_ford step 2095 current loss 0.011994, current_train_items 67072.
I0304 19:28:56.251571 22579586809984 run.py:483] Algo bellman_ford step 2096 current loss 0.048435, current_train_items 67104.
I0304 19:28:56.275158 22579586809984 run.py:483] Algo bellman_ford step 2097 current loss 0.104817, current_train_items 67136.
I0304 19:28:56.304123 22579586809984 run.py:483] Algo bellman_ford step 2098 current loss 0.111715, current_train_items 67168.
I0304 19:28:56.336465 22579586809984 run.py:483] Algo bellman_ford step 2099 current loss 0.150288, current_train_items 67200.
I0304 19:28:56.356475 22579586809984 run.py:483] Algo bellman_ford step 2100 current loss 0.005261, current_train_items 67232.
I0304 19:28:56.364131 22579586809984 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0304 19:28:56.364237 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:28:56.380816 22579586809984 run.py:483] Algo bellman_ford step 2101 current loss 0.050093, current_train_items 67264.
I0304 19:28:56.404606 22579586809984 run.py:483] Algo bellman_ford step 2102 current loss 0.146344, current_train_items 67296.
I0304 19:28:56.434815 22579586809984 run.py:483] Algo bellman_ford step 2103 current loss 0.067956, current_train_items 67328.
I0304 19:28:56.467040 22579586809984 run.py:483] Algo bellman_ford step 2104 current loss 0.081573, current_train_items 67360.
I0304 19:28:56.486794 22579586809984 run.py:483] Algo bellman_ford step 2105 current loss 0.009984, current_train_items 67392.
I0304 19:28:56.502066 22579586809984 run.py:483] Algo bellman_ford step 2106 current loss 0.015355, current_train_items 67424.
I0304 19:28:56.526387 22579586809984 run.py:483] Algo bellman_ford step 2107 current loss 0.125120, current_train_items 67456.
I0304 19:28:56.557453 22579586809984 run.py:483] Algo bellman_ford step 2108 current loss 0.148630, current_train_items 67488.
I0304 19:28:56.588875 22579586809984 run.py:483] Algo bellman_ford step 2109 current loss 0.090998, current_train_items 67520.
I0304 19:28:56.607973 22579586809984 run.py:483] Algo bellman_ford step 2110 current loss 0.016345, current_train_items 67552.
I0304 19:28:56.624201 22579586809984 run.py:483] Algo bellman_ford step 2111 current loss 0.019779, current_train_items 67584.
I0304 19:28:56.647370 22579586809984 run.py:483] Algo bellman_ford step 2112 current loss 0.077382, current_train_items 67616.
I0304 19:28:56.676321 22579586809984 run.py:483] Algo bellman_ford step 2113 current loss 0.077490, current_train_items 67648.
I0304 19:28:56.707233 22579586809984 run.py:483] Algo bellman_ford step 2114 current loss 0.078306, current_train_items 67680.
I0304 19:28:56.726597 22579586809984 run.py:483] Algo bellman_ford step 2115 current loss 0.017519, current_train_items 67712.
I0304 19:28:56.742731 22579586809984 run.py:483] Algo bellman_ford step 2116 current loss 0.047801, current_train_items 67744.
I0304 19:28:56.767003 22579586809984 run.py:483] Algo bellman_ford step 2117 current loss 0.049473, current_train_items 67776.
I0304 19:28:56.797203 22579586809984 run.py:483] Algo bellman_ford step 2118 current loss 0.086388, current_train_items 67808.
I0304 19:28:56.828809 22579586809984 run.py:483] Algo bellman_ford step 2119 current loss 0.064080, current_train_items 67840.
I0304 19:28:56.847994 22579586809984 run.py:483] Algo bellman_ford step 2120 current loss 0.004735, current_train_items 67872.
I0304 19:28:56.863954 22579586809984 run.py:483] Algo bellman_ford step 2121 current loss 0.035061, current_train_items 67904.
I0304 19:28:56.887417 22579586809984 run.py:483] Algo bellman_ford step 2122 current loss 0.026842, current_train_items 67936.
I0304 19:28:56.917355 22579586809984 run.py:483] Algo bellman_ford step 2123 current loss 0.068618, current_train_items 67968.
I0304 19:28:56.946243 22579586809984 run.py:483] Algo bellman_ford step 2124 current loss 0.105316, current_train_items 68000.
I0304 19:28:56.965149 22579586809984 run.py:483] Algo bellman_ford step 2125 current loss 0.005039, current_train_items 68032.
I0304 19:28:56.981727 22579586809984 run.py:483] Algo bellman_ford step 2126 current loss 0.075011, current_train_items 68064.
I0304 19:28:57.006847 22579586809984 run.py:483] Algo bellman_ford step 2127 current loss 0.058243, current_train_items 68096.
I0304 19:28:57.036262 22579586809984 run.py:483] Algo bellman_ford step 2128 current loss 0.110462, current_train_items 68128.
I0304 19:28:57.067734 22579586809984 run.py:483] Algo bellman_ford step 2129 current loss 0.072282, current_train_items 68160.
I0304 19:28:57.086700 22579586809984 run.py:483] Algo bellman_ford step 2130 current loss 0.006703, current_train_items 68192.
I0304 19:28:57.103486 22579586809984 run.py:483] Algo bellman_ford step 2131 current loss 0.038512, current_train_items 68224.
I0304 19:28:57.127551 22579586809984 run.py:483] Algo bellman_ford step 2132 current loss 0.088493, current_train_items 68256.
I0304 19:28:57.156803 22579586809984 run.py:483] Algo bellman_ford step 2133 current loss 0.092804, current_train_items 68288.
I0304 19:28:57.190404 22579586809984 run.py:483] Algo bellman_ford step 2134 current loss 0.085523, current_train_items 68320.
I0304 19:28:57.209661 22579586809984 run.py:483] Algo bellman_ford step 2135 current loss 0.005162, current_train_items 68352.
I0304 19:28:57.226055 22579586809984 run.py:483] Algo bellman_ford step 2136 current loss 0.034974, current_train_items 68384.
I0304 19:28:57.249093 22579586809984 run.py:483] Algo bellman_ford step 2137 current loss 0.066645, current_train_items 68416.
I0304 19:28:57.278830 22579586809984 run.py:483] Algo bellman_ford step 2138 current loss 0.062100, current_train_items 68448.
I0304 19:28:57.314157 22579586809984 run.py:483] Algo bellman_ford step 2139 current loss 0.122866, current_train_items 68480.
I0304 19:28:57.332994 22579586809984 run.py:483] Algo bellman_ford step 2140 current loss 0.039787, current_train_items 68512.
I0304 19:28:57.349070 22579586809984 run.py:483] Algo bellman_ford step 2141 current loss 0.017745, current_train_items 68544.
I0304 19:28:57.370917 22579586809984 run.py:483] Algo bellman_ford step 2142 current loss 0.031692, current_train_items 68576.
I0304 19:28:57.400044 22579586809984 run.py:483] Algo bellman_ford step 2143 current loss 0.060092, current_train_items 68608.
I0304 19:28:57.432562 22579586809984 run.py:483] Algo bellman_ford step 2144 current loss 0.097937, current_train_items 68640.
I0304 19:28:57.451866 22579586809984 run.py:483] Algo bellman_ford step 2145 current loss 0.016676, current_train_items 68672.
I0304 19:28:57.468474 22579586809984 run.py:483] Algo bellman_ford step 2146 current loss 0.045435, current_train_items 68704.
I0304 19:28:57.492365 22579586809984 run.py:483] Algo bellman_ford step 2147 current loss 0.065533, current_train_items 68736.
I0304 19:28:57.522366 22579586809984 run.py:483] Algo bellman_ford step 2148 current loss 0.065507, current_train_items 68768.
I0304 19:28:57.553930 22579586809984 run.py:483] Algo bellman_ford step 2149 current loss 0.068346, current_train_items 68800.
I0304 19:28:57.573262 22579586809984 run.py:483] Algo bellman_ford step 2150 current loss 0.004863, current_train_items 68832.
I0304 19:28:57.581229 22579586809984 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0304 19:28:57.581335 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:28:57.598229 22579586809984 run.py:483] Algo bellman_ford step 2151 current loss 0.012193, current_train_items 68864.
I0304 19:28:57.621953 22579586809984 run.py:483] Algo bellman_ford step 2152 current loss 0.091605, current_train_items 68896.
I0304 19:28:57.651508 22579586809984 run.py:483] Algo bellman_ford step 2153 current loss 0.086087, current_train_items 68928.
I0304 19:28:57.683338 22579586809984 run.py:483] Algo bellman_ford step 2154 current loss 0.110492, current_train_items 68960.
I0304 19:28:57.702578 22579586809984 run.py:483] Algo bellman_ford step 2155 current loss 0.005612, current_train_items 68992.
I0304 19:28:57.718459 22579586809984 run.py:483] Algo bellman_ford step 2156 current loss 0.027207, current_train_items 69024.
I0304 19:28:57.742426 22579586809984 run.py:483] Algo bellman_ford step 2157 current loss 0.059915, current_train_items 69056.
I0304 19:28:57.772498 22579586809984 run.py:483] Algo bellman_ford step 2158 current loss 0.087209, current_train_items 69088.
I0304 19:28:57.804961 22579586809984 run.py:483] Algo bellman_ford step 2159 current loss 0.148540, current_train_items 69120.
I0304 19:28:57.824660 22579586809984 run.py:483] Algo bellman_ford step 2160 current loss 0.024279, current_train_items 69152.
I0304 19:28:57.840852 22579586809984 run.py:483] Algo bellman_ford step 2161 current loss 0.021853, current_train_items 69184.
I0304 19:28:57.864253 22579586809984 run.py:483] Algo bellman_ford step 2162 current loss 0.101012, current_train_items 69216.
I0304 19:28:57.892489 22579586809984 run.py:483] Algo bellman_ford step 2163 current loss 0.106140, current_train_items 69248.
I0304 19:28:57.925902 22579586809984 run.py:483] Algo bellman_ford step 2164 current loss 0.116617, current_train_items 69280.
I0304 19:28:57.944829 22579586809984 run.py:483] Algo bellman_ford step 2165 current loss 0.007365, current_train_items 69312.
I0304 19:28:57.961483 22579586809984 run.py:483] Algo bellman_ford step 2166 current loss 0.053227, current_train_items 69344.
I0304 19:28:57.984251 22579586809984 run.py:483] Algo bellman_ford step 2167 current loss 0.077932, current_train_items 69376.
I0304 19:28:58.012576 22579586809984 run.py:483] Algo bellman_ford step 2168 current loss 0.072904, current_train_items 69408.
I0304 19:28:58.044980 22579586809984 run.py:483] Algo bellman_ford step 2169 current loss 0.100432, current_train_items 69440.
I0304 19:28:58.064159 22579586809984 run.py:483] Algo bellman_ford step 2170 current loss 0.020682, current_train_items 69472.
I0304 19:28:58.080299 22579586809984 run.py:483] Algo bellman_ford step 2171 current loss 0.028496, current_train_items 69504.
I0304 19:28:58.103281 22579586809984 run.py:483] Algo bellman_ford step 2172 current loss 0.053399, current_train_items 69536.
I0304 19:28:58.133909 22579586809984 run.py:483] Algo bellman_ford step 2173 current loss 0.089185, current_train_items 69568.
I0304 19:28:58.164514 22579586809984 run.py:483] Algo bellman_ford step 2174 current loss 0.109160, current_train_items 69600.
I0304 19:28:58.183743 22579586809984 run.py:483] Algo bellman_ford step 2175 current loss 0.004908, current_train_items 69632.
I0304 19:28:58.199620 22579586809984 run.py:483] Algo bellman_ford step 2176 current loss 0.036960, current_train_items 69664.
I0304 19:28:58.224354 22579586809984 run.py:483] Algo bellman_ford step 2177 current loss 0.103740, current_train_items 69696.
I0304 19:28:58.253001 22579586809984 run.py:483] Algo bellman_ford step 2178 current loss 0.106447, current_train_items 69728.
I0304 19:28:58.285985 22579586809984 run.py:483] Algo bellman_ford step 2179 current loss 0.085901, current_train_items 69760.
I0304 19:28:58.304510 22579586809984 run.py:483] Algo bellman_ford step 2180 current loss 0.032793, current_train_items 69792.
I0304 19:28:58.320845 22579586809984 run.py:483] Algo bellman_ford step 2181 current loss 0.039788, current_train_items 69824.
I0304 19:28:58.344202 22579586809984 run.py:483] Algo bellman_ford step 2182 current loss 0.091319, current_train_items 69856.
I0304 19:28:58.372888 22579586809984 run.py:483] Algo bellman_ford step 2183 current loss 0.141384, current_train_items 69888.
I0304 19:28:58.406338 22579586809984 run.py:483] Algo bellman_ford step 2184 current loss 0.181626, current_train_items 69920.
I0304 19:28:58.425805 22579586809984 run.py:483] Algo bellman_ford step 2185 current loss 0.009065, current_train_items 69952.
I0304 19:28:58.441727 22579586809984 run.py:483] Algo bellman_ford step 2186 current loss 0.036001, current_train_items 69984.
I0304 19:28:58.464750 22579586809984 run.py:483] Algo bellman_ford step 2187 current loss 0.161299, current_train_items 70016.
I0304 19:28:58.494303 22579586809984 run.py:483] Algo bellman_ford step 2188 current loss 0.087502, current_train_items 70048.
W0304 19:28:58.518697 22579586809984 samplers.py:155] Increasing hint lengh from 12 to 13
I0304 19:29:05.217816 22579586809984 run.py:483] Algo bellman_ford step 2189 current loss 0.224709, current_train_items 70080.
I0304 19:29:05.238527 22579586809984 run.py:483] Algo bellman_ford step 2190 current loss 0.005699, current_train_items 70112.
I0304 19:29:05.255351 22579586809984 run.py:483] Algo bellman_ford step 2191 current loss 0.069696, current_train_items 70144.
I0304 19:29:05.279014 22579586809984 run.py:483] Algo bellman_ford step 2192 current loss 0.125527, current_train_items 70176.
W0304 19:29:05.300965 22579586809984 samplers.py:155] Increasing hint lengh from 10 to 12
I0304 19:29:12.438469 22579586809984 run.py:483] Algo bellman_ford step 2193 current loss 0.154736, current_train_items 70208.
I0304 19:29:12.470791 22579586809984 run.py:483] Algo bellman_ford step 2194 current loss 0.101874, current_train_items 70240.
I0304 19:29:12.491217 22579586809984 run.py:483] Algo bellman_ford step 2195 current loss 0.024588, current_train_items 70272.
I0304 19:29:12.507781 22579586809984 run.py:483] Algo bellman_ford step 2196 current loss 0.021083, current_train_items 70304.
I0304 19:29:12.531221 22579586809984 run.py:483] Algo bellman_ford step 2197 current loss 0.087793, current_train_items 70336.
I0304 19:29:12.560741 22579586809984 run.py:483] Algo bellman_ford step 2198 current loss 0.099202, current_train_items 70368.
I0304 19:29:12.593895 22579586809984 run.py:483] Algo bellman_ford step 2199 current loss 0.100464, current_train_items 70400.
I0304 19:29:12.613936 22579586809984 run.py:483] Algo bellman_ford step 2200 current loss 0.015458, current_train_items 70432.
I0304 19:29:12.623414 22579586809984 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0304 19:29:12.623520 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:29:12.640707 22579586809984 run.py:483] Algo bellman_ford step 2201 current loss 0.035329, current_train_items 70464.
I0304 19:29:12.664381 22579586809984 run.py:483] Algo bellman_ford step 2202 current loss 0.076347, current_train_items 70496.
I0304 19:29:12.695013 22579586809984 run.py:483] Algo bellman_ford step 2203 current loss 0.118186, current_train_items 70528.
I0304 19:29:12.728881 22579586809984 run.py:483] Algo bellman_ford step 2204 current loss 0.084063, current_train_items 70560.
I0304 19:29:12.748933 22579586809984 run.py:483] Algo bellman_ford step 2205 current loss 0.006640, current_train_items 70592.
I0304 19:29:12.765300 22579586809984 run.py:483] Algo bellman_ford step 2206 current loss 0.041642, current_train_items 70624.
I0304 19:29:12.788432 22579586809984 run.py:483] Algo bellman_ford step 2207 current loss 0.085139, current_train_items 70656.
I0304 19:29:12.818565 22579586809984 run.py:483] Algo bellman_ford step 2208 current loss 0.123683, current_train_items 70688.
I0304 19:29:12.852753 22579586809984 run.py:483] Algo bellman_ford step 2209 current loss 0.104617, current_train_items 70720.
I0304 19:29:12.872407 22579586809984 run.py:483] Algo bellman_ford step 2210 current loss 0.011976, current_train_items 70752.
I0304 19:29:12.888841 22579586809984 run.py:483] Algo bellman_ford step 2211 current loss 0.031321, current_train_items 70784.
I0304 19:29:12.912919 22579586809984 run.py:483] Algo bellman_ford step 2212 current loss 0.126723, current_train_items 70816.
I0304 19:29:12.944662 22579586809984 run.py:483] Algo bellman_ford step 2213 current loss 0.152897, current_train_items 70848.
I0304 19:29:12.979440 22579586809984 run.py:483] Algo bellman_ford step 2214 current loss 0.117321, current_train_items 70880.
I0304 19:29:12.999325 22579586809984 run.py:483] Algo bellman_ford step 2215 current loss 0.016446, current_train_items 70912.
I0304 19:29:13.015565 22579586809984 run.py:483] Algo bellman_ford step 2216 current loss 0.013716, current_train_items 70944.
I0304 19:29:13.037767 22579586809984 run.py:483] Algo bellman_ford step 2217 current loss 0.055420, current_train_items 70976.
I0304 19:29:13.068906 22579586809984 run.py:483] Algo bellman_ford step 2218 current loss 0.137642, current_train_items 71008.
I0304 19:29:13.101087 22579586809984 run.py:483] Algo bellman_ford step 2219 current loss 0.135713, current_train_items 71040.
I0304 19:29:13.120604 22579586809984 run.py:483] Algo bellman_ford step 2220 current loss 0.006282, current_train_items 71072.
I0304 19:29:13.136798 22579586809984 run.py:483] Algo bellman_ford step 2221 current loss 0.019510, current_train_items 71104.
I0304 19:29:13.159886 22579586809984 run.py:483] Algo bellman_ford step 2222 current loss 0.104115, current_train_items 71136.
I0304 19:29:13.191139 22579586809984 run.py:483] Algo bellman_ford step 2223 current loss 0.075116, current_train_items 71168.
I0304 19:29:13.225356 22579586809984 run.py:483] Algo bellman_ford step 2224 current loss 0.102803, current_train_items 71200.
I0304 19:29:13.244797 22579586809984 run.py:483] Algo bellman_ford step 2225 current loss 0.008115, current_train_items 71232.
I0304 19:29:13.261119 22579586809984 run.py:483] Algo bellman_ford step 2226 current loss 0.028773, current_train_items 71264.
I0304 19:29:13.284931 22579586809984 run.py:483] Algo bellman_ford step 2227 current loss 0.044660, current_train_items 71296.
I0304 19:29:13.315785 22579586809984 run.py:483] Algo bellman_ford step 2228 current loss 0.129291, current_train_items 71328.
I0304 19:29:13.346521 22579586809984 run.py:483] Algo bellman_ford step 2229 current loss 0.100363, current_train_items 71360.
I0304 19:29:13.366225 22579586809984 run.py:483] Algo bellman_ford step 2230 current loss 0.006081, current_train_items 71392.
I0304 19:29:13.382751 22579586809984 run.py:483] Algo bellman_ford step 2231 current loss 0.047773, current_train_items 71424.
I0304 19:29:13.406386 22579586809984 run.py:483] Algo bellman_ford step 2232 current loss 0.094105, current_train_items 71456.
I0304 19:29:13.436201 22579586809984 run.py:483] Algo bellman_ford step 2233 current loss 0.111017, current_train_items 71488.
I0304 19:29:13.468714 22579586809984 run.py:483] Algo bellman_ford step 2234 current loss 0.075885, current_train_items 71520.
I0304 19:29:13.488238 22579586809984 run.py:483] Algo bellman_ford step 2235 current loss 0.004921, current_train_items 71552.
I0304 19:29:13.504740 22579586809984 run.py:483] Algo bellman_ford step 2236 current loss 0.047242, current_train_items 71584.
I0304 19:29:13.529103 22579586809984 run.py:483] Algo bellman_ford step 2237 current loss 0.087311, current_train_items 71616.
I0304 19:29:13.559826 22579586809984 run.py:483] Algo bellman_ford step 2238 current loss 0.072173, current_train_items 71648.
I0304 19:29:13.591891 22579586809984 run.py:483] Algo bellman_ford step 2239 current loss 0.098554, current_train_items 71680.
I0304 19:29:13.611298 22579586809984 run.py:483] Algo bellman_ford step 2240 current loss 0.044463, current_train_items 71712.
I0304 19:29:13.627974 22579586809984 run.py:483] Algo bellman_ford step 2241 current loss 0.032804, current_train_items 71744.
I0304 19:29:13.652380 22579586809984 run.py:483] Algo bellman_ford step 2242 current loss 0.052059, current_train_items 71776.
I0304 19:29:13.683116 22579586809984 run.py:483] Algo bellman_ford step 2243 current loss 0.115655, current_train_items 71808.
I0304 19:29:13.715087 22579586809984 run.py:483] Algo bellman_ford step 2244 current loss 0.073427, current_train_items 71840.
I0304 19:29:13.734636 22579586809984 run.py:483] Algo bellman_ford step 2245 current loss 0.016063, current_train_items 71872.
I0304 19:29:13.751214 22579586809984 run.py:483] Algo bellman_ford step 2246 current loss 0.036982, current_train_items 71904.
I0304 19:29:13.773942 22579586809984 run.py:483] Algo bellman_ford step 2247 current loss 0.081942, current_train_items 71936.
I0304 19:29:13.805481 22579586809984 run.py:483] Algo bellman_ford step 2248 current loss 0.077790, current_train_items 71968.
I0304 19:29:13.838896 22579586809984 run.py:483] Algo bellman_ford step 2249 current loss 0.090700, current_train_items 72000.
I0304 19:29:13.858471 22579586809984 run.py:483] Algo bellman_ford step 2250 current loss 0.006871, current_train_items 72032.
I0304 19:29:13.867117 22579586809984 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0304 19:29:13.867222 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:29:13.884017 22579586809984 run.py:483] Algo bellman_ford step 2251 current loss 0.038553, current_train_items 72064.
I0304 19:29:13.907178 22579586809984 run.py:483] Algo bellman_ford step 2252 current loss 0.048257, current_train_items 72096.
I0304 19:29:13.939440 22579586809984 run.py:483] Algo bellman_ford step 2253 current loss 0.113612, current_train_items 72128.
I0304 19:29:13.973922 22579586809984 run.py:483] Algo bellman_ford step 2254 current loss 0.111108, current_train_items 72160.
I0304 19:29:13.993839 22579586809984 run.py:483] Algo bellman_ford step 2255 current loss 0.008643, current_train_items 72192.
I0304 19:29:14.010096 22579586809984 run.py:483] Algo bellman_ford step 2256 current loss 0.019303, current_train_items 72224.
I0304 19:29:14.033888 22579586809984 run.py:483] Algo bellman_ford step 2257 current loss 0.076906, current_train_items 72256.
I0304 19:29:14.063204 22579586809984 run.py:483] Algo bellman_ford step 2258 current loss 0.128476, current_train_items 72288.
I0304 19:29:14.097676 22579586809984 run.py:483] Algo bellman_ford step 2259 current loss 0.142009, current_train_items 72320.
I0304 19:29:14.117963 22579586809984 run.py:483] Algo bellman_ford step 2260 current loss 0.020037, current_train_items 72352.
I0304 19:29:14.134840 22579586809984 run.py:483] Algo bellman_ford step 2261 current loss 0.030503, current_train_items 72384.
I0304 19:29:14.159743 22579586809984 run.py:483] Algo bellman_ford step 2262 current loss 0.092793, current_train_items 72416.
I0304 19:29:14.189456 22579586809984 run.py:483] Algo bellman_ford step 2263 current loss 0.050846, current_train_items 72448.
I0304 19:29:14.222734 22579586809984 run.py:483] Algo bellman_ford step 2264 current loss 0.128781, current_train_items 72480.
I0304 19:29:14.242769 22579586809984 run.py:483] Algo bellman_ford step 2265 current loss 0.051905, current_train_items 72512.
I0304 19:29:14.259044 22579586809984 run.py:483] Algo bellman_ford step 2266 current loss 0.055380, current_train_items 72544.
I0304 19:29:14.283587 22579586809984 run.py:483] Algo bellman_ford step 2267 current loss 0.072520, current_train_items 72576.
I0304 19:29:14.314465 22579586809984 run.py:483] Algo bellman_ford step 2268 current loss 0.144378, current_train_items 72608.
I0304 19:29:14.347244 22579586809984 run.py:483] Algo bellman_ford step 2269 current loss 0.116105, current_train_items 72640.
I0304 19:29:14.367440 22579586809984 run.py:483] Algo bellman_ford step 2270 current loss 0.007677, current_train_items 72672.
I0304 19:29:14.383602 22579586809984 run.py:483] Algo bellman_ford step 2271 current loss 0.020949, current_train_items 72704.
I0304 19:29:14.406119 22579586809984 run.py:483] Algo bellman_ford step 2272 current loss 0.061098, current_train_items 72736.
I0304 19:29:14.437351 22579586809984 run.py:483] Algo bellman_ford step 2273 current loss 0.068670, current_train_items 72768.
I0304 19:29:14.473299 22579586809984 run.py:483] Algo bellman_ford step 2274 current loss 0.111658, current_train_items 72800.
I0304 19:29:14.493646 22579586809984 run.py:483] Algo bellman_ford step 2275 current loss 0.005705, current_train_items 72832.
I0304 19:29:14.509897 22579586809984 run.py:483] Algo bellman_ford step 2276 current loss 0.035537, current_train_items 72864.
I0304 19:29:14.531677 22579586809984 run.py:483] Algo bellman_ford step 2277 current loss 0.030222, current_train_items 72896.
I0304 19:29:14.561386 22579586809984 run.py:483] Algo bellman_ford step 2278 current loss 0.072292, current_train_items 72928.
I0304 19:29:14.596109 22579586809984 run.py:483] Algo bellman_ford step 2279 current loss 0.116951, current_train_items 72960.
I0304 19:29:14.615880 22579586809984 run.py:483] Algo bellman_ford step 2280 current loss 0.004575, current_train_items 72992.
I0304 19:29:14.631890 22579586809984 run.py:483] Algo bellman_ford step 2281 current loss 0.052718, current_train_items 73024.
I0304 19:29:14.656028 22579586809984 run.py:483] Algo bellman_ford step 2282 current loss 0.039740, current_train_items 73056.
I0304 19:29:14.688970 22579586809984 run.py:483] Algo bellman_ford step 2283 current loss 0.103123, current_train_items 73088.
I0304 19:29:14.723550 22579586809984 run.py:483] Algo bellman_ford step 2284 current loss 0.090897, current_train_items 73120.
I0304 19:29:14.743456 22579586809984 run.py:483] Algo bellman_ford step 2285 current loss 0.015330, current_train_items 73152.
I0304 19:29:14.759920 22579586809984 run.py:483] Algo bellman_ford step 2286 current loss 0.028150, current_train_items 73184.
I0304 19:29:14.783415 22579586809984 run.py:483] Algo bellman_ford step 2287 current loss 0.098973, current_train_items 73216.
I0304 19:29:14.813814 22579586809984 run.py:483] Algo bellman_ford step 2288 current loss 0.093939, current_train_items 73248.
I0304 19:29:14.849340 22579586809984 run.py:483] Algo bellman_ford step 2289 current loss 0.134079, current_train_items 73280.
I0304 19:29:14.869440 22579586809984 run.py:483] Algo bellman_ford step 2290 current loss 0.005475, current_train_items 73312.
I0304 19:29:14.885364 22579586809984 run.py:483] Algo bellman_ford step 2291 current loss 0.017182, current_train_items 73344.
I0304 19:29:14.908403 22579586809984 run.py:483] Algo bellman_ford step 2292 current loss 0.057568, current_train_items 73376.
I0304 19:29:14.939347 22579586809984 run.py:483] Algo bellman_ford step 2293 current loss 0.068181, current_train_items 73408.
I0304 19:29:14.973445 22579586809984 run.py:483] Algo bellman_ford step 2294 current loss 0.124146, current_train_items 73440.
I0304 19:29:14.993145 22579586809984 run.py:483] Algo bellman_ford step 2295 current loss 0.004492, current_train_items 73472.
I0304 19:29:15.009470 22579586809984 run.py:483] Algo bellman_ford step 2296 current loss 0.049521, current_train_items 73504.
I0304 19:29:15.032273 22579586809984 run.py:483] Algo bellman_ford step 2297 current loss 0.034133, current_train_items 73536.
I0304 19:29:15.062192 22579586809984 run.py:483] Algo bellman_ford step 2298 current loss 0.080720, current_train_items 73568.
I0304 19:29:15.095767 22579586809984 run.py:483] Algo bellman_ford step 2299 current loss 0.145584, current_train_items 73600.
I0304 19:29:15.115929 22579586809984 run.py:483] Algo bellman_ford step 2300 current loss 0.005611, current_train_items 73632.
I0304 19:29:15.123810 22579586809984 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0304 19:29:15.123915 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:15.141191 22579586809984 run.py:483] Algo bellman_ford step 2301 current loss 0.043978, current_train_items 73664.
I0304 19:29:15.165510 22579586809984 run.py:483] Algo bellman_ford step 2302 current loss 0.064688, current_train_items 73696.
I0304 19:29:15.197709 22579586809984 run.py:483] Algo bellman_ford step 2303 current loss 0.085134, current_train_items 73728.
I0304 19:29:15.231885 22579586809984 run.py:483] Algo bellman_ford step 2304 current loss 0.095879, current_train_items 73760.
I0304 19:29:15.252108 22579586809984 run.py:483] Algo bellman_ford step 2305 current loss 0.027424, current_train_items 73792.
I0304 19:29:15.268724 22579586809984 run.py:483] Algo bellman_ford step 2306 current loss 0.064536, current_train_items 73824.
I0304 19:29:15.292549 22579586809984 run.py:483] Algo bellman_ford step 2307 current loss 0.059146, current_train_items 73856.
I0304 19:29:15.322037 22579586809984 run.py:483] Algo bellman_ford step 2308 current loss 0.076557, current_train_items 73888.
I0304 19:29:15.356265 22579586809984 run.py:483] Algo bellman_ford step 2309 current loss 0.087891, current_train_items 73920.
I0304 19:29:15.376141 22579586809984 run.py:483] Algo bellman_ford step 2310 current loss 0.008666, current_train_items 73952.
I0304 19:29:15.392765 22579586809984 run.py:483] Algo bellman_ford step 2311 current loss 0.023842, current_train_items 73984.
I0304 19:29:15.416674 22579586809984 run.py:483] Algo bellman_ford step 2312 current loss 0.077084, current_train_items 74016.
I0304 19:29:15.447563 22579586809984 run.py:483] Algo bellman_ford step 2313 current loss 0.073928, current_train_items 74048.
I0304 19:29:15.481546 22579586809984 run.py:483] Algo bellman_ford step 2314 current loss 0.077349, current_train_items 74080.
I0304 19:29:15.501340 22579586809984 run.py:483] Algo bellman_ford step 2315 current loss 0.010436, current_train_items 74112.
I0304 19:29:15.517772 22579586809984 run.py:483] Algo bellman_ford step 2316 current loss 0.095426, current_train_items 74144.
I0304 19:29:15.541143 22579586809984 run.py:483] Algo bellman_ford step 2317 current loss 0.081337, current_train_items 74176.
I0304 19:29:15.572660 22579586809984 run.py:483] Algo bellman_ford step 2318 current loss 0.073114, current_train_items 74208.
I0304 19:29:15.606636 22579586809984 run.py:483] Algo bellman_ford step 2319 current loss 0.110740, current_train_items 74240.
I0304 19:29:15.626293 22579586809984 run.py:483] Algo bellman_ford step 2320 current loss 0.010371, current_train_items 74272.
I0304 19:29:15.642393 22579586809984 run.py:483] Algo bellman_ford step 2321 current loss 0.033016, current_train_items 74304.
I0304 19:29:15.666676 22579586809984 run.py:483] Algo bellman_ford step 2322 current loss 0.088550, current_train_items 74336.
I0304 19:29:15.696735 22579586809984 run.py:483] Algo bellman_ford step 2323 current loss 0.099113, current_train_items 74368.
I0304 19:29:15.731028 22579586809984 run.py:483] Algo bellman_ford step 2324 current loss 0.155596, current_train_items 74400.
I0304 19:29:15.751026 22579586809984 run.py:483] Algo bellman_ford step 2325 current loss 0.007262, current_train_items 74432.
I0304 19:29:15.767223 22579586809984 run.py:483] Algo bellman_ford step 2326 current loss 0.077486, current_train_items 74464.
I0304 19:29:15.790340 22579586809984 run.py:483] Algo bellman_ford step 2327 current loss 0.217070, current_train_items 74496.
I0304 19:29:15.820095 22579586809984 run.py:483] Algo bellman_ford step 2328 current loss 0.131422, current_train_items 74528.
I0304 19:29:15.852432 22579586809984 run.py:483] Algo bellman_ford step 2329 current loss 0.147564, current_train_items 74560.
I0304 19:29:15.872100 22579586809984 run.py:483] Algo bellman_ford step 2330 current loss 0.009059, current_train_items 74592.
I0304 19:29:15.887836 22579586809984 run.py:483] Algo bellman_ford step 2331 current loss 0.036150, current_train_items 74624.
I0304 19:29:15.911943 22579586809984 run.py:483] Algo bellman_ford step 2332 current loss 0.187815, current_train_items 74656.
I0304 19:29:15.943216 22579586809984 run.py:483] Algo bellman_ford step 2333 current loss 0.089217, current_train_items 74688.
I0304 19:29:15.977990 22579586809984 run.py:483] Algo bellman_ford step 2334 current loss 0.150330, current_train_items 74720.
I0304 19:29:15.997953 22579586809984 run.py:483] Algo bellman_ford step 2335 current loss 0.007384, current_train_items 74752.
I0304 19:29:16.014497 22579586809984 run.py:483] Algo bellman_ford step 2336 current loss 0.033225, current_train_items 74784.
I0304 19:29:16.038093 22579586809984 run.py:483] Algo bellman_ford step 2337 current loss 0.048631, current_train_items 74816.
I0304 19:29:16.068362 22579586809984 run.py:483] Algo bellman_ford step 2338 current loss 0.068741, current_train_items 74848.
I0304 19:29:16.101449 22579586809984 run.py:483] Algo bellman_ford step 2339 current loss 0.114393, current_train_items 74880.
I0304 19:29:16.121408 22579586809984 run.py:483] Algo bellman_ford step 2340 current loss 0.008174, current_train_items 74912.
I0304 19:29:16.137901 22579586809984 run.py:483] Algo bellman_ford step 2341 current loss 0.023628, current_train_items 74944.
I0304 19:29:16.160829 22579586809984 run.py:483] Algo bellman_ford step 2342 current loss 0.056321, current_train_items 74976.
I0304 19:29:16.190332 22579586809984 run.py:483] Algo bellman_ford step 2343 current loss 0.095744, current_train_items 75008.
I0304 19:29:16.222275 22579586809984 run.py:483] Algo bellman_ford step 2344 current loss 0.114754, current_train_items 75040.
I0304 19:29:16.241847 22579586809984 run.py:483] Algo bellman_ford step 2345 current loss 0.017188, current_train_items 75072.
I0304 19:29:16.257872 22579586809984 run.py:483] Algo bellman_ford step 2346 current loss 0.047438, current_train_items 75104.
I0304 19:29:16.281331 22579586809984 run.py:483] Algo bellman_ford step 2347 current loss 0.025734, current_train_items 75136.
I0304 19:29:16.310763 22579586809984 run.py:483] Algo bellman_ford step 2348 current loss 0.043938, current_train_items 75168.
I0304 19:29:16.342995 22579586809984 run.py:483] Algo bellman_ford step 2349 current loss 0.104475, current_train_items 75200.
I0304 19:29:16.362875 22579586809984 run.py:483] Algo bellman_ford step 2350 current loss 0.012304, current_train_items 75232.
I0304 19:29:16.370838 22579586809984 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0304 19:29:16.370942 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:29:16.387670 22579586809984 run.py:483] Algo bellman_ford step 2351 current loss 0.020713, current_train_items 75264.
I0304 19:29:16.411547 22579586809984 run.py:483] Algo bellman_ford step 2352 current loss 0.095085, current_train_items 75296.
I0304 19:29:16.440924 22579586809984 run.py:483] Algo bellman_ford step 2353 current loss 0.080154, current_train_items 75328.
I0304 19:29:16.476490 22579586809984 run.py:483] Algo bellman_ford step 2354 current loss 0.099266, current_train_items 75360.
I0304 19:29:16.496652 22579586809984 run.py:483] Algo bellman_ford step 2355 current loss 0.019018, current_train_items 75392.
I0304 19:29:16.512303 22579586809984 run.py:483] Algo bellman_ford step 2356 current loss 0.019661, current_train_items 75424.
I0304 19:29:16.536035 22579586809984 run.py:483] Algo bellman_ford step 2357 current loss 0.067901, current_train_items 75456.
I0304 19:29:16.565557 22579586809984 run.py:483] Algo bellman_ford step 2358 current loss 0.085730, current_train_items 75488.
I0304 19:29:16.600609 22579586809984 run.py:483] Algo bellman_ford step 2359 current loss 0.078074, current_train_items 75520.
I0304 19:29:16.620561 22579586809984 run.py:483] Algo bellman_ford step 2360 current loss 0.005489, current_train_items 75552.
I0304 19:29:16.637566 22579586809984 run.py:483] Algo bellman_ford step 2361 current loss 0.056553, current_train_items 75584.
I0304 19:29:16.662094 22579586809984 run.py:483] Algo bellman_ford step 2362 current loss 0.068183, current_train_items 75616.
I0304 19:29:16.692032 22579586809984 run.py:483] Algo bellman_ford step 2363 current loss 0.060416, current_train_items 75648.
I0304 19:29:16.725212 22579586809984 run.py:483] Algo bellman_ford step 2364 current loss 0.080421, current_train_items 75680.
I0304 19:29:16.744687 22579586809984 run.py:483] Algo bellman_ford step 2365 current loss 0.004371, current_train_items 75712.
I0304 19:29:16.761171 22579586809984 run.py:483] Algo bellman_ford step 2366 current loss 0.018965, current_train_items 75744.
I0304 19:29:16.785291 22579586809984 run.py:483] Algo bellman_ford step 2367 current loss 0.128385, current_train_items 75776.
I0304 19:29:16.816888 22579586809984 run.py:483] Algo bellman_ford step 2368 current loss 0.094896, current_train_items 75808.
I0304 19:29:16.851586 22579586809984 run.py:483] Algo bellman_ford step 2369 current loss 0.099249, current_train_items 75840.
I0304 19:29:16.871783 22579586809984 run.py:483] Algo bellman_ford step 2370 current loss 0.005813, current_train_items 75872.
I0304 19:29:16.888047 22579586809984 run.py:483] Algo bellman_ford step 2371 current loss 0.092786, current_train_items 75904.
I0304 19:29:16.911439 22579586809984 run.py:483] Algo bellman_ford step 2372 current loss 0.063760, current_train_items 75936.
I0304 19:29:16.941283 22579586809984 run.py:483] Algo bellman_ford step 2373 current loss 0.131485, current_train_items 75968.
I0304 19:29:16.974043 22579586809984 run.py:483] Algo bellman_ford step 2374 current loss 0.084140, current_train_items 76000.
I0304 19:29:16.994166 22579586809984 run.py:483] Algo bellman_ford step 2375 current loss 0.011185, current_train_items 76032.
I0304 19:29:17.010651 22579586809984 run.py:483] Algo bellman_ford step 2376 current loss 0.030652, current_train_items 76064.
I0304 19:29:17.034040 22579586809984 run.py:483] Algo bellman_ford step 2377 current loss 0.121235, current_train_items 76096.
I0304 19:29:17.065836 22579586809984 run.py:483] Algo bellman_ford step 2378 current loss 0.156524, current_train_items 76128.
I0304 19:29:17.100596 22579586809984 run.py:483] Algo bellman_ford step 2379 current loss 0.192786, current_train_items 76160.
I0304 19:29:17.120527 22579586809984 run.py:483] Algo bellman_ford step 2380 current loss 0.030252, current_train_items 76192.
I0304 19:29:17.137284 22579586809984 run.py:483] Algo bellman_ford step 2381 current loss 0.058271, current_train_items 76224.
I0304 19:29:17.161611 22579586809984 run.py:483] Algo bellman_ford step 2382 current loss 0.121977, current_train_items 76256.
I0304 19:29:17.191029 22579586809984 run.py:483] Algo bellman_ford step 2383 current loss 0.138696, current_train_items 76288.
I0304 19:29:17.224905 22579586809984 run.py:483] Algo bellman_ford step 2384 current loss 0.180916, current_train_items 76320.
I0304 19:29:17.244986 22579586809984 run.py:483] Algo bellman_ford step 2385 current loss 0.011653, current_train_items 76352.
I0304 19:29:17.261204 22579586809984 run.py:483] Algo bellman_ford step 2386 current loss 0.017854, current_train_items 76384.
I0304 19:29:17.283842 22579586809984 run.py:483] Algo bellman_ford step 2387 current loss 0.067064, current_train_items 76416.
I0304 19:29:17.314509 22579586809984 run.py:483] Algo bellman_ford step 2388 current loss 0.078004, current_train_items 76448.
I0304 19:29:17.349408 22579586809984 run.py:483] Algo bellman_ford step 2389 current loss 0.132768, current_train_items 76480.
I0304 19:29:17.369447 22579586809984 run.py:483] Algo bellman_ford step 2390 current loss 0.006626, current_train_items 76512.
I0304 19:29:17.385578 22579586809984 run.py:483] Algo bellman_ford step 2391 current loss 0.064480, current_train_items 76544.
I0304 19:29:17.407799 22579586809984 run.py:483] Algo bellman_ford step 2392 current loss 0.065707, current_train_items 76576.
I0304 19:29:17.438060 22579586809984 run.py:483] Algo bellman_ford step 2393 current loss 0.080796, current_train_items 76608.
I0304 19:29:17.474451 22579586809984 run.py:483] Algo bellman_ford step 2394 current loss 0.135151, current_train_items 76640.
I0304 19:29:17.494382 22579586809984 run.py:483] Algo bellman_ford step 2395 current loss 0.012310, current_train_items 76672.
I0304 19:29:17.510759 22579586809984 run.py:483] Algo bellman_ford step 2396 current loss 0.013686, current_train_items 76704.
I0304 19:29:17.533411 22579586809984 run.py:483] Algo bellman_ford step 2397 current loss 0.037921, current_train_items 76736.
I0304 19:29:17.563249 22579586809984 run.py:483] Algo bellman_ford step 2398 current loss 0.096260, current_train_items 76768.
I0304 19:29:17.597888 22579586809984 run.py:483] Algo bellman_ford step 2399 current loss 0.107389, current_train_items 76800.
I0304 19:29:17.618245 22579586809984 run.py:483] Algo bellman_ford step 2400 current loss 0.004800, current_train_items 76832.
I0304 19:29:17.626023 22579586809984 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0304 19:29:17.626130 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:29:17.642654 22579586809984 run.py:483] Algo bellman_ford step 2401 current loss 0.025374, current_train_items 76864.
I0304 19:29:17.666773 22579586809984 run.py:483] Algo bellman_ford step 2402 current loss 0.066072, current_train_items 76896.
I0304 19:29:17.698585 22579586809984 run.py:483] Algo bellman_ford step 2403 current loss 0.121719, current_train_items 76928.
I0304 19:29:17.735746 22579586809984 run.py:483] Algo bellman_ford step 2404 current loss 0.103859, current_train_items 76960.
I0304 19:29:17.756013 22579586809984 run.py:483] Algo bellman_ford step 2405 current loss 0.007975, current_train_items 76992.
I0304 19:29:17.772177 22579586809984 run.py:483] Algo bellman_ford step 2406 current loss 0.051404, current_train_items 77024.
I0304 19:29:17.796187 22579586809984 run.py:483] Algo bellman_ford step 2407 current loss 0.069343, current_train_items 77056.
I0304 19:29:17.826145 22579586809984 run.py:483] Algo bellman_ford step 2408 current loss 0.137903, current_train_items 77088.
I0304 19:29:17.859204 22579586809984 run.py:483] Algo bellman_ford step 2409 current loss 0.114619, current_train_items 77120.
I0304 19:29:17.878695 22579586809984 run.py:483] Algo bellman_ford step 2410 current loss 0.004880, current_train_items 77152.
I0304 19:29:17.894891 22579586809984 run.py:483] Algo bellman_ford step 2411 current loss 0.029431, current_train_items 77184.
I0304 19:29:17.920049 22579586809984 run.py:483] Algo bellman_ford step 2412 current loss 0.102042, current_train_items 77216.
I0304 19:29:17.951328 22579586809984 run.py:483] Algo bellman_ford step 2413 current loss 0.070904, current_train_items 77248.
I0304 19:29:17.986058 22579586809984 run.py:483] Algo bellman_ford step 2414 current loss 0.136281, current_train_items 77280.
I0304 19:29:18.006037 22579586809984 run.py:483] Algo bellman_ford step 2415 current loss 0.006582, current_train_items 77312.
I0304 19:29:18.021678 22579586809984 run.py:483] Algo bellman_ford step 2416 current loss 0.032671, current_train_items 77344.
I0304 19:29:18.045547 22579586809984 run.py:483] Algo bellman_ford step 2417 current loss 0.070563, current_train_items 77376.
I0304 19:29:18.077615 22579586809984 run.py:483] Algo bellman_ford step 2418 current loss 0.091053, current_train_items 77408.
I0304 19:29:18.109587 22579586809984 run.py:483] Algo bellman_ford step 2419 current loss 0.063087, current_train_items 77440.
I0304 19:29:18.129186 22579586809984 run.py:483] Algo bellman_ford step 2420 current loss 0.007745, current_train_items 77472.
I0304 19:29:18.145199 22579586809984 run.py:483] Algo bellman_ford step 2421 current loss 0.040166, current_train_items 77504.
I0304 19:29:18.169615 22579586809984 run.py:483] Algo bellman_ford step 2422 current loss 0.096439, current_train_items 77536.
I0304 19:29:18.200283 22579586809984 run.py:483] Algo bellman_ford step 2423 current loss 0.067916, current_train_items 77568.
I0304 19:29:18.235021 22579586809984 run.py:483] Algo bellman_ford step 2424 current loss 0.119856, current_train_items 77600.
I0304 19:29:18.254755 22579586809984 run.py:483] Algo bellman_ford step 2425 current loss 0.037253, current_train_items 77632.
I0304 19:29:18.270787 22579586809984 run.py:483] Algo bellman_ford step 2426 current loss 0.103707, current_train_items 77664.
I0304 19:29:18.295565 22579586809984 run.py:483] Algo bellman_ford step 2427 current loss 0.145610, current_train_items 77696.
I0304 19:29:18.325069 22579586809984 run.py:483] Algo bellman_ford step 2428 current loss 0.120496, current_train_items 77728.
I0304 19:29:18.358423 22579586809984 run.py:483] Algo bellman_ford step 2429 current loss 0.088273, current_train_items 77760.
I0304 19:29:18.378071 22579586809984 run.py:483] Algo bellman_ford step 2430 current loss 0.019322, current_train_items 77792.
I0304 19:29:18.394476 22579586809984 run.py:483] Algo bellman_ford step 2431 current loss 0.028497, current_train_items 77824.
I0304 19:29:18.418928 22579586809984 run.py:483] Algo bellman_ford step 2432 current loss 0.069226, current_train_items 77856.
I0304 19:29:18.449119 22579586809984 run.py:483] Algo bellman_ford step 2433 current loss 0.066308, current_train_items 77888.
I0304 19:29:18.483223 22579586809984 run.py:483] Algo bellman_ford step 2434 current loss 0.073133, current_train_items 77920.
I0304 19:29:18.502792 22579586809984 run.py:483] Algo bellman_ford step 2435 current loss 0.009199, current_train_items 77952.
I0304 19:29:18.519146 22579586809984 run.py:483] Algo bellman_ford step 2436 current loss 0.042051, current_train_items 77984.
I0304 19:29:18.542726 22579586809984 run.py:483] Algo bellman_ford step 2437 current loss 0.106678, current_train_items 78016.
I0304 19:29:18.571989 22579586809984 run.py:483] Algo bellman_ford step 2438 current loss 0.145600, current_train_items 78048.
I0304 19:29:18.605091 22579586809984 run.py:483] Algo bellman_ford step 2439 current loss 0.079896, current_train_items 78080.
I0304 19:29:18.625015 22579586809984 run.py:483] Algo bellman_ford step 2440 current loss 0.027341, current_train_items 78112.
I0304 19:29:18.641275 22579586809984 run.py:483] Algo bellman_ford step 2441 current loss 0.023765, current_train_items 78144.
I0304 19:29:18.666075 22579586809984 run.py:483] Algo bellman_ford step 2442 current loss 0.218256, current_train_items 78176.
I0304 19:29:18.696739 22579586809984 run.py:483] Algo bellman_ford step 2443 current loss 0.184085, current_train_items 78208.
I0304 19:29:18.731814 22579586809984 run.py:483] Algo bellman_ford step 2444 current loss 0.175664, current_train_items 78240.
I0304 19:29:18.751535 22579586809984 run.py:483] Algo bellman_ford step 2445 current loss 0.029788, current_train_items 78272.
I0304 19:29:18.767777 22579586809984 run.py:483] Algo bellman_ford step 2446 current loss 0.033405, current_train_items 78304.
I0304 19:29:18.792361 22579586809984 run.py:483] Algo bellman_ford step 2447 current loss 0.064656, current_train_items 78336.
I0304 19:29:18.822110 22579586809984 run.py:483] Algo bellman_ford step 2448 current loss 0.096057, current_train_items 78368.
I0304 19:29:18.855190 22579586809984 run.py:483] Algo bellman_ford step 2449 current loss 0.147026, current_train_items 78400.
I0304 19:29:18.874636 22579586809984 run.py:483] Algo bellman_ford step 2450 current loss 0.005437, current_train_items 78432.
I0304 19:29:18.882777 22579586809984 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0304 19:29:18.882882 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:29:18.899647 22579586809984 run.py:483] Algo bellman_ford step 2451 current loss 0.025644, current_train_items 78464.
I0304 19:29:18.923903 22579586809984 run.py:483] Algo bellman_ford step 2452 current loss 0.070021, current_train_items 78496.
I0304 19:29:18.954730 22579586809984 run.py:483] Algo bellman_ford step 2453 current loss 0.088600, current_train_items 78528.
I0304 19:29:18.989985 22579586809984 run.py:483] Algo bellman_ford step 2454 current loss 0.110883, current_train_items 78560.
I0304 19:29:19.010295 22579586809984 run.py:483] Algo bellman_ford step 2455 current loss 0.042024, current_train_items 78592.
I0304 19:29:19.026374 22579586809984 run.py:483] Algo bellman_ford step 2456 current loss 0.035621, current_train_items 78624.
I0304 19:29:19.049413 22579586809984 run.py:483] Algo bellman_ford step 2457 current loss 0.038231, current_train_items 78656.
I0304 19:29:19.080474 22579586809984 run.py:483] Algo bellman_ford step 2458 current loss 0.094072, current_train_items 78688.
I0304 19:29:19.115816 22579586809984 run.py:483] Algo bellman_ford step 2459 current loss 0.211749, current_train_items 78720.
I0304 19:29:19.135856 22579586809984 run.py:483] Algo bellman_ford step 2460 current loss 0.006126, current_train_items 78752.
I0304 19:29:19.152770 22579586809984 run.py:483] Algo bellman_ford step 2461 current loss 0.045818, current_train_items 78784.
I0304 19:29:19.175911 22579586809984 run.py:483] Algo bellman_ford step 2462 current loss 0.099141, current_train_items 78816.
I0304 19:29:19.207514 22579586809984 run.py:483] Algo bellman_ford step 2463 current loss 0.130194, current_train_items 78848.
I0304 19:29:19.241621 22579586809984 run.py:483] Algo bellman_ford step 2464 current loss 0.125832, current_train_items 78880.
I0304 19:29:19.262103 22579586809984 run.py:483] Algo bellman_ford step 2465 current loss 0.018911, current_train_items 78912.
I0304 19:29:19.278421 22579586809984 run.py:483] Algo bellman_ford step 2466 current loss 0.053555, current_train_items 78944.
I0304 19:29:19.302393 22579586809984 run.py:483] Algo bellman_ford step 2467 current loss 0.055053, current_train_items 78976.
I0304 19:29:19.333572 22579586809984 run.py:483] Algo bellman_ford step 2468 current loss 0.079391, current_train_items 79008.
I0304 19:29:19.369256 22579586809984 run.py:483] Algo bellman_ford step 2469 current loss 0.103104, current_train_items 79040.
I0304 19:29:19.389226 22579586809984 run.py:483] Algo bellman_ford step 2470 current loss 0.006475, current_train_items 79072.
I0304 19:29:19.405638 22579586809984 run.py:483] Algo bellman_ford step 2471 current loss 0.086960, current_train_items 79104.
I0304 19:29:19.429882 22579586809984 run.py:483] Algo bellman_ford step 2472 current loss 0.146320, current_train_items 79136.
I0304 19:29:19.461549 22579586809984 run.py:483] Algo bellman_ford step 2473 current loss 0.159525, current_train_items 79168.
I0304 19:29:19.494662 22579586809984 run.py:483] Algo bellman_ford step 2474 current loss 0.072749, current_train_items 79200.
I0304 19:29:19.515003 22579586809984 run.py:483] Algo bellman_ford step 2475 current loss 0.064785, current_train_items 79232.
I0304 19:29:19.531325 22579586809984 run.py:483] Algo bellman_ford step 2476 current loss 0.039296, current_train_items 79264.
I0304 19:29:19.553469 22579586809984 run.py:483] Algo bellman_ford step 2477 current loss 0.042586, current_train_items 79296.
I0304 19:29:19.583535 22579586809984 run.py:483] Algo bellman_ford step 2478 current loss 0.139766, current_train_items 79328.
I0304 19:29:19.619067 22579586809984 run.py:483] Algo bellman_ford step 2479 current loss 0.147594, current_train_items 79360.
I0304 19:29:19.638861 22579586809984 run.py:483] Algo bellman_ford step 2480 current loss 0.008284, current_train_items 79392.
I0304 19:29:19.654836 22579586809984 run.py:483] Algo bellman_ford step 2481 current loss 0.037036, current_train_items 79424.
I0304 19:29:19.679614 22579586809984 run.py:483] Algo bellman_ford step 2482 current loss 0.104428, current_train_items 79456.
I0304 19:29:19.710502 22579586809984 run.py:483] Algo bellman_ford step 2483 current loss 0.154219, current_train_items 79488.
I0304 19:29:19.746089 22579586809984 run.py:483] Algo bellman_ford step 2484 current loss 0.107446, current_train_items 79520.
I0304 19:29:19.766608 22579586809984 run.py:483] Algo bellman_ford step 2485 current loss 0.007061, current_train_items 79552.
I0304 19:29:19.783115 22579586809984 run.py:483] Algo bellman_ford step 2486 current loss 0.054918, current_train_items 79584.
I0304 19:29:19.807698 22579586809984 run.py:483] Algo bellman_ford step 2487 current loss 0.077709, current_train_items 79616.
I0304 19:29:19.835764 22579586809984 run.py:483] Algo bellman_ford step 2488 current loss 0.045240, current_train_items 79648.
I0304 19:29:19.870317 22579586809984 run.py:483] Algo bellman_ford step 2489 current loss 0.119958, current_train_items 79680.
I0304 19:29:19.890212 22579586809984 run.py:483] Algo bellman_ford step 2490 current loss 0.020888, current_train_items 79712.
I0304 19:29:19.906574 22579586809984 run.py:483] Algo bellman_ford step 2491 current loss 0.049383, current_train_items 79744.
I0304 19:29:19.929886 22579586809984 run.py:483] Algo bellman_ford step 2492 current loss 0.045605, current_train_items 79776.
I0304 19:29:19.960309 22579586809984 run.py:483] Algo bellman_ford step 2493 current loss 0.060433, current_train_items 79808.
I0304 19:29:19.992950 22579586809984 run.py:483] Algo bellman_ford step 2494 current loss 0.117072, current_train_items 79840.
I0304 19:29:20.012673 22579586809984 run.py:483] Algo bellman_ford step 2495 current loss 0.007216, current_train_items 79872.
I0304 19:29:20.029717 22579586809984 run.py:483] Algo bellman_ford step 2496 current loss 0.039571, current_train_items 79904.
I0304 19:29:20.053045 22579586809984 run.py:483] Algo bellman_ford step 2497 current loss 0.069627, current_train_items 79936.
I0304 19:29:20.082181 22579586809984 run.py:483] Algo bellman_ford step 2498 current loss 0.105827, current_train_items 79968.
I0304 19:29:20.115943 22579586809984 run.py:483] Algo bellman_ford step 2499 current loss 0.105123, current_train_items 80000.
I0304 19:29:20.136386 22579586809984 run.py:483] Algo bellman_ford step 2500 current loss 0.004604, current_train_items 80032.
I0304 19:29:20.144188 22579586809984 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0304 19:29:20.144292 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.985, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0304 19:29:20.161228 22579586809984 run.py:483] Algo bellman_ford step 2501 current loss 0.041871, current_train_items 80064.
I0304 19:29:20.186045 22579586809984 run.py:483] Algo bellman_ford step 2502 current loss 0.123687, current_train_items 80096.
I0304 19:29:20.215954 22579586809984 run.py:483] Algo bellman_ford step 2503 current loss 0.101193, current_train_items 80128.
I0304 19:29:20.251078 22579586809984 run.py:483] Algo bellman_ford step 2504 current loss 0.115310, current_train_items 80160.
I0304 19:29:20.271292 22579586809984 run.py:483] Algo bellman_ford step 2505 current loss 0.008406, current_train_items 80192.
I0304 19:29:20.287367 22579586809984 run.py:483] Algo bellman_ford step 2506 current loss 0.030782, current_train_items 80224.
I0304 19:29:20.311475 22579586809984 run.py:483] Algo bellman_ford step 2507 current loss 0.104920, current_train_items 80256.
I0304 19:29:20.341248 22579586809984 run.py:483] Algo bellman_ford step 2508 current loss 0.123118, current_train_items 80288.
I0304 19:29:20.375817 22579586809984 run.py:483] Algo bellman_ford step 2509 current loss 0.155572, current_train_items 80320.
I0304 19:29:20.395489 22579586809984 run.py:483] Algo bellman_ford step 2510 current loss 0.019022, current_train_items 80352.
I0304 19:29:20.412033 22579586809984 run.py:483] Algo bellman_ford step 2511 current loss 0.082987, current_train_items 80384.
I0304 19:29:20.435890 22579586809984 run.py:483] Algo bellman_ford step 2512 current loss 0.109048, current_train_items 80416.
I0304 19:29:20.465519 22579586809984 run.py:483] Algo bellman_ford step 2513 current loss 0.181090, current_train_items 80448.
I0304 19:29:20.500563 22579586809984 run.py:483] Algo bellman_ford step 2514 current loss 0.104380, current_train_items 80480.
I0304 19:29:20.520493 22579586809984 run.py:483] Algo bellman_ford step 2515 current loss 0.009332, current_train_items 80512.
I0304 19:29:20.536943 22579586809984 run.py:483] Algo bellman_ford step 2516 current loss 0.048966, current_train_items 80544.
I0304 19:29:20.560748 22579586809984 run.py:483] Algo bellman_ford step 2517 current loss 0.286778, current_train_items 80576.
I0304 19:29:20.590622 22579586809984 run.py:483] Algo bellman_ford step 2518 current loss 0.138718, current_train_items 80608.
I0304 19:29:20.624828 22579586809984 run.py:483] Algo bellman_ford step 2519 current loss 0.135792, current_train_items 80640.
I0304 19:29:20.644694 22579586809984 run.py:483] Algo bellman_ford step 2520 current loss 0.008295, current_train_items 80672.
I0304 19:29:20.660935 22579586809984 run.py:483] Algo bellman_ford step 2521 current loss 0.050455, current_train_items 80704.
I0304 19:29:20.683336 22579586809984 run.py:483] Algo bellman_ford step 2522 current loss 0.036895, current_train_items 80736.
I0304 19:29:20.712971 22579586809984 run.py:483] Algo bellman_ford step 2523 current loss 0.072082, current_train_items 80768.
I0304 19:29:20.746337 22579586809984 run.py:483] Algo bellman_ford step 2524 current loss 0.130398, current_train_items 80800.
I0304 19:29:20.766298 22579586809984 run.py:483] Algo bellman_ford step 2525 current loss 0.010567, current_train_items 80832.
I0304 19:29:20.782562 22579586809984 run.py:483] Algo bellman_ford step 2526 current loss 0.019554, current_train_items 80864.
I0304 19:29:20.806500 22579586809984 run.py:483] Algo bellman_ford step 2527 current loss 0.096409, current_train_items 80896.
I0304 19:29:20.836110 22579586809984 run.py:483] Algo bellman_ford step 2528 current loss 0.064297, current_train_items 80928.
I0304 19:29:20.870671 22579586809984 run.py:483] Algo bellman_ford step 2529 current loss 0.100323, current_train_items 80960.
I0304 19:29:20.890379 22579586809984 run.py:483] Algo bellman_ford step 2530 current loss 0.005507, current_train_items 80992.
I0304 19:29:20.906251 22579586809984 run.py:483] Algo bellman_ford step 2531 current loss 0.032427, current_train_items 81024.
I0304 19:29:20.930126 22579586809984 run.py:483] Algo bellman_ford step 2532 current loss 0.079609, current_train_items 81056.
I0304 19:29:20.960800 22579586809984 run.py:483] Algo bellman_ford step 2533 current loss 0.090003, current_train_items 81088.
I0304 19:29:20.994107 22579586809984 run.py:483] Algo bellman_ford step 2534 current loss 0.093722, current_train_items 81120.
I0304 19:29:21.013781 22579586809984 run.py:483] Algo bellman_ford step 2535 current loss 0.004298, current_train_items 81152.
I0304 19:29:21.029660 22579586809984 run.py:483] Algo bellman_ford step 2536 current loss 0.031708, current_train_items 81184.
W0304 19:29:21.046390 22579586809984 samplers.py:155] Increasing hint lengh from 9 to 10
I0304 19:29:27.692215 22579586809984 run.py:483] Algo bellman_ford step 2537 current loss 0.120855, current_train_items 81216.
I0304 19:29:27.724868 22579586809984 run.py:483] Algo bellman_ford step 2538 current loss 0.084346, current_train_items 81248.
I0304 19:29:27.759631 22579586809984 run.py:483] Algo bellman_ford step 2539 current loss 0.115560, current_train_items 81280.
I0304 19:29:27.780078 22579586809984 run.py:483] Algo bellman_ford step 2540 current loss 0.016225, current_train_items 81312.
I0304 19:29:27.796207 22579586809984 run.py:483] Algo bellman_ford step 2541 current loss 0.017002, current_train_items 81344.
I0304 19:29:27.820634 22579586809984 run.py:483] Algo bellman_ford step 2542 current loss 0.216572, current_train_items 81376.
I0304 19:29:27.850497 22579586809984 run.py:483] Algo bellman_ford step 2543 current loss 0.177460, current_train_items 81408.
I0304 19:29:27.884670 22579586809984 run.py:483] Algo bellman_ford step 2544 current loss 0.167359, current_train_items 81440.
I0304 19:29:27.904803 22579586809984 run.py:483] Algo bellman_ford step 2545 current loss 0.009833, current_train_items 81472.
I0304 19:29:27.921293 22579586809984 run.py:483] Algo bellman_ford step 2546 current loss 0.025908, current_train_items 81504.
I0304 19:29:27.945794 22579586809984 run.py:483] Algo bellman_ford step 2547 current loss 0.054330, current_train_items 81536.
I0304 19:29:27.977943 22579586809984 run.py:483] Algo bellman_ford step 2548 current loss 0.085088, current_train_items 81568.
I0304 19:29:28.011247 22579586809984 run.py:483] Algo bellman_ford step 2549 current loss 0.091743, current_train_items 81600.
I0304 19:29:28.031212 22579586809984 run.py:483] Algo bellman_ford step 2550 current loss 0.005987, current_train_items 81632.
I0304 19:29:28.041560 22579586809984 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0304 19:29:28.041666 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.985, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:29:28.071661 22579586809984 run.py:483] Algo bellman_ford step 2551 current loss 0.016609, current_train_items 81664.
I0304 19:29:28.096562 22579586809984 run.py:483] Algo bellman_ford step 2552 current loss 0.048727, current_train_items 81696.
I0304 19:29:28.129343 22579586809984 run.py:483] Algo bellman_ford step 2553 current loss 0.104532, current_train_items 81728.
I0304 19:29:28.164069 22579586809984 run.py:483] Algo bellman_ford step 2554 current loss 0.094724, current_train_items 81760.
I0304 19:29:28.184526 22579586809984 run.py:483] Algo bellman_ford step 2555 current loss 0.008126, current_train_items 81792.
I0304 19:29:28.201058 22579586809984 run.py:483] Algo bellman_ford step 2556 current loss 0.039393, current_train_items 81824.
I0304 19:29:28.225525 22579586809984 run.py:483] Algo bellman_ford step 2557 current loss 0.064333, current_train_items 81856.
I0304 19:29:28.257429 22579586809984 run.py:483] Algo bellman_ford step 2558 current loss 0.084791, current_train_items 81888.
I0304 19:29:28.293368 22579586809984 run.py:483] Algo bellman_ford step 2559 current loss 0.103891, current_train_items 81920.
I0304 19:29:28.313738 22579586809984 run.py:483] Algo bellman_ford step 2560 current loss 0.017572, current_train_items 81952.
I0304 19:29:28.330551 22579586809984 run.py:483] Algo bellman_ford step 2561 current loss 0.013879, current_train_items 81984.
I0304 19:29:28.354458 22579586809984 run.py:483] Algo bellman_ford step 2562 current loss 0.050635, current_train_items 82016.
I0304 19:29:28.387157 22579586809984 run.py:483] Algo bellman_ford step 2563 current loss 0.109646, current_train_items 82048.
I0304 19:29:28.421450 22579586809984 run.py:483] Algo bellman_ford step 2564 current loss 0.067697, current_train_items 82080.
I0304 19:29:28.441555 22579586809984 run.py:483] Algo bellman_ford step 2565 current loss 0.004621, current_train_items 82112.
I0304 19:29:28.458071 22579586809984 run.py:483] Algo bellman_ford step 2566 current loss 0.023586, current_train_items 82144.
I0304 19:29:28.482212 22579586809984 run.py:483] Algo bellman_ford step 2567 current loss 0.044466, current_train_items 82176.
I0304 19:29:28.514102 22579586809984 run.py:483] Algo bellman_ford step 2568 current loss 0.066720, current_train_items 82208.
I0304 19:29:28.547550 22579586809984 run.py:483] Algo bellman_ford step 2569 current loss 0.066070, current_train_items 82240.
I0304 19:29:28.567623 22579586809984 run.py:483] Algo bellman_ford step 2570 current loss 0.003729, current_train_items 82272.
I0304 19:29:28.583968 22579586809984 run.py:483] Algo bellman_ford step 2571 current loss 0.017056, current_train_items 82304.
I0304 19:29:28.606705 22579586809984 run.py:483] Algo bellman_ford step 2572 current loss 0.034396, current_train_items 82336.
I0304 19:29:28.638008 22579586809984 run.py:483] Algo bellman_ford step 2573 current loss 0.036508, current_train_items 82368.
I0304 19:29:28.673376 22579586809984 run.py:483] Algo bellman_ford step 2574 current loss 0.110505, current_train_items 82400.
I0304 19:29:28.693561 22579586809984 run.py:483] Algo bellman_ford step 2575 current loss 0.015639, current_train_items 82432.
I0304 19:29:28.709768 22579586809984 run.py:483] Algo bellman_ford step 2576 current loss 0.019397, current_train_items 82464.
I0304 19:29:28.732911 22579586809984 run.py:483] Algo bellman_ford step 2577 current loss 0.040226, current_train_items 82496.
I0304 19:29:28.765130 22579586809984 run.py:483] Algo bellman_ford step 2578 current loss 0.045334, current_train_items 82528.
I0304 19:29:28.797833 22579586809984 run.py:483] Algo bellman_ford step 2579 current loss 0.089532, current_train_items 82560.
I0304 19:29:28.817751 22579586809984 run.py:483] Algo bellman_ford step 2580 current loss 0.004575, current_train_items 82592.
I0304 19:29:28.834188 22579586809984 run.py:483] Algo bellman_ford step 2581 current loss 0.019365, current_train_items 82624.
I0304 19:29:28.857969 22579586809984 run.py:483] Algo bellman_ford step 2582 current loss 0.060149, current_train_items 82656.
I0304 19:29:28.888528 22579586809984 run.py:483] Algo bellman_ford step 2583 current loss 0.049763, current_train_items 82688.
I0304 19:29:28.923540 22579586809984 run.py:483] Algo bellman_ford step 2584 current loss 0.070275, current_train_items 82720.
I0304 19:29:28.943796 22579586809984 run.py:483] Algo bellman_ford step 2585 current loss 0.004237, current_train_items 82752.
I0304 19:29:28.960284 22579586809984 run.py:483] Algo bellman_ford step 2586 current loss 0.029789, current_train_items 82784.
I0304 19:29:28.983666 22579586809984 run.py:483] Algo bellman_ford step 2587 current loss 0.050355, current_train_items 82816.
I0304 19:29:29.014708 22579586809984 run.py:483] Algo bellman_ford step 2588 current loss 0.097750, current_train_items 82848.
I0304 19:29:29.049324 22579586809984 run.py:483] Algo bellman_ford step 2589 current loss 0.105751, current_train_items 82880.
I0304 19:29:29.069320 22579586809984 run.py:483] Algo bellman_ford step 2590 current loss 0.003675, current_train_items 82912.
I0304 19:29:29.085920 22579586809984 run.py:483] Algo bellman_ford step 2591 current loss 0.019141, current_train_items 82944.
I0304 19:29:29.110453 22579586809984 run.py:483] Algo bellman_ford step 2592 current loss 0.128836, current_train_items 82976.
I0304 19:29:29.142297 22579586809984 run.py:483] Algo bellman_ford step 2593 current loss 0.100739, current_train_items 83008.
I0304 19:29:29.174453 22579586809984 run.py:483] Algo bellman_ford step 2594 current loss 0.110226, current_train_items 83040.
I0304 19:29:29.193951 22579586809984 run.py:483] Algo bellman_ford step 2595 current loss 0.011557, current_train_items 83072.
I0304 19:29:29.210572 22579586809984 run.py:483] Algo bellman_ford step 2596 current loss 0.039766, current_train_items 83104.
I0304 19:29:29.235738 22579586809984 run.py:483] Algo bellman_ford step 2597 current loss 0.077280, current_train_items 83136.
I0304 19:29:29.267459 22579586809984 run.py:483] Algo bellman_ford step 2598 current loss 0.093661, current_train_items 83168.
I0304 19:29:29.301238 22579586809984 run.py:483] Algo bellman_ford step 2599 current loss 0.145289, current_train_items 83200.
I0304 19:29:29.321702 22579586809984 run.py:483] Algo bellman_ford step 2600 current loss 0.010684, current_train_items 83232.
I0304 19:29:29.329321 22579586809984 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0304 19:29:29.329427 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:29:29.346345 22579586809984 run.py:483] Algo bellman_ford step 2601 current loss 0.023279, current_train_items 83264.
I0304 19:29:29.370756 22579586809984 run.py:483] Algo bellman_ford step 2602 current loss 0.061216, current_train_items 83296.
I0304 19:29:29.400660 22579586809984 run.py:483] Algo bellman_ford step 2603 current loss 0.068051, current_train_items 83328.
I0304 19:29:29.434442 22579586809984 run.py:483] Algo bellman_ford step 2604 current loss 0.064033, current_train_items 83360.
I0304 19:29:29.454596 22579586809984 run.py:483] Algo bellman_ford step 2605 current loss 0.028338, current_train_items 83392.
I0304 19:29:29.470617 22579586809984 run.py:483] Algo bellman_ford step 2606 current loss 0.012902, current_train_items 83424.
I0304 19:29:29.495018 22579586809984 run.py:483] Algo bellman_ford step 2607 current loss 0.036479, current_train_items 83456.
I0304 19:29:29.526832 22579586809984 run.py:483] Algo bellman_ford step 2608 current loss 0.064035, current_train_items 83488.
I0304 19:29:29.561454 22579586809984 run.py:483] Algo bellman_ford step 2609 current loss 0.099567, current_train_items 83520.
I0304 19:29:29.581179 22579586809984 run.py:483] Algo bellman_ford step 2610 current loss 0.007768, current_train_items 83552.
I0304 19:29:29.597416 22579586809984 run.py:483] Algo bellman_ford step 2611 current loss 0.012100, current_train_items 83584.
I0304 19:29:29.621162 22579586809984 run.py:483] Algo bellman_ford step 2612 current loss 0.039075, current_train_items 83616.
I0304 19:29:29.652041 22579586809984 run.py:483] Algo bellman_ford step 2613 current loss 0.054422, current_train_items 83648.
I0304 19:29:29.684979 22579586809984 run.py:483] Algo bellman_ford step 2614 current loss 0.073924, current_train_items 83680.
I0304 19:29:29.705028 22579586809984 run.py:483] Algo bellman_ford step 2615 current loss 0.003689, current_train_items 83712.
I0304 19:29:29.721027 22579586809984 run.py:483] Algo bellman_ford step 2616 current loss 0.021383, current_train_items 83744.
I0304 19:29:29.746005 22579586809984 run.py:483] Algo bellman_ford step 2617 current loss 0.065264, current_train_items 83776.
I0304 19:29:29.778303 22579586809984 run.py:483] Algo bellman_ford step 2618 current loss 0.096390, current_train_items 83808.
I0304 19:29:29.810887 22579586809984 run.py:483] Algo bellman_ford step 2619 current loss 0.070145, current_train_items 83840.
I0304 19:29:29.830558 22579586809984 run.py:483] Algo bellman_ford step 2620 current loss 0.004453, current_train_items 83872.
I0304 19:29:29.846888 22579586809984 run.py:483] Algo bellman_ford step 2621 current loss 0.012106, current_train_items 83904.
I0304 19:29:29.871466 22579586809984 run.py:483] Algo bellman_ford step 2622 current loss 0.050766, current_train_items 83936.
I0304 19:29:29.902262 22579586809984 run.py:483] Algo bellman_ford step 2623 current loss 0.043733, current_train_items 83968.
I0304 19:29:29.934948 22579586809984 run.py:483] Algo bellman_ford step 2624 current loss 0.077530, current_train_items 84000.
I0304 19:29:29.954756 22579586809984 run.py:483] Algo bellman_ford step 2625 current loss 0.017967, current_train_items 84032.
I0304 19:29:29.971004 22579586809984 run.py:483] Algo bellman_ford step 2626 current loss 0.018323, current_train_items 84064.
I0304 19:29:29.995347 22579586809984 run.py:483] Algo bellman_ford step 2627 current loss 0.035397, current_train_items 84096.
I0304 19:29:30.027153 22579586809984 run.py:483] Algo bellman_ford step 2628 current loss 0.075282, current_train_items 84128.
I0304 19:29:30.060872 22579586809984 run.py:483] Algo bellman_ford step 2629 current loss 0.102356, current_train_items 84160.
I0304 19:29:30.080693 22579586809984 run.py:483] Algo bellman_ford step 2630 current loss 0.006949, current_train_items 84192.
I0304 19:29:30.096975 22579586809984 run.py:483] Algo bellman_ford step 2631 current loss 0.032455, current_train_items 84224.
I0304 19:29:30.120436 22579586809984 run.py:483] Algo bellman_ford step 2632 current loss 0.053727, current_train_items 84256.
I0304 19:29:30.151219 22579586809984 run.py:483] Algo bellman_ford step 2633 current loss 0.040555, current_train_items 84288.
I0304 19:29:30.186870 22579586809984 run.py:483] Algo bellman_ford step 2634 current loss 0.130292, current_train_items 84320.
I0304 19:29:30.207059 22579586809984 run.py:483] Algo bellman_ford step 2635 current loss 0.006620, current_train_items 84352.
I0304 19:29:30.223102 22579586809984 run.py:483] Algo bellman_ford step 2636 current loss 0.028177, current_train_items 84384.
I0304 19:29:30.247618 22579586809984 run.py:483] Algo bellman_ford step 2637 current loss 0.091646, current_train_items 84416.
I0304 19:29:30.279827 22579586809984 run.py:483] Algo bellman_ford step 2638 current loss 0.092723, current_train_items 84448.
I0304 19:29:30.313967 22579586809984 run.py:483] Algo bellman_ford step 2639 current loss 0.114002, current_train_items 84480.
I0304 19:29:30.333497 22579586809984 run.py:483] Algo bellman_ford step 2640 current loss 0.010088, current_train_items 84512.
I0304 19:29:30.349774 22579586809984 run.py:483] Algo bellman_ford step 2641 current loss 0.017339, current_train_items 84544.
I0304 19:29:30.373057 22579586809984 run.py:483] Algo bellman_ford step 2642 current loss 0.060100, current_train_items 84576.
I0304 19:29:30.404203 22579586809984 run.py:483] Algo bellman_ford step 2643 current loss 0.100136, current_train_items 84608.
I0304 19:29:30.438879 22579586809984 run.py:483] Algo bellman_ford step 2644 current loss 0.084992, current_train_items 84640.
I0304 19:29:30.458609 22579586809984 run.py:483] Algo bellman_ford step 2645 current loss 0.008230, current_train_items 84672.
I0304 19:29:30.474734 22579586809984 run.py:483] Algo bellman_ford step 2646 current loss 0.045159, current_train_items 84704.
I0304 19:29:30.498491 22579586809984 run.py:483] Algo bellman_ford step 2647 current loss 0.062933, current_train_items 84736.
I0304 19:29:30.530299 22579586809984 run.py:483] Algo bellman_ford step 2648 current loss 0.070502, current_train_items 84768.
I0304 19:29:30.566382 22579586809984 run.py:483] Algo bellman_ford step 2649 current loss 0.122510, current_train_items 84800.
I0304 19:29:30.586150 22579586809984 run.py:483] Algo bellman_ford step 2650 current loss 0.007646, current_train_items 84832.
I0304 19:29:30.594070 22579586809984 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0304 19:29:30.594177 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:29:30.610774 22579586809984 run.py:483] Algo bellman_ford step 2651 current loss 0.028676, current_train_items 84864.
I0304 19:29:30.634796 22579586809984 run.py:483] Algo bellman_ford step 2652 current loss 0.113360, current_train_items 84896.
I0304 19:29:30.666889 22579586809984 run.py:483] Algo bellman_ford step 2653 current loss 0.235801, current_train_items 84928.
I0304 19:29:30.700807 22579586809984 run.py:483] Algo bellman_ford step 2654 current loss 0.150981, current_train_items 84960.
I0304 19:29:30.720574 22579586809984 run.py:483] Algo bellman_ford step 2655 current loss 0.009288, current_train_items 84992.
I0304 19:29:30.736676 22579586809984 run.py:483] Algo bellman_ford step 2656 current loss 0.029119, current_train_items 85024.
I0304 19:29:30.760321 22579586809984 run.py:483] Algo bellman_ford step 2657 current loss 0.040369, current_train_items 85056.
I0304 19:29:30.790575 22579586809984 run.py:483] Algo bellman_ford step 2658 current loss 0.057413, current_train_items 85088.
I0304 19:29:30.826315 22579586809984 run.py:483] Algo bellman_ford step 2659 current loss 0.104277, current_train_items 85120.
I0304 19:29:30.846312 22579586809984 run.py:483] Algo bellman_ford step 2660 current loss 0.007874, current_train_items 85152.
I0304 19:29:30.863223 22579586809984 run.py:483] Algo bellman_ford step 2661 current loss 0.022549, current_train_items 85184.
I0304 19:29:30.886636 22579586809984 run.py:483] Algo bellman_ford step 2662 current loss 0.061522, current_train_items 85216.
I0304 19:29:30.917739 22579586809984 run.py:483] Algo bellman_ford step 2663 current loss 0.109235, current_train_items 85248.
I0304 19:29:30.953745 22579586809984 run.py:483] Algo bellman_ford step 2664 current loss 0.112147, current_train_items 85280.
I0304 19:29:30.973406 22579586809984 run.py:483] Algo bellman_ford step 2665 current loss 0.008846, current_train_items 85312.
I0304 19:29:30.989747 22579586809984 run.py:483] Algo bellman_ford step 2666 current loss 0.049324, current_train_items 85344.
I0304 19:29:31.014290 22579586809984 run.py:483] Algo bellman_ford step 2667 current loss 0.052133, current_train_items 85376.
I0304 19:29:31.045926 22579586809984 run.py:483] Algo bellman_ford step 2668 current loss 0.102161, current_train_items 85408.
I0304 19:29:31.078792 22579586809984 run.py:483] Algo bellman_ford step 2669 current loss 0.067349, current_train_items 85440.
I0304 19:29:31.098577 22579586809984 run.py:483] Algo bellman_ford step 2670 current loss 0.006733, current_train_items 85472.
I0304 19:29:31.115160 22579586809984 run.py:483] Algo bellman_ford step 2671 current loss 0.016400, current_train_items 85504.
I0304 19:29:31.138812 22579586809984 run.py:483] Algo bellman_ford step 2672 current loss 0.054996, current_train_items 85536.
I0304 19:29:31.170897 22579586809984 run.py:483] Algo bellman_ford step 2673 current loss 0.067798, current_train_items 85568.
I0304 19:29:31.206189 22579586809984 run.py:483] Algo bellman_ford step 2674 current loss 0.062858, current_train_items 85600.
I0304 19:29:31.225984 22579586809984 run.py:483] Algo bellman_ford step 2675 current loss 0.010634, current_train_items 85632.
I0304 19:29:31.242314 22579586809984 run.py:483] Algo bellman_ford step 2676 current loss 0.054286, current_train_items 85664.
I0304 19:29:31.266800 22579586809984 run.py:483] Algo bellman_ford step 2677 current loss 0.089490, current_train_items 85696.
I0304 19:29:31.296314 22579586809984 run.py:483] Algo bellman_ford step 2678 current loss 0.019607, current_train_items 85728.
I0304 19:29:31.328753 22579586809984 run.py:483] Algo bellman_ford step 2679 current loss 0.064273, current_train_items 85760.
I0304 19:29:31.348342 22579586809984 run.py:483] Algo bellman_ford step 2680 current loss 0.003891, current_train_items 85792.
I0304 19:29:31.364709 22579586809984 run.py:483] Algo bellman_ford step 2681 current loss 0.018467, current_train_items 85824.
I0304 19:29:31.388567 22579586809984 run.py:483] Algo bellman_ford step 2682 current loss 0.035679, current_train_items 85856.
I0304 19:29:31.420544 22579586809984 run.py:483] Algo bellman_ford step 2683 current loss 0.060333, current_train_items 85888.
I0304 19:29:31.454506 22579586809984 run.py:483] Algo bellman_ford step 2684 current loss 0.087637, current_train_items 85920.
I0304 19:29:31.474408 22579586809984 run.py:483] Algo bellman_ford step 2685 current loss 0.004127, current_train_items 85952.
I0304 19:29:31.490752 22579586809984 run.py:483] Algo bellman_ford step 2686 current loss 0.030587, current_train_items 85984.
I0304 19:29:31.514653 22579586809984 run.py:483] Algo bellman_ford step 2687 current loss 0.048888, current_train_items 86016.
I0304 19:29:31.545488 22579586809984 run.py:483] Algo bellman_ford step 2688 current loss 0.092057, current_train_items 86048.
I0304 19:29:31.578078 22579586809984 run.py:483] Algo bellman_ford step 2689 current loss 0.062937, current_train_items 86080.
I0304 19:29:31.598247 22579586809984 run.py:483] Algo bellman_ford step 2690 current loss 0.026707, current_train_items 86112.
I0304 19:29:31.614663 22579586809984 run.py:483] Algo bellman_ford step 2691 current loss 0.022350, current_train_items 86144.
I0304 19:29:31.639385 22579586809984 run.py:483] Algo bellman_ford step 2692 current loss 0.048457, current_train_items 86176.
I0304 19:29:31.671590 22579586809984 run.py:483] Algo bellman_ford step 2693 current loss 0.084962, current_train_items 86208.
I0304 19:29:31.705547 22579586809984 run.py:483] Algo bellman_ford step 2694 current loss 0.092177, current_train_items 86240.
I0304 19:29:31.725189 22579586809984 run.py:483] Algo bellman_ford step 2695 current loss 0.005013, current_train_items 86272.
I0304 19:29:31.741260 22579586809984 run.py:483] Algo bellman_ford step 2696 current loss 0.009808, current_train_items 86304.
I0304 19:29:31.765460 22579586809984 run.py:483] Algo bellman_ford step 2697 current loss 0.076655, current_train_items 86336.
I0304 19:29:31.796192 22579586809984 run.py:483] Algo bellman_ford step 2698 current loss 0.071997, current_train_items 86368.
I0304 19:29:31.828227 22579586809984 run.py:483] Algo bellman_ford step 2699 current loss 0.111627, current_train_items 86400.
I0304 19:29:31.848364 22579586809984 run.py:483] Algo bellman_ford step 2700 current loss 0.016787, current_train_items 86432.
I0304 19:29:31.856346 22579586809984 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0304 19:29:31.856451 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:31.873819 22579586809984 run.py:483] Algo bellman_ford step 2701 current loss 0.033966, current_train_items 86464.
I0304 19:29:31.898059 22579586809984 run.py:483] Algo bellman_ford step 2702 current loss 0.042255, current_train_items 86496.
I0304 19:29:31.931291 22579586809984 run.py:483] Algo bellman_ford step 2703 current loss 0.050023, current_train_items 86528.
I0304 19:29:31.967434 22579586809984 run.py:483] Algo bellman_ford step 2704 current loss 0.098783, current_train_items 86560.
I0304 19:29:31.987462 22579586809984 run.py:483] Algo bellman_ford step 2705 current loss 0.003928, current_train_items 86592.
I0304 19:29:32.003400 22579586809984 run.py:483] Algo bellman_ford step 2706 current loss 0.055667, current_train_items 86624.
I0304 19:29:32.027369 22579586809984 run.py:483] Algo bellman_ford step 2707 current loss 0.067050, current_train_items 86656.
I0304 19:29:32.057169 22579586809984 run.py:483] Algo bellman_ford step 2708 current loss 0.080153, current_train_items 86688.
I0304 19:29:32.089245 22579586809984 run.py:483] Algo bellman_ford step 2709 current loss 0.072421, current_train_items 86720.
I0304 19:29:32.109146 22579586809984 run.py:483] Algo bellman_ford step 2710 current loss 0.010153, current_train_items 86752.
I0304 19:29:32.125888 22579586809984 run.py:483] Algo bellman_ford step 2711 current loss 0.016341, current_train_items 86784.
I0304 19:29:32.150235 22579586809984 run.py:483] Algo bellman_ford step 2712 current loss 0.046389, current_train_items 86816.
I0304 19:29:32.182178 22579586809984 run.py:483] Algo bellman_ford step 2713 current loss 0.071767, current_train_items 86848.
I0304 19:29:32.218036 22579586809984 run.py:483] Algo bellman_ford step 2714 current loss 0.100709, current_train_items 86880.
I0304 19:29:32.237661 22579586809984 run.py:483] Algo bellman_ford step 2715 current loss 0.037761, current_train_items 86912.
I0304 19:29:32.253814 22579586809984 run.py:483] Algo bellman_ford step 2716 current loss 0.006152, current_train_items 86944.
I0304 19:29:32.278919 22579586809984 run.py:483] Algo bellman_ford step 2717 current loss 0.082779, current_train_items 86976.
I0304 19:29:32.308481 22579586809984 run.py:483] Algo bellman_ford step 2718 current loss 0.054772, current_train_items 87008.
I0304 19:29:32.342489 22579586809984 run.py:483] Algo bellman_ford step 2719 current loss 0.084042, current_train_items 87040.
I0304 19:29:32.362205 22579586809984 run.py:483] Algo bellman_ford step 2720 current loss 0.003811, current_train_items 87072.
I0304 19:29:32.378443 22579586809984 run.py:483] Algo bellman_ford step 2721 current loss 0.147510, current_train_items 87104.
I0304 19:29:32.402303 22579586809984 run.py:483] Algo bellman_ford step 2722 current loss 0.096830, current_train_items 87136.
I0304 19:29:32.434467 22579586809984 run.py:483] Algo bellman_ford step 2723 current loss 0.086240, current_train_items 87168.
I0304 19:29:32.467936 22579586809984 run.py:483] Algo bellman_ford step 2724 current loss 0.103307, current_train_items 87200.
I0304 19:29:32.487343 22579586809984 run.py:483] Algo bellman_ford step 2725 current loss 0.005787, current_train_items 87232.
I0304 19:29:32.503473 22579586809984 run.py:483] Algo bellman_ford step 2726 current loss 0.029295, current_train_items 87264.
I0304 19:29:32.527575 22579586809984 run.py:483] Algo bellman_ford step 2727 current loss 0.062624, current_train_items 87296.
I0304 19:29:32.558737 22579586809984 run.py:483] Algo bellman_ford step 2728 current loss 0.110811, current_train_items 87328.
I0304 19:29:32.592018 22579586809984 run.py:483] Algo bellman_ford step 2729 current loss 0.098720, current_train_items 87360.
I0304 19:29:32.611700 22579586809984 run.py:483] Algo bellman_ford step 2730 current loss 0.006447, current_train_items 87392.
I0304 19:29:32.627915 22579586809984 run.py:483] Algo bellman_ford step 2731 current loss 0.017760, current_train_items 87424.
I0304 19:29:32.651160 22579586809984 run.py:483] Algo bellman_ford step 2732 current loss 0.038240, current_train_items 87456.
I0304 19:29:32.681149 22579586809984 run.py:483] Algo bellman_ford step 2733 current loss 0.065369, current_train_items 87488.
I0304 19:29:32.714991 22579586809984 run.py:483] Algo bellman_ford step 2734 current loss 0.093695, current_train_items 87520.
I0304 19:29:32.734687 22579586809984 run.py:483] Algo bellman_ford step 2735 current loss 0.018580, current_train_items 87552.
I0304 19:29:32.750452 22579586809984 run.py:483] Algo bellman_ford step 2736 current loss 0.027644, current_train_items 87584.
I0304 19:29:32.774013 22579586809984 run.py:483] Algo bellman_ford step 2737 current loss 0.098195, current_train_items 87616.
I0304 19:29:32.804150 22579586809984 run.py:483] Algo bellman_ford step 2738 current loss 0.046957, current_train_items 87648.
I0304 19:29:32.836874 22579586809984 run.py:483] Algo bellman_ford step 2739 current loss 0.091981, current_train_items 87680.
I0304 19:29:32.856492 22579586809984 run.py:483] Algo bellman_ford step 2740 current loss 0.010410, current_train_items 87712.
I0304 19:29:32.872542 22579586809984 run.py:483] Algo bellman_ford step 2741 current loss 0.050111, current_train_items 87744.
I0304 19:29:32.896825 22579586809984 run.py:483] Algo bellman_ford step 2742 current loss 0.067789, current_train_items 87776.
I0304 19:29:32.929287 22579586809984 run.py:483] Algo bellman_ford step 2743 current loss 0.117879, current_train_items 87808.
I0304 19:29:32.962335 22579586809984 run.py:483] Algo bellman_ford step 2744 current loss 0.087226, current_train_items 87840.
I0304 19:29:32.981659 22579586809984 run.py:483] Algo bellman_ford step 2745 current loss 0.006105, current_train_items 87872.
I0304 19:29:32.997649 22579586809984 run.py:483] Algo bellman_ford step 2746 current loss 0.043710, current_train_items 87904.
I0304 19:29:33.021674 22579586809984 run.py:483] Algo bellman_ford step 2747 current loss 0.094070, current_train_items 87936.
I0304 19:29:33.053275 22579586809984 run.py:483] Algo bellman_ford step 2748 current loss 0.066009, current_train_items 87968.
I0304 19:29:33.086306 22579586809984 run.py:483] Algo bellman_ford step 2749 current loss 0.079905, current_train_items 88000.
I0304 19:29:33.105882 22579586809984 run.py:483] Algo bellman_ford step 2750 current loss 0.006358, current_train_items 88032.
I0304 19:29:33.114053 22579586809984 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0304 19:29:33.114159 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:29:33.131185 22579586809984 run.py:483] Algo bellman_ford step 2751 current loss 0.030583, current_train_items 88064.
I0304 19:29:33.155455 22579586809984 run.py:483] Algo bellman_ford step 2752 current loss 0.137274, current_train_items 88096.
I0304 19:29:33.187636 22579586809984 run.py:483] Algo bellman_ford step 2753 current loss 0.144868, current_train_items 88128.
I0304 19:29:33.220573 22579586809984 run.py:483] Algo bellman_ford step 2754 current loss 0.092132, current_train_items 88160.
I0304 19:29:33.240360 22579586809984 run.py:483] Algo bellman_ford step 2755 current loss 0.007141, current_train_items 88192.
I0304 19:29:33.256752 22579586809984 run.py:483] Algo bellman_ford step 2756 current loss 0.039242, current_train_items 88224.
I0304 19:29:33.280123 22579586809984 run.py:483] Algo bellman_ford step 2757 current loss 0.063455, current_train_items 88256.
I0304 19:29:33.311722 22579586809984 run.py:483] Algo bellman_ford step 2758 current loss 0.153659, current_train_items 88288.
I0304 19:29:33.348130 22579586809984 run.py:483] Algo bellman_ford step 2759 current loss 0.276474, current_train_items 88320.
I0304 19:29:33.367918 22579586809984 run.py:483] Algo bellman_ford step 2760 current loss 0.005081, current_train_items 88352.
I0304 19:29:33.384763 22579586809984 run.py:483] Algo bellman_ford step 2761 current loss 0.019197, current_train_items 88384.
I0304 19:29:33.408879 22579586809984 run.py:483] Algo bellman_ford step 2762 current loss 0.204838, current_train_items 88416.
I0304 19:29:33.440262 22579586809984 run.py:483] Algo bellman_ford step 2763 current loss 0.155922, current_train_items 88448.
I0304 19:29:33.473717 22579586809984 run.py:483] Algo bellman_ford step 2764 current loss 0.160114, current_train_items 88480.
I0304 19:29:33.493054 22579586809984 run.py:483] Algo bellman_ford step 2765 current loss 0.010252, current_train_items 88512.
I0304 19:29:33.509554 22579586809984 run.py:483] Algo bellman_ford step 2766 current loss 0.047734, current_train_items 88544.
I0304 19:29:33.533950 22579586809984 run.py:483] Algo bellman_ford step 2767 current loss 0.054515, current_train_items 88576.
I0304 19:29:33.566175 22579586809984 run.py:483] Algo bellman_ford step 2768 current loss 0.159497, current_train_items 88608.
I0304 19:29:33.599758 22579586809984 run.py:483] Algo bellman_ford step 2769 current loss 0.131814, current_train_items 88640.
I0304 19:29:33.619759 22579586809984 run.py:483] Algo bellman_ford step 2770 current loss 0.003637, current_train_items 88672.
I0304 19:29:33.636393 22579586809984 run.py:483] Algo bellman_ford step 2771 current loss 0.026501, current_train_items 88704.
I0304 19:29:33.660754 22579586809984 run.py:483] Algo bellman_ford step 2772 current loss 0.058821, current_train_items 88736.
I0304 19:29:33.691166 22579586809984 run.py:483] Algo bellman_ford step 2773 current loss 0.069160, current_train_items 88768.
I0304 19:29:33.724890 22579586809984 run.py:483] Algo bellman_ford step 2774 current loss 0.086387, current_train_items 88800.
I0304 19:29:33.744920 22579586809984 run.py:483] Algo bellman_ford step 2775 current loss 0.003667, current_train_items 88832.
I0304 19:29:33.761886 22579586809984 run.py:483] Algo bellman_ford step 2776 current loss 0.090392, current_train_items 88864.
I0304 19:29:33.785307 22579586809984 run.py:483] Algo bellman_ford step 2777 current loss 0.056527, current_train_items 88896.
I0304 19:29:33.816161 22579586809984 run.py:483] Algo bellman_ford step 2778 current loss 0.080225, current_train_items 88928.
I0304 19:29:33.850621 22579586809984 run.py:483] Algo bellman_ford step 2779 current loss 0.084288, current_train_items 88960.
I0304 19:29:33.870276 22579586809984 run.py:483] Algo bellman_ford step 2780 current loss 0.005983, current_train_items 88992.
I0304 19:29:33.886539 22579586809984 run.py:483] Algo bellman_ford step 2781 current loss 0.049023, current_train_items 89024.
I0304 19:29:33.910650 22579586809984 run.py:483] Algo bellman_ford step 2782 current loss 0.176710, current_train_items 89056.
I0304 19:29:33.941483 22579586809984 run.py:483] Algo bellman_ford step 2783 current loss 0.155176, current_train_items 89088.
I0304 19:29:33.976421 22579586809984 run.py:483] Algo bellman_ford step 2784 current loss 0.105826, current_train_items 89120.
I0304 19:29:33.996372 22579586809984 run.py:483] Algo bellman_ford step 2785 current loss 0.003742, current_train_items 89152.
I0304 19:29:34.012591 22579586809984 run.py:483] Algo bellman_ford step 2786 current loss 0.048855, current_train_items 89184.
I0304 19:29:34.034979 22579586809984 run.py:483] Algo bellman_ford step 2787 current loss 0.047111, current_train_items 89216.
I0304 19:29:34.065581 22579586809984 run.py:483] Algo bellman_ford step 2788 current loss 0.148699, current_train_items 89248.
I0304 19:29:34.096446 22579586809984 run.py:483] Algo bellman_ford step 2789 current loss 0.145305, current_train_items 89280.
I0304 19:29:34.116489 22579586809984 run.py:483] Algo bellman_ford step 2790 current loss 0.006165, current_train_items 89312.
I0304 19:29:34.132755 22579586809984 run.py:483] Algo bellman_ford step 2791 current loss 0.014253, current_train_items 89344.
I0304 19:29:34.156435 22579586809984 run.py:483] Algo bellman_ford step 2792 current loss 0.033754, current_train_items 89376.
I0304 19:29:34.187433 22579586809984 run.py:483] Algo bellman_ford step 2793 current loss 0.074878, current_train_items 89408.
I0304 19:29:34.223306 22579586809984 run.py:483] Algo bellman_ford step 2794 current loss 0.087257, current_train_items 89440.
I0304 19:29:34.242898 22579586809984 run.py:483] Algo bellman_ford step 2795 current loss 0.005848, current_train_items 89472.
I0304 19:29:34.259672 22579586809984 run.py:483] Algo bellman_ford step 2796 current loss 0.039914, current_train_items 89504.
I0304 19:29:34.283288 22579586809984 run.py:483] Algo bellman_ford step 2797 current loss 0.097106, current_train_items 89536.
I0304 19:29:34.315074 22579586809984 run.py:483] Algo bellman_ford step 2798 current loss 0.068928, current_train_items 89568.
I0304 19:29:34.349019 22579586809984 run.py:483] Algo bellman_ford step 2799 current loss 0.078559, current_train_items 89600.
I0304 19:29:34.368925 22579586809984 run.py:483] Algo bellman_ford step 2800 current loss 0.003752, current_train_items 89632.
I0304 19:29:34.376623 22579586809984 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0304 19:29:34.376736 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:29:34.393668 22579586809984 run.py:483] Algo bellman_ford step 2801 current loss 0.021831, current_train_items 89664.
I0304 19:29:34.418762 22579586809984 run.py:483] Algo bellman_ford step 2802 current loss 0.112946, current_train_items 89696.
I0304 19:29:34.451159 22579586809984 run.py:483] Algo bellman_ford step 2803 current loss 0.109199, current_train_items 89728.
I0304 19:29:34.485565 22579586809984 run.py:483] Algo bellman_ford step 2804 current loss 0.068245, current_train_items 89760.
I0304 19:29:34.505546 22579586809984 run.py:483] Algo bellman_ford step 2805 current loss 0.008819, current_train_items 89792.
I0304 19:29:34.521900 22579586809984 run.py:483] Algo bellman_ford step 2806 current loss 0.054144, current_train_items 89824.
I0304 19:29:34.545398 22579586809984 run.py:483] Algo bellman_ford step 2807 current loss 0.087827, current_train_items 89856.
I0304 19:29:34.577689 22579586809984 run.py:483] Algo bellman_ford step 2808 current loss 0.143416, current_train_items 89888.
I0304 19:29:34.612128 22579586809984 run.py:483] Algo bellman_ford step 2809 current loss 0.102310, current_train_items 89920.
I0304 19:29:34.632266 22579586809984 run.py:483] Algo bellman_ford step 2810 current loss 0.005006, current_train_items 89952.
I0304 19:29:34.648925 22579586809984 run.py:483] Algo bellman_ford step 2811 current loss 0.062404, current_train_items 89984.
I0304 19:29:34.672197 22579586809984 run.py:483] Algo bellman_ford step 2812 current loss 0.055037, current_train_items 90016.
I0304 19:29:34.704867 22579586809984 run.py:483] Algo bellman_ford step 2813 current loss 0.144995, current_train_items 90048.
I0304 19:29:34.738070 22579586809984 run.py:483] Algo bellman_ford step 2814 current loss 0.197660, current_train_items 90080.
I0304 19:29:34.757992 22579586809984 run.py:483] Algo bellman_ford step 2815 current loss 0.012398, current_train_items 90112.
I0304 19:29:34.774437 22579586809984 run.py:483] Algo bellman_ford step 2816 current loss 0.037288, current_train_items 90144.
I0304 19:29:34.799009 22579586809984 run.py:483] Algo bellman_ford step 2817 current loss 0.067847, current_train_items 90176.
I0304 19:29:34.827154 22579586809984 run.py:483] Algo bellman_ford step 2818 current loss 0.046982, current_train_items 90208.
I0304 19:29:34.860772 22579586809984 run.py:483] Algo bellman_ford step 2819 current loss 0.077853, current_train_items 90240.
I0304 19:29:34.880552 22579586809984 run.py:483] Algo bellman_ford step 2820 current loss 0.007468, current_train_items 90272.
I0304 19:29:34.896719 22579586809984 run.py:483] Algo bellman_ford step 2821 current loss 0.033600, current_train_items 90304.
I0304 19:29:34.920581 22579586809984 run.py:483] Algo bellman_ford step 2822 current loss 0.082767, current_train_items 90336.
I0304 19:29:34.951909 22579586809984 run.py:483] Algo bellman_ford step 2823 current loss 0.135380, current_train_items 90368.
I0304 19:29:34.988164 22579586809984 run.py:483] Algo bellman_ford step 2824 current loss 0.112200, current_train_items 90400.
I0304 19:29:35.008045 22579586809984 run.py:483] Algo bellman_ford step 2825 current loss 0.004913, current_train_items 90432.
I0304 19:29:35.024004 22579586809984 run.py:483] Algo bellman_ford step 2826 current loss 0.069102, current_train_items 90464.
I0304 19:29:35.048183 22579586809984 run.py:483] Algo bellman_ford step 2827 current loss 0.102627, current_train_items 90496.
I0304 19:29:35.079627 22579586809984 run.py:483] Algo bellman_ford step 2828 current loss 0.087682, current_train_items 90528.
I0304 19:29:35.112056 22579586809984 run.py:483] Algo bellman_ford step 2829 current loss 0.077164, current_train_items 90560.
I0304 19:29:35.132055 22579586809984 run.py:483] Algo bellman_ford step 2830 current loss 0.013460, current_train_items 90592.
I0304 19:29:35.148619 22579586809984 run.py:483] Algo bellman_ford step 2831 current loss 0.051708, current_train_items 90624.
I0304 19:29:35.172206 22579586809984 run.py:483] Algo bellman_ford step 2832 current loss 0.069859, current_train_items 90656.
I0304 19:29:35.202653 22579586809984 run.py:483] Algo bellman_ford step 2833 current loss 0.053573, current_train_items 90688.
I0304 19:29:35.236402 22579586809984 run.py:483] Algo bellman_ford step 2834 current loss 0.098035, current_train_items 90720.
I0304 19:29:35.256308 22579586809984 run.py:483] Algo bellman_ford step 2835 current loss 0.018570, current_train_items 90752.
I0304 19:29:35.272127 22579586809984 run.py:483] Algo bellman_ford step 2836 current loss 0.030118, current_train_items 90784.
I0304 19:29:35.295659 22579586809984 run.py:483] Algo bellman_ford step 2837 current loss 0.062897, current_train_items 90816.
I0304 19:29:35.326853 22579586809984 run.py:483] Algo bellman_ford step 2838 current loss 0.072888, current_train_items 90848.
I0304 19:29:35.363078 22579586809984 run.py:483] Algo bellman_ford step 2839 current loss 0.112165, current_train_items 90880.
I0304 19:29:35.382950 22579586809984 run.py:483] Algo bellman_ford step 2840 current loss 0.007639, current_train_items 90912.
I0304 19:29:35.398991 22579586809984 run.py:483] Algo bellman_ford step 2841 current loss 0.027564, current_train_items 90944.
I0304 19:29:35.422979 22579586809984 run.py:483] Algo bellman_ford step 2842 current loss 0.099243, current_train_items 90976.
I0304 19:29:35.455088 22579586809984 run.py:483] Algo bellman_ford step 2843 current loss 0.083168, current_train_items 91008.
I0304 19:29:35.489564 22579586809984 run.py:483] Algo bellman_ford step 2844 current loss 0.083967, current_train_items 91040.
I0304 19:29:35.509315 22579586809984 run.py:483] Algo bellman_ford step 2845 current loss 0.005489, current_train_items 91072.
I0304 19:29:35.525967 22579586809984 run.py:483] Algo bellman_ford step 2846 current loss 0.037314, current_train_items 91104.
I0304 19:29:35.549551 22579586809984 run.py:483] Algo bellman_ford step 2847 current loss 0.052413, current_train_items 91136.
I0304 19:29:35.580541 22579586809984 run.py:483] Algo bellman_ford step 2848 current loss 0.083208, current_train_items 91168.
I0304 19:29:35.615675 22579586809984 run.py:483] Algo bellman_ford step 2849 current loss 0.123787, current_train_items 91200.
I0304 19:29:35.635328 22579586809984 run.py:483] Algo bellman_ford step 2850 current loss 0.007922, current_train_items 91232.
I0304 19:29:35.643219 22579586809984 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.962890625, 'score': 0.962890625, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0304 19:29:35.643324 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.963, val scores are: bellman_ford: 0.963
I0304 19:29:35.659739 22579586809984 run.py:483] Algo bellman_ford step 2851 current loss 0.023649, current_train_items 91264.
I0304 19:29:35.684048 22579586809984 run.py:483] Algo bellman_ford step 2852 current loss 0.054668, current_train_items 91296.
I0304 19:29:35.716531 22579586809984 run.py:483] Algo bellman_ford step 2853 current loss 0.075768, current_train_items 91328.
I0304 19:29:35.750594 22579586809984 run.py:483] Algo bellman_ford step 2854 current loss 0.114733, current_train_items 91360.
I0304 19:29:35.770542 22579586809984 run.py:483] Algo bellman_ford step 2855 current loss 0.003789, current_train_items 91392.
I0304 19:29:35.786718 22579586809984 run.py:483] Algo bellman_ford step 2856 current loss 0.059282, current_train_items 91424.
I0304 19:29:35.811228 22579586809984 run.py:483] Algo bellman_ford step 2857 current loss 0.117329, current_train_items 91456.
I0304 19:29:35.841113 22579586809984 run.py:483] Algo bellman_ford step 2858 current loss 0.083373, current_train_items 91488.
I0304 19:29:35.873627 22579586809984 run.py:483] Algo bellman_ford step 2859 current loss 0.068326, current_train_items 91520.
I0304 19:29:35.894099 22579586809984 run.py:483] Algo bellman_ford step 2860 current loss 0.005064, current_train_items 91552.
I0304 19:29:35.910693 22579586809984 run.py:483] Algo bellman_ford step 2861 current loss 0.028772, current_train_items 91584.
I0304 19:29:35.934885 22579586809984 run.py:483] Algo bellman_ford step 2862 current loss 0.074648, current_train_items 91616.
I0304 19:29:35.966909 22579586809984 run.py:483] Algo bellman_ford step 2863 current loss 0.078775, current_train_items 91648.
I0304 19:29:35.998300 22579586809984 run.py:483] Algo bellman_ford step 2864 current loss 0.083286, current_train_items 91680.
I0304 19:29:36.018396 22579586809984 run.py:483] Algo bellman_ford step 2865 current loss 0.009016, current_train_items 91712.
I0304 19:29:36.034715 22579586809984 run.py:483] Algo bellman_ford step 2866 current loss 0.032999, current_train_items 91744.
I0304 19:29:36.058631 22579586809984 run.py:483] Algo bellman_ford step 2867 current loss 0.068202, current_train_items 91776.
I0304 19:29:36.090272 22579586809984 run.py:483] Algo bellman_ford step 2868 current loss 0.049008, current_train_items 91808.
I0304 19:29:36.124666 22579586809984 run.py:483] Algo bellman_ford step 2869 current loss 0.098146, current_train_items 91840.
I0304 19:29:36.144750 22579586809984 run.py:483] Algo bellman_ford step 2870 current loss 0.004910, current_train_items 91872.
I0304 19:29:36.161180 22579586809984 run.py:483] Algo bellman_ford step 2871 current loss 0.037649, current_train_items 91904.
I0304 19:29:36.185563 22579586809984 run.py:483] Algo bellman_ford step 2872 current loss 0.088853, current_train_items 91936.
I0304 19:29:36.217741 22579586809984 run.py:483] Algo bellman_ford step 2873 current loss 0.105135, current_train_items 91968.
I0304 19:29:36.250638 22579586809984 run.py:483] Algo bellman_ford step 2874 current loss 0.108622, current_train_items 92000.
I0304 19:29:36.270840 22579586809984 run.py:483] Algo bellman_ford step 2875 current loss 0.004506, current_train_items 92032.
I0304 19:29:36.287286 22579586809984 run.py:483] Algo bellman_ford step 2876 current loss 0.022877, current_train_items 92064.
I0304 19:29:36.311001 22579586809984 run.py:483] Algo bellman_ford step 2877 current loss 0.050396, current_train_items 92096.
I0304 19:29:36.343356 22579586809984 run.py:483] Algo bellman_ford step 2878 current loss 0.179655, current_train_items 92128.
I0304 19:29:36.375821 22579586809984 run.py:483] Algo bellman_ford step 2879 current loss 0.082729, current_train_items 92160.
I0304 19:29:36.395855 22579586809984 run.py:483] Algo bellman_ford step 2880 current loss 0.048306, current_train_items 92192.
I0304 19:29:36.412088 22579586809984 run.py:483] Algo bellman_ford step 2881 current loss 0.008494, current_train_items 92224.
I0304 19:29:36.436583 22579586809984 run.py:483] Algo bellman_ford step 2882 current loss 0.055760, current_train_items 92256.
I0304 19:29:36.468475 22579586809984 run.py:483] Algo bellman_ford step 2883 current loss 0.098850, current_train_items 92288.
I0304 19:29:36.500312 22579586809984 run.py:483] Algo bellman_ford step 2884 current loss 0.123923, current_train_items 92320.
I0304 19:29:36.520273 22579586809984 run.py:483] Algo bellman_ford step 2885 current loss 0.003415, current_train_items 92352.
I0304 19:29:36.536575 22579586809984 run.py:483] Algo bellman_ford step 2886 current loss 0.035898, current_train_items 92384.
I0304 19:29:36.560432 22579586809984 run.py:483] Algo bellman_ford step 2887 current loss 0.042785, current_train_items 92416.
I0304 19:29:36.590901 22579586809984 run.py:483] Algo bellman_ford step 2888 current loss 0.049653, current_train_items 92448.
I0304 19:29:36.625370 22579586809984 run.py:483] Algo bellman_ford step 2889 current loss 0.110572, current_train_items 92480.
I0304 19:29:36.645563 22579586809984 run.py:483] Algo bellman_ford step 2890 current loss 0.006266, current_train_items 92512.
I0304 19:29:36.662073 22579586809984 run.py:483] Algo bellman_ford step 2891 current loss 0.033922, current_train_items 92544.
I0304 19:29:36.687245 22579586809984 run.py:483] Algo bellman_ford step 2892 current loss 0.041335, current_train_items 92576.
I0304 19:29:36.717600 22579586809984 run.py:483] Algo bellman_ford step 2893 current loss 0.054835, current_train_items 92608.
I0304 19:29:36.751779 22579586809984 run.py:483] Algo bellman_ford step 2894 current loss 0.094988, current_train_items 92640.
I0304 19:29:36.771454 22579586809984 run.py:483] Algo bellman_ford step 2895 current loss 0.003803, current_train_items 92672.
I0304 19:29:36.788426 22579586809984 run.py:483] Algo bellman_ford step 2896 current loss 0.049456, current_train_items 92704.
I0304 19:29:36.811123 22579586809984 run.py:483] Algo bellman_ford step 2897 current loss 0.076315, current_train_items 92736.
I0304 19:29:36.842159 22579586809984 run.py:483] Algo bellman_ford step 2898 current loss 0.046144, current_train_items 92768.
I0304 19:29:36.873534 22579586809984 run.py:483] Algo bellman_ford step 2899 current loss 0.073486, current_train_items 92800.
I0304 19:29:36.893622 22579586809984 run.py:483] Algo bellman_ford step 2900 current loss 0.003463, current_train_items 92832.
I0304 19:29:36.901182 22579586809984 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0304 19:29:36.901288 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:29:36.917757 22579586809984 run.py:483] Algo bellman_ford step 2901 current loss 0.022592, current_train_items 92864.
I0304 19:29:36.941706 22579586809984 run.py:483] Algo bellman_ford step 2902 current loss 0.069521, current_train_items 92896.
I0304 19:29:36.973006 22579586809984 run.py:483] Algo bellman_ford step 2903 current loss 0.094336, current_train_items 92928.
I0304 19:29:37.007465 22579586809984 run.py:483] Algo bellman_ford step 2904 current loss 0.114988, current_train_items 92960.
I0304 19:29:37.027488 22579586809984 run.py:483] Algo bellman_ford step 2905 current loss 0.013506, current_train_items 92992.
I0304 19:29:37.043586 22579586809984 run.py:483] Algo bellman_ford step 2906 current loss 0.046921, current_train_items 93024.
I0304 19:29:37.068161 22579586809984 run.py:483] Algo bellman_ford step 2907 current loss 0.089684, current_train_items 93056.
I0304 19:29:37.100640 22579586809984 run.py:483] Algo bellman_ford step 2908 current loss 0.091711, current_train_items 93088.
I0304 19:29:37.134859 22579586809984 run.py:483] Algo bellman_ford step 2909 current loss 0.108226, current_train_items 93120.
I0304 19:29:37.154433 22579586809984 run.py:483] Algo bellman_ford step 2910 current loss 0.013388, current_train_items 93152.
I0304 19:29:37.171255 22579586809984 run.py:483] Algo bellman_ford step 2911 current loss 0.055934, current_train_items 93184.
I0304 19:29:37.195022 22579586809984 run.py:483] Algo bellman_ford step 2912 current loss 0.066637, current_train_items 93216.
I0304 19:29:37.225893 22579586809984 run.py:483] Algo bellman_ford step 2913 current loss 0.050937, current_train_items 93248.
I0304 19:29:37.260694 22579586809984 run.py:483] Algo bellman_ford step 2914 current loss 0.097787, current_train_items 93280.
I0304 19:29:37.280160 22579586809984 run.py:483] Algo bellman_ford step 2915 current loss 0.005186, current_train_items 93312.
I0304 19:29:37.296406 22579586809984 run.py:483] Algo bellman_ford step 2916 current loss 0.011744, current_train_items 93344.
I0304 19:29:37.319909 22579586809984 run.py:483] Algo bellman_ford step 2917 current loss 0.053395, current_train_items 93376.
I0304 19:29:37.352367 22579586809984 run.py:483] Algo bellman_ford step 2918 current loss 0.137044, current_train_items 93408.
I0304 19:29:37.387732 22579586809984 run.py:483] Algo bellman_ford step 2919 current loss 0.140622, current_train_items 93440.
I0304 19:29:37.407497 22579586809984 run.py:483] Algo bellman_ford step 2920 current loss 0.007132, current_train_items 93472.
I0304 19:29:37.423827 22579586809984 run.py:483] Algo bellman_ford step 2921 current loss 0.029877, current_train_items 93504.
I0304 19:29:37.448250 22579586809984 run.py:483] Algo bellman_ford step 2922 current loss 0.054342, current_train_items 93536.
I0304 19:29:37.478661 22579586809984 run.py:483] Algo bellman_ford step 2923 current loss 0.069386, current_train_items 93568.
I0304 19:29:37.514246 22579586809984 run.py:483] Algo bellman_ford step 2924 current loss 0.106323, current_train_items 93600.
I0304 19:29:37.533946 22579586809984 run.py:483] Algo bellman_ford step 2925 current loss 0.020989, current_train_items 93632.
I0304 19:29:37.550031 22579586809984 run.py:483] Algo bellman_ford step 2926 current loss 0.040765, current_train_items 93664.
I0304 19:29:37.574738 22579586809984 run.py:483] Algo bellman_ford step 2927 current loss 0.110317, current_train_items 93696.
I0304 19:29:37.606469 22579586809984 run.py:483] Algo bellman_ford step 2928 current loss 0.102458, current_train_items 93728.
I0304 19:29:37.640623 22579586809984 run.py:483] Algo bellman_ford step 2929 current loss 0.080345, current_train_items 93760.
I0304 19:29:37.660345 22579586809984 run.py:483] Algo bellman_ford step 2930 current loss 0.005919, current_train_items 93792.
I0304 19:29:37.676423 22579586809984 run.py:483] Algo bellman_ford step 2931 current loss 0.014821, current_train_items 93824.
I0304 19:29:37.701774 22579586809984 run.py:483] Algo bellman_ford step 2932 current loss 0.110552, current_train_items 93856.
I0304 19:29:37.731988 22579586809984 run.py:483] Algo bellman_ford step 2933 current loss 0.110507, current_train_items 93888.
I0304 19:29:37.765051 22579586809984 run.py:483] Algo bellman_ford step 2934 current loss 0.095244, current_train_items 93920.
I0304 19:29:37.784933 22579586809984 run.py:483] Algo bellman_ford step 2935 current loss 0.004454, current_train_items 93952.
I0304 19:29:37.801052 22579586809984 run.py:483] Algo bellman_ford step 2936 current loss 0.027042, current_train_items 93984.
I0304 19:29:37.825030 22579586809984 run.py:483] Algo bellman_ford step 2937 current loss 0.083289, current_train_items 94016.
I0304 19:29:37.854532 22579586809984 run.py:483] Algo bellman_ford step 2938 current loss 0.062422, current_train_items 94048.
I0304 19:29:37.888420 22579586809984 run.py:483] Algo bellman_ford step 2939 current loss 0.085875, current_train_items 94080.
I0304 19:29:37.907948 22579586809984 run.py:483] Algo bellman_ford step 2940 current loss 0.028779, current_train_items 94112.
I0304 19:29:37.924513 22579586809984 run.py:483] Algo bellman_ford step 2941 current loss 0.049864, current_train_items 94144.
I0304 19:29:37.948968 22579586809984 run.py:483] Algo bellman_ford step 2942 current loss 0.079683, current_train_items 94176.
I0304 19:29:37.981071 22579586809984 run.py:483] Algo bellman_ford step 2943 current loss 0.089603, current_train_items 94208.
I0304 19:29:38.016294 22579586809984 run.py:483] Algo bellman_ford step 2944 current loss 0.106725, current_train_items 94240.
I0304 19:29:38.035745 22579586809984 run.py:483] Algo bellman_ford step 2945 current loss 0.030338, current_train_items 94272.
I0304 19:29:38.052343 22579586809984 run.py:483] Algo bellman_ford step 2946 current loss 0.027651, current_train_items 94304.
I0304 19:29:38.076468 22579586809984 run.py:483] Algo bellman_ford step 2947 current loss 0.101674, current_train_items 94336.
I0304 19:29:38.107515 22579586809984 run.py:483] Algo bellman_ford step 2948 current loss 0.097515, current_train_items 94368.
I0304 19:29:38.141161 22579586809984 run.py:483] Algo bellman_ford step 2949 current loss 0.070796, current_train_items 94400.
I0304 19:29:38.160695 22579586809984 run.py:483] Algo bellman_ford step 2950 current loss 0.006028, current_train_items 94432.
I0304 19:29:38.169010 22579586809984 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0304 19:29:38.169115 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:29:38.186070 22579586809984 run.py:483] Algo bellman_ford step 2951 current loss 0.060425, current_train_items 94464.
I0304 19:29:38.209833 22579586809984 run.py:483] Algo bellman_ford step 2952 current loss 0.057056, current_train_items 94496.
I0304 19:29:38.240945 22579586809984 run.py:483] Algo bellman_ford step 2953 current loss 0.077971, current_train_items 94528.
I0304 19:29:38.275520 22579586809984 run.py:483] Algo bellman_ford step 2954 current loss 0.067375, current_train_items 94560.
I0304 19:29:38.295526 22579586809984 run.py:483] Algo bellman_ford step 2955 current loss 0.003850, current_train_items 94592.
I0304 19:29:38.311379 22579586809984 run.py:483] Algo bellman_ford step 2956 current loss 0.047402, current_train_items 94624.
I0304 19:29:38.336140 22579586809984 run.py:483] Algo bellman_ford step 2957 current loss 0.068621, current_train_items 94656.
I0304 19:29:38.368776 22579586809984 run.py:483] Algo bellman_ford step 2958 current loss 0.063166, current_train_items 94688.
I0304 19:29:38.402997 22579586809984 run.py:483] Algo bellman_ford step 2959 current loss 0.098217, current_train_items 94720.
I0304 19:29:38.423062 22579586809984 run.py:483] Algo bellman_ford step 2960 current loss 0.032232, current_train_items 94752.
I0304 19:29:38.440054 22579586809984 run.py:483] Algo bellman_ford step 2961 current loss 0.032476, current_train_items 94784.
I0304 19:29:38.463307 22579586809984 run.py:483] Algo bellman_ford step 2962 current loss 0.078565, current_train_items 94816.
I0304 19:29:38.493788 22579586809984 run.py:483] Algo bellman_ford step 2963 current loss 0.069423, current_train_items 94848.
I0304 19:29:38.528799 22579586809984 run.py:483] Algo bellman_ford step 2964 current loss 0.078252, current_train_items 94880.
I0304 19:29:38.548492 22579586809984 run.py:483] Algo bellman_ford step 2965 current loss 0.003574, current_train_items 94912.
I0304 19:29:38.565120 22579586809984 run.py:483] Algo bellman_ford step 2966 current loss 0.032589, current_train_items 94944.
I0304 19:29:38.589284 22579586809984 run.py:483] Algo bellman_ford step 2967 current loss 0.090659, current_train_items 94976.
I0304 19:29:38.619975 22579586809984 run.py:483] Algo bellman_ford step 2968 current loss 0.065731, current_train_items 95008.
I0304 19:29:38.654648 22579586809984 run.py:483] Algo bellman_ford step 2969 current loss 0.074565, current_train_items 95040.
I0304 19:29:38.675223 22579586809984 run.py:483] Algo bellman_ford step 2970 current loss 0.006844, current_train_items 95072.
I0304 19:29:38.691323 22579586809984 run.py:483] Algo bellman_ford step 2971 current loss 0.050040, current_train_items 95104.
I0304 19:29:38.715151 22579586809984 run.py:483] Algo bellman_ford step 2972 current loss 0.073348, current_train_items 95136.
I0304 19:29:38.745571 22579586809984 run.py:483] Algo bellman_ford step 2973 current loss 0.076338, current_train_items 95168.
I0304 19:29:38.782277 22579586809984 run.py:483] Algo bellman_ford step 2974 current loss 0.142220, current_train_items 95200.
I0304 19:29:38.802551 22579586809984 run.py:483] Algo bellman_ford step 2975 current loss 0.009205, current_train_items 95232.
I0304 19:29:38.819671 22579586809984 run.py:483] Algo bellman_ford step 2976 current loss 0.030905, current_train_items 95264.
I0304 19:29:38.843446 22579586809984 run.py:483] Algo bellman_ford step 2977 current loss 0.078736, current_train_items 95296.
I0304 19:29:38.874399 22579586809984 run.py:483] Algo bellman_ford step 2978 current loss 0.089152, current_train_items 95328.
I0304 19:29:38.909674 22579586809984 run.py:483] Algo bellman_ford step 2979 current loss 0.108861, current_train_items 95360.
I0304 19:29:38.929513 22579586809984 run.py:483] Algo bellman_ford step 2980 current loss 0.004772, current_train_items 95392.
I0304 19:29:38.945706 22579586809984 run.py:483] Algo bellman_ford step 2981 current loss 0.020275, current_train_items 95424.
I0304 19:29:38.970690 22579586809984 run.py:483] Algo bellman_ford step 2982 current loss 0.107522, current_train_items 95456.
I0304 19:29:39.003777 22579586809984 run.py:483] Algo bellman_ford step 2983 current loss 0.077962, current_train_items 95488.
I0304 19:29:39.037849 22579586809984 run.py:483] Algo bellman_ford step 2984 current loss 0.082688, current_train_items 95520.
I0304 19:29:39.057885 22579586809984 run.py:483] Algo bellman_ford step 2985 current loss 0.004327, current_train_items 95552.
I0304 19:29:39.074728 22579586809984 run.py:483] Algo bellman_ford step 2986 current loss 0.048621, current_train_items 95584.
I0304 19:29:39.099515 22579586809984 run.py:483] Algo bellman_ford step 2987 current loss 0.125214, current_train_items 95616.
I0304 19:29:39.130883 22579586809984 run.py:483] Algo bellman_ford step 2988 current loss 0.064091, current_train_items 95648.
I0304 19:29:39.165495 22579586809984 run.py:483] Algo bellman_ford step 2989 current loss 0.092857, current_train_items 95680.
I0304 19:29:39.185431 22579586809984 run.py:483] Algo bellman_ford step 2990 current loss 0.018982, current_train_items 95712.
I0304 19:29:39.202375 22579586809984 run.py:483] Algo bellman_ford step 2991 current loss 0.046356, current_train_items 95744.
I0304 19:29:39.226724 22579586809984 run.py:483] Algo bellman_ford step 2992 current loss 0.068765, current_train_items 95776.
I0304 19:29:39.258960 22579586809984 run.py:483] Algo bellman_ford step 2993 current loss 0.115232, current_train_items 95808.
I0304 19:29:39.294542 22579586809984 run.py:483] Algo bellman_ford step 2994 current loss 0.125255, current_train_items 95840.
I0304 19:29:39.314240 22579586809984 run.py:483] Algo bellman_ford step 2995 current loss 0.013654, current_train_items 95872.
I0304 19:29:39.330706 22579586809984 run.py:483] Algo bellman_ford step 2996 current loss 0.065398, current_train_items 95904.
I0304 19:29:39.356243 22579586809984 run.py:483] Algo bellman_ford step 2997 current loss 0.089805, current_train_items 95936.
I0304 19:29:39.388332 22579586809984 run.py:483] Algo bellman_ford step 2998 current loss 0.085197, current_train_items 95968.
I0304 19:29:39.421493 22579586809984 run.py:483] Algo bellman_ford step 2999 current loss 0.102657, current_train_items 96000.
I0304 19:29:39.441632 22579586809984 run.py:483] Algo bellman_ford step 3000 current loss 0.004617, current_train_items 96032.
I0304 19:29:39.449231 22579586809984 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0304 19:29:39.449337 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:29:39.466092 22579586809984 run.py:483] Algo bellman_ford step 3001 current loss 0.075339, current_train_items 96064.
I0304 19:29:39.489905 22579586809984 run.py:483] Algo bellman_ford step 3002 current loss 0.048736, current_train_items 96096.
I0304 19:29:39.521913 22579586809984 run.py:483] Algo bellman_ford step 3003 current loss 0.152287, current_train_items 96128.
I0304 19:29:39.555016 22579586809984 run.py:483] Algo bellman_ford step 3004 current loss 0.134060, current_train_items 96160.
I0304 19:29:39.574728 22579586809984 run.py:483] Algo bellman_ford step 3005 current loss 0.005233, current_train_items 96192.
I0304 19:29:39.590672 22579586809984 run.py:483] Algo bellman_ford step 3006 current loss 0.026588, current_train_items 96224.
I0304 19:29:39.615720 22579586809984 run.py:483] Algo bellman_ford step 3007 current loss 0.094048, current_train_items 96256.
I0304 19:29:39.647488 22579586809984 run.py:483] Algo bellman_ford step 3008 current loss 0.128116, current_train_items 96288.
I0304 19:29:39.683491 22579586809984 run.py:483] Algo bellman_ford step 3009 current loss 0.151868, current_train_items 96320.
I0304 19:29:39.702849 22579586809984 run.py:483] Algo bellman_ford step 3010 current loss 0.004007, current_train_items 96352.
I0304 19:29:39.719142 22579586809984 run.py:483] Algo bellman_ford step 3011 current loss 0.031407, current_train_items 96384.
I0304 19:29:39.742847 22579586809984 run.py:483] Algo bellman_ford step 3012 current loss 0.071542, current_train_items 96416.
I0304 19:29:39.773771 22579586809984 run.py:483] Algo bellman_ford step 3013 current loss 0.102198, current_train_items 96448.
I0304 19:29:39.808386 22579586809984 run.py:483] Algo bellman_ford step 3014 current loss 0.115816, current_train_items 96480.
I0304 19:29:39.828142 22579586809984 run.py:483] Algo bellman_ford step 3015 current loss 0.009634, current_train_items 96512.
I0304 19:29:39.844799 22579586809984 run.py:483] Algo bellman_ford step 3016 current loss 0.015546, current_train_items 96544.
I0304 19:29:39.868468 22579586809984 run.py:483] Algo bellman_ford step 3017 current loss 0.100310, current_train_items 96576.
I0304 19:29:39.898807 22579586809984 run.py:483] Algo bellman_ford step 3018 current loss 0.132549, current_train_items 96608.
I0304 19:29:39.931104 22579586809984 run.py:483] Algo bellman_ford step 3019 current loss 0.111869, current_train_items 96640.
I0304 19:29:39.950488 22579586809984 run.py:483] Algo bellman_ford step 3020 current loss 0.004005, current_train_items 96672.
I0304 19:29:39.966385 22579586809984 run.py:483] Algo bellman_ford step 3021 current loss 0.027535, current_train_items 96704.
I0304 19:29:39.991224 22579586809984 run.py:483] Algo bellman_ford step 3022 current loss 0.072149, current_train_items 96736.
I0304 19:29:40.022859 22579586809984 run.py:483] Algo bellman_ford step 3023 current loss 0.098706, current_train_items 96768.
I0304 19:29:40.056240 22579586809984 run.py:483] Algo bellman_ford step 3024 current loss 0.080800, current_train_items 96800.
I0304 19:29:40.075788 22579586809984 run.py:483] Algo bellman_ford step 3025 current loss 0.005586, current_train_items 96832.
I0304 19:29:40.091931 22579586809984 run.py:483] Algo bellman_ford step 3026 current loss 0.038691, current_train_items 96864.
I0304 19:29:40.114590 22579586809984 run.py:483] Algo bellman_ford step 3027 current loss 0.040582, current_train_items 96896.
I0304 19:29:40.145493 22579586809984 run.py:483] Algo bellman_ford step 3028 current loss 0.065788, current_train_items 96928.
I0304 19:29:40.177900 22579586809984 run.py:483] Algo bellman_ford step 3029 current loss 0.060767, current_train_items 96960.
I0304 19:29:40.197488 22579586809984 run.py:483] Algo bellman_ford step 3030 current loss 0.007887, current_train_items 96992.
I0304 19:29:40.213492 22579586809984 run.py:483] Algo bellman_ford step 3031 current loss 0.027492, current_train_items 97024.
I0304 19:29:40.238140 22579586809984 run.py:483] Algo bellman_ford step 3032 current loss 0.064446, current_train_items 97056.
I0304 19:29:40.269828 22579586809984 run.py:483] Algo bellman_ford step 3033 current loss 0.066792, current_train_items 97088.
I0304 19:29:40.305020 22579586809984 run.py:483] Algo bellman_ford step 3034 current loss 0.067093, current_train_items 97120.
I0304 19:29:40.324660 22579586809984 run.py:483] Algo bellman_ford step 3035 current loss 0.026056, current_train_items 97152.
I0304 19:29:40.341233 22579586809984 run.py:483] Algo bellman_ford step 3036 current loss 0.015580, current_train_items 97184.
I0304 19:29:40.364944 22579586809984 run.py:483] Algo bellman_ford step 3037 current loss 0.053961, current_train_items 97216.
I0304 19:29:40.396459 22579586809984 run.py:483] Algo bellman_ford step 3038 current loss 0.085836, current_train_items 97248.
I0304 19:29:40.429179 22579586809984 run.py:483] Algo bellman_ford step 3039 current loss 0.134515, current_train_items 97280.
I0304 19:29:40.448944 22579586809984 run.py:483] Algo bellman_ford step 3040 current loss 0.003822, current_train_items 97312.
I0304 19:29:40.465040 22579586809984 run.py:483] Algo bellman_ford step 3041 current loss 0.013318, current_train_items 97344.
I0304 19:29:40.488998 22579586809984 run.py:483] Algo bellman_ford step 3042 current loss 0.053332, current_train_items 97376.
I0304 19:29:40.520108 22579586809984 run.py:483] Algo bellman_ford step 3043 current loss 0.078969, current_train_items 97408.
I0304 19:29:40.552563 22579586809984 run.py:483] Algo bellman_ford step 3044 current loss 0.120649, current_train_items 97440.
I0304 19:29:40.572156 22579586809984 run.py:483] Algo bellman_ford step 3045 current loss 0.010935, current_train_items 97472.
I0304 19:29:40.588396 22579586809984 run.py:483] Algo bellman_ford step 3046 current loss 0.027062, current_train_items 97504.
I0304 19:29:40.611973 22579586809984 run.py:483] Algo bellman_ford step 3047 current loss 0.090055, current_train_items 97536.
I0304 19:29:40.643899 22579586809984 run.py:483] Algo bellman_ford step 3048 current loss 0.123591, current_train_items 97568.
I0304 19:29:40.677386 22579586809984 run.py:483] Algo bellman_ford step 3049 current loss 0.125646, current_train_items 97600.
I0304 19:29:40.696689 22579586809984 run.py:483] Algo bellman_ford step 3050 current loss 0.005834, current_train_items 97632.
I0304 19:29:40.704695 22579586809984 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0304 19:29:40.704802 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:29:40.721551 22579586809984 run.py:483] Algo bellman_ford step 3051 current loss 0.017728, current_train_items 97664.
I0304 19:29:40.745733 22579586809984 run.py:483] Algo bellman_ford step 3052 current loss 0.065337, current_train_items 97696.
I0304 19:29:40.778085 22579586809984 run.py:483] Algo bellman_ford step 3053 current loss 0.054966, current_train_items 97728.
I0304 19:29:40.810070 22579586809984 run.py:483] Algo bellman_ford step 3054 current loss 0.069173, current_train_items 97760.
I0304 19:29:40.830236 22579586809984 run.py:483] Algo bellman_ford step 3055 current loss 0.004658, current_train_items 97792.
I0304 19:29:40.846210 22579586809984 run.py:483] Algo bellman_ford step 3056 current loss 0.047146, current_train_items 97824.
I0304 19:29:40.869987 22579586809984 run.py:483] Algo bellman_ford step 3057 current loss 0.058070, current_train_items 97856.
I0304 19:29:40.901278 22579586809984 run.py:483] Algo bellman_ford step 3058 current loss 0.069520, current_train_items 97888.
I0304 19:29:40.933920 22579586809984 run.py:483] Algo bellman_ford step 3059 current loss 0.076222, current_train_items 97920.
I0304 19:29:40.953490 22579586809984 run.py:483] Algo bellman_ford step 3060 current loss 0.005675, current_train_items 97952.
I0304 19:29:40.970073 22579586809984 run.py:483] Algo bellman_ford step 3061 current loss 0.048590, current_train_items 97984.
I0304 19:29:40.993246 22579586809984 run.py:483] Algo bellman_ford step 3062 current loss 0.056883, current_train_items 98016.
I0304 19:29:41.025764 22579586809984 run.py:483] Algo bellman_ford step 3063 current loss 0.064113, current_train_items 98048.
I0304 19:29:41.060400 22579586809984 run.py:483] Algo bellman_ford step 3064 current loss 0.052521, current_train_items 98080.
I0304 19:29:41.080534 22579586809984 run.py:483] Algo bellman_ford step 3065 current loss 0.004317, current_train_items 98112.
I0304 19:29:41.096649 22579586809984 run.py:483] Algo bellman_ford step 3066 current loss 0.029451, current_train_items 98144.
I0304 19:29:41.121305 22579586809984 run.py:483] Algo bellman_ford step 3067 current loss 0.041722, current_train_items 98176.
I0304 19:29:41.151973 22579586809984 run.py:483] Algo bellman_ford step 3068 current loss 0.053432, current_train_items 98208.
I0304 19:29:41.187963 22579586809984 run.py:483] Algo bellman_ford step 3069 current loss 0.097247, current_train_items 98240.
I0304 19:29:41.207938 22579586809984 run.py:483] Algo bellman_ford step 3070 current loss 0.005926, current_train_items 98272.
I0304 19:29:41.224341 22579586809984 run.py:483] Algo bellman_ford step 3071 current loss 0.030481, current_train_items 98304.
I0304 19:29:41.247363 22579586809984 run.py:483] Algo bellman_ford step 3072 current loss 0.051684, current_train_items 98336.
I0304 19:29:41.277925 22579586809984 run.py:483] Algo bellman_ford step 3073 current loss 0.041444, current_train_items 98368.
I0304 19:29:41.313249 22579586809984 run.py:483] Algo bellman_ford step 3074 current loss 0.112201, current_train_items 98400.
I0304 19:29:41.333099 22579586809984 run.py:483] Algo bellman_ford step 3075 current loss 0.003573, current_train_items 98432.
I0304 19:29:41.349778 22579586809984 run.py:483] Algo bellman_ford step 3076 current loss 0.031298, current_train_items 98464.
I0304 19:29:41.373710 22579586809984 run.py:483] Algo bellman_ford step 3077 current loss 0.071670, current_train_items 98496.
I0304 19:29:41.405021 22579586809984 run.py:483] Algo bellman_ford step 3078 current loss 0.089133, current_train_items 98528.
I0304 19:29:41.437174 22579586809984 run.py:483] Algo bellman_ford step 3079 current loss 0.068482, current_train_items 98560.
I0304 19:29:41.457048 22579586809984 run.py:483] Algo bellman_ford step 3080 current loss 0.005942, current_train_items 98592.
I0304 19:29:41.473576 22579586809984 run.py:483] Algo bellman_ford step 3081 current loss 0.033131, current_train_items 98624.
I0304 19:29:41.497257 22579586809984 run.py:483] Algo bellman_ford step 3082 current loss 0.043111, current_train_items 98656.
I0304 19:29:41.528181 22579586809984 run.py:483] Algo bellman_ford step 3083 current loss 0.102871, current_train_items 98688.
I0304 19:29:41.561706 22579586809984 run.py:483] Algo bellman_ford step 3084 current loss 0.093652, current_train_items 98720.
I0304 19:29:41.581743 22579586809984 run.py:483] Algo bellman_ford step 3085 current loss 0.005452, current_train_items 98752.
I0304 19:29:41.598326 22579586809984 run.py:483] Algo bellman_ford step 3086 current loss 0.011865, current_train_items 98784.
I0304 19:29:41.620853 22579586809984 run.py:483] Algo bellman_ford step 3087 current loss 0.049215, current_train_items 98816.
I0304 19:29:41.651838 22579586809984 run.py:483] Algo bellman_ford step 3088 current loss 0.123165, current_train_items 98848.
I0304 19:29:41.687057 22579586809984 run.py:483] Algo bellman_ford step 3089 current loss 0.143804, current_train_items 98880.
I0304 19:29:41.707023 22579586809984 run.py:483] Algo bellman_ford step 3090 current loss 0.003507, current_train_items 98912.
I0304 19:29:41.723181 22579586809984 run.py:483] Algo bellman_ford step 3091 current loss 0.012398, current_train_items 98944.
I0304 19:29:41.745715 22579586809984 run.py:483] Algo bellman_ford step 3092 current loss 0.074056, current_train_items 98976.
I0304 19:29:41.776987 22579586809984 run.py:483] Algo bellman_ford step 3093 current loss 0.113553, current_train_items 99008.
I0304 19:29:41.810552 22579586809984 run.py:483] Algo bellman_ford step 3094 current loss 0.104063, current_train_items 99040.
I0304 19:29:41.829826 22579586809984 run.py:483] Algo bellman_ford step 3095 current loss 0.006553, current_train_items 99072.
I0304 19:29:41.846609 22579586809984 run.py:483] Algo bellman_ford step 3096 current loss 0.018242, current_train_items 99104.
I0304 19:29:41.871417 22579586809984 run.py:483] Algo bellman_ford step 3097 current loss 0.081588, current_train_items 99136.
I0304 19:29:41.902177 22579586809984 run.py:483] Algo bellman_ford step 3098 current loss 0.076945, current_train_items 99168.
I0304 19:29:41.935117 22579586809984 run.py:483] Algo bellman_ford step 3099 current loss 0.079047, current_train_items 99200.
I0304 19:29:41.955052 22579586809984 run.py:483] Algo bellman_ford step 3100 current loss 0.003440, current_train_items 99232.
I0304 19:29:41.962771 22579586809984 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0304 19:29:41.962875 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.986, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:29:41.993294 22579586809984 run.py:483] Algo bellman_ford step 3101 current loss 0.021892, current_train_items 99264.
I0304 19:29:42.017991 22579586809984 run.py:483] Algo bellman_ford step 3102 current loss 0.046533, current_train_items 99296.
I0304 19:29:42.050495 22579586809984 run.py:483] Algo bellman_ford step 3103 current loss 0.046212, current_train_items 99328.
I0304 19:29:42.085658 22579586809984 run.py:483] Algo bellman_ford step 3104 current loss 0.067867, current_train_items 99360.
I0304 19:29:42.106087 22579586809984 run.py:483] Algo bellman_ford step 3105 current loss 0.018977, current_train_items 99392.
I0304 19:29:42.121761 22579586809984 run.py:483] Algo bellman_ford step 3106 current loss 0.005160, current_train_items 99424.
I0304 19:29:42.146153 22579586809984 run.py:483] Algo bellman_ford step 3107 current loss 0.036493, current_train_items 99456.
I0304 19:29:42.178750 22579586809984 run.py:483] Algo bellman_ford step 3108 current loss 0.053402, current_train_items 99488.
I0304 19:29:42.211624 22579586809984 run.py:483] Algo bellman_ford step 3109 current loss 0.083206, current_train_items 99520.
I0304 19:29:42.231760 22579586809984 run.py:483] Algo bellman_ford step 3110 current loss 0.007524, current_train_items 99552.
I0304 19:29:42.248106 22579586809984 run.py:483] Algo bellman_ford step 3111 current loss 0.017411, current_train_items 99584.
I0304 19:29:42.272246 22579586809984 run.py:483] Algo bellman_ford step 3112 current loss 0.091007, current_train_items 99616.
I0304 19:29:42.303420 22579586809984 run.py:483] Algo bellman_ford step 3113 current loss 0.057236, current_train_items 99648.
I0304 19:29:42.337748 22579586809984 run.py:483] Algo bellman_ford step 3114 current loss 0.059015, current_train_items 99680.
I0304 19:29:42.357869 22579586809984 run.py:483] Algo bellman_ford step 3115 current loss 0.004205, current_train_items 99712.
I0304 19:29:42.374412 22579586809984 run.py:483] Algo bellman_ford step 3116 current loss 0.016936, current_train_items 99744.
I0304 19:29:42.398584 22579586809984 run.py:483] Algo bellman_ford step 3117 current loss 0.076937, current_train_items 99776.
I0304 19:29:42.430768 22579586809984 run.py:483] Algo bellman_ford step 3118 current loss 0.137146, current_train_items 99808.
I0304 19:29:42.463989 22579586809984 run.py:483] Algo bellman_ford step 3119 current loss 0.090819, current_train_items 99840.
I0304 19:29:42.483902 22579586809984 run.py:483] Algo bellman_ford step 3120 current loss 0.002618, current_train_items 99872.
I0304 19:29:42.500159 22579586809984 run.py:483] Algo bellman_ford step 3121 current loss 0.043193, current_train_items 99904.
I0304 19:29:42.524460 22579586809984 run.py:483] Algo bellman_ford step 3122 current loss 0.060100, current_train_items 99936.
I0304 19:29:42.556082 22579586809984 run.py:483] Algo bellman_ford step 3123 current loss 0.092493, current_train_items 99968.
I0304 19:29:42.589447 22579586809984 run.py:483] Algo bellman_ford step 3124 current loss 0.117385, current_train_items 100000.
I0304 19:29:42.609365 22579586809984 run.py:483] Algo bellman_ford step 3125 current loss 0.004265, current_train_items 100032.
I0304 19:29:42.625620 22579586809984 run.py:483] Algo bellman_ford step 3126 current loss 0.025337, current_train_items 100064.
I0304 19:29:42.648887 22579586809984 run.py:483] Algo bellman_ford step 3127 current loss 0.077359, current_train_items 100096.
I0304 19:29:42.681067 22579586809984 run.py:483] Algo bellman_ford step 3128 current loss 0.083657, current_train_items 100128.
I0304 19:29:42.713399 22579586809984 run.py:483] Algo bellman_ford step 3129 current loss 0.089007, current_train_items 100160.
I0304 19:29:42.733543 22579586809984 run.py:483] Algo bellman_ford step 3130 current loss 0.003415, current_train_items 100192.
I0304 19:29:42.750047 22579586809984 run.py:483] Algo bellman_ford step 3131 current loss 0.031987, current_train_items 100224.
I0304 19:29:42.774725 22579586809984 run.py:483] Algo bellman_ford step 3132 current loss 0.072493, current_train_items 100256.
I0304 19:29:42.805273 22579586809984 run.py:483] Algo bellman_ford step 3133 current loss 0.140669, current_train_items 100288.
I0304 19:29:42.840543 22579586809984 run.py:483] Algo bellman_ford step 3134 current loss 0.114620, current_train_items 100320.
I0304 19:29:42.859989 22579586809984 run.py:483] Algo bellman_ford step 3135 current loss 0.013006, current_train_items 100352.
I0304 19:29:42.876090 22579586809984 run.py:483] Algo bellman_ford step 3136 current loss 0.021140, current_train_items 100384.
I0304 19:29:42.900275 22579586809984 run.py:483] Algo bellman_ford step 3137 current loss 0.097472, current_train_items 100416.
I0304 19:29:42.931074 22579586809984 run.py:483] Algo bellman_ford step 3138 current loss 0.054251, current_train_items 100448.
I0304 19:29:42.965547 22579586809984 run.py:483] Algo bellman_ford step 3139 current loss 0.108796, current_train_items 100480.
I0304 19:29:42.984988 22579586809984 run.py:483] Algo bellman_ford step 3140 current loss 0.004651, current_train_items 100512.
I0304 19:29:43.001805 22579586809984 run.py:483] Algo bellman_ford step 3141 current loss 0.041597, current_train_items 100544.
I0304 19:29:43.024789 22579586809984 run.py:483] Algo bellman_ford step 3142 current loss 0.048262, current_train_items 100576.
I0304 19:29:43.057670 22579586809984 run.py:483] Algo bellman_ford step 3143 current loss 0.076849, current_train_items 100608.
I0304 19:29:43.091040 22579586809984 run.py:483] Algo bellman_ford step 3144 current loss 0.089139, current_train_items 100640.
I0304 19:29:43.110616 22579586809984 run.py:483] Algo bellman_ford step 3145 current loss 0.004936, current_train_items 100672.
I0304 19:29:43.126856 22579586809984 run.py:483] Algo bellman_ford step 3146 current loss 0.042306, current_train_items 100704.
I0304 19:29:43.150336 22579586809984 run.py:483] Algo bellman_ford step 3147 current loss 0.046695, current_train_items 100736.
I0304 19:29:43.180307 22579586809984 run.py:483] Algo bellman_ford step 3148 current loss 0.039742, current_train_items 100768.
I0304 19:29:43.215655 22579586809984 run.py:483] Algo bellman_ford step 3149 current loss 0.100081, current_train_items 100800.
I0304 19:29:43.235316 22579586809984 run.py:483] Algo bellman_ford step 3150 current loss 0.004354, current_train_items 100832.
I0304 19:29:43.243231 22579586809984 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0304 19:29:43.243335 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:43.260409 22579586809984 run.py:483] Algo bellman_ford step 3151 current loss 0.030621, current_train_items 100864.
I0304 19:29:43.285652 22579586809984 run.py:483] Algo bellman_ford step 3152 current loss 0.092235, current_train_items 100896.
I0304 19:29:43.317285 22579586809984 run.py:483] Algo bellman_ford step 3153 current loss 0.104113, current_train_items 100928.
I0304 19:29:43.351832 22579586809984 run.py:483] Algo bellman_ford step 3154 current loss 0.067684, current_train_items 100960.
I0304 19:29:43.371714 22579586809984 run.py:483] Algo bellman_ford step 3155 current loss 0.007182, current_train_items 100992.
I0304 19:29:43.387746 22579586809984 run.py:483] Algo bellman_ford step 3156 current loss 0.015828, current_train_items 101024.
I0304 19:29:43.412110 22579586809984 run.py:483] Algo bellman_ford step 3157 current loss 0.073435, current_train_items 101056.
I0304 19:29:43.442250 22579586809984 run.py:483] Algo bellman_ford step 3158 current loss 0.098685, current_train_items 101088.
I0304 19:29:43.475521 22579586809984 run.py:483] Algo bellman_ford step 3159 current loss 0.098159, current_train_items 101120.
I0304 19:29:43.495581 22579586809984 run.py:483] Algo bellman_ford step 3160 current loss 0.005483, current_train_items 101152.
I0304 19:29:43.511717 22579586809984 run.py:483] Algo bellman_ford step 3161 current loss 0.013530, current_train_items 101184.
I0304 19:29:43.534955 22579586809984 run.py:483] Algo bellman_ford step 3162 current loss 0.048668, current_train_items 101216.
I0304 19:29:43.565145 22579586809984 run.py:483] Algo bellman_ford step 3163 current loss 0.084808, current_train_items 101248.
I0304 19:29:43.599984 22579586809984 run.py:483] Algo bellman_ford step 3164 current loss 0.121244, current_train_items 101280.
I0304 19:29:43.619479 22579586809984 run.py:483] Algo bellman_ford step 3165 current loss 0.007618, current_train_items 101312.
I0304 19:29:43.636450 22579586809984 run.py:483] Algo bellman_ford step 3166 current loss 0.073989, current_train_items 101344.
I0304 19:29:43.660517 22579586809984 run.py:483] Algo bellman_ford step 3167 current loss 0.068653, current_train_items 101376.
I0304 19:29:43.691694 22579586809984 run.py:483] Algo bellman_ford step 3168 current loss 0.055663, current_train_items 101408.
I0304 19:29:43.724691 22579586809984 run.py:483] Algo bellman_ford step 3169 current loss 0.114327, current_train_items 101440.
I0304 19:29:43.744688 22579586809984 run.py:483] Algo bellman_ford step 3170 current loss 0.008961, current_train_items 101472.
I0304 19:29:43.761451 22579586809984 run.py:483] Algo bellman_ford step 3171 current loss 0.084514, current_train_items 101504.
I0304 19:29:43.785384 22579586809984 run.py:483] Algo bellman_ford step 3172 current loss 0.055943, current_train_items 101536.
I0304 19:29:43.816859 22579586809984 run.py:483] Algo bellman_ford step 3173 current loss 0.079906, current_train_items 101568.
I0304 19:29:43.848396 22579586809984 run.py:483] Algo bellman_ford step 3174 current loss 0.066179, current_train_items 101600.
I0304 19:29:43.868167 22579586809984 run.py:483] Algo bellman_ford step 3175 current loss 0.003892, current_train_items 101632.
I0304 19:29:43.884520 22579586809984 run.py:483] Algo bellman_ford step 3176 current loss 0.023874, current_train_items 101664.
I0304 19:29:43.907718 22579586809984 run.py:483] Algo bellman_ford step 3177 current loss 0.051719, current_train_items 101696.
I0304 19:29:43.936843 22579586809984 run.py:483] Algo bellman_ford step 3178 current loss 0.056071, current_train_items 101728.
I0304 19:29:43.971254 22579586809984 run.py:483] Algo bellman_ford step 3179 current loss 0.092351, current_train_items 101760.
I0304 19:29:43.990593 22579586809984 run.py:483] Algo bellman_ford step 3180 current loss 0.004805, current_train_items 101792.
I0304 19:29:44.006998 22579586809984 run.py:483] Algo bellman_ford step 3181 current loss 0.023751, current_train_items 101824.
I0304 19:29:44.031141 22579586809984 run.py:483] Algo bellman_ford step 3182 current loss 0.049117, current_train_items 101856.
I0304 19:29:44.062505 22579586809984 run.py:483] Algo bellman_ford step 3183 current loss 0.055733, current_train_items 101888.
I0304 19:29:44.098794 22579586809984 run.py:483] Algo bellman_ford step 3184 current loss 0.088376, current_train_items 101920.
I0304 19:29:44.118964 22579586809984 run.py:483] Algo bellman_ford step 3185 current loss 0.006558, current_train_items 101952.
I0304 19:29:44.135420 22579586809984 run.py:483] Algo bellman_ford step 3186 current loss 0.020435, current_train_items 101984.
I0304 19:29:44.159740 22579586809984 run.py:483] Algo bellman_ford step 3187 current loss 0.067147, current_train_items 102016.
I0304 19:29:44.190472 22579586809984 run.py:483] Algo bellman_ford step 3188 current loss 0.043091, current_train_items 102048.
I0304 19:29:44.224008 22579586809984 run.py:483] Algo bellman_ford step 3189 current loss 0.058539, current_train_items 102080.
I0304 19:29:44.243978 22579586809984 run.py:483] Algo bellman_ford step 3190 current loss 0.030085, current_train_items 102112.
I0304 19:29:44.260592 22579586809984 run.py:483] Algo bellman_ford step 3191 current loss 0.021382, current_train_items 102144.
I0304 19:29:44.284729 22579586809984 run.py:483] Algo bellman_ford step 3192 current loss 0.061889, current_train_items 102176.
I0304 19:29:44.315279 22579586809984 run.py:483] Algo bellman_ford step 3193 current loss 0.049610, current_train_items 102208.
I0304 19:29:44.349501 22579586809984 run.py:483] Algo bellman_ford step 3194 current loss 0.082219, current_train_items 102240.
I0304 19:29:44.369248 22579586809984 run.py:483] Algo bellman_ford step 3195 current loss 0.007098, current_train_items 102272.
I0304 19:29:44.385118 22579586809984 run.py:483] Algo bellman_ford step 3196 current loss 0.011032, current_train_items 102304.
I0304 19:29:44.408818 22579586809984 run.py:483] Algo bellman_ford step 3197 current loss 0.091819, current_train_items 102336.
I0304 19:29:44.440235 22579586809984 run.py:483] Algo bellman_ford step 3198 current loss 0.098803, current_train_items 102368.
I0304 19:29:44.474332 22579586809984 run.py:483] Algo bellman_ford step 3199 current loss 0.096066, current_train_items 102400.
I0304 19:29:44.494568 22579586809984 run.py:483] Algo bellman_ford step 3200 current loss 0.014143, current_train_items 102432.
I0304 19:29:44.502813 22579586809984 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0304 19:29:44.502918 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:29:44.519373 22579586809984 run.py:483] Algo bellman_ford step 3201 current loss 0.018455, current_train_items 102464.
I0304 19:29:44.543434 22579586809984 run.py:483] Algo bellman_ford step 3202 current loss 0.064771, current_train_items 102496.
I0304 19:29:44.576766 22579586809984 run.py:483] Algo bellman_ford step 3203 current loss 0.230254, current_train_items 102528.
I0304 19:29:44.611736 22579586809984 run.py:483] Algo bellman_ford step 3204 current loss 0.111333, current_train_items 102560.
I0304 19:29:44.631756 22579586809984 run.py:483] Algo bellman_ford step 3205 current loss 0.007878, current_train_items 102592.
I0304 19:29:44.647645 22579586809984 run.py:483] Algo bellman_ford step 3206 current loss 0.028662, current_train_items 102624.
I0304 19:29:44.670883 22579586809984 run.py:483] Algo bellman_ford step 3207 current loss 0.035374, current_train_items 102656.
I0304 19:29:44.703605 22579586809984 run.py:483] Algo bellman_ford step 3208 current loss 0.151167, current_train_items 102688.
I0304 19:29:44.738096 22579586809984 run.py:483] Algo bellman_ford step 3209 current loss 0.213539, current_train_items 102720.
I0304 19:29:44.757947 22579586809984 run.py:483] Algo bellman_ford step 3210 current loss 0.026889, current_train_items 102752.
I0304 19:29:44.773951 22579586809984 run.py:483] Algo bellman_ford step 3211 current loss 0.010024, current_train_items 102784.
I0304 19:29:44.797907 22579586809984 run.py:483] Algo bellman_ford step 3212 current loss 0.057242, current_train_items 102816.
I0304 19:29:44.829197 22579586809984 run.py:483] Algo bellman_ford step 3213 current loss 0.098286, current_train_items 102848.
I0304 19:29:44.863003 22579586809984 run.py:483] Algo bellman_ford step 3214 current loss 0.075072, current_train_items 102880.
I0304 19:29:44.883403 22579586809984 run.py:483] Algo bellman_ford step 3215 current loss 0.012837, current_train_items 102912.
I0304 19:29:44.899621 22579586809984 run.py:483] Algo bellman_ford step 3216 current loss 0.026152, current_train_items 102944.
I0304 19:29:44.924174 22579586809984 run.py:483] Algo bellman_ford step 3217 current loss 0.053517, current_train_items 102976.
I0304 19:29:44.954298 22579586809984 run.py:483] Algo bellman_ford step 3218 current loss 0.078239, current_train_items 103008.
I0304 19:29:44.987719 22579586809984 run.py:483] Algo bellman_ford step 3219 current loss 0.046176, current_train_items 103040.
I0304 19:29:45.007784 22579586809984 run.py:483] Algo bellman_ford step 3220 current loss 0.009792, current_train_items 103072.
I0304 19:29:45.024133 22579586809984 run.py:483] Algo bellman_ford step 3221 current loss 0.037515, current_train_items 103104.
I0304 19:29:45.048678 22579586809984 run.py:483] Algo bellman_ford step 3222 current loss 0.094920, current_train_items 103136.
I0304 19:29:45.080455 22579586809984 run.py:483] Algo bellman_ford step 3223 current loss 0.059951, current_train_items 103168.
I0304 19:29:45.112846 22579586809984 run.py:483] Algo bellman_ford step 3224 current loss 0.053822, current_train_items 103200.
I0304 19:29:45.132392 22579586809984 run.py:483] Algo bellman_ford step 3225 current loss 0.006511, current_train_items 103232.
I0304 19:29:45.148026 22579586809984 run.py:483] Algo bellman_ford step 3226 current loss 0.029804, current_train_items 103264.
I0304 19:29:45.172239 22579586809984 run.py:483] Algo bellman_ford step 3227 current loss 0.029700, current_train_items 103296.
I0304 19:29:45.204336 22579586809984 run.py:483] Algo bellman_ford step 3228 current loss 0.089329, current_train_items 103328.
I0304 19:29:45.242199 22579586809984 run.py:483] Algo bellman_ford step 3229 current loss 0.093438, current_train_items 103360.
I0304 19:29:45.262018 22579586809984 run.py:483] Algo bellman_ford step 3230 current loss 0.018044, current_train_items 103392.
I0304 19:29:45.278376 22579586809984 run.py:483] Algo bellman_ford step 3231 current loss 0.033605, current_train_items 103424.
I0304 19:29:45.301742 22579586809984 run.py:483] Algo bellman_ford step 3232 current loss 0.048336, current_train_items 103456.
I0304 19:29:45.333131 22579586809984 run.py:483] Algo bellman_ford step 3233 current loss 0.036705, current_train_items 103488.
I0304 19:29:45.368159 22579586809984 run.py:483] Algo bellman_ford step 3234 current loss 0.067837, current_train_items 103520.
I0304 19:29:45.387712 22579586809984 run.py:483] Algo bellman_ford step 3235 current loss 0.018302, current_train_items 103552.
I0304 19:29:45.404112 22579586809984 run.py:483] Algo bellman_ford step 3236 current loss 0.025714, current_train_items 103584.
I0304 19:29:45.427616 22579586809984 run.py:483] Algo bellman_ford step 3237 current loss 0.034363, current_train_items 103616.
I0304 19:29:45.460309 22579586809984 run.py:483] Algo bellman_ford step 3238 current loss 0.048871, current_train_items 103648.
I0304 19:29:45.493910 22579586809984 run.py:483] Algo bellman_ford step 3239 current loss 0.074337, current_train_items 103680.
I0304 19:29:45.513628 22579586809984 run.py:483] Algo bellman_ford step 3240 current loss 0.016011, current_train_items 103712.
I0304 19:29:45.530047 22579586809984 run.py:483] Algo bellman_ford step 3241 current loss 0.037408, current_train_items 103744.
I0304 19:29:45.553640 22579586809984 run.py:483] Algo bellman_ford step 3242 current loss 0.071744, current_train_items 103776.
I0304 19:29:45.584055 22579586809984 run.py:483] Algo bellman_ford step 3243 current loss 0.065156, current_train_items 103808.
I0304 19:29:45.618763 22579586809984 run.py:483] Algo bellman_ford step 3244 current loss 0.091054, current_train_items 103840.
I0304 19:29:45.638351 22579586809984 run.py:483] Algo bellman_ford step 3245 current loss 0.013234, current_train_items 103872.
I0304 19:29:45.654760 22579586809984 run.py:483] Algo bellman_ford step 3246 current loss 0.029592, current_train_items 103904.
I0304 19:29:45.678650 22579586809984 run.py:483] Algo bellman_ford step 3247 current loss 0.056491, current_train_items 103936.
I0304 19:29:45.710359 22579586809984 run.py:483] Algo bellman_ford step 3248 current loss 0.058043, current_train_items 103968.
I0304 19:29:45.743332 22579586809984 run.py:483] Algo bellman_ford step 3249 current loss 0.043955, current_train_items 104000.
I0304 19:29:45.763366 22579586809984 run.py:483] Algo bellman_ford step 3250 current loss 0.008486, current_train_items 104032.
I0304 19:29:45.771384 22579586809984 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0304 19:29:45.771490 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:29:45.788275 22579586809984 run.py:483] Algo bellman_ford step 3251 current loss 0.025078, current_train_items 104064.
I0304 19:29:45.810907 22579586809984 run.py:483] Algo bellman_ford step 3252 current loss 0.023190, current_train_items 104096.
I0304 19:29:45.843878 22579586809984 run.py:483] Algo bellman_ford step 3253 current loss 0.080428, current_train_items 104128.
I0304 19:29:45.877965 22579586809984 run.py:483] Algo bellman_ford step 3254 current loss 0.054076, current_train_items 104160.
I0304 19:29:45.897962 22579586809984 run.py:483] Algo bellman_ford step 3255 current loss 0.004397, current_train_items 104192.
I0304 19:29:45.913726 22579586809984 run.py:483] Algo bellman_ford step 3256 current loss 0.039170, current_train_items 104224.
I0304 19:29:45.936960 22579586809984 run.py:483] Algo bellman_ford step 3257 current loss 0.068247, current_train_items 104256.
I0304 19:29:45.969547 22579586809984 run.py:483] Algo bellman_ford step 3258 current loss 0.120039, current_train_items 104288.
I0304 19:29:46.004388 22579586809984 run.py:483] Algo bellman_ford step 3259 current loss 0.069933, current_train_items 104320.
I0304 19:29:46.024078 22579586809984 run.py:483] Algo bellman_ford step 3260 current loss 0.003585, current_train_items 104352.
I0304 19:29:46.040395 22579586809984 run.py:483] Algo bellman_ford step 3261 current loss 0.018734, current_train_items 104384.
I0304 19:29:46.063512 22579586809984 run.py:483] Algo bellman_ford step 3262 current loss 0.050126, current_train_items 104416.
I0304 19:29:46.095353 22579586809984 run.py:483] Algo bellman_ford step 3263 current loss 0.121666, current_train_items 104448.
I0304 19:29:46.129399 22579586809984 run.py:483] Algo bellman_ford step 3264 current loss 0.092243, current_train_items 104480.
I0304 19:29:46.148963 22579586809984 run.py:483] Algo bellman_ford step 3265 current loss 0.034885, current_train_items 104512.
I0304 19:29:46.165330 22579586809984 run.py:483] Algo bellman_ford step 3266 current loss 0.021940, current_train_items 104544.
I0304 19:29:46.188600 22579586809984 run.py:483] Algo bellman_ford step 3267 current loss 0.053640, current_train_items 104576.
I0304 19:29:46.217983 22579586809984 run.py:483] Algo bellman_ford step 3268 current loss 0.062813, current_train_items 104608.
I0304 19:29:46.254093 22579586809984 run.py:483] Algo bellman_ford step 3269 current loss 0.123694, current_train_items 104640.
I0304 19:29:46.274353 22579586809984 run.py:483] Algo bellman_ford step 3270 current loss 0.009361, current_train_items 104672.
I0304 19:29:46.290848 22579586809984 run.py:483] Algo bellman_ford step 3271 current loss 0.023024, current_train_items 104704.
I0304 19:29:46.314328 22579586809984 run.py:483] Algo bellman_ford step 3272 current loss 0.050108, current_train_items 104736.
I0304 19:29:46.344570 22579586809984 run.py:483] Algo bellman_ford step 3273 current loss 0.059297, current_train_items 104768.
I0304 19:29:46.376434 22579586809984 run.py:483] Algo bellman_ford step 3274 current loss 0.047551, current_train_items 104800.
I0304 19:29:46.396554 22579586809984 run.py:483] Algo bellman_ford step 3275 current loss 0.012073, current_train_items 104832.
I0304 19:29:46.412769 22579586809984 run.py:483] Algo bellman_ford step 3276 current loss 0.054368, current_train_items 104864.
I0304 19:29:46.436076 22579586809984 run.py:483] Algo bellman_ford step 3277 current loss 0.043282, current_train_items 104896.
I0304 19:29:46.468181 22579586809984 run.py:483] Algo bellman_ford step 3278 current loss 0.068017, current_train_items 104928.
I0304 19:29:46.502346 22579586809984 run.py:483] Algo bellman_ford step 3279 current loss 0.129082, current_train_items 104960.
I0304 19:29:46.521987 22579586809984 run.py:483] Algo bellman_ford step 3280 current loss 0.013533, current_train_items 104992.
I0304 19:29:46.538075 22579586809984 run.py:483] Algo bellman_ford step 3281 current loss 0.026840, current_train_items 105024.
I0304 19:29:46.561478 22579586809984 run.py:483] Algo bellman_ford step 3282 current loss 0.057501, current_train_items 105056.
I0304 19:29:46.593531 22579586809984 run.py:483] Algo bellman_ford step 3283 current loss 0.094179, current_train_items 105088.
I0304 19:29:46.627557 22579586809984 run.py:483] Algo bellman_ford step 3284 current loss 0.082157, current_train_items 105120.
I0304 19:29:46.647652 22579586809984 run.py:483] Algo bellman_ford step 3285 current loss 0.005157, current_train_items 105152.
I0304 19:29:46.664326 22579586809984 run.py:483] Algo bellman_ford step 3286 current loss 0.020693, current_train_items 105184.
I0304 19:29:46.688106 22579586809984 run.py:483] Algo bellman_ford step 3287 current loss 0.040718, current_train_items 105216.
I0304 19:29:46.718261 22579586809984 run.py:483] Algo bellman_ford step 3288 current loss 0.036213, current_train_items 105248.
I0304 19:29:46.750341 22579586809984 run.py:483] Algo bellman_ford step 3289 current loss 0.151624, current_train_items 105280.
I0304 19:29:46.770303 22579586809984 run.py:483] Algo bellman_ford step 3290 current loss 0.006151, current_train_items 105312.
I0304 19:29:46.787129 22579586809984 run.py:483] Algo bellman_ford step 3291 current loss 0.019165, current_train_items 105344.
I0304 19:29:46.810777 22579586809984 run.py:483] Algo bellman_ford step 3292 current loss 0.060115, current_train_items 105376.
I0304 19:29:46.842308 22579586809984 run.py:483] Algo bellman_ford step 3293 current loss 0.080358, current_train_items 105408.
I0304 19:29:46.876473 22579586809984 run.py:483] Algo bellman_ford step 3294 current loss 0.087795, current_train_items 105440.
I0304 19:29:46.896187 22579586809984 run.py:483] Algo bellman_ford step 3295 current loss 0.009053, current_train_items 105472.
I0304 19:29:46.912129 22579586809984 run.py:483] Algo bellman_ford step 3296 current loss 0.005791, current_train_items 105504.
I0304 19:29:46.936606 22579586809984 run.py:483] Algo bellman_ford step 3297 current loss 0.093057, current_train_items 105536.
I0304 19:29:46.969696 22579586809984 run.py:483] Algo bellman_ford step 3298 current loss 0.087817, current_train_items 105568.
I0304 19:29:47.004356 22579586809984 run.py:483] Algo bellman_ford step 3299 current loss 0.094204, current_train_items 105600.
I0304 19:29:47.024137 22579586809984 run.py:483] Algo bellman_ford step 3300 current loss 0.009075, current_train_items 105632.
I0304 19:29:47.031673 22579586809984 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0304 19:29:47.031786 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:29:47.048598 22579586809984 run.py:483] Algo bellman_ford step 3301 current loss 0.033959, current_train_items 105664.
I0304 19:29:47.072617 22579586809984 run.py:483] Algo bellman_ford step 3302 current loss 0.056541, current_train_items 105696.
I0304 19:29:47.103318 22579586809984 run.py:483] Algo bellman_ford step 3303 current loss 0.070194, current_train_items 105728.
I0304 19:29:47.135502 22579586809984 run.py:483] Algo bellman_ford step 3304 current loss 0.078039, current_train_items 105760.
I0304 19:29:47.155854 22579586809984 run.py:483] Algo bellman_ford step 3305 current loss 0.003678, current_train_items 105792.
I0304 19:29:47.172107 22579586809984 run.py:483] Algo bellman_ford step 3306 current loss 0.019867, current_train_items 105824.
I0304 19:29:47.196805 22579586809984 run.py:483] Algo bellman_ford step 3307 current loss 0.030057, current_train_items 105856.
I0304 19:29:47.228394 22579586809984 run.py:483] Algo bellman_ford step 3308 current loss 0.058938, current_train_items 105888.
I0304 19:29:47.264023 22579586809984 run.py:483] Algo bellman_ford step 3309 current loss 0.120388, current_train_items 105920.
I0304 19:29:47.284120 22579586809984 run.py:483] Algo bellman_ford step 3310 current loss 0.019825, current_train_items 105952.
I0304 19:29:47.300700 22579586809984 run.py:483] Algo bellman_ford step 3311 current loss 0.064899, current_train_items 105984.
I0304 19:29:47.325079 22579586809984 run.py:483] Algo bellman_ford step 3312 current loss 0.061276, current_train_items 106016.
I0304 19:29:47.357146 22579586809984 run.py:483] Algo bellman_ford step 3313 current loss 0.060783, current_train_items 106048.
I0304 19:29:47.390022 22579586809984 run.py:483] Algo bellman_ford step 3314 current loss 0.072458, current_train_items 106080.
I0304 19:29:47.409557 22579586809984 run.py:483] Algo bellman_ford step 3315 current loss 0.003631, current_train_items 106112.
I0304 19:29:47.426223 22579586809984 run.py:483] Algo bellman_ford step 3316 current loss 0.018491, current_train_items 106144.
I0304 19:29:47.449764 22579586809984 run.py:483] Algo bellman_ford step 3317 current loss 0.029439, current_train_items 106176.
I0304 19:29:47.481021 22579586809984 run.py:483] Algo bellman_ford step 3318 current loss 0.043329, current_train_items 106208.
I0304 19:29:47.515656 22579586809984 run.py:483] Algo bellman_ford step 3319 current loss 0.122850, current_train_items 106240.
I0304 19:29:47.535559 22579586809984 run.py:483] Algo bellman_ford step 3320 current loss 0.003490, current_train_items 106272.
I0304 19:29:47.551877 22579586809984 run.py:483] Algo bellman_ford step 3321 current loss 0.018042, current_train_items 106304.
I0304 19:29:47.576588 22579586809984 run.py:483] Algo bellman_ford step 3322 current loss 0.055871, current_train_items 106336.
I0304 19:29:47.608395 22579586809984 run.py:483] Algo bellman_ford step 3323 current loss 0.086716, current_train_items 106368.
I0304 19:29:47.641291 22579586809984 run.py:483] Algo bellman_ford step 3324 current loss 0.075905, current_train_items 106400.
I0304 19:29:47.660874 22579586809984 run.py:483] Algo bellman_ford step 3325 current loss 0.002559, current_train_items 106432.
I0304 19:29:47.677304 22579586809984 run.py:483] Algo bellman_ford step 3326 current loss 0.024944, current_train_items 106464.
I0304 19:29:47.701125 22579586809984 run.py:483] Algo bellman_ford step 3327 current loss 0.042364, current_train_items 106496.
I0304 19:29:47.731955 22579586809984 run.py:483] Algo bellman_ford step 3328 current loss 0.051676, current_train_items 106528.
I0304 19:29:47.765528 22579586809984 run.py:483] Algo bellman_ford step 3329 current loss 0.105522, current_train_items 106560.
I0304 19:29:47.785316 22579586809984 run.py:483] Algo bellman_ford step 3330 current loss 0.007431, current_train_items 106592.
I0304 19:29:47.801486 22579586809984 run.py:483] Algo bellman_ford step 3331 current loss 0.022998, current_train_items 106624.
I0304 19:29:47.824943 22579586809984 run.py:483] Algo bellman_ford step 3332 current loss 0.045532, current_train_items 106656.
I0304 19:29:47.855661 22579586809984 run.py:483] Algo bellman_ford step 3333 current loss 0.049886, current_train_items 106688.
I0304 19:29:47.888275 22579586809984 run.py:483] Algo bellman_ford step 3334 current loss 0.081631, current_train_items 106720.
I0304 19:29:47.907975 22579586809984 run.py:483] Algo bellman_ford step 3335 current loss 0.004312, current_train_items 106752.
I0304 19:29:47.923775 22579586809984 run.py:483] Algo bellman_ford step 3336 current loss 0.023108, current_train_items 106784.
I0304 19:29:47.948489 22579586809984 run.py:483] Algo bellman_ford step 3337 current loss 0.077707, current_train_items 106816.
I0304 19:29:47.981060 22579586809984 run.py:483] Algo bellman_ford step 3338 current loss 0.118183, current_train_items 106848.
I0304 19:29:48.016316 22579586809984 run.py:483] Algo bellman_ford step 3339 current loss 0.071408, current_train_items 106880.
I0304 19:29:48.035653 22579586809984 run.py:483] Algo bellman_ford step 3340 current loss 0.004497, current_train_items 106912.
I0304 19:29:48.052350 22579586809984 run.py:483] Algo bellman_ford step 3341 current loss 0.028013, current_train_items 106944.
I0304 19:29:48.076630 22579586809984 run.py:483] Algo bellman_ford step 3342 current loss 0.093396, current_train_items 106976.
I0304 19:29:48.108937 22579586809984 run.py:483] Algo bellman_ford step 3343 current loss 0.139413, current_train_items 107008.
I0304 19:29:48.140876 22579586809984 run.py:483] Algo bellman_ford step 3344 current loss 0.108293, current_train_items 107040.
I0304 19:29:48.161008 22579586809984 run.py:483] Algo bellman_ford step 3345 current loss 0.010185, current_train_items 107072.
I0304 19:29:48.176959 22579586809984 run.py:483] Algo bellman_ford step 3346 current loss 0.019405, current_train_items 107104.
I0304 19:29:48.200821 22579586809984 run.py:483] Algo bellman_ford step 3347 current loss 0.060544, current_train_items 107136.
I0304 19:29:48.231846 22579586809984 run.py:483] Algo bellman_ford step 3348 current loss 0.048218, current_train_items 107168.
I0304 19:29:48.263461 22579586809984 run.py:483] Algo bellman_ford step 3349 current loss 0.088047, current_train_items 107200.
I0304 19:29:48.283387 22579586809984 run.py:483] Algo bellman_ford step 3350 current loss 0.038229, current_train_items 107232.
I0304 19:29:48.291292 22579586809984 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0304 19:29:48.291395 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:29:48.308839 22579586809984 run.py:483] Algo bellman_ford step 3351 current loss 0.027719, current_train_items 107264.
I0304 19:29:48.333755 22579586809984 run.py:483] Algo bellman_ford step 3352 current loss 0.064338, current_train_items 107296.
I0304 19:29:48.366411 22579586809984 run.py:483] Algo bellman_ford step 3353 current loss 0.058981, current_train_items 107328.
I0304 19:29:48.400751 22579586809984 run.py:483] Algo bellman_ford step 3354 current loss 0.078617, current_train_items 107360.
I0304 19:29:48.420358 22579586809984 run.py:483] Algo bellman_ford step 3355 current loss 0.004261, current_train_items 107392.
I0304 19:29:48.436321 22579586809984 run.py:483] Algo bellman_ford step 3356 current loss 0.133942, current_train_items 107424.
I0304 19:29:48.460448 22579586809984 run.py:483] Algo bellman_ford step 3357 current loss 0.119468, current_train_items 107456.
I0304 19:29:48.491775 22579586809984 run.py:483] Algo bellman_ford step 3358 current loss 0.109603, current_train_items 107488.
I0304 19:29:48.527293 22579586809984 run.py:483] Algo bellman_ford step 3359 current loss 0.088616, current_train_items 107520.
I0304 19:29:48.547160 22579586809984 run.py:483] Algo bellman_ford step 3360 current loss 0.005620, current_train_items 107552.
I0304 19:29:48.563467 22579586809984 run.py:483] Algo bellman_ford step 3361 current loss 0.013816, current_train_items 107584.
I0304 19:29:48.588027 22579586809984 run.py:483] Algo bellman_ford step 3362 current loss 0.127376, current_train_items 107616.
I0304 19:29:48.618997 22579586809984 run.py:483] Algo bellman_ford step 3363 current loss 0.173714, current_train_items 107648.
I0304 19:29:48.652158 22579586809984 run.py:483] Algo bellman_ford step 3364 current loss 0.135179, current_train_items 107680.
I0304 19:29:48.671886 22579586809984 run.py:483] Algo bellman_ford step 3365 current loss 0.012773, current_train_items 107712.
I0304 19:29:48.687852 22579586809984 run.py:483] Algo bellman_ford step 3366 current loss 0.018785, current_train_items 107744.
I0304 19:29:48.712573 22579586809984 run.py:483] Algo bellman_ford step 3367 current loss 0.062414, current_train_items 107776.
I0304 19:29:48.742827 22579586809984 run.py:483] Algo bellman_ford step 3368 current loss 0.075402, current_train_items 107808.
I0304 19:29:48.775842 22579586809984 run.py:483] Algo bellman_ford step 3369 current loss 0.084931, current_train_items 107840.
I0304 19:29:48.796323 22579586809984 run.py:483] Algo bellman_ford step 3370 current loss 0.007042, current_train_items 107872.
I0304 19:29:48.812903 22579586809984 run.py:483] Algo bellman_ford step 3371 current loss 0.026361, current_train_items 107904.
I0304 19:29:48.837099 22579586809984 run.py:483] Algo bellman_ford step 3372 current loss 0.079460, current_train_items 107936.
I0304 19:29:48.869969 22579586809984 run.py:483] Algo bellman_ford step 3373 current loss 0.150457, current_train_items 107968.
I0304 19:29:48.902866 22579586809984 run.py:483] Algo bellman_ford step 3374 current loss 0.187052, current_train_items 108000.
I0304 19:29:48.922766 22579586809984 run.py:483] Algo bellman_ford step 3375 current loss 0.006823, current_train_items 108032.
I0304 19:29:48.938867 22579586809984 run.py:483] Algo bellman_ford step 3376 current loss 0.026383, current_train_items 108064.
I0304 19:29:48.961889 22579586809984 run.py:483] Algo bellman_ford step 3377 current loss 0.061702, current_train_items 108096.
I0304 19:29:48.992270 22579586809984 run.py:483] Algo bellman_ford step 3378 current loss 0.039260, current_train_items 108128.
I0304 19:29:49.027497 22579586809984 run.py:483] Algo bellman_ford step 3379 current loss 0.123783, current_train_items 108160.
I0304 19:29:49.047320 22579586809984 run.py:483] Algo bellman_ford step 3380 current loss 0.005734, current_train_items 108192.
I0304 19:29:49.063946 22579586809984 run.py:483] Algo bellman_ford step 3381 current loss 0.056511, current_train_items 108224.
I0304 19:29:49.089309 22579586809984 run.py:483] Algo bellman_ford step 3382 current loss 0.037013, current_train_items 108256.
I0304 19:29:49.119995 22579586809984 run.py:483] Algo bellman_ford step 3383 current loss 0.086862, current_train_items 108288.
I0304 19:29:49.155521 22579586809984 run.py:483] Algo bellman_ford step 3384 current loss 0.097851, current_train_items 108320.
I0304 19:29:49.175497 22579586809984 run.py:483] Algo bellman_ford step 3385 current loss 0.004842, current_train_items 108352.
I0304 19:29:49.192117 22579586809984 run.py:483] Algo bellman_ford step 3386 current loss 0.022473, current_train_items 108384.
I0304 19:29:49.215903 22579586809984 run.py:483] Algo bellman_ford step 3387 current loss 0.055709, current_train_items 108416.
I0304 19:29:49.246777 22579586809984 run.py:483] Algo bellman_ford step 3388 current loss 0.079677, current_train_items 108448.
I0304 19:29:49.279217 22579586809984 run.py:483] Algo bellman_ford step 3389 current loss 0.107983, current_train_items 108480.
I0304 19:29:49.299366 22579586809984 run.py:483] Algo bellman_ford step 3390 current loss 0.016650, current_train_items 108512.
I0304 19:29:49.315887 22579586809984 run.py:483] Algo bellman_ford step 3391 current loss 0.014152, current_train_items 108544.
I0304 19:29:49.339950 22579586809984 run.py:483] Algo bellman_ford step 3392 current loss 0.034124, current_train_items 108576.
I0304 19:29:49.370659 22579586809984 run.py:483] Algo bellman_ford step 3393 current loss 0.066825, current_train_items 108608.
I0304 19:29:49.402703 22579586809984 run.py:483] Algo bellman_ford step 3394 current loss 0.050928, current_train_items 108640.
I0304 19:29:49.422499 22579586809984 run.py:483] Algo bellman_ford step 3395 current loss 0.006303, current_train_items 108672.
I0304 19:29:49.439043 22579586809984 run.py:483] Algo bellman_ford step 3396 current loss 0.029956, current_train_items 108704.
I0304 19:29:49.463155 22579586809984 run.py:483] Algo bellman_ford step 3397 current loss 0.034797, current_train_items 108736.
I0304 19:29:49.495371 22579586809984 run.py:483] Algo bellman_ford step 3398 current loss 0.105148, current_train_items 108768.
I0304 19:29:49.528240 22579586809984 run.py:483] Algo bellman_ford step 3399 current loss 0.057738, current_train_items 108800.
I0304 19:29:49.548246 22579586809984 run.py:483] Algo bellman_ford step 3400 current loss 0.004555, current_train_items 108832.
I0304 19:29:49.556042 22579586809984 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0304 19:29:49.556145 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:29:49.573361 22579586809984 run.py:483] Algo bellman_ford step 3401 current loss 0.024998, current_train_items 108864.
I0304 19:29:49.597629 22579586809984 run.py:483] Algo bellman_ford step 3402 current loss 0.049094, current_train_items 108896.
I0304 19:29:49.628910 22579586809984 run.py:483] Algo bellman_ford step 3403 current loss 0.071781, current_train_items 108928.
I0304 19:29:49.664010 22579586809984 run.py:483] Algo bellman_ford step 3404 current loss 0.106924, current_train_items 108960.
I0304 19:29:49.683759 22579586809984 run.py:483] Algo bellman_ford step 3405 current loss 0.003622, current_train_items 108992.
I0304 19:29:49.699444 22579586809984 run.py:483] Algo bellman_ford step 3406 current loss 0.050089, current_train_items 109024.
I0304 19:29:49.723671 22579586809984 run.py:483] Algo bellman_ford step 3407 current loss 0.091633, current_train_items 109056.
I0304 19:29:49.756367 22579586809984 run.py:483] Algo bellman_ford step 3408 current loss 0.126519, current_train_items 109088.
I0304 19:29:49.789269 22579586809984 run.py:483] Algo bellman_ford step 3409 current loss 0.101270, current_train_items 109120.
I0304 19:29:49.808657 22579586809984 run.py:483] Algo bellman_ford step 3410 current loss 0.005140, current_train_items 109152.
I0304 19:29:49.825093 22579586809984 run.py:483] Algo bellman_ford step 3411 current loss 0.032325, current_train_items 109184.
I0304 19:29:49.849618 22579586809984 run.py:483] Algo bellman_ford step 3412 current loss 0.068408, current_train_items 109216.
I0304 19:29:49.882137 22579586809984 run.py:483] Algo bellman_ford step 3413 current loss 0.107024, current_train_items 109248.
I0304 19:29:49.918708 22579586809984 run.py:483] Algo bellman_ford step 3414 current loss 0.096031, current_train_items 109280.
I0304 19:29:49.938144 22579586809984 run.py:483] Algo bellman_ford step 3415 current loss 0.006522, current_train_items 109312.
I0304 19:29:49.954525 22579586809984 run.py:483] Algo bellman_ford step 3416 current loss 0.017984, current_train_items 109344.
I0304 19:29:49.978462 22579586809984 run.py:483] Algo bellman_ford step 3417 current loss 0.043241, current_train_items 109376.
I0304 19:29:50.009840 22579586809984 run.py:483] Algo bellman_ford step 3418 current loss 0.130653, current_train_items 109408.
I0304 19:29:50.042495 22579586809984 run.py:483] Algo bellman_ford step 3419 current loss 0.106871, current_train_items 109440.
I0304 19:29:50.062517 22579586809984 run.py:483] Algo bellman_ford step 3420 current loss 0.007383, current_train_items 109472.
I0304 19:29:50.078507 22579586809984 run.py:483] Algo bellman_ford step 3421 current loss 0.027921, current_train_items 109504.
I0304 19:29:50.102354 22579586809984 run.py:483] Algo bellman_ford step 3422 current loss 0.056390, current_train_items 109536.
I0304 19:29:50.133921 22579586809984 run.py:483] Algo bellman_ford step 3423 current loss 0.033654, current_train_items 109568.
I0304 19:29:50.166916 22579586809984 run.py:483] Algo bellman_ford step 3424 current loss 0.097531, current_train_items 109600.
I0304 19:29:50.186336 22579586809984 run.py:483] Algo bellman_ford step 3425 current loss 0.010072, current_train_items 109632.
I0304 19:29:50.202991 22579586809984 run.py:483] Algo bellman_ford step 3426 current loss 0.019879, current_train_items 109664.
I0304 19:29:50.226401 22579586809984 run.py:483] Algo bellman_ford step 3427 current loss 0.042628, current_train_items 109696.
I0304 19:29:50.257995 22579586809984 run.py:483] Algo bellman_ford step 3428 current loss 0.073973, current_train_items 109728.
I0304 19:29:50.289011 22579586809984 run.py:483] Algo bellman_ford step 3429 current loss 0.063211, current_train_items 109760.
I0304 19:29:50.308610 22579586809984 run.py:483] Algo bellman_ford step 3430 current loss 0.005504, current_train_items 109792.
I0304 19:29:50.325236 22579586809984 run.py:483] Algo bellman_ford step 3431 current loss 0.012107, current_train_items 109824.
I0304 19:29:50.348605 22579586809984 run.py:483] Algo bellman_ford step 3432 current loss 0.028328, current_train_items 109856.
I0304 19:29:50.381630 22579586809984 run.py:483] Algo bellman_ford step 3433 current loss 0.105511, current_train_items 109888.
I0304 19:29:50.413988 22579586809984 run.py:483] Algo bellman_ford step 3434 current loss 0.117330, current_train_items 109920.
I0304 19:29:50.433559 22579586809984 run.py:483] Algo bellman_ford step 3435 current loss 0.003851, current_train_items 109952.
I0304 19:29:50.450007 22579586809984 run.py:483] Algo bellman_ford step 3436 current loss 0.025723, current_train_items 109984.
I0304 19:29:50.474057 22579586809984 run.py:483] Algo bellman_ford step 3437 current loss 0.050132, current_train_items 110016.
I0304 19:29:50.504199 22579586809984 run.py:483] Algo bellman_ford step 3438 current loss 0.070027, current_train_items 110048.
I0304 19:29:50.536070 22579586809984 run.py:483] Algo bellman_ford step 3439 current loss 0.060199, current_train_items 110080.
I0304 19:29:50.555630 22579586809984 run.py:483] Algo bellman_ford step 3440 current loss 0.033672, current_train_items 110112.
I0304 19:29:50.572304 22579586809984 run.py:483] Algo bellman_ford step 3441 current loss 0.032379, current_train_items 110144.
I0304 19:29:50.595872 22579586809984 run.py:483] Algo bellman_ford step 3442 current loss 0.049188, current_train_items 110176.
I0304 19:29:50.627691 22579586809984 run.py:483] Algo bellman_ford step 3443 current loss 0.115843, current_train_items 110208.
I0304 19:29:50.662230 22579586809984 run.py:483] Algo bellman_ford step 3444 current loss 0.102062, current_train_items 110240.
I0304 19:29:50.681634 22579586809984 run.py:483] Algo bellman_ford step 3445 current loss 0.004031, current_train_items 110272.
I0304 19:29:50.698364 22579586809984 run.py:483] Algo bellman_ford step 3446 current loss 0.027070, current_train_items 110304.
I0304 19:29:50.722448 22579586809984 run.py:483] Algo bellman_ford step 3447 current loss 0.042672, current_train_items 110336.
I0304 19:29:50.753431 22579586809984 run.py:483] Algo bellman_ford step 3448 current loss 0.043039, current_train_items 110368.
I0304 19:29:50.786931 22579586809984 run.py:483] Algo bellman_ford step 3449 current loss 0.073377, current_train_items 110400.
I0304 19:29:50.806450 22579586809984 run.py:483] Algo bellman_ford step 3450 current loss 0.011934, current_train_items 110432.
I0304 19:29:50.814807 22579586809984 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0304 19:29:50.814913 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:50.832132 22579586809984 run.py:483] Algo bellman_ford step 3451 current loss 0.028431, current_train_items 110464.
I0304 19:29:50.857081 22579586809984 run.py:483] Algo bellman_ford step 3452 current loss 0.048729, current_train_items 110496.
I0304 19:29:50.887358 22579586809984 run.py:483] Algo bellman_ford step 3453 current loss 0.058814, current_train_items 110528.
I0304 19:29:50.923289 22579586809984 run.py:483] Algo bellman_ford step 3454 current loss 0.069434, current_train_items 110560.
I0304 19:29:50.943189 22579586809984 run.py:483] Algo bellman_ford step 3455 current loss 0.002896, current_train_items 110592.
I0304 19:29:50.958884 22579586809984 run.py:483] Algo bellman_ford step 3456 current loss 0.020105, current_train_items 110624.
I0304 19:29:50.982737 22579586809984 run.py:483] Algo bellman_ford step 3457 current loss 0.036034, current_train_items 110656.
I0304 19:29:51.014994 22579586809984 run.py:483] Algo bellman_ford step 3458 current loss 0.059576, current_train_items 110688.
I0304 19:29:51.047379 22579586809984 run.py:483] Algo bellman_ford step 3459 current loss 0.062024, current_train_items 110720.
I0304 19:29:51.067772 22579586809984 run.py:483] Algo bellman_ford step 3460 current loss 0.004107, current_train_items 110752.
I0304 19:29:51.084479 22579586809984 run.py:483] Algo bellman_ford step 3461 current loss 0.046474, current_train_items 110784.
I0304 19:29:51.108060 22579586809984 run.py:483] Algo bellman_ford step 3462 current loss 0.061993, current_train_items 110816.
I0304 19:29:51.138892 22579586809984 run.py:483] Algo bellman_ford step 3463 current loss 0.036567, current_train_items 110848.
I0304 19:29:51.171965 22579586809984 run.py:483] Algo bellman_ford step 3464 current loss 0.082711, current_train_items 110880.
I0304 19:29:51.192092 22579586809984 run.py:483] Algo bellman_ford step 3465 current loss 0.010631, current_train_items 110912.
I0304 19:29:51.208644 22579586809984 run.py:483] Algo bellman_ford step 3466 current loss 0.020493, current_train_items 110944.
I0304 19:29:51.233036 22579586809984 run.py:483] Algo bellman_ford step 3467 current loss 0.090353, current_train_items 110976.
I0304 19:29:51.265394 22579586809984 run.py:483] Algo bellman_ford step 3468 current loss 0.140463, current_train_items 111008.
I0304 19:29:51.301186 22579586809984 run.py:483] Algo bellman_ford step 3469 current loss 0.119357, current_train_items 111040.
I0304 19:29:51.320978 22579586809984 run.py:483] Algo bellman_ford step 3470 current loss 0.003992, current_train_items 111072.
I0304 19:29:51.337845 22579586809984 run.py:483] Algo bellman_ford step 3471 current loss 0.029374, current_train_items 111104.
I0304 19:29:51.361136 22579586809984 run.py:483] Algo bellman_ford step 3472 current loss 0.048289, current_train_items 111136.
I0304 19:29:51.393187 22579586809984 run.py:483] Algo bellman_ford step 3473 current loss 0.074110, current_train_items 111168.
I0304 19:29:51.426932 22579586809984 run.py:483] Algo bellman_ford step 3474 current loss 0.070177, current_train_items 111200.
I0304 19:29:51.446803 22579586809984 run.py:483] Algo bellman_ford step 3475 current loss 0.004703, current_train_items 111232.
I0304 19:29:51.463096 22579586809984 run.py:483] Algo bellman_ford step 3476 current loss 0.057215, current_train_items 111264.
I0304 19:29:51.485939 22579586809984 run.py:483] Algo bellman_ford step 3477 current loss 0.090809, current_train_items 111296.
I0304 19:29:51.516088 22579586809984 run.py:483] Algo bellman_ford step 3478 current loss 0.081131, current_train_items 111328.
I0304 19:29:51.550859 22579586809984 run.py:483] Algo bellman_ford step 3479 current loss 0.101559, current_train_items 111360.
I0304 19:29:51.570363 22579586809984 run.py:483] Algo bellman_ford step 3480 current loss 0.006066, current_train_items 111392.
I0304 19:29:51.586481 22579586809984 run.py:483] Algo bellman_ford step 3481 current loss 0.039715, current_train_items 111424.
I0304 19:29:51.610337 22579586809984 run.py:483] Algo bellman_ford step 3482 current loss 0.045882, current_train_items 111456.
I0304 19:29:51.640509 22579586809984 run.py:483] Algo bellman_ford step 3483 current loss 0.058499, current_train_items 111488.
I0304 19:29:51.673487 22579586809984 run.py:483] Algo bellman_ford step 3484 current loss 0.071605, current_train_items 111520.
I0304 19:29:51.693271 22579586809984 run.py:483] Algo bellman_ford step 3485 current loss 0.003568, current_train_items 111552.
I0304 19:29:51.709827 22579586809984 run.py:483] Algo bellman_ford step 3486 current loss 0.022216, current_train_items 111584.
I0304 19:29:51.733725 22579586809984 run.py:483] Algo bellman_ford step 3487 current loss 0.101467, current_train_items 111616.
I0304 19:29:51.764183 22579586809984 run.py:483] Algo bellman_ford step 3488 current loss 0.064745, current_train_items 111648.
I0304 19:29:51.795983 22579586809984 run.py:483] Algo bellman_ford step 3489 current loss 0.092069, current_train_items 111680.
I0304 19:29:51.815868 22579586809984 run.py:483] Algo bellman_ford step 3490 current loss 0.004455, current_train_items 111712.
I0304 19:29:51.832000 22579586809984 run.py:483] Algo bellman_ford step 3491 current loss 0.011540, current_train_items 111744.
I0304 19:29:51.855516 22579586809984 run.py:483] Algo bellman_ford step 3492 current loss 0.057871, current_train_items 111776.
I0304 19:29:51.887672 22579586809984 run.py:483] Algo bellman_ford step 3493 current loss 0.088850, current_train_items 111808.
I0304 19:29:51.920309 22579586809984 run.py:483] Algo bellman_ford step 3494 current loss 0.065590, current_train_items 111840.
I0304 19:29:51.939755 22579586809984 run.py:483] Algo bellman_ford step 3495 current loss 0.002605, current_train_items 111872.
I0304 19:29:51.956000 22579586809984 run.py:483] Algo bellman_ford step 3496 current loss 0.036254, current_train_items 111904.
I0304 19:29:51.979341 22579586809984 run.py:483] Algo bellman_ford step 3497 current loss 0.035385, current_train_items 111936.
I0304 19:29:52.010571 22579586809984 run.py:483] Algo bellman_ford step 3498 current loss 0.089947, current_train_items 111968.
I0304 19:29:52.044615 22579586809984 run.py:483] Algo bellman_ford step 3499 current loss 0.087232, current_train_items 112000.
I0304 19:29:52.064379 22579586809984 run.py:483] Algo bellman_ford step 3500 current loss 0.002744, current_train_items 112032.
I0304 19:29:52.071919 22579586809984 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0304 19:29:52.072025 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:29:52.088381 22579586809984 run.py:483] Algo bellman_ford step 3501 current loss 0.012718, current_train_items 112064.
I0304 19:29:52.112609 22579586809984 run.py:483] Algo bellman_ford step 3502 current loss 0.046710, current_train_items 112096.
I0304 19:29:52.144600 22579586809984 run.py:483] Algo bellman_ford step 3503 current loss 0.052477, current_train_items 112128.
I0304 19:29:52.180306 22579586809984 run.py:483] Algo bellman_ford step 3504 current loss 0.105509, current_train_items 112160.
I0304 19:29:52.200530 22579586809984 run.py:483] Algo bellman_ford step 3505 current loss 0.011553, current_train_items 112192.
I0304 19:29:52.215854 22579586809984 run.py:483] Algo bellman_ford step 3506 current loss 0.028047, current_train_items 112224.
I0304 19:29:52.240422 22579586809984 run.py:483] Algo bellman_ford step 3507 current loss 0.070151, current_train_items 112256.
I0304 19:29:52.270800 22579586809984 run.py:483] Algo bellman_ford step 3508 current loss 0.112204, current_train_items 112288.
I0304 19:29:52.301677 22579586809984 run.py:483] Algo bellman_ford step 3509 current loss 0.063436, current_train_items 112320.
I0304 19:29:52.321276 22579586809984 run.py:483] Algo bellman_ford step 3510 current loss 0.014121, current_train_items 112352.
I0304 19:29:52.337346 22579586809984 run.py:483] Algo bellman_ford step 3511 current loss 0.017157, current_train_items 112384.
I0304 19:29:52.360693 22579586809984 run.py:483] Algo bellman_ford step 3512 current loss 0.018832, current_train_items 112416.
I0304 19:29:52.390858 22579586809984 run.py:483] Algo bellman_ford step 3513 current loss 0.043228, current_train_items 112448.
I0304 19:29:52.423646 22579586809984 run.py:483] Algo bellman_ford step 3514 current loss 0.041504, current_train_items 112480.
I0304 19:29:52.442903 22579586809984 run.py:483] Algo bellman_ford step 3515 current loss 0.003059, current_train_items 112512.
I0304 19:29:52.458697 22579586809984 run.py:483] Algo bellman_ford step 3516 current loss 0.007576, current_train_items 112544.
I0304 19:29:52.482480 22579586809984 run.py:483] Algo bellman_ford step 3517 current loss 0.044215, current_train_items 112576.
I0304 19:29:52.513924 22579586809984 run.py:483] Algo bellman_ford step 3518 current loss 0.084655, current_train_items 112608.
I0304 19:29:52.549777 22579586809984 run.py:483] Algo bellman_ford step 3519 current loss 0.114318, current_train_items 112640.
I0304 19:29:52.569290 22579586809984 run.py:483] Algo bellman_ford step 3520 current loss 0.005237, current_train_items 112672.
I0304 19:29:52.585363 22579586809984 run.py:483] Algo bellman_ford step 3521 current loss 0.021099, current_train_items 112704.
I0304 19:29:52.608282 22579586809984 run.py:483] Algo bellman_ford step 3522 current loss 0.031235, current_train_items 112736.
I0304 19:29:52.639328 22579586809984 run.py:483] Algo bellman_ford step 3523 current loss 0.091833, current_train_items 112768.
I0304 19:29:52.673143 22579586809984 run.py:483] Algo bellman_ford step 3524 current loss 0.142184, current_train_items 112800.
I0304 19:29:52.693075 22579586809984 run.py:483] Algo bellman_ford step 3525 current loss 0.051068, current_train_items 112832.
I0304 19:29:52.709202 22579586809984 run.py:483] Algo bellman_ford step 3526 current loss 0.029459, current_train_items 112864.
I0304 19:29:52.733293 22579586809984 run.py:483] Algo bellman_ford step 3527 current loss 0.106059, current_train_items 112896.
I0304 19:29:52.766055 22579586809984 run.py:483] Algo bellman_ford step 3528 current loss 0.106326, current_train_items 112928.
I0304 19:29:52.798591 22579586809984 run.py:483] Algo bellman_ford step 3529 current loss 0.079893, current_train_items 112960.
I0304 19:29:52.818187 22579586809984 run.py:483] Algo bellman_ford step 3530 current loss 0.011977, current_train_items 112992.
I0304 19:29:52.834629 22579586809984 run.py:483] Algo bellman_ford step 3531 current loss 0.014062, current_train_items 113024.
I0304 19:29:52.859671 22579586809984 run.py:483] Algo bellman_ford step 3532 current loss 0.058408, current_train_items 113056.
I0304 19:29:52.893558 22579586809984 run.py:483] Algo bellman_ford step 3533 current loss 0.212817, current_train_items 113088.
I0304 19:29:52.927571 22579586809984 run.py:483] Algo bellman_ford step 3534 current loss 0.149493, current_train_items 113120.
I0304 19:29:52.947029 22579586809984 run.py:483] Algo bellman_ford step 3535 current loss 0.021821, current_train_items 113152.
I0304 19:29:52.963463 22579586809984 run.py:483] Algo bellman_ford step 3536 current loss 0.046092, current_train_items 113184.
I0304 19:29:52.987098 22579586809984 run.py:483] Algo bellman_ford step 3537 current loss 0.057566, current_train_items 113216.
I0304 19:29:53.018934 22579586809984 run.py:483] Algo bellman_ford step 3538 current loss 0.126535, current_train_items 113248.
I0304 19:29:53.054742 22579586809984 run.py:483] Algo bellman_ford step 3539 current loss 0.065233, current_train_items 113280.
I0304 19:29:53.074303 22579586809984 run.py:483] Algo bellman_ford step 3540 current loss 0.004466, current_train_items 113312.
I0304 19:29:53.090400 22579586809984 run.py:483] Algo bellman_ford step 3541 current loss 0.021356, current_train_items 113344.
I0304 19:29:53.114978 22579586809984 run.py:483] Algo bellman_ford step 3542 current loss 0.098032, current_train_items 113376.
I0304 19:29:53.145547 22579586809984 run.py:483] Algo bellman_ford step 3543 current loss 0.055125, current_train_items 113408.
I0304 19:29:53.178942 22579586809984 run.py:483] Algo bellman_ford step 3544 current loss 0.068119, current_train_items 113440.
I0304 19:29:53.198427 22579586809984 run.py:483] Algo bellman_ford step 3545 current loss 0.003871, current_train_items 113472.
I0304 19:29:53.214494 22579586809984 run.py:483] Algo bellman_ford step 3546 current loss 0.019384, current_train_items 113504.
I0304 19:29:53.238452 22579586809984 run.py:483] Algo bellman_ford step 3547 current loss 0.092089, current_train_items 113536.
I0304 19:29:53.268675 22579586809984 run.py:483] Algo bellman_ford step 3548 current loss 0.065977, current_train_items 113568.
I0304 19:29:53.302381 22579586809984 run.py:483] Algo bellman_ford step 3549 current loss 0.085894, current_train_items 113600.
I0304 19:29:53.322010 22579586809984 run.py:483] Algo bellman_ford step 3550 current loss 0.009010, current_train_items 113632.
I0304 19:29:53.330080 22579586809984 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0304 19:29:53.330184 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:29:53.347447 22579586809984 run.py:483] Algo bellman_ford step 3551 current loss 0.035330, current_train_items 113664.
I0304 19:29:53.373301 22579586809984 run.py:483] Algo bellman_ford step 3552 current loss 0.095734, current_train_items 113696.
I0304 19:29:53.405120 22579586809984 run.py:483] Algo bellman_ford step 3553 current loss 0.039849, current_train_items 113728.
I0304 19:29:53.439371 22579586809984 run.py:483] Algo bellman_ford step 3554 current loss 0.085476, current_train_items 113760.
I0304 19:29:53.459338 22579586809984 run.py:483] Algo bellman_ford step 3555 current loss 0.011699, current_train_items 113792.
I0304 19:29:53.475549 22579586809984 run.py:483] Algo bellman_ford step 3556 current loss 0.013530, current_train_items 113824.
I0304 19:29:53.499963 22579586809984 run.py:483] Algo bellman_ford step 3557 current loss 0.048450, current_train_items 113856.
I0304 19:29:53.532291 22579586809984 run.py:483] Algo bellman_ford step 3558 current loss 0.063676, current_train_items 113888.
I0304 19:29:53.565526 22579586809984 run.py:483] Algo bellman_ford step 3559 current loss 0.076283, current_train_items 113920.
I0304 19:29:53.585625 22579586809984 run.py:483] Algo bellman_ford step 3560 current loss 0.003904, current_train_items 113952.
I0304 19:29:53.602128 22579586809984 run.py:483] Algo bellman_ford step 3561 current loss 0.020033, current_train_items 113984.
I0304 19:29:53.627228 22579586809984 run.py:483] Algo bellman_ford step 3562 current loss 0.070069, current_train_items 114016.
I0304 19:29:53.657865 22579586809984 run.py:483] Algo bellman_ford step 3563 current loss 0.065921, current_train_items 114048.
I0304 19:29:53.694564 22579586809984 run.py:483] Algo bellman_ford step 3564 current loss 0.114627, current_train_items 114080.
I0304 19:29:53.714639 22579586809984 run.py:483] Algo bellman_ford step 3565 current loss 0.004303, current_train_items 114112.
I0304 19:29:53.731199 22579586809984 run.py:483] Algo bellman_ford step 3566 current loss 0.015578, current_train_items 114144.
I0304 19:29:53.755403 22579586809984 run.py:483] Algo bellman_ford step 3567 current loss 0.062535, current_train_items 114176.
I0304 19:29:53.786533 22579586809984 run.py:483] Algo bellman_ford step 3568 current loss 0.075836, current_train_items 114208.
I0304 19:29:53.821004 22579586809984 run.py:483] Algo bellman_ford step 3569 current loss 0.094473, current_train_items 114240.
I0304 19:29:53.840837 22579586809984 run.py:483] Algo bellman_ford step 3570 current loss 0.003043, current_train_items 114272.
I0304 19:29:53.856852 22579586809984 run.py:483] Algo bellman_ford step 3571 current loss 0.013012, current_train_items 114304.
I0304 19:29:53.880540 22579586809984 run.py:483] Algo bellman_ford step 3572 current loss 0.079009, current_train_items 114336.
I0304 19:29:53.911006 22579586809984 run.py:483] Algo bellman_ford step 3573 current loss 0.059811, current_train_items 114368.
I0304 19:29:53.946699 22579586809984 run.py:483] Algo bellman_ford step 3574 current loss 0.110838, current_train_items 114400.
I0304 19:29:53.966819 22579586809984 run.py:483] Algo bellman_ford step 3575 current loss 0.004699, current_train_items 114432.
I0304 19:29:53.983290 22579586809984 run.py:483] Algo bellman_ford step 3576 current loss 0.017668, current_train_items 114464.
I0304 19:29:54.006871 22579586809984 run.py:483] Algo bellman_ford step 3577 current loss 0.069897, current_train_items 114496.
I0304 19:29:54.038146 22579586809984 run.py:483] Algo bellman_ford step 3578 current loss 0.063087, current_train_items 114528.
I0304 19:29:54.071120 22579586809984 run.py:483] Algo bellman_ford step 3579 current loss 0.083097, current_train_items 114560.
I0304 19:29:54.091163 22579586809984 run.py:483] Algo bellman_ford step 3580 current loss 0.004264, current_train_items 114592.
I0304 19:29:54.107534 22579586809984 run.py:483] Algo bellman_ford step 3581 current loss 0.108139, current_train_items 114624.
I0304 19:29:54.132881 22579586809984 run.py:483] Algo bellman_ford step 3582 current loss 0.065449, current_train_items 114656.
I0304 19:29:54.164859 22579586809984 run.py:483] Algo bellman_ford step 3583 current loss 0.041334, current_train_items 114688.
I0304 19:29:54.199025 22579586809984 run.py:483] Algo bellman_ford step 3584 current loss 0.080295, current_train_items 114720.
I0304 19:29:54.218944 22579586809984 run.py:483] Algo bellman_ford step 3585 current loss 0.003644, current_train_items 114752.
I0304 19:29:54.235385 22579586809984 run.py:483] Algo bellman_ford step 3586 current loss 0.057263, current_train_items 114784.
I0304 19:29:54.258747 22579586809984 run.py:483] Algo bellman_ford step 3587 current loss 0.053097, current_train_items 114816.
I0304 19:29:54.290771 22579586809984 run.py:483] Algo bellman_ford step 3588 current loss 0.054220, current_train_items 114848.
I0304 19:29:54.325771 22579586809984 run.py:483] Algo bellman_ford step 3589 current loss 0.082286, current_train_items 114880.
I0304 19:29:54.345589 22579586809984 run.py:483] Algo bellman_ford step 3590 current loss 0.003983, current_train_items 114912.
I0304 19:29:54.361716 22579586809984 run.py:483] Algo bellman_ford step 3591 current loss 0.017990, current_train_items 114944.
I0304 19:29:54.387075 22579586809984 run.py:483] Algo bellman_ford step 3592 current loss 0.039316, current_train_items 114976.
I0304 19:29:54.419640 22579586809984 run.py:483] Algo bellman_ford step 3593 current loss 0.083429, current_train_items 115008.
I0304 19:29:54.454418 22579586809984 run.py:483] Algo bellman_ford step 3594 current loss 0.066120, current_train_items 115040.
I0304 19:29:54.474021 22579586809984 run.py:483] Algo bellman_ford step 3595 current loss 0.004525, current_train_items 115072.
I0304 19:29:54.490618 22579586809984 run.py:483] Algo bellman_ford step 3596 current loss 0.046935, current_train_items 115104.
I0304 19:29:54.514381 22579586809984 run.py:483] Algo bellman_ford step 3597 current loss 0.047207, current_train_items 115136.
I0304 19:29:54.544772 22579586809984 run.py:483] Algo bellman_ford step 3598 current loss 0.066642, current_train_items 115168.
I0304 19:29:54.579248 22579586809984 run.py:483] Algo bellman_ford step 3599 current loss 0.071635, current_train_items 115200.
I0304 19:29:54.599380 22579586809984 run.py:483] Algo bellman_ford step 3600 current loss 0.002821, current_train_items 115232.
I0304 19:29:54.606843 22579586809984 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0304 19:29:54.606975 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:29:54.624264 22579586809984 run.py:483] Algo bellman_ford step 3601 current loss 0.012956, current_train_items 115264.
I0304 19:29:54.649110 22579586809984 run.py:483] Algo bellman_ford step 3602 current loss 0.031514, current_train_items 115296.
I0304 19:29:54.680494 22579586809984 run.py:483] Algo bellman_ford step 3603 current loss 0.071111, current_train_items 115328.
I0304 19:29:54.716907 22579586809984 run.py:483] Algo bellman_ford step 3604 current loss 0.111852, current_train_items 115360.
I0304 19:29:54.736835 22579586809984 run.py:483] Algo bellman_ford step 3605 current loss 0.003394, current_train_items 115392.
I0304 19:29:54.753484 22579586809984 run.py:483] Algo bellman_ford step 3606 current loss 0.025185, current_train_items 115424.
I0304 19:29:54.776362 22579586809984 run.py:483] Algo bellman_ford step 3607 current loss 0.023600, current_train_items 115456.
I0304 19:29:54.806610 22579586809984 run.py:483] Algo bellman_ford step 3608 current loss 0.043228, current_train_items 115488.
I0304 19:29:54.838708 22579586809984 run.py:483] Algo bellman_ford step 3609 current loss 0.075857, current_train_items 115520.
I0304 19:29:54.858369 22579586809984 run.py:483] Algo bellman_ford step 3610 current loss 0.004845, current_train_items 115552.
I0304 19:29:54.874969 22579586809984 run.py:483] Algo bellman_ford step 3611 current loss 0.029516, current_train_items 115584.
I0304 19:29:54.899945 22579586809984 run.py:483] Algo bellman_ford step 3612 current loss 0.064297, current_train_items 115616.
I0304 19:29:54.930606 22579586809984 run.py:483] Algo bellman_ford step 3613 current loss 0.058355, current_train_items 115648.
I0304 19:29:54.962836 22579586809984 run.py:483] Algo bellman_ford step 3614 current loss 0.077507, current_train_items 115680.
I0304 19:29:54.982310 22579586809984 run.py:483] Algo bellman_ford step 3615 current loss 0.047183, current_train_items 115712.
I0304 19:29:54.998907 22579586809984 run.py:483] Algo bellman_ford step 3616 current loss 0.041796, current_train_items 115744.
I0304 19:29:55.022318 22579586809984 run.py:483] Algo bellman_ford step 3617 current loss 0.049656, current_train_items 115776.
I0304 19:29:55.054201 22579586809984 run.py:483] Algo bellman_ford step 3618 current loss 0.075614, current_train_items 115808.
I0304 19:29:55.084758 22579586809984 run.py:483] Algo bellman_ford step 3619 current loss 0.064025, current_train_items 115840.
I0304 19:29:55.104387 22579586809984 run.py:483] Algo bellman_ford step 3620 current loss 0.002954, current_train_items 115872.
I0304 19:29:55.120734 22579586809984 run.py:483] Algo bellman_ford step 3621 current loss 0.040386, current_train_items 115904.
I0304 19:29:55.144685 22579586809984 run.py:483] Algo bellman_ford step 3622 current loss 0.060101, current_train_items 115936.
I0304 19:29:55.176976 22579586809984 run.py:483] Algo bellman_ford step 3623 current loss 0.115399, current_train_items 115968.
I0304 19:29:55.210098 22579586809984 run.py:483] Algo bellman_ford step 3624 current loss 0.085701, current_train_items 116000.
I0304 19:29:55.229602 22579586809984 run.py:483] Algo bellman_ford step 3625 current loss 0.004241, current_train_items 116032.
I0304 19:29:55.246096 22579586809984 run.py:483] Algo bellman_ford step 3626 current loss 0.051849, current_train_items 116064.
I0304 19:29:55.270476 22579586809984 run.py:483] Algo bellman_ford step 3627 current loss 0.046186, current_train_items 116096.
I0304 19:29:55.302540 22579586809984 run.py:483] Algo bellman_ford step 3628 current loss 0.137598, current_train_items 116128.
I0304 19:29:55.335257 22579586809984 run.py:483] Algo bellman_ford step 3629 current loss 0.066698, current_train_items 116160.
I0304 19:29:55.355005 22579586809984 run.py:483] Algo bellman_ford step 3630 current loss 0.031963, current_train_items 116192.
I0304 19:29:55.371312 22579586809984 run.py:483] Algo bellman_ford step 3631 current loss 0.037041, current_train_items 116224.
I0304 19:29:55.395518 22579586809984 run.py:483] Algo bellman_ford step 3632 current loss 0.027569, current_train_items 116256.
I0304 19:29:55.426774 22579586809984 run.py:483] Algo bellman_ford step 3633 current loss 0.069216, current_train_items 116288.
I0304 19:29:55.459887 22579586809984 run.py:483] Algo bellman_ford step 3634 current loss 0.174636, current_train_items 116320.
I0304 19:29:55.479287 22579586809984 run.py:483] Algo bellman_ford step 3635 current loss 0.004585, current_train_items 116352.
I0304 19:29:55.495540 22579586809984 run.py:483] Algo bellman_ford step 3636 current loss 0.066302, current_train_items 116384.
I0304 19:29:55.518399 22579586809984 run.py:483] Algo bellman_ford step 3637 current loss 0.130665, current_train_items 116416.
I0304 19:29:55.550434 22579586809984 run.py:483] Algo bellman_ford step 3638 current loss 0.091097, current_train_items 116448.
I0304 19:29:55.583233 22579586809984 run.py:483] Algo bellman_ford step 3639 current loss 0.090227, current_train_items 116480.
I0304 19:29:55.602655 22579586809984 run.py:483] Algo bellman_ford step 3640 current loss 0.009299, current_train_items 116512.
I0304 19:29:55.618544 22579586809984 run.py:483] Algo bellman_ford step 3641 current loss 0.029085, current_train_items 116544.
I0304 19:29:55.642979 22579586809984 run.py:483] Algo bellman_ford step 3642 current loss 0.126272, current_train_items 116576.
I0304 19:29:55.677450 22579586809984 run.py:483] Algo bellman_ford step 3643 current loss 0.093731, current_train_items 116608.
I0304 19:29:55.711656 22579586809984 run.py:483] Algo bellman_ford step 3644 current loss 0.113822, current_train_items 116640.
I0304 19:29:55.730969 22579586809984 run.py:483] Algo bellman_ford step 3645 current loss 0.004902, current_train_items 116672.
I0304 19:29:55.747290 22579586809984 run.py:483] Algo bellman_ford step 3646 current loss 0.019001, current_train_items 116704.
I0304 19:29:55.772636 22579586809984 run.py:483] Algo bellman_ford step 3647 current loss 0.083935, current_train_items 116736.
I0304 19:29:55.803491 22579586809984 run.py:483] Algo bellman_ford step 3648 current loss 0.057684, current_train_items 116768.
I0304 19:29:55.837364 22579586809984 run.py:483] Algo bellman_ford step 3649 current loss 0.137277, current_train_items 116800.
I0304 19:29:55.856648 22579586809984 run.py:483] Algo bellman_ford step 3650 current loss 0.006592, current_train_items 116832.
I0304 19:29:55.864661 22579586809984 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0304 19:29:55.864774 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:29:55.881868 22579586809984 run.py:483] Algo bellman_ford step 3651 current loss 0.018832, current_train_items 116864.
I0304 19:29:55.905091 22579586809984 run.py:483] Algo bellman_ford step 3652 current loss 0.039191, current_train_items 116896.
I0304 19:29:55.936097 22579586809984 run.py:483] Algo bellman_ford step 3653 current loss 0.052447, current_train_items 116928.
I0304 19:29:55.969413 22579586809984 run.py:483] Algo bellman_ford step 3654 current loss 0.121966, current_train_items 116960.
I0304 19:29:55.989156 22579586809984 run.py:483] Algo bellman_ford step 3655 current loss 0.004114, current_train_items 116992.
I0304 19:29:56.004794 22579586809984 run.py:483] Algo bellman_ford step 3656 current loss 0.026395, current_train_items 117024.
I0304 19:29:56.029042 22579586809984 run.py:483] Algo bellman_ford step 3657 current loss 0.055425, current_train_items 117056.
I0304 19:29:56.059690 22579586809984 run.py:483] Algo bellman_ford step 3658 current loss 0.060980, current_train_items 117088.
I0304 19:29:56.093255 22579586809984 run.py:483] Algo bellman_ford step 3659 current loss 0.085577, current_train_items 117120.
I0304 19:29:56.113276 22579586809984 run.py:483] Algo bellman_ford step 3660 current loss 0.005291, current_train_items 117152.
I0304 19:29:56.129920 22579586809984 run.py:483] Algo bellman_ford step 3661 current loss 0.026201, current_train_items 117184.
I0304 19:29:56.153187 22579586809984 run.py:483] Algo bellman_ford step 3662 current loss 0.036305, current_train_items 117216.
I0304 19:29:56.185252 22579586809984 run.py:483] Algo bellman_ford step 3663 current loss 0.065601, current_train_items 117248.
I0304 19:29:56.219900 22579586809984 run.py:483] Algo bellman_ford step 3664 current loss 0.082167, current_train_items 117280.
I0304 19:29:56.239431 22579586809984 run.py:483] Algo bellman_ford step 3665 current loss 0.038743, current_train_items 117312.
I0304 19:29:56.255541 22579586809984 run.py:483] Algo bellman_ford step 3666 current loss 0.050047, current_train_items 117344.
I0304 19:29:56.279729 22579586809984 run.py:483] Algo bellman_ford step 3667 current loss 0.056008, current_train_items 117376.
I0304 19:29:56.311222 22579586809984 run.py:483] Algo bellman_ford step 3668 current loss 0.059713, current_train_items 117408.
I0304 19:29:56.346410 22579586809984 run.py:483] Algo bellman_ford step 3669 current loss 0.063584, current_train_items 117440.
I0304 19:29:56.366278 22579586809984 run.py:483] Algo bellman_ford step 3670 current loss 0.005398, current_train_items 117472.
I0304 19:29:56.382824 22579586809984 run.py:483] Algo bellman_ford step 3671 current loss 0.151956, current_train_items 117504.
I0304 19:29:56.405975 22579586809984 run.py:483] Algo bellman_ford step 3672 current loss 0.096453, current_train_items 117536.
I0304 19:29:56.437457 22579586809984 run.py:483] Algo bellman_ford step 3673 current loss 0.080450, current_train_items 117568.
I0304 19:29:56.472105 22579586809984 run.py:483] Algo bellman_ford step 3674 current loss 0.094505, current_train_items 117600.
I0304 19:29:56.491755 22579586809984 run.py:483] Algo bellman_ford step 3675 current loss 0.005046, current_train_items 117632.
I0304 19:29:56.507748 22579586809984 run.py:483] Algo bellman_ford step 3676 current loss 0.017426, current_train_items 117664.
I0304 19:29:56.531223 22579586809984 run.py:483] Algo bellman_ford step 3677 current loss 0.074388, current_train_items 117696.
I0304 19:29:56.562525 22579586809984 run.py:483] Algo bellman_ford step 3678 current loss 0.127138, current_train_items 117728.
I0304 19:29:56.596101 22579586809984 run.py:483] Algo bellman_ford step 3679 current loss 0.129983, current_train_items 117760.
I0304 19:29:56.615451 22579586809984 run.py:483] Algo bellman_ford step 3680 current loss 0.005143, current_train_items 117792.
I0304 19:29:56.631927 22579586809984 run.py:483] Algo bellman_ford step 3681 current loss 0.014447, current_train_items 117824.
I0304 19:29:56.656363 22579586809984 run.py:483] Algo bellman_ford step 3682 current loss 0.081498, current_train_items 117856.
I0304 19:29:56.686839 22579586809984 run.py:483] Algo bellman_ford step 3683 current loss 0.123334, current_train_items 117888.
I0304 19:29:56.720758 22579586809984 run.py:483] Algo bellman_ford step 3684 current loss 0.146708, current_train_items 117920.
I0304 19:29:56.740474 22579586809984 run.py:483] Algo bellman_ford step 3685 current loss 0.004864, current_train_items 117952.
I0304 19:29:56.757014 22579586809984 run.py:483] Algo bellman_ford step 3686 current loss 0.026195, current_train_items 117984.
I0304 19:29:56.780357 22579586809984 run.py:483] Algo bellman_ford step 3687 current loss 0.055714, current_train_items 118016.
I0304 19:29:56.812840 22579586809984 run.py:483] Algo bellman_ford step 3688 current loss 0.087062, current_train_items 118048.
I0304 19:29:56.848097 22579586809984 run.py:483] Algo bellman_ford step 3689 current loss 0.066019, current_train_items 118080.
I0304 19:29:56.868323 22579586809984 run.py:483] Algo bellman_ford step 3690 current loss 0.009850, current_train_items 118112.
I0304 19:29:56.884643 22579586809984 run.py:483] Algo bellman_ford step 3691 current loss 0.015310, current_train_items 118144.
I0304 19:29:56.906847 22579586809984 run.py:483] Algo bellman_ford step 3692 current loss 0.058412, current_train_items 118176.
I0304 19:29:56.939369 22579586809984 run.py:483] Algo bellman_ford step 3693 current loss 0.075008, current_train_items 118208.
I0304 19:29:56.972373 22579586809984 run.py:483] Algo bellman_ford step 3694 current loss 0.084418, current_train_items 118240.
I0304 19:29:56.992150 22579586809984 run.py:483] Algo bellman_ford step 3695 current loss 0.010759, current_train_items 118272.
I0304 19:29:57.009089 22579586809984 run.py:483] Algo bellman_ford step 3696 current loss 0.034557, current_train_items 118304.
I0304 19:29:57.033159 22579586809984 run.py:483] Algo bellman_ford step 3697 current loss 0.034654, current_train_items 118336.
I0304 19:29:57.065111 22579586809984 run.py:483] Algo bellman_ford step 3698 current loss 0.099563, current_train_items 118368.
I0304 19:29:57.097117 22579586809984 run.py:483] Algo bellman_ford step 3699 current loss 0.091045, current_train_items 118400.
I0304 19:29:57.117337 22579586809984 run.py:483] Algo bellman_ford step 3700 current loss 0.003455, current_train_items 118432.
I0304 19:29:57.125385 22579586809984 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0304 19:29:57.125490 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:29:57.142515 22579586809984 run.py:483] Algo bellman_ford step 3701 current loss 0.018177, current_train_items 118464.
I0304 19:29:57.166888 22579586809984 run.py:483] Algo bellman_ford step 3702 current loss 0.116868, current_train_items 118496.
I0304 19:29:57.199922 22579586809984 run.py:483] Algo bellman_ford step 3703 current loss 0.098008, current_train_items 118528.
I0304 19:29:57.233501 22579586809984 run.py:483] Algo bellman_ford step 3704 current loss 0.074056, current_train_items 118560.
I0304 19:29:57.253477 22579586809984 run.py:483] Algo bellman_ford step 3705 current loss 0.003047, current_train_items 118592.
I0304 19:29:57.269769 22579586809984 run.py:483] Algo bellman_ford step 3706 current loss 0.044947, current_train_items 118624.
I0304 19:29:57.293878 22579586809984 run.py:483] Algo bellman_ford step 3707 current loss 0.111395, current_train_items 118656.
I0304 19:29:57.324182 22579586809984 run.py:483] Algo bellman_ford step 3708 current loss 0.048479, current_train_items 118688.
I0304 19:29:57.357883 22579586809984 run.py:483] Algo bellman_ford step 3709 current loss 0.093312, current_train_items 118720.
I0304 19:29:57.377700 22579586809984 run.py:483] Algo bellman_ford step 3710 current loss 0.012789, current_train_items 118752.
I0304 19:29:57.394351 22579586809984 run.py:483] Algo bellman_ford step 3711 current loss 0.054952, current_train_items 118784.
I0304 19:29:57.418246 22579586809984 run.py:483] Algo bellman_ford step 3712 current loss 0.075996, current_train_items 118816.
I0304 19:29:57.450725 22579586809984 run.py:483] Algo bellman_ford step 3713 current loss 0.084392, current_train_items 118848.
I0304 19:29:57.485188 22579586809984 run.py:483] Algo bellman_ford step 3714 current loss 0.065751, current_train_items 118880.
I0304 19:29:57.504840 22579586809984 run.py:483] Algo bellman_ford step 3715 current loss 0.005177, current_train_items 118912.
I0304 19:29:57.520998 22579586809984 run.py:483] Algo bellman_ford step 3716 current loss 0.010022, current_train_items 118944.
I0304 19:29:57.544782 22579586809984 run.py:483] Algo bellman_ford step 3717 current loss 0.089292, current_train_items 118976.
I0304 19:29:57.576156 22579586809984 run.py:483] Algo bellman_ford step 3718 current loss 0.066318, current_train_items 119008.
I0304 19:29:57.608533 22579586809984 run.py:483] Algo bellman_ford step 3719 current loss 0.053464, current_train_items 119040.
I0304 19:29:57.628264 22579586809984 run.py:483] Algo bellman_ford step 3720 current loss 0.005928, current_train_items 119072.
I0304 19:29:57.644735 22579586809984 run.py:483] Algo bellman_ford step 3721 current loss 0.048937, current_train_items 119104.
I0304 19:29:57.668548 22579586809984 run.py:483] Algo bellman_ford step 3722 current loss 0.021948, current_train_items 119136.
I0304 19:29:57.699384 22579586809984 run.py:483] Algo bellman_ford step 3723 current loss 0.038935, current_train_items 119168.
I0304 19:29:57.732641 22579586809984 run.py:483] Algo bellman_ford step 3724 current loss 0.089209, current_train_items 119200.
I0304 19:29:57.751924 22579586809984 run.py:483] Algo bellman_ford step 3725 current loss 0.003107, current_train_items 119232.
I0304 19:29:57.768667 22579586809984 run.py:483] Algo bellman_ford step 3726 current loss 0.013994, current_train_items 119264.
I0304 19:29:57.793190 22579586809984 run.py:483] Algo bellman_ford step 3727 current loss 0.072124, current_train_items 119296.
I0304 19:29:57.824816 22579586809984 run.py:483] Algo bellman_ford step 3728 current loss 0.077572, current_train_items 119328.
I0304 19:29:57.858265 22579586809984 run.py:483] Algo bellman_ford step 3729 current loss 0.051545, current_train_items 119360.
I0304 19:29:57.877795 22579586809984 run.py:483] Algo bellman_ford step 3730 current loss 0.005535, current_train_items 119392.
I0304 19:29:57.893977 22579586809984 run.py:483] Algo bellman_ford step 3731 current loss 0.020965, current_train_items 119424.
I0304 19:29:57.918338 22579586809984 run.py:483] Algo bellman_ford step 3732 current loss 0.054521, current_train_items 119456.
I0304 19:29:57.948127 22579586809984 run.py:483] Algo bellman_ford step 3733 current loss 0.052047, current_train_items 119488.
I0304 19:29:57.982238 22579586809984 run.py:483] Algo bellman_ford step 3734 current loss 0.078968, current_train_items 119520.
I0304 19:29:58.001913 22579586809984 run.py:483] Algo bellman_ford step 3735 current loss 0.009009, current_train_items 119552.
I0304 19:29:58.018003 22579586809984 run.py:483] Algo bellman_ford step 3736 current loss 0.028934, current_train_items 119584.
I0304 19:29:58.041214 22579586809984 run.py:483] Algo bellman_ford step 3737 current loss 0.028029, current_train_items 119616.
I0304 19:29:58.073061 22579586809984 run.py:483] Algo bellman_ford step 3738 current loss 0.056158, current_train_items 119648.
I0304 19:29:58.106234 22579586809984 run.py:483] Algo bellman_ford step 3739 current loss 0.066836, current_train_items 119680.
I0304 19:29:58.126054 22579586809984 run.py:483] Algo bellman_ford step 3740 current loss 0.023371, current_train_items 119712.
I0304 19:29:58.142172 22579586809984 run.py:483] Algo bellman_ford step 3741 current loss 0.024061, current_train_items 119744.
I0304 19:29:58.165603 22579586809984 run.py:483] Algo bellman_ford step 3742 current loss 0.057942, current_train_items 119776.
I0304 19:29:58.196923 22579586809984 run.py:483] Algo bellman_ford step 3743 current loss 0.067775, current_train_items 119808.
I0304 19:29:58.228332 22579586809984 run.py:483] Algo bellman_ford step 3744 current loss 0.060895, current_train_items 119840.
I0304 19:29:58.247933 22579586809984 run.py:483] Algo bellman_ford step 3745 current loss 0.016251, current_train_items 119872.
I0304 19:29:58.263903 22579586809984 run.py:483] Algo bellman_ford step 3746 current loss 0.019947, current_train_items 119904.
I0304 19:29:58.288076 22579586809984 run.py:483] Algo bellman_ford step 3747 current loss 0.050081, current_train_items 119936.
I0304 19:29:58.319822 22579586809984 run.py:483] Algo bellman_ford step 3748 current loss 0.079550, current_train_items 119968.
I0304 19:29:58.354764 22579586809984 run.py:483] Algo bellman_ford step 3749 current loss 0.111411, current_train_items 120000.
I0304 19:29:58.374459 22579586809984 run.py:483] Algo bellman_ford step 3750 current loss 0.006599, current_train_items 120032.
I0304 19:29:58.382421 22579586809984 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0304 19:29:58.382549 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:29:58.399705 22579586809984 run.py:483] Algo bellman_ford step 3751 current loss 0.030849, current_train_items 120064.
I0304 19:29:58.424017 22579586809984 run.py:483] Algo bellman_ford step 3752 current loss 0.076628, current_train_items 120096.
I0304 19:29:58.455455 22579586809984 run.py:483] Algo bellman_ford step 3753 current loss 0.073324, current_train_items 120128.
I0304 19:29:58.491267 22579586809984 run.py:483] Algo bellman_ford step 3754 current loss 0.072379, current_train_items 120160.
I0304 19:29:58.510934 22579586809984 run.py:483] Algo bellman_ford step 3755 current loss 0.013610, current_train_items 120192.
I0304 19:29:58.526981 22579586809984 run.py:483] Algo bellman_ford step 3756 current loss 0.033519, current_train_items 120224.
I0304 19:29:58.551188 22579586809984 run.py:483] Algo bellman_ford step 3757 current loss 0.056897, current_train_items 120256.
I0304 19:29:58.582432 22579586809984 run.py:483] Algo bellman_ford step 3758 current loss 0.089471, current_train_items 120288.
I0304 19:29:58.614235 22579586809984 run.py:483] Algo bellman_ford step 3759 current loss 0.062901, current_train_items 120320.
I0304 19:29:58.633965 22579586809984 run.py:483] Algo bellman_ford step 3760 current loss 0.004727, current_train_items 120352.
I0304 19:29:58.650377 22579586809984 run.py:483] Algo bellman_ford step 3761 current loss 0.009018, current_train_items 120384.
I0304 19:29:58.673420 22579586809984 run.py:483] Algo bellman_ford step 3762 current loss 0.043430, current_train_items 120416.
I0304 19:29:58.704275 22579586809984 run.py:483] Algo bellman_ford step 3763 current loss 0.068465, current_train_items 120448.
I0304 19:29:58.738070 22579586809984 run.py:483] Algo bellman_ford step 3764 current loss 0.081458, current_train_items 120480.
I0304 19:29:58.757524 22579586809984 run.py:483] Algo bellman_ford step 3765 current loss 0.004835, current_train_items 120512.
I0304 19:29:58.774208 22579586809984 run.py:483] Algo bellman_ford step 3766 current loss 0.025463, current_train_items 120544.
I0304 19:29:58.798433 22579586809984 run.py:483] Algo bellman_ford step 3767 current loss 0.072072, current_train_items 120576.
I0304 19:29:58.830587 22579586809984 run.py:483] Algo bellman_ford step 3768 current loss 0.082829, current_train_items 120608.
I0304 19:29:58.863124 22579586809984 run.py:483] Algo bellman_ford step 3769 current loss 0.057317, current_train_items 120640.
I0304 19:29:58.883136 22579586809984 run.py:483] Algo bellman_ford step 3770 current loss 0.003813, current_train_items 120672.
I0304 19:29:58.899630 22579586809984 run.py:483] Algo bellman_ford step 3771 current loss 0.037263, current_train_items 120704.
I0304 19:29:58.922423 22579586809984 run.py:483] Algo bellman_ford step 3772 current loss 0.054869, current_train_items 120736.
I0304 19:29:58.952772 22579586809984 run.py:483] Algo bellman_ford step 3773 current loss 0.055965, current_train_items 120768.
I0304 19:29:58.983342 22579586809984 run.py:483] Algo bellman_ford step 3774 current loss 0.050563, current_train_items 120800.
I0304 19:29:59.003250 22579586809984 run.py:483] Algo bellman_ford step 3775 current loss 0.003614, current_train_items 120832.
I0304 19:29:59.020117 22579586809984 run.py:483] Algo bellman_ford step 3776 current loss 0.023879, current_train_items 120864.
I0304 19:29:59.043216 22579586809984 run.py:483] Algo bellman_ford step 3777 current loss 0.048524, current_train_items 120896.
I0304 19:29:59.075054 22579586809984 run.py:483] Algo bellman_ford step 3778 current loss 0.052334, current_train_items 120928.
I0304 19:29:59.108823 22579586809984 run.py:483] Algo bellman_ford step 3779 current loss 0.062055, current_train_items 120960.
I0304 19:29:59.128289 22579586809984 run.py:483] Algo bellman_ford step 3780 current loss 0.003781, current_train_items 120992.
I0304 19:29:59.144644 22579586809984 run.py:483] Algo bellman_ford step 3781 current loss 0.033799, current_train_items 121024.
I0304 19:29:59.168214 22579586809984 run.py:483] Algo bellman_ford step 3782 current loss 0.049619, current_train_items 121056.
I0304 19:29:59.197849 22579586809984 run.py:483] Algo bellman_ford step 3783 current loss 0.037093, current_train_items 121088.
I0304 19:29:59.234122 22579586809984 run.py:483] Algo bellman_ford step 3784 current loss 0.106814, current_train_items 121120.
I0304 19:29:59.253968 22579586809984 run.py:483] Algo bellman_ford step 3785 current loss 0.006258, current_train_items 121152.
I0304 19:29:59.270179 22579586809984 run.py:483] Algo bellman_ford step 3786 current loss 0.015435, current_train_items 121184.
I0304 19:29:59.294218 22579586809984 run.py:483] Algo bellman_ford step 3787 current loss 0.052443, current_train_items 121216.
I0304 19:29:59.325179 22579586809984 run.py:483] Algo bellman_ford step 3788 current loss 0.039065, current_train_items 121248.
I0304 19:29:59.358880 22579586809984 run.py:483] Algo bellman_ford step 3789 current loss 0.070589, current_train_items 121280.
I0304 19:29:59.378843 22579586809984 run.py:483] Algo bellman_ford step 3790 current loss 0.004693, current_train_items 121312.
I0304 19:29:59.395216 22579586809984 run.py:483] Algo bellman_ford step 3791 current loss 0.017356, current_train_items 121344.
I0304 19:29:59.418330 22579586809984 run.py:483] Algo bellman_ford step 3792 current loss 0.045502, current_train_items 121376.
I0304 19:29:59.450528 22579586809984 run.py:483] Algo bellman_ford step 3793 current loss 0.080620, current_train_items 121408.
I0304 19:29:59.483756 22579586809984 run.py:483] Algo bellman_ford step 3794 current loss 0.040589, current_train_items 121440.
I0304 19:29:59.503250 22579586809984 run.py:483] Algo bellman_ford step 3795 current loss 0.018840, current_train_items 121472.
I0304 19:29:59.519804 22579586809984 run.py:483] Algo bellman_ford step 3796 current loss 0.030597, current_train_items 121504.
I0304 19:29:59.543973 22579586809984 run.py:483] Algo bellman_ford step 3797 current loss 0.052426, current_train_items 121536.
I0304 19:29:59.575972 22579586809984 run.py:483] Algo bellman_ford step 3798 current loss 0.051536, current_train_items 121568.
I0304 19:29:59.608598 22579586809984 run.py:483] Algo bellman_ford step 3799 current loss 0.067585, current_train_items 121600.
I0304 19:29:59.628512 22579586809984 run.py:483] Algo bellman_ford step 3800 current loss 0.004545, current_train_items 121632.
I0304 19:29:59.636139 22579586809984 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0304 19:29:59.636244 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:29:59.653123 22579586809984 run.py:483] Algo bellman_ford step 3801 current loss 0.019867, current_train_items 121664.
I0304 19:29:59.678226 22579586809984 run.py:483] Algo bellman_ford step 3802 current loss 0.023008, current_train_items 121696.
I0304 19:29:59.709640 22579586809984 run.py:483] Algo bellman_ford step 3803 current loss 0.059487, current_train_items 121728.
I0304 19:29:59.741453 22579586809984 run.py:483] Algo bellman_ford step 3804 current loss 0.057662, current_train_items 121760.
I0304 19:29:59.761372 22579586809984 run.py:483] Algo bellman_ford step 3805 current loss 0.004695, current_train_items 121792.
I0304 19:29:59.777489 22579586809984 run.py:483] Algo bellman_ford step 3806 current loss 0.052581, current_train_items 121824.
I0304 19:29:59.801673 22579586809984 run.py:483] Algo bellman_ford step 3807 current loss 0.054092, current_train_items 121856.
I0304 19:29:59.833132 22579586809984 run.py:483] Algo bellman_ford step 3808 current loss 0.139577, current_train_items 121888.
I0304 19:29:59.868205 22579586809984 run.py:483] Algo bellman_ford step 3809 current loss 0.102327, current_train_items 121920.
I0304 19:29:59.887636 22579586809984 run.py:483] Algo bellman_ford step 3810 current loss 0.019785, current_train_items 121952.
I0304 19:29:59.904299 22579586809984 run.py:483] Algo bellman_ford step 3811 current loss 0.007425, current_train_items 121984.
I0304 19:29:59.928304 22579586809984 run.py:483] Algo bellman_ford step 3812 current loss 0.061728, current_train_items 122016.
I0304 19:29:59.961541 22579586809984 run.py:483] Algo bellman_ford step 3813 current loss 0.072081, current_train_items 122048.
I0304 19:29:59.996843 22579586809984 run.py:483] Algo bellman_ford step 3814 current loss 0.155717, current_train_items 122080.
I0304 19:30:00.016786 22579586809984 run.py:483] Algo bellman_ford step 3815 current loss 0.016872, current_train_items 122112.
I0304 19:30:00.033400 22579586809984 run.py:483] Algo bellman_ford step 3816 current loss 0.059587, current_train_items 122144.
I0304 19:30:00.057355 22579586809984 run.py:483] Algo bellman_ford step 3817 current loss 0.068908, current_train_items 122176.
I0304 19:30:00.088590 22579586809984 run.py:483] Algo bellman_ford step 3818 current loss 0.041629, current_train_items 122208.
I0304 19:30:00.122326 22579586809984 run.py:483] Algo bellman_ford step 3819 current loss 0.133990, current_train_items 122240.
I0304 19:30:00.141948 22579586809984 run.py:483] Algo bellman_ford step 3820 current loss 0.003566, current_train_items 122272.
I0304 19:30:00.157872 22579586809984 run.py:483] Algo bellman_ford step 3821 current loss 0.016270, current_train_items 122304.
I0304 19:30:00.182122 22579586809984 run.py:483] Algo bellman_ford step 3822 current loss 0.053304, current_train_items 122336.
I0304 19:30:00.213616 22579586809984 run.py:483] Algo bellman_ford step 3823 current loss 0.077228, current_train_items 122368.
I0304 19:30:00.247837 22579586809984 run.py:483] Algo bellman_ford step 3824 current loss 0.099502, current_train_items 122400.
I0304 19:30:00.267488 22579586809984 run.py:483] Algo bellman_ford step 3825 current loss 0.021264, current_train_items 122432.
I0304 19:30:00.283409 22579586809984 run.py:483] Algo bellman_ford step 3826 current loss 0.022931, current_train_items 122464.
I0304 19:30:00.307765 22579586809984 run.py:483] Algo bellman_ford step 3827 current loss 0.041566, current_train_items 122496.
I0304 19:30:00.339318 22579586809984 run.py:483] Algo bellman_ford step 3828 current loss 0.066076, current_train_items 122528.
I0304 19:30:00.371736 22579586809984 run.py:483] Algo bellman_ford step 3829 current loss 0.065221, current_train_items 122560.
I0304 19:30:00.391337 22579586809984 run.py:483] Algo bellman_ford step 3830 current loss 0.002870, current_train_items 122592.
I0304 19:30:00.407705 22579586809984 run.py:483] Algo bellman_ford step 3831 current loss 0.043950, current_train_items 122624.
I0304 19:30:00.430665 22579586809984 run.py:483] Algo bellman_ford step 3832 current loss 0.051947, current_train_items 122656.
I0304 19:30:00.463045 22579586809984 run.py:483] Algo bellman_ford step 3833 current loss 0.072622, current_train_items 122688.
I0304 19:30:00.496022 22579586809984 run.py:483] Algo bellman_ford step 3834 current loss 0.075845, current_train_items 122720.
I0304 19:30:00.515560 22579586809984 run.py:483] Algo bellman_ford step 3835 current loss 0.007201, current_train_items 122752.
I0304 19:30:00.531306 22579586809984 run.py:483] Algo bellman_ford step 3836 current loss 0.006479, current_train_items 122784.
I0304 19:30:00.556276 22579586809984 run.py:483] Algo bellman_ford step 3837 current loss 0.148279, current_train_items 122816.
I0304 19:30:00.587837 22579586809984 run.py:483] Algo bellman_ford step 3838 current loss 0.156400, current_train_items 122848.
I0304 19:30:00.621068 22579586809984 run.py:483] Algo bellman_ford step 3839 current loss 0.086079, current_train_items 122880.
I0304 19:30:00.640502 22579586809984 run.py:483] Algo bellman_ford step 3840 current loss 0.016208, current_train_items 122912.
I0304 19:30:00.656433 22579586809984 run.py:483] Algo bellman_ford step 3841 current loss 0.021478, current_train_items 122944.
I0304 19:30:00.679431 22579586809984 run.py:483] Algo bellman_ford step 3842 current loss 0.040345, current_train_items 122976.
I0304 19:30:00.710756 22579586809984 run.py:483] Algo bellman_ford step 3843 current loss 0.080642, current_train_items 123008.
I0304 19:30:00.745417 22579586809984 run.py:483] Algo bellman_ford step 3844 current loss 0.284654, current_train_items 123040.
I0304 19:30:00.765400 22579586809984 run.py:483] Algo bellman_ford step 3845 current loss 0.006998, current_train_items 123072.
I0304 19:30:00.781510 22579586809984 run.py:483] Algo bellman_ford step 3846 current loss 0.041419, current_train_items 123104.
I0304 19:30:00.806744 22579586809984 run.py:483] Algo bellman_ford step 3847 current loss 0.116427, current_train_items 123136.
I0304 19:30:00.837601 22579586809984 run.py:483] Algo bellman_ford step 3848 current loss 0.052788, current_train_items 123168.
I0304 19:30:00.870902 22579586809984 run.py:483] Algo bellman_ford step 3849 current loss 0.052880, current_train_items 123200.
I0304 19:30:00.890875 22579586809984 run.py:483] Algo bellman_ford step 3850 current loss 0.004968, current_train_items 123232.
I0304 19:30:00.898776 22579586809984 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0304 19:30:00.898881 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:30:00.915325 22579586809984 run.py:483] Algo bellman_ford step 3851 current loss 0.012701, current_train_items 123264.
I0304 19:30:00.938644 22579586809984 run.py:483] Algo bellman_ford step 3852 current loss 0.031288, current_train_items 123296.
I0304 19:30:00.970959 22579586809984 run.py:483] Algo bellman_ford step 3853 current loss 0.102609, current_train_items 123328.
I0304 19:30:01.006287 22579586809984 run.py:483] Algo bellman_ford step 3854 current loss 0.102230, current_train_items 123360.
I0304 19:30:01.026091 22579586809984 run.py:483] Algo bellman_ford step 3855 current loss 0.009733, current_train_items 123392.
I0304 19:30:01.041924 22579586809984 run.py:483] Algo bellman_ford step 3856 current loss 0.018894, current_train_items 123424.
I0304 19:30:01.064912 22579586809984 run.py:483] Algo bellman_ford step 3857 current loss 0.032268, current_train_items 123456.
I0304 19:30:01.096521 22579586809984 run.py:483] Algo bellman_ford step 3858 current loss 0.106895, current_train_items 123488.
I0304 19:30:01.132333 22579586809984 run.py:483] Algo bellman_ford step 3859 current loss 0.145085, current_train_items 123520.
I0304 19:30:01.152116 22579586809984 run.py:483] Algo bellman_ford step 3860 current loss 0.006060, current_train_items 123552.
I0304 19:30:01.168804 22579586809984 run.py:483] Algo bellman_ford step 3861 current loss 0.058610, current_train_items 123584.
I0304 19:30:01.192134 22579586809984 run.py:483] Algo bellman_ford step 3862 current loss 0.038816, current_train_items 123616.
I0304 19:30:01.223436 22579586809984 run.py:483] Algo bellman_ford step 3863 current loss 0.145920, current_train_items 123648.
I0304 19:30:01.259426 22579586809984 run.py:483] Algo bellman_ford step 3864 current loss 0.101530, current_train_items 123680.
I0304 19:30:01.278898 22579586809984 run.py:483] Algo bellman_ford step 3865 current loss 0.009079, current_train_items 123712.
I0304 19:30:01.294945 22579586809984 run.py:483] Algo bellman_ford step 3866 current loss 0.033738, current_train_items 123744.
I0304 19:30:01.318623 22579586809984 run.py:483] Algo bellman_ford step 3867 current loss 0.033169, current_train_items 123776.
I0304 19:30:01.349380 22579586809984 run.py:483] Algo bellman_ford step 3868 current loss 0.038396, current_train_items 123808.
I0304 19:30:01.382548 22579586809984 run.py:483] Algo bellman_ford step 3869 current loss 0.071881, current_train_items 123840.
I0304 19:30:01.402435 22579586809984 run.py:483] Algo bellman_ford step 3870 current loss 0.015023, current_train_items 123872.
I0304 19:30:01.418324 22579586809984 run.py:483] Algo bellman_ford step 3871 current loss 0.017527, current_train_items 123904.
I0304 19:30:01.442371 22579586809984 run.py:483] Algo bellman_ford step 3872 current loss 0.026735, current_train_items 123936.
I0304 19:30:01.472003 22579586809984 run.py:483] Algo bellman_ford step 3873 current loss 0.030475, current_train_items 123968.
I0304 19:30:01.504729 22579586809984 run.py:483] Algo bellman_ford step 3874 current loss 0.051801, current_train_items 124000.
I0304 19:30:01.524545 22579586809984 run.py:483] Algo bellman_ford step 3875 current loss 0.007692, current_train_items 124032.
I0304 19:30:01.541238 22579586809984 run.py:483] Algo bellman_ford step 3876 current loss 0.062703, current_train_items 124064.
I0304 19:30:01.565401 22579586809984 run.py:483] Algo bellman_ford step 3877 current loss 0.057146, current_train_items 124096.
I0304 19:30:01.597256 22579586809984 run.py:483] Algo bellman_ford step 3878 current loss 0.038777, current_train_items 124128.
I0304 19:30:01.631146 22579586809984 run.py:483] Algo bellman_ford step 3879 current loss 0.067643, current_train_items 124160.
I0304 19:30:01.650812 22579586809984 run.py:483] Algo bellman_ford step 3880 current loss 0.070621, current_train_items 124192.
I0304 19:30:01.666930 22579586809984 run.py:483] Algo bellman_ford step 3881 current loss 0.025012, current_train_items 124224.
I0304 19:30:01.691261 22579586809984 run.py:483] Algo bellman_ford step 3882 current loss 0.056016, current_train_items 124256.
I0304 19:30:01.722749 22579586809984 run.py:483] Algo bellman_ford step 3883 current loss 0.052755, current_train_items 124288.
I0304 19:30:01.755288 22579586809984 run.py:483] Algo bellman_ford step 3884 current loss 0.102073, current_train_items 124320.
I0304 19:30:01.775338 22579586809984 run.py:483] Algo bellman_ford step 3885 current loss 0.006922, current_train_items 124352.
I0304 19:30:01.792101 22579586809984 run.py:483] Algo bellman_ford step 3886 current loss 0.018812, current_train_items 124384.
I0304 19:30:01.815614 22579586809984 run.py:483] Algo bellman_ford step 3887 current loss 0.062991, current_train_items 124416.
I0304 19:30:01.846756 22579586809984 run.py:483] Algo bellman_ford step 3888 current loss 0.047092, current_train_items 124448.
I0304 19:30:01.881426 22579586809984 run.py:483] Algo bellman_ford step 3889 current loss 0.067617, current_train_items 124480.
I0304 19:30:01.901020 22579586809984 run.py:483] Algo bellman_ford step 3890 current loss 0.003796, current_train_items 124512.
I0304 19:30:01.917491 22579586809984 run.py:483] Algo bellman_ford step 3891 current loss 0.023243, current_train_items 124544.
I0304 19:30:01.942079 22579586809984 run.py:483] Algo bellman_ford step 3892 current loss 0.023009, current_train_items 124576.
I0304 19:30:01.971463 22579586809984 run.py:483] Algo bellman_ford step 3893 current loss 0.030960, current_train_items 124608.
I0304 19:30:02.005605 22579586809984 run.py:483] Algo bellman_ford step 3894 current loss 0.057586, current_train_items 124640.
I0304 19:30:02.024951 22579586809984 run.py:483] Algo bellman_ford step 3895 current loss 0.003562, current_train_items 124672.
I0304 19:30:02.041439 22579586809984 run.py:483] Algo bellman_ford step 3896 current loss 0.038420, current_train_items 124704.
I0304 19:30:02.066394 22579586809984 run.py:483] Algo bellman_ford step 3897 current loss 0.064893, current_train_items 124736.
I0304 19:30:02.097915 22579586809984 run.py:483] Algo bellman_ford step 3898 current loss 0.054967, current_train_items 124768.
I0304 19:30:02.131436 22579586809984 run.py:483] Algo bellman_ford step 3899 current loss 0.112618, current_train_items 124800.
I0304 19:30:02.151477 22579586809984 run.py:483] Algo bellman_ford step 3900 current loss 0.004170, current_train_items 124832.
I0304 19:30:02.159257 22579586809984 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0304 19:30:02.159362 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:02.176461 22579586809984 run.py:483] Algo bellman_ford step 3901 current loss 0.042481, current_train_items 124864.
I0304 19:30:02.201047 22579586809984 run.py:483] Algo bellman_ford step 3902 current loss 0.070080, current_train_items 124896.
I0304 19:30:02.233170 22579586809984 run.py:483] Algo bellman_ford step 3903 current loss 0.072178, current_train_items 124928.
I0304 19:30:02.267188 22579586809984 run.py:483] Algo bellman_ford step 3904 current loss 0.058530, current_train_items 124960.
I0304 19:30:02.287554 22579586809984 run.py:483] Algo bellman_ford step 3905 current loss 0.005104, current_train_items 124992.
I0304 19:30:02.303498 22579586809984 run.py:483] Algo bellman_ford step 3906 current loss 0.040196, current_train_items 125024.
I0304 19:30:02.326570 22579586809984 run.py:483] Algo bellman_ford step 3907 current loss 0.028353, current_train_items 125056.
I0304 19:30:02.358268 22579586809984 run.py:483] Algo bellman_ford step 3908 current loss 0.066485, current_train_items 125088.
I0304 19:30:02.392333 22579586809984 run.py:483] Algo bellman_ford step 3909 current loss 0.090749, current_train_items 125120.
I0304 19:30:02.411850 22579586809984 run.py:483] Algo bellman_ford step 3910 current loss 0.004260, current_train_items 125152.
I0304 19:30:02.427906 22579586809984 run.py:483] Algo bellman_ford step 3911 current loss 0.007073, current_train_items 125184.
I0304 19:30:02.451945 22579586809984 run.py:483] Algo bellman_ford step 3912 current loss 0.036699, current_train_items 125216.
I0304 19:30:02.482794 22579586809984 run.py:483] Algo bellman_ford step 3913 current loss 0.035051, current_train_items 125248.
I0304 19:30:02.513844 22579586809984 run.py:483] Algo bellman_ford step 3914 current loss 0.055597, current_train_items 125280.
I0304 19:30:02.533529 22579586809984 run.py:483] Algo bellman_ford step 3915 current loss 0.017219, current_train_items 125312.
I0304 19:30:02.549677 22579586809984 run.py:483] Algo bellman_ford step 3916 current loss 0.009130, current_train_items 125344.
I0304 19:30:02.575132 22579586809984 run.py:483] Algo bellman_ford step 3917 current loss 0.064584, current_train_items 125376.
I0304 19:30:02.605480 22579586809984 run.py:483] Algo bellman_ford step 3918 current loss 0.091972, current_train_items 125408.
I0304 19:30:02.639151 22579586809984 run.py:483] Algo bellman_ford step 3919 current loss 0.054657, current_train_items 125440.
I0304 19:30:02.659064 22579586809984 run.py:483] Algo bellman_ford step 3920 current loss 0.004452, current_train_items 125472.
I0304 19:30:02.675546 22579586809984 run.py:483] Algo bellman_ford step 3921 current loss 0.012819, current_train_items 125504.
I0304 19:30:02.700935 22579586809984 run.py:483] Algo bellman_ford step 3922 current loss 0.051759, current_train_items 125536.
I0304 19:30:02.733200 22579586809984 run.py:483] Algo bellman_ford step 3923 current loss 0.119290, current_train_items 125568.
I0304 19:30:02.769347 22579586809984 run.py:483] Algo bellman_ford step 3924 current loss 0.073171, current_train_items 125600.
I0304 19:30:02.789481 22579586809984 run.py:483] Algo bellman_ford step 3925 current loss 0.003585, current_train_items 125632.
I0304 19:30:02.805892 22579586809984 run.py:483] Algo bellman_ford step 3926 current loss 0.038568, current_train_items 125664.
I0304 19:30:02.830283 22579586809984 run.py:483] Algo bellman_ford step 3927 current loss 0.047600, current_train_items 125696.
I0304 19:30:02.861215 22579586809984 run.py:483] Algo bellman_ford step 3928 current loss 0.103648, current_train_items 125728.
I0304 19:30:02.894555 22579586809984 run.py:483] Algo bellman_ford step 3929 current loss 0.076591, current_train_items 125760.
I0304 19:30:02.914328 22579586809984 run.py:483] Algo bellman_ford step 3930 current loss 0.003128, current_train_items 125792.
I0304 19:30:02.930382 22579586809984 run.py:483] Algo bellman_ford step 3931 current loss 0.030819, current_train_items 125824.
I0304 19:30:02.954629 22579586809984 run.py:483] Algo bellman_ford step 3932 current loss 0.061903, current_train_items 125856.
I0304 19:30:02.987301 22579586809984 run.py:483] Algo bellman_ford step 3933 current loss 0.094985, current_train_items 125888.
I0304 19:30:03.021207 22579586809984 run.py:483] Algo bellman_ford step 3934 current loss 0.103745, current_train_items 125920.
I0304 19:30:03.040971 22579586809984 run.py:483] Algo bellman_ford step 3935 current loss 0.011113, current_train_items 125952.
I0304 19:30:03.057526 22579586809984 run.py:483] Algo bellman_ford step 3936 current loss 0.029225, current_train_items 125984.
I0304 19:30:03.081513 22579586809984 run.py:483] Algo bellman_ford step 3937 current loss 0.047501, current_train_items 126016.
I0304 19:30:03.113452 22579586809984 run.py:483] Algo bellman_ford step 3938 current loss 0.047030, current_train_items 126048.
I0304 19:30:03.148319 22579586809984 run.py:483] Algo bellman_ford step 3939 current loss 0.082926, current_train_items 126080.
I0304 19:30:03.168108 22579586809984 run.py:483] Algo bellman_ford step 3940 current loss 0.004829, current_train_items 126112.
I0304 19:30:03.184420 22579586809984 run.py:483] Algo bellman_ford step 3941 current loss 0.024653, current_train_items 126144.
I0304 19:30:03.208325 22579586809984 run.py:483] Algo bellman_ford step 3942 current loss 0.055122, current_train_items 126176.
I0304 19:30:03.238706 22579586809984 run.py:483] Algo bellman_ford step 3943 current loss 0.038543, current_train_items 126208.
I0304 19:30:03.273532 22579586809984 run.py:483] Algo bellman_ford step 3944 current loss 0.111941, current_train_items 126240.
I0304 19:30:03.293324 22579586809984 run.py:483] Algo bellman_ford step 3945 current loss 0.010266, current_train_items 126272.
I0304 19:30:03.309941 22579586809984 run.py:483] Algo bellman_ford step 3946 current loss 0.019179, current_train_items 126304.
I0304 19:30:03.334079 22579586809984 run.py:483] Algo bellman_ford step 3947 current loss 0.063089, current_train_items 126336.
I0304 19:30:03.364099 22579586809984 run.py:483] Algo bellman_ford step 3948 current loss 0.091249, current_train_items 126368.
I0304 19:30:03.398382 22579586809984 run.py:483] Algo bellman_ford step 3949 current loss 0.080325, current_train_items 126400.
I0304 19:30:03.417963 22579586809984 run.py:483] Algo bellman_ford step 3950 current loss 0.005200, current_train_items 126432.
I0304 19:30:03.426443 22579586809984 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0304 19:30:03.426549 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:03.444207 22579586809984 run.py:483] Algo bellman_ford step 3951 current loss 0.031313, current_train_items 126464.
I0304 19:30:03.470297 22579586809984 run.py:483] Algo bellman_ford step 3952 current loss 0.048323, current_train_items 126496.
I0304 19:30:03.501453 22579586809984 run.py:483] Algo bellman_ford step 3953 current loss 0.057887, current_train_items 126528.
I0304 19:30:03.534036 22579586809984 run.py:483] Algo bellman_ford step 3954 current loss 0.046130, current_train_items 126560.
I0304 19:30:03.553909 22579586809984 run.py:483] Algo bellman_ford step 3955 current loss 0.003292, current_train_items 126592.
I0304 19:30:03.570206 22579586809984 run.py:483] Algo bellman_ford step 3956 current loss 0.041641, current_train_items 126624.
I0304 19:30:03.594437 22579586809984 run.py:483] Algo bellman_ford step 3957 current loss 0.056279, current_train_items 126656.
I0304 19:30:03.626743 22579586809984 run.py:483] Algo bellman_ford step 3958 current loss 0.071884, current_train_items 126688.
I0304 19:30:03.662534 22579586809984 run.py:483] Algo bellman_ford step 3959 current loss 0.083068, current_train_items 126720.
I0304 19:30:03.682657 22579586809984 run.py:483] Algo bellman_ford step 3960 current loss 0.019875, current_train_items 126752.
I0304 19:30:03.699316 22579586809984 run.py:483] Algo bellman_ford step 3961 current loss 0.020798, current_train_items 126784.
I0304 19:30:03.721998 22579586809984 run.py:483] Algo bellman_ford step 3962 current loss 0.030967, current_train_items 126816.
I0304 19:30:03.754070 22579586809984 run.py:483] Algo bellman_ford step 3963 current loss 0.062881, current_train_items 126848.
I0304 19:30:03.788026 22579586809984 run.py:483] Algo bellman_ford step 3964 current loss 0.090821, current_train_items 126880.
I0304 19:30:03.808196 22579586809984 run.py:483] Algo bellman_ford step 3965 current loss 0.005889, current_train_items 126912.
I0304 19:30:03.824839 22579586809984 run.py:483] Algo bellman_ford step 3966 current loss 0.028026, current_train_items 126944.
I0304 19:30:03.850444 22579586809984 run.py:483] Algo bellman_ford step 3967 current loss 0.037870, current_train_items 126976.
I0304 19:30:03.881929 22579586809984 run.py:483] Algo bellman_ford step 3968 current loss 0.047499, current_train_items 127008.
I0304 19:30:03.915082 22579586809984 run.py:483] Algo bellman_ford step 3969 current loss 0.052567, current_train_items 127040.
I0304 19:30:03.935293 22579586809984 run.py:483] Algo bellman_ford step 3970 current loss 0.003959, current_train_items 127072.
I0304 19:30:03.951658 22579586809984 run.py:483] Algo bellman_ford step 3971 current loss 0.010733, current_train_items 127104.
I0304 19:30:03.976157 22579586809984 run.py:483] Algo bellman_ford step 3972 current loss 0.034328, current_train_items 127136.
I0304 19:30:04.008263 22579586809984 run.py:483] Algo bellman_ford step 3973 current loss 0.073478, current_train_items 127168.
I0304 19:30:04.040871 22579586809984 run.py:483] Algo bellman_ford step 3974 current loss 0.060945, current_train_items 127200.
I0304 19:30:04.061219 22579586809984 run.py:483] Algo bellman_ford step 3975 current loss 0.004032, current_train_items 127232.
I0304 19:30:04.077886 22579586809984 run.py:483] Algo bellman_ford step 3976 current loss 0.035727, current_train_items 127264.
I0304 19:30:04.101657 22579586809984 run.py:483] Algo bellman_ford step 3977 current loss 0.052777, current_train_items 127296.
I0304 19:30:04.133926 22579586809984 run.py:483] Algo bellman_ford step 3978 current loss 0.071701, current_train_items 127328.
I0304 19:30:04.167524 22579586809984 run.py:483] Algo bellman_ford step 3979 current loss 0.061541, current_train_items 127360.
I0304 19:30:04.187659 22579586809984 run.py:483] Algo bellman_ford step 3980 current loss 0.002862, current_train_items 127392.
I0304 19:30:04.203906 22579586809984 run.py:483] Algo bellman_ford step 3981 current loss 0.022395, current_train_items 127424.
I0304 19:30:04.227489 22579586809984 run.py:483] Algo bellman_ford step 3982 current loss 0.039896, current_train_items 127456.
I0304 19:30:04.259351 22579586809984 run.py:483] Algo bellman_ford step 3983 current loss 0.075849, current_train_items 127488.
I0304 19:30:04.292469 22579586809984 run.py:483] Algo bellman_ford step 3984 current loss 0.072002, current_train_items 127520.
I0304 19:30:04.312741 22579586809984 run.py:483] Algo bellman_ford step 3985 current loss 0.004073, current_train_items 127552.
I0304 19:30:04.329342 22579586809984 run.py:483] Algo bellman_ford step 3986 current loss 0.042059, current_train_items 127584.
I0304 19:30:04.352997 22579586809984 run.py:483] Algo bellman_ford step 3987 current loss 0.061494, current_train_items 127616.
I0304 19:30:04.385051 22579586809984 run.py:483] Algo bellman_ford step 3988 current loss 0.068359, current_train_items 127648.
I0304 19:30:04.419341 22579586809984 run.py:483] Algo bellman_ford step 3989 current loss 0.078650, current_train_items 127680.
I0304 19:30:04.439769 22579586809984 run.py:483] Algo bellman_ford step 3990 current loss 0.014652, current_train_items 127712.
I0304 19:30:04.456377 22579586809984 run.py:483] Algo bellman_ford step 3991 current loss 0.033449, current_train_items 127744.
I0304 19:30:04.479702 22579586809984 run.py:483] Algo bellman_ford step 3992 current loss 0.032469, current_train_items 127776.
I0304 19:30:04.510803 22579586809984 run.py:483] Algo bellman_ford step 3993 current loss 0.048346, current_train_items 127808.
I0304 19:30:04.545080 22579586809984 run.py:483] Algo bellman_ford step 3994 current loss 0.079833, current_train_items 127840.
I0304 19:30:04.564941 22579586809984 run.py:483] Algo bellman_ford step 3995 current loss 0.018411, current_train_items 127872.
I0304 19:30:04.581426 22579586809984 run.py:483] Algo bellman_ford step 3996 current loss 0.024251, current_train_items 127904.
I0304 19:30:04.605858 22579586809984 run.py:483] Algo bellman_ford step 3997 current loss 0.033216, current_train_items 127936.
I0304 19:30:04.636952 22579586809984 run.py:483] Algo bellman_ford step 3998 current loss 0.049939, current_train_items 127968.
I0304 19:30:04.671920 22579586809984 run.py:483] Algo bellman_ford step 3999 current loss 0.102652, current_train_items 128000.
I0304 19:30:04.692389 22579586809984 run.py:483] Algo bellman_ford step 4000 current loss 0.010907, current_train_items 128032.
I0304 19:30:04.700226 22579586809984 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0304 19:30:04.700333 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:30:04.717257 22579586809984 run.py:483] Algo bellman_ford step 4001 current loss 0.025108, current_train_items 128064.
I0304 19:30:04.741975 22579586809984 run.py:483] Algo bellman_ford step 4002 current loss 0.046888, current_train_items 128096.
I0304 19:30:04.773311 22579586809984 run.py:483] Algo bellman_ford step 4003 current loss 0.098076, current_train_items 128128.
I0304 19:30:04.806664 22579586809984 run.py:483] Algo bellman_ford step 4004 current loss 0.146769, current_train_items 128160.
I0304 19:30:04.826672 22579586809984 run.py:483] Algo bellman_ford step 4005 current loss 0.002709, current_train_items 128192.
I0304 19:30:04.842863 22579586809984 run.py:483] Algo bellman_ford step 4006 current loss 0.013188, current_train_items 128224.
I0304 19:30:04.867276 22579586809984 run.py:483] Algo bellman_ford step 4007 current loss 0.142887, current_train_items 128256.
I0304 19:30:04.899516 22579586809984 run.py:483] Algo bellman_ford step 4008 current loss 0.063080, current_train_items 128288.
I0304 19:30:04.932051 22579586809984 run.py:483] Algo bellman_ford step 4009 current loss 0.095725, current_train_items 128320.
I0304 19:30:04.952069 22579586809984 run.py:483] Algo bellman_ford step 4010 current loss 0.005508, current_train_items 128352.
I0304 19:30:04.968346 22579586809984 run.py:483] Algo bellman_ford step 4011 current loss 0.015369, current_train_items 128384.
I0304 19:30:04.991569 22579586809984 run.py:483] Algo bellman_ford step 4012 current loss 0.048123, current_train_items 128416.
I0304 19:30:05.023193 22579586809984 run.py:483] Algo bellman_ford step 4013 current loss 0.084943, current_train_items 128448.
I0304 19:30:05.057273 22579586809984 run.py:483] Algo bellman_ford step 4014 current loss 0.086140, current_train_items 128480.
I0304 19:30:05.077047 22579586809984 run.py:483] Algo bellman_ford step 4015 current loss 0.002919, current_train_items 128512.
I0304 19:30:05.093398 22579586809984 run.py:483] Algo bellman_ford step 4016 current loss 0.028863, current_train_items 128544.
I0304 19:30:05.117813 22579586809984 run.py:483] Algo bellman_ford step 4017 current loss 0.048208, current_train_items 128576.
I0304 19:30:05.149238 22579586809984 run.py:483] Algo bellman_ford step 4018 current loss 0.093011, current_train_items 128608.
I0304 19:30:05.182365 22579586809984 run.py:483] Algo bellman_ford step 4019 current loss 0.050854, current_train_items 128640.
I0304 19:30:05.202170 22579586809984 run.py:483] Algo bellman_ford step 4020 current loss 0.002075, current_train_items 128672.
I0304 19:30:05.218286 22579586809984 run.py:483] Algo bellman_ford step 4021 current loss 0.038087, current_train_items 128704.
I0304 19:30:05.243006 22579586809984 run.py:483] Algo bellman_ford step 4022 current loss 0.084070, current_train_items 128736.
I0304 19:30:05.273707 22579586809984 run.py:483] Algo bellman_ford step 4023 current loss 0.051367, current_train_items 128768.
I0304 19:30:05.310518 22579586809984 run.py:483] Algo bellman_ford step 4024 current loss 0.206090, current_train_items 128800.
I0304 19:30:05.330358 22579586809984 run.py:483] Algo bellman_ford step 4025 current loss 0.004136, current_train_items 128832.
I0304 19:30:05.346893 22579586809984 run.py:483] Algo bellman_ford step 4026 current loss 0.021348, current_train_items 128864.
I0304 19:30:05.370861 22579586809984 run.py:483] Algo bellman_ford step 4027 current loss 0.044460, current_train_items 128896.
I0304 19:30:05.401560 22579586809984 run.py:483] Algo bellman_ford step 4028 current loss 0.074743, current_train_items 128928.
I0304 19:30:05.434870 22579586809984 run.py:483] Algo bellman_ford step 4029 current loss 0.060386, current_train_items 128960.
I0304 19:30:05.454401 22579586809984 run.py:483] Algo bellman_ford step 4030 current loss 0.005003, current_train_items 128992.
I0304 19:30:05.470466 22579586809984 run.py:483] Algo bellman_ford step 4031 current loss 0.048305, current_train_items 129024.
I0304 19:30:05.494720 22579586809984 run.py:483] Algo bellman_ford step 4032 current loss 0.155447, current_train_items 129056.
I0304 19:30:05.527005 22579586809984 run.py:483] Algo bellman_ford step 4033 current loss 0.198044, current_train_items 129088.
I0304 19:30:05.560334 22579586809984 run.py:483] Algo bellman_ford step 4034 current loss 0.090248, current_train_items 129120.
I0304 19:30:05.579975 22579586809984 run.py:483] Algo bellman_ford step 4035 current loss 0.004719, current_train_items 129152.
I0304 19:30:05.596121 22579586809984 run.py:483] Algo bellman_ford step 4036 current loss 0.046721, current_train_items 129184.
I0304 19:30:05.620306 22579586809984 run.py:483] Algo bellman_ford step 4037 current loss 0.087569, current_train_items 129216.
I0304 19:30:05.652622 22579586809984 run.py:483] Algo bellman_ford step 4038 current loss 0.126280, current_train_items 129248.
I0304 19:30:05.687386 22579586809984 run.py:483] Algo bellman_ford step 4039 current loss 0.084549, current_train_items 129280.
I0304 19:30:05.707196 22579586809984 run.py:483] Algo bellman_ford step 4040 current loss 0.005655, current_train_items 129312.
I0304 19:30:05.723619 22579586809984 run.py:483] Algo bellman_ford step 4041 current loss 0.021854, current_train_items 129344.
I0304 19:30:05.747402 22579586809984 run.py:483] Algo bellman_ford step 4042 current loss 0.093474, current_train_items 129376.
I0304 19:30:05.780073 22579586809984 run.py:483] Algo bellman_ford step 4043 current loss 0.139305, current_train_items 129408.
I0304 19:30:05.813246 22579586809984 run.py:483] Algo bellman_ford step 4044 current loss 0.071968, current_train_items 129440.
I0304 19:30:05.833080 22579586809984 run.py:483] Algo bellman_ford step 4045 current loss 0.004690, current_train_items 129472.
I0304 19:30:05.849732 22579586809984 run.py:483] Algo bellman_ford step 4046 current loss 0.049947, current_train_items 129504.
I0304 19:30:05.873711 22579586809984 run.py:483] Algo bellman_ford step 4047 current loss 0.095239, current_train_items 129536.
I0304 19:30:05.904343 22579586809984 run.py:483] Algo bellman_ford step 4048 current loss 0.088600, current_train_items 129568.
I0304 19:30:05.939848 22579586809984 run.py:483] Algo bellman_ford step 4049 current loss 0.115973, current_train_items 129600.
I0304 19:30:05.959506 22579586809984 run.py:483] Algo bellman_ford step 4050 current loss 0.006145, current_train_items 129632.
I0304 19:30:05.967456 22579586809984 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0304 19:30:05.967561 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:30:05.984772 22579586809984 run.py:483] Algo bellman_ford step 4051 current loss 0.024086, current_train_items 129664.
I0304 19:30:06.008693 22579586809984 run.py:483] Algo bellman_ford step 4052 current loss 0.056500, current_train_items 129696.
I0304 19:30:06.040697 22579586809984 run.py:483] Algo bellman_ford step 4053 current loss 0.121555, current_train_items 129728.
I0304 19:30:06.076550 22579586809984 run.py:483] Algo bellman_ford step 4054 current loss 0.176494, current_train_items 129760.
I0304 19:30:06.096759 22579586809984 run.py:483] Algo bellman_ford step 4055 current loss 0.009221, current_train_items 129792.
I0304 19:30:06.112602 22579586809984 run.py:483] Algo bellman_ford step 4056 current loss 0.015366, current_train_items 129824.
I0304 19:30:06.136595 22579586809984 run.py:483] Algo bellman_ford step 4057 current loss 0.058939, current_train_items 129856.
I0304 19:30:06.167257 22579586809984 run.py:483] Algo bellman_ford step 4058 current loss 0.079698, current_train_items 129888.
I0304 19:30:06.199589 22579586809984 run.py:483] Algo bellman_ford step 4059 current loss 0.058522, current_train_items 129920.
I0304 19:30:06.219640 22579586809984 run.py:483] Algo bellman_ford step 4060 current loss 0.012330, current_train_items 129952.
I0304 19:30:06.236404 22579586809984 run.py:483] Algo bellman_ford step 4061 current loss 0.023095, current_train_items 129984.
I0304 19:30:06.259321 22579586809984 run.py:483] Algo bellman_ford step 4062 current loss 0.043466, current_train_items 130016.
I0304 19:30:06.289404 22579586809984 run.py:483] Algo bellman_ford step 4063 current loss 0.050680, current_train_items 130048.
I0304 19:30:06.324615 22579586809984 run.py:483] Algo bellman_ford step 4064 current loss 0.118257, current_train_items 130080.
I0304 19:30:06.344419 22579586809984 run.py:483] Algo bellman_ford step 4065 current loss 0.004735, current_train_items 130112.
I0304 19:30:06.360231 22579586809984 run.py:483] Algo bellman_ford step 4066 current loss 0.106226, current_train_items 130144.
I0304 19:30:06.385381 22579586809984 run.py:483] Algo bellman_ford step 4067 current loss 0.057578, current_train_items 130176.
I0304 19:30:06.416381 22579586809984 run.py:483] Algo bellman_ford step 4068 current loss 0.066886, current_train_items 130208.
I0304 19:30:06.451584 22579586809984 run.py:483] Algo bellman_ford step 4069 current loss 0.083171, current_train_items 130240.
I0304 19:30:06.471704 22579586809984 run.py:483] Algo bellman_ford step 4070 current loss 0.004914, current_train_items 130272.
I0304 19:30:06.488213 22579586809984 run.py:483] Algo bellman_ford step 4071 current loss 0.046504, current_train_items 130304.
I0304 19:30:06.512057 22579586809984 run.py:483] Algo bellman_ford step 4072 current loss 0.084497, current_train_items 130336.
I0304 19:30:06.542206 22579586809984 run.py:483] Algo bellman_ford step 4073 current loss 0.097225, current_train_items 130368.
I0304 19:30:06.575038 22579586809984 run.py:483] Algo bellman_ford step 4074 current loss 0.079029, current_train_items 130400.
I0304 19:30:06.595106 22579586809984 run.py:483] Algo bellman_ford step 4075 current loss 0.016128, current_train_items 130432.
I0304 19:30:06.611930 22579586809984 run.py:483] Algo bellman_ford step 4076 current loss 0.029806, current_train_items 130464.
I0304 19:30:06.636549 22579586809984 run.py:483] Algo bellman_ford step 4077 current loss 0.072579, current_train_items 130496.
I0304 19:30:06.667389 22579586809984 run.py:483] Algo bellman_ford step 4078 current loss 0.050790, current_train_items 130528.
I0304 19:30:06.700319 22579586809984 run.py:483] Algo bellman_ford step 4079 current loss 0.058277, current_train_items 130560.
I0304 19:30:06.719942 22579586809984 run.py:483] Algo bellman_ford step 4080 current loss 0.002968, current_train_items 130592.
I0304 19:30:06.736470 22579586809984 run.py:483] Algo bellman_ford step 4081 current loss 0.017754, current_train_items 130624.
I0304 19:30:06.761460 22579586809984 run.py:483] Algo bellman_ford step 4082 current loss 0.051512, current_train_items 130656.
I0304 19:30:06.794476 22579586809984 run.py:483] Algo bellman_ford step 4083 current loss 0.090897, current_train_items 130688.
I0304 19:30:06.825893 22579586809984 run.py:483] Algo bellman_ford step 4084 current loss 0.043556, current_train_items 130720.
I0304 19:30:06.846454 22579586809984 run.py:483] Algo bellman_ford step 4085 current loss 0.017849, current_train_items 130752.
I0304 19:30:06.862931 22579586809984 run.py:483] Algo bellman_ford step 4086 current loss 0.017397, current_train_items 130784.
I0304 19:30:06.886256 22579586809984 run.py:483] Algo bellman_ford step 4087 current loss 0.051497, current_train_items 130816.
I0304 19:30:06.917179 22579586809984 run.py:483] Algo bellman_ford step 4088 current loss 0.044306, current_train_items 130848.
I0304 19:30:06.949564 22579586809984 run.py:483] Algo bellman_ford step 4089 current loss 0.044686, current_train_items 130880.
I0304 19:30:06.969543 22579586809984 run.py:483] Algo bellman_ford step 4090 current loss 0.005274, current_train_items 130912.
I0304 19:30:06.985828 22579586809984 run.py:483] Algo bellman_ford step 4091 current loss 0.018749, current_train_items 130944.
I0304 19:30:07.010160 22579586809984 run.py:483] Algo bellman_ford step 4092 current loss 0.041931, current_train_items 130976.
I0304 19:30:07.042331 22579586809984 run.py:483] Algo bellman_ford step 4093 current loss 0.064190, current_train_items 131008.
I0304 19:30:07.076741 22579586809984 run.py:483] Algo bellman_ford step 4094 current loss 0.077679, current_train_items 131040.
I0304 19:30:07.096514 22579586809984 run.py:483] Algo bellman_ford step 4095 current loss 0.004268, current_train_items 131072.
I0304 19:30:07.113106 22579586809984 run.py:483] Algo bellman_ford step 4096 current loss 0.012442, current_train_items 131104.
I0304 19:30:07.137787 22579586809984 run.py:483] Algo bellman_ford step 4097 current loss 0.061282, current_train_items 131136.
I0304 19:30:07.169257 22579586809984 run.py:483] Algo bellman_ford step 4098 current loss 0.051076, current_train_items 131168.
I0304 19:30:07.203784 22579586809984 run.py:483] Algo bellman_ford step 4099 current loss 0.075637, current_train_items 131200.
I0304 19:30:07.224093 22579586809984 run.py:483] Algo bellman_ford step 4100 current loss 0.003253, current_train_items 131232.
I0304 19:30:07.231650 22579586809984 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0304 19:30:07.231763 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:07.248146 22579586809984 run.py:483] Algo bellman_ford step 4101 current loss 0.009264, current_train_items 131264.
I0304 19:30:07.270612 22579586809984 run.py:483] Algo bellman_ford step 4102 current loss 0.073330, current_train_items 131296.
I0304 19:30:07.303117 22579586809984 run.py:483] Algo bellman_ford step 4103 current loss 0.142745, current_train_items 131328.
I0304 19:30:07.338386 22579586809984 run.py:483] Algo bellman_ford step 4104 current loss 0.122512, current_train_items 131360.
I0304 19:30:07.358021 22579586809984 run.py:483] Algo bellman_ford step 4105 current loss 0.003231, current_train_items 131392.
I0304 19:30:07.374308 22579586809984 run.py:483] Algo bellman_ford step 4106 current loss 0.055707, current_train_items 131424.
I0304 19:30:07.398594 22579586809984 run.py:483] Algo bellman_ford step 4107 current loss 0.079766, current_train_items 131456.
I0304 19:30:07.429668 22579586809984 run.py:483] Algo bellman_ford step 4108 current loss 0.092144, current_train_items 131488.
I0304 19:30:07.465253 22579586809984 run.py:483] Algo bellman_ford step 4109 current loss 0.096195, current_train_items 131520.
I0304 19:30:07.484805 22579586809984 run.py:483] Algo bellman_ford step 4110 current loss 0.015295, current_train_items 131552.
I0304 19:30:07.501569 22579586809984 run.py:483] Algo bellman_ford step 4111 current loss 0.036757, current_train_items 131584.
I0304 19:30:07.524570 22579586809984 run.py:483] Algo bellman_ford step 4112 current loss 0.045307, current_train_items 131616.
I0304 19:30:07.556572 22579586809984 run.py:483] Algo bellman_ford step 4113 current loss 0.114957, current_train_items 131648.
I0304 19:30:07.591382 22579586809984 run.py:483] Algo bellman_ford step 4114 current loss 0.092707, current_train_items 131680.
I0304 19:30:07.610923 22579586809984 run.py:483] Algo bellman_ford step 4115 current loss 0.004166, current_train_items 131712.
I0304 19:30:07.627581 22579586809984 run.py:483] Algo bellman_ford step 4116 current loss 0.024528, current_train_items 131744.
I0304 19:30:07.652484 22579586809984 run.py:483] Algo bellman_ford step 4117 current loss 0.142250, current_train_items 131776.
I0304 19:30:07.682949 22579586809984 run.py:483] Algo bellman_ford step 4118 current loss 0.078178, current_train_items 131808.
I0304 19:30:07.717017 22579586809984 run.py:483] Algo bellman_ford step 4119 current loss 0.159484, current_train_items 131840.
I0304 19:30:07.736460 22579586809984 run.py:483] Algo bellman_ford step 4120 current loss 0.003613, current_train_items 131872.
I0304 19:30:07.752820 22579586809984 run.py:483] Algo bellman_ford step 4121 current loss 0.007213, current_train_items 131904.
I0304 19:30:07.777915 22579586809984 run.py:483] Algo bellman_ford step 4122 current loss 0.043988, current_train_items 131936.
I0304 19:30:07.810026 22579586809984 run.py:483] Algo bellman_ford step 4123 current loss 0.117391, current_train_items 131968.
I0304 19:30:07.843796 22579586809984 run.py:483] Algo bellman_ford step 4124 current loss 0.131259, current_train_items 132000.
I0304 19:30:07.863310 22579586809984 run.py:483] Algo bellman_ford step 4125 current loss 0.003099, current_train_items 132032.
I0304 19:30:07.879710 22579586809984 run.py:483] Algo bellman_ford step 4126 current loss 0.036696, current_train_items 132064.
I0304 19:30:07.904759 22579586809984 run.py:483] Algo bellman_ford step 4127 current loss 0.047052, current_train_items 132096.
I0304 19:30:07.935558 22579586809984 run.py:483] Algo bellman_ford step 4128 current loss 0.072575, current_train_items 132128.
I0304 19:30:07.967975 22579586809984 run.py:483] Algo bellman_ford step 4129 current loss 0.053366, current_train_items 132160.
I0304 19:30:07.987752 22579586809984 run.py:483] Algo bellman_ford step 4130 current loss 0.005099, current_train_items 132192.
I0304 19:30:08.004411 22579586809984 run.py:483] Algo bellman_ford step 4131 current loss 0.024770, current_train_items 132224.
I0304 19:30:08.028524 22579586809984 run.py:483] Algo bellman_ford step 4132 current loss 0.044150, current_train_items 132256.
I0304 19:30:08.060475 22579586809984 run.py:483] Algo bellman_ford step 4133 current loss 0.077213, current_train_items 132288.
I0304 19:30:08.095731 22579586809984 run.py:483] Algo bellman_ford step 4134 current loss 0.055178, current_train_items 132320.
I0304 19:30:08.115378 22579586809984 run.py:483] Algo bellman_ford step 4135 current loss 0.005158, current_train_items 132352.
I0304 19:30:08.131542 22579586809984 run.py:483] Algo bellman_ford step 4136 current loss 0.009862, current_train_items 132384.
I0304 19:30:08.154897 22579586809984 run.py:483] Algo bellman_ford step 4137 current loss 0.039460, current_train_items 132416.
I0304 19:30:08.186131 22579586809984 run.py:483] Algo bellman_ford step 4138 current loss 0.037219, current_train_items 132448.
I0304 19:30:08.220668 22579586809984 run.py:483] Algo bellman_ford step 4139 current loss 0.084043, current_train_items 132480.
I0304 19:30:08.240313 22579586809984 run.py:483] Algo bellman_ford step 4140 current loss 0.003126, current_train_items 132512.
I0304 19:30:08.256179 22579586809984 run.py:483] Algo bellman_ford step 4141 current loss 0.007532, current_train_items 132544.
I0304 19:30:08.281330 22579586809984 run.py:483] Algo bellman_ford step 4142 current loss 0.029630, current_train_items 132576.
I0304 19:30:08.312032 22579586809984 run.py:483] Algo bellman_ford step 4143 current loss 0.052397, current_train_items 132608.
I0304 19:30:08.345235 22579586809984 run.py:483] Algo bellman_ford step 4144 current loss 0.084578, current_train_items 132640.
I0304 19:30:08.364796 22579586809984 run.py:483] Algo bellman_ford step 4145 current loss 0.005861, current_train_items 132672.
I0304 19:30:08.380777 22579586809984 run.py:483] Algo bellman_ford step 4146 current loss 0.009801, current_train_items 132704.
I0304 19:30:08.404340 22579586809984 run.py:483] Algo bellman_ford step 4147 current loss 0.103956, current_train_items 132736.
I0304 19:30:08.434254 22579586809984 run.py:483] Algo bellman_ford step 4148 current loss 0.043153, current_train_items 132768.
I0304 19:30:08.467411 22579586809984 run.py:483] Algo bellman_ford step 4149 current loss 0.055308, current_train_items 132800.
I0304 19:30:08.486996 22579586809984 run.py:483] Algo bellman_ford step 4150 current loss 0.004148, current_train_items 132832.
I0304 19:30:08.494920 22579586809984 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0304 19:30:08.495025 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:08.512096 22579586809984 run.py:483] Algo bellman_ford step 4151 current loss 0.043959, current_train_items 132864.
I0304 19:30:08.536849 22579586809984 run.py:483] Algo bellman_ford step 4152 current loss 0.084374, current_train_items 132896.
I0304 19:30:08.568547 22579586809984 run.py:483] Algo bellman_ford step 4153 current loss 0.051889, current_train_items 132928.
I0304 19:30:08.603733 22579586809984 run.py:483] Algo bellman_ford step 4154 current loss 0.087330, current_train_items 132960.
I0304 19:30:08.623797 22579586809984 run.py:483] Algo bellman_ford step 4155 current loss 0.004431, current_train_items 132992.
I0304 19:30:08.640379 22579586809984 run.py:483] Algo bellman_ford step 4156 current loss 0.037650, current_train_items 133024.
I0304 19:30:08.663855 22579586809984 run.py:483] Algo bellman_ford step 4157 current loss 0.033835, current_train_items 133056.
I0304 19:30:08.694557 22579586809984 run.py:483] Algo bellman_ford step 4158 current loss 0.050989, current_train_items 133088.
I0304 19:30:08.728960 22579586809984 run.py:483] Algo bellman_ford step 4159 current loss 0.076890, current_train_items 133120.
I0304 19:30:08.749171 22579586809984 run.py:483] Algo bellman_ford step 4160 current loss 0.005024, current_train_items 133152.
I0304 19:30:08.765775 22579586809984 run.py:483] Algo bellman_ford step 4161 current loss 0.029978, current_train_items 133184.
I0304 19:30:08.788404 22579586809984 run.py:483] Algo bellman_ford step 4162 current loss 0.070895, current_train_items 133216.
I0304 19:30:08.820373 22579586809984 run.py:483] Algo bellman_ford step 4163 current loss 0.041827, current_train_items 133248.
I0304 19:30:08.854555 22579586809984 run.py:483] Algo bellman_ford step 4164 current loss 0.063025, current_train_items 133280.
I0304 19:30:08.874402 22579586809984 run.py:483] Algo bellman_ford step 4165 current loss 0.003813, current_train_items 133312.
I0304 19:30:08.890622 22579586809984 run.py:483] Algo bellman_ford step 4166 current loss 0.007016, current_train_items 133344.
I0304 19:30:08.915709 22579586809984 run.py:483] Algo bellman_ford step 4167 current loss 0.089226, current_train_items 133376.
I0304 19:30:08.946890 22579586809984 run.py:483] Algo bellman_ford step 4168 current loss 0.054254, current_train_items 133408.
I0304 19:30:08.980025 22579586809984 run.py:483] Algo bellman_ford step 4169 current loss 0.072704, current_train_items 133440.
I0304 19:30:09.000379 22579586809984 run.py:483] Algo bellman_ford step 4170 current loss 0.002615, current_train_items 133472.
I0304 19:30:09.016197 22579586809984 run.py:483] Algo bellman_ford step 4171 current loss 0.012347, current_train_items 133504.
I0304 19:30:09.039854 22579586809984 run.py:483] Algo bellman_ford step 4172 current loss 0.026478, current_train_items 133536.
I0304 19:30:09.070729 22579586809984 run.py:483] Algo bellman_ford step 4173 current loss 0.059549, current_train_items 133568.
I0304 19:30:09.106405 22579586809984 run.py:483] Algo bellman_ford step 4174 current loss 0.073761, current_train_items 133600.
I0304 19:30:09.126519 22579586809984 run.py:483] Algo bellman_ford step 4175 current loss 0.002859, current_train_items 133632.
I0304 19:30:09.143553 22579586809984 run.py:483] Algo bellman_ford step 4176 current loss 0.008134, current_train_items 133664.
I0304 19:30:09.167586 22579586809984 run.py:483] Algo bellman_ford step 4177 current loss 0.035056, current_train_items 133696.
I0304 19:30:09.199101 22579586809984 run.py:483] Algo bellman_ford step 4178 current loss 0.041133, current_train_items 133728.
I0304 19:30:09.233490 22579586809984 run.py:483] Algo bellman_ford step 4179 current loss 0.084974, current_train_items 133760.
I0304 19:30:09.253503 22579586809984 run.py:483] Algo bellman_ford step 4180 current loss 0.002525, current_train_items 133792.
I0304 19:30:09.269798 22579586809984 run.py:483] Algo bellman_ford step 4181 current loss 0.029491, current_train_items 133824.
I0304 19:30:09.294677 22579586809984 run.py:483] Algo bellman_ford step 4182 current loss 0.035364, current_train_items 133856.
I0304 19:30:09.324668 22579586809984 run.py:483] Algo bellman_ford step 4183 current loss 0.028934, current_train_items 133888.
I0304 19:30:09.359173 22579586809984 run.py:483] Algo bellman_ford step 4184 current loss 0.075478, current_train_items 133920.
I0304 19:30:09.379187 22579586809984 run.py:483] Algo bellman_ford step 4185 current loss 0.002869, current_train_items 133952.
I0304 19:30:09.395592 22579586809984 run.py:483] Algo bellman_ford step 4186 current loss 0.033320, current_train_items 133984.
I0304 19:30:09.419518 22579586809984 run.py:483] Algo bellman_ford step 4187 current loss 0.031011, current_train_items 134016.
I0304 19:30:09.450845 22579586809984 run.py:483] Algo bellman_ford step 4188 current loss 0.053865, current_train_items 134048.
I0304 19:30:09.484201 22579586809984 run.py:483] Algo bellman_ford step 4189 current loss 0.076324, current_train_items 134080.
I0304 19:30:09.504569 22579586809984 run.py:483] Algo bellman_ford step 4190 current loss 0.006174, current_train_items 134112.
I0304 19:30:09.521379 22579586809984 run.py:483] Algo bellman_ford step 4191 current loss 0.014775, current_train_items 134144.
I0304 19:30:09.543097 22579586809984 run.py:483] Algo bellman_ford step 4192 current loss 0.033810, current_train_items 134176.
I0304 19:30:09.575184 22579586809984 run.py:483] Algo bellman_ford step 4193 current loss 0.038928, current_train_items 134208.
I0304 19:30:09.611619 22579586809984 run.py:483] Algo bellman_ford step 4194 current loss 0.072974, current_train_items 134240.
I0304 19:30:09.631294 22579586809984 run.py:483] Algo bellman_ford step 4195 current loss 0.005362, current_train_items 134272.
I0304 19:30:09.647506 22579586809984 run.py:483] Algo bellman_ford step 4196 current loss 0.026615, current_train_items 134304.
I0304 19:30:09.671792 22579586809984 run.py:483] Algo bellman_ford step 4197 current loss 0.033179, current_train_items 134336.
I0304 19:30:09.702648 22579586809984 run.py:483] Algo bellman_ford step 4198 current loss 0.041946, current_train_items 134368.
I0304 19:30:09.736136 22579586809984 run.py:483] Algo bellman_ford step 4199 current loss 0.072031, current_train_items 134400.
I0304 19:30:09.755607 22579586809984 run.py:483] Algo bellman_ford step 4200 current loss 0.002286, current_train_items 134432.
I0304 19:30:09.763857 22579586809984 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0304 19:30:09.763962 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:09.781065 22579586809984 run.py:483] Algo bellman_ford step 4201 current loss 0.034562, current_train_items 134464.
I0304 19:30:09.804823 22579586809984 run.py:483] Algo bellman_ford step 4202 current loss 0.027048, current_train_items 134496.
I0304 19:30:09.836136 22579586809984 run.py:483] Algo bellman_ford step 4203 current loss 0.031109, current_train_items 134528.
I0304 19:30:09.869930 22579586809984 run.py:483] Algo bellman_ford step 4204 current loss 0.079692, current_train_items 134560.
I0304 19:30:09.889949 22579586809984 run.py:483] Algo bellman_ford step 4205 current loss 0.003122, current_train_items 134592.
I0304 19:30:09.906254 22579586809984 run.py:483] Algo bellman_ford step 4206 current loss 0.030407, current_train_items 134624.
I0304 19:30:09.930490 22579586809984 run.py:483] Algo bellman_ford step 4207 current loss 0.039170, current_train_items 134656.
I0304 19:30:09.962605 22579586809984 run.py:483] Algo bellman_ford step 4208 current loss 0.043960, current_train_items 134688.
I0304 19:30:09.996852 22579586809984 run.py:483] Algo bellman_ford step 4209 current loss 0.099784, current_train_items 134720.
I0304 19:30:10.016736 22579586809984 run.py:483] Algo bellman_ford step 4210 current loss 0.005591, current_train_items 134752.
I0304 19:30:10.032960 22579586809984 run.py:483] Algo bellman_ford step 4211 current loss 0.067342, current_train_items 134784.
I0304 19:30:10.057239 22579586809984 run.py:483] Algo bellman_ford step 4212 current loss 0.039562, current_train_items 134816.
I0304 19:30:10.088821 22579586809984 run.py:483] Algo bellman_ford step 4213 current loss 0.062722, current_train_items 134848.
I0304 19:30:10.122538 22579586809984 run.py:483] Algo bellman_ford step 4214 current loss 0.100909, current_train_items 134880.
I0304 19:30:10.142417 22579586809984 run.py:483] Algo bellman_ford step 4215 current loss 0.007104, current_train_items 134912.
I0304 19:30:10.158784 22579586809984 run.py:483] Algo bellman_ford step 4216 current loss 0.044661, current_train_items 134944.
I0304 19:30:10.182209 22579586809984 run.py:483] Algo bellman_ford step 4217 current loss 0.036383, current_train_items 134976.
I0304 19:30:10.213093 22579586809984 run.py:483] Algo bellman_ford step 4218 current loss 0.089758, current_train_items 135008.
I0304 19:30:10.246902 22579586809984 run.py:483] Algo bellman_ford step 4219 current loss 0.050762, current_train_items 135040.
I0304 19:30:10.266415 22579586809984 run.py:483] Algo bellman_ford step 4220 current loss 0.003747, current_train_items 135072.
I0304 19:30:10.282866 22579586809984 run.py:483] Algo bellman_ford step 4221 current loss 0.073552, current_train_items 135104.
I0304 19:30:10.306954 22579586809984 run.py:483] Algo bellman_ford step 4222 current loss 0.126786, current_train_items 135136.
I0304 19:30:10.338804 22579586809984 run.py:483] Algo bellman_ford step 4223 current loss 0.126846, current_train_items 135168.
I0304 19:30:10.374318 22579586809984 run.py:483] Algo bellman_ford step 4224 current loss 0.153154, current_train_items 135200.
I0304 19:30:10.393651 22579586809984 run.py:483] Algo bellman_ford step 4225 current loss 0.004525, current_train_items 135232.
I0304 19:30:10.409735 22579586809984 run.py:483] Algo bellman_ford step 4226 current loss 0.012724, current_train_items 135264.
I0304 19:30:10.433196 22579586809984 run.py:483] Algo bellman_ford step 4227 current loss 0.134152, current_train_items 135296.
I0304 19:30:10.463221 22579586809984 run.py:483] Algo bellman_ford step 4228 current loss 0.090949, current_train_items 135328.
I0304 19:30:10.495075 22579586809984 run.py:483] Algo bellman_ford step 4229 current loss 0.115440, current_train_items 135360.
I0304 19:30:10.514390 22579586809984 run.py:483] Algo bellman_ford step 4230 current loss 0.003536, current_train_items 135392.
I0304 19:30:10.530494 22579586809984 run.py:483] Algo bellman_ford step 4231 current loss 0.011819, current_train_items 135424.
I0304 19:30:10.554784 22579586809984 run.py:483] Algo bellman_ford step 4232 current loss 0.076735, current_train_items 135456.
I0304 19:30:10.586151 22579586809984 run.py:483] Algo bellman_ford step 4233 current loss 0.074485, current_train_items 135488.
I0304 19:30:10.619207 22579586809984 run.py:483] Algo bellman_ford step 4234 current loss 0.076992, current_train_items 135520.
I0304 19:30:10.638813 22579586809984 run.py:483] Algo bellman_ford step 4235 current loss 0.006886, current_train_items 135552.
I0304 19:30:10.655723 22579586809984 run.py:483] Algo bellman_ford step 4236 current loss 0.068566, current_train_items 135584.
I0304 19:30:10.679744 22579586809984 run.py:483] Algo bellman_ford step 4237 current loss 0.136912, current_train_items 135616.
I0304 19:30:10.711119 22579586809984 run.py:483] Algo bellman_ford step 4238 current loss 0.100145, current_train_items 135648.
I0304 19:30:10.746292 22579586809984 run.py:483] Algo bellman_ford step 4239 current loss 0.116218, current_train_items 135680.
I0304 19:30:10.765856 22579586809984 run.py:483] Algo bellman_ford step 4240 current loss 0.009704, current_train_items 135712.
I0304 19:30:10.781850 22579586809984 run.py:483] Algo bellman_ford step 4241 current loss 0.009183, current_train_items 135744.
I0304 19:30:10.805247 22579586809984 run.py:483] Algo bellman_ford step 4242 current loss 0.048743, current_train_items 135776.
I0304 19:30:10.837221 22579586809984 run.py:483] Algo bellman_ford step 4243 current loss 0.094518, current_train_items 135808.
I0304 19:30:10.870143 22579586809984 run.py:483] Algo bellman_ford step 4244 current loss 0.056727, current_train_items 135840.
I0304 19:30:10.889571 22579586809984 run.py:483] Algo bellman_ford step 4245 current loss 0.006385, current_train_items 135872.
I0304 19:30:10.905477 22579586809984 run.py:483] Algo bellman_ford step 4246 current loss 0.008469, current_train_items 135904.
I0304 19:30:10.930738 22579586809984 run.py:483] Algo bellman_ford step 4247 current loss 0.032795, current_train_items 135936.
I0304 19:30:10.962606 22579586809984 run.py:483] Algo bellman_ford step 4248 current loss 0.093158, current_train_items 135968.
I0304 19:30:10.997644 22579586809984 run.py:483] Algo bellman_ford step 4249 current loss 0.128315, current_train_items 136000.
I0304 19:30:11.017145 22579586809984 run.py:483] Algo bellman_ford step 4250 current loss 0.003617, current_train_items 136032.
I0304 19:30:11.025004 22579586809984 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0304 19:30:11.025110 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.989, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:30:11.054272 22579586809984 run.py:483] Algo bellman_ford step 4251 current loss 0.013497, current_train_items 136064.
I0304 19:30:11.078338 22579586809984 run.py:483] Algo bellman_ford step 4252 current loss 0.025206, current_train_items 136096.
I0304 19:30:11.110124 22579586809984 run.py:483] Algo bellman_ford step 4253 current loss 0.091308, current_train_items 136128.
I0304 19:30:11.144474 22579586809984 run.py:483] Algo bellman_ford step 4254 current loss 0.080275, current_train_items 136160.
I0304 19:30:11.164173 22579586809984 run.py:483] Algo bellman_ford step 4255 current loss 0.003133, current_train_items 136192.
I0304 19:30:11.180180 22579586809984 run.py:483] Algo bellman_ford step 4256 current loss 0.011189, current_train_items 136224.
I0304 19:30:11.203430 22579586809984 run.py:483] Algo bellman_ford step 4257 current loss 0.037734, current_train_items 136256.
I0304 19:30:11.234559 22579586809984 run.py:483] Algo bellman_ford step 4258 current loss 0.068655, current_train_items 136288.
I0304 19:30:11.269064 22579586809984 run.py:483] Algo bellman_ford step 4259 current loss 0.100069, current_train_items 136320.
I0304 19:30:11.288883 22579586809984 run.py:483] Algo bellman_ford step 4260 current loss 0.003994, current_train_items 136352.
I0304 19:30:11.305748 22579586809984 run.py:483] Algo bellman_ford step 4261 current loss 0.035820, current_train_items 136384.
I0304 19:30:11.329014 22579586809984 run.py:483] Algo bellman_ford step 4262 current loss 0.042372, current_train_items 136416.
I0304 19:30:11.360295 22579586809984 run.py:483] Algo bellman_ford step 4263 current loss 0.074666, current_train_items 136448.
I0304 19:30:11.394061 22579586809984 run.py:483] Algo bellman_ford step 4264 current loss 0.056552, current_train_items 136480.
I0304 19:30:11.413598 22579586809984 run.py:483] Algo bellman_ford step 4265 current loss 0.010919, current_train_items 136512.
I0304 19:30:11.429911 22579586809984 run.py:483] Algo bellman_ford step 4266 current loss 0.034982, current_train_items 136544.
I0304 19:30:11.453428 22579586809984 run.py:483] Algo bellman_ford step 4267 current loss 0.041901, current_train_items 136576.
I0304 19:30:11.484114 22579586809984 run.py:483] Algo bellman_ford step 4268 current loss 0.067945, current_train_items 136608.
I0304 19:30:11.517095 22579586809984 run.py:483] Algo bellman_ford step 4269 current loss 0.073278, current_train_items 136640.
I0304 19:30:11.537281 22579586809984 run.py:483] Algo bellman_ford step 4270 current loss 0.003401, current_train_items 136672.
I0304 19:30:11.553491 22579586809984 run.py:483] Algo bellman_ford step 4271 current loss 0.021486, current_train_items 136704.
I0304 19:30:11.576043 22579586809984 run.py:483] Algo bellman_ford step 4272 current loss 0.008898, current_train_items 136736.
I0304 19:30:11.607056 22579586809984 run.py:483] Algo bellman_ford step 4273 current loss 0.072796, current_train_items 136768.
I0304 19:30:11.639856 22579586809984 run.py:483] Algo bellman_ford step 4274 current loss 0.052015, current_train_items 136800.
I0304 19:30:11.659606 22579586809984 run.py:483] Algo bellman_ford step 4275 current loss 0.003486, current_train_items 136832.
I0304 19:30:11.675947 22579586809984 run.py:483] Algo bellman_ford step 4276 current loss 0.012970, current_train_items 136864.
I0304 19:30:11.699242 22579586809984 run.py:483] Algo bellman_ford step 4277 current loss 0.069446, current_train_items 136896.
I0304 19:30:11.732095 22579586809984 run.py:483] Algo bellman_ford step 4278 current loss 0.073554, current_train_items 136928.
I0304 19:30:11.766421 22579586809984 run.py:483] Algo bellman_ford step 4279 current loss 0.072465, current_train_items 136960.
I0304 19:30:11.786140 22579586809984 run.py:483] Algo bellman_ford step 4280 current loss 0.021307, current_train_items 136992.
I0304 19:30:11.802214 22579586809984 run.py:483] Algo bellman_ford step 4281 current loss 0.017623, current_train_items 137024.
I0304 19:30:11.826008 22579586809984 run.py:483] Algo bellman_ford step 4282 current loss 0.024208, current_train_items 137056.
I0304 19:30:11.856367 22579586809984 run.py:483] Algo bellman_ford step 4283 current loss 0.042647, current_train_items 137088.
I0304 19:30:11.891820 22579586809984 run.py:483] Algo bellman_ford step 4284 current loss 0.121195, current_train_items 137120.
I0304 19:30:11.911635 22579586809984 run.py:483] Algo bellman_ford step 4285 current loss 0.002889, current_train_items 137152.
I0304 19:30:11.927952 22579586809984 run.py:483] Algo bellman_ford step 4286 current loss 0.005809, current_train_items 137184.
I0304 19:30:11.951279 22579586809984 run.py:483] Algo bellman_ford step 4287 current loss 0.088284, current_train_items 137216.
I0304 19:30:11.983908 22579586809984 run.py:483] Algo bellman_ford step 4288 current loss 0.059138, current_train_items 137248.
I0304 19:30:12.018048 22579586809984 run.py:483] Algo bellman_ford step 4289 current loss 0.151172, current_train_items 137280.
I0304 19:30:12.037817 22579586809984 run.py:483] Algo bellman_ford step 4290 current loss 0.011216, current_train_items 137312.
I0304 19:30:12.054625 22579586809984 run.py:483] Algo bellman_ford step 4291 current loss 0.034692, current_train_items 137344.
I0304 19:30:12.079303 22579586809984 run.py:483] Algo bellman_ford step 4292 current loss 0.052704, current_train_items 137376.
I0304 19:30:12.112453 22579586809984 run.py:483] Algo bellman_ford step 4293 current loss 0.086766, current_train_items 137408.
I0304 19:30:12.148003 22579586809984 run.py:483] Algo bellman_ford step 4294 current loss 0.085345, current_train_items 137440.
I0304 19:30:12.167634 22579586809984 run.py:483] Algo bellman_ford step 4295 current loss 0.003802, current_train_items 137472.
I0304 19:30:12.183745 22579586809984 run.py:483] Algo bellman_ford step 4296 current loss 0.014934, current_train_items 137504.
I0304 19:30:12.207134 22579586809984 run.py:483] Algo bellman_ford step 4297 current loss 0.072540, current_train_items 137536.
I0304 19:30:12.239281 22579586809984 run.py:483] Algo bellman_ford step 4298 current loss 0.067132, current_train_items 137568.
I0304 19:30:12.269608 22579586809984 run.py:483] Algo bellman_ford step 4299 current loss 0.040397, current_train_items 137600.
I0304 19:30:12.289418 22579586809984 run.py:483] Algo bellman_ford step 4300 current loss 0.003381, current_train_items 137632.
I0304 19:30:12.297118 22579586809984 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0304 19:30:12.297223 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.993, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0304 19:30:12.325825 22579586809984 run.py:483] Algo bellman_ford step 4301 current loss 0.024248, current_train_items 137664.
I0304 19:30:12.350079 22579586809984 run.py:483] Algo bellman_ford step 4302 current loss 0.064371, current_train_items 137696.
I0304 19:30:12.383094 22579586809984 run.py:483] Algo bellman_ford step 4303 current loss 0.237965, current_train_items 137728.
I0304 19:30:12.419319 22579586809984 run.py:483] Algo bellman_ford step 4304 current loss 0.057412, current_train_items 137760.
I0304 19:30:12.439461 22579586809984 run.py:483] Algo bellman_ford step 4305 current loss 0.003907, current_train_items 137792.
I0304 19:30:12.455836 22579586809984 run.py:483] Algo bellman_ford step 4306 current loss 0.013559, current_train_items 137824.
I0304 19:30:12.481306 22579586809984 run.py:483] Algo bellman_ford step 4307 current loss 0.082953, current_train_items 137856.
I0304 19:30:12.513531 22579586809984 run.py:483] Algo bellman_ford step 4308 current loss 0.166707, current_train_items 137888.
I0304 19:30:12.549424 22579586809984 run.py:483] Algo bellman_ford step 4309 current loss 0.095636, current_train_items 137920.
I0304 19:30:12.569071 22579586809984 run.py:483] Algo bellman_ford step 4310 current loss 0.010932, current_train_items 137952.
I0304 19:30:12.585334 22579586809984 run.py:483] Algo bellman_ford step 4311 current loss 0.026340, current_train_items 137984.
I0304 19:30:12.608098 22579586809984 run.py:483] Algo bellman_ford step 4312 current loss 0.055173, current_train_items 138016.
I0304 19:30:12.639878 22579586809984 run.py:483] Algo bellman_ford step 4313 current loss 0.045193, current_train_items 138048.
I0304 19:30:12.674130 22579586809984 run.py:483] Algo bellman_ford step 4314 current loss 0.087031, current_train_items 138080.
I0304 19:30:12.693671 22579586809984 run.py:483] Algo bellman_ford step 4315 current loss 0.003314, current_train_items 138112.
I0304 19:30:12.709699 22579586809984 run.py:483] Algo bellman_ford step 4316 current loss 0.070677, current_train_items 138144.
I0304 19:30:12.733962 22579586809984 run.py:483] Algo bellman_ford step 4317 current loss 0.071846, current_train_items 138176.
I0304 19:30:12.764430 22579586809984 run.py:483] Algo bellman_ford step 4318 current loss 0.098561, current_train_items 138208.
I0304 19:30:12.798065 22579586809984 run.py:483] Algo bellman_ford step 4319 current loss 0.070871, current_train_items 138240.
I0304 19:30:12.817495 22579586809984 run.py:483] Algo bellman_ford step 4320 current loss 0.003312, current_train_items 138272.
I0304 19:30:12.833731 22579586809984 run.py:483] Algo bellman_ford step 4321 current loss 0.058843, current_train_items 138304.
I0304 19:30:12.857603 22579586809984 run.py:483] Algo bellman_ford step 4322 current loss 0.100531, current_train_items 138336.
I0304 19:30:12.887426 22579586809984 run.py:483] Algo bellman_ford step 4323 current loss 0.089918, current_train_items 138368.
I0304 19:30:12.921702 22579586809984 run.py:483] Algo bellman_ford step 4324 current loss 0.082201, current_train_items 138400.
I0304 19:30:12.941146 22579586809984 run.py:483] Algo bellman_ford step 4325 current loss 0.005228, current_train_items 138432.
I0304 19:30:12.957465 22579586809984 run.py:483] Algo bellman_ford step 4326 current loss 0.022270, current_train_items 138464.
I0304 19:30:12.981306 22579586809984 run.py:483] Algo bellman_ford step 4327 current loss 0.064011, current_train_items 138496.
I0304 19:30:13.013913 22579586809984 run.py:483] Algo bellman_ford step 4328 current loss 0.127606, current_train_items 138528.
I0304 19:30:13.046435 22579586809984 run.py:483] Algo bellman_ford step 4329 current loss 0.093550, current_train_items 138560.
I0304 19:30:13.066062 22579586809984 run.py:483] Algo bellman_ford step 4330 current loss 0.003662, current_train_items 138592.
I0304 19:30:13.082741 22579586809984 run.py:483] Algo bellman_ford step 4331 current loss 0.011871, current_train_items 138624.
I0304 19:30:13.107048 22579586809984 run.py:483] Algo bellman_ford step 4332 current loss 0.046505, current_train_items 138656.
I0304 19:30:13.137602 22579586809984 run.py:483] Algo bellman_ford step 4333 current loss 0.048473, current_train_items 138688.
I0304 19:30:13.169228 22579586809984 run.py:483] Algo bellman_ford step 4334 current loss 0.057564, current_train_items 138720.
I0304 19:30:13.188748 22579586809984 run.py:483] Algo bellman_ford step 4335 current loss 0.006107, current_train_items 138752.
I0304 19:30:13.204842 22579586809984 run.py:483] Algo bellman_ford step 4336 current loss 0.014156, current_train_items 138784.
I0304 19:30:13.228873 22579586809984 run.py:483] Algo bellman_ford step 4337 current loss 0.064403, current_train_items 138816.
I0304 19:30:13.261337 22579586809984 run.py:483] Algo bellman_ford step 4338 current loss 0.058122, current_train_items 138848.
I0304 19:30:13.294009 22579586809984 run.py:483] Algo bellman_ford step 4339 current loss 0.075261, current_train_items 138880.
I0304 19:30:13.313533 22579586809984 run.py:483] Algo bellman_ford step 4340 current loss 0.006994, current_train_items 138912.
I0304 19:30:13.329546 22579586809984 run.py:483] Algo bellman_ford step 4341 current loss 0.008176, current_train_items 138944.
I0304 19:30:13.353157 22579586809984 run.py:483] Algo bellman_ford step 4342 current loss 0.038222, current_train_items 138976.
I0304 19:30:13.384458 22579586809984 run.py:483] Algo bellman_ford step 4343 current loss 0.077556, current_train_items 139008.
I0304 19:30:13.418756 22579586809984 run.py:483] Algo bellman_ford step 4344 current loss 0.067931, current_train_items 139040.
I0304 19:30:13.438400 22579586809984 run.py:483] Algo bellman_ford step 4345 current loss 0.010629, current_train_items 139072.
I0304 19:30:13.454894 22579586809984 run.py:483] Algo bellman_ford step 4346 current loss 0.011642, current_train_items 139104.
I0304 19:30:13.479037 22579586809984 run.py:483] Algo bellman_ford step 4347 current loss 0.145668, current_train_items 139136.
I0304 19:30:13.510361 22579586809984 run.py:483] Algo bellman_ford step 4348 current loss 0.063329, current_train_items 139168.
I0304 19:30:13.543002 22579586809984 run.py:483] Algo bellman_ford step 4349 current loss 0.130136, current_train_items 139200.
I0304 19:30:13.562590 22579586809984 run.py:483] Algo bellman_ford step 4350 current loss 0.011420, current_train_items 139232.
I0304 19:30:13.570611 22579586809984 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0304 19:30:13.570723 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:30:13.587746 22579586809984 run.py:483] Algo bellman_ford step 4351 current loss 0.037518, current_train_items 139264.
I0304 19:30:13.612834 22579586809984 run.py:483] Algo bellman_ford step 4352 current loss 0.045933, current_train_items 139296.
I0304 19:30:13.644094 22579586809984 run.py:483] Algo bellman_ford step 4353 current loss 0.047934, current_train_items 139328.
I0304 19:30:13.678795 22579586809984 run.py:483] Algo bellman_ford step 4354 current loss 0.059216, current_train_items 139360.
I0304 19:30:13.699218 22579586809984 run.py:483] Algo bellman_ford step 4355 current loss 0.003016, current_train_items 139392.
I0304 19:30:13.715481 22579586809984 run.py:483] Algo bellman_ford step 4356 current loss 0.007383, current_train_items 139424.
I0304 19:30:13.740274 22579586809984 run.py:483] Algo bellman_ford step 4357 current loss 0.117914, current_train_items 139456.
I0304 19:30:13.770063 22579586809984 run.py:483] Algo bellman_ford step 4358 current loss 0.061546, current_train_items 139488.
I0304 19:30:13.803172 22579586809984 run.py:483] Algo bellman_ford step 4359 current loss 0.087102, current_train_items 139520.
I0304 19:30:13.823488 22579586809984 run.py:483] Algo bellman_ford step 4360 current loss 0.002922, current_train_items 139552.
I0304 19:30:13.840383 22579586809984 run.py:483] Algo bellman_ford step 4361 current loss 0.039112, current_train_items 139584.
I0304 19:30:13.863390 22579586809984 run.py:483] Algo bellman_ford step 4362 current loss 0.034821, current_train_items 139616.
I0304 19:30:13.895881 22579586809984 run.py:483] Algo bellman_ford step 4363 current loss 0.054578, current_train_items 139648.
I0304 19:30:13.931255 22579586809984 run.py:483] Algo bellman_ford step 4364 current loss 0.072764, current_train_items 139680.
I0304 19:30:13.950636 22579586809984 run.py:483] Algo bellman_ford step 4365 current loss 0.003069, current_train_items 139712.
I0304 19:30:13.967137 22579586809984 run.py:483] Algo bellman_ford step 4366 current loss 0.014277, current_train_items 139744.
I0304 19:30:13.991750 22579586809984 run.py:483] Algo bellman_ford step 4367 current loss 0.019891, current_train_items 139776.
I0304 19:30:14.022182 22579586809984 run.py:483] Algo bellman_ford step 4368 current loss 0.034583, current_train_items 139808.
I0304 19:30:14.056489 22579586809984 run.py:483] Algo bellman_ford step 4369 current loss 0.053074, current_train_items 139840.
I0304 19:30:14.075953 22579586809984 run.py:483] Algo bellman_ford step 4370 current loss 0.002910, current_train_items 139872.
I0304 19:30:14.092396 22579586809984 run.py:483] Algo bellman_ford step 4371 current loss 0.041858, current_train_items 139904.
I0304 19:30:14.115578 22579586809984 run.py:483] Algo bellman_ford step 4372 current loss 0.038007, current_train_items 139936.
I0304 19:30:14.146696 22579586809984 run.py:483] Algo bellman_ford step 4373 current loss 0.048348, current_train_items 139968.
I0304 19:30:14.180144 22579586809984 run.py:483] Algo bellman_ford step 4374 current loss 0.107076, current_train_items 140000.
I0304 19:30:14.200509 22579586809984 run.py:483] Algo bellman_ford step 4375 current loss 0.004572, current_train_items 140032.
I0304 19:30:14.216805 22579586809984 run.py:483] Algo bellman_ford step 4376 current loss 0.008224, current_train_items 140064.
I0304 19:30:14.240569 22579586809984 run.py:483] Algo bellman_ford step 4377 current loss 0.032480, current_train_items 140096.
I0304 19:30:14.271569 22579586809984 run.py:483] Algo bellman_ford step 4378 current loss 0.036123, current_train_items 140128.
I0304 19:30:14.305605 22579586809984 run.py:483] Algo bellman_ford step 4379 current loss 0.059410, current_train_items 140160.
I0304 19:30:14.325331 22579586809984 run.py:483] Algo bellman_ford step 4380 current loss 0.002403, current_train_items 140192.
I0304 19:30:14.341604 22579586809984 run.py:483] Algo bellman_ford step 4381 current loss 0.040556, current_train_items 140224.
I0304 19:30:14.366254 22579586809984 run.py:483] Algo bellman_ford step 4382 current loss 0.050804, current_train_items 140256.
I0304 19:30:14.397725 22579586809984 run.py:483] Algo bellman_ford step 4383 current loss 0.056676, current_train_items 140288.
I0304 19:30:14.431710 22579586809984 run.py:483] Algo bellman_ford step 4384 current loss 0.067654, current_train_items 140320.
I0304 19:30:14.451952 22579586809984 run.py:483] Algo bellman_ford step 4385 current loss 0.013577, current_train_items 140352.
I0304 19:30:14.468248 22579586809984 run.py:483] Algo bellman_ford step 4386 current loss 0.018951, current_train_items 140384.
I0304 19:30:14.492422 22579586809984 run.py:483] Algo bellman_ford step 4387 current loss 0.043614, current_train_items 140416.
I0304 19:30:14.524130 22579586809984 run.py:483] Algo bellman_ford step 4388 current loss 0.045660, current_train_items 140448.
I0304 19:30:14.556228 22579586809984 run.py:483] Algo bellman_ford step 4389 current loss 0.041851, current_train_items 140480.
I0304 19:30:14.576457 22579586809984 run.py:483] Algo bellman_ford step 4390 current loss 0.003472, current_train_items 140512.
I0304 19:30:14.592808 22579586809984 run.py:483] Algo bellman_ford step 4391 current loss 0.009838, current_train_items 140544.
I0304 19:30:14.616479 22579586809984 run.py:483] Algo bellman_ford step 4392 current loss 0.057262, current_train_items 140576.
I0304 19:30:14.648092 22579586809984 run.py:483] Algo bellman_ford step 4393 current loss 0.063525, current_train_items 140608.
I0304 19:30:14.682484 22579586809984 run.py:483] Algo bellman_ford step 4394 current loss 0.116750, current_train_items 140640.
I0304 19:30:14.702080 22579586809984 run.py:483] Algo bellman_ford step 4395 current loss 0.006264, current_train_items 140672.
I0304 19:30:14.718703 22579586809984 run.py:483] Algo bellman_ford step 4396 current loss 0.050271, current_train_items 140704.
I0304 19:30:14.743108 22579586809984 run.py:483] Algo bellman_ford step 4397 current loss 0.065958, current_train_items 140736.
I0304 19:30:14.774623 22579586809984 run.py:483] Algo bellman_ford step 4398 current loss 0.064946, current_train_items 140768.
I0304 19:30:14.807911 22579586809984 run.py:483] Algo bellman_ford step 4399 current loss 0.116883, current_train_items 140800.
I0304 19:30:14.827770 22579586809984 run.py:483] Algo bellman_ford step 4400 current loss 0.002760, current_train_items 140832.
I0304 19:30:14.835312 22579586809984 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0304 19:30:14.835419 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:30:14.852520 22579586809984 run.py:483] Algo bellman_ford step 4401 current loss 0.016079, current_train_items 140864.
I0304 19:30:14.876577 22579586809984 run.py:483] Algo bellman_ford step 4402 current loss 0.106815, current_train_items 140896.
I0304 19:30:14.908040 22579586809984 run.py:483] Algo bellman_ford step 4403 current loss 0.064796, current_train_items 140928.
I0304 19:30:14.946264 22579586809984 run.py:483] Algo bellman_ford step 4404 current loss 0.214803, current_train_items 140960.
I0304 19:30:14.966362 22579586809984 run.py:483] Algo bellman_ford step 4405 current loss 0.006296, current_train_items 140992.
I0304 19:30:14.982464 22579586809984 run.py:483] Algo bellman_ford step 4406 current loss 0.036055, current_train_items 141024.
I0304 19:30:15.006412 22579586809984 run.py:483] Algo bellman_ford step 4407 current loss 0.055953, current_train_items 141056.
I0304 19:30:15.037563 22579586809984 run.py:483] Algo bellman_ford step 4408 current loss 0.066507, current_train_items 141088.
I0304 19:30:15.073333 22579586809984 run.py:483] Algo bellman_ford step 4409 current loss 0.079235, current_train_items 141120.
I0304 19:30:15.092919 22579586809984 run.py:483] Algo bellman_ford step 4410 current loss 0.002741, current_train_items 141152.
I0304 19:30:15.109252 22579586809984 run.py:483] Algo bellman_ford step 4411 current loss 0.023259, current_train_items 141184.
I0304 19:30:15.133291 22579586809984 run.py:483] Algo bellman_ford step 4412 current loss 0.049684, current_train_items 141216.
I0304 19:30:15.165885 22579586809984 run.py:483] Algo bellman_ford step 4413 current loss 0.057378, current_train_items 141248.
I0304 19:30:15.198578 22579586809984 run.py:483] Algo bellman_ford step 4414 current loss 0.030804, current_train_items 141280.
I0304 19:30:15.218080 22579586809984 run.py:483] Algo bellman_ford step 4415 current loss 0.003182, current_train_items 141312.
I0304 19:30:15.234742 22579586809984 run.py:483] Algo bellman_ford step 4416 current loss 0.036304, current_train_items 141344.
I0304 19:30:15.258353 22579586809984 run.py:483] Algo bellman_ford step 4417 current loss 0.062709, current_train_items 141376.
I0304 19:30:15.289367 22579586809984 run.py:483] Algo bellman_ford step 4418 current loss 0.057543, current_train_items 141408.
I0304 19:30:15.322819 22579586809984 run.py:483] Algo bellman_ford step 4419 current loss 0.074543, current_train_items 141440.
I0304 19:30:15.342241 22579586809984 run.py:483] Algo bellman_ford step 4420 current loss 0.004564, current_train_items 141472.
I0304 19:30:15.358240 22579586809984 run.py:483] Algo bellman_ford step 4421 current loss 0.016784, current_train_items 141504.
I0304 19:30:15.381948 22579586809984 run.py:483] Algo bellman_ford step 4422 current loss 0.087643, current_train_items 141536.
I0304 19:30:15.413080 22579586809984 run.py:483] Algo bellman_ford step 4423 current loss 0.061062, current_train_items 141568.
I0304 19:30:15.447326 22579586809984 run.py:483] Algo bellman_ford step 4424 current loss 0.067082, current_train_items 141600.
I0304 19:30:15.467400 22579586809984 run.py:483] Algo bellman_ford step 4425 current loss 0.019981, current_train_items 141632.
I0304 19:30:15.482817 22579586809984 run.py:483] Algo bellman_ford step 4426 current loss 0.006418, current_train_items 141664.
I0304 19:30:15.507414 22579586809984 run.py:483] Algo bellman_ford step 4427 current loss 0.134299, current_train_items 141696.
I0304 19:30:15.538728 22579586809984 run.py:483] Algo bellman_ford step 4428 current loss 0.065739, current_train_items 141728.
I0304 19:30:15.572262 22579586809984 run.py:483] Algo bellman_ford step 4429 current loss 0.056453, current_train_items 141760.
I0304 19:30:15.591655 22579586809984 run.py:483] Algo bellman_ford step 4430 current loss 0.004157, current_train_items 141792.
I0304 19:30:15.607422 22579586809984 run.py:483] Algo bellman_ford step 4431 current loss 0.042010, current_train_items 141824.
I0304 19:30:15.631400 22579586809984 run.py:483] Algo bellman_ford step 4432 current loss 0.046058, current_train_items 141856.
I0304 19:30:15.661917 22579586809984 run.py:483] Algo bellman_ford step 4433 current loss 0.051780, current_train_items 141888.
I0304 19:30:15.695418 22579586809984 run.py:483] Algo bellman_ford step 4434 current loss 0.090447, current_train_items 141920.
I0304 19:30:15.715005 22579586809984 run.py:483] Algo bellman_ford step 4435 current loss 0.003987, current_train_items 141952.
I0304 19:30:15.731380 22579586809984 run.py:483] Algo bellman_ford step 4436 current loss 0.046226, current_train_items 141984.
I0304 19:30:15.755771 22579586809984 run.py:483] Algo bellman_ford step 4437 current loss 0.062096, current_train_items 142016.
I0304 19:30:15.785980 22579586809984 run.py:483] Algo bellman_ford step 4438 current loss 0.025321, current_train_items 142048.
I0304 19:30:15.820526 22579586809984 run.py:483] Algo bellman_ford step 4439 current loss 0.047558, current_train_items 142080.
I0304 19:30:15.839983 22579586809984 run.py:483] Algo bellman_ford step 4440 current loss 0.004911, current_train_items 142112.
I0304 19:30:15.856579 22579586809984 run.py:483] Algo bellman_ford step 4441 current loss 0.046361, current_train_items 142144.
I0304 19:30:15.880134 22579586809984 run.py:483] Algo bellman_ford step 4442 current loss 0.047265, current_train_items 142176.
I0304 19:30:15.910999 22579586809984 run.py:483] Algo bellman_ford step 4443 current loss 0.040643, current_train_items 142208.
I0304 19:30:15.945766 22579586809984 run.py:483] Algo bellman_ford step 4444 current loss 0.060468, current_train_items 142240.
I0304 19:30:15.965487 22579586809984 run.py:483] Algo bellman_ford step 4445 current loss 0.006247, current_train_items 142272.
I0304 19:30:15.981572 22579586809984 run.py:483] Algo bellman_ford step 4446 current loss 0.021492, current_train_items 142304.
I0304 19:30:16.005571 22579586809984 run.py:483] Algo bellman_ford step 4447 current loss 0.037692, current_train_items 142336.
I0304 19:30:16.037577 22579586809984 run.py:483] Algo bellman_ford step 4448 current loss 0.106937, current_train_items 142368.
I0304 19:30:16.071237 22579586809984 run.py:483] Algo bellman_ford step 4449 current loss 0.098389, current_train_items 142400.
I0304 19:30:16.091075 22579586809984 run.py:483] Algo bellman_ford step 4450 current loss 0.039962, current_train_items 142432.
I0304 19:30:16.099057 22579586809984 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0304 19:30:16.099163 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:30:16.115429 22579586809984 run.py:483] Algo bellman_ford step 4451 current loss 0.009337, current_train_items 142464.
I0304 19:30:16.140008 22579586809984 run.py:483] Algo bellman_ford step 4452 current loss 0.021634, current_train_items 142496.
I0304 19:30:16.170783 22579586809984 run.py:483] Algo bellman_ford step 4453 current loss 0.036800, current_train_items 142528.
I0304 19:30:16.203739 22579586809984 run.py:483] Algo bellman_ford step 4454 current loss 0.054077, current_train_items 142560.
I0304 19:30:16.223861 22579586809984 run.py:483] Algo bellman_ford step 4455 current loss 0.022202, current_train_items 142592.
I0304 19:30:16.239796 22579586809984 run.py:483] Algo bellman_ford step 4456 current loss 0.014593, current_train_items 142624.
I0304 19:30:16.264079 22579586809984 run.py:483] Algo bellman_ford step 4457 current loss 0.137702, current_train_items 142656.
I0304 19:30:16.293974 22579586809984 run.py:483] Algo bellman_ford step 4458 current loss 0.042352, current_train_items 142688.
I0304 19:30:16.331275 22579586809984 run.py:483] Algo bellman_ford step 4459 current loss 0.126146, current_train_items 142720.
I0304 19:30:16.351534 22579586809984 run.py:483] Algo bellman_ford step 4460 current loss 0.021211, current_train_items 142752.
I0304 19:30:16.368050 22579586809984 run.py:483] Algo bellman_ford step 4461 current loss 0.016663, current_train_items 142784.
I0304 19:30:16.393019 22579586809984 run.py:483] Algo bellman_ford step 4462 current loss 0.049393, current_train_items 142816.
I0304 19:30:16.423050 22579586809984 run.py:483] Algo bellman_ford step 4463 current loss 0.046455, current_train_items 142848.
I0304 19:30:16.457566 22579586809984 run.py:483] Algo bellman_ford step 4464 current loss 0.111188, current_train_items 142880.
I0304 19:30:16.477268 22579586809984 run.py:483] Algo bellman_ford step 4465 current loss 0.002315, current_train_items 142912.
I0304 19:30:16.493716 22579586809984 run.py:483] Algo bellman_ford step 4466 current loss 0.020096, current_train_items 142944.
I0304 19:30:16.517842 22579586809984 run.py:483] Algo bellman_ford step 4467 current loss 0.035791, current_train_items 142976.
I0304 19:30:16.551049 22579586809984 run.py:483] Algo bellman_ford step 4468 current loss 0.042968, current_train_items 143008.
I0304 19:30:16.585578 22579586809984 run.py:483] Algo bellman_ford step 4469 current loss 0.076637, current_train_items 143040.
I0304 19:30:16.605876 22579586809984 run.py:483] Algo bellman_ford step 4470 current loss 0.008396, current_train_items 143072.
I0304 19:30:16.622064 22579586809984 run.py:483] Algo bellman_ford step 4471 current loss 0.004283, current_train_items 143104.
I0304 19:30:16.645528 22579586809984 run.py:483] Algo bellman_ford step 4472 current loss 0.045947, current_train_items 143136.
I0304 19:30:16.676812 22579586809984 run.py:483] Algo bellman_ford step 4473 current loss 0.041972, current_train_items 143168.
I0304 19:30:16.713301 22579586809984 run.py:483] Algo bellman_ford step 4474 current loss 0.086769, current_train_items 143200.
I0304 19:30:16.733462 22579586809984 run.py:483] Algo bellman_ford step 4475 current loss 0.003771, current_train_items 143232.
I0304 19:30:16.749792 22579586809984 run.py:483] Algo bellman_ford step 4476 current loss 0.025099, current_train_items 143264.
I0304 19:30:16.773387 22579586809984 run.py:483] Algo bellman_ford step 4477 current loss 0.045640, current_train_items 143296.
I0304 19:30:16.804936 22579586809984 run.py:483] Algo bellman_ford step 4478 current loss 0.048551, current_train_items 143328.
I0304 19:30:16.840875 22579586809984 run.py:483] Algo bellman_ford step 4479 current loss 0.089409, current_train_items 143360.
I0304 19:30:16.861194 22579586809984 run.py:483] Algo bellman_ford step 4480 current loss 0.002241, current_train_items 143392.
I0304 19:30:16.877463 22579586809984 run.py:483] Algo bellman_ford step 4481 current loss 0.019524, current_train_items 143424.
I0304 19:30:16.901895 22579586809984 run.py:483] Algo bellman_ford step 4482 current loss 0.032627, current_train_items 143456.
I0304 19:30:16.933876 22579586809984 run.py:483] Algo bellman_ford step 4483 current loss 0.073818, current_train_items 143488.
I0304 19:30:16.965241 22579586809984 run.py:483] Algo bellman_ford step 4484 current loss 0.083367, current_train_items 143520.
I0304 19:30:16.985579 22579586809984 run.py:483] Algo bellman_ford step 4485 current loss 0.007147, current_train_items 143552.
I0304 19:30:17.002259 22579586809984 run.py:483] Algo bellman_ford step 4486 current loss 0.012617, current_train_items 143584.
I0304 19:30:17.027506 22579586809984 run.py:483] Algo bellman_ford step 4487 current loss 0.081180, current_train_items 143616.
I0304 19:30:17.059317 22579586809984 run.py:483] Algo bellman_ford step 4488 current loss 0.065513, current_train_items 143648.
I0304 19:30:17.092689 22579586809984 run.py:483] Algo bellman_ford step 4489 current loss 0.087284, current_train_items 143680.
I0304 19:30:17.112988 22579586809984 run.py:483] Algo bellman_ford step 4490 current loss 0.006071, current_train_items 143712.
I0304 19:30:17.129004 22579586809984 run.py:483] Algo bellman_ford step 4491 current loss 0.011705, current_train_items 143744.
I0304 19:30:17.153265 22579586809984 run.py:483] Algo bellman_ford step 4492 current loss 0.069180, current_train_items 143776.
I0304 19:30:17.185145 22579586809984 run.py:483] Algo bellman_ford step 4493 current loss 0.090041, current_train_items 143808.
I0304 19:30:17.219639 22579586809984 run.py:483] Algo bellman_ford step 4494 current loss 0.135512, current_train_items 143840.
I0304 19:30:17.239531 22579586809984 run.py:483] Algo bellman_ford step 4495 current loss 0.016620, current_train_items 143872.
I0304 19:30:17.256281 22579586809984 run.py:483] Algo bellman_ford step 4496 current loss 0.029175, current_train_items 143904.
I0304 19:30:17.280660 22579586809984 run.py:483] Algo bellman_ford step 4497 current loss 0.057651, current_train_items 143936.
I0304 19:30:17.310734 22579586809984 run.py:483] Algo bellman_ford step 4498 current loss 0.048866, current_train_items 143968.
I0304 19:30:17.342692 22579586809984 run.py:483] Algo bellman_ford step 4499 current loss 0.048883, current_train_items 144000.
I0304 19:30:17.362656 22579586809984 run.py:483] Algo bellman_ford step 4500 current loss 0.005731, current_train_items 144032.
I0304 19:30:17.370188 22579586809984 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0304 19:30:17.370293 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:17.387224 22579586809984 run.py:483] Algo bellman_ford step 4501 current loss 0.031881, current_train_items 144064.
I0304 19:30:17.412382 22579586809984 run.py:483] Algo bellman_ford step 4502 current loss 0.045208, current_train_items 144096.
I0304 19:30:17.443912 22579586809984 run.py:483] Algo bellman_ford step 4503 current loss 0.084508, current_train_items 144128.
I0304 19:30:17.478330 22579586809984 run.py:483] Algo bellman_ford step 4504 current loss 0.031447, current_train_items 144160.
I0304 19:30:17.498284 22579586809984 run.py:483] Algo bellman_ford step 4505 current loss 0.005983, current_train_items 144192.
I0304 19:30:17.514491 22579586809984 run.py:483] Algo bellman_ford step 4506 current loss 0.032102, current_train_items 144224.
I0304 19:30:17.537847 22579586809984 run.py:483] Algo bellman_ford step 4507 current loss 0.053051, current_train_items 144256.
I0304 19:30:17.568753 22579586809984 run.py:483] Algo bellman_ford step 4508 current loss 0.058542, current_train_items 144288.
I0304 19:30:17.601696 22579586809984 run.py:483] Algo bellman_ford step 4509 current loss 0.066116, current_train_items 144320.
I0304 19:30:17.621276 22579586809984 run.py:483] Algo bellman_ford step 4510 current loss 0.058369, current_train_items 144352.
I0304 19:30:17.637740 22579586809984 run.py:483] Algo bellman_ford step 4511 current loss 0.082906, current_train_items 144384.
I0304 19:30:17.661719 22579586809984 run.py:483] Algo bellman_ford step 4512 current loss 0.055796, current_train_items 144416.
I0304 19:30:17.692825 22579586809984 run.py:483] Algo bellman_ford step 4513 current loss 0.108077, current_train_items 144448.
I0304 19:30:17.725711 22579586809984 run.py:483] Algo bellman_ford step 4514 current loss 0.056008, current_train_items 144480.
I0304 19:30:17.745390 22579586809984 run.py:483] Algo bellman_ford step 4515 current loss 0.006216, current_train_items 144512.
I0304 19:30:17.761912 22579586809984 run.py:483] Algo bellman_ford step 4516 current loss 0.014092, current_train_items 144544.
I0304 19:30:17.784793 22579586809984 run.py:483] Algo bellman_ford step 4517 current loss 0.080284, current_train_items 144576.
I0304 19:30:17.816867 22579586809984 run.py:483] Algo bellman_ford step 4518 current loss 0.040261, current_train_items 144608.
I0304 19:30:17.854093 22579586809984 run.py:483] Algo bellman_ford step 4519 current loss 0.081777, current_train_items 144640.
I0304 19:30:17.873611 22579586809984 run.py:483] Algo bellman_ford step 4520 current loss 0.004633, current_train_items 144672.
I0304 19:30:17.890083 22579586809984 run.py:483] Algo bellman_ford step 4521 current loss 0.011177, current_train_items 144704.
I0304 19:30:17.914009 22579586809984 run.py:483] Algo bellman_ford step 4522 current loss 0.028907, current_train_items 144736.
I0304 19:30:17.946988 22579586809984 run.py:483] Algo bellman_ford step 4523 current loss 0.088909, current_train_items 144768.
I0304 19:30:17.980284 22579586809984 run.py:483] Algo bellman_ford step 4524 current loss 0.059710, current_train_items 144800.
I0304 19:30:17.999993 22579586809984 run.py:483] Algo bellman_ford step 4525 current loss 0.003287, current_train_items 144832.
I0304 19:30:18.016284 22579586809984 run.py:483] Algo bellman_ford step 4526 current loss 0.010697, current_train_items 144864.
I0304 19:30:18.040848 22579586809984 run.py:483] Algo bellman_ford step 4527 current loss 0.078262, current_train_items 144896.
I0304 19:30:18.072244 22579586809984 run.py:483] Algo bellman_ford step 4528 current loss 0.041475, current_train_items 144928.
I0304 19:30:18.106448 22579586809984 run.py:483] Algo bellman_ford step 4529 current loss 0.066155, current_train_items 144960.
I0304 19:30:18.126102 22579586809984 run.py:483] Algo bellman_ford step 4530 current loss 0.028093, current_train_items 144992.
I0304 19:30:18.142165 22579586809984 run.py:483] Algo bellman_ford step 4531 current loss 0.012227, current_train_items 145024.
I0304 19:30:18.165157 22579586809984 run.py:483] Algo bellman_ford step 4532 current loss 0.023528, current_train_items 145056.
I0304 19:30:18.195362 22579586809984 run.py:483] Algo bellman_ford step 4533 current loss 0.047837, current_train_items 145088.
I0304 19:30:18.230201 22579586809984 run.py:483] Algo bellman_ford step 4534 current loss 0.114519, current_train_items 145120.
I0304 19:30:18.249660 22579586809984 run.py:483] Algo bellman_ford step 4535 current loss 0.002847, current_train_items 145152.
I0304 19:30:18.265908 22579586809984 run.py:483] Algo bellman_ford step 4536 current loss 0.012968, current_train_items 145184.
I0304 19:30:18.289862 22579586809984 run.py:483] Algo bellman_ford step 4537 current loss 0.036373, current_train_items 145216.
I0304 19:30:18.320932 22579586809984 run.py:483] Algo bellman_ford step 4538 current loss 0.028637, current_train_items 145248.
I0304 19:30:18.356106 22579586809984 run.py:483] Algo bellman_ford step 4539 current loss 0.116131, current_train_items 145280.
I0304 19:30:18.375600 22579586809984 run.py:483] Algo bellman_ford step 4540 current loss 0.013434, current_train_items 145312.
I0304 19:30:18.391795 22579586809984 run.py:483] Algo bellman_ford step 4541 current loss 0.016348, current_train_items 145344.
I0304 19:30:18.414926 22579586809984 run.py:483] Algo bellman_ford step 4542 current loss 0.031988, current_train_items 145376.
I0304 19:30:18.446076 22579586809984 run.py:483] Algo bellman_ford step 4543 current loss 0.056838, current_train_items 145408.
I0304 19:30:18.480489 22579586809984 run.py:483] Algo bellman_ford step 4544 current loss 0.084284, current_train_items 145440.
I0304 19:30:18.499960 22579586809984 run.py:483] Algo bellman_ford step 4545 current loss 0.002957, current_train_items 145472.
I0304 19:30:18.516189 22579586809984 run.py:483] Algo bellman_ford step 4546 current loss 0.012827, current_train_items 145504.
I0304 19:30:18.539638 22579586809984 run.py:483] Algo bellman_ford step 4547 current loss 0.018413, current_train_items 145536.
I0304 19:30:18.569756 22579586809984 run.py:483] Algo bellman_ford step 4548 current loss 0.035975, current_train_items 145568.
I0304 19:30:18.604379 22579586809984 run.py:483] Algo bellman_ford step 4549 current loss 0.061839, current_train_items 145600.
I0304 19:30:18.623986 22579586809984 run.py:483] Algo bellman_ford step 4550 current loss 0.005081, current_train_items 145632.
I0304 19:30:18.631714 22579586809984 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0304 19:30:18.631819 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:30:18.649181 22579586809984 run.py:483] Algo bellman_ford step 4551 current loss 0.025474, current_train_items 145664.
I0304 19:30:18.673850 22579586809984 run.py:483] Algo bellman_ford step 4552 current loss 0.046372, current_train_items 145696.
I0304 19:30:18.706415 22579586809984 run.py:483] Algo bellman_ford step 4553 current loss 0.059705, current_train_items 145728.
I0304 19:30:18.740786 22579586809984 run.py:483] Algo bellman_ford step 4554 current loss 0.046968, current_train_items 145760.
I0304 19:30:18.760952 22579586809984 run.py:483] Algo bellman_ford step 4555 current loss 0.010302, current_train_items 145792.
I0304 19:30:18.776777 22579586809984 run.py:483] Algo bellman_ford step 4556 current loss 0.054685, current_train_items 145824.
I0304 19:30:18.800104 22579586809984 run.py:483] Algo bellman_ford step 4557 current loss 0.031779, current_train_items 145856.
I0304 19:30:18.832438 22579586809984 run.py:483] Algo bellman_ford step 4558 current loss 0.081074, current_train_items 145888.
I0304 19:30:18.867573 22579586809984 run.py:483] Algo bellman_ford step 4559 current loss 0.101612, current_train_items 145920.
I0304 19:30:18.887391 22579586809984 run.py:483] Algo bellman_ford step 4560 current loss 0.003951, current_train_items 145952.
I0304 19:30:18.904326 22579586809984 run.py:483] Algo bellman_ford step 4561 current loss 0.024193, current_train_items 145984.
I0304 19:30:18.927637 22579586809984 run.py:483] Algo bellman_ford step 4562 current loss 0.052849, current_train_items 146016.
I0304 19:30:18.958507 22579586809984 run.py:483] Algo bellman_ford step 4563 current loss 0.080541, current_train_items 146048.
I0304 19:30:18.994310 22579586809984 run.py:483] Algo bellman_ford step 4564 current loss 0.078690, current_train_items 146080.
I0304 19:30:19.014245 22579586809984 run.py:483] Algo bellman_ford step 4565 current loss 0.005260, current_train_items 146112.
I0304 19:30:19.030538 22579586809984 run.py:483] Algo bellman_ford step 4566 current loss 0.018720, current_train_items 146144.
I0304 19:30:19.055142 22579586809984 run.py:483] Algo bellman_ford step 4567 current loss 0.056803, current_train_items 146176.
I0304 19:30:19.087463 22579586809984 run.py:483] Algo bellman_ford step 4568 current loss 0.115283, current_train_items 146208.
I0304 19:30:19.120467 22579586809984 run.py:483] Algo bellman_ford step 4569 current loss 0.102177, current_train_items 146240.
I0304 19:30:19.140421 22579586809984 run.py:483] Algo bellman_ford step 4570 current loss 0.005440, current_train_items 146272.
I0304 19:30:19.156614 22579586809984 run.py:483] Algo bellman_ford step 4571 current loss 0.024616, current_train_items 146304.
I0304 19:30:19.180820 22579586809984 run.py:483] Algo bellman_ford step 4572 current loss 0.118043, current_train_items 146336.
I0304 19:30:19.211251 22579586809984 run.py:483] Algo bellman_ford step 4573 current loss 0.062803, current_train_items 146368.
I0304 19:30:19.244264 22579586809984 run.py:483] Algo bellman_ford step 4574 current loss 0.079018, current_train_items 146400.
I0304 19:30:19.264475 22579586809984 run.py:483] Algo bellman_ford step 4575 current loss 0.003017, current_train_items 146432.
I0304 19:30:19.280784 22579586809984 run.py:483] Algo bellman_ford step 4576 current loss 0.028840, current_train_items 146464.
I0304 19:30:19.304192 22579586809984 run.py:483] Algo bellman_ford step 4577 current loss 0.052819, current_train_items 146496.
I0304 19:30:19.336127 22579586809984 run.py:483] Algo bellman_ford step 4578 current loss 0.057132, current_train_items 146528.
I0304 19:30:19.370378 22579586809984 run.py:483] Algo bellman_ford step 4579 current loss 0.087917, current_train_items 146560.
I0304 19:30:19.389853 22579586809984 run.py:483] Algo bellman_ford step 4580 current loss 0.023193, current_train_items 146592.
I0304 19:30:19.406551 22579586809984 run.py:483] Algo bellman_ford step 4581 current loss 0.030892, current_train_items 146624.
I0304 19:30:19.430280 22579586809984 run.py:483] Algo bellman_ford step 4582 current loss 0.049873, current_train_items 146656.
I0304 19:30:19.461617 22579586809984 run.py:483] Algo bellman_ford step 4583 current loss 0.036635, current_train_items 146688.
I0304 19:30:19.495834 22579586809984 run.py:483] Algo bellman_ford step 4584 current loss 0.083439, current_train_items 146720.
I0304 19:30:19.516167 22579586809984 run.py:483] Algo bellman_ford step 4585 current loss 0.005614, current_train_items 146752.
I0304 19:30:19.532807 22579586809984 run.py:483] Algo bellman_ford step 4586 current loss 0.022970, current_train_items 146784.
I0304 19:30:19.556130 22579586809984 run.py:483] Algo bellman_ford step 4587 current loss 0.044334, current_train_items 146816.
I0304 19:30:19.587511 22579586809984 run.py:483] Algo bellman_ford step 4588 current loss 0.043903, current_train_items 146848.
I0304 19:30:19.620533 22579586809984 run.py:483] Algo bellman_ford step 4589 current loss 0.070693, current_train_items 146880.
I0304 19:30:19.640702 22579586809984 run.py:483] Algo bellman_ford step 4590 current loss 0.008920, current_train_items 146912.
I0304 19:30:19.657078 22579586809984 run.py:483] Algo bellman_ford step 4591 current loss 0.021736, current_train_items 146944.
I0304 19:30:19.680639 22579586809984 run.py:483] Algo bellman_ford step 4592 current loss 0.047547, current_train_items 146976.
I0304 19:30:19.712708 22579586809984 run.py:483] Algo bellman_ford step 4593 current loss 0.035641, current_train_items 147008.
I0304 19:30:19.747378 22579586809984 run.py:483] Algo bellman_ford step 4594 current loss 0.109466, current_train_items 147040.
I0304 19:30:19.767373 22579586809984 run.py:483] Algo bellman_ford step 4595 current loss 0.004832, current_train_items 147072.
I0304 19:30:19.783532 22579586809984 run.py:483] Algo bellman_ford step 4596 current loss 0.016982, current_train_items 147104.
I0304 19:30:19.809336 22579586809984 run.py:483] Algo bellman_ford step 4597 current loss 0.093134, current_train_items 147136.
I0304 19:30:19.840643 22579586809984 run.py:483] Algo bellman_ford step 4598 current loss 0.054978, current_train_items 147168.
I0304 19:30:19.872937 22579586809984 run.py:483] Algo bellman_ford step 4599 current loss 0.060234, current_train_items 147200.
I0304 19:30:19.893262 22579586809984 run.py:483] Algo bellman_ford step 4600 current loss 0.005321, current_train_items 147232.
I0304 19:30:19.900905 22579586809984 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0304 19:30:19.901011 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:30:19.918360 22579586809984 run.py:483] Algo bellman_ford step 4601 current loss 0.028838, current_train_items 147264.
I0304 19:30:19.943431 22579586809984 run.py:483] Algo bellman_ford step 4602 current loss 0.039194, current_train_items 147296.
I0304 19:30:19.975073 22579586809984 run.py:483] Algo bellman_ford step 4603 current loss 0.089975, current_train_items 147328.
I0304 19:30:20.009641 22579586809984 run.py:483] Algo bellman_ford step 4604 current loss 0.085024, current_train_items 147360.
I0304 19:30:20.029643 22579586809984 run.py:483] Algo bellman_ford step 4605 current loss 0.004985, current_train_items 147392.
I0304 19:30:20.046052 22579586809984 run.py:483] Algo bellman_ford step 4606 current loss 0.035780, current_train_items 147424.
I0304 19:30:20.070006 22579586809984 run.py:483] Algo bellman_ford step 4607 current loss 0.063992, current_train_items 147456.
I0304 19:30:20.101992 22579586809984 run.py:483] Algo bellman_ford step 4608 current loss 0.030056, current_train_items 147488.
I0304 19:30:20.133987 22579586809984 run.py:483] Algo bellman_ford step 4609 current loss 0.052069, current_train_items 147520.
I0304 19:30:20.153828 22579586809984 run.py:483] Algo bellman_ford step 4610 current loss 0.004301, current_train_items 147552.
I0304 19:30:20.169970 22579586809984 run.py:483] Algo bellman_ford step 4611 current loss 0.016326, current_train_items 147584.
I0304 19:30:20.194187 22579586809984 run.py:483] Algo bellman_ford step 4612 current loss 0.056892, current_train_items 147616.
I0304 19:30:20.226053 22579586809984 run.py:483] Algo bellman_ford step 4613 current loss 0.052244, current_train_items 147648.
I0304 19:30:20.260593 22579586809984 run.py:483] Algo bellman_ford step 4614 current loss 0.038847, current_train_items 147680.
I0304 19:30:20.280272 22579586809984 run.py:483] Algo bellman_ford step 4615 current loss 0.012332, current_train_items 147712.
I0304 19:30:20.296842 22579586809984 run.py:483] Algo bellman_ford step 4616 current loss 0.039433, current_train_items 147744.
I0304 19:30:20.321511 22579586809984 run.py:483] Algo bellman_ford step 4617 current loss 0.030922, current_train_items 147776.
I0304 19:30:20.351992 22579586809984 run.py:483] Algo bellman_ford step 4618 current loss 0.036151, current_train_items 147808.
I0304 19:30:20.385582 22579586809984 run.py:483] Algo bellman_ford step 4619 current loss 0.063478, current_train_items 147840.
I0304 19:30:20.405069 22579586809984 run.py:483] Algo bellman_ford step 4620 current loss 0.010669, current_train_items 147872.
I0304 19:30:20.421213 22579586809984 run.py:483] Algo bellman_ford step 4621 current loss 0.019800, current_train_items 147904.
I0304 19:30:20.444923 22579586809984 run.py:483] Algo bellman_ford step 4622 current loss 0.057535, current_train_items 147936.
I0304 19:30:20.476220 22579586809984 run.py:483] Algo bellman_ford step 4623 current loss 0.057167, current_train_items 147968.
I0304 19:30:20.509060 22579586809984 run.py:483] Algo bellman_ford step 4624 current loss 0.169161, current_train_items 148000.
I0304 19:30:20.528512 22579586809984 run.py:483] Algo bellman_ford step 4625 current loss 0.005366, current_train_items 148032.
I0304 19:30:20.544990 22579586809984 run.py:483] Algo bellman_ford step 4626 current loss 0.031640, current_train_items 148064.
I0304 19:30:20.569239 22579586809984 run.py:483] Algo bellman_ford step 4627 current loss 0.062594, current_train_items 148096.
I0304 19:30:20.601073 22579586809984 run.py:483] Algo bellman_ford step 4628 current loss 0.078996, current_train_items 148128.
I0304 19:30:20.636596 22579586809984 run.py:483] Algo bellman_ford step 4629 current loss 0.067848, current_train_items 148160.
I0304 19:30:20.656019 22579586809984 run.py:483] Algo bellman_ford step 4630 current loss 0.002305, current_train_items 148192.
I0304 19:30:20.672775 22579586809984 run.py:483] Algo bellman_ford step 4631 current loss 0.035300, current_train_items 148224.
I0304 19:30:20.697525 22579586809984 run.py:483] Algo bellman_ford step 4632 current loss 0.070761, current_train_items 148256.
I0304 19:30:20.727497 22579586809984 run.py:483] Algo bellman_ford step 4633 current loss 0.045286, current_train_items 148288.
I0304 19:30:20.761729 22579586809984 run.py:483] Algo bellman_ford step 4634 current loss 0.069126, current_train_items 148320.
I0304 19:30:20.781318 22579586809984 run.py:483] Algo bellman_ford step 4635 current loss 0.003857, current_train_items 148352.
I0304 19:30:20.798103 22579586809984 run.py:483] Algo bellman_ford step 4636 current loss 0.010658, current_train_items 148384.
I0304 19:30:20.822867 22579586809984 run.py:483] Algo bellman_ford step 4637 current loss 0.066106, current_train_items 148416.
I0304 19:30:20.855560 22579586809984 run.py:483] Algo bellman_ford step 4638 current loss 0.069298, current_train_items 148448.
I0304 19:30:20.889892 22579586809984 run.py:483] Algo bellman_ford step 4639 current loss 0.061108, current_train_items 148480.
I0304 19:30:20.909412 22579586809984 run.py:483] Algo bellman_ford step 4640 current loss 0.005875, current_train_items 148512.
I0304 19:30:20.925879 22579586809984 run.py:483] Algo bellman_ford step 4641 current loss 0.028431, current_train_items 148544.
I0304 19:30:20.949982 22579586809984 run.py:483] Algo bellman_ford step 4642 current loss 0.163441, current_train_items 148576.
I0304 19:30:20.981641 22579586809984 run.py:483] Algo bellman_ford step 4643 current loss 0.212735, current_train_items 148608.
I0304 19:30:21.016298 22579586809984 run.py:483] Algo bellman_ford step 4644 current loss 0.098448, current_train_items 148640.
I0304 19:30:21.035833 22579586809984 run.py:483] Algo bellman_ford step 4645 current loss 0.005111, current_train_items 148672.
I0304 19:30:21.052157 22579586809984 run.py:483] Algo bellman_ford step 4646 current loss 0.010846, current_train_items 148704.
I0304 19:30:21.075185 22579586809984 run.py:483] Algo bellman_ford step 4647 current loss 0.042956, current_train_items 148736.
I0304 19:30:21.104592 22579586809984 run.py:483] Algo bellman_ford step 4648 current loss 0.061543, current_train_items 148768.
I0304 19:30:21.136296 22579586809984 run.py:483] Algo bellman_ford step 4649 current loss 0.056453, current_train_items 148800.
I0304 19:30:21.155888 22579586809984 run.py:483] Algo bellman_ford step 4650 current loss 0.003998, current_train_items 148832.
I0304 19:30:21.163807 22579586809984 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0304 19:30:21.163913 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:30:21.181069 22579586809984 run.py:483] Algo bellman_ford step 4651 current loss 0.024369, current_train_items 148864.
I0304 19:30:21.205463 22579586809984 run.py:483] Algo bellman_ford step 4652 current loss 0.035812, current_train_items 148896.
I0304 19:30:21.238136 22579586809984 run.py:483] Algo bellman_ford step 4653 current loss 0.068437, current_train_items 148928.
I0304 19:30:21.272156 22579586809984 run.py:483] Algo bellman_ford step 4654 current loss 0.075418, current_train_items 148960.
I0304 19:30:21.292237 22579586809984 run.py:483] Algo bellman_ford step 4655 current loss 0.012204, current_train_items 148992.
I0304 19:30:21.308269 22579586809984 run.py:483] Algo bellman_ford step 4656 current loss 0.009780, current_train_items 149024.
I0304 19:30:21.332815 22579586809984 run.py:483] Algo bellman_ford step 4657 current loss 0.077048, current_train_items 149056.
I0304 19:30:21.363636 22579586809984 run.py:483] Algo bellman_ford step 4658 current loss 0.085763, current_train_items 149088.
I0304 19:30:21.396229 22579586809984 run.py:483] Algo bellman_ford step 4659 current loss 0.106024, current_train_items 149120.
I0304 19:30:21.416053 22579586809984 run.py:483] Algo bellman_ford step 4660 current loss 0.005133, current_train_items 149152.
I0304 19:30:21.432570 22579586809984 run.py:483] Algo bellman_ford step 4661 current loss 0.019704, current_train_items 149184.
I0304 19:30:21.456999 22579586809984 run.py:483] Algo bellman_ford step 4662 current loss 0.090980, current_train_items 149216.
I0304 19:30:21.487758 22579586809984 run.py:483] Algo bellman_ford step 4663 current loss 0.055873, current_train_items 149248.
I0304 19:30:21.524843 22579586809984 run.py:483] Algo bellman_ford step 4664 current loss 0.116132, current_train_items 149280.
I0304 19:30:21.544549 22579586809984 run.py:483] Algo bellman_ford step 4665 current loss 0.007594, current_train_items 149312.
I0304 19:30:21.560554 22579586809984 run.py:483] Algo bellman_ford step 4666 current loss 0.018283, current_train_items 149344.
I0304 19:30:21.584036 22579586809984 run.py:483] Algo bellman_ford step 4667 current loss 0.036424, current_train_items 149376.
I0304 19:30:21.616000 22579586809984 run.py:483] Algo bellman_ford step 4668 current loss 0.046878, current_train_items 149408.
I0304 19:30:21.650267 22579586809984 run.py:483] Algo bellman_ford step 4669 current loss 0.060354, current_train_items 149440.
I0304 19:30:21.670139 22579586809984 run.py:483] Algo bellman_ford step 4670 current loss 0.004226, current_train_items 149472.
I0304 19:30:21.686113 22579586809984 run.py:483] Algo bellman_ford step 4671 current loss 0.013071, current_train_items 149504.
I0304 19:30:21.708981 22579586809984 run.py:483] Algo bellman_ford step 4672 current loss 0.050553, current_train_items 149536.
I0304 19:30:21.739026 22579586809984 run.py:483] Algo bellman_ford step 4673 current loss 0.071767, current_train_items 149568.
I0304 19:30:21.772592 22579586809984 run.py:483] Algo bellman_ford step 4674 current loss 0.047846, current_train_items 149600.
I0304 19:30:21.792634 22579586809984 run.py:483] Algo bellman_ford step 4675 current loss 0.020493, current_train_items 149632.
I0304 19:30:21.809413 22579586809984 run.py:483] Algo bellman_ford step 4676 current loss 0.037185, current_train_items 149664.
I0304 19:30:21.833724 22579586809984 run.py:483] Algo bellman_ford step 4677 current loss 0.069994, current_train_items 149696.
I0304 19:30:21.866355 22579586809984 run.py:483] Algo bellman_ford step 4678 current loss 0.058369, current_train_items 149728.
I0304 19:30:21.900317 22579586809984 run.py:483] Algo bellman_ford step 4679 current loss 0.060202, current_train_items 149760.
I0304 19:30:21.920315 22579586809984 run.py:483] Algo bellman_ford step 4680 current loss 0.003550, current_train_items 149792.
I0304 19:30:21.936270 22579586809984 run.py:483] Algo bellman_ford step 4681 current loss 0.011248, current_train_items 149824.
I0304 19:30:21.959696 22579586809984 run.py:483] Algo bellman_ford step 4682 current loss 0.058749, current_train_items 149856.
I0304 19:30:21.990699 22579586809984 run.py:483] Algo bellman_ford step 4683 current loss 0.055722, current_train_items 149888.
I0304 19:30:22.025906 22579586809984 run.py:483] Algo bellman_ford step 4684 current loss 0.056967, current_train_items 149920.
I0304 19:30:22.045978 22579586809984 run.py:483] Algo bellman_ford step 4685 current loss 0.009670, current_train_items 149952.
I0304 19:30:22.061970 22579586809984 run.py:483] Algo bellman_ford step 4686 current loss 0.017781, current_train_items 149984.
I0304 19:30:22.085621 22579586809984 run.py:483] Algo bellman_ford step 4687 current loss 0.080823, current_train_items 150016.
I0304 19:30:22.117315 22579586809984 run.py:483] Algo bellman_ford step 4688 current loss 0.062854, current_train_items 150048.
I0304 19:30:22.152544 22579586809984 run.py:483] Algo bellman_ford step 4689 current loss 0.074448, current_train_items 150080.
I0304 19:30:22.172535 22579586809984 run.py:483] Algo bellman_ford step 4690 current loss 0.008108, current_train_items 150112.
I0304 19:30:22.188916 22579586809984 run.py:483] Algo bellman_ford step 4691 current loss 0.038919, current_train_items 150144.
I0304 19:30:22.212202 22579586809984 run.py:483] Algo bellman_ford step 4692 current loss 0.047736, current_train_items 150176.
I0304 19:30:22.243513 22579586809984 run.py:483] Algo bellman_ford step 4693 current loss 0.062766, current_train_items 150208.
I0304 19:30:22.276571 22579586809984 run.py:483] Algo bellman_ford step 4694 current loss 0.055819, current_train_items 150240.
I0304 19:30:22.296201 22579586809984 run.py:483] Algo bellman_ford step 4695 current loss 0.002310, current_train_items 150272.
I0304 19:30:22.312518 22579586809984 run.py:483] Algo bellman_ford step 4696 current loss 0.019055, current_train_items 150304.
I0304 19:30:22.337104 22579586809984 run.py:483] Algo bellman_ford step 4697 current loss 0.063232, current_train_items 150336.
I0304 19:30:22.367989 22579586809984 run.py:483] Algo bellman_ford step 4698 current loss 0.045492, current_train_items 150368.
I0304 19:30:22.400247 22579586809984 run.py:483] Algo bellman_ford step 4699 current loss 0.054999, current_train_items 150400.
I0304 19:30:22.420206 22579586809984 run.py:483] Algo bellman_ford step 4700 current loss 0.002462, current_train_items 150432.
I0304 19:30:22.428358 22579586809984 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0304 19:30:22.428463 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:30:22.445000 22579586809984 run.py:483] Algo bellman_ford step 4701 current loss 0.030862, current_train_items 150464.
I0304 19:30:22.469537 22579586809984 run.py:483] Algo bellman_ford step 4702 current loss 0.073854, current_train_items 150496.
I0304 19:30:22.502509 22579586809984 run.py:483] Algo bellman_ford step 4703 current loss 0.080057, current_train_items 150528.
I0304 19:30:22.537810 22579586809984 run.py:483] Algo bellman_ford step 4704 current loss 0.061452, current_train_items 150560.
I0304 19:30:22.557731 22579586809984 run.py:483] Algo bellman_ford step 4705 current loss 0.003917, current_train_items 150592.
I0304 19:30:22.573557 22579586809984 run.py:483] Algo bellman_ford step 4706 current loss 0.010149, current_train_items 150624.
I0304 19:30:22.597904 22579586809984 run.py:483] Algo bellman_ford step 4707 current loss 0.053786, current_train_items 150656.
I0304 19:30:22.630092 22579586809984 run.py:483] Algo bellman_ford step 4708 current loss 0.083844, current_train_items 150688.
I0304 19:30:22.663781 22579586809984 run.py:483] Algo bellman_ford step 4709 current loss 0.099376, current_train_items 150720.
I0304 19:30:22.683312 22579586809984 run.py:483] Algo bellman_ford step 4710 current loss 0.017595, current_train_items 150752.
I0304 19:30:22.699728 22579586809984 run.py:483] Algo bellman_ford step 4711 current loss 0.032049, current_train_items 150784.
I0304 19:30:22.724164 22579586809984 run.py:483] Algo bellman_ford step 4712 current loss 0.084313, current_train_items 150816.
I0304 19:30:22.755775 22579586809984 run.py:483] Algo bellman_ford step 4713 current loss 0.096157, current_train_items 150848.
I0304 19:30:22.788193 22579586809984 run.py:483] Algo bellman_ford step 4714 current loss 0.064671, current_train_items 150880.
I0304 19:30:22.807646 22579586809984 run.py:483] Algo bellman_ford step 4715 current loss 0.002132, current_train_items 150912.
I0304 19:30:22.824061 22579586809984 run.py:483] Algo bellman_ford step 4716 current loss 0.034845, current_train_items 150944.
I0304 19:30:22.846556 22579586809984 run.py:483] Algo bellman_ford step 4717 current loss 0.036070, current_train_items 150976.
I0304 19:30:22.877516 22579586809984 run.py:483] Algo bellman_ford step 4718 current loss 0.049857, current_train_items 151008.
I0304 19:30:22.913630 22579586809984 run.py:483] Algo bellman_ford step 4719 current loss 0.127132, current_train_items 151040.
I0304 19:30:22.933129 22579586809984 run.py:483] Algo bellman_ford step 4720 current loss 0.004349, current_train_items 151072.
I0304 19:30:22.949074 22579586809984 run.py:483] Algo bellman_ford step 4721 current loss 0.012432, current_train_items 151104.
I0304 19:30:22.973601 22579586809984 run.py:483] Algo bellman_ford step 4722 current loss 0.101282, current_train_items 151136.
I0304 19:30:23.003407 22579586809984 run.py:483] Algo bellman_ford step 4723 current loss 0.108472, current_train_items 151168.
I0304 19:30:23.037654 22579586809984 run.py:483] Algo bellman_ford step 4724 current loss 0.137493, current_train_items 151200.
I0304 19:30:23.057130 22579586809984 run.py:483] Algo bellman_ford step 4725 current loss 0.003363, current_train_items 151232.
I0304 19:30:23.073667 22579586809984 run.py:483] Algo bellman_ford step 4726 current loss 0.026925, current_train_items 151264.
I0304 19:30:23.098558 22579586809984 run.py:483] Algo bellman_ford step 4727 current loss 0.055414, current_train_items 151296.
I0304 19:30:23.128235 22579586809984 run.py:483] Algo bellman_ford step 4728 current loss 0.046537, current_train_items 151328.
I0304 19:30:23.158328 22579586809984 run.py:483] Algo bellman_ford step 4729 current loss 0.055740, current_train_items 151360.
I0304 19:30:23.177808 22579586809984 run.py:483] Algo bellman_ford step 4730 current loss 0.005531, current_train_items 151392.
I0304 19:30:23.193725 22579586809984 run.py:483] Algo bellman_ford step 4731 current loss 0.007180, current_train_items 151424.
I0304 19:30:23.216931 22579586809984 run.py:483] Algo bellman_ford step 4732 current loss 0.055214, current_train_items 151456.
I0304 19:30:23.248014 22579586809984 run.py:483] Algo bellman_ford step 4733 current loss 0.037501, current_train_items 151488.
I0304 19:30:23.282008 22579586809984 run.py:483] Algo bellman_ford step 4734 current loss 0.088091, current_train_items 151520.
I0304 19:30:23.301425 22579586809984 run.py:483] Algo bellman_ford step 4735 current loss 0.002562, current_train_items 151552.
I0304 19:30:23.318068 22579586809984 run.py:483] Algo bellman_ford step 4736 current loss 0.028181, current_train_items 151584.
I0304 19:30:23.341584 22579586809984 run.py:483] Algo bellman_ford step 4737 current loss 0.053015, current_train_items 151616.
I0304 19:30:23.372081 22579586809984 run.py:483] Algo bellman_ford step 4738 current loss 0.078165, current_train_items 151648.
I0304 19:30:23.404902 22579586809984 run.py:483] Algo bellman_ford step 4739 current loss 0.093060, current_train_items 151680.
I0304 19:30:23.424353 22579586809984 run.py:483] Algo bellman_ford step 4740 current loss 0.029551, current_train_items 151712.
I0304 19:30:23.440335 22579586809984 run.py:483] Algo bellman_ford step 4741 current loss 0.056735, current_train_items 151744.
I0304 19:30:23.463382 22579586809984 run.py:483] Algo bellman_ford step 4742 current loss 0.066459, current_train_items 151776.
I0304 19:30:23.495419 22579586809984 run.py:483] Algo bellman_ford step 4743 current loss 0.091707, current_train_items 151808.
I0304 19:30:23.527607 22579586809984 run.py:483] Algo bellman_ford step 4744 current loss 0.086542, current_train_items 151840.
I0304 19:30:23.547158 22579586809984 run.py:483] Algo bellman_ford step 4745 current loss 0.042199, current_train_items 151872.
I0304 19:30:23.563935 22579586809984 run.py:483] Algo bellman_ford step 4746 current loss 0.036660, current_train_items 151904.
I0304 19:30:23.589060 22579586809984 run.py:483] Algo bellman_ford step 4747 current loss 0.035209, current_train_items 151936.
I0304 19:30:23.620034 22579586809984 run.py:483] Algo bellman_ford step 4748 current loss 0.063448, current_train_items 151968.
I0304 19:30:23.653586 22579586809984 run.py:483] Algo bellman_ford step 4749 current loss 0.101298, current_train_items 152000.
I0304 19:30:23.673068 22579586809984 run.py:483] Algo bellman_ford step 4750 current loss 0.020613, current_train_items 152032.
I0304 19:30:23.681092 22579586809984 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0304 19:30:23.681197 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0304 19:30:23.698130 22579586809984 run.py:483] Algo bellman_ford step 4751 current loss 0.014865, current_train_items 152064.
I0304 19:30:23.723292 22579586809984 run.py:483] Algo bellman_ford step 4752 current loss 0.021817, current_train_items 152096.
I0304 19:30:23.756951 22579586809984 run.py:483] Algo bellman_ford step 4753 current loss 0.067027, current_train_items 152128.
I0304 19:30:23.790692 22579586809984 run.py:483] Algo bellman_ford step 4754 current loss 0.056096, current_train_items 152160.
I0304 19:30:23.810852 22579586809984 run.py:483] Algo bellman_ford step 4755 current loss 0.004361, current_train_items 152192.
I0304 19:30:23.826868 22579586809984 run.py:483] Algo bellman_ford step 4756 current loss 0.005294, current_train_items 152224.
I0304 19:30:23.851676 22579586809984 run.py:483] Algo bellman_ford step 4757 current loss 0.079288, current_train_items 152256.
I0304 19:30:23.883643 22579586809984 run.py:483] Algo bellman_ford step 4758 current loss 0.099127, current_train_items 152288.
I0304 19:30:23.918243 22579586809984 run.py:483] Algo bellman_ford step 4759 current loss 0.134635, current_train_items 152320.
I0304 19:30:23.938446 22579586809984 run.py:483] Algo bellman_ford step 4760 current loss 0.004537, current_train_items 152352.
I0304 19:30:23.954953 22579586809984 run.py:483] Algo bellman_ford step 4761 current loss 0.024993, current_train_items 152384.
I0304 19:30:23.978828 22579586809984 run.py:483] Algo bellman_ford step 4762 current loss 0.057333, current_train_items 152416.
I0304 19:30:24.009576 22579586809984 run.py:483] Algo bellman_ford step 4763 current loss 0.056033, current_train_items 152448.
I0304 19:30:24.044081 22579586809984 run.py:483] Algo bellman_ford step 4764 current loss 0.098070, current_train_items 152480.
I0304 19:30:24.063678 22579586809984 run.py:483] Algo bellman_ford step 4765 current loss 0.012527, current_train_items 152512.
I0304 19:30:24.079334 22579586809984 run.py:483] Algo bellman_ford step 4766 current loss 0.025573, current_train_items 152544.
I0304 19:30:24.104317 22579586809984 run.py:483] Algo bellman_ford step 4767 current loss 0.060681, current_train_items 152576.
I0304 19:30:24.134957 22579586809984 run.py:483] Algo bellman_ford step 4768 current loss 0.046031, current_train_items 152608.
I0304 19:30:24.167863 22579586809984 run.py:483] Algo bellman_ford step 4769 current loss 0.075607, current_train_items 152640.
I0304 19:30:24.187955 22579586809984 run.py:483] Algo bellman_ford step 4770 current loss 0.004582, current_train_items 152672.
I0304 19:30:24.204163 22579586809984 run.py:483] Algo bellman_ford step 4771 current loss 0.008401, current_train_items 152704.
I0304 19:30:24.227264 22579586809984 run.py:483] Algo bellman_ford step 4772 current loss 0.040456, current_train_items 152736.
I0304 19:30:24.258448 22579586809984 run.py:483] Algo bellman_ford step 4773 current loss 0.119930, current_train_items 152768.
I0304 19:30:24.292859 22579586809984 run.py:483] Algo bellman_ford step 4774 current loss 0.099182, current_train_items 152800.
I0304 19:30:24.313148 22579586809984 run.py:483] Algo bellman_ford step 4775 current loss 0.004167, current_train_items 152832.
I0304 19:30:24.329474 22579586809984 run.py:483] Algo bellman_ford step 4776 current loss 0.026967, current_train_items 152864.
I0304 19:30:24.353238 22579586809984 run.py:483] Algo bellman_ford step 4777 current loss 0.057091, current_train_items 152896.
I0304 19:30:24.384858 22579586809984 run.py:483] Algo bellman_ford step 4778 current loss 0.071255, current_train_items 152928.
I0304 19:30:24.417068 22579586809984 run.py:483] Algo bellman_ford step 4779 current loss 0.040150, current_train_items 152960.
I0304 19:30:24.437130 22579586809984 run.py:483] Algo bellman_ford step 4780 current loss 0.004611, current_train_items 152992.
I0304 19:30:24.453457 22579586809984 run.py:483] Algo bellman_ford step 4781 current loss 0.027938, current_train_items 153024.
I0304 19:30:24.477757 22579586809984 run.py:483] Algo bellman_ford step 4782 current loss 0.069634, current_train_items 153056.
I0304 19:30:24.509928 22579586809984 run.py:483] Algo bellman_ford step 4783 current loss 0.068545, current_train_items 153088.
I0304 19:30:24.544503 22579586809984 run.py:483] Algo bellman_ford step 4784 current loss 0.077340, current_train_items 153120.
I0304 19:30:24.564509 22579586809984 run.py:483] Algo bellman_ford step 4785 current loss 0.034017, current_train_items 153152.
I0304 19:30:24.581275 22579586809984 run.py:483] Algo bellman_ford step 4786 current loss 0.027592, current_train_items 153184.
I0304 19:30:24.606118 22579586809984 run.py:483] Algo bellman_ford step 4787 current loss 0.026424, current_train_items 153216.
I0304 19:30:24.636800 22579586809984 run.py:483] Algo bellman_ford step 4788 current loss 0.087293, current_train_items 153248.
I0304 19:30:24.669609 22579586809984 run.py:483] Algo bellman_ford step 4789 current loss 0.067937, current_train_items 153280.
I0304 19:30:24.689564 22579586809984 run.py:483] Algo bellman_ford step 4790 current loss 0.003430, current_train_items 153312.
I0304 19:30:24.705911 22579586809984 run.py:483] Algo bellman_ford step 4791 current loss 0.018284, current_train_items 153344.
I0304 19:30:24.730020 22579586809984 run.py:483] Algo bellman_ford step 4792 current loss 0.026717, current_train_items 153376.
I0304 19:30:24.761266 22579586809984 run.py:483] Algo bellman_ford step 4793 current loss 0.129060, current_train_items 153408.
I0304 19:30:24.796983 22579586809984 run.py:483] Algo bellman_ford step 4794 current loss 0.064436, current_train_items 153440.
I0304 19:30:24.816672 22579586809984 run.py:483] Algo bellman_ford step 4795 current loss 0.002901, current_train_items 153472.
I0304 19:30:24.832983 22579586809984 run.py:483] Algo bellman_ford step 4796 current loss 0.005821, current_train_items 153504.
I0304 19:30:24.857583 22579586809984 run.py:483] Algo bellman_ford step 4797 current loss 0.035938, current_train_items 153536.
I0304 19:30:24.888975 22579586809984 run.py:483] Algo bellman_ford step 4798 current loss 0.059285, current_train_items 153568.
I0304 19:30:24.922335 22579586809984 run.py:483] Algo bellman_ford step 4799 current loss 0.063641, current_train_items 153600.
I0304 19:30:24.942538 22579586809984 run.py:483] Algo bellman_ford step 4800 current loss 0.003380, current_train_items 153632.
I0304 19:30:24.950199 22579586809984 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0304 19:30:24.950304 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:30:24.967178 22579586809984 run.py:483] Algo bellman_ford step 4801 current loss 0.072332, current_train_items 153664.
I0304 19:30:24.991896 22579586809984 run.py:483] Algo bellman_ford step 4802 current loss 0.101193, current_train_items 153696.
I0304 19:30:25.023220 22579586809984 run.py:483] Algo bellman_ford step 4803 current loss 0.053591, current_train_items 153728.
I0304 19:30:25.059803 22579586809984 run.py:483] Algo bellman_ford step 4804 current loss 0.064667, current_train_items 153760.
I0304 19:30:25.079630 22579586809984 run.py:483] Algo bellman_ford step 4805 current loss 0.030432, current_train_items 153792.
I0304 19:30:25.096339 22579586809984 run.py:483] Algo bellman_ford step 4806 current loss 0.030161, current_train_items 153824.
I0304 19:30:25.120462 22579586809984 run.py:483] Algo bellman_ford step 4807 current loss 0.072370, current_train_items 153856.
I0304 19:30:25.151972 22579586809984 run.py:483] Algo bellman_ford step 4808 current loss 0.117926, current_train_items 153888.
I0304 19:30:25.185559 22579586809984 run.py:483] Algo bellman_ford step 4809 current loss 0.099510, current_train_items 153920.
I0304 19:30:25.205487 22579586809984 run.py:483] Algo bellman_ford step 4810 current loss 0.003284, current_train_items 153952.
I0304 19:30:25.221793 22579586809984 run.py:483] Algo bellman_ford step 4811 current loss 0.018011, current_train_items 153984.
I0304 19:30:25.245494 22579586809984 run.py:483] Algo bellman_ford step 4812 current loss 0.098151, current_train_items 154016.
I0304 19:30:25.276199 22579586809984 run.py:483] Algo bellman_ford step 4813 current loss 0.106411, current_train_items 154048.
I0304 19:30:25.311156 22579586809984 run.py:483] Algo bellman_ford step 4814 current loss 0.110624, current_train_items 154080.
I0304 19:30:25.331040 22579586809984 run.py:483] Algo bellman_ford step 4815 current loss 0.003400, current_train_items 154112.
I0304 19:30:25.347286 22579586809984 run.py:483] Algo bellman_ford step 4816 current loss 0.042629, current_train_items 154144.
I0304 19:30:25.372440 22579586809984 run.py:483] Algo bellman_ford step 4817 current loss 0.070113, current_train_items 154176.
I0304 19:30:25.403553 22579586809984 run.py:483] Algo bellman_ford step 4818 current loss 0.059127, current_train_items 154208.
I0304 19:30:25.433341 22579586809984 run.py:483] Algo bellman_ford step 4819 current loss 0.076508, current_train_items 154240.
I0304 19:30:25.452969 22579586809984 run.py:483] Algo bellman_ford step 4820 current loss 0.004674, current_train_items 154272.
I0304 19:30:25.469076 22579586809984 run.py:483] Algo bellman_ford step 4821 current loss 0.018126, current_train_items 154304.
I0304 19:30:25.493458 22579586809984 run.py:483] Algo bellman_ford step 4822 current loss 0.044888, current_train_items 154336.
I0304 19:30:25.524259 22579586809984 run.py:483] Algo bellman_ford step 4823 current loss 0.063112, current_train_items 154368.
I0304 19:30:25.558758 22579586809984 run.py:483] Algo bellman_ford step 4824 current loss 0.085715, current_train_items 154400.
I0304 19:30:25.578469 22579586809984 run.py:483] Algo bellman_ford step 4825 current loss 0.006341, current_train_items 154432.
I0304 19:30:25.595342 22579586809984 run.py:483] Algo bellman_ford step 4826 current loss 0.023110, current_train_items 154464.
I0304 19:30:25.618855 22579586809984 run.py:483] Algo bellman_ford step 4827 current loss 0.042623, current_train_items 154496.
I0304 19:30:25.650132 22579586809984 run.py:483] Algo bellman_ford step 4828 current loss 0.047803, current_train_items 154528.
I0304 19:30:25.684193 22579586809984 run.py:483] Algo bellman_ford step 4829 current loss 0.086967, current_train_items 154560.
I0304 19:30:25.703640 22579586809984 run.py:483] Algo bellman_ford step 4830 current loss 0.004327, current_train_items 154592.
I0304 19:30:25.720154 22579586809984 run.py:483] Algo bellman_ford step 4831 current loss 0.028373, current_train_items 154624.
I0304 19:30:25.744663 22579586809984 run.py:483] Algo bellman_ford step 4832 current loss 0.037934, current_train_items 154656.
I0304 19:30:25.776616 22579586809984 run.py:483] Algo bellman_ford step 4833 current loss 0.055382, current_train_items 154688.
I0304 19:30:25.810198 22579586809984 run.py:483] Algo bellman_ford step 4834 current loss 0.048844, current_train_items 154720.
I0304 19:30:25.829330 22579586809984 run.py:483] Algo bellman_ford step 4835 current loss 0.002092, current_train_items 154752.
I0304 19:30:25.845536 22579586809984 run.py:483] Algo bellman_ford step 4836 current loss 0.013476, current_train_items 154784.
I0304 19:30:25.868401 22579586809984 run.py:483] Algo bellman_ford step 4837 current loss 0.026612, current_train_items 154816.
I0304 19:30:25.899514 22579586809984 run.py:483] Algo bellman_ford step 4838 current loss 0.037491, current_train_items 154848.
I0304 19:30:25.931248 22579586809984 run.py:483] Algo bellman_ford step 4839 current loss 0.067233, current_train_items 154880.
I0304 19:30:25.950775 22579586809984 run.py:483] Algo bellman_ford step 4840 current loss 0.004166, current_train_items 154912.
I0304 19:30:25.966821 22579586809984 run.py:483] Algo bellman_ford step 4841 current loss 0.024210, current_train_items 154944.
I0304 19:30:25.990707 22579586809984 run.py:483] Algo bellman_ford step 4842 current loss 0.032689, current_train_items 154976.
I0304 19:30:26.021575 22579586809984 run.py:483] Algo bellman_ford step 4843 current loss 0.055992, current_train_items 155008.
I0304 19:30:26.056396 22579586809984 run.py:483] Algo bellman_ford step 4844 current loss 0.074273, current_train_items 155040.
I0304 19:30:26.076056 22579586809984 run.py:483] Algo bellman_ford step 4845 current loss 0.003303, current_train_items 155072.
I0304 19:30:26.092545 22579586809984 run.py:483] Algo bellman_ford step 4846 current loss 0.013467, current_train_items 155104.
I0304 19:30:26.115520 22579586809984 run.py:483] Algo bellman_ford step 4847 current loss 0.045010, current_train_items 155136.
I0304 19:30:26.146236 22579586809984 run.py:483] Algo bellman_ford step 4848 current loss 0.039857, current_train_items 155168.
I0304 19:30:26.179741 22579586809984 run.py:483] Algo bellman_ford step 4849 current loss 0.081924, current_train_items 155200.
I0304 19:30:26.199260 22579586809984 run.py:483] Algo bellman_ford step 4850 current loss 0.002407, current_train_items 155232.
I0304 19:30:26.206918 22579586809984 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0304 19:30:26.207026 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:30:26.224323 22579586809984 run.py:483] Algo bellman_ford step 4851 current loss 0.020609, current_train_items 155264.
I0304 19:30:26.247837 22579586809984 run.py:483] Algo bellman_ford step 4852 current loss 0.040520, current_train_items 155296.
I0304 19:30:26.279487 22579586809984 run.py:483] Algo bellman_ford step 4853 current loss 0.055146, current_train_items 155328.
I0304 19:30:26.313604 22579586809984 run.py:483] Algo bellman_ford step 4854 current loss 0.061795, current_train_items 155360.
I0304 19:30:26.333907 22579586809984 run.py:483] Algo bellman_ford step 4855 current loss 0.002420, current_train_items 155392.
I0304 19:30:26.350537 22579586809984 run.py:483] Algo bellman_ford step 4856 current loss 0.049420, current_train_items 155424.
I0304 19:30:26.375823 22579586809984 run.py:483] Algo bellman_ford step 4857 current loss 0.066126, current_train_items 155456.
I0304 19:30:26.408025 22579586809984 run.py:483] Algo bellman_ford step 4858 current loss 0.046454, current_train_items 155488.
I0304 19:30:26.443947 22579586809984 run.py:483] Algo bellman_ford step 4859 current loss 0.085755, current_train_items 155520.
I0304 19:30:26.464203 22579586809984 run.py:483] Algo bellman_ford step 4860 current loss 0.003199, current_train_items 155552.
I0304 19:30:26.481529 22579586809984 run.py:483] Algo bellman_ford step 4861 current loss 0.020785, current_train_items 155584.
I0304 19:30:26.505249 22579586809984 run.py:483] Algo bellman_ford step 4862 current loss 0.068272, current_train_items 155616.
I0304 19:30:26.537365 22579586809984 run.py:483] Algo bellman_ford step 4863 current loss 0.046025, current_train_items 155648.
I0304 19:30:26.570425 22579586809984 run.py:483] Algo bellman_ford step 4864 current loss 0.041504, current_train_items 155680.
I0304 19:30:26.590224 22579586809984 run.py:483] Algo bellman_ford step 4865 current loss 0.002367, current_train_items 155712.
I0304 19:30:26.606409 22579586809984 run.py:483] Algo bellman_ford step 4866 current loss 0.016638, current_train_items 155744.
I0304 19:30:26.630937 22579586809984 run.py:483] Algo bellman_ford step 4867 current loss 0.080886, current_train_items 155776.
I0304 19:30:26.663755 22579586809984 run.py:483] Algo bellman_ford step 4868 current loss 0.091847, current_train_items 155808.
I0304 19:30:26.696443 22579586809984 run.py:483] Algo bellman_ford step 4869 current loss 0.075460, current_train_items 155840.
I0304 19:30:26.717133 22579586809984 run.py:483] Algo bellman_ford step 4870 current loss 0.002588, current_train_items 155872.
I0304 19:30:26.733617 22579586809984 run.py:483] Algo bellman_ford step 4871 current loss 0.074271, current_train_items 155904.
I0304 19:30:26.757472 22579586809984 run.py:483] Algo bellman_ford step 4872 current loss 0.044599, current_train_items 155936.
I0304 19:30:26.787108 22579586809984 run.py:483] Algo bellman_ford step 4873 current loss 0.037091, current_train_items 155968.
I0304 19:30:26.819464 22579586809984 run.py:483] Algo bellman_ford step 4874 current loss 0.062734, current_train_items 156000.
I0304 19:30:26.839326 22579586809984 run.py:483] Algo bellman_ford step 4875 current loss 0.003213, current_train_items 156032.
I0304 19:30:26.855602 22579586809984 run.py:483] Algo bellman_ford step 4876 current loss 0.016759, current_train_items 156064.
I0304 19:30:26.878944 22579586809984 run.py:483] Algo bellman_ford step 4877 current loss 0.024713, current_train_items 156096.
I0304 19:30:26.909434 22579586809984 run.py:483] Algo bellman_ford step 4878 current loss 0.046229, current_train_items 156128.
I0304 19:30:26.943148 22579586809984 run.py:483] Algo bellman_ford step 4879 current loss 0.096122, current_train_items 156160.
I0304 19:30:26.963311 22579586809984 run.py:483] Algo bellman_ford step 4880 current loss 0.009539, current_train_items 156192.
I0304 19:30:26.979834 22579586809984 run.py:483] Algo bellman_ford step 4881 current loss 0.027534, current_train_items 156224.
I0304 19:30:27.004510 22579586809984 run.py:483] Algo bellman_ford step 4882 current loss 0.036239, current_train_items 156256.
I0304 19:30:27.036282 22579586809984 run.py:483] Algo bellman_ford step 4883 current loss 0.064880, current_train_items 156288.
I0304 19:30:27.070982 22579586809984 run.py:483] Algo bellman_ford step 4884 current loss 0.076936, current_train_items 156320.
I0304 19:30:27.090987 22579586809984 run.py:483] Algo bellman_ford step 4885 current loss 0.006060, current_train_items 156352.
I0304 19:30:27.107650 22579586809984 run.py:483] Algo bellman_ford step 4886 current loss 0.031852, current_train_items 156384.
I0304 19:30:27.131372 22579586809984 run.py:483] Algo bellman_ford step 4887 current loss 0.054278, current_train_items 156416.
I0304 19:30:27.162382 22579586809984 run.py:483] Algo bellman_ford step 4888 current loss 0.027157, current_train_items 156448.
I0304 19:30:27.195936 22579586809984 run.py:483] Algo bellman_ford step 4889 current loss 0.065242, current_train_items 156480.
I0304 19:30:27.216037 22579586809984 run.py:483] Algo bellman_ford step 4890 current loss 0.003166, current_train_items 156512.
I0304 19:30:27.232304 22579586809984 run.py:483] Algo bellman_ford step 4891 current loss 0.019706, current_train_items 156544.
I0304 19:30:27.256062 22579586809984 run.py:483] Algo bellman_ford step 4892 current loss 0.041039, current_train_items 156576.
I0304 19:30:27.285957 22579586809984 run.py:483] Algo bellman_ford step 4893 current loss 0.077814, current_train_items 156608.
I0304 19:30:27.318808 22579586809984 run.py:483] Algo bellman_ford step 4894 current loss 0.082627, current_train_items 156640.
I0304 19:30:27.338644 22579586809984 run.py:483] Algo bellman_ford step 4895 current loss 0.032348, current_train_items 156672.
I0304 19:30:27.355051 22579586809984 run.py:483] Algo bellman_ford step 4896 current loss 0.006975, current_train_items 156704.
I0304 19:30:27.378227 22579586809984 run.py:483] Algo bellman_ford step 4897 current loss 0.032084, current_train_items 156736.
I0304 19:30:27.409053 22579586809984 run.py:483] Algo bellman_ford step 4898 current loss 0.062741, current_train_items 156768.
I0304 19:30:27.443530 22579586809984 run.py:483] Algo bellman_ford step 4899 current loss 0.063634, current_train_items 156800.
I0304 19:30:27.463550 22579586809984 run.py:483] Algo bellman_ford step 4900 current loss 0.010355, current_train_items 156832.
I0304 19:30:27.471169 22579586809984 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0304 19:30:27.471300 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:30:27.487766 22579586809984 run.py:483] Algo bellman_ford step 4901 current loss 0.014683, current_train_items 156864.
I0304 19:30:27.512210 22579586809984 run.py:483] Algo bellman_ford step 4902 current loss 0.038020, current_train_items 156896.
I0304 19:30:27.546161 22579586809984 run.py:483] Algo bellman_ford step 4903 current loss 0.036167, current_train_items 156928.
I0304 19:30:27.579557 22579586809984 run.py:483] Algo bellman_ford step 4904 current loss 0.053693, current_train_items 156960.
I0304 19:30:27.599332 22579586809984 run.py:483] Algo bellman_ford step 4905 current loss 0.006748, current_train_items 156992.
I0304 19:30:27.615260 22579586809984 run.py:483] Algo bellman_ford step 4906 current loss 0.027174, current_train_items 157024.
I0304 19:30:27.639573 22579586809984 run.py:483] Algo bellman_ford step 4907 current loss 0.047150, current_train_items 157056.
I0304 19:30:27.670897 22579586809984 run.py:483] Algo bellman_ford step 4908 current loss 0.046553, current_train_items 157088.
I0304 19:30:27.703811 22579586809984 run.py:483] Algo bellman_ford step 4909 current loss 0.074469, current_train_items 157120.
I0304 19:30:27.723421 22579586809984 run.py:483] Algo bellman_ford step 4910 current loss 0.004419, current_train_items 157152.
I0304 19:30:27.740001 22579586809984 run.py:483] Algo bellman_ford step 4911 current loss 0.023869, current_train_items 157184.
I0304 19:30:27.763997 22579586809984 run.py:483] Algo bellman_ford step 4912 current loss 0.029976, current_train_items 157216.
I0304 19:30:27.796099 22579586809984 run.py:483] Algo bellman_ford step 4913 current loss 0.091173, current_train_items 157248.
I0304 19:30:27.829245 22579586809984 run.py:483] Algo bellman_ford step 4914 current loss 0.064912, current_train_items 157280.
I0304 19:30:27.848673 22579586809984 run.py:483] Algo bellman_ford step 4915 current loss 0.003558, current_train_items 157312.
I0304 19:30:27.864648 22579586809984 run.py:483] Algo bellman_ford step 4916 current loss 0.033583, current_train_items 157344.
I0304 19:30:27.888411 22579586809984 run.py:483] Algo bellman_ford step 4917 current loss 0.076484, current_train_items 157376.
I0304 19:30:27.918530 22579586809984 run.py:483] Algo bellman_ford step 4918 current loss 0.044081, current_train_items 157408.
I0304 19:30:27.952012 22579586809984 run.py:483] Algo bellman_ford step 4919 current loss 0.066764, current_train_items 157440.
I0304 19:30:27.971578 22579586809984 run.py:483] Algo bellman_ford step 4920 current loss 0.002715, current_train_items 157472.
I0304 19:30:27.987231 22579586809984 run.py:483] Algo bellman_ford step 4921 current loss 0.008054, current_train_items 157504.
I0304 19:30:28.011276 22579586809984 run.py:483] Algo bellman_ford step 4922 current loss 0.069075, current_train_items 157536.
I0304 19:30:28.042352 22579586809984 run.py:483] Algo bellman_ford step 4923 current loss 0.049222, current_train_items 157568.
I0304 19:30:28.075640 22579586809984 run.py:483] Algo bellman_ford step 4924 current loss 0.072838, current_train_items 157600.
I0304 19:30:28.095245 22579586809984 run.py:483] Algo bellman_ford step 4925 current loss 0.015553, current_train_items 157632.
I0304 19:30:28.111497 22579586809984 run.py:483] Algo bellman_ford step 4926 current loss 0.015152, current_train_items 157664.
I0304 19:30:28.136067 22579586809984 run.py:483] Algo bellman_ford step 4927 current loss 0.056082, current_train_items 157696.
I0304 19:30:28.166436 22579586809984 run.py:483] Algo bellman_ford step 4928 current loss 0.100445, current_train_items 157728.
I0304 19:30:28.200139 22579586809984 run.py:483] Algo bellman_ford step 4929 current loss 0.070738, current_train_items 157760.
I0304 19:30:28.219713 22579586809984 run.py:483] Algo bellman_ford step 4930 current loss 0.003875, current_train_items 157792.
I0304 19:30:28.235695 22579586809984 run.py:483] Algo bellman_ford step 4931 current loss 0.012057, current_train_items 157824.
I0304 19:30:28.259376 22579586809984 run.py:483] Algo bellman_ford step 4932 current loss 0.055451, current_train_items 157856.
I0304 19:30:28.289914 22579586809984 run.py:483] Algo bellman_ford step 4933 current loss 0.090490, current_train_items 157888.
I0304 19:30:28.323457 22579586809984 run.py:483] Algo bellman_ford step 4934 current loss 0.093494, current_train_items 157920.
I0304 19:30:28.343431 22579586809984 run.py:483] Algo bellman_ford step 4935 current loss 0.021162, current_train_items 157952.
I0304 19:30:28.359596 22579586809984 run.py:483] Algo bellman_ford step 4936 current loss 0.014920, current_train_items 157984.
I0304 19:30:28.384810 22579586809984 run.py:483] Algo bellman_ford step 4937 current loss 0.027595, current_train_items 158016.
I0304 19:30:28.415672 22579586809984 run.py:483] Algo bellman_ford step 4938 current loss 0.031703, current_train_items 158048.
I0304 19:30:28.451216 22579586809984 run.py:483] Algo bellman_ford step 4939 current loss 0.075764, current_train_items 158080.
I0304 19:30:28.470746 22579586809984 run.py:483] Algo bellman_ford step 4940 current loss 0.003510, current_train_items 158112.
I0304 19:30:28.486713 22579586809984 run.py:483] Algo bellman_ford step 4941 current loss 0.011139, current_train_items 158144.
I0304 19:30:28.510457 22579586809984 run.py:483] Algo bellman_ford step 4942 current loss 0.043815, current_train_items 158176.
I0304 19:30:28.542725 22579586809984 run.py:483] Algo bellman_ford step 4943 current loss 0.077546, current_train_items 158208.
I0304 19:30:28.574615 22579586809984 run.py:483] Algo bellman_ford step 4944 current loss 0.034773, current_train_items 158240.
I0304 19:30:28.594003 22579586809984 run.py:483] Algo bellman_ford step 4945 current loss 0.004319, current_train_items 158272.
I0304 19:30:28.610316 22579586809984 run.py:483] Algo bellman_ford step 4946 current loss 0.030065, current_train_items 158304.
I0304 19:30:28.633598 22579586809984 run.py:483] Algo bellman_ford step 4947 current loss 0.078191, current_train_items 158336.
I0304 19:30:28.664903 22579586809984 run.py:483] Algo bellman_ford step 4948 current loss 0.072437, current_train_items 158368.
I0304 19:30:28.699862 22579586809984 run.py:483] Algo bellman_ford step 4949 current loss 0.041210, current_train_items 158400.
I0304 19:30:28.719693 22579586809984 run.py:483] Algo bellman_ford step 4950 current loss 0.003018, current_train_items 158432.
I0304 19:30:28.728047 22579586809984 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0304 19:30:28.728153 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:30:28.744793 22579586809984 run.py:483] Algo bellman_ford step 4951 current loss 0.005584, current_train_items 158464.
I0304 19:30:28.768605 22579586809984 run.py:483] Algo bellman_ford step 4952 current loss 0.032412, current_train_items 158496.
I0304 19:30:28.800319 22579586809984 run.py:483] Algo bellman_ford step 4953 current loss 0.111650, current_train_items 158528.
I0304 19:30:28.836085 22579586809984 run.py:483] Algo bellman_ford step 4954 current loss 0.065881, current_train_items 158560.
I0304 19:30:28.856118 22579586809984 run.py:483] Algo bellman_ford step 4955 current loss 0.012052, current_train_items 158592.
I0304 19:30:28.872304 22579586809984 run.py:483] Algo bellman_ford step 4956 current loss 0.008858, current_train_items 158624.
I0304 19:30:28.897444 22579586809984 run.py:483] Algo bellman_ford step 4957 current loss 0.042372, current_train_items 158656.
I0304 19:30:28.929177 22579586809984 run.py:483] Algo bellman_ford step 4958 current loss 0.096615, current_train_items 158688.
I0304 19:30:28.960794 22579586809984 run.py:483] Algo bellman_ford step 4959 current loss 0.081450, current_train_items 158720.
I0304 19:30:28.980673 22579586809984 run.py:483] Algo bellman_ford step 4960 current loss 0.009207, current_train_items 158752.
I0304 19:30:28.997236 22579586809984 run.py:483] Algo bellman_ford step 4961 current loss 0.014850, current_train_items 158784.
I0304 19:30:29.020805 22579586809984 run.py:483] Algo bellman_ford step 4962 current loss 0.032542, current_train_items 158816.
I0304 19:30:29.050889 22579586809984 run.py:483] Algo bellman_ford step 4963 current loss 0.034658, current_train_items 158848.
I0304 19:30:29.085098 22579586809984 run.py:483] Algo bellman_ford step 4964 current loss 0.072095, current_train_items 158880.
I0304 19:30:29.104557 22579586809984 run.py:483] Algo bellman_ford step 4965 current loss 0.002285, current_train_items 158912.
I0304 19:30:29.120841 22579586809984 run.py:483] Algo bellman_ford step 4966 current loss 0.034399, current_train_items 158944.
I0304 19:30:29.145610 22579586809984 run.py:483] Algo bellman_ford step 4967 current loss 0.052948, current_train_items 158976.
I0304 19:30:29.177599 22579586809984 run.py:483] Algo bellman_ford step 4968 current loss 0.055130, current_train_items 159008.
I0304 19:30:29.209371 22579586809984 run.py:483] Algo bellman_ford step 4969 current loss 0.066313, current_train_items 159040.
I0304 19:30:29.229320 22579586809984 run.py:483] Algo bellman_ford step 4970 current loss 0.003248, current_train_items 159072.
I0304 19:30:29.245543 22579586809984 run.py:483] Algo bellman_ford step 4971 current loss 0.070448, current_train_items 159104.
I0304 19:30:29.269395 22579586809984 run.py:483] Algo bellman_ford step 4972 current loss 0.068716, current_train_items 159136.
I0304 19:30:29.301325 22579586809984 run.py:483] Algo bellman_ford step 4973 current loss 0.055939, current_train_items 159168.
I0304 19:30:29.334844 22579586809984 run.py:483] Algo bellman_ford step 4974 current loss 0.075961, current_train_items 159200.
I0304 19:30:29.354697 22579586809984 run.py:483] Algo bellman_ford step 4975 current loss 0.003660, current_train_items 159232.
I0304 19:30:29.371479 22579586809984 run.py:483] Algo bellman_ford step 4976 current loss 0.021404, current_train_items 159264.
I0304 19:30:29.395854 22579586809984 run.py:483] Algo bellman_ford step 4977 current loss 0.107965, current_train_items 159296.
I0304 19:30:29.427723 22579586809984 run.py:483] Algo bellman_ford step 4978 current loss 0.137429, current_train_items 159328.
I0304 19:30:29.462360 22579586809984 run.py:483] Algo bellman_ford step 4979 current loss 0.078642, current_train_items 159360.
I0304 19:30:29.481918 22579586809984 run.py:483] Algo bellman_ford step 4980 current loss 0.010207, current_train_items 159392.
I0304 19:30:29.498625 22579586809984 run.py:483] Algo bellman_ford step 4981 current loss 0.013620, current_train_items 159424.
I0304 19:30:29.523491 22579586809984 run.py:483] Algo bellman_ford step 4982 current loss 0.040143, current_train_items 159456.
I0304 19:30:29.552977 22579586809984 run.py:483] Algo bellman_ford step 4983 current loss 0.038141, current_train_items 159488.
I0304 19:30:29.586164 22579586809984 run.py:483] Algo bellman_ford step 4984 current loss 0.056756, current_train_items 159520.
I0304 19:30:29.606113 22579586809984 run.py:483] Algo bellman_ford step 4985 current loss 0.003572, current_train_items 159552.
I0304 19:30:29.622617 22579586809984 run.py:483] Algo bellman_ford step 4986 current loss 0.025276, current_train_items 159584.
I0304 19:30:29.645060 22579586809984 run.py:483] Algo bellman_ford step 4987 current loss 0.065982, current_train_items 159616.
I0304 19:30:29.675235 22579586809984 run.py:483] Algo bellman_ford step 4988 current loss 0.043794, current_train_items 159648.
I0304 19:30:29.711534 22579586809984 run.py:483] Algo bellman_ford step 4989 current loss 0.074030, current_train_items 159680.
I0304 19:30:29.731223 22579586809984 run.py:483] Algo bellman_ford step 4990 current loss 0.003702, current_train_items 159712.
I0304 19:30:29.747334 22579586809984 run.py:483] Algo bellman_ford step 4991 current loss 0.016423, current_train_items 159744.
I0304 19:30:29.771184 22579586809984 run.py:483] Algo bellman_ford step 4992 current loss 0.067166, current_train_items 159776.
I0304 19:30:29.802160 22579586809984 run.py:483] Algo bellman_ford step 4993 current loss 0.075729, current_train_items 159808.
I0304 19:30:29.836101 22579586809984 run.py:483] Algo bellman_ford step 4994 current loss 0.070299, current_train_items 159840.
I0304 19:30:29.855367 22579586809984 run.py:483] Algo bellman_ford step 4995 current loss 0.004210, current_train_items 159872.
I0304 19:30:29.871883 22579586809984 run.py:483] Algo bellman_ford step 4996 current loss 0.011155, current_train_items 159904.
I0304 19:30:29.895600 22579586809984 run.py:483] Algo bellman_ford step 4997 current loss 0.037827, current_train_items 159936.
I0304 19:30:29.927028 22579586809984 run.py:483] Algo bellman_ford step 4998 current loss 0.080320, current_train_items 159968.
I0304 19:30:29.961266 22579586809984 run.py:483] Algo bellman_ford step 4999 current loss 0.070271, current_train_items 160000.
I0304 19:30:29.981115 22579586809984 run.py:483] Algo bellman_ford step 5000 current loss 0.003358, current_train_items 160032.
I0304 19:30:29.988811 22579586809984 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0304 19:30:29.988916 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:30:30.006031 22579586809984 run.py:483] Algo bellman_ford step 5001 current loss 0.058050, current_train_items 160064.
I0304 19:30:30.030331 22579586809984 run.py:483] Algo bellman_ford step 5002 current loss 0.132202, current_train_items 160096.
I0304 19:30:30.060928 22579586809984 run.py:483] Algo bellman_ford step 5003 current loss 0.019406, current_train_items 160128.
I0304 19:30:30.094501 22579586809984 run.py:483] Algo bellman_ford step 5004 current loss 0.083392, current_train_items 160160.
I0304 19:30:30.114403 22579586809984 run.py:483] Algo bellman_ford step 5005 current loss 0.004611, current_train_items 160192.
I0304 19:30:30.130550 22579586809984 run.py:483] Algo bellman_ford step 5006 current loss 0.032740, current_train_items 160224.
I0304 19:30:30.155402 22579586809984 run.py:483] Algo bellman_ford step 5007 current loss 0.047542, current_train_items 160256.
I0304 19:30:30.188685 22579586809984 run.py:483] Algo bellman_ford step 5008 current loss 0.083685, current_train_items 160288.
I0304 19:30:30.220670 22579586809984 run.py:483] Algo bellman_ford step 5009 current loss 0.037575, current_train_items 160320.
I0304 19:30:30.240292 22579586809984 run.py:483] Algo bellman_ford step 5010 current loss 0.015341, current_train_items 160352.
I0304 19:30:30.256311 22579586809984 run.py:483] Algo bellman_ford step 5011 current loss 0.018359, current_train_items 160384.
I0304 19:30:30.280549 22579586809984 run.py:483] Algo bellman_ford step 5012 current loss 0.025834, current_train_items 160416.
I0304 19:30:30.311403 22579586809984 run.py:483] Algo bellman_ford step 5013 current loss 0.064506, current_train_items 160448.
I0304 19:30:30.346375 22579586809984 run.py:483] Algo bellman_ford step 5014 current loss 0.069408, current_train_items 160480.
I0304 19:30:30.366410 22579586809984 run.py:483] Algo bellman_ford step 5015 current loss 0.015035, current_train_items 160512.
I0304 19:30:30.383222 22579586809984 run.py:483] Algo bellman_ford step 5016 current loss 0.053271, current_train_items 160544.
I0304 19:30:30.407035 22579586809984 run.py:483] Algo bellman_ford step 5017 current loss 0.048588, current_train_items 160576.
I0304 19:30:30.438910 22579586809984 run.py:483] Algo bellman_ford step 5018 current loss 0.056725, current_train_items 160608.
I0304 19:30:30.474508 22579586809984 run.py:483] Algo bellman_ford step 5019 current loss 0.168496, current_train_items 160640.
I0304 19:30:30.494415 22579586809984 run.py:483] Algo bellman_ford step 5020 current loss 0.005476, current_train_items 160672.
I0304 19:30:30.510486 22579586809984 run.py:483] Algo bellman_ford step 5021 current loss 0.011543, current_train_items 160704.
I0304 19:30:30.535694 22579586809984 run.py:483] Algo bellman_ford step 5022 current loss 0.035124, current_train_items 160736.
I0304 19:30:30.567604 22579586809984 run.py:483] Algo bellman_ford step 5023 current loss 0.110612, current_train_items 160768.
I0304 19:30:30.601599 22579586809984 run.py:483] Algo bellman_ford step 5024 current loss 0.060809, current_train_items 160800.
I0304 19:30:30.621114 22579586809984 run.py:483] Algo bellman_ford step 5025 current loss 0.002678, current_train_items 160832.
I0304 19:30:30.636984 22579586809984 run.py:483] Algo bellman_ford step 5026 current loss 0.019000, current_train_items 160864.
I0304 19:30:30.660181 22579586809984 run.py:483] Algo bellman_ford step 5027 current loss 0.043863, current_train_items 160896.
I0304 19:30:30.690071 22579586809984 run.py:483] Algo bellman_ford step 5028 current loss 0.064891, current_train_items 160928.
I0304 19:30:30.723837 22579586809984 run.py:483] Algo bellman_ford step 5029 current loss 0.112877, current_train_items 160960.
I0304 19:30:30.743762 22579586809984 run.py:483] Algo bellman_ford step 5030 current loss 0.038090, current_train_items 160992.
I0304 19:30:30.760155 22579586809984 run.py:483] Algo bellman_ford step 5031 current loss 0.040664, current_train_items 161024.
I0304 19:30:30.784665 22579586809984 run.py:483] Algo bellman_ford step 5032 current loss 0.043322, current_train_items 161056.
I0304 19:30:30.814824 22579586809984 run.py:483] Algo bellman_ford step 5033 current loss 0.034838, current_train_items 161088.
I0304 19:30:30.849410 22579586809984 run.py:483] Algo bellman_ford step 5034 current loss 0.084652, current_train_items 161120.
I0304 19:30:30.869558 22579586809984 run.py:483] Algo bellman_ford step 5035 current loss 0.005580, current_train_items 161152.
I0304 19:30:30.886020 22579586809984 run.py:483] Algo bellman_ford step 5036 current loss 0.006781, current_train_items 161184.
I0304 19:30:30.910314 22579586809984 run.py:483] Algo bellman_ford step 5037 current loss 0.046544, current_train_items 161216.
I0304 19:30:30.943420 22579586809984 run.py:483] Algo bellman_ford step 5038 current loss 0.095774, current_train_items 161248.
I0304 19:30:30.977556 22579586809984 run.py:483] Algo bellman_ford step 5039 current loss 0.054226, current_train_items 161280.
I0304 19:30:30.997493 22579586809984 run.py:483] Algo bellman_ford step 5040 current loss 0.002805, current_train_items 161312.
I0304 19:30:31.013225 22579586809984 run.py:483] Algo bellman_ford step 5041 current loss 0.005853, current_train_items 161344.
I0304 19:30:31.038299 22579586809984 run.py:483] Algo bellman_ford step 5042 current loss 0.033075, current_train_items 161376.
I0304 19:30:31.069217 22579586809984 run.py:483] Algo bellman_ford step 5043 current loss 0.030264, current_train_items 161408.
I0304 19:30:31.102821 22579586809984 run.py:483] Algo bellman_ford step 5044 current loss 0.065347, current_train_items 161440.
I0304 19:30:31.122730 22579586809984 run.py:483] Algo bellman_ford step 5045 current loss 0.004794, current_train_items 161472.
I0304 19:30:31.138955 22579586809984 run.py:483] Algo bellman_ford step 5046 current loss 0.026178, current_train_items 161504.
I0304 19:30:31.161921 22579586809984 run.py:483] Algo bellman_ford step 5047 current loss 0.019839, current_train_items 161536.
I0304 19:30:31.193720 22579586809984 run.py:483] Algo bellman_ford step 5048 current loss 0.072649, current_train_items 161568.
I0304 19:30:31.227520 22579586809984 run.py:483] Algo bellman_ford step 5049 current loss 0.110897, current_train_items 161600.
I0304 19:30:31.247149 22579586809984 run.py:483] Algo bellman_ford step 5050 current loss 0.003136, current_train_items 161632.
I0304 19:30:31.254887 22579586809984 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0304 19:30:31.254993 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:31.271768 22579586809984 run.py:483] Algo bellman_ford step 5051 current loss 0.038018, current_train_items 161664.
I0304 19:30:31.297003 22579586809984 run.py:483] Algo bellman_ford step 5052 current loss 0.078689, current_train_items 161696.
I0304 19:30:31.328651 22579586809984 run.py:483] Algo bellman_ford step 5053 current loss 0.040710, current_train_items 161728.
I0304 19:30:31.361200 22579586809984 run.py:483] Algo bellman_ford step 5054 current loss 0.066380, current_train_items 161760.
I0304 19:30:31.381321 22579586809984 run.py:483] Algo bellman_ford step 5055 current loss 0.006790, current_train_items 161792.
I0304 19:30:31.398323 22579586809984 run.py:483] Algo bellman_ford step 5056 current loss 0.028142, current_train_items 161824.
I0304 19:30:31.423854 22579586809984 run.py:483] Algo bellman_ford step 5057 current loss 0.041237, current_train_items 161856.
I0304 19:30:31.454705 22579586809984 run.py:483] Algo bellman_ford step 5058 current loss 0.060896, current_train_items 161888.
I0304 19:30:31.488827 22579586809984 run.py:483] Algo bellman_ford step 5059 current loss 0.051769, current_train_items 161920.
I0304 19:30:31.508941 22579586809984 run.py:483] Algo bellman_ford step 5060 current loss 0.038932, current_train_items 161952.
I0304 19:30:31.525379 22579586809984 run.py:483] Algo bellman_ford step 5061 current loss 0.025799, current_train_items 161984.
I0304 19:30:31.547738 22579586809984 run.py:483] Algo bellman_ford step 5062 current loss 0.033469, current_train_items 162016.
I0304 19:30:31.578731 22579586809984 run.py:483] Algo bellman_ford step 5063 current loss 0.101680, current_train_items 162048.
I0304 19:30:31.610950 22579586809984 run.py:483] Algo bellman_ford step 5064 current loss 0.139765, current_train_items 162080.
I0304 19:30:31.630758 22579586809984 run.py:483] Algo bellman_ford step 5065 current loss 0.017029, current_train_items 162112.
I0304 19:30:31.647029 22579586809984 run.py:483] Algo bellman_ford step 5066 current loss 0.011932, current_train_items 162144.
I0304 19:30:31.671914 22579586809984 run.py:483] Algo bellman_ford step 5067 current loss 0.068826, current_train_items 162176.
I0304 19:30:31.702732 22579586809984 run.py:483] Algo bellman_ford step 5068 current loss 0.071823, current_train_items 162208.
I0304 19:30:31.737212 22579586809984 run.py:483] Algo bellman_ford step 5069 current loss 0.115197, current_train_items 162240.
I0304 19:30:31.757077 22579586809984 run.py:483] Algo bellman_ford step 5070 current loss 0.003474, current_train_items 162272.
I0304 19:30:31.773877 22579586809984 run.py:483] Algo bellman_ford step 5071 current loss 0.021491, current_train_items 162304.
I0304 19:30:31.796231 22579586809984 run.py:483] Algo bellman_ford step 5072 current loss 0.062464, current_train_items 162336.
I0304 19:30:31.828173 22579586809984 run.py:483] Algo bellman_ford step 5073 current loss 0.086287, current_train_items 162368.
I0304 19:30:31.859432 22579586809984 run.py:483] Algo bellman_ford step 5074 current loss 0.041267, current_train_items 162400.
I0304 19:30:31.879359 22579586809984 run.py:483] Algo bellman_ford step 5075 current loss 0.002965, current_train_items 162432.
I0304 19:30:31.895619 22579586809984 run.py:483] Algo bellman_ford step 5076 current loss 0.016018, current_train_items 162464.
I0304 19:30:31.919051 22579586809984 run.py:483] Algo bellman_ford step 5077 current loss 0.035055, current_train_items 162496.
I0304 19:30:31.950080 22579586809984 run.py:483] Algo bellman_ford step 5078 current loss 0.071600, current_train_items 162528.
I0304 19:30:31.982646 22579586809984 run.py:483] Algo bellman_ford step 5079 current loss 0.077451, current_train_items 162560.
I0304 19:30:32.002223 22579586809984 run.py:483] Algo bellman_ford step 5080 current loss 0.004758, current_train_items 162592.
I0304 19:30:32.018133 22579586809984 run.py:483] Algo bellman_ford step 5081 current loss 0.014910, current_train_items 162624.
I0304 19:30:32.041887 22579586809984 run.py:483] Algo bellman_ford step 5082 current loss 0.042937, current_train_items 162656.
I0304 19:30:32.073362 22579586809984 run.py:483] Algo bellman_ford step 5083 current loss 0.066442, current_train_items 162688.
I0304 19:30:32.106791 22579586809984 run.py:483] Algo bellman_ford step 5084 current loss 0.042539, current_train_items 162720.
I0304 19:30:32.126852 22579586809984 run.py:483] Algo bellman_ford step 5085 current loss 0.003572, current_train_items 162752.
I0304 19:30:32.143104 22579586809984 run.py:483] Algo bellman_ford step 5086 current loss 0.028883, current_train_items 162784.
I0304 19:30:32.166976 22579586809984 run.py:483] Algo bellman_ford step 5087 current loss 0.105743, current_train_items 162816.
I0304 19:30:32.198314 22579586809984 run.py:483] Algo bellman_ford step 5088 current loss 0.045142, current_train_items 162848.
I0304 19:30:32.232898 22579586809984 run.py:483] Algo bellman_ford step 5089 current loss 0.065138, current_train_items 162880.
I0304 19:30:32.252649 22579586809984 run.py:483] Algo bellman_ford step 5090 current loss 0.003083, current_train_items 162912.
I0304 19:30:32.269162 22579586809984 run.py:483] Algo bellman_ford step 5091 current loss 0.053947, current_train_items 162944.
I0304 19:30:32.291436 22579586809984 run.py:483] Algo bellman_ford step 5092 current loss 0.017101, current_train_items 162976.
I0304 19:30:32.321669 22579586809984 run.py:483] Algo bellman_ford step 5093 current loss 0.052867, current_train_items 163008.
I0304 19:30:32.354279 22579586809984 run.py:483] Algo bellman_ford step 5094 current loss 0.093170, current_train_items 163040.
I0304 19:30:32.373774 22579586809984 run.py:483] Algo bellman_ford step 5095 current loss 0.002455, current_train_items 163072.
I0304 19:30:32.390038 22579586809984 run.py:483] Algo bellman_ford step 5096 current loss 0.012867, current_train_items 163104.
I0304 19:30:32.414512 22579586809984 run.py:483] Algo bellman_ford step 5097 current loss 0.052501, current_train_items 163136.
I0304 19:30:32.446484 22579586809984 run.py:483] Algo bellman_ford step 5098 current loss 0.060087, current_train_items 163168.
I0304 19:30:32.481262 22579586809984 run.py:483] Algo bellman_ford step 5099 current loss 0.047106, current_train_items 163200.
I0304 19:30:32.501170 22579586809984 run.py:483] Algo bellman_ford step 5100 current loss 0.003934, current_train_items 163232.
I0304 19:30:32.508789 22579586809984 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0304 19:30:32.508895 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:32.525544 22579586809984 run.py:483] Algo bellman_ford step 5101 current loss 0.063383, current_train_items 163264.
I0304 19:30:32.548527 22579586809984 run.py:483] Algo bellman_ford step 5102 current loss 0.008589, current_train_items 163296.
I0304 19:30:32.580419 22579586809984 run.py:483] Algo bellman_ford step 5103 current loss 0.043135, current_train_items 163328.
I0304 19:30:32.615306 22579586809984 run.py:483] Algo bellman_ford step 5104 current loss 0.060395, current_train_items 163360.
I0304 19:30:32.635740 22579586809984 run.py:483] Algo bellman_ford step 5105 current loss 0.002907, current_train_items 163392.
I0304 19:30:32.652340 22579586809984 run.py:483] Algo bellman_ford step 5106 current loss 0.021524, current_train_items 163424.
I0304 19:30:32.676586 22579586809984 run.py:483] Algo bellman_ford step 5107 current loss 0.050513, current_train_items 163456.
I0304 19:30:32.706602 22579586809984 run.py:483] Algo bellman_ford step 5108 current loss 0.026865, current_train_items 163488.
I0304 19:30:32.740977 22579586809984 run.py:483] Algo bellman_ford step 5109 current loss 0.073867, current_train_items 163520.
I0304 19:30:32.760933 22579586809984 run.py:483] Algo bellman_ford step 5110 current loss 0.002440, current_train_items 163552.
I0304 19:30:32.777332 22579586809984 run.py:483] Algo bellman_ford step 5111 current loss 0.025430, current_train_items 163584.
I0304 19:30:32.801188 22579586809984 run.py:483] Algo bellman_ford step 5112 current loss 0.018957, current_train_items 163616.
I0304 19:30:32.831780 22579586809984 run.py:483] Algo bellman_ford step 5113 current loss 0.033093, current_train_items 163648.
I0304 19:30:32.864865 22579586809984 run.py:483] Algo bellman_ford step 5114 current loss 0.043807, current_train_items 163680.
I0304 19:30:32.884503 22579586809984 run.py:483] Algo bellman_ford step 5115 current loss 0.002229, current_train_items 163712.
I0304 19:30:32.900998 22579586809984 run.py:483] Algo bellman_ford step 5116 current loss 0.022099, current_train_items 163744.
I0304 19:30:32.925799 22579586809984 run.py:483] Algo bellman_ford step 5117 current loss 0.049374, current_train_items 163776.
I0304 19:30:32.956503 22579586809984 run.py:483] Algo bellman_ford step 5118 current loss 0.039194, current_train_items 163808.
I0304 19:30:32.990858 22579586809984 run.py:483] Algo bellman_ford step 5119 current loss 0.038922, current_train_items 163840.
I0304 19:30:33.010627 22579586809984 run.py:483] Algo bellman_ford step 5120 current loss 0.009958, current_train_items 163872.
I0304 19:30:33.026478 22579586809984 run.py:483] Algo bellman_ford step 5121 current loss 0.007180, current_train_items 163904.
I0304 19:30:33.050053 22579586809984 run.py:483] Algo bellman_ford step 5122 current loss 0.043064, current_train_items 163936.
I0304 19:30:33.080621 22579586809984 run.py:483] Algo bellman_ford step 5123 current loss 0.026640, current_train_items 163968.
I0304 19:30:33.114377 22579586809984 run.py:483] Algo bellman_ford step 5124 current loss 0.065319, current_train_items 164000.
I0304 19:30:33.133760 22579586809984 run.py:483] Algo bellman_ford step 5125 current loss 0.002472, current_train_items 164032.
I0304 19:30:33.149908 22579586809984 run.py:483] Algo bellman_ford step 5126 current loss 0.030114, current_train_items 164064.
I0304 19:30:33.174131 22579586809984 run.py:483] Algo bellman_ford step 5127 current loss 0.055284, current_train_items 164096.
I0304 19:30:33.205406 22579586809984 run.py:483] Algo bellman_ford step 5128 current loss 0.049240, current_train_items 164128.
I0304 19:30:33.240141 22579586809984 run.py:483] Algo bellman_ford step 5129 current loss 0.043861, current_train_items 164160.
I0304 19:30:33.259696 22579586809984 run.py:483] Algo bellman_ford step 5130 current loss 0.001965, current_train_items 164192.
I0304 19:30:33.276259 22579586809984 run.py:483] Algo bellman_ford step 5131 current loss 0.025409, current_train_items 164224.
I0304 19:30:33.299881 22579586809984 run.py:483] Algo bellman_ford step 5132 current loss 0.106763, current_train_items 164256.
I0304 19:30:33.331782 22579586809984 run.py:483] Algo bellman_ford step 5133 current loss 0.059566, current_train_items 164288.
I0304 19:30:33.364624 22579586809984 run.py:483] Algo bellman_ford step 5134 current loss 0.055709, current_train_items 164320.
I0304 19:30:33.384489 22579586809984 run.py:483] Algo bellman_ford step 5135 current loss 0.009382, current_train_items 164352.
I0304 19:30:33.400976 22579586809984 run.py:483] Algo bellman_ford step 5136 current loss 0.010959, current_train_items 164384.
I0304 19:30:33.425113 22579586809984 run.py:483] Algo bellman_ford step 5137 current loss 0.041127, current_train_items 164416.
I0304 19:30:33.456093 22579586809984 run.py:483] Algo bellman_ford step 5138 current loss 0.069248, current_train_items 164448.
I0304 19:30:33.490275 22579586809984 run.py:483] Algo bellman_ford step 5139 current loss 0.040562, current_train_items 164480.
I0304 19:30:33.510288 22579586809984 run.py:483] Algo bellman_ford step 5140 current loss 0.002723, current_train_items 164512.
I0304 19:30:33.526833 22579586809984 run.py:483] Algo bellman_ford step 5141 current loss 0.009916, current_train_items 164544.
I0304 19:30:33.551731 22579586809984 run.py:483] Algo bellman_ford step 5142 current loss 0.039323, current_train_items 164576.
I0304 19:30:33.583013 22579586809984 run.py:483] Algo bellman_ford step 5143 current loss 0.061377, current_train_items 164608.
I0304 19:30:33.616634 22579586809984 run.py:483] Algo bellman_ford step 5144 current loss 0.049993, current_train_items 164640.
I0304 19:30:33.636401 22579586809984 run.py:483] Algo bellman_ford step 5145 current loss 0.004260, current_train_items 164672.
I0304 19:30:33.652436 22579586809984 run.py:483] Algo bellman_ford step 5146 current loss 0.018837, current_train_items 164704.
I0304 19:30:33.675210 22579586809984 run.py:483] Algo bellman_ford step 5147 current loss 0.030328, current_train_items 164736.
I0304 19:30:33.706334 22579586809984 run.py:483] Algo bellman_ford step 5148 current loss 0.059131, current_train_items 164768.
I0304 19:30:33.739281 22579586809984 run.py:483] Algo bellman_ford step 5149 current loss 0.088309, current_train_items 164800.
I0304 19:30:33.758864 22579586809984 run.py:483] Algo bellman_ford step 5150 current loss 0.003630, current_train_items 164832.
I0304 19:30:33.766606 22579586809984 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0304 19:30:33.766721 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:33.783630 22579586809984 run.py:483] Algo bellman_ford step 5151 current loss 0.017607, current_train_items 164864.
I0304 19:30:33.807595 22579586809984 run.py:483] Algo bellman_ford step 5152 current loss 0.024636, current_train_items 164896.
I0304 19:30:33.838039 22579586809984 run.py:483] Algo bellman_ford step 5153 current loss 0.070374, current_train_items 164928.
I0304 19:30:33.871481 22579586809984 run.py:483] Algo bellman_ford step 5154 current loss 0.111332, current_train_items 164960.
I0304 19:30:33.891314 22579586809984 run.py:483] Algo bellman_ford step 5155 current loss 0.011714, current_train_items 164992.
I0304 19:30:33.907358 22579586809984 run.py:483] Algo bellman_ford step 5156 current loss 0.007107, current_train_items 165024.
I0304 19:30:33.932092 22579586809984 run.py:483] Algo bellman_ford step 5157 current loss 0.024057, current_train_items 165056.
I0304 19:30:33.964693 22579586809984 run.py:483] Algo bellman_ford step 5158 current loss 0.055050, current_train_items 165088.
I0304 19:30:33.997545 22579586809984 run.py:483] Algo bellman_ford step 5159 current loss 0.043985, current_train_items 165120.
I0304 19:30:34.017400 22579586809984 run.py:483] Algo bellman_ford step 5160 current loss 0.003715, current_train_items 165152.
I0304 19:30:34.033524 22579586809984 run.py:483] Algo bellman_ford step 5161 current loss 0.019783, current_train_items 165184.
I0304 19:30:34.058054 22579586809984 run.py:483] Algo bellman_ford step 5162 current loss 0.094046, current_train_items 165216.
I0304 19:30:34.090805 22579586809984 run.py:483] Algo bellman_ford step 5163 current loss 0.055750, current_train_items 165248.
I0304 19:30:34.123005 22579586809984 run.py:483] Algo bellman_ford step 5164 current loss 0.058624, current_train_items 165280.
I0304 19:30:34.142338 22579586809984 run.py:483] Algo bellman_ford step 5165 current loss 0.002582, current_train_items 165312.
I0304 19:30:34.158409 22579586809984 run.py:483] Algo bellman_ford step 5166 current loss 0.013767, current_train_items 165344.
I0304 19:30:34.181442 22579586809984 run.py:483] Algo bellman_ford step 5167 current loss 0.100839, current_train_items 165376.
I0304 19:30:34.212995 22579586809984 run.py:483] Algo bellman_ford step 5168 current loss 0.045889, current_train_items 165408.
I0304 19:30:34.247694 22579586809984 run.py:483] Algo bellman_ford step 5169 current loss 0.068258, current_train_items 165440.
I0304 19:30:34.267699 22579586809984 run.py:483] Algo bellman_ford step 5170 current loss 0.005487, current_train_items 165472.
I0304 19:30:34.284142 22579586809984 run.py:483] Algo bellman_ford step 5171 current loss 0.008259, current_train_items 165504.
I0304 19:30:34.308211 22579586809984 run.py:483] Algo bellman_ford step 5172 current loss 0.088776, current_train_items 165536.
I0304 19:30:34.340275 22579586809984 run.py:483] Algo bellman_ford step 5173 current loss 0.097609, current_train_items 165568.
I0304 19:30:34.373263 22579586809984 run.py:483] Algo bellman_ford step 5174 current loss 0.073013, current_train_items 165600.
I0304 19:30:34.392857 22579586809984 run.py:483] Algo bellman_ford step 5175 current loss 0.003337, current_train_items 165632.
I0304 19:30:34.409426 22579586809984 run.py:483] Algo bellman_ford step 5176 current loss 0.020416, current_train_items 165664.
I0304 19:30:34.433353 22579586809984 run.py:483] Algo bellman_ford step 5177 current loss 0.075369, current_train_items 165696.
I0304 19:30:34.464163 22579586809984 run.py:483] Algo bellman_ford step 5178 current loss 0.050859, current_train_items 165728.
I0304 19:30:34.496526 22579586809984 run.py:483] Algo bellman_ford step 5179 current loss 0.048598, current_train_items 165760.
I0304 19:30:34.515940 22579586809984 run.py:483] Algo bellman_ford step 5180 current loss 0.001929, current_train_items 165792.
I0304 19:30:34.532086 22579586809984 run.py:483] Algo bellman_ford step 5181 current loss 0.021920, current_train_items 165824.
I0304 19:30:34.555825 22579586809984 run.py:483] Algo bellman_ford step 5182 current loss 0.057271, current_train_items 165856.
I0304 19:30:34.586900 22579586809984 run.py:483] Algo bellman_ford step 5183 current loss 0.090201, current_train_items 165888.
I0304 19:30:34.619023 22579586809984 run.py:483] Algo bellman_ford step 5184 current loss 0.043701, current_train_items 165920.
I0304 19:30:34.638863 22579586809984 run.py:483] Algo bellman_ford step 5185 current loss 0.009429, current_train_items 165952.
I0304 19:30:34.655600 22579586809984 run.py:483] Algo bellman_ford step 5186 current loss 0.053746, current_train_items 165984.
I0304 19:30:34.678153 22579586809984 run.py:483] Algo bellman_ford step 5187 current loss 0.076997, current_train_items 166016.
I0304 19:30:34.710003 22579586809984 run.py:483] Algo bellman_ford step 5188 current loss 0.160580, current_train_items 166048.
I0304 19:30:34.742416 22579586809984 run.py:483] Algo bellman_ford step 5189 current loss 0.138397, current_train_items 166080.
I0304 19:30:34.762402 22579586809984 run.py:483] Algo bellman_ford step 5190 current loss 0.006850, current_train_items 166112.
I0304 19:30:34.778723 22579586809984 run.py:483] Algo bellman_ford step 5191 current loss 0.012033, current_train_items 166144.
I0304 19:30:34.802706 22579586809984 run.py:483] Algo bellman_ford step 5192 current loss 0.045873, current_train_items 166176.
I0304 19:30:34.833294 22579586809984 run.py:483] Algo bellman_ford step 5193 current loss 0.059353, current_train_items 166208.
I0304 19:30:34.866929 22579586809984 run.py:483] Algo bellman_ford step 5194 current loss 0.078725, current_train_items 166240.
I0304 19:30:34.887101 22579586809984 run.py:483] Algo bellman_ford step 5195 current loss 0.002320, current_train_items 166272.
I0304 19:30:34.903547 22579586809984 run.py:483] Algo bellman_ford step 5196 current loss 0.007192, current_train_items 166304.
I0304 19:30:34.927820 22579586809984 run.py:483] Algo bellman_ford step 5197 current loss 0.041286, current_train_items 166336.
I0304 19:30:34.960310 22579586809984 run.py:483] Algo bellman_ford step 5198 current loss 0.085583, current_train_items 166368.
I0304 19:30:34.993360 22579586809984 run.py:483] Algo bellman_ford step 5199 current loss 0.098158, current_train_items 166400.
I0304 19:30:35.013266 22579586809984 run.py:483] Algo bellman_ford step 5200 current loss 0.005482, current_train_items 166432.
I0304 19:30:35.021488 22579586809984 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0304 19:30:35.021595 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:35.038671 22579586809984 run.py:483] Algo bellman_ford step 5201 current loss 0.020446, current_train_items 166464.
I0304 19:30:35.063571 22579586809984 run.py:483] Algo bellman_ford step 5202 current loss 0.021952, current_train_items 166496.
I0304 19:30:35.096243 22579586809984 run.py:483] Algo bellman_ford step 5203 current loss 0.043870, current_train_items 166528.
I0304 19:30:35.131863 22579586809984 run.py:483] Algo bellman_ford step 5204 current loss 0.041113, current_train_items 166560.
I0304 19:30:35.151663 22579586809984 run.py:483] Algo bellman_ford step 5205 current loss 0.003469, current_train_items 166592.
I0304 19:30:35.167497 22579586809984 run.py:483] Algo bellman_ford step 5206 current loss 0.011560, current_train_items 166624.
I0304 19:30:35.191803 22579586809984 run.py:483] Algo bellman_ford step 5207 current loss 0.040287, current_train_items 166656.
I0304 19:30:35.221877 22579586809984 run.py:483] Algo bellman_ford step 5208 current loss 0.048239, current_train_items 166688.
I0304 19:30:35.258230 22579586809984 run.py:483] Algo bellman_ford step 5209 current loss 0.049256, current_train_items 166720.
I0304 19:30:35.278178 22579586809984 run.py:483] Algo bellman_ford step 5210 current loss 0.004627, current_train_items 166752.
I0304 19:30:35.294110 22579586809984 run.py:483] Algo bellman_ford step 5211 current loss 0.015638, current_train_items 166784.
I0304 19:30:35.318800 22579586809984 run.py:483] Algo bellman_ford step 5212 current loss 0.038853, current_train_items 166816.
I0304 19:30:35.351799 22579586809984 run.py:483] Algo bellman_ford step 5213 current loss 0.057524, current_train_items 166848.
I0304 19:30:35.385107 22579586809984 run.py:483] Algo bellman_ford step 5214 current loss 0.052514, current_train_items 166880.
I0304 19:30:35.405159 22579586809984 run.py:483] Algo bellman_ford step 5215 current loss 0.002166, current_train_items 166912.
I0304 19:30:35.421720 22579586809984 run.py:483] Algo bellman_ford step 5216 current loss 0.018674, current_train_items 166944.
I0304 19:30:35.445513 22579586809984 run.py:483] Algo bellman_ford step 5217 current loss 0.035305, current_train_items 166976.
I0304 19:30:35.475991 22579586809984 run.py:483] Algo bellman_ford step 5218 current loss 0.031511, current_train_items 167008.
I0304 19:30:35.509655 22579586809984 run.py:483] Algo bellman_ford step 5219 current loss 0.058663, current_train_items 167040.
I0304 19:30:35.529333 22579586809984 run.py:483] Algo bellman_ford step 5220 current loss 0.006211, current_train_items 167072.
I0304 19:30:35.545435 22579586809984 run.py:483] Algo bellman_ford step 5221 current loss 0.013139, current_train_items 167104.
I0304 19:30:35.568148 22579586809984 run.py:483] Algo bellman_ford step 5222 current loss 0.027066, current_train_items 167136.
I0304 19:30:35.600303 22579586809984 run.py:483] Algo bellman_ford step 5223 current loss 0.078804, current_train_items 167168.
I0304 19:30:35.633525 22579586809984 run.py:483] Algo bellman_ford step 5224 current loss 0.040297, current_train_items 167200.
I0304 19:30:35.653465 22579586809984 run.py:483] Algo bellman_ford step 5225 current loss 0.003132, current_train_items 167232.
I0304 19:30:35.669484 22579586809984 run.py:483] Algo bellman_ford step 5226 current loss 0.016013, current_train_items 167264.
I0304 19:30:35.693929 22579586809984 run.py:483] Algo bellman_ford step 5227 current loss 0.056143, current_train_items 167296.
I0304 19:30:35.726404 22579586809984 run.py:483] Algo bellman_ford step 5228 current loss 0.057835, current_train_items 167328.
I0304 19:30:35.761201 22579586809984 run.py:483] Algo bellman_ford step 5229 current loss 0.062993, current_train_items 167360.
I0304 19:30:35.780641 22579586809984 run.py:483] Algo bellman_ford step 5230 current loss 0.003039, current_train_items 167392.
I0304 19:30:35.797043 22579586809984 run.py:483] Algo bellman_ford step 5231 current loss 0.028812, current_train_items 167424.
I0304 19:30:35.821759 22579586809984 run.py:483] Algo bellman_ford step 5232 current loss 0.022752, current_train_items 167456.
I0304 19:30:35.852613 22579586809984 run.py:483] Algo bellman_ford step 5233 current loss 0.093907, current_train_items 167488.
I0304 19:30:35.887780 22579586809984 run.py:483] Algo bellman_ford step 5234 current loss 0.103482, current_train_items 167520.
I0304 19:30:35.907327 22579586809984 run.py:483] Algo bellman_ford step 5235 current loss 0.002541, current_train_items 167552.
I0304 19:30:35.923447 22579586809984 run.py:483] Algo bellman_ford step 5236 current loss 0.013696, current_train_items 167584.
I0304 19:30:35.947434 22579586809984 run.py:483] Algo bellman_ford step 5237 current loss 0.043720, current_train_items 167616.
I0304 19:30:35.980429 22579586809984 run.py:483] Algo bellman_ford step 5238 current loss 0.075399, current_train_items 167648.
I0304 19:30:36.014908 22579586809984 run.py:483] Algo bellman_ford step 5239 current loss 0.075831, current_train_items 167680.
I0304 19:30:36.034579 22579586809984 run.py:483] Algo bellman_ford step 5240 current loss 0.023165, current_train_items 167712.
I0304 19:30:36.050971 22579586809984 run.py:483] Algo bellman_ford step 5241 current loss 0.028433, current_train_items 167744.
I0304 19:30:36.074975 22579586809984 run.py:483] Algo bellman_ford step 5242 current loss 0.052714, current_train_items 167776.
I0304 19:30:36.106216 22579586809984 run.py:483] Algo bellman_ford step 5243 current loss 0.144587, current_train_items 167808.
I0304 19:30:36.137873 22579586809984 run.py:483] Algo bellman_ford step 5244 current loss 0.065023, current_train_items 167840.
I0304 19:30:36.157055 22579586809984 run.py:483] Algo bellman_ford step 5245 current loss 0.006036, current_train_items 167872.
I0304 19:30:36.173301 22579586809984 run.py:483] Algo bellman_ford step 5246 current loss 0.015173, current_train_items 167904.
I0304 19:30:36.197845 22579586809984 run.py:483] Algo bellman_ford step 5247 current loss 0.019575, current_train_items 167936.
I0304 19:30:36.229810 22579586809984 run.py:483] Algo bellman_ford step 5248 current loss 0.029139, current_train_items 167968.
I0304 19:30:36.263888 22579586809984 run.py:483] Algo bellman_ford step 5249 current loss 0.057300, current_train_items 168000.
I0304 19:30:36.283386 22579586809984 run.py:483] Algo bellman_ford step 5250 current loss 0.002822, current_train_items 168032.
I0304 19:30:36.291152 22579586809984 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0304 19:30:36.291258 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:36.308063 22579586809984 run.py:483] Algo bellman_ford step 5251 current loss 0.010398, current_train_items 168064.
I0304 19:30:36.332859 22579586809984 run.py:483] Algo bellman_ford step 5252 current loss 0.090278, current_train_items 168096.
I0304 19:30:36.365199 22579586809984 run.py:483] Algo bellman_ford step 5253 current loss 0.054313, current_train_items 168128.
I0304 19:30:36.396757 22579586809984 run.py:483] Algo bellman_ford step 5254 current loss 0.034154, current_train_items 168160.
I0304 19:30:36.416606 22579586809984 run.py:483] Algo bellman_ford step 5255 current loss 0.028354, current_train_items 168192.
I0304 19:30:36.432553 22579586809984 run.py:483] Algo bellman_ford step 5256 current loss 0.005956, current_train_items 168224.
I0304 19:30:36.455749 22579586809984 run.py:483] Algo bellman_ford step 5257 current loss 0.044684, current_train_items 168256.
I0304 19:30:36.487494 22579586809984 run.py:483] Algo bellman_ford step 5258 current loss 0.059653, current_train_items 168288.
I0304 19:30:36.519092 22579586809984 run.py:483] Algo bellman_ford step 5259 current loss 0.027321, current_train_items 168320.
I0304 19:30:36.539026 22579586809984 run.py:483] Algo bellman_ford step 5260 current loss 0.002654, current_train_items 168352.
I0304 19:30:36.555505 22579586809984 run.py:483] Algo bellman_ford step 5261 current loss 0.016646, current_train_items 168384.
I0304 19:30:36.578635 22579586809984 run.py:483] Algo bellman_ford step 5262 current loss 0.056577, current_train_items 168416.
I0304 19:30:36.610764 22579586809984 run.py:483] Algo bellman_ford step 5263 current loss 0.040836, current_train_items 168448.
I0304 19:30:36.643167 22579586809984 run.py:483] Algo bellman_ford step 5264 current loss 0.045985, current_train_items 168480.
I0304 19:30:36.662880 22579586809984 run.py:483] Algo bellman_ford step 5265 current loss 0.007628, current_train_items 168512.
I0304 19:30:36.679178 22579586809984 run.py:483] Algo bellman_ford step 5266 current loss 0.020183, current_train_items 168544.
I0304 19:30:36.703913 22579586809984 run.py:483] Algo bellman_ford step 5267 current loss 0.059674, current_train_items 168576.
I0304 19:30:36.734523 22579586809984 run.py:483] Algo bellman_ford step 5268 current loss 0.048783, current_train_items 168608.
I0304 19:30:36.766443 22579586809984 run.py:483] Algo bellman_ford step 5269 current loss 0.041874, current_train_items 168640.
I0304 19:30:36.786091 22579586809984 run.py:483] Algo bellman_ford step 5270 current loss 0.002194, current_train_items 168672.
I0304 19:30:36.802241 22579586809984 run.py:483] Algo bellman_ford step 5271 current loss 0.012124, current_train_items 168704.
I0304 19:30:36.825005 22579586809984 run.py:483] Algo bellman_ford step 5272 current loss 0.043973, current_train_items 168736.
I0304 19:30:36.855934 22579586809984 run.py:483] Algo bellman_ford step 5273 current loss 0.041386, current_train_items 168768.
I0304 19:30:36.888770 22579586809984 run.py:483] Algo bellman_ford step 5274 current loss 0.045794, current_train_items 168800.
I0304 19:30:36.908653 22579586809984 run.py:483] Algo bellman_ford step 5275 current loss 0.002903, current_train_items 168832.
I0304 19:30:36.924978 22579586809984 run.py:483] Algo bellman_ford step 5276 current loss 0.011865, current_train_items 168864.
I0304 19:30:36.948169 22579586809984 run.py:483] Algo bellman_ford step 5277 current loss 0.029525, current_train_items 168896.
I0304 19:30:36.979225 22579586809984 run.py:483] Algo bellman_ford step 5278 current loss 0.070912, current_train_items 168928.
I0304 19:30:37.013299 22579586809984 run.py:483] Algo bellman_ford step 5279 current loss 0.053213, current_train_items 168960.
I0304 19:30:37.032612 22579586809984 run.py:483] Algo bellman_ford step 5280 current loss 0.002315, current_train_items 168992.
I0304 19:30:37.049139 22579586809984 run.py:483] Algo bellman_ford step 5281 current loss 0.031749, current_train_items 169024.
I0304 19:30:37.074045 22579586809984 run.py:483] Algo bellman_ford step 5282 current loss 0.057626, current_train_items 169056.
I0304 19:30:37.105594 22579586809984 run.py:483] Algo bellman_ford step 5283 current loss 0.055063, current_train_items 169088.
I0304 19:30:37.138603 22579586809984 run.py:483] Algo bellman_ford step 5284 current loss 0.073634, current_train_items 169120.
I0304 19:30:37.158671 22579586809984 run.py:483] Algo bellman_ford step 5285 current loss 0.005043, current_train_items 169152.
I0304 19:30:37.175134 22579586809984 run.py:483] Algo bellman_ford step 5286 current loss 0.018769, current_train_items 169184.
I0304 19:30:37.199991 22579586809984 run.py:483] Algo bellman_ford step 5287 current loss 0.054070, current_train_items 169216.
I0304 19:30:37.231149 22579586809984 run.py:483] Algo bellman_ford step 5288 current loss 0.112991, current_train_items 169248.
I0304 19:30:37.265468 22579586809984 run.py:483] Algo bellman_ford step 5289 current loss 0.048907, current_train_items 169280.
I0304 19:30:37.285252 22579586809984 run.py:483] Algo bellman_ford step 5290 current loss 0.003960, current_train_items 169312.
I0304 19:30:37.302089 22579586809984 run.py:483] Algo bellman_ford step 5291 current loss 0.016843, current_train_items 169344.
I0304 19:30:37.325472 22579586809984 run.py:483] Algo bellman_ford step 5292 current loss 0.046502, current_train_items 169376.
I0304 19:30:37.357002 22579586809984 run.py:483] Algo bellman_ford step 5293 current loss 0.041677, current_train_items 169408.
I0304 19:30:37.390144 22579586809984 run.py:483] Algo bellman_ford step 5294 current loss 0.073099, current_train_items 169440.
I0304 19:30:37.409486 22579586809984 run.py:483] Algo bellman_ford step 5295 current loss 0.004367, current_train_items 169472.
I0304 19:30:37.425545 22579586809984 run.py:483] Algo bellman_ford step 5296 current loss 0.011980, current_train_items 169504.
I0304 19:30:37.448792 22579586809984 run.py:483] Algo bellman_ford step 5297 current loss 0.046341, current_train_items 169536.
I0304 19:30:37.480774 22579586809984 run.py:483] Algo bellman_ford step 5298 current loss 0.066611, current_train_items 169568.
I0304 19:30:37.516783 22579586809984 run.py:483] Algo bellman_ford step 5299 current loss 0.104830, current_train_items 169600.
I0304 19:30:37.536772 22579586809984 run.py:483] Algo bellman_ford step 5300 current loss 0.004147, current_train_items 169632.
I0304 19:30:37.544301 22579586809984 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0304 19:30:37.544407 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:37.560936 22579586809984 run.py:483] Algo bellman_ford step 5301 current loss 0.014366, current_train_items 169664.
I0304 19:30:37.585623 22579586809984 run.py:483] Algo bellman_ford step 5302 current loss 0.055163, current_train_items 169696.
I0304 19:30:37.618617 22579586809984 run.py:483] Algo bellman_ford step 5303 current loss 0.050640, current_train_items 169728.
I0304 19:30:37.653256 22579586809984 run.py:483] Algo bellman_ford step 5304 current loss 0.050767, current_train_items 169760.
I0304 19:30:37.673218 22579586809984 run.py:483] Algo bellman_ford step 5305 current loss 0.005424, current_train_items 169792.
I0304 19:30:37.689393 22579586809984 run.py:483] Algo bellman_ford step 5306 current loss 0.019965, current_train_items 169824.
I0304 19:30:37.714579 22579586809984 run.py:483] Algo bellman_ford step 5307 current loss 0.038840, current_train_items 169856.
I0304 19:30:37.746796 22579586809984 run.py:483] Algo bellman_ford step 5308 current loss 0.041580, current_train_items 169888.
I0304 19:30:37.778964 22579586809984 run.py:483] Algo bellman_ford step 5309 current loss 0.045536, current_train_items 169920.
I0304 19:30:37.798496 22579586809984 run.py:483] Algo bellman_ford step 5310 current loss 0.002845, current_train_items 169952.
I0304 19:30:37.814201 22579586809984 run.py:483] Algo bellman_ford step 5311 current loss 0.008169, current_train_items 169984.
I0304 19:30:37.838085 22579586809984 run.py:483] Algo bellman_ford step 5312 current loss 0.013219, current_train_items 170016.
I0304 19:30:37.869465 22579586809984 run.py:483] Algo bellman_ford step 5313 current loss 0.059763, current_train_items 170048.
I0304 19:30:37.904048 22579586809984 run.py:483] Algo bellman_ford step 5314 current loss 0.140521, current_train_items 170080.
I0304 19:30:37.923909 22579586809984 run.py:483] Algo bellman_ford step 5315 current loss 0.004891, current_train_items 170112.
I0304 19:30:37.940090 22579586809984 run.py:483] Algo bellman_ford step 5316 current loss 0.019009, current_train_items 170144.
I0304 19:30:37.964064 22579586809984 run.py:483] Algo bellman_ford step 5317 current loss 0.092736, current_train_items 170176.
I0304 19:30:37.994019 22579586809984 run.py:483] Algo bellman_ford step 5318 current loss 0.043452, current_train_items 170208.
I0304 19:30:38.028138 22579586809984 run.py:483] Algo bellman_ford step 5319 current loss 0.074442, current_train_items 170240.
I0304 19:30:38.047455 22579586809984 run.py:483] Algo bellman_ford step 5320 current loss 0.004022, current_train_items 170272.
I0304 19:30:38.063613 22579586809984 run.py:483] Algo bellman_ford step 5321 current loss 0.064717, current_train_items 170304.
I0304 19:30:38.088369 22579586809984 run.py:483] Algo bellman_ford step 5322 current loss 0.103157, current_train_items 170336.
I0304 19:30:38.118020 22579586809984 run.py:483] Algo bellman_ford step 5323 current loss 0.072647, current_train_items 170368.
I0304 19:30:38.152837 22579586809984 run.py:483] Algo bellman_ford step 5324 current loss 0.049046, current_train_items 170400.
I0304 19:30:38.172394 22579586809984 run.py:483] Algo bellman_ford step 5325 current loss 0.002412, current_train_items 170432.
I0304 19:30:38.188485 22579586809984 run.py:483] Algo bellman_ford step 5326 current loss 0.027795, current_train_items 170464.
I0304 19:30:38.211697 22579586809984 run.py:483] Algo bellman_ford step 5327 current loss 0.065176, current_train_items 170496.
I0304 19:30:38.243936 22579586809984 run.py:483] Algo bellman_ford step 5328 current loss 0.083594, current_train_items 170528.
I0304 19:30:38.276821 22579586809984 run.py:483] Algo bellman_ford step 5329 current loss 0.173907, current_train_items 170560.
I0304 19:30:38.296254 22579586809984 run.py:483] Algo bellman_ford step 5330 current loss 0.023277, current_train_items 170592.
I0304 19:30:38.312487 22579586809984 run.py:483] Algo bellman_ford step 5331 current loss 0.018955, current_train_items 170624.
I0304 19:30:38.336086 22579586809984 run.py:483] Algo bellman_ford step 5332 current loss 0.023319, current_train_items 170656.
I0304 19:30:38.366140 22579586809984 run.py:483] Algo bellman_ford step 5333 current loss 0.062770, current_train_items 170688.
I0304 19:30:38.398570 22579586809984 run.py:483] Algo bellman_ford step 5334 current loss 0.079228, current_train_items 170720.
I0304 19:30:38.418222 22579586809984 run.py:483] Algo bellman_ford step 5335 current loss 0.016601, current_train_items 170752.
I0304 19:30:38.434724 22579586809984 run.py:483] Algo bellman_ford step 5336 current loss 0.046468, current_train_items 170784.
I0304 19:30:38.458127 22579586809984 run.py:483] Algo bellman_ford step 5337 current loss 0.040001, current_train_items 170816.
I0304 19:30:38.488391 22579586809984 run.py:483] Algo bellman_ford step 5338 current loss 0.092049, current_train_items 170848.
I0304 19:30:38.522518 22579586809984 run.py:483] Algo bellman_ford step 5339 current loss 0.088075, current_train_items 170880.
I0304 19:30:38.541729 22579586809984 run.py:483] Algo bellman_ford step 5340 current loss 0.003719, current_train_items 170912.
I0304 19:30:38.558112 22579586809984 run.py:483] Algo bellman_ford step 5341 current loss 0.040815, current_train_items 170944.
I0304 19:30:38.583397 22579586809984 run.py:483] Algo bellman_ford step 5342 current loss 0.039269, current_train_items 170976.
I0304 19:30:38.614923 22579586809984 run.py:483] Algo bellman_ford step 5343 current loss 0.065492, current_train_items 171008.
I0304 19:30:38.648282 22579586809984 run.py:483] Algo bellman_ford step 5344 current loss 0.056658, current_train_items 171040.
I0304 19:30:38.667538 22579586809984 run.py:483] Algo bellman_ford step 5345 current loss 0.002776, current_train_items 171072.
I0304 19:30:38.683231 22579586809984 run.py:483] Algo bellman_ford step 5346 current loss 0.012642, current_train_items 171104.
I0304 19:30:38.706690 22579586809984 run.py:483] Algo bellman_ford step 5347 current loss 0.023398, current_train_items 171136.
I0304 19:30:38.739181 22579586809984 run.py:483] Algo bellman_ford step 5348 current loss 0.089148, current_train_items 171168.
I0304 19:30:38.773048 22579586809984 run.py:483] Algo bellman_ford step 5349 current loss 0.074915, current_train_items 171200.
I0304 19:30:38.792521 22579586809984 run.py:483] Algo bellman_ford step 5350 current loss 0.003665, current_train_items 171232.
I0304 19:30:38.800646 22579586809984 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0304 19:30:38.800758 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:30:38.817260 22579586809984 run.py:483] Algo bellman_ford step 5351 current loss 0.008274, current_train_items 171264.
I0304 19:30:38.841970 22579586809984 run.py:483] Algo bellman_ford step 5352 current loss 0.070322, current_train_items 171296.
I0304 19:30:38.872813 22579586809984 run.py:483] Algo bellman_ford step 5353 current loss 0.074604, current_train_items 171328.
I0304 19:30:38.906252 22579586809984 run.py:483] Algo bellman_ford step 5354 current loss 0.064087, current_train_items 171360.
I0304 19:30:38.926127 22579586809984 run.py:483] Algo bellman_ford step 5355 current loss 0.003339, current_train_items 171392.
I0304 19:30:38.942249 22579586809984 run.py:483] Algo bellman_ford step 5356 current loss 0.029883, current_train_items 171424.
I0304 19:30:38.966434 22579586809984 run.py:483] Algo bellman_ford step 5357 current loss 0.069251, current_train_items 171456.
I0304 19:30:38.996645 22579586809984 run.py:483] Algo bellman_ford step 5358 current loss 0.043648, current_train_items 171488.
I0304 19:30:39.030132 22579586809984 run.py:483] Algo bellman_ford step 5359 current loss 0.069190, current_train_items 171520.
I0304 19:30:39.050095 22579586809984 run.py:483] Algo bellman_ford step 5360 current loss 0.006816, current_train_items 171552.
I0304 19:30:39.066503 22579586809984 run.py:483] Algo bellman_ford step 5361 current loss 0.027758, current_train_items 171584.
I0304 19:30:39.090524 22579586809984 run.py:483] Algo bellman_ford step 5362 current loss 0.068876, current_train_items 171616.
I0304 19:30:39.122028 22579586809984 run.py:483] Algo bellman_ford step 5363 current loss 0.054823, current_train_items 171648.
I0304 19:30:39.155637 22579586809984 run.py:483] Algo bellman_ford step 5364 current loss 0.072141, current_train_items 171680.
I0304 19:30:39.175381 22579586809984 run.py:483] Algo bellman_ford step 5365 current loss 0.004781, current_train_items 171712.
I0304 19:30:39.191716 22579586809984 run.py:483] Algo bellman_ford step 5366 current loss 0.019106, current_train_items 171744.
I0304 19:30:39.215559 22579586809984 run.py:483] Algo bellman_ford step 5367 current loss 0.031744, current_train_items 171776.
I0304 19:30:39.246770 22579586809984 run.py:483] Algo bellman_ford step 5368 current loss 0.046987, current_train_items 171808.
I0304 19:30:39.279933 22579586809984 run.py:483] Algo bellman_ford step 5369 current loss 0.046792, current_train_items 171840.
I0304 19:30:39.299726 22579586809984 run.py:483] Algo bellman_ford step 5370 current loss 0.003026, current_train_items 171872.
I0304 19:30:39.316054 22579586809984 run.py:483] Algo bellman_ford step 5371 current loss 0.005615, current_train_items 171904.
I0304 19:30:39.339911 22579586809984 run.py:483] Algo bellman_ford step 5372 current loss 0.032782, current_train_items 171936.
I0304 19:30:39.371061 22579586809984 run.py:483] Algo bellman_ford step 5373 current loss 0.046381, current_train_items 171968.
I0304 19:30:39.402210 22579586809984 run.py:483] Algo bellman_ford step 5374 current loss 0.076171, current_train_items 172000.
I0304 19:30:39.421951 22579586809984 run.py:483] Algo bellman_ford step 5375 current loss 0.002886, current_train_items 172032.
I0304 19:30:39.438386 22579586809984 run.py:483] Algo bellman_ford step 5376 current loss 0.056332, current_train_items 172064.
I0304 19:30:39.461880 22579586809984 run.py:483] Algo bellman_ford step 5377 current loss 0.042783, current_train_items 172096.
I0304 19:30:39.492380 22579586809984 run.py:483] Algo bellman_ford step 5378 current loss 0.044063, current_train_items 172128.
I0304 19:30:39.523011 22579586809984 run.py:483] Algo bellman_ford step 5379 current loss 0.046687, current_train_items 172160.
I0304 19:30:39.542474 22579586809984 run.py:483] Algo bellman_ford step 5380 current loss 0.011261, current_train_items 172192.
I0304 19:30:39.558846 22579586809984 run.py:483] Algo bellman_ford step 5381 current loss 0.011808, current_train_items 172224.
I0304 19:30:39.583179 22579586809984 run.py:483] Algo bellman_ford step 5382 current loss 0.060442, current_train_items 172256.
I0304 19:30:39.615075 22579586809984 run.py:483] Algo bellman_ford step 5383 current loss 0.088202, current_train_items 172288.
I0304 19:30:39.647153 22579586809984 run.py:483] Algo bellman_ford step 5384 current loss 0.065638, current_train_items 172320.
I0304 19:30:39.667061 22579586809984 run.py:483] Algo bellman_ford step 5385 current loss 0.003219, current_train_items 172352.
I0304 19:30:39.683030 22579586809984 run.py:483] Algo bellman_ford step 5386 current loss 0.016484, current_train_items 172384.
I0304 19:30:39.706237 22579586809984 run.py:483] Algo bellman_ford step 5387 current loss 0.025657, current_train_items 172416.
I0304 19:30:39.736443 22579586809984 run.py:483] Algo bellman_ford step 5388 current loss 0.026498, current_train_items 172448.
I0304 19:30:39.770245 22579586809984 run.py:483] Algo bellman_ford step 5389 current loss 0.120264, current_train_items 172480.
I0304 19:30:39.790270 22579586809984 run.py:483] Algo bellman_ford step 5390 current loss 0.005432, current_train_items 172512.
I0304 19:30:39.807038 22579586809984 run.py:483] Algo bellman_ford step 5391 current loss 0.014221, current_train_items 172544.
I0304 19:30:39.830197 22579586809984 run.py:483] Algo bellman_ford step 5392 current loss 0.046852, current_train_items 172576.
I0304 19:30:39.862499 22579586809984 run.py:483] Algo bellman_ford step 5393 current loss 0.044201, current_train_items 172608.
I0304 19:30:39.897179 22579586809984 run.py:483] Algo bellman_ford step 5394 current loss 0.056873, current_train_items 172640.
I0304 19:30:39.916744 22579586809984 run.py:483] Algo bellman_ford step 5395 current loss 0.003764, current_train_items 172672.
I0304 19:30:39.933530 22579586809984 run.py:483] Algo bellman_ford step 5396 current loss 0.038253, current_train_items 172704.
I0304 19:30:39.957549 22579586809984 run.py:483] Algo bellman_ford step 5397 current loss 0.034267, current_train_items 172736.
I0304 19:30:39.988590 22579586809984 run.py:483] Algo bellman_ford step 5398 current loss 0.035101, current_train_items 172768.
I0304 19:30:40.023649 22579586809984 run.py:483] Algo bellman_ford step 5399 current loss 0.116454, current_train_items 172800.
I0304 19:30:40.043560 22579586809984 run.py:483] Algo bellman_ford step 5400 current loss 0.002391, current_train_items 172832.
I0304 19:30:40.051249 22579586809984 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0304 19:30:40.051354 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:40.068729 22579586809984 run.py:483] Algo bellman_ford step 5401 current loss 0.033434, current_train_items 172864.
I0304 19:30:40.094049 22579586809984 run.py:483] Algo bellman_ford step 5402 current loss 0.047093, current_train_items 172896.
I0304 19:30:40.127587 22579586809984 run.py:483] Algo bellman_ford step 5403 current loss 0.078645, current_train_items 172928.
I0304 19:30:40.162914 22579586809984 run.py:483] Algo bellman_ford step 5404 current loss 0.051501, current_train_items 172960.
I0304 19:30:40.183116 22579586809984 run.py:483] Algo bellman_ford step 5405 current loss 0.003199, current_train_items 172992.
I0304 19:30:40.198854 22579586809984 run.py:483] Algo bellman_ford step 5406 current loss 0.043153, current_train_items 173024.
I0304 19:30:40.222441 22579586809984 run.py:483] Algo bellman_ford step 5407 current loss 0.016781, current_train_items 173056.
I0304 19:30:40.252050 22579586809984 run.py:483] Algo bellman_ford step 5408 current loss 0.044544, current_train_items 173088.
I0304 19:30:40.285255 22579586809984 run.py:483] Algo bellman_ford step 5409 current loss 0.042118, current_train_items 173120.
I0304 19:30:40.305316 22579586809984 run.py:483] Algo bellman_ford step 5410 current loss 0.010617, current_train_items 173152.
I0304 19:30:40.322223 22579586809984 run.py:483] Algo bellman_ford step 5411 current loss 0.028225, current_train_items 173184.
I0304 19:30:40.346663 22579586809984 run.py:483] Algo bellman_ford step 5412 current loss 0.088821, current_train_items 173216.
I0304 19:30:40.378254 22579586809984 run.py:483] Algo bellman_ford step 5413 current loss 0.040877, current_train_items 173248.
I0304 19:30:40.412732 22579586809984 run.py:483] Algo bellman_ford step 5414 current loss 0.086864, current_train_items 173280.
I0304 19:30:40.432660 22579586809984 run.py:483] Algo bellman_ford step 5415 current loss 0.020584, current_train_items 173312.
I0304 19:30:40.449180 22579586809984 run.py:483] Algo bellman_ford step 5416 current loss 0.018869, current_train_items 173344.
I0304 19:30:40.473873 22579586809984 run.py:483] Algo bellman_ford step 5417 current loss 0.045589, current_train_items 173376.
I0304 19:30:40.506310 22579586809984 run.py:483] Algo bellman_ford step 5418 current loss 0.034630, current_train_items 173408.
I0304 19:30:40.541655 22579586809984 run.py:483] Algo bellman_ford step 5419 current loss 0.075177, current_train_items 173440.
I0304 19:30:40.561620 22579586809984 run.py:483] Algo bellman_ford step 5420 current loss 0.005875, current_train_items 173472.
I0304 19:30:40.578042 22579586809984 run.py:483] Algo bellman_ford step 5421 current loss 0.023892, current_train_items 173504.
I0304 19:30:40.602076 22579586809984 run.py:483] Algo bellman_ford step 5422 current loss 0.028706, current_train_items 173536.
I0304 19:30:40.632601 22579586809984 run.py:483] Algo bellman_ford step 5423 current loss 0.042548, current_train_items 173568.
I0304 19:30:40.664242 22579586809984 run.py:483] Algo bellman_ford step 5424 current loss 0.079618, current_train_items 173600.
I0304 19:30:40.684162 22579586809984 run.py:483] Algo bellman_ford step 5425 current loss 0.003821, current_train_items 173632.
I0304 19:30:40.700160 22579586809984 run.py:483] Algo bellman_ford step 5426 current loss 0.015560, current_train_items 173664.
I0304 19:30:40.724471 22579586809984 run.py:483] Algo bellman_ford step 5427 current loss 0.063249, current_train_items 173696.
I0304 19:30:40.755997 22579586809984 run.py:483] Algo bellman_ford step 5428 current loss 0.052134, current_train_items 173728.
I0304 19:30:40.790965 22579586809984 run.py:483] Algo bellman_ford step 5429 current loss 0.037986, current_train_items 173760.
I0304 19:30:40.810760 22579586809984 run.py:483] Algo bellman_ford step 5430 current loss 0.002917, current_train_items 173792.
I0304 19:30:40.827238 22579586809984 run.py:483] Algo bellman_ford step 5431 current loss 0.053492, current_train_items 173824.
I0304 19:30:40.851484 22579586809984 run.py:483] Algo bellman_ford step 5432 current loss 0.044759, current_train_items 173856.
I0304 19:30:40.884878 22579586809984 run.py:483] Algo bellman_ford step 5433 current loss 0.089229, current_train_items 173888.
I0304 19:30:40.920622 22579586809984 run.py:483] Algo bellman_ford step 5434 current loss 0.098694, current_train_items 173920.
I0304 19:30:40.940227 22579586809984 run.py:483] Algo bellman_ford step 5435 current loss 0.002180, current_train_items 173952.
I0304 19:30:40.956522 22579586809984 run.py:483] Algo bellman_ford step 5436 current loss 0.013899, current_train_items 173984.
I0304 19:30:40.980830 22579586809984 run.py:483] Algo bellman_ford step 5437 current loss 0.042552, current_train_items 174016.
I0304 19:30:41.011347 22579586809984 run.py:483] Algo bellman_ford step 5438 current loss 0.062009, current_train_items 174048.
I0304 19:30:41.048654 22579586809984 run.py:483] Algo bellman_ford step 5439 current loss 0.103390, current_train_items 174080.
I0304 19:30:41.068512 22579586809984 run.py:483] Algo bellman_ford step 5440 current loss 0.007384, current_train_items 174112.
I0304 19:30:41.084800 22579586809984 run.py:483] Algo bellman_ford step 5441 current loss 0.006975, current_train_items 174144.
I0304 19:30:41.108332 22579586809984 run.py:483] Algo bellman_ford step 5442 current loss 0.024881, current_train_items 174176.
I0304 19:30:41.139331 22579586809984 run.py:483] Algo bellman_ford step 5443 current loss 0.048297, current_train_items 174208.
I0304 19:30:41.173259 22579586809984 run.py:483] Algo bellman_ford step 5444 current loss 0.047186, current_train_items 174240.
I0304 19:30:41.192933 22579586809984 run.py:483] Algo bellman_ford step 5445 current loss 0.002282, current_train_items 174272.
I0304 19:30:41.209197 22579586809984 run.py:483] Algo bellman_ford step 5446 current loss 0.029840, current_train_items 174304.
I0304 19:30:41.233613 22579586809984 run.py:483] Algo bellman_ford step 5447 current loss 0.048830, current_train_items 174336.
I0304 19:30:41.266183 22579586809984 run.py:483] Algo bellman_ford step 5448 current loss 0.056013, current_train_items 174368.
I0304 19:30:41.300201 22579586809984 run.py:483] Algo bellman_ford step 5449 current loss 0.034612, current_train_items 174400.
I0304 19:30:41.319914 22579586809984 run.py:483] Algo bellman_ford step 5450 current loss 0.003869, current_train_items 174432.
I0304 19:30:41.328150 22579586809984 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0304 19:30:41.328257 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:30:41.344843 22579586809984 run.py:483] Algo bellman_ford step 5451 current loss 0.055041, current_train_items 174464.
I0304 19:30:41.369476 22579586809984 run.py:483] Algo bellman_ford step 5452 current loss 0.043435, current_train_items 174496.
I0304 19:30:41.400222 22579586809984 run.py:483] Algo bellman_ford step 5453 current loss 0.046493, current_train_items 174528.
I0304 19:30:41.434063 22579586809984 run.py:483] Algo bellman_ford step 5454 current loss 0.054181, current_train_items 174560.
I0304 19:30:41.454658 22579586809984 run.py:483] Algo bellman_ford step 5455 current loss 0.003299, current_train_items 174592.
I0304 19:30:41.471135 22579586809984 run.py:483] Algo bellman_ford step 5456 current loss 0.005616, current_train_items 174624.
I0304 19:30:41.495897 22579586809984 run.py:483] Algo bellman_ford step 5457 current loss 0.086076, current_train_items 174656.
I0304 19:30:41.527912 22579586809984 run.py:483] Algo bellman_ford step 5458 current loss 0.096317, current_train_items 174688.
I0304 19:30:41.559446 22579586809984 run.py:483] Algo bellman_ford step 5459 current loss 0.074261, current_train_items 174720.
I0304 19:30:41.579743 22579586809984 run.py:483] Algo bellman_ford step 5460 current loss 0.004316, current_train_items 174752.
I0304 19:30:41.596557 22579586809984 run.py:483] Algo bellman_ford step 5461 current loss 0.014118, current_train_items 174784.
I0304 19:30:41.621150 22579586809984 run.py:483] Algo bellman_ford step 5462 current loss 0.051578, current_train_items 174816.
I0304 19:30:41.651872 22579586809984 run.py:483] Algo bellman_ford step 5463 current loss 0.053217, current_train_items 174848.
I0304 19:30:41.686155 22579586809984 run.py:483] Algo bellman_ford step 5464 current loss 0.071957, current_train_items 174880.
I0304 19:30:41.706075 22579586809984 run.py:483] Algo bellman_ford step 5465 current loss 0.002588, current_train_items 174912.
I0304 19:30:41.722236 22579586809984 run.py:483] Algo bellman_ford step 5466 current loss 0.016685, current_train_items 174944.
I0304 19:30:41.746064 22579586809984 run.py:483] Algo bellman_ford step 5467 current loss 0.033406, current_train_items 174976.
I0304 19:30:41.776494 22579586809984 run.py:483] Algo bellman_ford step 5468 current loss 0.038865, current_train_items 175008.
I0304 19:30:41.809967 22579586809984 run.py:483] Algo bellman_ford step 5469 current loss 0.149910, current_train_items 175040.
I0304 19:30:41.830609 22579586809984 run.py:483] Algo bellman_ford step 5470 current loss 0.011850, current_train_items 175072.
I0304 19:30:41.847067 22579586809984 run.py:483] Algo bellman_ford step 5471 current loss 0.019793, current_train_items 175104.
I0304 19:30:41.870610 22579586809984 run.py:483] Algo bellman_ford step 5472 current loss 0.044500, current_train_items 175136.
I0304 19:30:41.901557 22579586809984 run.py:483] Algo bellman_ford step 5473 current loss 0.026085, current_train_items 175168.
I0304 19:30:41.936576 22579586809984 run.py:483] Algo bellman_ford step 5474 current loss 0.056699, current_train_items 175200.
I0304 19:30:41.956709 22579586809984 run.py:483] Algo bellman_ford step 5475 current loss 0.002611, current_train_items 175232.
I0304 19:30:41.973276 22579586809984 run.py:483] Algo bellman_ford step 5476 current loss 0.028213, current_train_items 175264.
I0304 19:30:41.996624 22579586809984 run.py:483] Algo bellman_ford step 5477 current loss 0.062418, current_train_items 175296.
I0304 19:30:42.028491 22579586809984 run.py:483] Algo bellman_ford step 5478 current loss 0.054022, current_train_items 175328.
I0304 19:30:42.061994 22579586809984 run.py:483] Algo bellman_ford step 5479 current loss 0.052349, current_train_items 175360.
I0304 19:30:42.081962 22579586809984 run.py:483] Algo bellman_ford step 5480 current loss 0.003861, current_train_items 175392.
I0304 19:30:42.098849 22579586809984 run.py:483] Algo bellman_ford step 5481 current loss 0.027313, current_train_items 175424.
I0304 19:30:42.124506 22579586809984 run.py:483] Algo bellman_ford step 5482 current loss 0.111595, current_train_items 175456.
I0304 19:30:42.156793 22579586809984 run.py:483] Algo bellman_ford step 5483 current loss 0.152249, current_train_items 175488.
I0304 19:30:42.192700 22579586809984 run.py:483] Algo bellman_ford step 5484 current loss 0.145343, current_train_items 175520.
I0304 19:30:42.212887 22579586809984 run.py:483] Algo bellman_ford step 5485 current loss 0.003975, current_train_items 175552.
I0304 19:30:42.228978 22579586809984 run.py:483] Algo bellman_ford step 5486 current loss 0.021368, current_train_items 175584.
I0304 19:30:42.252429 22579586809984 run.py:483] Algo bellman_ford step 5487 current loss 0.041525, current_train_items 175616.
I0304 19:30:42.285576 22579586809984 run.py:483] Algo bellman_ford step 5488 current loss 0.063296, current_train_items 175648.
I0304 19:30:42.318314 22579586809984 run.py:483] Algo bellman_ford step 5489 current loss 0.049139, current_train_items 175680.
I0304 19:30:42.338464 22579586809984 run.py:483] Algo bellman_ford step 5490 current loss 0.002669, current_train_items 175712.
I0304 19:30:42.354869 22579586809984 run.py:483] Algo bellman_ford step 5491 current loss 0.006491, current_train_items 175744.
I0304 19:30:42.379220 22579586809984 run.py:483] Algo bellman_ford step 5492 current loss 0.039240, current_train_items 175776.
I0304 19:30:42.410434 22579586809984 run.py:483] Algo bellman_ford step 5493 current loss 0.034846, current_train_items 175808.
I0304 19:30:42.443992 22579586809984 run.py:483] Algo bellman_ford step 5494 current loss 0.053494, current_train_items 175840.
I0304 19:30:42.463515 22579586809984 run.py:483] Algo bellman_ford step 5495 current loss 0.004012, current_train_items 175872.
I0304 19:30:42.480033 22579586809984 run.py:483] Algo bellman_ford step 5496 current loss 0.005591, current_train_items 175904.
I0304 19:30:42.504582 22579586809984 run.py:483] Algo bellman_ford step 5497 current loss 0.024519, current_train_items 175936.
I0304 19:30:42.536083 22579586809984 run.py:483] Algo bellman_ford step 5498 current loss 0.042119, current_train_items 175968.
I0304 19:30:42.570530 22579586809984 run.py:483] Algo bellman_ford step 5499 current loss 0.108184, current_train_items 176000.
I0304 19:30:42.590992 22579586809984 run.py:483] Algo bellman_ford step 5500 current loss 0.004258, current_train_items 176032.
I0304 19:30:42.598714 22579586809984 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0304 19:30:42.598821 22579586809984 run.py:519] Checkpointing best model, best avg val score was 0.995, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0304 19:30:42.629017 22579586809984 run.py:483] Algo bellman_ford step 5501 current loss 0.060857, current_train_items 176064.
I0304 19:30:42.653739 22579586809984 run.py:483] Algo bellman_ford step 5502 current loss 0.050618, current_train_items 176096.
I0304 19:30:42.685372 22579586809984 run.py:483] Algo bellman_ford step 5503 current loss 0.029104, current_train_items 176128.
I0304 19:30:42.720261 22579586809984 run.py:483] Algo bellman_ford step 5504 current loss 0.063084, current_train_items 176160.
I0304 19:30:42.740336 22579586809984 run.py:483] Algo bellman_ford step 5505 current loss 0.018269, current_train_items 176192.
I0304 19:30:42.756675 22579586809984 run.py:483] Algo bellman_ford step 5506 current loss 0.020931, current_train_items 176224.
I0304 19:30:42.780496 22579586809984 run.py:483] Algo bellman_ford step 5507 current loss 0.014979, current_train_items 176256.
I0304 19:30:42.810992 22579586809984 run.py:483] Algo bellman_ford step 5508 current loss 0.052734, current_train_items 176288.
I0304 19:30:42.844498 22579586809984 run.py:483] Algo bellman_ford step 5509 current loss 0.036910, current_train_items 176320.
I0304 19:30:42.864107 22579586809984 run.py:483] Algo bellman_ford step 5510 current loss 0.002355, current_train_items 176352.
I0304 19:30:42.880726 22579586809984 run.py:483] Algo bellman_ford step 5511 current loss 0.016062, current_train_items 176384.
I0304 19:30:42.905998 22579586809984 run.py:483] Algo bellman_ford step 5512 current loss 0.046456, current_train_items 176416.
I0304 19:30:42.936919 22579586809984 run.py:483] Algo bellman_ford step 5513 current loss 0.034081, current_train_items 176448.
I0304 19:30:42.968869 22579586809984 run.py:483] Algo bellman_ford step 5514 current loss 0.054244, current_train_items 176480.
I0304 19:30:42.988494 22579586809984 run.py:483] Algo bellman_ford step 5515 current loss 0.003339, current_train_items 176512.
I0304 19:30:43.004469 22579586809984 run.py:483] Algo bellman_ford step 5516 current loss 0.010871, current_train_items 176544.
I0304 19:30:43.028790 22579586809984 run.py:483] Algo bellman_ford step 5517 current loss 0.040770, current_train_items 176576.
I0304 19:30:43.059532 22579586809984 run.py:483] Algo bellman_ford step 5518 current loss 0.045120, current_train_items 176608.
I0304 19:30:43.095101 22579586809984 run.py:483] Algo bellman_ford step 5519 current loss 0.052284, current_train_items 176640.
I0304 19:30:43.114809 22579586809984 run.py:483] Algo bellman_ford step 5520 current loss 0.011287, current_train_items 176672.
I0304 19:30:43.131480 22579586809984 run.py:483] Algo bellman_ford step 5521 current loss 0.017453, current_train_items 176704.
I0304 19:30:43.155134 22579586809984 run.py:483] Algo bellman_ford step 5522 current loss 0.014388, current_train_items 176736.
I0304 19:30:43.187491 22579586809984 run.py:483] Algo bellman_ford step 5523 current loss 0.046717, current_train_items 176768.
I0304 19:30:43.222900 22579586809984 run.py:483] Algo bellman_ford step 5524 current loss 0.095898, current_train_items 176800.
I0304 19:30:43.242701 22579586809984 run.py:483] Algo bellman_ford step 5525 current loss 0.003209, current_train_items 176832.
I0304 19:30:43.258651 22579586809984 run.py:483] Algo bellman_ford step 5526 current loss 0.011332, current_train_items 176864.
I0304 19:30:43.283674 22579586809984 run.py:483] Algo bellman_ford step 5527 current loss 0.038097, current_train_items 176896.
I0304 19:30:43.314333 22579586809984 run.py:483] Algo bellman_ford step 5528 current loss 0.073922, current_train_items 176928.
I0304 19:30:43.346996 22579586809984 run.py:483] Algo bellman_ford step 5529 current loss 0.038365, current_train_items 176960.
I0304 19:30:43.367034 22579586809984 run.py:483] Algo bellman_ford step 5530 current loss 0.006130, current_train_items 176992.
I0304 19:30:43.383419 22579586809984 run.py:483] Algo bellman_ford step 5531 current loss 0.012652, current_train_items 177024.
I0304 19:30:43.408329 22579586809984 run.py:483] Algo bellman_ford step 5532 current loss 0.088274, current_train_items 177056.
I0304 19:30:43.439324 22579586809984 run.py:483] Algo bellman_ford step 5533 current loss 0.103958, current_train_items 177088.
I0304 19:30:43.475580 22579586809984 run.py:483] Algo bellman_ford step 5534 current loss 0.174301, current_train_items 177120.
I0304 19:30:43.495273 22579586809984 run.py:483] Algo bellman_ford step 5535 current loss 0.006189, current_train_items 177152.
I0304 19:30:43.511799 22579586809984 run.py:483] Algo bellman_ford step 5536 current loss 0.062754, current_train_items 177184.
I0304 19:30:43.535417 22579586809984 run.py:483] Algo bellman_ford step 5537 current loss 0.035855, current_train_items 177216.
I0304 19:30:43.566119 22579586809984 run.py:483] Algo bellman_ford step 5538 current loss 0.080677, current_train_items 177248.
I0304 19:30:43.601619 22579586809984 run.py:483] Algo bellman_ford step 5539 current loss 0.046842, current_train_items 177280.
I0304 19:30:43.621484 22579586809984 run.py:483] Algo bellman_ford step 5540 current loss 0.003516, current_train_items 177312.
I0304 19:30:43.638331 22579586809984 run.py:483] Algo bellman_ford step 5541 current loss 0.015456, current_train_items 177344.
I0304 19:30:43.663131 22579586809984 run.py:483] Algo bellman_ford step 5542 current loss 0.046201, current_train_items 177376.
I0304 19:30:43.695036 22579586809984 run.py:483] Algo bellman_ford step 5543 current loss 0.036537, current_train_items 177408.
I0304 19:30:43.728175 22579586809984 run.py:483] Algo bellman_ford step 5544 current loss 0.120654, current_train_items 177440.
I0304 19:30:43.747810 22579586809984 run.py:483] Algo bellman_ford step 5545 current loss 0.003419, current_train_items 177472.
I0304 19:30:43.764598 22579586809984 run.py:483] Algo bellman_ford step 5546 current loss 0.039569, current_train_items 177504.
I0304 19:30:43.788860 22579586809984 run.py:483] Algo bellman_ford step 5547 current loss 0.020868, current_train_items 177536.
I0304 19:30:43.820246 22579586809984 run.py:483] Algo bellman_ford step 5548 current loss 0.029985, current_train_items 177568.
I0304 19:30:43.852998 22579586809984 run.py:483] Algo bellman_ford step 5549 current loss 0.082932, current_train_items 177600.
I0304 19:30:43.872874 22579586809984 run.py:483] Algo bellman_ford step 5550 current loss 0.003506, current_train_items 177632.
I0304 19:30:43.880838 22579586809984 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0304 19:30:43.880943 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:43.897823 22579586809984 run.py:483] Algo bellman_ford step 5551 current loss 0.011569, current_train_items 177664.
I0304 19:30:43.923294 22579586809984 run.py:483] Algo bellman_ford step 5552 current loss 0.060152, current_train_items 177696.
I0304 19:30:43.954169 22579586809984 run.py:483] Algo bellman_ford step 5553 current loss 0.069925, current_train_items 177728.
I0304 19:30:43.991010 22579586809984 run.py:483] Algo bellman_ford step 5554 current loss 0.090630, current_train_items 177760.
I0304 19:30:44.011158 22579586809984 run.py:483] Algo bellman_ford step 5555 current loss 0.008180, current_train_items 177792.
I0304 19:30:44.027109 22579586809984 run.py:483] Algo bellman_ford step 5556 current loss 0.029132, current_train_items 177824.
I0304 19:30:44.051807 22579586809984 run.py:483] Algo bellman_ford step 5557 current loss 0.050245, current_train_items 177856.
I0304 19:30:44.082566 22579586809984 run.py:483] Algo bellman_ford step 5558 current loss 0.055375, current_train_items 177888.
I0304 19:30:44.117081 22579586809984 run.py:483] Algo bellman_ford step 5559 current loss 0.074174, current_train_items 177920.
I0304 19:30:44.137002 22579586809984 run.py:483] Algo bellman_ford step 5560 current loss 0.012872, current_train_items 177952.
I0304 19:30:44.153298 22579586809984 run.py:483] Algo bellman_ford step 5561 current loss 0.025286, current_train_items 177984.
I0304 19:30:44.176767 22579586809984 run.py:483] Algo bellman_ford step 5562 current loss 0.035919, current_train_items 178016.
I0304 19:30:44.208447 22579586809984 run.py:483] Algo bellman_ford step 5563 current loss 0.063378, current_train_items 178048.
I0304 19:30:44.243422 22579586809984 run.py:483] Algo bellman_ford step 5564 current loss 0.054656, current_train_items 178080.
I0304 19:30:44.263094 22579586809984 run.py:483] Algo bellman_ford step 5565 current loss 0.002508, current_train_items 178112.
I0304 19:30:44.279268 22579586809984 run.py:483] Algo bellman_ford step 5566 current loss 0.017109, current_train_items 178144.
I0304 19:30:44.303564 22579586809984 run.py:483] Algo bellman_ford step 5567 current loss 0.064908, current_train_items 178176.
I0304 19:30:44.335833 22579586809984 run.py:483] Algo bellman_ford step 5568 current loss 0.094917, current_train_items 178208.
I0304 19:30:44.370153 22579586809984 run.py:483] Algo bellman_ford step 5569 current loss 0.075764, current_train_items 178240.
I0304 19:30:44.390134 22579586809984 run.py:483] Algo bellman_ford step 5570 current loss 0.011079, current_train_items 178272.
I0304 19:30:44.406287 22579586809984 run.py:483] Algo bellman_ford step 5571 current loss 0.011113, current_train_items 178304.
I0304 19:30:44.429757 22579586809984 run.py:483] Algo bellman_ford step 5572 current loss 0.029644, current_train_items 178336.
I0304 19:30:44.460864 22579586809984 run.py:483] Algo bellman_ford step 5573 current loss 0.066775, current_train_items 178368.
I0304 19:30:44.494835 22579586809984 run.py:483] Algo bellman_ford step 5574 current loss 0.067836, current_train_items 178400.
I0304 19:30:44.514724 22579586809984 run.py:483] Algo bellman_ford step 5575 current loss 0.004620, current_train_items 178432.
I0304 19:30:44.530977 22579586809984 run.py:483] Algo bellman_ford step 5576 current loss 0.040709, current_train_items 178464.
I0304 19:30:44.553949 22579586809984 run.py:483] Algo bellman_ford step 5577 current loss 0.055848, current_train_items 178496.
I0304 19:30:44.585461 22579586809984 run.py:483] Algo bellman_ford step 5578 current loss 0.050886, current_train_items 178528.
I0304 19:30:44.620724 22579586809984 run.py:483] Algo bellman_ford step 5579 current loss 0.066038, current_train_items 178560.
I0304 19:30:44.640255 22579586809984 run.py:483] Algo bellman_ford step 5580 current loss 0.008378, current_train_items 178592.
I0304 19:30:44.656082 22579586809984 run.py:483] Algo bellman_ford step 5581 current loss 0.094603, current_train_items 178624.
I0304 19:30:44.680808 22579586809984 run.py:483] Algo bellman_ford step 5582 current loss 0.042381, current_train_items 178656.
I0304 19:30:44.711815 22579586809984 run.py:483] Algo bellman_ford step 5583 current loss 0.055526, current_train_items 178688.
I0304 19:30:44.745780 22579586809984 run.py:483] Algo bellman_ford step 5584 current loss 0.063186, current_train_items 178720.
I0304 19:30:44.765642 22579586809984 run.py:483] Algo bellman_ford step 5585 current loss 0.010873, current_train_items 178752.
I0304 19:30:44.782048 22579586809984 run.py:483] Algo bellman_ford step 5586 current loss 0.013616, current_train_items 178784.
I0304 19:30:44.806951 22579586809984 run.py:483] Algo bellman_ford step 5587 current loss 0.092292, current_train_items 178816.
I0304 19:30:44.838837 22579586809984 run.py:483] Algo bellman_ford step 5588 current loss 0.072345, current_train_items 178848.
I0304 19:30:44.870403 22579586809984 run.py:483] Algo bellman_ford step 5589 current loss 0.059067, current_train_items 178880.
I0304 19:30:44.890196 22579586809984 run.py:483] Algo bellman_ford step 5590 current loss 0.003785, current_train_items 178912.
I0304 19:30:44.906446 22579586809984 run.py:483] Algo bellman_ford step 5591 current loss 0.023869, current_train_items 178944.
I0304 19:30:44.928889 22579586809984 run.py:483] Algo bellman_ford step 5592 current loss 0.031604, current_train_items 178976.
I0304 19:30:44.960343 22579586809984 run.py:483] Algo bellman_ford step 5593 current loss 0.069997, current_train_items 179008.
I0304 19:30:44.995175 22579586809984 run.py:483] Algo bellman_ford step 5594 current loss 0.051661, current_train_items 179040.
I0304 19:30:45.014595 22579586809984 run.py:483] Algo bellman_ford step 5595 current loss 0.015930, current_train_items 179072.
I0304 19:30:45.030656 22579586809984 run.py:483] Algo bellman_ford step 5596 current loss 0.011793, current_train_items 179104.
I0304 19:30:45.054367 22579586809984 run.py:483] Algo bellman_ford step 5597 current loss 0.096598, current_train_items 179136.
I0304 19:30:45.085722 22579586809984 run.py:483] Algo bellman_ford step 5598 current loss 0.183670, current_train_items 179168.
I0304 19:30:45.115715 22579586809984 run.py:483] Algo bellman_ford step 5599 current loss 0.117894, current_train_items 179200.
I0304 19:30:45.135573 22579586809984 run.py:483] Algo bellman_ford step 5600 current loss 0.002780, current_train_items 179232.
I0304 19:30:45.143200 22579586809984 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0304 19:30:45.143305 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:30:45.160222 22579586809984 run.py:483] Algo bellman_ford step 5601 current loss 0.013829, current_train_items 179264.
I0304 19:30:45.184676 22579586809984 run.py:483] Algo bellman_ford step 5602 current loss 0.026522, current_train_items 179296.
I0304 19:30:45.216175 22579586809984 run.py:483] Algo bellman_ford step 5603 current loss 0.076093, current_train_items 179328.
I0304 19:30:45.248784 22579586809984 run.py:483] Algo bellman_ford step 5604 current loss 0.064308, current_train_items 179360.
I0304 19:30:45.268723 22579586809984 run.py:483] Algo bellman_ford step 5605 current loss 0.002315, current_train_items 179392.
I0304 19:30:45.284452 22579586809984 run.py:483] Algo bellman_ford step 5606 current loss 0.024618, current_train_items 179424.
I0304 19:30:45.308470 22579586809984 run.py:483] Algo bellman_ford step 5607 current loss 0.036289, current_train_items 179456.
I0304 19:30:45.338448 22579586809984 run.py:483] Algo bellman_ford step 5608 current loss 0.068094, current_train_items 179488.
I0304 19:30:45.372731 22579586809984 run.py:483] Algo bellman_ford step 5609 current loss 0.079478, current_train_items 179520.
I0304 19:30:45.392494 22579586809984 run.py:483] Algo bellman_ford step 5610 current loss 0.003049, current_train_items 179552.
I0304 19:30:45.408915 22579586809984 run.py:483] Algo bellman_ford step 5611 current loss 0.019609, current_train_items 179584.
I0304 19:30:45.432310 22579586809984 run.py:483] Algo bellman_ford step 5612 current loss 0.035333, current_train_items 179616.
I0304 19:30:45.464369 22579586809984 run.py:483] Algo bellman_ford step 5613 current loss 0.057226, current_train_items 179648.
I0304 19:30:45.496722 22579586809984 run.py:483] Algo bellman_ford step 5614 current loss 0.052468, current_train_items 179680.
I0304 19:30:45.516336 22579586809984 run.py:483] Algo bellman_ford step 5615 current loss 0.003729, current_train_items 179712.
I0304 19:30:45.532950 22579586809984 run.py:483] Algo bellman_ford step 5616 current loss 0.013683, current_train_items 179744.
I0304 19:30:45.557068 22579586809984 run.py:483] Algo bellman_ford step 5617 current loss 0.011350, current_train_items 179776.
I0304 19:30:45.587778 22579586809984 run.py:483] Algo bellman_ford step 5618 current loss 0.034114, current_train_items 179808.
I0304 19:30:45.622480 22579586809984 run.py:483] Algo bellman_ford step 5619 current loss 0.082344, current_train_items 179840.
I0304 19:30:45.641985 22579586809984 run.py:483] Algo bellman_ford step 5620 current loss 0.002559, current_train_items 179872.
I0304 19:30:45.658198 22579586809984 run.py:483] Algo bellman_ford step 5621 current loss 0.019268, current_train_items 179904.
I0304 19:30:45.681760 22579586809984 run.py:483] Algo bellman_ford step 5622 current loss 0.031432, current_train_items 179936.
I0304 19:30:45.712417 22579586809984 run.py:483] Algo bellman_ford step 5623 current loss 0.059342, current_train_items 179968.
I0304 19:30:45.746630 22579586809984 run.py:483] Algo bellman_ford step 5624 current loss 0.062662, current_train_items 180000.
I0304 19:30:45.766113 22579586809984 run.py:483] Algo bellman_ford step 5625 current loss 0.001903, current_train_items 180032.
I0304 19:30:45.782703 22579586809984 run.py:483] Algo bellman_ford step 5626 current loss 0.023904, current_train_items 180064.
I0304 19:30:45.807141 22579586809984 run.py:483] Algo bellman_ford step 5627 current loss 0.057902, current_train_items 180096.
I0304 19:30:45.838555 22579586809984 run.py:483] Algo bellman_ford step 5628 current loss 0.079729, current_train_items 180128.
I0304 19:30:45.873132 22579586809984 run.py:483] Algo bellman_ford step 5629 current loss 0.053154, current_train_items 180160.
I0304 19:30:45.892491 22579586809984 run.py:483] Algo bellman_ford step 5630 current loss 0.004517, current_train_items 180192.
I0304 19:30:45.908825 22579586809984 run.py:483] Algo bellman_ford step 5631 current loss 0.005368, current_train_items 180224.
I0304 19:30:45.933454 22579586809984 run.py:483] Algo bellman_ford step 5632 current loss 0.090742, current_train_items 180256.
I0304 19:30:45.964811 22579586809984 run.py:483] Algo bellman_ford step 5633 current loss 0.089144, current_train_items 180288.
I0304 19:30:45.998858 22579586809984 run.py:483] Algo bellman_ford step 5634 current loss 0.116333, current_train_items 180320.
I0304 19:30:46.018416 22579586809984 run.py:483] Algo bellman_ford step 5635 current loss 0.008019, current_train_items 180352.
I0304 19:30:46.034878 22579586809984 run.py:483] Algo bellman_ford step 5636 current loss 0.011695, current_train_items 180384.
I0304 19:30:46.058551 22579586809984 run.py:483] Algo bellman_ford step 5637 current loss 0.050555, current_train_items 180416.
I0304 19:30:46.088738 22579586809984 run.py:483] Algo bellman_ford step 5638 current loss 0.107309, current_train_items 180448.
I0304 19:30:46.121155 22579586809984 run.py:483] Algo bellman_ford step 5639 current loss 0.169724, current_train_items 180480.
I0304 19:30:46.140816 22579586809984 run.py:483] Algo bellman_ford step 5640 current loss 0.005767, current_train_items 180512.
I0304 19:30:46.156828 22579586809984 run.py:483] Algo bellman_ford step 5641 current loss 0.032572, current_train_items 180544.
I0304 19:30:46.180554 22579586809984 run.py:483] Algo bellman_ford step 5642 current loss 0.038419, current_train_items 180576.
I0304 19:30:46.211776 22579586809984 run.py:483] Algo bellman_ford step 5643 current loss 0.073147, current_train_items 180608.
I0304 19:30:46.244649 22579586809984 run.py:483] Algo bellman_ford step 5644 current loss 0.064074, current_train_items 180640.
I0304 19:30:46.264051 22579586809984 run.py:483] Algo bellman_ford step 5645 current loss 0.002483, current_train_items 180672.
I0304 19:30:46.280801 22579586809984 run.py:483] Algo bellman_ford step 5646 current loss 0.036650, current_train_items 180704.
I0304 19:30:46.304377 22579586809984 run.py:483] Algo bellman_ford step 5647 current loss 0.024664, current_train_items 180736.
I0304 19:30:46.335700 22579586809984 run.py:483] Algo bellman_ford step 5648 current loss 0.043633, current_train_items 180768.
I0304 19:30:46.370958 22579586809984 run.py:483] Algo bellman_ford step 5649 current loss 0.073656, current_train_items 180800.
I0304 19:30:46.390385 22579586809984 run.py:483] Algo bellman_ford step 5650 current loss 0.015191, current_train_items 180832.
I0304 19:30:46.398262 22579586809984 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0304 19:30:46.398367 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:46.415438 22579586809984 run.py:483] Algo bellman_ford step 5651 current loss 0.016431, current_train_items 180864.
I0304 19:30:46.439371 22579586809984 run.py:483] Algo bellman_ford step 5652 current loss 0.054702, current_train_items 180896.
I0304 19:30:46.472707 22579586809984 run.py:483] Algo bellman_ford step 5653 current loss 0.050699, current_train_items 180928.
I0304 19:30:46.507930 22579586809984 run.py:483] Algo bellman_ford step 5654 current loss 0.065090, current_train_items 180960.
I0304 19:30:46.527901 22579586809984 run.py:483] Algo bellman_ford step 5655 current loss 0.003434, current_train_items 180992.
I0304 19:30:46.544213 22579586809984 run.py:483] Algo bellman_ford step 5656 current loss 0.029242, current_train_items 181024.
I0304 19:30:46.567151 22579586809984 run.py:483] Algo bellman_ford step 5657 current loss 0.025132, current_train_items 181056.
I0304 19:30:46.599597 22579586809984 run.py:483] Algo bellman_ford step 5658 current loss 0.064946, current_train_items 181088.
I0304 19:30:46.633590 22579586809984 run.py:483] Algo bellman_ford step 5659 current loss 0.070446, current_train_items 181120.
I0304 19:30:46.653783 22579586809984 run.py:483] Algo bellman_ford step 5660 current loss 0.012396, current_train_items 181152.
I0304 19:30:46.669930 22579586809984 run.py:483] Algo bellman_ford step 5661 current loss 0.014194, current_train_items 181184.
I0304 19:30:46.693436 22579586809984 run.py:483] Algo bellman_ford step 5662 current loss 0.048060, current_train_items 181216.
I0304 19:30:46.726719 22579586809984 run.py:483] Algo bellman_ford step 5663 current loss 0.072359, current_train_items 181248.
I0304 19:30:46.760613 22579586809984 run.py:483] Algo bellman_ford step 5664 current loss 0.041177, current_train_items 181280.
I0304 19:30:46.780252 22579586809984 run.py:483] Algo bellman_ford step 5665 current loss 0.007100, current_train_items 181312.
I0304 19:30:46.796954 22579586809984 run.py:483] Algo bellman_ford step 5666 current loss 0.028173, current_train_items 181344.
I0304 19:30:46.820586 22579586809984 run.py:483] Algo bellman_ford step 5667 current loss 0.072263, current_train_items 181376.
I0304 19:30:46.852755 22579586809984 run.py:483] Algo bellman_ford step 5668 current loss 0.073150, current_train_items 181408.
I0304 19:30:46.885915 22579586809984 run.py:483] Algo bellman_ford step 5669 current loss 0.068798, current_train_items 181440.
I0304 19:30:46.905574 22579586809984 run.py:483] Algo bellman_ford step 5670 current loss 0.002898, current_train_items 181472.
I0304 19:30:46.921762 22579586809984 run.py:483] Algo bellman_ford step 5671 current loss 0.011875, current_train_items 181504.
I0304 19:30:46.944941 22579586809984 run.py:483] Algo bellman_ford step 5672 current loss 0.058832, current_train_items 181536.
I0304 19:30:46.975858 22579586809984 run.py:483] Algo bellman_ford step 5673 current loss 0.056582, current_train_items 181568.
I0304 19:30:47.008963 22579586809984 run.py:483] Algo bellman_ford step 5674 current loss 0.051723, current_train_items 181600.
I0304 19:30:47.028589 22579586809984 run.py:483] Algo bellman_ford step 5675 current loss 0.002755, current_train_items 181632.
I0304 19:30:47.044810 22579586809984 run.py:483] Algo bellman_ford step 5676 current loss 0.011841, current_train_items 181664.
I0304 19:30:47.068704 22579586809984 run.py:483] Algo bellman_ford step 5677 current loss 0.069618, current_train_items 181696.
I0304 19:30:47.101584 22579586809984 run.py:483] Algo bellman_ford step 5678 current loss 0.163749, current_train_items 181728.
I0304 19:30:47.135011 22579586809984 run.py:483] Algo bellman_ford step 5679 current loss 0.171732, current_train_items 181760.
I0304 19:30:47.154769 22579586809984 run.py:483] Algo bellman_ford step 5680 current loss 0.004884, current_train_items 181792.
I0304 19:30:47.171152 22579586809984 run.py:483] Algo bellman_ford step 5681 current loss 0.014373, current_train_items 181824.
I0304 19:30:47.195178 22579586809984 run.py:483] Algo bellman_ford step 5682 current loss 0.043024, current_train_items 181856.
I0304 19:30:47.226249 22579586809984 run.py:483] Algo bellman_ford step 5683 current loss 0.057220, current_train_items 181888.
I0304 19:30:47.259220 22579586809984 run.py:483] Algo bellman_ford step 5684 current loss 0.083908, current_train_items 181920.
I0304 19:30:47.279161 22579586809984 run.py:483] Algo bellman_ford step 5685 current loss 0.005602, current_train_items 181952.
I0304 19:30:47.295602 22579586809984 run.py:483] Algo bellman_ford step 5686 current loss 0.041446, current_train_items 181984.
I0304 19:30:47.319380 22579586809984 run.py:483] Algo bellman_ford step 5687 current loss 0.073556, current_train_items 182016.
I0304 19:30:47.351063 22579586809984 run.py:483] Algo bellman_ford step 5688 current loss 0.090838, current_train_items 182048.
I0304 19:30:47.384045 22579586809984 run.py:483] Algo bellman_ford step 5689 current loss 0.080699, current_train_items 182080.
I0304 19:30:47.403883 22579586809984 run.py:483] Algo bellman_ford step 5690 current loss 0.002759, current_train_items 182112.
I0304 19:30:47.420874 22579586809984 run.py:483] Algo bellman_ford step 5691 current loss 0.020222, current_train_items 182144.
I0304 19:30:47.444427 22579586809984 run.py:483] Algo bellman_ford step 5692 current loss 0.034286, current_train_items 182176.
I0304 19:30:47.475668 22579586809984 run.py:483] Algo bellman_ford step 5693 current loss 0.048649, current_train_items 182208.
I0304 19:30:47.508576 22579586809984 run.py:483] Algo bellman_ford step 5694 current loss 0.070150, current_train_items 182240.
I0304 19:30:47.528129 22579586809984 run.py:483] Algo bellman_ford step 5695 current loss 0.004133, current_train_items 182272.
I0304 19:30:47.544759 22579586809984 run.py:483] Algo bellman_ford step 5696 current loss 0.029544, current_train_items 182304.
I0304 19:30:47.568473 22579586809984 run.py:483] Algo bellman_ford step 5697 current loss 0.080936, current_train_items 182336.
I0304 19:30:47.600817 22579586809984 run.py:483] Algo bellman_ford step 5698 current loss 0.086434, current_train_items 182368.
I0304 19:30:47.634905 22579586809984 run.py:483] Algo bellman_ford step 5699 current loss 0.065773, current_train_items 182400.
I0304 19:30:47.654809 22579586809984 run.py:483] Algo bellman_ford step 5700 current loss 0.008787, current_train_items 182432.
I0304 19:30:47.662998 22579586809984 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0304 19:30:47.663104 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:30:47.679597 22579586809984 run.py:483] Algo bellman_ford step 5701 current loss 0.014366, current_train_items 182464.
I0304 19:30:47.703195 22579586809984 run.py:483] Algo bellman_ford step 5702 current loss 0.020344, current_train_items 182496.
I0304 19:30:47.735127 22579586809984 run.py:483] Algo bellman_ford step 5703 current loss 0.045826, current_train_items 182528.
I0304 19:30:47.770837 22579586809984 run.py:483] Algo bellman_ford step 5704 current loss 0.064155, current_train_items 182560.
I0304 19:30:47.790963 22579586809984 run.py:483] Algo bellman_ford step 5705 current loss 0.003694, current_train_items 182592.
I0304 19:30:47.807001 22579586809984 run.py:483] Algo bellman_ford step 5706 current loss 0.013958, current_train_items 182624.
I0304 19:30:47.831820 22579586809984 run.py:483] Algo bellman_ford step 5707 current loss 0.049385, current_train_items 182656.
I0304 19:30:47.862761 22579586809984 run.py:483] Algo bellman_ford step 5708 current loss 0.056182, current_train_items 182688.
I0304 19:30:47.896738 22579586809984 run.py:483] Algo bellman_ford step 5709 current loss 0.079998, current_train_items 182720.
I0304 19:30:47.916644 22579586809984 run.py:483] Algo bellman_ford step 5710 current loss 0.003017, current_train_items 182752.
I0304 19:30:47.932757 22579586809984 run.py:483] Algo bellman_ford step 5711 current loss 0.012748, current_train_items 182784.
I0304 19:30:47.956156 22579586809984 run.py:483] Algo bellman_ford step 5712 current loss 0.037842, current_train_items 182816.
I0304 19:30:47.987036 22579586809984 run.py:483] Algo bellman_ford step 5713 current loss 0.040029, current_train_items 182848.
I0304 19:30:48.020425 22579586809984 run.py:483] Algo bellman_ford step 5714 current loss 0.038057, current_train_items 182880.
I0304 19:30:48.040273 22579586809984 run.py:483] Algo bellman_ford step 5715 current loss 0.003558, current_train_items 182912.
I0304 19:30:48.056666 22579586809984 run.py:483] Algo bellman_ford step 5716 current loss 0.022356, current_train_items 182944.
I0304 19:30:48.081060 22579586809984 run.py:483] Algo bellman_ford step 5717 current loss 0.042860, current_train_items 182976.
I0304 19:30:48.111824 22579586809984 run.py:483] Algo bellman_ford step 5718 current loss 0.068567, current_train_items 183008.
I0304 19:30:48.146313 22579586809984 run.py:483] Algo bellman_ford step 5719 current loss 0.075064, current_train_items 183040.
I0304 19:30:48.166107 22579586809984 run.py:483] Algo bellman_ford step 5720 current loss 0.004488, current_train_items 183072.
I0304 19:30:48.182319 22579586809984 run.py:483] Algo bellman_ford step 5721 current loss 0.006070, current_train_items 183104.
I0304 19:30:48.205429 22579586809984 run.py:483] Algo bellman_ford step 5722 current loss 0.105666, current_train_items 183136.
I0304 19:30:48.236273 22579586809984 run.py:483] Algo bellman_ford step 5723 current loss 0.097492, current_train_items 183168.
I0304 19:30:48.269320 22579586809984 run.py:483] Algo bellman_ford step 5724 current loss 0.148125, current_train_items 183200.
I0304 19:30:48.289168 22579586809984 run.py:483] Algo bellman_ford step 5725 current loss 0.002977, current_train_items 183232.
I0304 19:30:48.305479 22579586809984 run.py:483] Algo bellman_ford step 5726 current loss 0.021882, current_train_items 183264.
I0304 19:30:48.330073 22579586809984 run.py:483] Algo bellman_ford step 5727 current loss 0.047611, current_train_items 183296.
I0304 19:30:48.360759 22579586809984 run.py:483] Algo bellman_ford step 5728 current loss 0.037642, current_train_items 183328.
I0304 19:30:48.394642 22579586809984 run.py:483] Algo bellman_ford step 5729 current loss 0.048798, current_train_items 183360.
I0304 19:30:48.414452 22579586809984 run.py:483] Algo bellman_ford step 5730 current loss 0.002716, current_train_items 183392.
I0304 19:30:48.431058 22579586809984 run.py:483] Algo bellman_ford step 5731 current loss 0.006518, current_train_items 183424.
I0304 19:30:48.456209 22579586809984 run.py:483] Algo bellman_ford step 5732 current loss 0.045998, current_train_items 183456.
I0304 19:30:48.487275 22579586809984 run.py:483] Algo bellman_ford step 5733 current loss 0.079693, current_train_items 183488.
I0304 19:30:48.521742 22579586809984 run.py:483] Algo bellman_ford step 5734 current loss 0.076720, current_train_items 183520.
I0304 19:30:48.541649 22579586809984 run.py:483] Algo bellman_ford step 5735 current loss 0.003746, current_train_items 183552.
I0304 19:30:48.558180 22579586809984 run.py:483] Algo bellman_ford step 5736 current loss 0.012153, current_train_items 183584.
I0304 19:30:48.582510 22579586809984 run.py:483] Algo bellman_ford step 5737 current loss 0.034784, current_train_items 183616.
I0304 19:30:48.613457 22579586809984 run.py:483] Algo bellman_ford step 5738 current loss 0.076398, current_train_items 183648.
I0304 19:30:48.647513 22579586809984 run.py:483] Algo bellman_ford step 5739 current loss 0.067225, current_train_items 183680.
I0304 19:30:48.667006 22579586809984 run.py:483] Algo bellman_ford step 5740 current loss 0.017252, current_train_items 183712.
I0304 19:30:48.683612 22579586809984 run.py:483] Algo bellman_ford step 5741 current loss 0.009331, current_train_items 183744.
I0304 19:30:48.708576 22579586809984 run.py:483] Algo bellman_ford step 5742 current loss 0.034980, current_train_items 183776.
I0304 19:30:48.740921 22579586809984 run.py:483] Algo bellman_ford step 5743 current loss 0.051840, current_train_items 183808.
I0304 19:30:48.775388 22579586809984 run.py:483] Algo bellman_ford step 5744 current loss 0.060394, current_train_items 183840.
I0304 19:30:48.795243 22579586809984 run.py:483] Algo bellman_ford step 5745 current loss 0.025037, current_train_items 183872.
I0304 19:30:48.811908 22579586809984 run.py:483] Algo bellman_ford step 5746 current loss 0.009979, current_train_items 183904.
I0304 19:30:48.836395 22579586809984 run.py:483] Algo bellman_ford step 5747 current loss 0.055349, current_train_items 183936.
I0304 19:30:48.867925 22579586809984 run.py:483] Algo bellman_ford step 5748 current loss 0.064575, current_train_items 183968.
I0304 19:30:48.901099 22579586809984 run.py:483] Algo bellman_ford step 5749 current loss 0.057039, current_train_items 184000.
I0304 19:30:48.920707 22579586809984 run.py:483] Algo bellman_ford step 5750 current loss 0.004321, current_train_items 184032.
I0304 19:30:48.928557 22579586809984 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0304 19:30:48.928661 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:30:48.945978 22579586809984 run.py:483] Algo bellman_ford step 5751 current loss 0.014939, current_train_items 184064.
I0304 19:30:48.969832 22579586809984 run.py:483] Algo bellman_ford step 5752 current loss 0.051164, current_train_items 184096.
I0304 19:30:49.000672 22579586809984 run.py:483] Algo bellman_ford step 5753 current loss 0.047049, current_train_items 184128.
I0304 19:30:49.035858 22579586809984 run.py:483] Algo bellman_ford step 5754 current loss 0.092907, current_train_items 184160.
I0304 19:30:49.055879 22579586809984 run.py:483] Algo bellman_ford step 5755 current loss 0.003128, current_train_items 184192.
I0304 19:30:49.071573 22579586809984 run.py:483] Algo bellman_ford step 5756 current loss 0.009851, current_train_items 184224.
I0304 19:30:49.095931 22579586809984 run.py:483] Algo bellman_ford step 5757 current loss 0.066340, current_train_items 184256.
I0304 19:30:49.128157 22579586809984 run.py:483] Algo bellman_ford step 5758 current loss 0.065708, current_train_items 184288.
I0304 19:30:49.162036 22579586809984 run.py:483] Algo bellman_ford step 5759 current loss 0.039725, current_train_items 184320.
I0304 19:30:49.181632 22579586809984 run.py:483] Algo bellman_ford step 5760 current loss 0.005747, current_train_items 184352.
I0304 19:30:49.198245 22579586809984 run.py:483] Algo bellman_ford step 5761 current loss 0.022121, current_train_items 184384.
I0304 19:30:49.222026 22579586809984 run.py:483] Algo bellman_ford step 5762 current loss 0.056851, current_train_items 184416.
I0304 19:30:49.252385 22579586809984 run.py:483] Algo bellman_ford step 5763 current loss 0.048292, current_train_items 184448.
I0304 19:30:49.286806 22579586809984 run.py:483] Algo bellman_ford step 5764 current loss 0.069020, current_train_items 184480.
I0304 19:30:49.306575 22579586809984 run.py:483] Algo bellman_ford step 5765 current loss 0.009974, current_train_items 184512.
I0304 19:30:49.322970 22579586809984 run.py:483] Algo bellman_ford step 5766 current loss 0.027796, current_train_items 184544.
I0304 19:30:49.346597 22579586809984 run.py:483] Algo bellman_ford step 5767 current loss 0.036508, current_train_items 184576.
I0304 19:30:49.378794 22579586809984 run.py:483] Algo bellman_ford step 5768 current loss 0.079683, current_train_items 184608.
I0304 19:30:49.412075 22579586809984 run.py:483] Algo bellman_ford step 5769 current loss 0.052565, current_train_items 184640.
I0304 19:30:49.431915 22579586809984 run.py:483] Algo bellman_ford step 5770 current loss 0.002788, current_train_items 184672.
I0304 19:30:49.448290 22579586809984 run.py:483] Algo bellman_ford step 5771 current loss 0.020034, current_train_items 184704.
I0304 19:30:49.470768 22579586809984 run.py:483] Algo bellman_ford step 5772 current loss 0.039766, current_train_items 184736.
I0304 19:30:49.501519 22579586809984 run.py:483] Algo bellman_ford step 5773 current loss 0.037473, current_train_items 184768.
I0304 19:30:49.534273 22579586809984 run.py:483] Algo bellman_ford step 5774 current loss 0.054847, current_train_items 184800.
I0304 19:30:49.553957 22579586809984 run.py:483] Algo bellman_ford step 5775 current loss 0.002596, current_train_items 184832.
I0304 19:30:49.570670 22579586809984 run.py:483] Algo bellman_ford step 5776 current loss 0.040428, current_train_items 184864.
I0304 19:30:49.593440 22579586809984 run.py:483] Algo bellman_ford step 5777 current loss 0.029931, current_train_items 184896.
I0304 19:30:49.624837 22579586809984 run.py:483] Algo bellman_ford step 5778 current loss 0.057274, current_train_items 184928.
I0304 19:30:49.659002 22579586809984 run.py:483] Algo bellman_ford step 5779 current loss 0.078527, current_train_items 184960.
I0304 19:30:49.678483 22579586809984 run.py:483] Algo bellman_ford step 5780 current loss 0.006551, current_train_items 184992.
I0304 19:30:49.694737 22579586809984 run.py:483] Algo bellman_ford step 5781 current loss 0.014068, current_train_items 185024.
I0304 19:30:49.719208 22579586809984 run.py:483] Algo bellman_ford step 5782 current loss 0.043443, current_train_items 185056.
I0304 19:30:49.749477 22579586809984 run.py:483] Algo bellman_ford step 5783 current loss 0.031294, current_train_items 185088.
I0304 19:30:49.781545 22579586809984 run.py:483] Algo bellman_ford step 5784 current loss 0.080981, current_train_items 185120.
I0304 19:30:49.801324 22579586809984 run.py:483] Algo bellman_ford step 5785 current loss 0.004515, current_train_items 185152.
I0304 19:30:49.817523 22579586809984 run.py:483] Algo bellman_ford step 5786 current loss 0.030770, current_train_items 185184.
I0304 19:30:49.839972 22579586809984 run.py:483] Algo bellman_ford step 5787 current loss 0.035943, current_train_items 185216.
I0304 19:30:49.871855 22579586809984 run.py:483] Algo bellman_ford step 5788 current loss 0.063927, current_train_items 185248.
I0304 19:30:49.904285 22579586809984 run.py:483] Algo bellman_ford step 5789 current loss 0.099730, current_train_items 185280.
I0304 19:30:49.924116 22579586809984 run.py:483] Algo bellman_ford step 5790 current loss 0.004543, current_train_items 185312.
I0304 19:30:49.940966 22579586809984 run.py:483] Algo bellman_ford step 5791 current loss 0.043196, current_train_items 185344.
I0304 19:30:49.963575 22579586809984 run.py:483] Algo bellman_ford step 5792 current loss 0.047036, current_train_items 185376.
I0304 19:30:49.994677 22579586809984 run.py:483] Algo bellman_ford step 5793 current loss 0.055342, current_train_items 185408.
I0304 19:30:50.028450 22579586809984 run.py:483] Algo bellman_ford step 5794 current loss 0.034103, current_train_items 185440.
I0304 19:30:50.048127 22579586809984 run.py:483] Algo bellman_ford step 5795 current loss 0.003786, current_train_items 185472.
I0304 19:30:50.064195 22579586809984 run.py:483] Algo bellman_ford step 5796 current loss 0.007805, current_train_items 185504.
I0304 19:30:50.088490 22579586809984 run.py:483] Algo bellman_ford step 5797 current loss 0.038109, current_train_items 185536.
I0304 19:30:50.119870 22579586809984 run.py:483] Algo bellman_ford step 5798 current loss 0.042800, current_train_items 185568.
I0304 19:30:50.154265 22579586809984 run.py:483] Algo bellman_ford step 5799 current loss 0.053538, current_train_items 185600.
I0304 19:30:50.173900 22579586809984 run.py:483] Algo bellman_ford step 5800 current loss 0.002715, current_train_items 185632.
I0304 19:30:50.181475 22579586809984 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0304 19:30:50.181579 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:50.198656 22579586809984 run.py:483] Algo bellman_ford step 5801 current loss 0.060873, current_train_items 185664.
I0304 19:30:50.222033 22579586809984 run.py:483] Algo bellman_ford step 5802 current loss 0.018603, current_train_items 185696.
I0304 19:30:50.254459 22579586809984 run.py:483] Algo bellman_ford step 5803 current loss 0.117027, current_train_items 185728.
I0304 19:30:50.289969 22579586809984 run.py:483] Algo bellman_ford step 5804 current loss 0.067136, current_train_items 185760.
I0304 19:30:50.310115 22579586809984 run.py:483] Algo bellman_ford step 5805 current loss 0.003169, current_train_items 185792.
I0304 19:30:50.326353 22579586809984 run.py:483] Algo bellman_ford step 5806 current loss 0.019942, current_train_items 185824.
I0304 19:30:50.351316 22579586809984 run.py:483] Algo bellman_ford step 5807 current loss 0.061416, current_train_items 185856.
I0304 19:30:50.383141 22579586809984 run.py:483] Algo bellman_ford step 5808 current loss 0.054880, current_train_items 185888.
I0304 19:30:50.416928 22579586809984 run.py:483] Algo bellman_ford step 5809 current loss 0.065569, current_train_items 185920.
I0304 19:30:50.436641 22579586809984 run.py:483] Algo bellman_ford step 5810 current loss 0.003496, current_train_items 185952.
I0304 19:30:50.452863 22579586809984 run.py:483] Algo bellman_ford step 5811 current loss 0.018535, current_train_items 185984.
I0304 19:30:50.477407 22579586809984 run.py:483] Algo bellman_ford step 5812 current loss 0.121818, current_train_items 186016.
I0304 19:30:50.509605 22579586809984 run.py:483] Algo bellman_ford step 5813 current loss 0.123855, current_train_items 186048.
I0304 19:30:50.544950 22579586809984 run.py:483] Algo bellman_ford step 5814 current loss 0.144379, current_train_items 186080.
I0304 19:30:50.564893 22579586809984 run.py:483] Algo bellman_ford step 5815 current loss 0.004727, current_train_items 186112.
I0304 19:30:50.581346 22579586809984 run.py:483] Algo bellman_ford step 5816 current loss 0.021840, current_train_items 186144.
I0304 19:30:50.605486 22579586809984 run.py:483] Algo bellman_ford step 5817 current loss 0.086162, current_train_items 186176.
I0304 19:30:50.636074 22579586809984 run.py:483] Algo bellman_ford step 5818 current loss 0.059604, current_train_items 186208.
I0304 19:30:50.670598 22579586809984 run.py:483] Algo bellman_ford step 5819 current loss 0.157149, current_train_items 186240.
I0304 19:30:50.690527 22579586809984 run.py:483] Algo bellman_ford step 5820 current loss 0.003763, current_train_items 186272.
I0304 19:30:50.706665 22579586809984 run.py:483] Algo bellman_ford step 5821 current loss 0.008945, current_train_items 186304.
I0304 19:30:50.730911 22579586809984 run.py:483] Algo bellman_ford step 5822 current loss 0.069483, current_train_items 186336.
I0304 19:30:50.762395 22579586809984 run.py:483] Algo bellman_ford step 5823 current loss 0.085811, current_train_items 186368.
I0304 19:30:50.795570 22579586809984 run.py:483] Algo bellman_ford step 5824 current loss 0.072885, current_train_items 186400.
I0304 19:30:50.815209 22579586809984 run.py:483] Algo bellman_ford step 5825 current loss 0.005935, current_train_items 186432.
I0304 19:30:50.831590 22579586809984 run.py:483] Algo bellman_ford step 5826 current loss 0.019686, current_train_items 186464.
I0304 19:30:50.856277 22579586809984 run.py:483] Algo bellman_ford step 5827 current loss 0.070931, current_train_items 186496.
I0304 19:30:50.888266 22579586809984 run.py:483] Algo bellman_ford step 5828 current loss 0.109386, current_train_items 186528.
I0304 19:30:50.923489 22579586809984 run.py:483] Algo bellman_ford step 5829 current loss 0.117891, current_train_items 186560.
I0304 19:30:50.943368 22579586809984 run.py:483] Algo bellman_ford step 5830 current loss 0.010203, current_train_items 186592.
I0304 19:30:50.959911 22579586809984 run.py:483] Algo bellman_ford step 5831 current loss 0.055846, current_train_items 186624.
I0304 19:30:50.983611 22579586809984 run.py:483] Algo bellman_ford step 5832 current loss 0.094562, current_train_items 186656.
I0304 19:30:51.014910 22579586809984 run.py:483] Algo bellman_ford step 5833 current loss 0.098306, current_train_items 186688.
I0304 19:30:51.045378 22579586809984 run.py:483] Algo bellman_ford step 5834 current loss 0.052219, current_train_items 186720.
I0304 19:30:51.065518 22579586809984 run.py:483] Algo bellman_ford step 5835 current loss 0.009476, current_train_items 186752.
I0304 19:30:51.081150 22579586809984 run.py:483] Algo bellman_ford step 5836 current loss 0.019981, current_train_items 186784.
I0304 19:30:51.106725 22579586809984 run.py:483] Algo bellman_ford step 5837 current loss 0.078626, current_train_items 186816.
I0304 19:30:51.136473 22579586809984 run.py:483] Algo bellman_ford step 5838 current loss 0.052477, current_train_items 186848.
I0304 19:30:51.171958 22579586809984 run.py:483] Algo bellman_ford step 5839 current loss 0.118694, current_train_items 186880.
I0304 19:30:51.191807 22579586809984 run.py:483] Algo bellman_ford step 5840 current loss 0.004916, current_train_items 186912.
I0304 19:30:51.208314 22579586809984 run.py:483] Algo bellman_ford step 5841 current loss 0.011521, current_train_items 186944.
I0304 19:30:51.232015 22579586809984 run.py:483] Algo bellman_ford step 5842 current loss 0.042517, current_train_items 186976.
I0304 19:30:51.261985 22579586809984 run.py:483] Algo bellman_ford step 5843 current loss 0.064154, current_train_items 187008.
I0304 19:30:51.295382 22579586809984 run.py:483] Algo bellman_ford step 5844 current loss 0.047918, current_train_items 187040.
I0304 19:30:51.315215 22579586809984 run.py:483] Algo bellman_ford step 5845 current loss 0.005907, current_train_items 187072.
I0304 19:30:51.331590 22579586809984 run.py:483] Algo bellman_ford step 5846 current loss 0.011391, current_train_items 187104.
I0304 19:30:51.356948 22579586809984 run.py:483] Algo bellman_ford step 5847 current loss 0.071176, current_train_items 187136.
I0304 19:30:51.386968 22579586809984 run.py:483] Algo bellman_ford step 5848 current loss 0.037696, current_train_items 187168.
I0304 19:30:51.421480 22579586809984 run.py:483] Algo bellman_ford step 5849 current loss 0.052209, current_train_items 187200.
I0304 19:30:51.441115 22579586809984 run.py:483] Algo bellman_ford step 5850 current loss 0.006863, current_train_items 187232.
I0304 19:30:51.448885 22579586809984 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0304 19:30:51.448992 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:30:51.466052 22579586809984 run.py:483] Algo bellman_ford step 5851 current loss 0.007951, current_train_items 187264.
I0304 19:30:51.489897 22579586809984 run.py:483] Algo bellman_ford step 5852 current loss 0.092737, current_train_items 187296.
I0304 19:30:51.520570 22579586809984 run.py:483] Algo bellman_ford step 5853 current loss 0.048591, current_train_items 187328.
I0304 19:30:51.554962 22579586809984 run.py:483] Algo bellman_ford step 5854 current loss 0.054419, current_train_items 187360.
I0304 19:30:51.574462 22579586809984 run.py:483] Algo bellman_ford step 5855 current loss 0.003123, current_train_items 187392.
I0304 19:30:51.590706 22579586809984 run.py:483] Algo bellman_ford step 5856 current loss 0.024566, current_train_items 187424.
I0304 19:30:51.614994 22579586809984 run.py:483] Algo bellman_ford step 5857 current loss 0.038047, current_train_items 187456.
I0304 19:30:51.645243 22579586809984 run.py:483] Algo bellman_ford step 5858 current loss 0.048875, current_train_items 187488.
I0304 19:30:51.678868 22579586809984 run.py:483] Algo bellman_ford step 5859 current loss 0.065790, current_train_items 187520.
I0304 19:30:51.699167 22579586809984 run.py:483] Algo bellman_ford step 5860 current loss 0.005827, current_train_items 187552.
I0304 19:30:51.715788 22579586809984 run.py:483] Algo bellman_ford step 5861 current loss 0.010662, current_train_items 187584.
I0304 19:30:51.738918 22579586809984 run.py:483] Algo bellman_ford step 5862 current loss 0.046284, current_train_items 187616.
I0304 19:30:51.770371 22579586809984 run.py:483] Algo bellman_ford step 5863 current loss 0.054154, current_train_items 187648.
I0304 19:30:51.804751 22579586809984 run.py:483] Algo bellman_ford step 5864 current loss 0.083673, current_train_items 187680.
I0304 19:30:51.824326 22579586809984 run.py:483] Algo bellman_ford step 5865 current loss 0.003279, current_train_items 187712.
I0304 19:30:51.841187 22579586809984 run.py:483] Algo bellman_ford step 5866 current loss 0.024548, current_train_items 187744.
I0304 19:30:51.865265 22579586809984 run.py:483] Algo bellman_ford step 5867 current loss 0.026092, current_train_items 187776.
I0304 19:30:51.896348 22579586809984 run.py:483] Algo bellman_ford step 5868 current loss 0.054958, current_train_items 187808.
I0304 19:30:51.931457 22579586809984 run.py:483] Algo bellman_ford step 5869 current loss 0.051747, current_train_items 187840.
I0304 19:30:51.951576 22579586809984 run.py:483] Algo bellman_ford step 5870 current loss 0.002774, current_train_items 187872.
I0304 19:30:51.968018 22579586809984 run.py:483] Algo bellman_ford step 5871 current loss 0.007575, current_train_items 187904.
I0304 19:30:51.990830 22579586809984 run.py:483] Algo bellman_ford step 5872 current loss 0.037230, current_train_items 187936.
I0304 19:30:52.021950 22579586809984 run.py:483] Algo bellman_ford step 5873 current loss 0.058221, current_train_items 187968.
I0304 19:30:52.055794 22579586809984 run.py:483] Algo bellman_ford step 5874 current loss 0.047839, current_train_items 188000.
I0304 19:30:52.075608 22579586809984 run.py:483] Algo bellman_ford step 5875 current loss 0.003366, current_train_items 188032.
I0304 19:30:52.092044 22579586809984 run.py:483] Algo bellman_ford step 5876 current loss 0.011226, current_train_items 188064.
I0304 19:30:52.115097 22579586809984 run.py:483] Algo bellman_ford step 5877 current loss 0.055169, current_train_items 188096.
I0304 19:30:52.145839 22579586809984 run.py:483] Algo bellman_ford step 5878 current loss 0.018066, current_train_items 188128.
I0304 19:30:52.180207 22579586809984 run.py:483] Algo bellman_ford step 5879 current loss 0.055952, current_train_items 188160.
I0304 19:30:52.199810 22579586809984 run.py:483] Algo bellman_ford step 5880 current loss 0.006664, current_train_items 188192.
I0304 19:30:52.216022 22579586809984 run.py:483] Algo bellman_ford step 5881 current loss 0.016962, current_train_items 188224.
I0304 19:30:52.240736 22579586809984 run.py:483] Algo bellman_ford step 5882 current loss 0.059422, current_train_items 188256.
I0304 19:30:52.271725 22579586809984 run.py:483] Algo bellman_ford step 5883 current loss 0.024169, current_train_items 188288.
I0304 19:30:52.306169 22579586809984 run.py:483] Algo bellman_ford step 5884 current loss 0.114133, current_train_items 188320.
I0304 19:30:52.326269 22579586809984 run.py:483] Algo bellman_ford step 5885 current loss 0.003412, current_train_items 188352.
I0304 19:30:52.342506 22579586809984 run.py:483] Algo bellman_ford step 5886 current loss 0.008391, current_train_items 188384.
I0304 19:30:52.365821 22579586809984 run.py:483] Algo bellman_ford step 5887 current loss 0.045264, current_train_items 188416.
I0304 19:30:52.396282 22579586809984 run.py:483] Algo bellman_ford step 5888 current loss 0.037105, current_train_items 188448.
I0304 19:30:52.428472 22579586809984 run.py:483] Algo bellman_ford step 5889 current loss 0.068354, current_train_items 188480.
I0304 19:30:52.448600 22579586809984 run.py:483] Algo bellman_ford step 5890 current loss 0.004436, current_train_items 188512.
I0304 19:30:52.465039 22579586809984 run.py:483] Algo bellman_ford step 5891 current loss 0.042565, current_train_items 188544.
I0304 19:30:52.488678 22579586809984 run.py:483] Algo bellman_ford step 5892 current loss 0.064067, current_train_items 188576.
I0304 19:30:52.518921 22579586809984 run.py:483] Algo bellman_ford step 5893 current loss 0.018199, current_train_items 188608.
I0304 19:30:52.550244 22579586809984 run.py:483] Algo bellman_ford step 5894 current loss 0.045091, current_train_items 188640.
I0304 19:30:52.569893 22579586809984 run.py:483] Algo bellman_ford step 5895 current loss 0.004939, current_train_items 188672.
I0304 19:30:52.586282 22579586809984 run.py:483] Algo bellman_ford step 5896 current loss 0.054807, current_train_items 188704.
I0304 19:30:52.610603 22579586809984 run.py:483] Algo bellman_ford step 5897 current loss 0.076479, current_train_items 188736.
I0304 19:30:52.642646 22579586809984 run.py:483] Algo bellman_ford step 5898 current loss 0.087364, current_train_items 188768.
I0304 19:30:52.675697 22579586809984 run.py:483] Algo bellman_ford step 5899 current loss 0.053139, current_train_items 188800.
I0304 19:30:52.695882 22579586809984 run.py:483] Algo bellman_ford step 5900 current loss 0.004520, current_train_items 188832.
I0304 19:30:52.703547 22579586809984 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0304 19:30:52.703651 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:52.720233 22579586809984 run.py:483] Algo bellman_ford step 5901 current loss 0.016575, current_train_items 188864.
I0304 19:30:52.744821 22579586809984 run.py:483] Algo bellman_ford step 5902 current loss 0.022328, current_train_items 188896.
I0304 19:30:52.777119 22579586809984 run.py:483] Algo bellman_ford step 5903 current loss 0.054092, current_train_items 188928.
I0304 19:30:52.809212 22579586809984 run.py:483] Algo bellman_ford step 5904 current loss 0.068832, current_train_items 188960.
I0304 19:30:52.829165 22579586809984 run.py:483] Algo bellman_ford step 5905 current loss 0.004052, current_train_items 188992.
I0304 19:30:52.845536 22579586809984 run.py:483] Algo bellman_ford step 5906 current loss 0.011122, current_train_items 189024.
I0304 19:30:52.869048 22579586809984 run.py:483] Algo bellman_ford step 5907 current loss 0.041458, current_train_items 189056.
I0304 19:30:52.900410 22579586809984 run.py:483] Algo bellman_ford step 5908 current loss 0.040224, current_train_items 189088.
I0304 19:30:52.932463 22579586809984 run.py:483] Algo bellman_ford step 5909 current loss 0.155740, current_train_items 189120.
I0304 19:30:52.952185 22579586809984 run.py:483] Algo bellman_ford step 5910 current loss 0.003157, current_train_items 189152.
I0304 19:30:52.968611 22579586809984 run.py:483] Algo bellman_ford step 5911 current loss 0.022556, current_train_items 189184.
I0304 19:30:52.992603 22579586809984 run.py:483] Algo bellman_ford step 5912 current loss 0.053203, current_train_items 189216.
I0304 19:30:53.023523 22579586809984 run.py:483] Algo bellman_ford step 5913 current loss 0.050156, current_train_items 189248.
I0304 19:30:53.056094 22579586809984 run.py:483] Algo bellman_ford step 5914 current loss 0.069689, current_train_items 189280.
I0304 19:30:53.075702 22579586809984 run.py:483] Algo bellman_ford step 5915 current loss 0.005942, current_train_items 189312.
I0304 19:30:53.092088 22579586809984 run.py:483] Algo bellman_ford step 5916 current loss 0.020013, current_train_items 189344.
I0304 19:30:53.115658 22579586809984 run.py:483] Algo bellman_ford step 5917 current loss 0.036047, current_train_items 189376.
I0304 19:30:53.145838 22579586809984 run.py:483] Algo bellman_ford step 5918 current loss 0.034826, current_train_items 189408.
I0304 19:30:53.180001 22579586809984 run.py:483] Algo bellman_ford step 5919 current loss 0.046036, current_train_items 189440.
I0304 19:30:53.199505 22579586809984 run.py:483] Algo bellman_ford step 5920 current loss 0.006517, current_train_items 189472.
I0304 19:30:53.215688 22579586809984 run.py:483] Algo bellman_ford step 5921 current loss 0.020106, current_train_items 189504.
I0304 19:30:53.238817 22579586809984 run.py:483] Algo bellman_ford step 5922 current loss 0.044239, current_train_items 189536.
I0304 19:30:53.268396 22579586809984 run.py:483] Algo bellman_ford step 5923 current loss 0.047360, current_train_items 189568.
I0304 19:30:53.301651 22579586809984 run.py:483] Algo bellman_ford step 5924 current loss 0.042273, current_train_items 189600.
I0304 19:30:53.320975 22579586809984 run.py:483] Algo bellman_ford step 5925 current loss 0.002068, current_train_items 189632.
I0304 19:30:53.336888 22579586809984 run.py:483] Algo bellman_ford step 5926 current loss 0.006686, current_train_items 189664.
I0304 19:30:53.359768 22579586809984 run.py:483] Algo bellman_ford step 5927 current loss 0.084156, current_train_items 189696.
I0304 19:30:53.390995 22579586809984 run.py:483] Algo bellman_ford step 5928 current loss 0.113221, current_train_items 189728.
I0304 19:30:53.423298 22579586809984 run.py:483] Algo bellman_ford step 5929 current loss 0.053951, current_train_items 189760.
I0304 19:30:53.442722 22579586809984 run.py:483] Algo bellman_ford step 5930 current loss 0.002975, current_train_items 189792.
I0304 19:30:53.459143 22579586809984 run.py:483] Algo bellman_ford step 5931 current loss 0.025364, current_train_items 189824.
I0304 19:30:53.482667 22579586809984 run.py:483] Algo bellman_ford step 5932 current loss 0.035899, current_train_items 189856.
I0304 19:30:53.512466 22579586809984 run.py:483] Algo bellman_ford step 5933 current loss 0.028649, current_train_items 189888.
I0304 19:30:53.546865 22579586809984 run.py:483] Algo bellman_ford step 5934 current loss 0.065156, current_train_items 189920.
I0304 19:30:53.566333 22579586809984 run.py:483] Algo bellman_ford step 5935 current loss 0.015336, current_train_items 189952.
I0304 19:30:53.582157 22579586809984 run.py:483] Algo bellman_ford step 5936 current loss 0.046188, current_train_items 189984.
I0304 19:30:53.605909 22579586809984 run.py:483] Algo bellman_ford step 5937 current loss 0.050352, current_train_items 190016.
I0304 19:30:53.637564 22579586809984 run.py:483] Algo bellman_ford step 5938 current loss 0.069154, current_train_items 190048.
I0304 19:30:53.669382 22579586809984 run.py:483] Algo bellman_ford step 5939 current loss 0.071676, current_train_items 190080.
I0304 19:30:53.689224 22579586809984 run.py:483] Algo bellman_ford step 5940 current loss 0.003745, current_train_items 190112.
I0304 19:30:53.705335 22579586809984 run.py:483] Algo bellman_ford step 5941 current loss 0.022044, current_train_items 190144.
I0304 19:30:53.729324 22579586809984 run.py:483] Algo bellman_ford step 5942 current loss 0.028824, current_train_items 190176.
I0304 19:30:53.760935 22579586809984 run.py:483] Algo bellman_ford step 5943 current loss 0.033649, current_train_items 190208.
I0304 19:30:53.794765 22579586809984 run.py:483] Algo bellman_ford step 5944 current loss 0.078869, current_train_items 190240.
I0304 19:30:53.814318 22579586809984 run.py:483] Algo bellman_ford step 5945 current loss 0.004210, current_train_items 190272.
I0304 19:30:53.830736 22579586809984 run.py:483] Algo bellman_ford step 5946 current loss 0.019306, current_train_items 190304.
I0304 19:30:53.854749 22579586809984 run.py:483] Algo bellman_ford step 5947 current loss 0.037412, current_train_items 190336.
I0304 19:30:53.884562 22579586809984 run.py:483] Algo bellman_ford step 5948 current loss 0.059183, current_train_items 190368.
I0304 19:30:53.918182 22579586809984 run.py:483] Algo bellman_ford step 5949 current loss 0.071096, current_train_items 190400.
I0304 19:30:53.937834 22579586809984 run.py:483] Algo bellman_ford step 5950 current loss 0.002905, current_train_items 190432.
I0304 19:30:53.946156 22579586809984 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0304 19:30:53.946262 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:30:53.963226 22579586809984 run.py:483] Algo bellman_ford step 5951 current loss 0.006131, current_train_items 190464.
I0304 19:30:53.989096 22579586809984 run.py:483] Algo bellman_ford step 5952 current loss 0.051543, current_train_items 190496.
I0304 19:30:54.021140 22579586809984 run.py:483] Algo bellman_ford step 5953 current loss 0.056220, current_train_items 190528.
I0304 19:30:54.055003 22579586809984 run.py:483] Algo bellman_ford step 5954 current loss 0.100301, current_train_items 190560.
I0304 19:30:54.075519 22579586809984 run.py:483] Algo bellman_ford step 5955 current loss 0.003081, current_train_items 190592.
I0304 19:30:54.092161 22579586809984 run.py:483] Algo bellman_ford step 5956 current loss 0.016777, current_train_items 190624.
I0304 19:30:54.115272 22579586809984 run.py:483] Algo bellman_ford step 5957 current loss 0.056375, current_train_items 190656.
I0304 19:30:54.146233 22579586809984 run.py:483] Algo bellman_ford step 5958 current loss 0.038050, current_train_items 190688.
I0304 19:30:54.179811 22579586809984 run.py:483] Algo bellman_ford step 5959 current loss 0.066317, current_train_items 190720.
I0304 19:30:54.200209 22579586809984 run.py:483] Algo bellman_ford step 5960 current loss 0.003156, current_train_items 190752.
I0304 19:30:54.216977 22579586809984 run.py:483] Algo bellman_ford step 5961 current loss 0.021989, current_train_items 190784.
I0304 19:30:54.241042 22579586809984 run.py:483] Algo bellman_ford step 5962 current loss 0.055308, current_train_items 190816.
I0304 19:30:54.271025 22579586809984 run.py:483] Algo bellman_ford step 5963 current loss 0.040659, current_train_items 190848.
I0304 19:30:54.304229 22579586809984 run.py:483] Algo bellman_ford step 5964 current loss 0.120318, current_train_items 190880.
I0304 19:30:54.323999 22579586809984 run.py:483] Algo bellman_ford step 5965 current loss 0.029202, current_train_items 190912.
I0304 19:30:54.340403 22579586809984 run.py:483] Algo bellman_ford step 5966 current loss 0.038666, current_train_items 190944.
I0304 19:30:54.364435 22579586809984 run.py:483] Algo bellman_ford step 5967 current loss 0.042795, current_train_items 190976.
I0304 19:30:54.395512 22579586809984 run.py:483] Algo bellman_ford step 5968 current loss 0.076010, current_train_items 191008.
I0304 19:30:54.429851 22579586809984 run.py:483] Algo bellman_ford step 5969 current loss 0.099961, current_train_items 191040.
I0304 19:30:54.450067 22579586809984 run.py:483] Algo bellman_ford step 5970 current loss 0.004907, current_train_items 191072.
I0304 19:30:54.466478 22579586809984 run.py:483] Algo bellman_ford step 5971 current loss 0.014624, current_train_items 191104.
I0304 19:30:54.489741 22579586809984 run.py:483] Algo bellman_ford step 5972 current loss 0.022532, current_train_items 191136.
I0304 19:30:54.521529 22579586809984 run.py:483] Algo bellman_ford step 5973 current loss 0.046320, current_train_items 191168.
I0304 19:30:54.554721 22579586809984 run.py:483] Algo bellman_ford step 5974 current loss 0.056148, current_train_items 191200.
I0304 19:30:54.575040 22579586809984 run.py:483] Algo bellman_ford step 5975 current loss 0.003836, current_train_items 191232.
I0304 19:30:54.591112 22579586809984 run.py:483] Algo bellman_ford step 5976 current loss 0.010599, current_train_items 191264.
I0304 19:30:54.614445 22579586809984 run.py:483] Algo bellman_ford step 5977 current loss 0.053217, current_train_items 191296.
I0304 19:30:54.645825 22579586809984 run.py:483] Algo bellman_ford step 5978 current loss 0.088530, current_train_items 191328.
I0304 19:30:54.678182 22579586809984 run.py:483] Algo bellman_ford step 5979 current loss 0.027360, current_train_items 191360.
I0304 19:30:54.698044 22579586809984 run.py:483] Algo bellman_ford step 5980 current loss 0.002361, current_train_items 191392.
I0304 19:30:54.714920 22579586809984 run.py:483] Algo bellman_ford step 5981 current loss 0.039964, current_train_items 191424.
I0304 19:30:54.738378 22579586809984 run.py:483] Algo bellman_ford step 5982 current loss 0.018775, current_train_items 191456.
I0304 19:30:54.769313 22579586809984 run.py:483] Algo bellman_ford step 5983 current loss 0.028996, current_train_items 191488.
I0304 19:30:54.802331 22579586809984 run.py:483] Algo bellman_ford step 5984 current loss 0.061732, current_train_items 191520.
I0304 19:30:54.822539 22579586809984 run.py:483] Algo bellman_ford step 5985 current loss 0.005693, current_train_items 191552.
I0304 19:30:54.839360 22579586809984 run.py:483] Algo bellman_ford step 5986 current loss 0.009625, current_train_items 191584.
I0304 19:30:54.863313 22579586809984 run.py:483] Algo bellman_ford step 5987 current loss 0.028949, current_train_items 191616.
I0304 19:30:54.893572 22579586809984 run.py:483] Algo bellman_ford step 5988 current loss 0.037902, current_train_items 191648.
I0304 19:30:54.926981 22579586809984 run.py:483] Algo bellman_ford step 5989 current loss 0.046875, current_train_items 191680.
I0304 19:30:54.947058 22579586809984 run.py:483] Algo bellman_ford step 5990 current loss 0.001765, current_train_items 191712.
I0304 19:30:54.962841 22579586809984 run.py:483] Algo bellman_ford step 5991 current loss 0.033940, current_train_items 191744.
I0304 19:30:54.986848 22579586809984 run.py:483] Algo bellman_ford step 5992 current loss 0.053777, current_train_items 191776.
I0304 19:30:55.019700 22579586809984 run.py:483] Algo bellman_ford step 5993 current loss 0.061061, current_train_items 191808.
I0304 19:30:55.053247 22579586809984 run.py:483] Algo bellman_ford step 5994 current loss 0.058929, current_train_items 191840.
I0304 19:30:55.072827 22579586809984 run.py:483] Algo bellman_ford step 5995 current loss 0.003322, current_train_items 191872.
I0304 19:30:55.088968 22579586809984 run.py:483] Algo bellman_ford step 5996 current loss 0.008833, current_train_items 191904.
I0304 19:30:55.113193 22579586809984 run.py:483] Algo bellman_ford step 5997 current loss 0.030480, current_train_items 191936.
I0304 19:30:55.144045 22579586809984 run.py:483] Algo bellman_ford step 5998 current loss 0.030007, current_train_items 191968.
I0304 19:30:55.176972 22579586809984 run.py:483] Algo bellman_ford step 5999 current loss 0.036369, current_train_items 192000.
I0304 19:30:55.197101 22579586809984 run.py:483] Algo bellman_ford step 6000 current loss 0.016089, current_train_items 192032.
I0304 19:30:55.204825 22579586809984 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0304 19:30:55.204931 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:30:55.221847 22579586809984 run.py:483] Algo bellman_ford step 6001 current loss 0.031086, current_train_items 192064.
I0304 19:30:55.246073 22579586809984 run.py:483] Algo bellman_ford step 6002 current loss 0.044732, current_train_items 192096.
I0304 19:30:55.276089 22579586809984 run.py:483] Algo bellman_ford step 6003 current loss 0.098121, current_train_items 192128.
I0304 19:30:55.309128 22579586809984 run.py:483] Algo bellman_ford step 6004 current loss 0.080685, current_train_items 192160.
I0304 19:30:55.329064 22579586809984 run.py:483] Algo bellman_ford step 6005 current loss 0.003760, current_train_items 192192.
I0304 19:30:55.345117 22579586809984 run.py:483] Algo bellman_ford step 6006 current loss 0.016412, current_train_items 192224.
I0304 19:30:55.369936 22579586809984 run.py:483] Algo bellman_ford step 6007 current loss 0.065570, current_train_items 192256.
I0304 19:30:55.402980 22579586809984 run.py:483] Algo bellman_ford step 6008 current loss 0.067947, current_train_items 192288.
I0304 19:30:55.437946 22579586809984 run.py:483] Algo bellman_ford step 6009 current loss 0.040504, current_train_items 192320.
I0304 19:30:55.457521 22579586809984 run.py:483] Algo bellman_ford step 6010 current loss 0.003359, current_train_items 192352.
I0304 19:30:55.473789 22579586809984 run.py:483] Algo bellman_ford step 6011 current loss 0.012792, current_train_items 192384.
I0304 19:30:55.497848 22579586809984 run.py:483] Algo bellman_ford step 6012 current loss 0.037044, current_train_items 192416.
I0304 19:30:55.528371 22579586809984 run.py:483] Algo bellman_ford step 6013 current loss 0.040509, current_train_items 192448.
I0304 19:30:55.563803 22579586809984 run.py:483] Algo bellman_ford step 6014 current loss 0.070873, current_train_items 192480.
I0304 19:30:55.583302 22579586809984 run.py:483] Algo bellman_ford step 6015 current loss 0.007036, current_train_items 192512.
I0304 19:30:55.599864 22579586809984 run.py:483] Algo bellman_ford step 6016 current loss 0.011968, current_train_items 192544.
I0304 19:30:55.623291 22579586809984 run.py:483] Algo bellman_ford step 6017 current loss 0.049822, current_train_items 192576.
I0304 19:30:55.654588 22579586809984 run.py:483] Algo bellman_ford step 6018 current loss 0.092548, current_train_items 192608.
I0304 19:30:55.688225 22579586809984 run.py:483] Algo bellman_ford step 6019 current loss 0.075207, current_train_items 192640.
I0304 19:30:55.707948 22579586809984 run.py:483] Algo bellman_ford step 6020 current loss 0.004313, current_train_items 192672.
I0304 19:30:55.724561 22579586809984 run.py:483] Algo bellman_ford step 6021 current loss 0.008309, current_train_items 192704.
I0304 19:30:55.748575 22579586809984 run.py:483] Algo bellman_ford step 6022 current loss 0.031247, current_train_items 192736.
I0304 19:30:55.781224 22579586809984 run.py:483] Algo bellman_ford step 6023 current loss 0.061865, current_train_items 192768.
I0304 19:30:55.815389 22579586809984 run.py:483] Algo bellman_ford step 6024 current loss 0.098537, current_train_items 192800.
I0304 19:30:55.834840 22579586809984 run.py:483] Algo bellman_ford step 6025 current loss 0.003689, current_train_items 192832.
I0304 19:30:55.851357 22579586809984 run.py:483] Algo bellman_ford step 6026 current loss 0.020754, current_train_items 192864.
I0304 19:30:55.875146 22579586809984 run.py:483] Algo bellman_ford step 6027 current loss 0.032418, current_train_items 192896.
I0304 19:30:55.906175 22579586809984 run.py:483] Algo bellman_ford step 6028 current loss 0.095893, current_train_items 192928.
I0304 19:30:55.939675 22579586809984 run.py:483] Algo bellman_ford step 6029 current loss 0.134063, current_train_items 192960.
I0304 19:30:55.959050 22579586809984 run.py:483] Algo bellman_ford step 6030 current loss 0.002884, current_train_items 192992.
I0304 19:30:55.975164 22579586809984 run.py:483] Algo bellman_ford step 6031 current loss 0.016235, current_train_items 193024.
I0304 19:30:55.998990 22579586809984 run.py:483] Algo bellman_ford step 6032 current loss 0.043253, current_train_items 193056.
I0304 19:30:56.031665 22579586809984 run.py:483] Algo bellman_ford step 6033 current loss 0.054443, current_train_items 193088.
I0304 19:30:56.064971 22579586809984 run.py:483] Algo bellman_ford step 6034 current loss 0.071747, current_train_items 193120.
I0304 19:30:56.085025 22579586809984 run.py:483] Algo bellman_ford step 6035 current loss 0.002980, current_train_items 193152.
I0304 19:30:56.101749 22579586809984 run.py:483] Algo bellman_ford step 6036 current loss 0.015488, current_train_items 193184.
I0304 19:30:56.125806 22579586809984 run.py:483] Algo bellman_ford step 6037 current loss 0.022707, current_train_items 193216.
I0304 19:30:56.157415 22579586809984 run.py:483] Algo bellman_ford step 6038 current loss 0.076772, current_train_items 193248.
I0304 19:30:56.190182 22579586809984 run.py:483] Algo bellman_ford step 6039 current loss 0.046202, current_train_items 193280.
I0304 19:30:56.209703 22579586809984 run.py:483] Algo bellman_ford step 6040 current loss 0.002214, current_train_items 193312.
I0304 19:30:56.226008 22579586809984 run.py:483] Algo bellman_ford step 6041 current loss 0.029469, current_train_items 193344.
I0304 19:30:56.250689 22579586809984 run.py:483] Algo bellman_ford step 6042 current loss 0.041973, current_train_items 193376.
I0304 19:30:56.281459 22579586809984 run.py:483] Algo bellman_ford step 6043 current loss 0.062811, current_train_items 193408.
I0304 19:30:56.317755 22579586809984 run.py:483] Algo bellman_ford step 6044 current loss 0.077637, current_train_items 193440.
I0304 19:30:56.337469 22579586809984 run.py:483] Algo bellman_ford step 6045 current loss 0.003459, current_train_items 193472.
I0304 19:30:56.354075 22579586809984 run.py:483] Algo bellman_ford step 6046 current loss 0.024746, current_train_items 193504.
I0304 19:30:56.377645 22579586809984 run.py:483] Algo bellman_ford step 6047 current loss 0.093758, current_train_items 193536.
I0304 19:30:56.409768 22579586809984 run.py:483] Algo bellman_ford step 6048 current loss 0.120607, current_train_items 193568.
I0304 19:30:56.444543 22579586809984 run.py:483] Algo bellman_ford step 6049 current loss 0.073223, current_train_items 193600.
I0304 19:30:56.464307 22579586809984 run.py:483] Algo bellman_ford step 6050 current loss 0.003730, current_train_items 193632.
I0304 19:30:56.472182 22579586809984 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0304 19:30:56.472314 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:56.489131 22579586809984 run.py:483] Algo bellman_ford step 6051 current loss 0.019308, current_train_items 193664.
I0304 19:30:56.513988 22579586809984 run.py:483] Algo bellman_ford step 6052 current loss 0.115586, current_train_items 193696.
I0304 19:30:56.545371 22579586809984 run.py:483] Algo bellman_ford step 6053 current loss 0.052420, current_train_items 193728.
I0304 19:30:56.580719 22579586809984 run.py:483] Algo bellman_ford step 6054 current loss 0.056490, current_train_items 193760.
I0304 19:30:56.600743 22579586809984 run.py:483] Algo bellman_ford step 6055 current loss 0.011850, current_train_items 193792.
I0304 19:30:56.616454 22579586809984 run.py:483] Algo bellman_ford step 6056 current loss 0.025441, current_train_items 193824.
I0304 19:30:56.639717 22579586809984 run.py:483] Algo bellman_ford step 6057 current loss 0.082679, current_train_items 193856.
I0304 19:30:56.670552 22579586809984 run.py:483] Algo bellman_ford step 6058 current loss 0.076306, current_train_items 193888.
I0304 19:30:56.701634 22579586809984 run.py:483] Algo bellman_ford step 6059 current loss 0.055934, current_train_items 193920.
I0304 19:30:56.721671 22579586809984 run.py:483] Algo bellman_ford step 6060 current loss 0.005356, current_train_items 193952.
I0304 19:30:56.738117 22579586809984 run.py:483] Algo bellman_ford step 6061 current loss 0.017565, current_train_items 193984.
I0304 19:30:56.760197 22579586809984 run.py:483] Algo bellman_ford step 6062 current loss 0.032378, current_train_items 194016.
I0304 19:30:56.789913 22579586809984 run.py:483] Algo bellman_ford step 6063 current loss 0.064253, current_train_items 194048.
I0304 19:30:56.823925 22579586809984 run.py:483] Algo bellman_ford step 6064 current loss 0.089602, current_train_items 194080.
I0304 19:30:56.843515 22579586809984 run.py:483] Algo bellman_ford step 6065 current loss 0.005749, current_train_items 194112.
I0304 19:30:56.860091 22579586809984 run.py:483] Algo bellman_ford step 6066 current loss 0.022844, current_train_items 194144.
I0304 19:30:56.884434 22579586809984 run.py:483] Algo bellman_ford step 6067 current loss 0.056673, current_train_items 194176.
I0304 19:30:56.916477 22579586809984 run.py:483] Algo bellman_ford step 6068 current loss 0.066539, current_train_items 194208.
I0304 19:30:56.949216 22579586809984 run.py:483] Algo bellman_ford step 6069 current loss 0.061128, current_train_items 194240.
I0304 19:30:56.968957 22579586809984 run.py:483] Algo bellman_ford step 6070 current loss 0.004105, current_train_items 194272.
I0304 19:30:56.985061 22579586809984 run.py:483] Algo bellman_ford step 6071 current loss 0.010842, current_train_items 194304.
I0304 19:30:57.008769 22579586809984 run.py:483] Algo bellman_ford step 6072 current loss 0.030066, current_train_items 194336.
I0304 19:30:57.039026 22579586809984 run.py:483] Algo bellman_ford step 6073 current loss 0.068513, current_train_items 194368.
I0304 19:30:57.072865 22579586809984 run.py:483] Algo bellman_ford step 6074 current loss 0.043060, current_train_items 194400.
I0304 19:30:57.092749 22579586809984 run.py:483] Algo bellman_ford step 6075 current loss 0.004037, current_train_items 194432.
I0304 19:30:57.109106 22579586809984 run.py:483] Algo bellman_ford step 6076 current loss 0.018031, current_train_items 194464.
I0304 19:30:57.131707 22579586809984 run.py:483] Algo bellman_ford step 6077 current loss 0.024686, current_train_items 194496.
I0304 19:30:57.162307 22579586809984 run.py:483] Algo bellman_ford step 6078 current loss 0.035568, current_train_items 194528.
I0304 19:30:57.196506 22579586809984 run.py:483] Algo bellman_ford step 6079 current loss 0.111689, current_train_items 194560.
I0304 19:30:57.215913 22579586809984 run.py:483] Algo bellman_ford step 6080 current loss 0.004443, current_train_items 194592.
I0304 19:30:57.232277 22579586809984 run.py:483] Algo bellman_ford step 6081 current loss 0.004867, current_train_items 194624.
I0304 19:30:57.255626 22579586809984 run.py:483] Algo bellman_ford step 6082 current loss 0.021644, current_train_items 194656.
I0304 19:30:57.287903 22579586809984 run.py:483] Algo bellman_ford step 6083 current loss 0.050097, current_train_items 194688.
I0304 19:30:57.320043 22579586809984 run.py:483] Algo bellman_ford step 6084 current loss 0.037928, current_train_items 194720.
I0304 19:30:57.339747 22579586809984 run.py:483] Algo bellman_ford step 6085 current loss 0.005690, current_train_items 194752.
I0304 19:30:57.356192 22579586809984 run.py:483] Algo bellman_ford step 6086 current loss 0.011567, current_train_items 194784.
I0304 19:30:57.379768 22579586809984 run.py:483] Algo bellman_ford step 6087 current loss 0.029924, current_train_items 194816.
I0304 19:30:57.410040 22579586809984 run.py:483] Algo bellman_ford step 6088 current loss 0.046085, current_train_items 194848.
I0304 19:30:57.444204 22579586809984 run.py:483] Algo bellman_ford step 6089 current loss 0.100196, current_train_items 194880.
I0304 19:30:57.464106 22579586809984 run.py:483] Algo bellman_ford step 6090 current loss 0.005248, current_train_items 194912.
I0304 19:30:57.480516 22579586809984 run.py:483] Algo bellman_ford step 6091 current loss 0.009387, current_train_items 194944.
I0304 19:30:57.503846 22579586809984 run.py:483] Algo bellman_ford step 6092 current loss 0.106526, current_train_items 194976.
I0304 19:30:57.534298 22579586809984 run.py:483] Algo bellman_ford step 6093 current loss 0.119728, current_train_items 195008.
I0304 19:30:57.566606 22579586809984 run.py:483] Algo bellman_ford step 6094 current loss 0.107604, current_train_items 195040.
I0304 19:30:57.586629 22579586809984 run.py:483] Algo bellman_ford step 6095 current loss 0.003511, current_train_items 195072.
I0304 19:30:57.602863 22579586809984 run.py:483] Algo bellman_ford step 6096 current loss 0.018037, current_train_items 195104.
I0304 19:30:57.626750 22579586809984 run.py:483] Algo bellman_ford step 6097 current loss 0.064272, current_train_items 195136.
I0304 19:30:57.656899 22579586809984 run.py:483] Algo bellman_ford step 6098 current loss 0.061535, current_train_items 195168.
I0304 19:30:57.690489 22579586809984 run.py:483] Algo bellman_ford step 6099 current loss 0.070276, current_train_items 195200.
I0304 19:30:57.710237 22579586809984 run.py:483] Algo bellman_ford step 6100 current loss 0.002190, current_train_items 195232.
I0304 19:30:57.717897 22579586809984 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0304 19:30:57.718003 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:30:57.734344 22579586809984 run.py:483] Algo bellman_ford step 6101 current loss 0.008822, current_train_items 195264.
I0304 19:30:57.759773 22579586809984 run.py:483] Algo bellman_ford step 6102 current loss 0.027775, current_train_items 195296.
I0304 19:30:57.792799 22579586809984 run.py:483] Algo bellman_ford step 6103 current loss 0.055376, current_train_items 195328.
I0304 19:30:57.826300 22579586809984 run.py:483] Algo bellman_ford step 6104 current loss 0.040501, current_train_items 195360.
I0304 19:30:57.846442 22579586809984 run.py:483] Algo bellman_ford step 6105 current loss 0.001869, current_train_items 195392.
I0304 19:30:57.862044 22579586809984 run.py:483] Algo bellman_ford step 6106 current loss 0.026451, current_train_items 195424.
I0304 19:30:57.886165 22579586809984 run.py:483] Algo bellman_ford step 6107 current loss 0.024909, current_train_items 195456.
I0304 19:30:57.917907 22579586809984 run.py:483] Algo bellman_ford step 6108 current loss 0.053459, current_train_items 195488.
I0304 19:30:57.951265 22579586809984 run.py:483] Algo bellman_ford step 6109 current loss 0.054712, current_train_items 195520.
I0304 19:30:57.971341 22579586809984 run.py:483] Algo bellman_ford step 6110 current loss 0.004793, current_train_items 195552.
I0304 19:30:57.987604 22579586809984 run.py:483] Algo bellman_ford step 6111 current loss 0.017539, current_train_items 195584.
I0304 19:30:58.011487 22579586809984 run.py:483] Algo bellman_ford step 6112 current loss 0.043130, current_train_items 195616.
I0304 19:30:58.042550 22579586809984 run.py:483] Algo bellman_ford step 6113 current loss 0.025795, current_train_items 195648.
I0304 19:30:58.076534 22579586809984 run.py:483] Algo bellman_ford step 6114 current loss 0.058467, current_train_items 195680.
I0304 19:30:58.096939 22579586809984 run.py:483] Algo bellman_ford step 6115 current loss 0.025650, current_train_items 195712.
I0304 19:30:58.113270 22579586809984 run.py:483] Algo bellman_ford step 6116 current loss 0.031786, current_train_items 195744.
I0304 19:30:58.137662 22579586809984 run.py:483] Algo bellman_ford step 6117 current loss 0.025672, current_train_items 195776.
I0304 19:30:58.169978 22579586809984 run.py:483] Algo bellman_ford step 6118 current loss 0.064126, current_train_items 195808.
I0304 19:30:58.204703 22579586809984 run.py:483] Algo bellman_ford step 6119 current loss 0.070162, current_train_items 195840.
I0304 19:30:58.224662 22579586809984 run.py:483] Algo bellman_ford step 6120 current loss 0.011041, current_train_items 195872.
I0304 19:30:58.241131 22579586809984 run.py:483] Algo bellman_ford step 6121 current loss 0.009181, current_train_items 195904.
I0304 19:30:58.264743 22579586809984 run.py:483] Algo bellman_ford step 6122 current loss 0.025428, current_train_items 195936.
I0304 19:30:58.297390 22579586809984 run.py:483] Algo bellman_ford step 6123 current loss 0.100181, current_train_items 195968.
I0304 19:30:58.331163 22579586809984 run.py:483] Algo bellman_ford step 6124 current loss 0.108692, current_train_items 196000.
I0304 19:30:58.351090 22579586809984 run.py:483] Algo bellman_ford step 6125 current loss 0.003276, current_train_items 196032.
I0304 19:30:58.366942 22579586809984 run.py:483] Algo bellman_ford step 6126 current loss 0.035975, current_train_items 196064.
I0304 19:30:58.392879 22579586809984 run.py:483] Algo bellman_ford step 6127 current loss 0.114761, current_train_items 196096.
I0304 19:30:58.424496 22579586809984 run.py:483] Algo bellman_ford step 6128 current loss 0.048041, current_train_items 196128.
I0304 19:30:58.457826 22579586809984 run.py:483] Algo bellman_ford step 6129 current loss 0.062729, current_train_items 196160.
I0304 19:30:58.477557 22579586809984 run.py:483] Algo bellman_ford step 6130 current loss 0.003385, current_train_items 196192.
I0304 19:30:58.493718 22579586809984 run.py:483] Algo bellman_ford step 6131 current loss 0.011392, current_train_items 196224.
I0304 19:30:58.518474 22579586809984 run.py:483] Algo bellman_ford step 6132 current loss 0.088844, current_train_items 196256.
I0304 19:30:58.549064 22579586809984 run.py:483] Algo bellman_ford step 6133 current loss 0.088595, current_train_items 196288.
I0304 19:30:58.581881 22579586809984 run.py:483] Algo bellman_ford step 6134 current loss 0.058879, current_train_items 196320.
I0304 19:30:58.601475 22579586809984 run.py:483] Algo bellman_ford step 6135 current loss 0.023728, current_train_items 196352.
I0304 19:30:58.617233 22579586809984 run.py:483] Algo bellman_ford step 6136 current loss 0.015090, current_train_items 196384.
I0304 19:30:58.641674 22579586809984 run.py:483] Algo bellman_ford step 6137 current loss 0.034267, current_train_items 196416.
I0304 19:30:58.674185 22579586809984 run.py:483] Algo bellman_ford step 6138 current loss 0.049579, current_train_items 196448.
I0304 19:30:58.707973 22579586809984 run.py:483] Algo bellman_ford step 6139 current loss 0.043659, current_train_items 196480.
I0304 19:30:58.728022 22579586809984 run.py:483] Algo bellman_ford step 6140 current loss 0.002033, current_train_items 196512.
I0304 19:30:58.744278 22579586809984 run.py:483] Algo bellman_ford step 6141 current loss 0.008886, current_train_items 196544.
I0304 19:30:58.769367 22579586809984 run.py:483] Algo bellman_ford step 6142 current loss 0.024864, current_train_items 196576.
I0304 19:30:58.800664 22579586809984 run.py:483] Algo bellman_ford step 6143 current loss 0.065595, current_train_items 196608.
I0304 19:30:58.834212 22579586809984 run.py:483] Algo bellman_ford step 6144 current loss 0.046051, current_train_items 196640.
I0304 19:30:58.854052 22579586809984 run.py:483] Algo bellman_ford step 6145 current loss 0.022470, current_train_items 196672.
I0304 19:30:58.870430 22579586809984 run.py:483] Algo bellman_ford step 6146 current loss 0.016306, current_train_items 196704.
I0304 19:30:58.893740 22579586809984 run.py:483] Algo bellman_ford step 6147 current loss 0.023883, current_train_items 196736.
I0304 19:30:58.924708 22579586809984 run.py:483] Algo bellman_ford step 6148 current loss 0.035116, current_train_items 196768.
I0304 19:30:58.959833 22579586809984 run.py:483] Algo bellman_ford step 6149 current loss 0.107263, current_train_items 196800.
I0304 19:30:58.979811 22579586809984 run.py:483] Algo bellman_ford step 6150 current loss 0.002879, current_train_items 196832.
I0304 19:30:58.987744 22579586809984 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0304 19:30:58.987848 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0304 19:30:59.004687 22579586809984 run.py:483] Algo bellman_ford step 6151 current loss 0.016381, current_train_items 196864.
I0304 19:30:59.028351 22579586809984 run.py:483] Algo bellman_ford step 6152 current loss 0.016564, current_train_items 196896.
I0304 19:30:59.058568 22579586809984 run.py:483] Algo bellman_ford step 6153 current loss 0.095798, current_train_items 196928.
I0304 19:30:59.092229 22579586809984 run.py:483] Algo bellman_ford step 6154 current loss 0.058753, current_train_items 196960.
I0304 19:30:59.112401 22579586809984 run.py:483] Algo bellman_ford step 6155 current loss 0.003002, current_train_items 196992.
I0304 19:30:59.128512 22579586809984 run.py:483] Algo bellman_ford step 6156 current loss 0.059426, current_train_items 197024.
I0304 19:30:59.152328 22579586809984 run.py:483] Algo bellman_ford step 6157 current loss 0.030622, current_train_items 197056.
I0304 19:30:59.182908 22579586809984 run.py:483] Algo bellman_ford step 6158 current loss 0.076414, current_train_items 197088.
I0304 19:30:59.216274 22579586809984 run.py:483] Algo bellman_ford step 6159 current loss 0.041637, current_train_items 197120.
I0304 19:30:59.236185 22579586809984 run.py:483] Algo bellman_ford step 6160 current loss 0.015783, current_train_items 197152.
I0304 19:30:59.252901 22579586809984 run.py:483] Algo bellman_ford step 6161 current loss 0.018813, current_train_items 197184.
I0304 19:30:59.276544 22579586809984 run.py:483] Algo bellman_ford step 6162 current loss 0.061694, current_train_items 197216.
I0304 19:30:59.309325 22579586809984 run.py:483] Algo bellman_ford step 6163 current loss 0.072067, current_train_items 197248.
I0304 19:30:59.343554 22579586809984 run.py:483] Algo bellman_ford step 6164 current loss 0.096508, current_train_items 197280.
I0304 19:30:59.363210 22579586809984 run.py:483] Algo bellman_ford step 6165 current loss 0.005859, current_train_items 197312.
I0304 19:30:59.379623 22579586809984 run.py:483] Algo bellman_ford step 6166 current loss 0.024157, current_train_items 197344.
I0304 19:30:59.403699 22579586809984 run.py:483] Algo bellman_ford step 6167 current loss 0.089078, current_train_items 197376.
I0304 19:30:59.435470 22579586809984 run.py:483] Algo bellman_ford step 6168 current loss 0.044101, current_train_items 197408.
I0304 19:30:59.469520 22579586809984 run.py:483] Algo bellman_ford step 6169 current loss 0.071317, current_train_items 197440.
I0304 19:30:59.489897 22579586809984 run.py:483] Algo bellman_ford step 6170 current loss 0.007182, current_train_items 197472.
I0304 19:30:59.506017 22579586809984 run.py:483] Algo bellman_ford step 6171 current loss 0.020676, current_train_items 197504.
I0304 19:30:59.529835 22579586809984 run.py:483] Algo bellman_ford step 6172 current loss 0.040839, current_train_items 197536.
I0304 19:30:59.562359 22579586809984 run.py:483] Algo bellman_ford step 6173 current loss 0.063451, current_train_items 197568.
I0304 19:30:59.596316 22579586809984 run.py:483] Algo bellman_ford step 6174 current loss 0.053166, current_train_items 197600.
I0304 19:30:59.616078 22579586809984 run.py:483] Algo bellman_ford step 6175 current loss 0.006407, current_train_items 197632.
I0304 19:30:59.632707 22579586809984 run.py:483] Algo bellman_ford step 6176 current loss 0.017073, current_train_items 197664.
I0304 19:30:59.655935 22579586809984 run.py:483] Algo bellman_ford step 6177 current loss 0.048347, current_train_items 197696.
I0304 19:30:59.687290 22579586809984 run.py:483] Algo bellman_ford step 6178 current loss 0.029350, current_train_items 197728.
I0304 19:30:59.721007 22579586809984 run.py:483] Algo bellman_ford step 6179 current loss 0.067239, current_train_items 197760.
I0304 19:30:59.740734 22579586809984 run.py:483] Algo bellman_ford step 6180 current loss 0.005731, current_train_items 197792.
I0304 19:30:59.757114 22579586809984 run.py:483] Algo bellman_ford step 6181 current loss 0.024915, current_train_items 197824.
I0304 19:30:59.780634 22579586809984 run.py:483] Algo bellman_ford step 6182 current loss 0.026155, current_train_items 197856.
I0304 19:30:59.812171 22579586809984 run.py:483] Algo bellman_ford step 6183 current loss 0.104048, current_train_items 197888.
I0304 19:30:59.848197 22579586809984 run.py:483] Algo bellman_ford step 6184 current loss 0.068644, current_train_items 197920.
I0304 19:30:59.868419 22579586809984 run.py:483] Algo bellman_ford step 6185 current loss 0.004618, current_train_items 197952.
I0304 19:30:59.884830 22579586809984 run.py:483] Algo bellman_ford step 6186 current loss 0.026417, current_train_items 197984.
I0304 19:30:59.908004 22579586809984 run.py:483] Algo bellman_ford step 6187 current loss 0.038590, current_train_items 198016.
I0304 19:30:59.938942 22579586809984 run.py:483] Algo bellman_ford step 6188 current loss 0.024079, current_train_items 198048.
I0304 19:30:59.972981 22579586809984 run.py:483] Algo bellman_ford step 6189 current loss 0.037048, current_train_items 198080.
I0304 19:30:59.993030 22579586809984 run.py:483] Algo bellman_ford step 6190 current loss 0.005221, current_train_items 198112.
I0304 19:31:00.009674 22579586809984 run.py:483] Algo bellman_ford step 6191 current loss 0.033084, current_train_items 198144.
I0304 19:31:00.033786 22579586809984 run.py:483] Algo bellman_ford step 6192 current loss 0.027460, current_train_items 198176.
I0304 19:31:00.065803 22579586809984 run.py:483] Algo bellman_ford step 6193 current loss 0.077523, current_train_items 198208.
I0304 19:31:00.098662 22579586809984 run.py:483] Algo bellman_ford step 6194 current loss 0.061994, current_train_items 198240.
I0304 19:31:00.118412 22579586809984 run.py:483] Algo bellman_ford step 6195 current loss 0.004025, current_train_items 198272.
I0304 19:31:00.134129 22579586809984 run.py:483] Algo bellman_ford step 6196 current loss 0.041111, current_train_items 198304.
I0304 19:31:00.158960 22579586809984 run.py:483] Algo bellman_ford step 6197 current loss 0.074620, current_train_items 198336.
I0304 19:31:00.189097 22579586809984 run.py:483] Algo bellman_ford step 6198 current loss 0.042779, current_train_items 198368.
I0304 19:31:00.223725 22579586809984 run.py:483] Algo bellman_ford step 6199 current loss 0.165920, current_train_items 198400.
I0304 19:31:00.244189 22579586809984 run.py:483] Algo bellman_ford step 6200 current loss 0.004729, current_train_items 198432.
I0304 19:31:00.252057 22579586809984 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0304 19:31:00.252165 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:31:00.269143 22579586809984 run.py:483] Algo bellman_ford step 6201 current loss 0.011910, current_train_items 198464.
I0304 19:31:00.294412 22579586809984 run.py:483] Algo bellman_ford step 6202 current loss 0.084606, current_train_items 198496.
I0304 19:31:00.325114 22579586809984 run.py:483] Algo bellman_ford step 6203 current loss 0.053115, current_train_items 198528.
I0304 19:31:00.358040 22579586809984 run.py:483] Algo bellman_ford step 6204 current loss 0.063107, current_train_items 198560.
I0304 19:31:00.377994 22579586809984 run.py:483] Algo bellman_ford step 6205 current loss 0.003516, current_train_items 198592.
I0304 19:31:00.393362 22579586809984 run.py:483] Algo bellman_ford step 6206 current loss 0.008735, current_train_items 198624.
I0304 19:31:00.417310 22579586809984 run.py:483] Algo bellman_ford step 6207 current loss 0.034738, current_train_items 198656.
I0304 19:31:00.448585 22579586809984 run.py:483] Algo bellman_ford step 6208 current loss 0.060751, current_train_items 198688.
I0304 19:31:00.483226 22579586809984 run.py:483] Algo bellman_ford step 6209 current loss 0.081207, current_train_items 198720.
I0304 19:31:00.502817 22579586809984 run.py:483] Algo bellman_ford step 6210 current loss 0.003258, current_train_items 198752.
I0304 19:31:00.519263 22579586809984 run.py:483] Algo bellman_ford step 6211 current loss 0.012683, current_train_items 198784.
I0304 19:31:00.542801 22579586809984 run.py:483] Algo bellman_ford step 6212 current loss 0.048995, current_train_items 198816.
I0304 19:31:00.572354 22579586809984 run.py:483] Algo bellman_ford step 6213 current loss 0.032805, current_train_items 198848.
I0304 19:31:00.607399 22579586809984 run.py:483] Algo bellman_ford step 6214 current loss 0.053406, current_train_items 198880.
I0304 19:31:00.626930 22579586809984 run.py:483] Algo bellman_ford step 6215 current loss 0.005741, current_train_items 198912.
I0304 19:31:00.643398 22579586809984 run.py:483] Algo bellman_ford step 6216 current loss 0.024089, current_train_items 198944.
I0304 19:31:00.666820 22579586809984 run.py:483] Algo bellman_ford step 6217 current loss 0.034792, current_train_items 198976.
I0304 19:31:00.697504 22579586809984 run.py:483] Algo bellman_ford step 6218 current loss 0.055007, current_train_items 199008.
I0304 19:31:00.730165 22579586809984 run.py:483] Algo bellman_ford step 6219 current loss 0.053485, current_train_items 199040.
I0304 19:31:00.749836 22579586809984 run.py:483] Algo bellman_ford step 6220 current loss 0.005806, current_train_items 199072.
I0304 19:31:00.765543 22579586809984 run.py:483] Algo bellman_ford step 6221 current loss 0.067110, current_train_items 199104.
I0304 19:31:00.790081 22579586809984 run.py:483] Algo bellman_ford step 6222 current loss 0.061758, current_train_items 199136.
I0304 19:31:00.819995 22579586809984 run.py:483] Algo bellman_ford step 6223 current loss 0.061182, current_train_items 199168.
I0304 19:31:00.854396 22579586809984 run.py:483] Algo bellman_ford step 6224 current loss 0.086752, current_train_items 199200.
I0304 19:31:00.874072 22579586809984 run.py:483] Algo bellman_ford step 6225 current loss 0.005959, current_train_items 199232.
I0304 19:31:00.890297 22579586809984 run.py:483] Algo bellman_ford step 6226 current loss 0.038372, current_train_items 199264.
I0304 19:31:00.913249 22579586809984 run.py:483] Algo bellman_ford step 6227 current loss 0.040998, current_train_items 199296.
I0304 19:31:00.943114 22579586809984 run.py:483] Algo bellman_ford step 6228 current loss 0.060125, current_train_items 199328.
I0304 19:31:00.974790 22579586809984 run.py:483] Algo bellman_ford step 6229 current loss 0.056779, current_train_items 199360.
I0304 19:31:00.994318 22579586809984 run.py:483] Algo bellman_ford step 6230 current loss 0.003988, current_train_items 199392.
I0304 19:31:01.010555 22579586809984 run.py:483] Algo bellman_ford step 6231 current loss 0.081958, current_train_items 199424.
I0304 19:31:01.034774 22579586809984 run.py:483] Algo bellman_ford step 6232 current loss 0.074669, current_train_items 199456.
I0304 19:31:01.065055 22579586809984 run.py:483] Algo bellman_ford step 6233 current loss 0.047759, current_train_items 199488.
I0304 19:31:01.098242 22579586809984 run.py:483] Algo bellman_ford step 6234 current loss 0.079521, current_train_items 199520.
I0304 19:31:01.117673 22579586809984 run.py:483] Algo bellman_ford step 6235 current loss 0.002989, current_train_items 199552.
I0304 19:31:01.133935 22579586809984 run.py:483] Algo bellman_ford step 6236 current loss 0.052612, current_train_items 199584.
I0304 19:31:01.156896 22579586809984 run.py:483] Algo bellman_ford step 6237 current loss 0.067495, current_train_items 199616.
I0304 19:31:01.188497 22579586809984 run.py:483] Algo bellman_ford step 6238 current loss 0.046585, current_train_items 199648.
I0304 19:31:01.219504 22579586809984 run.py:483] Algo bellman_ford step 6239 current loss 0.040166, current_train_items 199680.
I0304 19:31:01.239031 22579586809984 run.py:483] Algo bellman_ford step 6240 current loss 0.002768, current_train_items 199712.
I0304 19:31:01.255264 22579586809984 run.py:483] Algo bellman_ford step 6241 current loss 0.049436, current_train_items 199744.
I0304 19:31:01.279298 22579586809984 run.py:483] Algo bellman_ford step 6242 current loss 0.105710, current_train_items 199776.
I0304 19:31:01.310965 22579586809984 run.py:483] Algo bellman_ford step 6243 current loss 0.057048, current_train_items 199808.
I0304 19:31:01.343237 22579586809984 run.py:483] Algo bellman_ford step 6244 current loss 0.025319, current_train_items 199840.
I0304 19:31:01.362904 22579586809984 run.py:483] Algo bellman_ford step 6245 current loss 0.003224, current_train_items 199872.
I0304 19:31:01.379105 22579586809984 run.py:483] Algo bellman_ford step 6246 current loss 0.058065, current_train_items 199904.
I0304 19:31:01.402063 22579586809984 run.py:483] Algo bellman_ford step 6247 current loss 0.026035, current_train_items 199936.
I0304 19:31:01.433852 22579586809984 run.py:483] Algo bellman_ford step 6248 current loss 0.042745, current_train_items 199968.
I0304 19:31:01.465666 22579586809984 run.py:483] Algo bellman_ford step 6249 current loss 0.061775, current_train_items 200000.
I0304 19:31:01.484971 22579586809984 run.py:483] Algo bellman_ford step 6250 current loss 0.003162, current_train_items 200032.
I0304 19:31:01.492882 22579586809984 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0304 19:31:01.492987 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:31:01.510097 22579586809984 run.py:483] Algo bellman_ford step 6251 current loss 0.040745, current_train_items 200064.
I0304 19:31:01.533953 22579586809984 run.py:483] Algo bellman_ford step 6252 current loss 0.085262, current_train_items 200096.
I0304 19:31:01.566238 22579586809984 run.py:483] Algo bellman_ford step 6253 current loss 0.130091, current_train_items 200128.
I0304 19:31:01.598691 22579586809984 run.py:483] Algo bellman_ford step 6254 current loss 0.045737, current_train_items 200160.
I0304 19:31:01.618243 22579586809984 run.py:483] Algo bellman_ford step 6255 current loss 0.010989, current_train_items 200192.
I0304 19:31:01.634058 22579586809984 run.py:483] Algo bellman_ford step 6256 current loss 0.022745, current_train_items 200224.
I0304 19:31:01.658324 22579586809984 run.py:483] Algo bellman_ford step 6257 current loss 0.061608, current_train_items 200256.
I0304 19:31:01.689069 22579586809984 run.py:483] Algo bellman_ford step 6258 current loss 0.060787, current_train_items 200288.
I0304 19:31:01.721635 22579586809984 run.py:483] Algo bellman_ford step 6259 current loss 0.098263, current_train_items 200320.
I0304 19:31:01.741500 22579586809984 run.py:483] Algo bellman_ford step 6260 current loss 0.012722, current_train_items 200352.
I0304 19:31:01.758089 22579586809984 run.py:483] Algo bellman_ford step 6261 current loss 0.019359, current_train_items 200384.
I0304 19:31:01.781378 22579586809984 run.py:483] Algo bellman_ford step 6262 current loss 0.019370, current_train_items 200416.
I0304 19:31:01.812676 22579586809984 run.py:483] Algo bellman_ford step 6263 current loss 0.062044, current_train_items 200448.
I0304 19:31:01.846440 22579586809984 run.py:483] Algo bellman_ford step 6264 current loss 0.073224, current_train_items 200480.
I0304 19:31:01.865935 22579586809984 run.py:483] Algo bellman_ford step 6265 current loss 0.006452, current_train_items 200512.
I0304 19:31:01.882806 22579586809984 run.py:483] Algo bellman_ford step 6266 current loss 0.027318, current_train_items 200544.
I0304 19:31:01.907516 22579586809984 run.py:483] Algo bellman_ford step 6267 current loss 0.035905, current_train_items 200576.
I0304 19:31:01.938403 22579586809984 run.py:483] Algo bellman_ford step 6268 current loss 0.041609, current_train_items 200608.
I0304 19:31:01.969036 22579586809984 run.py:483] Algo bellman_ford step 6269 current loss 0.043053, current_train_items 200640.
I0304 19:31:01.988741 22579586809984 run.py:483] Algo bellman_ford step 6270 current loss 0.002535, current_train_items 200672.
I0304 19:31:02.005190 22579586809984 run.py:483] Algo bellman_ford step 6271 current loss 0.047455, current_train_items 200704.
I0304 19:31:02.028450 22579586809984 run.py:483] Algo bellman_ford step 6272 current loss 0.038366, current_train_items 200736.
I0304 19:31:02.058461 22579586809984 run.py:483] Algo bellman_ford step 6273 current loss 0.048734, current_train_items 200768.
I0304 19:31:02.093245 22579586809984 run.py:483] Algo bellman_ford step 6274 current loss 0.097993, current_train_items 200800.
I0304 19:31:02.113176 22579586809984 run.py:483] Algo bellman_ford step 6275 current loss 0.003846, current_train_items 200832.
I0304 19:31:02.129937 22579586809984 run.py:483] Algo bellman_ford step 6276 current loss 0.015750, current_train_items 200864.
I0304 19:31:02.153519 22579586809984 run.py:483] Algo bellman_ford step 6277 current loss 0.052637, current_train_items 200896.
I0304 19:31:02.183794 22579586809984 run.py:483] Algo bellman_ford step 6278 current loss 0.062443, current_train_items 200928.
I0304 19:31:02.218450 22579586809984 run.py:483] Algo bellman_ford step 6279 current loss 0.065029, current_train_items 200960.
I0304 19:31:02.238162 22579586809984 run.py:483] Algo bellman_ford step 6280 current loss 0.005389, current_train_items 200992.
I0304 19:31:02.254303 22579586809984 run.py:483] Algo bellman_ford step 6281 current loss 0.010134, current_train_items 201024.
I0304 19:31:02.278113 22579586809984 run.py:483] Algo bellman_ford step 6282 current loss 0.047817, current_train_items 201056.
I0304 19:31:02.309250 22579586809984 run.py:483] Algo bellman_ford step 6283 current loss 0.094284, current_train_items 201088.
I0304 19:31:02.343498 22579586809984 run.py:483] Algo bellman_ford step 6284 current loss 0.119580, current_train_items 201120.
I0304 19:31:02.363361 22579586809984 run.py:483] Algo bellman_ford step 6285 current loss 0.009602, current_train_items 201152.
I0304 19:31:02.379835 22579586809984 run.py:483] Algo bellman_ford step 6286 current loss 0.017723, current_train_items 201184.
I0304 19:31:02.403586 22579586809984 run.py:483] Algo bellman_ford step 6287 current loss 0.038349, current_train_items 201216.
I0304 19:31:02.435121 22579586809984 run.py:483] Algo bellman_ford step 6288 current loss 0.046252, current_train_items 201248.
I0304 19:31:02.468776 22579586809984 run.py:483] Algo bellman_ford step 6289 current loss 0.098678, current_train_items 201280.
I0304 19:31:02.488613 22579586809984 run.py:483] Algo bellman_ford step 6290 current loss 0.005038, current_train_items 201312.
I0304 19:31:02.505346 22579586809984 run.py:483] Algo bellman_ford step 6291 current loss 0.062316, current_train_items 201344.
I0304 19:31:02.528775 22579586809984 run.py:483] Algo bellman_ford step 6292 current loss 0.035100, current_train_items 201376.
I0304 19:31:02.560851 22579586809984 run.py:483] Algo bellman_ford step 6293 current loss 0.098539, current_train_items 201408.
I0304 19:31:02.595757 22579586809984 run.py:483] Algo bellman_ford step 6294 current loss 0.060669, current_train_items 201440.
I0304 19:31:02.615292 22579586809984 run.py:483] Algo bellman_ford step 6295 current loss 0.002846, current_train_items 201472.
I0304 19:31:02.632186 22579586809984 run.py:483] Algo bellman_ford step 6296 current loss 0.035087, current_train_items 201504.
I0304 19:31:02.655647 22579586809984 run.py:483] Algo bellman_ford step 6297 current loss 0.011468, current_train_items 201536.
I0304 19:31:02.686975 22579586809984 run.py:483] Algo bellman_ford step 6298 current loss 0.060069, current_train_items 201568.
I0304 19:31:02.719542 22579586809984 run.py:483] Algo bellman_ford step 6299 current loss 0.087969, current_train_items 201600.
I0304 19:31:02.739221 22579586809984 run.py:483] Algo bellman_ford step 6300 current loss 0.003335, current_train_items 201632.
I0304 19:31:02.747061 22579586809984 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0304 19:31:02.747168 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:02.764825 22579586809984 run.py:483] Algo bellman_ford step 6301 current loss 0.023367, current_train_items 201664.
I0304 19:31:02.788417 22579586809984 run.py:483] Algo bellman_ford step 6302 current loss 0.072419, current_train_items 201696.
I0304 19:31:02.819454 22579586809984 run.py:483] Algo bellman_ford step 6303 current loss 0.069066, current_train_items 201728.
I0304 19:31:02.851057 22579586809984 run.py:483] Algo bellman_ford step 6304 current loss 0.055706, current_train_items 201760.
I0304 19:31:02.871306 22579586809984 run.py:483] Algo bellman_ford step 6305 current loss 0.016628, current_train_items 201792.
I0304 19:31:02.887061 22579586809984 run.py:483] Algo bellman_ford step 6306 current loss 0.014916, current_train_items 201824.
I0304 19:31:02.910891 22579586809984 run.py:483] Algo bellman_ford step 6307 current loss 0.081301, current_train_items 201856.
I0304 19:31:02.942711 22579586809984 run.py:483] Algo bellman_ford step 6308 current loss 0.045271, current_train_items 201888.
I0304 19:31:02.975523 22579586809984 run.py:483] Algo bellman_ford step 6309 current loss 0.107480, current_train_items 201920.
I0304 19:31:02.995077 22579586809984 run.py:483] Algo bellman_ford step 6310 current loss 0.010649, current_train_items 201952.
I0304 19:31:03.011192 22579586809984 run.py:483] Algo bellman_ford step 6311 current loss 0.008911, current_train_items 201984.
I0304 19:31:03.033725 22579586809984 run.py:483] Algo bellman_ford step 6312 current loss 0.035464, current_train_items 202016.
I0304 19:31:03.065764 22579586809984 run.py:483] Algo bellman_ford step 6313 current loss 0.116036, current_train_items 202048.
I0304 19:31:03.099979 22579586809984 run.py:483] Algo bellman_ford step 6314 current loss 0.147013, current_train_items 202080.
I0304 19:31:03.119789 22579586809984 run.py:483] Algo bellman_ford step 6315 current loss 0.004407, current_train_items 202112.
I0304 19:31:03.135800 22579586809984 run.py:483] Algo bellman_ford step 6316 current loss 0.011876, current_train_items 202144.
I0304 19:31:03.159281 22579586809984 run.py:483] Algo bellman_ford step 6317 current loss 0.029975, current_train_items 202176.
I0304 19:31:03.189971 22579586809984 run.py:483] Algo bellman_ford step 6318 current loss 0.022384, current_train_items 202208.
I0304 19:31:03.222808 22579586809984 run.py:483] Algo bellman_ford step 6319 current loss 0.054372, current_train_items 202240.
I0304 19:31:03.242282 22579586809984 run.py:483] Algo bellman_ford step 6320 current loss 0.013610, current_train_items 202272.
I0304 19:31:03.258718 22579586809984 run.py:483] Algo bellman_ford step 6321 current loss 0.022494, current_train_items 202304.
I0304 19:31:03.283320 22579586809984 run.py:483] Algo bellman_ford step 6322 current loss 0.035097, current_train_items 202336.
I0304 19:31:03.315639 22579586809984 run.py:483] Algo bellman_ford step 6323 current loss 0.060782, current_train_items 202368.
I0304 19:31:03.349572 22579586809984 run.py:483] Algo bellman_ford step 6324 current loss 0.057282, current_train_items 202400.
I0304 19:31:03.369217 22579586809984 run.py:483] Algo bellman_ford step 6325 current loss 0.004371, current_train_items 202432.
I0304 19:31:03.385213 22579586809984 run.py:483] Algo bellman_ford step 6326 current loss 0.014381, current_train_items 202464.
I0304 19:31:03.410015 22579586809984 run.py:483] Algo bellman_ford step 6327 current loss 0.059025, current_train_items 202496.
I0304 19:31:03.440648 22579586809984 run.py:483] Algo bellman_ford step 6328 current loss 0.055885, current_train_items 202528.
I0304 19:31:03.474797 22579586809984 run.py:483] Algo bellman_ford step 6329 current loss 0.053811, current_train_items 202560.
I0304 19:31:03.494208 22579586809984 run.py:483] Algo bellman_ford step 6330 current loss 0.003300, current_train_items 202592.
I0304 19:31:03.510392 22579586809984 run.py:483] Algo bellman_ford step 6331 current loss 0.031248, current_train_items 202624.
I0304 19:31:03.533614 22579586809984 run.py:483] Algo bellman_ford step 6332 current loss 0.057654, current_train_items 202656.
I0304 19:31:03.564677 22579586809984 run.py:483] Algo bellman_ford step 6333 current loss 0.023574, current_train_items 202688.
I0304 19:31:03.597178 22579586809984 run.py:483] Algo bellman_ford step 6334 current loss 0.036248, current_train_items 202720.
I0304 19:31:03.616748 22579586809984 run.py:483] Algo bellman_ford step 6335 current loss 0.002765, current_train_items 202752.
I0304 19:31:03.633163 22579586809984 run.py:483] Algo bellman_ford step 6336 current loss 0.009553, current_train_items 202784.
I0304 19:31:03.657270 22579586809984 run.py:483] Algo bellman_ford step 6337 current loss 0.026232, current_train_items 202816.
I0304 19:31:03.689298 22579586809984 run.py:483] Algo bellman_ford step 6338 current loss 0.061695, current_train_items 202848.
I0304 19:31:03.722116 22579586809984 run.py:483] Algo bellman_ford step 6339 current loss 0.056313, current_train_items 202880.
I0304 19:31:03.741520 22579586809984 run.py:483] Algo bellman_ford step 6340 current loss 0.002885, current_train_items 202912.
I0304 19:31:03.757355 22579586809984 run.py:483] Algo bellman_ford step 6341 current loss 0.007274, current_train_items 202944.
I0304 19:31:03.781830 22579586809984 run.py:483] Algo bellman_ford step 6342 current loss 0.057161, current_train_items 202976.
I0304 19:31:03.812214 22579586809984 run.py:483] Algo bellman_ford step 6343 current loss 0.023487, current_train_items 203008.
I0304 19:31:03.842957 22579586809984 run.py:483] Algo bellman_ford step 6344 current loss 0.068491, current_train_items 203040.
I0304 19:31:03.862649 22579586809984 run.py:483] Algo bellman_ford step 6345 current loss 0.002732, current_train_items 203072.
I0304 19:31:03.879130 22579586809984 run.py:483] Algo bellman_ford step 6346 current loss 0.040328, current_train_items 203104.
I0304 19:31:03.902792 22579586809984 run.py:483] Algo bellman_ford step 6347 current loss 0.041476, current_train_items 203136.
I0304 19:31:03.933247 22579586809984 run.py:483] Algo bellman_ford step 6348 current loss 0.054627, current_train_items 203168.
I0304 19:31:03.968303 22579586809984 run.py:483] Algo bellman_ford step 6349 current loss 0.051314, current_train_items 203200.
I0304 19:31:03.987908 22579586809984 run.py:483] Algo bellman_ford step 6350 current loss 0.021032, current_train_items 203232.
I0304 19:31:03.995795 22579586809984 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0304 19:31:03.995899 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:04.013139 22579586809984 run.py:483] Algo bellman_ford step 6351 current loss 0.015225, current_train_items 203264.
I0304 19:31:04.036978 22579586809984 run.py:483] Algo bellman_ford step 6352 current loss 0.060702, current_train_items 203296.
I0304 19:31:04.068642 22579586809984 run.py:483] Algo bellman_ford step 6353 current loss 0.061601, current_train_items 203328.
I0304 19:31:04.102541 22579586809984 run.py:483] Algo bellman_ford step 6354 current loss 0.094065, current_train_items 203360.
I0304 19:31:04.122606 22579586809984 run.py:483] Algo bellman_ford step 6355 current loss 0.005888, current_train_items 203392.
I0304 19:31:04.138796 22579586809984 run.py:483] Algo bellman_ford step 6356 current loss 0.015030, current_train_items 203424.
I0304 19:31:04.163524 22579586809984 run.py:483] Algo bellman_ford step 6357 current loss 0.052467, current_train_items 203456.
I0304 19:31:04.194809 22579586809984 run.py:483] Algo bellman_ford step 6358 current loss 0.107982, current_train_items 203488.
I0304 19:31:04.228582 22579586809984 run.py:483] Algo bellman_ford step 6359 current loss 0.078932, current_train_items 203520.
I0304 19:31:04.248780 22579586809984 run.py:483] Algo bellman_ford step 6360 current loss 0.004725, current_train_items 203552.
I0304 19:31:04.265439 22579586809984 run.py:483] Algo bellman_ford step 6361 current loss 0.013687, current_train_items 203584.
I0304 19:31:04.289037 22579586809984 run.py:483] Algo bellman_ford step 6362 current loss 0.038564, current_train_items 203616.
I0304 19:31:04.320301 22579586809984 run.py:483] Algo bellman_ford step 6363 current loss 0.036777, current_train_items 203648.
I0304 19:31:04.352961 22579586809984 run.py:483] Algo bellman_ford step 6364 current loss 0.064122, current_train_items 203680.
I0304 19:31:04.373021 22579586809984 run.py:483] Algo bellman_ford step 6365 current loss 0.003668, current_train_items 203712.
I0304 19:31:04.389842 22579586809984 run.py:483] Algo bellman_ford step 6366 current loss 0.020879, current_train_items 203744.
I0304 19:31:04.413309 22579586809984 run.py:483] Algo bellman_ford step 6367 current loss 0.056801, current_train_items 203776.
I0304 19:31:04.445908 22579586809984 run.py:483] Algo bellman_ford step 6368 current loss 0.071914, current_train_items 203808.
I0304 19:31:04.478244 22579586809984 run.py:483] Algo bellman_ford step 6369 current loss 0.036246, current_train_items 203840.
I0304 19:31:04.498289 22579586809984 run.py:483] Algo bellman_ford step 6370 current loss 0.003397, current_train_items 203872.
I0304 19:31:04.514872 22579586809984 run.py:483] Algo bellman_ford step 6371 current loss 0.015707, current_train_items 203904.
I0304 19:31:04.539250 22579586809984 run.py:483] Algo bellman_ford step 6372 current loss 0.173280, current_train_items 203936.
I0304 19:31:04.569697 22579586809984 run.py:483] Algo bellman_ford step 6373 current loss 0.150036, current_train_items 203968.
I0304 19:31:04.603349 22579586809984 run.py:483] Algo bellman_ford step 6374 current loss 0.082390, current_train_items 204000.
I0304 19:31:04.623473 22579586809984 run.py:483] Algo bellman_ford step 6375 current loss 0.004498, current_train_items 204032.
I0304 19:31:04.639563 22579586809984 run.py:483] Algo bellman_ford step 6376 current loss 0.006315, current_train_items 204064.
I0304 19:31:04.663893 22579586809984 run.py:483] Algo bellman_ford step 6377 current loss 0.035127, current_train_items 204096.
I0304 19:31:04.695204 22579586809984 run.py:483] Algo bellman_ford step 6378 current loss 0.115367, current_train_items 204128.
I0304 19:31:04.731261 22579586809984 run.py:483] Algo bellman_ford step 6379 current loss 0.119194, current_train_items 204160.
I0304 19:31:04.751672 22579586809984 run.py:483] Algo bellman_ford step 6380 current loss 0.018073, current_train_items 204192.
I0304 19:31:04.767867 22579586809984 run.py:483] Algo bellman_ford step 6381 current loss 0.053048, current_train_items 204224.
I0304 19:31:04.791826 22579586809984 run.py:483] Algo bellman_ford step 6382 current loss 0.029294, current_train_items 204256.
I0304 19:31:04.822005 22579586809984 run.py:483] Algo bellman_ford step 6383 current loss 0.044880, current_train_items 204288.
I0304 19:31:04.855411 22579586809984 run.py:483] Algo bellman_ford step 6384 current loss 0.078505, current_train_items 204320.
I0304 19:31:04.876089 22579586809984 run.py:483] Algo bellman_ford step 6385 current loss 0.002758, current_train_items 204352.
I0304 19:31:04.891977 22579586809984 run.py:483] Algo bellman_ford step 6386 current loss 0.008300, current_train_items 204384.
I0304 19:31:04.916073 22579586809984 run.py:483] Algo bellman_ford step 6387 current loss 0.050823, current_train_items 204416.
I0304 19:31:04.946414 22579586809984 run.py:483] Algo bellman_ford step 6388 current loss 0.030546, current_train_items 204448.
I0304 19:31:04.979868 22579586809984 run.py:483] Algo bellman_ford step 6389 current loss 0.046213, current_train_items 204480.
I0304 19:31:04.999886 22579586809984 run.py:483] Algo bellman_ford step 6390 current loss 0.003089, current_train_items 204512.
I0304 19:31:05.016529 22579586809984 run.py:483] Algo bellman_ford step 6391 current loss 0.013893, current_train_items 204544.
I0304 19:31:05.040241 22579586809984 run.py:483] Algo bellman_ford step 6392 current loss 0.028361, current_train_items 204576.
I0304 19:31:05.069799 22579586809984 run.py:483] Algo bellman_ford step 6393 current loss 0.028278, current_train_items 204608.
I0304 19:31:05.104099 22579586809984 run.py:483] Algo bellman_ford step 6394 current loss 0.041485, current_train_items 204640.
I0304 19:31:05.124191 22579586809984 run.py:483] Algo bellman_ford step 6395 current loss 0.002911, current_train_items 204672.
I0304 19:31:05.140369 22579586809984 run.py:483] Algo bellman_ford step 6396 current loss 0.015128, current_train_items 204704.
I0304 19:31:05.164041 22579586809984 run.py:483] Algo bellman_ford step 6397 current loss 0.044333, current_train_items 204736.
I0304 19:31:05.194643 22579586809984 run.py:483] Algo bellman_ford step 6398 current loss 0.040153, current_train_items 204768.
I0304 19:31:05.226394 22579586809984 run.py:483] Algo bellman_ford step 6399 current loss 0.068057, current_train_items 204800.
I0304 19:31:05.246512 22579586809984 run.py:483] Algo bellman_ford step 6400 current loss 0.003191, current_train_items 204832.
I0304 19:31:05.254605 22579586809984 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0304 19:31:05.254719 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0304 19:31:05.270993 22579586809984 run.py:483] Algo bellman_ford step 6401 current loss 0.008709, current_train_items 204864.
I0304 19:31:05.295115 22579586809984 run.py:483] Algo bellman_ford step 6402 current loss 0.015981, current_train_items 204896.
I0304 19:31:05.326634 22579586809984 run.py:483] Algo bellman_ford step 6403 current loss 0.066247, current_train_items 204928.
I0304 19:31:05.360179 22579586809984 run.py:483] Algo bellman_ford step 6404 current loss 0.059376, current_train_items 204960.
I0304 19:31:05.380062 22579586809984 run.py:483] Algo bellman_ford step 6405 current loss 0.007953, current_train_items 204992.
I0304 19:31:05.395954 22579586809984 run.py:483] Algo bellman_ford step 6406 current loss 0.024127, current_train_items 205024.
I0304 19:31:05.419438 22579586809984 run.py:483] Algo bellman_ford step 6407 current loss 0.023514, current_train_items 205056.
I0304 19:31:05.450486 22579586809984 run.py:483] Algo bellman_ford step 6408 current loss 0.043076, current_train_items 205088.
I0304 19:31:05.485335 22579586809984 run.py:483] Algo bellman_ford step 6409 current loss 0.050859, current_train_items 205120.
I0304 19:31:05.504754 22579586809984 run.py:483] Algo bellman_ford step 6410 current loss 0.006181, current_train_items 205152.
I0304 19:31:05.520745 22579586809984 run.py:483] Algo bellman_ford step 6411 current loss 0.012500, current_train_items 205184.
I0304 19:31:05.543740 22579586809984 run.py:483] Algo bellman_ford step 6412 current loss 0.025363, current_train_items 205216.
I0304 19:31:05.574934 22579586809984 run.py:483] Algo bellman_ford step 6413 current loss 0.044729, current_train_items 205248.
I0304 19:31:05.608497 22579586809984 run.py:483] Algo bellman_ford step 6414 current loss 0.041761, current_train_items 205280.
I0304 19:31:05.627975 22579586809984 run.py:483] Algo bellman_ford step 6415 current loss 0.002394, current_train_items 205312.
I0304 19:31:05.644120 22579586809984 run.py:483] Algo bellman_ford step 6416 current loss 0.025367, current_train_items 205344.
I0304 19:31:05.668620 22579586809984 run.py:483] Algo bellman_ford step 6417 current loss 0.030980, current_train_items 205376.
I0304 19:31:05.701037 22579586809984 run.py:483] Algo bellman_ford step 6418 current loss 0.032942, current_train_items 205408.
I0304 19:31:05.735233 22579586809984 run.py:483] Algo bellman_ford step 6419 current loss 0.081086, current_train_items 205440.
I0304 19:31:05.754899 22579586809984 run.py:483] Algo bellman_ford step 6420 current loss 0.002768, current_train_items 205472.
I0304 19:31:05.771120 22579586809984 run.py:483] Algo bellman_ford step 6421 current loss 0.021402, current_train_items 205504.
I0304 19:31:05.794471 22579586809984 run.py:483] Algo bellman_ford step 6422 current loss 0.036520, current_train_items 205536.
I0304 19:31:05.825202 22579586809984 run.py:483] Algo bellman_ford step 6423 current loss 0.064161, current_train_items 205568.
I0304 19:31:05.858325 22579586809984 run.py:483] Algo bellman_ford step 6424 current loss 0.037891, current_train_items 205600.
I0304 19:31:05.877713 22579586809984 run.py:483] Algo bellman_ford step 6425 current loss 0.003423, current_train_items 205632.
I0304 19:31:05.894335 22579586809984 run.py:483] Algo bellman_ford step 6426 current loss 0.030235, current_train_items 205664.
I0304 19:31:05.918212 22579586809984 run.py:483] Algo bellman_ford step 6427 current loss 0.018667, current_train_items 205696.
I0304 19:31:05.949954 22579586809984 run.py:483] Algo bellman_ford step 6428 current loss 0.044908, current_train_items 205728.
I0304 19:31:05.985458 22579586809984 run.py:483] Algo bellman_ford step 6429 current loss 0.043650, current_train_items 205760.
I0304 19:31:06.005185 22579586809984 run.py:483] Algo bellman_ford step 6430 current loss 0.002646, current_train_items 205792.
I0304 19:31:06.020905 22579586809984 run.py:483] Algo bellman_ford step 6431 current loss 0.063102, current_train_items 205824.
I0304 19:31:06.044688 22579586809984 run.py:483] Algo bellman_ford step 6432 current loss 0.029308, current_train_items 205856.
I0304 19:31:06.076094 22579586809984 run.py:483] Algo bellman_ford step 6433 current loss 0.067595, current_train_items 205888.
I0304 19:31:06.108690 22579586809984 run.py:483] Algo bellman_ford step 6434 current loss 0.048206, current_train_items 205920.
I0304 19:31:06.128295 22579586809984 run.py:483] Algo bellman_ford step 6435 current loss 0.003974, current_train_items 205952.
I0304 19:31:06.144462 22579586809984 run.py:483] Algo bellman_ford step 6436 current loss 0.008411, current_train_items 205984.
I0304 19:31:06.167914 22579586809984 run.py:483] Algo bellman_ford step 6437 current loss 0.037564, current_train_items 206016.
I0304 19:31:06.199896 22579586809984 run.py:483] Algo bellman_ford step 6438 current loss 0.066494, current_train_items 206048.
I0304 19:31:06.234007 22579586809984 run.py:483] Algo bellman_ford step 6439 current loss 0.066171, current_train_items 206080.
I0304 19:31:06.253475 22579586809984 run.py:483] Algo bellman_ford step 6440 current loss 0.009983, current_train_items 206112.
I0304 19:31:06.269970 22579586809984 run.py:483] Algo bellman_ford step 6441 current loss 0.013211, current_train_items 206144.
I0304 19:31:06.292700 22579586809984 run.py:483] Algo bellman_ford step 6442 current loss 0.017157, current_train_items 206176.
I0304 19:31:06.322457 22579586809984 run.py:483] Algo bellman_ford step 6443 current loss 0.046428, current_train_items 206208.
I0304 19:31:06.356399 22579586809984 run.py:483] Algo bellman_ford step 6444 current loss 0.041168, current_train_items 206240.
I0304 19:31:06.376027 22579586809984 run.py:483] Algo bellman_ford step 6445 current loss 0.006611, current_train_items 206272.
I0304 19:31:06.392033 22579586809984 run.py:483] Algo bellman_ford step 6446 current loss 0.016366, current_train_items 206304.
I0304 19:31:06.416546 22579586809984 run.py:483] Algo bellman_ford step 6447 current loss 0.025654, current_train_items 206336.
I0304 19:31:06.447801 22579586809984 run.py:483] Algo bellman_ford step 6448 current loss 0.037855, current_train_items 206368.
I0304 19:31:06.481300 22579586809984 run.py:483] Algo bellman_ford step 6449 current loss 0.041366, current_train_items 206400.
I0304 19:31:06.500880 22579586809984 run.py:483] Algo bellman_ford step 6450 current loss 0.003914, current_train_items 206432.
I0304 19:31:06.509160 22579586809984 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0304 19:31:06.509265 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:06.526170 22579586809984 run.py:483] Algo bellman_ford step 6451 current loss 0.007473, current_train_items 206464.
I0304 19:31:06.551424 22579586809984 run.py:483] Algo bellman_ford step 6452 current loss 0.035266, current_train_items 206496.
I0304 19:31:06.581963 22579586809984 run.py:483] Algo bellman_ford step 6453 current loss 0.063985, current_train_items 206528.
I0304 19:31:06.614392 22579586809984 run.py:483] Algo bellman_ford step 6454 current loss 0.043527, current_train_items 206560.
I0304 19:31:06.634166 22579586809984 run.py:483] Algo bellman_ford step 6455 current loss 0.016608, current_train_items 206592.
I0304 19:31:06.650193 22579586809984 run.py:483] Algo bellman_ford step 6456 current loss 0.005401, current_train_items 206624.
I0304 19:31:06.673114 22579586809984 run.py:483] Algo bellman_ford step 6457 current loss 0.029363, current_train_items 206656.
I0304 19:31:06.703870 22579586809984 run.py:483] Algo bellman_ford step 6458 current loss 0.038784, current_train_items 206688.
I0304 19:31:06.735878 22579586809984 run.py:483] Algo bellman_ford step 6459 current loss 0.072113, current_train_items 206720.
I0304 19:31:06.756223 22579586809984 run.py:483] Algo bellman_ford step 6460 current loss 0.017258, current_train_items 206752.
I0304 19:31:06.772627 22579586809984 run.py:483] Algo bellman_ford step 6461 current loss 0.003229, current_train_items 206784.
I0304 19:31:06.795583 22579586809984 run.py:483] Algo bellman_ford step 6462 current loss 0.086259, current_train_items 206816.
I0304 19:31:06.826032 22579586809984 run.py:483] Algo bellman_ford step 6463 current loss 0.017584, current_train_items 206848.
I0304 19:31:06.861561 22579586809984 run.py:483] Algo bellman_ford step 6464 current loss 0.118437, current_train_items 206880.
I0304 19:31:06.880891 22579586809984 run.py:483] Algo bellman_ford step 6465 current loss 0.001849, current_train_items 206912.
I0304 19:31:06.897390 22579586809984 run.py:483] Algo bellman_ford step 6466 current loss 0.010444, current_train_items 206944.
I0304 19:31:06.922418 22579586809984 run.py:483] Algo bellman_ford step 6467 current loss 0.083666, current_train_items 206976.
I0304 19:31:06.954791 22579586809984 run.py:483] Algo bellman_ford step 6468 current loss 0.041754, current_train_items 207008.
I0304 19:31:06.989856 22579586809984 run.py:483] Algo bellman_ford step 6469 current loss 0.056045, current_train_items 207040.
I0304 19:31:07.009818 22579586809984 run.py:483] Algo bellman_ford step 6470 current loss 0.003926, current_train_items 207072.
I0304 19:31:07.026658 22579586809984 run.py:483] Algo bellman_ford step 6471 current loss 0.011869, current_train_items 207104.
I0304 19:31:07.049565 22579586809984 run.py:483] Algo bellman_ford step 6472 current loss 0.029414, current_train_items 207136.
I0304 19:31:07.080744 22579586809984 run.py:483] Algo bellman_ford step 6473 current loss 0.082964, current_train_items 207168.
I0304 19:31:07.113321 22579586809984 run.py:483] Algo bellman_ford step 6474 current loss 0.073561, current_train_items 207200.
I0304 19:31:07.132953 22579586809984 run.py:483] Algo bellman_ford step 6475 current loss 0.012967, current_train_items 207232.
I0304 19:31:07.149523 22579586809984 run.py:483] Algo bellman_ford step 6476 current loss 0.036580, current_train_items 207264.
I0304 19:31:07.172223 22579586809984 run.py:483] Algo bellman_ford step 6477 current loss 0.041360, current_train_items 207296.
I0304 19:31:07.203464 22579586809984 run.py:483] Algo bellman_ford step 6478 current loss 0.050720, current_train_items 207328.
I0304 19:31:07.238240 22579586809984 run.py:483] Algo bellman_ford step 6479 current loss 0.051607, current_train_items 207360.
I0304 19:31:07.258013 22579586809984 run.py:483] Algo bellman_ford step 6480 current loss 0.004668, current_train_items 207392.
I0304 19:31:07.274065 22579586809984 run.py:483] Algo bellman_ford step 6481 current loss 0.070980, current_train_items 207424.
I0304 19:31:07.297671 22579586809984 run.py:483] Algo bellman_ford step 6482 current loss 0.029991, current_train_items 207456.
I0304 19:31:07.329499 22579586809984 run.py:483] Algo bellman_ford step 6483 current loss 0.040807, current_train_items 207488.
I0304 19:31:07.364175 22579586809984 run.py:483] Algo bellman_ford step 6484 current loss 0.060310, current_train_items 207520.
I0304 19:31:07.384689 22579586809984 run.py:483] Algo bellman_ford step 6485 current loss 0.004275, current_train_items 207552.
I0304 19:31:07.401266 22579586809984 run.py:483] Algo bellman_ford step 6486 current loss 0.049075, current_train_items 207584.
I0304 19:31:07.426557 22579586809984 run.py:483] Algo bellman_ford step 6487 current loss 0.030071, current_train_items 207616.
I0304 19:31:07.456399 22579586809984 run.py:483] Algo bellman_ford step 6488 current loss 0.027821, current_train_items 207648.
I0304 19:31:07.490418 22579586809984 run.py:483] Algo bellman_ford step 6489 current loss 0.037156, current_train_items 207680.
I0304 19:31:07.510797 22579586809984 run.py:483] Algo bellman_ford step 6490 current loss 0.002589, current_train_items 207712.
I0304 19:31:07.526948 22579586809984 run.py:483] Algo bellman_ford step 6491 current loss 0.006150, current_train_items 207744.
I0304 19:31:07.549754 22579586809984 run.py:483] Algo bellman_ford step 6492 current loss 0.023848, current_train_items 207776.
I0304 19:31:07.580595 22579586809984 run.py:483] Algo bellman_ford step 6493 current loss 0.022715, current_train_items 207808.
I0304 19:31:07.613802 22579586809984 run.py:483] Algo bellman_ford step 6494 current loss 0.091444, current_train_items 207840.
I0304 19:31:07.633181 22579586809984 run.py:483] Algo bellman_ford step 6495 current loss 0.002877, current_train_items 207872.
I0304 19:31:07.648979 22579586809984 run.py:483] Algo bellman_ford step 6496 current loss 0.016276, current_train_items 207904.
I0304 19:31:07.673203 22579586809984 run.py:483] Algo bellman_ford step 6497 current loss 0.021251, current_train_items 207936.
I0304 19:31:07.703433 22579586809984 run.py:483] Algo bellman_ford step 6498 current loss 0.031317, current_train_items 207968.
I0304 19:31:07.738318 22579586809984 run.py:483] Algo bellman_ford step 6499 current loss 0.052214, current_train_items 208000.
I0304 19:31:07.758103 22579586809984 run.py:483] Algo bellman_ford step 6500 current loss 0.002482, current_train_items 208032.
I0304 19:31:07.765769 22579586809984 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0304 19:31:07.765875 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:31:07.782715 22579586809984 run.py:483] Algo bellman_ford step 6501 current loss 0.014498, current_train_items 208064.
I0304 19:31:07.807571 22579586809984 run.py:483] Algo bellman_ford step 6502 current loss 0.052312, current_train_items 208096.
I0304 19:31:07.838807 22579586809984 run.py:483] Algo bellman_ford step 6503 current loss 0.028127, current_train_items 208128.
I0304 19:31:07.873628 22579586809984 run.py:483] Algo bellman_ford step 6504 current loss 0.043317, current_train_items 208160.
I0304 19:31:07.893461 22579586809984 run.py:483] Algo bellman_ford step 6505 current loss 0.002661, current_train_items 208192.
I0304 19:31:07.908855 22579586809984 run.py:483] Algo bellman_ford step 6506 current loss 0.003948, current_train_items 208224.
I0304 19:31:07.933140 22579586809984 run.py:483] Algo bellman_ford step 6507 current loss 0.091571, current_train_items 208256.
I0304 19:31:07.965082 22579586809984 run.py:483] Algo bellman_ford step 6508 current loss 0.071205, current_train_items 208288.
I0304 19:31:07.996347 22579586809984 run.py:483] Algo bellman_ford step 6509 current loss 0.072377, current_train_items 208320.
I0304 19:31:08.015871 22579586809984 run.py:483] Algo bellman_ford step 6510 current loss 0.004642, current_train_items 208352.
I0304 19:31:08.032137 22579586809984 run.py:483] Algo bellman_ford step 6511 current loss 0.045227, current_train_items 208384.
I0304 19:31:08.056533 22579586809984 run.py:483] Algo bellman_ford step 6512 current loss 0.055930, current_train_items 208416.
I0304 19:31:08.087282 22579586809984 run.py:483] Algo bellman_ford step 6513 current loss 0.028685, current_train_items 208448.
I0304 19:31:08.121953 22579586809984 run.py:483] Algo bellman_ford step 6514 current loss 0.072768, current_train_items 208480.
I0304 19:31:08.141538 22579586809984 run.py:483] Algo bellman_ford step 6515 current loss 0.004257, current_train_items 208512.
I0304 19:31:08.157575 22579586809984 run.py:483] Algo bellman_ford step 6516 current loss 0.021935, current_train_items 208544.
I0304 19:31:08.181695 22579586809984 run.py:483] Algo bellman_ford step 6517 current loss 0.035103, current_train_items 208576.
I0304 19:31:08.212604 22579586809984 run.py:483] Algo bellman_ford step 6518 current loss 0.075695, current_train_items 208608.
I0304 19:31:08.245266 22579586809984 run.py:483] Algo bellman_ford step 6519 current loss 0.058706, current_train_items 208640.
I0304 19:31:08.264662 22579586809984 run.py:483] Algo bellman_ford step 6520 current loss 0.006666, current_train_items 208672.
I0304 19:31:08.280889 22579586809984 run.py:483] Algo bellman_ford step 6521 current loss 0.008515, current_train_items 208704.
I0304 19:31:08.305292 22579586809984 run.py:483] Algo bellman_ford step 6522 current loss 0.067116, current_train_items 208736.
I0304 19:31:08.335884 22579586809984 run.py:483] Algo bellman_ford step 6523 current loss 0.055944, current_train_items 208768.
I0304 19:31:08.367441 22579586809984 run.py:483] Algo bellman_ford step 6524 current loss 0.052619, current_train_items 208800.
I0304 19:31:08.387244 22579586809984 run.py:483] Algo bellman_ford step 6525 current loss 0.003061, current_train_items 208832.
I0304 19:31:08.403271 22579586809984 run.py:483] Algo bellman_ford step 6526 current loss 0.009631, current_train_items 208864.
I0304 19:31:08.426604 22579586809984 run.py:483] Algo bellman_ford step 6527 current loss 0.050604, current_train_items 208896.
I0304 19:31:08.457073 22579586809984 run.py:483] Algo bellman_ford step 6528 current loss 0.044880, current_train_items 208928.
I0304 19:31:08.490864 22579586809984 run.py:483] Algo bellman_ford step 6529 current loss 0.064896, current_train_items 208960.
I0304 19:31:08.510221 22579586809984 run.py:483] Algo bellman_ford step 6530 current loss 0.003322, current_train_items 208992.
I0304 19:31:08.526637 22579586809984 run.py:483] Algo bellman_ford step 6531 current loss 0.021396, current_train_items 209024.
I0304 19:31:08.551845 22579586809984 run.py:483] Algo bellman_ford step 6532 current loss 0.071053, current_train_items 209056.
I0304 19:31:08.582162 22579586809984 run.py:483] Algo bellman_ford step 6533 current loss 0.086946, current_train_items 209088.
I0304 19:31:08.615478 22579586809984 run.py:483] Algo bellman_ford step 6534 current loss 0.080095, current_train_items 209120.
I0304 19:31:08.635380 22579586809984 run.py:483] Algo bellman_ford step 6535 current loss 0.004859, current_train_items 209152.
I0304 19:31:08.651723 22579586809984 run.py:483] Algo bellman_ford step 6536 current loss 0.040512, current_train_items 209184.
I0304 19:31:08.675096 22579586809984 run.py:483] Algo bellman_ford step 6537 current loss 0.023556, current_train_items 209216.
I0304 19:31:08.705365 22579586809984 run.py:483] Algo bellman_ford step 6538 current loss 0.038777, current_train_items 209248.
I0304 19:31:08.738890 22579586809984 run.py:483] Algo bellman_ford step 6539 current loss 0.034671, current_train_items 209280.
I0304 19:31:08.758447 22579586809984 run.py:483] Algo bellman_ford step 6540 current loss 0.003846, current_train_items 209312.
I0304 19:31:08.774636 22579586809984 run.py:483] Algo bellman_ford step 6541 current loss 0.023354, current_train_items 209344.
I0304 19:31:08.798214 22579586809984 run.py:483] Algo bellman_ford step 6542 current loss 0.048639, current_train_items 209376.
I0304 19:31:08.829808 22579586809984 run.py:483] Algo bellman_ford step 6543 current loss 0.048833, current_train_items 209408.
I0304 19:31:08.865023 22579586809984 run.py:483] Algo bellman_ford step 6544 current loss 0.122513, current_train_items 209440.
I0304 19:31:08.885066 22579586809984 run.py:483] Algo bellman_ford step 6545 current loss 0.003848, current_train_items 209472.
I0304 19:31:08.901054 22579586809984 run.py:483] Algo bellman_ford step 6546 current loss 0.030397, current_train_items 209504.
I0304 19:31:08.924256 22579586809984 run.py:483] Algo bellman_ford step 6547 current loss 0.032786, current_train_items 209536.
I0304 19:31:08.956753 22579586809984 run.py:483] Algo bellman_ford step 6548 current loss 0.076073, current_train_items 209568.
I0304 19:31:08.991261 22579586809984 run.py:483] Algo bellman_ford step 6549 current loss 0.081936, current_train_items 209600.
I0304 19:31:09.010785 22579586809984 run.py:483] Algo bellman_ford step 6550 current loss 0.017441, current_train_items 209632.
I0304 19:31:09.018843 22579586809984 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0304 19:31:09.018959 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:09.036192 22579586809984 run.py:483] Algo bellman_ford step 6551 current loss 0.006731, current_train_items 209664.
I0304 19:31:09.060675 22579586809984 run.py:483] Algo bellman_ford step 6552 current loss 0.035209, current_train_items 209696.
I0304 19:31:09.093784 22579586809984 run.py:483] Algo bellman_ford step 6553 current loss 0.072434, current_train_items 209728.
I0304 19:31:09.129445 22579586809984 run.py:483] Algo bellman_ford step 6554 current loss 0.135573, current_train_items 209760.
I0304 19:31:09.149649 22579586809984 run.py:483] Algo bellman_ford step 6555 current loss 0.036394, current_train_items 209792.
I0304 19:31:09.165540 22579586809984 run.py:483] Algo bellman_ford step 6556 current loss 0.043135, current_train_items 209824.
I0304 19:31:09.190078 22579586809984 run.py:483] Algo bellman_ford step 6557 current loss 0.047462, current_train_items 209856.
I0304 19:31:09.221882 22579586809984 run.py:483] Algo bellman_ford step 6558 current loss 0.066750, current_train_items 209888.
I0304 19:31:09.256275 22579586809984 run.py:483] Algo bellman_ford step 6559 current loss 0.157659, current_train_items 209920.
I0304 19:31:09.276627 22579586809984 run.py:483] Algo bellman_ford step 6560 current loss 0.036768, current_train_items 209952.
I0304 19:31:09.292792 22579586809984 run.py:483] Algo bellman_ford step 6561 current loss 0.012635, current_train_items 209984.
I0304 19:31:09.316014 22579586809984 run.py:483] Algo bellman_ford step 6562 current loss 0.072046, current_train_items 210016.
I0304 19:31:09.347116 22579586809984 run.py:483] Algo bellman_ford step 6563 current loss 0.103910, current_train_items 210048.
I0304 19:31:09.378507 22579586809984 run.py:483] Algo bellman_ford step 6564 current loss 0.078840, current_train_items 210080.
I0304 19:31:09.398209 22579586809984 run.py:483] Algo bellman_ford step 6565 current loss 0.005124, current_train_items 210112.
I0304 19:31:09.414151 22579586809984 run.py:483] Algo bellman_ford step 6566 current loss 0.007940, current_train_items 210144.
I0304 19:31:09.438230 22579586809984 run.py:483] Algo bellman_ford step 6567 current loss 0.051657, current_train_items 210176.
I0304 19:31:09.467998 22579586809984 run.py:483] Algo bellman_ford step 6568 current loss 0.069565, current_train_items 210208.
I0304 19:31:09.501724 22579586809984 run.py:483] Algo bellman_ford step 6569 current loss 0.100188, current_train_items 210240.
I0304 19:31:09.521909 22579586809984 run.py:483] Algo bellman_ford step 6570 current loss 0.007608, current_train_items 210272.
I0304 19:31:09.538305 22579586809984 run.py:483] Algo bellman_ford step 6571 current loss 0.023581, current_train_items 210304.
I0304 19:31:09.561887 22579586809984 run.py:483] Algo bellman_ford step 6572 current loss 0.087643, current_train_items 210336.
I0304 19:31:09.592838 22579586809984 run.py:483] Algo bellman_ford step 6573 current loss 0.042774, current_train_items 210368.
I0304 19:31:09.626021 22579586809984 run.py:483] Algo bellman_ford step 6574 current loss 0.100984, current_train_items 210400.
I0304 19:31:09.646061 22579586809984 run.py:483] Algo bellman_ford step 6575 current loss 0.036183, current_train_items 210432.
I0304 19:31:09.662547 22579586809984 run.py:483] Algo bellman_ford step 6576 current loss 0.025455, current_train_items 210464.
I0304 19:31:09.685831 22579586809984 run.py:483] Algo bellman_ford step 6577 current loss 0.038177, current_train_items 210496.
I0304 19:31:09.717020 22579586809984 run.py:483] Algo bellman_ford step 6578 current loss 0.058521, current_train_items 210528.
I0304 19:31:09.750775 22579586809984 run.py:483] Algo bellman_ford step 6579 current loss 0.053110, current_train_items 210560.
I0304 19:31:09.770833 22579586809984 run.py:483] Algo bellman_ford step 6580 current loss 0.015549, current_train_items 210592.
I0304 19:31:09.787035 22579586809984 run.py:483] Algo bellman_ford step 6581 current loss 0.027460, current_train_items 210624.
I0304 19:31:09.810975 22579586809984 run.py:483] Algo bellman_ford step 6582 current loss 0.071147, current_train_items 210656.
I0304 19:31:09.843569 22579586809984 run.py:483] Algo bellman_ford step 6583 current loss 0.056785, current_train_items 210688.
I0304 19:31:09.876403 22579586809984 run.py:483] Algo bellman_ford step 6584 current loss 0.060274, current_train_items 210720.
I0304 19:31:09.896329 22579586809984 run.py:483] Algo bellman_ford step 6585 current loss 0.030367, current_train_items 210752.
I0304 19:31:09.912188 22579586809984 run.py:483] Algo bellman_ford step 6586 current loss 0.027274, current_train_items 210784.
I0304 19:31:09.934703 22579586809984 run.py:483] Algo bellman_ford step 6587 current loss 0.030592, current_train_items 210816.
I0304 19:31:09.965393 22579586809984 run.py:483] Algo bellman_ford step 6588 current loss 0.054782, current_train_items 210848.
I0304 19:31:09.997607 22579586809984 run.py:483] Algo bellman_ford step 6589 current loss 0.091717, current_train_items 210880.
I0304 19:31:10.017533 22579586809984 run.py:483] Algo bellman_ford step 6590 current loss 0.012755, current_train_items 210912.
I0304 19:31:10.033647 22579586809984 run.py:483] Algo bellman_ford step 6591 current loss 0.011266, current_train_items 210944.
I0304 19:31:10.057651 22579586809984 run.py:483] Algo bellman_ford step 6592 current loss 0.035571, current_train_items 210976.
I0304 19:31:10.088514 22579586809984 run.py:483] Algo bellman_ford step 6593 current loss 0.061600, current_train_items 211008.
I0304 19:31:10.121340 22579586809984 run.py:483] Algo bellman_ford step 6594 current loss 0.059970, current_train_items 211040.
I0304 19:31:10.141193 22579586809984 run.py:483] Algo bellman_ford step 6595 current loss 0.006468, current_train_items 211072.
I0304 19:31:10.157248 22579586809984 run.py:483] Algo bellman_ford step 6596 current loss 0.013202, current_train_items 211104.
I0304 19:31:10.181900 22579586809984 run.py:483] Algo bellman_ford step 6597 current loss 0.064104, current_train_items 211136.
I0304 19:31:10.213611 22579586809984 run.py:483] Algo bellman_ford step 6598 current loss 0.049700, current_train_items 211168.
I0304 19:31:10.244239 22579586809984 run.py:483] Algo bellman_ford step 6599 current loss 0.068341, current_train_items 211200.
I0304 19:31:10.264161 22579586809984 run.py:483] Algo bellman_ford step 6600 current loss 0.005729, current_train_items 211232.
I0304 19:31:10.271846 22579586809984 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0304 19:31:10.271952 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:31:10.288790 22579586809984 run.py:483] Algo bellman_ford step 6601 current loss 0.010758, current_train_items 211264.
I0304 19:31:10.311845 22579586809984 run.py:483] Algo bellman_ford step 6602 current loss 0.040626, current_train_items 211296.
I0304 19:31:10.343380 22579586809984 run.py:483] Algo bellman_ford step 6603 current loss 0.052421, current_train_items 211328.
I0304 19:31:10.377470 22579586809984 run.py:483] Algo bellman_ford step 6604 current loss 0.097652, current_train_items 211360.
I0304 19:31:10.397255 22579586809984 run.py:483] Algo bellman_ford step 6605 current loss 0.004059, current_train_items 211392.
I0304 19:31:10.413069 22579586809984 run.py:483] Algo bellman_ford step 6606 current loss 0.011487, current_train_items 211424.
I0304 19:31:10.436925 22579586809984 run.py:483] Algo bellman_ford step 6607 current loss 0.046779, current_train_items 211456.
I0304 19:31:10.468510 22579586809984 run.py:483] Algo bellman_ford step 6608 current loss 0.044829, current_train_items 211488.
I0304 19:31:10.500133 22579586809984 run.py:483] Algo bellman_ford step 6609 current loss 0.040824, current_train_items 211520.
I0304 19:31:10.519825 22579586809984 run.py:483] Algo bellman_ford step 6610 current loss 0.002382, current_train_items 211552.
I0304 19:31:10.535961 22579586809984 run.py:483] Algo bellman_ford step 6611 current loss 0.011478, current_train_items 211584.
I0304 19:31:10.558779 22579586809984 run.py:483] Algo bellman_ford step 6612 current loss 0.036188, current_train_items 211616.
I0304 19:31:10.590112 22579586809984 run.py:483] Algo bellman_ford step 6613 current loss 0.065234, current_train_items 211648.
I0304 19:31:10.625817 22579586809984 run.py:483] Algo bellman_ford step 6614 current loss 0.060911, current_train_items 211680.
I0304 19:31:10.645371 22579586809984 run.py:483] Algo bellman_ford step 6615 current loss 0.003975, current_train_items 211712.
I0304 19:31:10.661415 22579586809984 run.py:483] Algo bellman_ford step 6616 current loss 0.020117, current_train_items 211744.
I0304 19:31:10.685402 22579586809984 run.py:483] Algo bellman_ford step 6617 current loss 0.040560, current_train_items 211776.
I0304 19:31:10.716536 22579586809984 run.py:483] Algo bellman_ford step 6618 current loss 0.051008, current_train_items 211808.
I0304 19:31:10.750468 22579586809984 run.py:483] Algo bellman_ford step 6619 current loss 0.055313, current_train_items 211840.
I0304 19:31:10.769969 22579586809984 run.py:483] Algo bellman_ford step 6620 current loss 0.002651, current_train_items 211872.
I0304 19:31:10.785799 22579586809984 run.py:483] Algo bellman_ford step 6621 current loss 0.014629, current_train_items 211904.
I0304 19:31:10.808275 22579586809984 run.py:483] Algo bellman_ford step 6622 current loss 0.069333, current_train_items 211936.
I0304 19:31:10.840261 22579586809984 run.py:483] Algo bellman_ford step 6623 current loss 0.071444, current_train_items 211968.
I0304 19:31:10.872447 22579586809984 run.py:483] Algo bellman_ford step 6624 current loss 0.064661, current_train_items 212000.
I0304 19:31:10.891945 22579586809984 run.py:483] Algo bellman_ford step 6625 current loss 0.004252, current_train_items 212032.
I0304 19:31:10.908235 22579586809984 run.py:483] Algo bellman_ford step 6626 current loss 0.004925, current_train_items 212064.
I0304 19:31:10.932644 22579586809984 run.py:483] Algo bellman_ford step 6627 current loss 0.052645, current_train_items 212096.
I0304 19:31:10.962586 22579586809984 run.py:483] Algo bellman_ford step 6628 current loss 0.062548, current_train_items 212128.
I0304 19:31:10.994446 22579586809984 run.py:483] Algo bellman_ford step 6629 current loss 0.037901, current_train_items 212160.
I0304 19:31:11.013771 22579586809984 run.py:483] Algo bellman_ford step 6630 current loss 0.004263, current_train_items 212192.
I0304 19:31:11.029983 22579586809984 run.py:483] Algo bellman_ford step 6631 current loss 0.078032, current_train_items 212224.
I0304 19:31:11.054832 22579586809984 run.py:483] Algo bellman_ford step 6632 current loss 0.033333, current_train_items 212256.
I0304 19:31:11.086520 22579586809984 run.py:483] Algo bellman_ford step 6633 current loss 0.059398, current_train_items 212288.
I0304 19:31:11.118624 22579586809984 run.py:483] Algo bellman_ford step 6634 current loss 0.048157, current_train_items 212320.
I0304 19:31:11.138375 22579586809984 run.py:483] Algo bellman_ford step 6635 current loss 0.003001, current_train_items 212352.
I0304 19:31:11.154863 22579586809984 run.py:483] Algo bellman_ford step 6636 current loss 0.030769, current_train_items 212384.
I0304 19:31:11.178615 22579586809984 run.py:483] Algo bellman_ford step 6637 current loss 0.023490, current_train_items 212416.
I0304 19:31:11.209826 22579586809984 run.py:483] Algo bellman_ford step 6638 current loss 0.045218, current_train_items 212448.
I0304 19:31:11.243884 22579586809984 run.py:483] Algo bellman_ford step 6639 current loss 0.060429, current_train_items 212480.
I0304 19:31:11.263281 22579586809984 run.py:483] Algo bellman_ford step 6640 current loss 0.002299, current_train_items 212512.
I0304 19:31:11.279579 22579586809984 run.py:483] Algo bellman_ford step 6641 current loss 0.010110, current_train_items 212544.
I0304 19:31:11.304399 22579586809984 run.py:483] Algo bellman_ford step 6642 current loss 0.056043, current_train_items 212576.
I0304 19:31:11.335222 22579586809984 run.py:483] Algo bellman_ford step 6643 current loss 0.073099, current_train_items 212608.
I0304 19:31:11.369471 22579586809984 run.py:483] Algo bellman_ford step 6644 current loss 0.053983, current_train_items 212640.
I0304 19:31:11.388712 22579586809984 run.py:483] Algo bellman_ford step 6645 current loss 0.002580, current_train_items 212672.
I0304 19:31:11.404804 22579586809984 run.py:483] Algo bellman_ford step 6646 current loss 0.008878, current_train_items 212704.
I0304 19:31:11.428303 22579586809984 run.py:483] Algo bellman_ford step 6647 current loss 0.051171, current_train_items 212736.
I0304 19:31:11.459517 22579586809984 run.py:483] Algo bellman_ford step 6648 current loss 0.147438, current_train_items 212768.
I0304 19:31:11.493560 22579586809984 run.py:483] Algo bellman_ford step 6649 current loss 0.144900, current_train_items 212800.
I0304 19:31:11.513253 22579586809984 run.py:483] Algo bellman_ford step 6650 current loss 0.002776, current_train_items 212832.
I0304 19:31:11.521553 22579586809984 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0304 19:31:11.521658 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:11.539002 22579586809984 run.py:483] Algo bellman_ford step 6651 current loss 0.019187, current_train_items 212864.
I0304 19:31:11.563932 22579586809984 run.py:483] Algo bellman_ford step 6652 current loss 0.050600, current_train_items 212896.
I0304 19:31:11.596581 22579586809984 run.py:483] Algo bellman_ford step 6653 current loss 0.049214, current_train_items 212928.
I0304 19:31:11.626413 22579586809984 run.py:483] Algo bellman_ford step 6654 current loss 0.034370, current_train_items 212960.
I0304 19:31:11.646309 22579586809984 run.py:483] Algo bellman_ford step 6655 current loss 0.002443, current_train_items 212992.
I0304 19:31:11.662200 22579586809984 run.py:483] Algo bellman_ford step 6656 current loss 0.021150, current_train_items 213024.
I0304 19:31:11.686086 22579586809984 run.py:483] Algo bellman_ford step 6657 current loss 0.038666, current_train_items 213056.
I0304 19:31:11.717931 22579586809984 run.py:483] Algo bellman_ford step 6658 current loss 0.053403, current_train_items 213088.
I0304 19:31:11.753123 22579586809984 run.py:483] Algo bellman_ford step 6659 current loss 0.100522, current_train_items 213120.
I0304 19:31:11.773394 22579586809984 run.py:483] Algo bellman_ford step 6660 current loss 0.006305, current_train_items 213152.
I0304 19:31:11.790040 22579586809984 run.py:483] Algo bellman_ford step 6661 current loss 0.018531, current_train_items 213184.
I0304 19:31:11.815008 22579586809984 run.py:483] Algo bellman_ford step 6662 current loss 0.085260, current_train_items 213216.
I0304 19:31:11.845247 22579586809984 run.py:483] Algo bellman_ford step 6663 current loss 0.036261, current_train_items 213248.
I0304 19:31:11.878591 22579586809984 run.py:483] Algo bellman_ford step 6664 current loss 0.045099, current_train_items 213280.
I0304 19:31:11.898667 22579586809984 run.py:483] Algo bellman_ford step 6665 current loss 0.004618, current_train_items 213312.
I0304 19:31:11.914917 22579586809984 run.py:483] Algo bellman_ford step 6666 current loss 0.010084, current_train_items 213344.
I0304 19:31:11.940845 22579586809984 run.py:483] Algo bellman_ford step 6667 current loss 0.048589, current_train_items 213376.
I0304 19:31:11.973483 22579586809984 run.py:483] Algo bellman_ford step 6668 current loss 0.036773, current_train_items 213408.
I0304 19:31:12.008800 22579586809984 run.py:483] Algo bellman_ford step 6669 current loss 0.054184, current_train_items 213440.
I0304 19:31:12.029251 22579586809984 run.py:483] Algo bellman_ford step 6670 current loss 0.003812, current_train_items 213472.
I0304 19:31:12.045562 22579586809984 run.py:483] Algo bellman_ford step 6671 current loss 0.021422, current_train_items 213504.
I0304 19:31:12.069538 22579586809984 run.py:483] Algo bellman_ford step 6672 current loss 0.041907, current_train_items 213536.
I0304 19:31:12.101308 22579586809984 run.py:483] Algo bellman_ford step 6673 current loss 0.039028, current_train_items 213568.
I0304 19:31:12.133776 22579586809984 run.py:483] Algo bellman_ford step 6674 current loss 0.051689, current_train_items 213600.
I0304 19:31:12.153880 22579586809984 run.py:483] Algo bellman_ford step 6675 current loss 0.010387, current_train_items 213632.
I0304 19:31:12.169691 22579586809984 run.py:483] Algo bellman_ford step 6676 current loss 0.007769, current_train_items 213664.
I0304 19:31:12.194262 22579586809984 run.py:483] Algo bellman_ford step 6677 current loss 0.032456, current_train_items 213696.
I0304 19:31:12.226310 22579586809984 run.py:483] Algo bellman_ford step 6678 current loss 0.033629, current_train_items 213728.
I0304 19:31:12.262241 22579586809984 run.py:483] Algo bellman_ford step 6679 current loss 0.055028, current_train_items 213760.
I0304 19:31:12.282398 22579586809984 run.py:483] Algo bellman_ford step 6680 current loss 0.002596, current_train_items 213792.
I0304 19:31:12.298791 22579586809984 run.py:483] Algo bellman_ford step 6681 current loss 0.021666, current_train_items 213824.
I0304 19:31:12.323629 22579586809984 run.py:483] Algo bellman_ford step 6682 current loss 0.038689, current_train_items 213856.
I0304 19:31:12.354706 22579586809984 run.py:483] Algo bellman_ford step 6683 current loss 0.092376, current_train_items 213888.
I0304 19:31:12.391321 22579586809984 run.py:483] Algo bellman_ford step 6684 current loss 0.094466, current_train_items 213920.
I0304 19:31:12.411296 22579586809984 run.py:483] Algo bellman_ford step 6685 current loss 0.002981, current_train_items 213952.
I0304 19:31:12.427403 22579586809984 run.py:483] Algo bellman_ford step 6686 current loss 0.040886, current_train_items 213984.
I0304 19:31:12.451410 22579586809984 run.py:483] Algo bellman_ford step 6687 current loss 0.054668, current_train_items 214016.
I0304 19:31:12.483480 22579586809984 run.py:483] Algo bellman_ford step 6688 current loss 0.035595, current_train_items 214048.
I0304 19:31:12.516870 22579586809984 run.py:483] Algo bellman_ford step 6689 current loss 0.036505, current_train_items 214080.
I0304 19:31:12.537128 22579586809984 run.py:483] Algo bellman_ford step 6690 current loss 0.003253, current_train_items 214112.
I0304 19:31:12.553160 22579586809984 run.py:483] Algo bellman_ford step 6691 current loss 0.003869, current_train_items 214144.
I0304 19:31:12.577589 22579586809984 run.py:483] Algo bellman_ford step 6692 current loss 0.052959, current_train_items 214176.
I0304 19:31:12.609374 22579586809984 run.py:483] Algo bellman_ford step 6693 current loss 0.093507, current_train_items 214208.
I0304 19:31:12.644020 22579586809984 run.py:483] Algo bellman_ford step 6694 current loss 0.039357, current_train_items 214240.
I0304 19:31:12.664021 22579586809984 run.py:483] Algo bellman_ford step 6695 current loss 0.002564, current_train_items 214272.
I0304 19:31:12.680448 22579586809984 run.py:483] Algo bellman_ford step 6696 current loss 0.011928, current_train_items 214304.
I0304 19:31:12.704710 22579586809984 run.py:483] Algo bellman_ford step 6697 current loss 0.038896, current_train_items 214336.
I0304 19:31:12.736818 22579586809984 run.py:483] Algo bellman_ford step 6698 current loss 0.074253, current_train_items 214368.
I0304 19:31:12.771664 22579586809984 run.py:483] Algo bellman_ford step 6699 current loss 0.072572, current_train_items 214400.
I0304 19:31:12.791620 22579586809984 run.py:483] Algo bellman_ford step 6700 current loss 0.017496, current_train_items 214432.
I0304 19:31:12.799618 22579586809984 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0304 19:31:12.799733 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:12.817018 22579586809984 run.py:483] Algo bellman_ford step 6701 current loss 0.038868, current_train_items 214464.
I0304 19:31:12.841268 22579586809984 run.py:483] Algo bellman_ford step 6702 current loss 0.097839, current_train_items 214496.
I0304 19:31:12.873131 22579586809984 run.py:483] Algo bellman_ford step 6703 current loss 0.126767, current_train_items 214528.
I0304 19:31:12.906443 22579586809984 run.py:483] Algo bellman_ford step 6704 current loss 0.091650, current_train_items 214560.
I0304 19:31:12.926528 22579586809984 run.py:483] Algo bellman_ford step 6705 current loss 0.019594, current_train_items 214592.
I0304 19:31:12.942569 22579586809984 run.py:483] Algo bellman_ford step 6706 current loss 0.018105, current_train_items 214624.
I0304 19:31:12.967836 22579586809984 run.py:483] Algo bellman_ford step 6707 current loss 0.053484, current_train_items 214656.
I0304 19:31:12.999187 22579586809984 run.py:483] Algo bellman_ford step 6708 current loss 0.058867, current_train_items 214688.
I0304 19:31:13.033769 22579586809984 run.py:483] Algo bellman_ford step 6709 current loss 0.096679, current_train_items 214720.
I0304 19:31:13.053540 22579586809984 run.py:483] Algo bellman_ford step 6710 current loss 0.003541, current_train_items 214752.
I0304 19:31:13.069934 22579586809984 run.py:483] Algo bellman_ford step 6711 current loss 0.022423, current_train_items 214784.
I0304 19:31:13.093283 22579586809984 run.py:483] Algo bellman_ford step 6712 current loss 0.059352, current_train_items 214816.
I0304 19:31:13.125823 22579586809984 run.py:483] Algo bellman_ford step 6713 current loss 0.096927, current_train_items 214848.
I0304 19:31:13.159301 22579586809984 run.py:483] Algo bellman_ford step 6714 current loss 0.065813, current_train_items 214880.
I0304 19:31:13.179077 22579586809984 run.py:483] Algo bellman_ford step 6715 current loss 0.016258, current_train_items 214912.
I0304 19:31:13.195309 22579586809984 run.py:483] Algo bellman_ford step 6716 current loss 0.010282, current_train_items 214944.
I0304 19:31:13.219135 22579586809984 run.py:483] Algo bellman_ford step 6717 current loss 0.040863, current_train_items 214976.
I0304 19:31:13.250527 22579586809984 run.py:483] Algo bellman_ford step 6718 current loss 0.053632, current_train_items 215008.
I0304 19:31:13.286244 22579586809984 run.py:483] Algo bellman_ford step 6719 current loss 0.084816, current_train_items 215040.
I0304 19:31:13.305742 22579586809984 run.py:483] Algo bellman_ford step 6720 current loss 0.003275, current_train_items 215072.
I0304 19:31:13.321911 22579586809984 run.py:483] Algo bellman_ford step 6721 current loss 0.006794, current_train_items 215104.
I0304 19:31:13.345309 22579586809984 run.py:483] Algo bellman_ford step 6722 current loss 0.029013, current_train_items 215136.
I0304 19:31:13.377466 22579586809984 run.py:483] Algo bellman_ford step 6723 current loss 0.049084, current_train_items 215168.
I0304 19:31:13.409768 22579586809984 run.py:483] Algo bellman_ford step 6724 current loss 0.091581, current_train_items 215200.
I0304 19:31:13.429186 22579586809984 run.py:483] Algo bellman_ford step 6725 current loss 0.003115, current_train_items 215232.
I0304 19:31:13.445416 22579586809984 run.py:483] Algo bellman_ford step 6726 current loss 0.026756, current_train_items 215264.
I0304 19:31:13.468794 22579586809984 run.py:483] Algo bellman_ford step 6727 current loss 0.023912, current_train_items 215296.
I0304 19:31:13.499662 22579586809984 run.py:483] Algo bellman_ford step 6728 current loss 0.033832, current_train_items 215328.
I0304 19:31:13.531875 22579586809984 run.py:483] Algo bellman_ford step 6729 current loss 0.087599, current_train_items 215360.
I0304 19:31:13.551388 22579586809984 run.py:483] Algo bellman_ford step 6730 current loss 0.006866, current_train_items 215392.
I0304 19:31:13.567324 22579586809984 run.py:483] Algo bellman_ford step 6731 current loss 0.006066, current_train_items 215424.
I0304 19:31:13.591662 22579586809984 run.py:483] Algo bellman_ford step 6732 current loss 0.079449, current_train_items 215456.
I0304 19:31:13.621421 22579586809984 run.py:483] Algo bellman_ford step 6733 current loss 0.042704, current_train_items 215488.
I0304 19:31:13.652857 22579586809984 run.py:483] Algo bellman_ford step 6734 current loss 0.068081, current_train_items 215520.
I0304 19:31:13.672385 22579586809984 run.py:483] Algo bellman_ford step 6735 current loss 0.005758, current_train_items 215552.
I0304 19:31:13.688802 22579586809984 run.py:483] Algo bellman_ford step 6736 current loss 0.014669, current_train_items 215584.
I0304 19:31:13.713366 22579586809984 run.py:483] Algo bellman_ford step 6737 current loss 0.051568, current_train_items 215616.
I0304 19:31:13.743365 22579586809984 run.py:483] Algo bellman_ford step 6738 current loss 0.045392, current_train_items 215648.
I0304 19:31:13.778459 22579586809984 run.py:483] Algo bellman_ford step 6739 current loss 0.081905, current_train_items 215680.
I0304 19:31:13.797841 22579586809984 run.py:483] Algo bellman_ford step 6740 current loss 0.021178, current_train_items 215712.
I0304 19:31:13.814239 22579586809984 run.py:483] Algo bellman_ford step 6741 current loss 0.011406, current_train_items 215744.
I0304 19:31:13.838723 22579586809984 run.py:483] Algo bellman_ford step 6742 current loss 0.072158, current_train_items 215776.
I0304 19:31:13.868908 22579586809984 run.py:483] Algo bellman_ford step 6743 current loss 0.033715, current_train_items 215808.
I0304 19:31:13.903351 22579586809984 run.py:483] Algo bellman_ford step 6744 current loss 0.090837, current_train_items 215840.
I0304 19:31:13.923056 22579586809984 run.py:483] Algo bellman_ford step 6745 current loss 0.005245, current_train_items 215872.
I0304 19:31:13.939332 22579586809984 run.py:483] Algo bellman_ford step 6746 current loss 0.040353, current_train_items 215904.
I0304 19:31:13.963188 22579586809984 run.py:483] Algo bellman_ford step 6747 current loss 0.022196, current_train_items 215936.
I0304 19:31:13.994099 22579586809984 run.py:483] Algo bellman_ford step 6748 current loss 0.058606, current_train_items 215968.
I0304 19:31:14.027790 22579586809984 run.py:483] Algo bellman_ford step 6749 current loss 0.072462, current_train_items 216000.
I0304 19:31:14.047065 22579586809984 run.py:483] Algo bellman_ford step 6750 current loss 0.005531, current_train_items 216032.
I0304 19:31:14.055025 22579586809984 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0304 19:31:14.055131 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:14.072051 22579586809984 run.py:483] Algo bellman_ford step 6751 current loss 0.013998, current_train_items 216064.
I0304 19:31:14.096787 22579586809984 run.py:483] Algo bellman_ford step 6752 current loss 0.040868, current_train_items 216096.
I0304 19:31:14.129237 22579586809984 run.py:483] Algo bellman_ford step 6753 current loss 0.054408, current_train_items 216128.
I0304 19:31:14.163923 22579586809984 run.py:483] Algo bellman_ford step 6754 current loss 0.052957, current_train_items 216160.
I0304 19:31:14.183754 22579586809984 run.py:483] Algo bellman_ford step 6755 current loss 0.022497, current_train_items 216192.
I0304 19:31:14.200273 22579586809984 run.py:483] Algo bellman_ford step 6756 current loss 0.035372, current_train_items 216224.
I0304 19:31:14.223758 22579586809984 run.py:483] Algo bellman_ford step 6757 current loss 0.019934, current_train_items 216256.
I0304 19:31:14.255345 22579586809984 run.py:483] Algo bellman_ford step 6758 current loss 0.078196, current_train_items 216288.
I0304 19:31:14.290470 22579586809984 run.py:483] Algo bellman_ford step 6759 current loss 0.096225, current_train_items 216320.
I0304 19:31:14.310451 22579586809984 run.py:483] Algo bellman_ford step 6760 current loss 0.013232, current_train_items 216352.
I0304 19:31:14.327055 22579586809984 run.py:483] Algo bellman_ford step 6761 current loss 0.023968, current_train_items 216384.
I0304 19:31:14.350760 22579586809984 run.py:483] Algo bellman_ford step 6762 current loss 0.013950, current_train_items 216416.
I0304 19:31:14.382286 22579586809984 run.py:483] Algo bellman_ford step 6763 current loss 0.048154, current_train_items 216448.
I0304 19:31:14.415110 22579586809984 run.py:483] Algo bellman_ford step 6764 current loss 0.058845, current_train_items 216480.
I0304 19:31:14.434630 22579586809984 run.py:483] Algo bellman_ford step 6765 current loss 0.009440, current_train_items 216512.
I0304 19:31:14.451186 22579586809984 run.py:483] Algo bellman_ford step 6766 current loss 0.041379, current_train_items 216544.
I0304 19:31:14.475174 22579586809984 run.py:483] Algo bellman_ford step 6767 current loss 0.021128, current_train_items 216576.
I0304 19:31:14.505578 22579586809984 run.py:483] Algo bellman_ford step 6768 current loss 0.021937, current_train_items 216608.
I0304 19:31:14.538819 22579586809984 run.py:483] Algo bellman_ford step 6769 current loss 0.090284, current_train_items 216640.
I0304 19:31:14.558672 22579586809984 run.py:483] Algo bellman_ford step 6770 current loss 0.002710, current_train_items 216672.
I0304 19:31:14.574854 22579586809984 run.py:483] Algo bellman_ford step 6771 current loss 0.008862, current_train_items 216704.
I0304 19:31:14.597451 22579586809984 run.py:483] Algo bellman_ford step 6772 current loss 0.016834, current_train_items 216736.
I0304 19:31:14.628885 22579586809984 run.py:483] Algo bellman_ford step 6773 current loss 0.062566, current_train_items 216768.
I0304 19:31:14.660870 22579586809984 run.py:483] Algo bellman_ford step 6774 current loss 0.048169, current_train_items 216800.
I0304 19:31:14.680640 22579586809984 run.py:483] Algo bellman_ford step 6775 current loss 0.005816, current_train_items 216832.
I0304 19:31:14.697174 22579586809984 run.py:483] Algo bellman_ford step 6776 current loss 0.024207, current_train_items 216864.
I0304 19:31:14.720323 22579586809984 run.py:483] Algo bellman_ford step 6777 current loss 0.028686, current_train_items 216896.
I0304 19:31:14.752853 22579586809984 run.py:483] Algo bellman_ford step 6778 current loss 0.050305, current_train_items 216928.
I0304 19:31:14.785936 22579586809984 run.py:483] Algo bellman_ford step 6779 current loss 0.029597, current_train_items 216960.
I0304 19:31:14.805367 22579586809984 run.py:483] Algo bellman_ford step 6780 current loss 0.003663, current_train_items 216992.
I0304 19:31:14.821418 22579586809984 run.py:483] Algo bellman_ford step 6781 current loss 0.020570, current_train_items 217024.
I0304 19:31:14.844497 22579586809984 run.py:483] Algo bellman_ford step 6782 current loss 0.038547, current_train_items 217056.
I0304 19:31:14.876773 22579586809984 run.py:483] Algo bellman_ford step 6783 current loss 0.029813, current_train_items 217088.
I0304 19:31:14.909580 22579586809984 run.py:483] Algo bellman_ford step 6784 current loss 0.075901, current_train_items 217120.
I0304 19:31:14.929366 22579586809984 run.py:483] Algo bellman_ford step 6785 current loss 0.005398, current_train_items 217152.
I0304 19:31:14.945412 22579586809984 run.py:483] Algo bellman_ford step 6786 current loss 0.009921, current_train_items 217184.
I0304 19:31:14.968564 22579586809984 run.py:483] Algo bellman_ford step 6787 current loss 0.049536, current_train_items 217216.
I0304 19:31:14.998557 22579586809984 run.py:483] Algo bellman_ford step 6788 current loss 0.030379, current_train_items 217248.
I0304 19:31:15.032779 22579586809984 run.py:483] Algo bellman_ford step 6789 current loss 0.072854, current_train_items 217280.
I0304 19:31:15.052702 22579586809984 run.py:483] Algo bellman_ford step 6790 current loss 0.006090, current_train_items 217312.
I0304 19:31:15.068891 22579586809984 run.py:483] Algo bellman_ford step 6791 current loss 0.009217, current_train_items 217344.
I0304 19:31:15.092712 22579586809984 run.py:483] Algo bellman_ford step 6792 current loss 0.012174, current_train_items 217376.
I0304 19:31:15.123510 22579586809984 run.py:483] Algo bellman_ford step 6793 current loss 0.042465, current_train_items 217408.
I0304 19:31:15.156752 22579586809984 run.py:483] Algo bellman_ford step 6794 current loss 0.061678, current_train_items 217440.
I0304 19:31:15.176559 22579586809984 run.py:483] Algo bellman_ford step 6795 current loss 0.005063, current_train_items 217472.
I0304 19:31:15.192701 22579586809984 run.py:483] Algo bellman_ford step 6796 current loss 0.016662, current_train_items 217504.
I0304 19:31:15.217316 22579586809984 run.py:483] Algo bellman_ford step 6797 current loss 0.025630, current_train_items 217536.
I0304 19:31:15.247634 22579586809984 run.py:483] Algo bellman_ford step 6798 current loss 0.034604, current_train_items 217568.
I0304 19:31:15.280958 22579586809984 run.py:483] Algo bellman_ford step 6799 current loss 0.058974, current_train_items 217600.
I0304 19:31:15.300918 22579586809984 run.py:483] Algo bellman_ford step 6800 current loss 0.025387, current_train_items 217632.
I0304 19:31:15.308591 22579586809984 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0304 19:31:15.308705 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:31:15.325382 22579586809984 run.py:483] Algo bellman_ford step 6801 current loss 0.029084, current_train_items 217664.
I0304 19:31:15.349844 22579586809984 run.py:483] Algo bellman_ford step 6802 current loss 0.036467, current_train_items 217696.
I0304 19:31:15.382929 22579586809984 run.py:483] Algo bellman_ford step 6803 current loss 0.047132, current_train_items 217728.
I0304 19:31:15.417475 22579586809984 run.py:483] Algo bellman_ford step 6804 current loss 0.075064, current_train_items 217760.
I0304 19:31:15.437366 22579586809984 run.py:483] Algo bellman_ford step 6805 current loss 0.001931, current_train_items 217792.
I0304 19:31:15.453464 22579586809984 run.py:483] Algo bellman_ford step 6806 current loss 0.006620, current_train_items 217824.
I0304 19:31:15.477974 22579586809984 run.py:483] Algo bellman_ford step 6807 current loss 0.031574, current_train_items 217856.
I0304 19:31:15.508269 22579586809984 run.py:483] Algo bellman_ford step 6808 current loss 0.023027, current_train_items 217888.
I0304 19:31:15.541613 22579586809984 run.py:483] Algo bellman_ford step 6809 current loss 0.078282, current_train_items 217920.
I0304 19:31:15.561304 22579586809984 run.py:483] Algo bellman_ford step 6810 current loss 0.005137, current_train_items 217952.
I0304 19:31:15.577131 22579586809984 run.py:483] Algo bellman_ford step 6811 current loss 0.006813, current_train_items 217984.
I0304 19:31:15.602105 22579586809984 run.py:483] Algo bellman_ford step 6812 current loss 0.032663, current_train_items 218016.
I0304 19:31:15.632690 22579586809984 run.py:483] Algo bellman_ford step 6813 current loss 0.067945, current_train_items 218048.
I0304 19:31:15.664293 22579586809984 run.py:483] Algo bellman_ford step 6814 current loss 0.049010, current_train_items 218080.
I0304 19:31:15.683621 22579586809984 run.py:483] Algo bellman_ford step 6815 current loss 0.003989, current_train_items 218112.
I0304 19:31:15.699771 22579586809984 run.py:483] Algo bellman_ford step 6816 current loss 0.010276, current_train_items 218144.
I0304 19:31:15.722396 22579586809984 run.py:483] Algo bellman_ford step 6817 current loss 0.022574, current_train_items 218176.
I0304 19:31:15.753658 22579586809984 run.py:483] Algo bellman_ford step 6818 current loss 0.058163, current_train_items 218208.
I0304 19:31:15.788904 22579586809984 run.py:483] Algo bellman_ford step 6819 current loss 0.089493, current_train_items 218240.
I0304 19:31:15.808909 22579586809984 run.py:483] Algo bellman_ford step 6820 current loss 0.005463, current_train_items 218272.
I0304 19:31:15.825058 22579586809984 run.py:483] Algo bellman_ford step 6821 current loss 0.009345, current_train_items 218304.
I0304 19:31:15.848463 22579586809984 run.py:483] Algo bellman_ford step 6822 current loss 0.036009, current_train_items 218336.
I0304 19:31:15.879783 22579586809984 run.py:483] Algo bellman_ford step 6823 current loss 0.052999, current_train_items 218368.
I0304 19:31:15.914416 22579586809984 run.py:483] Algo bellman_ford step 6824 current loss 0.070050, current_train_items 218400.
I0304 19:31:15.934060 22579586809984 run.py:483] Algo bellman_ford step 6825 current loss 0.002930, current_train_items 218432.
I0304 19:31:15.950481 22579586809984 run.py:483] Algo bellman_ford step 6826 current loss 0.010980, current_train_items 218464.
I0304 19:31:15.974362 22579586809984 run.py:483] Algo bellman_ford step 6827 current loss 0.022592, current_train_items 218496.
I0304 19:31:16.005002 22579586809984 run.py:483] Algo bellman_ford step 6828 current loss 0.023512, current_train_items 218528.
I0304 19:31:16.040481 22579586809984 run.py:483] Algo bellman_ford step 6829 current loss 0.052552, current_train_items 218560.
I0304 19:31:16.060128 22579586809984 run.py:483] Algo bellman_ford step 6830 current loss 0.002288, current_train_items 218592.
I0304 19:31:16.076056 22579586809984 run.py:483] Algo bellman_ford step 6831 current loss 0.024710, current_train_items 218624.
I0304 19:31:16.101423 22579586809984 run.py:483] Algo bellman_ford step 6832 current loss 0.050675, current_train_items 218656.
I0304 19:31:16.132449 22579586809984 run.py:483] Algo bellman_ford step 6833 current loss 0.032339, current_train_items 218688.
I0304 19:31:16.165939 22579586809984 run.py:483] Algo bellman_ford step 6834 current loss 0.038046, current_train_items 218720.
I0304 19:31:16.185567 22579586809984 run.py:483] Algo bellman_ford step 6835 current loss 0.002622, current_train_items 218752.
I0304 19:31:16.201924 22579586809984 run.py:483] Algo bellman_ford step 6836 current loss 0.033397, current_train_items 218784.
I0304 19:31:16.225474 22579586809984 run.py:483] Algo bellman_ford step 6837 current loss 0.030114, current_train_items 218816.
I0304 19:31:16.257502 22579586809984 run.py:483] Algo bellman_ford step 6838 current loss 0.060872, current_train_items 218848.
I0304 19:31:16.289179 22579586809984 run.py:483] Algo bellman_ford step 6839 current loss 0.051776, current_train_items 218880.
I0304 19:31:16.308522 22579586809984 run.py:483] Algo bellman_ford step 6840 current loss 0.002264, current_train_items 218912.
I0304 19:31:16.325053 22579586809984 run.py:483] Algo bellman_ford step 6841 current loss 0.031393, current_train_items 218944.
I0304 19:31:16.349595 22579586809984 run.py:483] Algo bellman_ford step 6842 current loss 0.028557, current_train_items 218976.
I0304 19:31:16.379719 22579586809984 run.py:483] Algo bellman_ford step 6843 current loss 0.020437, current_train_items 219008.
I0304 19:31:16.412652 22579586809984 run.py:483] Algo bellman_ford step 6844 current loss 0.051265, current_train_items 219040.
I0304 19:31:16.432036 22579586809984 run.py:483] Algo bellman_ford step 6845 current loss 0.002630, current_train_items 219072.
I0304 19:31:16.448739 22579586809984 run.py:483] Algo bellman_ford step 6846 current loss 0.026583, current_train_items 219104.
I0304 19:31:16.473252 22579586809984 run.py:483] Algo bellman_ford step 6847 current loss 0.063772, current_train_items 219136.
I0304 19:31:16.505117 22579586809984 run.py:483] Algo bellman_ford step 6848 current loss 0.046123, current_train_items 219168.
I0304 19:31:16.539322 22579586809984 run.py:483] Algo bellman_ford step 6849 current loss 0.063063, current_train_items 219200.
I0304 19:31:16.558966 22579586809984 run.py:483] Algo bellman_ford step 6850 current loss 0.002612, current_train_items 219232.
I0304 19:31:16.566821 22579586809984 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0304 19:31:16.566928 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:31:16.583875 22579586809984 run.py:483] Algo bellman_ford step 6851 current loss 0.025110, current_train_items 219264.
I0304 19:31:16.608541 22579586809984 run.py:483] Algo bellman_ford step 6852 current loss 0.033627, current_train_items 219296.
I0304 19:31:16.640020 22579586809984 run.py:483] Algo bellman_ford step 6853 current loss 0.057690, current_train_items 219328.
I0304 19:31:16.674376 22579586809984 run.py:483] Algo bellman_ford step 6854 current loss 0.100380, current_train_items 219360.
I0304 19:31:16.694187 22579586809984 run.py:483] Algo bellman_ford step 6855 current loss 0.003440, current_train_items 219392.
I0304 19:31:16.709991 22579586809984 run.py:483] Algo bellman_ford step 6856 current loss 0.006791, current_train_items 219424.
I0304 19:31:16.733861 22579586809984 run.py:483] Algo bellman_ford step 6857 current loss 0.036709, current_train_items 219456.
I0304 19:31:16.764595 22579586809984 run.py:483] Algo bellman_ford step 6858 current loss 0.049884, current_train_items 219488.
I0304 19:31:16.798177 22579586809984 run.py:483] Algo bellman_ford step 6859 current loss 0.087393, current_train_items 219520.
I0304 19:31:16.817964 22579586809984 run.py:483] Algo bellman_ford step 6860 current loss 0.003216, current_train_items 219552.
I0304 19:31:16.834204 22579586809984 run.py:483] Algo bellman_ford step 6861 current loss 0.009083, current_train_items 219584.
I0304 19:31:16.857209 22579586809984 run.py:483] Algo bellman_ford step 6862 current loss 0.021810, current_train_items 219616.
I0304 19:31:16.887621 22579586809984 run.py:483] Algo bellman_ford step 6863 current loss 0.078055, current_train_items 219648.
I0304 19:31:16.919725 22579586809984 run.py:483] Algo bellman_ford step 6864 current loss 0.058401, current_train_items 219680.
I0304 19:31:16.939363 22579586809984 run.py:483] Algo bellman_ford step 6865 current loss 0.031457, current_train_items 219712.
I0304 19:31:16.955567 22579586809984 run.py:483] Algo bellman_ford step 6866 current loss 0.008868, current_train_items 219744.
I0304 19:31:16.980184 22579586809984 run.py:483] Algo bellman_ford step 6867 current loss 0.047339, current_train_items 219776.
I0304 19:31:17.011667 22579586809984 run.py:483] Algo bellman_ford step 6868 current loss 0.035637, current_train_items 219808.
I0304 19:31:17.047332 22579586809984 run.py:483] Algo bellman_ford step 6869 current loss 0.071520, current_train_items 219840.
I0304 19:31:17.067448 22579586809984 run.py:483] Algo bellman_ford step 6870 current loss 0.012970, current_train_items 219872.
I0304 19:31:17.083996 22579586809984 run.py:483] Algo bellman_ford step 6871 current loss 0.012557, current_train_items 219904.
I0304 19:31:17.107336 22579586809984 run.py:483] Algo bellman_ford step 6872 current loss 0.027419, current_train_items 219936.
I0304 19:31:17.138451 22579586809984 run.py:483] Algo bellman_ford step 6873 current loss 0.087799, current_train_items 219968.
I0304 19:31:17.172053 22579586809984 run.py:483] Algo bellman_ford step 6874 current loss 0.055085, current_train_items 220000.
I0304 19:31:17.191865 22579586809984 run.py:483] Algo bellman_ford step 6875 current loss 0.006650, current_train_items 220032.
I0304 19:31:17.207863 22579586809984 run.py:483] Algo bellman_ford step 6876 current loss 0.007650, current_train_items 220064.
I0304 19:31:17.231415 22579586809984 run.py:483] Algo bellman_ford step 6877 current loss 0.067137, current_train_items 220096.
I0304 19:31:17.262766 22579586809984 run.py:483] Algo bellman_ford step 6878 current loss 0.026190, current_train_items 220128.
I0304 19:31:17.296885 22579586809984 run.py:483] Algo bellman_ford step 6879 current loss 0.060772, current_train_items 220160.
I0304 19:31:17.316546 22579586809984 run.py:483] Algo bellman_ford step 6880 current loss 0.018430, current_train_items 220192.
I0304 19:31:17.332539 22579586809984 run.py:483] Algo bellman_ford step 6881 current loss 0.005791, current_train_items 220224.
I0304 19:31:17.357166 22579586809984 run.py:483] Algo bellman_ford step 6882 current loss 0.030675, current_train_items 220256.
I0304 19:31:17.387234 22579586809984 run.py:483] Algo bellman_ford step 6883 current loss 0.035451, current_train_items 220288.
I0304 19:31:17.420339 22579586809984 run.py:483] Algo bellman_ford step 6884 current loss 0.131757, current_train_items 220320.
I0304 19:31:17.440262 22579586809984 run.py:483] Algo bellman_ford step 6885 current loss 0.002355, current_train_items 220352.
I0304 19:31:17.456741 22579586809984 run.py:483] Algo bellman_ford step 6886 current loss 0.014962, current_train_items 220384.
I0304 19:31:17.480811 22579586809984 run.py:483] Algo bellman_ford step 6887 current loss 0.039264, current_train_items 220416.
I0304 19:31:17.513097 22579586809984 run.py:483] Algo bellman_ford step 6888 current loss 0.052399, current_train_items 220448.
I0304 19:31:17.546804 22579586809984 run.py:483] Algo bellman_ford step 6889 current loss 0.023389, current_train_items 220480.
I0304 19:31:17.566580 22579586809984 run.py:483] Algo bellman_ford step 6890 current loss 0.017470, current_train_items 220512.
I0304 19:31:17.583140 22579586809984 run.py:483] Algo bellman_ford step 6891 current loss 0.037644, current_train_items 220544.
I0304 19:31:17.606990 22579586809984 run.py:483] Algo bellman_ford step 6892 current loss 0.042539, current_train_items 220576.
I0304 19:31:17.638663 22579586809984 run.py:483] Algo bellman_ford step 6893 current loss 0.061903, current_train_items 220608.
I0304 19:31:17.672019 22579586809984 run.py:483] Algo bellman_ford step 6894 current loss 0.047232, current_train_items 220640.
I0304 19:31:17.691676 22579586809984 run.py:483] Algo bellman_ford step 6895 current loss 0.004766, current_train_items 220672.
I0304 19:31:17.707792 22579586809984 run.py:483] Algo bellman_ford step 6896 current loss 0.007020, current_train_items 220704.
I0304 19:31:17.732103 22579586809984 run.py:483] Algo bellman_ford step 6897 current loss 0.075956, current_train_items 220736.
I0304 19:31:17.762324 22579586809984 run.py:483] Algo bellman_ford step 6898 current loss 0.041421, current_train_items 220768.
I0304 19:31:17.796634 22579586809984 run.py:483] Algo bellman_ford step 6899 current loss 0.103904, current_train_items 220800.
I0304 19:31:17.816667 22579586809984 run.py:483] Algo bellman_ford step 6900 current loss 0.007426, current_train_items 220832.
I0304 19:31:17.824777 22579586809984 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0304 19:31:17.824882 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:31:17.841510 22579586809984 run.py:483] Algo bellman_ford step 6901 current loss 0.008151, current_train_items 220864.
I0304 19:31:17.865597 22579586809984 run.py:483] Algo bellman_ford step 6902 current loss 0.037633, current_train_items 220896.
I0304 19:31:17.898271 22579586809984 run.py:483] Algo bellman_ford step 6903 current loss 0.045785, current_train_items 220928.
I0304 19:31:17.933945 22579586809984 run.py:483] Algo bellman_ford step 6904 current loss 0.057117, current_train_items 220960.
I0304 19:31:17.953696 22579586809984 run.py:483] Algo bellman_ford step 6905 current loss 0.005323, current_train_items 220992.
I0304 19:31:17.969329 22579586809984 run.py:483] Algo bellman_ford step 6906 current loss 0.010104, current_train_items 221024.
I0304 19:31:17.993428 22579586809984 run.py:483] Algo bellman_ford step 6907 current loss 0.024698, current_train_items 221056.
I0304 19:31:18.025601 22579586809984 run.py:483] Algo bellman_ford step 6908 current loss 0.051128, current_train_items 221088.
I0304 19:31:18.060412 22579586809984 run.py:483] Algo bellman_ford step 6909 current loss 0.086953, current_train_items 221120.
I0304 19:31:18.080043 22579586809984 run.py:483] Algo bellman_ford step 6910 current loss 0.011655, current_train_items 221152.
I0304 19:31:18.096844 22579586809984 run.py:483] Algo bellman_ford step 6911 current loss 0.043106, current_train_items 221184.
I0304 19:31:18.119803 22579586809984 run.py:483] Algo bellman_ford step 6912 current loss 0.061219, current_train_items 221216.
I0304 19:31:18.151919 22579586809984 run.py:483] Algo bellman_ford step 6913 current loss 0.049779, current_train_items 221248.
I0304 19:31:18.187291 22579586809984 run.py:483] Algo bellman_ford step 6914 current loss 0.082053, current_train_items 221280.
I0304 19:31:18.206471 22579586809984 run.py:483] Algo bellman_ford step 6915 current loss 0.006297, current_train_items 221312.
I0304 19:31:18.222223 22579586809984 run.py:483] Algo bellman_ford step 6916 current loss 0.003704, current_train_items 221344.
I0304 19:31:18.247139 22579586809984 run.py:483] Algo bellman_ford step 6917 current loss 0.069269, current_train_items 221376.
I0304 19:31:18.279146 22579586809984 run.py:483] Algo bellman_ford step 6918 current loss 0.127006, current_train_items 221408.
I0304 19:31:18.314360 22579586809984 run.py:483] Algo bellman_ford step 6919 current loss 0.100413, current_train_items 221440.
I0304 19:31:18.333769 22579586809984 run.py:483] Algo bellman_ford step 6920 current loss 0.004266, current_train_items 221472.
I0304 19:31:18.349964 22579586809984 run.py:483] Algo bellman_ford step 6921 current loss 0.005654, current_train_items 221504.
I0304 19:31:18.373166 22579586809984 run.py:483] Algo bellman_ford step 6922 current loss 0.034615, current_train_items 221536.
I0304 19:31:18.404823 22579586809984 run.py:483] Algo bellman_ford step 6923 current loss 0.117267, current_train_items 221568.
I0304 19:31:18.438224 22579586809984 run.py:483] Algo bellman_ford step 6924 current loss 0.068935, current_train_items 221600.
I0304 19:31:18.457863 22579586809984 run.py:483] Algo bellman_ford step 6925 current loss 0.002121, current_train_items 221632.
I0304 19:31:18.473837 22579586809984 run.py:483] Algo bellman_ford step 6926 current loss 0.007738, current_train_items 221664.
I0304 19:31:18.498017 22579586809984 run.py:483] Algo bellman_ford step 6927 current loss 0.054478, current_train_items 221696.
I0304 19:31:18.529659 22579586809984 run.py:483] Algo bellman_ford step 6928 current loss 0.058075, current_train_items 221728.
I0304 19:31:18.562979 22579586809984 run.py:483] Algo bellman_ford step 6929 current loss 0.046480, current_train_items 221760.
I0304 19:31:18.582478 22579586809984 run.py:483] Algo bellman_ford step 6930 current loss 0.002267, current_train_items 221792.
I0304 19:31:18.598928 22579586809984 run.py:483] Algo bellman_ford step 6931 current loss 0.018193, current_train_items 221824.
I0304 19:31:18.622322 22579586809984 run.py:483] Algo bellman_ford step 6932 current loss 0.037817, current_train_items 221856.
I0304 19:31:18.653688 22579586809984 run.py:483] Algo bellman_ford step 6933 current loss 0.043437, current_train_items 221888.
I0304 19:31:18.688463 22579586809984 run.py:483] Algo bellman_ford step 6934 current loss 0.076574, current_train_items 221920.
I0304 19:31:18.708039 22579586809984 run.py:483] Algo bellman_ford step 6935 current loss 0.002414, current_train_items 221952.
I0304 19:31:18.724070 22579586809984 run.py:483] Algo bellman_ford step 6936 current loss 0.015550, current_train_items 221984.
I0304 19:31:18.747749 22579586809984 run.py:483] Algo bellman_ford step 6937 current loss 0.054833, current_train_items 222016.
I0304 19:31:18.779072 22579586809984 run.py:483] Algo bellman_ford step 6938 current loss 0.048204, current_train_items 222048.
I0304 19:31:18.811618 22579586809984 run.py:483] Algo bellman_ford step 6939 current loss 0.056638, current_train_items 222080.
I0304 19:31:18.831701 22579586809984 run.py:483] Algo bellman_ford step 6940 current loss 0.005628, current_train_items 222112.
I0304 19:31:18.848254 22579586809984 run.py:483] Algo bellman_ford step 6941 current loss 0.025888, current_train_items 222144.
I0304 19:31:18.872889 22579586809984 run.py:483] Algo bellman_ford step 6942 current loss 0.083989, current_train_items 222176.
I0304 19:31:18.904428 22579586809984 run.py:483] Algo bellman_ford step 6943 current loss 0.085792, current_train_items 222208.
I0304 19:31:18.938840 22579586809984 run.py:483] Algo bellman_ford step 6944 current loss 0.123584, current_train_items 222240.
I0304 19:31:18.958580 22579586809984 run.py:483] Algo bellman_ford step 6945 current loss 0.021736, current_train_items 222272.
I0304 19:31:18.974985 22579586809984 run.py:483] Algo bellman_ford step 6946 current loss 0.011770, current_train_items 222304.
I0304 19:31:18.999250 22579586809984 run.py:483] Algo bellman_ford step 6947 current loss 0.076406, current_train_items 222336.
I0304 19:31:19.028480 22579586809984 run.py:483] Algo bellman_ford step 6948 current loss 0.046030, current_train_items 222368.
I0304 19:31:19.062857 22579586809984 run.py:483] Algo bellman_ford step 6949 current loss 0.074148, current_train_items 222400.
I0304 19:31:19.082251 22579586809984 run.py:483] Algo bellman_ford step 6950 current loss 0.020504, current_train_items 222432.
I0304 19:31:19.090514 22579586809984 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0304 19:31:19.090621 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0304 19:31:19.107628 22579586809984 run.py:483] Algo bellman_ford step 6951 current loss 0.033227, current_train_items 222464.
I0304 19:31:19.132353 22579586809984 run.py:483] Algo bellman_ford step 6952 current loss 0.054350, current_train_items 222496.
I0304 19:31:19.163447 22579586809984 run.py:483] Algo bellman_ford step 6953 current loss 0.060064, current_train_items 222528.
I0304 19:31:19.198194 22579586809984 run.py:483] Algo bellman_ford step 6954 current loss 0.092092, current_train_items 222560.
I0304 19:31:19.218186 22579586809984 run.py:483] Algo bellman_ford step 6955 current loss 0.003816, current_train_items 222592.
I0304 19:31:19.234296 22579586809984 run.py:483] Algo bellman_ford step 6956 current loss 0.018025, current_train_items 222624.
I0304 19:31:19.260251 22579586809984 run.py:483] Algo bellman_ford step 6957 current loss 0.109550, current_train_items 222656.
I0304 19:31:19.291365 22579586809984 run.py:483] Algo bellman_ford step 6958 current loss 0.051050, current_train_items 222688.
I0304 19:31:19.323128 22579586809984 run.py:483] Algo bellman_ford step 6959 current loss 0.054714, current_train_items 222720.
I0304 19:31:19.342921 22579586809984 run.py:483] Algo bellman_ford step 6960 current loss 0.002026, current_train_items 222752.
I0304 19:31:19.359827 22579586809984 run.py:483] Algo bellman_ford step 6961 current loss 0.023386, current_train_items 222784.
I0304 19:31:19.383018 22579586809984 run.py:483] Algo bellman_ford step 6962 current loss 0.037988, current_train_items 222816.
I0304 19:31:19.414289 22579586809984 run.py:483] Algo bellman_ford step 6963 current loss 0.063654, current_train_items 222848.
I0304 19:31:19.449375 22579586809984 run.py:483] Algo bellman_ford step 6964 current loss 0.063323, current_train_items 222880.
I0304 19:31:19.469039 22579586809984 run.py:483] Algo bellman_ford step 6965 current loss 0.002494, current_train_items 222912.
I0304 19:31:19.485446 22579586809984 run.py:483] Algo bellman_ford step 6966 current loss 0.023366, current_train_items 222944.
I0304 19:31:19.509518 22579586809984 run.py:483] Algo bellman_ford step 6967 current loss 0.011861, current_train_items 222976.
I0304 19:31:19.539689 22579586809984 run.py:483] Algo bellman_ford step 6968 current loss 0.037527, current_train_items 223008.
I0304 19:31:19.573370 22579586809984 run.py:483] Algo bellman_ford step 6969 current loss 0.046196, current_train_items 223040.
I0304 19:31:19.593524 22579586809984 run.py:483] Algo bellman_ford step 6970 current loss 0.002831, current_train_items 223072.
I0304 19:31:19.609849 22579586809984 run.py:483] Algo bellman_ford step 6971 current loss 0.024749, current_train_items 223104.
I0304 19:31:19.633378 22579586809984 run.py:483] Algo bellman_ford step 6972 current loss 0.028654, current_train_items 223136.
I0304 19:31:19.664337 22579586809984 run.py:483] Algo bellman_ford step 6973 current loss 0.032931, current_train_items 223168.
I0304 19:31:19.698444 22579586809984 run.py:483] Algo bellman_ford step 6974 current loss 0.050151, current_train_items 223200.
I0304 19:31:19.718340 22579586809984 run.py:483] Algo bellman_ford step 6975 current loss 0.001740, current_train_items 223232.
I0304 19:31:19.734812 22579586809984 run.py:483] Algo bellman_ford step 6976 current loss 0.016915, current_train_items 223264.
I0304 19:31:19.758053 22579586809984 run.py:483] Algo bellman_ford step 6977 current loss 0.029742, current_train_items 223296.
I0304 19:31:19.788236 22579586809984 run.py:483] Algo bellman_ford step 6978 current loss 0.030702, current_train_items 223328.
I0304 19:31:19.822494 22579586809984 run.py:483] Algo bellman_ford step 6979 current loss 0.037823, current_train_items 223360.
I0304 19:31:19.842041 22579586809984 run.py:483] Algo bellman_ford step 6980 current loss 0.009543, current_train_items 223392.
I0304 19:31:19.858818 22579586809984 run.py:483] Algo bellman_ford step 6981 current loss 0.008295, current_train_items 223424.
I0304 19:31:19.881731 22579586809984 run.py:483] Algo bellman_ford step 6982 current loss 0.033563, current_train_items 223456.
I0304 19:31:19.913905 22579586809984 run.py:483] Algo bellman_ford step 6983 current loss 0.056777, current_train_items 223488.
I0304 19:31:19.947662 22579586809984 run.py:483] Algo bellman_ford step 6984 current loss 0.046691, current_train_items 223520.
I0304 19:31:19.967472 22579586809984 run.py:483] Algo bellman_ford step 6985 current loss 0.003140, current_train_items 223552.
I0304 19:31:19.983626 22579586809984 run.py:483] Algo bellman_ford step 6986 current loss 0.010890, current_train_items 223584.
I0304 19:31:20.007265 22579586809984 run.py:483] Algo bellman_ford step 6987 current loss 0.022168, current_train_items 223616.
I0304 19:31:20.038519 22579586809984 run.py:483] Algo bellman_ford step 6988 current loss 0.030875, current_train_items 223648.
I0304 19:31:20.074007 22579586809984 run.py:483] Algo bellman_ford step 6989 current loss 0.071037, current_train_items 223680.
I0304 19:31:20.093908 22579586809984 run.py:483] Algo bellman_ford step 6990 current loss 0.003292, current_train_items 223712.
I0304 19:31:20.109990 22579586809984 run.py:483] Algo bellman_ford step 6991 current loss 0.028581, current_train_items 223744.
I0304 19:31:20.133414 22579586809984 run.py:483] Algo bellman_ford step 6992 current loss 0.032043, current_train_items 223776.
I0304 19:31:20.165288 22579586809984 run.py:483] Algo bellman_ford step 6993 current loss 0.088538, current_train_items 223808.
I0304 19:31:20.201486 22579586809984 run.py:483] Algo bellman_ford step 6994 current loss 0.072661, current_train_items 223840.
I0304 19:31:20.221074 22579586809984 run.py:483] Algo bellman_ford step 6995 current loss 0.002410, current_train_items 223872.
I0304 19:31:20.237528 22579586809984 run.py:483] Algo bellman_ford step 6996 current loss 0.008324, current_train_items 223904.
I0304 19:31:20.260075 22579586809984 run.py:483] Algo bellman_ford step 6997 current loss 0.061773, current_train_items 223936.
I0304 19:31:20.290399 22579586809984 run.py:483] Algo bellman_ford step 6998 current loss 0.028669, current_train_items 223968.
I0304 19:31:20.323985 22579586809984 run.py:483] Algo bellman_ford step 6999 current loss 0.112038, current_train_items 224000.
I0304 19:31:20.343747 22579586809984 run.py:483] Algo bellman_ford step 7000 current loss 0.003011, current_train_items 224032.
I0304 19:31:20.351114 22579586809984 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0304 19:31:20.351220 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:20.367896 22579586809984 run.py:483] Algo bellman_ford step 7001 current loss 0.015455, current_train_items 224064.
I0304 19:31:20.391742 22579586809984 run.py:483] Algo bellman_ford step 7002 current loss 0.020092, current_train_items 224096.
I0304 19:31:20.421770 22579586809984 run.py:483] Algo bellman_ford step 7003 current loss 0.050757, current_train_items 224128.
I0304 19:31:20.455130 22579586809984 run.py:483] Algo bellman_ford step 7004 current loss 0.068281, current_train_items 224160.
I0304 19:31:20.475056 22579586809984 run.py:483] Algo bellman_ford step 7005 current loss 0.002187, current_train_items 224192.
I0304 19:31:20.491326 22579586809984 run.py:483] Algo bellman_ford step 7006 current loss 0.037584, current_train_items 224224.
I0304 19:31:20.516304 22579586809984 run.py:483] Algo bellman_ford step 7007 current loss 0.067163, current_train_items 224256.
I0304 19:31:20.546323 22579586809984 run.py:483] Algo bellman_ford step 7008 current loss 0.028966, current_train_items 224288.
I0304 19:31:20.582231 22579586809984 run.py:483] Algo bellman_ford step 7009 current loss 0.073297, current_train_items 224320.
I0304 19:31:20.602279 22579586809984 run.py:483] Algo bellman_ford step 7010 current loss 0.004980, current_train_items 224352.
I0304 19:31:20.618698 22579586809984 run.py:483] Algo bellman_ford step 7011 current loss 0.004830, current_train_items 224384.
I0304 19:31:20.642605 22579586809984 run.py:483] Algo bellman_ford step 7012 current loss 0.041955, current_train_items 224416.
I0304 19:31:20.673349 22579586809984 run.py:483] Algo bellman_ford step 7013 current loss 0.031462, current_train_items 224448.
I0304 19:31:20.707534 22579586809984 run.py:483] Algo bellman_ford step 7014 current loss 0.070478, current_train_items 224480.
I0304 19:31:20.727402 22579586809984 run.py:483] Algo bellman_ford step 7015 current loss 0.003282, current_train_items 224512.
I0304 19:31:20.743554 22579586809984 run.py:483] Algo bellman_ford step 7016 current loss 0.004649, current_train_items 224544.
I0304 19:31:20.767656 22579586809984 run.py:483] Algo bellman_ford step 7017 current loss 0.021063, current_train_items 224576.
I0304 19:31:20.798165 22579586809984 run.py:483] Algo bellman_ford step 7018 current loss 0.045578, current_train_items 224608.
I0304 19:31:20.831135 22579586809984 run.py:483] Algo bellman_ford step 7019 current loss 0.036279, current_train_items 224640.
I0304 19:31:20.850652 22579586809984 run.py:483] Algo bellman_ford step 7020 current loss 0.002569, current_train_items 224672.
I0304 19:31:20.866833 22579586809984 run.py:483] Algo bellman_ford step 7021 current loss 0.010950, current_train_items 224704.
I0304 19:31:20.890271 22579586809984 run.py:483] Algo bellman_ford step 7022 current loss 0.043259, current_train_items 224736.
I0304 19:31:20.920903 22579586809984 run.py:483] Algo bellman_ford step 7023 current loss 0.046471, current_train_items 224768.
I0304 19:31:20.955278 22579586809984 run.py:483] Algo bellman_ford step 7024 current loss 0.059106, current_train_items 224800.
I0304 19:31:20.975058 22579586809984 run.py:483] Algo bellman_ford step 7025 current loss 0.002524, current_train_items 224832.
I0304 19:31:20.991685 22579586809984 run.py:483] Algo bellman_ford step 7026 current loss 0.011342, current_train_items 224864.
I0304 19:31:21.017219 22579586809984 run.py:483] Algo bellman_ford step 7027 current loss 0.090591, current_train_items 224896.
I0304 19:31:21.050467 22579586809984 run.py:483] Algo bellman_ford step 7028 current loss 0.097508, current_train_items 224928.
I0304 19:31:21.082557 22579586809984 run.py:483] Algo bellman_ford step 7029 current loss 0.058675, current_train_items 224960.
I0304 19:31:21.102264 22579586809984 run.py:483] Algo bellman_ford step 7030 current loss 0.002729, current_train_items 224992.
I0304 19:31:21.118618 22579586809984 run.py:483] Algo bellman_ford step 7031 current loss 0.015984, current_train_items 225024.
I0304 19:31:21.142625 22579586809984 run.py:483] Algo bellman_ford step 7032 current loss 0.032405, current_train_items 225056.
I0304 19:31:21.173324 22579586809984 run.py:483] Algo bellman_ford step 7033 current loss 0.024939, current_train_items 225088.
I0304 19:31:21.209146 22579586809984 run.py:483] Algo bellman_ford step 7034 current loss 0.038549, current_train_items 225120.
I0304 19:31:21.229350 22579586809984 run.py:483] Algo bellman_ford step 7035 current loss 0.004594, current_train_items 225152.
I0304 19:31:21.245691 22579586809984 run.py:483] Algo bellman_ford step 7036 current loss 0.024647, current_train_items 225184.
I0304 19:31:21.269934 22579586809984 run.py:483] Algo bellman_ford step 7037 current loss 0.026184, current_train_items 225216.
I0304 19:31:21.302178 22579586809984 run.py:483] Algo bellman_ford step 7038 current loss 0.058610, current_train_items 225248.
I0304 19:31:21.335219 22579586809984 run.py:483] Algo bellman_ford step 7039 current loss 0.043645, current_train_items 225280.
I0304 19:31:21.356028 22579586809984 run.py:483] Algo bellman_ford step 7040 current loss 0.004177, current_train_items 225312.
I0304 19:31:21.372401 22579586809984 run.py:483] Algo bellman_ford step 7041 current loss 0.023328, current_train_items 225344.
I0304 19:31:21.396382 22579586809984 run.py:483] Algo bellman_ford step 7042 current loss 0.033167, current_train_items 225376.
I0304 19:31:21.428494 22579586809984 run.py:483] Algo bellman_ford step 7043 current loss 0.052863, current_train_items 225408.
I0304 19:31:21.462482 22579586809984 run.py:483] Algo bellman_ford step 7044 current loss 0.038818, current_train_items 225440.
I0304 19:31:21.482233 22579586809984 run.py:483] Algo bellman_ford step 7045 current loss 0.002804, current_train_items 225472.
I0304 19:31:21.498812 22579586809984 run.py:483] Algo bellman_ford step 7046 current loss 0.010755, current_train_items 225504.
I0304 19:31:21.522576 22579586809984 run.py:483] Algo bellman_ford step 7047 current loss 0.053699, current_train_items 225536.
I0304 19:31:21.554809 22579586809984 run.py:483] Algo bellman_ford step 7048 current loss 0.056160, current_train_items 225568.
I0304 19:31:21.588367 22579586809984 run.py:483] Algo bellman_ford step 7049 current loss 0.062965, current_train_items 225600.
I0304 19:31:21.608205 22579586809984 run.py:483] Algo bellman_ford step 7050 current loss 0.002191, current_train_items 225632.
I0304 19:31:21.616310 22579586809984 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0304 19:31:21.616415 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:31:21.632994 22579586809984 run.py:483] Algo bellman_ford step 7051 current loss 0.005906, current_train_items 225664.
I0304 19:31:21.658001 22579586809984 run.py:483] Algo bellman_ford step 7052 current loss 0.119419, current_train_items 225696.
I0304 19:31:21.691336 22579586809984 run.py:483] Algo bellman_ford step 7053 current loss 0.066611, current_train_items 225728.
I0304 19:31:21.726888 22579586809984 run.py:483] Algo bellman_ford step 7054 current loss 0.067624, current_train_items 225760.
I0304 19:31:21.746689 22579586809984 run.py:483] Algo bellman_ford step 7055 current loss 0.007405, current_train_items 225792.
I0304 19:31:21.762825 22579586809984 run.py:483] Algo bellman_ford step 7056 current loss 0.021856, current_train_items 225824.
I0304 19:31:21.787908 22579586809984 run.py:483] Algo bellman_ford step 7057 current loss 0.079441, current_train_items 225856.
I0304 19:31:21.819139 22579586809984 run.py:483] Algo bellman_ford step 7058 current loss 0.067961, current_train_items 225888.
I0304 19:31:21.854754 22579586809984 run.py:483] Algo bellman_ford step 7059 current loss 0.097268, current_train_items 225920.
I0304 19:31:21.874741 22579586809984 run.py:483] Algo bellman_ford step 7060 current loss 0.009011, current_train_items 225952.
I0304 19:31:21.891091 22579586809984 run.py:483] Algo bellman_ford step 7061 current loss 0.029901, current_train_items 225984.
I0304 19:31:21.915560 22579586809984 run.py:483] Algo bellman_ford step 7062 current loss 0.050690, current_train_items 226016.
I0304 19:31:21.944982 22579586809984 run.py:483] Algo bellman_ford step 7063 current loss 0.077136, current_train_items 226048.
I0304 19:31:21.978717 22579586809984 run.py:483] Algo bellman_ford step 7064 current loss 0.032709, current_train_items 226080.
I0304 19:31:21.998616 22579586809984 run.py:483] Algo bellman_ford step 7065 current loss 0.003100, current_train_items 226112.
I0304 19:31:22.015305 22579586809984 run.py:483] Algo bellman_ford step 7066 current loss 0.032782, current_train_items 226144.
I0304 19:31:22.039381 22579586809984 run.py:483] Algo bellman_ford step 7067 current loss 0.014618, current_train_items 226176.
I0304 19:31:22.072840 22579586809984 run.py:483] Algo bellman_ford step 7068 current loss 0.088815, current_train_items 226208.
I0304 19:31:22.108645 22579586809984 run.py:483] Algo bellman_ford step 7069 current loss 0.064566, current_train_items 226240.
I0304 19:31:22.129239 22579586809984 run.py:483] Algo bellman_ford step 7070 current loss 0.025758, current_train_items 226272.
I0304 19:31:22.145437 22579586809984 run.py:483] Algo bellman_ford step 7071 current loss 0.019273, current_train_items 226304.
I0304 19:31:22.167701 22579586809984 run.py:483] Algo bellman_ford step 7072 current loss 0.027419, current_train_items 226336.
I0304 19:31:22.198607 22579586809984 run.py:483] Algo bellman_ford step 7073 current loss 0.030231, current_train_items 226368.
I0304 19:31:22.232228 22579586809984 run.py:483] Algo bellman_ford step 7074 current loss 0.051480, current_train_items 226400.
I0304 19:31:22.252470 22579586809984 run.py:483] Algo bellman_ford step 7075 current loss 0.001812, current_train_items 226432.
I0304 19:31:22.268175 22579586809984 run.py:483] Algo bellman_ford step 7076 current loss 0.013576, current_train_items 226464.
I0304 19:31:22.291809 22579586809984 run.py:483] Algo bellman_ford step 7077 current loss 0.019590, current_train_items 226496.
I0304 19:31:22.323482 22579586809984 run.py:483] Algo bellman_ford step 7078 current loss 0.041921, current_train_items 226528.
I0304 19:31:22.358183 22579586809984 run.py:483] Algo bellman_ford step 7079 current loss 0.056093, current_train_items 226560.
I0304 19:31:22.377610 22579586809984 run.py:483] Algo bellman_ford step 7080 current loss 0.003703, current_train_items 226592.
I0304 19:31:22.393652 22579586809984 run.py:483] Algo bellman_ford step 7081 current loss 0.006893, current_train_items 226624.
I0304 19:31:22.417472 22579586809984 run.py:483] Algo bellman_ford step 7082 current loss 0.055752, current_train_items 226656.
I0304 19:31:22.449007 22579586809984 run.py:483] Algo bellman_ford step 7083 current loss 0.069493, current_train_items 226688.
I0304 19:31:22.482263 22579586809984 run.py:483] Algo bellman_ford step 7084 current loss 0.066515, current_train_items 226720.
I0304 19:31:22.502276 22579586809984 run.py:483] Algo bellman_ford step 7085 current loss 0.005606, current_train_items 226752.
I0304 19:31:22.518361 22579586809984 run.py:483] Algo bellman_ford step 7086 current loss 0.015096, current_train_items 226784.
I0304 19:31:22.541011 22579586809984 run.py:483] Algo bellman_ford step 7087 current loss 0.020042, current_train_items 226816.
I0304 19:31:22.572466 22579586809984 run.py:483] Algo bellman_ford step 7088 current loss 0.129888, current_train_items 226848.
I0304 19:31:22.605803 22579586809984 run.py:483] Algo bellman_ford step 7089 current loss 0.063911, current_train_items 226880.
I0304 19:31:22.626284 22579586809984 run.py:483] Algo bellman_ford step 7090 current loss 0.007484, current_train_items 226912.
I0304 19:31:22.642298 22579586809984 run.py:483] Algo bellman_ford step 7091 current loss 0.022022, current_train_items 226944.
I0304 19:31:22.666199 22579586809984 run.py:483] Algo bellman_ford step 7092 current loss 0.025777, current_train_items 226976.
I0304 19:31:22.698276 22579586809984 run.py:483] Algo bellman_ford step 7093 current loss 0.090362, current_train_items 227008.
I0304 19:31:22.730367 22579586809984 run.py:483] Algo bellman_ford step 7094 current loss 0.053418, current_train_items 227040.
I0304 19:31:22.750293 22579586809984 run.py:483] Algo bellman_ford step 7095 current loss 0.008427, current_train_items 227072.
I0304 19:31:22.766406 22579586809984 run.py:483] Algo bellman_ford step 7096 current loss 0.025091, current_train_items 227104.
I0304 19:31:22.789123 22579586809984 run.py:483] Algo bellman_ford step 7097 current loss 0.060765, current_train_items 227136.
I0304 19:31:22.821220 22579586809984 run.py:483] Algo bellman_ford step 7098 current loss 0.074935, current_train_items 227168.
I0304 19:31:22.854148 22579586809984 run.py:483] Algo bellman_ford step 7099 current loss 0.102950, current_train_items 227200.
I0304 19:31:22.874186 22579586809984 run.py:483] Algo bellman_ford step 7100 current loss 0.003772, current_train_items 227232.
I0304 19:31:22.881965 22579586809984 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0304 19:31:22.882069 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:22.898900 22579586809984 run.py:483] Algo bellman_ford step 7101 current loss 0.015943, current_train_items 227264.
I0304 19:31:22.922960 22579586809984 run.py:483] Algo bellman_ford step 7102 current loss 0.025109, current_train_items 227296.
I0304 19:31:22.955304 22579586809984 run.py:483] Algo bellman_ford step 7103 current loss 0.040958, current_train_items 227328.
I0304 19:31:22.990820 22579586809984 run.py:483] Algo bellman_ford step 7104 current loss 0.080180, current_train_items 227360.
I0304 19:31:23.010671 22579586809984 run.py:483] Algo bellman_ford step 7105 current loss 0.012019, current_train_items 227392.
I0304 19:31:23.026852 22579586809984 run.py:483] Algo bellman_ford step 7106 current loss 0.013633, current_train_items 227424.
I0304 19:31:23.050992 22579586809984 run.py:483] Algo bellman_ford step 7107 current loss 0.030193, current_train_items 227456.
I0304 19:31:23.082265 22579586809984 run.py:483] Algo bellman_ford step 7108 current loss 0.103265, current_train_items 227488.
I0304 19:31:23.117762 22579586809984 run.py:483] Algo bellman_ford step 7109 current loss 0.065695, current_train_items 227520.
I0304 19:31:23.137485 22579586809984 run.py:483] Algo bellman_ford step 7110 current loss 0.004814, current_train_items 227552.
I0304 19:31:23.153946 22579586809984 run.py:483] Algo bellman_ford step 7111 current loss 0.018073, current_train_items 227584.
I0304 19:31:23.177988 22579586809984 run.py:483] Algo bellman_ford step 7112 current loss 0.023973, current_train_items 227616.
I0304 19:31:23.210070 22579586809984 run.py:483] Algo bellman_ford step 7113 current loss 0.054846, current_train_items 227648.
I0304 19:31:23.244253 22579586809984 run.py:483] Algo bellman_ford step 7114 current loss 0.041201, current_train_items 227680.
I0304 19:31:23.263826 22579586809984 run.py:483] Algo bellman_ford step 7115 current loss 0.002881, current_train_items 227712.
I0304 19:31:23.280106 22579586809984 run.py:483] Algo bellman_ford step 7116 current loss 0.021466, current_train_items 227744.
I0304 19:31:23.304301 22579586809984 run.py:483] Algo bellman_ford step 7117 current loss 0.028154, current_train_items 227776.
I0304 19:31:23.335887 22579586809984 run.py:483] Algo bellman_ford step 7118 current loss 0.065846, current_train_items 227808.
I0304 19:31:23.369580 22579586809984 run.py:483] Algo bellman_ford step 7119 current loss 0.064001, current_train_items 227840.
I0304 19:31:23.388957 22579586809984 run.py:483] Algo bellman_ford step 7120 current loss 0.009628, current_train_items 227872.
I0304 19:31:23.405364 22579586809984 run.py:483] Algo bellman_ford step 7121 current loss 0.028068, current_train_items 227904.
I0304 19:31:23.429045 22579586809984 run.py:483] Algo bellman_ford step 7122 current loss 0.064691, current_train_items 227936.
I0304 19:31:23.460492 22579586809984 run.py:483] Algo bellman_ford step 7123 current loss 0.029233, current_train_items 227968.
I0304 19:31:23.494619 22579586809984 run.py:483] Algo bellman_ford step 7124 current loss 0.077309, current_train_items 228000.
I0304 19:31:23.514092 22579586809984 run.py:483] Algo bellman_ford step 7125 current loss 0.004826, current_train_items 228032.
I0304 19:31:23.529729 22579586809984 run.py:483] Algo bellman_ford step 7126 current loss 0.006731, current_train_items 228064.
I0304 19:31:23.552849 22579586809984 run.py:483] Algo bellman_ford step 7127 current loss 0.030723, current_train_items 228096.
I0304 19:31:23.583347 22579586809984 run.py:483] Algo bellman_ford step 7128 current loss 0.072763, current_train_items 228128.
I0304 19:31:23.618338 22579586809984 run.py:483] Algo bellman_ford step 7129 current loss 0.129293, current_train_items 228160.
I0304 19:31:23.637819 22579586809984 run.py:483] Algo bellman_ford step 7130 current loss 0.006504, current_train_items 228192.
I0304 19:31:23.654356 22579586809984 run.py:483] Algo bellman_ford step 7131 current loss 0.027392, current_train_items 228224.
I0304 19:31:23.678639 22579586809984 run.py:483] Algo bellman_ford step 7132 current loss 0.047709, current_train_items 228256.
I0304 19:31:23.709844 22579586809984 run.py:483] Algo bellman_ford step 7133 current loss 0.072295, current_train_items 228288.
I0304 19:31:23.743019 22579586809984 run.py:483] Algo bellman_ford step 7134 current loss 0.043996, current_train_items 228320.
I0304 19:31:23.762608 22579586809984 run.py:483] Algo bellman_ford step 7135 current loss 0.020676, current_train_items 228352.
I0304 19:31:23.778568 22579586809984 run.py:483] Algo bellman_ford step 7136 current loss 0.030404, current_train_items 228384.
I0304 19:31:23.802157 22579586809984 run.py:483] Algo bellman_ford step 7137 current loss 0.079769, current_train_items 228416.
I0304 19:31:23.832197 22579586809984 run.py:483] Algo bellman_ford step 7138 current loss 0.050445, current_train_items 228448.
I0304 19:31:23.868230 22579586809984 run.py:483] Algo bellman_ford step 7139 current loss 0.083189, current_train_items 228480.
I0304 19:31:23.887856 22579586809984 run.py:483] Algo bellman_ford step 7140 current loss 0.004733, current_train_items 228512.
I0304 19:31:23.903878 22579586809984 run.py:483] Algo bellman_ford step 7141 current loss 0.016717, current_train_items 228544.
I0304 19:31:23.928616 22579586809984 run.py:483] Algo bellman_ford step 7142 current loss 0.051920, current_train_items 228576.
I0304 19:31:23.958114 22579586809984 run.py:483] Algo bellman_ford step 7143 current loss 0.032602, current_train_items 228608.
I0304 19:31:23.991116 22579586809984 run.py:483] Algo bellman_ford step 7144 current loss 0.062358, current_train_items 228640.
I0304 19:31:24.010576 22579586809984 run.py:483] Algo bellman_ford step 7145 current loss 0.011649, current_train_items 228672.
I0304 19:31:24.027127 22579586809984 run.py:483] Algo bellman_ford step 7146 current loss 0.012438, current_train_items 228704.
I0304 19:31:24.051668 22579586809984 run.py:483] Algo bellman_ford step 7147 current loss 0.052122, current_train_items 228736.
I0304 19:31:24.081602 22579586809984 run.py:483] Algo bellman_ford step 7148 current loss 0.039206, current_train_items 228768.
I0304 19:31:24.112814 22579586809984 run.py:483] Algo bellman_ford step 7149 current loss 0.065681, current_train_items 228800.
I0304 19:31:24.132243 22579586809984 run.py:483] Algo bellman_ford step 7150 current loss 0.002985, current_train_items 228832.
I0304 19:31:24.140496 22579586809984 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0304 19:31:24.140600 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:31:24.157870 22579586809984 run.py:483] Algo bellman_ford step 7151 current loss 0.036919, current_train_items 228864.
I0304 19:31:24.182886 22579586809984 run.py:483] Algo bellman_ford step 7152 current loss 0.028557, current_train_items 228896.
I0304 19:31:24.214787 22579586809984 run.py:483] Algo bellman_ford step 7153 current loss 0.057030, current_train_items 228928.
I0304 19:31:24.248172 22579586809984 run.py:483] Algo bellman_ford step 7154 current loss 0.030485, current_train_items 228960.
I0304 19:31:24.268111 22579586809984 run.py:483] Algo bellman_ford step 7155 current loss 0.006484, current_train_items 228992.
I0304 19:31:24.284349 22579586809984 run.py:483] Algo bellman_ford step 7156 current loss 0.016522, current_train_items 229024.
I0304 19:31:24.307900 22579586809984 run.py:483] Algo bellman_ford step 7157 current loss 0.029179, current_train_items 229056.
I0304 19:31:24.338524 22579586809984 run.py:483] Algo bellman_ford step 7158 current loss 0.024181, current_train_items 229088.
I0304 19:31:24.372378 22579586809984 run.py:483] Algo bellman_ford step 7159 current loss 0.092558, current_train_items 229120.
I0304 19:31:24.392257 22579586809984 run.py:483] Algo bellman_ford step 7160 current loss 0.002605, current_train_items 229152.
I0304 19:31:24.408934 22579586809984 run.py:483] Algo bellman_ford step 7161 current loss 0.048710, current_train_items 229184.
W0304 19:31:24.423542 22579586809984 samplers.py:155] Increasing hint lengh from 10 to 11
I0304 19:31:31.370367 22579586809984 run.py:483] Algo bellman_ford step 7162 current loss 0.054740, current_train_items 229216.
I0304 19:31:31.403107 22579586809984 run.py:483] Algo bellman_ford step 7163 current loss 0.037750, current_train_items 229248.
I0304 19:31:31.438344 22579586809984 run.py:483] Algo bellman_ford step 7164 current loss 0.050746, current_train_items 229280.
I0304 19:31:31.458165 22579586809984 run.py:483] Algo bellman_ford step 7165 current loss 0.003650, current_train_items 229312.
I0304 19:31:31.474539 22579586809984 run.py:483] Algo bellman_ford step 7166 current loss 0.032062, current_train_items 229344.
I0304 19:31:31.498983 22579586809984 run.py:483] Algo bellman_ford step 7167 current loss 0.037969, current_train_items 229376.
I0304 19:31:31.531066 22579586809984 run.py:483] Algo bellman_ford step 7168 current loss 0.036811, current_train_items 229408.
I0304 19:31:31.562772 22579586809984 run.py:483] Algo bellman_ford step 7169 current loss 0.042188, current_train_items 229440.
I0304 19:31:31.583429 22579586809984 run.py:483] Algo bellman_ford step 7170 current loss 0.004092, current_train_items 229472.
I0304 19:31:31.600276 22579586809984 run.py:483] Algo bellman_ford step 7171 current loss 0.020762, current_train_items 229504.
I0304 19:31:31.624170 22579586809984 run.py:483] Algo bellman_ford step 7172 current loss 0.070927, current_train_items 229536.
I0304 19:31:31.656102 22579586809984 run.py:483] Algo bellman_ford step 7173 current loss 0.028306, current_train_items 229568.
I0304 19:31:31.691062 22579586809984 run.py:483] Algo bellman_ford step 7174 current loss 0.055140, current_train_items 229600.
I0304 19:31:31.711595 22579586809984 run.py:483] Algo bellman_ford step 7175 current loss 0.003933, current_train_items 229632.
I0304 19:31:31.727759 22579586809984 run.py:483] Algo bellman_ford step 7176 current loss 0.012611, current_train_items 229664.
I0304 19:31:31.751802 22579586809984 run.py:483] Algo bellman_ford step 7177 current loss 0.050894, current_train_items 229696.
I0304 19:31:31.785478 22579586809984 run.py:483] Algo bellman_ford step 7178 current loss 0.056504, current_train_items 229728.
I0304 19:31:31.820654 22579586809984 run.py:483] Algo bellman_ford step 7179 current loss 0.061306, current_train_items 229760.
I0304 19:31:31.840597 22579586809984 run.py:483] Algo bellman_ford step 7180 current loss 0.003036, current_train_items 229792.
I0304 19:31:31.856895 22579586809984 run.py:483] Algo bellman_ford step 7181 current loss 0.047391, current_train_items 229824.
I0304 19:31:31.883118 22579586809984 run.py:483] Algo bellman_ford step 7182 current loss 0.117618, current_train_items 229856.
I0304 19:31:31.914297 22579586809984 run.py:483] Algo bellman_ford step 7183 current loss 0.049087, current_train_items 229888.
I0304 19:31:31.950917 22579586809984 run.py:483] Algo bellman_ford step 7184 current loss 0.059117, current_train_items 229920.
I0304 19:31:31.970847 22579586809984 run.py:483] Algo bellman_ford step 7185 current loss 0.003590, current_train_items 229952.
I0304 19:31:31.987338 22579586809984 run.py:483] Algo bellman_ford step 7186 current loss 0.014671, current_train_items 229984.
I0304 19:31:32.012502 22579586809984 run.py:483] Algo bellman_ford step 7187 current loss 0.056239, current_train_items 230016.
I0304 19:31:32.044853 22579586809984 run.py:483] Algo bellman_ford step 7188 current loss 0.065294, current_train_items 230048.
I0304 19:31:32.080569 22579586809984 run.py:483] Algo bellman_ford step 7189 current loss 0.049750, current_train_items 230080.
I0304 19:31:32.100582 22579586809984 run.py:483] Algo bellman_ford step 7190 current loss 0.002411, current_train_items 230112.
I0304 19:31:32.117409 22579586809984 run.py:483] Algo bellman_ford step 7191 current loss 0.051151, current_train_items 230144.
I0304 19:31:32.141315 22579586809984 run.py:483] Algo bellman_ford step 7192 current loss 0.052523, current_train_items 230176.
I0304 19:31:32.173431 22579586809984 run.py:483] Algo bellman_ford step 7193 current loss 0.048074, current_train_items 230208.
I0304 19:31:32.206995 22579586809984 run.py:483] Algo bellman_ford step 7194 current loss 0.057491, current_train_items 230240.
I0304 19:31:32.227269 22579586809984 run.py:483] Algo bellman_ford step 7195 current loss 0.002082, current_train_items 230272.
I0304 19:31:32.243423 22579586809984 run.py:483] Algo bellman_ford step 7196 current loss 0.005957, current_train_items 230304.
I0304 19:31:32.267757 22579586809984 run.py:483] Algo bellman_ford step 7197 current loss 0.048019, current_train_items 230336.
I0304 19:31:32.299880 22579586809984 run.py:483] Algo bellman_ford step 7198 current loss 0.138193, current_train_items 230368.
I0304 19:31:32.334069 22579586809984 run.py:483] Algo bellman_ford step 7199 current loss 0.118755, current_train_items 230400.
I0304 19:31:32.354663 22579586809984 run.py:483] Algo bellman_ford step 7200 current loss 0.005342, current_train_items 230432.
I0304 19:31:32.364137 22579586809984 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0304 19:31:32.364264 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:32.381288 22579586809984 run.py:483] Algo bellman_ford step 7201 current loss 0.021745, current_train_items 230464.
I0304 19:31:32.405556 22579586809984 run.py:483] Algo bellman_ford step 7202 current loss 0.025463, current_train_items 230496.
I0304 19:31:32.438278 22579586809984 run.py:483] Algo bellman_ford step 7203 current loss 0.062020, current_train_items 230528.
I0304 19:31:32.472379 22579586809984 run.py:483] Algo bellman_ford step 7204 current loss 0.059809, current_train_items 230560.
I0304 19:31:32.492801 22579586809984 run.py:483] Algo bellman_ford step 7205 current loss 0.004161, current_train_items 230592.
I0304 19:31:32.508653 22579586809984 run.py:483] Algo bellman_ford step 7206 current loss 0.012941, current_train_items 230624.
I0304 19:31:32.533157 22579586809984 run.py:483] Algo bellman_ford step 7207 current loss 0.039544, current_train_items 230656.
I0304 19:31:32.564533 22579586809984 run.py:483] Algo bellman_ford step 7208 current loss 0.036785, current_train_items 230688.
I0304 19:31:32.599360 22579586809984 run.py:483] Algo bellman_ford step 7209 current loss 0.066141, current_train_items 230720.
I0304 19:31:32.619101 22579586809984 run.py:483] Algo bellman_ford step 7210 current loss 0.002259, current_train_items 230752.
I0304 19:31:32.635498 22579586809984 run.py:483] Algo bellman_ford step 7211 current loss 0.011377, current_train_items 230784.
I0304 19:31:32.659291 22579586809984 run.py:483] Algo bellman_ford step 7212 current loss 0.055686, current_train_items 230816.
I0304 19:31:32.690770 22579586809984 run.py:483] Algo bellman_ford step 7213 current loss 0.038354, current_train_items 230848.
I0304 19:31:32.722743 22579586809984 run.py:483] Algo bellman_ford step 7214 current loss 0.052599, current_train_items 230880.
I0304 19:31:32.742152 22579586809984 run.py:483] Algo bellman_ford step 7215 current loss 0.003203, current_train_items 230912.
I0304 19:31:32.758361 22579586809984 run.py:483] Algo bellman_ford step 7216 current loss 0.010364, current_train_items 230944.
I0304 19:31:32.782846 22579586809984 run.py:483] Algo bellman_ford step 7217 current loss 0.050732, current_train_items 230976.
I0304 19:31:32.815083 22579586809984 run.py:483] Algo bellman_ford step 7218 current loss 0.076522, current_train_items 231008.
I0304 19:31:32.848549 22579586809984 run.py:483] Algo bellman_ford step 7219 current loss 0.043325, current_train_items 231040.
I0304 19:31:32.868344 22579586809984 run.py:483] Algo bellman_ford step 7220 current loss 0.011321, current_train_items 231072.
I0304 19:31:32.884673 22579586809984 run.py:483] Algo bellman_ford step 7221 current loss 0.019750, current_train_items 231104.
I0304 19:31:32.910080 22579586809984 run.py:483] Algo bellman_ford step 7222 current loss 0.038370, current_train_items 231136.
I0304 19:31:32.941973 22579586809984 run.py:483] Algo bellman_ford step 7223 current loss 0.051604, current_train_items 231168.
I0304 19:31:32.976495 22579586809984 run.py:483] Algo bellman_ford step 7224 current loss 0.063735, current_train_items 231200.
I0304 19:31:32.996655 22579586809984 run.py:483] Algo bellman_ford step 7225 current loss 0.004676, current_train_items 231232.
I0304 19:31:33.012933 22579586809984 run.py:483] Algo bellman_ford step 7226 current loss 0.026748, current_train_items 231264.
I0304 19:31:33.038718 22579586809984 run.py:483] Algo bellman_ford step 7227 current loss 0.040146, current_train_items 231296.
I0304 19:31:33.070583 22579586809984 run.py:483] Algo bellman_ford step 7228 current loss 0.025675, current_train_items 231328.
I0304 19:31:33.103108 22579586809984 run.py:483] Algo bellman_ford step 7229 current loss 0.055976, current_train_items 231360.
I0304 19:31:33.122748 22579586809984 run.py:483] Algo bellman_ford step 7230 current loss 0.003783, current_train_items 231392.
I0304 19:31:33.139233 22579586809984 run.py:483] Algo bellman_ford step 7231 current loss 0.009489, current_train_items 231424.
I0304 19:31:33.161574 22579586809984 run.py:483] Algo bellman_ford step 7232 current loss 0.023078, current_train_items 231456.
I0304 19:31:33.194386 22579586809984 run.py:483] Algo bellman_ford step 7233 current loss 0.047082, current_train_items 231488.
I0304 19:31:33.227231 22579586809984 run.py:483] Algo bellman_ford step 7234 current loss 0.030490, current_train_items 231520.
I0304 19:31:33.247136 22579586809984 run.py:483] Algo bellman_ford step 7235 current loss 0.004970, current_train_items 231552.
I0304 19:31:33.263742 22579586809984 run.py:483] Algo bellman_ford step 7236 current loss 0.013206, current_train_items 231584.
I0304 19:31:33.288947 22579586809984 run.py:483] Algo bellman_ford step 7237 current loss 0.032343, current_train_items 231616.
I0304 19:31:33.321990 22579586809984 run.py:483] Algo bellman_ford step 7238 current loss 0.073512, current_train_items 231648.
I0304 19:31:33.355746 22579586809984 run.py:483] Algo bellman_ford step 7239 current loss 0.042700, current_train_items 231680.
I0304 19:31:33.375465 22579586809984 run.py:483] Algo bellman_ford step 7240 current loss 0.009234, current_train_items 231712.
I0304 19:31:33.391593 22579586809984 run.py:483] Algo bellman_ford step 7241 current loss 0.009123, current_train_items 231744.
I0304 19:31:33.417200 22579586809984 run.py:483] Algo bellman_ford step 7242 current loss 0.068840, current_train_items 231776.
I0304 19:31:33.449438 22579586809984 run.py:483] Algo bellman_ford step 7243 current loss 0.059415, current_train_items 231808.
I0304 19:31:33.485349 22579586809984 run.py:483] Algo bellman_ford step 7244 current loss 0.065705, current_train_items 231840.
I0304 19:31:33.505247 22579586809984 run.py:483] Algo bellman_ford step 7245 current loss 0.002487, current_train_items 231872.
I0304 19:31:33.521468 22579586809984 run.py:483] Algo bellman_ford step 7246 current loss 0.004503, current_train_items 231904.
I0304 19:31:33.546181 22579586809984 run.py:483] Algo bellman_ford step 7247 current loss 0.028341, current_train_items 231936.
I0304 19:31:33.579288 22579586809984 run.py:483] Algo bellman_ford step 7248 current loss 0.124391, current_train_items 231968.
I0304 19:31:33.613739 22579586809984 run.py:483] Algo bellman_ford step 7249 current loss 0.102779, current_train_items 232000.
I0304 19:31:33.633890 22579586809984 run.py:483] Algo bellman_ford step 7250 current loss 0.010893, current_train_items 232032.
I0304 19:31:33.642362 22579586809984 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0304 19:31:33.642467 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:31:33.659439 22579586809984 run.py:483] Algo bellman_ford step 7251 current loss 0.031437, current_train_items 232064.
I0304 19:31:33.683928 22579586809984 run.py:483] Algo bellman_ford step 7252 current loss 0.045411, current_train_items 232096.
I0304 19:31:33.715365 22579586809984 run.py:483] Algo bellman_ford step 7253 current loss 0.044812, current_train_items 232128.
I0304 19:31:33.749775 22579586809984 run.py:483] Algo bellman_ford step 7254 current loss 0.049424, current_train_items 232160.
I0304 19:31:33.770039 22579586809984 run.py:483] Algo bellman_ford step 7255 current loss 0.003167, current_train_items 232192.
I0304 19:31:33.785785 22579586809984 run.py:483] Algo bellman_ford step 7256 current loss 0.020538, current_train_items 232224.
I0304 19:31:33.810016 22579586809984 run.py:483] Algo bellman_ford step 7257 current loss 0.063155, current_train_items 232256.
I0304 19:31:33.844516 22579586809984 run.py:483] Algo bellman_ford step 7258 current loss 0.071525, current_train_items 232288.
I0304 19:31:33.878984 22579586809984 run.py:483] Algo bellman_ford step 7259 current loss 0.067072, current_train_items 232320.
I0304 19:31:33.898958 22579586809984 run.py:483] Algo bellman_ford step 7260 current loss 0.003003, current_train_items 232352.
I0304 19:31:33.915454 22579586809984 run.py:483] Algo bellman_ford step 7261 current loss 0.013726, current_train_items 232384.
I0304 19:31:33.939658 22579586809984 run.py:483] Algo bellman_ford step 7262 current loss 0.021392, current_train_items 232416.
I0304 19:31:33.972050 22579586809984 run.py:483] Algo bellman_ford step 7263 current loss 0.049253, current_train_items 232448.
I0304 19:31:34.005877 22579586809984 run.py:483] Algo bellman_ford step 7264 current loss 0.046519, current_train_items 232480.
I0304 19:31:34.025294 22579586809984 run.py:483] Algo bellman_ford step 7265 current loss 0.003020, current_train_items 232512.
I0304 19:31:34.041596 22579586809984 run.py:483] Algo bellman_ford step 7266 current loss 0.021575, current_train_items 232544.
I0304 19:31:34.065644 22579586809984 run.py:483] Algo bellman_ford step 7267 current loss 0.058921, current_train_items 232576.
I0304 19:31:34.097703 22579586809984 run.py:483] Algo bellman_ford step 7268 current loss 0.074620, current_train_items 232608.
I0304 19:31:34.130985 22579586809984 run.py:483] Algo bellman_ford step 7269 current loss 0.078557, current_train_items 232640.
I0304 19:31:34.151050 22579586809984 run.py:483] Algo bellman_ford step 7270 current loss 0.004034, current_train_items 232672.
I0304 19:31:34.167747 22579586809984 run.py:483] Algo bellman_ford step 7271 current loss 0.041572, current_train_items 232704.
I0304 19:31:34.192414 22579586809984 run.py:483] Algo bellman_ford step 7272 current loss 0.022316, current_train_items 232736.
I0304 19:31:34.224097 22579586809984 run.py:483] Algo bellman_ford step 7273 current loss 0.039652, current_train_items 232768.
I0304 19:31:34.258052 22579586809984 run.py:483] Algo bellman_ford step 7274 current loss 0.140111, current_train_items 232800.
I0304 19:31:34.278451 22579586809984 run.py:483] Algo bellman_ford step 7275 current loss 0.001928, current_train_items 232832.
I0304 19:31:34.295223 22579586809984 run.py:483] Algo bellman_ford step 7276 current loss 0.016213, current_train_items 232864.
I0304 19:31:34.319356 22579586809984 run.py:483] Algo bellman_ford step 7277 current loss 0.049681, current_train_items 232896.
I0304 19:31:34.351975 22579586809984 run.py:483] Algo bellman_ford step 7278 current loss 0.092100, current_train_items 232928.
I0304 19:31:34.386027 22579586809984 run.py:483] Algo bellman_ford step 7279 current loss 0.047780, current_train_items 232960.
I0304 19:31:34.405735 22579586809984 run.py:483] Algo bellman_ford step 7280 current loss 0.028786, current_train_items 232992.
I0304 19:31:34.422400 22579586809984 run.py:483] Algo bellman_ford step 7281 current loss 0.020955, current_train_items 233024.
I0304 19:31:34.446274 22579586809984 run.py:483] Algo bellman_ford step 7282 current loss 0.027191, current_train_items 233056.
I0304 19:31:34.478624 22579586809984 run.py:483] Algo bellman_ford step 7283 current loss 0.076730, current_train_items 233088.
I0304 19:31:34.511839 22579586809984 run.py:483] Algo bellman_ford step 7284 current loss 0.049006, current_train_items 233120.
I0304 19:31:34.531932 22579586809984 run.py:483] Algo bellman_ford step 7285 current loss 0.002604, current_train_items 233152.
I0304 19:31:34.549282 22579586809984 run.py:483] Algo bellman_ford step 7286 current loss 0.030033, current_train_items 233184.
I0304 19:31:34.573188 22579586809984 run.py:483] Algo bellman_ford step 7287 current loss 0.040239, current_train_items 233216.
I0304 19:31:34.604756 22579586809984 run.py:483] Algo bellman_ford step 7288 current loss 0.040695, current_train_items 233248.
I0304 19:31:34.638093 22579586809984 run.py:483] Algo bellman_ford step 7289 current loss 0.049536, current_train_items 233280.
I0304 19:31:34.658339 22579586809984 run.py:483] Algo bellman_ford step 7290 current loss 0.020574, current_train_items 233312.
I0304 19:31:34.674896 22579586809984 run.py:483] Algo bellman_ford step 7291 current loss 0.015787, current_train_items 233344.
I0304 19:31:34.698151 22579586809984 run.py:483] Algo bellman_ford step 7292 current loss 0.018182, current_train_items 233376.
I0304 19:31:34.731203 22579586809984 run.py:483] Algo bellman_ford step 7293 current loss 0.029777, current_train_items 233408.
I0304 19:31:34.763583 22579586809984 run.py:483] Algo bellman_ford step 7294 current loss 0.059994, current_train_items 233440.
I0304 19:31:34.783042 22579586809984 run.py:483] Algo bellman_ford step 7295 current loss 0.005534, current_train_items 233472.
I0304 19:31:34.799448 22579586809984 run.py:483] Algo bellman_ford step 7296 current loss 0.008901, current_train_items 233504.
I0304 19:31:34.823999 22579586809984 run.py:483] Algo bellman_ford step 7297 current loss 0.056882, current_train_items 233536.
I0304 19:31:34.856249 22579586809984 run.py:483] Algo bellman_ford step 7298 current loss 0.034562, current_train_items 233568.
I0304 19:31:34.889985 22579586809984 run.py:483] Algo bellman_ford step 7299 current loss 0.066259, current_train_items 233600.
I0304 19:31:34.910336 22579586809984 run.py:483] Algo bellman_ford step 7300 current loss 0.009962, current_train_items 233632.
I0304 19:31:34.918216 22579586809984 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0304 19:31:34.918323 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:31:34.935660 22579586809984 run.py:483] Algo bellman_ford step 7301 current loss 0.011708, current_train_items 233664.
I0304 19:31:34.960215 22579586809984 run.py:483] Algo bellman_ford step 7302 current loss 0.059231, current_train_items 233696.
I0304 19:31:34.992252 22579586809984 run.py:483] Algo bellman_ford step 7303 current loss 0.047506, current_train_items 233728.
I0304 19:31:35.026911 22579586809984 run.py:483] Algo bellman_ford step 7304 current loss 0.056956, current_train_items 233760.
I0304 19:31:35.046844 22579586809984 run.py:483] Algo bellman_ford step 7305 current loss 0.002264, current_train_items 233792.
I0304 19:31:35.062778 22579586809984 run.py:483] Algo bellman_ford step 7306 current loss 0.009653, current_train_items 233824.
I0304 19:31:35.086546 22579586809984 run.py:483] Algo bellman_ford step 7307 current loss 0.089075, current_train_items 233856.
I0304 19:31:35.117498 22579586809984 run.py:483] Algo bellman_ford step 7308 current loss 0.054650, current_train_items 233888.
I0304 19:31:35.152626 22579586809984 run.py:483] Algo bellman_ford step 7309 current loss 0.129484, current_train_items 233920.
I0304 19:31:35.172554 22579586809984 run.py:483] Algo bellman_ford step 7310 current loss 0.002667, current_train_items 233952.
I0304 19:31:35.188614 22579586809984 run.py:483] Algo bellman_ford step 7311 current loss 0.009404, current_train_items 233984.
I0304 19:31:35.214307 22579586809984 run.py:483] Algo bellman_ford step 7312 current loss 0.043007, current_train_items 234016.
I0304 19:31:35.246219 22579586809984 run.py:483] Algo bellman_ford step 7313 current loss 0.052809, current_train_items 234048.
I0304 19:31:35.280915 22579586809984 run.py:483] Algo bellman_ford step 7314 current loss 0.050244, current_train_items 234080.
I0304 19:31:35.300798 22579586809984 run.py:483] Algo bellman_ford step 7315 current loss 0.004226, current_train_items 234112.
I0304 19:31:35.316909 22579586809984 run.py:483] Algo bellman_ford step 7316 current loss 0.011378, current_train_items 234144.
I0304 19:31:35.341903 22579586809984 run.py:483] Algo bellman_ford step 7317 current loss 0.051448, current_train_items 234176.
I0304 19:31:35.373846 22579586809984 run.py:483] Algo bellman_ford step 7318 current loss 0.038067, current_train_items 234208.
I0304 19:31:35.408911 22579586809984 run.py:483] Algo bellman_ford step 7319 current loss 0.083749, current_train_items 234240.
I0304 19:31:35.429066 22579586809984 run.py:483] Algo bellman_ford step 7320 current loss 0.002115, current_train_items 234272.
I0304 19:31:35.445806 22579586809984 run.py:483] Algo bellman_ford step 7321 current loss 0.018754, current_train_items 234304.
I0304 19:31:35.471522 22579586809984 run.py:483] Algo bellman_ford step 7322 current loss 0.042452, current_train_items 234336.
I0304 19:31:35.502897 22579586809984 run.py:483] Algo bellman_ford step 7323 current loss 0.022531, current_train_items 234368.
I0304 19:31:35.537994 22579586809984 run.py:483] Algo bellman_ford step 7324 current loss 0.063601, current_train_items 234400.
I0304 19:31:35.557524 22579586809984 run.py:483] Algo bellman_ford step 7325 current loss 0.001893, current_train_items 234432.
I0304 19:31:35.574168 22579586809984 run.py:483] Algo bellman_ford step 7326 current loss 0.022268, current_train_items 234464.
I0304 19:31:35.598716 22579586809984 run.py:483] Algo bellman_ford step 7327 current loss 0.025016, current_train_items 234496.
I0304 19:31:35.630166 22579586809984 run.py:483] Algo bellman_ford step 7328 current loss 0.042124, current_train_items 234528.
I0304 19:31:35.664737 22579586809984 run.py:483] Algo bellman_ford step 7329 current loss 0.039461, current_train_items 234560.
I0304 19:31:35.684373 22579586809984 run.py:483] Algo bellman_ford step 7330 current loss 0.002305, current_train_items 234592.
I0304 19:31:35.701025 22579586809984 run.py:483] Algo bellman_ford step 7331 current loss 0.027378, current_train_items 234624.
I0304 19:31:35.726648 22579586809984 run.py:483] Algo bellman_ford step 7332 current loss 0.062197, current_train_items 234656.
I0304 19:31:35.759410 22579586809984 run.py:483] Algo bellman_ford step 7333 current loss 0.047830, current_train_items 234688.
I0304 19:31:35.792007 22579586809984 run.py:483] Algo bellman_ford step 7334 current loss 0.042997, current_train_items 234720.
I0304 19:31:35.811866 22579586809984 run.py:483] Algo bellman_ford step 7335 current loss 0.003364, current_train_items 234752.
I0304 19:31:35.827944 22579586809984 run.py:483] Algo bellman_ford step 7336 current loss 0.023895, current_train_items 234784.
I0304 19:31:35.852128 22579586809984 run.py:483] Algo bellman_ford step 7337 current loss 0.044853, current_train_items 234816.
I0304 19:31:35.884434 22579586809984 run.py:483] Algo bellman_ford step 7338 current loss 0.054993, current_train_items 234848.
I0304 19:31:35.920126 22579586809984 run.py:483] Algo bellman_ford step 7339 current loss 0.071244, current_train_items 234880.
I0304 19:31:35.940135 22579586809984 run.py:483] Algo bellman_ford step 7340 current loss 0.003778, current_train_items 234912.
I0304 19:31:35.956335 22579586809984 run.py:483] Algo bellman_ford step 7341 current loss 0.013423, current_train_items 234944.
I0304 19:31:35.981542 22579586809984 run.py:483] Algo bellman_ford step 7342 current loss 0.056968, current_train_items 234976.
I0304 19:31:36.014448 22579586809984 run.py:483] Algo bellman_ford step 7343 current loss 0.049646, current_train_items 235008.
I0304 19:31:36.048168 22579586809984 run.py:483] Algo bellman_ford step 7344 current loss 0.095691, current_train_items 235040.
I0304 19:31:36.068226 22579586809984 run.py:483] Algo bellman_ford step 7345 current loss 0.009941, current_train_items 235072.
I0304 19:31:36.084294 22579586809984 run.py:483] Algo bellman_ford step 7346 current loss 0.008678, current_train_items 235104.
I0304 19:31:36.109429 22579586809984 run.py:483] Algo bellman_ford step 7347 current loss 0.054981, current_train_items 235136.
I0304 19:31:36.142130 22579586809984 run.py:483] Algo bellman_ford step 7348 current loss 0.056181, current_train_items 235168.
I0304 19:31:36.172873 22579586809984 run.py:483] Algo bellman_ford step 7349 current loss 0.025052, current_train_items 235200.
I0304 19:31:36.192949 22579586809984 run.py:483] Algo bellman_ford step 7350 current loss 0.003345, current_train_items 235232.
I0304 19:31:36.201195 22579586809984 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0304 19:31:36.201305 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:36.218754 22579586809984 run.py:483] Algo bellman_ford step 7351 current loss 0.015130, current_train_items 235264.
I0304 19:31:36.244307 22579586809984 run.py:483] Algo bellman_ford step 7352 current loss 0.052617, current_train_items 235296.
I0304 19:31:36.277295 22579586809984 run.py:483] Algo bellman_ford step 7353 current loss 0.079301, current_train_items 235328.
I0304 19:31:36.311557 22579586809984 run.py:483] Algo bellman_ford step 7354 current loss 0.074370, current_train_items 235360.
I0304 19:31:36.331379 22579586809984 run.py:483] Algo bellman_ford step 7355 current loss 0.004548, current_train_items 235392.
I0304 19:31:36.347211 22579586809984 run.py:483] Algo bellman_ford step 7356 current loss 0.007523, current_train_items 235424.
I0304 19:31:36.372816 22579586809984 run.py:483] Algo bellman_ford step 7357 current loss 0.070464, current_train_items 235456.
I0304 19:31:36.404425 22579586809984 run.py:483] Algo bellman_ford step 7358 current loss 0.069911, current_train_items 235488.
I0304 19:31:36.438498 22579586809984 run.py:483] Algo bellman_ford step 7359 current loss 0.141631, current_train_items 235520.
I0304 19:31:36.458429 22579586809984 run.py:483] Algo bellman_ford step 7360 current loss 0.002711, current_train_items 235552.
I0304 19:31:36.475482 22579586809984 run.py:483] Algo bellman_ford step 7361 current loss 0.038240, current_train_items 235584.
I0304 19:31:36.498354 22579586809984 run.py:483] Algo bellman_ford step 7362 current loss 0.021344, current_train_items 235616.
I0304 19:31:36.531906 22579586809984 run.py:483] Algo bellman_ford step 7363 current loss 0.090715, current_train_items 235648.
I0304 19:31:36.563853 22579586809984 run.py:483] Algo bellman_ford step 7364 current loss 0.097623, current_train_items 235680.
I0304 19:31:36.583768 22579586809984 run.py:483] Algo bellman_ford step 7365 current loss 0.028946, current_train_items 235712.
I0304 19:31:36.600207 22579586809984 run.py:483] Algo bellman_ford step 7366 current loss 0.019993, current_train_items 235744.
I0304 19:31:36.624951 22579586809984 run.py:483] Algo bellman_ford step 7367 current loss 0.031092, current_train_items 235776.
I0304 19:31:36.658612 22579586809984 run.py:483] Algo bellman_ford step 7368 current loss 0.062537, current_train_items 235808.
I0304 19:31:36.691496 22579586809984 run.py:483] Algo bellman_ford step 7369 current loss 0.026774, current_train_items 235840.
I0304 19:31:36.711418 22579586809984 run.py:483] Algo bellman_ford step 7370 current loss 0.003218, current_train_items 235872.
I0304 19:31:36.727949 22579586809984 run.py:483] Algo bellman_ford step 7371 current loss 0.015370, current_train_items 235904.
I0304 19:31:36.750837 22579586809984 run.py:483] Algo bellman_ford step 7372 current loss 0.053947, current_train_items 235936.
I0304 19:31:36.782473 22579586809984 run.py:483] Algo bellman_ford step 7373 current loss 0.037848, current_train_items 235968.
I0304 19:31:36.815501 22579586809984 run.py:483] Algo bellman_ford step 7374 current loss 0.133506, current_train_items 236000.
I0304 19:31:36.835892 22579586809984 run.py:483] Algo bellman_ford step 7375 current loss 0.002631, current_train_items 236032.
I0304 19:31:36.852768 22579586809984 run.py:483] Algo bellman_ford step 7376 current loss 0.043746, current_train_items 236064.
I0304 19:31:36.877408 22579586809984 run.py:483] Algo bellman_ford step 7377 current loss 0.029797, current_train_items 236096.
I0304 19:31:36.908953 22579586809984 run.py:483] Algo bellman_ford step 7378 current loss 0.075830, current_train_items 236128.
I0304 19:31:36.941334 22579586809984 run.py:483] Algo bellman_ford step 7379 current loss 0.099699, current_train_items 236160.
I0304 19:31:36.960718 22579586809984 run.py:483] Algo bellman_ford step 7380 current loss 0.003956, current_train_items 236192.
I0304 19:31:36.976605 22579586809984 run.py:483] Algo bellman_ford step 7381 current loss 0.016090, current_train_items 236224.
I0304 19:31:37.001858 22579586809984 run.py:483] Algo bellman_ford step 7382 current loss 0.034136, current_train_items 236256.
I0304 19:31:37.033241 22579586809984 run.py:483] Algo bellman_ford step 7383 current loss 0.030215, current_train_items 236288.
I0304 19:31:37.065801 22579586809984 run.py:483] Algo bellman_ford step 7384 current loss 0.048396, current_train_items 236320.
I0304 19:31:37.085553 22579586809984 run.py:483] Algo bellman_ford step 7385 current loss 0.002131, current_train_items 236352.
I0304 19:31:37.102010 22579586809984 run.py:483] Algo bellman_ford step 7386 current loss 0.013614, current_train_items 236384.
I0304 19:31:37.126337 22579586809984 run.py:483] Algo bellman_ford step 7387 current loss 0.025672, current_train_items 236416.
I0304 19:31:37.158927 22579586809984 run.py:483] Algo bellman_ford step 7388 current loss 0.035490, current_train_items 236448.
I0304 19:31:37.191386 22579586809984 run.py:483] Algo bellman_ford step 7389 current loss 0.041243, current_train_items 236480.
I0304 19:31:37.211396 22579586809984 run.py:483] Algo bellman_ford step 7390 current loss 0.003482, current_train_items 236512.
I0304 19:31:37.228103 22579586809984 run.py:483] Algo bellman_ford step 7391 current loss 0.008606, current_train_items 236544.
I0304 19:31:37.251812 22579586809984 run.py:483] Algo bellman_ford step 7392 current loss 0.073954, current_train_items 236576.
I0304 19:31:37.282899 22579586809984 run.py:483] Algo bellman_ford step 7393 current loss 0.102892, current_train_items 236608.
I0304 19:31:37.316857 22579586809984 run.py:483] Algo bellman_ford step 7394 current loss 0.158592, current_train_items 236640.
I0304 19:31:37.336155 22579586809984 run.py:483] Algo bellman_ford step 7395 current loss 0.023810, current_train_items 236672.
I0304 19:31:37.352784 22579586809984 run.py:483] Algo bellman_ford step 7396 current loss 0.013419, current_train_items 236704.
I0304 19:31:37.377405 22579586809984 run.py:483] Algo bellman_ford step 7397 current loss 0.037235, current_train_items 236736.
I0304 19:31:37.408410 22579586809984 run.py:483] Algo bellman_ford step 7398 current loss 0.108433, current_train_items 236768.
I0304 19:31:37.440397 22579586809984 run.py:483] Algo bellman_ford step 7399 current loss 0.095935, current_train_items 236800.
I0304 19:31:37.459953 22579586809984 run.py:483] Algo bellman_ford step 7400 current loss 0.001641, current_train_items 236832.
I0304 19:31:37.467539 22579586809984 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0304 19:31:37.467642 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:31:37.484853 22579586809984 run.py:483] Algo bellman_ford step 7401 current loss 0.018127, current_train_items 236864.
I0304 19:31:37.510113 22579586809984 run.py:483] Algo bellman_ford step 7402 current loss 0.038063, current_train_items 236896.
I0304 19:31:37.542996 22579586809984 run.py:483] Algo bellman_ford step 7403 current loss 0.059123, current_train_items 236928.
I0304 19:31:37.577574 22579586809984 run.py:483] Algo bellman_ford step 7404 current loss 0.063739, current_train_items 236960.
I0304 19:31:37.597456 22579586809984 run.py:483] Algo bellman_ford step 7405 current loss 0.004096, current_train_items 236992.
I0304 19:31:37.613117 22579586809984 run.py:483] Algo bellman_ford step 7406 current loss 0.006190, current_train_items 237024.
I0304 19:31:37.637569 22579586809984 run.py:483] Algo bellman_ford step 7407 current loss 0.033595, current_train_items 237056.
I0304 19:31:37.670301 22579586809984 run.py:483] Algo bellman_ford step 7408 current loss 0.061629, current_train_items 237088.
I0304 19:31:37.702431 22579586809984 run.py:483] Algo bellman_ford step 7409 current loss 0.057998, current_train_items 237120.
I0304 19:31:37.722050 22579586809984 run.py:483] Algo bellman_ford step 7410 current loss 0.002016, current_train_items 237152.
I0304 19:31:37.738922 22579586809984 run.py:483] Algo bellman_ford step 7411 current loss 0.009973, current_train_items 237184.
I0304 19:31:37.764758 22579586809984 run.py:483] Algo bellman_ford step 7412 current loss 0.047785, current_train_items 237216.
I0304 19:31:37.797143 22579586809984 run.py:483] Algo bellman_ford step 7413 current loss 0.101396, current_train_items 237248.
I0304 19:31:37.829499 22579586809984 run.py:483] Algo bellman_ford step 7414 current loss 0.075273, current_train_items 237280.
I0304 19:31:37.849154 22579586809984 run.py:483] Algo bellman_ford step 7415 current loss 0.004921, current_train_items 237312.
I0304 19:31:37.865170 22579586809984 run.py:483] Algo bellman_ford step 7416 current loss 0.008468, current_train_items 237344.
I0304 19:31:37.889769 22579586809984 run.py:483] Algo bellman_ford step 7417 current loss 0.097033, current_train_items 237376.
I0304 19:31:37.921478 22579586809984 run.py:483] Algo bellman_ford step 7418 current loss 0.087064, current_train_items 237408.
I0304 19:31:37.954335 22579586809984 run.py:483] Algo bellman_ford step 7419 current loss 0.105953, current_train_items 237440.
I0304 19:31:37.974099 22579586809984 run.py:483] Algo bellman_ford step 7420 current loss 0.004535, current_train_items 237472.
I0304 19:31:37.990221 22579586809984 run.py:483] Algo bellman_ford step 7421 current loss 0.009951, current_train_items 237504.
I0304 19:31:38.014272 22579586809984 run.py:483] Algo bellman_ford step 7422 current loss 0.063532, current_train_items 237536.
I0304 19:31:38.046112 22579586809984 run.py:483] Algo bellman_ford step 7423 current loss 0.060043, current_train_items 237568.
I0304 19:31:38.079287 22579586809984 run.py:483] Algo bellman_ford step 7424 current loss 0.034360, current_train_items 237600.
I0304 19:31:38.099309 22579586809984 run.py:483] Algo bellman_ford step 7425 current loss 0.003421, current_train_items 237632.
I0304 19:31:38.115492 22579586809984 run.py:483] Algo bellman_ford step 7426 current loss 0.009202, current_train_items 237664.
I0304 19:31:38.140482 22579586809984 run.py:483] Algo bellman_ford step 7427 current loss 0.060071, current_train_items 237696.
I0304 19:31:38.172433 22579586809984 run.py:483] Algo bellman_ford step 7428 current loss 0.050041, current_train_items 237728.
I0304 19:31:38.206771 22579586809984 run.py:483] Algo bellman_ford step 7429 current loss 0.093956, current_train_items 237760.
I0304 19:31:38.226438 22579586809984 run.py:483] Algo bellman_ford step 7430 current loss 0.002656, current_train_items 237792.
I0304 19:31:38.242388 22579586809984 run.py:483] Algo bellman_ford step 7431 current loss 0.020587, current_train_items 237824.
I0304 19:31:38.267195 22579586809984 run.py:483] Algo bellman_ford step 7432 current loss 0.048681, current_train_items 237856.
I0304 19:31:38.299752 22579586809984 run.py:483] Algo bellman_ford step 7433 current loss 0.064541, current_train_items 237888.
I0304 19:31:38.331010 22579586809984 run.py:483] Algo bellman_ford step 7434 current loss 0.048387, current_train_items 237920.
I0304 19:31:38.350699 22579586809984 run.py:483] Algo bellman_ford step 7435 current loss 0.003303, current_train_items 237952.
I0304 19:31:38.366964 22579586809984 run.py:483] Algo bellman_ford step 7436 current loss 0.007764, current_train_items 237984.
I0304 19:31:38.392064 22579586809984 run.py:483] Algo bellman_ford step 7437 current loss 0.106665, current_train_items 238016.
I0304 19:31:38.423645 22579586809984 run.py:483] Algo bellman_ford step 7438 current loss 0.070593, current_train_items 238048.
I0304 19:31:38.458333 22579586809984 run.py:483] Algo bellman_ford step 7439 current loss 0.110549, current_train_items 238080.
I0304 19:31:38.477956 22579586809984 run.py:483] Algo bellman_ford step 7440 current loss 0.006763, current_train_items 238112.
I0304 19:31:38.494616 22579586809984 run.py:483] Algo bellman_ford step 7441 current loss 0.022458, current_train_items 238144.
I0304 19:31:38.518529 22579586809984 run.py:483] Algo bellman_ford step 7442 current loss 0.014951, current_train_items 238176.
I0304 19:31:38.551559 22579586809984 run.py:483] Algo bellman_ford step 7443 current loss 0.059367, current_train_items 238208.
I0304 19:31:38.585832 22579586809984 run.py:483] Algo bellman_ford step 7444 current loss 0.071838, current_train_items 238240.
I0304 19:31:38.605331 22579586809984 run.py:483] Algo bellman_ford step 7445 current loss 0.002841, current_train_items 238272.
I0304 19:31:38.621636 22579586809984 run.py:483] Algo bellman_ford step 7446 current loss 0.011248, current_train_items 238304.
I0304 19:31:38.645995 22579586809984 run.py:483] Algo bellman_ford step 7447 current loss 0.016583, current_train_items 238336.
I0304 19:31:38.677782 22579586809984 run.py:483] Algo bellman_ford step 7448 current loss 0.034164, current_train_items 238368.
I0304 19:31:38.712868 22579586809984 run.py:483] Algo bellman_ford step 7449 current loss 0.088372, current_train_items 238400.
I0304 19:31:38.732507 22579586809984 run.py:483] Algo bellman_ford step 7450 current loss 0.002025, current_train_items 238432.
I0304 19:31:38.740581 22579586809984 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0304 19:31:38.740692 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:38.757883 22579586809984 run.py:483] Algo bellman_ford step 7451 current loss 0.014375, current_train_items 238464.
I0304 19:31:38.783861 22579586809984 run.py:483] Algo bellman_ford step 7452 current loss 0.035328, current_train_items 238496.
I0304 19:31:38.815500 22579586809984 run.py:483] Algo bellman_ford step 7453 current loss 0.027453, current_train_items 238528.
I0304 19:31:38.848839 22579586809984 run.py:483] Algo bellman_ford step 7454 current loss 0.042205, current_train_items 238560.
I0304 19:31:38.868955 22579586809984 run.py:483] Algo bellman_ford step 7455 current loss 0.011387, current_train_items 238592.
I0304 19:31:38.885574 22579586809984 run.py:483] Algo bellman_ford step 7456 current loss 0.022131, current_train_items 238624.
I0304 19:31:38.909893 22579586809984 run.py:483] Algo bellman_ford step 7457 current loss 0.028590, current_train_items 238656.
I0304 19:31:38.942990 22579586809984 run.py:483] Algo bellman_ford step 7458 current loss 0.049409, current_train_items 238688.
I0304 19:31:38.976551 22579586809984 run.py:483] Algo bellman_ford step 7459 current loss 0.047104, current_train_items 238720.
I0304 19:31:38.996590 22579586809984 run.py:483] Algo bellman_ford step 7460 current loss 0.002512, current_train_items 238752.
I0304 19:31:39.013586 22579586809984 run.py:483] Algo bellman_ford step 7461 current loss 0.013550, current_train_items 238784.
I0304 19:31:39.036994 22579586809984 run.py:483] Algo bellman_ford step 7462 current loss 0.049942, current_train_items 238816.
I0304 19:31:39.068600 22579586809984 run.py:483] Algo bellman_ford step 7463 current loss 0.029280, current_train_items 238848.
I0304 19:31:39.102371 22579586809984 run.py:483] Algo bellman_ford step 7464 current loss 0.064635, current_train_items 238880.
I0304 19:31:39.122155 22579586809984 run.py:483] Algo bellman_ford step 7465 current loss 0.001991, current_train_items 238912.
I0304 19:31:39.138234 22579586809984 run.py:483] Algo bellman_ford step 7466 current loss 0.004628, current_train_items 238944.
I0304 19:31:39.162159 22579586809984 run.py:483] Algo bellman_ford step 7467 current loss 0.082569, current_train_items 238976.
I0304 19:31:39.195205 22579586809984 run.py:483] Algo bellman_ford step 7468 current loss 0.106041, current_train_items 239008.
I0304 19:31:39.227509 22579586809984 run.py:483] Algo bellman_ford step 7469 current loss 0.069960, current_train_items 239040.
I0304 19:31:39.247906 22579586809984 run.py:483] Algo bellman_ford step 7470 current loss 0.004026, current_train_items 239072.
I0304 19:31:39.264308 22579586809984 run.py:483] Algo bellman_ford step 7471 current loss 0.008070, current_train_items 239104.
I0304 19:31:39.287972 22579586809984 run.py:483] Algo bellman_ford step 7472 current loss 0.073351, current_train_items 239136.
I0304 19:31:39.320091 22579586809984 run.py:483] Algo bellman_ford step 7473 current loss 0.076794, current_train_items 239168.
I0304 19:31:39.355469 22579586809984 run.py:483] Algo bellman_ford step 7474 current loss 0.050040, current_train_items 239200.
I0304 19:31:39.375860 22579586809984 run.py:483] Algo bellman_ford step 7475 current loss 0.002297, current_train_items 239232.
I0304 19:31:39.392413 22579586809984 run.py:483] Algo bellman_ford step 7476 current loss 0.004671, current_train_items 239264.
I0304 19:31:39.415965 22579586809984 run.py:483] Algo bellman_ford step 7477 current loss 0.020241, current_train_items 239296.
I0304 19:31:39.448377 22579586809984 run.py:483] Algo bellman_ford step 7478 current loss 0.025651, current_train_items 239328.
I0304 19:31:39.484896 22579586809984 run.py:483] Algo bellman_ford step 7479 current loss 0.072301, current_train_items 239360.
I0304 19:31:39.504764 22579586809984 run.py:483] Algo bellman_ford step 7480 current loss 0.002246, current_train_items 239392.
I0304 19:31:39.521270 22579586809984 run.py:483] Algo bellman_ford step 7481 current loss 0.005129, current_train_items 239424.
I0304 19:31:39.546802 22579586809984 run.py:483] Algo bellman_ford step 7482 current loss 0.031492, current_train_items 239456.
I0304 19:31:39.579852 22579586809984 run.py:483] Algo bellman_ford step 7483 current loss 0.052369, current_train_items 239488.
I0304 19:31:39.614697 22579586809984 run.py:483] Algo bellman_ford step 7484 current loss 0.056886, current_train_items 239520.
I0304 19:31:39.634659 22579586809984 run.py:483] Algo bellman_ford step 7485 current loss 0.002393, current_train_items 239552.
I0304 19:31:39.651021 22579586809984 run.py:483] Algo bellman_ford step 7486 current loss 0.013326, current_train_items 239584.
I0304 19:31:39.674403 22579586809984 run.py:483] Algo bellman_ford step 7487 current loss 0.017726, current_train_items 239616.
I0304 19:31:39.706727 22579586809984 run.py:483] Algo bellman_ford step 7488 current loss 0.034261, current_train_items 239648.
I0304 19:31:39.742203 22579586809984 run.py:483] Algo bellman_ford step 7489 current loss 0.060571, current_train_items 239680.
I0304 19:31:39.762377 22579586809984 run.py:483] Algo bellman_ford step 7490 current loss 0.002019, current_train_items 239712.
I0304 19:31:39.778395 22579586809984 run.py:483] Algo bellman_ford step 7491 current loss 0.018609, current_train_items 239744.
I0304 19:31:39.801874 22579586809984 run.py:483] Algo bellman_ford step 7492 current loss 0.015152, current_train_items 239776.
I0304 19:31:39.833408 22579586809984 run.py:483] Algo bellman_ford step 7493 current loss 0.025657, current_train_items 239808.
I0304 19:31:39.866905 22579586809984 run.py:483] Algo bellman_ford step 7494 current loss 0.106200, current_train_items 239840.
I0304 19:31:39.886670 22579586809984 run.py:483] Algo bellman_ford step 7495 current loss 0.002648, current_train_items 239872.
I0304 19:31:39.903296 22579586809984 run.py:483] Algo bellman_ford step 7496 current loss 0.015169, current_train_items 239904.
I0304 19:31:39.927907 22579586809984 run.py:483] Algo bellman_ford step 7497 current loss 0.058913, current_train_items 239936.
I0304 19:31:39.959315 22579586809984 run.py:483] Algo bellman_ford step 7498 current loss 0.038450, current_train_items 239968.
I0304 19:31:39.992449 22579586809984 run.py:483] Algo bellman_ford step 7499 current loss 0.058759, current_train_items 240000.
I0304 19:31:40.012428 22579586809984 run.py:483] Algo bellman_ford step 7500 current loss 0.005833, current_train_items 240032.
I0304 19:31:40.020330 22579586809984 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0304 19:31:40.020435 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:40.037289 22579586809984 run.py:483] Algo bellman_ford step 7501 current loss 0.018575, current_train_items 240064.
I0304 19:31:40.062043 22579586809984 run.py:483] Algo bellman_ford step 7502 current loss 0.079377, current_train_items 240096.
I0304 19:31:40.094332 22579586809984 run.py:483] Algo bellman_ford step 7503 current loss 0.019172, current_train_items 240128.
I0304 19:31:40.128451 22579586809984 run.py:483] Algo bellman_ford step 7504 current loss 0.050588, current_train_items 240160.
I0304 19:31:40.148608 22579586809984 run.py:483] Algo bellman_ford step 7505 current loss 0.015238, current_train_items 240192.
I0304 19:31:40.164767 22579586809984 run.py:483] Algo bellman_ford step 7506 current loss 0.008224, current_train_items 240224.
I0304 19:31:40.188822 22579586809984 run.py:483] Algo bellman_ford step 7507 current loss 0.075965, current_train_items 240256.
I0304 19:31:40.220824 22579586809984 run.py:483] Algo bellman_ford step 7508 current loss 0.050410, current_train_items 240288.
I0304 19:31:40.253773 22579586809984 run.py:483] Algo bellman_ford step 7509 current loss 0.039858, current_train_items 240320.
I0304 19:31:40.273572 22579586809984 run.py:483] Algo bellman_ford step 7510 current loss 0.012856, current_train_items 240352.
I0304 19:31:40.290165 22579586809984 run.py:483] Algo bellman_ford step 7511 current loss 0.031610, current_train_items 240384.
I0304 19:31:40.313602 22579586809984 run.py:483] Algo bellman_ford step 7512 current loss 0.035358, current_train_items 240416.
I0304 19:31:40.345273 22579586809984 run.py:483] Algo bellman_ford step 7513 current loss 0.059617, current_train_items 240448.
I0304 19:31:40.378254 22579586809984 run.py:483] Algo bellman_ford step 7514 current loss 0.066973, current_train_items 240480.
I0304 19:31:40.398307 22579586809984 run.py:483] Algo bellman_ford step 7515 current loss 0.003450, current_train_items 240512.
I0304 19:31:40.414018 22579586809984 run.py:483] Algo bellman_ford step 7516 current loss 0.030175, current_train_items 240544.
I0304 19:31:40.439040 22579586809984 run.py:483] Algo bellman_ford step 7517 current loss 0.061072, current_train_items 240576.
I0304 19:31:40.471304 22579586809984 run.py:483] Algo bellman_ford step 7518 current loss 0.048641, current_train_items 240608.
I0304 19:31:40.505371 22579586809984 run.py:483] Algo bellman_ford step 7519 current loss 0.072270, current_train_items 240640.
I0304 19:31:40.525460 22579586809984 run.py:483] Algo bellman_ford step 7520 current loss 0.003361, current_train_items 240672.
I0304 19:31:40.541710 22579586809984 run.py:483] Algo bellman_ford step 7521 current loss 0.013540, current_train_items 240704.
I0304 19:31:40.565981 22579586809984 run.py:483] Algo bellman_ford step 7522 current loss 0.026687, current_train_items 240736.
I0304 19:31:40.598109 22579586809984 run.py:483] Algo bellman_ford step 7523 current loss 0.041024, current_train_items 240768.
I0304 19:31:40.629895 22579586809984 run.py:483] Algo bellman_ford step 7524 current loss 0.030976, current_train_items 240800.
I0304 19:31:40.649902 22579586809984 run.py:483] Algo bellman_ford step 7525 current loss 0.013814, current_train_items 240832.
I0304 19:31:40.666063 22579586809984 run.py:483] Algo bellman_ford step 7526 current loss 0.030729, current_train_items 240864.
I0304 19:31:40.691082 22579586809984 run.py:483] Algo bellman_ford step 7527 current loss 0.046406, current_train_items 240896.
I0304 19:31:40.722330 22579586809984 run.py:483] Algo bellman_ford step 7528 current loss 0.058628, current_train_items 240928.
I0304 19:31:40.756373 22579586809984 run.py:483] Algo bellman_ford step 7529 current loss 0.094965, current_train_items 240960.
I0304 19:31:40.776210 22579586809984 run.py:483] Algo bellman_ford step 7530 current loss 0.006235, current_train_items 240992.
I0304 19:31:40.792299 22579586809984 run.py:483] Algo bellman_ford step 7531 current loss 0.006168, current_train_items 241024.
I0304 19:31:40.815781 22579586809984 run.py:483] Algo bellman_ford step 7532 current loss 0.025866, current_train_items 241056.
I0304 19:31:40.849122 22579586809984 run.py:483] Algo bellman_ford step 7533 current loss 0.056597, current_train_items 241088.
I0304 19:31:40.883428 22579586809984 run.py:483] Algo bellman_ford step 7534 current loss 0.058375, current_train_items 241120.
I0304 19:31:40.903463 22579586809984 run.py:483] Algo bellman_ford step 7535 current loss 0.004267, current_train_items 241152.
I0304 19:31:40.919899 22579586809984 run.py:483] Algo bellman_ford step 7536 current loss 0.007452, current_train_items 241184.
I0304 19:31:40.944415 22579586809984 run.py:483] Algo bellman_ford step 7537 current loss 0.045513, current_train_items 241216.
I0304 19:31:40.977077 22579586809984 run.py:483] Algo bellman_ford step 7538 current loss 0.032017, current_train_items 241248.
I0304 19:31:41.010406 22579586809984 run.py:483] Algo bellman_ford step 7539 current loss 0.022707, current_train_items 241280.
I0304 19:31:41.030309 22579586809984 run.py:483] Algo bellman_ford step 7540 current loss 0.002955, current_train_items 241312.
I0304 19:31:41.047424 22579586809984 run.py:483] Algo bellman_ford step 7541 current loss 0.015704, current_train_items 241344.
I0304 19:31:41.072932 22579586809984 run.py:483] Algo bellman_ford step 7542 current loss 0.063877, current_train_items 241376.
I0304 19:31:41.106001 22579586809984 run.py:483] Algo bellman_ford step 7543 current loss 0.040731, current_train_items 241408.
I0304 19:31:41.139832 22579586809984 run.py:483] Algo bellman_ford step 7544 current loss 0.038754, current_train_items 241440.
I0304 19:31:41.159520 22579586809984 run.py:483] Algo bellman_ford step 7545 current loss 0.004756, current_train_items 241472.
I0304 19:31:41.175852 22579586809984 run.py:483] Algo bellman_ford step 7546 current loss 0.011650, current_train_items 241504.
I0304 19:31:41.200977 22579586809984 run.py:483] Algo bellman_ford step 7547 current loss 0.061945, current_train_items 241536.
I0304 19:31:41.233837 22579586809984 run.py:483] Algo bellman_ford step 7548 current loss 0.067951, current_train_items 241568.
I0304 19:31:41.267585 22579586809984 run.py:483] Algo bellman_ford step 7549 current loss 0.044139, current_train_items 241600.
I0304 19:31:41.287470 22579586809984 run.py:483] Algo bellman_ford step 7550 current loss 0.016213, current_train_items 241632.
I0304 19:31:41.295466 22579586809984 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0304 19:31:41.295570 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:31:41.312930 22579586809984 run.py:483] Algo bellman_ford step 7551 current loss 0.016685, current_train_items 241664.
I0304 19:31:41.337527 22579586809984 run.py:483] Algo bellman_ford step 7552 current loss 0.013546, current_train_items 241696.
I0304 19:31:41.370018 22579586809984 run.py:483] Algo bellman_ford step 7553 current loss 0.064882, current_train_items 241728.
I0304 19:31:41.403919 22579586809984 run.py:483] Algo bellman_ford step 7554 current loss 0.063422, current_train_items 241760.
I0304 19:31:41.423642 22579586809984 run.py:483] Algo bellman_ford step 7555 current loss 0.002417, current_train_items 241792.
I0304 19:31:41.439844 22579586809984 run.py:483] Algo bellman_ford step 7556 current loss 0.057965, current_train_items 241824.
I0304 19:31:41.464257 22579586809984 run.py:483] Algo bellman_ford step 7557 current loss 0.023387, current_train_items 241856.
I0304 19:31:41.497385 22579586809984 run.py:483] Algo bellman_ford step 7558 current loss 0.092809, current_train_items 241888.
I0304 19:31:41.529866 22579586809984 run.py:483] Algo bellman_ford step 7559 current loss 0.050955, current_train_items 241920.
I0304 19:31:41.550022 22579586809984 run.py:483] Algo bellman_ford step 7560 current loss 0.002634, current_train_items 241952.
I0304 19:31:41.567198 22579586809984 run.py:483] Algo bellman_ford step 7561 current loss 0.055887, current_train_items 241984.
I0304 19:31:41.590535 22579586809984 run.py:483] Algo bellman_ford step 7562 current loss 0.050589, current_train_items 242016.
I0304 19:31:41.622663 22579586809984 run.py:483] Algo bellman_ford step 7563 current loss 0.048767, current_train_items 242048.
I0304 19:31:41.656505 22579586809984 run.py:483] Algo bellman_ford step 7564 current loss 0.050685, current_train_items 242080.
I0304 19:31:41.675944 22579586809984 run.py:483] Algo bellman_ford step 7565 current loss 0.003048, current_train_items 242112.
I0304 19:31:41.692004 22579586809984 run.py:483] Algo bellman_ford step 7566 current loss 0.079994, current_train_items 242144.
I0304 19:31:41.716239 22579586809984 run.py:483] Algo bellman_ford step 7567 current loss 0.040527, current_train_items 242176.
I0304 19:31:41.747673 22579586809984 run.py:483] Algo bellman_ford step 7568 current loss 0.048504, current_train_items 242208.
I0304 19:31:41.780262 22579586809984 run.py:483] Algo bellman_ford step 7569 current loss 0.086227, current_train_items 242240.
I0304 19:31:41.800213 22579586809984 run.py:483] Algo bellman_ford step 7570 current loss 0.004107, current_train_items 242272.
I0304 19:31:41.816734 22579586809984 run.py:483] Algo bellman_ford step 7571 current loss 0.015742, current_train_items 242304.
I0304 19:31:41.840269 22579586809984 run.py:483] Algo bellman_ford step 7572 current loss 0.020247, current_train_items 242336.
I0304 19:31:41.870976 22579586809984 run.py:483] Algo bellman_ford step 7573 current loss 0.027350, current_train_items 242368.
I0304 19:31:41.902199 22579586809984 run.py:483] Algo bellman_ford step 7574 current loss 0.023016, current_train_items 242400.
I0304 19:31:41.922040 22579586809984 run.py:483] Algo bellman_ford step 7575 current loss 0.004475, current_train_items 242432.
I0304 19:31:41.939009 22579586809984 run.py:483] Algo bellman_ford step 7576 current loss 0.031109, current_train_items 242464.
I0304 19:31:41.961941 22579586809984 run.py:483] Algo bellman_ford step 7577 current loss 0.057115, current_train_items 242496.
I0304 19:31:41.994144 22579586809984 run.py:483] Algo bellman_ford step 7578 current loss 0.051749, current_train_items 242528.
I0304 19:31:42.029129 22579586809984 run.py:483] Algo bellman_ford step 7579 current loss 0.089579, current_train_items 242560.
I0304 19:31:42.048746 22579586809984 run.py:483] Algo bellman_ford step 7580 current loss 0.004387, current_train_items 242592.
I0304 19:31:42.065258 22579586809984 run.py:483] Algo bellman_ford step 7581 current loss 0.044217, current_train_items 242624.
I0304 19:31:42.089717 22579586809984 run.py:483] Algo bellman_ford step 7582 current loss 0.048614, current_train_items 242656.
I0304 19:31:42.121363 22579586809984 run.py:483] Algo bellman_ford step 7583 current loss 0.059313, current_train_items 242688.
I0304 19:31:42.154094 22579586809984 run.py:483] Algo bellman_ford step 7584 current loss 0.056216, current_train_items 242720.
I0304 19:31:42.174036 22579586809984 run.py:483] Algo bellman_ford step 7585 current loss 0.002648, current_train_items 242752.
I0304 19:31:42.190116 22579586809984 run.py:483] Algo bellman_ford step 7586 current loss 0.030245, current_train_items 242784.
I0304 19:31:42.213760 22579586809984 run.py:483] Algo bellman_ford step 7587 current loss 0.046806, current_train_items 242816.
I0304 19:31:42.243958 22579586809984 run.py:483] Algo bellman_ford step 7588 current loss 0.031307, current_train_items 242848.
I0304 19:31:42.278097 22579586809984 run.py:483] Algo bellman_ford step 7589 current loss 0.046117, current_train_items 242880.
I0304 19:31:42.297944 22579586809984 run.py:483] Algo bellman_ford step 7590 current loss 0.014013, current_train_items 242912.
I0304 19:31:42.314081 22579586809984 run.py:483] Algo bellman_ford step 7591 current loss 0.010904, current_train_items 242944.
I0304 19:31:42.339201 22579586809984 run.py:483] Algo bellman_ford step 7592 current loss 0.102547, current_train_items 242976.
I0304 19:31:42.369618 22579586809984 run.py:483] Algo bellman_ford step 7593 current loss 0.036763, current_train_items 243008.
I0304 19:31:42.403383 22579586809984 run.py:483] Algo bellman_ford step 7594 current loss 0.090046, current_train_items 243040.
I0304 19:31:42.422883 22579586809984 run.py:483] Algo bellman_ford step 7595 current loss 0.004298, current_train_items 243072.
I0304 19:31:42.439037 22579586809984 run.py:483] Algo bellman_ford step 7596 current loss 0.010423, current_train_items 243104.
I0304 19:31:42.463464 22579586809984 run.py:483] Algo bellman_ford step 7597 current loss 0.034140, current_train_items 243136.
I0304 19:31:42.496020 22579586809984 run.py:483] Algo bellman_ford step 7598 current loss 0.046817, current_train_items 243168.
I0304 19:31:42.526848 22579586809984 run.py:483] Algo bellman_ford step 7599 current loss 0.047613, current_train_items 243200.
I0304 19:31:42.546692 22579586809984 run.py:483] Algo bellman_ford step 7600 current loss 0.006764, current_train_items 243232.
I0304 19:31:42.554598 22579586809984 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0304 19:31:42.554712 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:31:42.571887 22579586809984 run.py:483] Algo bellman_ford step 7601 current loss 0.012945, current_train_items 243264.
I0304 19:31:42.598087 22579586809984 run.py:483] Algo bellman_ford step 7602 current loss 0.110456, current_train_items 243296.
I0304 19:31:42.630173 22579586809984 run.py:483] Algo bellman_ford step 7603 current loss 0.025779, current_train_items 243328.
I0304 19:31:42.664828 22579586809984 run.py:483] Algo bellman_ford step 7604 current loss 0.095674, current_train_items 243360.
I0304 19:31:42.684712 22579586809984 run.py:483] Algo bellman_ford step 7605 current loss 0.053437, current_train_items 243392.
I0304 19:31:42.700174 22579586809984 run.py:483] Algo bellman_ford step 7606 current loss 0.012909, current_train_items 243424.
I0304 19:31:42.724278 22579586809984 run.py:483] Algo bellman_ford step 7607 current loss 0.025290, current_train_items 243456.
I0304 19:31:42.756734 22579586809984 run.py:483] Algo bellman_ford step 7608 current loss 0.035462, current_train_items 243488.
I0304 19:31:42.791552 22579586809984 run.py:483] Algo bellman_ford step 7609 current loss 0.056704, current_train_items 243520.
I0304 19:31:42.811032 22579586809984 run.py:483] Algo bellman_ford step 7610 current loss 0.001989, current_train_items 243552.
I0304 19:31:42.827727 22579586809984 run.py:483] Algo bellman_ford step 7611 current loss 0.014504, current_train_items 243584.
I0304 19:31:42.851830 22579586809984 run.py:483] Algo bellman_ford step 7612 current loss 0.037214, current_train_items 243616.
I0304 19:31:42.882062 22579586809984 run.py:483] Algo bellman_ford step 7613 current loss 0.039652, current_train_items 243648.
I0304 19:31:42.915036 22579586809984 run.py:483] Algo bellman_ford step 7614 current loss 0.053875, current_train_items 243680.
I0304 19:31:42.934720 22579586809984 run.py:483] Algo bellman_ford step 7615 current loss 0.001936, current_train_items 243712.
I0304 19:31:42.951107 22579586809984 run.py:483] Algo bellman_ford step 7616 current loss 0.004554, current_train_items 243744.
I0304 19:31:42.975340 22579586809984 run.py:483] Algo bellman_ford step 7617 current loss 0.039775, current_train_items 243776.
I0304 19:31:43.006407 22579586809984 run.py:483] Algo bellman_ford step 7618 current loss 0.044386, current_train_items 243808.
I0304 19:31:43.040349 22579586809984 run.py:483] Algo bellman_ford step 7619 current loss 0.145681, current_train_items 243840.
I0304 19:31:43.059915 22579586809984 run.py:483] Algo bellman_ford step 7620 current loss 0.004409, current_train_items 243872.
I0304 19:31:43.076459 22579586809984 run.py:483] Algo bellman_ford step 7621 current loss 0.009570, current_train_items 243904.
I0304 19:31:43.100734 22579586809984 run.py:483] Algo bellman_ford step 7622 current loss 0.036030, current_train_items 243936.
I0304 19:31:43.133725 22579586809984 run.py:483] Algo bellman_ford step 7623 current loss 0.041982, current_train_items 243968.
I0304 19:31:43.167095 22579586809984 run.py:483] Algo bellman_ford step 7624 current loss 0.077496, current_train_items 244000.
I0304 19:31:43.186724 22579586809984 run.py:483] Algo bellman_ford step 7625 current loss 0.002894, current_train_items 244032.
I0304 19:31:43.202270 22579586809984 run.py:483] Algo bellman_ford step 7626 current loss 0.013542, current_train_items 244064.
I0304 19:31:43.227142 22579586809984 run.py:483] Algo bellman_ford step 7627 current loss 0.058658, current_train_items 244096.
I0304 19:31:43.259753 22579586809984 run.py:483] Algo bellman_ford step 7628 current loss 0.045394, current_train_items 244128.
I0304 19:31:43.292620 22579586809984 run.py:483] Algo bellman_ford step 7629 current loss 0.037144, current_train_items 244160.
I0304 19:31:43.312009 22579586809984 run.py:483] Algo bellman_ford step 7630 current loss 0.022396, current_train_items 244192.
I0304 19:31:43.328606 22579586809984 run.py:483] Algo bellman_ford step 7631 current loss 0.019473, current_train_items 244224.
I0304 19:31:43.352454 22579586809984 run.py:483] Algo bellman_ford step 7632 current loss 0.036139, current_train_items 244256.
I0304 19:31:43.384816 22579586809984 run.py:483] Algo bellman_ford step 7633 current loss 0.028805, current_train_items 244288.
I0304 19:31:43.418872 22579586809984 run.py:483] Algo bellman_ford step 7634 current loss 0.065509, current_train_items 244320.
I0304 19:31:43.438545 22579586809984 run.py:483] Algo bellman_ford step 7635 current loss 0.023585, current_train_items 244352.
I0304 19:31:43.454595 22579586809984 run.py:483] Algo bellman_ford step 7636 current loss 0.007332, current_train_items 244384.
I0304 19:31:43.478828 22579586809984 run.py:483] Algo bellman_ford step 7637 current loss 0.035434, current_train_items 244416.
I0304 19:31:43.509331 22579586809984 run.py:483] Algo bellman_ford step 7638 current loss 0.052266, current_train_items 244448.
I0304 19:31:43.545373 22579586809984 run.py:483] Algo bellman_ford step 7639 current loss 0.055684, current_train_items 244480.
I0304 19:31:43.565088 22579586809984 run.py:483] Algo bellman_ford step 7640 current loss 0.002427, current_train_items 244512.
I0304 19:31:43.581190 22579586809984 run.py:483] Algo bellman_ford step 7641 current loss 0.014596, current_train_items 244544.
I0304 19:31:43.604389 22579586809984 run.py:483] Algo bellman_ford step 7642 current loss 0.037568, current_train_items 244576.
I0304 19:31:43.638491 22579586809984 run.py:483] Algo bellman_ford step 7643 current loss 0.091742, current_train_items 244608.
I0304 19:31:43.673694 22579586809984 run.py:483] Algo bellman_ford step 7644 current loss 0.071216, current_train_items 244640.
I0304 19:31:43.693464 22579586809984 run.py:483] Algo bellman_ford step 7645 current loss 0.005558, current_train_items 244672.
I0304 19:31:43.709841 22579586809984 run.py:483] Algo bellman_ford step 7646 current loss 0.013543, current_train_items 244704.
I0304 19:31:43.734729 22579586809984 run.py:483] Algo bellman_ford step 7647 current loss 0.034017, current_train_items 244736.
I0304 19:31:43.768113 22579586809984 run.py:483] Algo bellman_ford step 7648 current loss 0.061485, current_train_items 244768.
I0304 19:31:43.803593 22579586809984 run.py:483] Algo bellman_ford step 7649 current loss 0.091139, current_train_items 244800.
I0304 19:31:43.823158 22579586809984 run.py:483] Algo bellman_ford step 7650 current loss 0.002785, current_train_items 244832.
I0304 19:31:43.830942 22579586809984 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0304 19:31:43.831048 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:43.847658 22579586809984 run.py:483] Algo bellman_ford step 7651 current loss 0.003929, current_train_items 244864.
I0304 19:31:43.872971 22579586809984 run.py:483] Algo bellman_ford step 7652 current loss 0.077185, current_train_items 244896.
I0304 19:31:43.904979 22579586809984 run.py:483] Algo bellman_ford step 7653 current loss 0.027981, current_train_items 244928.
I0304 19:31:43.938840 22579586809984 run.py:483] Algo bellman_ford step 7654 current loss 0.085244, current_train_items 244960.
I0304 19:31:43.958805 22579586809984 run.py:483] Algo bellman_ford step 7655 current loss 0.003621, current_train_items 244992.
I0304 19:31:43.974762 22579586809984 run.py:483] Algo bellman_ford step 7656 current loss 0.008795, current_train_items 245024.
I0304 19:31:43.999162 22579586809984 run.py:483] Algo bellman_ford step 7657 current loss 0.037929, current_train_items 245056.
I0304 19:31:44.031696 22579586809984 run.py:483] Algo bellman_ford step 7658 current loss 0.053718, current_train_items 245088.
I0304 19:31:44.066331 22579586809984 run.py:483] Algo bellman_ford step 7659 current loss 0.086539, current_train_items 245120.
I0304 19:31:44.086299 22579586809984 run.py:483] Algo bellman_ford step 7660 current loss 0.002955, current_train_items 245152.
I0304 19:31:44.102862 22579586809984 run.py:483] Algo bellman_ford step 7661 current loss 0.011256, current_train_items 245184.
I0304 19:31:44.126592 22579586809984 run.py:483] Algo bellman_ford step 7662 current loss 0.062995, current_train_items 245216.
I0304 19:31:44.157739 22579586809984 run.py:483] Algo bellman_ford step 7663 current loss 0.119759, current_train_items 245248.
I0304 19:31:44.191564 22579586809984 run.py:483] Algo bellman_ford step 7664 current loss 0.084440, current_train_items 245280.
I0304 19:31:44.211250 22579586809984 run.py:483] Algo bellman_ford step 7665 current loss 0.008667, current_train_items 245312.
I0304 19:31:44.227556 22579586809984 run.py:483] Algo bellman_ford step 7666 current loss 0.026258, current_train_items 245344.
I0304 19:31:44.251328 22579586809984 run.py:483] Algo bellman_ford step 7667 current loss 0.057971, current_train_items 245376.
I0304 19:31:44.285302 22579586809984 run.py:483] Algo bellman_ford step 7668 current loss 0.050026, current_train_items 245408.
I0304 19:31:44.319642 22579586809984 run.py:483] Algo bellman_ford step 7669 current loss 0.050780, current_train_items 245440.
I0304 19:31:44.339322 22579586809984 run.py:483] Algo bellman_ford step 7670 current loss 0.019565, current_train_items 245472.
I0304 19:31:44.356058 22579586809984 run.py:483] Algo bellman_ford step 7671 current loss 0.047539, current_train_items 245504.
I0304 19:31:44.379645 22579586809984 run.py:483] Algo bellman_ford step 7672 current loss 0.054151, current_train_items 245536.
I0304 19:31:44.411726 22579586809984 run.py:483] Algo bellman_ford step 7673 current loss 0.050350, current_train_items 245568.
I0304 19:31:44.446012 22579586809984 run.py:483] Algo bellman_ford step 7674 current loss 0.102967, current_train_items 245600.
I0304 19:31:44.465873 22579586809984 run.py:483] Algo bellman_ford step 7675 current loss 0.003097, current_train_items 245632.
I0304 19:31:44.482795 22579586809984 run.py:483] Algo bellman_ford step 7676 current loss 0.035917, current_train_items 245664.
I0304 19:31:44.507911 22579586809984 run.py:483] Algo bellman_ford step 7677 current loss 0.029676, current_train_items 245696.
I0304 19:31:44.539277 22579586809984 run.py:483] Algo bellman_ford step 7678 current loss 0.030897, current_train_items 245728.
I0304 19:31:44.574155 22579586809984 run.py:483] Algo bellman_ford step 7679 current loss 0.046668, current_train_items 245760.
I0304 19:31:44.593769 22579586809984 run.py:483] Algo bellman_ford step 7680 current loss 0.002345, current_train_items 245792.
I0304 19:31:44.610254 22579586809984 run.py:483] Algo bellman_ford step 7681 current loss 0.021155, current_train_items 245824.
I0304 19:31:44.633955 22579586809984 run.py:483] Algo bellman_ford step 7682 current loss 0.027199, current_train_items 245856.
I0304 19:31:44.665871 22579586809984 run.py:483] Algo bellman_ford step 7683 current loss 0.025987, current_train_items 245888.
I0304 19:31:44.700513 22579586809984 run.py:483] Algo bellman_ford step 7684 current loss 0.091006, current_train_items 245920.
I0304 19:31:44.720770 22579586809984 run.py:483] Algo bellman_ford step 7685 current loss 0.009899, current_train_items 245952.
I0304 19:31:44.737387 22579586809984 run.py:483] Algo bellman_ford step 7686 current loss 0.022380, current_train_items 245984.
I0304 19:31:44.760509 22579586809984 run.py:483] Algo bellman_ford step 7687 current loss 0.065518, current_train_items 246016.
I0304 19:31:44.793379 22579586809984 run.py:483] Algo bellman_ford step 7688 current loss 0.028935, current_train_items 246048.
I0304 19:31:44.827363 22579586809984 run.py:483] Algo bellman_ford step 7689 current loss 0.062220, current_train_items 246080.
I0304 19:31:44.847548 22579586809984 run.py:483] Algo bellman_ford step 7690 current loss 0.009617, current_train_items 246112.
I0304 19:31:44.863983 22579586809984 run.py:483] Algo bellman_ford step 7691 current loss 0.013579, current_train_items 246144.
I0304 19:31:44.887484 22579586809984 run.py:483] Algo bellman_ford step 7692 current loss 0.026373, current_train_items 246176.
I0304 19:31:44.920796 22579586809984 run.py:483] Algo bellman_ford step 7693 current loss 0.054669, current_train_items 246208.
I0304 19:31:44.954154 22579586809984 run.py:483] Algo bellman_ford step 7694 current loss 0.045844, current_train_items 246240.
I0304 19:31:44.973644 22579586809984 run.py:483] Algo bellman_ford step 7695 current loss 0.005290, current_train_items 246272.
I0304 19:31:44.990234 22579586809984 run.py:483] Algo bellman_ford step 7696 current loss 0.013120, current_train_items 246304.
I0304 19:31:45.014941 22579586809984 run.py:483] Algo bellman_ford step 7697 current loss 0.034805, current_train_items 246336.
I0304 19:31:45.047863 22579586809984 run.py:483] Algo bellman_ford step 7698 current loss 0.041943, current_train_items 246368.
I0304 19:31:45.083452 22579586809984 run.py:483] Algo bellman_ford step 7699 current loss 0.057949, current_train_items 246400.
I0304 19:31:45.103677 22579586809984 run.py:483] Algo bellman_ford step 7700 current loss 0.008963, current_train_items 246432.
I0304 19:31:45.111701 22579586809984 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0304 19:31:45.111808 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0304 19:31:45.129216 22579586809984 run.py:483] Algo bellman_ford step 7701 current loss 0.016157, current_train_items 246464.
I0304 19:31:45.153882 22579586809984 run.py:483] Algo bellman_ford step 7702 current loss 0.025769, current_train_items 246496.
I0304 19:31:45.185297 22579586809984 run.py:483] Algo bellman_ford step 7703 current loss 0.027334, current_train_items 246528.
I0304 19:31:45.219369 22579586809984 run.py:483] Algo bellman_ford step 7704 current loss 0.048328, current_train_items 246560.
I0304 19:31:45.239436 22579586809984 run.py:483] Algo bellman_ford step 7705 current loss 0.006383, current_train_items 246592.
I0304 19:31:45.255878 22579586809984 run.py:483] Algo bellman_ford step 7706 current loss 0.009902, current_train_items 246624.
I0304 19:31:45.281124 22579586809984 run.py:483] Algo bellman_ford step 7707 current loss 0.026052, current_train_items 246656.
I0304 19:31:45.312206 22579586809984 run.py:483] Algo bellman_ford step 7708 current loss 0.058115, current_train_items 246688.
I0304 19:31:45.344858 22579586809984 run.py:483] Algo bellman_ford step 7709 current loss 0.029209, current_train_items 246720.
I0304 19:31:45.365078 22579586809984 run.py:483] Algo bellman_ford step 7710 current loss 0.003448, current_train_items 246752.
I0304 19:31:45.381481 22579586809984 run.py:483] Algo bellman_ford step 7711 current loss 0.005722, current_train_items 246784.
I0304 19:31:45.405699 22579586809984 run.py:483] Algo bellman_ford step 7712 current loss 0.029530, current_train_items 246816.
I0304 19:31:45.436867 22579586809984 run.py:483] Algo bellman_ford step 7713 current loss 0.039052, current_train_items 246848.
I0304 19:31:45.470793 22579586809984 run.py:483] Algo bellman_ford step 7714 current loss 0.041220, current_train_items 246880.
I0304 19:31:45.490322 22579586809984 run.py:483] Algo bellman_ford step 7715 current loss 0.028338, current_train_items 246912.
I0304 19:31:45.506484 22579586809984 run.py:483] Algo bellman_ford step 7716 current loss 0.006513, current_train_items 246944.
I0304 19:31:45.531062 22579586809984 run.py:483] Algo bellman_ford step 7717 current loss 0.068092, current_train_items 246976.
I0304 19:31:45.562762 22579586809984 run.py:483] Algo bellman_ford step 7718 current loss 0.083180, current_train_items 247008.
I0304 19:31:45.595124 22579586809984 run.py:483] Algo bellman_ford step 7719 current loss 0.057061, current_train_items 247040.
I0304 19:31:45.614999 22579586809984 run.py:483] Algo bellman_ford step 7720 current loss 0.002504, current_train_items 247072.
I0304 19:31:45.631107 22579586809984 run.py:483] Algo bellman_ford step 7721 current loss 0.016435, current_train_items 247104.
I0304 19:31:45.655905 22579586809984 run.py:483] Algo bellman_ford step 7722 current loss 0.030923, current_train_items 247136.
I0304 19:31:45.688761 22579586809984 run.py:483] Algo bellman_ford step 7723 current loss 0.070248, current_train_items 247168.
I0304 19:31:45.723130 22579586809984 run.py:483] Algo bellman_ford step 7724 current loss 0.083331, current_train_items 247200.
I0304 19:31:45.742717 22579586809984 run.py:483] Algo bellman_ford step 7725 current loss 0.002563, current_train_items 247232.
I0304 19:31:45.758974 22579586809984 run.py:483] Algo bellman_ford step 7726 current loss 0.019865, current_train_items 247264.
I0304 19:31:45.782111 22579586809984 run.py:483] Algo bellman_ford step 7727 current loss 0.021768, current_train_items 247296.
I0304 19:31:45.813740 22579586809984 run.py:483] Algo bellman_ford step 7728 current loss 0.033579, current_train_items 247328.
I0304 19:31:45.846491 22579586809984 run.py:483] Algo bellman_ford step 7729 current loss 0.094091, current_train_items 247360.
I0304 19:31:45.865987 22579586809984 run.py:483] Algo bellman_ford step 7730 current loss 0.009983, current_train_items 247392.
I0304 19:31:45.881962 22579586809984 run.py:483] Algo bellman_ford step 7731 current loss 0.006137, current_train_items 247424.
I0304 19:31:45.905624 22579586809984 run.py:483] Algo bellman_ford step 7732 current loss 0.030751, current_train_items 247456.
I0304 19:31:45.937903 22579586809984 run.py:483] Algo bellman_ford step 7733 current loss 0.045471, current_train_items 247488.
I0304 19:31:45.970095 22579586809984 run.py:483] Algo bellman_ford step 7734 current loss 0.074217, current_train_items 247520.
I0304 19:31:45.989500 22579586809984 run.py:483] Algo bellman_ford step 7735 current loss 0.006274, current_train_items 247552.
I0304 19:31:46.005836 22579586809984 run.py:483] Algo bellman_ford step 7736 current loss 0.027128, current_train_items 247584.
I0304 19:31:46.030037 22579586809984 run.py:483] Algo bellman_ford step 7737 current loss 0.027855, current_train_items 247616.
I0304 19:31:46.061507 22579586809984 run.py:483] Algo bellman_ford step 7738 current loss 0.048372, current_train_items 247648.
I0304 19:31:46.095599 22579586809984 run.py:483] Algo bellman_ford step 7739 current loss 0.058804, current_train_items 247680.
I0304 19:31:46.114950 22579586809984 run.py:483] Algo bellman_ford step 7740 current loss 0.008131, current_train_items 247712.
I0304 19:31:46.131400 22579586809984 run.py:483] Algo bellman_ford step 7741 current loss 0.014063, current_train_items 247744.
I0304 19:31:46.156708 22579586809984 run.py:483] Algo bellman_ford step 7742 current loss 0.063211, current_train_items 247776.
I0304 19:31:46.189050 22579586809984 run.py:483] Algo bellman_ford step 7743 current loss 0.053308, current_train_items 247808.
I0304 19:31:46.222843 22579586809984 run.py:483] Algo bellman_ford step 7744 current loss 0.090055, current_train_items 247840.
I0304 19:31:46.242411 22579586809984 run.py:483] Algo bellman_ford step 7745 current loss 0.059711, current_train_items 247872.
I0304 19:31:46.258551 22579586809984 run.py:483] Algo bellman_ford step 7746 current loss 0.009683, current_train_items 247904.
I0304 19:31:46.282491 22579586809984 run.py:483] Algo bellman_ford step 7747 current loss 0.030772, current_train_items 247936.
I0304 19:31:46.316086 22579586809984 run.py:483] Algo bellman_ford step 7748 current loss 0.075648, current_train_items 247968.
I0304 19:31:46.350352 22579586809984 run.py:483] Algo bellman_ford step 7749 current loss 0.043934, current_train_items 248000.
I0304 19:31:46.370260 22579586809984 run.py:483] Algo bellman_ford step 7750 current loss 0.007462, current_train_items 248032.
I0304 19:31:46.378376 22579586809984 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0304 19:31:46.378483 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:31:46.395714 22579586809984 run.py:483] Algo bellman_ford step 7751 current loss 0.019143, current_train_items 248064.
I0304 19:31:46.420129 22579586809984 run.py:483] Algo bellman_ford step 7752 current loss 0.039742, current_train_items 248096.
I0304 19:31:46.452061 22579586809984 run.py:483] Algo bellman_ford step 7753 current loss 0.040416, current_train_items 248128.
I0304 19:31:46.487770 22579586809984 run.py:483] Algo bellman_ford step 7754 current loss 0.056966, current_train_items 248160.
I0304 19:31:46.507991 22579586809984 run.py:483] Algo bellman_ford step 7755 current loss 0.003219, current_train_items 248192.
I0304 19:31:46.523656 22579586809984 run.py:483] Algo bellman_ford step 7756 current loss 0.022647, current_train_items 248224.
I0304 19:31:46.548055 22579586809984 run.py:483] Algo bellman_ford step 7757 current loss 0.020175, current_train_items 248256.
I0304 19:31:46.579025 22579586809984 run.py:483] Algo bellman_ford step 7758 current loss 0.053013, current_train_items 248288.
I0304 19:31:46.612718 22579586809984 run.py:483] Algo bellman_ford step 7759 current loss 0.045639, current_train_items 248320.
I0304 19:31:46.632622 22579586809984 run.py:483] Algo bellman_ford step 7760 current loss 0.003402, current_train_items 248352.
I0304 19:31:46.649021 22579586809984 run.py:483] Algo bellman_ford step 7761 current loss 0.021660, current_train_items 248384.
I0304 19:31:46.673432 22579586809984 run.py:483] Algo bellman_ford step 7762 current loss 0.031127, current_train_items 248416.
I0304 19:31:46.705310 22579586809984 run.py:483] Algo bellman_ford step 7763 current loss 0.034408, current_train_items 248448.
I0304 19:31:46.737893 22579586809984 run.py:483] Algo bellman_ford step 7764 current loss 0.043572, current_train_items 248480.
I0304 19:31:46.757499 22579586809984 run.py:483] Algo bellman_ford step 7765 current loss 0.002826, current_train_items 248512.
I0304 19:31:46.773878 22579586809984 run.py:483] Algo bellman_ford step 7766 current loss 0.019968, current_train_items 248544.
I0304 19:31:46.797719 22579586809984 run.py:483] Algo bellman_ford step 7767 current loss 0.032299, current_train_items 248576.
I0304 19:31:46.829895 22579586809984 run.py:483] Algo bellman_ford step 7768 current loss 0.043036, current_train_items 248608.
I0304 19:31:46.863590 22579586809984 run.py:483] Algo bellman_ford step 7769 current loss 0.044289, current_train_items 248640.
I0304 19:31:46.883524 22579586809984 run.py:483] Algo bellman_ford step 7770 current loss 0.002404, current_train_items 248672.
I0304 19:31:46.900063 22579586809984 run.py:483] Algo bellman_ford step 7771 current loss 0.018104, current_train_items 248704.
I0304 19:31:46.923912 22579586809984 run.py:483] Algo bellman_ford step 7772 current loss 0.022912, current_train_items 248736.
I0304 19:31:46.956717 22579586809984 run.py:483] Algo bellman_ford step 7773 current loss 0.034649, current_train_items 248768.
I0304 19:31:46.990063 22579586809984 run.py:483] Algo bellman_ford step 7774 current loss 0.061844, current_train_items 248800.
I0304 19:31:47.010392 22579586809984 run.py:483] Algo bellman_ford step 7775 current loss 0.003491, current_train_items 248832.
I0304 19:31:47.026870 22579586809984 run.py:483] Algo bellman_ford step 7776 current loss 0.041580, current_train_items 248864.
I0304 19:31:47.052016 22579586809984 run.py:483] Algo bellman_ford step 7777 current loss 0.022887, current_train_items 248896.
I0304 19:31:47.084370 22579586809984 run.py:483] Algo bellman_ford step 7778 current loss 0.056077, current_train_items 248928.
I0304 19:31:47.119900 22579586809984 run.py:483] Algo bellman_ford step 7779 current loss 0.099691, current_train_items 248960.
I0304 19:31:47.139279 22579586809984 run.py:483] Algo bellman_ford step 7780 current loss 0.001914, current_train_items 248992.
I0304 19:31:47.155826 22579586809984 run.py:483] Algo bellman_ford step 7781 current loss 0.011693, current_train_items 249024.
I0304 19:31:47.179975 22579586809984 run.py:483] Algo bellman_ford step 7782 current loss 0.033958, current_train_items 249056.
I0304 19:31:47.212660 22579586809984 run.py:483] Algo bellman_ford step 7783 current loss 0.066447, current_train_items 249088.
I0304 19:31:47.246715 22579586809984 run.py:483] Algo bellman_ford step 7784 current loss 0.039457, current_train_items 249120.
I0304 19:31:47.266694 22579586809984 run.py:483] Algo bellman_ford step 7785 current loss 0.005865, current_train_items 249152.
I0304 19:31:47.283104 22579586809984 run.py:483] Algo bellman_ford step 7786 current loss 0.018172, current_train_items 249184.
I0304 19:31:47.306425 22579586809984 run.py:483] Algo bellman_ford step 7787 current loss 0.047597, current_train_items 249216.
I0304 19:31:47.338139 22579586809984 run.py:483] Algo bellman_ford step 7788 current loss 0.043134, current_train_items 249248.
I0304 19:31:47.370117 22579586809984 run.py:483] Algo bellman_ford step 7789 current loss 0.049818, current_train_items 249280.
I0304 19:31:47.390017 22579586809984 run.py:483] Algo bellman_ford step 7790 current loss 0.002439, current_train_items 249312.
I0304 19:31:47.406006 22579586809984 run.py:483] Algo bellman_ford step 7791 current loss 0.030913, current_train_items 249344.
I0304 19:31:47.429873 22579586809984 run.py:483] Algo bellman_ford step 7792 current loss 0.045816, current_train_items 249376.
I0304 19:31:47.461169 22579586809984 run.py:483] Algo bellman_ford step 7793 current loss 0.062073, current_train_items 249408.
I0304 19:31:47.494544 22579586809984 run.py:483] Algo bellman_ford step 7794 current loss 0.041126, current_train_items 249440.
I0304 19:31:47.514064 22579586809984 run.py:483] Algo bellman_ford step 7795 current loss 0.017246, current_train_items 249472.
I0304 19:31:47.529896 22579586809984 run.py:483] Algo bellman_ford step 7796 current loss 0.009728, current_train_items 249504.
I0304 19:31:47.553174 22579586809984 run.py:483] Algo bellman_ford step 7797 current loss 0.034983, current_train_items 249536.
I0304 19:31:47.584040 22579586809984 run.py:483] Algo bellman_ford step 7798 current loss 0.081482, current_train_items 249568.
I0304 19:31:47.619306 22579586809984 run.py:483] Algo bellman_ford step 7799 current loss 0.130486, current_train_items 249600.
I0304 19:31:47.639178 22579586809984 run.py:483] Algo bellman_ford step 7800 current loss 0.002876, current_train_items 249632.
I0304 19:31:47.646726 22579586809984 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0304 19:31:47.646831 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:47.663756 22579586809984 run.py:483] Algo bellman_ford step 7801 current loss 0.032027, current_train_items 249664.
I0304 19:31:47.688208 22579586809984 run.py:483] Algo bellman_ford step 7802 current loss 0.027529, current_train_items 249696.
I0304 19:31:47.721440 22579586809984 run.py:483] Algo bellman_ford step 7803 current loss 0.040357, current_train_items 249728.
I0304 19:31:47.757299 22579586809984 run.py:483] Algo bellman_ford step 7804 current loss 0.079685, current_train_items 249760.
I0304 19:31:47.777367 22579586809984 run.py:483] Algo bellman_ford step 7805 current loss 0.007310, current_train_items 249792.
I0304 19:31:47.793644 22579586809984 run.py:483] Algo bellman_ford step 7806 current loss 0.004337, current_train_items 249824.
I0304 19:31:47.816824 22579586809984 run.py:483] Algo bellman_ford step 7807 current loss 0.013096, current_train_items 249856.
I0304 19:31:47.847864 22579586809984 run.py:483] Algo bellman_ford step 7808 current loss 0.027154, current_train_items 249888.
I0304 19:31:47.881860 22579586809984 run.py:483] Algo bellman_ford step 7809 current loss 0.090851, current_train_items 249920.
I0304 19:31:47.901548 22579586809984 run.py:483] Algo bellman_ford step 7810 current loss 0.002587, current_train_items 249952.
I0304 19:31:47.918139 22579586809984 run.py:483] Algo bellman_ford step 7811 current loss 0.012259, current_train_items 249984.
I0304 19:31:47.942531 22579586809984 run.py:483] Algo bellman_ford step 7812 current loss 0.042377, current_train_items 250016.
I0304 19:31:47.975338 22579586809984 run.py:483] Algo bellman_ford step 7813 current loss 0.036420, current_train_items 250048.
I0304 19:31:48.009098 22579586809984 run.py:483] Algo bellman_ford step 7814 current loss 0.049007, current_train_items 250080.
I0304 19:31:48.028481 22579586809984 run.py:483] Algo bellman_ford step 7815 current loss 0.010789, current_train_items 250112.
I0304 19:31:48.044600 22579586809984 run.py:483] Algo bellman_ford step 7816 current loss 0.014326, current_train_items 250144.
I0304 19:31:48.069086 22579586809984 run.py:483] Algo bellman_ford step 7817 current loss 0.018136, current_train_items 250176.
I0304 19:31:48.099575 22579586809984 run.py:483] Algo bellman_ford step 7818 current loss 0.033385, current_train_items 250208.
I0304 19:31:48.132220 22579586809984 run.py:483] Algo bellman_ford step 7819 current loss 0.038464, current_train_items 250240.
I0304 19:31:48.151790 22579586809984 run.py:483] Algo bellman_ford step 7820 current loss 0.002268, current_train_items 250272.
I0304 19:31:48.168398 22579586809984 run.py:483] Algo bellman_ford step 7821 current loss 0.026492, current_train_items 250304.
I0304 19:31:48.193581 22579586809984 run.py:483] Algo bellman_ford step 7822 current loss 0.054923, current_train_items 250336.
I0304 19:31:48.226788 22579586809984 run.py:483] Algo bellman_ford step 7823 current loss 0.114290, current_train_items 250368.
I0304 19:31:48.259555 22579586809984 run.py:483] Algo bellman_ford step 7824 current loss 0.111589, current_train_items 250400.
I0304 19:31:48.279068 22579586809984 run.py:483] Algo bellman_ford step 7825 current loss 0.002015, current_train_items 250432.
I0304 19:31:48.295600 22579586809984 run.py:483] Algo bellman_ford step 7826 current loss 0.014993, current_train_items 250464.
I0304 19:31:48.320174 22579586809984 run.py:483] Algo bellman_ford step 7827 current loss 0.014552, current_train_items 250496.
I0304 19:31:48.352370 22579586809984 run.py:483] Algo bellman_ford step 7828 current loss 0.104113, current_train_items 250528.
I0304 19:31:48.386676 22579586809984 run.py:483] Algo bellman_ford step 7829 current loss 0.071075, current_train_items 250560.
I0304 19:31:48.406368 22579586809984 run.py:483] Algo bellman_ford step 7830 current loss 0.003294, current_train_items 250592.
I0304 19:31:48.422395 22579586809984 run.py:483] Algo bellman_ford step 7831 current loss 0.016342, current_train_items 250624.
I0304 19:31:48.446599 22579586809984 run.py:483] Algo bellman_ford step 7832 current loss 0.037896, current_train_items 250656.
I0304 19:31:48.479499 22579586809984 run.py:483] Algo bellman_ford step 7833 current loss 0.034677, current_train_items 250688.
I0304 19:31:48.511218 22579586809984 run.py:483] Algo bellman_ford step 7834 current loss 0.047057, current_train_items 250720.
I0304 19:31:48.530709 22579586809984 run.py:483] Algo bellman_ford step 7835 current loss 0.003297, current_train_items 250752.
I0304 19:31:48.547126 22579586809984 run.py:483] Algo bellman_ford step 7836 current loss 0.015288, current_train_items 250784.
I0304 19:31:48.571323 22579586809984 run.py:483] Algo bellman_ford step 7837 current loss 0.013391, current_train_items 250816.
I0304 19:31:48.604477 22579586809984 run.py:483] Algo bellman_ford step 7838 current loss 0.043304, current_train_items 250848.
I0304 19:31:48.639582 22579586809984 run.py:483] Algo bellman_ford step 7839 current loss 0.115823, current_train_items 250880.
I0304 19:31:48.659098 22579586809984 run.py:483] Algo bellman_ford step 7840 current loss 0.005014, current_train_items 250912.
I0304 19:31:48.675444 22579586809984 run.py:483] Algo bellman_ford step 7841 current loss 0.020556, current_train_items 250944.
I0304 19:31:48.700032 22579586809984 run.py:483] Algo bellman_ford step 7842 current loss 0.015688, current_train_items 250976.
I0304 19:31:48.733733 22579586809984 run.py:483] Algo bellman_ford step 7843 current loss 0.049025, current_train_items 251008.
I0304 19:31:48.768492 22579586809984 run.py:483] Algo bellman_ford step 7844 current loss 0.057021, current_train_items 251040.
I0304 19:31:48.787920 22579586809984 run.py:483] Algo bellman_ford step 7845 current loss 0.003011, current_train_items 251072.
I0304 19:31:48.804359 22579586809984 run.py:483] Algo bellman_ford step 7846 current loss 0.008786, current_train_items 251104.
I0304 19:31:48.828895 22579586809984 run.py:483] Algo bellman_ford step 7847 current loss 0.042349, current_train_items 251136.
I0304 19:31:48.861463 22579586809984 run.py:483] Algo bellman_ford step 7848 current loss 0.054973, current_train_items 251168.
I0304 19:31:48.895916 22579586809984 run.py:483] Algo bellman_ford step 7849 current loss 0.056364, current_train_items 251200.
I0304 19:31:48.915346 22579586809984 run.py:483] Algo bellman_ford step 7850 current loss 0.002301, current_train_items 251232.
I0304 19:31:48.923721 22579586809984 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0304 19:31:48.924029 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0304 19:31:48.941177 22579586809984 run.py:483] Algo bellman_ford step 7851 current loss 0.031923, current_train_items 251264.
I0304 19:31:48.965454 22579586809984 run.py:483] Algo bellman_ford step 7852 current loss 0.017928, current_train_items 251296.
I0304 19:31:48.998706 22579586809984 run.py:483] Algo bellman_ford step 7853 current loss 0.050338, current_train_items 251328.
I0304 19:31:49.035272 22579586809984 run.py:483] Algo bellman_ford step 7854 current loss 0.063295, current_train_items 251360.
I0304 19:31:49.055083 22579586809984 run.py:483] Algo bellman_ford step 7855 current loss 0.002554, current_train_items 251392.
I0304 19:31:49.071652 22579586809984 run.py:483] Algo bellman_ford step 7856 current loss 0.012926, current_train_items 251424.
I0304 19:31:49.096292 22579586809984 run.py:483] Algo bellman_ford step 7857 current loss 0.075267, current_train_items 251456.
I0304 19:31:49.128274 22579586809984 run.py:483] Algo bellman_ford step 7858 current loss 0.017499, current_train_items 251488.
I0304 19:31:49.163102 22579586809984 run.py:483] Algo bellman_ford step 7859 current loss 0.112859, current_train_items 251520.
I0304 19:31:49.183045 22579586809984 run.py:483] Algo bellman_ford step 7860 current loss 0.006673, current_train_items 251552.
I0304 19:31:49.199352 22579586809984 run.py:483] Algo bellman_ford step 7861 current loss 0.008411, current_train_items 251584.
I0304 19:31:49.223386 22579586809984 run.py:483] Algo bellman_ford step 7862 current loss 0.055833, current_train_items 251616.
I0304 19:31:49.255199 22579586809984 run.py:483] Algo bellman_ford step 7863 current loss 0.129527, current_train_items 251648.
I0304 19:31:49.287823 22579586809984 run.py:483] Algo bellman_ford step 7864 current loss 0.046762, current_train_items 251680.
I0304 19:31:49.307387 22579586809984 run.py:483] Algo bellman_ford step 7865 current loss 0.003831, current_train_items 251712.
I0304 19:31:49.324241 22579586809984 run.py:483] Algo bellman_ford step 7866 current loss 0.015255, current_train_items 251744.
I0304 19:31:49.349195 22579586809984 run.py:483] Algo bellman_ford step 7867 current loss 0.041433, current_train_items 251776.
I0304 19:31:49.380700 22579586809984 run.py:483] Algo bellman_ford step 7868 current loss 0.038783, current_train_items 251808.
I0304 19:31:49.413514 22579586809984 run.py:483] Algo bellman_ford step 7869 current loss 0.056690, current_train_items 251840.
I0304 19:31:49.433441 22579586809984 run.py:483] Algo bellman_ford step 7870 current loss 0.004862, current_train_items 251872.
I0304 19:31:49.449828 22579586809984 run.py:483] Algo bellman_ford step 7871 current loss 0.016117, current_train_items 251904.
I0304 19:31:49.474348 22579586809984 run.py:483] Algo bellman_ford step 7872 current loss 0.033859, current_train_items 251936.
I0304 19:31:49.505679 22579586809984 run.py:483] Algo bellman_ford step 7873 current loss 0.059636, current_train_items 251968.
I0304 19:31:49.539115 22579586809984 run.py:483] Algo bellman_ford step 7874 current loss 0.059458, current_train_items 252000.
I0304 19:31:49.558963 22579586809984 run.py:483] Algo bellman_ford step 7875 current loss 0.006618, current_train_items 252032.
I0304 19:31:49.574959 22579586809984 run.py:483] Algo bellman_ford step 7876 current loss 0.039177, current_train_items 252064.
I0304 19:31:49.598548 22579586809984 run.py:483] Algo bellman_ford step 7877 current loss 0.095237, current_train_items 252096.
I0304 19:31:49.630405 22579586809984 run.py:483] Algo bellman_ford step 7878 current loss 0.056238, current_train_items 252128.
I0304 19:31:49.664207 22579586809984 run.py:483] Algo bellman_ford step 7879 current loss 0.107371, current_train_items 252160.
I0304 19:31:49.683578 22579586809984 run.py:483] Algo bellman_ford step 7880 current loss 0.005933, current_train_items 252192.
I0304 19:31:49.699395 22579586809984 run.py:483] Algo bellman_ford step 7881 current loss 0.013133, current_train_items 252224.
I0304 19:31:49.723213 22579586809984 run.py:483] Algo bellman_ford step 7882 current loss 0.027124, current_train_items 252256.
I0304 19:31:49.755336 22579586809984 run.py:483] Algo bellman_ford step 7883 current loss 0.066076, current_train_items 252288.
I0304 19:31:49.789469 22579586809984 run.py:483] Algo bellman_ford step 7884 current loss 0.097162, current_train_items 252320.
I0304 19:31:49.809513 22579586809984 run.py:483] Algo bellman_ford step 7885 current loss 0.005157, current_train_items 252352.
I0304 19:31:49.825891 22579586809984 run.py:483] Algo bellman_ford step 7886 current loss 0.017012, current_train_items 252384.
I0304 19:31:49.848725 22579586809984 run.py:483] Algo bellman_ford step 7887 current loss 0.052100, current_train_items 252416.
I0304 19:31:49.878767 22579586809984 run.py:483] Algo bellman_ford step 7888 current loss 0.018199, current_train_items 252448.
I0304 19:31:49.912160 22579586809984 run.py:483] Algo bellman_ford step 7889 current loss 0.029633, current_train_items 252480.
I0304 19:31:49.931994 22579586809984 run.py:483] Algo bellman_ford step 7890 current loss 0.005010, current_train_items 252512.
I0304 19:31:49.948148 22579586809984 run.py:483] Algo bellman_ford step 7891 current loss 0.005580, current_train_items 252544.
I0304 19:31:49.972078 22579586809984 run.py:483] Algo bellman_ford step 7892 current loss 0.042338, current_train_items 252576.
I0304 19:31:50.004289 22579586809984 run.py:483] Algo bellman_ford step 7893 current loss 0.046548, current_train_items 252608.
I0304 19:31:50.037367 22579586809984 run.py:483] Algo bellman_ford step 7894 current loss 0.033267, current_train_items 252640.
I0304 19:31:50.056673 22579586809984 run.py:483] Algo bellman_ford step 7895 current loss 0.004711, current_train_items 252672.
I0304 19:31:50.072639 22579586809984 run.py:483] Algo bellman_ford step 7896 current loss 0.015661, current_train_items 252704.
I0304 19:31:50.097379 22579586809984 run.py:483] Algo bellman_ford step 7897 current loss 0.048810, current_train_items 252736.
I0304 19:31:50.129929 22579586809984 run.py:483] Algo bellman_ford step 7898 current loss 0.040865, current_train_items 252768.
I0304 19:31:50.164162 22579586809984 run.py:483] Algo bellman_ford step 7899 current loss 0.058913, current_train_items 252800.
I0304 19:31:50.184007 22579586809984 run.py:483] Algo bellman_ford step 7900 current loss 0.002180, current_train_items 252832.
I0304 19:31:50.191708 22579586809984 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0304 19:31:50.191813 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:31:50.208687 22579586809984 run.py:483] Algo bellman_ford step 7901 current loss 0.016970, current_train_items 252864.
I0304 19:31:50.233540 22579586809984 run.py:483] Algo bellman_ford step 7902 current loss 0.028459, current_train_items 252896.
I0304 19:31:50.266576 22579586809984 run.py:483] Algo bellman_ford step 7903 current loss 0.045330, current_train_items 252928.
I0304 19:31:50.303075 22579586809984 run.py:483] Algo bellman_ford step 7904 current loss 0.066862, current_train_items 252960.
I0304 19:31:50.322899 22579586809984 run.py:483] Algo bellman_ford step 7905 current loss 0.003295, current_train_items 252992.
I0304 19:31:50.339568 22579586809984 run.py:483] Algo bellman_ford step 7906 current loss 0.023298, current_train_items 253024.
I0304 19:31:50.364443 22579586809984 run.py:483] Algo bellman_ford step 7907 current loss 0.026666, current_train_items 253056.
I0304 19:31:50.395903 22579586809984 run.py:483] Algo bellman_ford step 7908 current loss 0.029097, current_train_items 253088.
I0304 19:31:50.427479 22579586809984 run.py:483] Algo bellman_ford step 7909 current loss 0.047350, current_train_items 253120.
I0304 19:31:50.447129 22579586809984 run.py:483] Algo bellman_ford step 7910 current loss 0.009575, current_train_items 253152.
I0304 19:31:50.464077 22579586809984 run.py:483] Algo bellman_ford step 7911 current loss 0.006860, current_train_items 253184.
I0304 19:31:50.488920 22579586809984 run.py:483] Algo bellman_ford step 7912 current loss 0.029956, current_train_items 253216.
I0304 19:31:50.521233 22579586809984 run.py:483] Algo bellman_ford step 7913 current loss 0.033558, current_train_items 253248.
I0304 19:31:50.555347 22579586809984 run.py:483] Algo bellman_ford step 7914 current loss 0.068770, current_train_items 253280.
I0304 19:31:50.575030 22579586809984 run.py:483] Algo bellman_ford step 7915 current loss 0.030430, current_train_items 253312.
I0304 19:31:50.591610 22579586809984 run.py:483] Algo bellman_ford step 7916 current loss 0.033566, current_train_items 253344.
I0304 19:31:50.615675 22579586809984 run.py:483] Algo bellman_ford step 7917 current loss 0.034023, current_train_items 253376.
I0304 19:31:50.647832 22579586809984 run.py:483] Algo bellman_ford step 7918 current loss 0.030728, current_train_items 253408.
I0304 19:31:50.683469 22579586809984 run.py:483] Algo bellman_ford step 7919 current loss 0.038163, current_train_items 253440.
I0304 19:31:50.703054 22579586809984 run.py:483] Algo bellman_ford step 7920 current loss 0.002839, current_train_items 253472.
I0304 19:31:50.719601 22579586809984 run.py:483] Algo bellman_ford step 7921 current loss 0.012007, current_train_items 253504.
I0304 19:31:50.744493 22579586809984 run.py:483] Algo bellman_ford step 7922 current loss 0.041071, current_train_items 253536.
I0304 19:31:50.775817 22579586809984 run.py:483] Algo bellman_ford step 7923 current loss 0.046931, current_train_items 253568.
I0304 19:31:50.810049 22579586809984 run.py:483] Algo bellman_ford step 7924 current loss 0.050324, current_train_items 253600.
I0304 19:31:50.829407 22579586809984 run.py:483] Algo bellman_ford step 7925 current loss 0.011833, current_train_items 253632.
I0304 19:31:50.845857 22579586809984 run.py:483] Algo bellman_ford step 7926 current loss 0.006169, current_train_items 253664.
I0304 19:31:50.870024 22579586809984 run.py:483] Algo bellman_ford step 7927 current loss 0.128907, current_train_items 253696.
I0304 19:31:50.903407 22579586809984 run.py:483] Algo bellman_ford step 7928 current loss 0.088692, current_train_items 253728.
I0304 19:31:50.936504 22579586809984 run.py:483] Algo bellman_ford step 7929 current loss 0.061250, current_train_items 253760.
I0304 19:31:50.956274 22579586809984 run.py:483] Algo bellman_ford step 7930 current loss 0.004214, current_train_items 253792.
I0304 19:31:50.972570 22579586809984 run.py:483] Algo bellman_ford step 7931 current loss 0.017163, current_train_items 253824.
I0304 19:31:50.995973 22579586809984 run.py:483] Algo bellman_ford step 7932 current loss 0.033016, current_train_items 253856.
I0304 19:31:51.028570 22579586809984 run.py:483] Algo bellman_ford step 7933 current loss 0.041242, current_train_items 253888.
I0304 19:31:51.063098 22579586809984 run.py:483] Algo bellman_ford step 7934 current loss 0.052785, current_train_items 253920.
I0304 19:31:51.082758 22579586809984 run.py:483] Algo bellman_ford step 7935 current loss 0.003500, current_train_items 253952.
I0304 19:31:51.098751 22579586809984 run.py:483] Algo bellman_ford step 7936 current loss 0.013061, current_train_items 253984.
I0304 19:31:51.123467 22579586809984 run.py:483] Algo bellman_ford step 7937 current loss 0.039184, current_train_items 254016.
I0304 19:31:51.155269 22579586809984 run.py:483] Algo bellman_ford step 7938 current loss 0.065669, current_train_items 254048.
I0304 19:31:51.189928 22579586809984 run.py:483] Algo bellman_ford step 7939 current loss 0.090544, current_train_items 254080.
I0304 19:31:51.209655 22579586809984 run.py:483] Algo bellman_ford step 7940 current loss 0.003649, current_train_items 254112.
I0304 19:31:51.225453 22579586809984 run.py:483] Algo bellman_ford step 7941 current loss 0.004423, current_train_items 254144.
I0304 19:31:51.250458 22579586809984 run.py:483] Algo bellman_ford step 7942 current loss 0.115393, current_train_items 254176.
I0304 19:31:51.284071 22579586809984 run.py:483] Algo bellman_ford step 7943 current loss 0.244166, current_train_items 254208.
I0304 19:31:51.317260 22579586809984 run.py:483] Algo bellman_ford step 7944 current loss 0.147471, current_train_items 254240.
I0304 19:31:51.336769 22579586809984 run.py:483] Algo bellman_ford step 7945 current loss 0.004447, current_train_items 254272.
I0304 19:31:51.352612 22579586809984 run.py:483] Algo bellman_ford step 7946 current loss 0.015410, current_train_items 254304.
I0304 19:31:51.376104 22579586809984 run.py:483] Algo bellman_ford step 7947 current loss 0.053617, current_train_items 254336.
I0304 19:31:51.406704 22579586809984 run.py:483] Algo bellman_ford step 7948 current loss 0.018814, current_train_items 254368.
I0304 19:31:51.441242 22579586809984 run.py:483] Algo bellman_ford step 7949 current loss 0.084658, current_train_items 254400.
I0304 19:31:51.460803 22579586809984 run.py:483] Algo bellman_ford step 7950 current loss 0.004484, current_train_items 254432.
I0304 19:31:51.469403 22579586809984 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0304 19:31:51.469510 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:51.486597 22579586809984 run.py:483] Algo bellman_ford step 7951 current loss 0.023810, current_train_items 254464.
I0304 19:31:51.511938 22579586809984 run.py:483] Algo bellman_ford step 7952 current loss 0.039709, current_train_items 254496.
I0304 19:31:51.545316 22579586809984 run.py:483] Algo bellman_ford step 7953 current loss 0.035882, current_train_items 254528.
I0304 19:31:51.579513 22579586809984 run.py:483] Algo bellman_ford step 7954 current loss 0.047836, current_train_items 254560.
I0304 19:31:51.599430 22579586809984 run.py:483] Algo bellman_ford step 7955 current loss 0.021788, current_train_items 254592.
I0304 19:31:51.615744 22579586809984 run.py:483] Algo bellman_ford step 7956 current loss 0.037421, current_train_items 254624.
I0304 19:31:51.640094 22579586809984 run.py:483] Algo bellman_ford step 7957 current loss 0.026133, current_train_items 254656.
I0304 19:31:51.672158 22579586809984 run.py:483] Algo bellman_ford step 7958 current loss 0.059848, current_train_items 254688.
I0304 19:31:51.705605 22579586809984 run.py:483] Algo bellman_ford step 7959 current loss 0.038250, current_train_items 254720.
I0304 19:31:51.725469 22579586809984 run.py:483] Algo bellman_ford step 7960 current loss 0.003148, current_train_items 254752.
I0304 19:31:51.742165 22579586809984 run.py:483] Algo bellman_ford step 7961 current loss 0.018618, current_train_items 254784.
I0304 19:31:51.766004 22579586809984 run.py:483] Algo bellman_ford step 7962 current loss 0.067076, current_train_items 254816.
I0304 19:31:51.797271 22579586809984 run.py:483] Algo bellman_ford step 7963 current loss 0.025227, current_train_items 254848.
I0304 19:31:51.831605 22579586809984 run.py:483] Algo bellman_ford step 7964 current loss 0.037428, current_train_items 254880.
I0304 19:31:51.850998 22579586809984 run.py:483] Algo bellman_ford step 7965 current loss 0.002996, current_train_items 254912.
I0304 19:31:51.867579 22579586809984 run.py:483] Algo bellman_ford step 7966 current loss 0.026372, current_train_items 254944.
I0304 19:31:51.892878 22579586809984 run.py:483] Algo bellman_ford step 7967 current loss 0.045664, current_train_items 254976.
I0304 19:31:51.925046 22579586809984 run.py:483] Algo bellman_ford step 7968 current loss 0.043361, current_train_items 255008.
I0304 19:31:51.957472 22579586809984 run.py:483] Algo bellman_ford step 7969 current loss 0.051671, current_train_items 255040.
I0304 19:31:51.977326 22579586809984 run.py:483] Algo bellman_ford step 7970 current loss 0.003954, current_train_items 255072.
I0304 19:31:51.993654 22579586809984 run.py:483] Algo bellman_ford step 7971 current loss 0.026260, current_train_items 255104.
I0304 19:31:52.018148 22579586809984 run.py:483] Algo bellman_ford step 7972 current loss 0.041617, current_train_items 255136.
I0304 19:31:52.049480 22579586809984 run.py:483] Algo bellman_ford step 7973 current loss 0.041825, current_train_items 255168.
I0304 19:31:52.085102 22579586809984 run.py:483] Algo bellman_ford step 7974 current loss 0.056368, current_train_items 255200.
I0304 19:31:52.104924 22579586809984 run.py:483] Algo bellman_ford step 7975 current loss 0.014067, current_train_items 255232.
I0304 19:31:52.121485 22579586809984 run.py:483] Algo bellman_ford step 7976 current loss 0.017872, current_train_items 255264.
I0304 19:31:52.145736 22579586809984 run.py:483] Algo bellman_ford step 7977 current loss 0.062873, current_train_items 255296.
I0304 19:31:52.177622 22579586809984 run.py:483] Algo bellman_ford step 7978 current loss 0.037603, current_train_items 255328.
I0304 19:31:52.209542 22579586809984 run.py:483] Algo bellman_ford step 7979 current loss 0.041552, current_train_items 255360.
I0304 19:31:52.229109 22579586809984 run.py:483] Algo bellman_ford step 7980 current loss 0.002667, current_train_items 255392.
I0304 19:31:52.245227 22579586809984 run.py:483] Algo bellman_ford step 7981 current loss 0.042141, current_train_items 255424.
I0304 19:31:52.268599 22579586809984 run.py:483] Algo bellman_ford step 7982 current loss 0.024909, current_train_items 255456.
I0304 19:31:52.299965 22579586809984 run.py:483] Algo bellman_ford step 7983 current loss 0.028962, current_train_items 255488.
I0304 19:31:52.334235 22579586809984 run.py:483] Algo bellman_ford step 7984 current loss 0.058277, current_train_items 255520.
I0304 19:31:52.354020 22579586809984 run.py:483] Algo bellman_ford step 7985 current loss 0.002288, current_train_items 255552.
I0304 19:31:52.370635 22579586809984 run.py:483] Algo bellman_ford step 7986 current loss 0.016628, current_train_items 255584.
I0304 19:31:52.393855 22579586809984 run.py:483] Algo bellman_ford step 7987 current loss 0.054397, current_train_items 255616.
I0304 19:31:52.426282 22579586809984 run.py:483] Algo bellman_ford step 7988 current loss 0.043478, current_train_items 255648.
I0304 19:31:52.459229 22579586809984 run.py:483] Algo bellman_ford step 7989 current loss 0.044734, current_train_items 255680.
I0304 19:31:52.479213 22579586809984 run.py:483] Algo bellman_ford step 7990 current loss 0.002295, current_train_items 255712.
I0304 19:31:52.495612 22579586809984 run.py:483] Algo bellman_ford step 7991 current loss 0.026769, current_train_items 255744.
I0304 19:31:52.520063 22579586809984 run.py:483] Algo bellman_ford step 7992 current loss 0.047502, current_train_items 255776.
I0304 19:31:52.553621 22579586809984 run.py:483] Algo bellman_ford step 7993 current loss 0.047561, current_train_items 255808.
I0304 19:31:52.588598 22579586809984 run.py:483] Algo bellman_ford step 7994 current loss 0.038164, current_train_items 255840.
I0304 19:31:52.608070 22579586809984 run.py:483] Algo bellman_ford step 7995 current loss 0.003258, current_train_items 255872.
I0304 19:31:52.624140 22579586809984 run.py:483] Algo bellman_ford step 7996 current loss 0.005778, current_train_items 255904.
I0304 19:31:52.648164 22579586809984 run.py:483] Algo bellman_ford step 7997 current loss 0.040780, current_train_items 255936.
I0304 19:31:52.679893 22579586809984 run.py:483] Algo bellman_ford step 7998 current loss 0.017387, current_train_items 255968.
I0304 19:31:52.712078 22579586809984 run.py:483] Algo bellman_ford step 7999 current loss 0.045421, current_train_items 256000.
I0304 19:31:52.732017 22579586809984 run.py:483] Algo bellman_ford step 8000 current loss 0.001953, current_train_items 256032.
I0304 19:31:52.739848 22579586809984 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0304 19:31:52.739955 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:52.757109 22579586809984 run.py:483] Algo bellman_ford step 8001 current loss 0.007052, current_train_items 256064.
I0304 19:31:52.781437 22579586809984 run.py:483] Algo bellman_ford step 8002 current loss 0.034613, current_train_items 256096.
I0304 19:31:52.814839 22579586809984 run.py:483] Algo bellman_ford step 8003 current loss 0.041149, current_train_items 256128.
I0304 19:31:52.848338 22579586809984 run.py:483] Algo bellman_ford step 8004 current loss 0.043459, current_train_items 256160.
I0304 19:31:52.868358 22579586809984 run.py:483] Algo bellman_ford step 8005 current loss 0.002616, current_train_items 256192.
I0304 19:31:52.884561 22579586809984 run.py:483] Algo bellman_ford step 8006 current loss 0.019105, current_train_items 256224.
I0304 19:31:52.909573 22579586809984 run.py:483] Algo bellman_ford step 8007 current loss 0.010418, current_train_items 256256.
I0304 19:31:52.942979 22579586809984 run.py:483] Algo bellman_ford step 8008 current loss 0.029013, current_train_items 256288.
I0304 19:31:52.977080 22579586809984 run.py:483] Algo bellman_ford step 8009 current loss 0.044019, current_train_items 256320.
I0304 19:31:52.997110 22579586809984 run.py:483] Algo bellman_ford step 8010 current loss 0.001720, current_train_items 256352.
I0304 19:31:53.013241 22579586809984 run.py:483] Algo bellman_ford step 8011 current loss 0.012491, current_train_items 256384.
I0304 19:31:53.038265 22579586809984 run.py:483] Algo bellman_ford step 8012 current loss 0.015791, current_train_items 256416.
I0304 19:31:53.070149 22579586809984 run.py:483] Algo bellman_ford step 8013 current loss 0.038362, current_train_items 256448.
I0304 19:31:53.101945 22579586809984 run.py:483] Algo bellman_ford step 8014 current loss 0.057227, current_train_items 256480.
I0304 19:31:53.121658 22579586809984 run.py:483] Algo bellman_ford step 8015 current loss 0.031589, current_train_items 256512.
I0304 19:31:53.138850 22579586809984 run.py:483] Algo bellman_ford step 8016 current loss 0.012477, current_train_items 256544.
I0304 19:31:53.162993 22579586809984 run.py:483] Algo bellman_ford step 8017 current loss 0.022903, current_train_items 256576.
I0304 19:31:53.194838 22579586809984 run.py:483] Algo bellman_ford step 8018 current loss 0.098450, current_train_items 256608.
I0304 19:31:53.227360 22579586809984 run.py:483] Algo bellman_ford step 8019 current loss 0.040800, current_train_items 256640.
I0304 19:31:53.247043 22579586809984 run.py:483] Algo bellman_ford step 8020 current loss 0.002075, current_train_items 256672.
I0304 19:31:53.263638 22579586809984 run.py:483] Algo bellman_ford step 8021 current loss 0.011249, current_train_items 256704.
I0304 19:31:53.287592 22579586809984 run.py:483] Algo bellman_ford step 8022 current loss 0.026246, current_train_items 256736.
I0304 19:31:53.319013 22579586809984 run.py:483] Algo bellman_ford step 8023 current loss 0.046236, current_train_items 256768.
I0304 19:31:53.354435 22579586809984 run.py:483] Algo bellman_ford step 8024 current loss 0.079695, current_train_items 256800.
I0304 19:31:53.374209 22579586809984 run.py:483] Algo bellman_ford step 8025 current loss 0.005973, current_train_items 256832.
I0304 19:31:53.390853 22579586809984 run.py:483] Algo bellman_ford step 8026 current loss 0.011090, current_train_items 256864.
I0304 19:31:53.415301 22579586809984 run.py:483] Algo bellman_ford step 8027 current loss 0.028993, current_train_items 256896.
I0304 19:31:53.446375 22579586809984 run.py:483] Algo bellman_ford step 8028 current loss 0.026049, current_train_items 256928.
I0304 19:31:53.480509 22579586809984 run.py:483] Algo bellman_ford step 8029 current loss 0.082313, current_train_items 256960.
I0304 19:31:53.500265 22579586809984 run.py:483] Algo bellman_ford step 8030 current loss 0.002195, current_train_items 256992.
I0304 19:31:53.516413 22579586809984 run.py:483] Algo bellman_ford step 8031 current loss 0.021893, current_train_items 257024.
I0304 19:31:53.541218 22579586809984 run.py:483] Algo bellman_ford step 8032 current loss 0.022853, current_train_items 257056.
I0304 19:31:53.574006 22579586809984 run.py:483] Algo bellman_ford step 8033 current loss 0.048856, current_train_items 257088.
I0304 19:31:53.608878 22579586809984 run.py:483] Algo bellman_ford step 8034 current loss 0.044793, current_train_items 257120.
I0304 19:31:53.628733 22579586809984 run.py:483] Algo bellman_ford step 8035 current loss 0.002438, current_train_items 257152.
I0304 19:31:53.645510 22579586809984 run.py:483] Algo bellman_ford step 8036 current loss 0.025534, current_train_items 257184.
I0304 19:31:53.670287 22579586809984 run.py:483] Algo bellman_ford step 8037 current loss 0.044433, current_train_items 257216.
I0304 19:31:53.702281 22579586809984 run.py:483] Algo bellman_ford step 8038 current loss 0.029134, current_train_items 257248.
I0304 19:31:53.737343 22579586809984 run.py:483] Algo bellman_ford step 8039 current loss 0.046702, current_train_items 257280.
I0304 19:31:53.756988 22579586809984 run.py:483] Algo bellman_ford step 8040 current loss 0.003204, current_train_items 257312.
I0304 19:31:53.773454 22579586809984 run.py:483] Algo bellman_ford step 8041 current loss 0.028421, current_train_items 257344.
I0304 19:31:53.798103 22579586809984 run.py:483] Algo bellman_ford step 8042 current loss 0.051191, current_train_items 257376.
I0304 19:31:53.830335 22579586809984 run.py:483] Algo bellman_ford step 8043 current loss 0.043973, current_train_items 257408.
I0304 19:31:53.864621 22579586809984 run.py:483] Algo bellman_ford step 8044 current loss 0.053327, current_train_items 257440.
I0304 19:31:53.884553 22579586809984 run.py:483] Algo bellman_ford step 8045 current loss 0.005315, current_train_items 257472.
I0304 19:31:53.901424 22579586809984 run.py:483] Algo bellman_ford step 8046 current loss 0.036787, current_train_items 257504.
I0304 19:31:53.925541 22579586809984 run.py:483] Algo bellman_ford step 8047 current loss 0.032121, current_train_items 257536.
I0304 19:31:53.957900 22579586809984 run.py:483] Algo bellman_ford step 8048 current loss 0.095202, current_train_items 257568.
I0304 19:31:53.990227 22579586809984 run.py:483] Algo bellman_ford step 8049 current loss 0.044727, current_train_items 257600.
I0304 19:31:54.010089 22579586809984 run.py:483] Algo bellman_ford step 8050 current loss 0.002410, current_train_items 257632.
I0304 19:31:54.017899 22579586809984 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0304 19:31:54.018008 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:54.035151 22579586809984 run.py:483] Algo bellman_ford step 8051 current loss 0.022140, current_train_items 257664.
I0304 19:31:54.059585 22579586809984 run.py:483] Algo bellman_ford step 8052 current loss 0.019898, current_train_items 257696.
I0304 19:31:54.091604 22579586809984 run.py:483] Algo bellman_ford step 8053 current loss 0.046018, current_train_items 257728.
I0304 19:31:54.127108 22579586809984 run.py:483] Algo bellman_ford step 8054 current loss 0.126513, current_train_items 257760.
I0304 19:31:54.147420 22579586809984 run.py:483] Algo bellman_ford step 8055 current loss 0.006882, current_train_items 257792.
I0304 19:31:54.162748 22579586809984 run.py:483] Algo bellman_ford step 8056 current loss 0.004508, current_train_items 257824.
I0304 19:31:54.185829 22579586809984 run.py:483] Algo bellman_ford step 8057 current loss 0.020387, current_train_items 257856.
I0304 19:31:54.219259 22579586809984 run.py:483] Algo bellman_ford step 8058 current loss 0.032582, current_train_items 257888.
I0304 19:31:54.255068 22579586809984 run.py:483] Algo bellman_ford step 8059 current loss 0.053763, current_train_items 257920.
I0304 19:31:54.275042 22579586809984 run.py:483] Algo bellman_ford step 8060 current loss 0.004747, current_train_items 257952.
I0304 19:31:54.291150 22579586809984 run.py:483] Algo bellman_ford step 8061 current loss 0.009594, current_train_items 257984.
I0304 19:31:54.316331 22579586809984 run.py:483] Algo bellman_ford step 8062 current loss 0.038494, current_train_items 258016.
I0304 19:31:54.348316 22579586809984 run.py:483] Algo bellman_ford step 8063 current loss 0.059472, current_train_items 258048.
I0304 19:31:54.381181 22579586809984 run.py:483] Algo bellman_ford step 8064 current loss 0.040374, current_train_items 258080.
I0304 19:31:54.400883 22579586809984 run.py:483] Algo bellman_ford step 8065 current loss 0.001977, current_train_items 258112.
I0304 19:31:54.416913 22579586809984 run.py:483] Algo bellman_ford step 8066 current loss 0.014673, current_train_items 258144.
I0304 19:31:54.441207 22579586809984 run.py:483] Algo bellman_ford step 8067 current loss 0.038154, current_train_items 258176.
I0304 19:31:54.473133 22579586809984 run.py:483] Algo bellman_ford step 8068 current loss 0.041432, current_train_items 258208.
I0304 19:31:54.506127 22579586809984 run.py:483] Algo bellman_ford step 8069 current loss 0.030759, current_train_items 258240.
I0304 19:31:54.526433 22579586809984 run.py:483] Algo bellman_ford step 8070 current loss 0.002272, current_train_items 258272.
I0304 19:31:54.543586 22579586809984 run.py:483] Algo bellman_ford step 8071 current loss 0.059126, current_train_items 258304.
I0304 19:31:54.567654 22579586809984 run.py:483] Algo bellman_ford step 8072 current loss 0.024960, current_train_items 258336.
I0304 19:31:54.599696 22579586809984 run.py:483] Algo bellman_ford step 8073 current loss 0.061409, current_train_items 258368.
I0304 19:31:54.634569 22579586809984 run.py:483] Algo bellman_ford step 8074 current loss 0.044947, current_train_items 258400.
I0304 19:31:54.654715 22579586809984 run.py:483] Algo bellman_ford step 8075 current loss 0.002411, current_train_items 258432.
I0304 19:31:54.671106 22579586809984 run.py:483] Algo bellman_ford step 8076 current loss 0.015568, current_train_items 258464.
I0304 19:31:54.695300 22579586809984 run.py:483] Algo bellman_ford step 8077 current loss 0.019080, current_train_items 258496.
I0304 19:31:54.727376 22579586809984 run.py:483] Algo bellman_ford step 8078 current loss 0.050873, current_train_items 258528.
I0304 19:31:54.759753 22579586809984 run.py:483] Algo bellman_ford step 8079 current loss 0.043589, current_train_items 258560.
I0304 19:31:54.779750 22579586809984 run.py:483] Algo bellman_ford step 8080 current loss 0.006346, current_train_items 258592.
I0304 19:31:54.796213 22579586809984 run.py:483] Algo bellman_ford step 8081 current loss 0.010007, current_train_items 258624.
I0304 19:31:54.820527 22579586809984 run.py:483] Algo bellman_ford step 8082 current loss 0.046516, current_train_items 258656.
I0304 19:31:54.853150 22579586809984 run.py:483] Algo bellman_ford step 8083 current loss 0.056638, current_train_items 258688.
I0304 19:31:54.890068 22579586809984 run.py:483] Algo bellman_ford step 8084 current loss 0.048317, current_train_items 258720.
I0304 19:31:54.910228 22579586809984 run.py:483] Algo bellman_ford step 8085 current loss 0.002363, current_train_items 258752.
I0304 19:31:54.927030 22579586809984 run.py:483] Algo bellman_ford step 8086 current loss 0.027366, current_train_items 258784.
I0304 19:31:54.950674 22579586809984 run.py:483] Algo bellman_ford step 8087 current loss 0.058098, current_train_items 258816.
I0304 19:31:54.984405 22579586809984 run.py:483] Algo bellman_ford step 8088 current loss 0.033969, current_train_items 258848.
I0304 19:31:55.016933 22579586809984 run.py:483] Algo bellman_ford step 8089 current loss 0.047611, current_train_items 258880.
I0304 19:31:55.037230 22579586809984 run.py:483] Algo bellman_ford step 8090 current loss 0.002374, current_train_items 258912.
I0304 19:31:55.053165 22579586809984 run.py:483] Algo bellman_ford step 8091 current loss 0.005147, current_train_items 258944.
I0304 19:31:55.077932 22579586809984 run.py:483] Algo bellman_ford step 8092 current loss 0.029941, current_train_items 258976.
I0304 19:31:55.109670 22579586809984 run.py:483] Algo bellman_ford step 8093 current loss 0.036606, current_train_items 259008.
I0304 19:31:55.144348 22579586809984 run.py:483] Algo bellman_ford step 8094 current loss 0.057285, current_train_items 259040.
I0304 19:31:55.164493 22579586809984 run.py:483] Algo bellman_ford step 8095 current loss 0.007335, current_train_items 259072.
I0304 19:31:55.180598 22579586809984 run.py:483] Algo bellman_ford step 8096 current loss 0.033461, current_train_items 259104.
I0304 19:31:55.206022 22579586809984 run.py:483] Algo bellman_ford step 8097 current loss 0.044303, current_train_items 259136.
I0304 19:31:55.239013 22579586809984 run.py:483] Algo bellman_ford step 8098 current loss 0.043523, current_train_items 259168.
I0304 19:31:55.272731 22579586809984 run.py:483] Algo bellman_ford step 8099 current loss 0.034537, current_train_items 259200.
I0304 19:31:55.292717 22579586809984 run.py:483] Algo bellman_ford step 8100 current loss 0.002251, current_train_items 259232.
I0304 19:31:55.300372 22579586809984 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.962890625, 'score': 0.962890625, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0304 19:31:55.300475 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.963, val scores are: bellman_ford: 0.963
I0304 19:31:55.317599 22579586809984 run.py:483] Algo bellman_ford step 8101 current loss 0.011657, current_train_items 259264.
I0304 19:31:55.343952 22579586809984 run.py:483] Algo bellman_ford step 8102 current loss 0.090276, current_train_items 259296.
I0304 19:31:55.376795 22579586809984 run.py:483] Algo bellman_ford step 8103 current loss 0.064973, current_train_items 259328.
I0304 19:31:55.410807 22579586809984 run.py:483] Algo bellman_ford step 8104 current loss 0.098853, current_train_items 259360.
I0304 19:31:55.431080 22579586809984 run.py:483] Algo bellman_ford step 8105 current loss 0.003044, current_train_items 259392.
I0304 19:31:55.447391 22579586809984 run.py:483] Algo bellman_ford step 8106 current loss 0.031705, current_train_items 259424.
I0304 19:31:55.471992 22579586809984 run.py:483] Algo bellman_ford step 8107 current loss 0.058542, current_train_items 259456.
I0304 19:31:55.503738 22579586809984 run.py:483] Algo bellman_ford step 8108 current loss 0.078508, current_train_items 259488.
I0304 19:31:55.538298 22579586809984 run.py:483] Algo bellman_ford step 8109 current loss 0.066115, current_train_items 259520.
I0304 19:31:55.558323 22579586809984 run.py:483] Algo bellman_ford step 8110 current loss 0.015047, current_train_items 259552.
I0304 19:31:55.574222 22579586809984 run.py:483] Algo bellman_ford step 8111 current loss 0.021538, current_train_items 259584.
I0304 19:31:55.597964 22579586809984 run.py:483] Algo bellman_ford step 8112 current loss 0.047755, current_train_items 259616.
I0304 19:31:55.628764 22579586809984 run.py:483] Algo bellman_ford step 8113 current loss 0.051536, current_train_items 259648.
I0304 19:31:55.664465 22579586809984 run.py:483] Algo bellman_ford step 8114 current loss 0.085971, current_train_items 259680.
I0304 19:31:55.684476 22579586809984 run.py:483] Algo bellman_ford step 8115 current loss 0.009927, current_train_items 259712.
I0304 19:31:55.700919 22579586809984 run.py:483] Algo bellman_ford step 8116 current loss 0.006167, current_train_items 259744.
I0304 19:31:55.725900 22579586809984 run.py:483] Algo bellman_ford step 8117 current loss 0.035991, current_train_items 259776.
I0304 19:31:55.758309 22579586809984 run.py:483] Algo bellman_ford step 8118 current loss 0.050179, current_train_items 259808.
I0304 19:31:55.792258 22579586809984 run.py:483] Algo bellman_ford step 8119 current loss 0.055755, current_train_items 259840.
I0304 19:31:55.811793 22579586809984 run.py:483] Algo bellman_ford step 8120 current loss 0.004976, current_train_items 259872.
I0304 19:31:55.828291 22579586809984 run.py:483] Algo bellman_ford step 8121 current loss 0.006562, current_train_items 259904.
I0304 19:31:55.852762 22579586809984 run.py:483] Algo bellman_ford step 8122 current loss 0.048260, current_train_items 259936.
I0304 19:31:55.886005 22579586809984 run.py:483] Algo bellman_ford step 8123 current loss 0.048141, current_train_items 259968.
I0304 19:31:55.918348 22579586809984 run.py:483] Algo bellman_ford step 8124 current loss 0.051303, current_train_items 260000.
I0304 19:31:55.937978 22579586809984 run.py:483] Algo bellman_ford step 8125 current loss 0.002427, current_train_items 260032.
I0304 19:31:55.954508 22579586809984 run.py:483] Algo bellman_ford step 8126 current loss 0.018465, current_train_items 260064.
I0304 19:31:55.979733 22579586809984 run.py:483] Algo bellman_ford step 8127 current loss 0.029284, current_train_items 260096.
I0304 19:31:56.012612 22579586809984 run.py:483] Algo bellman_ford step 8128 current loss 0.051997, current_train_items 260128.
I0304 19:31:56.048768 22579586809984 run.py:483] Algo bellman_ford step 8129 current loss 0.093915, current_train_items 260160.
I0304 19:31:56.068965 22579586809984 run.py:483] Algo bellman_ford step 8130 current loss 0.010546, current_train_items 260192.
I0304 19:31:56.084862 22579586809984 run.py:483] Algo bellman_ford step 8131 current loss 0.009979, current_train_items 260224.
I0304 19:31:56.109399 22579586809984 run.py:483] Algo bellman_ford step 8132 current loss 0.039033, current_train_items 260256.
I0304 19:31:56.141415 22579586809984 run.py:483] Algo bellman_ford step 8133 current loss 0.073690, current_train_items 260288.
I0304 19:31:56.175915 22579586809984 run.py:483] Algo bellman_ford step 8134 current loss 0.050359, current_train_items 260320.
I0304 19:31:56.195484 22579586809984 run.py:483] Algo bellman_ford step 8135 current loss 0.002846, current_train_items 260352.
I0304 19:31:56.211533 22579586809984 run.py:483] Algo bellman_ford step 8136 current loss 0.011169, current_train_items 260384.
I0304 19:31:56.235557 22579586809984 run.py:483] Algo bellman_ford step 8137 current loss 0.052677, current_train_items 260416.
I0304 19:31:56.268023 22579586809984 run.py:483] Algo bellman_ford step 8138 current loss 0.048186, current_train_items 260448.
I0304 19:31:56.303622 22579586809984 run.py:483] Algo bellman_ford step 8139 current loss 0.091216, current_train_items 260480.
I0304 19:31:56.323258 22579586809984 run.py:483] Algo bellman_ford step 8140 current loss 0.002558, current_train_items 260512.
I0304 19:31:56.339271 22579586809984 run.py:483] Algo bellman_ford step 8141 current loss 0.022234, current_train_items 260544.
I0304 19:31:56.364456 22579586809984 run.py:483] Algo bellman_ford step 8142 current loss 0.083693, current_train_items 260576.
I0304 19:31:56.397310 22579586809984 run.py:483] Algo bellman_ford step 8143 current loss 0.107525, current_train_items 260608.
I0304 19:31:56.431854 22579586809984 run.py:483] Algo bellman_ford step 8144 current loss 0.086546, current_train_items 260640.
I0304 19:31:56.451629 22579586809984 run.py:483] Algo bellman_ford step 8145 current loss 0.005330, current_train_items 260672.
I0304 19:31:56.468144 22579586809984 run.py:483] Algo bellman_ford step 8146 current loss 0.014960, current_train_items 260704.
I0304 19:31:56.492667 22579586809984 run.py:483] Algo bellman_ford step 8147 current loss 0.083430, current_train_items 260736.
I0304 19:31:56.525247 22579586809984 run.py:483] Algo bellman_ford step 8148 current loss 0.110402, current_train_items 260768.
I0304 19:31:56.558449 22579586809984 run.py:483] Algo bellman_ford step 8149 current loss 0.070431, current_train_items 260800.
I0304 19:31:56.578341 22579586809984 run.py:483] Algo bellman_ford step 8150 current loss 0.004006, current_train_items 260832.
I0304 19:31:56.586175 22579586809984 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0304 19:31:56.586281 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:31:56.603161 22579586809984 run.py:483] Algo bellman_ford step 8151 current loss 0.011410, current_train_items 260864.
I0304 19:31:56.628068 22579586809984 run.py:483] Algo bellman_ford step 8152 current loss 0.039364, current_train_items 260896.
I0304 19:31:56.658908 22579586809984 run.py:483] Algo bellman_ford step 8153 current loss 0.016291, current_train_items 260928.
I0304 19:31:56.694183 22579586809984 run.py:483] Algo bellman_ford step 8154 current loss 0.074392, current_train_items 260960.
I0304 19:31:56.714217 22579586809984 run.py:483] Algo bellman_ford step 8155 current loss 0.003746, current_train_items 260992.
I0304 19:31:56.729972 22579586809984 run.py:483] Algo bellman_ford step 8156 current loss 0.045041, current_train_items 261024.
I0304 19:31:56.754411 22579586809984 run.py:483] Algo bellman_ford step 8157 current loss 0.016787, current_train_items 261056.
I0304 19:31:56.786072 22579586809984 run.py:483] Algo bellman_ford step 8158 current loss 0.038643, current_train_items 261088.
I0304 19:31:56.819070 22579586809984 run.py:483] Algo bellman_ford step 8159 current loss 0.098734, current_train_items 261120.
I0304 19:31:56.838867 22579586809984 run.py:483] Algo bellman_ford step 8160 current loss 0.001873, current_train_items 261152.
I0304 19:31:56.855386 22579586809984 run.py:483] Algo bellman_ford step 8161 current loss 0.008150, current_train_items 261184.
I0304 19:31:56.879209 22579586809984 run.py:483] Algo bellman_ford step 8162 current loss 0.071403, current_train_items 261216.
I0304 19:31:56.911365 22579586809984 run.py:483] Algo bellman_ford step 8163 current loss 0.039099, current_train_items 261248.
I0304 19:31:56.941882 22579586809984 run.py:483] Algo bellman_ford step 8164 current loss 0.016027, current_train_items 261280.
I0304 19:31:56.961351 22579586809984 run.py:483] Algo bellman_ford step 8165 current loss 0.002455, current_train_items 261312.
I0304 19:31:56.977704 22579586809984 run.py:483] Algo bellman_ford step 8166 current loss 0.036595, current_train_items 261344.
I0304 19:31:57.001708 22579586809984 run.py:483] Algo bellman_ford step 8167 current loss 0.060584, current_train_items 261376.
I0304 19:31:57.033102 22579586809984 run.py:483] Algo bellman_ford step 8168 current loss 0.045726, current_train_items 261408.
I0304 19:31:57.067790 22579586809984 run.py:483] Algo bellman_ford step 8169 current loss 0.064357, current_train_items 261440.
I0304 19:31:57.087671 22579586809984 run.py:483] Algo bellman_ford step 8170 current loss 0.002839, current_train_items 261472.
I0304 19:31:57.104172 22579586809984 run.py:483] Algo bellman_ford step 8171 current loss 0.007007, current_train_items 261504.
I0304 19:31:57.128366 22579586809984 run.py:483] Algo bellman_ford step 8172 current loss 0.098693, current_train_items 261536.
I0304 19:31:57.160418 22579586809984 run.py:483] Algo bellman_ford step 8173 current loss 0.099979, current_train_items 261568.
I0304 19:31:57.193439 22579586809984 run.py:483] Algo bellman_ford step 8174 current loss 0.069888, current_train_items 261600.
I0304 19:31:57.213569 22579586809984 run.py:483] Algo bellman_ford step 8175 current loss 0.005412, current_train_items 261632.
I0304 19:31:57.230413 22579586809984 run.py:483] Algo bellman_ford step 8176 current loss 0.028292, current_train_items 261664.
I0304 19:31:57.254108 22579586809984 run.py:483] Algo bellman_ford step 8177 current loss 0.084251, current_train_items 261696.
I0304 19:31:57.286998 22579586809984 run.py:483] Algo bellman_ford step 8178 current loss 0.053225, current_train_items 261728.
I0304 19:31:57.318227 22579586809984 run.py:483] Algo bellman_ford step 8179 current loss 0.062100, current_train_items 261760.
I0304 19:31:57.338050 22579586809984 run.py:483] Algo bellman_ford step 8180 current loss 0.003929, current_train_items 261792.
I0304 19:31:57.354214 22579586809984 run.py:483] Algo bellman_ford step 8181 current loss 0.030292, current_train_items 261824.
I0304 19:31:57.378126 22579586809984 run.py:483] Algo bellman_ford step 8182 current loss 0.027779, current_train_items 261856.
I0304 19:31:57.410366 22579586809984 run.py:483] Algo bellman_ford step 8183 current loss 0.037074, current_train_items 261888.
I0304 19:31:57.446182 22579586809984 run.py:483] Algo bellman_ford step 8184 current loss 0.100518, current_train_items 261920.
I0304 19:31:57.466812 22579586809984 run.py:483] Algo bellman_ford step 8185 current loss 0.006220, current_train_items 261952.
I0304 19:31:57.483142 22579586809984 run.py:483] Algo bellman_ford step 8186 current loss 0.021194, current_train_items 261984.
I0304 19:31:57.507222 22579586809984 run.py:483] Algo bellman_ford step 8187 current loss 0.056410, current_train_items 262016.
I0304 19:31:57.539102 22579586809984 run.py:483] Algo bellman_ford step 8188 current loss 0.051965, current_train_items 262048.
I0304 19:31:57.572525 22579586809984 run.py:483] Algo bellman_ford step 8189 current loss 0.057879, current_train_items 262080.
I0304 19:31:57.592429 22579586809984 run.py:483] Algo bellman_ford step 8190 current loss 0.014067, current_train_items 262112.
I0304 19:31:57.608862 22579586809984 run.py:483] Algo bellman_ford step 8191 current loss 0.016695, current_train_items 262144.
I0304 19:31:57.632847 22579586809984 run.py:483] Algo bellman_ford step 8192 current loss 0.044282, current_train_items 262176.
I0304 19:31:57.664317 22579586809984 run.py:483] Algo bellman_ford step 8193 current loss 0.035117, current_train_items 262208.
I0304 19:31:57.697931 22579586809984 run.py:483] Algo bellman_ford step 8194 current loss 0.051558, current_train_items 262240.
I0304 19:31:57.717197 22579586809984 run.py:483] Algo bellman_ford step 8195 current loss 0.003828, current_train_items 262272.
I0304 19:31:57.733474 22579586809984 run.py:483] Algo bellman_ford step 8196 current loss 0.004602, current_train_items 262304.
I0304 19:31:57.756810 22579586809984 run.py:483] Algo bellman_ford step 8197 current loss 0.050304, current_train_items 262336.
I0304 19:31:57.788412 22579586809984 run.py:483] Algo bellman_ford step 8198 current loss 0.056987, current_train_items 262368.
I0304 19:31:57.821904 22579586809984 run.py:483] Algo bellman_ford step 8199 current loss 0.082946, current_train_items 262400.
I0304 19:31:57.841918 22579586809984 run.py:483] Algo bellman_ford step 8200 current loss 0.003842, current_train_items 262432.
I0304 19:31:57.850034 22579586809984 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0304 19:31:57.850141 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:57.866914 22579586809984 run.py:483] Algo bellman_ford step 8201 current loss 0.005712, current_train_items 262464.
I0304 19:31:57.891911 22579586809984 run.py:483] Algo bellman_ford step 8202 current loss 0.026230, current_train_items 262496.
I0304 19:31:57.923920 22579586809984 run.py:483] Algo bellman_ford step 8203 current loss 0.041553, current_train_items 262528.
I0304 19:31:57.958856 22579586809984 run.py:483] Algo bellman_ford step 8204 current loss 0.086121, current_train_items 262560.
I0304 19:31:57.978520 22579586809984 run.py:483] Algo bellman_ford step 8205 current loss 0.002877, current_train_items 262592.
I0304 19:31:57.994942 22579586809984 run.py:483] Algo bellman_ford step 8206 current loss 0.006116, current_train_items 262624.
I0304 19:31:58.019744 22579586809984 run.py:483] Algo bellman_ford step 8207 current loss 0.058170, current_train_items 262656.
I0304 19:31:58.051572 22579586809984 run.py:483] Algo bellman_ford step 8208 current loss 0.041370, current_train_items 262688.
I0304 19:31:58.082797 22579586809984 run.py:483] Algo bellman_ford step 8209 current loss 0.029963, current_train_items 262720.
I0304 19:31:58.102301 22579586809984 run.py:483] Algo bellman_ford step 8210 current loss 0.002872, current_train_items 262752.
I0304 19:31:58.118375 22579586809984 run.py:483] Algo bellman_ford step 8211 current loss 0.050365, current_train_items 262784.
I0304 19:31:58.141976 22579586809984 run.py:483] Algo bellman_ford step 8212 current loss 0.027453, current_train_items 262816.
I0304 19:31:58.173622 22579586809984 run.py:483] Algo bellman_ford step 8213 current loss 0.034996, current_train_items 262848.
I0304 19:31:58.207217 22579586809984 run.py:483] Algo bellman_ford step 8214 current loss 0.092674, current_train_items 262880.
I0304 19:31:58.226605 22579586809984 run.py:483] Algo bellman_ford step 8215 current loss 0.004584, current_train_items 262912.
I0304 19:31:58.242254 22579586809984 run.py:483] Algo bellman_ford step 8216 current loss 0.005480, current_train_items 262944.
I0304 19:31:58.265388 22579586809984 run.py:483] Algo bellman_ford step 8217 current loss 0.048468, current_train_items 262976.
I0304 19:31:58.298267 22579586809984 run.py:483] Algo bellman_ford step 8218 current loss 0.063509, current_train_items 263008.
I0304 19:31:58.332756 22579586809984 run.py:483] Algo bellman_ford step 8219 current loss 0.050241, current_train_items 263040.
I0304 19:31:58.352064 22579586809984 run.py:483] Algo bellman_ford step 8220 current loss 0.004608, current_train_items 263072.
I0304 19:31:58.368050 22579586809984 run.py:483] Algo bellman_ford step 8221 current loss 0.023050, current_train_items 263104.
I0304 19:31:58.393195 22579586809984 run.py:483] Algo bellman_ford step 8222 current loss 0.042935, current_train_items 263136.
I0304 19:31:58.425601 22579586809984 run.py:483] Algo bellman_ford step 8223 current loss 0.072038, current_train_items 263168.
I0304 19:31:58.456988 22579586809984 run.py:483] Algo bellman_ford step 8224 current loss 0.037144, current_train_items 263200.
I0304 19:31:58.476712 22579586809984 run.py:483] Algo bellman_ford step 8225 current loss 0.011434, current_train_items 263232.
I0304 19:31:58.493158 22579586809984 run.py:483] Algo bellman_ford step 8226 current loss 0.015542, current_train_items 263264.
I0304 19:31:58.518399 22579586809984 run.py:483] Algo bellman_ford step 8227 current loss 0.018634, current_train_items 263296.
I0304 19:31:58.549381 22579586809984 run.py:483] Algo bellman_ford step 8228 current loss 0.045496, current_train_items 263328.
I0304 19:31:58.583127 22579586809984 run.py:483] Algo bellman_ford step 8229 current loss 0.035070, current_train_items 263360.
I0304 19:31:58.602562 22579586809984 run.py:483] Algo bellman_ford step 8230 current loss 0.002767, current_train_items 263392.
I0304 19:31:58.618778 22579586809984 run.py:483] Algo bellman_ford step 8231 current loss 0.014202, current_train_items 263424.
I0304 19:31:58.643514 22579586809984 run.py:483] Algo bellman_ford step 8232 current loss 0.022912, current_train_items 263456.
I0304 19:31:58.674964 22579586809984 run.py:483] Algo bellman_ford step 8233 current loss 0.032273, current_train_items 263488.
I0304 19:31:58.708498 22579586809984 run.py:483] Algo bellman_ford step 8234 current loss 0.049724, current_train_items 263520.
I0304 19:31:58.727794 22579586809984 run.py:483] Algo bellman_ford step 8235 current loss 0.002641, current_train_items 263552.
I0304 19:31:58.744393 22579586809984 run.py:483] Algo bellman_ford step 8236 current loss 0.013373, current_train_items 263584.
I0304 19:31:58.769021 22579586809984 run.py:483] Algo bellman_ford step 8237 current loss 0.028032, current_train_items 263616.
I0304 19:31:58.801275 22579586809984 run.py:483] Algo bellman_ford step 8238 current loss 0.058347, current_train_items 263648.
I0304 19:31:58.834561 22579586809984 run.py:483] Algo bellman_ford step 8239 current loss 0.057518, current_train_items 263680.
I0304 19:31:58.854004 22579586809984 run.py:483] Algo bellman_ford step 8240 current loss 0.001795, current_train_items 263712.
I0304 19:31:58.870261 22579586809984 run.py:483] Algo bellman_ford step 8241 current loss 0.010856, current_train_items 263744.
I0304 19:31:58.893815 22579586809984 run.py:483] Algo bellman_ford step 8242 current loss 0.035205, current_train_items 263776.
I0304 19:31:58.925572 22579586809984 run.py:483] Algo bellman_ford step 8243 current loss 0.042430, current_train_items 263808.
I0304 19:31:58.961250 22579586809984 run.py:483] Algo bellman_ford step 8244 current loss 0.052436, current_train_items 263840.
I0304 19:31:58.980937 22579586809984 run.py:483] Algo bellman_ford step 8245 current loss 0.010685, current_train_items 263872.
I0304 19:31:58.997056 22579586809984 run.py:483] Algo bellman_ford step 8246 current loss 0.024681, current_train_items 263904.
I0304 19:31:59.020348 22579586809984 run.py:483] Algo bellman_ford step 8247 current loss 0.030679, current_train_items 263936.
I0304 19:31:59.051898 22579586809984 run.py:483] Algo bellman_ford step 8248 current loss 0.024474, current_train_items 263968.
I0304 19:31:59.085958 22579586809984 run.py:483] Algo bellman_ford step 8249 current loss 0.053631, current_train_items 264000.
I0304 19:31:59.105470 22579586809984 run.py:483] Algo bellman_ford step 8250 current loss 0.001939, current_train_items 264032.
I0304 19:31:59.113880 22579586809984 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0304 19:31:59.113987 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:31:59.130916 22579586809984 run.py:483] Algo bellman_ford step 8251 current loss 0.011822, current_train_items 264064.
I0304 19:31:59.155729 22579586809984 run.py:483] Algo bellman_ford step 8252 current loss 0.061622, current_train_items 264096.
I0304 19:31:59.189107 22579586809984 run.py:483] Algo bellman_ford step 8253 current loss 0.052017, current_train_items 264128.
I0304 19:31:59.222478 22579586809984 run.py:483] Algo bellman_ford step 8254 current loss 0.028359, current_train_items 264160.
I0304 19:31:59.242100 22579586809984 run.py:483] Algo bellman_ford step 8255 current loss 0.013517, current_train_items 264192.
I0304 19:31:59.258625 22579586809984 run.py:483] Algo bellman_ford step 8256 current loss 0.027023, current_train_items 264224.
I0304 19:31:59.283286 22579586809984 run.py:483] Algo bellman_ford step 8257 current loss 0.027171, current_train_items 264256.
I0304 19:31:59.315908 22579586809984 run.py:483] Algo bellman_ford step 8258 current loss 0.067457, current_train_items 264288.
I0304 19:31:59.350918 22579586809984 run.py:483] Algo bellman_ford step 8259 current loss 0.056354, current_train_items 264320.
I0304 19:31:59.370594 22579586809984 run.py:483] Algo bellman_ford step 8260 current loss 0.016707, current_train_items 264352.
I0304 19:31:59.387166 22579586809984 run.py:483] Algo bellman_ford step 8261 current loss 0.007792, current_train_items 264384.
I0304 19:31:59.411925 22579586809984 run.py:483] Algo bellman_ford step 8262 current loss 0.047405, current_train_items 264416.
I0304 19:31:59.444548 22579586809984 run.py:483] Algo bellman_ford step 8263 current loss 0.032149, current_train_items 264448.
I0304 19:31:59.479700 22579586809984 run.py:483] Algo bellman_ford step 8264 current loss 0.044697, current_train_items 264480.
I0304 19:31:59.499096 22579586809984 run.py:483] Algo bellman_ford step 8265 current loss 0.006359, current_train_items 264512.
I0304 19:31:59.515039 22579586809984 run.py:483] Algo bellman_ford step 8266 current loss 0.011043, current_train_items 264544.
I0304 19:31:59.539582 22579586809984 run.py:483] Algo bellman_ford step 8267 current loss 0.044297, current_train_items 264576.
I0304 19:31:59.572983 22579586809984 run.py:483] Algo bellman_ford step 8268 current loss 0.061743, current_train_items 264608.
I0304 19:31:59.605368 22579586809984 run.py:483] Algo bellman_ford step 8269 current loss 0.049665, current_train_items 264640.
I0304 19:31:59.625349 22579586809984 run.py:483] Algo bellman_ford step 8270 current loss 0.002483, current_train_items 264672.
I0304 19:31:59.641207 22579586809984 run.py:483] Algo bellman_ford step 8271 current loss 0.007182, current_train_items 264704.
I0304 19:31:59.664920 22579586809984 run.py:483] Algo bellman_ford step 8272 current loss 0.028895, current_train_items 264736.
I0304 19:31:59.696047 22579586809984 run.py:483] Algo bellman_ford step 8273 current loss 0.030229, current_train_items 264768.
I0304 19:31:59.730984 22579586809984 run.py:483] Algo bellman_ford step 8274 current loss 0.056390, current_train_items 264800.
I0304 19:31:59.750938 22579586809984 run.py:483] Algo bellman_ford step 8275 current loss 0.003798, current_train_items 264832.
I0304 19:31:59.767709 22579586809984 run.py:483] Algo bellman_ford step 8276 current loss 0.042404, current_train_items 264864.
I0304 19:31:59.792561 22579586809984 run.py:483] Algo bellman_ford step 8277 current loss 0.091466, current_train_items 264896.
I0304 19:31:59.824137 22579586809984 run.py:483] Algo bellman_ford step 8278 current loss 0.092884, current_train_items 264928.
I0304 19:31:59.857040 22579586809984 run.py:483] Algo bellman_ford step 8279 current loss 0.062470, current_train_items 264960.
I0304 19:31:59.876392 22579586809984 run.py:483] Algo bellman_ford step 8280 current loss 0.006073, current_train_items 264992.
I0304 19:31:59.893225 22579586809984 run.py:483] Algo bellman_ford step 8281 current loss 0.013403, current_train_items 265024.
I0304 19:31:59.917232 22579586809984 run.py:483] Algo bellman_ford step 8282 current loss 0.013168, current_train_items 265056.
I0304 19:31:59.948615 22579586809984 run.py:483] Algo bellman_ford step 8283 current loss 0.057899, current_train_items 265088.
I0304 19:31:59.981931 22579586809984 run.py:483] Algo bellman_ford step 8284 current loss 0.056251, current_train_items 265120.
I0304 19:32:00.002041 22579586809984 run.py:483] Algo bellman_ford step 8285 current loss 0.004285, current_train_items 265152.
I0304 19:32:00.018648 22579586809984 run.py:483] Algo bellman_ford step 8286 current loss 0.028272, current_train_items 265184.
I0304 19:32:00.042375 22579586809984 run.py:483] Algo bellman_ford step 8287 current loss 0.032453, current_train_items 265216.
I0304 19:32:00.075707 22579586809984 run.py:483] Algo bellman_ford step 8288 current loss 0.064720, current_train_items 265248.
I0304 19:32:00.109712 22579586809984 run.py:483] Algo bellman_ford step 8289 current loss 0.033335, current_train_items 265280.
I0304 19:32:00.129346 22579586809984 run.py:483] Algo bellman_ford step 8290 current loss 0.002669, current_train_items 265312.
I0304 19:32:00.145753 22579586809984 run.py:483] Algo bellman_ford step 8291 current loss 0.009864, current_train_items 265344.
I0304 19:32:00.170334 22579586809984 run.py:483] Algo bellman_ford step 8292 current loss 0.027427, current_train_items 265376.
I0304 19:32:00.201011 22579586809984 run.py:483] Algo bellman_ford step 8293 current loss 0.048819, current_train_items 265408.
I0304 19:32:00.236623 22579586809984 run.py:483] Algo bellman_ford step 8294 current loss 0.054387, current_train_items 265440.
I0304 19:32:00.256168 22579586809984 run.py:483] Algo bellman_ford step 8295 current loss 0.020563, current_train_items 265472.
I0304 19:32:00.272560 22579586809984 run.py:483] Algo bellman_ford step 8296 current loss 0.022983, current_train_items 265504.
I0304 19:32:00.297787 22579586809984 run.py:483] Algo bellman_ford step 8297 current loss 0.037381, current_train_items 265536.
I0304 19:32:00.330167 22579586809984 run.py:483] Algo bellman_ford step 8298 current loss 0.038416, current_train_items 265568.
I0304 19:32:00.362979 22579586809984 run.py:483] Algo bellman_ford step 8299 current loss 0.053972, current_train_items 265600.
I0304 19:32:00.382917 22579586809984 run.py:483] Algo bellman_ford step 8300 current loss 0.008004, current_train_items 265632.
I0304 19:32:00.390590 22579586809984 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0304 19:32:00.390704 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:00.407282 22579586809984 run.py:483] Algo bellman_ford step 8301 current loss 0.013497, current_train_items 265664.
I0304 19:32:00.433653 22579586809984 run.py:483] Algo bellman_ford step 8302 current loss 0.039715, current_train_items 265696.
I0304 19:32:00.466784 22579586809984 run.py:483] Algo bellman_ford step 8303 current loss 0.061327, current_train_items 265728.
I0304 19:32:00.500476 22579586809984 run.py:483] Algo bellman_ford step 8304 current loss 0.058794, current_train_items 265760.
I0304 19:32:00.520651 22579586809984 run.py:483] Algo bellman_ford step 8305 current loss 0.003497, current_train_items 265792.
I0304 19:32:00.536577 22579586809984 run.py:483] Algo bellman_ford step 8306 current loss 0.015377, current_train_items 265824.
I0304 19:32:00.561064 22579586809984 run.py:483] Algo bellman_ford step 8307 current loss 0.052357, current_train_items 265856.
I0304 19:32:00.591003 22579586809984 run.py:483] Algo bellman_ford step 8308 current loss 0.041654, current_train_items 265888.
I0304 19:32:00.626934 22579586809984 run.py:483] Algo bellman_ford step 8309 current loss 0.114891, current_train_items 265920.
I0304 19:32:00.646841 22579586809984 run.py:483] Algo bellman_ford step 8310 current loss 0.004725, current_train_items 265952.
I0304 19:32:00.663041 22579586809984 run.py:483] Algo bellman_ford step 8311 current loss 0.016282, current_train_items 265984.
I0304 19:32:00.687864 22579586809984 run.py:483] Algo bellman_ford step 8312 current loss 0.043173, current_train_items 266016.
I0304 19:32:00.719295 22579586809984 run.py:483] Algo bellman_ford step 8313 current loss 0.032018, current_train_items 266048.
I0304 19:32:00.751547 22579586809984 run.py:483] Algo bellman_ford step 8314 current loss 0.047416, current_train_items 266080.
I0304 19:32:00.771011 22579586809984 run.py:483] Algo bellman_ford step 8315 current loss 0.002462, current_train_items 266112.
I0304 19:32:00.787616 22579586809984 run.py:483] Algo bellman_ford step 8316 current loss 0.026459, current_train_items 266144.
I0304 19:32:00.811845 22579586809984 run.py:483] Algo bellman_ford step 8317 current loss 0.041021, current_train_items 266176.
I0304 19:32:00.842524 22579586809984 run.py:483] Algo bellman_ford step 8318 current loss 0.027595, current_train_items 266208.
I0304 19:32:00.876244 22579586809984 run.py:483] Algo bellman_ford step 8319 current loss 0.128730, current_train_items 266240.
I0304 19:32:00.896060 22579586809984 run.py:483] Algo bellman_ford step 8320 current loss 0.004381, current_train_items 266272.
I0304 19:32:00.912292 22579586809984 run.py:483] Algo bellman_ford step 8321 current loss 0.016164, current_train_items 266304.
I0304 19:32:00.937469 22579586809984 run.py:483] Algo bellman_ford step 8322 current loss 0.029264, current_train_items 266336.
I0304 19:32:00.969126 22579586809984 run.py:483] Algo bellman_ford step 8323 current loss 0.053777, current_train_items 266368.
I0304 19:32:01.004738 22579586809984 run.py:483] Algo bellman_ford step 8324 current loss 0.046082, current_train_items 266400.
I0304 19:32:01.024367 22579586809984 run.py:483] Algo bellman_ford step 8325 current loss 0.004799, current_train_items 266432.
I0304 19:32:01.040421 22579586809984 run.py:483] Algo bellman_ford step 8326 current loss 0.012169, current_train_items 266464.
I0304 19:32:01.064755 22579586809984 run.py:483] Algo bellman_ford step 8327 current loss 0.031932, current_train_items 266496.
I0304 19:32:01.095923 22579586809984 run.py:483] Algo bellman_ford step 8328 current loss 0.074683, current_train_items 266528.
I0304 19:32:01.130737 22579586809984 run.py:483] Algo bellman_ford step 8329 current loss 0.038385, current_train_items 266560.
I0304 19:32:01.150611 22579586809984 run.py:483] Algo bellman_ford step 8330 current loss 0.003325, current_train_items 266592.
I0304 19:32:01.166731 22579586809984 run.py:483] Algo bellman_ford step 8331 current loss 0.055537, current_train_items 266624.
I0304 19:32:01.190529 22579586809984 run.py:483] Algo bellman_ford step 8332 current loss 0.021666, current_train_items 266656.
I0304 19:32:01.223330 22579586809984 run.py:483] Algo bellman_ford step 8333 current loss 0.091310, current_train_items 266688.
I0304 19:32:01.256763 22579586809984 run.py:483] Algo bellman_ford step 8334 current loss 0.074960, current_train_items 266720.
I0304 19:32:01.276764 22579586809984 run.py:483] Algo bellman_ford step 8335 current loss 0.005064, current_train_items 266752.
I0304 19:32:01.293090 22579586809984 run.py:483] Algo bellman_ford step 8336 current loss 0.038827, current_train_items 266784.
I0304 19:32:01.318689 22579586809984 run.py:483] Algo bellman_ford step 8337 current loss 0.039796, current_train_items 266816.
I0304 19:32:01.350641 22579586809984 run.py:483] Algo bellman_ford step 8338 current loss 0.099100, current_train_items 266848.
I0304 19:32:01.383567 22579586809984 run.py:483] Algo bellman_ford step 8339 current loss 0.046427, current_train_items 266880.
I0304 19:32:01.403320 22579586809984 run.py:483] Algo bellman_ford step 8340 current loss 0.003531, current_train_items 266912.
I0304 19:32:01.419661 22579586809984 run.py:483] Algo bellman_ford step 8341 current loss 0.006403, current_train_items 266944.
I0304 19:32:01.443553 22579586809984 run.py:483] Algo bellman_ford step 8342 current loss 0.044215, current_train_items 266976.
I0304 19:32:01.474633 22579586809984 run.py:483] Algo bellman_ford step 8343 current loss 0.098018, current_train_items 267008.
I0304 19:32:01.509593 22579586809984 run.py:483] Algo bellman_ford step 8344 current loss 0.081342, current_train_items 267040.
I0304 19:32:01.529448 22579586809984 run.py:483] Algo bellman_ford step 8345 current loss 0.003415, current_train_items 267072.
I0304 19:32:01.545886 22579586809984 run.py:483] Algo bellman_ford step 8346 current loss 0.009918, current_train_items 267104.
I0304 19:32:01.571125 22579586809984 run.py:483] Algo bellman_ford step 8347 current loss 0.139641, current_train_items 267136.
I0304 19:32:01.603806 22579586809984 run.py:483] Algo bellman_ford step 8348 current loss 0.060710, current_train_items 267168.
I0304 19:32:01.638764 22579586809984 run.py:483] Algo bellman_ford step 8349 current loss 0.081651, current_train_items 267200.
I0304 19:32:01.658838 22579586809984 run.py:483] Algo bellman_ford step 8350 current loss 0.004421, current_train_items 267232.
I0304 19:32:01.667148 22579586809984 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0304 19:32:01.667289 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:01.684733 22579586809984 run.py:483] Algo bellman_ford step 8351 current loss 0.009905, current_train_items 267264.
I0304 19:32:01.710112 22579586809984 run.py:483] Algo bellman_ford step 8352 current loss 0.035625, current_train_items 267296.
I0304 19:32:01.742846 22579586809984 run.py:483] Algo bellman_ford step 8353 current loss 0.028886, current_train_items 267328.
I0304 19:32:01.779171 22579586809984 run.py:483] Algo bellman_ford step 8354 current loss 0.151480, current_train_items 267360.
I0304 19:32:01.799143 22579586809984 run.py:483] Algo bellman_ford step 8355 current loss 0.004496, current_train_items 267392.
I0304 19:32:01.814931 22579586809984 run.py:483] Algo bellman_ford step 8356 current loss 0.023295, current_train_items 267424.
I0304 19:32:01.839131 22579586809984 run.py:483] Algo bellman_ford step 8357 current loss 0.029291, current_train_items 267456.
I0304 19:32:01.871380 22579586809984 run.py:483] Algo bellman_ford step 8358 current loss 0.041289, current_train_items 267488.
I0304 19:32:01.904397 22579586809984 run.py:483] Algo bellman_ford step 8359 current loss 0.051529, current_train_items 267520.
I0304 19:32:01.924551 22579586809984 run.py:483] Algo bellman_ford step 8360 current loss 0.002545, current_train_items 267552.
I0304 19:32:01.940786 22579586809984 run.py:483] Algo bellman_ford step 8361 current loss 0.006937, current_train_items 267584.
I0304 19:32:01.965056 22579586809984 run.py:483] Algo bellman_ford step 8362 current loss 0.019223, current_train_items 267616.
I0304 19:32:01.996796 22579586809984 run.py:483] Algo bellman_ford step 8363 current loss 0.037785, current_train_items 267648.
I0304 19:32:02.031012 22579586809984 run.py:483] Algo bellman_ford step 8364 current loss 0.055462, current_train_items 267680.
I0304 19:32:02.050639 22579586809984 run.py:483] Algo bellman_ford step 8365 current loss 0.003087, current_train_items 267712.
I0304 19:32:02.067384 22579586809984 run.py:483] Algo bellman_ford step 8366 current loss 0.005729, current_train_items 267744.
I0304 19:32:02.091288 22579586809984 run.py:483] Algo bellman_ford step 8367 current loss 0.016725, current_train_items 267776.
I0304 19:32:02.123486 22579586809984 run.py:483] Algo bellman_ford step 8368 current loss 0.038158, current_train_items 267808.
I0304 19:32:02.159785 22579586809984 run.py:483] Algo bellman_ford step 8369 current loss 0.079772, current_train_items 267840.
I0304 19:32:02.179989 22579586809984 run.py:483] Algo bellman_ford step 8370 current loss 0.002560, current_train_items 267872.
I0304 19:32:02.196577 22579586809984 run.py:483] Algo bellman_ford step 8371 current loss 0.016218, current_train_items 267904.
I0304 19:32:02.220176 22579586809984 run.py:483] Algo bellman_ford step 8372 current loss 0.064330, current_train_items 267936.
I0304 19:32:02.251976 22579586809984 run.py:483] Algo bellman_ford step 8373 current loss 0.028767, current_train_items 267968.
I0304 19:32:02.286736 22579586809984 run.py:483] Algo bellman_ford step 8374 current loss 0.025970, current_train_items 268000.
I0304 19:32:02.307257 22579586809984 run.py:483] Algo bellman_ford step 8375 current loss 0.003318, current_train_items 268032.
I0304 19:32:02.323262 22579586809984 run.py:483] Algo bellman_ford step 8376 current loss 0.005807, current_train_items 268064.
I0304 19:32:02.346685 22579586809984 run.py:483] Algo bellman_ford step 8377 current loss 0.029980, current_train_items 268096.
I0304 19:32:02.379014 22579586809984 run.py:483] Algo bellman_ford step 8378 current loss 0.045822, current_train_items 268128.
I0304 19:32:02.413486 22579586809984 run.py:483] Algo bellman_ford step 8379 current loss 0.049358, current_train_items 268160.
I0304 19:32:02.433324 22579586809984 run.py:483] Algo bellman_ford step 8380 current loss 0.013999, current_train_items 268192.
I0304 19:32:02.449806 22579586809984 run.py:483] Algo bellman_ford step 8381 current loss 0.008501, current_train_items 268224.
I0304 19:32:02.473356 22579586809984 run.py:483] Algo bellman_ford step 8382 current loss 0.058212, current_train_items 268256.
I0304 19:32:02.506154 22579586809984 run.py:483] Algo bellman_ford step 8383 current loss 0.056215, current_train_items 268288.
I0304 19:32:02.538885 22579586809984 run.py:483] Algo bellman_ford step 8384 current loss 0.033533, current_train_items 268320.
I0304 19:32:02.558840 22579586809984 run.py:483] Algo bellman_ford step 8385 current loss 0.005619, current_train_items 268352.
I0304 19:32:02.574749 22579586809984 run.py:483] Algo bellman_ford step 8386 current loss 0.008097, current_train_items 268384.
I0304 19:32:02.598402 22579586809984 run.py:483] Algo bellman_ford step 8387 current loss 0.022659, current_train_items 268416.
I0304 19:32:02.630622 22579586809984 run.py:483] Algo bellman_ford step 8388 current loss 0.037084, current_train_items 268448.
I0304 19:32:02.664347 22579586809984 run.py:483] Algo bellman_ford step 8389 current loss 0.059741, current_train_items 268480.
I0304 19:32:02.684754 22579586809984 run.py:483] Algo bellman_ford step 8390 current loss 0.003840, current_train_items 268512.
I0304 19:32:02.701216 22579586809984 run.py:483] Algo bellman_ford step 8391 current loss 0.009539, current_train_items 268544.
I0304 19:32:02.724766 22579586809984 run.py:483] Algo bellman_ford step 8392 current loss 0.050621, current_train_items 268576.
I0304 19:32:02.758201 22579586809984 run.py:483] Algo bellman_ford step 8393 current loss 0.070579, current_train_items 268608.
I0304 19:32:02.791036 22579586809984 run.py:483] Algo bellman_ford step 8394 current loss 0.031206, current_train_items 268640.
I0304 19:32:02.810952 22579586809984 run.py:483] Algo bellman_ford step 8395 current loss 0.003066, current_train_items 268672.
I0304 19:32:02.827313 22579586809984 run.py:483] Algo bellman_ford step 8396 current loss 0.055143, current_train_items 268704.
I0304 19:32:02.851644 22579586809984 run.py:483] Algo bellman_ford step 8397 current loss 0.022832, current_train_items 268736.
I0304 19:32:02.884427 22579586809984 run.py:483] Algo bellman_ford step 8398 current loss 0.044744, current_train_items 268768.
I0304 19:32:02.919239 22579586809984 run.py:483] Algo bellman_ford step 8399 current loss 0.049852, current_train_items 268800.
I0304 19:32:02.939361 22579586809984 run.py:483] Algo bellman_ford step 8400 current loss 0.002799, current_train_items 268832.
I0304 19:32:02.946966 22579586809984 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0304 19:32:02.947072 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:32:02.964273 22579586809984 run.py:483] Algo bellman_ford step 8401 current loss 0.024407, current_train_items 268864.
I0304 19:32:02.989911 22579586809984 run.py:483] Algo bellman_ford step 8402 current loss 0.043244, current_train_items 268896.
I0304 19:32:03.023093 22579586809984 run.py:483] Algo bellman_ford step 8403 current loss 0.048999, current_train_items 268928.
I0304 19:32:03.060135 22579586809984 run.py:483] Algo bellman_ford step 8404 current loss 0.050439, current_train_items 268960.
I0304 19:32:03.080281 22579586809984 run.py:483] Algo bellman_ford step 8405 current loss 0.001951, current_train_items 268992.
I0304 19:32:03.096445 22579586809984 run.py:483] Algo bellman_ford step 8406 current loss 0.018469, current_train_items 269024.
I0304 19:32:03.120260 22579586809984 run.py:483] Algo bellman_ford step 8407 current loss 0.036297, current_train_items 269056.
I0304 19:32:03.152063 22579586809984 run.py:483] Algo bellman_ford step 8408 current loss 0.060290, current_train_items 269088.
I0304 19:32:03.187079 22579586809984 run.py:483] Algo bellman_ford step 8409 current loss 0.054137, current_train_items 269120.
I0304 19:32:03.206821 22579586809984 run.py:483] Algo bellman_ford step 8410 current loss 0.002996, current_train_items 269152.
I0304 19:32:03.223167 22579586809984 run.py:483] Algo bellman_ford step 8411 current loss 0.014632, current_train_items 269184.
I0304 19:32:03.248023 22579586809984 run.py:483] Algo bellman_ford step 8412 current loss 0.070674, current_train_items 269216.
I0304 19:32:03.281221 22579586809984 run.py:483] Algo bellman_ford step 8413 current loss 0.098615, current_train_items 269248.
I0304 19:32:03.315937 22579586809984 run.py:483] Algo bellman_ford step 8414 current loss 0.087909, current_train_items 269280.
I0304 19:32:03.336050 22579586809984 run.py:483] Algo bellman_ford step 8415 current loss 0.004184, current_train_items 269312.
I0304 19:32:03.352440 22579586809984 run.py:483] Algo bellman_ford step 8416 current loss 0.004856, current_train_items 269344.
I0304 19:32:03.376930 22579586809984 run.py:483] Algo bellman_ford step 8417 current loss 0.052642, current_train_items 269376.
I0304 19:32:03.409919 22579586809984 run.py:483] Algo bellman_ford step 8418 current loss 0.042403, current_train_items 269408.
I0304 19:32:03.447724 22579586809984 run.py:483] Algo bellman_ford step 8419 current loss 0.099949, current_train_items 269440.
I0304 19:32:03.467705 22579586809984 run.py:483] Algo bellman_ford step 8420 current loss 0.005547, current_train_items 269472.
I0304 19:32:03.484094 22579586809984 run.py:483] Algo bellman_ford step 8421 current loss 0.035221, current_train_items 269504.
I0304 19:32:03.509536 22579586809984 run.py:483] Algo bellman_ford step 8422 current loss 0.059168, current_train_items 269536.
I0304 19:32:03.542120 22579586809984 run.py:483] Algo bellman_ford step 8423 current loss 0.044689, current_train_items 269568.
I0304 19:32:03.577489 22579586809984 run.py:483] Algo bellman_ford step 8424 current loss 0.032866, current_train_items 269600.
I0304 19:32:03.597363 22579586809984 run.py:483] Algo bellman_ford step 8425 current loss 0.005432, current_train_items 269632.
I0304 19:32:03.613312 22579586809984 run.py:483] Algo bellman_ford step 8426 current loss 0.082768, current_train_items 269664.
I0304 19:32:03.637752 22579586809984 run.py:483] Algo bellman_ford step 8427 current loss 0.079150, current_train_items 269696.
I0304 19:32:03.669831 22579586809984 run.py:483] Algo bellman_ford step 8428 current loss 0.101789, current_train_items 269728.
I0304 19:32:03.703837 22579586809984 run.py:483] Algo bellman_ford step 8429 current loss 0.057581, current_train_items 269760.
I0304 19:32:03.723702 22579586809984 run.py:483] Algo bellman_ford step 8430 current loss 0.010688, current_train_items 269792.
I0304 19:32:03.740388 22579586809984 run.py:483] Algo bellman_ford step 8431 current loss 0.042129, current_train_items 269824.
I0304 19:32:03.764823 22579586809984 run.py:483] Algo bellman_ford step 8432 current loss 0.053528, current_train_items 269856.
I0304 19:32:03.798422 22579586809984 run.py:483] Algo bellman_ford step 8433 current loss 0.099615, current_train_items 269888.
I0304 19:32:03.830761 22579586809984 run.py:483] Algo bellman_ford step 8434 current loss 0.028118, current_train_items 269920.
I0304 19:32:03.850365 22579586809984 run.py:483] Algo bellman_ford step 8435 current loss 0.002843, current_train_items 269952.
I0304 19:32:03.866334 22579586809984 run.py:483] Algo bellman_ford step 8436 current loss 0.019244, current_train_items 269984.
I0304 19:32:03.890910 22579586809984 run.py:483] Algo bellman_ford step 8437 current loss 0.027104, current_train_items 270016.
I0304 19:32:03.922956 22579586809984 run.py:483] Algo bellman_ford step 8438 current loss 0.054892, current_train_items 270048.
I0304 19:32:03.955562 22579586809984 run.py:483] Algo bellman_ford step 8439 current loss 0.061568, current_train_items 270080.
I0304 19:32:03.975253 22579586809984 run.py:483] Algo bellman_ford step 8440 current loss 0.003094, current_train_items 270112.
I0304 19:32:03.991802 22579586809984 run.py:483] Algo bellman_ford step 8441 current loss 0.014100, current_train_items 270144.
I0304 19:32:04.016529 22579586809984 run.py:483] Algo bellman_ford step 8442 current loss 0.023387, current_train_items 270176.
I0304 19:32:04.048552 22579586809984 run.py:483] Algo bellman_ford step 8443 current loss 0.034529, current_train_items 270208.
I0304 19:32:04.083216 22579586809984 run.py:483] Algo bellman_ford step 8444 current loss 0.049868, current_train_items 270240.
I0304 19:32:04.103058 22579586809984 run.py:483] Algo bellman_ford step 8445 current loss 0.003081, current_train_items 270272.
I0304 19:32:04.119136 22579586809984 run.py:483] Algo bellman_ford step 8446 current loss 0.010961, current_train_items 270304.
I0304 19:32:04.143424 22579586809984 run.py:483] Algo bellman_ford step 8447 current loss 0.036530, current_train_items 270336.
I0304 19:32:04.175257 22579586809984 run.py:483] Algo bellman_ford step 8448 current loss 0.036892, current_train_items 270368.
I0304 19:32:04.209822 22579586809984 run.py:483] Algo bellman_ford step 8449 current loss 0.069110, current_train_items 270400.
I0304 19:32:04.229336 22579586809984 run.py:483] Algo bellman_ford step 8450 current loss 0.002487, current_train_items 270432.
I0304 19:32:04.237080 22579586809984 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0304 19:32:04.237185 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:04.253928 22579586809984 run.py:483] Algo bellman_ford step 8451 current loss 0.008562, current_train_items 270464.
I0304 19:32:04.278788 22579586809984 run.py:483] Algo bellman_ford step 8452 current loss 0.056198, current_train_items 270496.
I0304 19:32:04.311833 22579586809984 run.py:483] Algo bellman_ford step 8453 current loss 0.094699, current_train_items 270528.
I0304 19:32:04.346333 22579586809984 run.py:483] Algo bellman_ford step 8454 current loss 0.084422, current_train_items 270560.
I0304 19:32:04.366345 22579586809984 run.py:483] Algo bellman_ford step 8455 current loss 0.005912, current_train_items 270592.
I0304 19:32:04.382652 22579586809984 run.py:483] Algo bellman_ford step 8456 current loss 0.023775, current_train_items 270624.
I0304 19:32:04.407368 22579586809984 run.py:483] Algo bellman_ford step 8457 current loss 0.061361, current_train_items 270656.
I0304 19:32:04.437880 22579586809984 run.py:483] Algo bellman_ford step 8458 current loss 0.057305, current_train_items 270688.
I0304 19:32:04.469341 22579586809984 run.py:483] Algo bellman_ford step 8459 current loss 0.066606, current_train_items 270720.
I0304 19:32:04.489218 22579586809984 run.py:483] Algo bellman_ford step 8460 current loss 0.005229, current_train_items 270752.
I0304 19:32:04.505362 22579586809984 run.py:483] Algo bellman_ford step 8461 current loss 0.012596, current_train_items 270784.
I0304 19:32:04.528218 22579586809984 run.py:483] Algo bellman_ford step 8462 current loss 0.022646, current_train_items 270816.
I0304 19:32:04.559175 22579586809984 run.py:483] Algo bellman_ford step 8463 current loss 0.028726, current_train_items 270848.
I0304 19:32:04.593614 22579586809984 run.py:483] Algo bellman_ford step 8464 current loss 0.062632, current_train_items 270880.
I0304 19:32:04.612976 22579586809984 run.py:483] Algo bellman_ford step 8465 current loss 0.002534, current_train_items 270912.
I0304 19:32:04.629619 22579586809984 run.py:483] Algo bellman_ford step 8466 current loss 0.052866, current_train_items 270944.
I0304 19:32:04.653373 22579586809984 run.py:483] Algo bellman_ford step 8467 current loss 0.031741, current_train_items 270976.
I0304 19:32:04.684613 22579586809984 run.py:483] Algo bellman_ford step 8468 current loss 0.029475, current_train_items 271008.
I0304 19:32:04.717635 22579586809984 run.py:483] Algo bellman_ford step 8469 current loss 0.035912, current_train_items 271040.
I0304 19:32:04.737480 22579586809984 run.py:483] Algo bellman_ford step 8470 current loss 0.003311, current_train_items 271072.
I0304 19:32:04.754211 22579586809984 run.py:483] Algo bellman_ford step 8471 current loss 0.042226, current_train_items 271104.
I0304 19:32:04.777870 22579586809984 run.py:483] Algo bellman_ford step 8472 current loss 0.013746, current_train_items 271136.
I0304 19:32:04.807272 22579586809984 run.py:483] Algo bellman_ford step 8473 current loss 0.019614, current_train_items 271168.
I0304 19:32:04.841327 22579586809984 run.py:483] Algo bellman_ford step 8474 current loss 0.066404, current_train_items 271200.
I0304 19:32:04.861227 22579586809984 run.py:483] Algo bellman_ford step 8475 current loss 0.003865, current_train_items 271232.
I0304 19:32:04.877816 22579586809984 run.py:483] Algo bellman_ford step 8476 current loss 0.026768, current_train_items 271264.
I0304 19:32:04.902927 22579586809984 run.py:483] Algo bellman_ford step 8477 current loss 0.083582, current_train_items 271296.
I0304 19:32:04.934866 22579586809984 run.py:483] Algo bellman_ford step 8478 current loss 0.042158, current_train_items 271328.
I0304 19:32:04.969616 22579586809984 run.py:483] Algo bellman_ford step 8479 current loss 0.053070, current_train_items 271360.
I0304 19:32:04.988904 22579586809984 run.py:483] Algo bellman_ford step 8480 current loss 0.003848, current_train_items 271392.
I0304 19:32:05.005290 22579586809984 run.py:483] Algo bellman_ford step 8481 current loss 0.028581, current_train_items 271424.
I0304 19:32:05.029769 22579586809984 run.py:483] Algo bellman_ford step 8482 current loss 0.049007, current_train_items 271456.
I0304 19:32:05.061773 22579586809984 run.py:483] Algo bellman_ford step 8483 current loss 0.058024, current_train_items 271488.
I0304 19:32:05.094238 22579586809984 run.py:483] Algo bellman_ford step 8484 current loss 0.059651, current_train_items 271520.
I0304 19:32:05.114151 22579586809984 run.py:483] Algo bellman_ford step 8485 current loss 0.004250, current_train_items 271552.
I0304 19:32:05.130186 22579586809984 run.py:483] Algo bellman_ford step 8486 current loss 0.006189, current_train_items 271584.
I0304 19:32:05.153870 22579586809984 run.py:483] Algo bellman_ford step 8487 current loss 0.016368, current_train_items 271616.
I0304 19:32:05.186393 22579586809984 run.py:483] Algo bellman_ford step 8488 current loss 0.064154, current_train_items 271648.
I0304 19:32:05.220266 22579586809984 run.py:483] Algo bellman_ford step 8489 current loss 0.043368, current_train_items 271680.
I0304 19:32:05.240548 22579586809984 run.py:483] Algo bellman_ford step 8490 current loss 0.006300, current_train_items 271712.
I0304 19:32:05.256645 22579586809984 run.py:483] Algo bellman_ford step 8491 current loss 0.035420, current_train_items 271744.
I0304 19:32:05.281075 22579586809984 run.py:483] Algo bellman_ford step 8492 current loss 0.028033, current_train_items 271776.
I0304 19:32:05.312331 22579586809984 run.py:483] Algo bellman_ford step 8493 current loss 0.030509, current_train_items 271808.
I0304 19:32:05.345279 22579586809984 run.py:483] Algo bellman_ford step 8494 current loss 0.084977, current_train_items 271840.
I0304 19:32:05.364809 22579586809984 run.py:483] Algo bellman_ford step 8495 current loss 0.006240, current_train_items 271872.
I0304 19:32:05.381503 22579586809984 run.py:483] Algo bellman_ford step 8496 current loss 0.046271, current_train_items 271904.
I0304 19:32:05.405830 22579586809984 run.py:483] Algo bellman_ford step 8497 current loss 0.020839, current_train_items 271936.
I0304 19:32:05.438073 22579586809984 run.py:483] Algo bellman_ford step 8498 current loss 0.032669, current_train_items 271968.
I0304 19:32:05.470653 22579586809984 run.py:483] Algo bellman_ford step 8499 current loss 0.035712, current_train_items 272000.
I0304 19:32:05.490609 22579586809984 run.py:483] Algo bellman_ford step 8500 current loss 0.003872, current_train_items 272032.
I0304 19:32:05.498568 22579586809984 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0304 19:32:05.498673 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:05.516350 22579586809984 run.py:483] Algo bellman_ford step 8501 current loss 0.030052, current_train_items 272064.
I0304 19:32:05.541861 22579586809984 run.py:483] Algo bellman_ford step 8502 current loss 0.031250, current_train_items 272096.
I0304 19:32:05.573459 22579586809984 run.py:483] Algo bellman_ford step 8503 current loss 0.040624, current_train_items 272128.
I0304 19:32:05.606402 22579586809984 run.py:483] Algo bellman_ford step 8504 current loss 0.051977, current_train_items 272160.
I0304 19:32:05.626782 22579586809984 run.py:483] Algo bellman_ford step 8505 current loss 0.003303, current_train_items 272192.
I0304 19:32:05.642315 22579586809984 run.py:483] Algo bellman_ford step 8506 current loss 0.003717, current_train_items 272224.
I0304 19:32:05.665246 22579586809984 run.py:483] Algo bellman_ford step 8507 current loss 0.048277, current_train_items 272256.
I0304 19:32:05.697820 22579586809984 run.py:483] Algo bellman_ford step 8508 current loss 0.046728, current_train_items 272288.
I0304 19:32:05.733341 22579586809984 run.py:483] Algo bellman_ford step 8509 current loss 0.073629, current_train_items 272320.
I0304 19:32:05.753462 22579586809984 run.py:483] Algo bellman_ford step 8510 current loss 0.002080, current_train_items 272352.
I0304 19:32:05.769330 22579586809984 run.py:483] Algo bellman_ford step 8511 current loss 0.037186, current_train_items 272384.
I0304 19:32:05.794597 22579586809984 run.py:483] Algo bellman_ford step 8512 current loss 0.026379, current_train_items 272416.
I0304 19:32:05.826281 22579586809984 run.py:483] Algo bellman_ford step 8513 current loss 0.036091, current_train_items 272448.
I0304 19:32:05.861094 22579586809984 run.py:483] Algo bellman_ford step 8514 current loss 0.042561, current_train_items 272480.
I0304 19:32:05.880879 22579586809984 run.py:483] Algo bellman_ford step 8515 current loss 0.003747, current_train_items 272512.
I0304 19:32:05.897224 22579586809984 run.py:483] Algo bellman_ford step 8516 current loss 0.015225, current_train_items 272544.
I0304 19:32:05.921789 22579586809984 run.py:483] Algo bellman_ford step 8517 current loss 0.108284, current_train_items 272576.
I0304 19:32:05.955302 22579586809984 run.py:483] Algo bellman_ford step 8518 current loss 0.055172, current_train_items 272608.
I0304 19:32:05.988412 22579586809984 run.py:483] Algo bellman_ford step 8519 current loss 0.053205, current_train_items 272640.
I0304 19:32:06.008584 22579586809984 run.py:483] Algo bellman_ford step 8520 current loss 0.003138, current_train_items 272672.
I0304 19:32:06.024707 22579586809984 run.py:483] Algo bellman_ford step 8521 current loss 0.008695, current_train_items 272704.
I0304 19:32:06.048905 22579586809984 run.py:483] Algo bellman_ford step 8522 current loss 0.088956, current_train_items 272736.
I0304 19:32:06.080546 22579586809984 run.py:483] Algo bellman_ford step 8523 current loss 0.078888, current_train_items 272768.
I0304 19:32:06.115488 22579586809984 run.py:483] Algo bellman_ford step 8524 current loss 0.167089, current_train_items 272800.
I0304 19:32:06.135429 22579586809984 run.py:483] Algo bellman_ford step 8525 current loss 0.005830, current_train_items 272832.
I0304 19:32:06.151824 22579586809984 run.py:483] Algo bellman_ford step 8526 current loss 0.027053, current_train_items 272864.
I0304 19:32:06.175998 22579586809984 run.py:483] Algo bellman_ford step 8527 current loss 0.078697, current_train_items 272896.
I0304 19:32:06.207850 22579586809984 run.py:483] Algo bellman_ford step 8528 current loss 0.029513, current_train_items 272928.
I0304 19:32:06.242482 22579586809984 run.py:483] Algo bellman_ford step 8529 current loss 0.140168, current_train_items 272960.
I0304 19:32:06.262486 22579586809984 run.py:483] Algo bellman_ford step 8530 current loss 0.003496, current_train_items 272992.
I0304 19:32:06.278860 22579586809984 run.py:483] Algo bellman_ford step 8531 current loss 0.046771, current_train_items 273024.
I0304 19:32:06.302506 22579586809984 run.py:483] Algo bellman_ford step 8532 current loss 0.056087, current_train_items 273056.
I0304 19:32:06.334059 22579586809984 run.py:483] Algo bellman_ford step 8533 current loss 0.052452, current_train_items 273088.
I0304 19:32:06.366979 22579586809984 run.py:483] Algo bellman_ford step 8534 current loss 0.076560, current_train_items 273120.
I0304 19:32:06.386651 22579586809984 run.py:483] Algo bellman_ford step 8535 current loss 0.003747, current_train_items 273152.
I0304 19:32:06.402944 22579586809984 run.py:483] Algo bellman_ford step 8536 current loss 0.024413, current_train_items 273184.
I0304 19:32:06.427410 22579586809984 run.py:483] Algo bellman_ford step 8537 current loss 0.057933, current_train_items 273216.
I0304 19:32:06.459669 22579586809984 run.py:483] Algo bellman_ford step 8538 current loss 0.053698, current_train_items 273248.
I0304 19:32:06.495771 22579586809984 run.py:483] Algo bellman_ford step 8539 current loss 0.077171, current_train_items 273280.
I0304 19:32:06.515873 22579586809984 run.py:483] Algo bellman_ford step 8540 current loss 0.026043, current_train_items 273312.
I0304 19:32:06.532117 22579586809984 run.py:483] Algo bellman_ford step 8541 current loss 0.033634, current_train_items 273344.
I0304 19:32:06.557130 22579586809984 run.py:483] Algo bellman_ford step 8542 current loss 0.022076, current_train_items 273376.
I0304 19:32:06.589650 22579586809984 run.py:483] Algo bellman_ford step 8543 current loss 0.064495, current_train_items 273408.
I0304 19:32:06.623295 22579586809984 run.py:483] Algo bellman_ford step 8544 current loss 0.089308, current_train_items 273440.
I0304 19:32:06.642972 22579586809984 run.py:483] Algo bellman_ford step 8545 current loss 0.003435, current_train_items 273472.
I0304 19:32:06.658821 22579586809984 run.py:483] Algo bellman_ford step 8546 current loss 0.015615, current_train_items 273504.
I0304 19:32:06.683378 22579586809984 run.py:483] Algo bellman_ford step 8547 current loss 0.054701, current_train_items 273536.
I0304 19:32:06.714978 22579586809984 run.py:483] Algo bellman_ford step 8548 current loss 0.048893, current_train_items 273568.
I0304 19:32:06.749696 22579586809984 run.py:483] Algo bellman_ford step 8549 current loss 0.063832, current_train_items 273600.
I0304 19:32:06.769383 22579586809984 run.py:483] Algo bellman_ford step 8550 current loss 0.023811, current_train_items 273632.
I0304 19:32:06.777344 22579586809984 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0304 19:32:06.777449 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0304 19:32:06.794767 22579586809984 run.py:483] Algo bellman_ford step 8551 current loss 0.010280, current_train_items 273664.
I0304 19:32:06.820037 22579586809984 run.py:483] Algo bellman_ford step 8552 current loss 0.022520, current_train_items 273696.
I0304 19:32:06.852808 22579586809984 run.py:483] Algo bellman_ford step 8553 current loss 0.037410, current_train_items 273728.
I0304 19:32:06.884750 22579586809984 run.py:483] Algo bellman_ford step 8554 current loss 0.026111, current_train_items 273760.
I0304 19:32:06.904584 22579586809984 run.py:483] Algo bellman_ford step 8555 current loss 0.005461, current_train_items 273792.
I0304 19:32:06.920496 22579586809984 run.py:483] Algo bellman_ford step 8556 current loss 0.040782, current_train_items 273824.
I0304 19:32:06.944357 22579586809984 run.py:483] Algo bellman_ford step 8557 current loss 0.029065, current_train_items 273856.
I0304 19:32:06.976365 22579586809984 run.py:483] Algo bellman_ford step 8558 current loss 0.038673, current_train_items 273888.
I0304 19:32:07.010912 22579586809984 run.py:483] Algo bellman_ford step 8559 current loss 0.053184, current_train_items 273920.
I0304 19:32:07.030743 22579586809984 run.py:483] Algo bellman_ford step 8560 current loss 0.002195, current_train_items 273952.
I0304 19:32:07.047394 22579586809984 run.py:483] Algo bellman_ford step 8561 current loss 0.022252, current_train_items 273984.
I0304 19:32:07.071277 22579586809984 run.py:483] Algo bellman_ford step 8562 current loss 0.047365, current_train_items 274016.
I0304 19:32:07.104026 22579586809984 run.py:483] Algo bellman_ford step 8563 current loss 0.042210, current_train_items 274048.
I0304 19:32:07.136581 22579586809984 run.py:483] Algo bellman_ford step 8564 current loss 0.045237, current_train_items 274080.
I0304 19:32:07.155870 22579586809984 run.py:483] Algo bellman_ford step 8565 current loss 0.002081, current_train_items 274112.
I0304 19:32:07.172697 22579586809984 run.py:483] Algo bellman_ford step 8566 current loss 0.055462, current_train_items 274144.
I0304 19:32:07.198061 22579586809984 run.py:483] Algo bellman_ford step 8567 current loss 0.025702, current_train_items 274176.
I0304 19:32:07.229972 22579586809984 run.py:483] Algo bellman_ford step 8568 current loss 0.078349, current_train_items 274208.
I0304 19:32:07.264693 22579586809984 run.py:483] Algo bellman_ford step 8569 current loss 0.085780, current_train_items 274240.
I0304 19:32:07.284514 22579586809984 run.py:483] Algo bellman_ford step 8570 current loss 0.003851, current_train_items 274272.
I0304 19:32:07.300853 22579586809984 run.py:483] Algo bellman_ford step 8571 current loss 0.034450, current_train_items 274304.
I0304 19:32:07.324192 22579586809984 run.py:483] Algo bellman_ford step 8572 current loss 0.044898, current_train_items 274336.
I0304 19:32:07.356195 22579586809984 run.py:483] Algo bellman_ford step 8573 current loss 0.043511, current_train_items 274368.
I0304 19:32:07.389082 22579586809984 run.py:483] Algo bellman_ford step 8574 current loss 0.060221, current_train_items 274400.
I0304 19:32:07.408978 22579586809984 run.py:483] Algo bellman_ford step 8575 current loss 0.003405, current_train_items 274432.
I0304 19:32:07.425301 22579586809984 run.py:483] Algo bellman_ford step 8576 current loss 0.017611, current_train_items 274464.
I0304 19:32:07.449361 22579586809984 run.py:483] Algo bellman_ford step 8577 current loss 0.028203, current_train_items 274496.
I0304 19:32:07.481758 22579586809984 run.py:483] Algo bellman_ford step 8578 current loss 0.063392, current_train_items 274528.
I0304 19:32:07.516009 22579586809984 run.py:483] Algo bellman_ford step 8579 current loss 0.062308, current_train_items 274560.
I0304 19:32:07.535530 22579586809984 run.py:483] Algo bellman_ford step 8580 current loss 0.006486, current_train_items 274592.
I0304 19:32:07.551867 22579586809984 run.py:483] Algo bellman_ford step 8581 current loss 0.005094, current_train_items 274624.
I0304 19:32:07.574952 22579586809984 run.py:483] Algo bellman_ford step 8582 current loss 0.055807, current_train_items 274656.
I0304 19:32:07.606983 22579586809984 run.py:483] Algo bellman_ford step 8583 current loss 0.051641, current_train_items 274688.
I0304 19:32:07.642378 22579586809984 run.py:483] Algo bellman_ford step 8584 current loss 0.072525, current_train_items 274720.
I0304 19:32:07.662026 22579586809984 run.py:483] Algo bellman_ford step 8585 current loss 0.003602, current_train_items 274752.
I0304 19:32:07.678301 22579586809984 run.py:483] Algo bellman_ford step 8586 current loss 0.007325, current_train_items 274784.
I0304 19:32:07.703366 22579586809984 run.py:483] Algo bellman_ford step 8587 current loss 0.067126, current_train_items 274816.
I0304 19:32:07.735347 22579586809984 run.py:483] Algo bellman_ford step 8588 current loss 0.031734, current_train_items 274848.
I0304 19:32:07.768254 22579586809984 run.py:483] Algo bellman_ford step 8589 current loss 0.045313, current_train_items 274880.
I0304 19:32:07.788479 22579586809984 run.py:483] Algo bellman_ford step 8590 current loss 0.007750, current_train_items 274912.
I0304 19:32:07.804117 22579586809984 run.py:483] Algo bellman_ford step 8591 current loss 0.014780, current_train_items 274944.
I0304 19:32:07.828121 22579586809984 run.py:483] Algo bellman_ford step 8592 current loss 0.028607, current_train_items 274976.
I0304 19:32:07.860902 22579586809984 run.py:483] Algo bellman_ford step 8593 current loss 0.071278, current_train_items 275008.
I0304 19:32:07.894778 22579586809984 run.py:483] Algo bellman_ford step 8594 current loss 0.127789, current_train_items 275040.
I0304 19:32:07.914176 22579586809984 run.py:483] Algo bellman_ford step 8595 current loss 0.003297, current_train_items 275072.
I0304 19:32:07.930205 22579586809984 run.py:483] Algo bellman_ford step 8596 current loss 0.019662, current_train_items 275104.
I0304 19:32:07.954279 22579586809984 run.py:483] Algo bellman_ford step 8597 current loss 0.036843, current_train_items 275136.
I0304 19:32:07.985531 22579586809984 run.py:483] Algo bellman_ford step 8598 current loss 0.036088, current_train_items 275168.
I0304 19:32:08.020079 22579586809984 run.py:483] Algo bellman_ford step 8599 current loss 0.057890, current_train_items 275200.
I0304 19:32:08.039952 22579586809984 run.py:483] Algo bellman_ford step 8600 current loss 0.004351, current_train_items 275232.
I0304 19:32:08.047674 22579586809984 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0304 19:32:08.047787 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:08.064930 22579586809984 run.py:483] Algo bellman_ford step 8601 current loss 0.024642, current_train_items 275264.
I0304 19:32:08.090733 22579586809984 run.py:483] Algo bellman_ford step 8602 current loss 0.039532, current_train_items 275296.
I0304 19:32:08.124200 22579586809984 run.py:483] Algo bellman_ford step 8603 current loss 0.054436, current_train_items 275328.
I0304 19:32:08.158833 22579586809984 run.py:483] Algo bellman_ford step 8604 current loss 0.071983, current_train_items 275360.
I0304 19:32:08.179057 22579586809984 run.py:483] Algo bellman_ford step 8605 current loss 0.003385, current_train_items 275392.
I0304 19:32:08.195034 22579586809984 run.py:483] Algo bellman_ford step 8606 current loss 0.010373, current_train_items 275424.
I0304 19:32:08.218892 22579586809984 run.py:483] Algo bellman_ford step 8607 current loss 0.032371, current_train_items 275456.
I0304 19:32:08.251228 22579586809984 run.py:483] Algo bellman_ford step 8608 current loss 0.043500, current_train_items 275488.
I0304 19:32:08.286960 22579586809984 run.py:483] Algo bellman_ford step 8609 current loss 0.073761, current_train_items 275520.
I0304 19:32:08.307233 22579586809984 run.py:483] Algo bellman_ford step 8610 current loss 0.014035, current_train_items 275552.
I0304 19:32:08.323739 22579586809984 run.py:483] Algo bellman_ford step 8611 current loss 0.007831, current_train_items 275584.
I0304 19:32:08.348737 22579586809984 run.py:483] Algo bellman_ford step 8612 current loss 0.070370, current_train_items 275616.
I0304 19:32:08.380925 22579586809984 run.py:483] Algo bellman_ford step 8613 current loss 0.053699, current_train_items 275648.
I0304 19:32:08.413160 22579586809984 run.py:483] Algo bellman_ford step 8614 current loss 0.034376, current_train_items 275680.
I0304 19:32:08.433161 22579586809984 run.py:483] Algo bellman_ford step 8615 current loss 0.003544, current_train_items 275712.
I0304 19:32:08.449440 22579586809984 run.py:483] Algo bellman_ford step 8616 current loss 0.019664, current_train_items 275744.
I0304 19:32:08.473720 22579586809984 run.py:483] Algo bellman_ford step 8617 current loss 0.027943, current_train_items 275776.
I0304 19:32:08.506624 22579586809984 run.py:483] Algo bellman_ford step 8618 current loss 0.030283, current_train_items 275808.
I0304 19:32:08.540897 22579586809984 run.py:483] Algo bellman_ford step 8619 current loss 0.068562, current_train_items 275840.
I0304 19:32:08.560617 22579586809984 run.py:483] Algo bellman_ford step 8620 current loss 0.003189, current_train_items 275872.
I0304 19:32:08.576933 22579586809984 run.py:483] Algo bellman_ford step 8621 current loss 0.057450, current_train_items 275904.
I0304 19:32:08.601609 22579586809984 run.py:483] Algo bellman_ford step 8622 current loss 0.035621, current_train_items 275936.
I0304 19:32:08.634551 22579586809984 run.py:483] Algo bellman_ford step 8623 current loss 0.036825, current_train_items 275968.
I0304 19:32:08.668083 22579586809984 run.py:483] Algo bellman_ford step 8624 current loss 0.032749, current_train_items 276000.
I0304 19:32:08.688006 22579586809984 run.py:483] Algo bellman_ford step 8625 current loss 0.002482, current_train_items 276032.
I0304 19:32:08.704385 22579586809984 run.py:483] Algo bellman_ford step 8626 current loss 0.009254, current_train_items 276064.
I0304 19:32:08.729201 22579586809984 run.py:483] Algo bellman_ford step 8627 current loss 0.026477, current_train_items 276096.
I0304 19:32:08.762440 22579586809984 run.py:483] Algo bellman_ford step 8628 current loss 0.039595, current_train_items 276128.
I0304 19:32:08.795872 22579586809984 run.py:483] Algo bellman_ford step 8629 current loss 0.070949, current_train_items 276160.
I0304 19:32:08.815939 22579586809984 run.py:483] Algo bellman_ford step 8630 current loss 0.003124, current_train_items 276192.
I0304 19:32:08.832331 22579586809984 run.py:483] Algo bellman_ford step 8631 current loss 0.029363, current_train_items 276224.
I0304 19:32:08.856457 22579586809984 run.py:483] Algo bellman_ford step 8632 current loss 0.033041, current_train_items 276256.
I0304 19:32:08.888151 22579586809984 run.py:483] Algo bellman_ford step 8633 current loss 0.030582, current_train_items 276288.
I0304 19:32:08.922268 22579586809984 run.py:483] Algo bellman_ford step 8634 current loss 0.049384, current_train_items 276320.
I0304 19:32:08.942436 22579586809984 run.py:483] Algo bellman_ford step 8635 current loss 0.013221, current_train_items 276352.
I0304 19:32:08.958689 22579586809984 run.py:483] Algo bellman_ford step 8636 current loss 0.010724, current_train_items 276384.
I0304 19:32:08.983988 22579586809984 run.py:483] Algo bellman_ford step 8637 current loss 0.031012, current_train_items 276416.
I0304 19:32:09.015181 22579586809984 run.py:483] Algo bellman_ford step 8638 current loss 0.045290, current_train_items 276448.
I0304 19:32:09.049790 22579586809984 run.py:483] Algo bellman_ford step 8639 current loss 0.044933, current_train_items 276480.
I0304 19:32:09.069786 22579586809984 run.py:483] Algo bellman_ford step 8640 current loss 0.017376, current_train_items 276512.
I0304 19:32:09.086388 22579586809984 run.py:483] Algo bellman_ford step 8641 current loss 0.040514, current_train_items 276544.
I0304 19:32:09.110777 22579586809984 run.py:483] Algo bellman_ford step 8642 current loss 0.030044, current_train_items 276576.
I0304 19:32:09.143179 22579586809984 run.py:483] Algo bellman_ford step 8643 current loss 0.040210, current_train_items 276608.
I0304 19:32:09.177445 22579586809984 run.py:483] Algo bellman_ford step 8644 current loss 0.053371, current_train_items 276640.
I0304 19:32:09.196979 22579586809984 run.py:483] Algo bellman_ford step 8645 current loss 0.003412, current_train_items 276672.
I0304 19:32:09.213240 22579586809984 run.py:483] Algo bellman_ford step 8646 current loss 0.033086, current_train_items 276704.
I0304 19:32:09.237952 22579586809984 run.py:483] Algo bellman_ford step 8647 current loss 0.045084, current_train_items 276736.
I0304 19:32:09.271231 22579586809984 run.py:483] Algo bellman_ford step 8648 current loss 0.071998, current_train_items 276768.
I0304 19:32:09.304950 22579586809984 run.py:483] Algo bellman_ford step 8649 current loss 0.050648, current_train_items 276800.
I0304 19:32:09.324806 22579586809984 run.py:483] Algo bellman_ford step 8650 current loss 0.005422, current_train_items 276832.
I0304 19:32:09.332731 22579586809984 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0304 19:32:09.332835 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:32:09.350080 22579586809984 run.py:483] Algo bellman_ford step 8651 current loss 0.037262, current_train_items 276864.
I0304 19:32:09.375090 22579586809984 run.py:483] Algo bellman_ford step 8652 current loss 0.082841, current_train_items 276896.
I0304 19:32:09.407831 22579586809984 run.py:483] Algo bellman_ford step 8653 current loss 0.041115, current_train_items 276928.
I0304 19:32:09.443597 22579586809984 run.py:483] Algo bellman_ford step 8654 current loss 0.049279, current_train_items 276960.
I0304 19:32:09.463705 22579586809984 run.py:483] Algo bellman_ford step 8655 current loss 0.004095, current_train_items 276992.
I0304 19:32:09.479727 22579586809984 run.py:483] Algo bellman_ford step 8656 current loss 0.010557, current_train_items 277024.
I0304 19:32:09.505471 22579586809984 run.py:483] Algo bellman_ford step 8657 current loss 0.070669, current_train_items 277056.
I0304 19:32:09.538340 22579586809984 run.py:483] Algo bellman_ford step 8658 current loss 0.081183, current_train_items 277088.
I0304 19:32:09.572535 22579586809984 run.py:483] Algo bellman_ford step 8659 current loss 0.104331, current_train_items 277120.
I0304 19:32:09.592955 22579586809984 run.py:483] Algo bellman_ford step 8660 current loss 0.004964, current_train_items 277152.
I0304 19:32:09.609139 22579586809984 run.py:483] Algo bellman_ford step 8661 current loss 0.011219, current_train_items 277184.
I0304 19:32:09.632961 22579586809984 run.py:483] Algo bellman_ford step 8662 current loss 0.042449, current_train_items 277216.
I0304 19:32:09.664852 22579586809984 run.py:483] Algo bellman_ford step 8663 current loss 0.092821, current_train_items 277248.
I0304 19:32:09.699923 22579586809984 run.py:483] Algo bellman_ford step 8664 current loss 0.076663, current_train_items 277280.
I0304 19:32:09.719914 22579586809984 run.py:483] Algo bellman_ford step 8665 current loss 0.003763, current_train_items 277312.
I0304 19:32:09.735826 22579586809984 run.py:483] Algo bellman_ford step 8666 current loss 0.030791, current_train_items 277344.
I0304 19:32:09.760040 22579586809984 run.py:483] Algo bellman_ford step 8667 current loss 0.010686, current_train_items 277376.
I0304 19:32:09.791543 22579586809984 run.py:483] Algo bellman_ford step 8668 current loss 0.027580, current_train_items 277408.
I0304 19:32:09.824690 22579586809984 run.py:483] Algo bellman_ford step 8669 current loss 0.024434, current_train_items 277440.
I0304 19:32:09.845193 22579586809984 run.py:483] Algo bellman_ford step 8670 current loss 0.002698, current_train_items 277472.
I0304 19:32:09.861739 22579586809984 run.py:483] Algo bellman_ford step 8671 current loss 0.032483, current_train_items 277504.
I0304 19:32:09.885344 22579586809984 run.py:483] Algo bellman_ford step 8672 current loss 0.032815, current_train_items 277536.
I0304 19:32:09.917455 22579586809984 run.py:483] Algo bellman_ford step 8673 current loss 0.055050, current_train_items 277568.
I0304 19:32:09.949823 22579586809984 run.py:483] Algo bellman_ford step 8674 current loss 0.058140, current_train_items 277600.
I0304 19:32:09.969951 22579586809984 run.py:483] Algo bellman_ford step 8675 current loss 0.004721, current_train_items 277632.
I0304 19:32:09.986181 22579586809984 run.py:483] Algo bellman_ford step 8676 current loss 0.014657, current_train_items 277664.
I0304 19:32:10.010698 22579586809984 run.py:483] Algo bellman_ford step 8677 current loss 0.027664, current_train_items 277696.
I0304 19:32:10.043030 22579586809984 run.py:483] Algo bellman_ford step 8678 current loss 0.044776, current_train_items 277728.
I0304 19:32:10.076501 22579586809984 run.py:483] Algo bellman_ford step 8679 current loss 0.061449, current_train_items 277760.
I0304 19:32:10.096471 22579586809984 run.py:483] Algo bellman_ford step 8680 current loss 0.002595, current_train_items 277792.
I0304 19:32:10.112854 22579586809984 run.py:483] Algo bellman_ford step 8681 current loss 0.010184, current_train_items 277824.
I0304 19:32:10.137859 22579586809984 run.py:483] Algo bellman_ford step 8682 current loss 0.056238, current_train_items 277856.
I0304 19:32:10.169831 22579586809984 run.py:483] Algo bellman_ford step 8683 current loss 0.041216, current_train_items 277888.
I0304 19:32:10.203554 22579586809984 run.py:483] Algo bellman_ford step 8684 current loss 0.074108, current_train_items 277920.
I0304 19:32:10.223961 22579586809984 run.py:483] Algo bellman_ford step 8685 current loss 0.003168, current_train_items 277952.
I0304 19:32:10.239969 22579586809984 run.py:483] Algo bellman_ford step 8686 current loss 0.022038, current_train_items 277984.
I0304 19:32:10.264874 22579586809984 run.py:483] Algo bellman_ford step 8687 current loss 0.085531, current_train_items 278016.
I0304 19:32:10.297927 22579586809984 run.py:483] Algo bellman_ford step 8688 current loss 0.055515, current_train_items 278048.
I0304 19:32:10.331289 22579586809984 run.py:483] Algo bellman_ford step 8689 current loss 0.061708, current_train_items 278080.
I0304 19:32:10.351176 22579586809984 run.py:483] Algo bellman_ford step 8690 current loss 0.001626, current_train_items 278112.
I0304 19:32:10.367728 22579586809984 run.py:483] Algo bellman_ford step 8691 current loss 0.035620, current_train_items 278144.
I0304 19:32:10.391889 22579586809984 run.py:483] Algo bellman_ford step 8692 current loss 0.058168, current_train_items 278176.
I0304 19:32:10.424148 22579586809984 run.py:483] Algo bellman_ford step 8693 current loss 0.064056, current_train_items 278208.
I0304 19:32:10.458012 22579586809984 run.py:483] Algo bellman_ford step 8694 current loss 0.075621, current_train_items 278240.
I0304 19:32:10.477555 22579586809984 run.py:483] Algo bellman_ford step 8695 current loss 0.002725, current_train_items 278272.
I0304 19:32:10.493964 22579586809984 run.py:483] Algo bellman_ford step 8696 current loss 0.010214, current_train_items 278304.
I0304 19:32:10.518652 22579586809984 run.py:483] Algo bellman_ford step 8697 current loss 0.062874, current_train_items 278336.
I0304 19:32:10.551145 22579586809984 run.py:483] Algo bellman_ford step 8698 current loss 0.033342, current_train_items 278368.
I0304 19:32:10.586755 22579586809984 run.py:483] Algo bellman_ford step 8699 current loss 0.086620, current_train_items 278400.
I0304 19:32:10.606826 22579586809984 run.py:483] Algo bellman_ford step 8700 current loss 0.003732, current_train_items 278432.
I0304 19:32:10.614564 22579586809984 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0304 19:32:10.614669 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:32:10.631794 22579586809984 run.py:483] Algo bellman_ford step 8701 current loss 0.010709, current_train_items 278464.
I0304 19:32:10.656604 22579586809984 run.py:483] Algo bellman_ford step 8702 current loss 0.017263, current_train_items 278496.
I0304 19:32:10.689723 22579586809984 run.py:483] Algo bellman_ford step 8703 current loss 0.063617, current_train_items 278528.
I0304 19:32:10.725236 22579586809984 run.py:483] Algo bellman_ford step 8704 current loss 0.113245, current_train_items 278560.
I0304 19:32:10.745336 22579586809984 run.py:483] Algo bellman_ford step 8705 current loss 0.011236, current_train_items 278592.
I0304 19:32:10.761184 22579586809984 run.py:483] Algo bellman_ford step 8706 current loss 0.027324, current_train_items 278624.
I0304 19:32:10.785865 22579586809984 run.py:483] Algo bellman_ford step 8707 current loss 0.018966, current_train_items 278656.
I0304 19:32:10.818725 22579586809984 run.py:483] Algo bellman_ford step 8708 current loss 0.058768, current_train_items 278688.
I0304 19:32:10.852995 22579586809984 run.py:483] Algo bellman_ford step 8709 current loss 0.062770, current_train_items 278720.
I0304 19:32:10.872935 22579586809984 run.py:483] Algo bellman_ford step 8710 current loss 0.022671, current_train_items 278752.
I0304 19:32:10.889333 22579586809984 run.py:483] Algo bellman_ford step 8711 current loss 0.008485, current_train_items 278784.
I0304 19:32:10.913983 22579586809984 run.py:483] Algo bellman_ford step 8712 current loss 0.029541, current_train_items 278816.
I0304 19:32:10.948041 22579586809984 run.py:483] Algo bellman_ford step 8713 current loss 0.056144, current_train_items 278848.
I0304 19:32:10.980882 22579586809984 run.py:483] Algo bellman_ford step 8714 current loss 0.042474, current_train_items 278880.
I0304 19:32:11.000376 22579586809984 run.py:483] Algo bellman_ford step 8715 current loss 0.004334, current_train_items 278912.
I0304 19:32:11.016980 22579586809984 run.py:483] Algo bellman_ford step 8716 current loss 0.016471, current_train_items 278944.
I0304 19:32:11.041014 22579586809984 run.py:483] Algo bellman_ford step 8717 current loss 0.017051, current_train_items 278976.
I0304 19:32:11.072594 22579586809984 run.py:483] Algo bellman_ford step 8718 current loss 0.026622, current_train_items 279008.
I0304 19:32:11.105598 22579586809984 run.py:483] Algo bellman_ford step 8719 current loss 0.040304, current_train_items 279040.
I0304 19:32:11.125497 22579586809984 run.py:483] Algo bellman_ford step 8720 current loss 0.003394, current_train_items 279072.
I0304 19:32:11.142106 22579586809984 run.py:483] Algo bellman_ford step 8721 current loss 0.014935, current_train_items 279104.
I0304 19:32:11.167469 22579586809984 run.py:483] Algo bellman_ford step 8722 current loss 0.036343, current_train_items 279136.
I0304 19:32:11.199020 22579586809984 run.py:483] Algo bellman_ford step 8723 current loss 0.043997, current_train_items 279168.
I0304 19:32:11.232455 22579586809984 run.py:483] Algo bellman_ford step 8724 current loss 0.039431, current_train_items 279200.
I0304 19:32:11.252554 22579586809984 run.py:483] Algo bellman_ford step 8725 current loss 0.008449, current_train_items 279232.
I0304 19:32:11.269021 22579586809984 run.py:483] Algo bellman_ford step 8726 current loss 0.025130, current_train_items 279264.
I0304 19:32:11.293437 22579586809984 run.py:483] Algo bellman_ford step 8727 current loss 0.072272, current_train_items 279296.
I0304 19:32:11.325612 22579586809984 run.py:483] Algo bellman_ford step 8728 current loss 0.042461, current_train_items 279328.
I0304 19:32:11.359058 22579586809984 run.py:483] Algo bellman_ford step 8729 current loss 0.042960, current_train_items 279360.
I0304 19:32:11.378957 22579586809984 run.py:483] Algo bellman_ford step 8730 current loss 0.017030, current_train_items 279392.
I0304 19:32:11.394762 22579586809984 run.py:483] Algo bellman_ford step 8731 current loss 0.003251, current_train_items 279424.
I0304 19:32:11.419133 22579586809984 run.py:483] Algo bellman_ford step 8732 current loss 0.053532, current_train_items 279456.
I0304 19:32:11.451696 22579586809984 run.py:483] Algo bellman_ford step 8733 current loss 0.023097, current_train_items 279488.
I0304 19:32:11.487272 22579586809984 run.py:483] Algo bellman_ford step 8734 current loss 0.048961, current_train_items 279520.
I0304 19:32:11.506942 22579586809984 run.py:483] Algo bellman_ford step 8735 current loss 0.003253, current_train_items 279552.
I0304 19:32:11.523102 22579586809984 run.py:483] Algo bellman_ford step 8736 current loss 0.053089, current_train_items 279584.
I0304 19:32:11.548791 22579586809984 run.py:483] Algo bellman_ford step 8737 current loss 0.039304, current_train_items 279616.
I0304 19:32:11.580541 22579586809984 run.py:483] Algo bellman_ford step 8738 current loss 0.027522, current_train_items 279648.
I0304 19:32:11.615304 22579586809984 run.py:483] Algo bellman_ford step 8739 current loss 0.046096, current_train_items 279680.
I0304 19:32:11.635378 22579586809984 run.py:483] Algo bellman_ford step 8740 current loss 0.003205, current_train_items 279712.
I0304 19:32:11.651493 22579586809984 run.py:483] Algo bellman_ford step 8741 current loss 0.004838, current_train_items 279744.
I0304 19:32:11.676153 22579586809984 run.py:483] Algo bellman_ford step 8742 current loss 0.030847, current_train_items 279776.
I0304 19:32:11.708988 22579586809984 run.py:483] Algo bellman_ford step 8743 current loss 0.030154, current_train_items 279808.
I0304 19:32:11.743352 22579586809984 run.py:483] Algo bellman_ford step 8744 current loss 0.048126, current_train_items 279840.
I0304 19:32:11.763083 22579586809984 run.py:483] Algo bellman_ford step 8745 current loss 0.034418, current_train_items 279872.
I0304 19:32:11.779651 22579586809984 run.py:483] Algo bellman_ford step 8746 current loss 0.068880, current_train_items 279904.
I0304 19:32:11.803781 22579586809984 run.py:483] Algo bellman_ford step 8747 current loss 0.036373, current_train_items 279936.
I0304 19:32:11.838558 22579586809984 run.py:483] Algo bellman_ford step 8748 current loss 0.032403, current_train_items 279968.
I0304 19:32:11.873544 22579586809984 run.py:483] Algo bellman_ford step 8749 current loss 0.035929, current_train_items 280000.
I0304 19:32:11.892868 22579586809984 run.py:483] Algo bellman_ford step 8750 current loss 0.003050, current_train_items 280032.
I0304 19:32:11.900869 22579586809984 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0304 19:32:11.900974 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:11.917981 22579586809984 run.py:483] Algo bellman_ford step 8751 current loss 0.011471, current_train_items 280064.
I0304 19:32:11.942896 22579586809984 run.py:483] Algo bellman_ford step 8752 current loss 0.082676, current_train_items 280096.
I0304 19:32:11.976127 22579586809984 run.py:483] Algo bellman_ford step 8753 current loss 0.116806, current_train_items 280128.
I0304 19:32:12.010800 22579586809984 run.py:483] Algo bellman_ford step 8754 current loss 0.104249, current_train_items 280160.
I0304 19:32:12.031207 22579586809984 run.py:483] Algo bellman_ford step 8755 current loss 0.017288, current_train_items 280192.
I0304 19:32:12.047223 22579586809984 run.py:483] Algo bellman_ford step 8756 current loss 0.010491, current_train_items 280224.
I0304 19:32:12.072919 22579586809984 run.py:483] Algo bellman_ford step 8757 current loss 0.100924, current_train_items 280256.
I0304 19:32:12.104397 22579586809984 run.py:483] Algo bellman_ford step 8758 current loss 0.036118, current_train_items 280288.
I0304 19:32:12.138529 22579586809984 run.py:483] Algo bellman_ford step 8759 current loss 0.097889, current_train_items 280320.
I0304 19:32:12.158752 22579586809984 run.py:483] Algo bellman_ford step 8760 current loss 0.006452, current_train_items 280352.
I0304 19:32:12.175301 22579586809984 run.py:483] Algo bellman_ford step 8761 current loss 0.027684, current_train_items 280384.
I0304 19:32:12.200578 22579586809984 run.py:483] Algo bellman_ford step 8762 current loss 0.035237, current_train_items 280416.
I0304 19:32:12.232251 22579586809984 run.py:483] Algo bellman_ford step 8763 current loss 0.032359, current_train_items 280448.
I0304 19:32:12.267793 22579586809984 run.py:483] Algo bellman_ford step 8764 current loss 0.085030, current_train_items 280480.
I0304 19:32:12.287485 22579586809984 run.py:483] Algo bellman_ford step 8765 current loss 0.004663, current_train_items 280512.
I0304 19:32:12.304345 22579586809984 run.py:483] Algo bellman_ford step 8766 current loss 0.010860, current_train_items 280544.
I0304 19:32:12.327699 22579586809984 run.py:483] Algo bellman_ford step 8767 current loss 0.025846, current_train_items 280576.
I0304 19:32:12.359312 22579586809984 run.py:483] Algo bellman_ford step 8768 current loss 0.064023, current_train_items 280608.
I0304 19:32:12.391491 22579586809984 run.py:483] Algo bellman_ford step 8769 current loss 0.042952, current_train_items 280640.
I0304 19:32:12.411600 22579586809984 run.py:483] Algo bellman_ford step 8770 current loss 0.005120, current_train_items 280672.
I0304 19:32:12.428195 22579586809984 run.py:483] Algo bellman_ford step 8771 current loss 0.013381, current_train_items 280704.
I0304 19:32:12.452784 22579586809984 run.py:483] Algo bellman_ford step 8772 current loss 0.037953, current_train_items 280736.
I0304 19:32:12.485513 22579586809984 run.py:483] Algo bellman_ford step 8773 current loss 0.080288, current_train_items 280768.
I0304 19:32:12.518786 22579586809984 run.py:483] Algo bellman_ford step 8774 current loss 0.048789, current_train_items 280800.
I0304 19:32:12.538678 22579586809984 run.py:483] Algo bellman_ford step 8775 current loss 0.004936, current_train_items 280832.
I0304 19:32:12.555129 22579586809984 run.py:483] Algo bellman_ford step 8776 current loss 0.037237, current_train_items 280864.
I0304 19:32:12.578773 22579586809984 run.py:483] Algo bellman_ford step 8777 current loss 0.035864, current_train_items 280896.
I0304 19:32:12.611007 22579586809984 run.py:483] Algo bellman_ford step 8778 current loss 0.073485, current_train_items 280928.
I0304 19:32:12.645603 22579586809984 run.py:483] Algo bellman_ford step 8779 current loss 0.041812, current_train_items 280960.
I0304 19:32:12.665198 22579586809984 run.py:483] Algo bellman_ford step 8780 current loss 0.003102, current_train_items 280992.
I0304 19:32:12.681352 22579586809984 run.py:483] Algo bellman_ford step 8781 current loss 0.084637, current_train_items 281024.
I0304 19:32:12.705639 22579586809984 run.py:483] Algo bellman_ford step 8782 current loss 0.051981, current_train_items 281056.
I0304 19:32:12.738504 22579586809984 run.py:483] Algo bellman_ford step 8783 current loss 0.057257, current_train_items 281088.
I0304 19:32:12.773318 22579586809984 run.py:483] Algo bellman_ford step 8784 current loss 0.060165, current_train_items 281120.
I0304 19:32:12.793099 22579586809984 run.py:483] Algo bellman_ford step 8785 current loss 0.001973, current_train_items 281152.
I0304 19:32:12.809381 22579586809984 run.py:483] Algo bellman_ford step 8786 current loss 0.046841, current_train_items 281184.
I0304 19:32:12.834109 22579586809984 run.py:483] Algo bellman_ford step 8787 current loss 0.078198, current_train_items 281216.
I0304 19:32:12.865848 22579586809984 run.py:483] Algo bellman_ford step 8788 current loss 0.058111, current_train_items 281248.
I0304 19:32:12.900046 22579586809984 run.py:483] Algo bellman_ford step 8789 current loss 0.060236, current_train_items 281280.
I0304 19:32:12.920128 22579586809984 run.py:483] Algo bellman_ford step 8790 current loss 0.044992, current_train_items 281312.
I0304 19:32:12.936922 22579586809984 run.py:483] Algo bellman_ford step 8791 current loss 0.021173, current_train_items 281344.
I0304 19:32:12.960976 22579586809984 run.py:483] Algo bellman_ford step 8792 current loss 0.026776, current_train_items 281376.
I0304 19:32:12.993413 22579586809984 run.py:483] Algo bellman_ford step 8793 current loss 0.055035, current_train_items 281408.
I0304 19:32:13.028168 22579586809984 run.py:483] Algo bellman_ford step 8794 current loss 0.108336, current_train_items 281440.
I0304 19:32:13.047979 22579586809984 run.py:483] Algo bellman_ford step 8795 current loss 0.005494, current_train_items 281472.
I0304 19:32:13.064258 22579586809984 run.py:483] Algo bellman_ford step 8796 current loss 0.016069, current_train_items 281504.
I0304 19:32:13.088215 22579586809984 run.py:483] Algo bellman_ford step 8797 current loss 0.026628, current_train_items 281536.
I0304 19:32:13.120883 22579586809984 run.py:483] Algo bellman_ford step 8798 current loss 0.043703, current_train_items 281568.
I0304 19:32:13.156163 22579586809984 run.py:483] Algo bellman_ford step 8799 current loss 0.101449, current_train_items 281600.
I0304 19:32:13.176042 22579586809984 run.py:483] Algo bellman_ford step 8800 current loss 0.005335, current_train_items 281632.
I0304 19:32:13.183800 22579586809984 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0304 19:32:13.183906 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:32:13.200485 22579586809984 run.py:483] Algo bellman_ford step 8801 current loss 0.020794, current_train_items 281664.
I0304 19:32:13.225519 22579586809984 run.py:483] Algo bellman_ford step 8802 current loss 0.050552, current_train_items 281696.
I0304 19:32:13.258996 22579586809984 run.py:483] Algo bellman_ford step 8803 current loss 0.054327, current_train_items 281728.
I0304 19:32:13.293101 22579586809984 run.py:483] Algo bellman_ford step 8804 current loss 0.031881, current_train_items 281760.
I0304 19:32:13.313402 22579586809984 run.py:483] Algo bellman_ford step 8805 current loss 0.003002, current_train_items 281792.
I0304 19:32:13.329401 22579586809984 run.py:483] Algo bellman_ford step 8806 current loss 0.021488, current_train_items 281824.
I0304 19:32:13.353914 22579586809984 run.py:483] Algo bellman_ford step 8807 current loss 0.044554, current_train_items 281856.
I0304 19:32:13.385789 22579586809984 run.py:483] Algo bellman_ford step 8808 current loss 0.077750, current_train_items 281888.
I0304 19:32:13.422087 22579586809984 run.py:483] Algo bellman_ford step 8809 current loss 0.061695, current_train_items 281920.
I0304 19:32:13.441833 22579586809984 run.py:483] Algo bellman_ford step 8810 current loss 0.004331, current_train_items 281952.
I0304 19:32:13.458305 22579586809984 run.py:483] Algo bellman_ford step 8811 current loss 0.015756, current_train_items 281984.
I0304 19:32:13.482962 22579586809984 run.py:483] Algo bellman_ford step 8812 current loss 0.057904, current_train_items 282016.
I0304 19:32:13.515478 22579586809984 run.py:483] Algo bellman_ford step 8813 current loss 0.056343, current_train_items 282048.
I0304 19:32:13.549627 22579586809984 run.py:483] Algo bellman_ford step 8814 current loss 0.105511, current_train_items 282080.
I0304 19:32:13.569130 22579586809984 run.py:483] Algo bellman_ford step 8815 current loss 0.007061, current_train_items 282112.
I0304 19:32:13.585561 22579586809984 run.py:483] Algo bellman_ford step 8816 current loss 0.049222, current_train_items 282144.
I0304 19:32:13.609916 22579586809984 run.py:483] Algo bellman_ford step 8817 current loss 0.030949, current_train_items 282176.
I0304 19:32:13.641394 22579586809984 run.py:483] Algo bellman_ford step 8818 current loss 0.025153, current_train_items 282208.
I0304 19:32:13.679285 22579586809984 run.py:483] Algo bellman_ford step 8819 current loss 0.038770, current_train_items 282240.
I0304 19:32:13.699337 22579586809984 run.py:483] Algo bellman_ford step 8820 current loss 0.004935, current_train_items 282272.
I0304 19:32:13.715490 22579586809984 run.py:483] Algo bellman_ford step 8821 current loss 0.023925, current_train_items 282304.
I0304 19:32:13.739503 22579586809984 run.py:483] Algo bellman_ford step 8822 current loss 0.039385, current_train_items 282336.
I0304 19:32:13.772457 22579586809984 run.py:483] Algo bellman_ford step 8823 current loss 0.052087, current_train_items 282368.
I0304 19:32:13.805838 22579586809984 run.py:483] Algo bellman_ford step 8824 current loss 0.076215, current_train_items 282400.
I0304 19:32:13.825373 22579586809984 run.py:483] Algo bellman_ford step 8825 current loss 0.003825, current_train_items 282432.
I0304 19:32:13.841921 22579586809984 run.py:483] Algo bellman_ford step 8826 current loss 0.011272, current_train_items 282464.
I0304 19:32:13.866595 22579586809984 run.py:483] Algo bellman_ford step 8827 current loss 0.058740, current_train_items 282496.
I0304 19:32:13.899099 22579586809984 run.py:483] Algo bellman_ford step 8828 current loss 0.041899, current_train_items 282528.
I0304 19:32:13.932409 22579586809984 run.py:483] Algo bellman_ford step 8829 current loss 0.043128, current_train_items 282560.
I0304 19:32:13.952070 22579586809984 run.py:483] Algo bellman_ford step 8830 current loss 0.003865, current_train_items 282592.
I0304 19:32:13.968114 22579586809984 run.py:483] Algo bellman_ford step 8831 current loss 0.013579, current_train_items 282624.
I0304 19:32:13.993386 22579586809984 run.py:483] Algo bellman_ford step 8832 current loss 0.046171, current_train_items 282656.
I0304 19:32:14.024112 22579586809984 run.py:483] Algo bellman_ford step 8833 current loss 0.047442, current_train_items 282688.
I0304 19:32:14.057214 22579586809984 run.py:483] Algo bellman_ford step 8834 current loss 0.050791, current_train_items 282720.
I0304 19:32:14.077339 22579586809984 run.py:483] Algo bellman_ford step 8835 current loss 0.004416, current_train_items 282752.
I0304 19:32:14.093456 22579586809984 run.py:483] Algo bellman_ford step 8836 current loss 0.017459, current_train_items 282784.
I0304 19:32:14.118719 22579586809984 run.py:483] Algo bellman_ford step 8837 current loss 0.038013, current_train_items 282816.
I0304 19:32:14.150655 22579586809984 run.py:483] Algo bellman_ford step 8838 current loss 0.054614, current_train_items 282848.
I0304 19:32:14.184659 22579586809984 run.py:483] Algo bellman_ford step 8839 current loss 0.061048, current_train_items 282880.
I0304 19:32:14.204571 22579586809984 run.py:483] Algo bellman_ford step 8840 current loss 0.004978, current_train_items 282912.
I0304 19:32:14.221184 22579586809984 run.py:483] Algo bellman_ford step 8841 current loss 0.010662, current_train_items 282944.
I0304 19:32:14.245972 22579586809984 run.py:483] Algo bellman_ford step 8842 current loss 0.036806, current_train_items 282976.
I0304 19:32:14.278107 22579586809984 run.py:483] Algo bellman_ford step 8843 current loss 0.033703, current_train_items 283008.
I0304 19:32:14.313918 22579586809984 run.py:483] Algo bellman_ford step 8844 current loss 0.072910, current_train_items 283040.
I0304 19:32:14.333667 22579586809984 run.py:483] Algo bellman_ford step 8845 current loss 0.011911, current_train_items 283072.
I0304 19:32:14.349859 22579586809984 run.py:483] Algo bellman_ford step 8846 current loss 0.027661, current_train_items 283104.
I0304 19:32:14.374467 22579586809984 run.py:483] Algo bellman_ford step 8847 current loss 0.033262, current_train_items 283136.
I0304 19:32:14.405325 22579586809984 run.py:483] Algo bellman_ford step 8848 current loss 0.034691, current_train_items 283168.
I0304 19:32:14.439137 22579586809984 run.py:483] Algo bellman_ford step 8849 current loss 0.065022, current_train_items 283200.
I0304 19:32:14.458657 22579586809984 run.py:483] Algo bellman_ford step 8850 current loss 0.002059, current_train_items 283232.
I0304 19:32:14.466432 22579586809984 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0304 19:32:14.466541 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:32:14.483545 22579586809984 run.py:483] Algo bellman_ford step 8851 current loss 0.017596, current_train_items 283264.
I0304 19:32:14.508895 22579586809984 run.py:483] Algo bellman_ford step 8852 current loss 0.040138, current_train_items 283296.
I0304 19:32:14.541790 22579586809984 run.py:483] Algo bellman_ford step 8853 current loss 0.082713, current_train_items 283328.
I0304 19:32:14.576256 22579586809984 run.py:483] Algo bellman_ford step 8854 current loss 0.075638, current_train_items 283360.
I0304 19:32:14.596201 22579586809984 run.py:483] Algo bellman_ford step 8855 current loss 0.002557, current_train_items 283392.
I0304 19:32:14.612533 22579586809984 run.py:483] Algo bellman_ford step 8856 current loss 0.025024, current_train_items 283424.
I0304 19:32:14.636571 22579586809984 run.py:483] Algo bellman_ford step 8857 current loss 0.031605, current_train_items 283456.
I0304 19:32:14.669168 22579586809984 run.py:483] Algo bellman_ford step 8858 current loss 0.037250, current_train_items 283488.
I0304 19:32:14.702211 22579586809984 run.py:483] Algo bellman_ford step 8859 current loss 0.038905, current_train_items 283520.
I0304 19:32:14.722015 22579586809984 run.py:483] Algo bellman_ford step 8860 current loss 0.002625, current_train_items 283552.
I0304 19:32:14.738590 22579586809984 run.py:483] Algo bellman_ford step 8861 current loss 0.009688, current_train_items 283584.
I0304 19:32:14.763149 22579586809984 run.py:483] Algo bellman_ford step 8862 current loss 0.025313, current_train_items 283616.
I0304 19:32:14.794169 22579586809984 run.py:483] Algo bellman_ford step 8863 current loss 0.017567, current_train_items 283648.
I0304 19:32:14.827041 22579586809984 run.py:483] Algo bellman_ford step 8864 current loss 0.062683, current_train_items 283680.
I0304 19:32:14.846711 22579586809984 run.py:483] Algo bellman_ford step 8865 current loss 0.006116, current_train_items 283712.
I0304 19:32:14.862876 22579586809984 run.py:483] Algo bellman_ford step 8866 current loss 0.014679, current_train_items 283744.
I0304 19:32:14.886617 22579586809984 run.py:483] Algo bellman_ford step 8867 current loss 0.032656, current_train_items 283776.
I0304 19:32:14.919029 22579586809984 run.py:483] Algo bellman_ford step 8868 current loss 0.070801, current_train_items 283808.
I0304 19:32:14.951971 22579586809984 run.py:483] Algo bellman_ford step 8869 current loss 0.043730, current_train_items 283840.
I0304 19:32:14.972029 22579586809984 run.py:483] Algo bellman_ford step 8870 current loss 0.003827, current_train_items 283872.
I0304 19:32:14.988266 22579586809984 run.py:483] Algo bellman_ford step 8871 current loss 0.003307, current_train_items 283904.
I0304 19:32:15.012694 22579586809984 run.py:483] Algo bellman_ford step 8872 current loss 0.052824, current_train_items 283936.
I0304 19:32:15.044150 22579586809984 run.py:483] Algo bellman_ford step 8873 current loss 0.032414, current_train_items 283968.
I0304 19:32:15.079111 22579586809984 run.py:483] Algo bellman_ford step 8874 current loss 0.152577, current_train_items 284000.
I0304 19:32:15.099019 22579586809984 run.py:483] Algo bellman_ford step 8875 current loss 0.002233, current_train_items 284032.
I0304 19:32:15.115458 22579586809984 run.py:483] Algo bellman_ford step 8876 current loss 0.021295, current_train_items 284064.
I0304 19:32:15.138870 22579586809984 run.py:483] Algo bellman_ford step 8877 current loss 0.028136, current_train_items 284096.
I0304 19:32:15.170397 22579586809984 run.py:483] Algo bellman_ford step 8878 current loss 0.029609, current_train_items 284128.
I0304 19:32:15.204673 22579586809984 run.py:483] Algo bellman_ford step 8879 current loss 0.089294, current_train_items 284160.
I0304 19:32:15.224180 22579586809984 run.py:483] Algo bellman_ford step 8880 current loss 0.002375, current_train_items 284192.
I0304 19:32:15.240366 22579586809984 run.py:483] Algo bellman_ford step 8881 current loss 0.022347, current_train_items 284224.
I0304 19:32:15.263579 22579586809984 run.py:483] Algo bellman_ford step 8882 current loss 0.037286, current_train_items 284256.
I0304 19:32:15.296573 22579586809984 run.py:483] Algo bellman_ford step 8883 current loss 0.040407, current_train_items 284288.
I0304 19:32:15.330461 22579586809984 run.py:483] Algo bellman_ford step 8884 current loss 0.045822, current_train_items 284320.
I0304 19:32:15.350334 22579586809984 run.py:483] Algo bellman_ford step 8885 current loss 0.004618, current_train_items 284352.
I0304 19:32:15.367177 22579586809984 run.py:483] Algo bellman_ford step 8886 current loss 0.022793, current_train_items 284384.
I0304 19:32:15.391236 22579586809984 run.py:483] Algo bellman_ford step 8887 current loss 0.035965, current_train_items 284416.
I0304 19:32:15.422487 22579586809984 run.py:483] Algo bellman_ford step 8888 current loss 0.053329, current_train_items 284448.
I0304 19:32:15.456834 22579586809984 run.py:483] Algo bellman_ford step 8889 current loss 0.066184, current_train_items 284480.
I0304 19:32:15.476567 22579586809984 run.py:483] Algo bellman_ford step 8890 current loss 0.001129, current_train_items 284512.
I0304 19:32:15.492455 22579586809984 run.py:483] Algo bellman_ford step 8891 current loss 0.014940, current_train_items 284544.
I0304 19:32:15.516563 22579586809984 run.py:483] Algo bellman_ford step 8892 current loss 0.031285, current_train_items 284576.
I0304 19:32:15.549891 22579586809984 run.py:483] Algo bellman_ford step 8893 current loss 0.047194, current_train_items 284608.
I0304 19:32:15.583714 22579586809984 run.py:483] Algo bellman_ford step 8894 current loss 0.072561, current_train_items 284640.
I0304 19:32:15.603665 22579586809984 run.py:483] Algo bellman_ford step 8895 current loss 0.002781, current_train_items 284672.
I0304 19:32:15.619766 22579586809984 run.py:483] Algo bellman_ford step 8896 current loss 0.013747, current_train_items 284704.
I0304 19:32:15.642929 22579586809984 run.py:483] Algo bellman_ford step 8897 current loss 0.042671, current_train_items 284736.
I0304 19:32:15.675957 22579586809984 run.py:483] Algo bellman_ford step 8898 current loss 0.043997, current_train_items 284768.
I0304 19:32:15.711284 22579586809984 run.py:483] Algo bellman_ford step 8899 current loss 0.043760, current_train_items 284800.
I0304 19:32:15.731276 22579586809984 run.py:483] Algo bellman_ford step 8900 current loss 0.002103, current_train_items 284832.
I0304 19:32:15.738951 22579586809984 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0304 19:32:15.739056 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0304 19:32:15.756037 22579586809984 run.py:483] Algo bellman_ford step 8901 current loss 0.018216, current_train_items 284864.
I0304 19:32:15.780796 22579586809984 run.py:483] Algo bellman_ford step 8902 current loss 0.024274, current_train_items 284896.
I0304 19:32:15.812156 22579586809984 run.py:483] Algo bellman_ford step 8903 current loss 0.065663, current_train_items 284928.
I0304 19:32:15.847872 22579586809984 run.py:483] Algo bellman_ford step 8904 current loss 0.050508, current_train_items 284960.
I0304 19:32:15.867637 22579586809984 run.py:483] Algo bellman_ford step 8905 current loss 0.002823, current_train_items 284992.
I0304 19:32:15.883814 22579586809984 run.py:483] Algo bellman_ford step 8906 current loss 0.014336, current_train_items 285024.
I0304 19:32:15.908154 22579586809984 run.py:483] Algo bellman_ford step 8907 current loss 0.058368, current_train_items 285056.
I0304 19:32:15.939635 22579586809984 run.py:483] Algo bellman_ford step 8908 current loss 0.048584, current_train_items 285088.
I0304 19:32:15.972022 22579586809984 run.py:483] Algo bellman_ford step 8909 current loss 0.051191, current_train_items 285120.
I0304 19:32:15.991696 22579586809984 run.py:483] Algo bellman_ford step 8910 current loss 0.003542, current_train_items 285152.
I0304 19:32:16.007810 22579586809984 run.py:483] Algo bellman_ford step 8911 current loss 0.024750, current_train_items 285184.
I0304 19:32:16.032402 22579586809984 run.py:483] Algo bellman_ford step 8912 current loss 0.044408, current_train_items 285216.
I0304 19:32:16.063452 22579586809984 run.py:483] Algo bellman_ford step 8913 current loss 0.039055, current_train_items 285248.
I0304 19:32:16.098394 22579586809984 run.py:483] Algo bellman_ford step 8914 current loss 0.045372, current_train_items 285280.
I0304 19:32:16.118036 22579586809984 run.py:483] Algo bellman_ford step 8915 current loss 0.003168, current_train_items 285312.
I0304 19:32:16.133912 22579586809984 run.py:483] Algo bellman_ford step 8916 current loss 0.007695, current_train_items 285344.
I0304 19:32:16.158560 22579586809984 run.py:483] Algo bellman_ford step 8917 current loss 0.043119, current_train_items 285376.
I0304 19:32:16.190263 22579586809984 run.py:483] Algo bellman_ford step 8918 current loss 0.015030, current_train_items 285408.
I0304 19:32:16.223641 22579586809984 run.py:483] Algo bellman_ford step 8919 current loss 0.066465, current_train_items 285440.
I0304 19:32:16.243204 22579586809984 run.py:483] Algo bellman_ford step 8920 current loss 0.003257, current_train_items 285472.
I0304 19:32:16.259568 22579586809984 run.py:483] Algo bellman_ford step 8921 current loss 0.044884, current_train_items 285504.
I0304 19:32:16.284788 22579586809984 run.py:483] Algo bellman_ford step 8922 current loss 0.038184, current_train_items 285536.
I0304 19:32:16.316205 22579586809984 run.py:483] Algo bellman_ford step 8923 current loss 0.035903, current_train_items 285568.
I0304 19:32:16.350443 22579586809984 run.py:483] Algo bellman_ford step 8924 current loss 0.053058, current_train_items 285600.
I0304 19:32:16.370011 22579586809984 run.py:483] Algo bellman_ford step 8925 current loss 0.002403, current_train_items 285632.
I0304 19:32:16.386067 22579586809984 run.py:483] Algo bellman_ford step 8926 current loss 0.018569, current_train_items 285664.
I0304 19:32:16.410759 22579586809984 run.py:483] Algo bellman_ford step 8927 current loss 0.029592, current_train_items 285696.
I0304 19:32:16.441385 22579586809984 run.py:483] Algo bellman_ford step 8928 current loss 0.026619, current_train_items 285728.
I0304 19:32:16.475475 22579586809984 run.py:483] Algo bellman_ford step 8929 current loss 0.036404, current_train_items 285760.
I0304 19:32:16.495157 22579586809984 run.py:483] Algo bellman_ford step 8930 current loss 0.002615, current_train_items 285792.
I0304 19:32:16.512285 22579586809984 run.py:483] Algo bellman_ford step 8931 current loss 0.010149, current_train_items 285824.
I0304 19:32:16.536288 22579586809984 run.py:483] Algo bellman_ford step 8932 current loss 0.031528, current_train_items 285856.
I0304 19:32:16.569458 22579586809984 run.py:483] Algo bellman_ford step 8933 current loss 0.073176, current_train_items 285888.
I0304 19:32:16.603816 22579586809984 run.py:483] Algo bellman_ford step 8934 current loss 0.052324, current_train_items 285920.
I0304 19:32:16.623661 22579586809984 run.py:483] Algo bellman_ford step 8935 current loss 0.002129, current_train_items 285952.
I0304 19:32:16.639744 22579586809984 run.py:483] Algo bellman_ford step 8936 current loss 0.017204, current_train_items 285984.
I0304 19:32:16.663906 22579586809984 run.py:483] Algo bellman_ford step 8937 current loss 0.065077, current_train_items 286016.
I0304 19:32:16.695298 22579586809984 run.py:483] Algo bellman_ford step 8938 current loss 0.036222, current_train_items 286048.
I0304 19:32:16.728150 22579586809984 run.py:483] Algo bellman_ford step 8939 current loss 0.065979, current_train_items 286080.
I0304 19:32:16.747758 22579586809984 run.py:483] Algo bellman_ford step 8940 current loss 0.003992, current_train_items 286112.
I0304 19:32:16.763803 22579586809984 run.py:483] Algo bellman_ford step 8941 current loss 0.008419, current_train_items 286144.
I0304 19:32:16.788290 22579586809984 run.py:483] Algo bellman_ford step 8942 current loss 0.039832, current_train_items 286176.
I0304 19:32:16.821368 22579586809984 run.py:483] Algo bellman_ford step 8943 current loss 0.063796, current_train_items 286208.
I0304 19:32:16.855453 22579586809984 run.py:483] Algo bellman_ford step 8944 current loss 0.047412, current_train_items 286240.
I0304 19:32:16.874921 22579586809984 run.py:483] Algo bellman_ford step 8945 current loss 0.008094, current_train_items 286272.
I0304 19:32:16.891103 22579586809984 run.py:483] Algo bellman_ford step 8946 current loss 0.012865, current_train_items 286304.
I0304 19:32:16.914948 22579586809984 run.py:483] Algo bellman_ford step 8947 current loss 0.031252, current_train_items 286336.
I0304 19:32:16.947126 22579586809984 run.py:483] Algo bellman_ford step 8948 current loss 0.063200, current_train_items 286368.
I0304 19:32:16.980339 22579586809984 run.py:483] Algo bellman_ford step 8949 current loss 0.068246, current_train_items 286400.
I0304 19:32:16.999882 22579586809984 run.py:483] Algo bellman_ford step 8950 current loss 0.002345, current_train_items 286432.
I0304 19:32:17.007951 22579586809984 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0304 19:32:17.008058 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:17.025228 22579586809984 run.py:483] Algo bellman_ford step 8951 current loss 0.021151, current_train_items 286464.
I0304 19:32:17.049552 22579586809984 run.py:483] Algo bellman_ford step 8952 current loss 0.025697, current_train_items 286496.
I0304 19:32:17.084640 22579586809984 run.py:483] Algo bellman_ford step 8953 current loss 0.065693, current_train_items 286528.
I0304 19:32:17.120301 22579586809984 run.py:483] Algo bellman_ford step 8954 current loss 0.051653, current_train_items 286560.
I0304 19:32:17.140210 22579586809984 run.py:483] Algo bellman_ford step 8955 current loss 0.003915, current_train_items 286592.
I0304 19:32:17.156805 22579586809984 run.py:483] Algo bellman_ford step 8956 current loss 0.016845, current_train_items 286624.
I0304 19:32:17.181697 22579586809984 run.py:483] Algo bellman_ford step 8957 current loss 0.044260, current_train_items 286656.
I0304 19:32:17.213619 22579586809984 run.py:483] Algo bellman_ford step 8958 current loss 0.031272, current_train_items 286688.
I0304 19:32:17.248598 22579586809984 run.py:483] Algo bellman_ford step 8959 current loss 0.073386, current_train_items 286720.
I0304 19:32:17.268732 22579586809984 run.py:483] Algo bellman_ford step 8960 current loss 0.006572, current_train_items 286752.
I0304 19:32:17.284575 22579586809984 run.py:483] Algo bellman_ford step 8961 current loss 0.015614, current_train_items 286784.
I0304 19:32:17.309628 22579586809984 run.py:483] Algo bellman_ford step 8962 current loss 0.045194, current_train_items 286816.
I0304 19:32:17.341897 22579586809984 run.py:483] Algo bellman_ford step 8963 current loss 0.029525, current_train_items 286848.
I0304 19:32:17.374485 22579586809984 run.py:483] Algo bellman_ford step 8964 current loss 0.034316, current_train_items 286880.
I0304 19:32:17.394340 22579586809984 run.py:483] Algo bellman_ford step 8965 current loss 0.002320, current_train_items 286912.
I0304 19:32:17.410766 22579586809984 run.py:483] Algo bellman_ford step 8966 current loss 0.008710, current_train_items 286944.
I0304 19:32:17.435356 22579586809984 run.py:483] Algo bellman_ford step 8967 current loss 0.023198, current_train_items 286976.
I0304 19:32:17.467911 22579586809984 run.py:483] Algo bellman_ford step 8968 current loss 0.069281, current_train_items 287008.
I0304 19:32:17.501572 22579586809984 run.py:483] Algo bellman_ford step 8969 current loss 0.052810, current_train_items 287040.
I0304 19:32:17.521540 22579586809984 run.py:483] Algo bellman_ford step 8970 current loss 0.002077, current_train_items 287072.
I0304 19:32:17.537764 22579586809984 run.py:483] Algo bellman_ford step 8971 current loss 0.005035, current_train_items 287104.
I0304 19:32:17.562050 22579586809984 run.py:483] Algo bellman_ford step 8972 current loss 0.037638, current_train_items 287136.
I0304 19:32:17.593500 22579586809984 run.py:483] Algo bellman_ford step 8973 current loss 0.014655, current_train_items 287168.
I0304 19:32:17.627073 22579586809984 run.py:483] Algo bellman_ford step 8974 current loss 0.077364, current_train_items 287200.
I0304 19:32:17.647168 22579586809984 run.py:483] Algo bellman_ford step 8975 current loss 0.020342, current_train_items 287232.
I0304 19:32:17.663313 22579586809984 run.py:483] Algo bellman_ford step 8976 current loss 0.005786, current_train_items 287264.
I0304 19:32:17.686333 22579586809984 run.py:483] Algo bellman_ford step 8977 current loss 0.016269, current_train_items 287296.
I0304 19:32:17.718753 22579586809984 run.py:483] Algo bellman_ford step 8978 current loss 0.038793, current_train_items 287328.
I0304 19:32:17.752154 22579586809984 run.py:483] Algo bellman_ford step 8979 current loss 0.056009, current_train_items 287360.
I0304 19:32:17.772010 22579586809984 run.py:483] Algo bellman_ford step 8980 current loss 0.002465, current_train_items 287392.
I0304 19:32:17.788423 22579586809984 run.py:483] Algo bellman_ford step 8981 current loss 0.010945, current_train_items 287424.
I0304 19:32:17.813134 22579586809984 run.py:483] Algo bellman_ford step 8982 current loss 0.046413, current_train_items 287456.
I0304 19:32:17.845575 22579586809984 run.py:483] Algo bellman_ford step 8983 current loss 0.033519, current_train_items 287488.
I0304 19:32:17.879925 22579586809984 run.py:483] Algo bellman_ford step 8984 current loss 0.057422, current_train_items 287520.
I0304 19:32:17.899903 22579586809984 run.py:483] Algo bellman_ford step 8985 current loss 0.026741, current_train_items 287552.
I0304 19:32:17.916374 22579586809984 run.py:483] Algo bellman_ford step 8986 current loss 0.003790, current_train_items 287584.
I0304 19:32:17.939817 22579586809984 run.py:483] Algo bellman_ford step 8987 current loss 0.021628, current_train_items 287616.
I0304 19:32:17.970292 22579586809984 run.py:483] Algo bellman_ford step 8988 current loss 0.050585, current_train_items 287648.
I0304 19:32:18.006161 22579586809984 run.py:483] Algo bellman_ford step 8989 current loss 0.060332, current_train_items 287680.
I0304 19:32:18.025784 22579586809984 run.py:483] Algo bellman_ford step 8990 current loss 0.035503, current_train_items 287712.
I0304 19:32:18.041789 22579586809984 run.py:483] Algo bellman_ford step 8991 current loss 0.004872, current_train_items 287744.
I0304 19:32:18.065535 22579586809984 run.py:483] Algo bellman_ford step 8992 current loss 0.055952, current_train_items 287776.
I0304 19:32:18.097632 22579586809984 run.py:483] Algo bellman_ford step 8993 current loss 0.052801, current_train_items 287808.
I0304 19:32:18.131645 22579586809984 run.py:483] Algo bellman_ford step 8994 current loss 0.049481, current_train_items 287840.
I0304 19:32:18.151596 22579586809984 run.py:483] Algo bellman_ford step 8995 current loss 0.010500, current_train_items 287872.
I0304 19:32:18.167977 22579586809984 run.py:483] Algo bellman_ford step 8996 current loss 0.031924, current_train_items 287904.
I0304 19:32:18.192772 22579586809984 run.py:483] Algo bellman_ford step 8997 current loss 0.050298, current_train_items 287936.
I0304 19:32:18.226548 22579586809984 run.py:483] Algo bellman_ford step 8998 current loss 0.045478, current_train_items 287968.
I0304 19:32:18.259542 22579586809984 run.py:483] Algo bellman_ford step 8999 current loss 0.041841, current_train_items 288000.
I0304 19:32:18.279769 22579586809984 run.py:483] Algo bellman_ford step 9000 current loss 0.003721, current_train_items 288032.
I0304 19:32:18.287837 22579586809984 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0304 19:32:18.287944 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0304 19:32:18.305153 22579586809984 run.py:483] Algo bellman_ford step 9001 current loss 0.019368, current_train_items 288064.
I0304 19:32:18.329357 22579586809984 run.py:483] Algo bellman_ford step 9002 current loss 0.038953, current_train_items 288096.
I0304 19:32:18.360931 22579586809984 run.py:483] Algo bellman_ford step 9003 current loss 0.072633, current_train_items 288128.
I0304 19:32:18.395470 22579586809984 run.py:483] Algo bellman_ford step 9004 current loss 0.089982, current_train_items 288160.
I0304 19:32:18.415425 22579586809984 run.py:483] Algo bellman_ford step 9005 current loss 0.003773, current_train_items 288192.
I0304 19:32:18.431748 22579586809984 run.py:483] Algo bellman_ford step 9006 current loss 0.006099, current_train_items 288224.
I0304 19:32:18.455636 22579586809984 run.py:483] Algo bellman_ford step 9007 current loss 0.024757, current_train_items 288256.
I0304 19:32:18.488732 22579586809984 run.py:483] Algo bellman_ford step 9008 current loss 0.034777, current_train_items 288288.
I0304 19:32:18.523092 22579586809984 run.py:483] Algo bellman_ford step 9009 current loss 0.062334, current_train_items 288320.
I0304 19:32:18.542693 22579586809984 run.py:483] Algo bellman_ford step 9010 current loss 0.032699, current_train_items 288352.
I0304 19:32:18.558943 22579586809984 run.py:483] Algo bellman_ford step 9011 current loss 0.042623, current_train_items 288384.
I0304 19:32:18.582931 22579586809984 run.py:483] Algo bellman_ford step 9012 current loss 0.013070, current_train_items 288416.
I0304 19:32:18.614066 22579586809984 run.py:483] Algo bellman_ford step 9013 current loss 0.027150, current_train_items 288448.
I0304 19:32:18.649898 22579586809984 run.py:483] Algo bellman_ford step 9014 current loss 0.037487, current_train_items 288480.
I0304 19:32:18.669332 22579586809984 run.py:483] Algo bellman_ford step 9015 current loss 0.001953, current_train_items 288512.
I0304 19:32:18.685714 22579586809984 run.py:483] Algo bellman_ford step 9016 current loss 0.054547, current_train_items 288544.
I0304 19:32:18.710805 22579586809984 run.py:483] Algo bellman_ford step 9017 current loss 0.022240, current_train_items 288576.
I0304 19:32:18.743049 22579586809984 run.py:483] Algo bellman_ford step 9018 current loss 0.050353, current_train_items 288608.
I0304 19:32:18.778135 22579586809984 run.py:483] Algo bellman_ford step 9019 current loss 0.078356, current_train_items 288640.
I0304 19:32:18.797535 22579586809984 run.py:483] Algo bellman_ford step 9020 current loss 0.002165, current_train_items 288672.
I0304 19:32:18.813733 22579586809984 run.py:483] Algo bellman_ford step 9021 current loss 0.012117, current_train_items 288704.
I0304 19:32:18.839307 22579586809984 run.py:483] Algo bellman_ford step 9022 current loss 0.038353, current_train_items 288736.
I0304 19:32:18.872268 22579586809984 run.py:483] Algo bellman_ford step 9023 current loss 0.036437, current_train_items 288768.
I0304 19:32:18.906330 22579586809984 run.py:483] Algo bellman_ford step 9024 current loss 0.056648, current_train_items 288800.
I0304 19:32:18.926092 22579586809984 run.py:483] Algo bellman_ford step 9025 current loss 0.002356, current_train_items 288832.
I0304 19:32:18.942026 22579586809984 run.py:483] Algo bellman_ford step 9026 current loss 0.007177, current_train_items 288864.
I0304 19:32:18.966600 22579586809984 run.py:483] Algo bellman_ford step 9027 current loss 0.040719, current_train_items 288896.
I0304 19:32:18.998957 22579586809984 run.py:483] Algo bellman_ford step 9028 current loss 0.033229, current_train_items 288928.
I0304 19:32:19.032873 22579586809984 run.py:483] Algo bellman_ford step 9029 current loss 0.064862, current_train_items 288960.
I0304 19:32:19.052758 22579586809984 run.py:483] Algo bellman_ford step 9030 current loss 0.002406, current_train_items 288992.
I0304 19:32:19.068925 22579586809984 run.py:483] Algo bellman_ford step 9031 current loss 0.039241, current_train_items 289024.
I0304 19:32:19.094194 22579586809984 run.py:483] Algo bellman_ford step 9032 current loss 0.044646, current_train_items 289056.
I0304 19:32:19.126671 22579586809984 run.py:483] Algo bellman_ford step 9033 current loss 0.029328, current_train_items 289088.
I0304 19:32:19.162900 22579586809984 run.py:483] Algo bellman_ford step 9034 current loss 0.100334, current_train_items 289120.
I0304 19:32:19.182262 22579586809984 run.py:483] Algo bellman_ford step 9035 current loss 0.003205, current_train_items 289152.
I0304 19:32:19.198246 22579586809984 run.py:483] Algo bellman_ford step 9036 current loss 0.024051, current_train_items 289184.
I0304 19:32:19.223465 22579586809984 run.py:483] Algo bellman_ford step 9037 current loss 0.032832, current_train_items 289216.
I0304 19:32:19.255975 22579586809984 run.py:483] Algo bellman_ford step 9038 current loss 0.049415, current_train_items 289248.
I0304 19:32:19.289635 22579586809984 run.py:483] Algo bellman_ford step 9039 current loss 0.048776, current_train_items 289280.
I0304 19:32:19.309323 22579586809984 run.py:483] Algo bellman_ford step 9040 current loss 0.005621, current_train_items 289312.
I0304 19:32:19.325828 22579586809984 run.py:483] Algo bellman_ford step 9041 current loss 0.014794, current_train_items 289344.
I0304 19:32:19.349790 22579586809984 run.py:483] Algo bellman_ford step 9042 current loss 0.016913, current_train_items 289376.
I0304 19:32:19.381578 22579586809984 run.py:483] Algo bellman_ford step 9043 current loss 0.037854, current_train_items 289408.
I0304 19:32:19.415807 22579586809984 run.py:483] Algo bellman_ford step 9044 current loss 0.029167, current_train_items 289440.
I0304 19:32:19.435252 22579586809984 run.py:483] Algo bellman_ford step 9045 current loss 0.004303, current_train_items 289472.
I0304 19:32:19.451815 22579586809984 run.py:483] Algo bellman_ford step 9046 current loss 0.009815, current_train_items 289504.
I0304 19:32:19.474901 22579586809984 run.py:483] Algo bellman_ford step 9047 current loss 0.025869, current_train_items 289536.
I0304 19:32:19.507388 22579586809984 run.py:483] Algo bellman_ford step 9048 current loss 0.080448, current_train_items 289568.
I0304 19:32:19.541137 22579586809984 run.py:483] Algo bellman_ford step 9049 current loss 0.078242, current_train_items 289600.
I0304 19:32:19.560622 22579586809984 run.py:483] Algo bellman_ford step 9050 current loss 0.003317, current_train_items 289632.
I0304 19:32:19.568629 22579586809984 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0304 19:32:19.568742 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:32:19.585425 22579586809984 run.py:483] Algo bellman_ford step 9051 current loss 0.015967, current_train_items 289664.
I0304 19:32:19.610918 22579586809984 run.py:483] Algo bellman_ford step 9052 current loss 0.033120, current_train_items 289696.
I0304 19:32:19.643814 22579586809984 run.py:483] Algo bellman_ford step 9053 current loss 0.042264, current_train_items 289728.
I0304 19:32:19.676860 22579586809984 run.py:483] Algo bellman_ford step 9054 current loss 0.050434, current_train_items 289760.
I0304 19:32:19.696652 22579586809984 run.py:483] Algo bellman_ford step 9055 current loss 0.002331, current_train_items 289792.
I0304 19:32:19.712526 22579586809984 run.py:483] Algo bellman_ford step 9056 current loss 0.025007, current_train_items 289824.
I0304 19:32:19.736344 22579586809984 run.py:483] Algo bellman_ford step 9057 current loss 0.027892, current_train_items 289856.
I0304 19:32:19.766775 22579586809984 run.py:483] Algo bellman_ford step 9058 current loss 0.041984, current_train_items 289888.
I0304 19:32:19.798847 22579586809984 run.py:483] Algo bellman_ford step 9059 current loss 0.079667, current_train_items 289920.
I0304 19:32:19.818552 22579586809984 run.py:483] Algo bellman_ford step 9060 current loss 0.012230, current_train_items 289952.
I0304 19:32:19.835397 22579586809984 run.py:483] Algo bellman_ford step 9061 current loss 0.011748, current_train_items 289984.
I0304 19:32:19.860777 22579586809984 run.py:483] Algo bellman_ford step 9062 current loss 0.037859, current_train_items 290016.
I0304 19:32:19.891482 22579586809984 run.py:483] Algo bellman_ford step 9063 current loss 0.028253, current_train_items 290048.
I0304 19:32:19.926331 22579586809984 run.py:483] Algo bellman_ford step 9064 current loss 0.090229, current_train_items 290080.
I0304 19:32:19.946094 22579586809984 run.py:483] Algo bellman_ford step 9065 current loss 0.004242, current_train_items 290112.
I0304 19:32:19.962600 22579586809984 run.py:483] Algo bellman_ford step 9066 current loss 0.032032, current_train_items 290144.
I0304 19:32:19.985036 22579586809984 run.py:483] Algo bellman_ford step 9067 current loss 0.033955, current_train_items 290176.
I0304 19:32:20.017168 22579586809984 run.py:483] Algo bellman_ford step 9068 current loss 0.027581, current_train_items 290208.
I0304 19:32:20.051756 22579586809984 run.py:483] Algo bellman_ford step 9069 current loss 0.076641, current_train_items 290240.
I0304 19:32:20.071848 22579586809984 run.py:483] Algo bellman_ford step 9070 current loss 0.002230, current_train_items 290272.
I0304 19:32:20.088521 22579586809984 run.py:483] Algo bellman_ford step 9071 current loss 0.032421, current_train_items 290304.
I0304 19:32:20.113241 22579586809984 run.py:483] Algo bellman_ford step 9072 current loss 0.037103, current_train_items 290336.
I0304 19:32:20.145872 22579586809984 run.py:483] Algo bellman_ford step 9073 current loss 0.044969, current_train_items 290368.
I0304 19:32:20.179547 22579586809984 run.py:483] Algo bellman_ford step 9074 current loss 0.042222, current_train_items 290400.
I0304 19:32:20.199255 22579586809984 run.py:483] Algo bellman_ford step 9075 current loss 0.005910, current_train_items 290432.
I0304 19:32:20.215643 22579586809984 run.py:483] Algo bellman_ford step 9076 current loss 0.021665, current_train_items 290464.
I0304 19:32:20.239860 22579586809984 run.py:483] Algo bellman_ford step 9077 current loss 0.062114, current_train_items 290496.
I0304 19:32:20.271892 22579586809984 run.py:483] Algo bellman_ford step 9078 current loss 0.043401, current_train_items 290528.
I0304 19:32:20.306356 22579586809984 run.py:483] Algo bellman_ford step 9079 current loss 0.070695, current_train_items 290560.
I0304 19:32:20.325873 22579586809984 run.py:483] Algo bellman_ford step 9080 current loss 0.002375, current_train_items 290592.
I0304 19:32:20.341956 22579586809984 run.py:483] Algo bellman_ford step 9081 current loss 0.040502, current_train_items 290624.
I0304 19:32:20.365797 22579586809984 run.py:483] Algo bellman_ford step 9082 current loss 0.012574, current_train_items 290656.
I0304 19:32:20.397282 22579586809984 run.py:483] Algo bellman_ford step 9083 current loss 0.038086, current_train_items 290688.
I0304 19:32:20.430194 22579586809984 run.py:483] Algo bellman_ford step 9084 current loss 0.052866, current_train_items 290720.
I0304 19:32:20.450178 22579586809984 run.py:483] Algo bellman_ford step 9085 current loss 0.002629, current_train_items 290752.
I0304 19:32:20.466633 22579586809984 run.py:483] Algo bellman_ford step 9086 current loss 0.013869, current_train_items 290784.
I0304 19:32:20.491226 22579586809984 run.py:483] Algo bellman_ford step 9087 current loss 0.036385, current_train_items 290816.
I0304 19:32:20.522490 22579586809984 run.py:483] Algo bellman_ford step 9088 current loss 0.024722, current_train_items 290848.
I0304 19:32:20.557717 22579586809984 run.py:483] Algo bellman_ford step 9089 current loss 0.086933, current_train_items 290880.
I0304 19:32:20.577607 22579586809984 run.py:483] Algo bellman_ford step 9090 current loss 0.001921, current_train_items 290912.
I0304 19:32:20.593891 22579586809984 run.py:483] Algo bellman_ford step 9091 current loss 0.012283, current_train_items 290944.
I0304 19:32:20.617603 22579586809984 run.py:483] Algo bellman_ford step 9092 current loss 0.028869, current_train_items 290976.
I0304 19:32:20.647608 22579586809984 run.py:483] Algo bellman_ford step 9093 current loss 0.029951, current_train_items 291008.
I0304 19:32:20.683390 22579586809984 run.py:483] Algo bellman_ford step 9094 current loss 0.076687, current_train_items 291040.
I0304 19:32:20.702995 22579586809984 run.py:483] Algo bellman_ford step 9095 current loss 0.005831, current_train_items 291072.
I0304 19:32:20.719462 22579586809984 run.py:483] Algo bellman_ford step 9096 current loss 0.011457, current_train_items 291104.
I0304 19:32:20.743281 22579586809984 run.py:483] Algo bellman_ford step 9097 current loss 0.046040, current_train_items 291136.
I0304 19:32:20.775585 22579586809984 run.py:483] Algo bellman_ford step 9098 current loss 0.039915, current_train_items 291168.
I0304 19:32:20.808595 22579586809984 run.py:483] Algo bellman_ford step 9099 current loss 0.031433, current_train_items 291200.
I0304 19:32:20.828660 22579586809984 run.py:483] Algo bellman_ford step 9100 current loss 0.002738, current_train_items 291232.
I0304 19:32:20.836457 22579586809984 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0304 19:32:20.836562 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:32:20.853451 22579586809984 run.py:483] Algo bellman_ford step 9101 current loss 0.004154, current_train_items 291264.
I0304 19:32:20.879015 22579586809984 run.py:483] Algo bellman_ford step 9102 current loss 0.054987, current_train_items 291296.
I0304 19:32:20.911420 22579586809984 run.py:483] Algo bellman_ford step 9103 current loss 0.024887, current_train_items 291328.
I0304 19:32:20.947189 22579586809984 run.py:483] Algo bellman_ford step 9104 current loss 0.042450, current_train_items 291360.
I0304 19:32:20.967085 22579586809984 run.py:483] Algo bellman_ford step 9105 current loss 0.007481, current_train_items 291392.
I0304 19:32:20.983414 22579586809984 run.py:483] Algo bellman_ford step 9106 current loss 0.024779, current_train_items 291424.
I0304 19:32:21.008103 22579586809984 run.py:483] Algo bellman_ford step 9107 current loss 0.050882, current_train_items 291456.
I0304 19:32:21.040618 22579586809984 run.py:483] Algo bellman_ford step 9108 current loss 0.042914, current_train_items 291488.
I0304 19:32:21.074898 22579586809984 run.py:483] Algo bellman_ford step 9109 current loss 0.049221, current_train_items 291520.
I0304 19:32:21.094417 22579586809984 run.py:483] Algo bellman_ford step 9110 current loss 0.004560, current_train_items 291552.
I0304 19:32:21.110545 22579586809984 run.py:483] Algo bellman_ford step 9111 current loss 0.024540, current_train_items 291584.
I0304 19:32:21.134463 22579586809984 run.py:483] Algo bellman_ford step 9112 current loss 0.024806, current_train_items 291616.
I0304 19:32:21.168117 22579586809984 run.py:483] Algo bellman_ford step 9113 current loss 0.087911, current_train_items 291648.
I0304 19:32:21.203099 22579586809984 run.py:483] Algo bellman_ford step 9114 current loss 0.136162, current_train_items 291680.
I0304 19:32:21.222630 22579586809984 run.py:483] Algo bellman_ford step 9115 current loss 0.288143, current_train_items 291712.
I0304 19:32:21.239162 22579586809984 run.py:483] Algo bellman_ford step 9116 current loss 0.009856, current_train_items 291744.
I0304 19:32:21.263217 22579586809984 run.py:483] Algo bellman_ford step 9117 current loss 0.028891, current_train_items 291776.
I0304 19:32:21.294833 22579586809984 run.py:483] Algo bellman_ford step 9118 current loss 0.033424, current_train_items 291808.
I0304 19:32:21.330475 22579586809984 run.py:483] Algo bellman_ford step 9119 current loss 0.106842, current_train_items 291840.
I0304 19:32:21.350064 22579586809984 run.py:483] Algo bellman_ford step 9120 current loss 0.006177, current_train_items 291872.
I0304 19:32:21.366258 22579586809984 run.py:483] Algo bellman_ford step 9121 current loss 0.013049, current_train_items 291904.
I0304 19:32:21.390295 22579586809984 run.py:483] Algo bellman_ford step 9122 current loss 0.028000, current_train_items 291936.
I0304 19:32:21.423480 22579586809984 run.py:483] Algo bellman_ford step 9123 current loss 0.038051, current_train_items 291968.
I0304 19:32:21.459120 22579586809984 run.py:483] Algo bellman_ford step 9124 current loss 0.059030, current_train_items 292000.
I0304 19:32:21.478830 22579586809984 run.py:483] Algo bellman_ford step 9125 current loss 0.004287, current_train_items 292032.
I0304 19:32:21.494814 22579586809984 run.py:483] Algo bellman_ford step 9126 current loss 0.020455, current_train_items 292064.
I0304 19:32:21.518332 22579586809984 run.py:483] Algo bellman_ford step 9127 current loss 0.043405, current_train_items 292096.
I0304 19:32:21.551149 22579586809984 run.py:483] Algo bellman_ford step 9128 current loss 0.039627, current_train_items 292128.
I0304 19:32:21.584487 22579586809984 run.py:483] Algo bellman_ford step 9129 current loss 0.069849, current_train_items 292160.
I0304 19:32:21.604162 22579586809984 run.py:483] Algo bellman_ford step 9130 current loss 0.004292, current_train_items 292192.
I0304 19:32:21.620358 22579586809984 run.py:483] Algo bellman_ford step 9131 current loss 0.005257, current_train_items 292224.
I0304 19:32:21.645290 22579586809984 run.py:483] Algo bellman_ford step 9132 current loss 0.061092, current_train_items 292256.
I0304 19:32:21.677760 22579586809984 run.py:483] Algo bellman_ford step 9133 current loss 0.089260, current_train_items 292288.
I0304 19:32:21.710371 22579586809984 run.py:483] Algo bellman_ford step 9134 current loss 0.063536, current_train_items 292320.
I0304 19:32:21.729930 22579586809984 run.py:483] Algo bellman_ford step 9135 current loss 0.002649, current_train_items 292352.
I0304 19:32:21.745764 22579586809984 run.py:483] Algo bellman_ford step 9136 current loss 0.005082, current_train_items 292384.
I0304 19:32:21.769226 22579586809984 run.py:483] Algo bellman_ford step 9137 current loss 0.034369, current_train_items 292416.
I0304 19:32:21.800956 22579586809984 run.py:483] Algo bellman_ford step 9138 current loss 0.037371, current_train_items 292448.
I0304 19:32:21.833823 22579586809984 run.py:483] Algo bellman_ford step 9139 current loss 0.058234, current_train_items 292480.
I0304 19:32:21.853295 22579586809984 run.py:483] Algo bellman_ford step 9140 current loss 0.002440, current_train_items 292512.
I0304 19:32:21.869498 22579586809984 run.py:483] Algo bellman_ford step 9141 current loss 0.008582, current_train_items 292544.
I0304 19:32:21.893277 22579586809984 run.py:483] Algo bellman_ford step 9142 current loss 0.046402, current_train_items 292576.
I0304 19:32:21.924612 22579586809984 run.py:483] Algo bellman_ford step 9143 current loss 0.039721, current_train_items 292608.
I0304 19:32:21.959278 22579586809984 run.py:483] Algo bellman_ford step 9144 current loss 0.104000, current_train_items 292640.
I0304 19:32:21.978854 22579586809984 run.py:483] Algo bellman_ford step 9145 current loss 0.004450, current_train_items 292672.
I0304 19:32:21.995230 22579586809984 run.py:483] Algo bellman_ford step 9146 current loss 0.066012, current_train_items 292704.
I0304 19:32:22.019548 22579586809984 run.py:483] Algo bellman_ford step 9147 current loss 0.023499, current_train_items 292736.
I0304 19:32:22.051511 22579586809984 run.py:483] Algo bellman_ford step 9148 current loss 0.039703, current_train_items 292768.
I0304 19:32:22.082106 22579586809984 run.py:483] Algo bellman_ford step 9149 current loss 0.033145, current_train_items 292800.
I0304 19:32:22.101608 22579586809984 run.py:483] Algo bellman_ford step 9150 current loss 0.002960, current_train_items 292832.
I0304 19:32:22.109857 22579586809984 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0304 19:32:22.109962 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:32:22.127012 22579586809984 run.py:483] Algo bellman_ford step 9151 current loss 0.009986, current_train_items 292864.
I0304 19:32:22.151654 22579586809984 run.py:483] Algo bellman_ford step 9152 current loss 0.022799, current_train_items 292896.
I0304 19:32:22.184728 22579586809984 run.py:483] Algo bellman_ford step 9153 current loss 0.044132, current_train_items 292928.
I0304 19:32:22.217087 22579586809984 run.py:483] Algo bellman_ford step 9154 current loss 0.035819, current_train_items 292960.
I0304 19:32:22.237155 22579586809984 run.py:483] Algo bellman_ford step 9155 current loss 0.003134, current_train_items 292992.
I0304 19:32:22.253881 22579586809984 run.py:483] Algo bellman_ford step 9156 current loss 0.013307, current_train_items 293024.
I0304 19:32:22.279065 22579586809984 run.py:483] Algo bellman_ford step 9157 current loss 0.042334, current_train_items 293056.
I0304 19:32:22.310767 22579586809984 run.py:483] Algo bellman_ford step 9158 current loss 0.024535, current_train_items 293088.
I0304 19:32:22.346609 22579586809984 run.py:483] Algo bellman_ford step 9159 current loss 0.042522, current_train_items 293120.
I0304 19:32:22.367317 22579586809984 run.py:483] Algo bellman_ford step 9160 current loss 0.002497, current_train_items 293152.
I0304 19:32:22.383879 22579586809984 run.py:483] Algo bellman_ford step 9161 current loss 0.020321, current_train_items 293184.
I0304 19:32:22.407759 22579586809984 run.py:483] Algo bellman_ford step 9162 current loss 0.046773, current_train_items 293216.
I0304 19:32:22.439378 22579586809984 run.py:483] Algo bellman_ford step 9163 current loss 0.035517, current_train_items 293248.
I0304 19:32:22.474994 22579586809984 run.py:483] Algo bellman_ford step 9164 current loss 0.057778, current_train_items 293280.
I0304 19:32:22.494905 22579586809984 run.py:483] Algo bellman_ford step 9165 current loss 0.002746, current_train_items 293312.
I0304 19:32:22.511370 22579586809984 run.py:483] Algo bellman_ford step 9166 current loss 0.020215, current_train_items 293344.
I0304 19:32:22.535948 22579586809984 run.py:483] Algo bellman_ford step 9167 current loss 0.031965, current_train_items 293376.
I0304 19:32:22.568617 22579586809984 run.py:483] Algo bellman_ford step 9168 current loss 0.054118, current_train_items 293408.
I0304 19:32:22.602063 22579586809984 run.py:483] Algo bellman_ford step 9169 current loss 0.041229, current_train_items 293440.
I0304 19:32:22.622247 22579586809984 run.py:483] Algo bellman_ford step 9170 current loss 0.001868, current_train_items 293472.
I0304 19:32:22.638457 22579586809984 run.py:483] Algo bellman_ford step 9171 current loss 0.008143, current_train_items 293504.
I0304 19:32:22.664179 22579586809984 run.py:483] Algo bellman_ford step 9172 current loss 0.039973, current_train_items 293536.
I0304 19:32:22.696795 22579586809984 run.py:483] Algo bellman_ford step 9173 current loss 0.029747, current_train_items 293568.
I0304 19:32:22.731573 22579586809984 run.py:483] Algo bellman_ford step 9174 current loss 0.050654, current_train_items 293600.
I0304 19:32:22.751658 22579586809984 run.py:483] Algo bellman_ford step 9175 current loss 0.006183, current_train_items 293632.
I0304 19:32:22.768297 22579586809984 run.py:483] Algo bellman_ford step 9176 current loss 0.018461, current_train_items 293664.
I0304 19:32:22.791725 22579586809984 run.py:483] Algo bellman_ford step 9177 current loss 0.028270, current_train_items 293696.
I0304 19:32:22.822634 22579586809984 run.py:483] Algo bellman_ford step 9178 current loss 0.024716, current_train_items 293728.
I0304 19:32:22.858176 22579586809984 run.py:483] Algo bellman_ford step 9179 current loss 0.098994, current_train_items 293760.
I0304 19:32:22.877609 22579586809984 run.py:483] Algo bellman_ford step 9180 current loss 0.002910, current_train_items 293792.
I0304 19:32:22.893929 22579586809984 run.py:483] Algo bellman_ford step 9181 current loss 0.035907, current_train_items 293824.
I0304 19:32:22.918911 22579586809984 run.py:483] Algo bellman_ford step 9182 current loss 0.056606, current_train_items 293856.
I0304 19:32:22.950186 22579586809984 run.py:483] Algo bellman_ford step 9183 current loss 0.039688, current_train_items 293888.
I0304 19:32:22.983506 22579586809984 run.py:483] Algo bellman_ford step 9184 current loss 0.056610, current_train_items 293920.
I0304 19:32:23.003370 22579586809984 run.py:483] Algo bellman_ford step 9185 current loss 0.016018, current_train_items 293952.
I0304 19:32:23.020004 22579586809984 run.py:483] Algo bellman_ford step 9186 current loss 0.011552, current_train_items 293984.
I0304 19:32:23.044763 22579586809984 run.py:483] Algo bellman_ford step 9187 current loss 0.014582, current_train_items 294016.
I0304 19:32:23.078497 22579586809984 run.py:483] Algo bellman_ford step 9188 current loss 0.074818, current_train_items 294048.
I0304 19:32:23.114660 22579586809984 run.py:483] Algo bellman_ford step 9189 current loss 0.035994, current_train_items 294080.
I0304 19:32:23.134737 22579586809984 run.py:483] Algo bellman_ford step 9190 current loss 0.003920, current_train_items 294112.
I0304 19:32:23.150926 22579586809984 run.py:483] Algo bellman_ford step 9191 current loss 0.019686, current_train_items 294144.
I0304 19:32:23.175585 22579586809984 run.py:483] Algo bellman_ford step 9192 current loss 0.027038, current_train_items 294176.
I0304 19:32:23.207790 22579586809984 run.py:483] Algo bellman_ford step 9193 current loss 0.058449, current_train_items 294208.
I0304 19:32:23.240962 22579586809984 run.py:483] Algo bellman_ford step 9194 current loss 0.035677, current_train_items 294240.
I0304 19:32:23.260574 22579586809984 run.py:483] Algo bellman_ford step 9195 current loss 0.015906, current_train_items 294272.
I0304 19:32:23.277386 22579586809984 run.py:483] Algo bellman_ford step 9196 current loss 0.012345, current_train_items 294304.
I0304 19:32:23.302007 22579586809984 run.py:483] Algo bellman_ford step 9197 current loss 0.054121, current_train_items 294336.
I0304 19:32:23.333376 22579586809984 run.py:483] Algo bellman_ford step 9198 current loss 0.021694, current_train_items 294368.
I0304 19:32:23.366467 22579586809984 run.py:483] Algo bellman_ford step 9199 current loss 0.049980, current_train_items 294400.
I0304 19:32:23.386801 22579586809984 run.py:483] Algo bellman_ford step 9200 current loss 0.002809, current_train_items 294432.
I0304 19:32:23.394437 22579586809984 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0304 19:32:23.394547 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:23.411452 22579586809984 run.py:483] Algo bellman_ford step 9201 current loss 0.009244, current_train_items 294464.
I0304 19:32:23.436255 22579586809984 run.py:483] Algo bellman_ford step 9202 current loss 0.035648, current_train_items 294496.
I0304 19:32:23.469882 22579586809984 run.py:483] Algo bellman_ford step 9203 current loss 0.046399, current_train_items 294528.
I0304 19:32:23.505218 22579586809984 run.py:483] Algo bellman_ford step 9204 current loss 0.039390, current_train_items 294560.
I0304 19:32:23.525299 22579586809984 run.py:483] Algo bellman_ford step 9205 current loss 0.007350, current_train_items 294592.
I0304 19:32:23.541515 22579586809984 run.py:483] Algo bellman_ford step 9206 current loss 0.012568, current_train_items 294624.
I0304 19:32:23.566228 22579586809984 run.py:483] Algo bellman_ford step 9207 current loss 0.040194, current_train_items 294656.
I0304 19:32:23.598731 22579586809984 run.py:483] Algo bellman_ford step 9208 current loss 0.051581, current_train_items 294688.
I0304 19:32:23.630545 22579586809984 run.py:483] Algo bellman_ford step 9209 current loss 0.049060, current_train_items 294720.
I0304 19:32:23.650273 22579586809984 run.py:483] Algo bellman_ford step 9210 current loss 0.006021, current_train_items 294752.
I0304 19:32:23.667049 22579586809984 run.py:483] Algo bellman_ford step 9211 current loss 0.043445, current_train_items 294784.
I0304 19:32:23.691205 22579586809984 run.py:483] Algo bellman_ford step 9212 current loss 0.074722, current_train_items 294816.
I0304 19:32:23.723408 22579586809984 run.py:483] Algo bellman_ford step 9213 current loss 0.059770, current_train_items 294848.
I0304 19:32:23.756606 22579586809984 run.py:483] Algo bellman_ford step 9214 current loss 0.069015, current_train_items 294880.
I0304 19:32:23.776174 22579586809984 run.py:483] Algo bellman_ford step 9215 current loss 0.015818, current_train_items 294912.
I0304 19:32:23.792613 22579586809984 run.py:483] Algo bellman_ford step 9216 current loss 0.011400, current_train_items 294944.
I0304 19:32:23.816549 22579586809984 run.py:483] Algo bellman_ford step 9217 current loss 0.017148, current_train_items 294976.
I0304 19:32:23.848273 22579586809984 run.py:483] Algo bellman_ford step 9218 current loss 0.123389, current_train_items 295008.
I0304 19:32:23.882440 22579586809984 run.py:483] Algo bellman_ford step 9219 current loss 0.205866, current_train_items 295040.
I0304 19:32:23.902240 22579586809984 run.py:483] Algo bellman_ford step 9220 current loss 0.085262, current_train_items 295072.
I0304 19:32:23.918292 22579586809984 run.py:483] Algo bellman_ford step 9221 current loss 0.021051, current_train_items 295104.
I0304 19:32:23.943256 22579586809984 run.py:483] Algo bellman_ford step 9222 current loss 0.038476, current_train_items 295136.
I0304 19:32:23.974599 22579586809984 run.py:483] Algo bellman_ford step 9223 current loss 0.046676, current_train_items 295168.
I0304 19:32:24.008893 22579586809984 run.py:483] Algo bellman_ford step 9224 current loss 0.038670, current_train_items 295200.
I0304 19:32:24.028744 22579586809984 run.py:483] Algo bellman_ford step 9225 current loss 0.003170, current_train_items 295232.
I0304 19:32:24.044967 22579586809984 run.py:483] Algo bellman_ford step 9226 current loss 0.019757, current_train_items 295264.
I0304 19:32:24.069840 22579586809984 run.py:483] Algo bellman_ford step 9227 current loss 0.043174, current_train_items 295296.
I0304 19:32:24.102372 22579586809984 run.py:483] Algo bellman_ford step 9228 current loss 0.108389, current_train_items 295328.
I0304 19:32:24.134031 22579586809984 run.py:483] Algo bellman_ford step 9229 current loss 0.043497, current_train_items 295360.
I0304 19:32:24.153868 22579586809984 run.py:483] Algo bellman_ford step 9230 current loss 0.003534, current_train_items 295392.
I0304 19:32:24.170696 22579586809984 run.py:483] Algo bellman_ford step 9231 current loss 0.034179, current_train_items 295424.
I0304 19:32:24.195734 22579586809984 run.py:483] Algo bellman_ford step 9232 current loss 0.057168, current_train_items 295456.
I0304 19:32:24.229851 22579586809984 run.py:483] Algo bellman_ford step 9233 current loss 0.104191, current_train_items 295488.
I0304 19:32:24.264060 22579586809984 run.py:483] Algo bellman_ford step 9234 current loss 0.058645, current_train_items 295520.
I0304 19:32:24.284058 22579586809984 run.py:483] Algo bellman_ford step 9235 current loss 0.003342, current_train_items 295552.
I0304 19:32:24.300608 22579586809984 run.py:483] Algo bellman_ford step 9236 current loss 0.044262, current_train_items 295584.
I0304 19:32:24.324916 22579586809984 run.py:483] Algo bellman_ford step 9237 current loss 0.026514, current_train_items 295616.
I0304 19:32:24.358045 22579586809984 run.py:483] Algo bellman_ford step 9238 current loss 0.078007, current_train_items 295648.
I0304 19:32:24.392512 22579586809984 run.py:483] Algo bellman_ford step 9239 current loss 0.048865, current_train_items 295680.
I0304 19:32:24.412495 22579586809984 run.py:483] Algo bellman_ford step 9240 current loss 0.005784, current_train_items 295712.
I0304 19:32:24.429312 22579586809984 run.py:483] Algo bellman_ford step 9241 current loss 0.018216, current_train_items 295744.
I0304 19:32:24.454658 22579586809984 run.py:483] Algo bellman_ford step 9242 current loss 0.052137, current_train_items 295776.
I0304 19:32:24.487035 22579586809984 run.py:483] Algo bellman_ford step 9243 current loss 0.042783, current_train_items 295808.
I0304 19:32:24.520048 22579586809984 run.py:483] Algo bellman_ford step 9244 current loss 0.051918, current_train_items 295840.
I0304 19:32:24.539916 22579586809984 run.py:483] Algo bellman_ford step 9245 current loss 0.008437, current_train_items 295872.
I0304 19:32:24.556271 22579586809984 run.py:483] Algo bellman_ford step 9246 current loss 0.016843, current_train_items 295904.
I0304 19:32:24.580367 22579586809984 run.py:483] Algo bellman_ford step 9247 current loss 0.021586, current_train_items 295936.
I0304 19:32:24.612503 22579586809984 run.py:483] Algo bellman_ford step 9248 current loss 0.056121, current_train_items 295968.
I0304 19:32:24.647037 22579586809984 run.py:483] Algo bellman_ford step 9249 current loss 0.048367, current_train_items 296000.
I0304 19:32:24.667123 22579586809984 run.py:483] Algo bellman_ford step 9250 current loss 0.007523, current_train_items 296032.
I0304 19:32:24.675152 22579586809984 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0304 19:32:24.675260 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:24.692219 22579586809984 run.py:483] Algo bellman_ford step 9251 current loss 0.018881, current_train_items 296064.
I0304 19:32:24.717206 22579586809984 run.py:483] Algo bellman_ford step 9252 current loss 0.033576, current_train_items 296096.
I0304 19:32:24.748832 22579586809984 run.py:483] Algo bellman_ford step 9253 current loss 0.030402, current_train_items 296128.
I0304 19:32:24.781153 22579586809984 run.py:483] Algo bellman_ford step 9254 current loss 0.031947, current_train_items 296160.
I0304 19:32:24.801182 22579586809984 run.py:483] Algo bellman_ford step 9255 current loss 0.005872, current_train_items 296192.
I0304 19:32:24.817698 22579586809984 run.py:483] Algo bellman_ford step 9256 current loss 0.011416, current_train_items 296224.
I0304 19:32:24.842232 22579586809984 run.py:483] Algo bellman_ford step 9257 current loss 0.026655, current_train_items 296256.
I0304 19:32:24.874571 22579586809984 run.py:483] Algo bellman_ford step 9258 current loss 0.051849, current_train_items 296288.
I0304 19:32:24.908232 22579586809984 run.py:483] Algo bellman_ford step 9259 current loss 0.043751, current_train_items 296320.
I0304 19:32:24.928400 22579586809984 run.py:483] Algo bellman_ford step 9260 current loss 0.005710, current_train_items 296352.
I0304 19:32:24.945662 22579586809984 run.py:483] Algo bellman_ford step 9261 current loss 0.035269, current_train_items 296384.
I0304 19:32:24.968593 22579586809984 run.py:483] Algo bellman_ford step 9262 current loss 0.025774, current_train_items 296416.
I0304 19:32:25.000368 22579586809984 run.py:483] Algo bellman_ford step 9263 current loss 0.048713, current_train_items 296448.
I0304 19:32:25.034185 22579586809984 run.py:483] Algo bellman_ford step 9264 current loss 0.040330, current_train_items 296480.
I0304 19:32:25.053703 22579586809984 run.py:483] Algo bellman_ford step 9265 current loss 0.007003, current_train_items 296512.
I0304 19:32:25.070077 22579586809984 run.py:483] Algo bellman_ford step 9266 current loss 0.009868, current_train_items 296544.
I0304 19:32:25.093072 22579586809984 run.py:483] Algo bellman_ford step 9267 current loss 0.024746, current_train_items 296576.
I0304 19:32:25.124987 22579586809984 run.py:483] Algo bellman_ford step 9268 current loss 0.030189, current_train_items 296608.
I0304 19:32:25.158315 22579586809984 run.py:483] Algo bellman_ford step 9269 current loss 0.059750, current_train_items 296640.
I0304 19:32:25.178102 22579586809984 run.py:483] Algo bellman_ford step 9270 current loss 0.002268, current_train_items 296672.
I0304 19:32:25.194360 22579586809984 run.py:483] Algo bellman_ford step 9271 current loss 0.010029, current_train_items 296704.
I0304 19:32:25.218274 22579586809984 run.py:483] Algo bellman_ford step 9272 current loss 0.013236, current_train_items 296736.
I0304 19:32:25.251790 22579586809984 run.py:483] Algo bellman_ford step 9273 current loss 0.055295, current_train_items 296768.
I0304 19:32:25.286244 22579586809984 run.py:483] Algo bellman_ford step 9274 current loss 0.041706, current_train_items 296800.
I0304 19:32:25.306390 22579586809984 run.py:483] Algo bellman_ford step 9275 current loss 0.005964, current_train_items 296832.
I0304 19:32:25.322902 22579586809984 run.py:483] Algo bellman_ford step 9276 current loss 0.018020, current_train_items 296864.
I0304 19:32:25.346674 22579586809984 run.py:483] Algo bellman_ford step 9277 current loss 0.032823, current_train_items 296896.
I0304 19:32:25.378974 22579586809984 run.py:483] Algo bellman_ford step 9278 current loss 0.029920, current_train_items 296928.
I0304 19:32:25.412821 22579586809984 run.py:483] Algo bellman_ford step 9279 current loss 0.059798, current_train_items 296960.
I0304 19:32:25.432237 22579586809984 run.py:483] Algo bellman_ford step 9280 current loss 0.001633, current_train_items 296992.
I0304 19:32:25.448656 22579586809984 run.py:483] Algo bellman_ford step 9281 current loss 0.036248, current_train_items 297024.
I0304 19:32:25.472627 22579586809984 run.py:483] Algo bellman_ford step 9282 current loss 0.011204, current_train_items 297056.
I0304 19:32:25.503068 22579586809984 run.py:483] Algo bellman_ford step 9283 current loss 0.018362, current_train_items 297088.
I0304 19:32:25.536041 22579586809984 run.py:483] Algo bellman_ford step 9284 current loss 0.062750, current_train_items 297120.
I0304 19:32:25.555923 22579586809984 run.py:483] Algo bellman_ford step 9285 current loss 0.002427, current_train_items 297152.
I0304 19:32:25.571709 22579586809984 run.py:483] Algo bellman_ford step 9286 current loss 0.006272, current_train_items 297184.
I0304 19:32:25.596705 22579586809984 run.py:483] Algo bellman_ford step 9287 current loss 0.047855, current_train_items 297216.
I0304 19:32:25.627954 22579586809984 run.py:483] Algo bellman_ford step 9288 current loss 0.037304, current_train_items 297248.
I0304 19:32:25.663228 22579586809984 run.py:483] Algo bellman_ford step 9289 current loss 0.033636, current_train_items 297280.
I0304 19:32:25.682951 22579586809984 run.py:483] Algo bellman_ford step 9290 current loss 0.002387, current_train_items 297312.
I0304 19:32:25.699264 22579586809984 run.py:483] Algo bellman_ford step 9291 current loss 0.032810, current_train_items 297344.
I0304 19:32:25.723742 22579586809984 run.py:483] Algo bellman_ford step 9292 current loss 0.036107, current_train_items 297376.
I0304 19:32:25.754788 22579586809984 run.py:483] Algo bellman_ford step 9293 current loss 0.028322, current_train_items 297408.
I0304 19:32:25.789226 22579586809984 run.py:483] Algo bellman_ford step 9294 current loss 0.068113, current_train_items 297440.
I0304 19:32:25.808837 22579586809984 run.py:483] Algo bellman_ford step 9295 current loss 0.009579, current_train_items 297472.
I0304 19:32:25.825115 22579586809984 run.py:483] Algo bellman_ford step 9296 current loss 0.009258, current_train_items 297504.
I0304 19:32:25.849031 22579586809984 run.py:483] Algo bellman_ford step 9297 current loss 0.043046, current_train_items 297536.
I0304 19:32:25.881290 22579586809984 run.py:483] Algo bellman_ford step 9298 current loss 0.039920, current_train_items 297568.
I0304 19:32:25.915445 22579586809984 run.py:483] Algo bellman_ford step 9299 current loss 0.041436, current_train_items 297600.
I0304 19:32:25.935595 22579586809984 run.py:483] Algo bellman_ford step 9300 current loss 0.002590, current_train_items 297632.
I0304 19:32:25.943124 22579586809984 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0304 19:32:25.943229 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:25.960268 22579586809984 run.py:483] Algo bellman_ford step 9301 current loss 0.018738, current_train_items 297664.
I0304 19:32:25.985134 22579586809984 run.py:483] Algo bellman_ford step 9302 current loss 0.027911, current_train_items 297696.
I0304 19:32:26.017652 22579586809984 run.py:483] Algo bellman_ford step 9303 current loss 0.028131, current_train_items 297728.
I0304 19:32:26.053122 22579586809984 run.py:483] Algo bellman_ford step 9304 current loss 0.062677, current_train_items 297760.
I0304 19:32:26.072919 22579586809984 run.py:483] Algo bellman_ford step 9305 current loss 0.002170, current_train_items 297792.
I0304 19:32:26.088859 22579586809984 run.py:483] Algo bellman_ford step 9306 current loss 0.019684, current_train_items 297824.
I0304 19:32:26.113492 22579586809984 run.py:483] Algo bellman_ford step 9307 current loss 0.079803, current_train_items 297856.
I0304 19:32:26.145662 22579586809984 run.py:483] Algo bellman_ford step 9308 current loss 0.158503, current_train_items 297888.
I0304 19:32:26.179621 22579586809984 run.py:483] Algo bellman_ford step 9309 current loss 0.111060, current_train_items 297920.
I0304 19:32:26.199374 22579586809984 run.py:483] Algo bellman_ford step 9310 current loss 0.002901, current_train_items 297952.
I0304 19:32:26.215635 22579586809984 run.py:483] Algo bellman_ford step 9311 current loss 0.005605, current_train_items 297984.
I0304 19:32:26.239906 22579586809984 run.py:483] Algo bellman_ford step 9312 current loss 0.025197, current_train_items 298016.
I0304 19:32:26.271351 22579586809984 run.py:483] Algo bellman_ford step 9313 current loss 0.030430, current_train_items 298048.
I0304 19:32:26.305916 22579586809984 run.py:483] Algo bellman_ford step 9314 current loss 0.081028, current_train_items 298080.
I0304 19:32:26.325917 22579586809984 run.py:483] Algo bellman_ford step 9315 current loss 0.002977, current_train_items 298112.
I0304 19:32:26.341916 22579586809984 run.py:483] Algo bellman_ford step 9316 current loss 0.006117, current_train_items 298144.
I0304 19:32:26.366442 22579586809984 run.py:483] Algo bellman_ford step 9317 current loss 0.030910, current_train_items 298176.
I0304 19:32:26.399461 22579586809984 run.py:483] Algo bellman_ford step 9318 current loss 0.048884, current_train_items 298208.
I0304 19:32:26.433563 22579586809984 run.py:483] Algo bellman_ford step 9319 current loss 0.033307, current_train_items 298240.
I0304 19:32:26.453510 22579586809984 run.py:483] Algo bellman_ford step 9320 current loss 0.004083, current_train_items 298272.
I0304 19:32:26.469890 22579586809984 run.py:483] Algo bellman_ford step 9321 current loss 0.023014, current_train_items 298304.
I0304 19:32:26.493360 22579586809984 run.py:483] Algo bellman_ford step 9322 current loss 0.021973, current_train_items 298336.
I0304 19:32:26.524774 22579586809984 run.py:483] Algo bellman_ford step 9323 current loss 0.032215, current_train_items 298368.
I0304 19:32:26.561571 22579586809984 run.py:483] Algo bellman_ford step 9324 current loss 0.051786, current_train_items 298400.
I0304 19:32:26.581423 22579586809984 run.py:483] Algo bellman_ford step 9325 current loss 0.002230, current_train_items 298432.
I0304 19:32:26.597616 22579586809984 run.py:483] Algo bellman_ford step 9326 current loss 0.015514, current_train_items 298464.
I0304 19:32:26.622427 22579586809984 run.py:483] Algo bellman_ford step 9327 current loss 0.074796, current_train_items 298496.
I0304 19:32:26.655651 22579586809984 run.py:483] Algo bellman_ford step 9328 current loss 0.082279, current_train_items 298528.
I0304 19:32:26.687072 22579586809984 run.py:483] Algo bellman_ford step 9329 current loss 0.032029, current_train_items 298560.
I0304 19:32:26.707070 22579586809984 run.py:483] Algo bellman_ford step 9330 current loss 0.003147, current_train_items 298592.
I0304 19:32:26.723510 22579586809984 run.py:483] Algo bellman_ford step 9331 current loss 0.013244, current_train_items 298624.
I0304 19:32:26.747560 22579586809984 run.py:483] Algo bellman_ford step 9332 current loss 0.062821, current_train_items 298656.
I0304 19:32:26.779495 22579586809984 run.py:483] Algo bellman_ford step 9333 current loss 0.088247, current_train_items 298688.
I0304 19:32:26.812893 22579586809984 run.py:483] Algo bellman_ford step 9334 current loss 0.159034, current_train_items 298720.
I0304 19:32:26.832827 22579586809984 run.py:483] Algo bellman_ford step 9335 current loss 0.004003, current_train_items 298752.
I0304 19:32:26.849209 22579586809984 run.py:483] Algo bellman_ford step 9336 current loss 0.037274, current_train_items 298784.
I0304 19:32:26.874387 22579586809984 run.py:483] Algo bellman_ford step 9337 current loss 0.030159, current_train_items 298816.
I0304 19:32:26.906759 22579586809984 run.py:483] Algo bellman_ford step 9338 current loss 0.038241, current_train_items 298848.
I0304 19:32:26.941431 22579586809984 run.py:483] Algo bellman_ford step 9339 current loss 0.070183, current_train_items 298880.
I0304 19:32:26.961399 22579586809984 run.py:483] Algo bellman_ford step 9340 current loss 0.009873, current_train_items 298912.
I0304 19:32:26.977311 22579586809984 run.py:483] Algo bellman_ford step 9341 current loss 0.014705, current_train_items 298944.
I0304 19:32:27.001888 22579586809984 run.py:483] Algo bellman_ford step 9342 current loss 0.051731, current_train_items 298976.
I0304 19:32:27.034225 22579586809984 run.py:483] Algo bellman_ford step 9343 current loss 0.055408, current_train_items 299008.
I0304 19:32:27.068578 22579586809984 run.py:483] Algo bellman_ford step 9344 current loss 0.063073, current_train_items 299040.
I0304 19:32:27.088438 22579586809984 run.py:483] Algo bellman_ford step 9345 current loss 0.002182, current_train_items 299072.
I0304 19:32:27.104669 22579586809984 run.py:483] Algo bellman_ford step 9346 current loss 0.009383, current_train_items 299104.
I0304 19:32:27.129231 22579586809984 run.py:483] Algo bellman_ford step 9347 current loss 0.039323, current_train_items 299136.
I0304 19:32:27.161794 22579586809984 run.py:483] Algo bellman_ford step 9348 current loss 0.029165, current_train_items 299168.
I0304 19:32:27.194857 22579586809984 run.py:483] Algo bellman_ford step 9349 current loss 0.068117, current_train_items 299200.
I0304 19:32:27.214832 22579586809984 run.py:483] Algo bellman_ford step 9350 current loss 0.016054, current_train_items 299232.
I0304 19:32:27.223079 22579586809984 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0304 19:32:27.223184 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:27.239964 22579586809984 run.py:483] Algo bellman_ford step 9351 current loss 0.003291, current_train_items 299264.
I0304 19:32:27.265856 22579586809984 run.py:483] Algo bellman_ford step 9352 current loss 0.086061, current_train_items 299296.
I0304 19:32:27.298969 22579586809984 run.py:483] Algo bellman_ford step 9353 current loss 0.086637, current_train_items 299328.
I0304 19:32:27.335501 22579586809984 run.py:483] Algo bellman_ford step 9354 current loss 0.089421, current_train_items 299360.
I0304 19:32:27.355348 22579586809984 run.py:483] Algo bellman_ford step 9355 current loss 0.056782, current_train_items 299392.
I0304 19:32:27.371018 22579586809984 run.py:483] Algo bellman_ford step 9356 current loss 0.040158, current_train_items 299424.
I0304 19:32:27.395726 22579586809984 run.py:483] Algo bellman_ford step 9357 current loss 0.210649, current_train_items 299456.
I0304 19:32:27.427972 22579586809984 run.py:483] Algo bellman_ford step 9358 current loss 0.144860, current_train_items 299488.
I0304 19:32:27.461433 22579586809984 run.py:483] Algo bellman_ford step 9359 current loss 0.141834, current_train_items 299520.
I0304 19:32:27.481877 22579586809984 run.py:483] Algo bellman_ford step 9360 current loss 0.008348, current_train_items 299552.
I0304 19:32:27.498158 22579586809984 run.py:483] Algo bellman_ford step 9361 current loss 0.041582, current_train_items 299584.
I0304 19:32:27.522169 22579586809984 run.py:483] Algo bellman_ford step 9362 current loss 0.038636, current_train_items 299616.
I0304 19:32:27.555029 22579586809984 run.py:483] Algo bellman_ford step 9363 current loss 0.039083, current_train_items 299648.
I0304 19:32:27.589792 22579586809984 run.py:483] Algo bellman_ford step 9364 current loss 0.061512, current_train_items 299680.
I0304 19:32:27.609558 22579586809984 run.py:483] Algo bellman_ford step 9365 current loss 0.054553, current_train_items 299712.
I0304 19:32:27.626293 22579586809984 run.py:483] Algo bellman_ford step 9366 current loss 0.056645, current_train_items 299744.
I0304 19:32:27.652290 22579586809984 run.py:483] Algo bellman_ford step 9367 current loss 0.081853, current_train_items 299776.
I0304 19:32:27.685555 22579586809984 run.py:483] Algo bellman_ford step 9368 current loss 0.046619, current_train_items 299808.
I0304 19:32:27.719254 22579586809984 run.py:483] Algo bellman_ford step 9369 current loss 0.040140, current_train_items 299840.
I0304 19:32:27.739511 22579586809984 run.py:483] Algo bellman_ford step 9370 current loss 0.006839, current_train_items 299872.
I0304 19:32:27.756075 22579586809984 run.py:483] Algo bellman_ford step 9371 current loss 0.033993, current_train_items 299904.
I0304 19:32:27.780461 22579586809984 run.py:483] Algo bellman_ford step 9372 current loss 0.016630, current_train_items 299936.
I0304 19:32:27.813271 22579586809984 run.py:483] Algo bellman_ford step 9373 current loss 0.039358, current_train_items 299968.
I0304 19:32:27.847578 22579586809984 run.py:483] Algo bellman_ford step 9374 current loss 0.048792, current_train_items 300000.
I0304 19:32:27.867361 22579586809984 run.py:483] Algo bellman_ford step 9375 current loss 0.049132, current_train_items 300032.
I0304 19:32:27.883355 22579586809984 run.py:483] Algo bellman_ford step 9376 current loss 0.039849, current_train_items 300064.
I0304 19:32:27.907258 22579586809984 run.py:483] Algo bellman_ford step 9377 current loss 0.030065, current_train_items 300096.
I0304 19:32:27.938661 22579586809984 run.py:483] Algo bellman_ford step 9378 current loss 0.025429, current_train_items 300128.
I0304 19:32:27.972549 22579586809984 run.py:483] Algo bellman_ford step 9379 current loss 0.081917, current_train_items 300160.
I0304 19:32:27.992261 22579586809984 run.py:483] Algo bellman_ford step 9380 current loss 0.001630, current_train_items 300192.
I0304 19:32:28.008070 22579586809984 run.py:483] Algo bellman_ford step 9381 current loss 0.031186, current_train_items 300224.
I0304 19:32:28.032301 22579586809984 run.py:483] Algo bellman_ford step 9382 current loss 0.028782, current_train_items 300256.
I0304 19:32:28.063772 22579586809984 run.py:483] Algo bellman_ford step 9383 current loss 0.059031, current_train_items 300288.
I0304 19:32:28.097787 22579586809984 run.py:483] Algo bellman_ford step 9384 current loss 0.081873, current_train_items 300320.
I0304 19:32:28.118023 22579586809984 run.py:483] Algo bellman_ford step 9385 current loss 0.002518, current_train_items 300352.
I0304 19:32:28.134806 22579586809984 run.py:483] Algo bellman_ford step 9386 current loss 0.020901, current_train_items 300384.
I0304 19:32:28.158808 22579586809984 run.py:483] Algo bellman_ford step 9387 current loss 0.035190, current_train_items 300416.
I0304 19:32:28.190860 22579586809984 run.py:483] Algo bellman_ford step 9388 current loss 0.040505, current_train_items 300448.
I0304 19:32:28.226802 22579586809984 run.py:483] Algo bellman_ford step 9389 current loss 0.048850, current_train_items 300480.
I0304 19:32:28.247175 22579586809984 run.py:483] Algo bellman_ford step 9390 current loss 0.003983, current_train_items 300512.
I0304 19:32:28.263777 22579586809984 run.py:483] Algo bellman_ford step 9391 current loss 0.006057, current_train_items 300544.
I0304 19:32:28.287984 22579586809984 run.py:483] Algo bellman_ford step 9392 current loss 0.012358, current_train_items 300576.
I0304 19:32:28.319912 22579586809984 run.py:483] Algo bellman_ford step 9393 current loss 0.036886, current_train_items 300608.
I0304 19:32:28.353677 22579586809984 run.py:483] Algo bellman_ford step 9394 current loss 0.050287, current_train_items 300640.
I0304 19:32:28.373442 22579586809984 run.py:483] Algo bellman_ford step 9395 current loss 0.005682, current_train_items 300672.
I0304 19:32:28.389817 22579586809984 run.py:483] Algo bellman_ford step 9396 current loss 0.019311, current_train_items 300704.
I0304 19:32:28.415163 22579586809984 run.py:483] Algo bellman_ford step 9397 current loss 0.051146, current_train_items 300736.
I0304 19:32:28.447138 22579586809984 run.py:483] Algo bellman_ford step 9398 current loss 0.073946, current_train_items 300768.
I0304 19:32:28.481534 22579586809984 run.py:483] Algo bellman_ford step 9399 current loss 0.088628, current_train_items 300800.
I0304 19:32:28.501734 22579586809984 run.py:483] Algo bellman_ford step 9400 current loss 0.007260, current_train_items 300832.
I0304 19:32:28.509694 22579586809984 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0304 19:32:28.509800 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:28.526939 22579586809984 run.py:483] Algo bellman_ford step 9401 current loss 0.007083, current_train_items 300864.
I0304 19:32:28.552733 22579586809984 run.py:483] Algo bellman_ford step 9402 current loss 0.030484, current_train_items 300896.
I0304 19:32:28.585314 22579586809984 run.py:483] Algo bellman_ford step 9403 current loss 0.042938, current_train_items 300928.
I0304 19:32:28.620436 22579586809984 run.py:483] Algo bellman_ford step 9404 current loss 0.035468, current_train_items 300960.
I0304 19:32:28.640431 22579586809984 run.py:483] Algo bellman_ford step 9405 current loss 0.002709, current_train_items 300992.
I0304 19:32:28.656761 22579586809984 run.py:483] Algo bellman_ford step 9406 current loss 0.008422, current_train_items 301024.
I0304 19:32:28.682371 22579586809984 run.py:483] Algo bellman_ford step 9407 current loss 0.025565, current_train_items 301056.
I0304 19:32:28.714055 22579586809984 run.py:483] Algo bellman_ford step 9408 current loss 0.025651, current_train_items 301088.
I0304 19:32:28.749242 22579586809984 run.py:483] Algo bellman_ford step 9409 current loss 0.060173, current_train_items 301120.
I0304 19:32:28.768939 22579586809984 run.py:483] Algo bellman_ford step 9410 current loss 0.002100, current_train_items 301152.
I0304 19:32:28.785157 22579586809984 run.py:483] Algo bellman_ford step 9411 current loss 0.004581, current_train_items 301184.
I0304 19:32:28.810198 22579586809984 run.py:483] Algo bellman_ford step 9412 current loss 0.044164, current_train_items 301216.
I0304 19:32:28.842385 22579586809984 run.py:483] Algo bellman_ford step 9413 current loss 0.029636, current_train_items 301248.
I0304 19:32:28.875503 22579586809984 run.py:483] Algo bellman_ford step 9414 current loss 0.073962, current_train_items 301280.
I0304 19:32:28.895448 22579586809984 run.py:483] Algo bellman_ford step 9415 current loss 0.011092, current_train_items 301312.
I0304 19:32:28.911973 22579586809984 run.py:483] Algo bellman_ford step 9416 current loss 0.019812, current_train_items 301344.
I0304 19:32:28.937043 22579586809984 run.py:483] Algo bellman_ford step 9417 current loss 0.047681, current_train_items 301376.
I0304 19:32:28.968520 22579586809984 run.py:483] Algo bellman_ford step 9418 current loss 0.028452, current_train_items 301408.
I0304 19:32:29.000688 22579586809984 run.py:483] Algo bellman_ford step 9419 current loss 0.017995, current_train_items 301440.
I0304 19:32:29.020406 22579586809984 run.py:483] Algo bellman_ford step 9420 current loss 0.015042, current_train_items 301472.
I0304 19:32:29.037013 22579586809984 run.py:483] Algo bellman_ford step 9421 current loss 0.041991, current_train_items 301504.
I0304 19:32:29.061842 22579586809984 run.py:483] Algo bellman_ford step 9422 current loss 0.037226, current_train_items 301536.
I0304 19:32:29.093826 22579586809984 run.py:483] Algo bellman_ford step 9423 current loss 0.042915, current_train_items 301568.
I0304 19:32:29.127583 22579586809984 run.py:483] Algo bellman_ford step 9424 current loss 0.047680, current_train_items 301600.
I0304 19:32:29.147613 22579586809984 run.py:483] Algo bellman_ford step 9425 current loss 0.014209, current_train_items 301632.
I0304 19:32:29.163454 22579586809984 run.py:483] Algo bellman_ford step 9426 current loss 0.015879, current_train_items 301664.
I0304 19:32:29.187458 22579586809984 run.py:483] Algo bellman_ford step 9427 current loss 0.019579, current_train_items 301696.
I0304 19:32:29.220331 22579586809984 run.py:483] Algo bellman_ford step 9428 current loss 0.083323, current_train_items 301728.
I0304 19:32:29.251549 22579586809984 run.py:483] Algo bellman_ford step 9429 current loss 0.095475, current_train_items 301760.
I0304 19:32:29.271192 22579586809984 run.py:483] Algo bellman_ford step 9430 current loss 0.005109, current_train_items 301792.
I0304 19:32:29.287487 22579586809984 run.py:483] Algo bellman_ford step 9431 current loss 0.020544, current_train_items 301824.
I0304 19:32:29.311815 22579586809984 run.py:483] Algo bellman_ford step 9432 current loss 0.053669, current_train_items 301856.
I0304 19:32:29.343358 22579586809984 run.py:483] Algo bellman_ford step 9433 current loss 0.041958, current_train_items 301888.
I0304 19:32:29.377984 22579586809984 run.py:483] Algo bellman_ford step 9434 current loss 0.057382, current_train_items 301920.
I0304 19:32:29.397843 22579586809984 run.py:483] Algo bellman_ford step 9435 current loss 0.002469, current_train_items 301952.
I0304 19:32:29.414427 22579586809984 run.py:483] Algo bellman_ford step 9436 current loss 0.029959, current_train_items 301984.
I0304 19:32:29.439324 22579586809984 run.py:483] Algo bellman_ford step 9437 current loss 0.019245, current_train_items 302016.
I0304 19:32:29.471795 22579586809984 run.py:483] Algo bellman_ford step 9438 current loss 0.035369, current_train_items 302048.
I0304 19:32:29.504657 22579586809984 run.py:483] Algo bellman_ford step 9439 current loss 0.064211, current_train_items 302080.
I0304 19:32:29.524425 22579586809984 run.py:483] Algo bellman_ford step 9440 current loss 0.051840, current_train_items 302112.
I0304 19:32:29.541054 22579586809984 run.py:483] Algo bellman_ford step 9441 current loss 0.036858, current_train_items 302144.
I0304 19:32:29.565458 22579586809984 run.py:483] Algo bellman_ford step 9442 current loss 0.043836, current_train_items 302176.
I0304 19:32:29.598253 22579586809984 run.py:483] Algo bellman_ford step 9443 current loss 0.025655, current_train_items 302208.
I0304 19:32:29.634862 22579586809984 run.py:483] Algo bellman_ford step 9444 current loss 0.076547, current_train_items 302240.
I0304 19:32:29.654510 22579586809984 run.py:483] Algo bellman_ford step 9445 current loss 0.053686, current_train_items 302272.
I0304 19:32:29.670432 22579586809984 run.py:483] Algo bellman_ford step 9446 current loss 0.008546, current_train_items 302304.
I0304 19:32:29.694420 22579586809984 run.py:483] Algo bellman_ford step 9447 current loss 0.036135, current_train_items 302336.
I0304 19:32:29.726367 22579586809984 run.py:483] Algo bellman_ford step 9448 current loss 0.071243, current_train_items 302368.
I0304 19:32:29.758776 22579586809984 run.py:483] Algo bellman_ford step 9449 current loss 0.094470, current_train_items 302400.
I0304 19:32:29.778661 22579586809984 run.py:483] Algo bellman_ford step 9450 current loss 0.002816, current_train_items 302432.
I0304 19:32:29.786506 22579586809984 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0304 19:32:29.786611 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:29.803654 22579586809984 run.py:483] Algo bellman_ford step 9451 current loss 0.006047, current_train_items 302464.
I0304 19:32:29.827472 22579586809984 run.py:483] Algo bellman_ford step 9452 current loss 0.029432, current_train_items 302496.
I0304 19:32:29.860124 22579586809984 run.py:483] Algo bellman_ford step 9453 current loss 0.036555, current_train_items 302528.
I0304 19:32:29.894623 22579586809984 run.py:483] Algo bellman_ford step 9454 current loss 0.058548, current_train_items 302560.
I0304 19:32:29.914622 22579586809984 run.py:483] Algo bellman_ford step 9455 current loss 0.002039, current_train_items 302592.
I0304 19:32:29.930649 22579586809984 run.py:483] Algo bellman_ford step 9456 current loss 0.005523, current_train_items 302624.
I0304 19:32:29.955055 22579586809984 run.py:483] Algo bellman_ford step 9457 current loss 0.032936, current_train_items 302656.
I0304 19:32:29.987164 22579586809984 run.py:483] Algo bellman_ford step 9458 current loss 0.026882, current_train_items 302688.
I0304 19:32:30.019482 22579586809984 run.py:483] Algo bellman_ford step 9459 current loss 0.042123, current_train_items 302720.
I0304 19:32:30.039568 22579586809984 run.py:483] Algo bellman_ford step 9460 current loss 0.002732, current_train_items 302752.
I0304 19:32:30.056215 22579586809984 run.py:483] Algo bellman_ford step 9461 current loss 0.034445, current_train_items 302784.
I0304 19:32:30.079569 22579586809984 run.py:483] Algo bellman_ford step 9462 current loss 0.025207, current_train_items 302816.
I0304 19:32:30.112156 22579586809984 run.py:483] Algo bellman_ford step 9463 current loss 0.043497, current_train_items 302848.
I0304 19:32:30.145604 22579586809984 run.py:483] Algo bellman_ford step 9464 current loss 0.041752, current_train_items 302880.
I0304 19:32:30.165102 22579586809984 run.py:483] Algo bellman_ford step 9465 current loss 0.004053, current_train_items 302912.
I0304 19:32:30.181702 22579586809984 run.py:483] Algo bellman_ford step 9466 current loss 0.076559, current_train_items 302944.
I0304 19:32:30.207441 22579586809984 run.py:483] Algo bellman_ford step 9467 current loss 0.138868, current_train_items 302976.
I0304 19:32:30.240164 22579586809984 run.py:483] Algo bellman_ford step 9468 current loss 0.069993, current_train_items 303008.
I0304 19:32:30.274444 22579586809984 run.py:483] Algo bellman_ford step 9469 current loss 0.177448, current_train_items 303040.
I0304 19:32:30.294945 22579586809984 run.py:483] Algo bellman_ford step 9470 current loss 0.003613, current_train_items 303072.
I0304 19:32:30.310981 22579586809984 run.py:483] Algo bellman_ford step 9471 current loss 0.008717, current_train_items 303104.
I0304 19:32:30.334983 22579586809984 run.py:483] Algo bellman_ford step 9472 current loss 0.042484, current_train_items 303136.
I0304 19:32:30.367379 22579586809984 run.py:483] Algo bellman_ford step 9473 current loss 0.048424, current_train_items 303168.
I0304 19:32:30.400229 22579586809984 run.py:483] Algo bellman_ford step 9474 current loss 0.060942, current_train_items 303200.
I0304 19:32:30.419903 22579586809984 run.py:483] Algo bellman_ford step 9475 current loss 0.003315, current_train_items 303232.
I0304 19:32:30.436365 22579586809984 run.py:483] Algo bellman_ford step 9476 current loss 0.071492, current_train_items 303264.
I0304 19:32:30.459924 22579586809984 run.py:483] Algo bellman_ford step 9477 current loss 0.038475, current_train_items 303296.
I0304 19:32:30.491380 22579586809984 run.py:483] Algo bellman_ford step 9478 current loss 0.072791, current_train_items 303328.
I0304 19:32:30.524544 22579586809984 run.py:483] Algo bellman_ford step 9479 current loss 0.079598, current_train_items 303360.
I0304 19:32:30.544048 22579586809984 run.py:483] Algo bellman_ford step 9480 current loss 0.003026, current_train_items 303392.
I0304 19:32:30.560116 22579586809984 run.py:483] Algo bellman_ford step 9481 current loss 0.012706, current_train_items 303424.
I0304 19:32:30.585597 22579586809984 run.py:483] Algo bellman_ford step 9482 current loss 0.080413, current_train_items 303456.
I0304 19:32:30.616390 22579586809984 run.py:483] Algo bellman_ford step 9483 current loss 0.068181, current_train_items 303488.
I0304 19:32:30.649077 22579586809984 run.py:483] Algo bellman_ford step 9484 current loss 0.102333, current_train_items 303520.
I0304 19:32:30.668923 22579586809984 run.py:483] Algo bellman_ford step 9485 current loss 0.002019, current_train_items 303552.
I0304 19:32:30.685612 22579586809984 run.py:483] Algo bellman_ford step 9486 current loss 0.023495, current_train_items 303584.
I0304 19:32:30.710640 22579586809984 run.py:483] Algo bellman_ford step 9487 current loss 0.060711, current_train_items 303616.
I0304 19:32:30.743640 22579586809984 run.py:483] Algo bellman_ford step 9488 current loss 0.059909, current_train_items 303648.
I0304 19:32:30.776482 22579586809984 run.py:483] Algo bellman_ford step 9489 current loss 0.046714, current_train_items 303680.
I0304 19:32:30.796400 22579586809984 run.py:483] Algo bellman_ford step 9490 current loss 0.002674, current_train_items 303712.
I0304 19:32:30.812570 22579586809984 run.py:483] Algo bellman_ford step 9491 current loss 0.026194, current_train_items 303744.
I0304 19:32:30.837161 22579586809984 run.py:483] Algo bellman_ford step 9492 current loss 0.028147, current_train_items 303776.
I0304 19:32:30.868182 22579586809984 run.py:483] Algo bellman_ford step 9493 current loss 0.037024, current_train_items 303808.
I0304 19:32:30.901832 22579586809984 run.py:483] Algo bellman_ford step 9494 current loss 0.072680, current_train_items 303840.
I0304 19:32:30.921500 22579586809984 run.py:483] Algo bellman_ford step 9495 current loss 0.005960, current_train_items 303872.
I0304 19:32:30.938241 22579586809984 run.py:483] Algo bellman_ford step 9496 current loss 0.010269, current_train_items 303904.
I0304 19:32:30.962414 22579586809984 run.py:483] Algo bellman_ford step 9497 current loss 0.035523, current_train_items 303936.
I0304 19:32:30.993220 22579586809984 run.py:483] Algo bellman_ford step 9498 current loss 0.049997, current_train_items 303968.
I0304 19:32:31.027794 22579586809984 run.py:483] Algo bellman_ford step 9499 current loss 0.069313, current_train_items 304000.
I0304 19:32:31.048040 22579586809984 run.py:483] Algo bellman_ford step 9500 current loss 0.002940, current_train_items 304032.
I0304 19:32:31.055749 22579586809984 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0304 19:32:31.055880 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:32:31.072574 22579586809984 run.py:483] Algo bellman_ford step 9501 current loss 0.028241, current_train_items 304064.
I0304 19:32:31.098117 22579586809984 run.py:483] Algo bellman_ford step 9502 current loss 0.050949, current_train_items 304096.
I0304 19:32:31.130891 22579586809984 run.py:483] Algo bellman_ford step 9503 current loss 0.032222, current_train_items 304128.
I0304 19:32:31.164067 22579586809984 run.py:483] Algo bellman_ford step 9504 current loss 0.063563, current_train_items 304160.
I0304 19:32:31.184396 22579586809984 run.py:483] Algo bellman_ford step 9505 current loss 0.006594, current_train_items 304192.
I0304 19:32:31.200695 22579586809984 run.py:483] Algo bellman_ford step 9506 current loss 0.008735, current_train_items 304224.
I0304 19:32:31.226199 22579586809984 run.py:483] Algo bellman_ford step 9507 current loss 0.029222, current_train_items 304256.
I0304 19:32:31.257344 22579586809984 run.py:483] Algo bellman_ford step 9508 current loss 0.067406, current_train_items 304288.
I0304 19:32:31.293634 22579586809984 run.py:483] Algo bellman_ford step 9509 current loss 0.148744, current_train_items 304320.
I0304 19:32:31.313724 22579586809984 run.py:483] Algo bellman_ford step 9510 current loss 0.002317, current_train_items 304352.
I0304 19:32:31.330716 22579586809984 run.py:483] Algo bellman_ford step 9511 current loss 0.019797, current_train_items 304384.
I0304 19:32:31.355450 22579586809984 run.py:483] Algo bellman_ford step 9512 current loss 0.039563, current_train_items 304416.
I0304 19:32:31.387660 22579586809984 run.py:483] Algo bellman_ford step 9513 current loss 0.049877, current_train_items 304448.
I0304 19:32:31.420967 22579586809984 run.py:483] Algo bellman_ford step 9514 current loss 0.057065, current_train_items 304480.
I0304 19:32:31.440762 22579586809984 run.py:483] Algo bellman_ford step 9515 current loss 0.005286, current_train_items 304512.
I0304 19:32:31.456774 22579586809984 run.py:483] Algo bellman_ford step 9516 current loss 0.015636, current_train_items 304544.
I0304 19:32:31.481232 22579586809984 run.py:483] Algo bellman_ford step 9517 current loss 0.039723, current_train_items 304576.
I0304 19:32:31.513473 22579586809984 run.py:483] Algo bellman_ford step 9518 current loss 0.069569, current_train_items 304608.
I0304 19:32:31.546149 22579586809984 run.py:483] Algo bellman_ford step 9519 current loss 0.050032, current_train_items 304640.
I0304 19:32:31.565865 22579586809984 run.py:483] Algo bellman_ford step 9520 current loss 0.003096, current_train_items 304672.
I0304 19:32:31.581432 22579586809984 run.py:483] Algo bellman_ford step 9521 current loss 0.011632, current_train_items 304704.
I0304 19:32:31.605489 22579586809984 run.py:483] Algo bellman_ford step 9522 current loss 0.058493, current_train_items 304736.
I0304 19:32:31.636038 22579586809984 run.py:483] Algo bellman_ford step 9523 current loss 0.139407, current_train_items 304768.
I0304 19:32:31.669273 22579586809984 run.py:483] Algo bellman_ford step 9524 current loss 0.096023, current_train_items 304800.
I0304 19:32:31.688599 22579586809984 run.py:483] Algo bellman_ford step 9525 current loss 0.006027, current_train_items 304832.
I0304 19:32:31.704906 22579586809984 run.py:483] Algo bellman_ford step 9526 current loss 0.022945, current_train_items 304864.
I0304 19:32:31.729620 22579586809984 run.py:483] Algo bellman_ford step 9527 current loss 0.051175, current_train_items 304896.
I0304 19:32:31.762301 22579586809984 run.py:483] Algo bellman_ford step 9528 current loss 0.041280, current_train_items 304928.
I0304 19:32:31.797372 22579586809984 run.py:483] Algo bellman_ford step 9529 current loss 0.045936, current_train_items 304960.
I0304 19:32:31.817233 22579586809984 run.py:483] Algo bellman_ford step 9530 current loss 0.007677, current_train_items 304992.
I0304 19:32:31.833485 22579586809984 run.py:483] Algo bellman_ford step 9531 current loss 0.083409, current_train_items 305024.
I0304 19:32:31.857325 22579586809984 run.py:483] Algo bellman_ford step 9532 current loss 0.037980, current_train_items 305056.
I0304 19:32:31.891181 22579586809984 run.py:483] Algo bellman_ford step 9533 current loss 0.077322, current_train_items 305088.
I0304 19:32:31.924594 22579586809984 run.py:483] Algo bellman_ford step 9534 current loss 0.052058, current_train_items 305120.
I0304 19:32:31.944748 22579586809984 run.py:483] Algo bellman_ford step 9535 current loss 0.012354, current_train_items 305152.
I0304 19:32:31.960979 22579586809984 run.py:483] Algo bellman_ford step 9536 current loss 0.032112, current_train_items 305184.
I0304 19:32:31.985384 22579586809984 run.py:483] Algo bellman_ford step 9537 current loss 0.029959, current_train_items 305216.
I0304 19:32:32.018378 22579586809984 run.py:483] Algo bellman_ford step 9538 current loss 0.059294, current_train_items 305248.
I0304 19:32:32.049963 22579586809984 run.py:483] Algo bellman_ford step 9539 current loss 0.039386, current_train_items 305280.
I0304 19:32:32.069989 22579586809984 run.py:483] Algo bellman_ford step 9540 current loss 0.003992, current_train_items 305312.
I0304 19:32:32.086112 22579586809984 run.py:483] Algo bellman_ford step 9541 current loss 0.039778, current_train_items 305344.
I0304 19:32:32.110023 22579586809984 run.py:483] Algo bellman_ford step 9542 current loss 0.019661, current_train_items 305376.
I0304 19:32:32.143566 22579586809984 run.py:483] Algo bellman_ford step 9543 current loss 0.045039, current_train_items 305408.
I0304 19:32:32.177743 22579586809984 run.py:483] Algo bellman_ford step 9544 current loss 0.053975, current_train_items 305440.
I0304 19:32:32.197599 22579586809984 run.py:483] Algo bellman_ford step 9545 current loss 0.003067, current_train_items 305472.
I0304 19:32:32.213376 22579586809984 run.py:483] Algo bellman_ford step 9546 current loss 0.024538, current_train_items 305504.
I0304 19:32:32.235533 22579586809984 run.py:483] Algo bellman_ford step 9547 current loss 0.016370, current_train_items 305536.
I0304 19:32:32.267383 22579586809984 run.py:483] Algo bellman_ford step 9548 current loss 0.043830, current_train_items 305568.
I0304 19:32:32.301758 22579586809984 run.py:483] Algo bellman_ford step 9549 current loss 0.042292, current_train_items 305600.
I0304 19:32:32.321428 22579586809984 run.py:483] Algo bellman_ford step 9550 current loss 0.003491, current_train_items 305632.
I0304 19:32:32.329380 22579586809984 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0304 19:32:32.329486 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:32:32.346374 22579586809984 run.py:483] Algo bellman_ford step 9551 current loss 0.025309, current_train_items 305664.
I0304 19:32:32.371257 22579586809984 run.py:483] Algo bellman_ford step 9552 current loss 0.062017, current_train_items 305696.
I0304 19:32:32.405102 22579586809984 run.py:483] Algo bellman_ford step 9553 current loss 0.054175, current_train_items 305728.
I0304 19:32:32.438077 22579586809984 run.py:483] Algo bellman_ford step 9554 current loss 0.044965, current_train_items 305760.
I0304 19:32:32.457869 22579586809984 run.py:483] Algo bellman_ford step 9555 current loss 0.002423, current_train_items 305792.
I0304 19:32:32.473440 22579586809984 run.py:483] Algo bellman_ford step 9556 current loss 0.017943, current_train_items 305824.
I0304 19:32:32.498166 22579586809984 run.py:483] Algo bellman_ford step 9557 current loss 0.029913, current_train_items 305856.
I0304 19:32:32.529772 22579586809984 run.py:483] Algo bellman_ford step 9558 current loss 0.041960, current_train_items 305888.
I0304 19:32:32.562756 22579586809984 run.py:483] Algo bellman_ford step 9559 current loss 0.068415, current_train_items 305920.
I0304 19:32:32.583064 22579586809984 run.py:483] Algo bellman_ford step 9560 current loss 0.005617, current_train_items 305952.
I0304 19:32:32.599487 22579586809984 run.py:483] Algo bellman_ford step 9561 current loss 0.006376, current_train_items 305984.
I0304 19:32:32.623145 22579586809984 run.py:483] Algo bellman_ford step 9562 current loss 0.018236, current_train_items 306016.
I0304 19:32:32.654734 22579586809984 run.py:483] Algo bellman_ford step 9563 current loss 0.017931, current_train_items 306048.
I0304 19:32:32.687616 22579586809984 run.py:483] Algo bellman_ford step 9564 current loss 0.069362, current_train_items 306080.
I0304 19:32:32.707139 22579586809984 run.py:483] Algo bellman_ford step 9565 current loss 0.043428, current_train_items 306112.
I0304 19:32:32.723897 22579586809984 run.py:483] Algo bellman_ford step 9566 current loss 0.012225, current_train_items 306144.
I0304 19:32:32.749383 22579586809984 run.py:483] Algo bellman_ford step 9567 current loss 0.052907, current_train_items 306176.
I0304 19:32:32.781832 22579586809984 run.py:483] Algo bellman_ford step 9568 current loss 0.076766, current_train_items 306208.
I0304 19:32:32.815228 22579586809984 run.py:483] Algo bellman_ford step 9569 current loss 0.077608, current_train_items 306240.
I0304 19:32:32.835351 22579586809984 run.py:483] Algo bellman_ford step 9570 current loss 0.002779, current_train_items 306272.
I0304 19:32:32.851792 22579586809984 run.py:483] Algo bellman_ford step 9571 current loss 0.006477, current_train_items 306304.
I0304 19:32:32.877233 22579586809984 run.py:483] Algo bellman_ford step 9572 current loss 0.098790, current_train_items 306336.
I0304 19:32:32.908469 22579586809984 run.py:483] Algo bellman_ford step 9573 current loss 0.069259, current_train_items 306368.
I0304 19:32:32.942858 22579586809984 run.py:483] Algo bellman_ford step 9574 current loss 0.094197, current_train_items 306400.
I0304 19:32:32.962780 22579586809984 run.py:483] Algo bellman_ford step 9575 current loss 0.003250, current_train_items 306432.
I0304 19:32:32.979939 22579586809984 run.py:483] Algo bellman_ford step 9576 current loss 0.040018, current_train_items 306464.
I0304 19:32:33.005057 22579586809984 run.py:483] Algo bellman_ford step 9577 current loss 0.041029, current_train_items 306496.
I0304 19:32:33.036526 22579586809984 run.py:483] Algo bellman_ford step 9578 current loss 0.018829, current_train_items 306528.
I0304 19:32:33.070264 22579586809984 run.py:483] Algo bellman_ford step 9579 current loss 0.065657, current_train_items 306560.
I0304 19:32:33.089875 22579586809984 run.py:483] Algo bellman_ford step 9580 current loss 0.003529, current_train_items 306592.
I0304 19:32:33.106268 22579586809984 run.py:483] Algo bellman_ford step 9581 current loss 0.060700, current_train_items 306624.
I0304 19:32:33.131860 22579586809984 run.py:483] Algo bellman_ford step 9582 current loss 0.018882, current_train_items 306656.
I0304 19:32:33.165100 22579586809984 run.py:483] Algo bellman_ford step 9583 current loss 0.031289, current_train_items 306688.
I0304 19:32:33.199786 22579586809984 run.py:483] Algo bellman_ford step 9584 current loss 0.059340, current_train_items 306720.
I0304 19:32:33.220026 22579586809984 run.py:483] Algo bellman_ford step 9585 current loss 0.025330, current_train_items 306752.
I0304 19:32:33.236770 22579586809984 run.py:483] Algo bellman_ford step 9586 current loss 0.012710, current_train_items 306784.
I0304 19:32:33.260959 22579586809984 run.py:483] Algo bellman_ford step 9587 current loss 0.029308, current_train_items 306816.
I0304 19:32:33.293354 22579586809984 run.py:483] Algo bellman_ford step 9588 current loss 0.033092, current_train_items 306848.
I0304 19:32:33.328269 22579586809984 run.py:483] Algo bellman_ford step 9589 current loss 0.092935, current_train_items 306880.
I0304 19:32:33.348618 22579586809984 run.py:483] Algo bellman_ford step 9590 current loss 0.002358, current_train_items 306912.
I0304 19:32:33.364979 22579586809984 run.py:483] Algo bellman_ford step 9591 current loss 0.011626, current_train_items 306944.
I0304 19:32:33.389175 22579586809984 run.py:483] Algo bellman_ford step 9592 current loss 0.016652, current_train_items 306976.
I0304 19:32:33.421396 22579586809984 run.py:483] Algo bellman_ford step 9593 current loss 0.040995, current_train_items 307008.
I0304 19:32:33.454955 22579586809984 run.py:483] Algo bellman_ford step 9594 current loss 0.034138, current_train_items 307040.
I0304 19:32:33.474609 22579586809984 run.py:483] Algo bellman_ford step 9595 current loss 0.002939, current_train_items 307072.
I0304 19:32:33.491227 22579586809984 run.py:483] Algo bellman_ford step 9596 current loss 0.018858, current_train_items 307104.
I0304 19:32:33.516202 22579586809984 run.py:483] Algo bellman_ford step 9597 current loss 0.027361, current_train_items 307136.
I0304 19:32:33.547015 22579586809984 run.py:483] Algo bellman_ford step 9598 current loss 0.036571, current_train_items 307168.
I0304 19:32:33.580585 22579586809984 run.py:483] Algo bellman_ford step 9599 current loss 0.031666, current_train_items 307200.
I0304 19:32:33.601009 22579586809984 run.py:483] Algo bellman_ford step 9600 current loss 0.002331, current_train_items 307232.
I0304 19:32:33.609061 22579586809984 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0304 19:32:33.609167 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0304 19:32:33.626529 22579586809984 run.py:483] Algo bellman_ford step 9601 current loss 0.009635, current_train_items 307264.
I0304 19:32:33.652170 22579586809984 run.py:483] Algo bellman_ford step 9602 current loss 0.046937, current_train_items 307296.
I0304 19:32:33.683871 22579586809984 run.py:483] Algo bellman_ford step 9603 current loss 0.030437, current_train_items 307328.
I0304 19:32:33.719236 22579586809984 run.py:483] Algo bellman_ford step 9604 current loss 0.041113, current_train_items 307360.
I0304 19:32:33.739048 22579586809984 run.py:483] Algo bellman_ford step 9605 current loss 0.002535, current_train_items 307392.
I0304 19:32:33.754931 22579586809984 run.py:483] Algo bellman_ford step 9606 current loss 0.007980, current_train_items 307424.
I0304 19:32:33.779079 22579586809984 run.py:483] Algo bellman_ford step 9607 current loss 0.036147, current_train_items 307456.
I0304 19:32:33.810757 22579586809984 run.py:483] Algo bellman_ford step 9608 current loss 0.038135, current_train_items 307488.
I0304 19:32:33.844485 22579586809984 run.py:483] Algo bellman_ford step 9609 current loss 0.042450, current_train_items 307520.
I0304 19:32:33.864132 22579586809984 run.py:483] Algo bellman_ford step 9610 current loss 0.029414, current_train_items 307552.
I0304 19:32:33.880572 22579586809984 run.py:483] Algo bellman_ford step 9611 current loss 0.023090, current_train_items 307584.
I0304 19:32:33.905272 22579586809984 run.py:483] Algo bellman_ford step 9612 current loss 0.055350, current_train_items 307616.
I0304 19:32:33.937247 22579586809984 run.py:483] Algo bellman_ford step 9613 current loss 0.044115, current_train_items 307648.
I0304 19:32:33.970981 22579586809984 run.py:483] Algo bellman_ford step 9614 current loss 0.040753, current_train_items 307680.
I0304 19:32:33.990587 22579586809984 run.py:483] Algo bellman_ford step 9615 current loss 0.003958, current_train_items 307712.
I0304 19:32:34.006863 22579586809984 run.py:483] Algo bellman_ford step 9616 current loss 0.013930, current_train_items 307744.
I0304 19:32:34.030935 22579586809984 run.py:483] Algo bellman_ford step 9617 current loss 0.059770, current_train_items 307776.
I0304 19:32:34.062595 22579586809984 run.py:483] Algo bellman_ford step 9618 current loss 0.023267, current_train_items 307808.
I0304 19:32:34.096583 22579586809984 run.py:483] Algo bellman_ford step 9619 current loss 0.035773, current_train_items 307840.
I0304 19:32:34.116208 22579586809984 run.py:483] Algo bellman_ford step 9620 current loss 0.003100, current_train_items 307872.
I0304 19:32:34.132236 22579586809984 run.py:483] Algo bellman_ford step 9621 current loss 0.007592, current_train_items 307904.
I0304 19:32:34.156966 22579586809984 run.py:483] Algo bellman_ford step 9622 current loss 0.041952, current_train_items 307936.
I0304 19:32:34.188947 22579586809984 run.py:483] Algo bellman_ford step 9623 current loss 0.051880, current_train_items 307968.
I0304 19:32:34.223177 22579586809984 run.py:483] Algo bellman_ford step 9624 current loss 0.069266, current_train_items 308000.
I0304 19:32:34.242755 22579586809984 run.py:483] Algo bellman_ford step 9625 current loss 0.003092, current_train_items 308032.
I0304 19:32:34.259076 22579586809984 run.py:483] Algo bellman_ford step 9626 current loss 0.005610, current_train_items 308064.
I0304 19:32:34.283212 22579586809984 run.py:483] Algo bellman_ford step 9627 current loss 0.047688, current_train_items 308096.
I0304 19:32:34.314829 22579586809984 run.py:483] Algo bellman_ford step 9628 current loss 0.042155, current_train_items 308128.
I0304 19:32:34.349382 22579586809984 run.py:483] Algo bellman_ford step 9629 current loss 0.069146, current_train_items 308160.
I0304 19:32:34.369090 22579586809984 run.py:483] Algo bellman_ford step 9630 current loss 0.025346, current_train_items 308192.
I0304 19:32:34.385685 22579586809984 run.py:483] Algo bellman_ford step 9631 current loss 0.007454, current_train_items 308224.
I0304 19:32:34.411050 22579586809984 run.py:483] Algo bellman_ford step 9632 current loss 0.185464, current_train_items 308256.
I0304 19:32:34.442338 22579586809984 run.py:483] Algo bellman_ford step 9633 current loss 0.059817, current_train_items 308288.
I0304 19:32:34.476854 22579586809984 run.py:483] Algo bellman_ford step 9634 current loss 0.165193, current_train_items 308320.
I0304 19:32:34.496278 22579586809984 run.py:483] Algo bellman_ford step 9635 current loss 0.003109, current_train_items 308352.
I0304 19:32:34.512507 22579586809984 run.py:483] Algo bellman_ford step 9636 current loss 0.011760, current_train_items 308384.
I0304 19:32:34.536939 22579586809984 run.py:483] Algo bellman_ford step 9637 current loss 0.028970, current_train_items 308416.
I0304 19:32:34.569375 22579586809984 run.py:483] Algo bellman_ford step 9638 current loss 0.034028, current_train_items 308448.
I0304 19:32:34.602850 22579586809984 run.py:483] Algo bellman_ford step 9639 current loss 0.112008, current_train_items 308480.
I0304 19:32:34.622544 22579586809984 run.py:483] Algo bellman_ford step 9640 current loss 0.003756, current_train_items 308512.
I0304 19:32:34.638430 22579586809984 run.py:483] Algo bellman_ford step 9641 current loss 0.010104, current_train_items 308544.
I0304 19:32:34.663173 22579586809984 run.py:483] Algo bellman_ford step 9642 current loss 0.120847, current_train_items 308576.
I0304 19:32:34.696617 22579586809984 run.py:483] Algo bellman_ford step 9643 current loss 0.065716, current_train_items 308608.
I0304 19:32:34.728402 22579586809984 run.py:483] Algo bellman_ford step 9644 current loss 0.045383, current_train_items 308640.
I0304 19:32:34.747733 22579586809984 run.py:483] Algo bellman_ford step 9645 current loss 0.009593, current_train_items 308672.
I0304 19:32:34.764137 22579586809984 run.py:483] Algo bellman_ford step 9646 current loss 0.029585, current_train_items 308704.
I0304 19:32:34.787769 22579586809984 run.py:483] Algo bellman_ford step 9647 current loss 0.027567, current_train_items 308736.
I0304 19:32:34.820496 22579586809984 run.py:483] Algo bellman_ford step 9648 current loss 0.091362, current_train_items 308768.
I0304 19:32:34.854668 22579586809984 run.py:483] Algo bellman_ford step 9649 current loss 0.065019, current_train_items 308800.
I0304 19:32:34.874084 22579586809984 run.py:483] Algo bellman_ford step 9650 current loss 0.003927, current_train_items 308832.
I0304 19:32:34.882465 22579586809984 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0304 19:32:34.882595 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:34.899643 22579586809984 run.py:483] Algo bellman_ford step 9651 current loss 0.019141, current_train_items 308864.
I0304 19:32:34.926254 22579586809984 run.py:483] Algo bellman_ford step 9652 current loss 0.057440, current_train_items 308896.
I0304 19:32:34.959124 22579586809984 run.py:483] Algo bellman_ford step 9653 current loss 0.107933, current_train_items 308928.
I0304 19:32:34.995554 22579586809984 run.py:483] Algo bellman_ford step 9654 current loss 0.054377, current_train_items 308960.
I0304 19:32:35.015910 22579586809984 run.py:483] Algo bellman_ford step 9655 current loss 0.006060, current_train_items 308992.
I0304 19:32:35.032169 22579586809984 run.py:483] Algo bellman_ford step 9656 current loss 0.019385, current_train_items 309024.
I0304 19:32:35.056923 22579586809984 run.py:483] Algo bellman_ford step 9657 current loss 0.028616, current_train_items 309056.
I0304 19:32:35.090063 22579586809984 run.py:483] Algo bellman_ford step 9658 current loss 0.051825, current_train_items 309088.
I0304 19:32:35.122956 22579586809984 run.py:483] Algo bellman_ford step 9659 current loss 0.045549, current_train_items 309120.
I0304 19:32:35.143048 22579586809984 run.py:483] Algo bellman_ford step 9660 current loss 0.005664, current_train_items 309152.
I0304 19:32:35.159837 22579586809984 run.py:483] Algo bellman_ford step 9661 current loss 0.007148, current_train_items 309184.
I0304 19:32:35.184245 22579586809984 run.py:483] Algo bellman_ford step 9662 current loss 0.064342, current_train_items 309216.
I0304 19:32:35.216383 22579586809984 run.py:483] Algo bellman_ford step 9663 current loss 0.032850, current_train_items 309248.
I0304 19:32:35.250639 22579586809984 run.py:483] Algo bellman_ford step 9664 current loss 0.076271, current_train_items 309280.
I0304 19:32:35.270333 22579586809984 run.py:483] Algo bellman_ford step 9665 current loss 0.006064, current_train_items 309312.
I0304 19:32:35.286744 22579586809984 run.py:483] Algo bellman_ford step 9666 current loss 0.010807, current_train_items 309344.
I0304 19:32:35.310709 22579586809984 run.py:483] Algo bellman_ford step 9667 current loss 0.040154, current_train_items 309376.
I0304 19:32:35.342603 22579586809984 run.py:483] Algo bellman_ford step 9668 current loss 0.041726, current_train_items 309408.
I0304 19:32:35.376331 22579586809984 run.py:483] Algo bellman_ford step 9669 current loss 0.055785, current_train_items 309440.
I0304 19:32:35.396747 22579586809984 run.py:483] Algo bellman_ford step 9670 current loss 0.006225, current_train_items 309472.
I0304 19:32:35.413105 22579586809984 run.py:483] Algo bellman_ford step 9671 current loss 0.014713, current_train_items 309504.
I0304 19:32:35.437533 22579586809984 run.py:483] Algo bellman_ford step 9672 current loss 0.057340, current_train_items 309536.
I0304 19:32:35.469246 22579586809984 run.py:483] Algo bellman_ford step 9673 current loss 0.038129, current_train_items 309568.
I0304 19:32:35.503282 22579586809984 run.py:483] Algo bellman_ford step 9674 current loss 0.048196, current_train_items 309600.
I0304 19:32:35.523496 22579586809984 run.py:483] Algo bellman_ford step 9675 current loss 0.002725, current_train_items 309632.
I0304 19:32:35.540489 22579586809984 run.py:483] Algo bellman_ford step 9676 current loss 0.016330, current_train_items 309664.
I0304 19:32:35.565159 22579586809984 run.py:483] Algo bellman_ford step 9677 current loss 0.048834, current_train_items 309696.
I0304 19:32:35.595900 22579586809984 run.py:483] Algo bellman_ford step 9678 current loss 0.057486, current_train_items 309728.
I0304 19:32:35.630545 22579586809984 run.py:483] Algo bellman_ford step 9679 current loss 0.053493, current_train_items 309760.
I0304 19:32:35.650635 22579586809984 run.py:483] Algo bellman_ford step 9680 current loss 0.004988, current_train_items 309792.
I0304 19:32:35.667215 22579586809984 run.py:483] Algo bellman_ford step 9681 current loss 0.055284, current_train_items 309824.
I0304 19:32:35.691693 22579586809984 run.py:483] Algo bellman_ford step 9682 current loss 0.167834, current_train_items 309856.
I0304 19:32:35.723537 22579586809984 run.py:483] Algo bellman_ford step 9683 current loss 0.032666, current_train_items 309888.
I0304 19:32:35.756368 22579586809984 run.py:483] Algo bellman_ford step 9684 current loss 0.057971, current_train_items 309920.
I0304 19:32:35.776362 22579586809984 run.py:483] Algo bellman_ford step 9685 current loss 0.003751, current_train_items 309952.
I0304 19:32:35.792688 22579586809984 run.py:483] Algo bellman_ford step 9686 current loss 0.015179, current_train_items 309984.
I0304 19:32:35.817204 22579586809984 run.py:483] Algo bellman_ford step 9687 current loss 0.037235, current_train_items 310016.
I0304 19:32:35.849016 22579586809984 run.py:483] Algo bellman_ford step 9688 current loss 0.083700, current_train_items 310048.
I0304 19:32:35.883326 22579586809984 run.py:483] Algo bellman_ford step 9689 current loss 0.055166, current_train_items 310080.
I0304 19:32:35.903666 22579586809984 run.py:483] Algo bellman_ford step 9690 current loss 0.006621, current_train_items 310112.
I0304 19:32:35.919647 22579586809984 run.py:483] Algo bellman_ford step 9691 current loss 0.005862, current_train_items 310144.
I0304 19:32:35.943801 22579586809984 run.py:483] Algo bellman_ford step 9692 current loss 0.016846, current_train_items 310176.
I0304 19:32:35.975095 22579586809984 run.py:483] Algo bellman_ford step 9693 current loss 0.048015, current_train_items 310208.
I0304 19:32:36.008893 22579586809984 run.py:483] Algo bellman_ford step 9694 current loss 0.039648, current_train_items 310240.
I0304 19:32:36.028390 22579586809984 run.py:483] Algo bellman_ford step 9695 current loss 0.002599, current_train_items 310272.
I0304 19:32:36.044676 22579586809984 run.py:483] Algo bellman_ford step 9696 current loss 0.028715, current_train_items 310304.
I0304 19:32:36.068245 22579586809984 run.py:483] Algo bellman_ford step 9697 current loss 0.036260, current_train_items 310336.
I0304 19:32:36.102991 22579586809984 run.py:483] Algo bellman_ford step 9698 current loss 0.055680, current_train_items 310368.
I0304 19:32:36.138371 22579586809984 run.py:483] Algo bellman_ford step 9699 current loss 0.052374, current_train_items 310400.
I0304 19:32:36.158429 22579586809984 run.py:483] Algo bellman_ford step 9700 current loss 0.010034, current_train_items 310432.
I0304 19:32:36.166211 22579586809984 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0304 19:32:36.166316 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:36.183246 22579586809984 run.py:483] Algo bellman_ford step 9701 current loss 0.024011, current_train_items 310464.
I0304 19:32:36.208873 22579586809984 run.py:483] Algo bellman_ford step 9702 current loss 0.029495, current_train_items 310496.
I0304 19:32:36.242043 22579586809984 run.py:483] Algo bellman_ford step 9703 current loss 0.105053, current_train_items 310528.
I0304 19:32:36.275752 22579586809984 run.py:483] Algo bellman_ford step 9704 current loss 0.066010, current_train_items 310560.
I0304 19:32:36.296118 22579586809984 run.py:483] Algo bellman_ford step 9705 current loss 0.003968, current_train_items 310592.
I0304 19:32:36.312233 22579586809984 run.py:483] Algo bellman_ford step 9706 current loss 0.019877, current_train_items 310624.
I0304 19:32:36.337549 22579586809984 run.py:483] Algo bellman_ford step 9707 current loss 0.085708, current_train_items 310656.
I0304 19:32:36.370330 22579586809984 run.py:483] Algo bellman_ford step 9708 current loss 0.094462, current_train_items 310688.
I0304 19:32:36.402321 22579586809984 run.py:483] Algo bellman_ford step 9709 current loss 0.096138, current_train_items 310720.
I0304 19:32:36.422330 22579586809984 run.py:483] Algo bellman_ford step 9710 current loss 0.002962, current_train_items 310752.
I0304 19:32:36.438439 22579586809984 run.py:483] Algo bellman_ford step 9711 current loss 0.012576, current_train_items 310784.
I0304 19:32:36.462896 22579586809984 run.py:483] Algo bellman_ford step 9712 current loss 0.037376, current_train_items 310816.
I0304 19:32:36.495346 22579586809984 run.py:483] Algo bellman_ford step 9713 current loss 0.025510, current_train_items 310848.
I0304 19:32:36.529974 22579586809984 run.py:483] Algo bellman_ford step 9714 current loss 0.023494, current_train_items 310880.
I0304 19:32:36.549594 22579586809984 run.py:483] Algo bellman_ford step 9715 current loss 0.006251, current_train_items 310912.
I0304 19:32:36.566273 22579586809984 run.py:483] Algo bellman_ford step 9716 current loss 0.021880, current_train_items 310944.
I0304 19:32:36.589580 22579586809984 run.py:483] Algo bellman_ford step 9717 current loss 0.043374, current_train_items 310976.
I0304 19:32:36.622199 22579586809984 run.py:483] Algo bellman_ford step 9718 current loss 0.057325, current_train_items 311008.
I0304 19:32:36.653351 22579586809984 run.py:483] Algo bellman_ford step 9719 current loss 0.027549, current_train_items 311040.
I0304 19:32:36.673350 22579586809984 run.py:483] Algo bellman_ford step 9720 current loss 0.003392, current_train_items 311072.
I0304 19:32:36.689190 22579586809984 run.py:483] Algo bellman_ford step 9721 current loss 0.006576, current_train_items 311104.
I0304 19:32:36.713489 22579586809984 run.py:483] Algo bellman_ford step 9722 current loss 0.037430, current_train_items 311136.
I0304 19:32:36.746129 22579586809984 run.py:483] Algo bellman_ford step 9723 current loss 0.050756, current_train_items 311168.
I0304 19:32:36.780242 22579586809984 run.py:483] Algo bellman_ford step 9724 current loss 0.054593, current_train_items 311200.
I0304 19:32:36.800255 22579586809984 run.py:483] Algo bellman_ford step 9725 current loss 0.003837, current_train_items 311232.
I0304 19:32:36.816357 22579586809984 run.py:483] Algo bellman_ford step 9726 current loss 0.018462, current_train_items 311264.
I0304 19:32:36.840231 22579586809984 run.py:483] Algo bellman_ford step 9727 current loss 0.020949, current_train_items 311296.
I0304 19:32:36.872913 22579586809984 run.py:483] Algo bellman_ford step 9728 current loss 0.053982, current_train_items 311328.
I0304 19:32:36.907586 22579586809984 run.py:483] Algo bellman_ford step 9729 current loss 0.049813, current_train_items 311360.
I0304 19:32:36.927509 22579586809984 run.py:483] Algo bellman_ford step 9730 current loss 0.006651, current_train_items 311392.
I0304 19:32:36.944039 22579586809984 run.py:483] Algo bellman_ford step 9731 current loss 0.016375, current_train_items 311424.
I0304 19:32:36.968225 22579586809984 run.py:483] Algo bellman_ford step 9732 current loss 0.035864, current_train_items 311456.
I0304 19:32:37.000155 22579586809984 run.py:483] Algo bellman_ford step 9733 current loss 0.074924, current_train_items 311488.
I0304 19:32:37.033193 22579586809984 run.py:483] Algo bellman_ford step 9734 current loss 0.069148, current_train_items 311520.
I0304 19:32:37.053140 22579586809984 run.py:483] Algo bellman_ford step 9735 current loss 0.004635, current_train_items 311552.
I0304 19:32:37.069489 22579586809984 run.py:483] Algo bellman_ford step 9736 current loss 0.019302, current_train_items 311584.
I0304 19:32:37.094737 22579586809984 run.py:483] Algo bellman_ford step 9737 current loss 0.031490, current_train_items 311616.
I0304 19:32:37.128273 22579586809984 run.py:483] Algo bellman_ford step 9738 current loss 0.068334, current_train_items 311648.
I0304 19:32:37.162924 22579586809984 run.py:483] Algo bellman_ford step 9739 current loss 0.067798, current_train_items 311680.
I0304 19:32:37.182667 22579586809984 run.py:483] Algo bellman_ford step 9740 current loss 0.002064, current_train_items 311712.
I0304 19:32:37.199670 22579586809984 run.py:483] Algo bellman_ford step 9741 current loss 0.029567, current_train_items 311744.
I0304 19:32:37.225546 22579586809984 run.py:483] Algo bellman_ford step 9742 current loss 0.050389, current_train_items 311776.
I0304 19:32:37.258499 22579586809984 run.py:483] Algo bellman_ford step 9743 current loss 0.034154, current_train_items 311808.
I0304 19:32:37.292532 22579586809984 run.py:483] Algo bellman_ford step 9744 current loss 0.043299, current_train_items 311840.
I0304 19:32:37.312607 22579586809984 run.py:483] Algo bellman_ford step 9745 current loss 0.008076, current_train_items 311872.
I0304 19:32:37.329825 22579586809984 run.py:483] Algo bellman_ford step 9746 current loss 0.027281, current_train_items 311904.
I0304 19:32:37.353631 22579586809984 run.py:483] Algo bellman_ford step 9747 current loss 0.036995, current_train_items 311936.
I0304 19:32:37.384049 22579586809984 run.py:483] Algo bellman_ford step 9748 current loss 0.026361, current_train_items 311968.
I0304 19:32:37.416911 22579586809984 run.py:483] Algo bellman_ford step 9749 current loss 0.045274, current_train_items 312000.
I0304 19:32:37.437109 22579586809984 run.py:483] Algo bellman_ford step 9750 current loss 0.001899, current_train_items 312032.
I0304 19:32:37.445244 22579586809984 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0304 19:32:37.445350 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:32:37.462021 22579586809984 run.py:483] Algo bellman_ford step 9751 current loss 0.013210, current_train_items 312064.
I0304 19:32:37.488064 22579586809984 run.py:483] Algo bellman_ford step 9752 current loss 0.041962, current_train_items 312096.
I0304 19:32:37.519607 22579586809984 run.py:483] Algo bellman_ford step 9753 current loss 0.059836, current_train_items 312128.
I0304 19:32:37.552818 22579586809984 run.py:483] Algo bellman_ford step 9754 current loss 0.083688, current_train_items 312160.
I0304 19:32:37.572366 22579586809984 run.py:483] Algo bellman_ford step 9755 current loss 0.012489, current_train_items 312192.
I0304 19:32:37.589279 22579586809984 run.py:483] Algo bellman_ford step 9756 current loss 0.021407, current_train_items 312224.
I0304 19:32:37.612577 22579586809984 run.py:483] Algo bellman_ford step 9757 current loss 0.042195, current_train_items 312256.
I0304 19:32:37.645145 22579586809984 run.py:483] Algo bellman_ford step 9758 current loss 0.087735, current_train_items 312288.
I0304 19:32:37.678569 22579586809984 run.py:483] Algo bellman_ford step 9759 current loss 0.098332, current_train_items 312320.
I0304 19:32:37.698879 22579586809984 run.py:483] Algo bellman_ford step 9760 current loss 0.009560, current_train_items 312352.
I0304 19:32:37.715662 22579586809984 run.py:483] Algo bellman_ford step 9761 current loss 0.024625, current_train_items 312384.
I0304 19:32:37.740092 22579586809984 run.py:483] Algo bellman_ford step 9762 current loss 0.022788, current_train_items 312416.
I0304 19:32:37.772608 22579586809984 run.py:483] Algo bellman_ford step 9763 current loss 0.074768, current_train_items 312448.
I0304 19:32:37.803886 22579586809984 run.py:483] Algo bellman_ford step 9764 current loss 0.031042, current_train_items 312480.
I0304 19:32:37.823322 22579586809984 run.py:483] Algo bellman_ford step 9765 current loss 0.010293, current_train_items 312512.
I0304 19:32:37.839336 22579586809984 run.py:483] Algo bellman_ford step 9766 current loss 0.019027, current_train_items 312544.
I0304 19:32:37.864146 22579586809984 run.py:483] Algo bellman_ford step 9767 current loss 0.086049, current_train_items 312576.
I0304 19:32:37.895676 22579586809984 run.py:483] Algo bellman_ford step 9768 current loss 0.038424, current_train_items 312608.
I0304 19:32:37.929907 22579586809984 run.py:483] Algo bellman_ford step 9769 current loss 0.065351, current_train_items 312640.
I0304 19:32:37.949966 22579586809984 run.py:483] Algo bellman_ford step 9770 current loss 0.003253, current_train_items 312672.
I0304 19:32:37.966019 22579586809984 run.py:483] Algo bellman_ford step 9771 current loss 0.030128, current_train_items 312704.
I0304 19:32:37.989417 22579586809984 run.py:483] Algo bellman_ford step 9772 current loss 0.041775, current_train_items 312736.
I0304 19:32:38.022323 22579586809984 run.py:483] Algo bellman_ford step 9773 current loss 0.041072, current_train_items 312768.
I0304 19:32:38.056016 22579586809984 run.py:483] Algo bellman_ford step 9774 current loss 0.065380, current_train_items 312800.
I0304 19:32:38.075889 22579586809984 run.py:483] Algo bellman_ford step 9775 current loss 0.002823, current_train_items 312832.
I0304 19:32:38.092052 22579586809984 run.py:483] Algo bellman_ford step 9776 current loss 0.005343, current_train_items 312864.
I0304 19:32:38.116878 22579586809984 run.py:483] Algo bellman_ford step 9777 current loss 0.030922, current_train_items 312896.
I0304 19:32:38.149332 22579586809984 run.py:483] Algo bellman_ford step 9778 current loss 0.026241, current_train_items 312928.
I0304 19:32:38.183153 22579586809984 run.py:483] Algo bellman_ford step 9779 current loss 0.031952, current_train_items 312960.
I0304 19:32:38.202656 22579586809984 run.py:483] Algo bellman_ford step 9780 current loss 0.003737, current_train_items 312992.
I0304 19:32:38.219148 22579586809984 run.py:483] Algo bellman_ford step 9781 current loss 0.006188, current_train_items 313024.
I0304 19:32:38.242836 22579586809984 run.py:483] Algo bellman_ford step 9782 current loss 0.054456, current_train_items 313056.
I0304 19:32:38.276595 22579586809984 run.py:483] Algo bellman_ford step 9783 current loss 0.058659, current_train_items 313088.
I0304 19:32:38.308830 22579586809984 run.py:483] Algo bellman_ford step 9784 current loss 0.038726, current_train_items 313120.
I0304 19:32:38.328850 22579586809984 run.py:483] Algo bellman_ford step 9785 current loss 0.002226, current_train_items 313152.
I0304 19:32:38.345535 22579586809984 run.py:483] Algo bellman_ford step 9786 current loss 0.019540, current_train_items 313184.
I0304 19:32:38.369329 22579586809984 run.py:483] Algo bellman_ford step 9787 current loss 0.031032, current_train_items 313216.
I0304 19:32:38.400583 22579586809984 run.py:483] Algo bellman_ford step 9788 current loss 0.030937, current_train_items 313248.
I0304 19:32:38.434426 22579586809984 run.py:483] Algo bellman_ford step 9789 current loss 0.040511, current_train_items 313280.
I0304 19:32:38.454076 22579586809984 run.py:483] Algo bellman_ford step 9790 current loss 0.013544, current_train_items 313312.
I0304 19:32:38.470278 22579586809984 run.py:483] Algo bellman_ford step 9791 current loss 0.008028, current_train_items 313344.
I0304 19:32:38.493910 22579586809984 run.py:483] Algo bellman_ford step 9792 current loss 0.066213, current_train_items 313376.
I0304 19:32:38.525756 22579586809984 run.py:483] Algo bellman_ford step 9793 current loss 0.065302, current_train_items 313408.
I0304 19:32:38.557343 22579586809984 run.py:483] Algo bellman_ford step 9794 current loss 0.024030, current_train_items 313440.
I0304 19:32:38.577071 22579586809984 run.py:483] Algo bellman_ford step 9795 current loss 0.003931, current_train_items 313472.
I0304 19:32:38.593708 22579586809984 run.py:483] Algo bellman_ford step 9796 current loss 0.006476, current_train_items 313504.
I0304 19:32:38.617748 22579586809984 run.py:483] Algo bellman_ford step 9797 current loss 0.031599, current_train_items 313536.
I0304 19:32:38.649168 22579586809984 run.py:483] Algo bellman_ford step 9798 current loss 0.020699, current_train_items 313568.
I0304 19:32:38.681843 22579586809984 run.py:483] Algo bellman_ford step 9799 current loss 0.068595, current_train_items 313600.
I0304 19:32:38.701725 22579586809984 run.py:483] Algo bellman_ford step 9800 current loss 0.019843, current_train_items 313632.
I0304 19:32:38.709358 22579586809984 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0304 19:32:38.709462 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:32:38.726098 22579586809984 run.py:483] Algo bellman_ford step 9801 current loss 0.022195, current_train_items 313664.
I0304 19:32:38.750947 22579586809984 run.py:483] Algo bellman_ford step 9802 current loss 0.032428, current_train_items 313696.
I0304 19:32:38.784175 22579586809984 run.py:483] Algo bellman_ford step 9803 current loss 0.045177, current_train_items 313728.
I0304 19:32:38.817126 22579586809984 run.py:483] Algo bellman_ford step 9804 current loss 0.034012, current_train_items 313760.
I0304 19:32:38.837068 22579586809984 run.py:483] Algo bellman_ford step 9805 current loss 0.002592, current_train_items 313792.
I0304 19:32:38.853311 22579586809984 run.py:483] Algo bellman_ford step 9806 current loss 0.009408, current_train_items 313824.
I0304 19:32:38.876909 22579586809984 run.py:483] Algo bellman_ford step 9807 current loss 0.019162, current_train_items 313856.
I0304 19:32:38.909016 22579586809984 run.py:483] Algo bellman_ford step 9808 current loss 0.032804, current_train_items 313888.
I0304 19:32:38.944757 22579586809984 run.py:483] Algo bellman_ford step 9809 current loss 0.040619, current_train_items 313920.
I0304 19:32:38.964586 22579586809984 run.py:483] Algo bellman_ford step 9810 current loss 0.003186, current_train_items 313952.
I0304 19:32:38.980593 22579586809984 run.py:483] Algo bellman_ford step 9811 current loss 0.011517, current_train_items 313984.
I0304 19:32:39.005816 22579586809984 run.py:483] Algo bellman_ford step 9812 current loss 0.063109, current_train_items 314016.
I0304 19:32:39.035768 22579586809984 run.py:483] Algo bellman_ford step 9813 current loss 0.029982, current_train_items 314048.
I0304 19:32:39.069655 22579586809984 run.py:483] Algo bellman_ford step 9814 current loss 0.046211, current_train_items 314080.
I0304 19:32:39.089049 22579586809984 run.py:483] Algo bellman_ford step 9815 current loss 0.002431, current_train_items 314112.
I0304 19:32:39.105643 22579586809984 run.py:483] Algo bellman_ford step 9816 current loss 0.025019, current_train_items 314144.
I0304 19:32:39.129407 22579586809984 run.py:483] Algo bellman_ford step 9817 current loss 0.026245, current_train_items 314176.
I0304 19:32:39.162474 22579586809984 run.py:483] Algo bellman_ford step 9818 current loss 0.051381, current_train_items 314208.
I0304 19:32:39.196238 22579586809984 run.py:483] Algo bellman_ford step 9819 current loss 0.047921, current_train_items 314240.
I0304 19:32:39.215786 22579586809984 run.py:483] Algo bellman_ford step 9820 current loss 0.002916, current_train_items 314272.
I0304 19:32:39.231890 22579586809984 run.py:483] Algo bellman_ford step 9821 current loss 0.038466, current_train_items 314304.
I0304 19:32:39.256885 22579586809984 run.py:483] Algo bellman_ford step 9822 current loss 0.048096, current_train_items 314336.
I0304 19:32:39.288365 22579586809984 run.py:483] Algo bellman_ford step 9823 current loss 0.036027, current_train_items 314368.
I0304 19:32:39.321609 22579586809984 run.py:483] Algo bellman_ford step 9824 current loss 0.043653, current_train_items 314400.
I0304 19:32:39.341276 22579586809984 run.py:483] Algo bellman_ford step 9825 current loss 0.004278, current_train_items 314432.
I0304 19:32:39.357284 22579586809984 run.py:483] Algo bellman_ford step 9826 current loss 0.015000, current_train_items 314464.
I0304 19:32:39.381330 22579586809984 run.py:483] Algo bellman_ford step 9827 current loss 0.046513, current_train_items 314496.
I0304 19:32:39.412311 22579586809984 run.py:483] Algo bellman_ford step 9828 current loss 0.049858, current_train_items 314528.
I0304 19:32:39.447064 22579586809984 run.py:483] Algo bellman_ford step 9829 current loss 0.072885, current_train_items 314560.
I0304 19:32:39.466414 22579586809984 run.py:483] Algo bellman_ford step 9830 current loss 0.003339, current_train_items 314592.
I0304 19:32:39.483182 22579586809984 run.py:483] Algo bellman_ford step 9831 current loss 0.020404, current_train_items 314624.
I0304 19:32:39.507233 22579586809984 run.py:483] Algo bellman_ford step 9832 current loss 0.072434, current_train_items 314656.
I0304 19:32:39.540018 22579586809984 run.py:483] Algo bellman_ford step 9833 current loss 0.089253, current_train_items 314688.
I0304 19:32:39.572212 22579586809984 run.py:483] Algo bellman_ford step 9834 current loss 0.079055, current_train_items 314720.
I0304 19:32:39.591891 22579586809984 run.py:483] Algo bellman_ford step 9835 current loss 0.020072, current_train_items 314752.
I0304 19:32:39.608118 22579586809984 run.py:483] Algo bellman_ford step 9836 current loss 0.010140, current_train_items 314784.
I0304 19:32:39.632268 22579586809984 run.py:483] Algo bellman_ford step 9837 current loss 0.029904, current_train_items 314816.
I0304 19:32:39.665183 22579586809984 run.py:483] Algo bellman_ford step 9838 current loss 0.024497, current_train_items 314848.
I0304 19:32:39.700784 22579586809984 run.py:483] Algo bellman_ford step 9839 current loss 0.056536, current_train_items 314880.
I0304 19:32:39.720353 22579586809984 run.py:483] Algo bellman_ford step 9840 current loss 0.045346, current_train_items 314912.
I0304 19:32:39.736632 22579586809984 run.py:483] Algo bellman_ford step 9841 current loss 0.078883, current_train_items 314944.
I0304 19:32:39.761077 22579586809984 run.py:483] Algo bellman_ford step 9842 current loss 0.073669, current_train_items 314976.
I0304 19:32:39.792487 22579586809984 run.py:483] Algo bellman_ford step 9843 current loss 0.026950, current_train_items 315008.
I0304 19:32:39.824453 22579586809984 run.py:483] Algo bellman_ford step 9844 current loss 0.042950, current_train_items 315040.
I0304 19:32:39.844452 22579586809984 run.py:483] Algo bellman_ford step 9845 current loss 0.004048, current_train_items 315072.
I0304 19:32:39.860978 22579586809984 run.py:483] Algo bellman_ford step 9846 current loss 0.009017, current_train_items 315104.
I0304 19:32:39.885897 22579586809984 run.py:483] Algo bellman_ford step 9847 current loss 0.020504, current_train_items 315136.
I0304 19:32:39.917825 22579586809984 run.py:483] Algo bellman_ford step 9848 current loss 0.049914, current_train_items 315168.
I0304 19:32:39.952020 22579586809984 run.py:483] Algo bellman_ford step 9849 current loss 0.057434, current_train_items 315200.
I0304 19:32:39.971987 22579586809984 run.py:483] Algo bellman_ford step 9850 current loss 0.003467, current_train_items 315232.
I0304 19:32:39.980032 22579586809984 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0304 19:32:39.980139 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:32:39.997750 22579586809984 run.py:483] Algo bellman_ford step 9851 current loss 0.029412, current_train_items 315264.
I0304 19:32:40.023742 22579586809984 run.py:483] Algo bellman_ford step 9852 current loss 0.026238, current_train_items 315296.
I0304 19:32:40.055420 22579586809984 run.py:483] Algo bellman_ford step 9853 current loss 0.044011, current_train_items 315328.
I0304 19:32:40.089739 22579586809984 run.py:483] Algo bellman_ford step 9854 current loss 0.039346, current_train_items 315360.
I0304 19:32:40.109408 22579586809984 run.py:483] Algo bellman_ford step 9855 current loss 0.001688, current_train_items 315392.
I0304 19:32:40.125525 22579586809984 run.py:483] Algo bellman_ford step 9856 current loss 0.016950, current_train_items 315424.
I0304 19:32:40.150325 22579586809984 run.py:483] Algo bellman_ford step 9857 current loss 0.030696, current_train_items 315456.
I0304 19:32:40.182009 22579586809984 run.py:483] Algo bellman_ford step 9858 current loss 0.041943, current_train_items 315488.
I0304 19:32:40.215430 22579586809984 run.py:483] Algo bellman_ford step 9859 current loss 0.050148, current_train_items 315520.
I0304 19:32:40.235955 22579586809984 run.py:483] Algo bellman_ford step 9860 current loss 0.003067, current_train_items 315552.
I0304 19:32:40.252238 22579586809984 run.py:483] Algo bellman_ford step 9861 current loss 0.019155, current_train_items 315584.
I0304 19:32:40.276767 22579586809984 run.py:483] Algo bellman_ford step 9862 current loss 0.029001, current_train_items 315616.
I0304 19:32:40.308716 22579586809984 run.py:483] Algo bellman_ford step 9863 current loss 0.084078, current_train_items 315648.
I0304 19:32:40.342399 22579586809984 run.py:483] Algo bellman_ford step 9864 current loss 0.037831, current_train_items 315680.
I0304 19:32:40.362148 22579586809984 run.py:483] Algo bellman_ford step 9865 current loss 0.002061, current_train_items 315712.
I0304 19:32:40.378310 22579586809984 run.py:483] Algo bellman_ford step 9866 current loss 0.020471, current_train_items 315744.
I0304 19:32:40.402526 22579586809984 run.py:483] Algo bellman_ford step 9867 current loss 0.055248, current_train_items 315776.
I0304 19:32:40.435171 22579586809984 run.py:483] Algo bellman_ford step 9868 current loss 0.068745, current_train_items 315808.
I0304 19:32:40.468871 22579586809984 run.py:483] Algo bellman_ford step 9869 current loss 0.036537, current_train_items 315840.
I0304 19:32:40.489031 22579586809984 run.py:483] Algo bellman_ford step 9870 current loss 0.004407, current_train_items 315872.
I0304 19:32:40.505909 22579586809984 run.py:483] Algo bellman_ford step 9871 current loss 0.025626, current_train_items 315904.
I0304 19:32:40.530452 22579586809984 run.py:483] Algo bellman_ford step 9872 current loss 0.073361, current_train_items 315936.
I0304 19:32:40.562819 22579586809984 run.py:483] Algo bellman_ford step 9873 current loss 0.031755, current_train_items 315968.
I0304 19:32:40.595510 22579586809984 run.py:483] Algo bellman_ford step 9874 current loss 0.048356, current_train_items 316000.
I0304 19:32:40.615293 22579586809984 run.py:483] Algo bellman_ford step 9875 current loss 0.001656, current_train_items 316032.
I0304 19:32:40.631770 22579586809984 run.py:483] Algo bellman_ford step 9876 current loss 0.021337, current_train_items 316064.
I0304 19:32:40.654362 22579586809984 run.py:483] Algo bellman_ford step 9877 current loss 0.032391, current_train_items 316096.
I0304 19:32:40.686013 22579586809984 run.py:483] Algo bellman_ford step 9878 current loss 0.036955, current_train_items 316128.
I0304 19:32:40.719233 22579586809984 run.py:483] Algo bellman_ford step 9879 current loss 0.037772, current_train_items 316160.
I0304 19:32:40.738824 22579586809984 run.py:483] Algo bellman_ford step 9880 current loss 0.003780, current_train_items 316192.
I0304 19:32:40.755238 22579586809984 run.py:483] Algo bellman_ford step 9881 current loss 0.047712, current_train_items 316224.
I0304 19:32:40.779603 22579586809984 run.py:483] Algo bellman_ford step 9882 current loss 0.017150, current_train_items 316256.
I0304 19:32:40.811708 22579586809984 run.py:483] Algo bellman_ford step 9883 current loss 0.027141, current_train_items 316288.
I0304 19:32:40.845437 22579586809984 run.py:483] Algo bellman_ford step 9884 current loss 0.061133, current_train_items 316320.
I0304 19:32:40.865476 22579586809984 run.py:483] Algo bellman_ford step 9885 current loss 0.002428, current_train_items 316352.
I0304 19:32:40.881850 22579586809984 run.py:483] Algo bellman_ford step 9886 current loss 0.012247, current_train_items 316384.
I0304 19:32:40.906060 22579586809984 run.py:483] Algo bellman_ford step 9887 current loss 0.022924, current_train_items 316416.
I0304 19:32:40.937719 22579586809984 run.py:483] Algo bellman_ford step 9888 current loss 0.038207, current_train_items 316448.
I0304 19:32:40.970541 22579586809984 run.py:483] Algo bellman_ford step 9889 current loss 0.059636, current_train_items 316480.
I0304 19:32:40.990551 22579586809984 run.py:483] Algo bellman_ford step 9890 current loss 0.007013, current_train_items 316512.
I0304 19:32:41.007082 22579586809984 run.py:483] Algo bellman_ford step 9891 current loss 0.009451, current_train_items 316544.
I0304 19:32:41.030426 22579586809984 run.py:483] Algo bellman_ford step 9892 current loss 0.036366, current_train_items 316576.
I0304 19:32:41.062947 22579586809984 run.py:483] Algo bellman_ford step 9893 current loss 0.020740, current_train_items 316608.
I0304 19:32:41.097933 22579586809984 run.py:483] Algo bellman_ford step 9894 current loss 0.045086, current_train_items 316640.
I0304 19:32:41.117735 22579586809984 run.py:483] Algo bellman_ford step 9895 current loss 0.003529, current_train_items 316672.
I0304 19:32:41.133923 22579586809984 run.py:483] Algo bellman_ford step 9896 current loss 0.016415, current_train_items 316704.
I0304 19:32:41.158611 22579586809984 run.py:483] Algo bellman_ford step 9897 current loss 0.057086, current_train_items 316736.
I0304 19:32:41.190689 22579586809984 run.py:483] Algo bellman_ford step 9898 current loss 0.074263, current_train_items 316768.
I0304 19:32:41.226195 22579586809984 run.py:483] Algo bellman_ford step 9899 current loss 0.038503, current_train_items 316800.
I0304 19:32:41.246109 22579586809984 run.py:483] Algo bellman_ford step 9900 current loss 0.001962, current_train_items 316832.
I0304 19:32:41.254137 22579586809984 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0304 19:32:41.254240 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:32:41.271165 22579586809984 run.py:483] Algo bellman_ford step 9901 current loss 0.010916, current_train_items 316864.
I0304 19:32:41.296456 22579586809984 run.py:483] Algo bellman_ford step 9902 current loss 0.037872, current_train_items 316896.
I0304 19:32:41.328194 22579586809984 run.py:483] Algo bellman_ford step 9903 current loss 0.033067, current_train_items 316928.
I0304 19:32:41.362844 22579586809984 run.py:483] Algo bellman_ford step 9904 current loss 0.040973, current_train_items 316960.
I0304 19:32:41.382542 22579586809984 run.py:483] Algo bellman_ford step 9905 current loss 0.002353, current_train_items 316992.
I0304 19:32:41.399171 22579586809984 run.py:483] Algo bellman_ford step 9906 current loss 0.045758, current_train_items 317024.
I0304 19:32:41.424115 22579586809984 run.py:483] Algo bellman_ford step 9907 current loss 0.086001, current_train_items 317056.
I0304 19:32:41.456107 22579586809984 run.py:483] Algo bellman_ford step 9908 current loss 0.032902, current_train_items 317088.
I0304 19:32:41.488188 22579586809984 run.py:483] Algo bellman_ford step 9909 current loss 0.049045, current_train_items 317120.
I0304 19:32:41.507793 22579586809984 run.py:483] Algo bellman_ford step 9910 current loss 0.010002, current_train_items 317152.
I0304 19:32:41.524406 22579586809984 run.py:483] Algo bellman_ford step 9911 current loss 0.009707, current_train_items 317184.
I0304 19:32:41.548687 22579586809984 run.py:483] Algo bellman_ford step 9912 current loss 0.048465, current_train_items 317216.
I0304 19:32:41.581229 22579586809984 run.py:483] Algo bellman_ford step 9913 current loss 0.047745, current_train_items 317248.
I0304 19:32:41.614688 22579586809984 run.py:483] Algo bellman_ford step 9914 current loss 0.051094, current_train_items 317280.
I0304 19:32:41.634592 22579586809984 run.py:483] Algo bellman_ford step 9915 current loss 0.022293, current_train_items 317312.
I0304 19:32:41.651323 22579586809984 run.py:483] Algo bellman_ford step 9916 current loss 0.012145, current_train_items 317344.
I0304 19:32:41.676030 22579586809984 run.py:483] Algo bellman_ford step 9917 current loss 0.036100, current_train_items 317376.
I0304 19:32:41.708251 22579586809984 run.py:483] Algo bellman_ford step 9918 current loss 0.085117, current_train_items 317408.
I0304 19:32:41.741276 22579586809984 run.py:483] Algo bellman_ford step 9919 current loss 0.072881, current_train_items 317440.
I0304 19:32:41.761368 22579586809984 run.py:483] Algo bellman_ford step 9920 current loss 0.002833, current_train_items 317472.
I0304 19:32:41.777913 22579586809984 run.py:483] Algo bellman_ford step 9921 current loss 0.014682, current_train_items 317504.
I0304 19:32:41.802443 22579586809984 run.py:483] Algo bellman_ford step 9922 current loss 0.045673, current_train_items 317536.
I0304 19:32:41.832792 22579586809984 run.py:483] Algo bellman_ford step 9923 current loss 0.041760, current_train_items 317568.
I0304 19:32:41.868916 22579586809984 run.py:483] Algo bellman_ford step 9924 current loss 0.069063, current_train_items 317600.
I0304 19:32:41.888218 22579586809984 run.py:483] Algo bellman_ford step 9925 current loss 0.009384, current_train_items 317632.
I0304 19:32:41.904363 22579586809984 run.py:483] Algo bellman_ford step 9926 current loss 0.028426, current_train_items 317664.
I0304 19:32:41.929231 22579586809984 run.py:483] Algo bellman_ford step 9927 current loss 0.036676, current_train_items 317696.
I0304 19:32:41.962466 22579586809984 run.py:483] Algo bellman_ford step 9928 current loss 0.044415, current_train_items 317728.
I0304 19:32:41.993896 22579586809984 run.py:483] Algo bellman_ford step 9929 current loss 0.049044, current_train_items 317760.
I0304 19:32:42.013549 22579586809984 run.py:483] Algo bellman_ford step 9930 current loss 0.002091, current_train_items 317792.
I0304 19:32:42.029235 22579586809984 run.py:483] Algo bellman_ford step 9931 current loss 0.018182, current_train_items 317824.
I0304 19:32:42.053939 22579586809984 run.py:483] Algo bellman_ford step 9932 current loss 0.038716, current_train_items 317856.
I0304 19:32:42.085280 22579586809984 run.py:483] Algo bellman_ford step 9933 current loss 0.061154, current_train_items 317888.
I0304 19:32:42.117391 22579586809984 run.py:483] Algo bellman_ford step 9934 current loss 0.084730, current_train_items 317920.
I0304 19:32:42.136941 22579586809984 run.py:483] Algo bellman_ford step 9935 current loss 0.004406, current_train_items 317952.
I0304 19:32:42.152874 22579586809984 run.py:483] Algo bellman_ford step 9936 current loss 0.010244, current_train_items 317984.
I0304 19:32:42.176845 22579586809984 run.py:483] Algo bellman_ford step 9937 current loss 0.036347, current_train_items 318016.
I0304 19:32:42.208365 22579586809984 run.py:483] Algo bellman_ford step 9938 current loss 0.022604, current_train_items 318048.
I0304 19:32:42.242778 22579586809984 run.py:483] Algo bellman_ford step 9939 current loss 0.074026, current_train_items 318080.
I0304 19:32:42.262284 22579586809984 run.py:483] Algo bellman_ford step 9940 current loss 0.003509, current_train_items 318112.
I0304 19:32:42.278208 22579586809984 run.py:483] Algo bellman_ford step 9941 current loss 0.012868, current_train_items 318144.
I0304 19:32:42.301878 22579586809984 run.py:483] Algo bellman_ford step 9942 current loss 0.051973, current_train_items 318176.
I0304 19:32:42.333858 22579586809984 run.py:483] Algo bellman_ford step 9943 current loss 0.044613, current_train_items 318208.
I0304 19:32:42.365824 22579586809984 run.py:483] Algo bellman_ford step 9944 current loss 0.060545, current_train_items 318240.
I0304 19:32:42.385164 22579586809984 run.py:483] Algo bellman_ford step 9945 current loss 0.004936, current_train_items 318272.
I0304 19:32:42.401010 22579586809984 run.py:483] Algo bellman_ford step 9946 current loss 0.051886, current_train_items 318304.
I0304 19:32:42.425453 22579586809984 run.py:483] Algo bellman_ford step 9947 current loss 0.023599, current_train_items 318336.
I0304 19:32:42.457513 22579586809984 run.py:483] Algo bellman_ford step 9948 current loss 0.036681, current_train_items 318368.
I0304 19:32:42.492251 22579586809984 run.py:483] Algo bellman_ford step 9949 current loss 0.039836, current_train_items 318400.
I0304 19:32:42.511555 22579586809984 run.py:483] Algo bellman_ford step 9950 current loss 0.002306, current_train_items 318432.
I0304 19:32:42.519521 22579586809984 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0304 19:32:42.519626 22579586809984 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:42.536808 22579586809984 run.py:483] Algo bellman_ford step 9951 current loss 0.029880, current_train_items 318464.
I0304 19:32:42.561506 22579586809984 run.py:483] Algo bellman_ford step 9952 current loss 0.020967, current_train_items 318496.
I0304 19:32:42.594633 22579586809984 run.py:483] Algo bellman_ford step 9953 current loss 0.053272, current_train_items 318528.
I0304 19:32:42.630009 22579586809984 run.py:483] Algo bellman_ford step 9954 current loss 0.059053, current_train_items 318560.
I0304 19:32:42.649744 22579586809984 run.py:483] Algo bellman_ford step 9955 current loss 0.011425, current_train_items 318592.
I0304 19:32:42.666232 22579586809984 run.py:483] Algo bellman_ford step 9956 current loss 0.026631, current_train_items 318624.
I0304 19:32:42.690769 22579586809984 run.py:483] Algo bellman_ford step 9957 current loss 0.016420, current_train_items 318656.
I0304 19:32:42.723218 22579586809984 run.py:483] Algo bellman_ford step 9958 current loss 0.070399, current_train_items 318688.
I0304 19:32:42.758166 22579586809984 run.py:483] Algo bellman_ford step 9959 current loss 0.081224, current_train_items 318720.
I0304 19:32:42.778192 22579586809984 run.py:483] Algo bellman_ford step 9960 current loss 0.003458, current_train_items 318752.
I0304 19:32:42.794120 22579586809984 run.py:483] Algo bellman_ford step 9961 current loss 0.019607, current_train_items 318784.
I0304 19:32:42.819038 22579586809984 run.py:483] Algo bellman_ford step 9962 current loss 0.027260, current_train_items 318816.
I0304 19:32:42.851285 22579586809984 run.py:483] Algo bellman_ford step 9963 current loss 0.033993, current_train_items 318848.
I0304 19:32:42.884621 22579586809984 run.py:483] Algo bellman_ford step 9964 current loss 0.081234, current_train_items 318880.
I0304 19:32:42.904271 22579586809984 run.py:483] Algo bellman_ford step 9965 current loss 0.002284, current_train_items 318912.
I0304 19:32:42.920803 22579586809984 run.py:483] Algo bellman_ford step 9966 current loss 0.008943, current_train_items 318944.
I0304 19:32:42.945297 22579586809984 run.py:483] Algo bellman_ford step 9967 current loss 0.037199, current_train_items 318976.
I0304 19:32:42.977274 22579586809984 run.py:483] Algo bellman_ford step 9968 current loss 0.046955, current_train_items 319008.
I0304 19:32:43.011192 22579586809984 run.py:483] Algo bellman_ford step 9969 current loss 0.052169, current_train_items 319040.
I0304 19:32:43.031116 22579586809984 run.py:483] Algo bellman_ford step 9970 current loss 0.002536, current_train_items 319072.
I0304 19:32:43.047284 22579586809984 run.py:483] Algo bellman_ford step 9971 current loss 0.012368, current_train_items 319104.
I0304 19:32:43.071080 22579586809984 run.py:483] Algo bellman_ford step 9972 current loss 0.036338, current_train_items 319136.
I0304 19:32:43.102308 22579586809984 run.py:483] Algo bellman_ford step 9973 current loss 0.044930, current_train_items 319168.
I0304 19:32:43.135768 22579586809984 run.py:483] Algo bellman_ford step 9974 current loss 0.061908, current_train_items 319200.
I0304 19:32:43.155672 22579586809984 run.py:483] Algo bellman_ford step 9975 current loss 0.002164, current_train_items 319232.
I0304 19:32:43.172378 22579586809984 run.py:483] Algo bellman_ford step 9976 current loss 0.023376, current_train_items 319264.
I0304 19:32:43.196434 22579586809984 run.py:483] Algo bellman_ford step 9977 current loss 0.074768, current_train_items 319296.
I0304 19:32:43.228247 22579586809984 run.py:483] Algo bellman_ford step 9978 current loss 0.019209, current_train_items 319328.
I0304 19:32:43.260785 22579586809984 run.py:483] Algo bellman_ford step 9979 current loss 0.029724, current_train_items 319360.
I0304 19:32:43.280356 22579586809984 run.py:483] Algo bellman_ford step 9980 current loss 0.002375, current_train_items 319392.
I0304 19:32:43.296555 22579586809984 run.py:483] Algo bellman_ford step 9981 current loss 0.006501, current_train_items 319424.
I0304 19:32:43.321690 22579586809984 run.py:483] Algo bellman_ford step 9982 current loss 0.037813, current_train_items 319456.
I0304 19:32:43.355090 22579586809984 run.py:483] Algo bellman_ford step 9983 current loss 0.047301, current_train_items 319488.
I0304 19:32:43.390078 22579586809984 run.py:483] Algo bellman_ford step 9984 current loss 0.048833, current_train_items 319520.
I0304 19:32:43.409990 22579586809984 run.py:483] Algo bellman_ford step 9985 current loss 0.002660, current_train_items 319552.
I0304 19:32:43.425966 22579586809984 run.py:483] Algo bellman_ford step 9986 current loss 0.003064, current_train_items 319584.
I0304 19:32:43.449833 22579586809984 run.py:483] Algo bellman_ford step 9987 current loss 0.023771, current_train_items 319616.
I0304 19:32:43.481783 22579586809984 run.py:483] Algo bellman_ford step 9988 current loss 0.028732, current_train_items 319648.
I0304 19:32:43.515171 22579586809984 run.py:483] Algo bellman_ford step 9989 current loss 0.051041, current_train_items 319680.
I0304 19:32:43.535008 22579586809984 run.py:483] Algo bellman_ford step 9990 current loss 0.036730, current_train_items 319712.
I0304 19:32:43.551591 22579586809984 run.py:483] Algo bellman_ford step 9991 current loss 0.022551, current_train_items 319744.
I0304 19:32:43.575479 22579586809984 run.py:483] Algo bellman_ford step 9992 current loss 0.034821, current_train_items 319776.
I0304 19:32:43.606719 22579586809984 run.py:483] Algo bellman_ford step 9993 current loss 0.026693, current_train_items 319808.
I0304 19:32:43.640096 22579586809984 run.py:483] Algo bellman_ford step 9994 current loss 0.054343, current_train_items 319840.
I0304 19:32:43.659877 22579586809984 run.py:483] Algo bellman_ford step 9995 current loss 0.003316, current_train_items 319872.
I0304 19:32:43.676298 22579586809984 run.py:483] Algo bellman_ford step 9996 current loss 0.052544, current_train_items 319904.
I0304 19:32:43.700538 22579586809984 run.py:483] Algo bellman_ford step 9997 current loss 0.024618, current_train_items 319936.
I0304 19:32:43.732298 22579586809984 run.py:483] Algo bellman_ford step 9998 current loss 0.035222, current_train_items 319968.
I0304 19:32:43.764284 22579586809984 run.py:483] Algo bellman_ford step 9999 current loss 0.062406, current_train_items 320000.
I0304 19:32:43.770276 22579586809984 run.py:527] Restoring best model from checkpoint...
I0304 19:32:46.379245 22579586809984 run.py:542] (test) algo bellman_ford : {'pi': 0.95458984375, 'score': 0.95458984375, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0304 19:32:46.379440 22579586809984 run.py:544] Done!

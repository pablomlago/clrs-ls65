Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-02 18:56:31.952523: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-02 18:56:31.952830: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-02 18:56:31.979730: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-02 18:56:36.425387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0302 18:56:47.198304 22732373614720 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0302 18:56:47.208435 22732373614720 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0302 18:56:47.598953 22732373614720 run.py:307] Creating samplers for algo bellman_ford
W0302 18:56:47.599388 22732373614720 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:47.599672 22732373614720 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:47.803807 22732373614720 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:47.804049 22732373614720 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.046775 22732373614720 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.047031 22732373614720 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.384212 22732373614720 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.384454 22732373614720 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.798077 22732373614720 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.798335 22732373614720 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:49.326216 22732373614720 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0302 18:56:49.326481 22732373614720 samplers.py:112] Creating a dataset with 64 samples.
I0302 18:56:49.364598 22732373614720 run.py:166] Dataset not found in ./datasets_1/118/CLRS30_v1.0.0. Downloading...
I0302 18:57:05.801339 22732373614720 dataset_info.py:482] Load dataset info from ./datasets_1/118/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:05.803856 22732373614720 dataset_info.py:482] Load dataset info from ./datasets_1/118/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:05.804636 22732373614720 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/118/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0302 18:57:05.804713 22732373614720 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/118/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:22.287235 22732373614720 run.py:483] Algo bellman_ford step 0 current loss 14.604424, current_train_items 32.
I0302 18:57:25.255353 22732373614720 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.22265625, 'score': 0.22265625, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0302 18:57:25.255637 22732373614720 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.223, val scores are: bellman_ford: 0.223
I0302 18:57:35.251275 22732373614720 run.py:483] Algo bellman_ford step 1 current loss 1516.173340, current_train_items 64.
I0302 18:57:46.653869 22732373614720 run.py:483] Algo bellman_ford step 2 current loss 166676640.000000, current_train_items 96.
I0302 18:57:57.795587 22732373614720 run.py:483] Algo bellman_ford step 3 current loss 34929176576.000000, current_train_items 128.
I0302 18:58:07.845964 22732373614720 run.py:483] Algo bellman_ford step 4 current loss 11424131072.000000, current_train_items 160.
I0302 18:58:07.864569 22732373614720 run.py:483] Algo bellman_ford step 5 current loss 3.474677, current_train_items 192.
I0302 18:58:07.881285 22732373614720 run.py:483] Algo bellman_ford step 6 current loss 71.306404, current_train_items 224.
I0302 18:58:07.903339 22732373614720 run.py:483] Algo bellman_ford step 7 current loss 2443133.250000, current_train_items 256.
I0302 18:58:07.932555 22732373614720 run.py:483] Algo bellman_ford step 8 current loss 190749232.000000, current_train_items 288.
I0302 18:58:07.963863 22732373614720 run.py:483] Algo bellman_ford step 9 current loss 319763488.000000, current_train_items 320.
I0302 18:58:07.981425 22732373614720 run.py:483] Algo bellman_ford step 10 current loss 3.196623, current_train_items 352.
I0302 18:58:07.997980 22732373614720 run.py:483] Algo bellman_ford step 11 current loss 24.341858, current_train_items 384.
I0302 18:58:08.019460 22732373614720 run.py:483] Algo bellman_ford step 12 current loss 69831.179688, current_train_items 416.
I0302 18:58:08.049360 22732373614720 run.py:483] Algo bellman_ford step 13 current loss 6367588.500000, current_train_items 448.
I0302 18:58:08.076695 22732373614720 run.py:483] Algo bellman_ford step 14 current loss 7485969.500000, current_train_items 480.
I0302 18:58:08.094680 22732373614720 run.py:483] Algo bellman_ford step 15 current loss 1.687151, current_train_items 512.
I0302 18:58:08.110840 22732373614720 run.py:483] Algo bellman_ford step 16 current loss 4.984602, current_train_items 544.
I0302 18:58:08.134636 22732373614720 run.py:483] Algo bellman_ford step 17 current loss 3899.081787, current_train_items 576.
I0302 18:58:08.162843 22732373614720 run.py:483] Algo bellman_ford step 18 current loss 101316.726562, current_train_items 608.
I0302 18:58:08.194816 22732373614720 run.py:483] Algo bellman_ford step 19 current loss 118603.429688, current_train_items 640.
I0302 18:58:08.211976 22732373614720 run.py:483] Algo bellman_ford step 20 current loss 1.494248, current_train_items 672.
I0302 18:58:08.227515 22732373614720 run.py:483] Algo bellman_ford step 21 current loss 2.040315, current_train_items 704.
I0302 18:58:08.250288 22732373614720 run.py:483] Algo bellman_ford step 22 current loss 75.695793, current_train_items 736.
I0302 18:58:08.278831 22732373614720 run.py:483] Algo bellman_ford step 23 current loss 7817.854492, current_train_items 768.
I0302 18:58:08.309312 22732373614720 run.py:483] Algo bellman_ford step 24 current loss 24617.408203, current_train_items 800.
I0302 18:58:08.326799 22732373614720 run.py:483] Algo bellman_ford step 25 current loss 1.322882, current_train_items 832.
I0302 18:58:08.342826 22732373614720 run.py:483] Algo bellman_ford step 26 current loss 1.987284, current_train_items 864.
I0302 18:58:08.365867 22732373614720 run.py:483] Algo bellman_ford step 27 current loss 34.525959, current_train_items 896.
I0302 18:58:08.395236 22732373614720 run.py:483] Algo bellman_ford step 28 current loss 128.545044, current_train_items 928.
I0302 18:58:08.426911 22732373614720 run.py:483] Algo bellman_ford step 29 current loss 417.688782, current_train_items 960.
I0302 18:58:08.444279 22732373614720 run.py:483] Algo bellman_ford step 30 current loss 1.021701, current_train_items 992.
I0302 18:58:08.459435 22732373614720 run.py:483] Algo bellman_ford step 31 current loss 1.470408, current_train_items 1024.
I0302 18:58:08.481503 22732373614720 run.py:483] Algo bellman_ford step 32 current loss 2.324764, current_train_items 1056.
I0302 18:58:08.511185 22732373614720 run.py:483] Algo bellman_ford step 33 current loss 2.610386, current_train_items 1088.
I0302 18:58:08.541656 22732373614720 run.py:483] Algo bellman_ford step 34 current loss 2.835787, current_train_items 1120.
I0302 18:58:08.558772 22732373614720 run.py:483] Algo bellman_ford step 35 current loss 1.138177, current_train_items 1152.
I0302 18:58:08.574245 22732373614720 run.py:483] Algo bellman_ford step 36 current loss 1.491332, current_train_items 1184.
I0302 18:58:08.596937 22732373614720 run.py:483] Algo bellman_ford step 37 current loss 2.090037, current_train_items 1216.
I0302 18:58:08.625789 22732373614720 run.py:483] Algo bellman_ford step 38 current loss 2.327356, current_train_items 1248.
W0302 18:58:08.648055 22732373614720 samplers.py:155] Increasing hint lengh from 9 to 11
I0302 18:58:15.211226 22732373614720 run.py:483] Algo bellman_ford step 39 current loss 3.214904, current_train_items 1280.
I0302 18:58:15.231212 22732373614720 run.py:483] Algo bellman_ford step 40 current loss 1.002220, current_train_items 1312.
I0302 18:58:15.247765 22732373614720 run.py:483] Algo bellman_ford step 41 current loss 1.420390, current_train_items 1344.
I0302 18:58:15.270549 22732373614720 run.py:483] Algo bellman_ford step 42 current loss 1.842706, current_train_items 1376.
I0302 18:58:15.300482 22732373614720 run.py:483] Algo bellman_ford step 43 current loss 2.255641, current_train_items 1408.
I0302 18:58:15.332135 22732373614720 run.py:483] Algo bellman_ford step 44 current loss 2.488749, current_train_items 1440.
I0302 18:58:15.351171 22732373614720 run.py:483] Algo bellman_ford step 45 current loss 0.933493, current_train_items 1472.
I0302 18:58:15.367331 22732373614720 run.py:483] Algo bellman_ford step 46 current loss 1.515433, current_train_items 1504.
I0302 18:58:15.389302 22732373614720 run.py:483] Algo bellman_ford step 47 current loss 1.801185, current_train_items 1536.
I0302 18:58:15.415836 22732373614720 run.py:483] Algo bellman_ford step 48 current loss 1.718359, current_train_items 1568.
I0302 18:58:15.444518 22732373614720 run.py:483] Algo bellman_ford step 49 current loss 2.199314, current_train_items 1600.
I0302 18:58:15.462794 22732373614720 run.py:483] Algo bellman_ford step 50 current loss 0.800858, current_train_items 1632.
I0302 18:58:15.472708 22732373614720 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.6904296875, 'score': 0.6904296875, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0302 18:58:15.472823 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.223, current avg val score is 0.690, val scores are: bellman_ford: 0.690
I0302 18:58:15.502667 22732373614720 run.py:483] Algo bellman_ford step 51 current loss 1.244467, current_train_items 1664.
I0302 18:58:15.525565 22732373614720 run.py:483] Algo bellman_ford step 52 current loss 1.842489, current_train_items 1696.
I0302 18:58:15.554271 22732373614720 run.py:483] Algo bellman_ford step 53 current loss 1.949804, current_train_items 1728.
I0302 18:58:15.585964 22732373614720 run.py:483] Algo bellman_ford step 54 current loss 2.400299, current_train_items 1760.
I0302 18:58:15.605423 22732373614720 run.py:483] Algo bellman_ford step 55 current loss 0.856268, current_train_items 1792.
I0302 18:58:15.621260 22732373614720 run.py:483] Algo bellman_ford step 56 current loss 1.283615, current_train_items 1824.
I0302 18:58:15.643723 22732373614720 run.py:483] Algo bellman_ford step 57 current loss 1.813206, current_train_items 1856.
I0302 18:58:15.670965 22732373614720 run.py:483] Algo bellman_ford step 58 current loss 1.798600, current_train_items 1888.
I0302 18:58:15.703629 22732373614720 run.py:483] Algo bellman_ford step 59 current loss 2374.385010, current_train_items 1920.
I0302 18:58:15.722422 22732373614720 run.py:483] Algo bellman_ford step 60 current loss 0.796127, current_train_items 1952.
W0302 18:58:15.731475 22732373614720 samplers.py:155] Increasing hint lengh from 6 to 7
I0302 18:58:22.032708 22732373614720 run.py:483] Algo bellman_ford step 61 current loss 1.151438, current_train_items 1984.
I0302 18:58:22.056602 22732373614720 run.py:483] Algo bellman_ford step 62 current loss 1.674753, current_train_items 2016.
I0302 18:58:22.085511 22732373614720 run.py:483] Algo bellman_ford step 63 current loss 1.950391, current_train_items 2048.
I0302 18:58:22.119501 22732373614720 run.py:483] Algo bellman_ford step 64 current loss 2.442339, current_train_items 2080.
I0302 18:58:22.139042 22732373614720 run.py:483] Algo bellman_ford step 65 current loss 0.796725, current_train_items 2112.
I0302 18:58:22.155026 22732373614720 run.py:483] Algo bellman_ford step 66 current loss 1.185011, current_train_items 2144.
I0302 18:58:22.178741 22732373614720 run.py:483] Algo bellman_ford step 67 current loss 1.842721, current_train_items 2176.
I0302 18:58:22.206504 22732373614720 run.py:483] Algo bellman_ford step 68 current loss 1.848558, current_train_items 2208.
I0302 18:58:22.238215 22732373614720 run.py:483] Algo bellman_ford step 69 current loss 2.263211, current_train_items 2240.
I0302 18:58:22.256594 22732373614720 run.py:483] Algo bellman_ford step 70 current loss 0.723231, current_train_items 2272.
I0302 18:58:22.272737 22732373614720 run.py:483] Algo bellman_ford step 71 current loss 1.181589, current_train_items 2304.
I0302 18:58:22.295441 22732373614720 run.py:483] Algo bellman_ford step 72 current loss 1.624333, current_train_items 2336.
I0302 18:58:22.324398 22732373614720 run.py:483] Algo bellman_ford step 73 current loss 1.773510, current_train_items 2368.
I0302 18:58:22.356404 22732373614720 run.py:483] Algo bellman_ford step 74 current loss 2.053643, current_train_items 2400.
I0302 18:58:22.374701 22732373614720 run.py:483] Algo bellman_ford step 75 current loss 0.570649, current_train_items 2432.
I0302 18:58:22.391209 22732373614720 run.py:483] Algo bellman_ford step 76 current loss 1.253436, current_train_items 2464.
I0302 18:58:22.413468 22732373614720 run.py:483] Algo bellman_ford step 77 current loss 1.714311, current_train_items 2496.
I0302 18:58:22.441335 22732373614720 run.py:483] Algo bellman_ford step 78 current loss 1.754499, current_train_items 2528.
I0302 18:58:22.470239 22732373614720 run.py:483] Algo bellman_ford step 79 current loss 1.817127, current_train_items 2560.
I0302 18:58:22.488992 22732373614720 run.py:483] Algo bellman_ford step 80 current loss 0.673871, current_train_items 2592.
I0302 18:58:22.504791 22732373614720 run.py:483] Algo bellman_ford step 81 current loss 1.105939, current_train_items 2624.
I0302 18:58:22.527389 22732373614720 run.py:483] Algo bellman_ford step 82 current loss 1.547937, current_train_items 2656.
I0302 18:58:22.555821 22732373614720 run.py:483] Algo bellman_ford step 83 current loss 1.575737, current_train_items 2688.
I0302 18:58:22.585379 22732373614720 run.py:483] Algo bellman_ford step 84 current loss 1.814047, current_train_items 2720.
I0302 18:58:22.603576 22732373614720 run.py:483] Algo bellman_ford step 85 current loss 0.602559, current_train_items 2752.
I0302 18:58:22.619779 22732373614720 run.py:483] Algo bellman_ford step 86 current loss 1.098350, current_train_items 2784.
I0302 18:58:22.643654 22732373614720 run.py:483] Algo bellman_ford step 87 current loss 1.764615, current_train_items 2816.
I0302 18:58:22.672552 22732373614720 run.py:483] Algo bellman_ford step 88 current loss 1.649370, current_train_items 2848.
I0302 18:58:22.703872 22732373614720 run.py:483] Algo bellman_ford step 89 current loss 89.746368, current_train_items 2880.
I0302 18:58:22.722279 22732373614720 run.py:483] Algo bellman_ford step 90 current loss 0.687872, current_train_items 2912.
I0302 18:58:22.738310 22732373614720 run.py:483] Algo bellman_ford step 91 current loss 1.166988, current_train_items 2944.
I0302 18:58:22.760588 22732373614720 run.py:483] Algo bellman_ford step 92 current loss 1.424035, current_train_items 2976.
I0302 18:58:22.790417 22732373614720 run.py:483] Algo bellman_ford step 93 current loss 1.672506, current_train_items 3008.
I0302 18:58:22.821075 22732373614720 run.py:483] Algo bellman_ford step 94 current loss 1.873880, current_train_items 3040.
I0302 18:58:22.839739 22732373614720 run.py:483] Algo bellman_ford step 95 current loss 0.622981, current_train_items 3072.
I0302 18:58:22.855345 22732373614720 run.py:483] Algo bellman_ford step 96 current loss 0.997066, current_train_items 3104.
I0302 18:58:22.877966 22732373614720 run.py:483] Algo bellman_ford step 97 current loss 1.438939, current_train_items 3136.
I0302 18:58:22.906582 22732373614720 run.py:483] Algo bellman_ford step 98 current loss 1.586287, current_train_items 3168.
I0302 18:58:22.937694 22732373614720 run.py:483] Algo bellman_ford step 99 current loss 1.867996, current_train_items 3200.
I0302 18:58:22.956023 22732373614720 run.py:483] Algo bellman_ford step 100 current loss 0.593423, current_train_items 3232.
I0302 18:58:22.965687 22732373614720 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.744140625, 'score': 0.744140625, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0302 18:58:22.965799 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.690, current avg val score is 0.744, val scores are: bellman_ford: 0.744
I0302 18:58:22.996346 22732373614720 run.py:483] Algo bellman_ford step 101 current loss 1.107358, current_train_items 3264.
I0302 18:58:23.019032 22732373614720 run.py:483] Algo bellman_ford step 102 current loss 1.271756, current_train_items 3296.
I0302 18:58:23.047390 22732373614720 run.py:483] Algo bellman_ford step 103 current loss 1.488905, current_train_items 3328.
I0302 18:58:23.080849 22732373614720 run.py:483] Algo bellman_ford step 104 current loss 2.080802, current_train_items 3360.
I0302 18:58:23.100006 22732373614720 run.py:483] Algo bellman_ford step 105 current loss 0.550236, current_train_items 3392.
I0302 18:58:23.116004 22732373614720 run.py:483] Algo bellman_ford step 106 current loss 0.958244, current_train_items 3424.
I0302 18:58:23.139101 22732373614720 run.py:483] Algo bellman_ford step 107 current loss 1.210094, current_train_items 3456.
I0302 18:58:23.167442 22732373614720 run.py:483] Algo bellman_ford step 108 current loss 2.541869, current_train_items 3488.
I0302 18:58:23.197717 22732373614720 run.py:483] Algo bellman_ford step 109 current loss 2.997086, current_train_items 3520.
I0302 18:58:23.216253 22732373614720 run.py:483] Algo bellman_ford step 110 current loss 0.627079, current_train_items 3552.
I0302 18:58:23.232028 22732373614720 run.py:483] Algo bellman_ford step 111 current loss 0.940039, current_train_items 3584.
I0302 18:58:23.254604 22732373614720 run.py:483] Algo bellman_ford step 112 current loss 1.445587, current_train_items 3616.
I0302 18:58:23.283483 22732373614720 run.py:483] Algo bellman_ford step 113 current loss 2.069017, current_train_items 3648.
I0302 18:58:23.314171 22732373614720 run.py:483] Algo bellman_ford step 114 current loss 2.125942, current_train_items 3680.
I0302 18:58:23.332468 22732373614720 run.py:483] Algo bellman_ford step 115 current loss 0.515341, current_train_items 3712.
I0302 18:58:23.348737 22732373614720 run.py:483] Algo bellman_ford step 116 current loss 1.298697, current_train_items 3744.
I0302 18:58:23.372566 22732373614720 run.py:483] Algo bellman_ford step 117 current loss 1.672914, current_train_items 3776.
I0302 18:58:23.401317 22732373614720 run.py:483] Algo bellman_ford step 118 current loss 1.658523, current_train_items 3808.
I0302 18:58:23.430402 22732373614720 run.py:483] Algo bellman_ford step 119 current loss 1.721918, current_train_items 3840.
I0302 18:58:23.449331 22732373614720 run.py:483] Algo bellman_ford step 120 current loss 0.611684, current_train_items 3872.
I0302 18:58:23.465554 22732373614720 run.py:483] Algo bellman_ford step 121 current loss 1.041638, current_train_items 3904.
I0302 18:58:23.488968 22732373614720 run.py:483] Algo bellman_ford step 122 current loss 1.319042, current_train_items 3936.
I0302 18:58:23.518579 22732373614720 run.py:483] Algo bellman_ford step 123 current loss 1.615298, current_train_items 3968.
I0302 18:58:23.553004 22732373614720 run.py:483] Algo bellman_ford step 124 current loss 2.043274, current_train_items 4000.
I0302 18:58:23.571881 22732373614720 run.py:483] Algo bellman_ford step 125 current loss 0.784550, current_train_items 4032.
I0302 18:58:23.588169 22732373614720 run.py:483] Algo bellman_ford step 126 current loss 1.169163, current_train_items 4064.
I0302 18:58:23.611475 22732373614720 run.py:483] Algo bellman_ford step 127 current loss 1.515315, current_train_items 4096.
I0302 18:58:23.640238 22732373614720 run.py:483] Algo bellman_ford step 128 current loss 1.506731, current_train_items 4128.
I0302 18:58:23.671890 22732373614720 run.py:483] Algo bellman_ford step 129 current loss 1.731542, current_train_items 4160.
I0302 18:58:23.690648 22732373614720 run.py:483] Algo bellman_ford step 130 current loss 0.548861, current_train_items 4192.
I0302 18:58:23.706653 22732373614720 run.py:483] Algo bellman_ford step 131 current loss 0.838797, current_train_items 4224.
I0302 18:58:23.729758 22732373614720 run.py:483] Algo bellman_ford step 132 current loss 1.474422, current_train_items 4256.
I0302 18:58:23.758450 22732373614720 run.py:483] Algo bellman_ford step 133 current loss 1.607674, current_train_items 4288.
I0302 18:58:23.789380 22732373614720 run.py:483] Algo bellman_ford step 134 current loss 1.901168, current_train_items 4320.
I0302 18:58:23.807703 22732373614720 run.py:483] Algo bellman_ford step 135 current loss 0.609273, current_train_items 4352.
I0302 18:58:23.823564 22732373614720 run.py:483] Algo bellman_ford step 136 current loss 1.054160, current_train_items 4384.
I0302 18:58:23.846777 22732373614720 run.py:483] Algo bellman_ford step 137 current loss 1.425804, current_train_items 4416.
I0302 18:58:23.875436 22732373614720 run.py:483] Algo bellman_ford step 138 current loss 1.511574, current_train_items 4448.
I0302 18:58:23.907799 22732373614720 run.py:483] Algo bellman_ford step 139 current loss 1.755594, current_train_items 4480.
I0302 18:58:23.926206 22732373614720 run.py:483] Algo bellman_ford step 140 current loss 0.468593, current_train_items 4512.
I0302 18:58:23.942434 22732373614720 run.py:483] Algo bellman_ford step 141 current loss 1.001347, current_train_items 4544.
I0302 18:58:23.965192 22732373614720 run.py:483] Algo bellman_ford step 142 current loss 1.338476, current_train_items 4576.
I0302 18:58:23.994579 22732373614720 run.py:483] Algo bellman_ford step 143 current loss 1.700312, current_train_items 4608.
I0302 18:58:24.026128 22732373614720 run.py:483] Algo bellman_ford step 144 current loss 1.611609, current_train_items 4640.
I0302 18:58:24.044618 22732373614720 run.py:483] Algo bellman_ford step 145 current loss 0.598949, current_train_items 4672.
I0302 18:58:24.060379 22732373614720 run.py:483] Algo bellman_ford step 146 current loss 0.945768, current_train_items 4704.
I0302 18:58:24.082672 22732373614720 run.py:483] Algo bellman_ford step 147 current loss 1.308272, current_train_items 4736.
I0302 18:58:24.111334 22732373614720 run.py:483] Algo bellman_ford step 148 current loss 1.626736, current_train_items 4768.
I0302 18:58:24.141021 22732373614720 run.py:483] Algo bellman_ford step 149 current loss 1.519909, current_train_items 4800.
I0302 18:58:24.159736 22732373614720 run.py:483] Algo bellman_ford step 150 current loss 0.607609, current_train_items 4832.
I0302 18:58:24.167835 22732373614720 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.8564453125, 'score': 0.8564453125, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0302 18:58:24.167951 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.744, current avg val score is 0.856, val scores are: bellman_ford: 0.856
I0302 18:58:24.196882 22732373614720 run.py:483] Algo bellman_ford step 151 current loss 0.731676, current_train_items 4864.
I0302 18:58:24.219871 22732373614720 run.py:483] Algo bellman_ford step 152 current loss 1.191459, current_train_items 4896.
I0302 18:58:24.248381 22732373614720 run.py:483] Algo bellman_ford step 153 current loss 1.346330, current_train_items 4928.
I0302 18:58:24.283143 22732373614720 run.py:483] Algo bellman_ford step 154 current loss 1.874852, current_train_items 4960.
I0302 18:58:24.302233 22732373614720 run.py:483] Algo bellman_ford step 155 current loss 0.580744, current_train_items 4992.
I0302 18:58:24.318269 22732373614720 run.py:483] Algo bellman_ford step 156 current loss 0.970389, current_train_items 5024.
I0302 18:58:24.340594 22732373614720 run.py:483] Algo bellman_ford step 157 current loss 1.188405, current_train_items 5056.
I0302 18:58:24.367899 22732373614720 run.py:483] Algo bellman_ford step 158 current loss 1.315787, current_train_items 5088.
I0302 18:58:24.399225 22732373614720 run.py:483] Algo bellman_ford step 159 current loss 1.518531, current_train_items 5120.
I0302 18:58:24.417582 22732373614720 run.py:483] Algo bellman_ford step 160 current loss 0.585677, current_train_items 5152.
I0302 18:58:24.433643 22732373614720 run.py:483] Algo bellman_ford step 161 current loss 0.851868, current_train_items 5184.
I0302 18:58:24.455269 22732373614720 run.py:483] Algo bellman_ford step 162 current loss 1.183393, current_train_items 5216.
I0302 18:58:24.483571 22732373614720 run.py:483] Algo bellman_ford step 163 current loss 1.441084, current_train_items 5248.
I0302 18:58:24.513788 22732373614720 run.py:483] Algo bellman_ford step 164 current loss 1.486647, current_train_items 5280.
I0302 18:58:24.532458 22732373614720 run.py:483] Algo bellman_ford step 165 current loss 0.562164, current_train_items 5312.
I0302 18:58:24.548803 22732373614720 run.py:483] Algo bellman_ford step 166 current loss 1.030080, current_train_items 5344.
I0302 18:58:24.570713 22732373614720 run.py:483] Algo bellman_ford step 167 current loss 1.093391, current_train_items 5376.
I0302 18:58:24.600024 22732373614720 run.py:483] Algo bellman_ford step 168 current loss 1.373620, current_train_items 5408.
I0302 18:58:24.630692 22732373614720 run.py:483] Algo bellman_ford step 169 current loss 1.692727, current_train_items 5440.
I0302 18:58:24.649285 22732373614720 run.py:483] Algo bellman_ford step 170 current loss 0.721289, current_train_items 5472.
I0302 18:58:24.665318 22732373614720 run.py:483] Algo bellman_ford step 171 current loss 0.893970, current_train_items 5504.
I0302 18:58:24.686935 22732373614720 run.py:483] Algo bellman_ford step 172 current loss 1.201172, current_train_items 5536.
I0302 18:58:24.715834 22732373614720 run.py:483] Algo bellman_ford step 173 current loss 1.456771, current_train_items 5568.
I0302 18:58:24.749945 22732373614720 run.py:483] Algo bellman_ford step 174 current loss 1.704399, current_train_items 5600.
I0302 18:58:24.767852 22732373614720 run.py:483] Algo bellman_ford step 175 current loss 0.507754, current_train_items 5632.
I0302 18:58:24.783836 22732373614720 run.py:483] Algo bellman_ford step 176 current loss 0.899306, current_train_items 5664.
I0302 18:58:24.806896 22732373614720 run.py:483] Algo bellman_ford step 177 current loss 1.260751, current_train_items 5696.
I0302 18:58:24.833902 22732373614720 run.py:483] Algo bellman_ford step 178 current loss 1.145192, current_train_items 5728.
I0302 18:58:24.863570 22732373614720 run.py:483] Algo bellman_ford step 179 current loss 1.627002, current_train_items 5760.
I0302 18:58:24.882355 22732373614720 run.py:483] Algo bellman_ford step 180 current loss 0.676760, current_train_items 5792.
I0302 18:58:24.898459 22732373614720 run.py:483] Algo bellman_ford step 181 current loss 0.861956, current_train_items 5824.
I0302 18:58:24.921937 22732373614720 run.py:483] Algo bellman_ford step 182 current loss 1.277887, current_train_items 5856.
I0302 18:58:24.950331 22732373614720 run.py:483] Algo bellman_ford step 183 current loss 1.404785, current_train_items 5888.
I0302 18:58:24.979578 22732373614720 run.py:483] Algo bellman_ford step 184 current loss 2.152796, current_train_items 5920.
I0302 18:58:24.997660 22732373614720 run.py:483] Algo bellman_ford step 185 current loss 0.458624, current_train_items 5952.
I0302 18:58:25.013913 22732373614720 run.py:483] Algo bellman_ford step 186 current loss 0.930690, current_train_items 5984.
I0302 18:58:25.035663 22732373614720 run.py:483] Algo bellman_ford step 187 current loss 1.103688, current_train_items 6016.
I0302 18:58:25.063671 22732373614720 run.py:483] Algo bellman_ford step 188 current loss 1.546466, current_train_items 6048.
I0302 18:58:25.096211 22732373614720 run.py:483] Algo bellman_ford step 189 current loss 1.713900, current_train_items 6080.
I0302 18:58:25.114430 22732373614720 run.py:483] Algo bellman_ford step 190 current loss 0.499862, current_train_items 6112.
I0302 18:58:25.130555 22732373614720 run.py:483] Algo bellman_ford step 191 current loss 0.801529, current_train_items 6144.
I0302 18:58:25.153789 22732373614720 run.py:483] Algo bellman_ford step 192 current loss 1.365693, current_train_items 6176.
I0302 18:58:25.182147 22732373614720 run.py:483] Algo bellman_ford step 193 current loss 1.305918, current_train_items 6208.
I0302 18:58:25.212623 22732373614720 run.py:483] Algo bellman_ford step 194 current loss 1.555681, current_train_items 6240.
I0302 18:58:25.231259 22732373614720 run.py:483] Algo bellman_ford step 195 current loss 0.588404, current_train_items 6272.
I0302 18:58:25.247176 22732373614720 run.py:483] Algo bellman_ford step 196 current loss 0.923560, current_train_items 6304.
I0302 18:58:25.269401 22732373614720 run.py:483] Algo bellman_ford step 197 current loss 1.230857, current_train_items 6336.
I0302 18:58:25.298767 22732373614720 run.py:483] Algo bellman_ford step 198 current loss 1.624717, current_train_items 6368.
I0302 18:58:25.330376 22732373614720 run.py:483] Algo bellman_ford step 199 current loss 1.730145, current_train_items 6400.
I0302 18:58:25.348635 22732373614720 run.py:483] Algo bellman_ford step 200 current loss 0.537409, current_train_items 6432.
I0302 18:58:25.356548 22732373614720 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.8154296875, 'score': 0.8154296875, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0302 18:58:25.356656 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.856, current avg val score is 0.815, val scores are: bellman_ford: 0.815
I0302 18:58:25.373239 22732373614720 run.py:483] Algo bellman_ford step 201 current loss 0.806385, current_train_items 6464.
I0302 18:58:25.396541 22732373614720 run.py:483] Algo bellman_ford step 202 current loss 1.193141, current_train_items 6496.
I0302 18:58:25.426785 22732373614720 run.py:483] Algo bellman_ford step 203 current loss 1.440381, current_train_items 6528.
I0302 18:58:25.461139 22732373614720 run.py:483] Algo bellman_ford step 204 current loss 1.724245, current_train_items 6560.
I0302 18:58:25.479882 22732373614720 run.py:483] Algo bellman_ford step 205 current loss 0.560657, current_train_items 6592.
I0302 18:58:25.495011 22732373614720 run.py:483] Algo bellman_ford step 206 current loss 0.811950, current_train_items 6624.
I0302 18:58:25.518183 22732373614720 run.py:483] Algo bellman_ford step 207 current loss 1.306577, current_train_items 6656.
I0302 18:58:25.546936 22732373614720 run.py:483] Algo bellman_ford step 208 current loss 1.333420, current_train_items 6688.
I0302 18:58:25.577283 22732373614720 run.py:483] Algo bellman_ford step 209 current loss 1.493061, current_train_items 6720.
I0302 18:58:25.595536 22732373614720 run.py:483] Algo bellman_ford step 210 current loss 0.653413, current_train_items 6752.
I0302 18:58:25.611630 22732373614720 run.py:483] Algo bellman_ford step 211 current loss 0.808934, current_train_items 6784.
I0302 18:58:25.634738 22732373614720 run.py:483] Algo bellman_ford step 212 current loss 1.386799, current_train_items 6816.
I0302 18:58:25.663990 22732373614720 run.py:483] Algo bellman_ford step 213 current loss 1.392249, current_train_items 6848.
I0302 18:58:25.691457 22732373614720 run.py:483] Algo bellman_ford step 214 current loss 1.201240, current_train_items 6880.
I0302 18:58:25.709765 22732373614720 run.py:483] Algo bellman_ford step 215 current loss 0.501885, current_train_items 6912.
I0302 18:58:25.725581 22732373614720 run.py:483] Algo bellman_ford step 216 current loss 0.792329, current_train_items 6944.
I0302 18:58:25.747848 22732373614720 run.py:483] Algo bellman_ford step 217 current loss 1.400274, current_train_items 6976.
I0302 18:58:25.775990 22732373614720 run.py:483] Algo bellman_ford step 218 current loss 1.528106, current_train_items 7008.
I0302 18:58:25.806328 22732373614720 run.py:483] Algo bellman_ford step 219 current loss 1.586832, current_train_items 7040.
I0302 18:58:25.824583 22732373614720 run.py:483] Algo bellman_ford step 220 current loss 0.543224, current_train_items 7072.
I0302 18:58:25.840582 22732373614720 run.py:483] Algo bellman_ford step 221 current loss 0.952028, current_train_items 7104.
I0302 18:58:25.863719 22732373614720 run.py:483] Algo bellman_ford step 222 current loss 1.251813, current_train_items 7136.
I0302 18:58:25.891608 22732373614720 run.py:483] Algo bellman_ford step 223 current loss 1.359545, current_train_items 7168.
I0302 18:58:25.922711 22732373614720 run.py:483] Algo bellman_ford step 224 current loss 1.661256, current_train_items 7200.
I0302 18:58:25.940903 22732373614720 run.py:483] Algo bellman_ford step 225 current loss 0.533607, current_train_items 7232.
I0302 18:58:25.956986 22732373614720 run.py:483] Algo bellman_ford step 226 current loss 0.939387, current_train_items 7264.
I0302 18:58:25.979723 22732373614720 run.py:483] Algo bellman_ford step 227 current loss 1.345894, current_train_items 7296.
I0302 18:58:26.008551 22732373614720 run.py:483] Algo bellman_ford step 228 current loss 1.394361, current_train_items 7328.
I0302 18:58:26.040436 22732373614720 run.py:483] Algo bellman_ford step 229 current loss 1.819580, current_train_items 7360.
I0302 18:58:26.058511 22732373614720 run.py:483] Algo bellman_ford step 230 current loss 0.485236, current_train_items 7392.
I0302 18:58:26.074516 22732373614720 run.py:483] Algo bellman_ford step 231 current loss 0.850755, current_train_items 7424.
I0302 18:58:26.097506 22732373614720 run.py:483] Algo bellman_ford step 232 current loss 1.206651, current_train_items 7456.
I0302 18:58:26.126853 22732373614720 run.py:483] Algo bellman_ford step 233 current loss 1.467137, current_train_items 7488.
I0302 18:58:26.159260 22732373614720 run.py:483] Algo bellman_ford step 234 current loss 1.602166, current_train_items 7520.
I0302 18:58:26.177614 22732373614720 run.py:483] Algo bellman_ford step 235 current loss 0.645040, current_train_items 7552.
I0302 18:58:26.193271 22732373614720 run.py:483] Algo bellman_ford step 236 current loss 0.905620, current_train_items 7584.
I0302 18:58:26.215701 22732373614720 run.py:483] Algo bellman_ford step 237 current loss 1.170416, current_train_items 7616.
I0302 18:58:26.244329 22732373614720 run.py:483] Algo bellman_ford step 238 current loss 1.259636, current_train_items 7648.
I0302 18:58:26.275221 22732373614720 run.py:483] Algo bellman_ford step 239 current loss 1.502311, current_train_items 7680.
I0302 18:58:26.293313 22732373614720 run.py:483] Algo bellman_ford step 240 current loss 0.491476, current_train_items 7712.
I0302 18:58:26.309334 22732373614720 run.py:483] Algo bellman_ford step 241 current loss 1.018679, current_train_items 7744.
I0302 18:58:26.331877 22732373614720 run.py:483] Algo bellman_ford step 242 current loss 1.198311, current_train_items 7776.
I0302 18:58:26.360287 22732373614720 run.py:483] Algo bellman_ford step 243 current loss 1.500959, current_train_items 7808.
I0302 18:58:26.388679 22732373614720 run.py:483] Algo bellman_ford step 244 current loss 1.593393, current_train_items 7840.
I0302 18:58:26.406963 22732373614720 run.py:483] Algo bellman_ford step 245 current loss 0.535016, current_train_items 7872.
I0302 18:58:26.422740 22732373614720 run.py:483] Algo bellman_ford step 246 current loss 0.832095, current_train_items 7904.
I0302 18:58:26.444571 22732373614720 run.py:483] Algo bellman_ford step 247 current loss 1.215132, current_train_items 7936.
I0302 18:58:26.473835 22732373614720 run.py:483] Algo bellman_ford step 248 current loss 1.443485, current_train_items 7968.
I0302 18:58:26.504853 22732373614720 run.py:483] Algo bellman_ford step 249 current loss 1.294898, current_train_items 8000.
I0302 18:58:26.522992 22732373614720 run.py:483] Algo bellman_ford step 250 current loss 0.596940, current_train_items 8032.
I0302 18:58:26.531198 22732373614720 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.8125, 'score': 0.8125, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0302 18:58:26.531304 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.856, current avg val score is 0.812, val scores are: bellman_ford: 0.812
I0302 18:58:26.548230 22732373614720 run.py:483] Algo bellman_ford step 251 current loss 1.001051, current_train_items 8064.
I0302 18:58:26.571739 22732373614720 run.py:483] Algo bellman_ford step 252 current loss 1.189438, current_train_items 8096.
I0302 18:58:26.601151 22732373614720 run.py:483] Algo bellman_ford step 253 current loss 1.293218, current_train_items 8128.
I0302 18:58:26.633553 22732373614720 run.py:483] Algo bellman_ford step 254 current loss 1.518825, current_train_items 8160.
I0302 18:58:26.652828 22732373614720 run.py:483] Algo bellman_ford step 255 current loss 0.622312, current_train_items 8192.
I0302 18:58:26.668258 22732373614720 run.py:483] Algo bellman_ford step 256 current loss 0.865773, current_train_items 8224.
I0302 18:58:26.690094 22732373614720 run.py:483] Algo bellman_ford step 257 current loss 1.012639, current_train_items 8256.
I0302 18:58:26.718548 22732373614720 run.py:483] Algo bellman_ford step 258 current loss 1.354656, current_train_items 8288.
I0302 18:58:26.748269 22732373614720 run.py:483] Algo bellman_ford step 259 current loss 1.625946, current_train_items 8320.
I0302 18:58:26.766996 22732373614720 run.py:483] Algo bellman_ford step 260 current loss 0.534222, current_train_items 8352.
I0302 18:58:26.782900 22732373614720 run.py:483] Algo bellman_ford step 261 current loss 0.887200, current_train_items 8384.
I0302 18:58:26.804766 22732373614720 run.py:483] Algo bellman_ford step 262 current loss 1.177216, current_train_items 8416.
I0302 18:58:26.833075 22732373614720 run.py:483] Algo bellman_ford step 263 current loss 1.237109, current_train_items 8448.
I0302 18:58:26.862898 22732373614720 run.py:483] Algo bellman_ford step 264 current loss 1.304566, current_train_items 8480.
I0302 18:58:26.881044 22732373614720 run.py:483] Algo bellman_ford step 265 current loss 0.459847, current_train_items 8512.
I0302 18:58:26.897205 22732373614720 run.py:483] Algo bellman_ford step 266 current loss 0.881984, current_train_items 8544.
I0302 18:58:26.920606 22732373614720 run.py:483] Algo bellman_ford step 267 current loss 1.249759, current_train_items 8576.
I0302 18:58:26.950223 22732373614720 run.py:483] Algo bellman_ford step 268 current loss 1.397739, current_train_items 8608.
I0302 18:58:26.979956 22732373614720 run.py:483] Algo bellman_ford step 269 current loss 1.422617, current_train_items 8640.
I0302 18:58:26.998799 22732373614720 run.py:483] Algo bellman_ford step 270 current loss 0.515787, current_train_items 8672.
I0302 18:58:27.014303 22732373614720 run.py:483] Algo bellman_ford step 271 current loss 0.745590, current_train_items 8704.
I0302 18:58:27.037540 22732373614720 run.py:483] Algo bellman_ford step 272 current loss 1.064460, current_train_items 8736.
I0302 18:58:27.066031 22732373614720 run.py:483] Algo bellman_ford step 273 current loss 1.323846, current_train_items 8768.
I0302 18:58:27.095835 22732373614720 run.py:483] Algo bellman_ford step 274 current loss 1.542512, current_train_items 8800.
I0302 18:58:27.114541 22732373614720 run.py:483] Algo bellman_ford step 275 current loss 0.565773, current_train_items 8832.
I0302 18:58:27.130550 22732373614720 run.py:483] Algo bellman_ford step 276 current loss 0.743660, current_train_items 8864.
I0302 18:58:27.154020 22732373614720 run.py:483] Algo bellman_ford step 277 current loss 1.339408, current_train_items 8896.
I0302 18:58:27.183447 22732373614720 run.py:483] Algo bellman_ford step 278 current loss 1.504946, current_train_items 8928.
I0302 18:58:27.213912 22732373614720 run.py:483] Algo bellman_ford step 279 current loss 1.309375, current_train_items 8960.
I0302 18:58:27.232203 22732373614720 run.py:483] Algo bellman_ford step 280 current loss 0.456346, current_train_items 8992.
I0302 18:58:27.248371 22732373614720 run.py:483] Algo bellman_ford step 281 current loss 0.972422, current_train_items 9024.
I0302 18:58:27.271389 22732373614720 run.py:483] Algo bellman_ford step 282 current loss 1.105723, current_train_items 9056.
I0302 18:58:27.299581 22732373614720 run.py:483] Algo bellman_ford step 283 current loss 1.384343, current_train_items 9088.
I0302 18:58:27.331806 22732373614720 run.py:483] Algo bellman_ford step 284 current loss 2.129690, current_train_items 9120.
I0302 18:58:27.350187 22732373614720 run.py:483] Algo bellman_ford step 285 current loss 0.526499, current_train_items 9152.
I0302 18:58:27.366318 22732373614720 run.py:483] Algo bellman_ford step 286 current loss 0.865398, current_train_items 9184.
I0302 18:58:27.388374 22732373614720 run.py:483] Algo bellman_ford step 287 current loss 1.115152, current_train_items 9216.
I0302 18:58:27.417780 22732373614720 run.py:483] Algo bellman_ford step 288 current loss 1.402676, current_train_items 9248.
I0302 18:58:27.449415 22732373614720 run.py:483] Algo bellman_ford step 289 current loss 1.655400, current_train_items 9280.
I0302 18:58:27.467896 22732373614720 run.py:483] Algo bellman_ford step 290 current loss 0.497929, current_train_items 9312.
I0302 18:58:27.483907 22732373614720 run.py:483] Algo bellman_ford step 291 current loss 0.855241, current_train_items 9344.
I0302 18:58:27.506043 22732373614720 run.py:483] Algo bellman_ford step 292 current loss 1.035605, current_train_items 9376.
I0302 18:58:27.534147 22732373614720 run.py:483] Algo bellman_ford step 293 current loss 1.298544, current_train_items 9408.
I0302 18:58:27.564643 22732373614720 run.py:483] Algo bellman_ford step 294 current loss 1.423326, current_train_items 9440.
I0302 18:58:27.583044 22732373614720 run.py:483] Algo bellman_ford step 295 current loss 0.506849, current_train_items 9472.
I0302 18:58:27.598675 22732373614720 run.py:483] Algo bellman_ford step 296 current loss 0.806196, current_train_items 9504.
I0302 18:58:27.621690 22732373614720 run.py:483] Algo bellman_ford step 297 current loss 1.070213, current_train_items 9536.
I0302 18:58:27.651601 22732373614720 run.py:483] Algo bellman_ford step 298 current loss 1.241352, current_train_items 9568.
I0302 18:58:27.681769 22732373614720 run.py:483] Algo bellman_ford step 299 current loss 1.313208, current_train_items 9600.
I0302 18:58:27.700356 22732373614720 run.py:483] Algo bellman_ford step 300 current loss 0.613987, current_train_items 9632.
I0302 18:58:27.708276 22732373614720 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.8388671875, 'score': 0.8388671875, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0302 18:58:27.708386 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.856, current avg val score is 0.839, val scores are: bellman_ford: 0.839
I0302 18:58:27.724945 22732373614720 run.py:483] Algo bellman_ford step 301 current loss 0.860566, current_train_items 9664.
I0302 18:58:27.747237 22732373614720 run.py:483] Algo bellman_ford step 302 current loss 1.135770, current_train_items 9696.
I0302 18:58:27.775315 22732373614720 run.py:483] Algo bellman_ford step 303 current loss 1.132566, current_train_items 9728.
I0302 18:58:27.807614 22732373614720 run.py:483] Algo bellman_ford step 304 current loss 1.487541, current_train_items 9760.
I0302 18:58:27.826283 22732373614720 run.py:483] Algo bellman_ford step 305 current loss 0.547151, current_train_items 9792.
I0302 18:58:27.842530 22732373614720 run.py:483] Algo bellman_ford step 306 current loss 1.111785, current_train_items 9824.
I0302 18:58:27.865983 22732373614720 run.py:483] Algo bellman_ford step 307 current loss 1.254042, current_train_items 9856.
I0302 18:58:27.895112 22732373614720 run.py:483] Algo bellman_ford step 308 current loss 1.258816, current_train_items 9888.
I0302 18:58:27.926018 22732373614720 run.py:483] Algo bellman_ford step 309 current loss 1.322155, current_train_items 9920.
I0302 18:58:27.944480 22732373614720 run.py:483] Algo bellman_ford step 310 current loss 0.593007, current_train_items 9952.
I0302 18:58:27.960143 22732373614720 run.py:483] Algo bellman_ford step 311 current loss 0.785174, current_train_items 9984.
I0302 18:58:27.982611 22732373614720 run.py:483] Algo bellman_ford step 312 current loss 1.120435, current_train_items 10016.
I0302 18:58:28.011906 22732373614720 run.py:483] Algo bellman_ford step 313 current loss 1.335322, current_train_items 10048.
I0302 18:58:28.042785 22732373614720 run.py:483] Algo bellman_ford step 314 current loss 1.461065, current_train_items 10080.
I0302 18:58:28.060850 22732373614720 run.py:483] Algo bellman_ford step 315 current loss 0.584638, current_train_items 10112.
I0302 18:58:28.076934 22732373614720 run.py:483] Algo bellman_ford step 316 current loss 0.843203, current_train_items 10144.
I0302 18:58:28.098880 22732373614720 run.py:483] Algo bellman_ford step 317 current loss 0.936674, current_train_items 10176.
I0302 18:58:28.126291 22732373614720 run.py:483] Algo bellman_ford step 318 current loss 1.178644, current_train_items 10208.
I0302 18:58:28.156198 22732373614720 run.py:483] Algo bellman_ford step 319 current loss 1.570169, current_train_items 10240.
I0302 18:58:28.174391 22732373614720 run.py:483] Algo bellman_ford step 320 current loss 0.471074, current_train_items 10272.
I0302 18:58:28.190390 22732373614720 run.py:483] Algo bellman_ford step 321 current loss 1.012771, current_train_items 10304.
I0302 18:58:28.213132 22732373614720 run.py:483] Algo bellman_ford step 322 current loss 1.118468, current_train_items 10336.
I0302 18:58:28.240757 22732373614720 run.py:483] Algo bellman_ford step 323 current loss 1.070828, current_train_items 10368.
I0302 18:58:28.272274 22732373614720 run.py:483] Algo bellman_ford step 324 current loss 1.484847, current_train_items 10400.
I0302 18:58:28.290554 22732373614720 run.py:483] Algo bellman_ford step 325 current loss 0.559688, current_train_items 10432.
I0302 18:58:28.306125 22732373614720 run.py:483] Algo bellman_ford step 326 current loss 0.951094, current_train_items 10464.
I0302 18:58:28.328585 22732373614720 run.py:483] Algo bellman_ford step 327 current loss 1.271640, current_train_items 10496.
I0302 18:58:28.357697 22732373614720 run.py:483] Algo bellman_ford step 328 current loss 1.643623, current_train_items 10528.
I0302 18:58:28.389202 22732373614720 run.py:483] Algo bellman_ford step 329 current loss 1.524453, current_train_items 10560.
I0302 18:58:28.407735 22732373614720 run.py:483] Algo bellman_ford step 330 current loss 0.532752, current_train_items 10592.
I0302 18:58:28.423490 22732373614720 run.py:483] Algo bellman_ford step 331 current loss 0.912741, current_train_items 10624.
I0302 18:58:28.446001 22732373614720 run.py:483] Algo bellman_ford step 332 current loss 1.187553, current_train_items 10656.
I0302 18:58:28.475000 22732373614720 run.py:483] Algo bellman_ford step 333 current loss 1.290299, current_train_items 10688.
I0302 18:58:28.507028 22732373614720 run.py:483] Algo bellman_ford step 334 current loss 1.668626, current_train_items 10720.
I0302 18:58:28.525182 22732373614720 run.py:483] Algo bellman_ford step 335 current loss 0.515506, current_train_items 10752.
I0302 18:58:28.541602 22732373614720 run.py:483] Algo bellman_ford step 336 current loss 0.931815, current_train_items 10784.
I0302 18:58:28.565312 22732373614720 run.py:483] Algo bellman_ford step 337 current loss 1.266594, current_train_items 10816.
I0302 18:58:28.593968 22732373614720 run.py:483] Algo bellman_ford step 338 current loss 1.270957, current_train_items 10848.
I0302 18:58:28.626346 22732373614720 run.py:483] Algo bellman_ford step 339 current loss 1.426358, current_train_items 10880.
I0302 18:58:28.644690 22732373614720 run.py:483] Algo bellman_ford step 340 current loss 0.525906, current_train_items 10912.
I0302 18:58:28.660448 22732373614720 run.py:483] Algo bellman_ford step 341 current loss 0.895169, current_train_items 10944.
I0302 18:58:28.682814 22732373614720 run.py:483] Algo bellman_ford step 342 current loss 1.206198, current_train_items 10976.
I0302 18:58:28.710599 22732373614720 run.py:483] Algo bellman_ford step 343 current loss 1.187257, current_train_items 11008.
I0302 18:58:28.742377 22732373614720 run.py:483] Algo bellman_ford step 344 current loss 1.460882, current_train_items 11040.
I0302 18:58:28.760701 22732373614720 run.py:483] Algo bellman_ford step 345 current loss 0.600948, current_train_items 11072.
I0302 18:58:28.776558 22732373614720 run.py:483] Algo bellman_ford step 346 current loss 0.887560, current_train_items 11104.
I0302 18:58:28.800318 22732373614720 run.py:483] Algo bellman_ford step 347 current loss 1.273594, current_train_items 11136.
I0302 18:58:28.828469 22732373614720 run.py:483] Algo bellman_ford step 348 current loss 1.259287, current_train_items 11168.
I0302 18:58:28.860126 22732373614720 run.py:483] Algo bellman_ford step 349 current loss 1.621722, current_train_items 11200.
I0302 18:58:28.878525 22732373614720 run.py:483] Algo bellman_ford step 350 current loss 0.589274, current_train_items 11232.
I0302 18:58:28.886509 22732373614720 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.8154296875, 'score': 0.8154296875, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0302 18:58:28.886618 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.856, current avg val score is 0.815, val scores are: bellman_ford: 0.815
I0302 18:58:28.902965 22732373614720 run.py:483] Algo bellman_ford step 351 current loss 0.808618, current_train_items 11264.
I0302 18:58:28.926616 22732373614720 run.py:483] Algo bellman_ford step 352 current loss 1.214396, current_train_items 11296.
I0302 18:58:28.956080 22732373614720 run.py:483] Algo bellman_ford step 353 current loss 1.512969, current_train_items 11328.
I0302 18:58:28.989335 22732373614720 run.py:483] Algo bellman_ford step 354 current loss 1.647152, current_train_items 11360.
I0302 18:58:29.007959 22732373614720 run.py:483] Algo bellman_ford step 355 current loss 0.520486, current_train_items 11392.
I0302 18:58:29.023557 22732373614720 run.py:483] Algo bellman_ford step 356 current loss 0.855011, current_train_items 11424.
I0302 18:58:29.045682 22732373614720 run.py:483] Algo bellman_ford step 357 current loss 1.099285, current_train_items 11456.
I0302 18:58:29.075899 22732373614720 run.py:483] Algo bellman_ford step 358 current loss 1.510632, current_train_items 11488.
I0302 18:58:29.108344 22732373614720 run.py:483] Algo bellman_ford step 359 current loss 1.751510, current_train_items 11520.
I0302 18:58:29.127163 22732373614720 run.py:483] Algo bellman_ford step 360 current loss 0.471729, current_train_items 11552.
I0302 18:58:29.143444 22732373614720 run.py:483] Algo bellman_ford step 361 current loss 0.807310, current_train_items 11584.
I0302 18:58:29.164868 22732373614720 run.py:483] Algo bellman_ford step 362 current loss 1.002079, current_train_items 11616.
I0302 18:58:29.193353 22732373614720 run.py:483] Algo bellman_ford step 363 current loss 1.320169, current_train_items 11648.
I0302 18:58:29.222785 22732373614720 run.py:483] Algo bellman_ford step 364 current loss 1.285969, current_train_items 11680.
I0302 18:58:29.241058 22732373614720 run.py:483] Algo bellman_ford step 365 current loss 0.496883, current_train_items 11712.
I0302 18:58:29.257016 22732373614720 run.py:483] Algo bellman_ford step 366 current loss 0.937012, current_train_items 11744.
I0302 18:58:29.278925 22732373614720 run.py:483] Algo bellman_ford step 367 current loss 0.960996, current_train_items 11776.
I0302 18:58:29.308609 22732373614720 run.py:483] Algo bellman_ford step 368 current loss 1.453193, current_train_items 11808.
I0302 18:58:29.337648 22732373614720 run.py:483] Algo bellman_ford step 369 current loss 1.649534, current_train_items 11840.
I0302 18:58:29.356017 22732373614720 run.py:483] Algo bellman_ford step 370 current loss 0.455820, current_train_items 11872.
I0302 18:58:29.371773 22732373614720 run.py:483] Algo bellman_ford step 371 current loss 0.796752, current_train_items 11904.
I0302 18:58:29.394172 22732373614720 run.py:483] Algo bellman_ford step 372 current loss 1.104788, current_train_items 11936.
I0302 18:58:29.423867 22732373614720 run.py:483] Algo bellman_ford step 373 current loss 1.255513, current_train_items 11968.
I0302 18:58:29.456784 22732373614720 run.py:483] Algo bellman_ford step 374 current loss 1.666407, current_train_items 12000.
I0302 18:58:29.475243 22732373614720 run.py:483] Algo bellman_ford step 375 current loss 0.524892, current_train_items 12032.
I0302 18:58:29.490889 22732373614720 run.py:483] Algo bellman_ford step 376 current loss 0.793111, current_train_items 12064.
I0302 18:58:29.514139 22732373614720 run.py:483] Algo bellman_ford step 377 current loss 1.131094, current_train_items 12096.
I0302 18:58:29.542618 22732373614720 run.py:483] Algo bellman_ford step 378 current loss 1.128996, current_train_items 12128.
I0302 18:58:29.574792 22732373614720 run.py:483] Algo bellman_ford step 379 current loss 1.981362, current_train_items 12160.
I0302 18:58:29.593046 22732373614720 run.py:483] Algo bellman_ford step 380 current loss 0.498063, current_train_items 12192.
I0302 18:58:29.609241 22732373614720 run.py:483] Algo bellman_ford step 381 current loss 0.892547, current_train_items 12224.
I0302 18:58:29.632079 22732373614720 run.py:483] Algo bellman_ford step 382 current loss 1.101418, current_train_items 12256.
I0302 18:58:29.661024 22732373614720 run.py:483] Algo bellman_ford step 383 current loss 1.312376, current_train_items 12288.
I0302 18:58:29.693030 22732373614720 run.py:483] Algo bellman_ford step 384 current loss 1.472797, current_train_items 12320.
I0302 18:58:29.711839 22732373614720 run.py:483] Algo bellman_ford step 385 current loss 0.599953, current_train_items 12352.
I0302 18:58:29.728056 22732373614720 run.py:483] Algo bellman_ford step 386 current loss 0.739796, current_train_items 12384.
I0302 18:58:29.750163 22732373614720 run.py:483] Algo bellman_ford step 387 current loss 1.282228, current_train_items 12416.
I0302 18:58:29.778404 22732373614720 run.py:483] Algo bellman_ford step 388 current loss 1.349004, current_train_items 12448.
I0302 18:58:29.810117 22732373614720 run.py:483] Algo bellman_ford step 389 current loss 1.870026, current_train_items 12480.
I0302 18:58:29.828640 22732373614720 run.py:483] Algo bellman_ford step 390 current loss 0.483377, current_train_items 12512.
I0302 18:58:29.844424 22732373614720 run.py:483] Algo bellman_ford step 391 current loss 0.822020, current_train_items 12544.
I0302 18:58:29.866907 22732373614720 run.py:483] Algo bellman_ford step 392 current loss 1.151775, current_train_items 12576.
I0302 18:58:29.897392 22732373614720 run.py:483] Algo bellman_ford step 393 current loss 1.556337, current_train_items 12608.
I0302 18:58:29.928380 22732373614720 run.py:483] Algo bellman_ford step 394 current loss 1.517969, current_train_items 12640.
I0302 18:58:29.946662 22732373614720 run.py:483] Algo bellman_ford step 395 current loss 0.448813, current_train_items 12672.
I0302 18:58:29.962685 22732373614720 run.py:483] Algo bellman_ford step 396 current loss 0.907540, current_train_items 12704.
I0302 18:58:29.985720 22732373614720 run.py:483] Algo bellman_ford step 397 current loss 1.510917, current_train_items 12736.
I0302 18:58:30.015471 22732373614720 run.py:483] Algo bellman_ford step 398 current loss 1.527704, current_train_items 12768.
I0302 18:58:30.044802 22732373614720 run.py:483] Algo bellman_ford step 399 current loss 1.490091, current_train_items 12800.
I0302 18:58:30.063431 22732373614720 run.py:483] Algo bellman_ford step 400 current loss 0.428323, current_train_items 12832.
I0302 18:58:30.071185 22732373614720 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.8349609375, 'score': 0.8349609375, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0302 18:58:30.071295 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.856, current avg val score is 0.835, val scores are: bellman_ford: 0.835
I0302 18:58:30.088076 22732373614720 run.py:483] Algo bellman_ford step 401 current loss 0.861329, current_train_items 12864.
I0302 18:58:30.111377 22732373614720 run.py:483] Algo bellman_ford step 402 current loss 1.054874, current_train_items 12896.
I0302 18:58:30.140861 22732373614720 run.py:483] Algo bellman_ford step 403 current loss 1.235794, current_train_items 12928.
I0302 18:58:30.170979 22732373614720 run.py:483] Algo bellman_ford step 404 current loss 1.502168, current_train_items 12960.
I0302 18:58:30.189606 22732373614720 run.py:483] Algo bellman_ford step 405 current loss 0.657595, current_train_items 12992.
I0302 18:58:30.204713 22732373614720 run.py:483] Algo bellman_ford step 406 current loss 0.713441, current_train_items 13024.
I0302 18:58:30.227282 22732373614720 run.py:483] Algo bellman_ford step 407 current loss 1.101990, current_train_items 13056.
I0302 18:58:30.257308 22732373614720 run.py:483] Algo bellman_ford step 408 current loss 1.230736, current_train_items 13088.
I0302 18:58:30.288305 22732373614720 run.py:483] Algo bellman_ford step 409 current loss 1.511904, current_train_items 13120.
I0302 18:58:30.306520 22732373614720 run.py:483] Algo bellman_ford step 410 current loss 0.498085, current_train_items 13152.
I0302 18:58:30.322129 22732373614720 run.py:483] Algo bellman_ford step 411 current loss 0.753544, current_train_items 13184.
I0302 18:58:30.344802 22732373614720 run.py:483] Algo bellman_ford step 412 current loss 1.109045, current_train_items 13216.
I0302 18:58:30.373663 22732373614720 run.py:483] Algo bellman_ford step 413 current loss 1.205672, current_train_items 13248.
I0302 18:58:30.405306 22732373614720 run.py:483] Algo bellman_ford step 414 current loss 1.427546, current_train_items 13280.
I0302 18:58:30.423560 22732373614720 run.py:483] Algo bellman_ford step 415 current loss 0.515733, current_train_items 13312.
I0302 18:58:30.439281 22732373614720 run.py:483] Algo bellman_ford step 416 current loss 0.795547, current_train_items 13344.
I0302 18:58:30.462245 22732373614720 run.py:483] Algo bellman_ford step 417 current loss 1.175916, current_train_items 13376.
I0302 18:58:30.492951 22732373614720 run.py:483] Algo bellman_ford step 418 current loss 1.380102, current_train_items 13408.
I0302 18:58:30.524095 22732373614720 run.py:483] Algo bellman_ford step 419 current loss 1.506079, current_train_items 13440.
I0302 18:58:30.542390 22732373614720 run.py:483] Algo bellman_ford step 420 current loss 0.584588, current_train_items 13472.
I0302 18:58:30.558196 22732373614720 run.py:483] Algo bellman_ford step 421 current loss 0.810116, current_train_items 13504.
I0302 18:58:30.581422 22732373614720 run.py:483] Algo bellman_ford step 422 current loss 1.195004, current_train_items 13536.
I0302 18:58:30.609391 22732373614720 run.py:483] Algo bellman_ford step 423 current loss 1.132010, current_train_items 13568.
I0302 18:58:30.640450 22732373614720 run.py:483] Algo bellman_ford step 424 current loss 1.327486, current_train_items 13600.
I0302 18:58:30.658674 22732373614720 run.py:483] Algo bellman_ford step 425 current loss 0.527809, current_train_items 13632.
I0302 18:58:30.674307 22732373614720 run.py:483] Algo bellman_ford step 426 current loss 0.567881, current_train_items 13664.
I0302 18:58:30.696421 22732373614720 run.py:483] Algo bellman_ford step 427 current loss 1.089378, current_train_items 13696.
I0302 18:58:30.724687 22732373614720 run.py:483] Algo bellman_ford step 428 current loss 1.135236, current_train_items 13728.
I0302 18:58:30.755767 22732373614720 run.py:483] Algo bellman_ford step 429 current loss 1.353692, current_train_items 13760.
I0302 18:58:30.773887 22732373614720 run.py:483] Algo bellman_ford step 430 current loss 0.533557, current_train_items 13792.
I0302 18:58:30.789771 22732373614720 run.py:483] Algo bellman_ford step 431 current loss 0.898901, current_train_items 13824.
I0302 18:58:30.812883 22732373614720 run.py:483] Algo bellman_ford step 432 current loss 1.467245, current_train_items 13856.
I0302 18:58:30.840640 22732373614720 run.py:483] Algo bellman_ford step 433 current loss 1.333912, current_train_items 13888.
I0302 18:58:30.871673 22732373614720 run.py:483] Algo bellman_ford step 434 current loss 1.720527, current_train_items 13920.
I0302 18:58:30.889667 22732373614720 run.py:483] Algo bellman_ford step 435 current loss 0.456221, current_train_items 13952.
I0302 18:58:30.905849 22732373614720 run.py:483] Algo bellman_ford step 436 current loss 0.790701, current_train_items 13984.
I0302 18:58:30.927835 22732373614720 run.py:483] Algo bellman_ford step 437 current loss 1.139758, current_train_items 14016.
I0302 18:58:30.955817 22732373614720 run.py:483] Algo bellman_ford step 438 current loss 1.175446, current_train_items 14048.
I0302 18:58:30.987030 22732373614720 run.py:483] Algo bellman_ford step 439 current loss 1.252389, current_train_items 14080.
I0302 18:58:31.004955 22732373614720 run.py:483] Algo bellman_ford step 440 current loss 0.465620, current_train_items 14112.
I0302 18:58:31.020547 22732373614720 run.py:483] Algo bellman_ford step 441 current loss 0.759623, current_train_items 14144.
I0302 18:58:31.042551 22732373614720 run.py:483] Algo bellman_ford step 442 current loss 1.493520, current_train_items 14176.
I0302 18:58:31.071002 22732373614720 run.py:483] Algo bellman_ford step 443 current loss 1.449926, current_train_items 14208.
I0302 18:58:31.101218 22732373614720 run.py:483] Algo bellman_ford step 444 current loss 1.492485, current_train_items 14240.
I0302 18:58:31.119457 22732373614720 run.py:483] Algo bellman_ford step 445 current loss 0.472894, current_train_items 14272.
I0302 18:58:31.136142 22732373614720 run.py:483] Algo bellman_ford step 446 current loss 1.155543, current_train_items 14304.
I0302 18:58:31.159240 22732373614720 run.py:483] Algo bellman_ford step 447 current loss 1.207391, current_train_items 14336.
I0302 18:58:31.186514 22732373614720 run.py:483] Algo bellman_ford step 448 current loss 1.297083, current_train_items 14368.
I0302 18:58:31.215729 22732373614720 run.py:483] Algo bellman_ford step 449 current loss 1.354536, current_train_items 14400.
I0302 18:58:31.234041 22732373614720 run.py:483] Algo bellman_ford step 450 current loss 0.582814, current_train_items 14432.
I0302 18:58:31.242110 22732373614720 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.837890625, 'score': 0.837890625, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0302 18:58:31.242225 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.856, current avg val score is 0.838, val scores are: bellman_ford: 0.838
I0302 18:58:31.259007 22732373614720 run.py:483] Algo bellman_ford step 451 current loss 0.823603, current_train_items 14464.
I0302 18:58:31.282025 22732373614720 run.py:483] Algo bellman_ford step 452 current loss 1.003679, current_train_items 14496.
I0302 18:58:31.312577 22732373614720 run.py:483] Algo bellman_ford step 453 current loss 1.358866, current_train_items 14528.
I0302 18:58:31.344269 22732373614720 run.py:483] Algo bellman_ford step 454 current loss 1.429209, current_train_items 14560.
I0302 18:58:31.363110 22732373614720 run.py:483] Algo bellman_ford step 455 current loss 0.573753, current_train_items 14592.
I0302 18:58:31.378947 22732373614720 run.py:483] Algo bellman_ford step 456 current loss 0.762807, current_train_items 14624.
I0302 18:58:31.401333 22732373614720 run.py:483] Algo bellman_ford step 457 current loss 1.109816, current_train_items 14656.
I0302 18:58:31.429807 22732373614720 run.py:483] Algo bellman_ford step 458 current loss 1.104535, current_train_items 14688.
I0302 18:58:31.460191 22732373614720 run.py:483] Algo bellman_ford step 459 current loss 1.439261, current_train_items 14720.
I0302 18:58:31.478928 22732373614720 run.py:483] Algo bellman_ford step 460 current loss 0.401953, current_train_items 14752.
I0302 18:58:31.495129 22732373614720 run.py:483] Algo bellman_ford step 461 current loss 0.893695, current_train_items 14784.
I0302 18:58:31.518165 22732373614720 run.py:483] Algo bellman_ford step 462 current loss 1.020684, current_train_items 14816.
I0302 18:58:31.547397 22732373614720 run.py:483] Algo bellman_ford step 463 current loss 1.286716, current_train_items 14848.
I0302 18:58:31.579861 22732373614720 run.py:483] Algo bellman_ford step 464 current loss 1.478816, current_train_items 14880.
I0302 18:58:31.598215 22732373614720 run.py:483] Algo bellman_ford step 465 current loss 0.502980, current_train_items 14912.
I0302 18:58:31.613691 22732373614720 run.py:483] Algo bellman_ford step 466 current loss 0.657585, current_train_items 14944.
I0302 18:58:31.636905 22732373614720 run.py:483] Algo bellman_ford step 467 current loss 1.140292, current_train_items 14976.
I0302 18:58:31.664900 22732373614720 run.py:483] Algo bellman_ford step 468 current loss 1.271973, current_train_items 15008.
I0302 18:58:31.697421 22732373614720 run.py:483] Algo bellman_ford step 469 current loss 1.352586, current_train_items 15040.
I0302 18:58:31.716242 22732373614720 run.py:483] Algo bellman_ford step 470 current loss 0.486013, current_train_items 15072.
I0302 18:58:31.732146 22732373614720 run.py:483] Algo bellman_ford step 471 current loss 0.786973, current_train_items 15104.
I0302 18:58:31.755083 22732373614720 run.py:483] Algo bellman_ford step 472 current loss 1.204656, current_train_items 15136.
I0302 18:58:31.784301 22732373614720 run.py:483] Algo bellman_ford step 473 current loss 1.317520, current_train_items 15168.
I0302 18:58:31.816211 22732373614720 run.py:483] Algo bellman_ford step 474 current loss 1.769617, current_train_items 15200.
I0302 18:58:31.835090 22732373614720 run.py:483] Algo bellman_ford step 475 current loss 0.575495, current_train_items 15232.
I0302 18:58:31.850784 22732373614720 run.py:483] Algo bellman_ford step 476 current loss 0.769086, current_train_items 15264.
I0302 18:58:31.874145 22732373614720 run.py:483] Algo bellman_ford step 477 current loss 1.060515, current_train_items 15296.
I0302 18:58:31.901764 22732373614720 run.py:483] Algo bellman_ford step 478 current loss 1.212080, current_train_items 15328.
I0302 18:58:31.934138 22732373614720 run.py:483] Algo bellman_ford step 479 current loss 1.647126, current_train_items 15360.
I0302 18:58:31.952581 22732373614720 run.py:483] Algo bellman_ford step 480 current loss 0.481958, current_train_items 15392.
I0302 18:58:31.968517 22732373614720 run.py:483] Algo bellman_ford step 481 current loss 0.810449, current_train_items 15424.
I0302 18:58:31.990454 22732373614720 run.py:483] Algo bellman_ford step 482 current loss 0.976214, current_train_items 15456.
I0302 18:58:32.019416 22732373614720 run.py:483] Algo bellman_ford step 483 current loss 1.147624, current_train_items 15488.
I0302 18:58:32.051080 22732373614720 run.py:483] Algo bellman_ford step 484 current loss 1.377607, current_train_items 15520.
I0302 18:58:32.069923 22732373614720 run.py:483] Algo bellman_ford step 485 current loss 0.514944, current_train_items 15552.
I0302 18:58:32.085896 22732373614720 run.py:483] Algo bellman_ford step 486 current loss 0.816060, current_train_items 15584.
I0302 18:58:32.108572 22732373614720 run.py:483] Algo bellman_ford step 487 current loss 1.120478, current_train_items 15616.
I0302 18:58:32.137202 22732373614720 run.py:483] Algo bellman_ford step 488 current loss 1.152328, current_train_items 15648.
I0302 18:58:32.167679 22732373614720 run.py:483] Algo bellman_ford step 489 current loss 1.338772, current_train_items 15680.
I0302 18:58:32.186314 22732373614720 run.py:483] Algo bellman_ford step 490 current loss 0.428739, current_train_items 15712.
I0302 18:58:32.202466 22732373614720 run.py:483] Algo bellman_ford step 491 current loss 0.733865, current_train_items 15744.
I0302 18:58:32.224982 22732373614720 run.py:483] Algo bellman_ford step 492 current loss 1.015360, current_train_items 15776.
I0302 18:58:32.253445 22732373614720 run.py:483] Algo bellman_ford step 493 current loss 1.185530, current_train_items 15808.
I0302 18:58:32.286139 22732373614720 run.py:483] Algo bellman_ford step 494 current loss 1.487598, current_train_items 15840.
I0302 18:58:32.304426 22732373614720 run.py:483] Algo bellman_ford step 495 current loss 0.430897, current_train_items 15872.
I0302 18:58:32.320180 22732373614720 run.py:483] Algo bellman_ford step 496 current loss 0.700565, current_train_items 15904.
I0302 18:58:32.343979 22732373614720 run.py:483] Algo bellman_ford step 497 current loss 1.247067, current_train_items 15936.
I0302 18:58:32.373774 22732373614720 run.py:483] Algo bellman_ford step 498 current loss 1.188335, current_train_items 15968.
I0302 18:58:32.405185 22732373614720 run.py:483] Algo bellman_ford step 499 current loss 1.438951, current_train_items 16000.
I0302 18:58:32.423945 22732373614720 run.py:483] Algo bellman_ford step 500 current loss 0.471801, current_train_items 16032.
I0302 18:58:32.431709 22732373614720 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.830078125, 'score': 0.830078125, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0302 18:58:32.431819 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.856, current avg val score is 0.830, val scores are: bellman_ford: 0.830
I0302 18:58:32.447763 22732373614720 run.py:483] Algo bellman_ford step 501 current loss 0.640842, current_train_items 16064.
I0302 18:58:32.471064 22732373614720 run.py:483] Algo bellman_ford step 502 current loss 1.085409, current_train_items 16096.
I0302 18:58:32.501822 22732373614720 run.py:483] Algo bellman_ford step 503 current loss 1.302033, current_train_items 16128.
I0302 18:58:32.535641 22732373614720 run.py:483] Algo bellman_ford step 504 current loss 1.612244, current_train_items 16160.
I0302 18:58:32.554557 22732373614720 run.py:483] Algo bellman_ford step 505 current loss 0.504793, current_train_items 16192.
I0302 18:58:32.569725 22732373614720 run.py:483] Algo bellman_ford step 506 current loss 0.698845, current_train_items 16224.
I0302 18:58:32.591980 22732373614720 run.py:483] Algo bellman_ford step 507 current loss 0.997147, current_train_items 16256.
I0302 18:58:32.621707 22732373614720 run.py:483] Algo bellman_ford step 508 current loss 1.397494, current_train_items 16288.
I0302 18:58:32.653692 22732373614720 run.py:483] Algo bellman_ford step 509 current loss 1.865449, current_train_items 16320.
I0302 18:58:32.672336 22732373614720 run.py:483] Algo bellman_ford step 510 current loss 0.514371, current_train_items 16352.
I0302 18:58:32.688348 22732373614720 run.py:483] Algo bellman_ford step 511 current loss 0.845978, current_train_items 16384.
I0302 18:58:32.711450 22732373614720 run.py:483] Algo bellman_ford step 512 current loss 1.122567, current_train_items 16416.
I0302 18:58:32.740918 22732373614720 run.py:483] Algo bellman_ford step 513 current loss 1.330691, current_train_items 16448.
I0302 18:58:32.772616 22732373614720 run.py:483] Algo bellman_ford step 514 current loss 1.531376, current_train_items 16480.
I0302 18:58:32.790979 22732373614720 run.py:483] Algo bellman_ford step 515 current loss 0.460653, current_train_items 16512.
I0302 18:58:32.806800 22732373614720 run.py:483] Algo bellman_ford step 516 current loss 0.691042, current_train_items 16544.
I0302 18:58:32.830181 22732373614720 run.py:483] Algo bellman_ford step 517 current loss 0.975292, current_train_items 16576.
I0302 18:58:32.860622 22732373614720 run.py:483] Algo bellman_ford step 518 current loss 1.190829, current_train_items 16608.
I0302 18:58:32.892493 22732373614720 run.py:483] Algo bellman_ford step 519 current loss 1.425540, current_train_items 16640.
I0302 18:58:32.911078 22732373614720 run.py:483] Algo bellman_ford step 520 current loss 0.468136, current_train_items 16672.
I0302 18:58:32.927267 22732373614720 run.py:483] Algo bellman_ford step 521 current loss 0.735903, current_train_items 16704.
I0302 18:58:32.949898 22732373614720 run.py:483] Algo bellman_ford step 522 current loss 0.960469, current_train_items 16736.
I0302 18:58:32.979747 22732373614720 run.py:483] Algo bellman_ford step 523 current loss 1.254401, current_train_items 16768.
I0302 18:58:33.011023 22732373614720 run.py:483] Algo bellman_ford step 524 current loss 1.858079, current_train_items 16800.
I0302 18:58:33.029367 22732373614720 run.py:483] Algo bellman_ford step 525 current loss 0.406659, current_train_items 16832.
I0302 18:58:33.045565 22732373614720 run.py:483] Algo bellman_ford step 526 current loss 0.826202, current_train_items 16864.
I0302 18:58:33.068266 22732373614720 run.py:483] Algo bellman_ford step 527 current loss 1.137146, current_train_items 16896.
I0302 18:58:33.096492 22732373614720 run.py:483] Algo bellman_ford step 528 current loss 1.061209, current_train_items 16928.
I0302 18:58:33.128296 22732373614720 run.py:483] Algo bellman_ford step 529 current loss 1.417246, current_train_items 16960.
I0302 18:58:33.147036 22732373614720 run.py:483] Algo bellman_ford step 530 current loss 0.542692, current_train_items 16992.
I0302 18:58:33.162894 22732373614720 run.py:483] Algo bellman_ford step 531 current loss 0.891388, current_train_items 17024.
I0302 18:58:33.185233 22732373614720 run.py:483] Algo bellman_ford step 532 current loss 0.933555, current_train_items 17056.
I0302 18:58:33.214527 22732373614720 run.py:483] Algo bellman_ford step 533 current loss 1.537603, current_train_items 17088.
I0302 18:58:33.246166 22732373614720 run.py:483] Algo bellman_ford step 534 current loss 1.672603, current_train_items 17120.
I0302 18:58:33.264859 22732373614720 run.py:483] Algo bellman_ford step 535 current loss 0.427317, current_train_items 17152.
I0302 18:58:33.280391 22732373614720 run.py:483] Algo bellman_ford step 536 current loss 0.754179, current_train_items 17184.
I0302 18:58:33.302368 22732373614720 run.py:483] Algo bellman_ford step 537 current loss 0.993095, current_train_items 17216.
I0302 18:58:33.330796 22732373614720 run.py:483] Algo bellman_ford step 538 current loss 1.110682, current_train_items 17248.
I0302 18:58:33.361293 22732373614720 run.py:483] Algo bellman_ford step 539 current loss 1.299000, current_train_items 17280.
I0302 18:58:33.379856 22732373614720 run.py:483] Algo bellman_ford step 540 current loss 0.484525, current_train_items 17312.
I0302 18:58:33.395283 22732373614720 run.py:483] Algo bellman_ford step 541 current loss 0.623765, current_train_items 17344.
I0302 18:58:33.418608 22732373614720 run.py:483] Algo bellman_ford step 542 current loss 1.102718, current_train_items 17376.
I0302 18:58:33.448088 22732373614720 run.py:483] Algo bellman_ford step 543 current loss 1.253341, current_train_items 17408.
I0302 18:58:33.479774 22732373614720 run.py:483] Algo bellman_ford step 544 current loss 1.430645, current_train_items 17440.
I0302 18:58:33.498451 22732373614720 run.py:483] Algo bellman_ford step 545 current loss 0.517980, current_train_items 17472.
I0302 18:58:33.514302 22732373614720 run.py:483] Algo bellman_ford step 546 current loss 0.956884, current_train_items 17504.
I0302 18:58:33.536834 22732373614720 run.py:483] Algo bellman_ford step 547 current loss 1.171133, current_train_items 17536.
I0302 18:58:33.566503 22732373614720 run.py:483] Algo bellman_ford step 548 current loss 1.284353, current_train_items 17568.
I0302 18:58:33.595646 22732373614720 run.py:483] Algo bellman_ford step 549 current loss 1.132065, current_train_items 17600.
I0302 18:58:33.613952 22732373614720 run.py:483] Algo bellman_ford step 550 current loss 0.472869, current_train_items 17632.
I0302 18:58:33.621974 22732373614720 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.845703125, 'score': 0.845703125, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0302 18:58:33.622081 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.856, current avg val score is 0.846, val scores are: bellman_ford: 0.846
I0302 18:58:33.638485 22732373614720 run.py:483] Algo bellman_ford step 551 current loss 0.704235, current_train_items 17664.
I0302 18:58:33.661490 22732373614720 run.py:483] Algo bellman_ford step 552 current loss 0.951091, current_train_items 17696.
I0302 18:58:33.692361 22732373614720 run.py:483] Algo bellman_ford step 553 current loss 1.207896, current_train_items 17728.
I0302 18:58:33.723845 22732373614720 run.py:483] Algo bellman_ford step 554 current loss 1.328053, current_train_items 17760.
I0302 18:58:33.742903 22732373614720 run.py:483] Algo bellman_ford step 555 current loss 0.498764, current_train_items 17792.
I0302 18:58:33.758440 22732373614720 run.py:483] Algo bellman_ford step 556 current loss 0.744130, current_train_items 17824.
I0302 18:58:33.781996 22732373614720 run.py:483] Algo bellman_ford step 557 current loss 1.152042, current_train_items 17856.
I0302 18:58:33.811048 22732373614720 run.py:483] Algo bellman_ford step 558 current loss 1.279823, current_train_items 17888.
I0302 18:58:33.843782 22732373614720 run.py:483] Algo bellman_ford step 559 current loss 1.388681, current_train_items 17920.
I0302 18:58:33.862814 22732373614720 run.py:483] Algo bellman_ford step 560 current loss 0.539168, current_train_items 17952.
I0302 18:58:33.878867 22732373614720 run.py:483] Algo bellman_ford step 561 current loss 0.686805, current_train_items 17984.
I0302 18:58:33.902072 22732373614720 run.py:483] Algo bellman_ford step 562 current loss 1.034007, current_train_items 18016.
I0302 18:58:33.930494 22732373614720 run.py:483] Algo bellman_ford step 563 current loss 1.083847, current_train_items 18048.
I0302 18:58:33.960304 22732373614720 run.py:483] Algo bellman_ford step 564 current loss 1.343630, current_train_items 18080.
I0302 18:58:33.978579 22732373614720 run.py:483] Algo bellman_ford step 565 current loss 0.454476, current_train_items 18112.
I0302 18:58:33.994728 22732373614720 run.py:483] Algo bellman_ford step 566 current loss 0.782270, current_train_items 18144.
I0302 18:58:34.018767 22732373614720 run.py:483] Algo bellman_ford step 567 current loss 1.203343, current_train_items 18176.
I0302 18:58:34.047913 22732373614720 run.py:483] Algo bellman_ford step 568 current loss 1.044096, current_train_items 18208.
I0302 18:58:34.078766 22732373614720 run.py:483] Algo bellman_ford step 569 current loss 1.300021, current_train_items 18240.
I0302 18:58:34.097822 22732373614720 run.py:483] Algo bellman_ford step 570 current loss 0.590506, current_train_items 18272.
I0302 18:58:34.113579 22732373614720 run.py:483] Algo bellman_ford step 571 current loss 0.764402, current_train_items 18304.
I0302 18:58:34.136769 22732373614720 run.py:483] Algo bellman_ford step 572 current loss 1.238585, current_train_items 18336.
I0302 18:58:34.167374 22732373614720 run.py:483] Algo bellman_ford step 573 current loss 1.300579, current_train_items 18368.
I0302 18:58:34.197911 22732373614720 run.py:483] Algo bellman_ford step 574 current loss 1.481517, current_train_items 18400.
I0302 18:58:34.217176 22732373614720 run.py:483] Algo bellman_ford step 575 current loss 0.517324, current_train_items 18432.
I0302 18:58:34.232896 22732373614720 run.py:483] Algo bellman_ford step 576 current loss 0.990195, current_train_items 18464.
I0302 18:58:34.255554 22732373614720 run.py:483] Algo bellman_ford step 577 current loss 1.363627, current_train_items 18496.
I0302 18:58:34.284985 22732373614720 run.py:483] Algo bellman_ford step 578 current loss 1.433707, current_train_items 18528.
I0302 18:58:34.317332 22732373614720 run.py:483] Algo bellman_ford step 579 current loss 1.666719, current_train_items 18560.
I0302 18:58:34.336090 22732373614720 run.py:483] Algo bellman_ford step 580 current loss 0.486528, current_train_items 18592.
I0302 18:58:34.351889 22732373614720 run.py:483] Algo bellman_ford step 581 current loss 0.711072, current_train_items 18624.
I0302 18:58:34.373536 22732373614720 run.py:483] Algo bellman_ford step 582 current loss 0.990498, current_train_items 18656.
I0302 18:58:34.401632 22732373614720 run.py:483] Algo bellman_ford step 583 current loss 1.033180, current_train_items 18688.
I0302 18:58:34.433012 22732373614720 run.py:483] Algo bellman_ford step 584 current loss 1.454895, current_train_items 18720.
I0302 18:58:34.451351 22732373614720 run.py:483] Algo bellman_ford step 585 current loss 0.396685, current_train_items 18752.
I0302 18:58:34.467384 22732373614720 run.py:483] Algo bellman_ford step 586 current loss 0.747642, current_train_items 18784.
I0302 18:58:34.488927 22732373614720 run.py:483] Algo bellman_ford step 587 current loss 0.857267, current_train_items 18816.
I0302 18:58:34.517414 22732373614720 run.py:483] Algo bellman_ford step 588 current loss 1.172635, current_train_items 18848.
I0302 18:58:34.549933 22732373614720 run.py:483] Algo bellman_ford step 589 current loss 1.528437, current_train_items 18880.
I0302 18:58:34.568942 22732373614720 run.py:483] Algo bellman_ford step 590 current loss 0.565933, current_train_items 18912.
I0302 18:58:34.584985 22732373614720 run.py:483] Algo bellman_ford step 591 current loss 0.847693, current_train_items 18944.
I0302 18:58:34.608386 22732373614720 run.py:483] Algo bellman_ford step 592 current loss 1.189440, current_train_items 18976.
I0302 18:58:34.637260 22732373614720 run.py:483] Algo bellman_ford step 593 current loss 1.217363, current_train_items 19008.
I0302 18:58:34.668190 22732373614720 run.py:483] Algo bellman_ford step 594 current loss 1.446949, current_train_items 19040.
I0302 18:58:34.686690 22732373614720 run.py:483] Algo bellman_ford step 595 current loss 0.433702, current_train_items 19072.
I0302 18:58:34.702671 22732373614720 run.py:483] Algo bellman_ford step 596 current loss 0.787005, current_train_items 19104.
I0302 18:58:34.726059 22732373614720 run.py:483] Algo bellman_ford step 597 current loss 1.018926, current_train_items 19136.
I0302 18:58:34.755224 22732373614720 run.py:483] Algo bellman_ford step 598 current loss 1.117606, current_train_items 19168.
I0302 18:58:34.786931 22732373614720 run.py:483] Algo bellman_ford step 599 current loss 1.239235, current_train_items 19200.
I0302 18:58:34.805670 22732373614720 run.py:483] Algo bellman_ford step 600 current loss 0.619138, current_train_items 19232.
I0302 18:58:34.813622 22732373614720 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.8271484375, 'score': 0.8271484375, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0302 18:58:34.813730 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.856, current avg val score is 0.827, val scores are: bellman_ford: 0.827
I0302 18:58:34.829915 22732373614720 run.py:483] Algo bellman_ford step 601 current loss 0.676266, current_train_items 19264.
I0302 18:58:34.852812 22732373614720 run.py:483] Algo bellman_ford step 602 current loss 1.077783, current_train_items 19296.
I0302 18:58:34.881651 22732373614720 run.py:483] Algo bellman_ford step 603 current loss 1.221354, current_train_items 19328.
I0302 18:58:34.912036 22732373614720 run.py:483] Algo bellman_ford step 604 current loss 1.422923, current_train_items 19360.
I0302 18:58:34.930711 22732373614720 run.py:483] Algo bellman_ford step 605 current loss 0.482719, current_train_items 19392.
I0302 18:58:34.945960 22732373614720 run.py:483] Algo bellman_ford step 606 current loss 0.672746, current_train_items 19424.
I0302 18:58:34.969243 22732373614720 run.py:483] Algo bellman_ford step 607 current loss 1.160280, current_train_items 19456.
I0302 18:58:34.997098 22732373614720 run.py:483] Algo bellman_ford step 608 current loss 1.093171, current_train_items 19488.
I0302 18:58:35.028213 22732373614720 run.py:483] Algo bellman_ford step 609 current loss 1.321669, current_train_items 19520.
I0302 18:58:35.046594 22732373614720 run.py:483] Algo bellman_ford step 610 current loss 0.571199, current_train_items 19552.
I0302 18:58:35.062676 22732373614720 run.py:483] Algo bellman_ford step 611 current loss 0.759194, current_train_items 19584.
I0302 18:58:35.085477 22732373614720 run.py:483] Algo bellman_ford step 612 current loss 0.983974, current_train_items 19616.
I0302 18:58:35.112524 22732373614720 run.py:483] Algo bellman_ford step 613 current loss 1.145249, current_train_items 19648.
I0302 18:58:35.143427 22732373614720 run.py:483] Algo bellman_ford step 614 current loss 1.731687, current_train_items 19680.
I0302 18:58:35.161632 22732373614720 run.py:483] Algo bellman_ford step 615 current loss 0.444458, current_train_items 19712.
I0302 18:58:35.177745 22732373614720 run.py:483] Algo bellman_ford step 616 current loss 0.748423, current_train_items 19744.
I0302 18:58:35.200039 22732373614720 run.py:483] Algo bellman_ford step 617 current loss 1.011573, current_train_items 19776.
I0302 18:58:35.228030 22732373614720 run.py:483] Algo bellman_ford step 618 current loss 1.061823, current_train_items 19808.
I0302 18:58:35.259268 22732373614720 run.py:483] Algo bellman_ford step 619 current loss 1.251807, current_train_items 19840.
I0302 18:58:35.277686 22732373614720 run.py:483] Algo bellman_ford step 620 current loss 0.523974, current_train_items 19872.
I0302 18:58:35.294042 22732373614720 run.py:483] Algo bellman_ford step 621 current loss 0.953724, current_train_items 19904.
I0302 18:58:35.316952 22732373614720 run.py:483] Algo bellman_ford step 622 current loss 1.098312, current_train_items 19936.
I0302 18:58:35.345401 22732373614720 run.py:483] Algo bellman_ford step 623 current loss 1.268860, current_train_items 19968.
I0302 18:58:35.376372 22732373614720 run.py:483] Algo bellman_ford step 624 current loss 1.258083, current_train_items 20000.
I0302 18:58:35.394911 22732373614720 run.py:483] Algo bellman_ford step 625 current loss 0.583100, current_train_items 20032.
I0302 18:58:35.410617 22732373614720 run.py:483] Algo bellman_ford step 626 current loss 0.698507, current_train_items 20064.
I0302 18:58:35.433940 22732373614720 run.py:483] Algo bellman_ford step 627 current loss 1.036968, current_train_items 20096.
I0302 18:58:35.463340 22732373614720 run.py:483] Algo bellman_ford step 628 current loss 1.235757, current_train_items 20128.
I0302 18:58:35.495293 22732373614720 run.py:483] Algo bellman_ford step 629 current loss 1.445364, current_train_items 20160.
I0302 18:58:35.513523 22732373614720 run.py:483] Algo bellman_ford step 630 current loss 0.507481, current_train_items 20192.
I0302 18:58:35.529205 22732373614720 run.py:483] Algo bellman_ford step 631 current loss 0.681372, current_train_items 20224.
I0302 18:58:35.551490 22732373614720 run.py:483] Algo bellman_ford step 632 current loss 1.096759, current_train_items 20256.
I0302 18:58:35.580212 22732373614720 run.py:483] Algo bellman_ford step 633 current loss 1.119650, current_train_items 20288.
I0302 18:58:35.610856 22732373614720 run.py:483] Algo bellman_ford step 634 current loss 1.291103, current_train_items 20320.
I0302 18:58:35.629466 22732373614720 run.py:483] Algo bellman_ford step 635 current loss 0.512459, current_train_items 20352.
I0302 18:58:35.645204 22732373614720 run.py:483] Algo bellman_ford step 636 current loss 0.710063, current_train_items 20384.
I0302 18:58:35.667671 22732373614720 run.py:483] Algo bellman_ford step 637 current loss 1.043365, current_train_items 20416.
I0302 18:58:35.697324 22732373614720 run.py:483] Algo bellman_ford step 638 current loss 1.192189, current_train_items 20448.
I0302 18:58:35.725477 22732373614720 run.py:483] Algo bellman_ford step 639 current loss 1.535086, current_train_items 20480.
I0302 18:58:35.743867 22732373614720 run.py:483] Algo bellman_ford step 640 current loss 0.532982, current_train_items 20512.
I0302 18:58:35.759587 22732373614720 run.py:483] Algo bellman_ford step 641 current loss 0.758697, current_train_items 20544.
I0302 18:58:35.781163 22732373614720 run.py:483] Algo bellman_ford step 642 current loss 1.046655, current_train_items 20576.
I0302 18:58:35.809283 22732373614720 run.py:483] Algo bellman_ford step 643 current loss 1.036710, current_train_items 20608.
I0302 18:58:35.840176 22732373614720 run.py:483] Algo bellman_ford step 644 current loss 1.264103, current_train_items 20640.
I0302 18:58:35.858314 22732373614720 run.py:483] Algo bellman_ford step 645 current loss 0.583677, current_train_items 20672.
I0302 18:58:35.875232 22732373614720 run.py:483] Algo bellman_ford step 646 current loss 1.059773, current_train_items 20704.
I0302 18:58:35.897480 22732373614720 run.py:483] Algo bellman_ford step 647 current loss 1.093425, current_train_items 20736.
I0302 18:58:35.926900 22732373614720 run.py:483] Algo bellman_ford step 648 current loss 1.302380, current_train_items 20768.
I0302 18:58:35.958491 22732373614720 run.py:483] Algo bellman_ford step 649 current loss 1.280026, current_train_items 20800.
I0302 18:58:35.976859 22732373614720 run.py:483] Algo bellman_ford step 650 current loss 0.574285, current_train_items 20832.
I0302 18:58:35.984995 22732373614720 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.8505859375, 'score': 0.8505859375, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0302 18:58:35.985103 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.856, current avg val score is 0.851, val scores are: bellman_ford: 0.851
I0302 18:58:36.001874 22732373614720 run.py:483] Algo bellman_ford step 651 current loss 0.788284, current_train_items 20864.
I0302 18:58:36.024525 22732373614720 run.py:483] Algo bellman_ford step 652 current loss 1.125771, current_train_items 20896.
I0302 18:58:36.052459 22732373614720 run.py:483] Algo bellman_ford step 653 current loss 1.133781, current_train_items 20928.
I0302 18:58:36.083502 22732373614720 run.py:483] Algo bellman_ford step 654 current loss 1.362916, current_train_items 20960.
I0302 18:58:36.102350 22732373614720 run.py:483] Algo bellman_ford step 655 current loss 0.504895, current_train_items 20992.
I0302 18:58:36.117813 22732373614720 run.py:483] Algo bellman_ford step 656 current loss 0.924332, current_train_items 21024.
I0302 18:58:36.141032 22732373614720 run.py:483] Algo bellman_ford step 657 current loss 1.151955, current_train_items 21056.
I0302 18:58:36.170116 22732373614720 run.py:483] Algo bellman_ford step 658 current loss 1.274420, current_train_items 21088.
I0302 18:58:36.200351 22732373614720 run.py:483] Algo bellman_ford step 659 current loss 1.333201, current_train_items 21120.
I0302 18:58:36.218973 22732373614720 run.py:483] Algo bellman_ford step 660 current loss 0.476958, current_train_items 21152.
I0302 18:58:36.234768 22732373614720 run.py:483] Algo bellman_ford step 661 current loss 0.642687, current_train_items 21184.
I0302 18:58:36.256623 22732373614720 run.py:483] Algo bellman_ford step 662 current loss 1.021449, current_train_items 21216.
I0302 18:58:36.285081 22732373614720 run.py:483] Algo bellman_ford step 663 current loss 1.262762, current_train_items 21248.
I0302 18:58:36.318471 22732373614720 run.py:483] Algo bellman_ford step 664 current loss 1.383895, current_train_items 21280.
I0302 18:58:36.336983 22732373614720 run.py:483] Algo bellman_ford step 665 current loss 0.509835, current_train_items 21312.
I0302 18:58:36.352839 22732373614720 run.py:483] Algo bellman_ford step 666 current loss 0.744022, current_train_items 21344.
I0302 18:58:36.376095 22732373614720 run.py:483] Algo bellman_ford step 667 current loss 1.093716, current_train_items 21376.
I0302 18:58:36.404419 22732373614720 run.py:483] Algo bellman_ford step 668 current loss 1.179324, current_train_items 21408.
I0302 18:58:36.434970 22732373614720 run.py:483] Algo bellman_ford step 669 current loss 1.326886, current_train_items 21440.
I0302 18:58:36.453469 22732373614720 run.py:483] Algo bellman_ford step 670 current loss 0.397822, current_train_items 21472.
I0302 18:58:36.469178 22732373614720 run.py:483] Algo bellman_ford step 671 current loss 0.632057, current_train_items 21504.
I0302 18:58:36.492166 22732373614720 run.py:483] Algo bellman_ford step 672 current loss 0.959073, current_train_items 21536.
I0302 18:58:36.520490 22732373614720 run.py:483] Algo bellman_ford step 673 current loss 1.275609, current_train_items 21568.
I0302 18:58:36.552702 22732373614720 run.py:483] Algo bellman_ford step 674 current loss 1.366108, current_train_items 21600.
I0302 18:58:36.571484 22732373614720 run.py:483] Algo bellman_ford step 675 current loss 0.471359, current_train_items 21632.
I0302 18:58:36.586904 22732373614720 run.py:483] Algo bellman_ford step 676 current loss 0.621324, current_train_items 21664.
I0302 18:58:36.608929 22732373614720 run.py:483] Algo bellman_ford step 677 current loss 1.095575, current_train_items 21696.
I0302 18:58:36.638403 22732373614720 run.py:483] Algo bellman_ford step 678 current loss 1.132137, current_train_items 21728.
I0302 18:58:36.669924 22732373614720 run.py:483] Algo bellman_ford step 679 current loss 1.397757, current_train_items 21760.
I0302 18:58:36.688283 22732373614720 run.py:483] Algo bellman_ford step 680 current loss 0.481333, current_train_items 21792.
I0302 18:58:36.704293 22732373614720 run.py:483] Algo bellman_ford step 681 current loss 0.777000, current_train_items 21824.
I0302 18:58:36.727025 22732373614720 run.py:483] Algo bellman_ford step 682 current loss 1.069811, current_train_items 21856.
I0302 18:58:36.755067 22732373614720 run.py:483] Algo bellman_ford step 683 current loss 1.055715, current_train_items 21888.
I0302 18:58:36.786873 22732373614720 run.py:483] Algo bellman_ford step 684 current loss 1.405287, current_train_items 21920.
I0302 18:58:36.805468 22732373614720 run.py:483] Algo bellman_ford step 685 current loss 0.536878, current_train_items 21952.
I0302 18:58:36.821203 22732373614720 run.py:483] Algo bellman_ford step 686 current loss 0.708480, current_train_items 21984.
I0302 18:58:36.843974 22732373614720 run.py:483] Algo bellman_ford step 687 current loss 1.012675, current_train_items 22016.
I0302 18:58:36.871128 22732373614720 run.py:483] Algo bellman_ford step 688 current loss 0.975610, current_train_items 22048.
I0302 18:58:36.904027 22732373614720 run.py:483] Algo bellman_ford step 689 current loss 1.458720, current_train_items 22080.
I0302 18:58:36.922286 22732373614720 run.py:483] Algo bellman_ford step 690 current loss 0.507091, current_train_items 22112.
I0302 18:58:36.938612 22732373614720 run.py:483] Algo bellman_ford step 691 current loss 0.882568, current_train_items 22144.
I0302 18:58:36.961879 22732373614720 run.py:483] Algo bellman_ford step 692 current loss 1.067248, current_train_items 22176.
I0302 18:58:36.989894 22732373614720 run.py:483] Algo bellman_ford step 693 current loss 1.240163, current_train_items 22208.
I0302 18:58:37.022329 22732373614720 run.py:483] Algo bellman_ford step 694 current loss 1.444108, current_train_items 22240.
I0302 18:58:37.040750 22732373614720 run.py:483] Algo bellman_ford step 695 current loss 0.499126, current_train_items 22272.
I0302 18:58:37.056513 22732373614720 run.py:483] Algo bellman_ford step 696 current loss 0.815311, current_train_items 22304.
I0302 18:58:37.077872 22732373614720 run.py:483] Algo bellman_ford step 697 current loss 1.226528, current_train_items 22336.
I0302 18:58:37.106653 22732373614720 run.py:483] Algo bellman_ford step 698 current loss 1.197547, current_train_items 22368.
I0302 18:58:37.139115 22732373614720 run.py:483] Algo bellman_ford step 699 current loss 1.390112, current_train_items 22400.
I0302 18:58:37.157603 22732373614720 run.py:483] Algo bellman_ford step 700 current loss 0.446510, current_train_items 22432.
I0302 18:58:37.165358 22732373614720 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.859375, 'score': 0.859375, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0302 18:58:37.165465 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.856, current avg val score is 0.859, val scores are: bellman_ford: 0.859
I0302 18:58:37.195882 22732373614720 run.py:483] Algo bellman_ford step 701 current loss 0.614886, current_train_items 22464.
I0302 18:58:37.218989 22732373614720 run.py:483] Algo bellman_ford step 702 current loss 1.244318, current_train_items 22496.
I0302 18:58:37.246718 22732373614720 run.py:483] Algo bellman_ford step 703 current loss 1.330375, current_train_items 22528.
I0302 18:58:37.277314 22732373614720 run.py:483] Algo bellman_ford step 704 current loss 1.498081, current_train_items 22560.
I0302 18:58:37.296251 22732373614720 run.py:483] Algo bellman_ford step 705 current loss 0.492027, current_train_items 22592.
I0302 18:58:37.311786 22732373614720 run.py:483] Algo bellman_ford step 706 current loss 0.718717, current_train_items 22624.
I0302 18:58:37.334634 22732373614720 run.py:483] Algo bellman_ford step 707 current loss 0.866747, current_train_items 22656.
I0302 18:58:37.363102 22732373614720 run.py:483] Algo bellman_ford step 708 current loss 0.994124, current_train_items 22688.
I0302 18:58:37.395251 22732373614720 run.py:483] Algo bellman_ford step 709 current loss 1.404932, current_train_items 22720.
I0302 18:58:37.414144 22732373614720 run.py:483] Algo bellman_ford step 710 current loss 0.471331, current_train_items 22752.
I0302 18:58:37.430058 22732373614720 run.py:483] Algo bellman_ford step 711 current loss 0.661451, current_train_items 22784.
I0302 18:58:37.453707 22732373614720 run.py:483] Algo bellman_ford step 712 current loss 1.196437, current_train_items 22816.
I0302 18:58:37.483658 22732373614720 run.py:483] Algo bellman_ford step 713 current loss 1.257320, current_train_items 22848.
I0302 18:58:37.513301 22732373614720 run.py:483] Algo bellman_ford step 714 current loss 1.257750, current_train_items 22880.
I0302 18:58:37.532045 22732373614720 run.py:483] Algo bellman_ford step 715 current loss 0.418007, current_train_items 22912.
I0302 18:58:37.548309 22732373614720 run.py:483] Algo bellman_ford step 716 current loss 0.882791, current_train_items 22944.
I0302 18:58:37.572381 22732373614720 run.py:483] Algo bellman_ford step 717 current loss 1.169820, current_train_items 22976.
I0302 18:58:37.600484 22732373614720 run.py:483] Algo bellman_ford step 718 current loss 1.044663, current_train_items 23008.
I0302 18:58:37.631891 22732373614720 run.py:483] Algo bellman_ford step 719 current loss 1.382956, current_train_items 23040.
I0302 18:58:37.650499 22732373614720 run.py:483] Algo bellman_ford step 720 current loss 0.611662, current_train_items 23072.
I0302 18:58:37.666851 22732373614720 run.py:483] Algo bellman_ford step 721 current loss 0.826074, current_train_items 23104.
I0302 18:58:37.689595 22732373614720 run.py:483] Algo bellman_ford step 722 current loss 0.955669, current_train_items 23136.
I0302 18:58:37.717781 22732373614720 run.py:483] Algo bellman_ford step 723 current loss 1.087375, current_train_items 23168.
I0302 18:58:37.748449 22732373614720 run.py:483] Algo bellman_ford step 724 current loss 1.352361, current_train_items 23200.
I0302 18:58:37.766641 22732373614720 run.py:483] Algo bellman_ford step 725 current loss 0.366695, current_train_items 23232.
I0302 18:58:37.783039 22732373614720 run.py:483] Algo bellman_ford step 726 current loss 0.773953, current_train_items 23264.
I0302 18:58:37.806112 22732373614720 run.py:483] Algo bellman_ford step 727 current loss 1.404152, current_train_items 23296.
I0302 18:58:37.834303 22732373614720 run.py:483] Algo bellman_ford step 728 current loss 1.275087, current_train_items 23328.
I0302 18:58:37.865830 22732373614720 run.py:483] Algo bellman_ford step 729 current loss 1.623178, current_train_items 23360.
I0302 18:58:37.883944 22732373614720 run.py:483] Algo bellman_ford step 730 current loss 0.363067, current_train_items 23392.
I0302 18:58:37.899924 22732373614720 run.py:483] Algo bellman_ford step 731 current loss 0.794385, current_train_items 23424.
I0302 18:58:37.922730 22732373614720 run.py:483] Algo bellman_ford step 732 current loss 1.001480, current_train_items 23456.
I0302 18:58:37.951381 22732373614720 run.py:483] Algo bellman_ford step 733 current loss 1.142374, current_train_items 23488.
I0302 18:58:37.983226 22732373614720 run.py:483] Algo bellman_ford step 734 current loss 1.450115, current_train_items 23520.
I0302 18:58:38.001585 22732373614720 run.py:483] Algo bellman_ford step 735 current loss 0.505830, current_train_items 23552.
I0302 18:58:38.017829 22732373614720 run.py:483] Algo bellman_ford step 736 current loss 0.699325, current_train_items 23584.
I0302 18:58:38.040146 22732373614720 run.py:483] Algo bellman_ford step 737 current loss 1.007958, current_train_items 23616.
I0302 18:58:38.069292 22732373614720 run.py:483] Algo bellman_ford step 738 current loss 1.145161, current_train_items 23648.
I0302 18:58:38.098792 22732373614720 run.py:483] Algo bellman_ford step 739 current loss 1.105274, current_train_items 23680.
I0302 18:58:38.117334 22732373614720 run.py:483] Algo bellman_ford step 740 current loss 0.549942, current_train_items 23712.
I0302 18:58:38.133458 22732373614720 run.py:483] Algo bellman_ford step 741 current loss 0.786593, current_train_items 23744.
I0302 18:58:38.156242 22732373614720 run.py:483] Algo bellman_ford step 742 current loss 0.956959, current_train_items 23776.
I0302 18:58:38.183708 22732373614720 run.py:483] Algo bellman_ford step 743 current loss 1.120206, current_train_items 23808.
I0302 18:58:38.217129 22732373614720 run.py:483] Algo bellman_ford step 744 current loss 1.859096, current_train_items 23840.
I0302 18:58:38.235575 22732373614720 run.py:483] Algo bellman_ford step 745 current loss 0.445051, current_train_items 23872.
I0302 18:58:38.251435 22732373614720 run.py:483] Algo bellman_ford step 746 current loss 0.831870, current_train_items 23904.
I0302 18:58:38.273697 22732373614720 run.py:483] Algo bellman_ford step 747 current loss 1.116818, current_train_items 23936.
I0302 18:58:38.303011 22732373614720 run.py:483] Algo bellman_ford step 748 current loss 1.655472, current_train_items 23968.
I0302 18:58:38.334060 22732373614720 run.py:483] Algo bellman_ford step 749 current loss 1.476707, current_train_items 24000.
I0302 18:58:38.352483 22732373614720 run.py:483] Algo bellman_ford step 750 current loss 0.534821, current_train_items 24032.
I0302 18:58:38.360408 22732373614720 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.8369140625, 'score': 0.8369140625, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0302 18:58:38.360518 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.859, current avg val score is 0.837, val scores are: bellman_ford: 0.837
I0302 18:58:38.377388 22732373614720 run.py:483] Algo bellman_ford step 751 current loss 0.805954, current_train_items 24064.
I0302 18:58:38.399951 22732373614720 run.py:483] Algo bellman_ford step 752 current loss 1.054874, current_train_items 24096.
I0302 18:58:38.428644 22732373614720 run.py:483] Algo bellman_ford step 753 current loss 1.216596, current_train_items 24128.
I0302 18:58:38.460640 22732373614720 run.py:483] Algo bellman_ford step 754 current loss 1.401741, current_train_items 24160.
I0302 18:58:38.479771 22732373614720 run.py:483] Algo bellman_ford step 755 current loss 0.425171, current_train_items 24192.
I0302 18:58:38.495084 22732373614720 run.py:483] Algo bellman_ford step 756 current loss 0.703669, current_train_items 24224.
I0302 18:58:38.518713 22732373614720 run.py:483] Algo bellman_ford step 757 current loss 1.100631, current_train_items 24256.
I0302 18:58:38.547094 22732373614720 run.py:483] Algo bellman_ford step 758 current loss 1.179242, current_train_items 24288.
I0302 18:58:38.577631 22732373614720 run.py:483] Algo bellman_ford step 759 current loss 1.270315, current_train_items 24320.
I0302 18:58:38.596178 22732373614720 run.py:483] Algo bellman_ford step 760 current loss 0.480143, current_train_items 24352.
I0302 18:58:38.611987 22732373614720 run.py:483] Algo bellman_ford step 761 current loss 0.773350, current_train_items 24384.
I0302 18:58:38.634684 22732373614720 run.py:483] Algo bellman_ford step 762 current loss 1.100090, current_train_items 24416.
I0302 18:58:38.662542 22732373614720 run.py:483] Algo bellman_ford step 763 current loss 1.022878, current_train_items 24448.
I0302 18:58:38.694141 22732373614720 run.py:483] Algo bellman_ford step 764 current loss 1.302759, current_train_items 24480.
I0302 18:58:38.712511 22732373614720 run.py:483] Algo bellman_ford step 765 current loss 0.527216, current_train_items 24512.
I0302 18:58:38.728883 22732373614720 run.py:483] Algo bellman_ford step 766 current loss 0.818844, current_train_items 24544.
I0302 18:58:38.751663 22732373614720 run.py:483] Algo bellman_ford step 767 current loss 1.064999, current_train_items 24576.
I0302 18:58:38.780633 22732373614720 run.py:483] Algo bellman_ford step 768 current loss 1.158672, current_train_items 24608.
I0302 18:58:38.812826 22732373614720 run.py:483] Algo bellman_ford step 769 current loss 1.173622, current_train_items 24640.
I0302 18:58:38.831538 22732373614720 run.py:483] Algo bellman_ford step 770 current loss 0.421558, current_train_items 24672.
I0302 18:58:38.847740 22732373614720 run.py:483] Algo bellman_ford step 771 current loss 0.893362, current_train_items 24704.
I0302 18:58:38.869890 22732373614720 run.py:483] Algo bellman_ford step 772 current loss 1.147707, current_train_items 24736.
I0302 18:58:38.898309 22732373614720 run.py:483] Algo bellman_ford step 773 current loss 1.010617, current_train_items 24768.
I0302 18:58:38.930161 22732373614720 run.py:483] Algo bellman_ford step 774 current loss 1.363430, current_train_items 24800.
I0302 18:58:38.948944 22732373614720 run.py:483] Algo bellman_ford step 775 current loss 0.418315, current_train_items 24832.
I0302 18:58:38.965490 22732373614720 run.py:483] Algo bellman_ford step 776 current loss 0.755476, current_train_items 24864.
I0302 18:58:38.988518 22732373614720 run.py:483] Algo bellman_ford step 777 current loss 1.135132, current_train_items 24896.
I0302 18:58:39.016242 22732373614720 run.py:483] Algo bellman_ford step 778 current loss 1.024160, current_train_items 24928.
I0302 18:58:39.047104 22732373614720 run.py:483] Algo bellman_ford step 779 current loss 1.234456, current_train_items 24960.
I0302 18:58:39.065526 22732373614720 run.py:483] Algo bellman_ford step 780 current loss 0.537748, current_train_items 24992.
I0302 18:58:39.081689 22732373614720 run.py:483] Algo bellman_ford step 781 current loss 0.802366, current_train_items 25024.
I0302 18:58:39.103846 22732373614720 run.py:483] Algo bellman_ford step 782 current loss 0.893612, current_train_items 25056.
I0302 18:58:39.132307 22732373614720 run.py:483] Algo bellman_ford step 783 current loss 1.146826, current_train_items 25088.
I0302 18:58:39.164650 22732373614720 run.py:483] Algo bellman_ford step 784 current loss 1.231655, current_train_items 25120.
I0302 18:58:39.183113 22732373614720 run.py:483] Algo bellman_ford step 785 current loss 0.467632, current_train_items 25152.
I0302 18:58:39.199527 22732373614720 run.py:483] Algo bellman_ford step 786 current loss 0.751376, current_train_items 25184.
I0302 18:58:39.221703 22732373614720 run.py:483] Algo bellman_ford step 787 current loss 0.992030, current_train_items 25216.
I0302 18:58:39.250843 22732373614720 run.py:483] Algo bellman_ford step 788 current loss 1.170304, current_train_items 25248.
I0302 18:58:39.280107 22732373614720 run.py:483] Algo bellman_ford step 789 current loss 1.261412, current_train_items 25280.
I0302 18:58:39.298668 22732373614720 run.py:483] Algo bellman_ford step 790 current loss 0.402698, current_train_items 25312.
I0302 18:58:39.314936 22732373614720 run.py:483] Algo bellman_ford step 791 current loss 0.859240, current_train_items 25344.
I0302 18:58:39.336218 22732373614720 run.py:483] Algo bellman_ford step 792 current loss 0.802225, current_train_items 25376.
I0302 18:58:39.364932 22732373614720 run.py:483] Algo bellman_ford step 793 current loss 1.089389, current_train_items 25408.
I0302 18:58:39.394755 22732373614720 run.py:483] Algo bellman_ford step 794 current loss 1.219982, current_train_items 25440.
I0302 18:58:39.413001 22732373614720 run.py:483] Algo bellman_ford step 795 current loss 0.584807, current_train_items 25472.
I0302 18:58:39.428568 22732373614720 run.py:483] Algo bellman_ford step 796 current loss 0.670790, current_train_items 25504.
I0302 18:58:39.451498 22732373614720 run.py:483] Algo bellman_ford step 797 current loss 0.918648, current_train_items 25536.
I0302 18:58:39.479376 22732373614720 run.py:483] Algo bellman_ford step 798 current loss 1.217289, current_train_items 25568.
I0302 18:58:39.509871 22732373614720 run.py:483] Algo bellman_ford step 799 current loss 1.425243, current_train_items 25600.
I0302 18:58:39.528183 22732373614720 run.py:483] Algo bellman_ford step 800 current loss 0.364618, current_train_items 25632.
I0302 18:58:39.535719 22732373614720 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.8408203125, 'score': 0.8408203125, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0302 18:58:39.535830 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.859, current avg val score is 0.841, val scores are: bellman_ford: 0.841
I0302 18:58:39.552426 22732373614720 run.py:483] Algo bellman_ford step 801 current loss 0.864734, current_train_items 25664.
I0302 18:58:39.576237 22732373614720 run.py:483] Algo bellman_ford step 802 current loss 1.413185, current_train_items 25696.
I0302 18:58:39.605410 22732373614720 run.py:483] Algo bellman_ford step 803 current loss 1.532333, current_train_items 25728.
I0302 18:58:39.636199 22732373614720 run.py:483] Algo bellman_ford step 804 current loss 1.362583, current_train_items 25760.
I0302 18:58:39.655005 22732373614720 run.py:483] Algo bellman_ford step 805 current loss 0.385341, current_train_items 25792.
I0302 18:58:39.670478 22732373614720 run.py:483] Algo bellman_ford step 806 current loss 0.755199, current_train_items 25824.
I0302 18:58:39.693630 22732373614720 run.py:483] Algo bellman_ford step 807 current loss 1.171732, current_train_items 25856.
I0302 18:58:39.722342 22732373614720 run.py:483] Algo bellman_ford step 808 current loss 1.228177, current_train_items 25888.
I0302 18:58:39.752219 22732373614720 run.py:483] Algo bellman_ford step 809 current loss 1.562884, current_train_items 25920.
I0302 18:58:39.770567 22732373614720 run.py:483] Algo bellman_ford step 810 current loss 0.563163, current_train_items 25952.
I0302 18:58:39.786934 22732373614720 run.py:483] Algo bellman_ford step 811 current loss 0.990844, current_train_items 25984.
I0302 18:58:39.809037 22732373614720 run.py:483] Algo bellman_ford step 812 current loss 0.941631, current_train_items 26016.
I0302 18:58:39.836846 22732373614720 run.py:483] Algo bellman_ford step 813 current loss 0.998001, current_train_items 26048.
I0302 18:58:39.867840 22732373614720 run.py:483] Algo bellman_ford step 814 current loss 1.253985, current_train_items 26080.
I0302 18:58:39.886327 22732373614720 run.py:483] Algo bellman_ford step 815 current loss 0.488704, current_train_items 26112.
I0302 18:58:39.902517 22732373614720 run.py:483] Algo bellman_ford step 816 current loss 0.917321, current_train_items 26144.
I0302 18:58:39.925219 22732373614720 run.py:483] Algo bellman_ford step 817 current loss 1.062210, current_train_items 26176.
I0302 18:58:39.954706 22732373614720 run.py:483] Algo bellman_ford step 818 current loss 1.496088, current_train_items 26208.
I0302 18:58:39.987504 22732373614720 run.py:483] Algo bellman_ford step 819 current loss 1.928462, current_train_items 26240.
I0302 18:58:40.005900 22732373614720 run.py:483] Algo bellman_ford step 820 current loss 0.539967, current_train_items 26272.
I0302 18:58:40.021553 22732373614720 run.py:483] Algo bellman_ford step 821 current loss 0.742996, current_train_items 26304.
I0302 18:58:40.043834 22732373614720 run.py:483] Algo bellman_ford step 822 current loss 0.906135, current_train_items 26336.
I0302 18:58:40.072371 22732373614720 run.py:483] Algo bellman_ford step 823 current loss 1.208376, current_train_items 26368.
I0302 18:58:40.102344 22732373614720 run.py:483] Algo bellman_ford step 824 current loss 1.459718, current_train_items 26400.
I0302 18:58:40.120765 22732373614720 run.py:483] Algo bellman_ford step 825 current loss 0.622022, current_train_items 26432.
I0302 18:58:40.137028 22732373614720 run.py:483] Algo bellman_ford step 826 current loss 0.891535, current_train_items 26464.
I0302 18:58:40.160567 22732373614720 run.py:483] Algo bellman_ford step 827 current loss 1.040900, current_train_items 26496.
I0302 18:58:40.189291 22732373614720 run.py:483] Algo bellman_ford step 828 current loss 1.213503, current_train_items 26528.
I0302 18:58:40.220280 22732373614720 run.py:483] Algo bellman_ford step 829 current loss 1.430575, current_train_items 26560.
I0302 18:58:40.238298 22732373614720 run.py:483] Algo bellman_ford step 830 current loss 0.415533, current_train_items 26592.
I0302 18:58:40.254316 22732373614720 run.py:483] Algo bellman_ford step 831 current loss 0.724427, current_train_items 26624.
I0302 18:58:40.277150 22732373614720 run.py:483] Algo bellman_ford step 832 current loss 1.074190, current_train_items 26656.
I0302 18:58:40.305480 22732373614720 run.py:483] Algo bellman_ford step 833 current loss 1.264482, current_train_items 26688.
I0302 18:58:40.335320 22732373614720 run.py:483] Algo bellman_ford step 834 current loss 1.262675, current_train_items 26720.
I0302 18:58:40.353516 22732373614720 run.py:483] Algo bellman_ford step 835 current loss 0.455886, current_train_items 26752.
I0302 18:58:40.369281 22732373614720 run.py:483] Algo bellman_ford step 836 current loss 0.699420, current_train_items 26784.
I0302 18:58:40.392062 22732373614720 run.py:483] Algo bellman_ford step 837 current loss 1.127832, current_train_items 26816.
I0302 18:58:40.420642 22732373614720 run.py:483] Algo bellman_ford step 838 current loss 1.085380, current_train_items 26848.
I0302 18:58:40.452516 22732373614720 run.py:483] Algo bellman_ford step 839 current loss 1.406865, current_train_items 26880.
I0302 18:58:40.470394 22732373614720 run.py:483] Algo bellman_ford step 840 current loss 0.528407, current_train_items 26912.
I0302 18:58:40.486297 22732373614720 run.py:483] Algo bellman_ford step 841 current loss 0.770369, current_train_items 26944.
I0302 18:58:40.509174 22732373614720 run.py:483] Algo bellman_ford step 842 current loss 1.039805, current_train_items 26976.
I0302 18:58:40.536732 22732373614720 run.py:483] Algo bellman_ford step 843 current loss 0.997697, current_train_items 27008.
I0302 18:58:40.566498 22732373614720 run.py:483] Algo bellman_ford step 844 current loss 1.134554, current_train_items 27040.
I0302 18:58:40.584724 22732373614720 run.py:483] Algo bellman_ford step 845 current loss 0.622966, current_train_items 27072.
I0302 18:58:40.600798 22732373614720 run.py:483] Algo bellman_ford step 846 current loss 0.904418, current_train_items 27104.
I0302 18:58:40.624598 22732373614720 run.py:483] Algo bellman_ford step 847 current loss 1.109149, current_train_items 27136.
I0302 18:58:40.652582 22732373614720 run.py:483] Algo bellman_ford step 848 current loss 1.052388, current_train_items 27168.
I0302 18:58:40.684257 22732373614720 run.py:483] Algo bellman_ford step 849 current loss 1.655959, current_train_items 27200.
I0302 18:58:40.702682 22732373614720 run.py:483] Algo bellman_ford step 850 current loss 0.447674, current_train_items 27232.
I0302 18:58:40.710829 22732373614720 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.8046875, 'score': 0.8046875, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0302 18:58:40.710936 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.859, current avg val score is 0.805, val scores are: bellman_ford: 0.805
I0302 18:58:40.727026 22732373614720 run.py:483] Algo bellman_ford step 851 current loss 0.666873, current_train_items 27264.
I0302 18:58:40.750260 22732373614720 run.py:483] Algo bellman_ford step 852 current loss 1.034812, current_train_items 27296.
I0302 18:58:40.780513 22732373614720 run.py:483] Algo bellman_ford step 853 current loss 1.324683, current_train_items 27328.
I0302 18:58:40.811198 22732373614720 run.py:483] Algo bellman_ford step 854 current loss 1.420524, current_train_items 27360.
I0302 18:58:40.829921 22732373614720 run.py:483] Algo bellman_ford step 855 current loss 0.506424, current_train_items 27392.
I0302 18:58:40.845736 22732373614720 run.py:483] Algo bellman_ford step 856 current loss 0.622541, current_train_items 27424.
I0302 18:58:40.867832 22732373614720 run.py:483] Algo bellman_ford step 857 current loss 0.897306, current_train_items 27456.
I0302 18:58:40.897783 22732373614720 run.py:483] Algo bellman_ford step 858 current loss 1.213122, current_train_items 27488.
I0302 18:58:40.928272 22732373614720 run.py:483] Algo bellman_ford step 859 current loss 1.177929, current_train_items 27520.
I0302 18:58:40.946944 22732373614720 run.py:483] Algo bellman_ford step 860 current loss 0.474703, current_train_items 27552.
I0302 18:58:40.963150 22732373614720 run.py:483] Algo bellman_ford step 861 current loss 0.826362, current_train_items 27584.
I0302 18:58:40.985557 22732373614720 run.py:483] Algo bellman_ford step 862 current loss 1.018710, current_train_items 27616.
I0302 18:58:41.013821 22732373614720 run.py:483] Algo bellman_ford step 863 current loss 1.087484, current_train_items 27648.
I0302 18:58:41.044126 22732373614720 run.py:483] Algo bellman_ford step 864 current loss 1.391547, current_train_items 27680.
I0302 18:58:41.062216 22732373614720 run.py:483] Algo bellman_ford step 865 current loss 0.543006, current_train_items 27712.
I0302 18:58:41.077831 22732373614720 run.py:483] Algo bellman_ford step 866 current loss 0.850422, current_train_items 27744.
I0302 18:58:41.101165 22732373614720 run.py:483] Algo bellman_ford step 867 current loss 1.189021, current_train_items 27776.
I0302 18:58:41.129684 22732373614720 run.py:483] Algo bellman_ford step 868 current loss 0.999293, current_train_items 27808.
I0302 18:58:41.161850 22732373614720 run.py:483] Algo bellman_ford step 869 current loss 1.271671, current_train_items 27840.
I0302 18:58:41.180565 22732373614720 run.py:483] Algo bellman_ford step 870 current loss 0.471482, current_train_items 27872.
I0302 18:58:41.196799 22732373614720 run.py:483] Algo bellman_ford step 871 current loss 0.718002, current_train_items 27904.
I0302 18:58:41.218511 22732373614720 run.py:483] Algo bellman_ford step 872 current loss 0.814404, current_train_items 27936.
I0302 18:58:41.247893 22732373614720 run.py:483] Algo bellman_ford step 873 current loss 1.164043, current_train_items 27968.
I0302 18:58:41.278224 22732373614720 run.py:483] Algo bellman_ford step 874 current loss 1.214110, current_train_items 28000.
I0302 18:58:41.296661 22732373614720 run.py:483] Algo bellman_ford step 875 current loss 0.435645, current_train_items 28032.
I0302 18:58:41.313013 22732373614720 run.py:483] Algo bellman_ford step 876 current loss 0.785976, current_train_items 28064.
I0302 18:58:41.337371 22732373614720 run.py:483] Algo bellman_ford step 877 current loss 1.181736, current_train_items 28096.
I0302 18:58:41.365207 22732373614720 run.py:483] Algo bellman_ford step 878 current loss 1.068289, current_train_items 28128.
I0302 18:58:41.396988 22732373614720 run.py:483] Algo bellman_ford step 879 current loss 1.470646, current_train_items 28160.
I0302 18:58:41.415230 22732373614720 run.py:483] Algo bellman_ford step 880 current loss 0.430728, current_train_items 28192.
I0302 18:58:41.430988 22732373614720 run.py:483] Algo bellman_ford step 881 current loss 0.704279, current_train_items 28224.
I0302 18:58:41.452864 22732373614720 run.py:483] Algo bellman_ford step 882 current loss 0.842549, current_train_items 28256.
I0302 18:58:41.481817 22732373614720 run.py:483] Algo bellman_ford step 883 current loss 1.122526, current_train_items 28288.
I0302 18:58:41.514419 22732373614720 run.py:483] Algo bellman_ford step 884 current loss 1.524273, current_train_items 28320.
I0302 18:58:41.533146 22732373614720 run.py:483] Algo bellman_ford step 885 current loss 0.420374, current_train_items 28352.
I0302 18:58:41.549091 22732373614720 run.py:483] Algo bellman_ford step 886 current loss 0.698737, current_train_items 28384.
I0302 18:58:41.572317 22732373614720 run.py:483] Algo bellman_ford step 887 current loss 1.027483, current_train_items 28416.
I0302 18:58:41.600634 22732373614720 run.py:483] Algo bellman_ford step 888 current loss 1.084521, current_train_items 28448.
I0302 18:58:41.632797 22732373614720 run.py:483] Algo bellman_ford step 889 current loss 1.283047, current_train_items 28480.
I0302 18:58:41.651585 22732373614720 run.py:483] Algo bellman_ford step 890 current loss 0.469047, current_train_items 28512.
I0302 18:58:41.667860 22732373614720 run.py:483] Algo bellman_ford step 891 current loss 0.663195, current_train_items 28544.
I0302 18:58:41.690494 22732373614720 run.py:483] Algo bellman_ford step 892 current loss 1.169515, current_train_items 28576.
I0302 18:58:41.720065 22732373614720 run.py:483] Algo bellman_ford step 893 current loss 1.151828, current_train_items 28608.
I0302 18:58:41.750823 22732373614720 run.py:483] Algo bellman_ford step 894 current loss 1.224855, current_train_items 28640.
I0302 18:58:41.769246 22732373614720 run.py:483] Algo bellman_ford step 895 current loss 0.428301, current_train_items 28672.
I0302 18:58:41.784995 22732373614720 run.py:483] Algo bellman_ford step 896 current loss 0.623984, current_train_items 28704.
I0302 18:58:41.808103 22732373614720 run.py:483] Algo bellman_ford step 897 current loss 1.054403, current_train_items 28736.
I0302 18:58:41.837391 22732373614720 run.py:483] Algo bellman_ford step 898 current loss 1.124290, current_train_items 28768.
I0302 18:58:41.870373 22732373614720 run.py:483] Algo bellman_ford step 899 current loss 1.398387, current_train_items 28800.
I0302 18:58:41.888773 22732373614720 run.py:483] Algo bellman_ford step 900 current loss 0.453347, current_train_items 28832.
I0302 18:58:41.896417 22732373614720 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.8701171875, 'score': 0.8701171875, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0302 18:58:41.896527 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.859, current avg val score is 0.870, val scores are: bellman_ford: 0.870
I0302 18:58:41.927085 22732373614720 run.py:483] Algo bellman_ford step 901 current loss 0.722297, current_train_items 28864.
I0302 18:58:41.950761 22732373614720 run.py:483] Algo bellman_ford step 902 current loss 1.066032, current_train_items 28896.
I0302 18:58:41.981038 22732373614720 run.py:483] Algo bellman_ford step 903 current loss 1.100774, current_train_items 28928.
I0302 18:58:42.013061 22732373614720 run.py:483] Algo bellman_ford step 904 current loss 1.237970, current_train_items 28960.
I0302 18:58:42.032200 22732373614720 run.py:483] Algo bellman_ford step 905 current loss 0.344158, current_train_items 28992.
I0302 18:58:42.048458 22732373614720 run.py:483] Algo bellman_ford step 906 current loss 0.853431, current_train_items 29024.
I0302 18:58:42.071442 22732373614720 run.py:483] Algo bellman_ford step 907 current loss 0.934765, current_train_items 29056.
I0302 18:58:42.101441 22732373614720 run.py:483] Algo bellman_ford step 908 current loss 1.217195, current_train_items 29088.
I0302 18:58:42.133435 22732373614720 run.py:483] Algo bellman_ford step 909 current loss 1.372034, current_train_items 29120.
I0302 18:58:42.152138 22732373614720 run.py:483] Algo bellman_ford step 910 current loss 0.412511, current_train_items 29152.
I0302 18:58:42.168830 22732373614720 run.py:483] Algo bellman_ford step 911 current loss 0.819549, current_train_items 29184.
I0302 18:58:42.192378 22732373614720 run.py:483] Algo bellman_ford step 912 current loss 1.039825, current_train_items 29216.
I0302 18:58:42.220100 22732373614720 run.py:483] Algo bellman_ford step 913 current loss 0.969591, current_train_items 29248.
I0302 18:58:42.252350 22732373614720 run.py:483] Algo bellman_ford step 914 current loss 1.375965, current_train_items 29280.
I0302 18:58:42.270957 22732373614720 run.py:483] Algo bellman_ford step 915 current loss 0.414359, current_train_items 29312.
I0302 18:58:42.287185 22732373614720 run.py:483] Algo bellman_ford step 916 current loss 0.750478, current_train_items 29344.
I0302 18:58:42.310811 22732373614720 run.py:483] Algo bellman_ford step 917 current loss 1.178169, current_train_items 29376.
I0302 18:58:42.340361 22732373614720 run.py:483] Algo bellman_ford step 918 current loss 1.213895, current_train_items 29408.
I0302 18:58:42.372634 22732373614720 run.py:483] Algo bellman_ford step 919 current loss 1.482780, current_train_items 29440.
I0302 18:58:42.391687 22732373614720 run.py:483] Algo bellman_ford step 920 current loss 0.424436, current_train_items 29472.
I0302 18:58:42.408009 22732373614720 run.py:483] Algo bellman_ford step 921 current loss 0.781093, current_train_items 29504.
I0302 18:58:42.431417 22732373614720 run.py:483] Algo bellman_ford step 922 current loss 0.961446, current_train_items 29536.
I0302 18:58:42.460149 22732373614720 run.py:483] Algo bellman_ford step 923 current loss 0.934412, current_train_items 29568.
I0302 18:58:42.491521 22732373614720 run.py:483] Algo bellman_ford step 924 current loss 1.242669, current_train_items 29600.
I0302 18:58:42.509815 22732373614720 run.py:483] Algo bellman_ford step 925 current loss 0.533589, current_train_items 29632.
I0302 18:58:42.525577 22732373614720 run.py:483] Algo bellman_ford step 926 current loss 0.732337, current_train_items 29664.
I0302 18:58:42.548666 22732373614720 run.py:483] Algo bellman_ford step 927 current loss 1.057849, current_train_items 29696.
I0302 18:58:42.578890 22732373614720 run.py:483] Algo bellman_ford step 928 current loss 1.243489, current_train_items 29728.
I0302 18:58:42.609307 22732373614720 run.py:483] Algo bellman_ford step 929 current loss 1.174081, current_train_items 29760.
I0302 18:58:42.628060 22732373614720 run.py:483] Algo bellman_ford step 930 current loss 0.427397, current_train_items 29792.
I0302 18:58:42.644124 22732373614720 run.py:483] Algo bellman_ford step 931 current loss 0.807918, current_train_items 29824.
I0302 18:58:42.667307 22732373614720 run.py:483] Algo bellman_ford step 932 current loss 1.033581, current_train_items 29856.
I0302 18:58:42.695769 22732373614720 run.py:483] Algo bellman_ford step 933 current loss 1.172444, current_train_items 29888.
I0302 18:58:42.727883 22732373614720 run.py:483] Algo bellman_ford step 934 current loss 1.477268, current_train_items 29920.
I0302 18:58:42.746100 22732373614720 run.py:483] Algo bellman_ford step 935 current loss 0.433097, current_train_items 29952.
I0302 18:58:42.762048 22732373614720 run.py:483] Algo bellman_ford step 936 current loss 0.805185, current_train_items 29984.
I0302 18:58:42.784635 22732373614720 run.py:483] Algo bellman_ford step 937 current loss 0.855846, current_train_items 30016.
I0302 18:58:42.813175 22732373614720 run.py:483] Algo bellman_ford step 938 current loss 1.089518, current_train_items 30048.
I0302 18:58:42.844650 22732373614720 run.py:483] Algo bellman_ford step 939 current loss 1.324309, current_train_items 30080.
I0302 18:58:42.863194 22732373614720 run.py:483] Algo bellman_ford step 940 current loss 0.502199, current_train_items 30112.
I0302 18:58:42.879339 22732373614720 run.py:483] Algo bellman_ford step 941 current loss 0.891616, current_train_items 30144.
I0302 18:58:42.903055 22732373614720 run.py:483] Algo bellman_ford step 942 current loss 1.155441, current_train_items 30176.
I0302 18:58:42.932187 22732373614720 run.py:483] Algo bellman_ford step 943 current loss 1.012476, current_train_items 30208.
I0302 18:58:42.964434 22732373614720 run.py:483] Algo bellman_ford step 944 current loss 1.530786, current_train_items 30240.
I0302 18:58:42.982724 22732373614720 run.py:483] Algo bellman_ford step 945 current loss 0.384279, current_train_items 30272.
I0302 18:58:42.998258 22732373614720 run.py:483] Algo bellman_ford step 946 current loss 0.739742, current_train_items 30304.
I0302 18:58:43.022438 22732373614720 run.py:483] Algo bellman_ford step 947 current loss 1.122877, current_train_items 30336.
I0302 18:58:43.050668 22732373614720 run.py:483] Algo bellman_ford step 948 current loss 1.020948, current_train_items 30368.
I0302 18:58:43.081515 22732373614720 run.py:483] Algo bellman_ford step 949 current loss 1.395629, current_train_items 30400.
I0302 18:58:43.099970 22732373614720 run.py:483] Algo bellman_ford step 950 current loss 0.524800, current_train_items 30432.
I0302 18:58:43.107884 22732373614720 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.8369140625, 'score': 0.8369140625, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0302 18:58:43.107995 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.870, current avg val score is 0.837, val scores are: bellman_ford: 0.837
I0302 18:58:43.124640 22732373614720 run.py:483] Algo bellman_ford step 951 current loss 0.688625, current_train_items 30464.
I0302 18:58:43.148181 22732373614720 run.py:483] Algo bellman_ford step 952 current loss 0.972246, current_train_items 30496.
I0302 18:58:43.177456 22732373614720 run.py:483] Algo bellman_ford step 953 current loss 1.199540, current_train_items 30528.
I0302 18:58:43.206711 22732373614720 run.py:483] Algo bellman_ford step 954 current loss 1.112549, current_train_items 30560.
I0302 18:58:43.225712 22732373614720 run.py:483] Algo bellman_ford step 955 current loss 0.389438, current_train_items 30592.
I0302 18:58:43.241504 22732373614720 run.py:483] Algo bellman_ford step 956 current loss 0.774431, current_train_items 30624.
I0302 18:58:43.265023 22732373614720 run.py:483] Algo bellman_ford step 957 current loss 1.111128, current_train_items 30656.
I0302 18:58:43.292578 22732373614720 run.py:483] Algo bellman_ford step 958 current loss 1.109769, current_train_items 30688.
I0302 18:58:43.324036 22732373614720 run.py:483] Algo bellman_ford step 959 current loss 1.361026, current_train_items 30720.
I0302 18:58:43.342664 22732373614720 run.py:483] Algo bellman_ford step 960 current loss 0.434463, current_train_items 30752.
I0302 18:58:43.359287 22732373614720 run.py:483] Algo bellman_ford step 961 current loss 0.891976, current_train_items 30784.
I0302 18:58:43.380506 22732373614720 run.py:483] Algo bellman_ford step 962 current loss 0.805604, current_train_items 30816.
I0302 18:58:43.407827 22732373614720 run.py:483] Algo bellman_ford step 963 current loss 0.983982, current_train_items 30848.
I0302 18:58:43.440175 22732373614720 run.py:483] Algo bellman_ford step 964 current loss 1.316157, current_train_items 30880.
I0302 18:58:43.458442 22732373614720 run.py:483] Algo bellman_ford step 965 current loss 0.612684, current_train_items 30912.
I0302 18:58:43.474277 22732373614720 run.py:483] Algo bellman_ford step 966 current loss 0.638247, current_train_items 30944.
I0302 18:58:43.497339 22732373614720 run.py:483] Algo bellman_ford step 967 current loss 0.937136, current_train_items 30976.
I0302 18:58:43.526652 22732373614720 run.py:483] Algo bellman_ford step 968 current loss 1.022412, current_train_items 31008.
I0302 18:58:43.557484 22732373614720 run.py:483] Algo bellman_ford step 969 current loss 1.336388, current_train_items 31040.
I0302 18:58:43.575828 22732373614720 run.py:483] Algo bellman_ford step 970 current loss 0.482981, current_train_items 31072.
I0302 18:58:43.592329 22732373614720 run.py:483] Algo bellman_ford step 971 current loss 0.756587, current_train_items 31104.
I0302 18:58:43.615586 22732373614720 run.py:483] Algo bellman_ford step 972 current loss 1.052600, current_train_items 31136.
I0302 18:58:43.644967 22732373614720 run.py:483] Algo bellman_ford step 973 current loss 1.117234, current_train_items 31168.
I0302 18:58:43.677462 22732373614720 run.py:483] Algo bellman_ford step 974 current loss 1.476312, current_train_items 31200.
I0302 18:58:43.696070 22732373614720 run.py:483] Algo bellman_ford step 975 current loss 0.562379, current_train_items 31232.
I0302 18:58:43.711835 22732373614720 run.py:483] Algo bellman_ford step 976 current loss 0.580756, current_train_items 31264.
I0302 18:58:43.734284 22732373614720 run.py:483] Algo bellman_ford step 977 current loss 1.013425, current_train_items 31296.
I0302 18:58:43.762913 22732373614720 run.py:483] Algo bellman_ford step 978 current loss 0.980012, current_train_items 31328.
I0302 18:58:43.793940 22732373614720 run.py:483] Algo bellman_ford step 979 current loss 1.161768, current_train_items 31360.
I0302 18:58:43.811946 22732373614720 run.py:483] Algo bellman_ford step 980 current loss 0.390538, current_train_items 31392.
I0302 18:58:43.828025 22732373614720 run.py:483] Algo bellman_ford step 981 current loss 0.785170, current_train_items 31424.
I0302 18:58:43.850325 22732373614720 run.py:483] Algo bellman_ford step 982 current loss 0.895012, current_train_items 31456.
I0302 18:58:43.878696 22732373614720 run.py:483] Algo bellman_ford step 983 current loss 1.137705, current_train_items 31488.
I0302 18:58:43.910428 22732373614720 run.py:483] Algo bellman_ford step 984 current loss 1.272380, current_train_items 31520.
I0302 18:58:43.929552 22732373614720 run.py:483] Algo bellman_ford step 985 current loss 0.580113, current_train_items 31552.
I0302 18:58:43.945933 22732373614720 run.py:483] Algo bellman_ford step 986 current loss 0.851516, current_train_items 31584.
I0302 18:58:43.967933 22732373614720 run.py:483] Algo bellman_ford step 987 current loss 1.065492, current_train_items 31616.
I0302 18:58:43.996877 22732373614720 run.py:483] Algo bellman_ford step 988 current loss 1.071531, current_train_items 31648.
I0302 18:58:44.027434 22732373614720 run.py:483] Algo bellman_ford step 989 current loss 1.110696, current_train_items 31680.
I0302 18:58:44.045686 22732373614720 run.py:483] Algo bellman_ford step 990 current loss 0.384750, current_train_items 31712.
I0302 18:58:44.061396 22732373614720 run.py:483] Algo bellman_ford step 991 current loss 0.629446, current_train_items 31744.
I0302 18:58:44.084363 22732373614720 run.py:483] Algo bellman_ford step 992 current loss 0.984110, current_train_items 31776.
I0302 18:58:44.111490 22732373614720 run.py:483] Algo bellman_ford step 993 current loss 1.050449, current_train_items 31808.
I0302 18:58:44.144954 22732373614720 run.py:483] Algo bellman_ford step 994 current loss 1.613590, current_train_items 31840.
I0302 18:58:44.163250 22732373614720 run.py:483] Algo bellman_ford step 995 current loss 0.481927, current_train_items 31872.
I0302 18:58:44.178969 22732373614720 run.py:483] Algo bellman_ford step 996 current loss 0.749710, current_train_items 31904.
I0302 18:58:44.201664 22732373614720 run.py:483] Algo bellman_ford step 997 current loss 0.911334, current_train_items 31936.
I0302 18:58:44.231112 22732373614720 run.py:483] Algo bellman_ford step 998 current loss 1.077826, current_train_items 31968.
I0302 18:58:44.261819 22732373614720 run.py:483] Algo bellman_ford step 999 current loss 1.190732, current_train_items 32000.
I0302 18:58:44.280197 22732373614720 run.py:483] Algo bellman_ford step 1000 current loss 0.401610, current_train_items 32032.
I0302 18:58:44.287944 22732373614720 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.87109375, 'score': 0.87109375, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0302 18:58:44.288053 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.870, current avg val score is 0.871, val scores are: bellman_ford: 0.871
I0302 18:58:44.316093 22732373614720 run.py:483] Algo bellman_ford step 1001 current loss 0.737885, current_train_items 32064.
I0302 18:58:44.339483 22732373614720 run.py:483] Algo bellman_ford step 1002 current loss 0.817375, current_train_items 32096.
I0302 18:58:44.367346 22732373614720 run.py:483] Algo bellman_ford step 1003 current loss 1.253178, current_train_items 32128.
I0302 18:58:44.401855 22732373614720 run.py:483] Algo bellman_ford step 1004 current loss 1.446227, current_train_items 32160.
I0302 18:58:44.420850 22732373614720 run.py:483] Algo bellman_ford step 1005 current loss 0.455607, current_train_items 32192.
I0302 18:58:44.436413 22732373614720 run.py:483] Algo bellman_ford step 1006 current loss 0.741733, current_train_items 32224.
I0302 18:58:44.460506 22732373614720 run.py:483] Algo bellman_ford step 1007 current loss 0.946305, current_train_items 32256.
I0302 18:58:44.488856 22732373614720 run.py:483] Algo bellman_ford step 1008 current loss 0.999552, current_train_items 32288.
I0302 18:58:44.521564 22732373614720 run.py:483] Algo bellman_ford step 1009 current loss 1.233393, current_train_items 32320.
I0302 18:58:44.540282 22732373614720 run.py:483] Algo bellman_ford step 1010 current loss 0.423867, current_train_items 32352.
I0302 18:58:44.556467 22732373614720 run.py:483] Algo bellman_ford step 1011 current loss 0.697314, current_train_items 32384.
I0302 18:58:44.580333 22732373614720 run.py:483] Algo bellman_ford step 1012 current loss 1.008246, current_train_items 32416.
I0302 18:58:44.609735 22732373614720 run.py:483] Algo bellman_ford step 1013 current loss 1.168712, current_train_items 32448.
I0302 18:58:44.642326 22732373614720 run.py:483] Algo bellman_ford step 1014 current loss 1.143976, current_train_items 32480.
I0302 18:58:44.660900 22732373614720 run.py:483] Algo bellman_ford step 1015 current loss 0.482358, current_train_items 32512.
I0302 18:58:44.677246 22732373614720 run.py:483] Algo bellman_ford step 1016 current loss 0.756507, current_train_items 32544.
I0302 18:58:44.700638 22732373614720 run.py:483] Algo bellman_ford step 1017 current loss 1.131499, current_train_items 32576.
I0302 18:58:44.729732 22732373614720 run.py:483] Algo bellman_ford step 1018 current loss 1.068041, current_train_items 32608.
I0302 18:58:44.763716 22732373614720 run.py:483] Algo bellman_ford step 1019 current loss 1.455568, current_train_items 32640.
I0302 18:58:44.782049 22732373614720 run.py:483] Algo bellman_ford step 1020 current loss 0.439070, current_train_items 32672.
I0302 18:58:44.797937 22732373614720 run.py:483] Algo bellman_ford step 1021 current loss 0.658552, current_train_items 32704.
I0302 18:58:44.820549 22732373614720 run.py:483] Algo bellman_ford step 1022 current loss 0.853376, current_train_items 32736.
I0302 18:58:44.848712 22732373614720 run.py:483] Algo bellman_ford step 1023 current loss 1.070017, current_train_items 32768.
I0302 18:58:44.878834 22732373614720 run.py:483] Algo bellman_ford step 1024 current loss 1.316668, current_train_items 32800.
I0302 18:58:44.897497 22732373614720 run.py:483] Algo bellman_ford step 1025 current loss 0.489282, current_train_items 32832.
I0302 18:58:44.913994 22732373614720 run.py:483] Algo bellman_ford step 1026 current loss 0.779635, current_train_items 32864.
I0302 18:58:44.936772 22732373614720 run.py:483] Algo bellman_ford step 1027 current loss 0.962512, current_train_items 32896.
I0302 18:58:44.966535 22732373614720 run.py:483] Algo bellman_ford step 1028 current loss 1.101188, current_train_items 32928.
I0302 18:58:44.997431 22732373614720 run.py:483] Algo bellman_ford step 1029 current loss 1.194854, current_train_items 32960.
I0302 18:58:45.016229 22732373614720 run.py:483] Algo bellman_ford step 1030 current loss 0.534995, current_train_items 32992.
I0302 18:58:45.032009 22732373614720 run.py:483] Algo bellman_ford step 1031 current loss 0.571889, current_train_items 33024.
I0302 18:58:45.054477 22732373614720 run.py:483] Algo bellman_ford step 1032 current loss 0.820732, current_train_items 33056.
I0302 18:58:45.082956 22732373614720 run.py:483] Algo bellman_ford step 1033 current loss 1.010775, current_train_items 33088.
I0302 18:58:45.115161 22732373614720 run.py:483] Algo bellman_ford step 1034 current loss 1.312481, current_train_items 33120.
I0302 18:58:45.133681 22732373614720 run.py:483] Algo bellman_ford step 1035 current loss 0.433100, current_train_items 33152.
I0302 18:58:45.149825 22732373614720 run.py:483] Algo bellman_ford step 1036 current loss 0.646373, current_train_items 33184.
I0302 18:58:45.173639 22732373614720 run.py:483] Algo bellman_ford step 1037 current loss 0.879925, current_train_items 33216.
I0302 18:58:45.203512 22732373614720 run.py:483] Algo bellman_ford step 1038 current loss 1.192470, current_train_items 33248.
I0302 18:58:45.236367 22732373614720 run.py:483] Algo bellman_ford step 1039 current loss 1.401540, current_train_items 33280.
I0302 18:58:45.254859 22732373614720 run.py:483] Algo bellman_ford step 1040 current loss 0.394811, current_train_items 33312.
I0302 18:58:45.271563 22732373614720 run.py:483] Algo bellman_ford step 1041 current loss 0.855035, current_train_items 33344.
I0302 18:58:45.294850 22732373614720 run.py:483] Algo bellman_ford step 1042 current loss 1.025983, current_train_items 33376.
I0302 18:58:45.324034 22732373614720 run.py:483] Algo bellman_ford step 1043 current loss 1.005395, current_train_items 33408.
I0302 18:58:45.355050 22732373614720 run.py:483] Algo bellman_ford step 1044 current loss 1.149060, current_train_items 33440.
I0302 18:58:45.373257 22732373614720 run.py:483] Algo bellman_ford step 1045 current loss 0.338149, current_train_items 33472.
I0302 18:58:45.388889 22732373614720 run.py:483] Algo bellman_ford step 1046 current loss 0.546022, current_train_items 33504.
I0302 18:58:45.412149 22732373614720 run.py:483] Algo bellman_ford step 1047 current loss 1.124828, current_train_items 33536.
I0302 18:58:45.442321 22732373614720 run.py:483] Algo bellman_ford step 1048 current loss 1.258230, current_train_items 33568.
I0302 18:58:45.474681 22732373614720 run.py:483] Algo bellman_ford step 1049 current loss 1.317912, current_train_items 33600.
I0302 18:58:45.493490 22732373614720 run.py:483] Algo bellman_ford step 1050 current loss 0.440519, current_train_items 33632.
I0302 18:58:45.501594 22732373614720 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0302 18:58:45.501704 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.871, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 18:58:45.529441 22732373614720 run.py:483] Algo bellman_ford step 1051 current loss 0.640567, current_train_items 33664.
I0302 18:58:45.553272 22732373614720 run.py:483] Algo bellman_ford step 1052 current loss 1.009048, current_train_items 33696.
I0302 18:58:45.581992 22732373614720 run.py:483] Algo bellman_ford step 1053 current loss 1.038182, current_train_items 33728.
I0302 18:58:45.614300 22732373614720 run.py:483] Algo bellman_ford step 1054 current loss 1.207509, current_train_items 33760.
I0302 18:58:45.633573 22732373614720 run.py:483] Algo bellman_ford step 1055 current loss 0.441590, current_train_items 33792.
I0302 18:58:45.649647 22732373614720 run.py:483] Algo bellman_ford step 1056 current loss 0.757162, current_train_items 33824.
I0302 18:58:45.673393 22732373614720 run.py:483] Algo bellman_ford step 1057 current loss 1.041438, current_train_items 33856.
I0302 18:58:45.703291 22732373614720 run.py:483] Algo bellman_ford step 1058 current loss 1.178217, current_train_items 33888.
I0302 18:58:45.733100 22732373614720 run.py:483] Algo bellman_ford step 1059 current loss 1.316758, current_train_items 33920.
I0302 18:58:45.752020 22732373614720 run.py:483] Algo bellman_ford step 1060 current loss 0.394681, current_train_items 33952.
I0302 18:58:45.768040 22732373614720 run.py:483] Algo bellman_ford step 1061 current loss 0.734452, current_train_items 33984.
I0302 18:58:45.789482 22732373614720 run.py:483] Algo bellman_ford step 1062 current loss 0.778567, current_train_items 34016.
I0302 18:58:45.817819 22732373614720 run.py:483] Algo bellman_ford step 1063 current loss 1.072590, current_train_items 34048.
I0302 18:58:45.849851 22732373614720 run.py:483] Algo bellman_ford step 1064 current loss 1.319996, current_train_items 34080.
I0302 18:58:45.868202 22732373614720 run.py:483] Algo bellman_ford step 1065 current loss 0.407180, current_train_items 34112.
I0302 18:58:45.884137 22732373614720 run.py:483] Algo bellman_ford step 1066 current loss 0.722802, current_train_items 34144.
I0302 18:58:45.907769 22732373614720 run.py:483] Algo bellman_ford step 1067 current loss 0.964606, current_train_items 34176.
I0302 18:58:45.936866 22732373614720 run.py:483] Algo bellman_ford step 1068 current loss 0.976168, current_train_items 34208.
I0302 18:58:45.969722 22732373614720 run.py:483] Algo bellman_ford step 1069 current loss 1.264014, current_train_items 34240.
I0302 18:58:45.988422 22732373614720 run.py:483] Algo bellman_ford step 1070 current loss 0.366648, current_train_items 34272.
I0302 18:58:46.004877 22732373614720 run.py:483] Algo bellman_ford step 1071 current loss 0.772055, current_train_items 34304.
I0302 18:58:46.027845 22732373614720 run.py:483] Algo bellman_ford step 1072 current loss 0.951113, current_train_items 34336.
I0302 18:58:46.056822 22732373614720 run.py:483] Algo bellman_ford step 1073 current loss 0.911837, current_train_items 34368.
I0302 18:58:46.087487 22732373614720 run.py:483] Algo bellman_ford step 1074 current loss 1.163815, current_train_items 34400.
I0302 18:58:46.106524 22732373614720 run.py:483] Algo bellman_ford step 1075 current loss 0.388019, current_train_items 34432.
I0302 18:58:46.122812 22732373614720 run.py:483] Algo bellman_ford step 1076 current loss 0.853416, current_train_items 34464.
I0302 18:58:46.146113 22732373614720 run.py:483] Algo bellman_ford step 1077 current loss 0.986159, current_train_items 34496.
I0302 18:58:46.175256 22732373614720 run.py:483] Algo bellman_ford step 1078 current loss 1.077281, current_train_items 34528.
I0302 18:58:46.206484 22732373614720 run.py:483] Algo bellman_ford step 1079 current loss 1.179227, current_train_items 34560.
I0302 18:58:46.225185 22732373614720 run.py:483] Algo bellman_ford step 1080 current loss 0.397025, current_train_items 34592.
I0302 18:58:46.241794 22732373614720 run.py:483] Algo bellman_ford step 1081 current loss 0.719408, current_train_items 34624.
I0302 18:58:46.264479 22732373614720 run.py:483] Algo bellman_ford step 1082 current loss 0.980050, current_train_items 34656.
I0302 18:58:46.292763 22732373614720 run.py:483] Algo bellman_ford step 1083 current loss 1.050080, current_train_items 34688.
I0302 18:58:46.324716 22732373614720 run.py:483] Algo bellman_ford step 1084 current loss 1.413651, current_train_items 34720.
I0302 18:58:46.343414 22732373614720 run.py:483] Algo bellman_ford step 1085 current loss 0.473400, current_train_items 34752.
I0302 18:58:46.359668 22732373614720 run.py:483] Algo bellman_ford step 1086 current loss 0.727716, current_train_items 34784.
I0302 18:58:46.382764 22732373614720 run.py:483] Algo bellman_ford step 1087 current loss 0.908069, current_train_items 34816.
I0302 18:58:46.410342 22732373614720 run.py:483] Algo bellman_ford step 1088 current loss 0.767486, current_train_items 34848.
I0302 18:58:46.440425 22732373614720 run.py:483] Algo bellman_ford step 1089 current loss 1.048541, current_train_items 34880.
I0302 18:58:46.459379 22732373614720 run.py:483] Algo bellman_ford step 1090 current loss 0.475931, current_train_items 34912.
I0302 18:58:46.475771 22732373614720 run.py:483] Algo bellman_ford step 1091 current loss 0.767027, current_train_items 34944.
I0302 18:58:46.498337 22732373614720 run.py:483] Algo bellman_ford step 1092 current loss 1.024233, current_train_items 34976.
I0302 18:58:46.528357 22732373614720 run.py:483] Algo bellman_ford step 1093 current loss 1.102145, current_train_items 35008.
I0302 18:58:46.560696 22732373614720 run.py:483] Algo bellman_ford step 1094 current loss 1.175419, current_train_items 35040.
I0302 18:58:46.578909 22732373614720 run.py:483] Algo bellman_ford step 1095 current loss 0.434286, current_train_items 35072.
I0302 18:58:46.594959 22732373614720 run.py:483] Algo bellman_ford step 1096 current loss 0.792699, current_train_items 35104.
I0302 18:58:46.617988 22732373614720 run.py:483] Algo bellman_ford step 1097 current loss 0.983932, current_train_items 35136.
I0302 18:58:46.647346 22732373614720 run.py:483] Algo bellman_ford step 1098 current loss 1.092244, current_train_items 35168.
I0302 18:58:46.678300 22732373614720 run.py:483] Algo bellman_ford step 1099 current loss 1.188765, current_train_items 35200.
I0302 18:58:46.697001 22732373614720 run.py:483] Algo bellman_ford step 1100 current loss 0.420007, current_train_items 35232.
I0302 18:58:46.704605 22732373614720 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0302 18:58:46.704714 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.895, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 18:58:46.733252 22732373614720 run.py:483] Algo bellman_ford step 1101 current loss 0.718415, current_train_items 35264.
I0302 18:58:46.755845 22732373614720 run.py:483] Algo bellman_ford step 1102 current loss 0.939072, current_train_items 35296.
I0302 18:58:46.785968 22732373614720 run.py:483] Algo bellman_ford step 1103 current loss 1.035890, current_train_items 35328.
I0302 18:58:46.819329 22732373614720 run.py:483] Algo bellman_ford step 1104 current loss 1.426394, current_train_items 35360.
I0302 18:58:46.837854 22732373614720 run.py:483] Algo bellman_ford step 1105 current loss 0.460796, current_train_items 35392.
I0302 18:58:46.853541 22732373614720 run.py:483] Algo bellman_ford step 1106 current loss 0.716106, current_train_items 35424.
I0302 18:58:46.876675 22732373614720 run.py:483] Algo bellman_ford step 1107 current loss 0.915640, current_train_items 35456.
I0302 18:58:46.904700 22732373614720 run.py:483] Algo bellman_ford step 1108 current loss 1.100306, current_train_items 35488.
I0302 18:58:46.934284 22732373614720 run.py:483] Algo bellman_ford step 1109 current loss 1.159086, current_train_items 35520.
I0302 18:58:46.952738 22732373614720 run.py:483] Algo bellman_ford step 1110 current loss 0.504581, current_train_items 35552.
I0302 18:58:46.968839 22732373614720 run.py:483] Algo bellman_ford step 1111 current loss 0.746577, current_train_items 35584.
I0302 18:58:46.991623 22732373614720 run.py:483] Algo bellman_ford step 1112 current loss 0.901874, current_train_items 35616.
I0302 18:58:47.018762 22732373614720 run.py:483] Algo bellman_ford step 1113 current loss 1.025731, current_train_items 35648.
I0302 18:58:47.051225 22732373614720 run.py:483] Algo bellman_ford step 1114 current loss 1.319215, current_train_items 35680.
I0302 18:58:47.069395 22732373614720 run.py:483] Algo bellman_ford step 1115 current loss 0.395979, current_train_items 35712.
I0302 18:58:47.084984 22732373614720 run.py:483] Algo bellman_ford step 1116 current loss 0.638028, current_train_items 35744.
I0302 18:58:47.108293 22732373614720 run.py:483] Algo bellman_ford step 1117 current loss 0.904752, current_train_items 35776.
I0302 18:58:47.137359 22732373614720 run.py:483] Algo bellman_ford step 1118 current loss 0.960175, current_train_items 35808.
I0302 18:58:47.169029 22732373614720 run.py:483] Algo bellman_ford step 1119 current loss 1.526387, current_train_items 35840.
I0302 18:58:47.187495 22732373614720 run.py:483] Algo bellman_ford step 1120 current loss 0.511900, current_train_items 35872.
I0302 18:58:47.202995 22732373614720 run.py:483] Algo bellman_ford step 1121 current loss 0.575094, current_train_items 35904.
I0302 18:58:47.226026 22732373614720 run.py:483] Algo bellman_ford step 1122 current loss 0.943141, current_train_items 35936.
I0302 18:58:47.253579 22732373614720 run.py:483] Algo bellman_ford step 1123 current loss 1.075978, current_train_items 35968.
I0302 18:58:47.286295 22732373614720 run.py:483] Algo bellman_ford step 1124 current loss 1.305734, current_train_items 36000.
I0302 18:58:47.304576 22732373614720 run.py:483] Algo bellman_ford step 1125 current loss 0.412373, current_train_items 36032.
I0302 18:58:47.320170 22732373614720 run.py:483] Algo bellman_ford step 1126 current loss 0.633480, current_train_items 36064.
I0302 18:58:47.342830 22732373614720 run.py:483] Algo bellman_ford step 1127 current loss 1.084016, current_train_items 36096.
I0302 18:58:47.371934 22732373614720 run.py:483] Algo bellman_ford step 1128 current loss 1.109103, current_train_items 36128.
I0302 18:58:47.401166 22732373614720 run.py:483] Algo bellman_ford step 1129 current loss 1.318183, current_train_items 36160.
I0302 18:58:47.419552 22732373614720 run.py:483] Algo bellman_ford step 1130 current loss 0.446167, current_train_items 36192.
I0302 18:58:47.435489 22732373614720 run.py:483] Algo bellman_ford step 1131 current loss 0.649625, current_train_items 36224.
I0302 18:58:47.458442 22732373614720 run.py:483] Algo bellman_ford step 1132 current loss 0.939624, current_train_items 36256.
I0302 18:58:47.487126 22732373614720 run.py:483] Algo bellman_ford step 1133 current loss 0.990851, current_train_items 36288.
I0302 18:58:47.517299 22732373614720 run.py:483] Algo bellman_ford step 1134 current loss 1.140115, current_train_items 36320.
I0302 18:58:47.535444 22732373614720 run.py:483] Algo bellman_ford step 1135 current loss 0.510769, current_train_items 36352.
I0302 18:58:47.551417 22732373614720 run.py:483] Algo bellman_ford step 1136 current loss 0.678659, current_train_items 36384.
I0302 18:58:47.573593 22732373614720 run.py:483] Algo bellman_ford step 1137 current loss 0.785137, current_train_items 36416.
I0302 18:58:47.603174 22732373614720 run.py:483] Algo bellman_ford step 1138 current loss 1.067646, current_train_items 36448.
I0302 18:58:47.633739 22732373614720 run.py:483] Algo bellman_ford step 1139 current loss 1.045364, current_train_items 36480.
I0302 18:58:47.652059 22732373614720 run.py:483] Algo bellman_ford step 1140 current loss 0.428398, current_train_items 36512.
I0302 18:58:47.668192 22732373614720 run.py:483] Algo bellman_ford step 1141 current loss 0.670988, current_train_items 36544.
I0302 18:58:47.690060 22732373614720 run.py:483] Algo bellman_ford step 1142 current loss 0.859835, current_train_items 36576.
I0302 18:58:47.717897 22732373614720 run.py:483] Algo bellman_ford step 1143 current loss 1.031632, current_train_items 36608.
I0302 18:58:47.748918 22732373614720 run.py:483] Algo bellman_ford step 1144 current loss 1.173240, current_train_items 36640.
I0302 18:58:47.767476 22732373614720 run.py:483] Algo bellman_ford step 1145 current loss 0.548809, current_train_items 36672.
I0302 18:58:47.783484 22732373614720 run.py:483] Algo bellman_ford step 1146 current loss 0.773886, current_train_items 36704.
I0302 18:58:47.806240 22732373614720 run.py:483] Algo bellman_ford step 1147 current loss 0.970739, current_train_items 36736.
I0302 18:58:47.835134 22732373614720 run.py:483] Algo bellman_ford step 1148 current loss 1.007740, current_train_items 36768.
I0302 18:58:47.864066 22732373614720 run.py:483] Algo bellman_ford step 1149 current loss 1.049852, current_train_items 36800.
I0302 18:58:47.882418 22732373614720 run.py:483] Algo bellman_ford step 1150 current loss 0.497343, current_train_items 36832.
I0302 18:58:47.890515 22732373614720 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.8359375, 'score': 0.8359375, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0302 18:58:47.890624 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.901, current avg val score is 0.836, val scores are: bellman_ford: 0.836
I0302 18:58:47.907063 22732373614720 run.py:483] Algo bellman_ford step 1151 current loss 0.684463, current_train_items 36864.
I0302 18:58:47.930775 22732373614720 run.py:483] Algo bellman_ford step 1152 current loss 1.075724, current_train_items 36896.
I0302 18:58:47.961372 22732373614720 run.py:483] Algo bellman_ford step 1153 current loss 1.191411, current_train_items 36928.
W0302 18:58:47.981946 22732373614720 samplers.py:155] Increasing hint lengh from 11 to 12
I0302 18:58:54.552364 22732373614720 run.py:483] Algo bellman_ford step 1154 current loss 1.349620, current_train_items 36960.
I0302 18:58:54.573056 22732373614720 run.py:483] Algo bellman_ford step 1155 current loss 0.452716, current_train_items 36992.
I0302 18:58:54.589094 22732373614720 run.py:483] Algo bellman_ford step 1156 current loss 0.635120, current_train_items 37024.
I0302 18:58:54.612107 22732373614720 run.py:483] Algo bellman_ford step 1157 current loss 0.887085, current_train_items 37056.
I0302 18:58:54.641334 22732373614720 run.py:483] Algo bellman_ford step 1158 current loss 1.119009, current_train_items 37088.
I0302 18:58:54.671478 22732373614720 run.py:483] Algo bellman_ford step 1159 current loss 1.223103, current_train_items 37120.
I0302 18:58:54.691264 22732373614720 run.py:483] Algo bellman_ford step 1160 current loss 0.577051, current_train_items 37152.
I0302 18:58:54.707713 22732373614720 run.py:483] Algo bellman_ford step 1161 current loss 0.659993, current_train_items 37184.
I0302 18:58:54.729784 22732373614720 run.py:483] Algo bellman_ford step 1162 current loss 0.873825, current_train_items 37216.
I0302 18:58:54.758400 22732373614720 run.py:483] Algo bellman_ford step 1163 current loss 0.959848, current_train_items 37248.
I0302 18:58:54.790408 22732373614720 run.py:483] Algo bellman_ford step 1164 current loss 1.266051, current_train_items 37280.
I0302 18:58:54.809055 22732373614720 run.py:483] Algo bellman_ford step 1165 current loss 0.344602, current_train_items 37312.
I0302 18:58:54.825179 22732373614720 run.py:483] Algo bellman_ford step 1166 current loss 0.696360, current_train_items 37344.
I0302 18:58:54.848425 22732373614720 run.py:483] Algo bellman_ford step 1167 current loss 1.012455, current_train_items 37376.
I0302 18:58:54.877632 22732373614720 run.py:483] Algo bellman_ford step 1168 current loss 1.115582, current_train_items 37408.
I0302 18:58:54.910668 22732373614720 run.py:483] Algo bellman_ford step 1169 current loss 1.180676, current_train_items 37440.
I0302 18:58:54.929745 22732373614720 run.py:483] Algo bellman_ford step 1170 current loss 0.415574, current_train_items 37472.
I0302 18:58:54.946267 22732373614720 run.py:483] Algo bellman_ford step 1171 current loss 0.798511, current_train_items 37504.
I0302 18:58:54.968899 22732373614720 run.py:483] Algo bellman_ford step 1172 current loss 1.118286, current_train_items 37536.
I0302 18:58:54.998559 22732373614720 run.py:483] Algo bellman_ford step 1173 current loss 1.227054, current_train_items 37568.
I0302 18:58:55.030613 22732373614720 run.py:483] Algo bellman_ford step 1174 current loss 1.358468, current_train_items 37600.
I0302 18:58:55.049910 22732373614720 run.py:483] Algo bellman_ford step 1175 current loss 0.393162, current_train_items 37632.
I0302 18:58:55.066127 22732373614720 run.py:483] Algo bellman_ford step 1176 current loss 0.769103, current_train_items 37664.
I0302 18:58:55.088323 22732373614720 run.py:483] Algo bellman_ford step 1177 current loss 0.863538, current_train_items 37696.
I0302 18:58:55.116698 22732373614720 run.py:483] Algo bellman_ford step 1178 current loss 0.968126, current_train_items 37728.
I0302 18:58:55.150305 22732373614720 run.py:483] Algo bellman_ford step 1179 current loss 1.229412, current_train_items 37760.
I0302 18:58:55.169328 22732373614720 run.py:483] Algo bellman_ford step 1180 current loss 0.407512, current_train_items 37792.
I0302 18:58:55.185666 22732373614720 run.py:483] Algo bellman_ford step 1181 current loss 0.877085, current_train_items 37824.
I0302 18:58:55.208350 22732373614720 run.py:483] Algo bellman_ford step 1182 current loss 0.891679, current_train_items 37856.
I0302 18:58:55.236433 22732373614720 run.py:483] Algo bellman_ford step 1183 current loss 1.026602, current_train_items 37888.
I0302 18:58:55.268225 22732373614720 run.py:483] Algo bellman_ford step 1184 current loss 1.226846, current_train_items 37920.
I0302 18:58:55.287564 22732373614720 run.py:483] Algo bellman_ford step 1185 current loss 0.460626, current_train_items 37952.
I0302 18:58:55.303403 22732373614720 run.py:483] Algo bellman_ford step 1186 current loss 0.652348, current_train_items 37984.
I0302 18:58:55.326093 22732373614720 run.py:483] Algo bellman_ford step 1187 current loss 0.904620, current_train_items 38016.
I0302 18:58:55.354875 22732373614720 run.py:483] Algo bellman_ford step 1188 current loss 0.985443, current_train_items 38048.
I0302 18:58:55.385151 22732373614720 run.py:483] Algo bellman_ford step 1189 current loss 1.246715, current_train_items 38080.
I0302 18:58:55.404502 22732373614720 run.py:483] Algo bellman_ford step 1190 current loss 0.441388, current_train_items 38112.
I0302 18:58:55.420829 22732373614720 run.py:483] Algo bellman_ford step 1191 current loss 0.628905, current_train_items 38144.
I0302 18:58:55.444513 22732373614720 run.py:483] Algo bellman_ford step 1192 current loss 1.023035, current_train_items 38176.
I0302 18:58:55.473677 22732373614720 run.py:483] Algo bellman_ford step 1193 current loss 1.137596, current_train_items 38208.
I0302 18:58:55.505486 22732373614720 run.py:483] Algo bellman_ford step 1194 current loss 1.192087, current_train_items 38240.
I0302 18:58:55.524410 22732373614720 run.py:483] Algo bellman_ford step 1195 current loss 0.421687, current_train_items 38272.
I0302 18:58:55.540475 22732373614720 run.py:483] Algo bellman_ford step 1196 current loss 0.725086, current_train_items 38304.
I0302 18:58:55.563114 22732373614720 run.py:483] Algo bellman_ford step 1197 current loss 1.106007, current_train_items 38336.
I0302 18:58:55.593266 22732373614720 run.py:483] Algo bellman_ford step 1198 current loss 1.194792, current_train_items 38368.
I0302 18:58:55.626117 22732373614720 run.py:483] Algo bellman_ford step 1199 current loss 1.348325, current_train_items 38400.
I0302 18:58:55.645439 22732373614720 run.py:483] Algo bellman_ford step 1200 current loss 0.492031, current_train_items 38432.
I0302 18:58:55.654924 22732373614720 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.8828125, 'score': 0.8828125, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0302 18:58:55.655034 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.901, current avg val score is 0.883, val scores are: bellman_ford: 0.883
I0302 18:58:55.671925 22732373614720 run.py:483] Algo bellman_ford step 1201 current loss 0.744897, current_train_items 38464.
I0302 18:58:55.695206 22732373614720 run.py:483] Algo bellman_ford step 1202 current loss 0.830484, current_train_items 38496.
I0302 18:58:55.725561 22732373614720 run.py:483] Algo bellman_ford step 1203 current loss 1.054361, current_train_items 38528.
I0302 18:58:55.758553 22732373614720 run.py:483] Algo bellman_ford step 1204 current loss 1.571265, current_train_items 38560.
I0302 18:58:55.777827 22732373614720 run.py:483] Algo bellman_ford step 1205 current loss 0.371443, current_train_items 38592.
I0302 18:58:55.793614 22732373614720 run.py:483] Algo bellman_ford step 1206 current loss 0.645332, current_train_items 38624.
I0302 18:58:55.815782 22732373614720 run.py:483] Algo bellman_ford step 1207 current loss 1.091019, current_train_items 38656.
I0302 18:58:55.845135 22732373614720 run.py:483] Algo bellman_ford step 1208 current loss 1.126937, current_train_items 38688.
I0302 18:58:55.878389 22732373614720 run.py:483] Algo bellman_ford step 1209 current loss 1.552550, current_train_items 38720.
I0302 18:58:55.897279 22732373614720 run.py:483] Algo bellman_ford step 1210 current loss 0.369272, current_train_items 38752.
I0302 18:58:55.913352 22732373614720 run.py:483] Algo bellman_ford step 1211 current loss 0.750496, current_train_items 38784.
I0302 18:58:55.935421 22732373614720 run.py:483] Algo bellman_ford step 1212 current loss 1.028454, current_train_items 38816.
I0302 18:58:55.963505 22732373614720 run.py:483] Algo bellman_ford step 1213 current loss 0.982679, current_train_items 38848.
I0302 18:58:55.995477 22732373614720 run.py:483] Algo bellman_ford step 1214 current loss 1.311611, current_train_items 38880.
I0302 18:58:56.014334 22732373614720 run.py:483] Algo bellman_ford step 1215 current loss 0.494016, current_train_items 38912.
I0302 18:58:56.029984 22732373614720 run.py:483] Algo bellman_ford step 1216 current loss 0.663181, current_train_items 38944.
I0302 18:58:56.052649 22732373614720 run.py:483] Algo bellman_ford step 1217 current loss 0.876989, current_train_items 38976.
I0302 18:58:56.082711 22732373614720 run.py:483] Algo bellman_ford step 1218 current loss 1.186718, current_train_items 39008.
I0302 18:58:56.114957 22732373614720 run.py:483] Algo bellman_ford step 1219 current loss 1.510282, current_train_items 39040.
I0302 18:58:56.134126 22732373614720 run.py:483] Algo bellman_ford step 1220 current loss 0.387560, current_train_items 39072.
I0302 18:58:56.149949 22732373614720 run.py:483] Algo bellman_ford step 1221 current loss 0.687882, current_train_items 39104.
I0302 18:58:56.173772 22732373614720 run.py:483] Algo bellman_ford step 1222 current loss 1.080645, current_train_items 39136.
I0302 18:58:56.203024 22732373614720 run.py:483] Algo bellman_ford step 1223 current loss 1.150210, current_train_items 39168.
I0302 18:58:56.234796 22732373614720 run.py:483] Algo bellman_ford step 1224 current loss 1.259872, current_train_items 39200.
I0302 18:58:56.253822 22732373614720 run.py:483] Algo bellman_ford step 1225 current loss 0.470933, current_train_items 39232.
I0302 18:58:56.269914 22732373614720 run.py:483] Algo bellman_ford step 1226 current loss 0.753929, current_train_items 39264.
I0302 18:58:56.293853 22732373614720 run.py:483] Algo bellman_ford step 1227 current loss 1.026554, current_train_items 39296.
I0302 18:58:56.322393 22732373614720 run.py:483] Algo bellman_ford step 1228 current loss 1.002137, current_train_items 39328.
I0302 18:58:56.355585 22732373614720 run.py:483] Algo bellman_ford step 1229 current loss 1.356212, current_train_items 39360.
I0302 18:58:56.374452 22732373614720 run.py:483] Algo bellman_ford step 1230 current loss 0.367707, current_train_items 39392.
I0302 18:58:56.390566 22732373614720 run.py:483] Algo bellman_ford step 1231 current loss 0.615043, current_train_items 39424.
I0302 18:58:56.414015 22732373614720 run.py:483] Algo bellman_ford step 1232 current loss 1.026762, current_train_items 39456.
I0302 18:58:56.442851 22732373614720 run.py:483] Algo bellman_ford step 1233 current loss 1.162517, current_train_items 39488.
I0302 18:58:56.473094 22732373614720 run.py:483] Algo bellman_ford step 1234 current loss 1.370932, current_train_items 39520.
I0302 18:58:56.491771 22732373614720 run.py:483] Algo bellman_ford step 1235 current loss 0.351223, current_train_items 39552.
I0302 18:58:56.507415 22732373614720 run.py:483] Algo bellman_ford step 1236 current loss 0.690336, current_train_items 39584.
I0302 18:58:56.529922 22732373614720 run.py:483] Algo bellman_ford step 1237 current loss 0.874313, current_train_items 39616.
I0302 18:58:56.558759 22732373614720 run.py:483] Algo bellman_ford step 1238 current loss 1.029736, current_train_items 39648.
I0302 18:58:56.591424 22732373614720 run.py:483] Algo bellman_ford step 1239 current loss 1.277090, current_train_items 39680.
I0302 18:58:56.610237 22732373614720 run.py:483] Algo bellman_ford step 1240 current loss 0.423478, current_train_items 39712.
I0302 18:58:56.626249 22732373614720 run.py:483] Algo bellman_ford step 1241 current loss 0.837577, current_train_items 39744.
I0302 18:58:56.649349 22732373614720 run.py:483] Algo bellman_ford step 1242 current loss 0.877488, current_train_items 39776.
I0302 18:58:56.678043 22732373614720 run.py:483] Algo bellman_ford step 1243 current loss 1.074417, current_train_items 39808.
I0302 18:58:56.709923 22732373614720 run.py:483] Algo bellman_ford step 1244 current loss 1.176026, current_train_items 39840.
I0302 18:58:56.728411 22732373614720 run.py:483] Algo bellman_ford step 1245 current loss 0.374486, current_train_items 39872.
I0302 18:58:56.744217 22732373614720 run.py:483] Algo bellman_ford step 1246 current loss 0.542238, current_train_items 39904.
I0302 18:58:56.766148 22732373614720 run.py:483] Algo bellman_ford step 1247 current loss 0.882905, current_train_items 39936.
I0302 18:58:56.795240 22732373614720 run.py:483] Algo bellman_ford step 1248 current loss 1.105506, current_train_items 39968.
I0302 18:58:56.826259 22732373614720 run.py:483] Algo bellman_ford step 1249 current loss 1.300179, current_train_items 40000.
I0302 18:58:56.844971 22732373614720 run.py:483] Algo bellman_ford step 1250 current loss 0.408800, current_train_items 40032.
I0302 18:58:56.853073 22732373614720 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.8544921875, 'score': 0.8544921875, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0302 18:58:56.853192 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.901, current avg val score is 0.854, val scores are: bellman_ford: 0.854
I0302 18:58:56.869826 22732373614720 run.py:483] Algo bellman_ford step 1251 current loss 0.631830, current_train_items 40064.
I0302 18:58:56.893360 22732373614720 run.py:483] Algo bellman_ford step 1252 current loss 0.970341, current_train_items 40096.
I0302 18:58:56.921125 22732373614720 run.py:483] Algo bellman_ford step 1253 current loss 0.930462, current_train_items 40128.
I0302 18:58:56.950448 22732373614720 run.py:483] Algo bellman_ford step 1254 current loss 1.024681, current_train_items 40160.
I0302 18:58:56.969585 22732373614720 run.py:483] Algo bellman_ford step 1255 current loss 0.400318, current_train_items 40192.
I0302 18:58:56.985627 22732373614720 run.py:483] Algo bellman_ford step 1256 current loss 0.690107, current_train_items 40224.
I0302 18:58:57.008257 22732373614720 run.py:483] Algo bellman_ford step 1257 current loss 0.972176, current_train_items 40256.
I0302 18:58:57.036621 22732373614720 run.py:483] Algo bellman_ford step 1258 current loss 1.026586, current_train_items 40288.
I0302 18:58:57.067596 22732373614720 run.py:483] Algo bellman_ford step 1259 current loss 1.332273, current_train_items 40320.
I0302 18:58:57.086646 22732373614720 run.py:483] Algo bellman_ford step 1260 current loss 0.519162, current_train_items 40352.
I0302 18:58:57.103248 22732373614720 run.py:483] Algo bellman_ford step 1261 current loss 0.799883, current_train_items 40384.
I0302 18:58:57.126074 22732373614720 run.py:483] Algo bellman_ford step 1262 current loss 1.112928, current_train_items 40416.
I0302 18:58:57.152449 22732373614720 run.py:483] Algo bellman_ford step 1263 current loss 0.850186, current_train_items 40448.
I0302 18:58:57.185961 22732373614720 run.py:483] Algo bellman_ford step 1264 current loss 1.323282, current_train_items 40480.
I0302 18:58:57.204535 22732373614720 run.py:483] Algo bellman_ford step 1265 current loss 0.350161, current_train_items 40512.
I0302 18:58:57.220482 22732373614720 run.py:483] Algo bellman_ford step 1266 current loss 0.763008, current_train_items 40544.
I0302 18:58:57.242297 22732373614720 run.py:483] Algo bellman_ford step 1267 current loss 1.016093, current_train_items 40576.
I0302 18:58:57.270476 22732373614720 run.py:483] Algo bellman_ford step 1268 current loss 1.082795, current_train_items 40608.
I0302 18:58:57.302589 22732373614720 run.py:483] Algo bellman_ford step 1269 current loss 1.211251, current_train_items 40640.
I0302 18:58:57.321853 22732373614720 run.py:483] Algo bellman_ford step 1270 current loss 0.403807, current_train_items 40672.
I0302 18:58:57.337752 22732373614720 run.py:483] Algo bellman_ford step 1271 current loss 0.619813, current_train_items 40704.
I0302 18:58:57.359133 22732373614720 run.py:483] Algo bellman_ford step 1272 current loss 0.836542, current_train_items 40736.
I0302 18:58:57.386691 22732373614720 run.py:483] Algo bellman_ford step 1273 current loss 0.878478, current_train_items 40768.
I0302 18:58:57.418579 22732373614720 run.py:483] Algo bellman_ford step 1274 current loss 1.352164, current_train_items 40800.
I0302 18:58:57.437738 22732373614720 run.py:483] Algo bellman_ford step 1275 current loss 0.526300, current_train_items 40832.
I0302 18:58:57.453864 22732373614720 run.py:483] Algo bellman_ford step 1276 current loss 0.770140, current_train_items 40864.
I0302 18:58:57.475281 22732373614720 run.py:483] Algo bellman_ford step 1277 current loss 0.957200, current_train_items 40896.
I0302 18:58:57.503675 22732373614720 run.py:483] Algo bellman_ford step 1278 current loss 0.938726, current_train_items 40928.
I0302 18:58:57.534765 22732373614720 run.py:483] Algo bellman_ford step 1279 current loss 1.173583, current_train_items 40960.
I0302 18:58:57.553725 22732373614720 run.py:483] Algo bellman_ford step 1280 current loss 0.452157, current_train_items 40992.
I0302 18:58:57.569562 22732373614720 run.py:483] Algo bellman_ford step 1281 current loss 0.595863, current_train_items 41024.
I0302 18:58:57.592274 22732373614720 run.py:483] Algo bellman_ford step 1282 current loss 1.004015, current_train_items 41056.
I0302 18:58:57.621757 22732373614720 run.py:483] Algo bellman_ford step 1283 current loss 1.193716, current_train_items 41088.
I0302 18:58:57.652614 22732373614720 run.py:483] Algo bellman_ford step 1284 current loss 1.365625, current_train_items 41120.
I0302 18:58:57.671741 22732373614720 run.py:483] Algo bellman_ford step 1285 current loss 0.426260, current_train_items 41152.
I0302 18:58:57.687804 22732373614720 run.py:483] Algo bellman_ford step 1286 current loss 0.701433, current_train_items 41184.
I0302 18:58:57.711691 22732373614720 run.py:483] Algo bellman_ford step 1287 current loss 1.183048, current_train_items 41216.
I0302 18:58:57.740806 22732373614720 run.py:483] Algo bellman_ford step 1288 current loss 1.110815, current_train_items 41248.
I0302 18:58:57.772704 22732373614720 run.py:483] Algo bellman_ford step 1289 current loss 1.162396, current_train_items 41280.
I0302 18:58:57.791946 22732373614720 run.py:483] Algo bellman_ford step 1290 current loss 0.451048, current_train_items 41312.
I0302 18:58:57.808186 22732373614720 run.py:483] Algo bellman_ford step 1291 current loss 0.850469, current_train_items 41344.
I0302 18:58:57.830090 22732373614720 run.py:483] Algo bellman_ford step 1292 current loss 0.912070, current_train_items 41376.
I0302 18:58:57.858638 22732373614720 run.py:483] Algo bellman_ford step 1293 current loss 1.158774, current_train_items 41408.
I0302 18:58:57.891159 22732373614720 run.py:483] Algo bellman_ford step 1294 current loss 1.289529, current_train_items 41440.
I0302 18:58:57.909920 22732373614720 run.py:483] Algo bellman_ford step 1295 current loss 0.477590, current_train_items 41472.
I0302 18:58:57.925924 22732373614720 run.py:483] Algo bellman_ford step 1296 current loss 0.593744, current_train_items 41504.
I0302 18:58:57.948997 22732373614720 run.py:483] Algo bellman_ford step 1297 current loss 0.961379, current_train_items 41536.
I0302 18:58:57.976402 22732373614720 run.py:483] Algo bellman_ford step 1298 current loss 0.846962, current_train_items 41568.
I0302 18:58:58.005994 22732373614720 run.py:483] Algo bellman_ford step 1299 current loss 1.378385, current_train_items 41600.
I0302 18:58:58.024933 22732373614720 run.py:483] Algo bellman_ford step 1300 current loss 0.409545, current_train_items 41632.
I0302 18:58:58.032796 22732373614720 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.875, 'score': 0.875, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0302 18:58:58.032904 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.901, current avg val score is 0.875, val scores are: bellman_ford: 0.875
I0302 18:58:58.049410 22732373614720 run.py:483] Algo bellman_ford step 1301 current loss 0.693443, current_train_items 41664.
I0302 18:58:58.072846 22732373614720 run.py:483] Algo bellman_ford step 1302 current loss 1.014061, current_train_items 41696.
I0302 18:58:58.102542 22732373614720 run.py:483] Algo bellman_ford step 1303 current loss 1.147952, current_train_items 41728.
I0302 18:58:58.133239 22732373614720 run.py:483] Algo bellman_ford step 1304 current loss 1.129358, current_train_items 41760.
I0302 18:58:58.152097 22732373614720 run.py:483] Algo bellman_ford step 1305 current loss 0.371009, current_train_items 41792.
I0302 18:58:58.167728 22732373614720 run.py:483] Algo bellman_ford step 1306 current loss 0.779371, current_train_items 41824.
I0302 18:58:58.189767 22732373614720 run.py:483] Algo bellman_ford step 1307 current loss 0.963733, current_train_items 41856.
I0302 18:58:58.217927 22732373614720 run.py:483] Algo bellman_ford step 1308 current loss 1.014967, current_train_items 41888.
I0302 18:58:58.248864 22732373614720 run.py:483] Algo bellman_ford step 1309 current loss 1.220398, current_train_items 41920.
I0302 18:58:58.267745 22732373614720 run.py:483] Algo bellman_ford step 1310 current loss 0.337350, current_train_items 41952.
I0302 18:58:58.283684 22732373614720 run.py:483] Algo bellman_ford step 1311 current loss 0.676324, current_train_items 41984.
I0302 18:58:58.305854 22732373614720 run.py:483] Algo bellman_ford step 1312 current loss 0.953674, current_train_items 42016.
I0302 18:58:58.335356 22732373614720 run.py:483] Algo bellman_ford step 1313 current loss 1.155336, current_train_items 42048.
I0302 18:58:58.367067 22732373614720 run.py:483] Algo bellman_ford step 1314 current loss 1.274922, current_train_items 42080.
I0302 18:58:58.385646 22732373614720 run.py:483] Algo bellman_ford step 1315 current loss 0.326692, current_train_items 42112.
I0302 18:58:58.401478 22732373614720 run.py:483] Algo bellman_ford step 1316 current loss 0.649380, current_train_items 42144.
I0302 18:58:58.423985 22732373614720 run.py:483] Algo bellman_ford step 1317 current loss 0.946017, current_train_items 42176.
I0302 18:58:58.452810 22732373614720 run.py:483] Algo bellman_ford step 1318 current loss 0.999143, current_train_items 42208.
I0302 18:58:58.484010 22732373614720 run.py:483] Algo bellman_ford step 1319 current loss 1.198489, current_train_items 42240.
I0302 18:58:58.502683 22732373614720 run.py:483] Algo bellman_ford step 1320 current loss 0.409979, current_train_items 42272.
I0302 18:58:58.518709 22732373614720 run.py:483] Algo bellman_ford step 1321 current loss 0.695457, current_train_items 42304.
I0302 18:58:58.541793 22732373614720 run.py:483] Algo bellman_ford step 1322 current loss 1.249892, current_train_items 42336.
I0302 18:58:58.570953 22732373614720 run.py:483] Algo bellman_ford step 1323 current loss 1.072913, current_train_items 42368.
I0302 18:58:58.601753 22732373614720 run.py:483] Algo bellman_ford step 1324 current loss 1.241357, current_train_items 42400.
I0302 18:58:58.620876 22732373614720 run.py:483] Algo bellman_ford step 1325 current loss 0.432821, current_train_items 42432.
I0302 18:58:58.636739 22732373614720 run.py:483] Algo bellman_ford step 1326 current loss 0.581356, current_train_items 42464.
I0302 18:58:58.658707 22732373614720 run.py:483] Algo bellman_ford step 1327 current loss 0.747543, current_train_items 42496.
I0302 18:58:58.687763 22732373614720 run.py:483] Algo bellman_ford step 1328 current loss 1.069879, current_train_items 42528.
I0302 18:58:58.719913 22732373614720 run.py:483] Algo bellman_ford step 1329 current loss 1.223870, current_train_items 42560.
I0302 18:58:58.738648 22732373614720 run.py:483] Algo bellman_ford step 1330 current loss 0.418730, current_train_items 42592.
I0302 18:58:58.754445 22732373614720 run.py:483] Algo bellman_ford step 1331 current loss 0.655967, current_train_items 42624.
I0302 18:58:58.776760 22732373614720 run.py:483] Algo bellman_ford step 1332 current loss 0.855631, current_train_items 42656.
I0302 18:58:58.805532 22732373614720 run.py:483] Algo bellman_ford step 1333 current loss 1.060058, current_train_items 42688.
I0302 18:58:58.835910 22732373614720 run.py:483] Algo bellman_ford step 1334 current loss 1.144852, current_train_items 42720.
I0302 18:58:58.854871 22732373614720 run.py:483] Algo bellman_ford step 1335 current loss 0.418154, current_train_items 42752.
I0302 18:58:58.870497 22732373614720 run.py:483] Algo bellman_ford step 1336 current loss 0.582108, current_train_items 42784.
I0302 18:58:58.894466 22732373614720 run.py:483] Algo bellman_ford step 1337 current loss 0.895406, current_train_items 42816.
I0302 18:58:58.923204 22732373614720 run.py:483] Algo bellman_ford step 1338 current loss 1.139119, current_train_items 42848.
I0302 18:58:58.956973 22732373614720 run.py:483] Algo bellman_ford step 1339 current loss 1.252270, current_train_items 42880.
I0302 18:58:58.975731 22732373614720 run.py:483] Algo bellman_ford step 1340 current loss 0.381077, current_train_items 42912.
I0302 18:58:58.991896 22732373614720 run.py:483] Algo bellman_ford step 1341 current loss 0.732501, current_train_items 42944.
I0302 18:58:59.014755 22732373614720 run.py:483] Algo bellman_ford step 1342 current loss 0.912722, current_train_items 42976.
I0302 18:58:59.043258 22732373614720 run.py:483] Algo bellman_ford step 1343 current loss 0.968626, current_train_items 43008.
I0302 18:58:59.074820 22732373614720 run.py:483] Algo bellman_ford step 1344 current loss 1.179261, current_train_items 43040.
I0302 18:58:59.094089 22732373614720 run.py:483] Algo bellman_ford step 1345 current loss 0.435132, current_train_items 43072.
I0302 18:58:59.110067 22732373614720 run.py:483] Algo bellman_ford step 1346 current loss 0.650702, current_train_items 43104.
I0302 18:58:59.134016 22732373614720 run.py:483] Algo bellman_ford step 1347 current loss 1.024764, current_train_items 43136.
I0302 18:58:59.161564 22732373614720 run.py:483] Algo bellman_ford step 1348 current loss 0.990593, current_train_items 43168.
I0302 18:58:59.192288 22732373614720 run.py:483] Algo bellman_ford step 1349 current loss 1.164174, current_train_items 43200.
I0302 18:58:59.211487 22732373614720 run.py:483] Algo bellman_ford step 1350 current loss 0.411875, current_train_items 43232.
I0302 18:58:59.219791 22732373614720 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.85546875, 'score': 0.85546875, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0302 18:58:59.219900 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.901, current avg val score is 0.855, val scores are: bellman_ford: 0.855
I0302 18:58:59.236947 22732373614720 run.py:483] Algo bellman_ford step 1351 current loss 0.714545, current_train_items 43264.
I0302 18:58:59.260883 22732373614720 run.py:483] Algo bellman_ford step 1352 current loss 0.860832, current_train_items 43296.
I0302 18:58:59.288462 22732373614720 run.py:483] Algo bellman_ford step 1353 current loss 0.940084, current_train_items 43328.
I0302 18:58:59.322914 22732373614720 run.py:483] Algo bellman_ford step 1354 current loss 1.320556, current_train_items 43360.
I0302 18:58:59.342627 22732373614720 run.py:483] Algo bellman_ford step 1355 current loss 0.439038, current_train_items 43392.
I0302 18:58:59.358230 22732373614720 run.py:483] Algo bellman_ford step 1356 current loss 0.670527, current_train_items 43424.
I0302 18:58:59.380438 22732373614720 run.py:483] Algo bellman_ford step 1357 current loss 0.878289, current_train_items 43456.
I0302 18:58:59.408990 22732373614720 run.py:483] Algo bellman_ford step 1358 current loss 0.938282, current_train_items 43488.
I0302 18:58:59.442447 22732373614720 run.py:483] Algo bellman_ford step 1359 current loss 1.382284, current_train_items 43520.
I0302 18:58:59.462113 22732373614720 run.py:483] Algo bellman_ford step 1360 current loss 0.433358, current_train_items 43552.
I0302 18:58:59.478823 22732373614720 run.py:483] Algo bellman_ford step 1361 current loss 0.744417, current_train_items 43584.
I0302 18:58:59.502118 22732373614720 run.py:483] Algo bellman_ford step 1362 current loss 0.872505, current_train_items 43616.
I0302 18:58:59.531122 22732373614720 run.py:483] Algo bellman_ford step 1363 current loss 1.154945, current_train_items 43648.
I0302 18:58:59.564280 22732373614720 run.py:483] Algo bellman_ford step 1364 current loss 1.406317, current_train_items 43680.
I0302 18:58:59.583439 22732373614720 run.py:483] Algo bellman_ford step 1365 current loss 0.409047, current_train_items 43712.
I0302 18:58:59.599225 22732373614720 run.py:483] Algo bellman_ford step 1366 current loss 0.636212, current_train_items 43744.
I0302 18:58:59.621687 22732373614720 run.py:483] Algo bellman_ford step 1367 current loss 0.782125, current_train_items 43776.
I0302 18:58:59.649063 22732373614720 run.py:483] Algo bellman_ford step 1368 current loss 0.880298, current_train_items 43808.
I0302 18:58:59.680834 22732373614720 run.py:483] Algo bellman_ford step 1369 current loss 1.247903, current_train_items 43840.
I0302 18:58:59.700392 22732373614720 run.py:483] Algo bellman_ford step 1370 current loss 0.483618, current_train_items 43872.
I0302 18:58:59.716750 22732373614720 run.py:483] Algo bellman_ford step 1371 current loss 0.781364, current_train_items 43904.
I0302 18:58:59.740101 22732373614720 run.py:483] Algo bellman_ford step 1372 current loss 1.006524, current_train_items 43936.
I0302 18:58:59.769937 22732373614720 run.py:483] Algo bellman_ford step 1373 current loss 1.187883, current_train_items 43968.
I0302 18:58:59.802531 22732373614720 run.py:483] Algo bellman_ford step 1374 current loss 1.183971, current_train_items 44000.
I0302 18:58:59.821983 22732373614720 run.py:483] Algo bellman_ford step 1375 current loss 0.452343, current_train_items 44032.
I0302 18:58:59.837686 22732373614720 run.py:483] Algo bellman_ford step 1376 current loss 0.675568, current_train_items 44064.
I0302 18:58:59.860116 22732373614720 run.py:483] Algo bellman_ford step 1377 current loss 0.988678, current_train_items 44096.
I0302 18:58:59.889766 22732373614720 run.py:483] Algo bellman_ford step 1378 current loss 1.107041, current_train_items 44128.
I0302 18:58:59.922078 22732373614720 run.py:483] Algo bellman_ford step 1379 current loss 1.330987, current_train_items 44160.
I0302 18:58:59.941149 22732373614720 run.py:483] Algo bellman_ford step 1380 current loss 0.488654, current_train_items 44192.
I0302 18:58:59.957600 22732373614720 run.py:483] Algo bellman_ford step 1381 current loss 0.744899, current_train_items 44224.
I0302 18:58:59.980314 22732373614720 run.py:483] Algo bellman_ford step 1382 current loss 0.910989, current_train_items 44256.
I0302 18:59:00.009413 22732373614720 run.py:483] Algo bellman_ford step 1383 current loss 1.018919, current_train_items 44288.
I0302 18:59:00.039129 22732373614720 run.py:483] Algo bellman_ford step 1384 current loss 1.213534, current_train_items 44320.
I0302 18:59:00.058345 22732373614720 run.py:483] Algo bellman_ford step 1385 current loss 0.341876, current_train_items 44352.
I0302 18:59:00.074033 22732373614720 run.py:483] Algo bellman_ford step 1386 current loss 0.725087, current_train_items 44384.
I0302 18:59:00.095852 22732373614720 run.py:483] Algo bellman_ford step 1387 current loss 0.781095, current_train_items 44416.
I0302 18:59:00.125097 22732373614720 run.py:483] Algo bellman_ford step 1388 current loss 0.947397, current_train_items 44448.
I0302 18:59:00.156890 22732373614720 run.py:483] Algo bellman_ford step 1389 current loss 1.148702, current_train_items 44480.
I0302 18:59:00.176367 22732373614720 run.py:483] Algo bellman_ford step 1390 current loss 0.397814, current_train_items 44512.
I0302 18:59:00.192283 22732373614720 run.py:483] Algo bellman_ford step 1391 current loss 0.619911, current_train_items 44544.
I0302 18:59:00.215772 22732373614720 run.py:483] Algo bellman_ford step 1392 current loss 0.935245, current_train_items 44576.
I0302 18:59:00.244661 22732373614720 run.py:483] Algo bellman_ford step 1393 current loss 1.024421, current_train_items 44608.
I0302 18:59:00.276585 22732373614720 run.py:483] Algo bellman_ford step 1394 current loss 1.323076, current_train_items 44640.
I0302 18:59:00.295603 22732373614720 run.py:483] Algo bellman_ford step 1395 current loss 0.316948, current_train_items 44672.
I0302 18:59:00.311419 22732373614720 run.py:483] Algo bellman_ford step 1396 current loss 0.634739, current_train_items 44704.
I0302 18:59:00.334582 22732373614720 run.py:483] Algo bellman_ford step 1397 current loss 0.961755, current_train_items 44736.
I0302 18:59:00.364305 22732373614720 run.py:483] Algo bellman_ford step 1398 current loss 1.170236, current_train_items 44768.
I0302 18:59:00.394730 22732373614720 run.py:483] Algo bellman_ford step 1399 current loss 1.081326, current_train_items 44800.
I0302 18:59:00.414115 22732373614720 run.py:483] Algo bellman_ford step 1400 current loss 0.468354, current_train_items 44832.
I0302 18:59:00.421475 22732373614720 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0302 18:59:00.421584 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.901, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 18:59:00.437805 22732373614720 run.py:483] Algo bellman_ford step 1401 current loss 0.659174, current_train_items 44864.
I0302 18:59:00.461677 22732373614720 run.py:483] Algo bellman_ford step 1402 current loss 0.982690, current_train_items 44896.
I0302 18:59:00.491335 22732373614720 run.py:483] Algo bellman_ford step 1403 current loss 1.166701, current_train_items 44928.
I0302 18:59:00.523763 22732373614720 run.py:483] Algo bellman_ford step 1404 current loss 1.212501, current_train_items 44960.
I0302 18:59:00.542749 22732373614720 run.py:483] Algo bellman_ford step 1405 current loss 0.484314, current_train_items 44992.
I0302 18:59:00.558812 22732373614720 run.py:483] Algo bellman_ford step 1406 current loss 0.749548, current_train_items 45024.
I0302 18:59:00.581401 22732373614720 run.py:483] Algo bellman_ford step 1407 current loss 0.822687, current_train_items 45056.
I0302 18:59:00.610799 22732373614720 run.py:483] Algo bellman_ford step 1408 current loss 1.003866, current_train_items 45088.
I0302 18:59:00.642279 22732373614720 run.py:483] Algo bellman_ford step 1409 current loss 1.136316, current_train_items 45120.
I0302 18:59:00.660920 22732373614720 run.py:483] Algo bellman_ford step 1410 current loss 0.489889, current_train_items 45152.
I0302 18:59:00.677150 22732373614720 run.py:483] Algo bellman_ford step 1411 current loss 0.614708, current_train_items 45184.
I0302 18:59:00.699882 22732373614720 run.py:483] Algo bellman_ford step 1412 current loss 0.910392, current_train_items 45216.
I0302 18:59:00.727061 22732373614720 run.py:483] Algo bellman_ford step 1413 current loss 0.806812, current_train_items 45248.
I0302 18:59:00.757820 22732373614720 run.py:483] Algo bellman_ford step 1414 current loss 1.255226, current_train_items 45280.
I0302 18:59:00.776714 22732373614720 run.py:483] Algo bellman_ford step 1415 current loss 0.358833, current_train_items 45312.
I0302 18:59:00.793071 22732373614720 run.py:483] Algo bellman_ford step 1416 current loss 0.789068, current_train_items 45344.
I0302 18:59:00.815064 22732373614720 run.py:483] Algo bellman_ford step 1417 current loss 0.808634, current_train_items 45376.
I0302 18:59:00.843014 22732373614720 run.py:483] Algo bellman_ford step 1418 current loss 1.070415, current_train_items 45408.
I0302 18:59:00.875269 22732373614720 run.py:483] Algo bellman_ford step 1419 current loss 1.084517, current_train_items 45440.
I0302 18:59:00.894436 22732373614720 run.py:483] Algo bellman_ford step 1420 current loss 0.609362, current_train_items 45472.
I0302 18:59:00.910402 22732373614720 run.py:483] Algo bellman_ford step 1421 current loss 0.703418, current_train_items 45504.
I0302 18:59:00.933565 22732373614720 run.py:483] Algo bellman_ford step 1422 current loss 0.927107, current_train_items 45536.
I0302 18:59:00.963281 22732373614720 run.py:483] Algo bellman_ford step 1423 current loss 1.031531, current_train_items 45568.
I0302 18:59:00.993823 22732373614720 run.py:483] Algo bellman_ford step 1424 current loss 1.158512, current_train_items 45600.
I0302 18:59:01.012522 22732373614720 run.py:483] Algo bellman_ford step 1425 current loss 0.430232, current_train_items 45632.
I0302 18:59:01.028880 22732373614720 run.py:483] Algo bellman_ford step 1426 current loss 0.718460, current_train_items 45664.
I0302 18:59:01.052842 22732373614720 run.py:483] Algo bellman_ford step 1427 current loss 0.908529, current_train_items 45696.
I0302 18:59:01.082013 22732373614720 run.py:483] Algo bellman_ford step 1428 current loss 1.064748, current_train_items 45728.
I0302 18:59:01.112174 22732373614720 run.py:483] Algo bellman_ford step 1429 current loss 1.077173, current_train_items 45760.
I0302 18:59:01.131006 22732373614720 run.py:483] Algo bellman_ford step 1430 current loss 0.432705, current_train_items 45792.
I0302 18:59:01.146822 22732373614720 run.py:483] Algo bellman_ford step 1431 current loss 0.668174, current_train_items 45824.
I0302 18:59:01.171357 22732373614720 run.py:483] Algo bellman_ford step 1432 current loss 0.973969, current_train_items 45856.
I0302 18:59:01.199051 22732373614720 run.py:483] Algo bellman_ford step 1433 current loss 0.954723, current_train_items 45888.
I0302 18:59:01.229183 22732373614720 run.py:483] Algo bellman_ford step 1434 current loss 1.061230, current_train_items 45920.
I0302 18:59:01.247960 22732373614720 run.py:483] Algo bellman_ford step 1435 current loss 0.394320, current_train_items 45952.
I0302 18:59:01.263932 22732373614720 run.py:483] Algo bellman_ford step 1436 current loss 0.816105, current_train_items 45984.
I0302 18:59:01.286342 22732373614720 run.py:483] Algo bellman_ford step 1437 current loss 0.971319, current_train_items 46016.
I0302 18:59:01.315050 22732373614720 run.py:483] Algo bellman_ford step 1438 current loss 0.953910, current_train_items 46048.
I0302 18:59:01.347734 22732373614720 run.py:483] Algo bellman_ford step 1439 current loss 1.290014, current_train_items 46080.
I0302 18:59:01.366723 22732373614720 run.py:483] Algo bellman_ford step 1440 current loss 0.418251, current_train_items 46112.
I0302 18:59:01.382843 22732373614720 run.py:483] Algo bellman_ford step 1441 current loss 0.690526, current_train_items 46144.
I0302 18:59:01.405428 22732373614720 run.py:483] Algo bellman_ford step 1442 current loss 0.804320, current_train_items 46176.
I0302 18:59:01.434433 22732373614720 run.py:483] Algo bellman_ford step 1443 current loss 0.918699, current_train_items 46208.
I0302 18:59:01.469791 22732373614720 run.py:483] Algo bellman_ford step 1444 current loss 1.303221, current_train_items 46240.
I0302 18:59:01.488859 22732373614720 run.py:483] Algo bellman_ford step 1445 current loss 0.496545, current_train_items 46272.
I0302 18:59:01.504300 22732373614720 run.py:483] Algo bellman_ford step 1446 current loss 0.667355, current_train_items 46304.
I0302 18:59:01.527467 22732373614720 run.py:483] Algo bellman_ford step 1447 current loss 0.954673, current_train_items 46336.
I0302 18:59:01.555885 22732373614720 run.py:483] Algo bellman_ford step 1448 current loss 1.059664, current_train_items 46368.
I0302 18:59:01.586696 22732373614720 run.py:483] Algo bellman_ford step 1449 current loss 1.254685, current_train_items 46400.
I0302 18:59:01.605784 22732373614720 run.py:483] Algo bellman_ford step 1450 current loss 0.445963, current_train_items 46432.
I0302 18:59:01.613797 22732373614720 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.873046875, 'score': 0.873046875, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0302 18:59:01.613904 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.901, current avg val score is 0.873, val scores are: bellman_ford: 0.873
I0302 18:59:01.630059 22732373614720 run.py:483] Algo bellman_ford step 1451 current loss 0.646960, current_train_items 46464.
I0302 18:59:01.653130 22732373614720 run.py:483] Algo bellman_ford step 1452 current loss 0.756483, current_train_items 46496.
I0302 18:59:01.683315 22732373614720 run.py:483] Algo bellman_ford step 1453 current loss 1.074126, current_train_items 46528.
I0302 18:59:01.716948 22732373614720 run.py:483] Algo bellman_ford step 1454 current loss 1.275230, current_train_items 46560.
I0302 18:59:01.736042 22732373614720 run.py:483] Algo bellman_ford step 1455 current loss 0.529761, current_train_items 46592.
I0302 18:59:01.751830 22732373614720 run.py:483] Algo bellman_ford step 1456 current loss 0.724559, current_train_items 46624.
I0302 18:59:01.773488 22732373614720 run.py:483] Algo bellman_ford step 1457 current loss 0.688100, current_train_items 46656.
I0302 18:59:01.802950 22732373614720 run.py:483] Algo bellman_ford step 1458 current loss 1.073572, current_train_items 46688.
I0302 18:59:01.833998 22732373614720 run.py:483] Algo bellman_ford step 1459 current loss 1.042091, current_train_items 46720.
I0302 18:59:01.853482 22732373614720 run.py:483] Algo bellman_ford step 1460 current loss 0.483156, current_train_items 46752.
I0302 18:59:01.869066 22732373614720 run.py:483] Algo bellman_ford step 1461 current loss 0.700679, current_train_items 46784.
I0302 18:59:01.890935 22732373614720 run.py:483] Algo bellman_ford step 1462 current loss 0.895154, current_train_items 46816.
I0302 18:59:01.919363 22732373614720 run.py:483] Algo bellman_ford step 1463 current loss 1.117905, current_train_items 46848.
I0302 18:59:01.949898 22732373614720 run.py:483] Algo bellman_ford step 1464 current loss 1.457785, current_train_items 46880.
I0302 18:59:01.968822 22732373614720 run.py:483] Algo bellman_ford step 1465 current loss 0.474832, current_train_items 46912.
I0302 18:59:01.985087 22732373614720 run.py:483] Algo bellman_ford step 1466 current loss 0.637720, current_train_items 46944.
I0302 18:59:02.007663 22732373614720 run.py:483] Algo bellman_ford step 1467 current loss 0.806442, current_train_items 46976.
I0302 18:59:02.035196 22732373614720 run.py:483] Algo bellman_ford step 1468 current loss 0.924267, current_train_items 47008.
I0302 18:59:02.065183 22732373614720 run.py:483] Algo bellman_ford step 1469 current loss 1.216794, current_train_items 47040.
I0302 18:59:02.084242 22732373614720 run.py:483] Algo bellman_ford step 1470 current loss 0.413410, current_train_items 47072.
I0302 18:59:02.100576 22732373614720 run.py:483] Algo bellman_ford step 1471 current loss 0.713682, current_train_items 47104.
I0302 18:59:02.122564 22732373614720 run.py:483] Algo bellman_ford step 1472 current loss 0.808670, current_train_items 47136.
I0302 18:59:02.151416 22732373614720 run.py:483] Algo bellman_ford step 1473 current loss 1.106290, current_train_items 47168.
I0302 18:59:02.183460 22732373614720 run.py:483] Algo bellman_ford step 1474 current loss 1.381222, current_train_items 47200.
I0302 18:59:02.202709 22732373614720 run.py:483] Algo bellman_ford step 1475 current loss 0.548105, current_train_items 47232.
I0302 18:59:02.218618 22732373614720 run.py:483] Algo bellman_ford step 1476 current loss 0.587398, current_train_items 47264.
I0302 18:59:02.241022 22732373614720 run.py:483] Algo bellman_ford step 1477 current loss 0.821709, current_train_items 47296.
I0302 18:59:02.269628 22732373614720 run.py:483] Algo bellman_ford step 1478 current loss 1.051454, current_train_items 47328.
I0302 18:59:02.302912 22732373614720 run.py:483] Algo bellman_ford step 1479 current loss 1.170716, current_train_items 47360.
I0302 18:59:02.321595 22732373614720 run.py:483] Algo bellman_ford step 1480 current loss 0.403526, current_train_items 47392.
I0302 18:59:02.337312 22732373614720 run.py:483] Algo bellman_ford step 1481 current loss 0.577231, current_train_items 47424.
I0302 18:59:02.359388 22732373614720 run.py:483] Algo bellman_ford step 1482 current loss 0.845805, current_train_items 47456.
I0302 18:59:02.388392 22732373614720 run.py:483] Algo bellman_ford step 1483 current loss 1.030677, current_train_items 47488.
I0302 18:59:02.418107 22732373614720 run.py:483] Algo bellman_ford step 1484 current loss 0.961132, current_train_items 47520.
I0302 18:59:02.437198 22732373614720 run.py:483] Algo bellman_ford step 1485 current loss 0.471575, current_train_items 47552.
I0302 18:59:02.452909 22732373614720 run.py:483] Algo bellman_ford step 1486 current loss 0.632169, current_train_items 47584.
I0302 18:59:02.476014 22732373614720 run.py:483] Algo bellman_ford step 1487 current loss 0.959393, current_train_items 47616.
I0302 18:59:02.503213 22732373614720 run.py:483] Algo bellman_ford step 1488 current loss 0.872588, current_train_items 47648.
I0302 18:59:02.535367 22732373614720 run.py:483] Algo bellman_ford step 1489 current loss 1.184568, current_train_items 47680.
I0302 18:59:02.554416 22732373614720 run.py:483] Algo bellman_ford step 1490 current loss 0.470012, current_train_items 47712.
I0302 18:59:02.570456 22732373614720 run.py:483] Algo bellman_ford step 1491 current loss 0.600553, current_train_items 47744.
I0302 18:59:02.592878 22732373614720 run.py:483] Algo bellman_ford step 1492 current loss 0.901028, current_train_items 47776.
I0302 18:59:02.621515 22732373614720 run.py:483] Algo bellman_ford step 1493 current loss 1.013680, current_train_items 47808.
I0302 18:59:02.654150 22732373614720 run.py:483] Algo bellman_ford step 1494 current loss 1.219852, current_train_items 47840.
I0302 18:59:02.672916 22732373614720 run.py:483] Algo bellman_ford step 1495 current loss 0.403312, current_train_items 47872.
I0302 18:59:02.689127 22732373614720 run.py:483] Algo bellman_ford step 1496 current loss 0.709226, current_train_items 47904.
I0302 18:59:02.712258 22732373614720 run.py:483] Algo bellman_ford step 1497 current loss 0.925580, current_train_items 47936.
I0302 18:59:02.741672 22732373614720 run.py:483] Algo bellman_ford step 1498 current loss 1.019227, current_train_items 47968.
I0302 18:59:02.774723 22732373614720 run.py:483] Algo bellman_ford step 1499 current loss 1.124034, current_train_items 48000.
I0302 18:59:02.793832 22732373614720 run.py:483] Algo bellman_ford step 1500 current loss 0.389081, current_train_items 48032.
I0302 18:59:02.801789 22732373614720 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0302 18:59:02.801926 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.901, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 18:59:02.832302 22732373614720 run.py:483] Algo bellman_ford step 1501 current loss 0.710550, current_train_items 48064.
I0302 18:59:02.856346 22732373614720 run.py:483] Algo bellman_ford step 1502 current loss 0.861751, current_train_items 48096.
I0302 18:59:02.885819 22732373614720 run.py:483] Algo bellman_ford step 1503 current loss 1.101430, current_train_items 48128.
I0302 18:59:02.917674 22732373614720 run.py:483] Algo bellman_ford step 1504 current loss 1.165239, current_train_items 48160.
I0302 18:59:02.937271 22732373614720 run.py:483] Algo bellman_ford step 1505 current loss 0.361031, current_train_items 48192.
I0302 18:59:02.952903 22732373614720 run.py:483] Algo bellman_ford step 1506 current loss 0.556965, current_train_items 48224.
I0302 18:59:02.975517 22732373614720 run.py:483] Algo bellman_ford step 1507 current loss 0.877319, current_train_items 48256.
I0302 18:59:03.004503 22732373614720 run.py:483] Algo bellman_ford step 1508 current loss 0.954353, current_train_items 48288.
I0302 18:59:03.036024 22732373614720 run.py:483] Algo bellman_ford step 1509 current loss 1.078158, current_train_items 48320.
I0302 18:59:03.055005 22732373614720 run.py:483] Algo bellman_ford step 1510 current loss 0.386826, current_train_items 48352.
I0302 18:59:03.071210 22732373614720 run.py:483] Algo bellman_ford step 1511 current loss 0.750897, current_train_items 48384.
I0302 18:59:03.094302 22732373614720 run.py:483] Algo bellman_ford step 1512 current loss 0.828934, current_train_items 48416.
I0302 18:59:03.121768 22732373614720 run.py:483] Algo bellman_ford step 1513 current loss 0.838275, current_train_items 48448.
I0302 18:59:03.154818 22732373614720 run.py:483] Algo bellman_ford step 1514 current loss 1.242013, current_train_items 48480.
I0302 18:59:03.173970 22732373614720 run.py:483] Algo bellman_ford step 1515 current loss 0.432917, current_train_items 48512.
I0302 18:59:03.189602 22732373614720 run.py:483] Algo bellman_ford step 1516 current loss 0.585599, current_train_items 48544.
I0302 18:59:03.212343 22732373614720 run.py:483] Algo bellman_ford step 1517 current loss 0.974067, current_train_items 48576.
I0302 18:59:03.241995 22732373614720 run.py:483] Algo bellman_ford step 1518 current loss 1.119335, current_train_items 48608.
I0302 18:59:03.273475 22732373614720 run.py:483] Algo bellman_ford step 1519 current loss 1.103754, current_train_items 48640.
I0302 18:59:03.292302 22732373614720 run.py:483] Algo bellman_ford step 1520 current loss 0.349342, current_train_items 48672.
I0302 18:59:03.308027 22732373614720 run.py:483] Algo bellman_ford step 1521 current loss 0.750542, current_train_items 48704.
I0302 18:59:03.331335 22732373614720 run.py:483] Algo bellman_ford step 1522 current loss 0.879681, current_train_items 48736.
I0302 18:59:03.360373 22732373614720 run.py:483] Algo bellman_ford step 1523 current loss 1.049145, current_train_items 48768.
I0302 18:59:03.392371 22732373614720 run.py:483] Algo bellman_ford step 1524 current loss 1.297243, current_train_items 48800.
I0302 18:59:03.411556 22732373614720 run.py:483] Algo bellman_ford step 1525 current loss 0.443262, current_train_items 48832.
I0302 18:59:03.427211 22732373614720 run.py:483] Algo bellman_ford step 1526 current loss 0.609205, current_train_items 48864.
I0302 18:59:03.450679 22732373614720 run.py:483] Algo bellman_ford step 1527 current loss 0.925782, current_train_items 48896.
I0302 18:59:03.479737 22732373614720 run.py:483] Algo bellman_ford step 1528 current loss 1.050678, current_train_items 48928.
I0302 18:59:03.512206 22732373614720 run.py:483] Algo bellman_ford step 1529 current loss 1.176747, current_train_items 48960.
I0302 18:59:03.531413 22732373614720 run.py:483] Algo bellman_ford step 1530 current loss 0.407192, current_train_items 48992.
I0302 18:59:03.547090 22732373614720 run.py:483] Algo bellman_ford step 1531 current loss 0.628162, current_train_items 49024.
I0302 18:59:03.569787 22732373614720 run.py:483] Algo bellman_ford step 1532 current loss 0.867741, current_train_items 49056.
I0302 18:59:03.599412 22732373614720 run.py:483] Algo bellman_ford step 1533 current loss 1.119211, current_train_items 49088.
I0302 18:59:03.632071 22732373614720 run.py:483] Algo bellman_ford step 1534 current loss 1.369242, current_train_items 49120.
I0302 18:59:03.651177 22732373614720 run.py:483] Algo bellman_ford step 1535 current loss 0.432268, current_train_items 49152.
I0302 18:59:03.667450 22732373614720 run.py:483] Algo bellman_ford step 1536 current loss 0.662444, current_train_items 49184.
I0302 18:59:03.690657 22732373614720 run.py:483] Algo bellman_ford step 1537 current loss 0.827136, current_train_items 49216.
I0302 18:59:03.719955 22732373614720 run.py:483] Algo bellman_ford step 1538 current loss 0.915655, current_train_items 49248.
I0302 18:59:03.754385 22732373614720 run.py:483] Algo bellman_ford step 1539 current loss 1.220404, current_train_items 49280.
I0302 18:59:03.773295 22732373614720 run.py:483] Algo bellman_ford step 1540 current loss 0.477953, current_train_items 49312.
I0302 18:59:03.788868 22732373614720 run.py:483] Algo bellman_ford step 1541 current loss 0.595321, current_train_items 49344.
I0302 18:59:03.811480 22732373614720 run.py:483] Algo bellman_ford step 1542 current loss 0.818974, current_train_items 49376.
I0302 18:59:03.840018 22732373614720 run.py:483] Algo bellman_ford step 1543 current loss 0.952556, current_train_items 49408.
I0302 18:59:03.871612 22732373614720 run.py:483] Algo bellman_ford step 1544 current loss 1.191307, current_train_items 49440.
I0302 18:59:03.890647 22732373614720 run.py:483] Algo bellman_ford step 1545 current loss 0.389985, current_train_items 49472.
I0302 18:59:03.906602 22732373614720 run.py:483] Algo bellman_ford step 1546 current loss 0.663508, current_train_items 49504.
I0302 18:59:03.929639 22732373614720 run.py:483] Algo bellman_ford step 1547 current loss 0.835273, current_train_items 49536.
I0302 18:59:03.959845 22732373614720 run.py:483] Algo bellman_ford step 1548 current loss 1.013998, current_train_items 49568.
I0302 18:59:03.993495 22732373614720 run.py:483] Algo bellman_ford step 1549 current loss 1.287417, current_train_items 49600.
I0302 18:59:04.012486 22732373614720 run.py:483] Algo bellman_ford step 1550 current loss 0.441977, current_train_items 49632.
I0302 18:59:04.020627 22732373614720 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0302 18:59:04.020733 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.910, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 18:59:04.037903 22732373614720 run.py:483] Algo bellman_ford step 1551 current loss 0.647488, current_train_items 49664.
I0302 18:59:04.060605 22732373614720 run.py:483] Algo bellman_ford step 1552 current loss 0.799010, current_train_items 49696.
I0302 18:59:04.090216 22732373614720 run.py:483] Algo bellman_ford step 1553 current loss 0.922313, current_train_items 49728.
I0302 18:59:04.120825 22732373614720 run.py:483] Algo bellman_ford step 1554 current loss 0.993098, current_train_items 49760.
I0302 18:59:04.139988 22732373614720 run.py:483] Algo bellman_ford step 1555 current loss 0.363062, current_train_items 49792.
I0302 18:59:04.155579 22732373614720 run.py:483] Algo bellman_ford step 1556 current loss 0.666404, current_train_items 49824.
I0302 18:59:04.178677 22732373614720 run.py:483] Algo bellman_ford step 1557 current loss 1.091478, current_train_items 49856.
I0302 18:59:04.208073 22732373614720 run.py:483] Algo bellman_ford step 1558 current loss 0.967034, current_train_items 49888.
I0302 18:59:04.240583 22732373614720 run.py:483] Algo bellman_ford step 1559 current loss 1.249501, current_train_items 49920.
I0302 18:59:04.260230 22732373614720 run.py:483] Algo bellman_ford step 1560 current loss 0.380035, current_train_items 49952.
I0302 18:59:04.276360 22732373614720 run.py:483] Algo bellman_ford step 1561 current loss 0.618690, current_train_items 49984.
I0302 18:59:04.299206 22732373614720 run.py:483] Algo bellman_ford step 1562 current loss 0.879386, current_train_items 50016.
I0302 18:59:04.329043 22732373614720 run.py:483] Algo bellman_ford step 1563 current loss 0.971766, current_train_items 50048.
I0302 18:59:04.361867 22732373614720 run.py:483] Algo bellman_ford step 1564 current loss 1.111042, current_train_items 50080.
I0302 18:59:04.380943 22732373614720 run.py:483] Algo bellman_ford step 1565 current loss 0.406303, current_train_items 50112.
I0302 18:59:04.397146 22732373614720 run.py:483] Algo bellman_ford step 1566 current loss 0.730305, current_train_items 50144.
I0302 18:59:04.420204 22732373614720 run.py:483] Algo bellman_ford step 1567 current loss 0.805333, current_train_items 50176.
I0302 18:59:04.448894 22732373614720 run.py:483] Algo bellman_ford step 1568 current loss 1.061345, current_train_items 50208.
I0302 18:59:04.482041 22732373614720 run.py:483] Algo bellman_ford step 1569 current loss 1.204006, current_train_items 50240.
I0302 18:59:04.501538 22732373614720 run.py:483] Algo bellman_ford step 1570 current loss 0.375834, current_train_items 50272.
I0302 18:59:04.517693 22732373614720 run.py:483] Algo bellman_ford step 1571 current loss 0.589639, current_train_items 50304.
I0302 18:59:04.541140 22732373614720 run.py:483] Algo bellman_ford step 1572 current loss 0.855724, current_train_items 50336.
I0302 18:59:04.569981 22732373614720 run.py:483] Algo bellman_ford step 1573 current loss 0.969091, current_train_items 50368.
I0302 18:59:04.600000 22732373614720 run.py:483] Algo bellman_ford step 1574 current loss 1.185885, current_train_items 50400.
I0302 18:59:04.619366 22732373614720 run.py:483] Algo bellman_ford step 1575 current loss 0.398844, current_train_items 50432.
I0302 18:59:04.635489 22732373614720 run.py:483] Algo bellman_ford step 1576 current loss 0.761034, current_train_items 50464.
I0302 18:59:04.657190 22732373614720 run.py:483] Algo bellman_ford step 1577 current loss 0.759256, current_train_items 50496.
I0302 18:59:04.687231 22732373614720 run.py:483] Algo bellman_ford step 1578 current loss 1.079903, current_train_items 50528.
I0302 18:59:04.719050 22732373614720 run.py:483] Algo bellman_ford step 1579 current loss 1.055169, current_train_items 50560.
I0302 18:59:04.738004 22732373614720 run.py:483] Algo bellman_ford step 1580 current loss 0.610775, current_train_items 50592.
I0302 18:59:04.753647 22732373614720 run.py:483] Algo bellman_ford step 1581 current loss 0.761916, current_train_items 50624.
I0302 18:59:04.776142 22732373614720 run.py:483] Algo bellman_ford step 1582 current loss 0.775164, current_train_items 50656.
I0302 18:59:04.805226 22732373614720 run.py:483] Algo bellman_ford step 1583 current loss 1.093125, current_train_items 50688.
I0302 18:59:04.837988 22732373614720 run.py:483] Algo bellman_ford step 1584 current loss 1.225222, current_train_items 50720.
I0302 18:59:04.857267 22732373614720 run.py:483] Algo bellman_ford step 1585 current loss 0.445503, current_train_items 50752.
I0302 18:59:04.872794 22732373614720 run.py:483] Algo bellman_ford step 1586 current loss 0.678177, current_train_items 50784.
I0302 18:59:04.895662 22732373614720 run.py:483] Algo bellman_ford step 1587 current loss 0.788994, current_train_items 50816.
I0302 18:59:04.924503 22732373614720 run.py:483] Algo bellman_ford step 1588 current loss 1.024424, current_train_items 50848.
I0302 18:59:04.956395 22732373614720 run.py:483] Algo bellman_ford step 1589 current loss 1.127694, current_train_items 50880.
I0302 18:59:04.975700 22732373614720 run.py:483] Algo bellman_ford step 1590 current loss 0.373984, current_train_items 50912.
I0302 18:59:04.991730 22732373614720 run.py:483] Algo bellman_ford step 1591 current loss 0.580746, current_train_items 50944.
I0302 18:59:05.013922 22732373614720 run.py:483] Algo bellman_ford step 1592 current loss 0.768417, current_train_items 50976.
I0302 18:59:05.044076 22732373614720 run.py:483] Algo bellman_ford step 1593 current loss 1.014340, current_train_items 51008.
I0302 18:59:05.077163 22732373614720 run.py:483] Algo bellman_ford step 1594 current loss 1.199220, current_train_items 51040.
I0302 18:59:05.096524 22732373614720 run.py:483] Algo bellman_ford step 1595 current loss 0.439404, current_train_items 51072.
I0302 18:59:05.111945 22732373614720 run.py:483] Algo bellman_ford step 1596 current loss 0.683536, current_train_items 51104.
I0302 18:59:05.135438 22732373614720 run.py:483] Algo bellman_ford step 1597 current loss 0.873650, current_train_items 51136.
I0302 18:59:05.163314 22732373614720 run.py:483] Algo bellman_ford step 1598 current loss 0.832601, current_train_items 51168.
I0302 18:59:05.195407 22732373614720 run.py:483] Algo bellman_ford step 1599 current loss 1.096288, current_train_items 51200.
I0302 18:59:05.214834 22732373614720 run.py:483] Algo bellman_ford step 1600 current loss 0.384071, current_train_items 51232.
I0302 18:59:05.222306 22732373614720 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0302 18:59:05.222416 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.910, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 18:59:05.238794 22732373614720 run.py:483] Algo bellman_ford step 1601 current loss 0.574426, current_train_items 51264.
I0302 18:59:05.262627 22732373614720 run.py:483] Algo bellman_ford step 1602 current loss 0.925882, current_train_items 51296.
I0302 18:59:05.291813 22732373614720 run.py:483] Algo bellman_ford step 1603 current loss 1.057851, current_train_items 51328.
I0302 18:59:05.324091 22732373614720 run.py:483] Algo bellman_ford step 1604 current loss 1.234325, current_train_items 51360.
I0302 18:59:05.343419 22732373614720 run.py:483] Algo bellman_ford step 1605 current loss 0.433729, current_train_items 51392.
I0302 18:59:05.358613 22732373614720 run.py:483] Algo bellman_ford step 1606 current loss 0.635022, current_train_items 51424.
I0302 18:59:05.380807 22732373614720 run.py:483] Algo bellman_ford step 1607 current loss 0.817812, current_train_items 51456.
I0302 18:59:05.410419 22732373614720 run.py:483] Algo bellman_ford step 1608 current loss 0.969044, current_train_items 51488.
I0302 18:59:05.443013 22732373614720 run.py:483] Algo bellman_ford step 1609 current loss 1.190788, current_train_items 51520.
I0302 18:59:05.461856 22732373614720 run.py:483] Algo bellman_ford step 1610 current loss 0.510210, current_train_items 51552.
I0302 18:59:05.477927 22732373614720 run.py:483] Algo bellman_ford step 1611 current loss 0.682873, current_train_items 51584.
I0302 18:59:05.500459 22732373614720 run.py:483] Algo bellman_ford step 1612 current loss 0.790421, current_train_items 51616.
I0302 18:59:05.530011 22732373614720 run.py:483] Algo bellman_ford step 1613 current loss 1.104224, current_train_items 51648.
I0302 18:59:05.560939 22732373614720 run.py:483] Algo bellman_ford step 1614 current loss 1.270711, current_train_items 51680.
I0302 18:59:05.579750 22732373614720 run.py:483] Algo bellman_ford step 1615 current loss 0.427107, current_train_items 51712.
I0302 18:59:05.595831 22732373614720 run.py:483] Algo bellman_ford step 1616 current loss 0.594778, current_train_items 51744.
I0302 18:59:05.618845 22732373614720 run.py:483] Algo bellman_ford step 1617 current loss 0.997125, current_train_items 51776.
I0302 18:59:05.647017 22732373614720 run.py:483] Algo bellman_ford step 1618 current loss 0.856234, current_train_items 51808.
I0302 18:59:05.680496 22732373614720 run.py:483] Algo bellman_ford step 1619 current loss 1.314873, current_train_items 51840.
I0302 18:59:05.699298 22732373614720 run.py:483] Algo bellman_ford step 1620 current loss 0.545816, current_train_items 51872.
I0302 18:59:05.715018 22732373614720 run.py:483] Algo bellman_ford step 1621 current loss 0.732810, current_train_items 51904.
I0302 18:59:05.737917 22732373614720 run.py:483] Algo bellman_ford step 1622 current loss 0.951534, current_train_items 51936.
I0302 18:59:05.766985 22732373614720 run.py:483] Algo bellman_ford step 1623 current loss 1.062822, current_train_items 51968.
I0302 18:59:05.798843 22732373614720 run.py:483] Algo bellman_ford step 1624 current loss 1.157082, current_train_items 52000.
I0302 18:59:05.817715 22732373614720 run.py:483] Algo bellman_ford step 1625 current loss 0.431207, current_train_items 52032.
I0302 18:59:05.833863 22732373614720 run.py:483] Algo bellman_ford step 1626 current loss 0.628283, current_train_items 52064.
I0302 18:59:05.856357 22732373614720 run.py:483] Algo bellman_ford step 1627 current loss 0.904527, current_train_items 52096.
I0302 18:59:05.886779 22732373614720 run.py:483] Algo bellman_ford step 1628 current loss 1.018092, current_train_items 52128.
I0302 18:59:05.918060 22732373614720 run.py:483] Algo bellman_ford step 1629 current loss 1.104954, current_train_items 52160.
I0302 18:59:05.937116 22732373614720 run.py:483] Algo bellman_ford step 1630 current loss 0.431759, current_train_items 52192.
I0302 18:59:05.953601 22732373614720 run.py:483] Algo bellman_ford step 1631 current loss 0.688084, current_train_items 52224.
I0302 18:59:05.976737 22732373614720 run.py:483] Algo bellman_ford step 1632 current loss 1.003986, current_train_items 52256.
I0302 18:59:06.004918 22732373614720 run.py:483] Algo bellman_ford step 1633 current loss 0.943711, current_train_items 52288.
I0302 18:59:06.036944 22732373614720 run.py:483] Algo bellman_ford step 1634 current loss 1.095506, current_train_items 52320.
I0302 18:59:06.055725 22732373614720 run.py:483] Algo bellman_ford step 1635 current loss 0.548207, current_train_items 52352.
I0302 18:59:06.071580 22732373614720 run.py:483] Algo bellman_ford step 1636 current loss 0.672917, current_train_items 52384.
I0302 18:59:06.094590 22732373614720 run.py:483] Algo bellman_ford step 1637 current loss 0.934357, current_train_items 52416.
I0302 18:59:06.124291 22732373614720 run.py:483] Algo bellman_ford step 1638 current loss 0.945723, current_train_items 52448.
I0302 18:59:06.158294 22732373614720 run.py:483] Algo bellman_ford step 1639 current loss 1.130208, current_train_items 52480.
I0302 18:59:06.176972 22732373614720 run.py:483] Algo bellman_ford step 1640 current loss 0.412660, current_train_items 52512.
I0302 18:59:06.192496 22732373614720 run.py:483] Algo bellman_ford step 1641 current loss 0.582326, current_train_items 52544.
I0302 18:59:06.215274 22732373614720 run.py:483] Algo bellman_ford step 1642 current loss 0.889033, current_train_items 52576.
I0302 18:59:06.243895 22732373614720 run.py:483] Algo bellman_ford step 1643 current loss 1.048557, current_train_items 52608.
I0302 18:59:06.275039 22732373614720 run.py:483] Algo bellman_ford step 1644 current loss 1.171550, current_train_items 52640.
I0302 18:59:06.293721 22732373614720 run.py:483] Algo bellman_ford step 1645 current loss 0.404647, current_train_items 52672.
I0302 18:59:06.309664 22732373614720 run.py:483] Algo bellman_ford step 1646 current loss 0.634133, current_train_items 52704.
I0302 18:59:06.333202 22732373614720 run.py:483] Algo bellman_ford step 1647 current loss 0.930490, current_train_items 52736.
I0302 18:59:06.361007 22732373614720 run.py:483] Algo bellman_ford step 1648 current loss 0.862750, current_train_items 52768.
I0302 18:59:06.394020 22732373614720 run.py:483] Algo bellman_ford step 1649 current loss 1.167830, current_train_items 52800.
I0302 18:59:06.412780 22732373614720 run.py:483] Algo bellman_ford step 1650 current loss 0.527974, current_train_items 52832.
I0302 18:59:06.420782 22732373614720 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0302 18:59:06.420887 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.910, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 18:59:06.437965 22732373614720 run.py:483] Algo bellman_ford step 1651 current loss 0.622524, current_train_items 52864.
I0302 18:59:06.462025 22732373614720 run.py:483] Algo bellman_ford step 1652 current loss 0.969095, current_train_items 52896.
I0302 18:59:06.491219 22732373614720 run.py:483] Algo bellman_ford step 1653 current loss 0.936833, current_train_items 52928.
I0302 18:59:06.521996 22732373614720 run.py:483] Algo bellman_ford step 1654 current loss 1.179576, current_train_items 52960.
I0302 18:59:06.541383 22732373614720 run.py:483] Algo bellman_ford step 1655 current loss 0.439673, current_train_items 52992.
I0302 18:59:06.557068 22732373614720 run.py:483] Algo bellman_ford step 1656 current loss 0.635101, current_train_items 53024.
I0302 18:59:06.579632 22732373614720 run.py:483] Algo bellman_ford step 1657 current loss 0.822956, current_train_items 53056.
I0302 18:59:06.607637 22732373614720 run.py:483] Algo bellman_ford step 1658 current loss 0.953343, current_train_items 53088.
I0302 18:59:06.638010 22732373614720 run.py:483] Algo bellman_ford step 1659 current loss 1.155214, current_train_items 53120.
I0302 18:59:06.657143 22732373614720 run.py:483] Algo bellman_ford step 1660 current loss 0.502097, current_train_items 53152.
I0302 18:59:06.673121 22732373614720 run.py:483] Algo bellman_ford step 1661 current loss 0.611083, current_train_items 53184.
I0302 18:59:06.695005 22732373614720 run.py:483] Algo bellman_ford step 1662 current loss 0.891230, current_train_items 53216.
I0302 18:59:06.725357 22732373614720 run.py:483] Algo bellman_ford step 1663 current loss 0.984949, current_train_items 53248.
I0302 18:59:06.757944 22732373614720 run.py:483] Algo bellman_ford step 1664 current loss 1.322380, current_train_items 53280.
I0302 18:59:06.776732 22732373614720 run.py:483] Algo bellman_ford step 1665 current loss 0.360029, current_train_items 53312.
I0302 18:59:06.792683 22732373614720 run.py:483] Algo bellman_ford step 1666 current loss 0.622239, current_train_items 53344.
I0302 18:59:06.815163 22732373614720 run.py:483] Algo bellman_ford step 1667 current loss 0.886894, current_train_items 53376.
I0302 18:59:06.843046 22732373614720 run.py:483] Algo bellman_ford step 1668 current loss 0.870010, current_train_items 53408.
I0302 18:59:06.873524 22732373614720 run.py:483] Algo bellman_ford step 1669 current loss 1.064268, current_train_items 53440.
I0302 18:59:06.892730 22732373614720 run.py:483] Algo bellman_ford step 1670 current loss 0.432405, current_train_items 53472.
I0302 18:59:06.908493 22732373614720 run.py:483] Algo bellman_ford step 1671 current loss 0.627891, current_train_items 53504.
I0302 18:59:06.930641 22732373614720 run.py:483] Algo bellman_ford step 1672 current loss 0.808786, current_train_items 53536.
I0302 18:59:06.960324 22732373614720 run.py:483] Algo bellman_ford step 1673 current loss 0.951990, current_train_items 53568.
I0302 18:59:06.992572 22732373614720 run.py:483] Algo bellman_ford step 1674 current loss 1.094419, current_train_items 53600.
I0302 18:59:07.011463 22732373614720 run.py:483] Algo bellman_ford step 1675 current loss 0.479129, current_train_items 53632.
I0302 18:59:07.027626 22732373614720 run.py:483] Algo bellman_ford step 1676 current loss 0.664128, current_train_items 53664.
I0302 18:59:07.049866 22732373614720 run.py:483] Algo bellman_ford step 1677 current loss 0.695859, current_train_items 53696.
I0302 18:59:07.078453 22732373614720 run.py:483] Algo bellman_ford step 1678 current loss 0.998431, current_train_items 53728.
I0302 18:59:07.111218 22732373614720 run.py:483] Algo bellman_ford step 1679 current loss 1.128700, current_train_items 53760.
I0302 18:59:07.130047 22732373614720 run.py:483] Algo bellman_ford step 1680 current loss 0.418137, current_train_items 53792.
I0302 18:59:07.145757 22732373614720 run.py:483] Algo bellman_ford step 1681 current loss 0.536379, current_train_items 53824.
I0302 18:59:07.167610 22732373614720 run.py:483] Algo bellman_ford step 1682 current loss 0.896729, current_train_items 53856.
I0302 18:59:07.195441 22732373614720 run.py:483] Algo bellman_ford step 1683 current loss 0.931517, current_train_items 53888.
I0302 18:59:07.227317 22732373614720 run.py:483] Algo bellman_ford step 1684 current loss 1.076204, current_train_items 53920.
I0302 18:59:07.246735 22732373614720 run.py:483] Algo bellman_ford step 1685 current loss 0.369439, current_train_items 53952.
I0302 18:59:07.262631 22732373614720 run.py:483] Algo bellman_ford step 1686 current loss 0.748688, current_train_items 53984.
I0302 18:59:07.285023 22732373614720 run.py:483] Algo bellman_ford step 1687 current loss 1.004306, current_train_items 54016.
I0302 18:59:07.313794 22732373614720 run.py:483] Algo bellman_ford step 1688 current loss 0.968929, current_train_items 54048.
I0302 18:59:07.347651 22732373614720 run.py:483] Algo bellman_ford step 1689 current loss 1.246061, current_train_items 54080.
I0302 18:59:07.366941 22732373614720 run.py:483] Algo bellman_ford step 1690 current loss 0.341796, current_train_items 54112.
I0302 18:59:07.383304 22732373614720 run.py:483] Algo bellman_ford step 1691 current loss 0.640681, current_train_items 54144.
I0302 18:59:07.406271 22732373614720 run.py:483] Algo bellman_ford step 1692 current loss 0.882819, current_train_items 54176.
I0302 18:59:07.435975 22732373614720 run.py:483] Algo bellman_ford step 1693 current loss 0.931505, current_train_items 54208.
I0302 18:59:07.468092 22732373614720 run.py:483] Algo bellman_ford step 1694 current loss 1.205305, current_train_items 54240.
I0302 18:59:07.486809 22732373614720 run.py:483] Algo bellman_ford step 1695 current loss 0.560683, current_train_items 54272.
I0302 18:59:07.503018 22732373614720 run.py:483] Algo bellman_ford step 1696 current loss 0.691139, current_train_items 54304.
I0302 18:59:07.523950 22732373614720 run.py:483] Algo bellman_ford step 1697 current loss 0.853793, current_train_items 54336.
I0302 18:59:07.551466 22732373614720 run.py:483] Algo bellman_ford step 1698 current loss 0.814251, current_train_items 54368.
I0302 18:59:07.583979 22732373614720 run.py:483] Algo bellman_ford step 1699 current loss 1.127586, current_train_items 54400.
I0302 18:59:07.603216 22732373614720 run.py:483] Algo bellman_ford step 1700 current loss 0.452346, current_train_items 54432.
I0302 18:59:07.610975 22732373614720 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.8828125, 'score': 0.8828125, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0302 18:59:07.611091 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.910, current avg val score is 0.883, val scores are: bellman_ford: 0.883
I0302 18:59:07.627918 22732373614720 run.py:483] Algo bellman_ford step 1701 current loss 0.624664, current_train_items 54464.
I0302 18:59:07.650784 22732373614720 run.py:483] Algo bellman_ford step 1702 current loss 0.764550, current_train_items 54496.
I0302 18:59:07.680044 22732373614720 run.py:483] Algo bellman_ford step 1703 current loss 0.886746, current_train_items 54528.
I0302 18:59:07.711840 22732373614720 run.py:483] Algo bellman_ford step 1704 current loss 1.211043, current_train_items 54560.
I0302 18:59:07.731918 22732373614720 run.py:483] Algo bellman_ford step 1705 current loss 0.387842, current_train_items 54592.
I0302 18:59:07.747420 22732373614720 run.py:483] Algo bellman_ford step 1706 current loss 0.578975, current_train_items 54624.
I0302 18:59:07.770152 22732373614720 run.py:483] Algo bellman_ford step 1707 current loss 0.792873, current_train_items 54656.
I0302 18:59:07.799284 22732373614720 run.py:483] Algo bellman_ford step 1708 current loss 1.045373, current_train_items 54688.
I0302 18:59:07.832918 22732373614720 run.py:483] Algo bellman_ford step 1709 current loss 1.121456, current_train_items 54720.
I0302 18:59:07.851927 22732373614720 run.py:483] Algo bellman_ford step 1710 current loss 0.497719, current_train_items 54752.
I0302 18:59:07.868117 22732373614720 run.py:483] Algo bellman_ford step 1711 current loss 0.722906, current_train_items 54784.
I0302 18:59:07.891248 22732373614720 run.py:483] Algo bellman_ford step 1712 current loss 0.841975, current_train_items 54816.
I0302 18:59:07.919844 22732373614720 run.py:483] Algo bellman_ford step 1713 current loss 0.943234, current_train_items 54848.
I0302 18:59:07.951393 22732373614720 run.py:483] Algo bellman_ford step 1714 current loss 1.072984, current_train_items 54880.
I0302 18:59:07.970224 22732373614720 run.py:483] Algo bellman_ford step 1715 current loss 0.410721, current_train_items 54912.
I0302 18:59:07.985792 22732373614720 run.py:483] Algo bellman_ford step 1716 current loss 0.560239, current_train_items 54944.
I0302 18:59:08.008848 22732373614720 run.py:483] Algo bellman_ford step 1717 current loss 0.825231, current_train_items 54976.
I0302 18:59:08.036406 22732373614720 run.py:483] Algo bellman_ford step 1718 current loss 0.869055, current_train_items 55008.
I0302 18:59:08.068338 22732373614720 run.py:483] Algo bellman_ford step 1719 current loss 1.056094, current_train_items 55040.
I0302 18:59:08.087456 22732373614720 run.py:483] Algo bellman_ford step 1720 current loss 0.434289, current_train_items 55072.
I0302 18:59:08.103295 22732373614720 run.py:483] Algo bellman_ford step 1721 current loss 0.683689, current_train_items 55104.
I0302 18:59:08.126099 22732373614720 run.py:483] Algo bellman_ford step 1722 current loss 0.720842, current_train_items 55136.
I0302 18:59:08.154889 22732373614720 run.py:483] Algo bellman_ford step 1723 current loss 0.839664, current_train_items 55168.
I0302 18:59:08.186442 22732373614720 run.py:483] Algo bellman_ford step 1724 current loss 0.968869, current_train_items 55200.
I0302 18:59:08.205281 22732373614720 run.py:483] Algo bellman_ford step 1725 current loss 0.386560, current_train_items 55232.
I0302 18:59:08.221323 22732373614720 run.py:483] Algo bellman_ford step 1726 current loss 0.683509, current_train_items 55264.
I0302 18:59:08.245416 22732373614720 run.py:483] Algo bellman_ford step 1727 current loss 0.886656, current_train_items 55296.
I0302 18:59:08.274095 22732373614720 run.py:483] Algo bellman_ford step 1728 current loss 0.977356, current_train_items 55328.
I0302 18:59:08.305869 22732373614720 run.py:483] Algo bellman_ford step 1729 current loss 1.223620, current_train_items 55360.
I0302 18:59:08.324876 22732373614720 run.py:483] Algo bellman_ford step 1730 current loss 0.331613, current_train_items 55392.
I0302 18:59:08.340903 22732373614720 run.py:483] Algo bellman_ford step 1731 current loss 0.612740, current_train_items 55424.
I0302 18:59:08.363986 22732373614720 run.py:483] Algo bellman_ford step 1732 current loss 0.887206, current_train_items 55456.
I0302 18:59:08.393065 22732373614720 run.py:483] Algo bellman_ford step 1733 current loss 0.929190, current_train_items 55488.
I0302 18:59:08.423177 22732373614720 run.py:483] Algo bellman_ford step 1734 current loss 1.069139, current_train_items 55520.
I0302 18:59:08.442288 22732373614720 run.py:483] Algo bellman_ford step 1735 current loss 0.564584, current_train_items 55552.
I0302 18:59:08.458440 22732373614720 run.py:483] Algo bellman_ford step 1736 current loss 0.634933, current_train_items 55584.
I0302 18:59:08.481560 22732373614720 run.py:483] Algo bellman_ford step 1737 current loss 0.819801, current_train_items 55616.
I0302 18:59:08.510341 22732373614720 run.py:483] Algo bellman_ford step 1738 current loss 0.998268, current_train_items 55648.
I0302 18:59:08.542839 22732373614720 run.py:483] Algo bellman_ford step 1739 current loss 1.095023, current_train_items 55680.
I0302 18:59:08.561917 22732373614720 run.py:483] Algo bellman_ford step 1740 current loss 0.351048, current_train_items 55712.
I0302 18:59:08.578096 22732373614720 run.py:483] Algo bellman_ford step 1741 current loss 0.633484, current_train_items 55744.
I0302 18:59:08.600793 22732373614720 run.py:483] Algo bellman_ford step 1742 current loss 0.692009, current_train_items 55776.
I0302 18:59:08.630304 22732373614720 run.py:483] Algo bellman_ford step 1743 current loss 0.889960, current_train_items 55808.
I0302 18:59:08.661262 22732373614720 run.py:483] Algo bellman_ford step 1744 current loss 1.008510, current_train_items 55840.
I0302 18:59:08.680446 22732373614720 run.py:483] Algo bellman_ford step 1745 current loss 0.422438, current_train_items 55872.
I0302 18:59:08.696649 22732373614720 run.py:483] Algo bellman_ford step 1746 current loss 0.636986, current_train_items 55904.
I0302 18:59:08.719263 22732373614720 run.py:483] Algo bellman_ford step 1747 current loss 0.963049, current_train_items 55936.
I0302 18:59:08.748118 22732373614720 run.py:483] Algo bellman_ford step 1748 current loss 0.909853, current_train_items 55968.
I0302 18:59:08.778718 22732373614720 run.py:483] Algo bellman_ford step 1749 current loss 1.028185, current_train_items 56000.
I0302 18:59:08.797886 22732373614720 run.py:483] Algo bellman_ford step 1750 current loss 0.316345, current_train_items 56032.
I0302 18:59:08.805790 22732373614720 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0302 18:59:08.805898 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.910, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 18:59:08.822720 22732373614720 run.py:483] Algo bellman_ford step 1751 current loss 0.546354, current_train_items 56064.
I0302 18:59:08.845851 22732373614720 run.py:483] Algo bellman_ford step 1752 current loss 0.745024, current_train_items 56096.
I0302 18:59:08.876853 22732373614720 run.py:483] Algo bellman_ford step 1753 current loss 1.029358, current_train_items 56128.
I0302 18:59:08.910704 22732373614720 run.py:483] Algo bellman_ford step 1754 current loss 1.076318, current_train_items 56160.
I0302 18:59:08.930142 22732373614720 run.py:483] Algo bellman_ford step 1755 current loss 0.355570, current_train_items 56192.
I0302 18:59:08.946106 22732373614720 run.py:483] Algo bellman_ford step 1756 current loss 0.614386, current_train_items 56224.
I0302 18:59:08.969763 22732373614720 run.py:483] Algo bellman_ford step 1757 current loss 0.810023, current_train_items 56256.
I0302 18:59:08.999317 22732373614720 run.py:483] Algo bellman_ford step 1758 current loss 1.004696, current_train_items 56288.
I0302 18:59:09.029283 22732373614720 run.py:483] Algo bellman_ford step 1759 current loss 0.921347, current_train_items 56320.
I0302 18:59:09.048653 22732373614720 run.py:483] Algo bellman_ford step 1760 current loss 0.392839, current_train_items 56352.
I0302 18:59:09.064712 22732373614720 run.py:483] Algo bellman_ford step 1761 current loss 0.555878, current_train_items 56384.
I0302 18:59:09.088245 22732373614720 run.py:483] Algo bellman_ford step 1762 current loss 0.832893, current_train_items 56416.
I0302 18:59:09.119146 22732373614720 run.py:483] Algo bellman_ford step 1763 current loss 0.935399, current_train_items 56448.
I0302 18:59:09.152994 22732373614720 run.py:483] Algo bellman_ford step 1764 current loss 1.154265, current_train_items 56480.
I0302 18:59:09.172224 22732373614720 run.py:483] Algo bellman_ford step 1765 current loss 0.430396, current_train_items 56512.
I0302 18:59:09.187995 22732373614720 run.py:483] Algo bellman_ford step 1766 current loss 0.573815, current_train_items 56544.
I0302 18:59:09.210422 22732373614720 run.py:483] Algo bellman_ford step 1767 current loss 0.734417, current_train_items 56576.
I0302 18:59:09.239575 22732373614720 run.py:483] Algo bellman_ford step 1768 current loss 0.900461, current_train_items 56608.
I0302 18:59:09.270411 22732373614720 run.py:483] Algo bellman_ford step 1769 current loss 1.004366, current_train_items 56640.
I0302 18:59:09.290047 22732373614720 run.py:483] Algo bellman_ford step 1770 current loss 0.371585, current_train_items 56672.
I0302 18:59:09.306267 22732373614720 run.py:483] Algo bellman_ford step 1771 current loss 0.625376, current_train_items 56704.
I0302 18:59:09.329717 22732373614720 run.py:483] Algo bellman_ford step 1772 current loss 0.928272, current_train_items 56736.
I0302 18:59:09.359650 22732373614720 run.py:483] Algo bellman_ford step 1773 current loss 1.016421, current_train_items 56768.
I0302 18:59:09.390560 22732373614720 run.py:483] Algo bellman_ford step 1774 current loss 0.989491, current_train_items 56800.
I0302 18:59:09.409970 22732373614720 run.py:483] Algo bellman_ford step 1775 current loss 0.403780, current_train_items 56832.
I0302 18:59:09.426187 22732373614720 run.py:483] Algo bellman_ford step 1776 current loss 0.682467, current_train_items 56864.
I0302 18:59:09.449399 22732373614720 run.py:483] Algo bellman_ford step 1777 current loss 0.900364, current_train_items 56896.
I0302 18:59:09.478231 22732373614720 run.py:483] Algo bellman_ford step 1778 current loss 1.081842, current_train_items 56928.
I0302 18:59:09.508393 22732373614720 run.py:483] Algo bellman_ford step 1779 current loss 0.933184, current_train_items 56960.
I0302 18:59:09.527179 22732373614720 run.py:483] Algo bellman_ford step 1780 current loss 0.408933, current_train_items 56992.
I0302 18:59:09.543099 22732373614720 run.py:483] Algo bellman_ford step 1781 current loss 0.597311, current_train_items 57024.
I0302 18:59:09.565939 22732373614720 run.py:483] Algo bellman_ford step 1782 current loss 0.718283, current_train_items 57056.
I0302 18:59:09.594923 22732373614720 run.py:483] Algo bellman_ford step 1783 current loss 1.013835, current_train_items 57088.
I0302 18:59:09.626298 22732373614720 run.py:483] Algo bellman_ford step 1784 current loss 1.069378, current_train_items 57120.
I0302 18:59:09.645563 22732373614720 run.py:483] Algo bellman_ford step 1785 current loss 0.451307, current_train_items 57152.
I0302 18:59:09.661823 22732373614720 run.py:483] Algo bellman_ford step 1786 current loss 0.651231, current_train_items 57184.
I0302 18:59:09.683978 22732373614720 run.py:483] Algo bellman_ford step 1787 current loss 0.745501, current_train_items 57216.
I0302 18:59:09.712018 22732373614720 run.py:483] Algo bellman_ford step 1788 current loss 0.825170, current_train_items 57248.
I0302 18:59:09.744148 22732373614720 run.py:483] Algo bellman_ford step 1789 current loss 0.990628, current_train_items 57280.
I0302 18:59:09.763610 22732373614720 run.py:483] Algo bellman_ford step 1790 current loss 0.403287, current_train_items 57312.
I0302 18:59:09.780224 22732373614720 run.py:483] Algo bellman_ford step 1791 current loss 0.640647, current_train_items 57344.
I0302 18:59:09.803342 22732373614720 run.py:483] Algo bellman_ford step 1792 current loss 0.738948, current_train_items 57376.
I0302 18:59:09.830913 22732373614720 run.py:483] Algo bellman_ford step 1793 current loss 0.971476, current_train_items 57408.
I0302 18:59:09.862733 22732373614720 run.py:483] Algo bellman_ford step 1794 current loss 1.234524, current_train_items 57440.
I0302 18:59:09.881816 22732373614720 run.py:483] Algo bellman_ford step 1795 current loss 0.462057, current_train_items 57472.
I0302 18:59:09.897902 22732373614720 run.py:483] Algo bellman_ford step 1796 current loss 0.666736, current_train_items 57504.
I0302 18:59:09.921546 22732373614720 run.py:483] Algo bellman_ford step 1797 current loss 0.872002, current_train_items 57536.
I0302 18:59:09.950926 22732373614720 run.py:483] Algo bellman_ford step 1798 current loss 0.949133, current_train_items 57568.
I0302 18:59:09.984075 22732373614720 run.py:483] Algo bellman_ford step 1799 current loss 1.221282, current_train_items 57600.
I0302 18:59:10.003435 22732373614720 run.py:483] Algo bellman_ford step 1800 current loss 0.486024, current_train_items 57632.
I0302 18:59:10.010874 22732373614720 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.8662109375, 'score': 0.8662109375, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0302 18:59:10.010980 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.910, current avg val score is 0.866, val scores are: bellman_ford: 0.866
I0302 18:59:10.027245 22732373614720 run.py:483] Algo bellman_ford step 1801 current loss 0.549142, current_train_items 57664.
I0302 18:59:10.050843 22732373614720 run.py:483] Algo bellman_ford step 1802 current loss 0.849871, current_train_items 57696.
I0302 18:59:10.079773 22732373614720 run.py:483] Algo bellman_ford step 1803 current loss 0.916600, current_train_items 57728.
I0302 18:59:10.111066 22732373614720 run.py:483] Algo bellman_ford step 1804 current loss 1.289473, current_train_items 57760.
I0302 18:59:10.130361 22732373614720 run.py:483] Algo bellman_ford step 1805 current loss 0.401332, current_train_items 57792.
I0302 18:59:10.145840 22732373614720 run.py:483] Algo bellman_ford step 1806 current loss 0.569370, current_train_items 57824.
I0302 18:59:10.168246 22732373614720 run.py:483] Algo bellman_ford step 1807 current loss 0.843535, current_train_items 57856.
I0302 18:59:10.196353 22732373614720 run.py:483] Algo bellman_ford step 1808 current loss 0.850370, current_train_items 57888.
I0302 18:59:10.229353 22732373614720 run.py:483] Algo bellman_ford step 1809 current loss 1.062628, current_train_items 57920.
I0302 18:59:10.248209 22732373614720 run.py:483] Algo bellman_ford step 1810 current loss 0.422855, current_train_items 57952.
I0302 18:59:10.264116 22732373614720 run.py:483] Algo bellman_ford step 1811 current loss 0.697622, current_train_items 57984.
I0302 18:59:10.287167 22732373614720 run.py:483] Algo bellman_ford step 1812 current loss 1.054334, current_train_items 58016.
I0302 18:59:10.314018 22732373614720 run.py:483] Algo bellman_ford step 1813 current loss 0.896512, current_train_items 58048.
I0302 18:59:10.345685 22732373614720 run.py:483] Algo bellman_ford step 1814 current loss 1.053646, current_train_items 58080.
I0302 18:59:10.364658 22732373614720 run.py:483] Algo bellman_ford step 1815 current loss 0.431793, current_train_items 58112.
I0302 18:59:10.381102 22732373614720 run.py:483] Algo bellman_ford step 1816 current loss 0.739545, current_train_items 58144.
I0302 18:59:10.403612 22732373614720 run.py:483] Algo bellman_ford step 1817 current loss 0.764284, current_train_items 58176.
I0302 18:59:10.432680 22732373614720 run.py:483] Algo bellman_ford step 1818 current loss 0.958222, current_train_items 58208.
I0302 18:59:10.464123 22732373614720 run.py:483] Algo bellman_ford step 1819 current loss 1.087901, current_train_items 58240.
I0302 18:59:10.482991 22732373614720 run.py:483] Algo bellman_ford step 1820 current loss 0.432866, current_train_items 58272.
I0302 18:59:10.499451 22732373614720 run.py:483] Algo bellman_ford step 1821 current loss 0.607337, current_train_items 58304.
I0302 18:59:10.524241 22732373614720 run.py:483] Algo bellman_ford step 1822 current loss 0.847702, current_train_items 58336.
I0302 18:59:10.553822 22732373614720 run.py:483] Algo bellman_ford step 1823 current loss 0.956920, current_train_items 58368.
I0302 18:59:10.584604 22732373614720 run.py:483] Algo bellman_ford step 1824 current loss 0.983151, current_train_items 58400.
I0302 18:59:10.603744 22732373614720 run.py:483] Algo bellman_ford step 1825 current loss 0.404455, current_train_items 58432.
I0302 18:59:10.619930 22732373614720 run.py:483] Algo bellman_ford step 1826 current loss 0.651844, current_train_items 58464.
I0302 18:59:10.642450 22732373614720 run.py:483] Algo bellman_ford step 1827 current loss 0.692131, current_train_items 58496.
I0302 18:59:10.672507 22732373614720 run.py:483] Algo bellman_ford step 1828 current loss 1.123429, current_train_items 58528.
I0302 18:59:10.706794 22732373614720 run.py:483] Algo bellman_ford step 1829 current loss 1.244950, current_train_items 58560.
I0302 18:59:10.726190 22732373614720 run.py:483] Algo bellman_ford step 1830 current loss 0.401323, current_train_items 58592.
I0302 18:59:10.742444 22732373614720 run.py:483] Algo bellman_ford step 1831 current loss 0.696221, current_train_items 58624.
I0302 18:59:10.766854 22732373614720 run.py:483] Algo bellman_ford step 1832 current loss 0.996138, current_train_items 58656.
I0302 18:59:10.796028 22732373614720 run.py:483] Algo bellman_ford step 1833 current loss 1.095573, current_train_items 58688.
I0302 18:59:10.827228 22732373614720 run.py:483] Algo bellman_ford step 1834 current loss 0.923892, current_train_items 58720.
I0302 18:59:10.846376 22732373614720 run.py:483] Algo bellman_ford step 1835 current loss 0.332159, current_train_items 58752.
I0302 18:59:10.862644 22732373614720 run.py:483] Algo bellman_ford step 1836 current loss 0.583877, current_train_items 58784.
I0302 18:59:10.886876 22732373614720 run.py:483] Algo bellman_ford step 1837 current loss 1.013003, current_train_items 58816.
I0302 18:59:10.915470 22732373614720 run.py:483] Algo bellman_ford step 1838 current loss 0.979432, current_train_items 58848.
I0302 18:59:10.947470 22732373614720 run.py:483] Algo bellman_ford step 1839 current loss 1.072673, current_train_items 58880.
I0302 18:59:10.966527 22732373614720 run.py:483] Algo bellman_ford step 1840 current loss 0.367183, current_train_items 58912.
I0302 18:59:10.982725 22732373614720 run.py:483] Algo bellman_ford step 1841 current loss 0.630264, current_train_items 58944.
I0302 18:59:11.006503 22732373614720 run.py:483] Algo bellman_ford step 1842 current loss 0.849417, current_train_items 58976.
I0302 18:59:11.036003 22732373614720 run.py:483] Algo bellman_ford step 1843 current loss 1.042071, current_train_items 59008.
I0302 18:59:11.068190 22732373614720 run.py:483] Algo bellman_ford step 1844 current loss 1.223132, current_train_items 59040.
I0302 18:59:11.087146 22732373614720 run.py:483] Algo bellman_ford step 1845 current loss 0.402875, current_train_items 59072.
I0302 18:59:11.103277 22732373614720 run.py:483] Algo bellman_ford step 1846 current loss 0.561405, current_train_items 59104.
I0302 18:59:11.126547 22732373614720 run.py:483] Algo bellman_ford step 1847 current loss 0.886078, current_train_items 59136.
I0302 18:59:11.155861 22732373614720 run.py:483] Algo bellman_ford step 1848 current loss 0.897176, current_train_items 59168.
I0302 18:59:11.188997 22732373614720 run.py:483] Algo bellman_ford step 1849 current loss 1.182521, current_train_items 59200.
I0302 18:59:11.208226 22732373614720 run.py:483] Algo bellman_ford step 1850 current loss 0.385975, current_train_items 59232.
I0302 18:59:11.216202 22732373614720 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0302 18:59:11.216307 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.910, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 18:59:11.233062 22732373614720 run.py:483] Algo bellman_ford step 1851 current loss 0.591378, current_train_items 59264.
I0302 18:59:11.256729 22732373614720 run.py:483] Algo bellman_ford step 1852 current loss 0.816431, current_train_items 59296.
I0302 18:59:11.286300 22732373614720 run.py:483] Algo bellman_ford step 1853 current loss 0.939421, current_train_items 59328.
I0302 18:59:11.318791 22732373614720 run.py:483] Algo bellman_ford step 1854 current loss 1.083506, current_train_items 59360.
I0302 18:59:11.338370 22732373614720 run.py:483] Algo bellman_ford step 1855 current loss 0.363929, current_train_items 59392.
I0302 18:59:11.353592 22732373614720 run.py:483] Algo bellman_ford step 1856 current loss 0.590797, current_train_items 59424.
I0302 18:59:11.377120 22732373614720 run.py:483] Algo bellman_ford step 1857 current loss 0.833679, current_train_items 59456.
I0302 18:59:11.405421 22732373614720 run.py:483] Algo bellman_ford step 1858 current loss 0.789394, current_train_items 59488.
I0302 18:59:11.437554 22732373614720 run.py:483] Algo bellman_ford step 1859 current loss 1.156678, current_train_items 59520.
I0302 18:59:11.456985 22732373614720 run.py:483] Algo bellman_ford step 1860 current loss 0.367583, current_train_items 59552.
I0302 18:59:11.473452 22732373614720 run.py:483] Algo bellman_ford step 1861 current loss 0.514829, current_train_items 59584.
I0302 18:59:11.496161 22732373614720 run.py:483] Algo bellman_ford step 1862 current loss 0.819198, current_train_items 59616.
I0302 18:59:11.525131 22732373614720 run.py:483] Algo bellman_ford step 1863 current loss 0.833643, current_train_items 59648.
I0302 18:59:11.558105 22732373614720 run.py:483] Algo bellman_ford step 1864 current loss 1.159760, current_train_items 59680.
I0302 18:59:11.577177 22732373614720 run.py:483] Algo bellman_ford step 1865 current loss 0.505685, current_train_items 59712.
I0302 18:59:11.593245 22732373614720 run.py:483] Algo bellman_ford step 1866 current loss 0.684677, current_train_items 59744.
I0302 18:59:11.617336 22732373614720 run.py:483] Algo bellman_ford step 1867 current loss 0.977304, current_train_items 59776.
I0302 18:59:11.646778 22732373614720 run.py:483] Algo bellman_ford step 1868 current loss 0.935171, current_train_items 59808.
I0302 18:59:11.678469 22732373614720 run.py:483] Algo bellman_ford step 1869 current loss 1.020497, current_train_items 59840.
I0302 18:59:11.697663 22732373614720 run.py:483] Algo bellman_ford step 1870 current loss 0.373650, current_train_items 59872.
I0302 18:59:11.713605 22732373614720 run.py:483] Algo bellman_ford step 1871 current loss 0.534170, current_train_items 59904.
I0302 18:59:11.736135 22732373614720 run.py:483] Algo bellman_ford step 1872 current loss 0.763614, current_train_items 59936.
I0302 18:59:11.765588 22732373614720 run.py:483] Algo bellman_ford step 1873 current loss 0.891499, current_train_items 59968.
I0302 18:59:11.797243 22732373614720 run.py:483] Algo bellman_ford step 1874 current loss 1.127310, current_train_items 60000.
I0302 18:59:11.816625 22732373614720 run.py:483] Algo bellman_ford step 1875 current loss 0.408998, current_train_items 60032.
I0302 18:59:11.832587 22732373614720 run.py:483] Algo bellman_ford step 1876 current loss 0.542643, current_train_items 60064.
I0302 18:59:11.855264 22732373614720 run.py:483] Algo bellman_ford step 1877 current loss 0.895301, current_train_items 60096.
I0302 18:59:11.882929 22732373614720 run.py:483] Algo bellman_ford step 1878 current loss 0.782632, current_train_items 60128.
I0302 18:59:11.916906 22732373614720 run.py:483] Algo bellman_ford step 1879 current loss 1.231343, current_train_items 60160.
I0302 18:59:11.935748 22732373614720 run.py:483] Algo bellman_ford step 1880 current loss 0.412610, current_train_items 60192.
I0302 18:59:11.952028 22732373614720 run.py:483] Algo bellman_ford step 1881 current loss 0.701951, current_train_items 60224.
I0302 18:59:11.974384 22732373614720 run.py:483] Algo bellman_ford step 1882 current loss 0.808402, current_train_items 60256.
I0302 18:59:12.003407 22732373614720 run.py:483] Algo bellman_ford step 1883 current loss 1.071744, current_train_items 60288.
I0302 18:59:12.035475 22732373614720 run.py:483] Algo bellman_ford step 1884 current loss 1.346419, current_train_items 60320.
I0302 18:59:12.054855 22732373614720 run.py:483] Algo bellman_ford step 1885 current loss 0.354854, current_train_items 60352.
I0302 18:59:12.071271 22732373614720 run.py:483] Algo bellman_ford step 1886 current loss 0.632472, current_train_items 60384.
I0302 18:59:12.094167 22732373614720 run.py:483] Algo bellman_ford step 1887 current loss 1.012185, current_train_items 60416.
I0302 18:59:12.122231 22732373614720 run.py:483] Algo bellman_ford step 1888 current loss 0.955983, current_train_items 60448.
I0302 18:59:12.155927 22732373614720 run.py:483] Algo bellman_ford step 1889 current loss 1.226348, current_train_items 60480.
I0302 18:59:12.175113 22732373614720 run.py:483] Algo bellman_ford step 1890 current loss 0.394468, current_train_items 60512.
I0302 18:59:12.191236 22732373614720 run.py:483] Algo bellman_ford step 1891 current loss 0.629688, current_train_items 60544.
I0302 18:59:12.213350 22732373614720 run.py:483] Algo bellman_ford step 1892 current loss 0.794277, current_train_items 60576.
I0302 18:59:12.243366 22732373614720 run.py:483] Algo bellman_ford step 1893 current loss 0.990382, current_train_items 60608.
I0302 18:59:12.278197 22732373614720 run.py:483] Algo bellman_ford step 1894 current loss 1.305699, current_train_items 60640.
I0302 18:59:12.297152 22732373614720 run.py:483] Algo bellman_ford step 1895 current loss 0.534332, current_train_items 60672.
I0302 18:59:12.313118 22732373614720 run.py:483] Algo bellman_ford step 1896 current loss 0.619747, current_train_items 60704.
I0302 18:59:12.336024 22732373614720 run.py:483] Algo bellman_ford step 1897 current loss 0.875749, current_train_items 60736.
I0302 18:59:12.365240 22732373614720 run.py:483] Algo bellman_ford step 1898 current loss 0.883690, current_train_items 60768.
I0302 18:59:12.397307 22732373614720 run.py:483] Algo bellman_ford step 1899 current loss 1.125491, current_train_items 60800.
I0302 18:59:12.416858 22732373614720 run.py:483] Algo bellman_ford step 1900 current loss 0.444488, current_train_items 60832.
I0302 18:59:12.424300 22732373614720 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.8916015625, 'score': 0.8916015625, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0302 18:59:12.424413 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.910, current avg val score is 0.892, val scores are: bellman_ford: 0.892
I0302 18:59:12.440950 22732373614720 run.py:483] Algo bellman_ford step 1901 current loss 0.574859, current_train_items 60864.
I0302 18:59:12.463743 22732373614720 run.py:483] Algo bellman_ford step 1902 current loss 0.624039, current_train_items 60896.
I0302 18:59:12.491511 22732373614720 run.py:483] Algo bellman_ford step 1903 current loss 0.792535, current_train_items 60928.
I0302 18:59:12.523673 22732373614720 run.py:483] Algo bellman_ford step 1904 current loss 1.033610, current_train_items 60960.
I0302 18:59:12.542937 22732373614720 run.py:483] Algo bellman_ford step 1905 current loss 0.482414, current_train_items 60992.
I0302 18:59:12.558545 22732373614720 run.py:483] Algo bellman_ford step 1906 current loss 0.592380, current_train_items 61024.
I0302 18:59:12.580447 22732373614720 run.py:483] Algo bellman_ford step 1907 current loss 0.781524, current_train_items 61056.
I0302 18:59:12.608994 22732373614720 run.py:483] Algo bellman_ford step 1908 current loss 0.960511, current_train_items 61088.
I0302 18:59:12.640589 22732373614720 run.py:483] Algo bellman_ford step 1909 current loss 0.950874, current_train_items 61120.
I0302 18:59:12.659777 22732373614720 run.py:483] Algo bellman_ford step 1910 current loss 0.431222, current_train_items 61152.
I0302 18:59:12.676064 22732373614720 run.py:483] Algo bellman_ford step 1911 current loss 0.601337, current_train_items 61184.
I0302 18:59:12.699778 22732373614720 run.py:483] Algo bellman_ford step 1912 current loss 0.866300, current_train_items 61216.
I0302 18:59:12.728308 22732373614720 run.py:483] Algo bellman_ford step 1913 current loss 0.872045, current_train_items 61248.
I0302 18:59:12.760772 22732373614720 run.py:483] Algo bellman_ford step 1914 current loss 0.982581, current_train_items 61280.
I0302 18:59:12.779906 22732373614720 run.py:483] Algo bellman_ford step 1915 current loss 0.319344, current_train_items 61312.
I0302 18:59:12.796175 22732373614720 run.py:483] Algo bellman_ford step 1916 current loss 0.616221, current_train_items 61344.
I0302 18:59:12.819514 22732373614720 run.py:483] Algo bellman_ford step 1917 current loss 0.925384, current_train_items 61376.
I0302 18:59:12.849218 22732373614720 run.py:483] Algo bellman_ford step 1918 current loss 0.975544, current_train_items 61408.
I0302 18:59:12.882375 22732373614720 run.py:483] Algo bellman_ford step 1919 current loss 1.138795, current_train_items 61440.
I0302 18:59:12.901131 22732373614720 run.py:483] Algo bellman_ford step 1920 current loss 0.426953, current_train_items 61472.
I0302 18:59:12.917317 22732373614720 run.py:483] Algo bellman_ford step 1921 current loss 0.662409, current_train_items 61504.
I0302 18:59:12.939716 22732373614720 run.py:483] Algo bellman_ford step 1922 current loss 0.773910, current_train_items 61536.
I0302 18:59:12.968029 22732373614720 run.py:483] Algo bellman_ford step 1923 current loss 0.789556, current_train_items 61568.
I0302 18:59:13.000160 22732373614720 run.py:483] Algo bellman_ford step 1924 current loss 1.053138, current_train_items 61600.
I0302 18:59:13.019386 22732373614720 run.py:483] Algo bellman_ford step 1925 current loss 0.386948, current_train_items 61632.
I0302 18:59:13.035500 22732373614720 run.py:483] Algo bellman_ford step 1926 current loss 0.657740, current_train_items 61664.
I0302 18:59:13.057101 22732373614720 run.py:483] Algo bellman_ford step 1927 current loss 0.632920, current_train_items 61696.
I0302 18:59:13.085420 22732373614720 run.py:483] Algo bellman_ford step 1928 current loss 0.822681, current_train_items 61728.
I0302 18:59:13.115908 22732373614720 run.py:483] Algo bellman_ford step 1929 current loss 1.018310, current_train_items 61760.
I0302 18:59:13.135016 22732373614720 run.py:483] Algo bellman_ford step 1930 current loss 0.422416, current_train_items 61792.
I0302 18:59:13.150564 22732373614720 run.py:483] Algo bellman_ford step 1931 current loss 0.450234, current_train_items 61824.
I0302 18:59:13.174446 22732373614720 run.py:483] Algo bellman_ford step 1932 current loss 0.888528, current_train_items 61856.
I0302 18:59:13.203308 22732373614720 run.py:483] Algo bellman_ford step 1933 current loss 1.062121, current_train_items 61888.
I0302 18:59:13.233820 22732373614720 run.py:483] Algo bellman_ford step 1934 current loss 0.948642, current_train_items 61920.
I0302 18:59:13.252957 22732373614720 run.py:483] Algo bellman_ford step 1935 current loss 0.569794, current_train_items 61952.
I0302 18:59:13.268978 22732373614720 run.py:483] Algo bellman_ford step 1936 current loss 0.605510, current_train_items 61984.
I0302 18:59:13.292391 22732373614720 run.py:483] Algo bellman_ford step 1937 current loss 0.685969, current_train_items 62016.
I0302 18:59:13.320356 22732373614720 run.py:483] Algo bellman_ford step 1938 current loss 0.860910, current_train_items 62048.
I0302 18:59:13.353070 22732373614720 run.py:483] Algo bellman_ford step 1939 current loss 1.055948, current_train_items 62080.
I0302 18:59:13.372295 22732373614720 run.py:483] Algo bellman_ford step 1940 current loss 0.393861, current_train_items 62112.
I0302 18:59:13.388433 22732373614720 run.py:483] Algo bellman_ford step 1941 current loss 0.540708, current_train_items 62144.
I0302 18:59:13.411289 22732373614720 run.py:483] Algo bellman_ford step 1942 current loss 0.841254, current_train_items 62176.
I0302 18:59:13.439983 22732373614720 run.py:483] Algo bellman_ford step 1943 current loss 0.818172, current_train_items 62208.
I0302 18:59:13.471953 22732373614720 run.py:483] Algo bellman_ford step 1944 current loss 1.016512, current_train_items 62240.
I0302 18:59:13.490987 22732373614720 run.py:483] Algo bellman_ford step 1945 current loss 0.373312, current_train_items 62272.
I0302 18:59:13.507054 22732373614720 run.py:483] Algo bellman_ford step 1946 current loss 0.613294, current_train_items 62304.
I0302 18:59:13.529969 22732373614720 run.py:483] Algo bellman_ford step 1947 current loss 0.719105, current_train_items 62336.
I0302 18:59:13.560510 22732373614720 run.py:483] Algo bellman_ford step 1948 current loss 1.031095, current_train_items 62368.
I0302 18:59:13.591201 22732373614720 run.py:483] Algo bellman_ford step 1949 current loss 0.993639, current_train_items 62400.
I0302 18:59:13.609997 22732373614720 run.py:483] Algo bellman_ford step 1950 current loss 0.385120, current_train_items 62432.
I0302 18:59:13.617890 22732373614720 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.8671875, 'score': 0.8671875, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0302 18:59:13.617994 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.910, current avg val score is 0.867, val scores are: bellman_ford: 0.867
I0302 18:59:13.634786 22732373614720 run.py:483] Algo bellman_ford step 1951 current loss 0.666465, current_train_items 62464.
I0302 18:59:13.658833 22732373614720 run.py:483] Algo bellman_ford step 1952 current loss 0.855797, current_train_items 62496.
I0302 18:59:13.690264 22732373614720 run.py:483] Algo bellman_ford step 1953 current loss 0.953992, current_train_items 62528.
I0302 18:59:13.722225 22732373614720 run.py:483] Algo bellman_ford step 1954 current loss 0.892130, current_train_items 62560.
I0302 18:59:13.741874 22732373614720 run.py:483] Algo bellman_ford step 1955 current loss 0.518880, current_train_items 62592.
I0302 18:59:13.756838 22732373614720 run.py:483] Algo bellman_ford step 1956 current loss 0.504210, current_train_items 62624.
I0302 18:59:13.779871 22732373614720 run.py:483] Algo bellman_ford step 1957 current loss 0.845912, current_train_items 62656.
I0302 18:59:13.809410 22732373614720 run.py:483] Algo bellman_ford step 1958 current loss 0.915908, current_train_items 62688.
I0302 18:59:13.842789 22732373614720 run.py:483] Algo bellman_ford step 1959 current loss 0.959454, current_train_items 62720.
I0302 18:59:13.862361 22732373614720 run.py:483] Algo bellman_ford step 1960 current loss 0.433466, current_train_items 62752.
I0302 18:59:13.878583 22732373614720 run.py:483] Algo bellman_ford step 1961 current loss 0.681368, current_train_items 62784.
I0302 18:59:13.901237 22732373614720 run.py:483] Algo bellman_ford step 1962 current loss 0.813284, current_train_items 62816.
I0302 18:59:13.929507 22732373614720 run.py:483] Algo bellman_ford step 1963 current loss 0.855285, current_train_items 62848.
I0302 18:59:13.964148 22732373614720 run.py:483] Algo bellman_ford step 1964 current loss 1.044766, current_train_items 62880.
I0302 18:59:13.983499 22732373614720 run.py:483] Algo bellman_ford step 1965 current loss 0.370313, current_train_items 62912.
I0302 18:59:13.999372 22732373614720 run.py:483] Algo bellman_ford step 1966 current loss 0.522981, current_train_items 62944.
I0302 18:59:14.022034 22732373614720 run.py:483] Algo bellman_ford step 1967 current loss 0.870955, current_train_items 62976.
I0302 18:59:14.050517 22732373614720 run.py:483] Algo bellman_ford step 1968 current loss 0.842563, current_train_items 63008.
I0302 18:59:14.082048 22732373614720 run.py:483] Algo bellman_ford step 1969 current loss 1.021055, current_train_items 63040.
I0302 18:59:14.101181 22732373614720 run.py:483] Algo bellman_ford step 1970 current loss 0.340184, current_train_items 63072.
I0302 18:59:14.117111 22732373614720 run.py:483] Algo bellman_ford step 1971 current loss 0.529685, current_train_items 63104.
I0302 18:59:14.139373 22732373614720 run.py:483] Algo bellman_ford step 1972 current loss 0.698810, current_train_items 63136.
I0302 18:59:14.168042 22732373614720 run.py:483] Algo bellman_ford step 1973 current loss 0.883156, current_train_items 63168.
I0302 18:59:14.203358 22732373614720 run.py:483] Algo bellman_ford step 1974 current loss 1.154018, current_train_items 63200.
I0302 18:59:14.222498 22732373614720 run.py:483] Algo bellman_ford step 1975 current loss 0.428235, current_train_items 63232.
I0302 18:59:14.238285 22732373614720 run.py:483] Algo bellman_ford step 1976 current loss 0.514379, current_train_items 63264.
I0302 18:59:14.260227 22732373614720 run.py:483] Algo bellman_ford step 1977 current loss 0.648373, current_train_items 63296.
I0302 18:59:14.288650 22732373614720 run.py:483] Algo bellman_ford step 1978 current loss 0.883113, current_train_items 63328.
I0302 18:59:14.321322 22732373614720 run.py:483] Algo bellman_ford step 1979 current loss 1.083003, current_train_items 63360.
I0302 18:59:14.340181 22732373614720 run.py:483] Algo bellman_ford step 1980 current loss 0.437583, current_train_items 63392.
I0302 18:59:14.356361 22732373614720 run.py:483] Algo bellman_ford step 1981 current loss 0.600405, current_train_items 63424.
I0302 18:59:14.379266 22732373614720 run.py:483] Algo bellman_ford step 1982 current loss 0.781299, current_train_items 63456.
I0302 18:59:14.407261 22732373614720 run.py:483] Algo bellman_ford step 1983 current loss 0.780552, current_train_items 63488.
I0302 18:59:14.441808 22732373614720 run.py:483] Algo bellman_ford step 1984 current loss 1.095282, current_train_items 63520.
I0302 18:59:14.460995 22732373614720 run.py:483] Algo bellman_ford step 1985 current loss 0.340551, current_train_items 63552.
I0302 18:59:14.477024 22732373614720 run.py:483] Algo bellman_ford step 1986 current loss 0.613672, current_train_items 63584.
I0302 18:59:14.498743 22732373614720 run.py:483] Algo bellman_ford step 1987 current loss 0.827319, current_train_items 63616.
I0302 18:59:14.526647 22732373614720 run.py:483] Algo bellman_ford step 1988 current loss 0.842481, current_train_items 63648.
I0302 18:59:14.559772 22732373614720 run.py:483] Algo bellman_ford step 1989 current loss 1.262514, current_train_items 63680.
I0302 18:59:14.578897 22732373614720 run.py:483] Algo bellman_ford step 1990 current loss 0.413830, current_train_items 63712.
I0302 18:59:14.594819 22732373614720 run.py:483] Algo bellman_ford step 1991 current loss 0.673931, current_train_items 63744.
I0302 18:59:14.617389 22732373614720 run.py:483] Algo bellman_ford step 1992 current loss 0.824871, current_train_items 63776.
I0302 18:59:14.646431 22732373614720 run.py:483] Algo bellman_ford step 1993 current loss 0.888607, current_train_items 63808.
I0302 18:59:14.674507 22732373614720 run.py:483] Algo bellman_ford step 1994 current loss 0.769938, current_train_items 63840.
I0302 18:59:14.693752 22732373614720 run.py:483] Algo bellman_ford step 1995 current loss 0.417961, current_train_items 63872.
I0302 18:59:14.710248 22732373614720 run.py:483] Algo bellman_ford step 1996 current loss 0.696800, current_train_items 63904.
I0302 18:59:14.732767 22732373614720 run.py:483] Algo bellman_ford step 1997 current loss 0.767600, current_train_items 63936.
I0302 18:59:14.762897 22732373614720 run.py:483] Algo bellman_ford step 1998 current loss 0.963629, current_train_items 63968.
I0302 18:59:14.793887 22732373614720 run.py:483] Algo bellman_ford step 1999 current loss 0.929352, current_train_items 64000.
I0302 18:59:14.813148 22732373614720 run.py:483] Algo bellman_ford step 2000 current loss 0.431675, current_train_items 64032.
I0302 18:59:14.820920 22732373614720 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.8798828125, 'score': 0.8798828125, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0302 18:59:14.821027 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.910, current avg val score is 0.880, val scores are: bellman_ford: 0.880
I0302 18:59:14.837434 22732373614720 run.py:483] Algo bellman_ford step 2001 current loss 0.526699, current_train_items 64064.
I0302 18:59:14.861255 22732373614720 run.py:483] Algo bellman_ford step 2002 current loss 0.877116, current_train_items 64096.
I0302 18:59:14.890673 22732373614720 run.py:483] Algo bellman_ford step 2003 current loss 0.829801, current_train_items 64128.
I0302 18:59:14.922545 22732373614720 run.py:483] Algo bellman_ford step 2004 current loss 0.965339, current_train_items 64160.
I0302 18:59:14.942010 22732373614720 run.py:483] Algo bellman_ford step 2005 current loss 0.422318, current_train_items 64192.
I0302 18:59:14.957782 22732373614720 run.py:483] Algo bellman_ford step 2006 current loss 0.518860, current_train_items 64224.
I0302 18:59:14.980578 22732373614720 run.py:483] Algo bellman_ford step 2007 current loss 0.840492, current_train_items 64256.
I0302 18:59:15.010411 22732373614720 run.py:483] Algo bellman_ford step 2008 current loss 0.961071, current_train_items 64288.
I0302 18:59:15.041652 22732373614720 run.py:483] Algo bellman_ford step 2009 current loss 0.983322, current_train_items 64320.
I0302 18:59:15.060666 22732373614720 run.py:483] Algo bellman_ford step 2010 current loss 0.474716, current_train_items 64352.
I0302 18:59:15.076788 22732373614720 run.py:483] Algo bellman_ford step 2011 current loss 0.610369, current_train_items 64384.
I0302 18:59:15.099959 22732373614720 run.py:483] Algo bellman_ford step 2012 current loss 0.811085, current_train_items 64416.
I0302 18:59:15.128325 22732373614720 run.py:483] Algo bellman_ford step 2013 current loss 0.931769, current_train_items 64448.
I0302 18:59:15.159950 22732373614720 run.py:483] Algo bellman_ford step 2014 current loss 1.049926, current_train_items 64480.
I0302 18:59:15.178660 22732373614720 run.py:483] Algo bellman_ford step 2015 current loss 0.468111, current_train_items 64512.
I0302 18:59:15.195064 22732373614720 run.py:483] Algo bellman_ford step 2016 current loss 0.660227, current_train_items 64544.
I0302 18:59:15.218331 22732373614720 run.py:483] Algo bellman_ford step 2017 current loss 1.273261, current_train_items 64576.
I0302 18:59:15.245454 22732373614720 run.py:483] Algo bellman_ford step 2018 current loss 0.945423, current_train_items 64608.
I0302 18:59:15.278330 22732373614720 run.py:483] Algo bellman_ford step 2019 current loss 1.118422, current_train_items 64640.
I0302 18:59:15.297085 22732373614720 run.py:483] Algo bellman_ford step 2020 current loss 0.417744, current_train_items 64672.
I0302 18:59:15.313017 22732373614720 run.py:483] Algo bellman_ford step 2021 current loss 0.581641, current_train_items 64704.
I0302 18:59:15.336354 22732373614720 run.py:483] Algo bellman_ford step 2022 current loss 0.888094, current_train_items 64736.
I0302 18:59:15.365165 22732373614720 run.py:483] Algo bellman_ford step 2023 current loss 0.918579, current_train_items 64768.
I0302 18:59:15.400147 22732373614720 run.py:483] Algo bellman_ford step 2024 current loss 1.201863, current_train_items 64800.
I0302 18:59:15.419450 22732373614720 run.py:483] Algo bellman_ford step 2025 current loss 0.492538, current_train_items 64832.
I0302 18:59:15.435194 22732373614720 run.py:483] Algo bellman_ford step 2026 current loss 0.514939, current_train_items 64864.
I0302 18:59:15.458443 22732373614720 run.py:483] Algo bellman_ford step 2027 current loss 0.891953, current_train_items 64896.
I0302 18:59:15.487302 22732373614720 run.py:483] Algo bellman_ford step 2028 current loss 0.932838, current_train_items 64928.
I0302 18:59:15.518474 22732373614720 run.py:483] Algo bellman_ford step 2029 current loss 0.958015, current_train_items 64960.
I0302 18:59:15.537638 22732373614720 run.py:483] Algo bellman_ford step 2030 current loss 0.435988, current_train_items 64992.
I0302 18:59:15.553963 22732373614720 run.py:483] Algo bellman_ford step 2031 current loss 0.671259, current_train_items 65024.
I0302 18:59:15.576254 22732373614720 run.py:483] Algo bellman_ford step 2032 current loss 0.693026, current_train_items 65056.
I0302 18:59:15.605824 22732373614720 run.py:483] Algo bellman_ford step 2033 current loss 0.913963, current_train_items 65088.
I0302 18:59:15.637199 22732373614720 run.py:483] Algo bellman_ford step 2034 current loss 1.052516, current_train_items 65120.
I0302 18:59:15.656363 22732373614720 run.py:483] Algo bellman_ford step 2035 current loss 0.440211, current_train_items 65152.
I0302 18:59:15.672329 22732373614720 run.py:483] Algo bellman_ford step 2036 current loss 0.610783, current_train_items 65184.
I0302 18:59:15.694437 22732373614720 run.py:483] Algo bellman_ford step 2037 current loss 0.730284, current_train_items 65216.
I0302 18:59:15.723302 22732373614720 run.py:483] Algo bellman_ford step 2038 current loss 0.858521, current_train_items 65248.
I0302 18:59:15.755427 22732373614720 run.py:483] Algo bellman_ford step 2039 current loss 0.890026, current_train_items 65280.
I0302 18:59:15.774823 22732373614720 run.py:483] Algo bellman_ford step 2040 current loss 0.671145, current_train_items 65312.
I0302 18:59:15.790586 22732373614720 run.py:483] Algo bellman_ford step 2041 current loss 0.698801, current_train_items 65344.
I0302 18:59:15.814116 22732373614720 run.py:483] Algo bellman_ford step 2042 current loss 0.947372, current_train_items 65376.
I0302 18:59:15.843518 22732373614720 run.py:483] Algo bellman_ford step 2043 current loss 0.883491, current_train_items 65408.
I0302 18:59:15.870521 22732373614720 run.py:483] Algo bellman_ford step 2044 current loss 0.873242, current_train_items 65440.
I0302 18:59:15.889469 22732373614720 run.py:483] Algo bellman_ford step 2045 current loss 0.380955, current_train_items 65472.
I0302 18:59:15.906065 22732373614720 run.py:483] Algo bellman_ford step 2046 current loss 0.672080, current_train_items 65504.
I0302 18:59:15.928874 22732373614720 run.py:483] Algo bellman_ford step 2047 current loss 0.982486, current_train_items 65536.
I0302 18:59:15.957638 22732373614720 run.py:483] Algo bellman_ford step 2048 current loss 0.851467, current_train_items 65568.
I0302 18:59:15.987664 22732373614720 run.py:483] Algo bellman_ford step 2049 current loss 1.073711, current_train_items 65600.
I0302 18:59:16.006693 22732373614720 run.py:483] Algo bellman_ford step 2050 current loss 0.399250, current_train_items 65632.
I0302 18:59:16.014765 22732373614720 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0302 18:59:16.014896 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.910, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 18:59:16.045858 22732373614720 run.py:483] Algo bellman_ford step 2051 current loss 0.610893, current_train_items 65664.
I0302 18:59:16.068708 22732373614720 run.py:483] Algo bellman_ford step 2052 current loss 0.754020, current_train_items 65696.
I0302 18:59:16.097670 22732373614720 run.py:483] Algo bellman_ford step 2053 current loss 0.895646, current_train_items 65728.
I0302 18:59:16.133114 22732373614720 run.py:483] Algo bellman_ford step 2054 current loss 1.150615, current_train_items 65760.
I0302 18:59:16.152800 22732373614720 run.py:483] Algo bellman_ford step 2055 current loss 0.483781, current_train_items 65792.
I0302 18:59:16.168583 22732373614720 run.py:483] Algo bellman_ford step 2056 current loss 0.567410, current_train_items 65824.
I0302 18:59:16.191288 22732373614720 run.py:483] Algo bellman_ford step 2057 current loss 0.802064, current_train_items 65856.
I0302 18:59:16.220015 22732373614720 run.py:483] Algo bellman_ford step 2058 current loss 0.763693, current_train_items 65888.
I0302 18:59:16.253057 22732373614720 run.py:483] Algo bellman_ford step 2059 current loss 1.083830, current_train_items 65920.
I0302 18:59:16.272598 22732373614720 run.py:483] Algo bellman_ford step 2060 current loss 0.486211, current_train_items 65952.
I0302 18:59:16.288853 22732373614720 run.py:483] Algo bellman_ford step 2061 current loss 0.551881, current_train_items 65984.
I0302 18:59:16.311601 22732373614720 run.py:483] Algo bellman_ford step 2062 current loss 0.810569, current_train_items 66016.
I0302 18:59:16.340877 22732373614720 run.py:483] Algo bellman_ford step 2063 current loss 0.830274, current_train_items 66048.
I0302 18:59:16.374422 22732373614720 run.py:483] Algo bellman_ford step 2064 current loss 1.038320, current_train_items 66080.
I0302 18:59:16.393317 22732373614720 run.py:483] Algo bellman_ford step 2065 current loss 0.344963, current_train_items 66112.
I0302 18:59:16.409390 22732373614720 run.py:483] Algo bellman_ford step 2066 current loss 0.563523, current_train_items 66144.
I0302 18:59:16.432480 22732373614720 run.py:483] Algo bellman_ford step 2067 current loss 0.776730, current_train_items 66176.
I0302 18:59:16.462051 22732373614720 run.py:483] Algo bellman_ford step 2068 current loss 0.841597, current_train_items 66208.
I0302 18:59:16.493649 22732373614720 run.py:483] Algo bellman_ford step 2069 current loss 0.961510, current_train_items 66240.
I0302 18:59:16.512883 22732373614720 run.py:483] Algo bellman_ford step 2070 current loss 0.388091, current_train_items 66272.
I0302 18:59:16.529199 22732373614720 run.py:483] Algo bellman_ford step 2071 current loss 0.606919, current_train_items 66304.
I0302 18:59:16.551996 22732373614720 run.py:483] Algo bellman_ford step 2072 current loss 0.742501, current_train_items 66336.
I0302 18:59:16.581356 22732373614720 run.py:483] Algo bellman_ford step 2073 current loss 1.026861, current_train_items 66368.
I0302 18:59:16.613328 22732373614720 run.py:483] Algo bellman_ford step 2074 current loss 0.970334, current_train_items 66400.
I0302 18:59:16.632647 22732373614720 run.py:483] Algo bellman_ford step 2075 current loss 0.461365, current_train_items 66432.
I0302 18:59:16.649075 22732373614720 run.py:483] Algo bellman_ford step 2076 current loss 0.677656, current_train_items 66464.
I0302 18:59:16.672308 22732373614720 run.py:483] Algo bellman_ford step 2077 current loss 0.744846, current_train_items 66496.
I0302 18:59:16.699117 22732373614720 run.py:483] Algo bellman_ford step 2078 current loss 0.793090, current_train_items 66528.
I0302 18:59:16.730626 22732373614720 run.py:483] Algo bellman_ford step 2079 current loss 1.022916, current_train_items 66560.
I0302 18:59:16.749715 22732373614720 run.py:483] Algo bellman_ford step 2080 current loss 0.336415, current_train_items 66592.
I0302 18:59:16.765812 22732373614720 run.py:483] Algo bellman_ford step 2081 current loss 0.582383, current_train_items 66624.
I0302 18:59:16.788754 22732373614720 run.py:483] Algo bellman_ford step 2082 current loss 0.769018, current_train_items 66656.
I0302 18:59:16.819383 22732373614720 run.py:483] Algo bellman_ford step 2083 current loss 1.006600, current_train_items 66688.
I0302 18:59:16.851638 22732373614720 run.py:483] Algo bellman_ford step 2084 current loss 1.105396, current_train_items 66720.
I0302 18:59:16.871024 22732373614720 run.py:483] Algo bellman_ford step 2085 current loss 0.402309, current_train_items 66752.
I0302 18:59:16.887084 22732373614720 run.py:483] Algo bellman_ford step 2086 current loss 0.673709, current_train_items 66784.
I0302 18:59:16.909814 22732373614720 run.py:483] Algo bellman_ford step 2087 current loss 0.981318, current_train_items 66816.
I0302 18:59:16.936550 22732373614720 run.py:483] Algo bellman_ford step 2088 current loss 0.725323, current_train_items 66848.
I0302 18:59:16.968607 22732373614720 run.py:483] Algo bellman_ford step 2089 current loss 0.988206, current_train_items 66880.
I0302 18:59:16.987977 22732373614720 run.py:483] Algo bellman_ford step 2090 current loss 0.365096, current_train_items 66912.
I0302 18:59:17.004050 22732373614720 run.py:483] Algo bellman_ford step 2091 current loss 0.639605, current_train_items 66944.
I0302 18:59:17.026357 22732373614720 run.py:483] Algo bellman_ford step 2092 current loss 1.017708, current_train_items 66976.
I0302 18:59:17.055117 22732373614720 run.py:483] Algo bellman_ford step 2093 current loss 1.148361, current_train_items 67008.
I0302 18:59:17.087036 22732373614720 run.py:483] Algo bellman_ford step 2094 current loss 1.336733, current_train_items 67040.
I0302 18:59:17.106329 22732373614720 run.py:483] Algo bellman_ford step 2095 current loss 0.357293, current_train_items 67072.
I0302 18:59:17.122401 22732373614720 run.py:483] Algo bellman_ford step 2096 current loss 0.497219, current_train_items 67104.
I0302 18:59:17.145267 22732373614720 run.py:483] Algo bellman_ford step 2097 current loss 0.775867, current_train_items 67136.
I0302 18:59:17.173385 22732373614720 run.py:483] Algo bellman_ford step 2098 current loss 0.804467, current_train_items 67168.
I0302 18:59:17.204781 22732373614720 run.py:483] Algo bellman_ford step 2099 current loss 1.157904, current_train_items 67200.
I0302 18:59:17.224195 22732373614720 run.py:483] Algo bellman_ford step 2100 current loss 0.399538, current_train_items 67232.
I0302 18:59:17.231938 22732373614720 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0302 18:59:17.232046 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 18:59:17.248429 22732373614720 run.py:483] Algo bellman_ford step 2101 current loss 0.556829, current_train_items 67264.
I0302 18:59:17.271575 22732373614720 run.py:483] Algo bellman_ford step 2102 current loss 0.817753, current_train_items 67296.
I0302 18:59:17.301043 22732373614720 run.py:483] Algo bellman_ford step 2103 current loss 0.930236, current_train_items 67328.
I0302 18:59:17.332592 22732373614720 run.py:483] Algo bellman_ford step 2104 current loss 1.040246, current_train_items 67360.
I0302 18:59:17.352001 22732373614720 run.py:483] Algo bellman_ford step 2105 current loss 0.408246, current_train_items 67392.
I0302 18:59:17.366899 22732373614720 run.py:483] Algo bellman_ford step 2106 current loss 0.482115, current_train_items 67424.
I0302 18:59:17.390413 22732373614720 run.py:483] Algo bellman_ford step 2107 current loss 0.945623, current_train_items 67456.
I0302 18:59:17.420594 22732373614720 run.py:483] Algo bellman_ford step 2108 current loss 1.094507, current_train_items 67488.
I0302 18:59:17.451029 22732373614720 run.py:483] Algo bellman_ford step 2109 current loss 0.912411, current_train_items 67520.
I0302 18:59:17.470152 22732373614720 run.py:483] Algo bellman_ford step 2110 current loss 0.439681, current_train_items 67552.
I0302 18:59:17.486152 22732373614720 run.py:483] Algo bellman_ford step 2111 current loss 0.629327, current_train_items 67584.
I0302 18:59:17.508570 22732373614720 run.py:483] Algo bellman_ford step 2112 current loss 0.850196, current_train_items 67616.
I0302 18:59:17.536623 22732373614720 run.py:483] Algo bellman_ford step 2113 current loss 0.923824, current_train_items 67648.
I0302 18:59:17.567299 22732373614720 run.py:483] Algo bellman_ford step 2114 current loss 1.053507, current_train_items 67680.
I0302 18:59:17.586245 22732373614720 run.py:483] Algo bellman_ford step 2115 current loss 0.453128, current_train_items 67712.
I0302 18:59:17.602125 22732373614720 run.py:483] Algo bellman_ford step 2116 current loss 0.570377, current_train_items 67744.
I0302 18:59:17.625278 22732373614720 run.py:483] Algo bellman_ford step 2117 current loss 0.864043, current_train_items 67776.
I0302 18:59:17.654982 22732373614720 run.py:483] Algo bellman_ford step 2118 current loss 0.926688, current_train_items 67808.
I0302 18:59:17.685994 22732373614720 run.py:483] Algo bellman_ford step 2119 current loss 1.010687, current_train_items 67840.
I0302 18:59:17.705209 22732373614720 run.py:483] Algo bellman_ford step 2120 current loss 0.422193, current_train_items 67872.
I0302 18:59:17.720888 22732373614720 run.py:483] Algo bellman_ford step 2121 current loss 0.580639, current_train_items 67904.
I0302 18:59:17.743612 22732373614720 run.py:483] Algo bellman_ford step 2122 current loss 0.676753, current_train_items 67936.
I0302 18:59:17.773094 22732373614720 run.py:483] Algo bellman_ford step 2123 current loss 0.838986, current_train_items 67968.
I0302 18:59:17.801589 22732373614720 run.py:483] Algo bellman_ford step 2124 current loss 0.878663, current_train_items 68000.
I0302 18:59:17.820458 22732373614720 run.py:483] Algo bellman_ford step 2125 current loss 0.300609, current_train_items 68032.
I0302 18:59:17.836696 22732373614720 run.py:483] Algo bellman_ford step 2126 current loss 0.668219, current_train_items 68064.
I0302 18:59:17.861125 22732373614720 run.py:483] Algo bellman_ford step 2127 current loss 0.975135, current_train_items 68096.
I0302 18:59:17.889539 22732373614720 run.py:483] Algo bellman_ford step 2128 current loss 0.909090, current_train_items 68128.
I0302 18:59:17.920553 22732373614720 run.py:483] Algo bellman_ford step 2129 current loss 0.855697, current_train_items 68160.
I0302 18:59:17.939730 22732373614720 run.py:483] Algo bellman_ford step 2130 current loss 0.386931, current_train_items 68192.
I0302 18:59:17.956186 22732373614720 run.py:483] Algo bellman_ford step 2131 current loss 0.609492, current_train_items 68224.
I0302 18:59:17.979503 22732373614720 run.py:483] Algo bellman_ford step 2132 current loss 0.776196, current_train_items 68256.
I0302 18:59:18.008001 22732373614720 run.py:483] Algo bellman_ford step 2133 current loss 0.913670, current_train_items 68288.
I0302 18:59:18.040950 22732373614720 run.py:483] Algo bellman_ford step 2134 current loss 0.989363, current_train_items 68320.
I0302 18:59:18.059983 22732373614720 run.py:483] Algo bellman_ford step 2135 current loss 0.515571, current_train_items 68352.
I0302 18:59:18.076067 22732373614720 run.py:483] Algo bellman_ford step 2136 current loss 0.508358, current_train_items 68384.
I0302 18:59:18.098287 22732373614720 run.py:483] Algo bellman_ford step 2137 current loss 0.730154, current_train_items 68416.
I0302 18:59:18.127554 22732373614720 run.py:483] Algo bellman_ford step 2138 current loss 0.884919, current_train_items 68448.
I0302 18:59:18.162351 22732373614720 run.py:483] Algo bellman_ford step 2139 current loss 1.061102, current_train_items 68480.
I0302 18:59:18.181222 22732373614720 run.py:483] Algo bellman_ford step 2140 current loss 0.473227, current_train_items 68512.
I0302 18:59:18.196984 22732373614720 run.py:483] Algo bellman_ford step 2141 current loss 0.494189, current_train_items 68544.
I0302 18:59:18.218232 22732373614720 run.py:483] Algo bellman_ford step 2142 current loss 0.596954, current_train_items 68576.
I0302 18:59:18.246590 22732373614720 run.py:483] Algo bellman_ford step 2143 current loss 0.736259, current_train_items 68608.
I0302 18:59:18.278964 22732373614720 run.py:483] Algo bellman_ford step 2144 current loss 1.025133, current_train_items 68640.
I0302 18:59:18.297693 22732373614720 run.py:483] Algo bellman_ford step 2145 current loss 0.309768, current_train_items 68672.
I0302 18:59:18.314103 22732373614720 run.py:483] Algo bellman_ford step 2146 current loss 0.596191, current_train_items 68704.
I0302 18:59:18.337406 22732373614720 run.py:483] Algo bellman_ford step 2147 current loss 0.930281, current_train_items 68736.
I0302 18:59:18.366698 22732373614720 run.py:483] Algo bellman_ford step 2148 current loss 0.877769, current_train_items 68768.
I0302 18:59:18.397452 22732373614720 run.py:483] Algo bellman_ford step 2149 current loss 0.946794, current_train_items 68800.
I0302 18:59:18.416574 22732373614720 run.py:483] Algo bellman_ford step 2150 current loss 0.490137, current_train_items 68832.
I0302 18:59:18.424529 22732373614720 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0302 18:59:18.424633 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 18:59:18.441383 22732373614720 run.py:483] Algo bellman_ford step 2151 current loss 0.531325, current_train_items 68864.
I0302 18:59:18.464473 22732373614720 run.py:483] Algo bellman_ford step 2152 current loss 0.707121, current_train_items 68896.
I0302 18:59:18.493723 22732373614720 run.py:483] Algo bellman_ford step 2153 current loss 0.864708, current_train_items 68928.
I0302 18:59:18.525476 22732373614720 run.py:483] Algo bellman_ford step 2154 current loss 1.038850, current_train_items 68960.
I0302 18:59:18.544929 22732373614720 run.py:483] Algo bellman_ford step 2155 current loss 0.415302, current_train_items 68992.
I0302 18:59:18.560562 22732373614720 run.py:483] Algo bellman_ford step 2156 current loss 0.512778, current_train_items 69024.
I0302 18:59:18.583519 22732373614720 run.py:483] Algo bellman_ford step 2157 current loss 0.739754, current_train_items 69056.
I0302 18:59:18.612961 22732373614720 run.py:483] Algo bellman_ford step 2158 current loss 0.892092, current_train_items 69088.
I0302 18:59:18.645190 22732373614720 run.py:483] Algo bellman_ford step 2159 current loss 1.069039, current_train_items 69120.
I0302 18:59:18.665028 22732373614720 run.py:483] Algo bellman_ford step 2160 current loss 0.478430, current_train_items 69152.
I0302 18:59:18.680880 22732373614720 run.py:483] Algo bellman_ford step 2161 current loss 0.566589, current_train_items 69184.
I0302 18:59:18.703852 22732373614720 run.py:483] Algo bellman_ford step 2162 current loss 0.778985, current_train_items 69216.
I0302 18:59:18.731582 22732373614720 run.py:483] Algo bellman_ford step 2163 current loss 0.800834, current_train_items 69248.
I0302 18:59:18.764634 22732373614720 run.py:483] Algo bellman_ford step 2164 current loss 1.113674, current_train_items 69280.
I0302 18:59:18.783592 22732373614720 run.py:483] Algo bellman_ford step 2165 current loss 0.428453, current_train_items 69312.
I0302 18:59:18.800012 22732373614720 run.py:483] Algo bellman_ford step 2166 current loss 0.628194, current_train_items 69344.
I0302 18:59:18.822321 22732373614720 run.py:483] Algo bellman_ford step 2167 current loss 0.692474, current_train_items 69376.
I0302 18:59:18.850133 22732373614720 run.py:483] Algo bellman_ford step 2168 current loss 0.857514, current_train_items 69408.
I0302 18:59:18.882234 22732373614720 run.py:483] Algo bellman_ford step 2169 current loss 1.164685, current_train_items 69440.
I0302 18:59:18.901380 22732373614720 run.py:483] Algo bellman_ford step 2170 current loss 0.362642, current_train_items 69472.
I0302 18:59:18.917284 22732373614720 run.py:483] Algo bellman_ford step 2171 current loss 0.602966, current_train_items 69504.
I0302 18:59:18.939719 22732373614720 run.py:483] Algo bellman_ford step 2172 current loss 0.814665, current_train_items 69536.
I0302 18:59:18.969228 22732373614720 run.py:483] Algo bellman_ford step 2173 current loss 0.964488, current_train_items 69568.
I0302 18:59:18.999667 22732373614720 run.py:483] Algo bellman_ford step 2174 current loss 1.063691, current_train_items 69600.
I0302 18:59:19.019066 22732373614720 run.py:483] Algo bellman_ford step 2175 current loss 0.386485, current_train_items 69632.
I0302 18:59:19.034640 22732373614720 run.py:483] Algo bellman_ford step 2176 current loss 0.569378, current_train_items 69664.
I0302 18:59:19.057995 22732373614720 run.py:483] Algo bellman_ford step 2177 current loss 0.938911, current_train_items 69696.
I0302 18:59:19.086020 22732373614720 run.py:483] Algo bellman_ford step 2178 current loss 0.869128, current_train_items 69728.
I0302 18:59:19.118331 22732373614720 run.py:483] Algo bellman_ford step 2179 current loss 0.979021, current_train_items 69760.
I0302 18:59:19.136973 22732373614720 run.py:483] Algo bellman_ford step 2180 current loss 0.292863, current_train_items 69792.
I0302 18:59:19.152912 22732373614720 run.py:483] Algo bellman_ford step 2181 current loss 0.507653, current_train_items 69824.
I0302 18:59:19.175684 22732373614720 run.py:483] Algo bellman_ford step 2182 current loss 0.817436, current_train_items 69856.
I0302 18:59:19.204092 22732373614720 run.py:483] Algo bellman_ford step 2183 current loss 0.872847, current_train_items 69888.
I0302 18:59:19.237215 22732373614720 run.py:483] Algo bellman_ford step 2184 current loss 1.176511, current_train_items 69920.
I0302 18:59:19.256720 22732373614720 run.py:483] Algo bellman_ford step 2185 current loss 0.478962, current_train_items 69952.
I0302 18:59:19.272182 22732373614720 run.py:483] Algo bellman_ford step 2186 current loss 0.464837, current_train_items 69984.
I0302 18:59:19.294746 22732373614720 run.py:483] Algo bellman_ford step 2187 current loss 0.722143, current_train_items 70016.
I0302 18:59:19.323835 22732373614720 run.py:483] Algo bellman_ford step 2188 current loss 0.810676, current_train_items 70048.
W0302 18:59:19.348254 22732373614720 samplers.py:155] Increasing hint lengh from 12 to 13
I0302 18:59:25.900569 22732373614720 run.py:483] Algo bellman_ford step 2189 current loss 1.333742, current_train_items 70080.
I0302 18:59:25.921255 22732373614720 run.py:483] Algo bellman_ford step 2190 current loss 0.361617, current_train_items 70112.
I0302 18:59:25.937866 22732373614720 run.py:483] Algo bellman_ford step 2191 current loss 0.546147, current_train_items 70144.
I0302 18:59:25.960954 22732373614720 run.py:483] Algo bellman_ford step 2192 current loss 0.853818, current_train_items 70176.
W0302 18:59:25.981801 22732373614720 samplers.py:155] Increasing hint lengh from 10 to 12
I0302 18:59:32.959285 22732373614720 run.py:483] Algo bellman_ford step 2193 current loss 1.133464, current_train_items 70208.
I0302 18:59:32.991074 22732373614720 run.py:483] Algo bellman_ford step 2194 current loss 0.980747, current_train_items 70240.
I0302 18:59:33.011547 22732373614720 run.py:483] Algo bellman_ford step 2195 current loss 0.413976, current_train_items 70272.
I0302 18:59:33.027955 22732373614720 run.py:483] Algo bellman_ford step 2196 current loss 0.612608, current_train_items 70304.
I0302 18:59:33.050827 22732373614720 run.py:483] Algo bellman_ford step 2197 current loss 0.736833, current_train_items 70336.
I0302 18:59:33.079246 22732373614720 run.py:483] Algo bellman_ford step 2198 current loss 0.898920, current_train_items 70368.
I0302 18:59:33.111639 22732373614720 run.py:483] Algo bellman_ford step 2199 current loss 1.077443, current_train_items 70400.
I0302 18:59:33.131253 22732373614720 run.py:483] Algo bellman_ford step 2200 current loss 0.389047, current_train_items 70432.
I0302 18:59:33.140760 22732373614720 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.87890625, 'score': 0.87890625, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0302 18:59:33.140899 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.879, val scores are: bellman_ford: 0.879
I0302 18:59:33.157934 22732373614720 run.py:483] Algo bellman_ford step 2201 current loss 0.626606, current_train_items 70464.
I0302 18:59:33.180847 22732373614720 run.py:483] Algo bellman_ford step 2202 current loss 0.731473, current_train_items 70496.
I0302 18:59:33.210523 22732373614720 run.py:483] Algo bellman_ford step 2203 current loss 0.811414, current_train_items 70528.
I0302 18:59:33.243781 22732373614720 run.py:483] Algo bellman_ford step 2204 current loss 0.962988, current_train_items 70560.
I0302 18:59:33.263958 22732373614720 run.py:483] Algo bellman_ford step 2205 current loss 0.303586, current_train_items 70592.
I0302 18:59:33.279872 22732373614720 run.py:483] Algo bellman_ford step 2206 current loss 0.570534, current_train_items 70624.
I0302 18:59:33.302315 22732373614720 run.py:483] Algo bellman_ford step 2207 current loss 0.850184, current_train_items 70656.
I0302 18:59:33.332101 22732373614720 run.py:483] Algo bellman_ford step 2208 current loss 0.985090, current_train_items 70688.
I0302 18:59:33.365750 22732373614720 run.py:483] Algo bellman_ford step 2209 current loss 1.018400, current_train_items 70720.
I0302 18:59:33.385420 22732373614720 run.py:483] Algo bellman_ford step 2210 current loss 0.438025, current_train_items 70752.
I0302 18:59:33.401466 22732373614720 run.py:483] Algo bellman_ford step 2211 current loss 0.604325, current_train_items 70784.
I0302 18:59:33.425002 22732373614720 run.py:483] Algo bellman_ford step 2212 current loss 0.898713, current_train_items 70816.
I0302 18:59:33.454948 22732373614720 run.py:483] Algo bellman_ford step 2213 current loss 0.938035, current_train_items 70848.
I0302 18:59:33.488769 22732373614720 run.py:483] Algo bellman_ford step 2214 current loss 1.048529, current_train_items 70880.
I0302 18:59:33.508631 22732373614720 run.py:483] Algo bellman_ford step 2215 current loss 0.383101, current_train_items 70912.
I0302 18:59:33.524572 22732373614720 run.py:483] Algo bellman_ford step 2216 current loss 0.519463, current_train_items 70944.
I0302 18:59:33.546500 22732373614720 run.py:483] Algo bellman_ford step 2217 current loss 0.803001, current_train_items 70976.
I0302 18:59:33.577238 22732373614720 run.py:483] Algo bellman_ford step 2218 current loss 0.914571, current_train_items 71008.
I0302 18:59:33.608742 22732373614720 run.py:483] Algo bellman_ford step 2219 current loss 0.977905, current_train_items 71040.
I0302 18:59:33.628462 22732373614720 run.py:483] Algo bellman_ford step 2220 current loss 0.344849, current_train_items 71072.
I0302 18:59:33.644261 22732373614720 run.py:483] Algo bellman_ford step 2221 current loss 0.565320, current_train_items 71104.
I0302 18:59:33.666604 22732373614720 run.py:483] Algo bellman_ford step 2222 current loss 0.834213, current_train_items 71136.
I0302 18:59:33.696415 22732373614720 run.py:483] Algo bellman_ford step 2223 current loss 1.005695, current_train_items 71168.
I0302 18:59:33.729820 22732373614720 run.py:483] Algo bellman_ford step 2224 current loss 1.217380, current_train_items 71200.
I0302 18:59:33.749075 22732373614720 run.py:483] Algo bellman_ford step 2225 current loss 0.311330, current_train_items 71232.
I0302 18:59:33.765187 22732373614720 run.py:483] Algo bellman_ford step 2226 current loss 0.588875, current_train_items 71264.
I0302 18:59:33.788357 22732373614720 run.py:483] Algo bellman_ford step 2227 current loss 0.816943, current_train_items 71296.
I0302 18:59:33.818372 22732373614720 run.py:483] Algo bellman_ford step 2228 current loss 1.018078, current_train_items 71328.
I0302 18:59:33.848853 22732373614720 run.py:483] Algo bellman_ford step 2229 current loss 0.988211, current_train_items 71360.
I0302 18:59:33.868505 22732373614720 run.py:483] Algo bellman_ford step 2230 current loss 0.408536, current_train_items 71392.
I0302 18:59:33.884737 22732373614720 run.py:483] Algo bellman_ford step 2231 current loss 0.583461, current_train_items 71424.
I0302 18:59:33.907228 22732373614720 run.py:483] Algo bellman_ford step 2232 current loss 0.860146, current_train_items 71456.
I0302 18:59:33.936150 22732373614720 run.py:483] Algo bellman_ford step 2233 current loss 0.899473, current_train_items 71488.
I0302 18:59:33.967650 22732373614720 run.py:483] Algo bellman_ford step 2234 current loss 1.129970, current_train_items 71520.
I0302 18:59:33.987299 22732373614720 run.py:483] Algo bellman_ford step 2235 current loss 0.368513, current_train_items 71552.
I0302 18:59:34.003466 22732373614720 run.py:483] Algo bellman_ford step 2236 current loss 0.565491, current_train_items 71584.
I0302 18:59:34.027096 22732373614720 run.py:483] Algo bellman_ford step 2237 current loss 0.845042, current_train_items 71616.
I0302 18:59:34.056716 22732373614720 run.py:483] Algo bellman_ford step 2238 current loss 0.983616, current_train_items 71648.
I0302 18:59:34.088078 22732373614720 run.py:483] Algo bellman_ford step 2239 current loss 0.882489, current_train_items 71680.
I0302 18:59:34.107290 22732373614720 run.py:483] Algo bellman_ford step 2240 current loss 0.474695, current_train_items 71712.
I0302 18:59:34.123610 22732373614720 run.py:483] Algo bellman_ford step 2241 current loss 0.594391, current_train_items 71744.
I0302 18:59:34.147237 22732373614720 run.py:483] Algo bellman_ford step 2242 current loss 0.747543, current_train_items 71776.
I0302 18:59:34.176909 22732373614720 run.py:483] Algo bellman_ford step 2243 current loss 0.926381, current_train_items 71808.
I0302 18:59:34.207461 22732373614720 run.py:483] Algo bellman_ford step 2244 current loss 0.861796, current_train_items 71840.
I0302 18:59:34.226665 22732373614720 run.py:483] Algo bellman_ford step 2245 current loss 0.384247, current_train_items 71872.
I0302 18:59:34.242856 22732373614720 run.py:483] Algo bellman_ford step 2246 current loss 0.607208, current_train_items 71904.
I0302 18:59:34.264937 22732373614720 run.py:483] Algo bellman_ford step 2247 current loss 0.718605, current_train_items 71936.
I0302 18:59:34.295406 22732373614720 run.py:483] Algo bellman_ford step 2248 current loss 0.778446, current_train_items 71968.
I0302 18:59:34.327800 22732373614720 run.py:483] Algo bellman_ford step 2249 current loss 1.084932, current_train_items 72000.
I0302 18:59:34.347094 22732373614720 run.py:483] Algo bellman_ford step 2250 current loss 0.420512, current_train_items 72032.
I0302 18:59:34.354992 22732373614720 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.880859375, 'score': 0.880859375, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0302 18:59:34.355103 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.881, val scores are: bellman_ford: 0.881
I0302 18:59:34.371542 22732373614720 run.py:483] Algo bellman_ford step 2251 current loss 0.653361, current_train_items 72064.
I0302 18:59:34.393811 22732373614720 run.py:483] Algo bellman_ford step 2252 current loss 0.740415, current_train_items 72096.
I0302 18:59:34.424860 22732373614720 run.py:483] Algo bellman_ford step 2253 current loss 1.004856, current_train_items 72128.
I0302 18:59:34.458112 22732373614720 run.py:483] Algo bellman_ford step 2254 current loss 1.041793, current_train_items 72160.
I0302 18:59:34.477851 22732373614720 run.py:483] Algo bellman_ford step 2255 current loss 0.439124, current_train_items 72192.
I0302 18:59:34.493647 22732373614720 run.py:483] Algo bellman_ford step 2256 current loss 0.569829, current_train_items 72224.
I0302 18:59:34.516105 22732373614720 run.py:483] Algo bellman_ford step 2257 current loss 0.758535, current_train_items 72256.
I0302 18:59:34.544458 22732373614720 run.py:483] Algo bellman_ford step 2258 current loss 0.824951, current_train_items 72288.
I0302 18:59:34.577887 22732373614720 run.py:483] Algo bellman_ford step 2259 current loss 1.141716, current_train_items 72320.
I0302 18:59:34.597599 22732373614720 run.py:483] Algo bellman_ford step 2260 current loss 0.460574, current_train_items 72352.
I0302 18:59:34.614143 22732373614720 run.py:483] Algo bellman_ford step 2261 current loss 0.785891, current_train_items 72384.
I0302 18:59:34.637988 22732373614720 run.py:483] Algo bellman_ford step 2262 current loss 0.825885, current_train_items 72416.
I0302 18:59:34.666909 22732373614720 run.py:483] Algo bellman_ford step 2263 current loss 0.794100, current_train_items 72448.
I0302 18:59:34.699320 22732373614720 run.py:483] Algo bellman_ford step 2264 current loss 0.988286, current_train_items 72480.
I0302 18:59:34.718685 22732373614720 run.py:483] Algo bellman_ford step 2265 current loss 0.379301, current_train_items 72512.
I0302 18:59:34.734623 22732373614720 run.py:483] Algo bellman_ford step 2266 current loss 0.574709, current_train_items 72544.
I0302 18:59:34.757951 22732373614720 run.py:483] Algo bellman_ford step 2267 current loss 0.881991, current_train_items 72576.
I0302 18:59:34.787449 22732373614720 run.py:483] Algo bellman_ford step 2268 current loss 1.004519, current_train_items 72608.
I0302 18:59:34.819369 22732373614720 run.py:483] Algo bellman_ford step 2269 current loss 1.093803, current_train_items 72640.
I0302 18:59:34.838937 22732373614720 run.py:483] Algo bellman_ford step 2270 current loss 0.419964, current_train_items 72672.
I0302 18:59:34.854744 22732373614720 run.py:483] Algo bellman_ford step 2271 current loss 0.547440, current_train_items 72704.
I0302 18:59:34.876279 22732373614720 run.py:483] Algo bellman_ford step 2272 current loss 0.788867, current_train_items 72736.
I0302 18:59:34.906363 22732373614720 run.py:483] Algo bellman_ford step 2273 current loss 0.934085, current_train_items 72768.
I0302 18:59:34.941038 22732373614720 run.py:483] Algo bellman_ford step 2274 current loss 1.108335, current_train_items 72800.
I0302 18:59:34.960730 22732373614720 run.py:483] Algo bellman_ford step 2275 current loss 0.458276, current_train_items 72832.
I0302 18:59:34.976549 22732373614720 run.py:483] Algo bellman_ford step 2276 current loss 0.540490, current_train_items 72864.
I0302 18:59:34.997877 22732373614720 run.py:483] Algo bellman_ford step 2277 current loss 0.630053, current_train_items 72896.
I0302 18:59:35.026630 22732373614720 run.py:483] Algo bellman_ford step 2278 current loss 0.831573, current_train_items 72928.
I0302 18:59:35.060238 22732373614720 run.py:483] Algo bellman_ford step 2279 current loss 1.018655, current_train_items 72960.
I0302 18:59:35.079353 22732373614720 run.py:483] Algo bellman_ford step 2280 current loss 0.344472, current_train_items 72992.
I0302 18:59:35.094969 22732373614720 run.py:483] Algo bellman_ford step 2281 current loss 0.668839, current_train_items 73024.
I0302 18:59:35.118115 22732373614720 run.py:483] Algo bellman_ford step 2282 current loss 0.762547, current_train_items 73056.
I0302 18:59:35.149832 22732373614720 run.py:483] Algo bellman_ford step 2283 current loss 1.029062, current_train_items 73088.
I0302 18:59:35.183695 22732373614720 run.py:483] Algo bellman_ford step 2284 current loss 1.095866, current_train_items 73120.
I0302 18:59:35.203362 22732373614720 run.py:483] Algo bellman_ford step 2285 current loss 0.413242, current_train_items 73152.
I0302 18:59:35.219401 22732373614720 run.py:483] Algo bellman_ford step 2286 current loss 0.557625, current_train_items 73184.
I0302 18:59:35.241714 22732373614720 run.py:483] Algo bellman_ford step 2287 current loss 0.654388, current_train_items 73216.
I0302 18:59:35.270853 22732373614720 run.py:483] Algo bellman_ford step 2288 current loss 0.839672, current_train_items 73248.
I0302 18:59:35.305482 22732373614720 run.py:483] Algo bellman_ford step 2289 current loss 1.093440, current_train_items 73280.
I0302 18:59:35.324933 22732373614720 run.py:483] Algo bellman_ford step 2290 current loss 0.490015, current_train_items 73312.
I0302 18:59:35.340383 22732373614720 run.py:483] Algo bellman_ford step 2291 current loss 0.588569, current_train_items 73344.
I0302 18:59:35.362391 22732373614720 run.py:483] Algo bellman_ford step 2292 current loss 0.704592, current_train_items 73376.
I0302 18:59:35.392211 22732373614720 run.py:483] Algo bellman_ford step 2293 current loss 0.816491, current_train_items 73408.
I0302 18:59:35.424962 22732373614720 run.py:483] Algo bellman_ford step 2294 current loss 0.988855, current_train_items 73440.
I0302 18:59:35.444056 22732373614720 run.py:483] Algo bellman_ford step 2295 current loss 0.369694, current_train_items 73472.
I0302 18:59:35.459908 22732373614720 run.py:483] Algo bellman_ford step 2296 current loss 0.575662, current_train_items 73504.
I0302 18:59:35.481750 22732373614720 run.py:483] Algo bellman_ford step 2297 current loss 0.662236, current_train_items 73536.
I0302 18:59:35.510469 22732373614720 run.py:483] Algo bellman_ford step 2298 current loss 0.827898, current_train_items 73568.
I0302 18:59:35.543273 22732373614720 run.py:483] Algo bellman_ford step 2299 current loss 1.033650, current_train_items 73600.
I0302 18:59:35.562978 22732373614720 run.py:483] Algo bellman_ford step 2300 current loss 0.328884, current_train_items 73632.
I0302 18:59:35.570810 22732373614720 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0302 18:59:35.570914 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 18:59:35.587627 22732373614720 run.py:483] Algo bellman_ford step 2301 current loss 0.614722, current_train_items 73664.
I0302 18:59:35.610778 22732373614720 run.py:483] Algo bellman_ford step 2302 current loss 0.802751, current_train_items 73696.
I0302 18:59:35.641839 22732373614720 run.py:483] Algo bellman_ford step 2303 current loss 0.821837, current_train_items 73728.
I0302 18:59:35.675606 22732373614720 run.py:483] Algo bellman_ford step 2304 current loss 0.932178, current_train_items 73760.
I0302 18:59:35.695747 22732373614720 run.py:483] Algo bellman_ford step 2305 current loss 0.368303, current_train_items 73792.
I0302 18:59:35.711853 22732373614720 run.py:483] Algo bellman_ford step 2306 current loss 0.667714, current_train_items 73824.
I0302 18:59:35.734600 22732373614720 run.py:483] Algo bellman_ford step 2307 current loss 0.709837, current_train_items 73856.
I0302 18:59:35.763576 22732373614720 run.py:483] Algo bellman_ford step 2308 current loss 0.833870, current_train_items 73888.
I0302 18:59:35.796926 22732373614720 run.py:483] Algo bellman_ford step 2309 current loss 1.041967, current_train_items 73920.
I0302 18:59:35.816786 22732373614720 run.py:483] Algo bellman_ford step 2310 current loss 0.422892, current_train_items 73952.
I0302 18:59:35.832910 22732373614720 run.py:483] Algo bellman_ford step 2311 current loss 0.529250, current_train_items 73984.
I0302 18:59:35.855884 22732373614720 run.py:483] Algo bellman_ford step 2312 current loss 0.782720, current_train_items 74016.
I0302 18:59:35.885903 22732373614720 run.py:483] Algo bellman_ford step 2313 current loss 0.761389, current_train_items 74048.
I0302 18:59:35.918796 22732373614720 run.py:483] Algo bellman_ford step 2314 current loss 0.925405, current_train_items 74080.
I0302 18:59:35.938268 22732373614720 run.py:483] Algo bellman_ford step 2315 current loss 0.431116, current_train_items 74112.
I0302 18:59:35.954076 22732373614720 run.py:483] Algo bellman_ford step 2316 current loss 0.589641, current_train_items 74144.
I0302 18:59:35.976389 22732373614720 run.py:483] Algo bellman_ford step 2317 current loss 0.762484, current_train_items 74176.
I0302 18:59:36.006962 22732373614720 run.py:483] Algo bellman_ford step 2318 current loss 0.886654, current_train_items 74208.
I0302 18:59:36.040422 22732373614720 run.py:483] Algo bellman_ford step 2319 current loss 1.017374, current_train_items 74240.
I0302 18:59:36.060208 22732373614720 run.py:483] Algo bellman_ford step 2320 current loss 0.403044, current_train_items 74272.
I0302 18:59:36.075971 22732373614720 run.py:483] Algo bellman_ford step 2321 current loss 0.511438, current_train_items 74304.
I0302 18:59:36.099244 22732373614720 run.py:483] Algo bellman_ford step 2322 current loss 0.799209, current_train_items 74336.
I0302 18:59:36.128243 22732373614720 run.py:483] Algo bellman_ford step 2323 current loss 0.882313, current_train_items 74368.
I0302 18:59:36.161444 22732373614720 run.py:483] Algo bellman_ford step 2324 current loss 1.235862, current_train_items 74400.
I0302 18:59:36.180961 22732373614720 run.py:483] Algo bellman_ford step 2325 current loss 0.360329, current_train_items 74432.
I0302 18:59:36.196723 22732373614720 run.py:483] Algo bellman_ford step 2326 current loss 0.607311, current_train_items 74464.
I0302 18:59:36.219105 22732373614720 run.py:483] Algo bellman_ford step 2327 current loss 0.747100, current_train_items 74496.
I0302 18:59:36.247476 22732373614720 run.py:483] Algo bellman_ford step 2328 current loss 0.764104, current_train_items 74528.
I0302 18:59:36.278784 22732373614720 run.py:483] Algo bellman_ford step 2329 current loss 0.873815, current_train_items 74560.
I0302 18:59:36.298585 22732373614720 run.py:483] Algo bellman_ford step 2330 current loss 0.450510, current_train_items 74592.
I0302 18:59:36.314064 22732373614720 run.py:483] Algo bellman_ford step 2331 current loss 0.569686, current_train_items 74624.
I0302 18:59:36.337372 22732373614720 run.py:483] Algo bellman_ford step 2332 current loss 0.821694, current_train_items 74656.
I0302 18:59:36.367861 22732373614720 run.py:483] Algo bellman_ford step 2333 current loss 0.797045, current_train_items 74688.
I0302 18:59:36.401981 22732373614720 run.py:483] Algo bellman_ford step 2334 current loss 0.958663, current_train_items 74720.
I0302 18:59:36.421593 22732373614720 run.py:483] Algo bellman_ford step 2335 current loss 0.379015, current_train_items 74752.
I0302 18:59:36.437899 22732373614720 run.py:483] Algo bellman_ford step 2336 current loss 0.569120, current_train_items 74784.
I0302 18:59:36.460763 22732373614720 run.py:483] Algo bellman_ford step 2337 current loss 0.717496, current_train_items 74816.
I0302 18:59:36.490098 22732373614720 run.py:483] Algo bellman_ford step 2338 current loss 0.930085, current_train_items 74848.
I0302 18:59:36.521977 22732373614720 run.py:483] Algo bellman_ford step 2339 current loss 0.971701, current_train_items 74880.
I0302 18:59:36.541546 22732373614720 run.py:483] Algo bellman_ford step 2340 current loss 0.390047, current_train_items 74912.
I0302 18:59:36.557750 22732373614720 run.py:483] Algo bellman_ford step 2341 current loss 0.513397, current_train_items 74944.
I0302 18:59:36.579588 22732373614720 run.py:483] Algo bellman_ford step 2342 current loss 0.661266, current_train_items 74976.
I0302 18:59:36.608525 22732373614720 run.py:483] Algo bellman_ford step 2343 current loss 0.852190, current_train_items 75008.
I0302 18:59:36.639597 22732373614720 run.py:483] Algo bellman_ford step 2344 current loss 0.810596, current_train_items 75040.
I0302 18:59:36.659329 22732373614720 run.py:483] Algo bellman_ford step 2345 current loss 0.410317, current_train_items 75072.
I0302 18:59:36.675005 22732373614720 run.py:483] Algo bellman_ford step 2346 current loss 0.472703, current_train_items 75104.
I0302 18:59:36.697759 22732373614720 run.py:483] Algo bellman_ford step 2347 current loss 0.683501, current_train_items 75136.
I0302 18:59:36.726351 22732373614720 run.py:483] Algo bellman_ford step 2348 current loss 0.778091, current_train_items 75168.
I0302 18:59:36.757838 22732373614720 run.py:483] Algo bellman_ford step 2349 current loss 0.856478, current_train_items 75200.
I0302 18:59:36.777487 22732373614720 run.py:483] Algo bellman_ford step 2350 current loss 0.324355, current_train_items 75232.
I0302 18:59:36.785315 22732373614720 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.8935546875, 'score': 0.8935546875, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0302 18:59:36.785427 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.894, val scores are: bellman_ford: 0.894
I0302 18:59:36.801885 22732373614720 run.py:483] Algo bellman_ford step 2351 current loss 0.501513, current_train_items 75264.
I0302 18:59:36.824982 22732373614720 run.py:483] Algo bellman_ford step 2352 current loss 0.685443, current_train_items 75296.
I0302 18:59:36.853527 22732373614720 run.py:483] Algo bellman_ford step 2353 current loss 0.801921, current_train_items 75328.
I0302 18:59:36.888472 22732373614720 run.py:483] Algo bellman_ford step 2354 current loss 1.291229, current_train_items 75360.
I0302 18:59:36.908425 22732373614720 run.py:483] Algo bellman_ford step 2355 current loss 0.430662, current_train_items 75392.
I0302 18:59:36.923741 22732373614720 run.py:483] Algo bellman_ford step 2356 current loss 0.624459, current_train_items 75424.
I0302 18:59:36.946666 22732373614720 run.py:483] Algo bellman_ford step 2357 current loss 0.785408, current_train_items 75456.
I0302 18:59:36.975091 22732373614720 run.py:483] Algo bellman_ford step 2358 current loss 0.715909, current_train_items 75488.
I0302 18:59:37.009311 22732373614720 run.py:483] Algo bellman_ford step 2359 current loss 1.059272, current_train_items 75520.
I0302 18:59:37.029231 22732373614720 run.py:483] Algo bellman_ford step 2360 current loss 0.389754, current_train_items 75552.
I0302 18:59:37.045848 22732373614720 run.py:483] Algo bellman_ford step 2361 current loss 0.631500, current_train_items 75584.
I0302 18:59:37.069740 22732373614720 run.py:483] Algo bellman_ford step 2362 current loss 0.689508, current_train_items 75616.
I0302 18:59:37.098722 22732373614720 run.py:483] Algo bellman_ford step 2363 current loss 0.887449, current_train_items 75648.
I0302 18:59:37.131468 22732373614720 run.py:483] Algo bellman_ford step 2364 current loss 0.982207, current_train_items 75680.
I0302 18:59:37.150957 22732373614720 run.py:483] Algo bellman_ford step 2365 current loss 0.335473, current_train_items 75712.
I0302 18:59:37.167075 22732373614720 run.py:483] Algo bellman_ford step 2366 current loss 0.503721, current_train_items 75744.
I0302 18:59:37.189820 22732373614720 run.py:483] Algo bellman_ford step 2367 current loss 0.774564, current_train_items 75776.
I0302 18:59:37.220183 22732373614720 run.py:483] Algo bellman_ford step 2368 current loss 0.816267, current_train_items 75808.
I0302 18:59:37.254066 22732373614720 run.py:483] Algo bellman_ford step 2369 current loss 1.014011, current_train_items 75840.
I0302 18:59:37.274152 22732373614720 run.py:483] Algo bellman_ford step 2370 current loss 0.362317, current_train_items 75872.
I0302 18:59:37.290119 22732373614720 run.py:483] Algo bellman_ford step 2371 current loss 0.574851, current_train_items 75904.
I0302 18:59:37.312906 22732373614720 run.py:483] Algo bellman_ford step 2372 current loss 0.732049, current_train_items 75936.
I0302 18:59:37.341877 22732373614720 run.py:483] Algo bellman_ford step 2373 current loss 0.928858, current_train_items 75968.
I0302 18:59:37.373687 22732373614720 run.py:483] Algo bellman_ford step 2374 current loss 0.932167, current_train_items 76000.
I0302 18:59:37.393118 22732373614720 run.py:483] Algo bellman_ford step 2375 current loss 0.292060, current_train_items 76032.
I0302 18:59:37.409244 22732373614720 run.py:483] Algo bellman_ford step 2376 current loss 0.612943, current_train_items 76064.
I0302 18:59:37.432136 22732373614720 run.py:483] Algo bellman_ford step 2377 current loss 0.846590, current_train_items 76096.
I0302 18:59:37.462999 22732373614720 run.py:483] Algo bellman_ford step 2378 current loss 0.954879, current_train_items 76128.
I0302 18:59:37.497181 22732373614720 run.py:483] Algo bellman_ford step 2379 current loss 1.095022, current_train_items 76160.
I0302 18:59:37.517055 22732373614720 run.py:483] Algo bellman_ford step 2380 current loss 0.335604, current_train_items 76192.
I0302 18:59:37.533535 22732373614720 run.py:483] Algo bellman_ford step 2381 current loss 0.584002, current_train_items 76224.
I0302 18:59:37.556856 22732373614720 run.py:483] Algo bellman_ford step 2382 current loss 0.818883, current_train_items 76256.
I0302 18:59:37.585291 22732373614720 run.py:483] Algo bellman_ford step 2383 current loss 0.775891, current_train_items 76288.
I0302 18:59:37.618211 22732373614720 run.py:483] Algo bellman_ford step 2384 current loss 1.026028, current_train_items 76320.
I0302 18:59:37.637889 22732373614720 run.py:483] Algo bellman_ford step 2385 current loss 0.343504, current_train_items 76352.
I0302 18:59:37.653698 22732373614720 run.py:483] Algo bellman_ford step 2386 current loss 0.505432, current_train_items 76384.
I0302 18:59:37.676024 22732373614720 run.py:483] Algo bellman_ford step 2387 current loss 0.842833, current_train_items 76416.
I0302 18:59:37.705720 22732373614720 run.py:483] Algo bellman_ford step 2388 current loss 0.881183, current_train_items 76448.
I0302 18:59:37.740223 22732373614720 run.py:483] Algo bellman_ford step 2389 current loss 0.937822, current_train_items 76480.
I0302 18:59:37.760042 22732373614720 run.py:483] Algo bellman_ford step 2390 current loss 0.333648, current_train_items 76512.
I0302 18:59:37.775648 22732373614720 run.py:483] Algo bellman_ford step 2391 current loss 0.501986, current_train_items 76544.
I0302 18:59:37.796698 22732373614720 run.py:483] Algo bellman_ford step 2392 current loss 0.722294, current_train_items 76576.
I0302 18:59:37.826237 22732373614720 run.py:483] Algo bellman_ford step 2393 current loss 0.914472, current_train_items 76608.
I0302 18:59:37.861820 22732373614720 run.py:483] Algo bellman_ford step 2394 current loss 1.217329, current_train_items 76640.
I0302 18:59:37.881589 22732373614720 run.py:483] Algo bellman_ford step 2395 current loss 0.641135, current_train_items 76672.
I0302 18:59:37.897324 22732373614720 run.py:483] Algo bellman_ford step 2396 current loss 0.584100, current_train_items 76704.
I0302 18:59:37.918967 22732373614720 run.py:483] Algo bellman_ford step 2397 current loss 0.642424, current_train_items 76736.
I0302 18:59:37.948070 22732373614720 run.py:483] Algo bellman_ford step 2398 current loss 0.926494, current_train_items 76768.
I0302 18:59:37.981559 22732373614720 run.py:483] Algo bellman_ford step 2399 current loss 0.986426, current_train_items 76800.
I0302 18:59:38.001508 22732373614720 run.py:483] Algo bellman_ford step 2400 current loss 0.326847, current_train_items 76832.
I0302 18:59:38.009201 22732373614720 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.884765625, 'score': 0.884765625, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0302 18:59:38.009325 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.885, val scores are: bellman_ford: 0.885
I0302 18:59:38.025933 22732373614720 run.py:483] Algo bellman_ford step 2401 current loss 0.553583, current_train_items 76864.
I0302 18:59:38.049268 22732373614720 run.py:483] Algo bellman_ford step 2402 current loss 0.744197, current_train_items 76896.
I0302 18:59:38.080028 22732373614720 run.py:483] Algo bellman_ford step 2403 current loss 0.961218, current_train_items 76928.
I0302 18:59:38.115950 22732373614720 run.py:483] Algo bellman_ford step 2404 current loss 1.168472, current_train_items 76960.
I0302 18:59:38.136259 22732373614720 run.py:483] Algo bellman_ford step 2405 current loss 0.339263, current_train_items 76992.
I0302 18:59:38.152028 22732373614720 run.py:483] Algo bellman_ford step 2406 current loss 0.672956, current_train_items 77024.
I0302 18:59:38.174807 22732373614720 run.py:483] Algo bellman_ford step 2407 current loss 0.870338, current_train_items 77056.
I0302 18:59:38.203573 22732373614720 run.py:483] Algo bellman_ford step 2408 current loss 0.835091, current_train_items 77088.
I0302 18:59:38.235542 22732373614720 run.py:483] Algo bellman_ford step 2409 current loss 0.954430, current_train_items 77120.
I0302 18:59:38.255077 22732373614720 run.py:483] Algo bellman_ford step 2410 current loss 0.304883, current_train_items 77152.
I0302 18:59:38.271039 22732373614720 run.py:483] Algo bellman_ford step 2411 current loss 0.565606, current_train_items 77184.
I0302 18:59:38.294951 22732373614720 run.py:483] Algo bellman_ford step 2412 current loss 0.890270, current_train_items 77216.
I0302 18:59:38.325267 22732373614720 run.py:483] Algo bellman_ford step 2413 current loss 0.835090, current_train_items 77248.
I0302 18:59:38.358917 22732373614720 run.py:483] Algo bellman_ford step 2414 current loss 1.052660, current_train_items 77280.
I0302 18:59:38.378464 22732373614720 run.py:483] Algo bellman_ford step 2415 current loss 0.380856, current_train_items 77312.
I0302 18:59:38.393858 22732373614720 run.py:483] Algo bellman_ford step 2416 current loss 0.569933, current_train_items 77344.
I0302 18:59:38.417019 22732373614720 run.py:483] Algo bellman_ford step 2417 current loss 0.785744, current_train_items 77376.
I0302 18:59:38.447757 22732373614720 run.py:483] Algo bellman_ford step 2418 current loss 0.949414, current_train_items 77408.
I0302 18:59:38.478829 22732373614720 run.py:483] Algo bellman_ford step 2419 current loss 0.953433, current_train_items 77440.
I0302 18:59:38.497942 22732373614720 run.py:483] Algo bellman_ford step 2420 current loss 0.338386, current_train_items 77472.
I0302 18:59:38.513653 22732373614720 run.py:483] Algo bellman_ford step 2421 current loss 0.551307, current_train_items 77504.
I0302 18:59:38.537053 22732373614720 run.py:483] Algo bellman_ford step 2422 current loss 0.841087, current_train_items 77536.
I0302 18:59:38.566984 22732373614720 run.py:483] Algo bellman_ford step 2423 current loss 1.010907, current_train_items 77568.
I0302 18:59:38.600659 22732373614720 run.py:483] Algo bellman_ford step 2424 current loss 1.023662, current_train_items 77600.
I0302 18:59:38.620283 22732373614720 run.py:483] Algo bellman_ford step 2425 current loss 0.293772, current_train_items 77632.
I0302 18:59:38.636215 22732373614720 run.py:483] Algo bellman_ford step 2426 current loss 0.517979, current_train_items 77664.
I0302 18:59:38.660282 22732373614720 run.py:483] Algo bellman_ford step 2427 current loss 0.859463, current_train_items 77696.
I0302 18:59:38.688992 22732373614720 run.py:483] Algo bellman_ford step 2428 current loss 0.862354, current_train_items 77728.
I0302 18:59:38.721240 22732373614720 run.py:483] Algo bellman_ford step 2429 current loss 0.941864, current_train_items 77760.
I0302 18:59:38.740781 22732373614720 run.py:483] Algo bellman_ford step 2430 current loss 0.404767, current_train_items 77792.
I0302 18:59:38.756923 22732373614720 run.py:483] Algo bellman_ford step 2431 current loss 0.648335, current_train_items 77824.
I0302 18:59:38.780280 22732373614720 run.py:483] Algo bellman_ford step 2432 current loss 0.784566, current_train_items 77856.
I0302 18:59:38.809565 22732373614720 run.py:483] Algo bellman_ford step 2433 current loss 0.794126, current_train_items 77888.
I0302 18:59:38.842315 22732373614720 run.py:483] Algo bellman_ford step 2434 current loss 0.951450, current_train_items 77920.
I0302 18:59:38.861634 22732373614720 run.py:483] Algo bellman_ford step 2435 current loss 0.406732, current_train_items 77952.
I0302 18:59:38.877820 22732373614720 run.py:483] Algo bellman_ford step 2436 current loss 0.734128, current_train_items 77984.
I0302 18:59:38.900619 22732373614720 run.py:483] Algo bellman_ford step 2437 current loss 0.774567, current_train_items 78016.
I0302 18:59:38.928771 22732373614720 run.py:483] Algo bellman_ford step 2438 current loss 0.830716, current_train_items 78048.
I0302 18:59:38.960779 22732373614720 run.py:483] Algo bellman_ford step 2439 current loss 0.849899, current_train_items 78080.
I0302 18:59:38.980086 22732373614720 run.py:483] Algo bellman_ford step 2440 current loss 0.434176, current_train_items 78112.
I0302 18:59:38.996029 22732373614720 run.py:483] Algo bellman_ford step 2441 current loss 0.494818, current_train_items 78144.
I0302 18:59:39.019842 22732373614720 run.py:483] Algo bellman_ford step 2442 current loss 0.888376, current_train_items 78176.
I0302 18:59:39.049413 22732373614720 run.py:483] Algo bellman_ford step 2443 current loss 0.896014, current_train_items 78208.
I0302 18:59:39.083458 22732373614720 run.py:483] Algo bellman_ford step 2444 current loss 1.030270, current_train_items 78240.
I0302 18:59:39.103203 22732373614720 run.py:483] Algo bellman_ford step 2445 current loss 0.394079, current_train_items 78272.
I0302 18:59:39.119111 22732373614720 run.py:483] Algo bellman_ford step 2446 current loss 0.647830, current_train_items 78304.
I0302 18:59:39.142572 22732373614720 run.py:483] Algo bellman_ford step 2447 current loss 0.796650, current_train_items 78336.
I0302 18:59:39.171291 22732373614720 run.py:483] Algo bellman_ford step 2448 current loss 0.897219, current_train_items 78368.
I0302 18:59:39.203578 22732373614720 run.py:483] Algo bellman_ford step 2449 current loss 1.215393, current_train_items 78400.
I0302 18:59:39.222690 22732373614720 run.py:483] Algo bellman_ford step 2450 current loss 0.355878, current_train_items 78432.
I0302 18:59:39.230651 22732373614720 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0302 18:59:39.230764 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 18:59:39.247330 22732373614720 run.py:483] Algo bellman_ford step 2451 current loss 0.482680, current_train_items 78464.
I0302 18:59:39.271185 22732373614720 run.py:483] Algo bellman_ford step 2452 current loss 0.852474, current_train_items 78496.
I0302 18:59:39.300958 22732373614720 run.py:483] Algo bellman_ford step 2453 current loss 0.848410, current_train_items 78528.
I0302 18:59:39.335039 22732373614720 run.py:483] Algo bellman_ford step 2454 current loss 0.937621, current_train_items 78560.
I0302 18:59:39.355010 22732373614720 run.py:483] Algo bellman_ford step 2455 current loss 0.350895, current_train_items 78592.
I0302 18:59:39.370761 22732373614720 run.py:483] Algo bellman_ford step 2456 current loss 0.550942, current_train_items 78624.
I0302 18:59:39.393079 22732373614720 run.py:483] Algo bellman_ford step 2457 current loss 0.658124, current_train_items 78656.
I0302 18:59:39.423046 22732373614720 run.py:483] Algo bellman_ford step 2458 current loss 0.786849, current_train_items 78688.
I0302 18:59:39.457715 22732373614720 run.py:483] Algo bellman_ford step 2459 current loss 1.017386, current_train_items 78720.
I0302 18:59:39.477545 22732373614720 run.py:483] Algo bellman_ford step 2460 current loss 0.376962, current_train_items 78752.
I0302 18:59:39.494233 22732373614720 run.py:483] Algo bellman_ford step 2461 current loss 0.610985, current_train_items 78784.
I0302 18:59:39.516950 22732373614720 run.py:483] Algo bellman_ford step 2462 current loss 0.690489, current_train_items 78816.
I0302 18:59:39.547746 22732373614720 run.py:483] Algo bellman_ford step 2463 current loss 0.927281, current_train_items 78848.
I0302 18:59:39.580810 22732373614720 run.py:483] Algo bellman_ford step 2464 current loss 0.910454, current_train_items 78880.
I0302 18:59:39.600032 22732373614720 run.py:483] Algo bellman_ford step 2465 current loss 0.321366, current_train_items 78912.
I0302 18:59:39.615955 22732373614720 run.py:483] Algo bellman_ford step 2466 current loss 0.554083, current_train_items 78944.
I0302 18:59:39.639011 22732373614720 run.py:483] Algo bellman_ford step 2467 current loss 0.741079, current_train_items 78976.
I0302 18:59:39.669346 22732373614720 run.py:483] Algo bellman_ford step 2468 current loss 0.862912, current_train_items 79008.
I0302 18:59:39.703711 22732373614720 run.py:483] Algo bellman_ford step 2469 current loss 0.964416, current_train_items 79040.
I0302 18:59:39.723615 22732373614720 run.py:483] Algo bellman_ford step 2470 current loss 0.381546, current_train_items 79072.
I0302 18:59:39.739597 22732373614720 run.py:483] Algo bellman_ford step 2471 current loss 0.498566, current_train_items 79104.
I0302 18:59:39.762812 22732373614720 run.py:483] Algo bellman_ford step 2472 current loss 0.796029, current_train_items 79136.
I0302 18:59:39.793781 22732373614720 run.py:483] Algo bellman_ford step 2473 current loss 0.966192, current_train_items 79168.
I0302 18:59:39.826334 22732373614720 run.py:483] Algo bellman_ford step 2474 current loss 0.841536, current_train_items 79200.
I0302 18:59:39.846083 22732373614720 run.py:483] Algo bellman_ford step 2475 current loss 0.411991, current_train_items 79232.
I0302 18:59:39.862232 22732373614720 run.py:483] Algo bellman_ford step 2476 current loss 0.675628, current_train_items 79264.
I0302 18:59:39.883837 22732373614720 run.py:483] Algo bellman_ford step 2477 current loss 0.726238, current_train_items 79296.
I0302 18:59:39.913032 22732373614720 run.py:483] Algo bellman_ford step 2478 current loss 0.919794, current_train_items 79328.
I0302 18:59:39.947615 22732373614720 run.py:483] Algo bellman_ford step 2479 current loss 1.050135, current_train_items 79360.
I0302 18:59:39.966886 22732373614720 run.py:483] Algo bellman_ford step 2480 current loss 0.406577, current_train_items 79392.
I0302 18:59:39.982553 22732373614720 run.py:483] Algo bellman_ford step 2481 current loss 0.657387, current_train_items 79424.
I0302 18:59:40.006257 22732373614720 run.py:483] Algo bellman_ford step 2482 current loss 1.039156, current_train_items 79456.
I0302 18:59:40.036118 22732373614720 run.py:483] Algo bellman_ford step 2483 current loss 1.154504, current_train_items 79488.
I0302 18:59:40.070763 22732373614720 run.py:483] Algo bellman_ford step 2484 current loss 1.117160, current_train_items 79520.
I0302 18:59:40.090735 22732373614720 run.py:483] Algo bellman_ford step 2485 current loss 0.399549, current_train_items 79552.
I0302 18:59:40.106952 22732373614720 run.py:483] Algo bellman_ford step 2486 current loss 0.624314, current_train_items 79584.
I0302 18:59:40.130615 22732373614720 run.py:483] Algo bellman_ford step 2487 current loss 0.853796, current_train_items 79616.
I0302 18:59:40.157945 22732373614720 run.py:483] Algo bellman_ford step 2488 current loss 0.820893, current_train_items 79648.
I0302 18:59:40.191642 22732373614720 run.py:483] Algo bellman_ford step 2489 current loss 1.048986, current_train_items 79680.
I0302 18:59:40.211506 22732373614720 run.py:483] Algo bellman_ford step 2490 current loss 0.328766, current_train_items 79712.
I0302 18:59:40.227463 22732373614720 run.py:483] Algo bellman_ford step 2491 current loss 0.562703, current_train_items 79744.
I0302 18:59:40.249881 22732373614720 run.py:483] Algo bellman_ford step 2492 current loss 0.700400, current_train_items 79776.
I0302 18:59:40.279495 22732373614720 run.py:483] Algo bellman_ford step 2493 current loss 0.912523, current_train_items 79808.
I0302 18:59:40.311419 22732373614720 run.py:483] Algo bellman_ford step 2494 current loss 0.936815, current_train_items 79840.
I0302 18:59:40.330680 22732373614720 run.py:483] Algo bellman_ford step 2495 current loss 0.345417, current_train_items 79872.
I0302 18:59:40.347104 22732373614720 run.py:483] Algo bellman_ford step 2496 current loss 0.499547, current_train_items 79904.
I0302 18:59:40.369763 22732373614720 run.py:483] Algo bellman_ford step 2497 current loss 0.779863, current_train_items 79936.
I0302 18:59:40.398358 22732373614720 run.py:483] Algo bellman_ford step 2498 current loss 0.871986, current_train_items 79968.
I0302 18:59:40.431628 22732373614720 run.py:483] Algo bellman_ford step 2499 current loss 0.979695, current_train_items 80000.
I0302 18:59:40.451413 22732373614720 run.py:483] Algo bellman_ford step 2500 current loss 0.385232, current_train_items 80032.
I0302 18:59:40.459088 22732373614720 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.884765625, 'score': 0.884765625, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0302 18:59:40.459205 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.885, val scores are: bellman_ford: 0.885
I0302 18:59:40.475858 22732373614720 run.py:483] Algo bellman_ford step 2501 current loss 0.569883, current_train_items 80064.
I0302 18:59:40.499683 22732373614720 run.py:483] Algo bellman_ford step 2502 current loss 0.822804, current_train_items 80096.
I0302 18:59:40.529107 22732373614720 run.py:483] Algo bellman_ford step 2503 current loss 0.818347, current_train_items 80128.
I0302 18:59:40.563371 22732373614720 run.py:483] Algo bellman_ford step 2504 current loss 0.967444, current_train_items 80160.
I0302 18:59:40.583119 22732373614720 run.py:483] Algo bellman_ford step 2505 current loss 0.518038, current_train_items 80192.
I0302 18:59:40.598915 22732373614720 run.py:483] Algo bellman_ford step 2506 current loss 0.651936, current_train_items 80224.
I0302 18:59:40.622234 22732373614720 run.py:483] Algo bellman_ford step 2507 current loss 0.780780, current_train_items 80256.
I0302 18:59:40.651168 22732373614720 run.py:483] Algo bellman_ford step 2508 current loss 0.893053, current_train_items 80288.
I0302 18:59:40.684977 22732373614720 run.py:483] Algo bellman_ford step 2509 current loss 1.069585, current_train_items 80320.
I0302 18:59:40.704440 22732373614720 run.py:483] Algo bellman_ford step 2510 current loss 0.385876, current_train_items 80352.
I0302 18:59:40.720746 22732373614720 run.py:483] Algo bellman_ford step 2511 current loss 0.695881, current_train_items 80384.
I0302 18:59:40.743660 22732373614720 run.py:483] Algo bellman_ford step 2512 current loss 0.884041, current_train_items 80416.
I0302 18:59:40.772797 22732373614720 run.py:483] Algo bellman_ford step 2513 current loss 0.843400, current_train_items 80448.
I0302 18:59:40.806893 22732373614720 run.py:483] Algo bellman_ford step 2514 current loss 1.019476, current_train_items 80480.
I0302 18:59:40.826464 22732373614720 run.py:483] Algo bellman_ford step 2515 current loss 0.318944, current_train_items 80512.
I0302 18:59:40.842581 22732373614720 run.py:483] Algo bellman_ford step 2516 current loss 0.541780, current_train_items 80544.
I0302 18:59:40.865950 22732373614720 run.py:483] Algo bellman_ford step 2517 current loss 0.804813, current_train_items 80576.
I0302 18:59:40.894914 22732373614720 run.py:483] Algo bellman_ford step 2518 current loss 0.778399, current_train_items 80608.
I0302 18:59:40.928534 22732373614720 run.py:483] Algo bellman_ford step 2519 current loss 0.990152, current_train_items 80640.
I0302 18:59:40.947861 22732373614720 run.py:483] Algo bellman_ford step 2520 current loss 0.446500, current_train_items 80672.
I0302 18:59:40.963932 22732373614720 run.py:483] Algo bellman_ford step 2521 current loss 0.569965, current_train_items 80704.
I0302 18:59:40.985806 22732373614720 run.py:483] Algo bellman_ford step 2522 current loss 0.641129, current_train_items 80736.
I0302 18:59:41.015216 22732373614720 run.py:483] Algo bellman_ford step 2523 current loss 0.853359, current_train_items 80768.
I0302 18:59:41.047680 22732373614720 run.py:483] Algo bellman_ford step 2524 current loss 0.932927, current_train_items 80800.
I0302 18:59:41.067310 22732373614720 run.py:483] Algo bellman_ford step 2525 current loss 0.393740, current_train_items 80832.
I0302 18:59:41.083319 22732373614720 run.py:483] Algo bellman_ford step 2526 current loss 0.537122, current_train_items 80864.
I0302 18:59:41.106373 22732373614720 run.py:483] Algo bellman_ford step 2527 current loss 0.906431, current_train_items 80896.
I0302 18:59:41.135487 22732373614720 run.py:483] Algo bellman_ford step 2528 current loss 0.888340, current_train_items 80928.
I0302 18:59:41.169516 22732373614720 run.py:483] Algo bellman_ford step 2529 current loss 1.218403, current_train_items 80960.
I0302 18:59:41.189047 22732373614720 run.py:483] Algo bellman_ford step 2530 current loss 0.416736, current_train_items 80992.
I0302 18:59:41.204735 22732373614720 run.py:483] Algo bellman_ford step 2531 current loss 0.579142, current_train_items 81024.
I0302 18:59:41.227918 22732373614720 run.py:483] Algo bellman_ford step 2532 current loss 0.940245, current_train_items 81056.
I0302 18:59:41.257411 22732373614720 run.py:483] Algo bellman_ford step 2533 current loss 0.828449, current_train_items 81088.
I0302 18:59:41.289707 22732373614720 run.py:483] Algo bellman_ford step 2534 current loss 0.976143, current_train_items 81120.
I0302 18:59:41.309373 22732373614720 run.py:483] Algo bellman_ford step 2535 current loss 0.382809, current_train_items 81152.
I0302 18:59:41.325018 22732373614720 run.py:483] Algo bellman_ford step 2536 current loss 0.501662, current_train_items 81184.
W0302 18:59:41.341142 22732373614720 samplers.py:155] Increasing hint lengh from 9 to 10
I0302 18:59:47.966127 22732373614720 run.py:483] Algo bellman_ford step 2537 current loss 0.950118, current_train_items 81216.
I0302 18:59:47.998271 22732373614720 run.py:483] Algo bellman_ford step 2538 current loss 0.904900, current_train_items 81248.
I0302 18:59:48.032295 22732373614720 run.py:483] Algo bellman_ford step 2539 current loss 1.145573, current_train_items 81280.
I0302 18:59:48.052494 22732373614720 run.py:483] Algo bellman_ford step 2540 current loss 0.384591, current_train_items 81312.
I0302 18:59:48.068496 22732373614720 run.py:483] Algo bellman_ford step 2541 current loss 0.465329, current_train_items 81344.
I0302 18:59:48.091752 22732373614720 run.py:483] Algo bellman_ford step 2542 current loss 0.693132, current_train_items 81376.
I0302 18:59:48.120404 22732373614720 run.py:483] Algo bellman_ford step 2543 current loss 0.772875, current_train_items 81408.
I0302 18:59:48.153185 22732373614720 run.py:483] Algo bellman_ford step 2544 current loss 0.995653, current_train_items 81440.
I0302 18:59:48.172908 22732373614720 run.py:483] Algo bellman_ford step 2545 current loss 0.373163, current_train_items 81472.
I0302 18:59:48.188788 22732373614720 run.py:483] Algo bellman_ford step 2546 current loss 0.594553, current_train_items 81504.
I0302 18:59:48.212061 22732373614720 run.py:483] Algo bellman_ford step 2547 current loss 0.693294, current_train_items 81536.
I0302 18:59:48.243112 22732373614720 run.py:483] Algo bellman_ford step 2548 current loss 0.955076, current_train_items 81568.
I0302 18:59:48.275008 22732373614720 run.py:483] Algo bellman_ford step 2549 current loss 0.970746, current_train_items 81600.
I0302 18:59:48.294723 22732373614720 run.py:483] Algo bellman_ford step 2550 current loss 0.339793, current_train_items 81632.
I0302 18:59:48.304570 22732373614720 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0302 18:59:48.304681 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 18:59:48.321662 22732373614720 run.py:483] Algo bellman_ford step 2551 current loss 0.587598, current_train_items 81664.
I0302 18:59:48.345812 22732373614720 run.py:483] Algo bellman_ford step 2552 current loss 0.751210, current_train_items 81696.
I0302 18:59:48.377472 22732373614720 run.py:483] Algo bellman_ford step 2553 current loss 0.854535, current_train_items 81728.
I0302 18:59:48.410821 22732373614720 run.py:483] Algo bellman_ford step 2554 current loss 1.021132, current_train_items 81760.
I0302 18:59:48.431100 22732373614720 run.py:483] Algo bellman_ford step 2555 current loss 0.379228, current_train_items 81792.
I0302 18:59:48.447206 22732373614720 run.py:483] Algo bellman_ford step 2556 current loss 0.672573, current_train_items 81824.
I0302 18:59:48.470977 22732373614720 run.py:483] Algo bellman_ford step 2557 current loss 0.715547, current_train_items 81856.
I0302 18:59:48.501741 22732373614720 run.py:483] Algo bellman_ford step 2558 current loss 0.808002, current_train_items 81888.
I0302 18:59:48.536509 22732373614720 run.py:483] Algo bellman_ford step 2559 current loss 1.123518, current_train_items 81920.
I0302 18:59:48.556438 22732373614720 run.py:483] Algo bellman_ford step 2560 current loss 0.422973, current_train_items 81952.
I0302 18:59:48.573038 22732373614720 run.py:483] Algo bellman_ford step 2561 current loss 0.583615, current_train_items 81984.
I0302 18:59:48.595996 22732373614720 run.py:483] Algo bellman_ford step 2562 current loss 0.688587, current_train_items 82016.
I0302 18:59:48.627714 22732373614720 run.py:483] Algo bellman_ford step 2563 current loss 0.943644, current_train_items 82048.
I0302 18:59:48.661403 22732373614720 run.py:483] Algo bellman_ford step 2564 current loss 1.041477, current_train_items 82080.
I0302 18:59:48.681011 22732373614720 run.py:483] Algo bellman_ford step 2565 current loss 0.436582, current_train_items 82112.
I0302 18:59:48.697240 22732373614720 run.py:483] Algo bellman_ford step 2566 current loss 0.576559, current_train_items 82144.
I0302 18:59:48.720008 22732373614720 run.py:483] Algo bellman_ford step 2567 current loss 0.691562, current_train_items 82176.
I0302 18:59:48.751113 22732373614720 run.py:483] Algo bellman_ford step 2568 current loss 0.786235, current_train_items 82208.
I0302 18:59:48.783856 22732373614720 run.py:483] Algo bellman_ford step 2569 current loss 0.924016, current_train_items 82240.
I0302 18:59:48.803898 22732373614720 run.py:483] Algo bellman_ford step 2570 current loss 0.296091, current_train_items 82272.
I0302 18:59:48.820038 22732373614720 run.py:483] Algo bellman_ford step 2571 current loss 0.573638, current_train_items 82304.
I0302 18:59:48.842085 22732373614720 run.py:483] Algo bellman_ford step 2572 current loss 0.711031, current_train_items 82336.
I0302 18:59:48.872397 22732373614720 run.py:483] Algo bellman_ford step 2573 current loss 0.725538, current_train_items 82368.
I0302 18:59:48.906788 22732373614720 run.py:483] Algo bellman_ford step 2574 current loss 1.011570, current_train_items 82400.
I0302 18:59:48.927008 22732373614720 run.py:483] Algo bellman_ford step 2575 current loss 0.407677, current_train_items 82432.
I0302 18:59:48.942849 22732373614720 run.py:483] Algo bellman_ford step 2576 current loss 0.521103, current_train_items 82464.
I0302 18:59:48.965131 22732373614720 run.py:483] Algo bellman_ford step 2577 current loss 0.735737, current_train_items 82496.
I0302 18:59:48.996185 22732373614720 run.py:483] Algo bellman_ford step 2578 current loss 0.884176, current_train_items 82528.
I0302 18:59:49.028553 22732373614720 run.py:483] Algo bellman_ford step 2579 current loss 1.155185, current_train_items 82560.
I0302 18:59:49.048213 22732373614720 run.py:483] Algo bellman_ford step 2580 current loss 0.445322, current_train_items 82592.
I0302 18:59:49.064300 22732373614720 run.py:483] Algo bellman_ford step 2581 current loss 0.510394, current_train_items 82624.
I0302 18:59:49.087393 22732373614720 run.py:483] Algo bellman_ford step 2582 current loss 0.771309, current_train_items 82656.
I0302 18:59:49.117584 22732373614720 run.py:483] Algo bellman_ford step 2583 current loss 0.741756, current_train_items 82688.
I0302 18:59:49.151551 22732373614720 run.py:483] Algo bellman_ford step 2584 current loss 0.977890, current_train_items 82720.
I0302 18:59:49.171439 22732373614720 run.py:483] Algo bellman_ford step 2585 current loss 0.390897, current_train_items 82752.
I0302 18:59:49.187555 22732373614720 run.py:483] Algo bellman_ford step 2586 current loss 0.589045, current_train_items 82784.
I0302 18:59:49.210630 22732373614720 run.py:483] Algo bellman_ford step 2587 current loss 0.681032, current_train_items 82816.
I0302 18:59:49.240802 22732373614720 run.py:483] Algo bellman_ford step 2588 current loss 0.799483, current_train_items 82848.
I0302 18:59:49.274429 22732373614720 run.py:483] Algo bellman_ford step 2589 current loss 0.963560, current_train_items 82880.
I0302 18:59:49.294971 22732373614720 run.py:483] Algo bellman_ford step 2590 current loss 0.324268, current_train_items 82912.
I0302 18:59:49.311172 22732373614720 run.py:483] Algo bellman_ford step 2591 current loss 0.515410, current_train_items 82944.
I0302 18:59:49.334847 22732373614720 run.py:483] Algo bellman_ford step 2592 current loss 0.882340, current_train_items 82976.
I0302 18:59:49.366067 22732373614720 run.py:483] Algo bellman_ford step 2593 current loss 0.949805, current_train_items 83008.
I0302 18:59:49.397744 22732373614720 run.py:483] Algo bellman_ford step 2594 current loss 1.065733, current_train_items 83040.
I0302 18:59:49.417296 22732373614720 run.py:483] Algo bellman_ford step 2595 current loss 0.416288, current_train_items 83072.
I0302 18:59:49.433598 22732373614720 run.py:483] Algo bellman_ford step 2596 current loss 0.583995, current_train_items 83104.
I0302 18:59:49.457575 22732373614720 run.py:483] Algo bellman_ford step 2597 current loss 0.834243, current_train_items 83136.
I0302 18:59:49.488178 22732373614720 run.py:483] Algo bellman_ford step 2598 current loss 0.818097, current_train_items 83168.
I0302 18:59:49.521275 22732373614720 run.py:483] Algo bellman_ford step 2599 current loss 0.973543, current_train_items 83200.
I0302 18:59:49.541334 22732373614720 run.py:483] Algo bellman_ford step 2600 current loss 0.366415, current_train_items 83232.
I0302 18:59:49.549326 22732373614720 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0302 18:59:49.549433 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 18:59:49.566107 22732373614720 run.py:483] Algo bellman_ford step 2601 current loss 0.577143, current_train_items 83264.
I0302 18:59:49.589386 22732373614720 run.py:483] Algo bellman_ford step 2602 current loss 0.703695, current_train_items 83296.
I0302 18:59:49.618499 22732373614720 run.py:483] Algo bellman_ford step 2603 current loss 0.718037, current_train_items 83328.
I0302 18:59:49.651651 22732373614720 run.py:483] Algo bellman_ford step 2604 current loss 1.027134, current_train_items 83360.
I0302 18:59:49.671351 22732373614720 run.py:483] Algo bellman_ford step 2605 current loss 0.345237, current_train_items 83392.
I0302 18:59:49.687008 22732373614720 run.py:483] Algo bellman_ford step 2606 current loss 0.562321, current_train_items 83424.
I0302 18:59:49.710268 22732373614720 run.py:483] Algo bellman_ford step 2607 current loss 0.701901, current_train_items 83456.
I0302 18:59:49.741236 22732373614720 run.py:483] Algo bellman_ford step 2608 current loss 0.842055, current_train_items 83488.
I0302 18:59:49.774428 22732373614720 run.py:483] Algo bellman_ford step 2609 current loss 1.059432, current_train_items 83520.
I0302 18:59:49.794261 22732373614720 run.py:483] Algo bellman_ford step 2610 current loss 0.346798, current_train_items 83552.
I0302 18:59:49.810078 22732373614720 run.py:483] Algo bellman_ford step 2611 current loss 0.537505, current_train_items 83584.
I0302 18:59:49.832839 22732373614720 run.py:483] Algo bellman_ford step 2612 current loss 0.636033, current_train_items 83616.
I0302 18:59:49.862949 22732373614720 run.py:483] Algo bellman_ford step 2613 current loss 0.779326, current_train_items 83648.
I0302 18:59:49.894845 22732373614720 run.py:483] Algo bellman_ford step 2614 current loss 0.935656, current_train_items 83680.
I0302 18:59:49.914614 22732373614720 run.py:483] Algo bellman_ford step 2615 current loss 0.386814, current_train_items 83712.
I0302 18:59:49.930296 22732373614720 run.py:483] Algo bellman_ford step 2616 current loss 0.451923, current_train_items 83744.
I0302 18:59:49.954240 22732373614720 run.py:483] Algo bellman_ford step 2617 current loss 0.761670, current_train_items 83776.
I0302 18:59:49.985296 22732373614720 run.py:483] Algo bellman_ford step 2618 current loss 0.899965, current_train_items 83808.
I0302 18:59:50.017029 22732373614720 run.py:483] Algo bellman_ford step 2619 current loss 0.949875, current_train_items 83840.
I0302 18:59:50.036475 22732373614720 run.py:483] Algo bellman_ford step 2620 current loss 0.395160, current_train_items 83872.
I0302 18:59:50.052480 22732373614720 run.py:483] Algo bellman_ford step 2621 current loss 0.526777, current_train_items 83904.
I0302 18:59:50.075919 22732373614720 run.py:483] Algo bellman_ford step 2622 current loss 0.714969, current_train_items 83936.
I0302 18:59:50.105788 22732373614720 run.py:483] Algo bellman_ford step 2623 current loss 0.762899, current_train_items 83968.
I0302 18:59:50.137987 22732373614720 run.py:483] Algo bellman_ford step 2624 current loss 0.951883, current_train_items 84000.
I0302 18:59:50.157443 22732373614720 run.py:483] Algo bellman_ford step 2625 current loss 0.342075, current_train_items 84032.
I0302 18:59:50.173420 22732373614720 run.py:483] Algo bellman_ford step 2626 current loss 0.461478, current_train_items 84064.
I0302 18:59:50.196944 22732373614720 run.py:483] Algo bellman_ford step 2627 current loss 0.653531, current_train_items 84096.
I0302 18:59:50.227366 22732373614720 run.py:483] Algo bellman_ford step 2628 current loss 0.729345, current_train_items 84128.
I0302 18:59:50.260066 22732373614720 run.py:483] Algo bellman_ford step 2629 current loss 0.873554, current_train_items 84160.
I0302 18:59:50.279489 22732373614720 run.py:483] Algo bellman_ford step 2630 current loss 0.349918, current_train_items 84192.
I0302 18:59:50.295427 22732373614720 run.py:483] Algo bellman_ford step 2631 current loss 0.507760, current_train_items 84224.
I0302 18:59:50.318393 22732373614720 run.py:483] Algo bellman_ford step 2632 current loss 0.712316, current_train_items 84256.
I0302 18:59:50.347782 22732373614720 run.py:483] Algo bellman_ford step 2633 current loss 0.812446, current_train_items 84288.
I0302 18:59:50.382718 22732373614720 run.py:483] Algo bellman_ford step 2634 current loss 1.038759, current_train_items 84320.
I0302 18:59:50.402615 22732373614720 run.py:483] Algo bellman_ford step 2635 current loss 0.281591, current_train_items 84352.
I0302 18:59:50.418448 22732373614720 run.py:483] Algo bellman_ford step 2636 current loss 0.544027, current_train_items 84384.
I0302 18:59:50.441888 22732373614720 run.py:483] Algo bellman_ford step 2637 current loss 0.867994, current_train_items 84416.
I0302 18:59:50.473062 22732373614720 run.py:483] Algo bellman_ford step 2638 current loss 0.951882, current_train_items 84448.
I0302 18:59:50.506607 22732373614720 run.py:483] Algo bellman_ford step 2639 current loss 0.939373, current_train_items 84480.
I0302 18:59:50.525990 22732373614720 run.py:483] Algo bellman_ford step 2640 current loss 0.388322, current_train_items 84512.
I0302 18:59:50.541865 22732373614720 run.py:483] Algo bellman_ford step 2641 current loss 0.637708, current_train_items 84544.
I0302 18:59:50.564332 22732373614720 run.py:483] Algo bellman_ford step 2642 current loss 0.714510, current_train_items 84576.
I0302 18:59:50.594469 22732373614720 run.py:483] Algo bellman_ford step 2643 current loss 0.783262, current_train_items 84608.
I0302 18:59:50.628762 22732373614720 run.py:483] Algo bellman_ford step 2644 current loss 1.030696, current_train_items 84640.
I0302 18:59:50.648266 22732373614720 run.py:483] Algo bellman_ford step 2645 current loss 0.587618, current_train_items 84672.
I0302 18:59:50.664057 22732373614720 run.py:483] Algo bellman_ford step 2646 current loss 0.620262, current_train_items 84704.
I0302 18:59:50.687400 22732373614720 run.py:483] Algo bellman_ford step 2647 current loss 0.765195, current_train_items 84736.
I0302 18:59:50.718336 22732373614720 run.py:483] Algo bellman_ford step 2648 current loss 0.774040, current_train_items 84768.
I0302 18:59:50.753366 22732373614720 run.py:483] Algo bellman_ford step 2649 current loss 1.153472, current_train_items 84800.
I0302 18:59:50.773268 22732373614720 run.py:483] Algo bellman_ford step 2650 current loss 0.349297, current_train_items 84832.
I0302 18:59:50.781262 22732373614720 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.8173828125, 'score': 0.8173828125, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0302 18:59:50.781371 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.817, val scores are: bellman_ford: 0.817
I0302 18:59:50.797724 22732373614720 run.py:483] Algo bellman_ford step 2651 current loss 0.588197, current_train_items 84864.
I0302 18:59:50.820962 22732373614720 run.py:483] Algo bellman_ford step 2652 current loss 0.796710, current_train_items 84896.
I0302 18:59:50.852326 22732373614720 run.py:483] Algo bellman_ford step 2653 current loss 0.920595, current_train_items 84928.
I0302 18:59:50.885742 22732373614720 run.py:483] Algo bellman_ford step 2654 current loss 0.970289, current_train_items 84960.
I0302 18:59:50.905625 22732373614720 run.py:483] Algo bellman_ford step 2655 current loss 0.333129, current_train_items 84992.
I0302 18:59:50.921322 22732373614720 run.py:483] Algo bellman_ford step 2656 current loss 0.594047, current_train_items 85024.
I0302 18:59:50.944225 22732373614720 run.py:483] Algo bellman_ford step 2657 current loss 0.719683, current_train_items 85056.
I0302 18:59:50.973772 22732373614720 run.py:483] Algo bellman_ford step 2658 current loss 0.826759, current_train_items 85088.
I0302 18:59:51.008598 22732373614720 run.py:483] Algo bellman_ford step 2659 current loss 0.970840, current_train_items 85120.
I0302 18:59:51.028475 22732373614720 run.py:483] Algo bellman_ford step 2660 current loss 0.398185, current_train_items 85152.
I0302 18:59:51.045009 22732373614720 run.py:483] Algo bellman_ford step 2661 current loss 0.563381, current_train_items 85184.
I0302 18:59:51.067670 22732373614720 run.py:483] Algo bellman_ford step 2662 current loss 0.765788, current_train_items 85216.
I0302 18:59:51.098074 22732373614720 run.py:483] Algo bellman_ford step 2663 current loss 0.792557, current_train_items 85248.
I0302 18:59:51.133530 22732373614720 run.py:483] Algo bellman_ford step 2664 current loss 0.923939, current_train_items 85280.
I0302 18:59:51.153026 22732373614720 run.py:483] Algo bellman_ford step 2665 current loss 0.351555, current_train_items 85312.
I0302 18:59:51.169066 22732373614720 run.py:483] Algo bellman_ford step 2666 current loss 0.589508, current_train_items 85344.
I0302 18:59:51.193152 22732373614720 run.py:483] Algo bellman_ford step 2667 current loss 0.708760, current_train_items 85376.
I0302 18:59:51.223792 22732373614720 run.py:483] Algo bellman_ford step 2668 current loss 0.825949, current_train_items 85408.
I0302 18:59:51.256208 22732373614720 run.py:483] Algo bellman_ford step 2669 current loss 0.950522, current_train_items 85440.
I0302 18:59:51.276268 22732373614720 run.py:483] Algo bellman_ford step 2670 current loss 0.522417, current_train_items 85472.
I0302 18:59:51.292527 22732373614720 run.py:483] Algo bellman_ford step 2671 current loss 0.574687, current_train_items 85504.
I0302 18:59:51.316114 22732373614720 run.py:483] Algo bellman_ford step 2672 current loss 0.691954, current_train_items 85536.
I0302 18:59:51.347458 22732373614720 run.py:483] Algo bellman_ford step 2673 current loss 0.802460, current_train_items 85568.
I0302 18:59:51.381896 22732373614720 run.py:483] Algo bellman_ford step 2674 current loss 0.920327, current_train_items 85600.
I0302 18:59:51.401776 22732373614720 run.py:483] Algo bellman_ford step 2675 current loss 0.310854, current_train_items 85632.
I0302 18:59:51.417850 22732373614720 run.py:483] Algo bellman_ford step 2676 current loss 0.545386, current_train_items 85664.
I0302 18:59:51.441875 22732373614720 run.py:483] Algo bellman_ford step 2677 current loss 0.766555, current_train_items 85696.
I0302 18:59:51.471007 22732373614720 run.py:483] Algo bellman_ford step 2678 current loss 0.674828, current_train_items 85728.
I0302 18:59:51.503073 22732373614720 run.py:483] Algo bellman_ford step 2679 current loss 0.836504, current_train_items 85760.
I0302 18:59:51.522661 22732373614720 run.py:483] Algo bellman_ford step 2680 current loss 0.361698, current_train_items 85792.
I0302 18:59:51.538622 22732373614720 run.py:483] Algo bellman_ford step 2681 current loss 0.511042, current_train_items 85824.
I0302 18:59:51.561818 22732373614720 run.py:483] Algo bellman_ford step 2682 current loss 0.798806, current_train_items 85856.
I0302 18:59:51.593190 22732373614720 run.py:483] Algo bellman_ford step 2683 current loss 0.853008, current_train_items 85888.
I0302 18:59:51.626885 22732373614720 run.py:483] Algo bellman_ford step 2684 current loss 0.965606, current_train_items 85920.
I0302 18:59:51.646999 22732373614720 run.py:483] Algo bellman_ford step 2685 current loss 0.316727, current_train_items 85952.
I0302 18:59:51.662935 22732373614720 run.py:483] Algo bellman_ford step 2686 current loss 0.544388, current_train_items 85984.
I0302 18:59:51.686187 22732373614720 run.py:483] Algo bellman_ford step 2687 current loss 0.742774, current_train_items 86016.
I0302 18:59:51.716696 22732373614720 run.py:483] Algo bellman_ford step 2688 current loss 0.864169, current_train_items 86048.
I0302 18:59:51.748680 22732373614720 run.py:483] Algo bellman_ford step 2689 current loss 0.862177, current_train_items 86080.
I0302 18:59:51.768672 22732373614720 run.py:483] Algo bellman_ford step 2690 current loss 0.349362, current_train_items 86112.
I0302 18:59:51.784828 22732373614720 run.py:483] Algo bellman_ford step 2691 current loss 0.629135, current_train_items 86144.
I0302 18:59:51.809119 22732373614720 run.py:483] Algo bellman_ford step 2692 current loss 0.779285, current_train_items 86176.
I0302 18:59:51.840425 22732373614720 run.py:483] Algo bellman_ford step 2693 current loss 0.820568, current_train_items 86208.
I0302 18:59:51.873762 22732373614720 run.py:483] Algo bellman_ford step 2694 current loss 0.970529, current_train_items 86240.
I0302 18:59:51.893563 22732373614720 run.py:483] Algo bellman_ford step 2695 current loss 0.353469, current_train_items 86272.
I0302 18:59:51.909352 22732373614720 run.py:483] Algo bellman_ford step 2696 current loss 0.444882, current_train_items 86304.
I0302 18:59:51.932738 22732373614720 run.py:483] Algo bellman_ford step 2697 current loss 0.756148, current_train_items 86336.
I0302 18:59:51.962951 22732373614720 run.py:483] Algo bellman_ford step 2698 current loss 0.766389, current_train_items 86368.
I0302 18:59:51.994649 22732373614720 run.py:483] Algo bellman_ford step 2699 current loss 1.107803, current_train_items 86400.
I0302 18:59:52.014754 22732373614720 run.py:483] Algo bellman_ford step 2700 current loss 0.417670, current_train_items 86432.
I0302 18:59:52.022574 22732373614720 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.861328125, 'score': 0.861328125, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0302 18:59:52.022683 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.861, val scores are: bellman_ford: 0.861
I0302 18:59:52.039318 22732373614720 run.py:483] Algo bellman_ford step 2701 current loss 0.564413, current_train_items 86464.
I0302 18:59:52.063277 22732373614720 run.py:483] Algo bellman_ford step 2702 current loss 0.715678, current_train_items 86496.
I0302 18:59:52.095861 22732373614720 run.py:483] Algo bellman_ford step 2703 current loss 0.877583, current_train_items 86528.
I0302 18:59:52.131566 22732373614720 run.py:483] Algo bellman_ford step 2704 current loss 1.015255, current_train_items 86560.
I0302 18:59:52.151588 22732373614720 run.py:483] Algo bellman_ford step 2705 current loss 0.392593, current_train_items 86592.
I0302 18:59:52.167142 22732373614720 run.py:483] Algo bellman_ford step 2706 current loss 0.607827, current_train_items 86624.
I0302 18:59:52.190597 22732373614720 run.py:483] Algo bellman_ford step 2707 current loss 0.759963, current_train_items 86656.
I0302 18:59:52.219720 22732373614720 run.py:483] Algo bellman_ford step 2708 current loss 0.729843, current_train_items 86688.
I0302 18:59:52.251252 22732373614720 run.py:483] Algo bellman_ford step 2709 current loss 0.817235, current_train_items 86720.
I0302 18:59:52.271299 22732373614720 run.py:483] Algo bellman_ford step 2710 current loss 0.407625, current_train_items 86752.
I0302 18:59:52.287499 22732373614720 run.py:483] Algo bellman_ford step 2711 current loss 0.498046, current_train_items 86784.
I0302 18:59:52.311383 22732373614720 run.py:483] Algo bellman_ford step 2712 current loss 0.773099, current_train_items 86816.
I0302 18:59:52.342269 22732373614720 run.py:483] Algo bellman_ford step 2713 current loss 0.894729, current_train_items 86848.
I0302 18:59:52.377371 22732373614720 run.py:483] Algo bellman_ford step 2714 current loss 1.136145, current_train_items 86880.
I0302 18:59:52.397221 22732373614720 run.py:483] Algo bellman_ford step 2715 current loss 0.354832, current_train_items 86912.
I0302 18:59:52.412754 22732373614720 run.py:483] Algo bellman_ford step 2716 current loss 0.453330, current_train_items 86944.
I0302 18:59:52.437171 22732373614720 run.py:483] Algo bellman_ford step 2717 current loss 0.787882, current_train_items 86976.
I0302 18:59:52.466094 22732373614720 run.py:483] Algo bellman_ford step 2718 current loss 0.837580, current_train_items 87008.
I0302 18:59:52.499193 22732373614720 run.py:483] Algo bellman_ford step 2719 current loss 1.014776, current_train_items 87040.
I0302 18:59:52.518628 22732373614720 run.py:483] Algo bellman_ford step 2720 current loss 0.411159, current_train_items 87072.
I0302 18:59:52.534539 22732373614720 run.py:483] Algo bellman_ford step 2721 current loss 0.787364, current_train_items 87104.
I0302 18:59:52.557445 22732373614720 run.py:483] Algo bellman_ford step 2722 current loss 0.936631, current_train_items 87136.
I0302 18:59:52.588858 22732373614720 run.py:483] Algo bellman_ford step 2723 current loss 0.901063, current_train_items 87168.
I0302 18:59:52.621533 22732373614720 run.py:483] Algo bellman_ford step 2724 current loss 0.871827, current_train_items 87200.
I0302 18:59:52.640877 22732373614720 run.py:483] Algo bellman_ford step 2725 current loss 0.338140, current_train_items 87232.
I0302 18:59:52.656601 22732373614720 run.py:483] Algo bellman_ford step 2726 current loss 0.651139, current_train_items 87264.
I0302 18:59:52.679958 22732373614720 run.py:483] Algo bellman_ford step 2727 current loss 0.839771, current_train_items 87296.
I0302 18:59:52.710291 22732373614720 run.py:483] Algo bellman_ford step 2728 current loss 0.877032, current_train_items 87328.
I0302 18:59:52.743132 22732373614720 run.py:483] Algo bellman_ford step 2729 current loss 1.006923, current_train_items 87360.
I0302 18:59:52.762964 22732373614720 run.py:483] Algo bellman_ford step 2730 current loss 0.338998, current_train_items 87392.
I0302 18:59:52.778801 22732373614720 run.py:483] Algo bellman_ford step 2731 current loss 0.440295, current_train_items 87424.
I0302 18:59:52.800844 22732373614720 run.py:483] Algo bellman_ford step 2732 current loss 0.735879, current_train_items 87456.
I0302 18:59:52.830243 22732373614720 run.py:483] Algo bellman_ford step 2733 current loss 0.730104, current_train_items 87488.
I0302 18:59:52.863236 22732373614720 run.py:483] Algo bellman_ford step 2734 current loss 0.876902, current_train_items 87520.
I0302 18:59:52.883170 22732373614720 run.py:483] Algo bellman_ford step 2735 current loss 0.383731, current_train_items 87552.
I0302 18:59:52.898591 22732373614720 run.py:483] Algo bellman_ford step 2736 current loss 0.484048, current_train_items 87584.
I0302 18:59:52.921577 22732373614720 run.py:483] Algo bellman_ford step 2737 current loss 0.721060, current_train_items 87616.
I0302 18:59:52.951008 22732373614720 run.py:483] Algo bellman_ford step 2738 current loss 0.711834, current_train_items 87648.
I0302 18:59:52.983168 22732373614720 run.py:483] Algo bellman_ford step 2739 current loss 0.839781, current_train_items 87680.
I0302 18:59:53.002707 22732373614720 run.py:483] Algo bellman_ford step 2740 current loss 0.418429, current_train_items 87712.
I0302 18:59:53.018420 22732373614720 run.py:483] Algo bellman_ford step 2741 current loss 0.554684, current_train_items 87744.
I0302 18:59:53.041397 22732373614720 run.py:483] Algo bellman_ford step 2742 current loss 0.854679, current_train_items 87776.
I0302 18:59:53.072958 22732373614720 run.py:483] Algo bellman_ford step 2743 current loss 0.912943, current_train_items 87808.
I0302 18:59:53.105682 22732373614720 run.py:483] Algo bellman_ford step 2744 current loss 0.974241, current_train_items 87840.
I0302 18:59:53.124810 22732373614720 run.py:483] Algo bellman_ford step 2745 current loss 0.355486, current_train_items 87872.
I0302 18:59:53.140551 22732373614720 run.py:483] Algo bellman_ford step 2746 current loss 0.598886, current_train_items 87904.
I0302 18:59:53.163699 22732373614720 run.py:483] Algo bellman_ford step 2747 current loss 0.873513, current_train_items 87936.
I0302 18:59:53.194648 22732373614720 run.py:483] Algo bellman_ford step 2748 current loss 0.965216, current_train_items 87968.
I0302 18:59:53.227041 22732373614720 run.py:483] Algo bellman_ford step 2749 current loss 0.981013, current_train_items 88000.
I0302 18:59:53.246716 22732373614720 run.py:483] Algo bellman_ford step 2750 current loss 0.403991, current_train_items 88032.
I0302 18:59:53.254712 22732373614720 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0302 18:59:53.254823 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 18:59:53.271505 22732373614720 run.py:483] Algo bellman_ford step 2751 current loss 0.673007, current_train_items 88064.
I0302 18:59:53.295213 22732373614720 run.py:483] Algo bellman_ford step 2752 current loss 0.854308, current_train_items 88096.
I0302 18:59:53.326701 22732373614720 run.py:483] Algo bellman_ford step 2753 current loss 0.854077, current_train_items 88128.
I0302 18:59:53.358897 22732373614720 run.py:483] Algo bellman_ford step 2754 current loss 0.891174, current_train_items 88160.
I0302 18:59:53.378879 22732373614720 run.py:483] Algo bellman_ford step 2755 current loss 0.377036, current_train_items 88192.
I0302 18:59:53.394881 22732373614720 run.py:483] Algo bellman_ford step 2756 current loss 0.513328, current_train_items 88224.
I0302 18:59:53.417495 22732373614720 run.py:483] Algo bellman_ford step 2757 current loss 0.623261, current_train_items 88256.
I0302 18:59:53.448731 22732373614720 run.py:483] Algo bellman_ford step 2758 current loss 0.836437, current_train_items 88288.
I0302 18:59:53.484502 22732373614720 run.py:483] Algo bellman_ford step 2759 current loss 1.040211, current_train_items 88320.
I0302 18:59:53.504447 22732373614720 run.py:483] Algo bellman_ford step 2760 current loss 0.353743, current_train_items 88352.
I0302 18:59:53.520930 22732373614720 run.py:483] Algo bellman_ford step 2761 current loss 0.555057, current_train_items 88384.
I0302 18:59:53.544542 22732373614720 run.py:483] Algo bellman_ford step 2762 current loss 0.864023, current_train_items 88416.
I0302 18:59:53.575054 22732373614720 run.py:483] Algo bellman_ford step 2763 current loss 0.832194, current_train_items 88448.
I0302 18:59:53.608115 22732373614720 run.py:483] Algo bellman_ford step 2764 current loss 1.015408, current_train_items 88480.
I0302 18:59:53.627578 22732373614720 run.py:483] Algo bellman_ford step 2765 current loss 0.396112, current_train_items 88512.
I0302 18:59:53.643831 22732373614720 run.py:483] Algo bellman_ford step 2766 current loss 0.554139, current_train_items 88544.
I0302 18:59:53.667345 22732373614720 run.py:483] Algo bellman_ford step 2767 current loss 0.852121, current_train_items 88576.
I0302 18:59:53.698699 22732373614720 run.py:483] Algo bellman_ford step 2768 current loss 0.849237, current_train_items 88608.
I0302 18:59:53.731584 22732373614720 run.py:483] Algo bellman_ford step 2769 current loss 0.928431, current_train_items 88640.
I0302 18:59:53.751194 22732373614720 run.py:483] Algo bellman_ford step 2770 current loss 0.303188, current_train_items 88672.
I0302 18:59:53.767366 22732373614720 run.py:483] Algo bellman_ford step 2771 current loss 0.514884, current_train_items 88704.
I0302 18:59:53.791002 22732373614720 run.py:483] Algo bellman_ford step 2772 current loss 0.816014, current_train_items 88736.
I0302 18:59:53.820644 22732373614720 run.py:483] Algo bellman_ford step 2773 current loss 0.832273, current_train_items 88768.
I0302 18:59:53.853719 22732373614720 run.py:483] Algo bellman_ford step 2774 current loss 1.037076, current_train_items 88800.
I0302 18:59:53.873440 22732373614720 run.py:483] Algo bellman_ford step 2775 current loss 0.320106, current_train_items 88832.
I0302 18:59:53.889961 22732373614720 run.py:483] Algo bellman_ford step 2776 current loss 0.729990, current_train_items 88864.
I0302 18:59:53.912900 22732373614720 run.py:483] Algo bellman_ford step 2777 current loss 0.803575, current_train_items 88896.
I0302 18:59:53.942840 22732373614720 run.py:483] Algo bellman_ford step 2778 current loss 0.890986, current_train_items 88928.
I0302 18:59:53.976710 22732373614720 run.py:483] Algo bellman_ford step 2779 current loss 1.096347, current_train_items 88960.
I0302 18:59:53.996513 22732373614720 run.py:483] Algo bellman_ford step 2780 current loss 0.379068, current_train_items 88992.
I0302 18:59:54.012348 22732373614720 run.py:483] Algo bellman_ford step 2781 current loss 0.583453, current_train_items 89024.
I0302 18:59:54.035677 22732373614720 run.py:483] Algo bellman_ford step 2782 current loss 0.771492, current_train_items 89056.
I0302 18:59:54.066073 22732373614720 run.py:483] Algo bellman_ford step 2783 current loss 0.809710, current_train_items 89088.
I0302 18:59:54.099708 22732373614720 run.py:483] Algo bellman_ford step 2784 current loss 1.000102, current_train_items 89120.
I0302 18:59:54.119840 22732373614720 run.py:483] Algo bellman_ford step 2785 current loss 0.387820, current_train_items 89152.
I0302 18:59:54.135768 22732373614720 run.py:483] Algo bellman_ford step 2786 current loss 0.542047, current_train_items 89184.
I0302 18:59:54.157644 22732373614720 run.py:483] Algo bellman_ford step 2787 current loss 0.685302, current_train_items 89216.
I0302 18:59:54.187303 22732373614720 run.py:483] Algo bellman_ford step 2788 current loss 0.860637, current_train_items 89248.
I0302 18:59:54.217749 22732373614720 run.py:483] Algo bellman_ford step 2789 current loss 0.792735, current_train_items 89280.
I0302 18:59:54.237266 22732373614720 run.py:483] Algo bellman_ford step 2790 current loss 0.524874, current_train_items 89312.
I0302 18:59:54.253220 22732373614720 run.py:483] Algo bellman_ford step 2791 current loss 0.551501, current_train_items 89344.
I0302 18:59:54.276398 22732373614720 run.py:483] Algo bellman_ford step 2792 current loss 0.637717, current_train_items 89376.
I0302 18:59:54.306168 22732373614720 run.py:483] Algo bellman_ford step 2793 current loss 0.803534, current_train_items 89408.
I0302 18:59:54.341079 22732373614720 run.py:483] Algo bellman_ford step 2794 current loss 1.126203, current_train_items 89440.
I0302 18:59:54.360967 22732373614720 run.py:483] Algo bellman_ford step 2795 current loss 0.379616, current_train_items 89472.
I0302 18:59:54.377408 22732373614720 run.py:483] Algo bellman_ford step 2796 current loss 0.579541, current_train_items 89504.
I0302 18:59:54.400359 22732373614720 run.py:483] Algo bellman_ford step 2797 current loss 0.864459, current_train_items 89536.
I0302 18:59:54.431593 22732373614720 run.py:483] Algo bellman_ford step 2798 current loss 0.847785, current_train_items 89568.
I0302 18:59:54.464866 22732373614720 run.py:483] Algo bellman_ford step 2799 current loss 0.889025, current_train_items 89600.
I0302 18:59:54.484728 22732373614720 run.py:483] Algo bellman_ford step 2800 current loss 0.431779, current_train_items 89632.
I0302 18:59:54.492307 22732373614720 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0302 18:59:54.492415 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.923, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 18:59:54.522737 22732373614720 run.py:483] Algo bellman_ford step 2801 current loss 0.522957, current_train_items 89664.
I0302 18:59:54.547059 22732373614720 run.py:483] Algo bellman_ford step 2802 current loss 0.762710, current_train_items 89696.
I0302 18:59:54.578614 22732373614720 run.py:483] Algo bellman_ford step 2803 current loss 0.876405, current_train_items 89728.
I0302 18:59:54.612323 22732373614720 run.py:483] Algo bellman_ford step 2804 current loss 0.950116, current_train_items 89760.
I0302 18:59:54.632894 22732373614720 run.py:483] Algo bellman_ford step 2805 current loss 0.333443, current_train_items 89792.
I0302 18:59:54.649023 22732373614720 run.py:483] Algo bellman_ford step 2806 current loss 0.616440, current_train_items 89824.
I0302 18:59:54.671708 22732373614720 run.py:483] Algo bellman_ford step 2807 current loss 0.656064, current_train_items 89856.
I0302 18:59:54.702921 22732373614720 run.py:483] Algo bellman_ford step 2808 current loss 0.910608, current_train_items 89888.
I0302 18:59:54.736407 22732373614720 run.py:483] Algo bellman_ford step 2809 current loss 1.023522, current_train_items 89920.
I0302 18:59:54.756102 22732373614720 run.py:483] Algo bellman_ford step 2810 current loss 0.315622, current_train_items 89952.
I0302 18:59:54.772491 22732373614720 run.py:483] Algo bellman_ford step 2811 current loss 0.652400, current_train_items 89984.
I0302 18:59:54.794950 22732373614720 run.py:483] Algo bellman_ford step 2812 current loss 0.615352, current_train_items 90016.
I0302 18:59:54.826300 22732373614720 run.py:483] Algo bellman_ford step 2813 current loss 0.872294, current_train_items 90048.
I0302 18:59:54.858804 22732373614720 run.py:483] Algo bellman_ford step 2814 current loss 0.866105, current_train_items 90080.
I0302 18:59:54.878148 22732373614720 run.py:483] Algo bellman_ford step 2815 current loss 0.400418, current_train_items 90112.
I0302 18:59:54.894361 22732373614720 run.py:483] Algo bellman_ford step 2816 current loss 0.487781, current_train_items 90144.
I0302 18:59:54.918095 22732373614720 run.py:483] Algo bellman_ford step 2817 current loss 0.730357, current_train_items 90176.
I0302 18:59:54.945503 22732373614720 run.py:483] Algo bellman_ford step 2818 current loss 0.641261, current_train_items 90208.
I0302 18:59:54.978995 22732373614720 run.py:483] Algo bellman_ford step 2819 current loss 0.836082, current_train_items 90240.
I0302 18:59:54.998394 22732373614720 run.py:483] Algo bellman_ford step 2820 current loss 0.423575, current_train_items 90272.
I0302 18:59:55.014269 22732373614720 run.py:483] Algo bellman_ford step 2821 current loss 0.470703, current_train_items 90304.
I0302 18:59:55.037289 22732373614720 run.py:483] Algo bellman_ford step 2822 current loss 0.662013, current_train_items 90336.
I0302 18:59:55.067339 22732373614720 run.py:483] Algo bellman_ford step 2823 current loss 1.003111, current_train_items 90368.
I0302 18:59:55.102305 22732373614720 run.py:483] Algo bellman_ford step 2824 current loss 0.982665, current_train_items 90400.
I0302 18:59:55.121862 22732373614720 run.py:483] Algo bellman_ford step 2825 current loss 0.322648, current_train_items 90432.
I0302 18:59:55.137513 22732373614720 run.py:483] Algo bellman_ford step 2826 current loss 0.633119, current_train_items 90464.
I0302 18:59:55.161138 22732373614720 run.py:483] Algo bellman_ford step 2827 current loss 0.707690, current_train_items 90496.
I0302 18:59:55.191840 22732373614720 run.py:483] Algo bellman_ford step 2828 current loss 0.891564, current_train_items 90528.
I0302 18:59:55.223729 22732373614720 run.py:483] Algo bellman_ford step 2829 current loss 0.859836, current_train_items 90560.
I0302 18:59:55.243411 22732373614720 run.py:483] Algo bellman_ford step 2830 current loss 0.341276, current_train_items 90592.
I0302 18:59:55.259752 22732373614720 run.py:483] Algo bellman_ford step 2831 current loss 0.757387, current_train_items 90624.
I0302 18:59:55.282904 22732373614720 run.py:483] Algo bellman_ford step 2832 current loss 0.856806, current_train_items 90656.
I0302 18:59:55.312273 22732373614720 run.py:483] Algo bellman_ford step 2833 current loss 0.801688, current_train_items 90688.
I0302 18:59:55.345093 22732373614720 run.py:483] Algo bellman_ford step 2834 current loss 0.951749, current_train_items 90720.
I0302 18:59:55.364758 22732373614720 run.py:483] Algo bellman_ford step 2835 current loss 0.338614, current_train_items 90752.
I0302 18:59:55.380323 22732373614720 run.py:483] Algo bellman_ford step 2836 current loss 0.578849, current_train_items 90784.
I0302 18:59:55.402950 22732373614720 run.py:483] Algo bellman_ford step 2837 current loss 0.753270, current_train_items 90816.
I0302 18:59:55.433163 22732373614720 run.py:483] Algo bellman_ford step 2838 current loss 1.187030, current_train_items 90848.
I0302 18:59:55.468276 22732373614720 run.py:483] Algo bellman_ford step 2839 current loss 1.710755, current_train_items 90880.
I0302 18:59:55.488048 22732373614720 run.py:483] Algo bellman_ford step 2840 current loss 0.362053, current_train_items 90912.
I0302 18:59:55.503669 22732373614720 run.py:483] Algo bellman_ford step 2841 current loss 0.554489, current_train_items 90944.
I0302 18:59:55.526805 22732373614720 run.py:483] Algo bellman_ford step 2842 current loss 0.778087, current_train_items 90976.
I0302 18:59:55.557954 22732373614720 run.py:483] Algo bellman_ford step 2843 current loss 0.901466, current_train_items 91008.
I0302 18:59:55.591470 22732373614720 run.py:483] Algo bellman_ford step 2844 current loss 1.101411, current_train_items 91040.
I0302 18:59:55.611305 22732373614720 run.py:483] Algo bellman_ford step 2845 current loss 0.392008, current_train_items 91072.
I0302 18:59:55.627652 22732373614720 run.py:483] Algo bellman_ford step 2846 current loss 0.596785, current_train_items 91104.
I0302 18:59:55.650688 22732373614720 run.py:483] Algo bellman_ford step 2847 current loss 0.748385, current_train_items 91136.
I0302 18:59:55.681235 22732373614720 run.py:483] Algo bellman_ford step 2848 current loss 0.757121, current_train_items 91168.
I0302 18:59:55.715551 22732373614720 run.py:483] Algo bellman_ford step 2849 current loss 0.968168, current_train_items 91200.
I0302 18:59:55.734893 22732373614720 run.py:483] Algo bellman_ford step 2850 current loss 0.401509, current_train_items 91232.
I0302 18:59:55.744709 22732373614720 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0302 18:59:55.744992 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.925, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 18:59:55.763528 22732373614720 run.py:483] Algo bellman_ford step 2851 current loss 0.498148, current_train_items 91264.
I0302 18:59:55.787294 22732373614720 run.py:483] Algo bellman_ford step 2852 current loss 0.767934, current_train_items 91296.
I0302 18:59:55.819740 22732373614720 run.py:483] Algo bellman_ford step 2853 current loss 0.823374, current_train_items 91328.
I0302 18:59:55.853631 22732373614720 run.py:483] Algo bellman_ford step 2854 current loss 0.919737, current_train_items 91360.
I0302 18:59:55.873970 22732373614720 run.py:483] Algo bellman_ford step 2855 current loss 0.404632, current_train_items 91392.
I0302 18:59:55.889958 22732373614720 run.py:483] Algo bellman_ford step 2856 current loss 0.593383, current_train_items 91424.
I0302 18:59:55.913377 22732373614720 run.py:483] Algo bellman_ford step 2857 current loss 0.796075, current_train_items 91456.
I0302 18:59:55.942080 22732373614720 run.py:483] Algo bellman_ford step 2858 current loss 0.647462, current_train_items 91488.
I0302 18:59:55.973920 22732373614720 run.py:483] Algo bellman_ford step 2859 current loss 0.905899, current_train_items 91520.
I0302 18:59:55.994055 22732373614720 run.py:483] Algo bellman_ford step 2860 current loss 0.416694, current_train_items 91552.
I0302 18:59:56.010357 22732373614720 run.py:483] Algo bellman_ford step 2861 current loss 0.518033, current_train_items 91584.
I0302 18:59:56.033348 22732373614720 run.py:483] Algo bellman_ford step 2862 current loss 0.697474, current_train_items 91616.
I0302 18:59:56.064394 22732373614720 run.py:483] Algo bellman_ford step 2863 current loss 0.889527, current_train_items 91648.
I0302 18:59:56.094558 22732373614720 run.py:483] Algo bellman_ford step 2864 current loss 0.827444, current_train_items 91680.
I0302 18:59:56.114378 22732373614720 run.py:483] Algo bellman_ford step 2865 current loss 0.542646, current_train_items 91712.
I0302 18:59:56.130503 22732373614720 run.py:483] Algo bellman_ford step 2866 current loss 0.716574, current_train_items 91744.
I0302 18:59:56.153393 22732373614720 run.py:483] Algo bellman_ford step 2867 current loss 0.802956, current_train_items 91776.
I0302 18:59:56.183236 22732373614720 run.py:483] Algo bellman_ford step 2868 current loss 0.762262, current_train_items 91808.
I0302 18:59:56.216682 22732373614720 run.py:483] Algo bellman_ford step 2869 current loss 0.895878, current_train_items 91840.
I0302 18:59:56.236943 22732373614720 run.py:483] Algo bellman_ford step 2870 current loss 0.357965, current_train_items 91872.
I0302 18:59:56.253074 22732373614720 run.py:483] Algo bellman_ford step 2871 current loss 0.484412, current_train_items 91904.
I0302 18:59:56.276533 22732373614720 run.py:483] Algo bellman_ford step 2872 current loss 0.669278, current_train_items 91936.
I0302 18:59:56.307781 22732373614720 run.py:483] Algo bellman_ford step 2873 current loss 0.829097, current_train_items 91968.
I0302 18:59:56.339592 22732373614720 run.py:483] Algo bellman_ford step 2874 current loss 1.074632, current_train_items 92000.
I0302 18:59:56.359264 22732373614720 run.py:483] Algo bellman_ford step 2875 current loss 0.460816, current_train_items 92032.
I0302 18:59:56.375309 22732373614720 run.py:483] Algo bellman_ford step 2876 current loss 0.669182, current_train_items 92064.
I0302 18:59:56.397867 22732373614720 run.py:483] Algo bellman_ford step 2877 current loss 0.670516, current_train_items 92096.
I0302 18:59:56.429214 22732373614720 run.py:483] Algo bellman_ford step 2878 current loss 0.946424, current_train_items 92128.
I0302 18:59:56.461365 22732373614720 run.py:483] Algo bellman_ford step 2879 current loss 0.915839, current_train_items 92160.
I0302 18:59:56.481173 22732373614720 run.py:483] Algo bellman_ford step 2880 current loss 0.446296, current_train_items 92192.
I0302 18:59:56.497117 22732373614720 run.py:483] Algo bellman_ford step 2881 current loss 0.590918, current_train_items 92224.
I0302 18:59:56.520301 22732373614720 run.py:483] Algo bellman_ford step 2882 current loss 0.692764, current_train_items 92256.
I0302 18:59:56.550937 22732373614720 run.py:483] Algo bellman_ford step 2883 current loss 0.805477, current_train_items 92288.
I0302 18:59:56.581770 22732373614720 run.py:483] Algo bellman_ford step 2884 current loss 0.970321, current_train_items 92320.
I0302 18:59:56.601380 22732373614720 run.py:483] Algo bellman_ford step 2885 current loss 0.326709, current_train_items 92352.
I0302 18:59:56.617324 22732373614720 run.py:483] Algo bellman_ford step 2886 current loss 0.537803, current_train_items 92384.
I0302 18:59:56.640416 22732373614720 run.py:483] Algo bellman_ford step 2887 current loss 0.749401, current_train_items 92416.
I0302 18:59:56.669703 22732373614720 run.py:483] Algo bellman_ford step 2888 current loss 0.757925, current_train_items 92448.
I0302 18:59:56.703071 22732373614720 run.py:483] Algo bellman_ford step 2889 current loss 0.981005, current_train_items 92480.
I0302 18:59:56.722999 22732373614720 run.py:483] Algo bellman_ford step 2890 current loss 0.396287, current_train_items 92512.
I0302 18:59:56.739076 22732373614720 run.py:483] Algo bellman_ford step 2891 current loss 0.554093, current_train_items 92544.
I0302 18:59:56.763134 22732373614720 run.py:483] Algo bellman_ford step 2892 current loss 0.737584, current_train_items 92576.
I0302 18:59:56.792234 22732373614720 run.py:483] Algo bellman_ford step 2893 current loss 0.739803, current_train_items 92608.
I0302 18:59:56.826101 22732373614720 run.py:483] Algo bellman_ford step 2894 current loss 0.903775, current_train_items 92640.
I0302 18:59:56.845420 22732373614720 run.py:483] Algo bellman_ford step 2895 current loss 0.370820, current_train_items 92672.
I0302 18:59:56.861888 22732373614720 run.py:483] Algo bellman_ford step 2896 current loss 0.604437, current_train_items 92704.
I0302 18:59:56.883632 22732373614720 run.py:483] Algo bellman_ford step 2897 current loss 0.686582, current_train_items 92736.
I0302 18:59:56.913317 22732373614720 run.py:483] Algo bellman_ford step 2898 current loss 0.710093, current_train_items 92768.
I0302 18:59:56.943668 22732373614720 run.py:483] Algo bellman_ford step 2899 current loss 0.797378, current_train_items 92800.
I0302 18:59:56.963094 22732373614720 run.py:483] Algo bellman_ford step 2900 current loss 0.304048, current_train_items 92832.
I0302 18:59:56.970949 22732373614720 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0302 18:59:56.971056 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.925, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:59:56.987270 22732373614720 run.py:483] Algo bellman_ford step 2901 current loss 0.463148, current_train_items 92864.
I0302 18:59:57.010244 22732373614720 run.py:483] Algo bellman_ford step 2902 current loss 0.746957, current_train_items 92896.
I0302 18:59:57.040961 22732373614720 run.py:483] Algo bellman_ford step 2903 current loss 0.792403, current_train_items 92928.
I0302 18:59:57.074675 22732373614720 run.py:483] Algo bellman_ford step 2904 current loss 0.997087, current_train_items 92960.
I0302 18:59:57.094410 22732373614720 run.py:483] Algo bellman_ford step 2905 current loss 0.420434, current_train_items 92992.
I0302 18:59:57.110198 22732373614720 run.py:483] Algo bellman_ford step 2906 current loss 0.554407, current_train_items 93024.
I0302 18:59:57.133494 22732373614720 run.py:483] Algo bellman_ford step 2907 current loss 0.859921, current_train_items 93056.
I0302 18:59:57.165554 22732373614720 run.py:483] Algo bellman_ford step 2908 current loss 0.998494, current_train_items 93088.
I0302 18:59:57.198848 22732373614720 run.py:483] Algo bellman_ford step 2909 current loss 0.900744, current_train_items 93120.
I0302 18:59:57.218210 22732373614720 run.py:483] Algo bellman_ford step 2910 current loss 0.408592, current_train_items 93152.
I0302 18:59:57.234745 22732373614720 run.py:483] Algo bellman_ford step 2911 current loss 0.628931, current_train_items 93184.
I0302 18:59:57.257892 22732373614720 run.py:483] Algo bellman_ford step 2912 current loss 0.643254, current_train_items 93216.
I0302 18:59:57.288007 22732373614720 run.py:483] Algo bellman_ford step 2913 current loss 0.849953, current_train_items 93248.
I0302 18:59:57.322853 22732373614720 run.py:483] Algo bellman_ford step 2914 current loss 1.084663, current_train_items 93280.
I0302 18:59:57.342298 22732373614720 run.py:483] Algo bellman_ford step 2915 current loss 0.378311, current_train_items 93312.
I0302 18:59:57.358355 22732373614720 run.py:483] Algo bellman_ford step 2916 current loss 0.560439, current_train_items 93344.
I0302 18:59:57.380954 22732373614720 run.py:483] Algo bellman_ford step 2917 current loss 0.716791, current_train_items 93376.
I0302 18:59:57.412616 22732373614720 run.py:483] Algo bellman_ford step 2918 current loss 1.036118, current_train_items 93408.
I0302 18:59:57.447210 22732373614720 run.py:483] Algo bellman_ford step 2919 current loss 1.083363, current_train_items 93440.
I0302 18:59:57.466805 22732373614720 run.py:483] Algo bellman_ford step 2920 current loss 0.388384, current_train_items 93472.
I0302 18:59:57.482805 22732373614720 run.py:483] Algo bellman_ford step 2921 current loss 0.679874, current_train_items 93504.
I0302 18:59:57.506668 22732373614720 run.py:483] Algo bellman_ford step 2922 current loss 0.821624, current_train_items 93536.
I0302 18:59:57.536031 22732373614720 run.py:483] Algo bellman_ford step 2923 current loss 0.803311, current_train_items 93568.
I0302 18:59:57.570826 22732373614720 run.py:483] Algo bellman_ford step 2924 current loss 1.171384, current_train_items 93600.
I0302 18:59:57.590388 22732373614720 run.py:483] Algo bellman_ford step 2925 current loss 0.398030, current_train_items 93632.
I0302 18:59:57.606209 22732373614720 run.py:483] Algo bellman_ford step 2926 current loss 0.629249, current_train_items 93664.
I0302 18:59:57.630134 22732373614720 run.py:483] Algo bellman_ford step 2927 current loss 0.863591, current_train_items 93696.
I0302 18:59:57.661085 22732373614720 run.py:483] Algo bellman_ford step 2928 current loss 0.793137, current_train_items 93728.
I0302 18:59:57.694727 22732373614720 run.py:483] Algo bellman_ford step 2929 current loss 0.929262, current_train_items 93760.
I0302 18:59:57.714498 22732373614720 run.py:483] Algo bellman_ford step 2930 current loss 0.372139, current_train_items 93792.
I0302 18:59:57.730332 22732373614720 run.py:483] Algo bellman_ford step 2931 current loss 0.534550, current_train_items 93824.
I0302 18:59:57.754675 22732373614720 run.py:483] Algo bellman_ford step 2932 current loss 0.905587, current_train_items 93856.
I0302 18:59:57.784485 22732373614720 run.py:483] Algo bellman_ford step 2933 current loss 0.934274, current_train_items 93888.
I0302 18:59:57.816660 22732373614720 run.py:483] Algo bellman_ford step 2934 current loss 0.864210, current_train_items 93920.
I0302 18:59:57.836468 22732373614720 run.py:483] Algo bellman_ford step 2935 current loss 0.432565, current_train_items 93952.
I0302 18:59:57.852311 22732373614720 run.py:483] Algo bellman_ford step 2936 current loss 0.523481, current_train_items 93984.
I0302 18:59:57.875737 22732373614720 run.py:483] Algo bellman_ford step 2937 current loss 0.837716, current_train_items 94016.
I0302 18:59:57.904187 22732373614720 run.py:483] Algo bellman_ford step 2938 current loss 0.765273, current_train_items 94048.
I0302 18:59:57.936978 22732373614720 run.py:483] Algo bellman_ford step 2939 current loss 1.002336, current_train_items 94080.
I0302 18:59:57.956268 22732373614720 run.py:483] Algo bellman_ford step 2940 current loss 0.381855, current_train_items 94112.
I0302 18:59:57.972573 22732373614720 run.py:483] Algo bellman_ford step 2941 current loss 0.644499, current_train_items 94144.
I0302 18:59:57.996403 22732373614720 run.py:483] Algo bellman_ford step 2942 current loss 0.759105, current_train_items 94176.
I0302 18:59:58.027484 22732373614720 run.py:483] Algo bellman_ford step 2943 current loss 0.857099, current_train_items 94208.
I0302 18:59:58.061763 22732373614720 run.py:483] Algo bellman_ford step 2944 current loss 0.895640, current_train_items 94240.
I0302 18:59:58.081060 22732373614720 run.py:483] Algo bellman_ford step 2945 current loss 0.514936, current_train_items 94272.
I0302 18:59:58.097518 22732373614720 run.py:483] Algo bellman_ford step 2946 current loss 0.604089, current_train_items 94304.
I0302 18:59:58.120718 22732373614720 run.py:483] Algo bellman_ford step 2947 current loss 0.621629, current_train_items 94336.
I0302 18:59:58.150690 22732373614720 run.py:483] Algo bellman_ford step 2948 current loss 0.822608, current_train_items 94368.
I0302 18:59:58.183379 22732373614720 run.py:483] Algo bellman_ford step 2949 current loss 0.904402, current_train_items 94400.
I0302 18:59:58.202707 22732373614720 run.py:483] Algo bellman_ford step 2950 current loss 0.389447, current_train_items 94432.
I0302 18:59:58.210835 22732373614720 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.87109375, 'score': 0.87109375, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0302 18:59:58.210945 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.925, current avg val score is 0.871, val scores are: bellman_ford: 0.871
I0302 18:59:58.227680 22732373614720 run.py:483] Algo bellman_ford step 2951 current loss 0.647666, current_train_items 94464.
I0302 18:59:58.249894 22732373614720 run.py:483] Algo bellman_ford step 2952 current loss 0.547019, current_train_items 94496.
I0302 18:59:58.279924 22732373614720 run.py:483] Algo bellman_ford step 2953 current loss 0.788689, current_train_items 94528.
I0302 18:59:58.313445 22732373614720 run.py:483] Algo bellman_ford step 2954 current loss 0.915085, current_train_items 94560.
I0302 18:59:58.333325 22732373614720 run.py:483] Algo bellman_ford step 2955 current loss 0.361282, current_train_items 94592.
I0302 18:59:58.348643 22732373614720 run.py:483] Algo bellman_ford step 2956 current loss 0.608070, current_train_items 94624.
I0302 18:59:58.372792 22732373614720 run.py:483] Algo bellman_ford step 2957 current loss 0.890642, current_train_items 94656.
I0302 18:59:58.403994 22732373614720 run.py:483] Algo bellman_ford step 2958 current loss 0.818260, current_train_items 94688.
I0302 18:59:58.437448 22732373614720 run.py:483] Algo bellman_ford step 2959 current loss 0.965884, current_train_items 94720.
I0302 18:59:58.457132 22732373614720 run.py:483] Algo bellman_ford step 2960 current loss 0.387470, current_train_items 94752.
I0302 18:59:58.473441 22732373614720 run.py:483] Algo bellman_ford step 2961 current loss 0.487334, current_train_items 94784.
I0302 18:59:58.496004 22732373614720 run.py:483] Algo bellman_ford step 2962 current loss 0.831528, current_train_items 94816.
I0302 18:59:58.525107 22732373614720 run.py:483] Algo bellman_ford step 2963 current loss 0.873853, current_train_items 94848.
I0302 18:59:58.559286 22732373614720 run.py:483] Algo bellman_ford step 2964 current loss 0.910499, current_train_items 94880.
I0302 18:59:58.578522 22732373614720 run.py:483] Algo bellman_ford step 2965 current loss 0.316239, current_train_items 94912.
I0302 18:59:58.594455 22732373614720 run.py:483] Algo bellman_ford step 2966 current loss 0.586521, current_train_items 94944.
I0302 18:59:58.617514 22732373614720 run.py:483] Algo bellman_ford step 2967 current loss 0.742839, current_train_items 94976.
I0302 18:59:58.647686 22732373614720 run.py:483] Algo bellman_ford step 2968 current loss 0.747597, current_train_items 95008.
I0302 18:59:58.681417 22732373614720 run.py:483] Algo bellman_ford step 2969 current loss 0.946836, current_train_items 95040.
I0302 18:59:58.701219 22732373614720 run.py:483] Algo bellman_ford step 2970 current loss 0.380084, current_train_items 95072.
I0302 18:59:58.717010 22732373614720 run.py:483] Algo bellman_ford step 2971 current loss 0.549281, current_train_items 95104.
I0302 18:59:58.739912 22732373614720 run.py:483] Algo bellman_ford step 2972 current loss 0.776458, current_train_items 95136.
I0302 18:59:58.769606 22732373614720 run.py:483] Algo bellman_ford step 2973 current loss 0.783547, current_train_items 95168.
I0302 18:59:58.805252 22732373614720 run.py:483] Algo bellman_ford step 2974 current loss 1.120972, current_train_items 95200.
I0302 18:59:58.824876 22732373614720 run.py:483] Algo bellman_ford step 2975 current loss 0.405088, current_train_items 95232.
I0302 18:59:58.841727 22732373614720 run.py:483] Algo bellman_ford step 2976 current loss 0.586872, current_train_items 95264.
I0302 18:59:58.864491 22732373614720 run.py:483] Algo bellman_ford step 2977 current loss 0.716767, current_train_items 95296.
I0302 18:59:58.894043 22732373614720 run.py:483] Algo bellman_ford step 2978 current loss 0.768914, current_train_items 95328.
I0302 18:59:58.928420 22732373614720 run.py:483] Algo bellman_ford step 2979 current loss 1.063100, current_train_items 95360.
I0302 18:59:58.947656 22732373614720 run.py:483] Algo bellman_ford step 2980 current loss 0.336087, current_train_items 95392.
I0302 18:59:58.963492 22732373614720 run.py:483] Algo bellman_ford step 2981 current loss 0.512997, current_train_items 95424.
I0302 18:59:58.987024 22732373614720 run.py:483] Algo bellman_ford step 2982 current loss 0.790196, current_train_items 95456.
I0302 18:59:59.019397 22732373614720 run.py:483] Algo bellman_ford step 2983 current loss 0.847447, current_train_items 95488.
I0302 18:59:59.052313 22732373614720 run.py:483] Algo bellman_ford step 2984 current loss 0.959188, current_train_items 95520.
I0302 18:59:59.072167 22732373614720 run.py:483] Algo bellman_ford step 2985 current loss 0.343164, current_train_items 95552.
I0302 18:59:59.088549 22732373614720 run.py:483] Algo bellman_ford step 2986 current loss 0.589651, current_train_items 95584.
I0302 18:59:59.112190 22732373614720 run.py:483] Algo bellman_ford step 2987 current loss 0.672089, current_train_items 95616.
I0302 18:59:59.142237 22732373614720 run.py:483] Algo bellman_ford step 2988 current loss 0.732794, current_train_items 95648.
I0302 18:59:59.175550 22732373614720 run.py:483] Algo bellman_ford step 2989 current loss 0.900354, current_train_items 95680.
I0302 18:59:59.195394 22732373614720 run.py:483] Algo bellman_ford step 2990 current loss 0.440661, current_train_items 95712.
I0302 18:59:59.212045 22732373614720 run.py:483] Algo bellman_ford step 2991 current loss 0.716089, current_train_items 95744.
I0302 18:59:59.235448 22732373614720 run.py:483] Algo bellman_ford step 2992 current loss 0.812670, current_train_items 95776.
I0302 18:59:59.266405 22732373614720 run.py:483] Algo bellman_ford step 2993 current loss 0.910918, current_train_items 95808.
I0302 18:59:59.300804 22732373614720 run.py:483] Algo bellman_ford step 2994 current loss 0.995329, current_train_items 95840.
I0302 18:59:59.320362 22732373614720 run.py:483] Algo bellman_ford step 2995 current loss 0.313672, current_train_items 95872.
I0302 18:59:59.336537 22732373614720 run.py:483] Algo bellman_ford step 2996 current loss 0.541662, current_train_items 95904.
I0302 18:59:59.361227 22732373614720 run.py:483] Algo bellman_ford step 2997 current loss 0.821082, current_train_items 95936.
I0302 18:59:59.392269 22732373614720 run.py:483] Algo bellman_ford step 2998 current loss 0.800538, current_train_items 95968.
I0302 18:59:59.424478 22732373614720 run.py:483] Algo bellman_ford step 2999 current loss 0.964380, current_train_items 96000.
I0302 18:59:59.444028 22732373614720 run.py:483] Algo bellman_ford step 3000 current loss 0.379719, current_train_items 96032.
I0302 18:59:59.451726 22732373614720 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.875, 'score': 0.875, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0302 18:59:59.451835 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.925, current avg val score is 0.875, val scores are: bellman_ford: 0.875
I0302 18:59:59.468362 22732373614720 run.py:483] Algo bellman_ford step 3001 current loss 0.676220, current_train_items 96064.
I0302 18:59:59.491900 22732373614720 run.py:483] Algo bellman_ford step 3002 current loss 0.682689, current_train_items 96096.
I0302 18:59:59.523080 22732373614720 run.py:483] Algo bellman_ford step 3003 current loss 0.958398, current_train_items 96128.
I0302 18:59:59.555982 22732373614720 run.py:483] Algo bellman_ford step 3004 current loss 0.777991, current_train_items 96160.
I0302 18:59:59.575915 22732373614720 run.py:483] Algo bellman_ford step 3005 current loss 0.313855, current_train_items 96192.
I0302 18:59:59.591568 22732373614720 run.py:483] Algo bellman_ford step 3006 current loss 0.542833, current_train_items 96224.
I0302 18:59:59.615856 22732373614720 run.py:483] Algo bellman_ford step 3007 current loss 1.091899, current_train_items 96256.
I0302 18:59:59.646978 22732373614720 run.py:483] Algo bellman_ford step 3008 current loss 1.405943, current_train_items 96288.
I0302 18:59:59.682735 22732373614720 run.py:483] Algo bellman_ford step 3009 current loss 1.456654, current_train_items 96320.
I0302 18:59:59.702023 22732373614720 run.py:483] Algo bellman_ford step 3010 current loss 0.349270, current_train_items 96352.
I0302 18:59:59.718057 22732373614720 run.py:483] Algo bellman_ford step 3011 current loss 0.542063, current_train_items 96384.
I0302 18:59:59.741204 22732373614720 run.py:483] Algo bellman_ford step 3012 current loss 0.758360, current_train_items 96416.
I0302 18:59:59.771140 22732373614720 run.py:483] Algo bellman_ford step 3013 current loss 0.782466, current_train_items 96448.
I0302 18:59:59.805671 22732373614720 run.py:483] Algo bellman_ford step 3014 current loss 0.917238, current_train_items 96480.
I0302 18:59:59.825372 22732373614720 run.py:483] Algo bellman_ford step 3015 current loss 0.411622, current_train_items 96512.
I0302 18:59:59.841743 22732373614720 run.py:483] Algo bellman_ford step 3016 current loss 0.648776, current_train_items 96544.
I0302 18:59:59.865082 22732373614720 run.py:483] Algo bellman_ford step 3017 current loss 0.900294, current_train_items 96576.
I0302 18:59:59.894884 22732373614720 run.py:483] Algo bellman_ford step 3018 current loss 0.809932, current_train_items 96608.
I0302 18:59:59.926923 22732373614720 run.py:483] Algo bellman_ford step 3019 current loss 0.843875, current_train_items 96640.
I0302 18:59:59.946517 22732373614720 run.py:483] Algo bellman_ford step 3020 current loss 0.365468, current_train_items 96672.
I0302 18:59:59.962089 22732373614720 run.py:483] Algo bellman_ford step 3021 current loss 0.531723, current_train_items 96704.
I0302 18:59:59.986465 22732373614720 run.py:483] Algo bellman_ford step 3022 current loss 0.786848, current_train_items 96736.
I0302 19:00:00.017146 22732373614720 run.py:483] Algo bellman_ford step 3023 current loss 0.853753, current_train_items 96768.
I0302 19:00:00.050340 22732373614720 run.py:483] Algo bellman_ford step 3024 current loss 0.936483, current_train_items 96800.
I0302 19:00:00.069891 22732373614720 run.py:483] Algo bellman_ford step 3025 current loss 0.320184, current_train_items 96832.
I0302 19:00:00.085756 22732373614720 run.py:483] Algo bellman_ford step 3026 current loss 0.603413, current_train_items 96864.
I0302 19:00:00.107617 22732373614720 run.py:483] Algo bellman_ford step 3027 current loss 0.621616, current_train_items 96896.
I0302 19:00:00.137777 22732373614720 run.py:483] Algo bellman_ford step 3028 current loss 0.749483, current_train_items 96928.
I0302 19:00:00.169720 22732373614720 run.py:483] Algo bellman_ford step 3029 current loss 0.869642, current_train_items 96960.
I0302 19:00:00.189198 22732373614720 run.py:483] Algo bellman_ford step 3030 current loss 0.347678, current_train_items 96992.
I0302 19:00:00.204860 22732373614720 run.py:483] Algo bellman_ford step 3031 current loss 0.467721, current_train_items 97024.
I0302 19:00:00.228704 22732373614720 run.py:483] Algo bellman_ford step 3032 current loss 0.730890, current_train_items 97056.
I0302 19:00:00.259992 22732373614720 run.py:483] Algo bellman_ford step 3033 current loss 0.833535, current_train_items 97088.
I0302 19:00:00.293933 22732373614720 run.py:483] Algo bellman_ford step 3034 current loss 0.822945, current_train_items 97120.
I0302 19:00:00.313581 22732373614720 run.py:483] Algo bellman_ford step 3035 current loss 0.370969, current_train_items 97152.
I0302 19:00:00.329959 22732373614720 run.py:483] Algo bellman_ford step 3036 current loss 0.566723, current_train_items 97184.
I0302 19:00:00.352906 22732373614720 run.py:483] Algo bellman_ford step 3037 current loss 0.731148, current_train_items 97216.
I0302 19:00:00.383734 22732373614720 run.py:483] Algo bellman_ford step 3038 current loss 0.783511, current_train_items 97248.
I0302 19:00:00.416043 22732373614720 run.py:483] Algo bellman_ford step 3039 current loss 0.900434, current_train_items 97280.
I0302 19:00:00.435593 22732373614720 run.py:483] Algo bellman_ford step 3040 current loss 0.346311, current_train_items 97312.
I0302 19:00:00.451554 22732373614720 run.py:483] Algo bellman_ford step 3041 current loss 0.503422, current_train_items 97344.
I0302 19:00:00.474545 22732373614720 run.py:483] Algo bellman_ford step 3042 current loss 0.697854, current_train_items 97376.
I0302 19:00:00.504858 22732373614720 run.py:483] Algo bellman_ford step 3043 current loss 0.790895, current_train_items 97408.
I0302 19:00:00.536554 22732373614720 run.py:483] Algo bellman_ford step 3044 current loss 0.894629, current_train_items 97440.
I0302 19:00:00.556056 22732373614720 run.py:483] Algo bellman_ford step 3045 current loss 0.397365, current_train_items 97472.
I0302 19:00:00.572012 22732373614720 run.py:483] Algo bellman_ford step 3046 current loss 0.609707, current_train_items 97504.
I0302 19:00:00.595425 22732373614720 run.py:483] Algo bellman_ford step 3047 current loss 0.818010, current_train_items 97536.
I0302 19:00:00.626559 22732373614720 run.py:483] Algo bellman_ford step 3048 current loss 0.827372, current_train_items 97568.
I0302 19:00:00.659369 22732373614720 run.py:483] Algo bellman_ford step 3049 current loss 0.861572, current_train_items 97600.
I0302 19:00:00.678962 22732373614720 run.py:483] Algo bellman_ford step 3050 current loss 0.343871, current_train_items 97632.
I0302 19:00:00.686884 22732373614720 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.8916015625, 'score': 0.8916015625, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0302 19:00:00.686995 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.925, current avg val score is 0.892, val scores are: bellman_ford: 0.892
I0302 19:00:00.703631 22732373614720 run.py:483] Algo bellman_ford step 3051 current loss 0.560092, current_train_items 97664.
I0302 19:00:00.727118 22732373614720 run.py:483] Algo bellman_ford step 3052 current loss 0.793527, current_train_items 97696.
I0302 19:00:00.758674 22732373614720 run.py:483] Algo bellman_ford step 3053 current loss 0.752189, current_train_items 97728.
I0302 19:00:00.790076 22732373614720 run.py:483] Algo bellman_ford step 3054 current loss 0.789448, current_train_items 97760.
I0302 19:00:00.810015 22732373614720 run.py:483] Algo bellman_ford step 3055 current loss 0.412001, current_train_items 97792.
I0302 19:00:00.825604 22732373614720 run.py:483] Algo bellman_ford step 3056 current loss 0.547934, current_train_items 97824.
I0302 19:00:00.848672 22732373614720 run.py:483] Algo bellman_ford step 3057 current loss 0.673755, current_train_items 97856.
I0302 19:00:00.879291 22732373614720 run.py:483] Algo bellman_ford step 3058 current loss 0.754117, current_train_items 97888.
I0302 19:00:00.911232 22732373614720 run.py:483] Algo bellman_ford step 3059 current loss 0.915967, current_train_items 97920.
I0302 19:00:00.930671 22732373614720 run.py:483] Algo bellman_ford step 3060 current loss 0.392393, current_train_items 97952.
I0302 19:00:00.946962 22732373614720 run.py:483] Algo bellman_ford step 3061 current loss 0.615355, current_train_items 97984.
I0302 19:00:00.969353 22732373614720 run.py:483] Algo bellman_ford step 3062 current loss 0.674464, current_train_items 98016.
I0302 19:00:01.000626 22732373614720 run.py:483] Algo bellman_ford step 3063 current loss 0.963312, current_train_items 98048.
I0302 19:00:01.034517 22732373614720 run.py:483] Algo bellman_ford step 3064 current loss 1.047387, current_train_items 98080.
I0302 19:00:01.053902 22732373614720 run.py:483] Algo bellman_ford step 3065 current loss 0.383633, current_train_items 98112.
I0302 19:00:01.069792 22732373614720 run.py:483] Algo bellman_ford step 3066 current loss 0.621622, current_train_items 98144.
I0302 19:00:01.093714 22732373614720 run.py:483] Algo bellman_ford step 3067 current loss 0.862891, current_train_items 98176.
I0302 19:00:01.123644 22732373614720 run.py:483] Algo bellman_ford step 3068 current loss 0.881521, current_train_items 98208.
I0302 19:00:01.159206 22732373614720 run.py:483] Algo bellman_ford step 3069 current loss 1.102805, current_train_items 98240.
I0302 19:00:01.179105 22732373614720 run.py:483] Algo bellman_ford step 3070 current loss 0.321823, current_train_items 98272.
I0302 19:00:01.195184 22732373614720 run.py:483] Algo bellman_ford step 3071 current loss 0.559426, current_train_items 98304.
I0302 19:00:01.217481 22732373614720 run.py:483] Algo bellman_ford step 3072 current loss 0.667455, current_train_items 98336.
I0302 19:00:01.247249 22732373614720 run.py:483] Algo bellman_ford step 3073 current loss 0.756977, current_train_items 98368.
I0302 19:00:01.281850 22732373614720 run.py:483] Algo bellman_ford step 3074 current loss 1.160989, current_train_items 98400.
I0302 19:00:01.301477 22732373614720 run.py:483] Algo bellman_ford step 3075 current loss 0.360052, current_train_items 98432.
I0302 19:00:01.317856 22732373614720 run.py:483] Algo bellman_ford step 3076 current loss 0.638330, current_train_items 98464.
I0302 19:00:01.340749 22732373614720 run.py:483] Algo bellman_ford step 3077 current loss 0.718979, current_train_items 98496.
I0302 19:00:01.371442 22732373614720 run.py:483] Algo bellman_ford step 3078 current loss 0.832029, current_train_items 98528.
I0302 19:00:01.402742 22732373614720 run.py:483] Algo bellman_ford step 3079 current loss 0.872991, current_train_items 98560.
I0302 19:00:01.422181 22732373614720 run.py:483] Algo bellman_ford step 3080 current loss 0.361230, current_train_items 98592.
I0302 19:00:01.438297 22732373614720 run.py:483] Algo bellman_ford step 3081 current loss 0.613526, current_train_items 98624.
I0302 19:00:01.461256 22732373614720 run.py:483] Algo bellman_ford step 3082 current loss 0.680111, current_train_items 98656.
I0302 19:00:01.491395 22732373614720 run.py:483] Algo bellman_ford step 3083 current loss 0.856984, current_train_items 98688.
I0302 19:00:01.524504 22732373614720 run.py:483] Algo bellman_ford step 3084 current loss 0.982520, current_train_items 98720.
I0302 19:00:01.544095 22732373614720 run.py:483] Algo bellman_ford step 3085 current loss 0.375060, current_train_items 98752.
I0302 19:00:01.560348 22732373614720 run.py:483] Algo bellman_ford step 3086 current loss 0.677368, current_train_items 98784.
I0302 19:00:01.582094 22732373614720 run.py:483] Algo bellman_ford step 3087 current loss 0.668125, current_train_items 98816.
I0302 19:00:01.612411 22732373614720 run.py:483] Algo bellman_ford step 3088 current loss 0.819749, current_train_items 98848.
I0302 19:00:01.646932 22732373614720 run.py:483] Algo bellman_ford step 3089 current loss 0.883672, current_train_items 98880.
I0302 19:00:01.666995 22732373614720 run.py:483] Algo bellman_ford step 3090 current loss 0.312235, current_train_items 98912.
I0302 19:00:01.682820 22732373614720 run.py:483] Algo bellman_ford step 3091 current loss 0.544628, current_train_items 98944.
I0302 19:00:01.704806 22732373614720 run.py:483] Algo bellman_ford step 3092 current loss 0.733538, current_train_items 98976.
I0302 19:00:01.735403 22732373614720 run.py:483] Algo bellman_ford step 3093 current loss 0.781449, current_train_items 99008.
I0302 19:00:01.768115 22732373614720 run.py:483] Algo bellman_ford step 3094 current loss 0.972965, current_train_items 99040.
I0302 19:00:01.787163 22732373614720 run.py:483] Algo bellman_ford step 3095 current loss 0.406437, current_train_items 99072.
I0302 19:00:01.803613 22732373614720 run.py:483] Algo bellman_ford step 3096 current loss 0.564505, current_train_items 99104.
I0302 19:00:01.827628 22732373614720 run.py:483] Algo bellman_ford step 3097 current loss 0.766294, current_train_items 99136.
I0302 19:00:01.858030 22732373614720 run.py:483] Algo bellman_ford step 3098 current loss 0.782484, current_train_items 99168.
I0302 19:00:01.890522 22732373614720 run.py:483] Algo bellman_ford step 3099 current loss 0.888878, current_train_items 99200.
I0302 19:00:01.910267 22732373614720 run.py:483] Algo bellman_ford step 3100 current loss 0.396885, current_train_items 99232.
I0302 19:00:01.918212 22732373614720 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0302 19:00:01.918320 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.925, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:00:01.948961 22732373614720 run.py:483] Algo bellman_ford step 3101 current loss 0.570884, current_train_items 99264.
I0302 19:00:01.972639 22732373614720 run.py:483] Algo bellman_ford step 3102 current loss 0.791872, current_train_items 99296.
I0302 19:00:02.004102 22732373614720 run.py:483] Algo bellman_ford step 3103 current loss 0.805813, current_train_items 99328.
I0302 19:00:02.038630 22732373614720 run.py:483] Algo bellman_ford step 3104 current loss 1.042766, current_train_items 99360.
I0302 19:00:02.058939 22732373614720 run.py:483] Algo bellman_ford step 3105 current loss 0.345711, current_train_items 99392.
I0302 19:00:02.074298 22732373614720 run.py:483] Algo bellman_ford step 3106 current loss 0.517737, current_train_items 99424.
I0302 19:00:02.097419 22732373614720 run.py:483] Algo bellman_ford step 3107 current loss 0.703986, current_train_items 99456.
I0302 19:00:02.128536 22732373614720 run.py:483] Algo bellman_ford step 3108 current loss 0.867038, current_train_items 99488.
I0302 19:00:02.160199 22732373614720 run.py:483] Algo bellman_ford step 3109 current loss 0.877081, current_train_items 99520.
I0302 19:00:02.180144 22732373614720 run.py:483] Algo bellman_ford step 3110 current loss 0.402424, current_train_items 99552.
I0302 19:00:02.196269 22732373614720 run.py:483] Algo bellman_ford step 3111 current loss 0.601755, current_train_items 99584.
I0302 19:00:02.219187 22732373614720 run.py:483] Algo bellman_ford step 3112 current loss 0.776421, current_train_items 99616.
I0302 19:00:02.249789 22732373614720 run.py:483] Algo bellman_ford step 3113 current loss 0.842920, current_train_items 99648.
I0302 19:00:02.283364 22732373614720 run.py:483] Algo bellman_ford step 3114 current loss 0.955798, current_train_items 99680.
I0302 19:00:02.303243 22732373614720 run.py:483] Algo bellman_ford step 3115 current loss 0.319448, current_train_items 99712.
I0302 19:00:02.319447 22732373614720 run.py:483] Algo bellman_ford step 3116 current loss 0.536718, current_train_items 99744.
I0302 19:00:02.342475 22732373614720 run.py:483] Algo bellman_ford step 3117 current loss 0.725756, current_train_items 99776.
I0302 19:00:02.373687 22732373614720 run.py:483] Algo bellman_ford step 3118 current loss 0.845356, current_train_items 99808.
I0302 19:00:02.406068 22732373614720 run.py:483] Algo bellman_ford step 3119 current loss 0.911573, current_train_items 99840.
I0302 19:00:02.425340 22732373614720 run.py:483] Algo bellman_ford step 3120 current loss 0.285333, current_train_items 99872.
I0302 19:00:02.441270 22732373614720 run.py:483] Algo bellman_ford step 3121 current loss 0.623262, current_train_items 99904.
I0302 19:00:02.464281 22732373614720 run.py:483] Algo bellman_ford step 3122 current loss 0.721167, current_train_items 99936.
I0302 19:00:02.494959 22732373614720 run.py:483] Algo bellman_ford step 3123 current loss 0.766178, current_train_items 99968.
I0302 19:00:02.528076 22732373614720 run.py:483] Algo bellman_ford step 3124 current loss 0.931238, current_train_items 100000.
I0302 19:00:02.547364 22732373614720 run.py:483] Algo bellman_ford step 3125 current loss 0.400148, current_train_items 100032.
I0302 19:00:02.563397 22732373614720 run.py:483] Algo bellman_ford step 3126 current loss 0.630400, current_train_items 100064.
I0302 19:00:02.585721 22732373614720 run.py:483] Algo bellman_ford step 3127 current loss 0.635962, current_train_items 100096.
I0302 19:00:02.616855 22732373614720 run.py:483] Algo bellman_ford step 3128 current loss 0.795247, current_train_items 100128.
I0302 19:00:02.648115 22732373614720 run.py:483] Algo bellman_ford step 3129 current loss 0.797740, current_train_items 100160.
I0302 19:00:02.667647 22732373614720 run.py:483] Algo bellman_ford step 3130 current loss 0.333094, current_train_items 100192.
I0302 19:00:02.683698 22732373614720 run.py:483] Algo bellman_ford step 3131 current loss 0.504874, current_train_items 100224.
I0302 19:00:02.706861 22732373614720 run.py:483] Algo bellman_ford step 3132 current loss 0.717086, current_train_items 100256.
I0302 19:00:02.736315 22732373614720 run.py:483] Algo bellman_ford step 3133 current loss 0.862164, current_train_items 100288.
I0302 19:00:02.770939 22732373614720 run.py:483] Algo bellman_ford step 3134 current loss 1.071650, current_train_items 100320.
I0302 19:00:02.790103 22732373614720 run.py:483] Algo bellman_ford step 3135 current loss 0.445251, current_train_items 100352.
I0302 19:00:02.805987 22732373614720 run.py:483] Algo bellman_ford step 3136 current loss 0.537101, current_train_items 100384.
I0302 19:00:02.829333 22732373614720 run.py:483] Algo bellman_ford step 3137 current loss 0.764584, current_train_items 100416.
I0302 19:00:02.858737 22732373614720 run.py:483] Algo bellman_ford step 3138 current loss 0.803036, current_train_items 100448.
I0302 19:00:02.892211 22732373614720 run.py:483] Algo bellman_ford step 3139 current loss 0.880422, current_train_items 100480.
I0302 19:00:02.911482 22732373614720 run.py:483] Algo bellman_ford step 3140 current loss 0.435137, current_train_items 100512.
I0302 19:00:02.928021 22732373614720 run.py:483] Algo bellman_ford step 3141 current loss 0.662113, current_train_items 100544.
I0302 19:00:02.949928 22732373614720 run.py:483] Algo bellman_ford step 3142 current loss 0.668742, current_train_items 100576.
I0302 19:00:02.981354 22732373614720 run.py:483] Algo bellman_ford step 3143 current loss 0.779026, current_train_items 100608.
I0302 19:00:03.014178 22732373614720 run.py:483] Algo bellman_ford step 3144 current loss 0.754718, current_train_items 100640.
I0302 19:00:03.033497 22732373614720 run.py:483] Algo bellman_ford step 3145 current loss 0.308189, current_train_items 100672.
I0302 19:00:03.049396 22732373614720 run.py:483] Algo bellman_ford step 3146 current loss 0.447814, current_train_items 100704.
I0302 19:00:03.071583 22732373614720 run.py:483] Algo bellman_ford step 3147 current loss 0.605252, current_train_items 100736.
I0302 19:00:03.099951 22732373614720 run.py:483] Algo bellman_ford step 3148 current loss 0.669450, current_train_items 100768.
I0302 19:00:03.134810 22732373614720 run.py:483] Algo bellman_ford step 3149 current loss 0.995492, current_train_items 100800.
I0302 19:00:03.154469 22732373614720 run.py:483] Algo bellman_ford step 3150 current loss 0.359613, current_train_items 100832.
I0302 19:00:03.162854 22732373614720 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.8876953125, 'score': 0.8876953125, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0302 19:00:03.162962 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.926, current avg val score is 0.888, val scores are: bellman_ford: 0.888
I0302 19:00:03.179808 22732373614720 run.py:483] Algo bellman_ford step 3151 current loss 0.521432, current_train_items 100864.
I0302 19:00:03.204451 22732373614720 run.py:483] Algo bellman_ford step 3152 current loss 0.739585, current_train_items 100896.
I0302 19:00:03.235435 22732373614720 run.py:483] Algo bellman_ford step 3153 current loss 0.738038, current_train_items 100928.
I0302 19:00:03.269363 22732373614720 run.py:483] Algo bellman_ford step 3154 current loss 0.943297, current_train_items 100960.
I0302 19:00:03.289365 22732373614720 run.py:483] Algo bellman_ford step 3155 current loss 0.365398, current_train_items 100992.
I0302 19:00:03.305089 22732373614720 run.py:483] Algo bellman_ford step 3156 current loss 0.586076, current_train_items 101024.
I0302 19:00:03.329139 22732373614720 run.py:483] Algo bellman_ford step 3157 current loss 0.759327, current_train_items 101056.
I0302 19:00:03.358662 22732373614720 run.py:483] Algo bellman_ford step 3158 current loss 0.850370, current_train_items 101088.
I0302 19:00:03.391026 22732373614720 run.py:483] Algo bellman_ford step 3159 current loss 0.833209, current_train_items 101120.
I0302 19:00:03.411054 22732373614720 run.py:483] Algo bellman_ford step 3160 current loss 0.344913, current_train_items 101152.
I0302 19:00:03.427076 22732373614720 run.py:483] Algo bellman_ford step 3161 current loss 0.502938, current_train_items 101184.
I0302 19:00:03.449905 22732373614720 run.py:483] Algo bellman_ford step 3162 current loss 0.798231, current_train_items 101216.
I0302 19:00:03.479573 22732373614720 run.py:483] Algo bellman_ford step 3163 current loss 0.732067, current_train_items 101248.
I0302 19:00:03.513557 22732373614720 run.py:483] Algo bellman_ford step 3164 current loss 0.926150, current_train_items 101280.
I0302 19:00:03.533572 22732373614720 run.py:483] Algo bellman_ford step 3165 current loss 0.490922, current_train_items 101312.
I0302 19:00:03.550191 22732373614720 run.py:483] Algo bellman_ford step 3166 current loss 0.694475, current_train_items 101344.
I0302 19:00:03.573750 22732373614720 run.py:483] Algo bellman_ford step 3167 current loss 0.726743, current_train_items 101376.
I0302 19:00:03.604639 22732373614720 run.py:483] Algo bellman_ford step 3168 current loss 0.891428, current_train_items 101408.
I0302 19:00:03.636890 22732373614720 run.py:483] Algo bellman_ford step 3169 current loss 0.958294, current_train_items 101440.
I0302 19:00:03.657054 22732373614720 run.py:483] Algo bellman_ford step 3170 current loss 0.420224, current_train_items 101472.
I0302 19:00:03.673567 22732373614720 run.py:483] Algo bellman_ford step 3171 current loss 0.575359, current_train_items 101504.
I0302 19:00:03.696895 22732373614720 run.py:483] Algo bellman_ford step 3172 current loss 0.681355, current_train_items 101536.
I0302 19:00:03.727372 22732373614720 run.py:483] Algo bellman_ford step 3173 current loss 0.804312, current_train_items 101568.
I0302 19:00:03.758504 22732373614720 run.py:483] Algo bellman_ford step 3174 current loss 0.916344, current_train_items 101600.
I0302 19:00:03.778369 22732373614720 run.py:483] Algo bellman_ford step 3175 current loss 0.313873, current_train_items 101632.
I0302 19:00:03.794471 22732373614720 run.py:483] Algo bellman_ford step 3176 current loss 0.510834, current_train_items 101664.
I0302 19:00:03.817263 22732373614720 run.py:483] Algo bellman_ford step 3177 current loss 0.632009, current_train_items 101696.
I0302 19:00:03.845903 22732373614720 run.py:483] Algo bellman_ford step 3178 current loss 0.748280, current_train_items 101728.
I0302 19:00:03.879708 22732373614720 run.py:483] Algo bellman_ford step 3179 current loss 0.931438, current_train_items 101760.
I0302 19:00:03.899049 22732373614720 run.py:483] Algo bellman_ford step 3180 current loss 0.373043, current_train_items 101792.
I0302 19:00:03.915297 22732373614720 run.py:483] Algo bellman_ford step 3181 current loss 0.485247, current_train_items 101824.
I0302 19:00:03.938969 22732373614720 run.py:483] Algo bellman_ford step 3182 current loss 0.712655, current_train_items 101856.
I0302 19:00:03.969909 22732373614720 run.py:483] Algo bellman_ford step 3183 current loss 0.760652, current_train_items 101888.
I0302 19:00:04.005688 22732373614720 run.py:483] Algo bellman_ford step 3184 current loss 0.904153, current_train_items 101920.
I0302 19:00:04.025921 22732373614720 run.py:483] Algo bellman_ford step 3185 current loss 0.415683, current_train_items 101952.
I0302 19:00:04.041947 22732373614720 run.py:483] Algo bellman_ford step 3186 current loss 0.574297, current_train_items 101984.
I0302 19:00:04.065776 22732373614720 run.py:483] Algo bellman_ford step 3187 current loss 0.733577, current_train_items 102016.
I0302 19:00:04.096168 22732373614720 run.py:483] Algo bellman_ford step 3188 current loss 0.796766, current_train_items 102048.
I0302 19:00:04.129379 22732373614720 run.py:483] Algo bellman_ford step 3189 current loss 0.842086, current_train_items 102080.
I0302 19:00:04.149259 22732373614720 run.py:483] Algo bellman_ford step 3190 current loss 0.426116, current_train_items 102112.
I0302 19:00:04.165470 22732373614720 run.py:483] Algo bellman_ford step 3191 current loss 0.509045, current_train_items 102144.
I0302 19:00:04.189072 22732373614720 run.py:483] Algo bellman_ford step 3192 current loss 0.783105, current_train_items 102176.
I0302 19:00:04.219366 22732373614720 run.py:483] Algo bellman_ford step 3193 current loss 0.714398, current_train_items 102208.
I0302 19:00:04.252477 22732373614720 run.py:483] Algo bellman_ford step 3194 current loss 0.896585, current_train_items 102240.
I0302 19:00:04.271883 22732373614720 run.py:483] Algo bellman_ford step 3195 current loss 0.415087, current_train_items 102272.
I0302 19:00:04.287510 22732373614720 run.py:483] Algo bellman_ford step 3196 current loss 0.438613, current_train_items 102304.
I0302 19:00:04.310563 22732373614720 run.py:483] Algo bellman_ford step 3197 current loss 0.710837, current_train_items 102336.
I0302 19:00:04.341399 22732373614720 run.py:483] Algo bellman_ford step 3198 current loss 0.760526, current_train_items 102368.
I0302 19:00:04.374368 22732373614720 run.py:483] Algo bellman_ford step 3199 current loss 0.928947, current_train_items 102400.
I0302 19:00:04.394453 22732373614720 run.py:483] Algo bellman_ford step 3200 current loss 0.344476, current_train_items 102432.
I0302 19:00:04.402263 22732373614720 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0302 19:00:04.402372 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.926, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:04.418524 22732373614720 run.py:483] Algo bellman_ford step 3201 current loss 0.440105, current_train_items 102464.
I0302 19:00:04.441521 22732373614720 run.py:483] Algo bellman_ford step 3202 current loss 0.703171, current_train_items 102496.
I0302 19:00:04.473609 22732373614720 run.py:483] Algo bellman_ford step 3203 current loss 0.921461, current_train_items 102528.
I0302 19:00:04.507133 22732373614720 run.py:483] Algo bellman_ford step 3204 current loss 0.933727, current_train_items 102560.
I0302 19:00:04.526984 22732373614720 run.py:483] Algo bellman_ford step 3205 current loss 0.406177, current_train_items 102592.
I0302 19:00:04.542268 22732373614720 run.py:483] Algo bellman_ford step 3206 current loss 0.580342, current_train_items 102624.
I0302 19:00:04.564093 22732373614720 run.py:483] Algo bellman_ford step 3207 current loss 0.542574, current_train_items 102656.
I0302 19:00:04.595628 22732373614720 run.py:483] Algo bellman_ford step 3208 current loss 0.791621, current_train_items 102688.
I0302 19:00:04.629129 22732373614720 run.py:483] Algo bellman_ford step 3209 current loss 1.041503, current_train_items 102720.
I0302 19:00:04.648511 22732373614720 run.py:483] Algo bellman_ford step 3210 current loss 0.357528, current_train_items 102752.
I0302 19:00:04.664259 22732373614720 run.py:483] Algo bellman_ford step 3211 current loss 0.446820, current_train_items 102784.
I0302 19:00:04.687458 22732373614720 run.py:483] Algo bellman_ford step 3212 current loss 0.737267, current_train_items 102816.
I0302 19:00:04.717715 22732373614720 run.py:483] Algo bellman_ford step 3213 current loss 0.886354, current_train_items 102848.
I0302 19:00:04.750524 22732373614720 run.py:483] Algo bellman_ford step 3214 current loss 0.863144, current_train_items 102880.
I0302 19:00:04.770180 22732373614720 run.py:483] Algo bellman_ford step 3215 current loss 0.452169, current_train_items 102912.
I0302 19:00:04.786213 22732373614720 run.py:483] Algo bellman_ford step 3216 current loss 0.624557, current_train_items 102944.
I0302 19:00:04.809873 22732373614720 run.py:483] Algo bellman_ford step 3217 current loss 0.827991, current_train_items 102976.
I0302 19:00:04.838961 22732373614720 run.py:483] Algo bellman_ford step 3218 current loss 0.975995, current_train_items 103008.
I0302 19:00:04.871039 22732373614720 run.py:483] Algo bellman_ford step 3219 current loss 1.248486, current_train_items 103040.
I0302 19:00:04.890587 22732373614720 run.py:483] Algo bellman_ford step 3220 current loss 0.434358, current_train_items 103072.
I0302 19:00:04.906634 22732373614720 run.py:483] Algo bellman_ford step 3221 current loss 0.672530, current_train_items 103104.
I0302 19:00:04.930311 22732373614720 run.py:483] Algo bellman_ford step 3222 current loss 1.029967, current_train_items 103136.
I0302 19:00:04.960914 22732373614720 run.py:483] Algo bellman_ford step 3223 current loss 2.414321, current_train_items 103168.
I0302 19:00:04.992539 22732373614720 run.py:483] Algo bellman_ford step 3224 current loss 1.475646, current_train_items 103200.
I0302 19:00:05.011861 22732373614720 run.py:483] Algo bellman_ford step 3225 current loss 0.466508, current_train_items 103232.
I0302 19:00:05.027165 22732373614720 run.py:483] Algo bellman_ford step 3226 current loss 0.542453, current_train_items 103264.
I0302 19:00:05.050272 22732373614720 run.py:483] Algo bellman_ford step 3227 current loss 0.855462, current_train_items 103296.
I0302 19:00:05.081147 22732373614720 run.py:483] Algo bellman_ford step 3228 current loss 1.061878, current_train_items 103328.
I0302 19:00:05.117651 22732373614720 run.py:483] Algo bellman_ford step 3229 current loss 1.382476, current_train_items 103360.
I0302 19:00:05.136909 22732373614720 run.py:483] Algo bellman_ford step 3230 current loss 0.470902, current_train_items 103392.
I0302 19:00:05.152959 22732373614720 run.py:483] Algo bellman_ford step 3231 current loss 0.680377, current_train_items 103424.
I0302 19:00:05.175249 22732373614720 run.py:483] Algo bellman_ford step 3232 current loss 0.755576, current_train_items 103456.
I0302 19:00:05.205913 22732373614720 run.py:483] Algo bellman_ford step 3233 current loss 0.810401, current_train_items 103488.
I0302 19:00:05.240271 22732373614720 run.py:483] Algo bellman_ford step 3234 current loss 1.118773, current_train_items 103520.
I0302 19:00:05.259623 22732373614720 run.py:483] Algo bellman_ford step 3235 current loss 0.498542, current_train_items 103552.
I0302 19:00:05.275797 22732373614720 run.py:483] Algo bellman_ford step 3236 current loss 0.693441, current_train_items 103584.
I0302 19:00:05.298242 22732373614720 run.py:483] Algo bellman_ford step 3237 current loss 0.785545, current_train_items 103616.
I0302 19:00:05.329666 22732373614720 run.py:483] Algo bellman_ford step 3238 current loss 0.907207, current_train_items 103648.
I0302 19:00:05.362410 22732373614720 run.py:483] Algo bellman_ford step 3239 current loss 1.070559, current_train_items 103680.
I0302 19:00:05.381605 22732373614720 run.py:483] Algo bellman_ford step 3240 current loss 0.443553, current_train_items 103712.
I0302 19:00:05.397791 22732373614720 run.py:483] Algo bellman_ford step 3241 current loss 0.684655, current_train_items 103744.
I0302 19:00:05.420517 22732373614720 run.py:483] Algo bellman_ford step 3242 current loss 0.875516, current_train_items 103776.
I0302 19:00:05.449760 22732373614720 run.py:483] Algo bellman_ford step 3243 current loss 0.802940, current_train_items 103808.
I0302 19:00:05.483234 22732373614720 run.py:483] Algo bellman_ford step 3244 current loss 1.221922, current_train_items 103840.
I0302 19:00:05.502569 22732373614720 run.py:483] Algo bellman_ford step 3245 current loss 0.346602, current_train_items 103872.
I0302 19:00:05.518759 22732373614720 run.py:483] Algo bellman_ford step 3246 current loss 0.626271, current_train_items 103904.
I0302 19:00:05.541537 22732373614720 run.py:483] Algo bellman_ford step 3247 current loss 0.827308, current_train_items 103936.
I0302 19:00:05.572437 22732373614720 run.py:483] Algo bellman_ford step 3248 current loss 0.869110, current_train_items 103968.
I0302 19:00:05.604123 22732373614720 run.py:483] Algo bellman_ford step 3249 current loss 0.905055, current_train_items 104000.
I0302 19:00:05.623565 22732373614720 run.py:483] Algo bellman_ford step 3250 current loss 0.435901, current_train_items 104032.
I0302 19:00:05.631623 22732373614720 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0302 19:00:05.631731 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.926, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:00:05.648289 22732373614720 run.py:483] Algo bellman_ford step 3251 current loss 0.638836, current_train_items 104064.
I0302 19:00:05.670674 22732373614720 run.py:483] Algo bellman_ford step 3252 current loss 0.712157, current_train_items 104096.
I0302 19:00:05.702979 22732373614720 run.py:483] Algo bellman_ford step 3253 current loss 1.001523, current_train_items 104128.
I0302 19:00:05.736660 22732373614720 run.py:483] Algo bellman_ford step 3254 current loss 0.885041, current_train_items 104160.
I0302 19:00:05.757109 22732373614720 run.py:483] Algo bellman_ford step 3255 current loss 0.478643, current_train_items 104192.
I0302 19:00:05.772586 22732373614720 run.py:483] Algo bellman_ford step 3256 current loss 0.496046, current_train_items 104224.
I0302 19:00:05.795037 22732373614720 run.py:483] Algo bellman_ford step 3257 current loss 0.690879, current_train_items 104256.
I0302 19:00:05.826964 22732373614720 run.py:483] Algo bellman_ford step 3258 current loss 0.850027, current_train_items 104288.
I0302 19:00:05.861644 22732373614720 run.py:483] Algo bellman_ford step 3259 current loss 1.048970, current_train_items 104320.
I0302 19:00:05.881307 22732373614720 run.py:483] Algo bellman_ford step 3260 current loss 0.431332, current_train_items 104352.
I0302 19:00:05.897314 22732373614720 run.py:483] Algo bellman_ford step 3261 current loss 0.603094, current_train_items 104384.
I0302 19:00:05.920200 22732373614720 run.py:483] Algo bellman_ford step 3262 current loss 0.695714, current_train_items 104416.
I0302 19:00:05.951315 22732373614720 run.py:483] Algo bellman_ford step 3263 current loss 0.948624, current_train_items 104448.
I0302 19:00:05.985604 22732373614720 run.py:483] Algo bellman_ford step 3264 current loss 1.163321, current_train_items 104480.
I0302 19:00:06.005269 22732373614720 run.py:483] Algo bellman_ford step 3265 current loss 0.437113, current_train_items 104512.
I0302 19:00:06.021654 22732373614720 run.py:483] Algo bellman_ford step 3266 current loss 0.625139, current_train_items 104544.
I0302 19:00:06.044488 22732373614720 run.py:483] Algo bellman_ford step 3267 current loss 0.803309, current_train_items 104576.
I0302 19:00:06.073490 22732373614720 run.py:483] Algo bellman_ford step 3268 current loss 0.799325, current_train_items 104608.
I0302 19:00:06.108832 22732373614720 run.py:483] Algo bellman_ford step 3269 current loss 1.373404, current_train_items 104640.
I0302 19:00:06.128706 22732373614720 run.py:483] Algo bellman_ford step 3270 current loss 0.426257, current_train_items 104672.
I0302 19:00:06.144951 22732373614720 run.py:483] Algo bellman_ford step 3271 current loss 0.657772, current_train_items 104704.
I0302 19:00:06.168135 22732373614720 run.py:483] Algo bellman_ford step 3272 current loss 0.751830, current_train_items 104736.
I0302 19:00:06.197518 22732373614720 run.py:483] Algo bellman_ford step 3273 current loss 0.824975, current_train_items 104768.
I0302 19:00:06.228845 22732373614720 run.py:483] Algo bellman_ford step 3274 current loss 0.852980, current_train_items 104800.
I0302 19:00:06.248992 22732373614720 run.py:483] Algo bellman_ford step 3275 current loss 0.401329, current_train_items 104832.
I0302 19:00:06.264928 22732373614720 run.py:483] Algo bellman_ford step 3276 current loss 0.490881, current_train_items 104864.
I0302 19:00:06.287374 22732373614720 run.py:483] Algo bellman_ford step 3277 current loss 0.725823, current_train_items 104896.
I0302 19:00:06.318828 22732373614720 run.py:483] Algo bellman_ford step 3278 current loss 0.778138, current_train_items 104928.
I0302 19:00:06.352791 22732373614720 run.py:483] Algo bellman_ford step 3279 current loss 0.929337, current_train_items 104960.
I0302 19:00:06.372205 22732373614720 run.py:483] Algo bellman_ford step 3280 current loss 0.436336, current_train_items 104992.
I0302 19:00:06.388000 22732373614720 run.py:483] Algo bellman_ford step 3281 current loss 0.593406, current_train_items 105024.
I0302 19:00:06.411027 22732373614720 run.py:483] Algo bellman_ford step 3282 current loss 0.778239, current_train_items 105056.
I0302 19:00:06.441940 22732373614720 run.py:483] Algo bellman_ford step 3283 current loss 0.887289, current_train_items 105088.
I0302 19:00:06.475493 22732373614720 run.py:483] Algo bellman_ford step 3284 current loss 0.909251, current_train_items 105120.
I0302 19:00:06.495476 22732373614720 run.py:483] Algo bellman_ford step 3285 current loss 0.369281, current_train_items 105152.
I0302 19:00:06.511873 22732373614720 run.py:483] Algo bellman_ford step 3286 current loss 0.582891, current_train_items 105184.
I0302 19:00:06.535326 22732373614720 run.py:483] Algo bellman_ford step 3287 current loss 0.768372, current_train_items 105216.
I0302 19:00:06.564642 22732373614720 run.py:483] Algo bellman_ford step 3288 current loss 0.677110, current_train_items 105248.
I0302 19:00:06.596286 22732373614720 run.py:483] Algo bellman_ford step 3289 current loss 0.862757, current_train_items 105280.
I0302 19:00:06.616046 22732373614720 run.py:483] Algo bellman_ford step 3290 current loss 0.409879, current_train_items 105312.
I0302 19:00:06.632541 22732373614720 run.py:483] Algo bellman_ford step 3291 current loss 0.492160, current_train_items 105344.
I0302 19:00:06.655459 22732373614720 run.py:483] Algo bellman_ford step 3292 current loss 0.681424, current_train_items 105376.
I0302 19:00:06.686103 22732373614720 run.py:483] Algo bellman_ford step 3293 current loss 0.776204, current_train_items 105408.
I0302 19:00:06.719561 22732373614720 run.py:483] Algo bellman_ford step 3294 current loss 0.918600, current_train_items 105440.
I0302 19:00:06.739424 22732373614720 run.py:483] Algo bellman_ford step 3295 current loss 0.392933, current_train_items 105472.
I0302 19:00:06.755075 22732373614720 run.py:483] Algo bellman_ford step 3296 current loss 0.502445, current_train_items 105504.
I0302 19:00:06.778955 22732373614720 run.py:483] Algo bellman_ford step 3297 current loss 0.846430, current_train_items 105536.
I0302 19:00:06.811042 22732373614720 run.py:483] Algo bellman_ford step 3298 current loss 0.921495, current_train_items 105568.
I0302 19:00:06.845533 22732373614720 run.py:483] Algo bellman_ford step 3299 current loss 1.012912, current_train_items 105600.
I0302 19:00:06.865216 22732373614720 run.py:483] Algo bellman_ford step 3300 current loss 0.447705, current_train_items 105632.
I0302 19:00:06.873335 22732373614720 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0302 19:00:06.873444 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.926, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:00:06.889830 22732373614720 run.py:483] Algo bellman_ford step 3301 current loss 0.563058, current_train_items 105664.
I0302 19:00:06.913010 22732373614720 run.py:483] Algo bellman_ford step 3302 current loss 0.731734, current_train_items 105696.
I0302 19:00:06.942522 22732373614720 run.py:483] Algo bellman_ford step 3303 current loss 0.765938, current_train_items 105728.
I0302 19:00:06.974002 22732373614720 run.py:483] Algo bellman_ford step 3304 current loss 0.818288, current_train_items 105760.
I0302 19:00:06.993682 22732373614720 run.py:483] Algo bellman_ford step 3305 current loss 0.288165, current_train_items 105792.
I0302 19:00:07.009517 22732373614720 run.py:483] Algo bellman_ford step 3306 current loss 0.648648, current_train_items 105824.
I0302 19:00:07.032976 22732373614720 run.py:483] Algo bellman_ford step 3307 current loss 0.811326, current_train_items 105856.
I0302 19:00:07.063221 22732373614720 run.py:483] Algo bellman_ford step 3308 current loss 0.822352, current_train_items 105888.
I0302 19:00:07.097599 22732373614720 run.py:483] Algo bellman_ford step 3309 current loss 1.046840, current_train_items 105920.
I0302 19:00:07.116926 22732373614720 run.py:483] Algo bellman_ford step 3310 current loss 0.332719, current_train_items 105952.
I0302 19:00:07.133052 22732373614720 run.py:483] Algo bellman_ford step 3311 current loss 0.545725, current_train_items 105984.
I0302 19:00:07.156053 22732373614720 run.py:483] Algo bellman_ford step 3312 current loss 0.695545, current_train_items 106016.
I0302 19:00:07.187122 22732373614720 run.py:483] Algo bellman_ford step 3313 current loss 0.826747, current_train_items 106048.
I0302 19:00:07.218725 22732373614720 run.py:483] Algo bellman_ford step 3314 current loss 0.925155, current_train_items 106080.
I0302 19:00:07.238014 22732373614720 run.py:483] Algo bellman_ford step 3315 current loss 0.384567, current_train_items 106112.
I0302 19:00:07.254412 22732373614720 run.py:483] Algo bellman_ford step 3316 current loss 0.544315, current_train_items 106144.
I0302 19:00:07.276884 22732373614720 run.py:483] Algo bellman_ford step 3317 current loss 0.723032, current_train_items 106176.
I0302 19:00:07.307375 22732373614720 run.py:483] Algo bellman_ford step 3318 current loss 0.798493, current_train_items 106208.
I0302 19:00:07.340762 22732373614720 run.py:483] Algo bellman_ford step 3319 current loss 0.951225, current_train_items 106240.
I0302 19:00:07.360596 22732373614720 run.py:483] Algo bellman_ford step 3320 current loss 0.359771, current_train_items 106272.
I0302 19:00:07.376635 22732373614720 run.py:483] Algo bellman_ford step 3321 current loss 0.574829, current_train_items 106304.
I0302 19:00:07.400365 22732373614720 run.py:483] Algo bellman_ford step 3322 current loss 0.711362, current_train_items 106336.
I0302 19:00:07.431226 22732373614720 run.py:483] Algo bellman_ford step 3323 current loss 0.754335, current_train_items 106368.
I0302 19:00:07.463304 22732373614720 run.py:483] Algo bellman_ford step 3324 current loss 0.801423, current_train_items 106400.
I0302 19:00:07.482483 22732373614720 run.py:483] Algo bellman_ford step 3325 current loss 0.406613, current_train_items 106432.
I0302 19:00:07.498694 22732373614720 run.py:483] Algo bellman_ford step 3326 current loss 0.540682, current_train_items 106464.
I0302 19:00:07.521150 22732373614720 run.py:483] Algo bellman_ford step 3327 current loss 0.683490, current_train_items 106496.
I0302 19:00:07.550859 22732373614720 run.py:483] Algo bellman_ford step 3328 current loss 0.774803, current_train_items 106528.
I0302 19:00:07.583206 22732373614720 run.py:483] Algo bellman_ford step 3329 current loss 0.884403, current_train_items 106560.
I0302 19:00:07.603082 22732373614720 run.py:483] Algo bellman_ford step 3330 current loss 0.441666, current_train_items 106592.
I0302 19:00:07.618934 22732373614720 run.py:483] Algo bellman_ford step 3331 current loss 0.520817, current_train_items 106624.
I0302 19:00:07.641567 22732373614720 run.py:483] Algo bellman_ford step 3332 current loss 0.750134, current_train_items 106656.
I0302 19:00:07.671583 22732373614720 run.py:483] Algo bellman_ford step 3333 current loss 0.718518, current_train_items 106688.
I0302 19:00:07.703519 22732373614720 run.py:483] Algo bellman_ford step 3334 current loss 0.799159, current_train_items 106720.
I0302 19:00:07.722668 22732373614720 run.py:483] Algo bellman_ford step 3335 current loss 0.398366, current_train_items 106752.
I0302 19:00:07.738166 22732373614720 run.py:483] Algo bellman_ford step 3336 current loss 0.609579, current_train_items 106784.
I0302 19:00:07.761607 22732373614720 run.py:483] Algo bellman_ford step 3337 current loss 0.834565, current_train_items 106816.
I0302 19:00:07.793104 22732373614720 run.py:483] Algo bellman_ford step 3338 current loss 0.848587, current_train_items 106848.
I0302 19:00:07.827497 22732373614720 run.py:483] Algo bellman_ford step 3339 current loss 0.933254, current_train_items 106880.
I0302 19:00:07.846551 22732373614720 run.py:483] Algo bellman_ford step 3340 current loss 0.366361, current_train_items 106912.
I0302 19:00:07.862931 22732373614720 run.py:483] Algo bellman_ford step 3341 current loss 0.570423, current_train_items 106944.
I0302 19:00:07.886543 22732373614720 run.py:483] Algo bellman_ford step 3342 current loss 0.909470, current_train_items 106976.
I0302 19:00:07.917745 22732373614720 run.py:483] Algo bellman_ford step 3343 current loss 0.973232, current_train_items 107008.
I0302 19:00:07.948590 22732373614720 run.py:483] Algo bellman_ford step 3344 current loss 0.816149, current_train_items 107040.
I0302 19:00:07.968002 22732373614720 run.py:483] Algo bellman_ford step 3345 current loss 0.347036, current_train_items 107072.
I0302 19:00:07.983711 22732373614720 run.py:483] Algo bellman_ford step 3346 current loss 0.511509, current_train_items 107104.
I0302 19:00:08.006453 22732373614720 run.py:483] Algo bellman_ford step 3347 current loss 0.756206, current_train_items 107136.
I0302 19:00:08.036271 22732373614720 run.py:483] Algo bellman_ford step 3348 current loss 0.841812, current_train_items 107168.
I0302 19:00:08.067249 22732373614720 run.py:483] Algo bellman_ford step 3349 current loss 0.888042, current_train_items 107200.
I0302 19:00:08.086882 22732373614720 run.py:483] Algo bellman_ford step 3350 current loss 0.452935, current_train_items 107232.
I0302 19:00:08.094827 22732373614720 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0302 19:00:08.094937 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.926, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:00:08.111988 22732373614720 run.py:483] Algo bellman_ford step 3351 current loss 0.645181, current_train_items 107264.
I0302 19:00:08.135965 22732373614720 run.py:483] Algo bellman_ford step 3352 current loss 0.851941, current_train_items 107296.
I0302 19:00:08.167412 22732373614720 run.py:483] Algo bellman_ford step 3353 current loss 0.708960, current_train_items 107328.
I0302 19:00:08.201214 22732373614720 run.py:483] Algo bellman_ford step 3354 current loss 0.945463, current_train_items 107360.
I0302 19:00:08.220801 22732373614720 run.py:483] Algo bellman_ford step 3355 current loss 0.318997, current_train_items 107392.
I0302 19:00:08.236275 22732373614720 run.py:483] Algo bellman_ford step 3356 current loss 0.522236, current_train_items 107424.
I0302 19:00:08.259171 22732373614720 run.py:483] Algo bellman_ford step 3357 current loss 0.722205, current_train_items 107456.
I0302 19:00:08.289448 22732373614720 run.py:483] Algo bellman_ford step 3358 current loss 0.951595, current_train_items 107488.
I0302 19:00:08.323515 22732373614720 run.py:483] Algo bellman_ford step 3359 current loss 1.067894, current_train_items 107520.
I0302 19:00:08.343026 22732373614720 run.py:483] Algo bellman_ford step 3360 current loss 0.349120, current_train_items 107552.
I0302 19:00:08.358949 22732373614720 run.py:483] Algo bellman_ford step 3361 current loss 0.580783, current_train_items 107584.
I0302 19:00:08.382324 22732373614720 run.py:483] Algo bellman_ford step 3362 current loss 0.791122, current_train_items 107616.
I0302 19:00:08.412694 22732373614720 run.py:483] Algo bellman_ford step 3363 current loss 0.756236, current_train_items 107648.
I0302 19:00:08.444746 22732373614720 run.py:483] Algo bellman_ford step 3364 current loss 0.964170, current_train_items 107680.
I0302 19:00:08.464063 22732373614720 run.py:483] Algo bellman_ford step 3365 current loss 0.343474, current_train_items 107712.
I0302 19:00:08.479689 22732373614720 run.py:483] Algo bellman_ford step 3366 current loss 0.483454, current_train_items 107744.
I0302 19:00:08.503264 22732373614720 run.py:483] Algo bellman_ford step 3367 current loss 0.770213, current_train_items 107776.
I0302 19:00:08.532669 22732373614720 run.py:483] Algo bellman_ford step 3368 current loss 0.755862, current_train_items 107808.
I0302 19:00:08.564803 22732373614720 run.py:483] Algo bellman_ford step 3369 current loss 0.827896, current_train_items 107840.
I0302 19:00:08.584532 22732373614720 run.py:483] Algo bellman_ford step 3370 current loss 0.424939, current_train_items 107872.
I0302 19:00:08.600742 22732373614720 run.py:483] Algo bellman_ford step 3371 current loss 0.513120, current_train_items 107904.
I0302 19:00:08.623552 22732373614720 run.py:483] Algo bellman_ford step 3372 current loss 0.779189, current_train_items 107936.
I0302 19:00:08.655498 22732373614720 run.py:483] Algo bellman_ford step 3373 current loss 0.989094, current_train_items 107968.
I0302 19:00:08.687679 22732373614720 run.py:483] Algo bellman_ford step 3374 current loss 1.059195, current_train_items 108000.
I0302 19:00:08.707272 22732373614720 run.py:483] Algo bellman_ford step 3375 current loss 0.359062, current_train_items 108032.
I0302 19:00:08.723054 22732373614720 run.py:483] Algo bellman_ford step 3376 current loss 0.532814, current_train_items 108064.
I0302 19:00:08.745218 22732373614720 run.py:483] Algo bellman_ford step 3377 current loss 0.779028, current_train_items 108096.
I0302 19:00:08.774784 22732373614720 run.py:483] Algo bellman_ford step 3378 current loss 0.736021, current_train_items 108128.
I0302 19:00:08.808794 22732373614720 run.py:483] Algo bellman_ford step 3379 current loss 0.922892, current_train_items 108160.
I0302 19:00:08.828045 22732373614720 run.py:483] Algo bellman_ford step 3380 current loss 0.343651, current_train_items 108192.
I0302 19:00:08.844314 22732373614720 run.py:483] Algo bellman_ford step 3381 current loss 0.589824, current_train_items 108224.
I0302 19:00:08.868192 22732373614720 run.py:483] Algo bellman_ford step 3382 current loss 0.716745, current_train_items 108256.
I0302 19:00:08.897840 22732373614720 run.py:483] Algo bellman_ford step 3383 current loss 0.769391, current_train_items 108288.
I0302 19:00:08.932164 22732373614720 run.py:483] Algo bellman_ford step 3384 current loss 1.059335, current_train_items 108320.
I0302 19:00:08.952238 22732373614720 run.py:483] Algo bellman_ford step 3385 current loss 0.414543, current_train_items 108352.
I0302 19:00:08.968559 22732373614720 run.py:483] Algo bellman_ford step 3386 current loss 0.592868, current_train_items 108384.
I0302 19:00:08.991294 22732373614720 run.py:483] Algo bellman_ford step 3387 current loss 0.833836, current_train_items 108416.
I0302 19:00:09.021042 22732373614720 run.py:483] Algo bellman_ford step 3388 current loss 1.093083, current_train_items 108448.
I0302 19:00:09.052075 22732373614720 run.py:483] Algo bellman_ford step 3389 current loss 1.111475, current_train_items 108480.
I0302 19:00:09.071707 22732373614720 run.py:483] Algo bellman_ford step 3390 current loss 0.371573, current_train_items 108512.
I0302 19:00:09.087714 22732373614720 run.py:483] Algo bellman_ford step 3391 current loss 0.594431, current_train_items 108544.
I0302 19:00:09.110475 22732373614720 run.py:483] Algo bellman_ford step 3392 current loss 0.758319, current_train_items 108576.
I0302 19:00:09.139991 22732373614720 run.py:483] Algo bellman_ford step 3393 current loss 0.868922, current_train_items 108608.
I0302 19:00:09.170891 22732373614720 run.py:483] Algo bellman_ford step 3394 current loss 0.998520, current_train_items 108640.
I0302 19:00:09.190257 22732373614720 run.py:483] Algo bellman_ford step 3395 current loss 0.467764, current_train_items 108672.
I0302 19:00:09.206423 22732373614720 run.py:483] Algo bellman_ford step 3396 current loss 0.557076, current_train_items 108704.
I0302 19:00:09.229435 22732373614720 run.py:483] Algo bellman_ford step 3397 current loss 0.677483, current_train_items 108736.
I0302 19:00:09.259997 22732373614720 run.py:483] Algo bellman_ford step 3398 current loss 0.806226, current_train_items 108768.
I0302 19:00:09.291945 22732373614720 run.py:483] Algo bellman_ford step 3399 current loss 0.876268, current_train_items 108800.
I0302 19:00:09.311869 22732373614720 run.py:483] Algo bellman_ford step 3400 current loss 0.388114, current_train_items 108832.
I0302 19:00:09.319822 22732373614720 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0302 19:00:09.319929 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.926, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:09.336890 22732373614720 run.py:483] Algo bellman_ford step 3401 current loss 0.598847, current_train_items 108864.
I0302 19:00:09.360467 22732373614720 run.py:483] Algo bellman_ford step 3402 current loss 0.682558, current_train_items 108896.
I0302 19:00:09.391472 22732373614720 run.py:483] Algo bellman_ford step 3403 current loss 0.809603, current_train_items 108928.
I0302 19:00:09.425799 22732373614720 run.py:483] Algo bellman_ford step 3404 current loss 0.967050, current_train_items 108960.
I0302 19:00:09.445665 22732373614720 run.py:483] Algo bellman_ford step 3405 current loss 0.351078, current_train_items 108992.
I0302 19:00:09.460979 22732373614720 run.py:483] Algo bellman_ford step 3406 current loss 0.615606, current_train_items 109024.
I0302 19:00:09.484676 22732373614720 run.py:483] Algo bellman_ford step 3407 current loss 0.774614, current_train_items 109056.
I0302 19:00:09.516440 22732373614720 run.py:483] Algo bellman_ford step 3408 current loss 0.851421, current_train_items 109088.
I0302 19:00:09.548812 22732373614720 run.py:483] Algo bellman_ford step 3409 current loss 0.857881, current_train_items 109120.
I0302 19:00:09.568028 22732373614720 run.py:483] Algo bellman_ford step 3410 current loss 0.334794, current_train_items 109152.
I0302 19:00:09.584243 22732373614720 run.py:483] Algo bellman_ford step 3411 current loss 0.646189, current_train_items 109184.
I0302 19:00:09.608287 22732373614720 run.py:483] Algo bellman_ford step 3412 current loss 0.814718, current_train_items 109216.
I0302 19:00:09.640084 22732373614720 run.py:483] Algo bellman_ford step 3413 current loss 1.072965, current_train_items 109248.
I0302 19:00:09.675268 22732373614720 run.py:483] Algo bellman_ford step 3414 current loss 1.278075, current_train_items 109280.
I0302 19:00:09.694932 22732373614720 run.py:483] Algo bellman_ford step 3415 current loss 0.326715, current_train_items 109312.
I0302 19:00:09.710943 22732373614720 run.py:483] Algo bellman_ford step 3416 current loss 0.563552, current_train_items 109344.
I0302 19:00:09.734373 22732373614720 run.py:483] Algo bellman_ford step 3417 current loss 0.740728, current_train_items 109376.
I0302 19:00:09.765077 22732373614720 run.py:483] Algo bellman_ford step 3418 current loss 0.884859, current_train_items 109408.
I0302 19:00:09.797346 22732373614720 run.py:483] Algo bellman_ford step 3419 current loss 0.797693, current_train_items 109440.
I0302 19:00:09.816827 22732373614720 run.py:483] Algo bellman_ford step 3420 current loss 0.363070, current_train_items 109472.
I0302 19:00:09.832569 22732373614720 run.py:483] Algo bellman_ford step 3421 current loss 0.567425, current_train_items 109504.
I0302 19:00:09.856119 22732373614720 run.py:483] Algo bellman_ford step 3422 current loss 0.728628, current_train_items 109536.
I0302 19:00:09.887164 22732373614720 run.py:483] Algo bellman_ford step 3423 current loss 0.765016, current_train_items 109568.
I0302 19:00:09.919934 22732373614720 run.py:483] Algo bellman_ford step 3424 current loss 1.031086, current_train_items 109600.
I0302 19:00:09.939534 22732373614720 run.py:483] Algo bellman_ford step 3425 current loss 0.375261, current_train_items 109632.
I0302 19:00:09.956041 22732373614720 run.py:483] Algo bellman_ford step 3426 current loss 0.579682, current_train_items 109664.
I0302 19:00:09.978908 22732373614720 run.py:483] Algo bellman_ford step 3427 current loss 0.671944, current_train_items 109696.
I0302 19:00:10.009956 22732373614720 run.py:483] Algo bellman_ford step 3428 current loss 0.825824, current_train_items 109728.
I0302 19:00:10.040292 22732373614720 run.py:483] Algo bellman_ford step 3429 current loss 0.870477, current_train_items 109760.
I0302 19:00:10.059887 22732373614720 run.py:483] Algo bellman_ford step 3430 current loss 0.344886, current_train_items 109792.
I0302 19:00:10.076138 22732373614720 run.py:483] Algo bellman_ford step 3431 current loss 0.562849, current_train_items 109824.
I0302 19:00:10.099017 22732373614720 run.py:483] Algo bellman_ford step 3432 current loss 0.741137, current_train_items 109856.
I0302 19:00:10.131077 22732373614720 run.py:483] Algo bellman_ford step 3433 current loss 1.002156, current_train_items 109888.
I0302 19:00:10.162679 22732373614720 run.py:483] Algo bellman_ford step 3434 current loss 0.883500, current_train_items 109920.
I0302 19:00:10.182180 22732373614720 run.py:483] Algo bellman_ford step 3435 current loss 0.383088, current_train_items 109952.
I0302 19:00:10.198198 22732373614720 run.py:483] Algo bellman_ford step 3436 current loss 0.520498, current_train_items 109984.
I0302 19:00:10.221870 22732373614720 run.py:483] Algo bellman_ford step 3437 current loss 0.730291, current_train_items 110016.
I0302 19:00:10.251316 22732373614720 run.py:483] Algo bellman_ford step 3438 current loss 0.841104, current_train_items 110048.
I0302 19:00:10.282240 22732373614720 run.py:483] Algo bellman_ford step 3439 current loss 0.856021, current_train_items 110080.
I0302 19:00:10.301589 22732373614720 run.py:483] Algo bellman_ford step 3440 current loss 0.386287, current_train_items 110112.
I0302 19:00:10.317893 22732373614720 run.py:483] Algo bellman_ford step 3441 current loss 0.549363, current_train_items 110144.
I0302 19:00:10.340638 22732373614720 run.py:483] Algo bellman_ford step 3442 current loss 0.651296, current_train_items 110176.
I0302 19:00:10.371797 22732373614720 run.py:483] Algo bellman_ford step 3443 current loss 0.878584, current_train_items 110208.
I0302 19:00:10.405853 22732373614720 run.py:483] Algo bellman_ford step 3444 current loss 0.869154, current_train_items 110240.
I0302 19:00:10.425499 22732373614720 run.py:483] Algo bellman_ford step 3445 current loss 0.366498, current_train_items 110272.
I0302 19:00:10.441828 22732373614720 run.py:483] Algo bellman_ford step 3446 current loss 0.585886, current_train_items 110304.
I0302 19:00:10.465124 22732373614720 run.py:483] Algo bellman_ford step 3447 current loss 0.682972, current_train_items 110336.
I0302 19:00:10.495648 22732373614720 run.py:483] Algo bellman_ford step 3448 current loss 0.705809, current_train_items 110368.
I0302 19:00:10.528562 22732373614720 run.py:483] Algo bellman_ford step 3449 current loss 0.979146, current_train_items 110400.
I0302 19:00:10.547967 22732373614720 run.py:483] Algo bellman_ford step 3450 current loss 0.449795, current_train_items 110432.
I0302 19:00:10.556069 22732373614720 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0302 19:00:10.556185 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.926, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:10.572799 22732373614720 run.py:483] Algo bellman_ford step 3451 current loss 0.669443, current_train_items 110464.
I0302 19:00:10.597035 22732373614720 run.py:483] Algo bellman_ford step 3452 current loss 0.732059, current_train_items 110496.
I0302 19:00:10.626952 22732373614720 run.py:483] Algo bellman_ford step 3453 current loss 0.786958, current_train_items 110528.
I0302 19:00:10.662464 22732373614720 run.py:483] Algo bellman_ford step 3454 current loss 0.944203, current_train_items 110560.
I0302 19:00:10.682368 22732373614720 run.py:483] Algo bellman_ford step 3455 current loss 0.334865, current_train_items 110592.
I0302 19:00:10.697813 22732373614720 run.py:483] Algo bellman_ford step 3456 current loss 0.505760, current_train_items 110624.
I0302 19:00:10.720843 22732373614720 run.py:483] Algo bellman_ford step 3457 current loss 0.694815, current_train_items 110656.
I0302 19:00:10.752375 22732373614720 run.py:483] Algo bellman_ford step 3458 current loss 0.804480, current_train_items 110688.
I0302 19:00:10.784095 22732373614720 run.py:483] Algo bellman_ford step 3459 current loss 0.911788, current_train_items 110720.
I0302 19:00:10.803753 22732373614720 run.py:483] Algo bellman_ford step 3460 current loss 0.347988, current_train_items 110752.
I0302 19:00:10.820064 22732373614720 run.py:483] Algo bellman_ford step 3461 current loss 0.588139, current_train_items 110784.
I0302 19:00:10.842891 22732373614720 run.py:483] Algo bellman_ford step 3462 current loss 0.716334, current_train_items 110816.
I0302 19:00:10.873030 22732373614720 run.py:483] Algo bellman_ford step 3463 current loss 0.818122, current_train_items 110848.
I0302 19:00:10.905344 22732373614720 run.py:483] Algo bellman_ford step 3464 current loss 0.863393, current_train_items 110880.
I0302 19:00:10.924778 22732373614720 run.py:483] Algo bellman_ford step 3465 current loss 0.340010, current_train_items 110912.
I0302 19:00:10.941001 22732373614720 run.py:483] Algo bellman_ford step 3466 current loss 0.645909, current_train_items 110944.
I0302 19:00:10.964834 22732373614720 run.py:483] Algo bellman_ford step 3467 current loss 0.781430, current_train_items 110976.
I0302 19:00:10.996841 22732373614720 run.py:483] Algo bellman_ford step 3468 current loss 0.883109, current_train_items 111008.
I0302 19:00:11.031754 22732373614720 run.py:483] Algo bellman_ford step 3469 current loss 1.038088, current_train_items 111040.
I0302 19:00:11.051858 22732373614720 run.py:483] Algo bellman_ford step 3470 current loss 0.389419, current_train_items 111072.
I0302 19:00:11.068428 22732373614720 run.py:483] Algo bellman_ford step 3471 current loss 0.533124, current_train_items 111104.
I0302 19:00:11.091309 22732373614720 run.py:483] Algo bellman_ford step 3472 current loss 0.665501, current_train_items 111136.
I0302 19:00:11.123061 22732373614720 run.py:483] Algo bellman_ford step 3473 current loss 0.778685, current_train_items 111168.
I0302 19:00:11.155842 22732373614720 run.py:483] Algo bellman_ford step 3474 current loss 0.868541, current_train_items 111200.
I0302 19:00:11.175524 22732373614720 run.py:483] Algo bellman_ford step 3475 current loss 0.321446, current_train_items 111232.
I0302 19:00:11.191481 22732373614720 run.py:483] Algo bellman_ford step 3476 current loss 0.449428, current_train_items 111264.
I0302 19:00:11.213916 22732373614720 run.py:483] Algo bellman_ford step 3477 current loss 0.641282, current_train_items 111296.
I0302 19:00:11.243378 22732373614720 run.py:483] Algo bellman_ford step 3478 current loss 0.618123, current_train_items 111328.
I0302 19:00:11.277737 22732373614720 run.py:483] Algo bellman_ford step 3479 current loss 0.933598, current_train_items 111360.
I0302 19:00:11.297266 22732373614720 run.py:483] Algo bellman_ford step 3480 current loss 0.360182, current_train_items 111392.
I0302 19:00:11.313093 22732373614720 run.py:483] Algo bellman_ford step 3481 current loss 0.532845, current_train_items 111424.
I0302 19:00:11.336009 22732373614720 run.py:483] Algo bellman_ford step 3482 current loss 0.700249, current_train_items 111456.
I0302 19:00:11.365490 22732373614720 run.py:483] Algo bellman_ford step 3483 current loss 0.739144, current_train_items 111488.
I0302 19:00:11.397675 22732373614720 run.py:483] Algo bellman_ford step 3484 current loss 0.829569, current_train_items 111520.
I0302 19:00:11.417577 22732373614720 run.py:483] Algo bellman_ford step 3485 current loss 0.327454, current_train_items 111552.
I0302 19:00:11.433855 22732373614720 run.py:483] Algo bellman_ford step 3486 current loss 0.567538, current_train_items 111584.
I0302 19:00:11.457375 22732373614720 run.py:483] Algo bellman_ford step 3487 current loss 0.872630, current_train_items 111616.
I0302 19:00:11.486960 22732373614720 run.py:483] Algo bellman_ford step 3488 current loss 0.816852, current_train_items 111648.
I0302 19:00:11.518505 22732373614720 run.py:483] Algo bellman_ford step 3489 current loss 0.792477, current_train_items 111680.
I0302 19:00:11.538255 22732373614720 run.py:483] Algo bellman_ford step 3490 current loss 0.360327, current_train_items 111712.
I0302 19:00:11.553950 22732373614720 run.py:483] Algo bellman_ford step 3491 current loss 0.465824, current_train_items 111744.
I0302 19:00:11.576557 22732373614720 run.py:483] Algo bellman_ford step 3492 current loss 0.778690, current_train_items 111776.
I0302 19:00:11.608036 22732373614720 run.py:483] Algo bellman_ford step 3493 current loss 0.883266, current_train_items 111808.
I0302 19:00:11.639946 22732373614720 run.py:483] Algo bellman_ford step 3494 current loss 0.855496, current_train_items 111840.
I0302 19:00:11.659594 22732373614720 run.py:483] Algo bellman_ford step 3495 current loss 0.329684, current_train_items 111872.
I0302 19:00:11.675673 22732373614720 run.py:483] Algo bellman_ford step 3496 current loss 0.548167, current_train_items 111904.
I0302 19:00:11.698389 22732373614720 run.py:483] Algo bellman_ford step 3497 current loss 0.745191, current_train_items 111936.
I0302 19:00:11.729136 22732373614720 run.py:483] Algo bellman_ford step 3498 current loss 0.840985, current_train_items 111968.
I0302 19:00:11.762766 22732373614720 run.py:483] Algo bellman_ford step 3499 current loss 0.961324, current_train_items 112000.
I0302 19:00:11.782553 22732373614720 run.py:483] Algo bellman_ford step 3500 current loss 0.364283, current_train_items 112032.
I0302 19:00:11.790123 22732373614720 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.8759765625, 'score': 0.8759765625, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0302 19:00:11.790241 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.926, current avg val score is 0.876, val scores are: bellman_ford: 0.876
I0302 19:00:11.806541 22732373614720 run.py:483] Algo bellman_ford step 3501 current loss 0.446145, current_train_items 112064.
I0302 19:00:11.830071 22732373614720 run.py:483] Algo bellman_ford step 3502 current loss 0.654976, current_train_items 112096.
I0302 19:00:11.861351 22732373614720 run.py:483] Algo bellman_ford step 3503 current loss 0.884609, current_train_items 112128.
I0302 19:00:11.896724 22732373614720 run.py:483] Algo bellman_ford step 3504 current loss 0.933127, current_train_items 112160.
I0302 19:00:11.916960 22732373614720 run.py:483] Algo bellman_ford step 3505 current loss 0.451123, current_train_items 112192.
I0302 19:00:11.932091 22732373614720 run.py:483] Algo bellman_ford step 3506 current loss 0.546247, current_train_items 112224.
I0302 19:00:11.955733 22732373614720 run.py:483] Algo bellman_ford step 3507 current loss 0.767710, current_train_items 112256.
I0302 19:00:11.985703 22732373614720 run.py:483] Algo bellman_ford step 3508 current loss 0.802942, current_train_items 112288.
I0302 19:00:12.016000 22732373614720 run.py:483] Algo bellman_ford step 3509 current loss 0.833880, current_train_items 112320.
I0302 19:00:12.035640 22732373614720 run.py:483] Algo bellman_ford step 3510 current loss 0.414684, current_train_items 112352.
I0302 19:00:12.051518 22732373614720 run.py:483] Algo bellman_ford step 3511 current loss 0.467872, current_train_items 112384.
I0302 19:00:12.074164 22732373614720 run.py:483] Algo bellman_ford step 3512 current loss 0.747773, current_train_items 112416.
I0302 19:00:12.103818 22732373614720 run.py:483] Algo bellman_ford step 3513 current loss 0.819262, current_train_items 112448.
I0302 19:00:12.135562 22732373614720 run.py:483] Algo bellman_ford step 3514 current loss 0.843938, current_train_items 112480.
I0302 19:00:12.154902 22732373614720 run.py:483] Algo bellman_ford step 3515 current loss 0.379681, current_train_items 112512.
I0302 19:00:12.170430 22732373614720 run.py:483] Algo bellman_ford step 3516 current loss 0.571918, current_train_items 112544.
I0302 19:00:12.193761 22732373614720 run.py:483] Algo bellman_ford step 3517 current loss 0.816216, current_train_items 112576.
I0302 19:00:12.224678 22732373614720 run.py:483] Algo bellman_ford step 3518 current loss 0.766452, current_train_items 112608.
I0302 19:00:12.259800 22732373614720 run.py:483] Algo bellman_ford step 3519 current loss 1.087309, current_train_items 112640.
I0302 19:00:12.279383 22732373614720 run.py:483] Algo bellman_ford step 3520 current loss 0.394666, current_train_items 112672.
I0302 19:00:12.295240 22732373614720 run.py:483] Algo bellman_ford step 3521 current loss 0.520392, current_train_items 112704.
I0302 19:00:12.317829 22732373614720 run.py:483] Algo bellman_ford step 3522 current loss 0.651415, current_train_items 112736.
I0302 19:00:12.348231 22732373614720 run.py:483] Algo bellman_ford step 3523 current loss 0.862741, current_train_items 112768.
I0302 19:00:12.380610 22732373614720 run.py:483] Algo bellman_ford step 3524 current loss 1.054144, current_train_items 112800.
I0302 19:00:12.400174 22732373614720 run.py:483] Algo bellman_ford step 3525 current loss 0.463352, current_train_items 112832.
I0302 19:00:12.415935 22732373614720 run.py:483] Algo bellman_ford step 3526 current loss 0.609323, current_train_items 112864.
I0302 19:00:12.439353 22732373614720 run.py:483] Algo bellman_ford step 3527 current loss 0.831598, current_train_items 112896.
I0302 19:00:12.471449 22732373614720 run.py:483] Algo bellman_ford step 3528 current loss 0.895447, current_train_items 112928.
I0302 19:00:12.503298 22732373614720 run.py:483] Algo bellman_ford step 3529 current loss 0.874557, current_train_items 112960.
I0302 19:00:12.522664 22732373614720 run.py:483] Algo bellman_ford step 3530 current loss 0.405291, current_train_items 112992.
I0302 19:00:12.538780 22732373614720 run.py:483] Algo bellman_ford step 3531 current loss 0.575465, current_train_items 113024.
I0302 19:00:12.563248 22732373614720 run.py:483] Algo bellman_ford step 3532 current loss 0.815644, current_train_items 113056.
I0302 19:00:12.596446 22732373614720 run.py:483] Algo bellman_ford step 3533 current loss 1.252780, current_train_items 113088.
I0302 19:00:12.630102 22732373614720 run.py:483] Algo bellman_ford step 3534 current loss 1.177932, current_train_items 113120.
I0302 19:00:12.649827 22732373614720 run.py:483] Algo bellman_ford step 3535 current loss 0.475215, current_train_items 113152.
I0302 19:00:12.665921 22732373614720 run.py:483] Algo bellman_ford step 3536 current loss 0.694667, current_train_items 113184.
I0302 19:00:12.689145 22732373614720 run.py:483] Algo bellman_ford step 3537 current loss 0.839861, current_train_items 113216.
I0302 19:00:12.720293 22732373614720 run.py:483] Algo bellman_ford step 3538 current loss 1.014435, current_train_items 113248.
I0302 19:00:12.755580 22732373614720 run.py:483] Algo bellman_ford step 3539 current loss 1.022316, current_train_items 113280.
I0302 19:00:12.775114 22732373614720 run.py:483] Algo bellman_ford step 3540 current loss 0.396972, current_train_items 113312.
I0302 19:00:12.790995 22732373614720 run.py:483] Algo bellman_ford step 3541 current loss 0.647612, current_train_items 113344.
I0302 19:00:12.814635 22732373614720 run.py:483] Algo bellman_ford step 3542 current loss 0.766938, current_train_items 113376.
I0302 19:00:12.844509 22732373614720 run.py:483] Algo bellman_ford step 3543 current loss 0.790374, current_train_items 113408.
I0302 19:00:12.877122 22732373614720 run.py:483] Algo bellman_ford step 3544 current loss 1.063684, current_train_items 113440.
I0302 19:00:12.896447 22732373614720 run.py:483] Algo bellman_ford step 3545 current loss 0.308700, current_train_items 113472.
I0302 19:00:12.912276 22732373614720 run.py:483] Algo bellman_ford step 3546 current loss 0.538767, current_train_items 113504.
I0302 19:00:12.935520 22732373614720 run.py:483] Algo bellman_ford step 3547 current loss 0.856538, current_train_items 113536.
I0302 19:00:12.964687 22732373614720 run.py:483] Algo bellman_ford step 3548 current loss 0.756664, current_train_items 113568.
I0302 19:00:12.997705 22732373614720 run.py:483] Algo bellman_ford step 3549 current loss 0.899226, current_train_items 113600.
I0302 19:00:13.017492 22732373614720 run.py:483] Algo bellman_ford step 3550 current loss 0.403896, current_train_items 113632.
I0302 19:00:13.025223 22732373614720 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0302 19:00:13.025333 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.926, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:00:13.042318 22732373614720 run.py:483] Algo bellman_ford step 3551 current loss 0.597534, current_train_items 113664.
I0302 19:00:13.067765 22732373614720 run.py:483] Algo bellman_ford step 3552 current loss 0.765425, current_train_items 113696.
I0302 19:00:13.098524 22732373614720 run.py:483] Algo bellman_ford step 3553 current loss 0.753146, current_train_items 113728.
I0302 19:00:13.131956 22732373614720 run.py:483] Algo bellman_ford step 3554 current loss 1.007540, current_train_items 113760.
I0302 19:00:13.151780 22732373614720 run.py:483] Algo bellman_ford step 3555 current loss 0.377048, current_train_items 113792.
I0302 19:00:13.167556 22732373614720 run.py:483] Algo bellman_ford step 3556 current loss 0.543024, current_train_items 113824.
I0302 19:00:13.190794 22732373614720 run.py:483] Algo bellman_ford step 3557 current loss 0.774154, current_train_items 113856.
I0302 19:00:13.221611 22732373614720 run.py:483] Algo bellman_ford step 3558 current loss 0.850745, current_train_items 113888.
I0302 19:00:13.253833 22732373614720 run.py:483] Algo bellman_ford step 3559 current loss 0.811753, current_train_items 113920.
I0302 19:00:13.273505 22732373614720 run.py:483] Algo bellman_ford step 3560 current loss 0.400117, current_train_items 113952.
I0302 19:00:13.289575 22732373614720 run.py:483] Algo bellman_ford step 3561 current loss 0.599647, current_train_items 113984.
I0302 19:00:13.313570 22732373614720 run.py:483] Algo bellman_ford step 3562 current loss 0.775802, current_train_items 114016.
I0302 19:00:13.343131 22732373614720 run.py:483] Algo bellman_ford step 3563 current loss 0.768881, current_train_items 114048.
I0302 19:00:13.379369 22732373614720 run.py:483] Algo bellman_ford step 3564 current loss 0.969801, current_train_items 114080.
I0302 19:00:13.399152 22732373614720 run.py:483] Algo bellman_ford step 3565 current loss 0.323130, current_train_items 114112.
I0302 19:00:13.415308 22732373614720 run.py:483] Algo bellman_ford step 3566 current loss 0.614490, current_train_items 114144.
I0302 19:00:13.438844 22732373614720 run.py:483] Algo bellman_ford step 3567 current loss 0.570977, current_train_items 114176.
I0302 19:00:13.469119 22732373614720 run.py:483] Algo bellman_ford step 3568 current loss 0.762393, current_train_items 114208.
I0302 19:00:13.502656 22732373614720 run.py:483] Algo bellman_ford step 3569 current loss 0.951455, current_train_items 114240.
I0302 19:00:13.522283 22732373614720 run.py:483] Algo bellman_ford step 3570 current loss 0.299019, current_train_items 114272.
I0302 19:00:13.538216 22732373614720 run.py:483] Algo bellman_ford step 3571 current loss 0.515284, current_train_items 114304.
I0302 19:00:13.561476 22732373614720 run.py:483] Algo bellman_ford step 3572 current loss 0.828510, current_train_items 114336.
I0302 19:00:13.591233 22732373614720 run.py:483] Algo bellman_ford step 3573 current loss 0.754650, current_train_items 114368.
I0302 19:00:13.626035 22732373614720 run.py:483] Algo bellman_ford step 3574 current loss 0.985459, current_train_items 114400.
I0302 19:00:13.645535 22732373614720 run.py:483] Algo bellman_ford step 3575 current loss 0.351653, current_train_items 114432.
I0302 19:00:13.661698 22732373614720 run.py:483] Algo bellman_ford step 3576 current loss 0.554645, current_train_items 114464.
I0302 19:00:13.684395 22732373614720 run.py:483] Algo bellman_ford step 3577 current loss 0.708843, current_train_items 114496.
I0302 19:00:13.714829 22732373614720 run.py:483] Algo bellman_ford step 3578 current loss 0.725422, current_train_items 114528.
I0302 19:00:13.747241 22732373614720 run.py:483] Algo bellman_ford step 3579 current loss 0.784131, current_train_items 114560.
I0302 19:00:13.766978 22732373614720 run.py:483] Algo bellman_ford step 3580 current loss 0.414180, current_train_items 114592.
I0302 19:00:13.783044 22732373614720 run.py:483] Algo bellman_ford step 3581 current loss 0.618470, current_train_items 114624.
I0302 19:00:13.807634 22732373614720 run.py:483] Algo bellman_ford step 3582 current loss 0.870038, current_train_items 114656.
I0302 19:00:13.839057 22732373614720 run.py:483] Algo bellman_ford step 3583 current loss 0.792842, current_train_items 114688.
I0302 19:00:13.872754 22732373614720 run.py:483] Algo bellman_ford step 3584 current loss 0.846224, current_train_items 114720.
I0302 19:00:13.892395 22732373614720 run.py:483] Algo bellman_ford step 3585 current loss 0.349709, current_train_items 114752.
I0302 19:00:13.908594 22732373614720 run.py:483] Algo bellman_ford step 3586 current loss 0.667293, current_train_items 114784.
I0302 19:00:13.931600 22732373614720 run.py:483] Algo bellman_ford step 3587 current loss 0.845953, current_train_items 114816.
I0302 19:00:13.963672 22732373614720 run.py:483] Algo bellman_ford step 3588 current loss 0.924013, current_train_items 114848.
I0302 19:00:13.997614 22732373614720 run.py:483] Algo bellman_ford step 3589 current loss 0.915893, current_train_items 114880.
I0302 19:00:14.017073 22732373614720 run.py:483] Algo bellman_ford step 3590 current loss 0.370443, current_train_items 114912.
I0302 19:00:14.032856 22732373614720 run.py:483] Algo bellman_ford step 3591 current loss 0.490368, current_train_items 114944.
I0302 19:00:14.057424 22732373614720 run.py:483] Algo bellman_ford step 3592 current loss 0.870801, current_train_items 114976.
I0302 19:00:14.089494 22732373614720 run.py:483] Algo bellman_ford step 3593 current loss 0.840333, current_train_items 115008.
I0302 19:00:14.123690 22732373614720 run.py:483] Algo bellman_ford step 3594 current loss 0.946719, current_train_items 115040.
I0302 19:00:14.143052 22732373614720 run.py:483] Algo bellman_ford step 3595 current loss 0.347532, current_train_items 115072.
I0302 19:00:14.159348 22732373614720 run.py:483] Algo bellman_ford step 3596 current loss 0.526729, current_train_items 115104.
I0302 19:00:14.181999 22732373614720 run.py:483] Algo bellman_ford step 3597 current loss 0.700701, current_train_items 115136.
I0302 19:00:14.211134 22732373614720 run.py:483] Algo bellman_ford step 3598 current loss 0.731072, current_train_items 115168.
I0302 19:00:14.244587 22732373614720 run.py:483] Algo bellman_ford step 3599 current loss 0.823003, current_train_items 115200.
I0302 19:00:14.264203 22732373614720 run.py:483] Algo bellman_ford step 3600 current loss 0.378556, current_train_items 115232.
I0302 19:00:14.271930 22732373614720 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0302 19:00:14.272042 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.926, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:00:14.302401 22732373614720 run.py:483] Algo bellman_ford step 3601 current loss 0.577894, current_train_items 115264.
I0302 19:00:14.326375 22732373614720 run.py:483] Algo bellman_ford step 3602 current loss 0.720887, current_train_items 115296.
I0302 19:00:14.357020 22732373614720 run.py:483] Algo bellman_ford step 3603 current loss 0.703373, current_train_items 115328.
I0302 19:00:14.393759 22732373614720 run.py:483] Algo bellman_ford step 3604 current loss 0.923160, current_train_items 115360.
I0302 19:00:14.413673 22732373614720 run.py:483] Algo bellman_ford step 3605 current loss 0.375669, current_train_items 115392.
I0302 19:00:14.429970 22732373614720 run.py:483] Algo bellman_ford step 3606 current loss 0.571718, current_train_items 115424.
I0302 19:00:14.452116 22732373614720 run.py:483] Algo bellman_ford step 3607 current loss 0.596308, current_train_items 115456.
I0302 19:00:14.481694 22732373614720 run.py:483] Algo bellman_ford step 3608 current loss 0.803919, current_train_items 115488.
I0302 19:00:14.513347 22732373614720 run.py:483] Algo bellman_ford step 3609 current loss 0.807885, current_train_items 115520.
I0302 19:00:14.532807 22732373614720 run.py:483] Algo bellman_ford step 3610 current loss 0.392030, current_train_items 115552.
I0302 19:00:14.549126 22732373614720 run.py:483] Algo bellman_ford step 3611 current loss 0.547572, current_train_items 115584.
I0302 19:00:14.573138 22732373614720 run.py:483] Algo bellman_ford step 3612 current loss 0.668058, current_train_items 115616.
I0302 19:00:14.603364 22732373614720 run.py:483] Algo bellman_ford step 3613 current loss 0.699038, current_train_items 115648.
I0302 19:00:14.634863 22732373614720 run.py:483] Algo bellman_ford step 3614 current loss 0.831691, current_train_items 115680.
I0302 19:00:14.655099 22732373614720 run.py:483] Algo bellman_ford step 3615 current loss 0.369775, current_train_items 115712.
I0302 19:00:14.671524 22732373614720 run.py:483] Algo bellman_ford step 3616 current loss 0.626399, current_train_items 115744.
I0302 19:00:14.694439 22732373614720 run.py:483] Algo bellman_ford step 3617 current loss 0.710065, current_train_items 115776.
I0302 19:00:14.725895 22732373614720 run.py:483] Algo bellman_ford step 3618 current loss 0.920140, current_train_items 115808.
I0302 19:00:14.755798 22732373614720 run.py:483] Algo bellman_ford step 3619 current loss 0.749665, current_train_items 115840.
I0302 19:00:14.775275 22732373614720 run.py:483] Algo bellman_ford step 3620 current loss 0.323423, current_train_items 115872.
I0302 19:00:14.791394 22732373614720 run.py:483] Algo bellman_ford step 3621 current loss 0.513281, current_train_items 115904.
I0302 19:00:14.814316 22732373614720 run.py:483] Algo bellman_ford step 3622 current loss 0.726741, current_train_items 115936.
I0302 19:00:14.845147 22732373614720 run.py:483] Algo bellman_ford step 3623 current loss 0.860237, current_train_items 115968.
I0302 19:00:14.877204 22732373614720 run.py:483] Algo bellman_ford step 3624 current loss 0.749755, current_train_items 116000.
I0302 19:00:14.896446 22732373614720 run.py:483] Algo bellman_ford step 3625 current loss 0.397376, current_train_items 116032.
I0302 19:00:14.912787 22732373614720 run.py:483] Algo bellman_ford step 3626 current loss 0.631177, current_train_items 116064.
I0302 19:00:14.936264 22732373614720 run.py:483] Algo bellman_ford step 3627 current loss 0.791770, current_train_items 116096.
I0302 19:00:14.966812 22732373614720 run.py:483] Algo bellman_ford step 3628 current loss 0.839679, current_train_items 116128.
I0302 19:00:14.999170 22732373614720 run.py:483] Algo bellman_ford step 3629 current loss 0.877433, current_train_items 116160.
I0302 19:00:15.018595 22732373614720 run.py:483] Algo bellman_ford step 3630 current loss 0.342204, current_train_items 116192.
I0302 19:00:15.034471 22732373614720 run.py:483] Algo bellman_ford step 3631 current loss 0.501952, current_train_items 116224.
I0302 19:00:15.057850 22732373614720 run.py:483] Algo bellman_ford step 3632 current loss 0.712456, current_train_items 116256.
I0302 19:00:15.088567 22732373614720 run.py:483] Algo bellman_ford step 3633 current loss 1.015927, current_train_items 116288.
I0302 19:00:15.120650 22732373614720 run.py:483] Algo bellman_ford step 3634 current loss 0.971355, current_train_items 116320.
I0302 19:00:15.139935 22732373614720 run.py:483] Algo bellman_ford step 3635 current loss 0.304758, current_train_items 116352.
I0302 19:00:15.155921 22732373614720 run.py:483] Algo bellman_ford step 3636 current loss 0.554366, current_train_items 116384.
I0302 19:00:15.178010 22732373614720 run.py:483] Algo bellman_ford step 3637 current loss 0.708532, current_train_items 116416.
I0302 19:00:15.208715 22732373614720 run.py:483] Algo bellman_ford step 3638 current loss 0.839557, current_train_items 116448.
I0302 19:00:15.240849 22732373614720 run.py:483] Algo bellman_ford step 3639 current loss 0.764493, current_train_items 116480.
I0302 19:00:15.260547 22732373614720 run.py:483] Algo bellman_ford step 3640 current loss 0.338420, current_train_items 116512.
I0302 19:00:15.276225 22732373614720 run.py:483] Algo bellman_ford step 3641 current loss 0.502472, current_train_items 116544.
I0302 19:00:15.299814 22732373614720 run.py:483] Algo bellman_ford step 3642 current loss 0.826868, current_train_items 116576.
I0302 19:00:15.333012 22732373614720 run.py:483] Algo bellman_ford step 3643 current loss 0.890761, current_train_items 116608.
I0302 19:00:15.366551 22732373614720 run.py:483] Algo bellman_ford step 3644 current loss 0.977753, current_train_items 116640.
I0302 19:00:15.385959 22732373614720 run.py:483] Algo bellman_ford step 3645 current loss 0.355297, current_train_items 116672.
I0302 19:00:15.401984 22732373614720 run.py:483] Algo bellman_ford step 3646 current loss 0.501592, current_train_items 116704.
I0302 19:00:15.426379 22732373614720 run.py:483] Algo bellman_ford step 3647 current loss 0.687313, current_train_items 116736.
I0302 19:00:15.456113 22732373614720 run.py:483] Algo bellman_ford step 3648 current loss 0.716691, current_train_items 116768.
I0302 19:00:15.489233 22732373614720 run.py:483] Algo bellman_ford step 3649 current loss 0.906424, current_train_items 116800.
I0302 19:00:15.508485 22732373614720 run.py:483] Algo bellman_ford step 3650 current loss 0.386362, current_train_items 116832.
I0302 19:00:15.516516 22732373614720 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0302 19:00:15.516626 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:00:15.533462 22732373614720 run.py:483] Algo bellman_ford step 3651 current loss 0.611421, current_train_items 116864.
I0302 19:00:15.556492 22732373614720 run.py:483] Algo bellman_ford step 3652 current loss 0.713717, current_train_items 116896.
I0302 19:00:15.586848 22732373614720 run.py:483] Algo bellman_ford step 3653 current loss 0.706789, current_train_items 116928.
I0302 19:00:15.619630 22732373614720 run.py:483] Algo bellman_ford step 3654 current loss 0.916070, current_train_items 116960.
I0302 19:00:15.639506 22732373614720 run.py:483] Algo bellman_ford step 3655 current loss 0.340653, current_train_items 116992.
I0302 19:00:15.654840 22732373614720 run.py:483] Algo bellman_ford step 3656 current loss 0.573583, current_train_items 117024.
I0302 19:00:15.678524 22732373614720 run.py:483] Algo bellman_ford step 3657 current loss 0.754465, current_train_items 117056.
I0302 19:00:15.708530 22732373614720 run.py:483] Algo bellman_ford step 3658 current loss 0.696951, current_train_items 117088.
I0302 19:00:15.741286 22732373614720 run.py:483] Algo bellman_ford step 3659 current loss 0.912615, current_train_items 117120.
I0302 19:00:15.761389 22732373614720 run.py:483] Algo bellman_ford step 3660 current loss 0.288884, current_train_items 117152.
I0302 19:00:15.777699 22732373614720 run.py:483] Algo bellman_ford step 3661 current loss 0.551278, current_train_items 117184.
I0302 19:00:15.800165 22732373614720 run.py:483] Algo bellman_ford step 3662 current loss 0.627626, current_train_items 117216.
I0302 19:00:15.831794 22732373614720 run.py:483] Algo bellman_ford step 3663 current loss 0.839325, current_train_items 117248.
I0302 19:00:15.865928 22732373614720 run.py:483] Algo bellman_ford step 3664 current loss 0.882844, current_train_items 117280.
I0302 19:00:15.885668 22732373614720 run.py:483] Algo bellman_ford step 3665 current loss 0.495450, current_train_items 117312.
I0302 19:00:15.901546 22732373614720 run.py:483] Algo bellman_ford step 3666 current loss 0.577876, current_train_items 117344.
I0302 19:00:15.925172 22732373614720 run.py:483] Algo bellman_ford step 3667 current loss 0.695150, current_train_items 117376.
I0302 19:00:15.956317 22732373614720 run.py:483] Algo bellman_ford step 3668 current loss 0.803308, current_train_items 117408.
I0302 19:00:15.991107 22732373614720 run.py:483] Algo bellman_ford step 3669 current loss 1.037195, current_train_items 117440.
I0302 19:00:16.011175 22732373614720 run.py:483] Algo bellman_ford step 3670 current loss 0.415067, current_train_items 117472.
I0302 19:00:16.027489 22732373614720 run.py:483] Algo bellman_ford step 3671 current loss 0.648278, current_train_items 117504.
I0302 19:00:16.049816 22732373614720 run.py:483] Algo bellman_ford step 3672 current loss 0.653889, current_train_items 117536.
I0302 19:00:16.080320 22732373614720 run.py:483] Algo bellman_ford step 3673 current loss 0.806658, current_train_items 117568.
I0302 19:00:16.114023 22732373614720 run.py:483] Algo bellman_ford step 3674 current loss 1.041604, current_train_items 117600.
I0302 19:00:16.133766 22732373614720 run.py:483] Algo bellman_ford step 3675 current loss 0.270863, current_train_items 117632.
I0302 19:00:16.149472 22732373614720 run.py:483] Algo bellman_ford step 3676 current loss 0.508076, current_train_items 117664.
I0302 19:00:16.172265 22732373614720 run.py:483] Algo bellman_ford step 3677 current loss 0.705556, current_train_items 117696.
I0302 19:00:16.202917 22732373614720 run.py:483] Algo bellman_ford step 3678 current loss 0.802817, current_train_items 117728.
I0302 19:00:16.236352 22732373614720 run.py:483] Algo bellman_ford step 3679 current loss 0.873803, current_train_items 117760.
I0302 19:00:16.255536 22732373614720 run.py:483] Algo bellman_ford step 3680 current loss 0.308646, current_train_items 117792.
I0302 19:00:16.271832 22732373614720 run.py:483] Algo bellman_ford step 3681 current loss 0.575594, current_train_items 117824.
I0302 19:00:16.295675 22732373614720 run.py:483] Algo bellman_ford step 3682 current loss 0.732871, current_train_items 117856.
I0302 19:00:16.325636 22732373614720 run.py:483] Algo bellman_ford step 3683 current loss 0.818884, current_train_items 117888.
I0302 19:00:16.358945 22732373614720 run.py:483] Algo bellman_ford step 3684 current loss 0.980664, current_train_items 117920.
I0302 19:00:16.378901 22732373614720 run.py:483] Algo bellman_ford step 3685 current loss 0.295633, current_train_items 117952.
I0302 19:00:16.394880 22732373614720 run.py:483] Algo bellman_ford step 3686 current loss 0.552542, current_train_items 117984.
I0302 19:00:16.417584 22732373614720 run.py:483] Algo bellman_ford step 3687 current loss 0.605511, current_train_items 118016.
I0302 19:00:16.449598 22732373614720 run.py:483] Algo bellman_ford step 3688 current loss 0.828087, current_train_items 118048.
I0302 19:00:16.484301 22732373614720 run.py:483] Algo bellman_ford step 3689 current loss 0.974216, current_train_items 118080.
I0302 19:00:16.504292 22732373614720 run.py:483] Algo bellman_ford step 3690 current loss 0.361697, current_train_items 118112.
I0302 19:00:16.520442 22732373614720 run.py:483] Algo bellman_ford step 3691 current loss 0.559138, current_train_items 118144.
I0302 19:00:16.542290 22732373614720 run.py:483] Algo bellman_ford step 3692 current loss 0.699147, current_train_items 118176.
I0302 19:00:16.574046 22732373614720 run.py:483] Algo bellman_ford step 3693 current loss 0.815735, current_train_items 118208.
I0302 19:00:16.606216 22732373614720 run.py:483] Algo bellman_ford step 3694 current loss 0.859700, current_train_items 118240.
I0302 19:00:16.625754 22732373614720 run.py:483] Algo bellman_ford step 3695 current loss 0.387893, current_train_items 118272.
I0302 19:00:16.642515 22732373614720 run.py:483] Algo bellman_ford step 3696 current loss 0.624908, current_train_items 118304.
I0302 19:00:16.666090 22732373614720 run.py:483] Algo bellman_ford step 3697 current loss 0.784476, current_train_items 118336.
I0302 19:00:16.697441 22732373614720 run.py:483] Algo bellman_ford step 3698 current loss 0.915026, current_train_items 118368.
I0302 19:00:16.728720 22732373614720 run.py:483] Algo bellman_ford step 3699 current loss 0.912087, current_train_items 118400.
I0302 19:00:16.748745 22732373614720 run.py:483] Algo bellman_ford step 3700 current loss 0.428521, current_train_items 118432.
I0302 19:00:16.756514 22732373614720 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0302 19:00:16.756622 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:00:16.773059 22732373614720 run.py:483] Algo bellman_ford step 3701 current loss 0.431218, current_train_items 118464.
I0302 19:00:16.796802 22732373614720 run.py:483] Algo bellman_ford step 3702 current loss 0.703861, current_train_items 118496.
I0302 19:00:16.829385 22732373614720 run.py:483] Algo bellman_ford step 3703 current loss 0.803383, current_train_items 118528.
I0302 19:00:16.862030 22732373614720 run.py:483] Algo bellman_ford step 3704 current loss 0.812996, current_train_items 118560.
I0302 19:00:16.881540 22732373614720 run.py:483] Algo bellman_ford step 3705 current loss 0.342856, current_train_items 118592.
I0302 19:00:16.897441 22732373614720 run.py:483] Algo bellman_ford step 3706 current loss 0.712038, current_train_items 118624.
I0302 19:00:16.920533 22732373614720 run.py:483] Algo bellman_ford step 3707 current loss 0.740670, current_train_items 118656.
I0302 19:00:16.949840 22732373614720 run.py:483] Algo bellman_ford step 3708 current loss 0.684694, current_train_items 118688.
I0302 19:00:16.982519 22732373614720 run.py:483] Algo bellman_ford step 3709 current loss 0.905093, current_train_items 118720.
I0302 19:00:17.002014 22732373614720 run.py:483] Algo bellman_ford step 3710 current loss 0.385878, current_train_items 118752.
I0302 19:00:17.018105 22732373614720 run.py:483] Algo bellman_ford step 3711 current loss 0.565816, current_train_items 118784.
I0302 19:00:17.041347 22732373614720 run.py:483] Algo bellman_ford step 3712 current loss 0.787443, current_train_items 118816.
I0302 19:00:17.073458 22732373614720 run.py:483] Algo bellman_ford step 3713 current loss 0.803875, current_train_items 118848.
I0302 19:00:17.107250 22732373614720 run.py:483] Algo bellman_ford step 3714 current loss 0.897712, current_train_items 118880.
I0302 19:00:17.126538 22732373614720 run.py:483] Algo bellman_ford step 3715 current loss 0.319053, current_train_items 118912.
I0302 19:00:17.142598 22732373614720 run.py:483] Algo bellman_ford step 3716 current loss 0.512144, current_train_items 118944.
I0302 19:00:17.165636 22732373614720 run.py:483] Algo bellman_ford step 3717 current loss 0.711321, current_train_items 118976.
I0302 19:00:17.196137 22732373614720 run.py:483] Algo bellman_ford step 3718 current loss 0.786250, current_train_items 119008.
I0302 19:00:17.227999 22732373614720 run.py:483] Algo bellman_ford step 3719 current loss 1.021998, current_train_items 119040.
I0302 19:00:17.247474 22732373614720 run.py:483] Algo bellman_ford step 3720 current loss 0.343223, current_train_items 119072.
I0302 19:00:17.263683 22732373614720 run.py:483] Algo bellman_ford step 3721 current loss 0.622212, current_train_items 119104.
I0302 19:00:17.286571 22732373614720 run.py:483] Algo bellman_ford step 3722 current loss 0.655936, current_train_items 119136.
I0302 19:00:17.316391 22732373614720 run.py:483] Algo bellman_ford step 3723 current loss 0.767031, current_train_items 119168.
I0302 19:00:17.348778 22732373614720 run.py:483] Algo bellman_ford step 3724 current loss 0.845042, current_train_items 119200.
I0302 19:00:17.368404 22732373614720 run.py:483] Algo bellman_ford step 3725 current loss 0.362004, current_train_items 119232.
I0302 19:00:17.384736 22732373614720 run.py:483] Algo bellman_ford step 3726 current loss 0.441117, current_train_items 119264.
I0302 19:00:17.408331 22732373614720 run.py:483] Algo bellman_ford step 3727 current loss 0.673422, current_train_items 119296.
I0302 19:00:17.438855 22732373614720 run.py:483] Algo bellman_ford step 3728 current loss 0.847958, current_train_items 119328.
I0302 19:00:17.471597 22732373614720 run.py:483] Algo bellman_ford step 3729 current loss 0.839254, current_train_items 119360.
I0302 19:00:17.490833 22732373614720 run.py:483] Algo bellman_ford step 3730 current loss 0.309404, current_train_items 119392.
I0302 19:00:17.506647 22732373614720 run.py:483] Algo bellman_ford step 3731 current loss 0.455930, current_train_items 119424.
I0302 19:00:17.529939 22732373614720 run.py:483] Algo bellman_ford step 3732 current loss 0.706254, current_train_items 119456.
I0302 19:00:17.558554 22732373614720 run.py:483] Algo bellman_ford step 3733 current loss 0.663554, current_train_items 119488.
I0302 19:00:17.592031 22732373614720 run.py:483] Algo bellman_ford step 3734 current loss 0.876549, current_train_items 119520.
I0302 19:00:17.611437 22732373614720 run.py:483] Algo bellman_ford step 3735 current loss 0.356683, current_train_items 119552.
I0302 19:00:17.627214 22732373614720 run.py:483] Algo bellman_ford step 3736 current loss 0.509512, current_train_items 119584.
I0302 19:00:17.649582 22732373614720 run.py:483] Algo bellman_ford step 3737 current loss 0.589986, current_train_items 119616.
I0302 19:00:17.680450 22732373614720 run.py:483] Algo bellman_ford step 3738 current loss 0.837816, current_train_items 119648.
I0302 19:00:17.712917 22732373614720 run.py:483] Algo bellman_ford step 3739 current loss 0.817794, current_train_items 119680.
I0302 19:00:17.732414 22732373614720 run.py:483] Algo bellman_ford step 3740 current loss 0.362210, current_train_items 119712.
I0302 19:00:17.748219 22732373614720 run.py:483] Algo bellman_ford step 3741 current loss 0.502513, current_train_items 119744.
I0302 19:00:17.770709 22732373614720 run.py:483] Algo bellman_ford step 3742 current loss 0.630363, current_train_items 119776.
I0302 19:00:17.801286 22732373614720 run.py:483] Algo bellman_ford step 3743 current loss 0.825267, current_train_items 119808.
I0302 19:00:17.832260 22732373614720 run.py:483] Algo bellman_ford step 3744 current loss 0.884068, current_train_items 119840.
I0302 19:00:17.851471 22732373614720 run.py:483] Algo bellman_ford step 3745 current loss 0.294502, current_train_items 119872.
I0302 19:00:17.867167 22732373614720 run.py:483] Algo bellman_ford step 3746 current loss 0.458828, current_train_items 119904.
I0302 19:00:17.890579 22732373614720 run.py:483] Algo bellman_ford step 3747 current loss 0.610039, current_train_items 119936.
I0302 19:00:17.921643 22732373614720 run.py:483] Algo bellman_ford step 3748 current loss 0.810899, current_train_items 119968.
I0302 19:00:17.955815 22732373614720 run.py:483] Algo bellman_ford step 3749 current loss 0.998843, current_train_items 120000.
I0302 19:00:17.975166 22732373614720 run.py:483] Algo bellman_ford step 3750 current loss 0.336372, current_train_items 120032.
I0302 19:00:17.983148 22732373614720 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0302 19:00:17.983287 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 19:00:18.000041 22732373614720 run.py:483] Algo bellman_ford step 3751 current loss 0.696573, current_train_items 120064.
I0302 19:00:18.023899 22732373614720 run.py:483] Algo bellman_ford step 3752 current loss 0.711423, current_train_items 120096.
I0302 19:00:18.055268 22732373614720 run.py:483] Algo bellman_ford step 3753 current loss 0.806958, current_train_items 120128.
I0302 19:00:18.091013 22732373614720 run.py:483] Algo bellman_ford step 3754 current loss 0.839779, current_train_items 120160.
I0302 19:00:18.111049 22732373614720 run.py:483] Algo bellman_ford step 3755 current loss 0.349764, current_train_items 120192.
I0302 19:00:18.126919 22732373614720 run.py:483] Algo bellman_ford step 3756 current loss 0.572743, current_train_items 120224.
I0302 19:00:18.150455 22732373614720 run.py:483] Algo bellman_ford step 3757 current loss 0.708362, current_train_items 120256.
I0302 19:00:18.181358 22732373614720 run.py:483] Algo bellman_ford step 3758 current loss 0.778424, current_train_items 120288.
I0302 19:00:18.212673 22732373614720 run.py:483] Algo bellman_ford step 3759 current loss 0.755877, current_train_items 120320.
I0302 19:00:18.232640 22732373614720 run.py:483] Algo bellman_ford step 3760 current loss 0.379396, current_train_items 120352.
I0302 19:00:18.248787 22732373614720 run.py:483] Algo bellman_ford step 3761 current loss 0.612373, current_train_items 120384.
I0302 19:00:18.271330 22732373614720 run.py:483] Algo bellman_ford step 3762 current loss 0.656961, current_train_items 120416.
I0302 19:00:18.301475 22732373614720 run.py:483] Algo bellman_ford step 3763 current loss 0.759707, current_train_items 120448.
I0302 19:00:18.334739 22732373614720 run.py:483] Algo bellman_ford step 3764 current loss 1.008789, current_train_items 120480.
I0302 19:00:18.354498 22732373614720 run.py:483] Algo bellman_ford step 3765 current loss 0.421048, current_train_items 120512.
I0302 19:00:18.371214 22732373614720 run.py:483] Algo bellman_ford step 3766 current loss 0.491429, current_train_items 120544.
I0302 19:00:18.394828 22732373614720 run.py:483] Algo bellman_ford step 3767 current loss 0.768223, current_train_items 120576.
I0302 19:00:18.426008 22732373614720 run.py:483] Algo bellman_ford step 3768 current loss 0.806953, current_train_items 120608.
I0302 19:00:18.458362 22732373614720 run.py:483] Algo bellman_ford step 3769 current loss 0.816832, current_train_items 120640.
I0302 19:00:18.478439 22732373614720 run.py:483] Algo bellman_ford step 3770 current loss 0.330987, current_train_items 120672.
I0302 19:00:18.494758 22732373614720 run.py:483] Algo bellman_ford step 3771 current loss 0.508203, current_train_items 120704.
I0302 19:00:18.517152 22732373614720 run.py:483] Algo bellman_ford step 3772 current loss 0.693207, current_train_items 120736.
I0302 19:00:18.547185 22732373614720 run.py:483] Algo bellman_ford step 3773 current loss 0.735596, current_train_items 120768.
I0302 19:00:18.577167 22732373614720 run.py:483] Algo bellman_ford step 3774 current loss 0.782906, current_train_items 120800.
I0302 19:00:18.596911 22732373614720 run.py:483] Algo bellman_ford step 3775 current loss 0.379502, current_train_items 120832.
I0302 19:00:18.613379 22732373614720 run.py:483] Algo bellman_ford step 3776 current loss 0.594571, current_train_items 120864.
I0302 19:00:18.635727 22732373614720 run.py:483] Algo bellman_ford step 3777 current loss 0.744141, current_train_items 120896.
I0302 19:00:18.667250 22732373614720 run.py:483] Algo bellman_ford step 3778 current loss 0.774870, current_train_items 120928.
I0302 19:00:18.700427 22732373614720 run.py:483] Algo bellman_ford step 3779 current loss 0.873260, current_train_items 120960.
I0302 19:00:18.720166 22732373614720 run.py:483] Algo bellman_ford step 3780 current loss 0.310384, current_train_items 120992.
I0302 19:00:18.736343 22732373614720 run.py:483] Algo bellman_ford step 3781 current loss 0.522317, current_train_items 121024.
I0302 19:00:18.759657 22732373614720 run.py:483] Algo bellman_ford step 3782 current loss 0.689357, current_train_items 121056.
I0302 19:00:18.788833 22732373614720 run.py:483] Algo bellman_ford step 3783 current loss 0.777890, current_train_items 121088.
I0302 19:00:18.824703 22732373614720 run.py:483] Algo bellman_ford step 3784 current loss 1.036781, current_train_items 121120.
I0302 19:00:18.844605 22732373614720 run.py:483] Algo bellman_ford step 3785 current loss 0.332391, current_train_items 121152.
I0302 19:00:18.860592 22732373614720 run.py:483] Algo bellman_ford step 3786 current loss 0.407761, current_train_items 121184.
I0302 19:00:18.884125 22732373614720 run.py:483] Algo bellman_ford step 3787 current loss 0.712684, current_train_items 121216.
I0302 19:00:18.914746 22732373614720 run.py:483] Algo bellman_ford step 3788 current loss 0.758792, current_train_items 121248.
I0302 19:00:18.947743 22732373614720 run.py:483] Algo bellman_ford step 3789 current loss 0.801578, current_train_items 121280.
I0302 19:00:18.967418 22732373614720 run.py:483] Algo bellman_ford step 3790 current loss 0.300683, current_train_items 121312.
I0302 19:00:18.983645 22732373614720 run.py:483] Algo bellman_ford step 3791 current loss 0.488671, current_train_items 121344.
I0302 19:00:19.006298 22732373614720 run.py:483] Algo bellman_ford step 3792 current loss 0.602836, current_train_items 121376.
I0302 19:00:19.037976 22732373614720 run.py:483] Algo bellman_ford step 3793 current loss 0.940816, current_train_items 121408.
I0302 19:00:19.070559 22732373614720 run.py:483] Algo bellman_ford step 3794 current loss 0.825686, current_train_items 121440.
I0302 19:00:19.090195 22732373614720 run.py:483] Algo bellman_ford step 3795 current loss 0.372266, current_train_items 121472.
I0302 19:00:19.106423 22732373614720 run.py:483] Algo bellman_ford step 3796 current loss 0.617647, current_train_items 121504.
I0302 19:00:19.130020 22732373614720 run.py:483] Algo bellman_ford step 3797 current loss 0.627582, current_train_items 121536.
I0302 19:00:19.161308 22732373614720 run.py:483] Algo bellman_ford step 3798 current loss 0.872050, current_train_items 121568.
I0302 19:00:19.193221 22732373614720 run.py:483] Algo bellman_ford step 3799 current loss 0.786735, current_train_items 121600.
I0302 19:00:19.212893 22732373614720 run.py:483] Algo bellman_ford step 3800 current loss 0.394071, current_train_items 121632.
I0302 19:00:19.220595 22732373614720 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.88671875, 'score': 0.88671875, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0302 19:00:19.220701 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.887, val scores are: bellman_ford: 0.887
I0302 19:00:19.237218 22732373614720 run.py:483] Algo bellman_ford step 3801 current loss 0.552212, current_train_items 121664.
I0302 19:00:19.261766 22732373614720 run.py:483] Algo bellman_ford step 3802 current loss 0.682100, current_train_items 121696.
I0302 19:00:19.292577 22732373614720 run.py:483] Algo bellman_ford step 3803 current loss 0.860781, current_train_items 121728.
I0302 19:00:19.323912 22732373614720 run.py:483] Algo bellman_ford step 3804 current loss 0.807331, current_train_items 121760.
I0302 19:00:19.343693 22732373614720 run.py:483] Algo bellman_ford step 3805 current loss 0.291449, current_train_items 121792.
I0302 19:00:19.359523 22732373614720 run.py:483] Algo bellman_ford step 3806 current loss 0.576635, current_train_items 121824.
I0302 19:00:19.382955 22732373614720 run.py:483] Algo bellman_ford step 3807 current loss 0.577867, current_train_items 121856.
I0302 19:00:19.413719 22732373614720 run.py:483] Algo bellman_ford step 3808 current loss 0.765740, current_train_items 121888.
I0302 19:00:19.447843 22732373614720 run.py:483] Algo bellman_ford step 3809 current loss 0.852796, current_train_items 121920.
I0302 19:00:19.467510 22732373614720 run.py:483] Algo bellman_ford step 3810 current loss 0.450540, current_train_items 121952.
I0302 19:00:19.483875 22732373614720 run.py:483] Algo bellman_ford step 3811 current loss 0.490947, current_train_items 121984.
I0302 19:00:19.507605 22732373614720 run.py:483] Algo bellman_ford step 3812 current loss 0.626238, current_train_items 122016.
I0302 19:00:19.540073 22732373614720 run.py:483] Algo bellman_ford step 3813 current loss 0.856252, current_train_items 122048.
I0302 19:00:19.574535 22732373614720 run.py:483] Algo bellman_ford step 3814 current loss 1.045348, current_train_items 122080.
I0302 19:00:19.594368 22732373614720 run.py:483] Algo bellman_ford step 3815 current loss 0.459505, current_train_items 122112.
I0302 19:00:19.610813 22732373614720 run.py:483] Algo bellman_ford step 3816 current loss 0.570982, current_train_items 122144.
I0302 19:00:19.634067 22732373614720 run.py:483] Algo bellman_ford step 3817 current loss 0.693018, current_train_items 122176.
I0302 19:00:19.664459 22732373614720 run.py:483] Algo bellman_ford step 3818 current loss 0.730328, current_train_items 122208.
I0302 19:00:19.697867 22732373614720 run.py:483] Algo bellman_ford step 3819 current loss 0.911808, current_train_items 122240.
I0302 19:00:19.717285 22732373614720 run.py:483] Algo bellman_ford step 3820 current loss 0.395874, current_train_items 122272.
I0302 19:00:19.732978 22732373614720 run.py:483] Algo bellman_ford step 3821 current loss 0.480569, current_train_items 122304.
I0302 19:00:19.756059 22732373614720 run.py:483] Algo bellman_ford step 3822 current loss 0.707313, current_train_items 122336.
I0302 19:00:19.786721 22732373614720 run.py:483] Algo bellman_ford step 3823 current loss 0.842515, current_train_items 122368.
I0302 19:00:19.820547 22732373614720 run.py:483] Algo bellman_ford step 3824 current loss 0.990215, current_train_items 122400.
I0302 19:00:19.840395 22732373614720 run.py:483] Algo bellman_ford step 3825 current loss 0.338832, current_train_items 122432.
I0302 19:00:19.856097 22732373614720 run.py:483] Algo bellman_ford step 3826 current loss 0.486110, current_train_items 122464.
I0302 19:00:19.879840 22732373614720 run.py:483] Algo bellman_ford step 3827 current loss 0.624363, current_train_items 122496.
I0302 19:00:19.911149 22732373614720 run.py:483] Algo bellman_ford step 3828 current loss 0.986383, current_train_items 122528.
I0302 19:00:19.942990 22732373614720 run.py:483] Algo bellman_ford step 3829 current loss 1.178150, current_train_items 122560.
I0302 19:00:19.962684 22732373614720 run.py:483] Algo bellman_ford step 3830 current loss 0.287961, current_train_items 122592.
I0302 19:00:19.978787 22732373614720 run.py:483] Algo bellman_ford step 3831 current loss 0.606410, current_train_items 122624.
I0302 19:00:20.001342 22732373614720 run.py:483] Algo bellman_ford step 3832 current loss 0.583930, current_train_items 122656.
I0302 19:00:20.032954 22732373614720 run.py:483] Algo bellman_ford step 3833 current loss 0.932882, current_train_items 122688.
I0302 19:00:20.065161 22732373614720 run.py:483] Algo bellman_ford step 3834 current loss 0.866635, current_train_items 122720.
I0302 19:00:20.084880 22732373614720 run.py:483] Algo bellman_ford step 3835 current loss 0.444470, current_train_items 122752.
I0302 19:00:20.100424 22732373614720 run.py:483] Algo bellman_ford step 3836 current loss 0.500246, current_train_items 122784.
I0302 19:00:20.124681 22732373614720 run.py:483] Algo bellman_ford step 3837 current loss 0.888224, current_train_items 122816.
I0302 19:00:20.155658 22732373614720 run.py:483] Algo bellman_ford step 3838 current loss 0.864543, current_train_items 122848.
I0302 19:00:20.188654 22732373614720 run.py:483] Algo bellman_ford step 3839 current loss 0.891667, current_train_items 122880.
I0302 19:00:20.208229 22732373614720 run.py:483] Algo bellman_ford step 3840 current loss 0.382228, current_train_items 122912.
I0302 19:00:20.223891 22732373614720 run.py:483] Algo bellman_ford step 3841 current loss 0.675635, current_train_items 122944.
I0302 19:00:20.246379 22732373614720 run.py:483] Algo bellman_ford step 3842 current loss 0.770209, current_train_items 122976.
I0302 19:00:20.277351 22732373614720 run.py:483] Algo bellman_ford step 3843 current loss 0.769119, current_train_items 123008.
I0302 19:00:20.311284 22732373614720 run.py:483] Algo bellman_ford step 3844 current loss 1.067361, current_train_items 123040.
I0302 19:00:20.330626 22732373614720 run.py:483] Algo bellman_ford step 3845 current loss 0.364128, current_train_items 123072.
I0302 19:00:20.346525 22732373614720 run.py:483] Algo bellman_ford step 3846 current loss 0.485610, current_train_items 123104.
I0302 19:00:20.371429 22732373614720 run.py:483] Algo bellman_ford step 3847 current loss 0.830269, current_train_items 123136.
I0302 19:00:20.401297 22732373614720 run.py:483] Algo bellman_ford step 3848 current loss 0.748850, current_train_items 123168.
I0302 19:00:20.434508 22732373614720 run.py:483] Algo bellman_ford step 3849 current loss 0.883065, current_train_items 123200.
I0302 19:00:20.454071 22732373614720 run.py:483] Algo bellman_ford step 3850 current loss 0.363537, current_train_items 123232.
I0302 19:00:20.462165 22732373614720 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0302 19:00:20.462273 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 19:00:20.478493 22732373614720 run.py:483] Algo bellman_ford step 3851 current loss 0.436184, current_train_items 123264.
I0302 19:00:20.501295 22732373614720 run.py:483] Algo bellman_ford step 3852 current loss 0.676494, current_train_items 123296.
I0302 19:00:20.533040 22732373614720 run.py:483] Algo bellman_ford step 3853 current loss 0.858292, current_train_items 123328.
I0302 19:00:20.567951 22732373614720 run.py:483] Algo bellman_ford step 3854 current loss 0.818616, current_train_items 123360.
I0302 19:00:20.587949 22732373614720 run.py:483] Algo bellman_ford step 3855 current loss 0.319882, current_train_items 123392.
I0302 19:00:20.603451 22732373614720 run.py:483] Algo bellman_ford step 3856 current loss 0.570197, current_train_items 123424.
I0302 19:00:20.625687 22732373614720 run.py:483] Algo bellman_ford step 3857 current loss 0.741859, current_train_items 123456.
I0302 19:00:20.656352 22732373614720 run.py:483] Algo bellman_ford step 3858 current loss 0.822460, current_train_items 123488.
I0302 19:00:20.692042 22732373614720 run.py:483] Algo bellman_ford step 3859 current loss 1.207440, current_train_items 123520.
I0302 19:00:20.711699 22732373614720 run.py:483] Algo bellman_ford step 3860 current loss 0.399262, current_train_items 123552.
I0302 19:00:20.728034 22732373614720 run.py:483] Algo bellman_ford step 3861 current loss 0.531248, current_train_items 123584.
I0302 19:00:20.750875 22732373614720 run.py:483] Algo bellman_ford step 3862 current loss 0.667393, current_train_items 123616.
I0302 19:00:20.781390 22732373614720 run.py:483] Algo bellman_ford step 3863 current loss 0.846063, current_train_items 123648.
I0302 19:00:20.816395 22732373614720 run.py:483] Algo bellman_ford step 3864 current loss 1.077563, current_train_items 123680.
I0302 19:00:20.835933 22732373614720 run.py:483] Algo bellman_ford step 3865 current loss 0.400574, current_train_items 123712.
I0302 19:00:20.851687 22732373614720 run.py:483] Algo bellman_ford step 3866 current loss 0.696702, current_train_items 123744.
I0302 19:00:20.874967 22732373614720 run.py:483] Algo bellman_ford step 3867 current loss 0.718487, current_train_items 123776.
I0302 19:00:20.905103 22732373614720 run.py:483] Algo bellman_ford step 3868 current loss 0.742128, current_train_items 123808.
I0302 19:00:20.937935 22732373614720 run.py:483] Algo bellman_ford step 3869 current loss 0.832285, current_train_items 123840.
I0302 19:00:20.957993 22732373614720 run.py:483] Algo bellman_ford step 3870 current loss 0.373270, current_train_items 123872.
I0302 19:00:20.973561 22732373614720 run.py:483] Algo bellman_ford step 3871 current loss 0.562171, current_train_items 123904.
I0302 19:00:20.996836 22732373614720 run.py:483] Algo bellman_ford step 3872 current loss 0.855493, current_train_items 123936.
I0302 19:00:21.026178 22732373614720 run.py:483] Algo bellman_ford step 3873 current loss 0.798674, current_train_items 123968.
I0302 19:00:21.058552 22732373614720 run.py:483] Algo bellman_ford step 3874 current loss 1.014050, current_train_items 124000.
I0302 19:00:21.078237 22732373614720 run.py:483] Algo bellman_ford step 3875 current loss 0.462020, current_train_items 124032.
I0302 19:00:21.094540 22732373614720 run.py:483] Algo bellman_ford step 3876 current loss 0.631072, current_train_items 124064.
I0302 19:00:21.118213 22732373614720 run.py:483] Algo bellman_ford step 3877 current loss 0.834740, current_train_items 124096.
I0302 19:00:21.149242 22732373614720 run.py:483] Algo bellman_ford step 3878 current loss 0.819532, current_train_items 124128.
I0302 19:00:21.182351 22732373614720 run.py:483] Algo bellman_ford step 3879 current loss 0.965441, current_train_items 124160.
I0302 19:00:21.202121 22732373614720 run.py:483] Algo bellman_ford step 3880 current loss 0.402548, current_train_items 124192.
I0302 19:00:21.217956 22732373614720 run.py:483] Algo bellman_ford step 3881 current loss 0.547390, current_train_items 124224.
I0302 19:00:21.241104 22732373614720 run.py:483] Algo bellman_ford step 3882 current loss 0.763158, current_train_items 124256.
I0302 19:00:21.271396 22732373614720 run.py:483] Algo bellman_ford step 3883 current loss 0.805388, current_train_items 124288.
I0302 19:00:21.303078 22732373614720 run.py:483] Algo bellman_ford step 3884 current loss 0.901707, current_train_items 124320.
I0302 19:00:21.322894 22732373614720 run.py:483] Algo bellman_ford step 3885 current loss 0.353169, current_train_items 124352.
I0302 19:00:21.339389 22732373614720 run.py:483] Algo bellman_ford step 3886 current loss 0.540224, current_train_items 124384.
I0302 19:00:21.362303 22732373614720 run.py:483] Algo bellman_ford step 3887 current loss 0.768296, current_train_items 124416.
I0302 19:00:21.392314 22732373614720 run.py:483] Algo bellman_ford step 3888 current loss 0.714852, current_train_items 124448.
I0302 19:00:21.426405 22732373614720 run.py:483] Algo bellman_ford step 3889 current loss 0.940786, current_train_items 124480.
I0302 19:00:21.446009 22732373614720 run.py:483] Algo bellman_ford step 3890 current loss 0.329976, current_train_items 124512.
I0302 19:00:21.462177 22732373614720 run.py:483] Algo bellman_ford step 3891 current loss 0.730574, current_train_items 124544.
I0302 19:00:21.485973 22732373614720 run.py:483] Algo bellman_ford step 3892 current loss 0.654732, current_train_items 124576.
I0302 19:00:21.514918 22732373614720 run.py:483] Algo bellman_ford step 3893 current loss 0.642059, current_train_items 124608.
I0302 19:00:21.548865 22732373614720 run.py:483] Algo bellman_ford step 3894 current loss 0.848259, current_train_items 124640.
I0302 19:00:21.568451 22732373614720 run.py:483] Algo bellman_ford step 3895 current loss 0.306955, current_train_items 124672.
I0302 19:00:21.584699 22732373614720 run.py:483] Algo bellman_ford step 3896 current loss 0.560121, current_train_items 124704.
I0302 19:00:21.608721 22732373614720 run.py:483] Algo bellman_ford step 3897 current loss 0.795830, current_train_items 124736.
I0302 19:00:21.639572 22732373614720 run.py:483] Algo bellman_ford step 3898 current loss 0.725082, current_train_items 124768.
I0302 19:00:21.672394 22732373614720 run.py:483] Algo bellman_ford step 3899 current loss 0.872163, current_train_items 124800.
I0302 19:00:21.692459 22732373614720 run.py:483] Algo bellman_ford step 3900 current loss 0.394007, current_train_items 124832.
I0302 19:00:21.700099 22732373614720 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0302 19:00:21.700216 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:00:21.716881 22732373614720 run.py:483] Algo bellman_ford step 3901 current loss 0.605377, current_train_items 124864.
I0302 19:00:21.740273 22732373614720 run.py:483] Algo bellman_ford step 3902 current loss 0.684460, current_train_items 124896.
I0302 19:00:21.771602 22732373614720 run.py:483] Algo bellman_ford step 3903 current loss 0.790445, current_train_items 124928.
I0302 19:00:21.804538 22732373614720 run.py:483] Algo bellman_ford step 3904 current loss 0.806249, current_train_items 124960.
I0302 19:00:21.824434 22732373614720 run.py:483] Algo bellman_ford step 3905 current loss 0.276307, current_train_items 124992.
I0302 19:00:21.840070 22732373614720 run.py:483] Algo bellman_ford step 3906 current loss 0.460560, current_train_items 125024.
I0302 19:00:21.862027 22732373614720 run.py:483] Algo bellman_ford step 3907 current loss 0.597312, current_train_items 125056.
I0302 19:00:21.892864 22732373614720 run.py:483] Algo bellman_ford step 3908 current loss 0.760238, current_train_items 125088.
I0302 19:00:21.926467 22732373614720 run.py:483] Algo bellman_ford step 3909 current loss 0.815753, current_train_items 125120.
I0302 19:00:21.945891 22732373614720 run.py:483] Algo bellman_ford step 3910 current loss 0.366156, current_train_items 125152.
I0302 19:00:21.961726 22732373614720 run.py:483] Algo bellman_ford step 3911 current loss 0.521414, current_train_items 125184.
I0302 19:00:21.985236 22732373614720 run.py:483] Algo bellman_ford step 3912 current loss 0.700293, current_train_items 125216.
I0302 19:00:22.015217 22732373614720 run.py:483] Algo bellman_ford step 3913 current loss 0.785514, current_train_items 125248.
I0302 19:00:22.045464 22732373614720 run.py:483] Algo bellman_ford step 3914 current loss 0.902495, current_train_items 125280.
I0302 19:00:22.064589 22732373614720 run.py:483] Algo bellman_ford step 3915 current loss 0.415034, current_train_items 125312.
I0302 19:00:22.080488 22732373614720 run.py:483] Algo bellman_ford step 3916 current loss 0.513126, current_train_items 125344.
I0302 19:00:22.104856 22732373614720 run.py:483] Algo bellman_ford step 3917 current loss 0.721976, current_train_items 125376.
I0302 19:00:22.134332 22732373614720 run.py:483] Algo bellman_ford step 3918 current loss 0.782105, current_train_items 125408.
I0302 19:00:22.167365 22732373614720 run.py:483] Algo bellman_ford step 3919 current loss 0.813725, current_train_items 125440.
I0302 19:00:22.186983 22732373614720 run.py:483] Algo bellman_ford step 3920 current loss 0.353548, current_train_items 125472.
I0302 19:00:22.203228 22732373614720 run.py:483] Algo bellman_ford step 3921 current loss 0.668142, current_train_items 125504.
I0302 19:00:22.227622 22732373614720 run.py:483] Algo bellman_ford step 3922 current loss 0.683085, current_train_items 125536.
I0302 19:00:22.258933 22732373614720 run.py:483] Algo bellman_ford step 3923 current loss 0.837400, current_train_items 125568.
I0302 19:00:22.294218 22732373614720 run.py:483] Algo bellman_ford step 3924 current loss 0.863481, current_train_items 125600.
I0302 19:00:22.314239 22732373614720 run.py:483] Algo bellman_ford step 3925 current loss 0.364151, current_train_items 125632.
I0302 19:00:22.330352 22732373614720 run.py:483] Algo bellman_ford step 3926 current loss 0.460820, current_train_items 125664.
I0302 19:00:22.353852 22732373614720 run.py:483] Algo bellman_ford step 3927 current loss 0.640926, current_train_items 125696.
I0302 19:00:22.384052 22732373614720 run.py:483] Algo bellman_ford step 3928 current loss 0.805863, current_train_items 125728.
I0302 19:00:22.416932 22732373614720 run.py:483] Algo bellman_ford step 3929 current loss 0.808604, current_train_items 125760.
I0302 19:00:22.436385 22732373614720 run.py:483] Algo bellman_ford step 3930 current loss 0.324187, current_train_items 125792.
I0302 19:00:22.452089 22732373614720 run.py:483] Algo bellman_ford step 3931 current loss 0.611530, current_train_items 125824.
I0302 19:00:22.475214 22732373614720 run.py:483] Algo bellman_ford step 3932 current loss 0.657209, current_train_items 125856.
I0302 19:00:22.506999 22732373614720 run.py:483] Algo bellman_ford step 3933 current loss 0.753808, current_train_items 125888.
I0302 19:00:22.540216 22732373614720 run.py:483] Algo bellman_ford step 3934 current loss 0.920773, current_train_items 125920.
I0302 19:00:22.559797 22732373614720 run.py:483] Algo bellman_ford step 3935 current loss 0.357070, current_train_items 125952.
I0302 19:00:22.575989 22732373614720 run.py:483] Algo bellman_ford step 3936 current loss 0.539320, current_train_items 125984.
I0302 19:00:22.599074 22732373614720 run.py:483] Algo bellman_ford step 3937 current loss 0.585299, current_train_items 126016.
I0302 19:00:22.630106 22732373614720 run.py:483] Algo bellman_ford step 3938 current loss 0.771559, current_train_items 126048.
I0302 19:00:22.664061 22732373614720 run.py:483] Algo bellman_ford step 3939 current loss 0.790093, current_train_items 126080.
I0302 19:00:22.683707 22732373614720 run.py:483] Algo bellman_ford step 3940 current loss 0.373986, current_train_items 126112.
I0302 19:00:22.699804 22732373614720 run.py:483] Algo bellman_ford step 3941 current loss 0.504076, current_train_items 126144.
I0302 19:00:22.722984 22732373614720 run.py:483] Algo bellman_ford step 3942 current loss 0.707216, current_train_items 126176.
I0302 19:00:22.752439 22732373614720 run.py:483] Algo bellman_ford step 3943 current loss 0.699341, current_train_items 126208.
I0302 19:00:22.786571 22732373614720 run.py:483] Algo bellman_ford step 3944 current loss 1.010035, current_train_items 126240.
I0302 19:00:22.806029 22732373614720 run.py:483] Algo bellman_ford step 3945 current loss 0.403134, current_train_items 126272.
I0302 19:00:22.822269 22732373614720 run.py:483] Algo bellman_ford step 3946 current loss 0.548010, current_train_items 126304.
I0302 19:00:22.845348 22732373614720 run.py:483] Algo bellman_ford step 3947 current loss 0.759722, current_train_items 126336.
I0302 19:00:22.874548 22732373614720 run.py:483] Algo bellman_ford step 3948 current loss 0.823730, current_train_items 126368.
I0302 19:00:22.908563 22732373614720 run.py:483] Algo bellman_ford step 3949 current loss 0.794365, current_train_items 126400.
I0302 19:00:22.928193 22732373614720 run.py:483] Algo bellman_ford step 3950 current loss 0.322997, current_train_items 126432.
I0302 19:00:22.936105 22732373614720 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0302 19:00:22.936225 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:00:22.953266 22732373614720 run.py:483] Algo bellman_ford step 3951 current loss 0.686781, current_train_items 126464.
I0302 19:00:22.977565 22732373614720 run.py:483] Algo bellman_ford step 3952 current loss 0.782077, current_train_items 126496.
I0302 19:00:23.008088 22732373614720 run.py:483] Algo bellman_ford step 3953 current loss 0.788820, current_train_items 126528.
I0302 19:00:23.039579 22732373614720 run.py:483] Algo bellman_ford step 3954 current loss 0.916715, current_train_items 126560.
I0302 19:00:23.059504 22732373614720 run.py:483] Algo bellman_ford step 3955 current loss 0.387048, current_train_items 126592.
I0302 19:00:23.075206 22732373614720 run.py:483] Algo bellman_ford step 3956 current loss 0.505155, current_train_items 126624.
I0302 19:00:23.098702 22732373614720 run.py:483] Algo bellman_ford step 3957 current loss 0.679559, current_train_items 126656.
I0302 19:00:23.130050 22732373614720 run.py:483] Algo bellman_ford step 3958 current loss 0.812507, current_train_items 126688.
I0302 19:00:23.165421 22732373614720 run.py:483] Algo bellman_ford step 3959 current loss 0.933258, current_train_items 126720.
I0302 19:00:23.185354 22732373614720 run.py:483] Algo bellman_ford step 3960 current loss 0.323401, current_train_items 126752.
I0302 19:00:23.201198 22732373614720 run.py:483] Algo bellman_ford step 3961 current loss 0.551479, current_train_items 126784.
I0302 19:00:23.223323 22732373614720 run.py:483] Algo bellman_ford step 3962 current loss 0.605412, current_train_items 126816.
I0302 19:00:23.254480 22732373614720 run.py:483] Algo bellman_ford step 3963 current loss 0.701594, current_train_items 126848.
I0302 19:00:23.287860 22732373614720 run.py:483] Algo bellman_ford step 3964 current loss 1.182713, current_train_items 126880.
I0302 19:00:23.307314 22732373614720 run.py:483] Algo bellman_ford step 3965 current loss 0.392712, current_train_items 126912.
I0302 19:00:23.323366 22732373614720 run.py:483] Algo bellman_ford step 3966 current loss 0.496577, current_train_items 126944.
I0302 19:00:23.348126 22732373614720 run.py:483] Algo bellman_ford step 3967 current loss 0.789552, current_train_items 126976.
I0302 19:00:23.379116 22732373614720 run.py:483] Algo bellman_ford step 3968 current loss 0.804475, current_train_items 127008.
I0302 19:00:23.411462 22732373614720 run.py:483] Algo bellman_ford step 3969 current loss 0.869598, current_train_items 127040.
I0302 19:00:23.431563 22732373614720 run.py:483] Algo bellman_ford step 3970 current loss 0.434590, current_train_items 127072.
I0302 19:00:23.447565 22732373614720 run.py:483] Algo bellman_ford step 3971 current loss 0.539782, current_train_items 127104.
I0302 19:00:23.471535 22732373614720 run.py:483] Algo bellman_ford step 3972 current loss 0.671607, current_train_items 127136.
I0302 19:00:23.503069 22732373614720 run.py:483] Algo bellman_ford step 3973 current loss 0.907021, current_train_items 127168.
I0302 19:00:23.535001 22732373614720 run.py:483] Algo bellman_ford step 3974 current loss 0.888624, current_train_items 127200.
I0302 19:00:23.554923 22732373614720 run.py:483] Algo bellman_ford step 3975 current loss 0.315091, current_train_items 127232.
I0302 19:00:23.571333 22732373614720 run.py:483] Algo bellman_ford step 3976 current loss 0.543353, current_train_items 127264.
I0302 19:00:23.594228 22732373614720 run.py:483] Algo bellman_ford step 3977 current loss 0.677041, current_train_items 127296.
I0302 19:00:23.625576 22732373614720 run.py:483] Algo bellman_ford step 3978 current loss 0.911536, current_train_items 127328.
I0302 19:00:23.658025 22732373614720 run.py:483] Algo bellman_ford step 3979 current loss 0.970486, current_train_items 127360.
I0302 19:00:23.677541 22732373614720 run.py:483] Algo bellman_ford step 3980 current loss 0.338924, current_train_items 127392.
I0302 19:00:23.693433 22732373614720 run.py:483] Algo bellman_ford step 3981 current loss 0.525273, current_train_items 127424.
I0302 19:00:23.716043 22732373614720 run.py:483] Algo bellman_ford step 3982 current loss 0.726611, current_train_items 127456.
I0302 19:00:23.747215 22732373614720 run.py:483] Algo bellman_ford step 3983 current loss 0.888601, current_train_items 127488.
I0302 19:00:23.779243 22732373614720 run.py:483] Algo bellman_ford step 3984 current loss 0.897525, current_train_items 127520.
I0302 19:00:23.799282 22732373614720 run.py:483] Algo bellman_ford step 3985 current loss 0.349416, current_train_items 127552.
I0302 19:00:23.815459 22732373614720 run.py:483] Algo bellman_ford step 3986 current loss 0.671875, current_train_items 127584.
I0302 19:00:23.838508 22732373614720 run.py:483] Algo bellman_ford step 3987 current loss 0.612684, current_train_items 127616.
I0302 19:00:23.869395 22732373614720 run.py:483] Algo bellman_ford step 3988 current loss 0.789044, current_train_items 127648.
I0302 19:00:23.903069 22732373614720 run.py:483] Algo bellman_ford step 3989 current loss 0.947363, current_train_items 127680.
I0302 19:00:23.922905 22732373614720 run.py:483] Algo bellman_ford step 3990 current loss 0.367877, current_train_items 127712.
I0302 19:00:23.939373 22732373614720 run.py:483] Algo bellman_ford step 3991 current loss 0.608213, current_train_items 127744.
I0302 19:00:23.961586 22732373614720 run.py:483] Algo bellman_ford step 3992 current loss 0.630044, current_train_items 127776.
I0302 19:00:23.991935 22732373614720 run.py:483] Algo bellman_ford step 3993 current loss 0.783501, current_train_items 127808.
I0302 19:00:24.025161 22732373614720 run.py:483] Algo bellman_ford step 3994 current loss 0.992318, current_train_items 127840.
I0302 19:00:24.044759 22732373614720 run.py:483] Algo bellman_ford step 3995 current loss 0.363128, current_train_items 127872.
I0302 19:00:24.060953 22732373614720 run.py:483] Algo bellman_ford step 3996 current loss 0.589229, current_train_items 127904.
I0302 19:00:24.084365 22732373614720 run.py:483] Algo bellman_ford step 3997 current loss 0.675756, current_train_items 127936.
I0302 19:00:24.114319 22732373614720 run.py:483] Algo bellman_ford step 3998 current loss 0.793057, current_train_items 127968.
I0302 19:00:24.148395 22732373614720 run.py:483] Algo bellman_ford step 3999 current loss 0.896855, current_train_items 128000.
I0302 19:00:24.168561 22732373614720 run.py:483] Algo bellman_ford step 4000 current loss 0.443724, current_train_items 128032.
I0302 19:00:24.176180 22732373614720 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0302 19:00:24.176290 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:00:24.192758 22732373614720 run.py:483] Algo bellman_ford step 4001 current loss 0.529220, current_train_items 128064.
I0302 19:00:24.216610 22732373614720 run.py:483] Algo bellman_ford step 4002 current loss 0.739347, current_train_items 128096.
I0302 19:00:24.247390 22732373614720 run.py:483] Algo bellman_ford step 4003 current loss 0.695975, current_train_items 128128.
I0302 19:00:24.280146 22732373614720 run.py:483] Algo bellman_ford step 4004 current loss 0.773975, current_train_items 128160.
I0302 19:00:24.300086 22732373614720 run.py:483] Algo bellman_ford step 4005 current loss 0.285420, current_train_items 128192.
I0302 19:00:24.315874 22732373614720 run.py:483] Algo bellman_ford step 4006 current loss 0.585543, current_train_items 128224.
I0302 19:00:24.339101 22732373614720 run.py:483] Algo bellman_ford step 4007 current loss 0.814833, current_train_items 128256.
I0302 19:00:24.370281 22732373614720 run.py:483] Algo bellman_ford step 4008 current loss 0.808443, current_train_items 128288.
I0302 19:00:24.402022 22732373614720 run.py:483] Algo bellman_ford step 4009 current loss 0.859850, current_train_items 128320.
I0302 19:00:24.421770 22732373614720 run.py:483] Algo bellman_ford step 4010 current loss 0.387248, current_train_items 128352.
I0302 19:00:24.437716 22732373614720 run.py:483] Algo bellman_ford step 4011 current loss 0.521131, current_train_items 128384.
I0302 19:00:24.460029 22732373614720 run.py:483] Algo bellman_ford step 4012 current loss 0.667417, current_train_items 128416.
I0302 19:00:24.490793 22732373614720 run.py:483] Algo bellman_ford step 4013 current loss 0.832380, current_train_items 128448.
I0302 19:00:24.524386 22732373614720 run.py:483] Algo bellman_ford step 4014 current loss 1.059886, current_train_items 128480.
I0302 19:00:24.544247 22732373614720 run.py:483] Algo bellman_ford step 4015 current loss 0.304966, current_train_items 128512.
I0302 19:00:24.560270 22732373614720 run.py:483] Algo bellman_ford step 4016 current loss 0.591740, current_train_items 128544.
I0302 19:00:24.583548 22732373614720 run.py:483] Algo bellman_ford step 4017 current loss 0.653623, current_train_items 128576.
I0302 19:00:24.613835 22732373614720 run.py:483] Algo bellman_ford step 4018 current loss 0.769743, current_train_items 128608.
I0302 19:00:24.646295 22732373614720 run.py:483] Algo bellman_ford step 4019 current loss 0.823959, current_train_items 128640.
I0302 19:00:24.665735 22732373614720 run.py:483] Algo bellman_ford step 4020 current loss 0.323277, current_train_items 128672.
I0302 19:00:24.681585 22732373614720 run.py:483] Algo bellman_ford step 4021 current loss 0.485586, current_train_items 128704.
I0302 19:00:24.705407 22732373614720 run.py:483] Algo bellman_ford step 4022 current loss 0.767060, current_train_items 128736.
I0302 19:00:24.735189 22732373614720 run.py:483] Algo bellman_ford step 4023 current loss 0.710229, current_train_items 128768.
I0302 19:00:24.770969 22732373614720 run.py:483] Algo bellman_ford step 4024 current loss 1.134759, current_train_items 128800.
I0302 19:00:24.790772 22732373614720 run.py:483] Algo bellman_ford step 4025 current loss 0.366536, current_train_items 128832.
I0302 19:00:24.807004 22732373614720 run.py:483] Algo bellman_ford step 4026 current loss 0.434605, current_train_items 128864.
I0302 19:00:24.830010 22732373614720 run.py:483] Algo bellman_ford step 4027 current loss 0.631454, current_train_items 128896.
I0302 19:00:24.859658 22732373614720 run.py:483] Algo bellman_ford step 4028 current loss 0.792355, current_train_items 128928.
I0302 19:00:24.892056 22732373614720 run.py:483] Algo bellman_ford step 4029 current loss 0.796347, current_train_items 128960.
I0302 19:00:24.911666 22732373614720 run.py:483] Algo bellman_ford step 4030 current loss 0.328286, current_train_items 128992.
I0302 19:00:24.927416 22732373614720 run.py:483] Algo bellman_ford step 4031 current loss 0.486094, current_train_items 129024.
I0302 19:00:24.951039 22732373614720 run.py:483] Algo bellman_ford step 4032 current loss 0.716181, current_train_items 129056.
I0302 19:00:24.982471 22732373614720 run.py:483] Algo bellman_ford step 4033 current loss 0.775663, current_train_items 129088.
I0302 19:00:25.015278 22732373614720 run.py:483] Algo bellman_ford step 4034 current loss 0.906130, current_train_items 129120.
I0302 19:00:25.035104 22732373614720 run.py:483] Algo bellman_ford step 4035 current loss 0.333099, current_train_items 129152.
I0302 19:00:25.050939 22732373614720 run.py:483] Algo bellman_ford step 4036 current loss 0.637982, current_train_items 129184.
I0302 19:00:25.074115 22732373614720 run.py:483] Algo bellman_ford step 4037 current loss 0.688431, current_train_items 129216.
I0302 19:00:25.105460 22732373614720 run.py:483] Algo bellman_ford step 4038 current loss 0.811933, current_train_items 129248.
I0302 19:00:25.138728 22732373614720 run.py:483] Algo bellman_ford step 4039 current loss 1.006168, current_train_items 129280.
I0302 19:00:25.158451 22732373614720 run.py:483] Algo bellman_ford step 4040 current loss 0.312847, current_train_items 129312.
I0302 19:00:25.174520 22732373614720 run.py:483] Algo bellman_ford step 4041 current loss 0.508255, current_train_items 129344.
I0302 19:00:25.197085 22732373614720 run.py:483] Algo bellman_ford step 4042 current loss 0.727527, current_train_items 129376.
I0302 19:00:25.228929 22732373614720 run.py:483] Algo bellman_ford step 4043 current loss 0.819621, current_train_items 129408.
I0302 19:00:25.261012 22732373614720 run.py:483] Algo bellman_ford step 4044 current loss 0.877619, current_train_items 129440.
I0302 19:00:25.280590 22732373614720 run.py:483] Algo bellman_ford step 4045 current loss 0.388531, current_train_items 129472.
I0302 19:00:25.296919 22732373614720 run.py:483] Algo bellman_ford step 4046 current loss 0.613528, current_train_items 129504.
I0302 19:00:25.320082 22732373614720 run.py:483] Algo bellman_ford step 4047 current loss 0.739116, current_train_items 129536.
I0302 19:00:25.349944 22732373614720 run.py:483] Algo bellman_ford step 4048 current loss 0.731854, current_train_items 129568.
I0302 19:00:25.384104 22732373614720 run.py:483] Algo bellman_ford step 4049 current loss 0.856778, current_train_items 129600.
I0302 19:00:25.404011 22732373614720 run.py:483] Algo bellman_ford step 4050 current loss 0.339425, current_train_items 129632.
I0302 19:00:25.411915 22732373614720 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0302 19:00:25.412024 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:00:25.428845 22732373614720 run.py:483] Algo bellman_ford step 4051 current loss 0.591302, current_train_items 129664.
I0302 19:00:25.451735 22732373614720 run.py:483] Algo bellman_ford step 4052 current loss 0.696026, current_train_items 129696.
I0302 19:00:25.482177 22732373614720 run.py:483] Algo bellman_ford step 4053 current loss 0.813798, current_train_items 129728.
I0302 19:00:25.517312 22732373614720 run.py:483] Algo bellman_ford step 4054 current loss 1.014629, current_train_items 129760.
I0302 19:00:25.537163 22732373614720 run.py:483] Algo bellman_ford step 4055 current loss 0.390242, current_train_items 129792.
I0302 19:00:25.552776 22732373614720 run.py:483] Algo bellman_ford step 4056 current loss 0.495499, current_train_items 129824.
I0302 19:00:25.575667 22732373614720 run.py:483] Algo bellman_ford step 4057 current loss 0.608455, current_train_items 129856.
I0302 19:00:25.605329 22732373614720 run.py:483] Algo bellman_ford step 4058 current loss 0.719622, current_train_items 129888.
I0302 19:00:25.636811 22732373614720 run.py:483] Algo bellman_ford step 4059 current loss 0.845606, current_train_items 129920.
I0302 19:00:25.656576 22732373614720 run.py:483] Algo bellman_ford step 4060 current loss 0.373015, current_train_items 129952.
I0302 19:00:25.672978 22732373614720 run.py:483] Algo bellman_ford step 4061 current loss 0.468429, current_train_items 129984.
I0302 19:00:25.695106 22732373614720 run.py:483] Algo bellman_ford step 4062 current loss 0.686159, current_train_items 130016.
I0302 19:00:25.724194 22732373614720 run.py:483] Algo bellman_ford step 4063 current loss 0.739278, current_train_items 130048.
I0302 19:00:25.758347 22732373614720 run.py:483] Algo bellman_ford step 4064 current loss 0.968645, current_train_items 130080.
I0302 19:00:25.777812 22732373614720 run.py:483] Algo bellman_ford step 4065 current loss 0.317944, current_train_items 130112.
I0302 19:00:25.793327 22732373614720 run.py:483] Algo bellman_ford step 4066 current loss 0.574152, current_train_items 130144.
I0302 19:00:25.817404 22732373614720 run.py:483] Algo bellman_ford step 4067 current loss 0.751826, current_train_items 130176.
I0302 19:00:25.847336 22732373614720 run.py:483] Algo bellman_ford step 4068 current loss 0.679268, current_train_items 130208.
I0302 19:00:25.881608 22732373614720 run.py:483] Algo bellman_ford step 4069 current loss 0.879653, current_train_items 130240.
I0302 19:00:25.901268 22732373614720 run.py:483] Algo bellman_ford step 4070 current loss 0.377519, current_train_items 130272.
I0302 19:00:25.917405 22732373614720 run.py:483] Algo bellman_ford step 4071 current loss 0.474395, current_train_items 130304.
I0302 19:00:25.940150 22732373614720 run.py:483] Algo bellman_ford step 4072 current loss 0.686805, current_train_items 130336.
I0302 19:00:25.968809 22732373614720 run.py:483] Algo bellman_ford step 4073 current loss 0.779210, current_train_items 130368.
I0302 19:00:26.000909 22732373614720 run.py:483] Algo bellman_ford step 4074 current loss 0.806826, current_train_items 130400.
I0302 19:00:26.020568 22732373614720 run.py:483] Algo bellman_ford step 4075 current loss 0.348759, current_train_items 130432.
I0302 19:00:26.037137 22732373614720 run.py:483] Algo bellman_ford step 4076 current loss 0.536277, current_train_items 130464.
I0302 19:00:26.060760 22732373614720 run.py:483] Algo bellman_ford step 4077 current loss 0.746656, current_train_items 130496.
I0302 19:00:26.090780 22732373614720 run.py:483] Algo bellman_ford step 4078 current loss 0.687676, current_train_items 130528.
I0302 19:00:26.122932 22732373614720 run.py:483] Algo bellman_ford step 4079 current loss 0.789444, current_train_items 130560.
I0302 19:00:26.142107 22732373614720 run.py:483] Algo bellman_ford step 4080 current loss 0.332316, current_train_items 130592.
I0302 19:00:26.158411 22732373614720 run.py:483] Algo bellman_ford step 4081 current loss 0.505780, current_train_items 130624.
I0302 19:00:26.182674 22732373614720 run.py:483] Algo bellman_ford step 4082 current loss 0.777136, current_train_items 130656.
I0302 19:00:26.214432 22732373614720 run.py:483] Algo bellman_ford step 4083 current loss 0.784635, current_train_items 130688.
I0302 19:00:26.244947 22732373614720 run.py:483] Algo bellman_ford step 4084 current loss 0.802478, current_train_items 130720.
I0302 19:00:26.264863 22732373614720 run.py:483] Algo bellman_ford step 4085 current loss 0.348585, current_train_items 130752.
I0302 19:00:26.281020 22732373614720 run.py:483] Algo bellman_ford step 4086 current loss 0.528431, current_train_items 130784.
I0302 19:00:26.303375 22732373614720 run.py:483] Algo bellman_ford step 4087 current loss 0.750558, current_train_items 130816.
I0302 19:00:26.333216 22732373614720 run.py:483] Algo bellman_ford step 4088 current loss 0.743146, current_train_items 130848.
I0302 19:00:26.364595 22732373614720 run.py:483] Algo bellman_ford step 4089 current loss 0.770738, current_train_items 130880.
I0302 19:00:26.384228 22732373614720 run.py:483] Algo bellman_ford step 4090 current loss 0.342155, current_train_items 130912.
I0302 19:00:26.400255 22732373614720 run.py:483] Algo bellman_ford step 4091 current loss 0.450472, current_train_items 130944.
I0302 19:00:26.424125 22732373614720 run.py:483] Algo bellman_ford step 4092 current loss 0.721854, current_train_items 130976.
I0302 19:00:26.455025 22732373614720 run.py:483] Algo bellman_ford step 4093 current loss 0.799223, current_train_items 131008.
I0302 19:00:26.488330 22732373614720 run.py:483] Algo bellman_ford step 4094 current loss 0.973798, current_train_items 131040.
I0302 19:00:26.507614 22732373614720 run.py:483] Algo bellman_ford step 4095 current loss 0.385491, current_train_items 131072.
I0302 19:00:26.523843 22732373614720 run.py:483] Algo bellman_ford step 4096 current loss 0.633164, current_train_items 131104.
I0302 19:00:26.547280 22732373614720 run.py:483] Algo bellman_ford step 4097 current loss 0.656678, current_train_items 131136.
I0302 19:00:26.577695 22732373614720 run.py:483] Algo bellman_ford step 4098 current loss 0.791167, current_train_items 131168.
I0302 19:00:26.611438 22732373614720 run.py:483] Algo bellman_ford step 4099 current loss 0.967901, current_train_items 131200.
I0302 19:00:26.631520 22732373614720 run.py:483] Algo bellman_ford step 4100 current loss 0.320019, current_train_items 131232.
I0302 19:00:26.639399 22732373614720 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0302 19:00:26.639508 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:26.655505 22732373614720 run.py:483] Algo bellman_ford step 4101 current loss 0.457611, current_train_items 131264.
I0302 19:00:26.677288 22732373614720 run.py:483] Algo bellman_ford step 4102 current loss 0.674846, current_train_items 131296.
I0302 19:00:26.709127 22732373614720 run.py:483] Algo bellman_ford step 4103 current loss 0.885827, current_train_items 131328.
I0302 19:00:26.743646 22732373614720 run.py:483] Algo bellman_ford step 4104 current loss 0.887546, current_train_items 131360.
I0302 19:00:26.763403 22732373614720 run.py:483] Algo bellman_ford step 4105 current loss 0.357052, current_train_items 131392.
I0302 19:00:26.779407 22732373614720 run.py:483] Algo bellman_ford step 4106 current loss 0.518204, current_train_items 131424.
I0302 19:00:26.802962 22732373614720 run.py:483] Algo bellman_ford step 4107 current loss 0.727156, current_train_items 131456.
I0302 19:00:26.833860 22732373614720 run.py:483] Algo bellman_ford step 4108 current loss 0.770464, current_train_items 131488.
I0302 19:00:26.869064 22732373614720 run.py:483] Algo bellman_ford step 4109 current loss 1.023388, current_train_items 131520.
I0302 19:00:26.888707 22732373614720 run.py:483] Algo bellman_ford step 4110 current loss 0.376974, current_train_items 131552.
I0302 19:00:26.905188 22732373614720 run.py:483] Algo bellman_ford step 4111 current loss 0.580384, current_train_items 131584.
I0302 19:00:26.927730 22732373614720 run.py:483] Algo bellman_ford step 4112 current loss 0.682432, current_train_items 131616.
I0302 19:00:26.958942 22732373614720 run.py:483] Algo bellman_ford step 4113 current loss 0.684847, current_train_items 131648.
I0302 19:00:26.992734 22732373614720 run.py:483] Algo bellman_ford step 4114 current loss 0.827245, current_train_items 131680.
I0302 19:00:27.012055 22732373614720 run.py:483] Algo bellman_ford step 4115 current loss 0.286849, current_train_items 131712.
I0302 19:00:27.028327 22732373614720 run.py:483] Algo bellman_ford step 4116 current loss 0.628025, current_train_items 131744.
I0302 19:00:27.052667 22732373614720 run.py:483] Algo bellman_ford step 4117 current loss 0.802201, current_train_items 131776.
I0302 19:00:27.082377 22732373614720 run.py:483] Algo bellman_ford step 4118 current loss 0.711489, current_train_items 131808.
I0302 19:00:27.115760 22732373614720 run.py:483] Algo bellman_ford step 4119 current loss 1.244395, current_train_items 131840.
I0302 19:00:27.135253 22732373614720 run.py:483] Algo bellman_ford step 4120 current loss 0.279894, current_train_items 131872.
I0302 19:00:27.151305 22732373614720 run.py:483] Algo bellman_ford step 4121 current loss 0.488923, current_train_items 131904.
I0302 19:00:27.175295 22732373614720 run.py:483] Algo bellman_ford step 4122 current loss 0.703852, current_train_items 131936.
I0302 19:00:27.206702 22732373614720 run.py:483] Algo bellman_ford step 4123 current loss 0.924699, current_train_items 131968.
I0302 19:00:27.240107 22732373614720 run.py:483] Algo bellman_ford step 4124 current loss 0.895343, current_train_items 132000.
I0302 19:00:27.259446 22732373614720 run.py:483] Algo bellman_ford step 4125 current loss 0.273349, current_train_items 132032.
I0302 19:00:27.275591 22732373614720 run.py:483] Algo bellman_ford step 4126 current loss 0.557421, current_train_items 132064.
I0302 19:00:27.300019 22732373614720 run.py:483] Algo bellman_ford step 4127 current loss 0.844459, current_train_items 132096.
I0302 19:00:27.330378 22732373614720 run.py:483] Algo bellman_ford step 4128 current loss 0.841531, current_train_items 132128.
I0302 19:00:27.362162 22732373614720 run.py:483] Algo bellman_ford step 4129 current loss 0.858625, current_train_items 132160.
I0302 19:00:27.381906 22732373614720 run.py:483] Algo bellman_ford step 4130 current loss 0.379821, current_train_items 132192.
I0302 19:00:27.398115 22732373614720 run.py:483] Algo bellman_ford step 4131 current loss 0.514805, current_train_items 132224.
I0302 19:00:27.421665 22732373614720 run.py:483] Algo bellman_ford step 4132 current loss 0.644329, current_train_items 132256.
I0302 19:00:27.452996 22732373614720 run.py:483] Algo bellman_ford step 4133 current loss 0.690708, current_train_items 132288.
I0302 19:00:27.486931 22732373614720 run.py:483] Algo bellman_ford step 4134 current loss 0.768399, current_train_items 132320.
I0302 19:00:27.506408 22732373614720 run.py:483] Algo bellman_ford step 4135 current loss 0.321400, current_train_items 132352.
I0302 19:00:27.522339 22732373614720 run.py:483] Algo bellman_ford step 4136 current loss 0.519463, current_train_items 132384.
I0302 19:00:27.545306 22732373614720 run.py:483] Algo bellman_ford step 4137 current loss 0.681358, current_train_items 132416.
I0302 19:00:27.575628 22732373614720 run.py:483] Algo bellman_ford step 4138 current loss 0.682454, current_train_items 132448.
I0302 19:00:27.609457 22732373614720 run.py:483] Algo bellman_ford step 4139 current loss 0.787695, current_train_items 132480.
I0302 19:00:27.629247 22732373614720 run.py:483] Algo bellman_ford step 4140 current loss 0.371847, current_train_items 132512.
I0302 19:00:27.644849 22732373614720 run.py:483] Algo bellman_ford step 4141 current loss 0.481643, current_train_items 132544.
I0302 19:00:27.669275 22732373614720 run.py:483] Algo bellman_ford step 4142 current loss 0.825953, current_train_items 132576.
I0302 19:00:27.698904 22732373614720 run.py:483] Algo bellman_ford step 4143 current loss 0.850114, current_train_items 132608.
I0302 19:00:27.731412 22732373614720 run.py:483] Algo bellman_ford step 4144 current loss 0.812645, current_train_items 132640.
I0302 19:00:27.750606 22732373614720 run.py:483] Algo bellman_ford step 4145 current loss 0.307911, current_train_items 132672.
I0302 19:00:27.766287 22732373614720 run.py:483] Algo bellman_ford step 4146 current loss 0.443364, current_train_items 132704.
I0302 19:00:27.789141 22732373614720 run.py:483] Algo bellman_ford step 4147 current loss 0.668548, current_train_items 132736.
I0302 19:00:27.818291 22732373614720 run.py:483] Algo bellman_ford step 4148 current loss 0.688985, current_train_items 132768.
I0302 19:00:27.850522 22732373614720 run.py:483] Algo bellman_ford step 4149 current loss 0.881842, current_train_items 132800.
I0302 19:00:27.869845 22732373614720 run.py:483] Algo bellman_ford step 4150 current loss 0.377858, current_train_items 132832.
I0302 19:00:27.877928 22732373614720 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0302 19:00:27.878037 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:00:27.894709 22732373614720 run.py:483] Algo bellman_ford step 4151 current loss 0.630956, current_train_items 132864.
I0302 19:00:27.919021 22732373614720 run.py:483] Algo bellman_ford step 4152 current loss 0.657889, current_train_items 132896.
I0302 19:00:27.950090 22732373614720 run.py:483] Algo bellman_ford step 4153 current loss 0.675549, current_train_items 132928.
I0302 19:00:27.985143 22732373614720 run.py:483] Algo bellman_ford step 4154 current loss 0.953261, current_train_items 132960.
I0302 19:00:28.005006 22732373614720 run.py:483] Algo bellman_ford step 4155 current loss 0.378955, current_train_items 132992.
I0302 19:00:28.021223 22732373614720 run.py:483] Algo bellman_ford step 4156 current loss 0.515018, current_train_items 133024.
I0302 19:00:28.044060 22732373614720 run.py:483] Algo bellman_ford step 4157 current loss 0.612311, current_train_items 133056.
I0302 19:00:28.073898 22732373614720 run.py:483] Algo bellman_ford step 4158 current loss 0.726536, current_train_items 133088.
I0302 19:00:28.107311 22732373614720 run.py:483] Algo bellman_ford step 4159 current loss 0.930909, current_train_items 133120.
I0302 19:00:28.127337 22732373614720 run.py:483] Algo bellman_ford step 4160 current loss 0.312823, current_train_items 133152.
I0302 19:00:28.143487 22732373614720 run.py:483] Algo bellman_ford step 4161 current loss 0.500078, current_train_items 133184.
I0302 19:00:28.165011 22732373614720 run.py:483] Algo bellman_ford step 4162 current loss 0.657871, current_train_items 133216.
I0302 19:00:28.195827 22732373614720 run.py:483] Algo bellman_ford step 4163 current loss 0.729313, current_train_items 133248.
I0302 19:00:28.229411 22732373614720 run.py:483] Algo bellman_ford step 4164 current loss 0.838689, current_train_items 133280.
I0302 19:00:28.249289 22732373614720 run.py:483] Algo bellman_ford step 4165 current loss 0.380094, current_train_items 133312.
I0302 19:00:28.265214 22732373614720 run.py:483] Algo bellman_ford step 4166 current loss 0.483944, current_train_items 133344.
I0302 19:00:28.289289 22732373614720 run.py:483] Algo bellman_ford step 4167 current loss 0.711613, current_train_items 133376.
I0302 19:00:28.319539 22732373614720 run.py:483] Algo bellman_ford step 4168 current loss 0.726359, current_train_items 133408.
I0302 19:00:28.351853 22732373614720 run.py:483] Algo bellman_ford step 4169 current loss 0.766471, current_train_items 133440.
I0302 19:00:28.371346 22732373614720 run.py:483] Algo bellman_ford step 4170 current loss 0.324308, current_train_items 133472.
I0302 19:00:28.386879 22732373614720 run.py:483] Algo bellman_ford step 4171 current loss 0.529609, current_train_items 133504.
I0302 19:00:28.409509 22732373614720 run.py:483] Algo bellman_ford step 4172 current loss 0.691147, current_train_items 133536.
I0302 19:00:28.439422 22732373614720 run.py:483] Algo bellman_ford step 4173 current loss 0.722008, current_train_items 133568.
I0302 19:00:28.474409 22732373614720 run.py:483] Algo bellman_ford step 4174 current loss 0.892814, current_train_items 133600.
I0302 19:00:28.494087 22732373614720 run.py:483] Algo bellman_ford step 4175 current loss 0.304808, current_train_items 133632.
I0302 19:00:28.510732 22732373614720 run.py:483] Algo bellman_ford step 4176 current loss 0.528946, current_train_items 133664.
I0302 19:00:28.533813 22732373614720 run.py:483] Algo bellman_ford step 4177 current loss 0.687231, current_train_items 133696.
I0302 19:00:28.564492 22732373614720 run.py:483] Algo bellman_ford step 4178 current loss 0.783891, current_train_items 133728.
I0302 19:00:28.598127 22732373614720 run.py:483] Algo bellman_ford step 4179 current loss 0.860472, current_train_items 133760.
I0302 19:00:28.618007 22732373614720 run.py:483] Algo bellman_ford step 4180 current loss 0.347148, current_train_items 133792.
I0302 19:00:28.633951 22732373614720 run.py:483] Algo bellman_ford step 4181 current loss 0.515844, current_train_items 133824.
I0302 19:00:28.658051 22732373614720 run.py:483] Algo bellman_ford step 4182 current loss 0.670735, current_train_items 133856.
I0302 19:00:28.686944 22732373614720 run.py:483] Algo bellman_ford step 4183 current loss 0.703902, current_train_items 133888.
I0302 19:00:28.720388 22732373614720 run.py:483] Algo bellman_ford step 4184 current loss 0.881183, current_train_items 133920.
I0302 19:00:28.740129 22732373614720 run.py:483] Algo bellman_ford step 4185 current loss 0.266323, current_train_items 133952.
I0302 19:00:28.756058 22732373614720 run.py:483] Algo bellman_ford step 4186 current loss 0.523210, current_train_items 133984.
I0302 19:00:28.779038 22732373614720 run.py:483] Algo bellman_ford step 4187 current loss 0.665658, current_train_items 134016.
I0302 19:00:28.809346 22732373614720 run.py:483] Algo bellman_ford step 4188 current loss 0.756169, current_train_items 134048.
I0302 19:00:28.841680 22732373614720 run.py:483] Algo bellman_ford step 4189 current loss 1.009907, current_train_items 134080.
I0302 19:00:28.861628 22732373614720 run.py:483] Algo bellman_ford step 4190 current loss 0.379696, current_train_items 134112.
I0302 19:00:28.878132 22732373614720 run.py:483] Algo bellman_ford step 4191 current loss 0.513698, current_train_items 134144.
I0302 19:00:28.899205 22732373614720 run.py:483] Algo bellman_ford step 4192 current loss 0.555750, current_train_items 134176.
I0302 19:00:28.930330 22732373614720 run.py:483] Algo bellman_ford step 4193 current loss 0.737482, current_train_items 134208.
I0302 19:00:28.965873 22732373614720 run.py:483] Algo bellman_ford step 4194 current loss 0.971209, current_train_items 134240.
I0302 19:00:28.985312 22732373614720 run.py:483] Algo bellman_ford step 4195 current loss 0.365374, current_train_items 134272.
I0302 19:00:29.001220 22732373614720 run.py:483] Algo bellman_ford step 4196 current loss 0.631127, current_train_items 134304.
I0302 19:00:29.024618 22732373614720 run.py:483] Algo bellman_ford step 4197 current loss 0.613295, current_train_items 134336.
I0302 19:00:29.054620 22732373614720 run.py:483] Algo bellman_ford step 4198 current loss 0.680806, current_train_items 134368.
I0302 19:00:29.087300 22732373614720 run.py:483] Algo bellman_ford step 4199 current loss 0.967328, current_train_items 134400.
I0302 19:00:29.106942 22732373614720 run.py:483] Algo bellman_ford step 4200 current loss 0.304222, current_train_items 134432.
I0302 19:00:29.114927 22732373614720 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0302 19:00:29.115034 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:00:29.131908 22732373614720 run.py:483] Algo bellman_ford step 4201 current loss 0.674670, current_train_items 134464.
I0302 19:00:29.155346 22732373614720 run.py:483] Algo bellman_ford step 4202 current loss 0.651879, current_train_items 134496.
I0302 19:00:29.185968 22732373614720 run.py:483] Algo bellman_ford step 4203 current loss 0.818775, current_train_items 134528.
I0302 19:00:29.218837 22732373614720 run.py:483] Algo bellman_ford step 4204 current loss 0.928863, current_train_items 134560.
I0302 19:00:29.238517 22732373614720 run.py:483] Algo bellman_ford step 4205 current loss 0.356419, current_train_items 134592.
I0302 19:00:29.254381 22732373614720 run.py:483] Algo bellman_ford step 4206 current loss 0.667802, current_train_items 134624.
I0302 19:00:29.278168 22732373614720 run.py:483] Algo bellman_ford step 4207 current loss 0.686425, current_train_items 134656.
I0302 19:00:29.309571 22732373614720 run.py:483] Algo bellman_ford step 4208 current loss 0.792118, current_train_items 134688.
I0302 19:00:29.343510 22732373614720 run.py:483] Algo bellman_ford step 4209 current loss 0.939629, current_train_items 134720.
I0302 19:00:29.363345 22732373614720 run.py:483] Algo bellman_ford step 4210 current loss 0.287365, current_train_items 134752.
I0302 19:00:29.379342 22732373614720 run.py:483] Algo bellman_ford step 4211 current loss 0.480097, current_train_items 134784.
I0302 19:00:29.402593 22732373614720 run.py:483] Algo bellman_ford step 4212 current loss 0.701735, current_train_items 134816.
I0302 19:00:29.433417 22732373614720 run.py:483] Algo bellman_ford step 4213 current loss 0.733677, current_train_items 134848.
I0302 19:00:29.466256 22732373614720 run.py:483] Algo bellman_ford step 4214 current loss 0.828997, current_train_items 134880.
I0302 19:00:29.485706 22732373614720 run.py:483] Algo bellman_ford step 4215 current loss 0.413235, current_train_items 134912.
I0302 19:00:29.501713 22732373614720 run.py:483] Algo bellman_ford step 4216 current loss 0.512865, current_train_items 134944.
I0302 19:00:29.524449 22732373614720 run.py:483] Algo bellman_ford step 4217 current loss 0.668939, current_train_items 134976.
I0302 19:00:29.554731 22732373614720 run.py:483] Algo bellman_ford step 4218 current loss 0.735152, current_train_items 135008.
I0302 19:00:29.587695 22732373614720 run.py:483] Algo bellman_ford step 4219 current loss 0.775465, current_train_items 135040.
I0302 19:00:29.607206 22732373614720 run.py:483] Algo bellman_ford step 4220 current loss 0.332337, current_train_items 135072.
I0302 19:00:29.623350 22732373614720 run.py:483] Algo bellman_ford step 4221 current loss 0.529126, current_train_items 135104.
I0302 19:00:29.647019 22732373614720 run.py:483] Algo bellman_ford step 4222 current loss 0.702766, current_train_items 135136.
I0302 19:00:29.678174 22732373614720 run.py:483] Algo bellman_ford step 4223 current loss 0.780981, current_train_items 135168.
I0302 19:00:29.713077 22732373614720 run.py:483] Algo bellman_ford step 4224 current loss 0.878555, current_train_items 135200.
I0302 19:00:29.732374 22732373614720 run.py:483] Algo bellman_ford step 4225 current loss 0.376411, current_train_items 135232.
I0302 19:00:29.748251 22732373614720 run.py:483] Algo bellman_ford step 4226 current loss 0.513061, current_train_items 135264.
I0302 19:00:29.771422 22732373614720 run.py:483] Algo bellman_ford step 4227 current loss 0.676259, current_train_items 135296.
I0302 19:00:29.800592 22732373614720 run.py:483] Algo bellman_ford step 4228 current loss 0.797733, current_train_items 135328.
I0302 19:00:29.832013 22732373614720 run.py:483] Algo bellman_ford step 4229 current loss 0.838630, current_train_items 135360.
I0302 19:00:29.851414 22732373614720 run.py:483] Algo bellman_ford step 4230 current loss 0.352321, current_train_items 135392.
I0302 19:00:29.867227 22732373614720 run.py:483] Algo bellman_ford step 4231 current loss 0.441869, current_train_items 135424.
I0302 19:00:29.890567 22732373614720 run.py:483] Algo bellman_ford step 4232 current loss 0.728816, current_train_items 135456.
I0302 19:00:29.920948 22732373614720 run.py:483] Algo bellman_ford step 4233 current loss 0.786363, current_train_items 135488.
I0302 19:00:29.953732 22732373614720 run.py:483] Algo bellman_ford step 4234 current loss 0.874053, current_train_items 135520.
I0302 19:00:29.973639 22732373614720 run.py:483] Algo bellman_ford step 4235 current loss 0.355630, current_train_items 135552.
I0302 19:00:29.990293 22732373614720 run.py:483] Algo bellman_ford step 4236 current loss 0.609487, current_train_items 135584.
I0302 19:00:30.013563 22732373614720 run.py:483] Algo bellman_ford step 4237 current loss 0.691079, current_train_items 135616.
I0302 19:00:30.044468 22732373614720 run.py:483] Algo bellman_ford step 4238 current loss 0.709172, current_train_items 135648.
I0302 19:00:30.079172 22732373614720 run.py:483] Algo bellman_ford step 4239 current loss 0.932239, current_train_items 135680.
I0302 19:00:30.098885 22732373614720 run.py:483] Algo bellman_ford step 4240 current loss 0.293075, current_train_items 135712.
I0302 19:00:30.114901 22732373614720 run.py:483] Algo bellman_ford step 4241 current loss 0.456261, current_train_items 135744.
I0302 19:00:30.137600 22732373614720 run.py:483] Algo bellman_ford step 4242 current loss 0.588381, current_train_items 135776.
I0302 19:00:30.169096 22732373614720 run.py:483] Algo bellman_ford step 4243 current loss 0.880269, current_train_items 135808.
I0302 19:00:30.201284 22732373614720 run.py:483] Algo bellman_ford step 4244 current loss 0.995862, current_train_items 135840.
I0302 19:00:30.220993 22732373614720 run.py:483] Algo bellman_ford step 4245 current loss 0.322705, current_train_items 135872.
I0302 19:00:30.236624 22732373614720 run.py:483] Algo bellman_ford step 4246 current loss 0.516693, current_train_items 135904.
I0302 19:00:30.260925 22732373614720 run.py:483] Algo bellman_ford step 4247 current loss 0.684174, current_train_items 135936.
I0302 19:00:30.291953 22732373614720 run.py:483] Algo bellman_ford step 4248 current loss 0.743612, current_train_items 135968.
I0302 19:00:30.326677 22732373614720 run.py:483] Algo bellman_ford step 4249 current loss 0.851271, current_train_items 136000.
I0302 19:00:30.346409 22732373614720 run.py:483] Algo bellman_ford step 4250 current loss 0.384661, current_train_items 136032.
I0302 19:00:30.354690 22732373614720 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.9091796875, 'score': 0.9091796875, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0302 19:00:30.354801 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.909, val scores are: bellman_ford: 0.909
I0302 19:00:30.371207 22732373614720 run.py:483] Algo bellman_ford step 4251 current loss 0.564837, current_train_items 136064.
I0302 19:00:30.394920 22732373614720 run.py:483] Algo bellman_ford step 4252 current loss 0.916096, current_train_items 136096.
I0302 19:00:30.425798 22732373614720 run.py:483] Algo bellman_ford step 4253 current loss 0.844540, current_train_items 136128.
I0302 19:00:30.459259 22732373614720 run.py:483] Algo bellman_ford step 4254 current loss 0.942593, current_train_items 136160.
I0302 19:00:30.479320 22732373614720 run.py:483] Algo bellman_ford step 4255 current loss 0.368360, current_train_items 136192.
I0302 19:00:30.494931 22732373614720 run.py:483] Algo bellman_ford step 4256 current loss 0.541494, current_train_items 136224.
I0302 19:00:30.517815 22732373614720 run.py:483] Algo bellman_ford step 4257 current loss 0.545389, current_train_items 136256.
I0302 19:00:30.548250 22732373614720 run.py:483] Algo bellman_ford step 4258 current loss 0.805033, current_train_items 136288.
I0302 19:00:30.581835 22732373614720 run.py:483] Algo bellman_ford step 4259 current loss 0.911060, current_train_items 136320.
I0302 19:00:30.601631 22732373614720 run.py:483] Algo bellman_ford step 4260 current loss 0.363589, current_train_items 136352.
I0302 19:00:30.618166 22732373614720 run.py:483] Algo bellman_ford step 4261 current loss 0.656890, current_train_items 136384.
I0302 19:00:30.640933 22732373614720 run.py:483] Algo bellman_ford step 4262 current loss 0.698969, current_train_items 136416.
I0302 19:00:30.671409 22732373614720 run.py:483] Algo bellman_ford step 4263 current loss 0.803385, current_train_items 136448.
I0302 19:00:30.704840 22732373614720 run.py:483] Algo bellman_ford step 4264 current loss 0.833468, current_train_items 136480.
I0302 19:00:30.724364 22732373614720 run.py:483] Algo bellman_ford step 4265 current loss 0.410771, current_train_items 136512.
I0302 19:00:30.740397 22732373614720 run.py:483] Algo bellman_ford step 4266 current loss 0.656842, current_train_items 136544.
I0302 19:00:30.763689 22732373614720 run.py:483] Algo bellman_ford step 4267 current loss 0.674321, current_train_items 136576.
I0302 19:00:30.793738 22732373614720 run.py:483] Algo bellman_ford step 4268 current loss 0.761421, current_train_items 136608.
I0302 19:00:30.826274 22732373614720 run.py:483] Algo bellman_ford step 4269 current loss 0.902419, current_train_items 136640.
I0302 19:00:30.846060 22732373614720 run.py:483] Algo bellman_ford step 4270 current loss 0.374048, current_train_items 136672.
I0302 19:00:30.861941 22732373614720 run.py:483] Algo bellman_ford step 4271 current loss 0.478223, current_train_items 136704.
I0302 19:00:30.884024 22732373614720 run.py:483] Algo bellman_ford step 4272 current loss 0.626304, current_train_items 136736.
I0302 19:00:30.914121 22732373614720 run.py:483] Algo bellman_ford step 4273 current loss 0.749064, current_train_items 136768.
I0302 19:00:30.946682 22732373614720 run.py:483] Algo bellman_ford step 4274 current loss 0.841835, current_train_items 136800.
I0302 19:00:30.966730 22732373614720 run.py:483] Algo bellman_ford step 4275 current loss 0.328806, current_train_items 136832.
I0302 19:00:30.982721 22732373614720 run.py:483] Algo bellman_ford step 4276 current loss 0.475370, current_train_items 136864.
I0302 19:00:31.005266 22732373614720 run.py:483] Algo bellman_ford step 4277 current loss 0.677513, current_train_items 136896.
I0302 19:00:31.037421 22732373614720 run.py:483] Algo bellman_ford step 4278 current loss 0.900918, current_train_items 136928.
I0302 19:00:31.071203 22732373614720 run.py:483] Algo bellman_ford step 4279 current loss 0.894138, current_train_items 136960.
I0302 19:00:31.090909 22732373614720 run.py:483] Algo bellman_ford step 4280 current loss 0.332156, current_train_items 136992.
I0302 19:00:31.106769 22732373614720 run.py:483] Algo bellman_ford step 4281 current loss 0.425546, current_train_items 137024.
I0302 19:00:31.129725 22732373614720 run.py:483] Algo bellman_ford step 4282 current loss 0.618222, current_train_items 137056.
I0302 19:00:31.159182 22732373614720 run.py:483] Algo bellman_ford step 4283 current loss 0.713258, current_train_items 137088.
I0302 19:00:31.194119 22732373614720 run.py:483] Algo bellman_ford step 4284 current loss 0.892074, current_train_items 137120.
I0302 19:00:31.214118 22732373614720 run.py:483] Algo bellman_ford step 4285 current loss 0.329967, current_train_items 137152.
I0302 19:00:31.229969 22732373614720 run.py:483] Algo bellman_ford step 4286 current loss 0.558831, current_train_items 137184.
I0302 19:00:31.252846 22732373614720 run.py:483] Algo bellman_ford step 4287 current loss 0.662768, current_train_items 137216.
I0302 19:00:31.284710 22732373614720 run.py:483] Algo bellman_ford step 4288 current loss 0.724211, current_train_items 137248.
I0302 19:00:31.318342 22732373614720 run.py:483] Algo bellman_ford step 4289 current loss 1.051306, current_train_items 137280.
I0302 19:00:31.338242 22732373614720 run.py:483] Algo bellman_ford step 4290 current loss 0.350717, current_train_items 137312.
I0302 19:00:31.354662 22732373614720 run.py:483] Algo bellman_ford step 4291 current loss 0.476154, current_train_items 137344.
I0302 19:00:31.378895 22732373614720 run.py:483] Algo bellman_ford step 4292 current loss 0.690581, current_train_items 137376.
I0302 19:00:31.410905 22732373614720 run.py:483] Algo bellman_ford step 4293 current loss 0.942135, current_train_items 137408.
I0302 19:00:31.445913 22732373614720 run.py:483] Algo bellman_ford step 4294 current loss 1.057833, current_train_items 137440.
I0302 19:00:31.465624 22732373614720 run.py:483] Algo bellman_ford step 4295 current loss 0.293958, current_train_items 137472.
I0302 19:00:31.481478 22732373614720 run.py:483] Algo bellman_ford step 4296 current loss 0.625034, current_train_items 137504.
I0302 19:00:31.503991 22732373614720 run.py:483] Algo bellman_ford step 4297 current loss 0.670740, current_train_items 137536.
I0302 19:00:31.534979 22732373614720 run.py:483] Algo bellman_ford step 4298 current loss 0.722154, current_train_items 137568.
I0302 19:00:31.564963 22732373614720 run.py:483] Algo bellman_ford step 4299 current loss 0.808119, current_train_items 137600.
I0302 19:00:31.584947 22732373614720 run.py:483] Algo bellman_ford step 4300 current loss 0.387786, current_train_items 137632.
I0302 19:00:31.592833 22732373614720 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0302 19:00:31.592942 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:00:31.609314 22732373614720 run.py:483] Algo bellman_ford step 4301 current loss 0.632314, current_train_items 137664.
I0302 19:00:31.632845 22732373614720 run.py:483] Algo bellman_ford step 4302 current loss 0.739371, current_train_items 137696.
I0302 19:00:31.665077 22732373614720 run.py:483] Algo bellman_ford step 4303 current loss 0.973386, current_train_items 137728.
I0302 19:00:31.700594 22732373614720 run.py:483] Algo bellman_ford step 4304 current loss 1.194590, current_train_items 137760.
I0302 19:00:31.720438 22732373614720 run.py:483] Algo bellman_ford step 4305 current loss 0.332998, current_train_items 137792.
I0302 19:00:31.736083 22732373614720 run.py:483] Algo bellman_ford step 4306 current loss 0.481607, current_train_items 137824.
I0302 19:00:31.760603 22732373614720 run.py:483] Algo bellman_ford step 4307 current loss 0.804459, current_train_items 137856.
I0302 19:00:31.791758 22732373614720 run.py:483] Algo bellman_ford step 4308 current loss 0.909439, current_train_items 137888.
I0302 19:00:31.827368 22732373614720 run.py:483] Algo bellman_ford step 4309 current loss 0.913045, current_train_items 137920.
I0302 19:00:31.846867 22732373614720 run.py:483] Algo bellman_ford step 4310 current loss 0.334511, current_train_items 137952.
I0302 19:00:31.862808 22732373614720 run.py:483] Algo bellman_ford step 4311 current loss 0.606040, current_train_items 137984.
I0302 19:00:31.884613 22732373614720 run.py:483] Algo bellman_ford step 4312 current loss 0.797625, current_train_items 138016.
I0302 19:00:31.915446 22732373614720 run.py:483] Algo bellman_ford step 4313 current loss 0.897408, current_train_items 138048.
I0302 19:00:31.948621 22732373614720 run.py:483] Algo bellman_ford step 4314 current loss 0.940262, current_train_items 138080.
I0302 19:00:31.967976 22732373614720 run.py:483] Algo bellman_ford step 4315 current loss 0.430217, current_train_items 138112.
I0302 19:00:31.983733 22732373614720 run.py:483] Algo bellman_ford step 4316 current loss 0.568349, current_train_items 138144.
I0302 19:00:32.007104 22732373614720 run.py:483] Algo bellman_ford step 4317 current loss 0.820066, current_train_items 138176.
I0302 19:00:32.036682 22732373614720 run.py:483] Algo bellman_ford step 4318 current loss 0.886629, current_train_items 138208.
I0302 19:00:32.069418 22732373614720 run.py:483] Algo bellman_ford step 4319 current loss 0.921574, current_train_items 138240.
I0302 19:00:32.088756 22732373614720 run.py:483] Algo bellman_ford step 4320 current loss 0.389763, current_train_items 138272.
I0302 19:00:32.104660 22732373614720 run.py:483] Algo bellman_ford step 4321 current loss 0.535094, current_train_items 138304.
I0302 19:00:32.127451 22732373614720 run.py:483] Algo bellman_ford step 4322 current loss 0.650403, current_train_items 138336.
I0302 19:00:32.156782 22732373614720 run.py:483] Algo bellman_ford step 4323 current loss 0.722161, current_train_items 138368.
I0302 19:00:32.190310 22732373614720 run.py:483] Algo bellman_ford step 4324 current loss 0.877625, current_train_items 138400.
I0302 19:00:32.209576 22732373614720 run.py:483] Algo bellman_ford step 4325 current loss 0.355578, current_train_items 138432.
I0302 19:00:32.225656 22732373614720 run.py:483] Algo bellman_ford step 4326 current loss 0.533308, current_train_items 138464.
I0302 19:00:32.248728 22732373614720 run.py:483] Algo bellman_ford step 4327 current loss 0.722454, current_train_items 138496.
I0302 19:00:32.280571 22732373614720 run.py:483] Algo bellman_ford step 4328 current loss 0.913441, current_train_items 138528.
I0302 19:00:32.312450 22732373614720 run.py:483] Algo bellman_ford step 4329 current loss 0.890286, current_train_items 138560.
I0302 19:00:32.332299 22732373614720 run.py:483] Algo bellman_ford step 4330 current loss 0.361868, current_train_items 138592.
I0302 19:00:32.348679 22732373614720 run.py:483] Algo bellman_ford step 4331 current loss 0.599786, current_train_items 138624.
I0302 19:00:32.372219 22732373614720 run.py:483] Algo bellman_ford step 4332 current loss 0.827982, current_train_items 138656.
I0302 19:00:32.401787 22732373614720 run.py:483] Algo bellman_ford step 4333 current loss 0.840045, current_train_items 138688.
I0302 19:00:32.432942 22732373614720 run.py:483] Algo bellman_ford step 4334 current loss 0.830457, current_train_items 138720.
I0302 19:00:32.452243 22732373614720 run.py:483] Algo bellman_ford step 4335 current loss 0.371385, current_train_items 138752.
I0302 19:00:32.468135 22732373614720 run.py:483] Algo bellman_ford step 4336 current loss 0.613184, current_train_items 138784.
I0302 19:00:32.491496 22732373614720 run.py:483] Algo bellman_ford step 4337 current loss 0.780839, current_train_items 138816.
I0302 19:00:32.523104 22732373614720 run.py:483] Algo bellman_ford step 4338 current loss 0.874561, current_train_items 138848.
I0302 19:00:32.555500 22732373614720 run.py:483] Algo bellman_ford step 4339 current loss 0.865019, current_train_items 138880.
I0302 19:00:32.574724 22732373614720 run.py:483] Algo bellman_ford step 4340 current loss 0.484346, current_train_items 138912.
I0302 19:00:32.590435 22732373614720 run.py:483] Algo bellman_ford step 4341 current loss 0.479367, current_train_items 138944.
I0302 19:00:32.613299 22732373614720 run.py:483] Algo bellman_ford step 4342 current loss 0.663520, current_train_items 138976.
I0302 19:00:32.643763 22732373614720 run.py:483] Algo bellman_ford step 4343 current loss 0.888110, current_train_items 139008.
I0302 19:00:32.677520 22732373614720 run.py:483] Algo bellman_ford step 4344 current loss 0.937042, current_train_items 139040.
I0302 19:00:32.696882 22732373614720 run.py:483] Algo bellman_ford step 4345 current loss 0.354235, current_train_items 139072.
I0302 19:00:32.713130 22732373614720 run.py:483] Algo bellman_ford step 4346 current loss 0.525310, current_train_items 139104.
I0302 19:00:32.736366 22732373614720 run.py:483] Algo bellman_ford step 4347 current loss 0.748599, current_train_items 139136.
I0302 19:00:32.766868 22732373614720 run.py:483] Algo bellman_ford step 4348 current loss 0.749426, current_train_items 139168.
I0302 19:00:32.798695 22732373614720 run.py:483] Algo bellman_ford step 4349 current loss 0.935622, current_train_items 139200.
I0302 19:00:32.818068 22732373614720 run.py:483] Algo bellman_ford step 4350 current loss 0.366300, current_train_items 139232.
I0302 19:00:32.826133 22732373614720 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0302 19:00:32.826249 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 19:00:32.842914 22732373614720 run.py:483] Algo bellman_ford step 4351 current loss 0.562774, current_train_items 139264.
I0302 19:00:32.867255 22732373614720 run.py:483] Algo bellman_ford step 4352 current loss 0.784505, current_train_items 139296.
I0302 19:00:32.898077 22732373614720 run.py:483] Algo bellman_ford step 4353 current loss 0.757052, current_train_items 139328.
I0302 19:00:32.931609 22732373614720 run.py:483] Algo bellman_ford step 4354 current loss 0.921056, current_train_items 139360.
I0302 19:00:32.951326 22732373614720 run.py:483] Algo bellman_ford step 4355 current loss 0.378023, current_train_items 139392.
I0302 19:00:32.967036 22732373614720 run.py:483] Algo bellman_ford step 4356 current loss 0.607921, current_train_items 139424.
I0302 19:00:32.990381 22732373614720 run.py:483] Algo bellman_ford step 4357 current loss 0.937332, current_train_items 139456.
I0302 19:00:33.018938 22732373614720 run.py:483] Algo bellman_ford step 4358 current loss 0.810610, current_train_items 139488.
I0302 19:00:33.050578 22732373614720 run.py:483] Algo bellman_ford step 4359 current loss 0.880623, current_train_items 139520.
I0302 19:00:33.070687 22732373614720 run.py:483] Algo bellman_ford step 4360 current loss 0.325458, current_train_items 139552.
I0302 19:00:33.087270 22732373614720 run.py:483] Algo bellman_ford step 4361 current loss 0.689864, current_train_items 139584.
I0302 19:00:33.109205 22732373614720 run.py:483] Algo bellman_ford step 4362 current loss 0.667514, current_train_items 139616.
I0302 19:00:33.140455 22732373614720 run.py:483] Algo bellman_ford step 4363 current loss 0.907797, current_train_items 139648.
I0302 19:00:33.175119 22732373614720 run.py:483] Algo bellman_ford step 4364 current loss 1.160133, current_train_items 139680.
I0302 19:00:33.194325 22732373614720 run.py:483] Algo bellman_ford step 4365 current loss 0.356930, current_train_items 139712.
I0302 19:00:33.210408 22732373614720 run.py:483] Algo bellman_ford step 4366 current loss 0.624787, current_train_items 139744.
I0302 19:00:33.233278 22732373614720 run.py:483] Algo bellman_ford step 4367 current loss 0.625522, current_train_items 139776.
I0302 19:00:33.262629 22732373614720 run.py:483] Algo bellman_ford step 4368 current loss 0.717797, current_train_items 139808.
I0302 19:00:33.296106 22732373614720 run.py:483] Algo bellman_ford step 4369 current loss 0.885692, current_train_items 139840.
I0302 19:00:33.315605 22732373614720 run.py:483] Algo bellman_ford step 4370 current loss 0.396569, current_train_items 139872.
I0302 19:00:33.331579 22732373614720 run.py:483] Algo bellman_ford step 4371 current loss 0.644302, current_train_items 139904.
I0302 19:00:33.353919 22732373614720 run.py:483] Algo bellman_ford step 4372 current loss 0.722115, current_train_items 139936.
I0302 19:00:33.383670 22732373614720 run.py:483] Algo bellman_ford step 4373 current loss 0.837060, current_train_items 139968.
I0302 19:00:33.416005 22732373614720 run.py:483] Algo bellman_ford step 4374 current loss 0.846782, current_train_items 140000.
I0302 19:00:33.435783 22732373614720 run.py:483] Algo bellman_ford step 4375 current loss 0.375509, current_train_items 140032.
I0302 19:00:33.451806 22732373614720 run.py:483] Algo bellman_ford step 4376 current loss 0.442871, current_train_items 140064.
I0302 19:00:33.474216 22732373614720 run.py:483] Algo bellman_ford step 4377 current loss 0.669046, current_train_items 140096.
I0302 19:00:33.503964 22732373614720 run.py:483] Algo bellman_ford step 4378 current loss 0.733211, current_train_items 140128.
I0302 19:00:33.537035 22732373614720 run.py:483] Algo bellman_ford step 4379 current loss 0.999775, current_train_items 140160.
I0302 19:00:33.556130 22732373614720 run.py:483] Algo bellman_ford step 4380 current loss 0.304347, current_train_items 140192.
I0302 19:00:33.572013 22732373614720 run.py:483] Algo bellman_ford step 4381 current loss 0.509841, current_train_items 140224.
I0302 19:00:33.595259 22732373614720 run.py:483] Algo bellman_ford step 4382 current loss 0.745800, current_train_items 140256.
I0302 19:00:33.625795 22732373614720 run.py:483] Algo bellman_ford step 4383 current loss 0.742797, current_train_items 140288.
I0302 19:00:33.658969 22732373614720 run.py:483] Algo bellman_ford step 4384 current loss 0.810402, current_train_items 140320.
I0302 19:00:33.678716 22732373614720 run.py:483] Algo bellman_ford step 4385 current loss 0.395045, current_train_items 140352.
I0302 19:00:33.694489 22732373614720 run.py:483] Algo bellman_ford step 4386 current loss 0.527655, current_train_items 140384.
I0302 19:00:33.717460 22732373614720 run.py:483] Algo bellman_ford step 4387 current loss 0.741692, current_train_items 140416.
I0302 19:00:33.747811 22732373614720 run.py:483] Algo bellman_ford step 4388 current loss 0.743745, current_train_items 140448.
I0302 19:00:33.779090 22732373614720 run.py:483] Algo bellman_ford step 4389 current loss 0.759364, current_train_items 140480.
I0302 19:00:33.799119 22732373614720 run.py:483] Algo bellman_ford step 4390 current loss 0.274473, current_train_items 140512.
I0302 19:00:33.814959 22732373614720 run.py:483] Algo bellman_ford step 4391 current loss 0.497995, current_train_items 140544.
I0302 19:00:33.837425 22732373614720 run.py:483] Algo bellman_ford step 4392 current loss 0.723092, current_train_items 140576.
I0302 19:00:33.868202 22732373614720 run.py:483] Algo bellman_ford step 4393 current loss 0.830806, current_train_items 140608.
I0302 19:00:33.901344 22732373614720 run.py:483] Algo bellman_ford step 4394 current loss 0.816914, current_train_items 140640.
I0302 19:00:33.920678 22732373614720 run.py:483] Algo bellman_ford step 4395 current loss 0.341706, current_train_items 140672.
I0302 19:00:33.936947 22732373614720 run.py:483] Algo bellman_ford step 4396 current loss 0.628518, current_train_items 140704.
I0302 19:00:33.960132 22732373614720 run.py:483] Algo bellman_ford step 4397 current loss 0.754072, current_train_items 140736.
I0302 19:00:33.990821 22732373614720 run.py:483] Algo bellman_ford step 4398 current loss 0.878294, current_train_items 140768.
I0302 19:00:34.023275 22732373614720 run.py:483] Algo bellman_ford step 4399 current loss 0.805583, current_train_items 140800.
I0302 19:00:34.042749 22732373614720 run.py:483] Algo bellman_ford step 4400 current loss 0.373032, current_train_items 140832.
I0302 19:00:34.050322 22732373614720 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0302 19:00:34.050436 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:00:34.067355 22732373614720 run.py:483] Algo bellman_ford step 4401 current loss 0.507686, current_train_items 140864.
I0302 19:00:34.090827 22732373614720 run.py:483] Algo bellman_ford step 4402 current loss 0.748559, current_train_items 140896.
I0302 19:00:34.121174 22732373614720 run.py:483] Algo bellman_ford step 4403 current loss 0.763217, current_train_items 140928.
I0302 19:00:34.158281 22732373614720 run.py:483] Algo bellman_ford step 4404 current loss 1.089998, current_train_items 140960.
I0302 19:00:34.178228 22732373614720 run.py:483] Algo bellman_ford step 4405 current loss 0.319794, current_train_items 140992.
I0302 19:00:34.194039 22732373614720 run.py:483] Algo bellman_ford step 4406 current loss 0.530178, current_train_items 141024.
I0302 19:00:34.216904 22732373614720 run.py:483] Algo bellman_ford step 4407 current loss 0.726914, current_train_items 141056.
I0302 19:00:34.247123 22732373614720 run.py:483] Algo bellman_ford step 4408 current loss 0.771942, current_train_items 141088.
I0302 19:00:34.281560 22732373614720 run.py:483] Algo bellman_ford step 4409 current loss 0.953077, current_train_items 141120.
I0302 19:00:34.301057 22732373614720 run.py:483] Algo bellman_ford step 4410 current loss 0.362983, current_train_items 141152.
I0302 19:00:34.317090 22732373614720 run.py:483] Algo bellman_ford step 4411 current loss 0.571574, current_train_items 141184.
I0302 19:00:34.340381 22732373614720 run.py:483] Algo bellman_ford step 4412 current loss 0.738283, current_train_items 141216.
I0302 19:00:34.371988 22732373614720 run.py:483] Algo bellman_ford step 4413 current loss 0.925815, current_train_items 141248.
I0302 19:00:34.403644 22732373614720 run.py:483] Algo bellman_ford step 4414 current loss 0.874874, current_train_items 141280.
I0302 19:00:34.423014 22732373614720 run.py:483] Algo bellman_ford step 4415 current loss 0.317399, current_train_items 141312.
I0302 19:00:34.439451 22732373614720 run.py:483] Algo bellman_ford step 4416 current loss 0.613211, current_train_items 141344.
I0302 19:00:34.462288 22732373614720 run.py:483] Algo bellman_ford step 4417 current loss 0.725409, current_train_items 141376.
I0302 19:00:34.492130 22732373614720 run.py:483] Algo bellman_ford step 4418 current loss 0.815029, current_train_items 141408.
I0302 19:00:34.524934 22732373614720 run.py:483] Algo bellman_ford step 4419 current loss 0.796129, current_train_items 141440.
I0302 19:00:34.544151 22732373614720 run.py:483] Algo bellman_ford step 4420 current loss 0.340131, current_train_items 141472.
I0302 19:00:34.559777 22732373614720 run.py:483] Algo bellman_ford step 4421 current loss 0.595993, current_train_items 141504.
I0302 19:00:34.582766 22732373614720 run.py:483] Algo bellman_ford step 4422 current loss 0.686620, current_train_items 141536.
I0302 19:00:34.612971 22732373614720 run.py:483] Algo bellman_ford step 4423 current loss 0.742351, current_train_items 141568.
I0302 19:00:34.646581 22732373614720 run.py:483] Algo bellman_ford step 4424 current loss 0.921852, current_train_items 141600.
I0302 19:00:34.666171 22732373614720 run.py:483] Algo bellman_ford step 4425 current loss 0.331893, current_train_items 141632.
I0302 19:00:34.681326 22732373614720 run.py:483] Algo bellman_ford step 4426 current loss 0.501331, current_train_items 141664.
I0302 19:00:34.704802 22732373614720 run.py:483] Algo bellman_ford step 4427 current loss 0.747997, current_train_items 141696.
I0302 19:00:34.735126 22732373614720 run.py:483] Algo bellman_ford step 4428 current loss 0.741964, current_train_items 141728.
I0302 19:00:34.767672 22732373614720 run.py:483] Algo bellman_ford step 4429 current loss 0.860205, current_train_items 141760.
I0302 19:00:34.786811 22732373614720 run.py:483] Algo bellman_ford step 4430 current loss 0.281357, current_train_items 141792.
I0302 19:00:34.802358 22732373614720 run.py:483] Algo bellman_ford step 4431 current loss 0.444546, current_train_items 141824.
I0302 19:00:34.825522 22732373614720 run.py:483] Algo bellman_ford step 4432 current loss 0.706996, current_train_items 141856.
I0302 19:00:34.854840 22732373614720 run.py:483] Algo bellman_ford step 4433 current loss 0.963777, current_train_items 141888.
I0302 19:00:34.887340 22732373614720 run.py:483] Algo bellman_ford step 4434 current loss 1.183414, current_train_items 141920.
I0302 19:00:34.906768 22732373614720 run.py:483] Algo bellman_ford step 4435 current loss 0.391017, current_train_items 141952.
I0302 19:00:34.922840 22732373614720 run.py:483] Algo bellman_ford step 4436 current loss 0.644716, current_train_items 141984.
I0302 19:00:34.946326 22732373614720 run.py:483] Algo bellman_ford step 4437 current loss 0.804159, current_train_items 142016.
I0302 19:00:34.975765 22732373614720 run.py:483] Algo bellman_ford step 4438 current loss 0.615657, current_train_items 142048.
I0302 19:00:35.009704 22732373614720 run.py:483] Algo bellman_ford step 4439 current loss 0.908505, current_train_items 142080.
I0302 19:00:35.028901 22732373614720 run.py:483] Algo bellman_ford step 4440 current loss 0.488405, current_train_items 142112.
I0302 19:00:35.045140 22732373614720 run.py:483] Algo bellman_ford step 4441 current loss 0.821960, current_train_items 142144.
I0302 19:00:35.067737 22732373614720 run.py:483] Algo bellman_ford step 4442 current loss 0.792661, current_train_items 142176.
I0302 19:00:35.097863 22732373614720 run.py:483] Algo bellman_ford step 4443 current loss 0.769670, current_train_items 142208.
I0302 19:00:35.131778 22732373614720 run.py:483] Algo bellman_ford step 4444 current loss 0.867666, current_train_items 142240.
I0302 19:00:35.151176 22732373614720 run.py:483] Algo bellman_ford step 4445 current loss 0.366790, current_train_items 142272.
I0302 19:00:35.166874 22732373614720 run.py:483] Algo bellman_ford step 4446 current loss 0.562944, current_train_items 142304.
I0302 19:00:35.190176 22732373614720 run.py:483] Algo bellman_ford step 4447 current loss 0.647953, current_train_items 142336.
I0302 19:00:35.221020 22732373614720 run.py:483] Algo bellman_ford step 4448 current loss 0.954988, current_train_items 142368.
I0302 19:00:35.254113 22732373614720 run.py:483] Algo bellman_ford step 4449 current loss 0.886154, current_train_items 142400.
I0302 19:00:35.273493 22732373614720 run.py:483] Algo bellman_ford step 4450 current loss 0.330868, current_train_items 142432.
I0302 19:00:35.281331 22732373614720 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0302 19:00:35.281442 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.929, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:00:35.311788 22732373614720 run.py:483] Algo bellman_ford step 4451 current loss 0.524215, current_train_items 142464.
I0302 19:00:35.335363 22732373614720 run.py:483] Algo bellman_ford step 4452 current loss 0.600703, current_train_items 142496.
I0302 19:00:35.365114 22732373614720 run.py:483] Algo bellman_ford step 4453 current loss 0.690234, current_train_items 142528.
I0302 19:00:35.397505 22732373614720 run.py:483] Algo bellman_ford step 4454 current loss 0.859142, current_train_items 142560.
I0302 19:00:35.417612 22732373614720 run.py:483] Algo bellman_ford step 4455 current loss 0.309862, current_train_items 142592.
I0302 19:00:35.433404 22732373614720 run.py:483] Algo bellman_ford step 4456 current loss 0.511999, current_train_items 142624.
I0302 19:00:35.456727 22732373614720 run.py:483] Algo bellman_ford step 4457 current loss 0.641167, current_train_items 142656.
I0302 19:00:35.485640 22732373614720 run.py:483] Algo bellman_ford step 4458 current loss 0.797961, current_train_items 142688.
I0302 19:00:35.521608 22732373614720 run.py:483] Algo bellman_ford step 4459 current loss 0.899706, current_train_items 142720.
I0302 19:00:35.542080 22732373614720 run.py:483] Algo bellman_ford step 4460 current loss 0.384956, current_train_items 142752.
I0302 19:00:35.558330 22732373614720 run.py:483] Algo bellman_ford step 4461 current loss 0.510369, current_train_items 142784.
I0302 19:00:35.581916 22732373614720 run.py:483] Algo bellman_ford step 4462 current loss 0.788365, current_train_items 142816.
I0302 19:00:35.610794 22732373614720 run.py:483] Algo bellman_ford step 4463 current loss 0.651472, current_train_items 142848.
I0302 19:00:35.643980 22732373614720 run.py:483] Algo bellman_ford step 4464 current loss 0.866509, current_train_items 142880.
I0302 19:00:35.663433 22732373614720 run.py:483] Algo bellman_ford step 4465 current loss 0.293021, current_train_items 142912.
I0302 19:00:35.679499 22732373614720 run.py:483] Algo bellman_ford step 4466 current loss 0.528411, current_train_items 142944.
I0302 19:00:35.702520 22732373614720 run.py:483] Algo bellman_ford step 4467 current loss 0.640512, current_train_items 142976.
I0302 19:00:35.734295 22732373614720 run.py:483] Algo bellman_ford step 4468 current loss 0.771424, current_train_items 143008.
I0302 19:00:35.767827 22732373614720 run.py:483] Algo bellman_ford step 4469 current loss 0.859627, current_train_items 143040.
I0302 19:00:35.787689 22732373614720 run.py:483] Algo bellman_ford step 4470 current loss 0.317097, current_train_items 143072.
I0302 19:00:35.803522 22732373614720 run.py:483] Algo bellman_ford step 4471 current loss 0.544452, current_train_items 143104.
I0302 19:00:35.826247 22732373614720 run.py:483] Algo bellman_ford step 4472 current loss 0.705844, current_train_items 143136.
I0302 19:00:35.856851 22732373614720 run.py:483] Algo bellman_ford step 4473 current loss 0.823764, current_train_items 143168.
I0302 19:00:35.891984 22732373614720 run.py:483] Algo bellman_ford step 4474 current loss 1.116393, current_train_items 143200.
I0302 19:00:35.911859 22732373614720 run.py:483] Algo bellman_ford step 4475 current loss 0.360464, current_train_items 143232.
I0302 19:00:35.927750 22732373614720 run.py:483] Algo bellman_ford step 4476 current loss 0.591971, current_train_items 143264.
I0302 19:00:35.950231 22732373614720 run.py:483] Algo bellman_ford step 4477 current loss 0.897484, current_train_items 143296.
I0302 19:00:35.980765 22732373614720 run.py:483] Algo bellman_ford step 4478 current loss 0.993489, current_train_items 143328.
I0302 19:00:36.015300 22732373614720 run.py:483] Algo bellman_ford step 4479 current loss 1.415448, current_train_items 143360.
I0302 19:00:36.034999 22732373614720 run.py:483] Algo bellman_ford step 4480 current loss 0.278550, current_train_items 143392.
I0302 19:00:36.050915 22732373614720 run.py:483] Algo bellman_ford step 4481 current loss 0.543965, current_train_items 143424.
I0302 19:00:36.074134 22732373614720 run.py:483] Algo bellman_ford step 4482 current loss 0.739038, current_train_items 143456.
I0302 19:00:36.104890 22732373614720 run.py:483] Algo bellman_ford step 4483 current loss 0.818075, current_train_items 143488.
I0302 19:00:36.135411 22732373614720 run.py:483] Algo bellman_ford step 4484 current loss 0.729979, current_train_items 143520.
I0302 19:00:36.155312 22732373614720 run.py:483] Algo bellman_ford step 4485 current loss 0.373433, current_train_items 143552.
I0302 19:00:36.171601 22732373614720 run.py:483] Algo bellman_ford step 4486 current loss 0.482204, current_train_items 143584.
I0302 19:00:36.195656 22732373614720 run.py:483] Algo bellman_ford step 4487 current loss 0.775543, current_train_items 143616.
I0302 19:00:36.226364 22732373614720 run.py:483] Algo bellman_ford step 4488 current loss 0.704746, current_train_items 143648.
I0302 19:00:36.258610 22732373614720 run.py:483] Algo bellman_ford step 4489 current loss 0.940933, current_train_items 143680.
I0302 19:00:36.278526 22732373614720 run.py:483] Algo bellman_ford step 4490 current loss 0.365636, current_train_items 143712.
I0302 19:00:36.294173 22732373614720 run.py:483] Algo bellman_ford step 4491 current loss 0.498440, current_train_items 143744.
I0302 19:00:36.317597 22732373614720 run.py:483] Algo bellman_ford step 4492 current loss 0.729381, current_train_items 143776.
I0302 19:00:36.348307 22732373614720 run.py:483] Algo bellman_ford step 4493 current loss 0.774890, current_train_items 143808.
I0302 19:00:36.381870 22732373614720 run.py:483] Algo bellman_ford step 4494 current loss 0.844235, current_train_items 143840.
I0302 19:00:36.401324 22732373614720 run.py:483] Algo bellman_ford step 4495 current loss 0.439202, current_train_items 143872.
I0302 19:00:36.417715 22732373614720 run.py:483] Algo bellman_ford step 4496 current loss 0.535161, current_train_items 143904.
I0302 19:00:36.440886 22732373614720 run.py:483] Algo bellman_ford step 4497 current loss 0.795489, current_train_items 143936.
I0302 19:00:36.469946 22732373614720 run.py:483] Algo bellman_ford step 4498 current loss 0.741543, current_train_items 143968.
I0302 19:00:36.500641 22732373614720 run.py:483] Algo bellman_ford step 4499 current loss 0.798293, current_train_items 144000.
I0302 19:00:36.520287 22732373614720 run.py:483] Algo bellman_ford step 4500 current loss 0.394500, current_train_items 144032.
I0302 19:00:36.528219 22732373614720 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0302 19:00:36.528330 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:00:36.544832 22732373614720 run.py:483] Algo bellman_ford step 4501 current loss 0.571196, current_train_items 144064.
I0302 19:00:36.568965 22732373614720 run.py:483] Algo bellman_ford step 4502 current loss 0.796688, current_train_items 144096.
I0302 19:00:36.599485 22732373614720 run.py:483] Algo bellman_ford step 4503 current loss 0.776279, current_train_items 144128.
I0302 19:00:36.633110 22732373614720 run.py:483] Algo bellman_ford step 4504 current loss 0.785014, current_train_items 144160.
I0302 19:00:36.653390 22732373614720 run.py:483] Algo bellman_ford step 4505 current loss 0.302554, current_train_items 144192.
I0302 19:00:36.669256 22732373614720 run.py:483] Algo bellman_ford step 4506 current loss 0.573186, current_train_items 144224.
I0302 19:00:36.691726 22732373614720 run.py:483] Algo bellman_ford step 4507 current loss 0.637659, current_train_items 144256.
I0302 19:00:36.721376 22732373614720 run.py:483] Algo bellman_ford step 4508 current loss 0.742518, current_train_items 144288.
I0302 19:00:36.753472 22732373614720 run.py:483] Algo bellman_ford step 4509 current loss 0.792105, current_train_items 144320.
I0302 19:00:36.772960 22732373614720 run.py:483] Algo bellman_ford step 4510 current loss 0.359850, current_train_items 144352.
I0302 19:00:36.789172 22732373614720 run.py:483] Algo bellman_ford step 4511 current loss 0.574163, current_train_items 144384.
I0302 19:00:36.812504 22732373614720 run.py:483] Algo bellman_ford step 4512 current loss 0.667900, current_train_items 144416.
I0302 19:00:36.842806 22732373614720 run.py:483] Algo bellman_ford step 4513 current loss 0.700467, current_train_items 144448.
I0302 19:00:36.875208 22732373614720 run.py:483] Algo bellman_ford step 4514 current loss 0.889733, current_train_items 144480.
I0302 19:00:36.894502 22732373614720 run.py:483] Algo bellman_ford step 4515 current loss 0.407909, current_train_items 144512.
I0302 19:00:36.910800 22732373614720 run.py:483] Algo bellman_ford step 4516 current loss 0.474067, current_train_items 144544.
I0302 19:00:36.932869 22732373614720 run.py:483] Algo bellman_ford step 4517 current loss 0.598453, current_train_items 144576.
I0302 19:00:36.964212 22732373614720 run.py:483] Algo bellman_ford step 4518 current loss 0.770565, current_train_items 144608.
I0302 19:00:37.000484 22732373614720 run.py:483] Algo bellman_ford step 4519 current loss 1.007035, current_train_items 144640.
I0302 19:00:37.020030 22732373614720 run.py:483] Algo bellman_ford step 4520 current loss 0.346953, current_train_items 144672.
I0302 19:00:37.036171 22732373614720 run.py:483] Algo bellman_ford step 4521 current loss 0.521490, current_train_items 144704.
I0302 19:00:37.059365 22732373614720 run.py:483] Algo bellman_ford step 4522 current loss 0.651741, current_train_items 144736.
I0302 19:00:37.091493 22732373614720 run.py:483] Algo bellman_ford step 4523 current loss 0.743250, current_train_items 144768.
I0302 19:00:37.123993 22732373614720 run.py:483] Algo bellman_ford step 4524 current loss 0.776201, current_train_items 144800.
I0302 19:00:37.143136 22732373614720 run.py:483] Algo bellman_ford step 4525 current loss 0.308357, current_train_items 144832.
I0302 19:00:37.159135 22732373614720 run.py:483] Algo bellman_ford step 4526 current loss 0.466700, current_train_items 144864.
I0302 19:00:37.182765 22732373614720 run.py:483] Algo bellman_ford step 4527 current loss 0.637133, current_train_items 144896.
I0302 19:00:37.213335 22732373614720 run.py:483] Algo bellman_ford step 4528 current loss 0.671054, current_train_items 144928.
I0302 19:00:37.246705 22732373614720 run.py:483] Algo bellman_ford step 4529 current loss 0.816767, current_train_items 144960.
I0302 19:00:37.266137 22732373614720 run.py:483] Algo bellman_ford step 4530 current loss 0.420383, current_train_items 144992.
I0302 19:00:37.281842 22732373614720 run.py:483] Algo bellman_ford step 4531 current loss 0.503418, current_train_items 145024.
I0302 19:00:37.303932 22732373614720 run.py:483] Algo bellman_ford step 4532 current loss 0.578176, current_train_items 145056.
I0302 19:00:37.332511 22732373614720 run.py:483] Algo bellman_ford step 4533 current loss 0.581276, current_train_items 145088.
I0302 19:00:37.366713 22732373614720 run.py:483] Algo bellman_ford step 4534 current loss 0.769594, current_train_items 145120.
I0302 19:00:37.386522 22732373614720 run.py:483] Algo bellman_ford step 4535 current loss 0.291991, current_train_items 145152.
I0302 19:00:37.402451 22732373614720 run.py:483] Algo bellman_ford step 4536 current loss 0.484639, current_train_items 145184.
I0302 19:00:37.425602 22732373614720 run.py:483] Algo bellman_ford step 4537 current loss 0.595404, current_train_items 145216.
I0302 19:00:37.455493 22732373614720 run.py:483] Algo bellman_ford step 4538 current loss 0.599668, current_train_items 145248.
I0302 19:00:37.489675 22732373614720 run.py:483] Algo bellman_ford step 4539 current loss 0.949461, current_train_items 145280.
I0302 19:00:37.508979 22732373614720 run.py:483] Algo bellman_ford step 4540 current loss 0.325267, current_train_items 145312.
I0302 19:00:37.524829 22732373614720 run.py:483] Algo bellman_ford step 4541 current loss 0.543286, current_train_items 145344.
I0302 19:00:37.547090 22732373614720 run.py:483] Algo bellman_ford step 4542 current loss 0.661603, current_train_items 145376.
I0302 19:00:37.577044 22732373614720 run.py:483] Algo bellman_ford step 4543 current loss 0.703667, current_train_items 145408.
I0302 19:00:37.610443 22732373614720 run.py:483] Algo bellman_ford step 4544 current loss 0.997310, current_train_items 145440.
I0302 19:00:37.629729 22732373614720 run.py:483] Algo bellman_ford step 4545 current loss 0.371985, current_train_items 145472.
I0302 19:00:37.645544 22732373614720 run.py:483] Algo bellman_ford step 4546 current loss 0.518309, current_train_items 145504.
I0302 19:00:37.668057 22732373614720 run.py:483] Algo bellman_ford step 4547 current loss 0.633385, current_train_items 145536.
I0302 19:00:37.697224 22732373614720 run.py:483] Algo bellman_ford step 4548 current loss 0.640311, current_train_items 145568.
I0302 19:00:37.731079 22732373614720 run.py:483] Algo bellman_ford step 4549 current loss 0.828810, current_train_items 145600.
I0302 19:00:37.750353 22732373614720 run.py:483] Algo bellman_ford step 4550 current loss 0.349294, current_train_items 145632.
I0302 19:00:37.758531 22732373614720 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0302 19:00:37.758639 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 19:00:37.775515 22732373614720 run.py:483] Algo bellman_ford step 4551 current loss 0.665557, current_train_items 145664.
I0302 19:00:37.799179 22732373614720 run.py:483] Algo bellman_ford step 4552 current loss 0.705968, current_train_items 145696.
I0302 19:00:37.830401 22732373614720 run.py:483] Algo bellman_ford step 4553 current loss 0.914843, current_train_items 145728.
I0302 19:00:37.864108 22732373614720 run.py:483] Algo bellman_ford step 4554 current loss 0.954230, current_train_items 145760.
I0302 19:00:37.883963 22732373614720 run.py:483] Algo bellman_ford step 4555 current loss 0.373018, current_train_items 145792.
I0302 19:00:37.899279 22732373614720 run.py:483] Algo bellman_ford step 4556 current loss 0.519217, current_train_items 145824.
I0302 19:00:37.921549 22732373614720 run.py:483] Algo bellman_ford step 4557 current loss 0.693255, current_train_items 145856.
I0302 19:00:37.952989 22732373614720 run.py:483] Algo bellman_ford step 4558 current loss 0.747169, current_train_items 145888.
I0302 19:00:37.987179 22732373614720 run.py:483] Algo bellman_ford step 4559 current loss 1.068423, current_train_items 145920.
I0302 19:00:38.006747 22732373614720 run.py:483] Algo bellman_ford step 4560 current loss 0.294395, current_train_items 145952.
I0302 19:00:38.022978 22732373614720 run.py:483] Algo bellman_ford step 4561 current loss 0.534613, current_train_items 145984.
I0302 19:00:38.045439 22732373614720 run.py:483] Algo bellman_ford step 4562 current loss 0.643612, current_train_items 146016.
I0302 19:00:38.075340 22732373614720 run.py:483] Algo bellman_ford step 4563 current loss 0.744076, current_train_items 146048.
I0302 19:00:38.109593 22732373614720 run.py:483] Algo bellman_ford step 4564 current loss 0.884798, current_train_items 146080.
I0302 19:00:38.129098 22732373614720 run.py:483] Algo bellman_ford step 4565 current loss 0.337298, current_train_items 146112.
I0302 19:00:38.145060 22732373614720 run.py:483] Algo bellman_ford step 4566 current loss 0.571422, current_train_items 146144.
I0302 19:00:38.168480 22732373614720 run.py:483] Algo bellman_ford step 4567 current loss 0.668712, current_train_items 146176.
I0302 19:00:38.200096 22732373614720 run.py:483] Algo bellman_ford step 4568 current loss 0.886431, current_train_items 146208.
I0302 19:00:38.232250 22732373614720 run.py:483] Algo bellman_ford step 4569 current loss 0.914001, current_train_items 146240.
I0302 19:00:38.251819 22732373614720 run.py:483] Algo bellman_ford step 4570 current loss 0.344473, current_train_items 146272.
I0302 19:00:38.267717 22732373614720 run.py:483] Algo bellman_ford step 4571 current loss 0.533984, current_train_items 146304.
I0302 19:00:38.290977 22732373614720 run.py:483] Algo bellman_ford step 4572 current loss 0.677394, current_train_items 146336.
I0302 19:00:38.320693 22732373614720 run.py:483] Algo bellman_ford step 4573 current loss 0.691627, current_train_items 146368.
I0302 19:00:38.352678 22732373614720 run.py:483] Algo bellman_ford step 4574 current loss 0.839571, current_train_items 146400.
I0302 19:00:38.372290 22732373614720 run.py:483] Algo bellman_ford step 4575 current loss 0.385128, current_train_items 146432.
I0302 19:00:38.388287 22732373614720 run.py:483] Algo bellman_ford step 4576 current loss 0.576898, current_train_items 146464.
I0302 19:00:38.411012 22732373614720 run.py:483] Algo bellman_ford step 4577 current loss 0.745471, current_train_items 146496.
I0302 19:00:38.441735 22732373614720 run.py:483] Algo bellman_ford step 4578 current loss 0.904320, current_train_items 146528.
I0302 19:00:38.474894 22732373614720 run.py:483] Algo bellman_ford step 4579 current loss 0.904964, current_train_items 146560.
I0302 19:00:38.494528 22732373614720 run.py:483] Algo bellman_ford step 4580 current loss 0.327839, current_train_items 146592.
I0302 19:00:38.510893 22732373614720 run.py:483] Algo bellman_ford step 4581 current loss 0.545740, current_train_items 146624.
I0302 19:00:38.533627 22732373614720 run.py:483] Algo bellman_ford step 4582 current loss 0.680303, current_train_items 146656.
I0302 19:00:38.564213 22732373614720 run.py:483] Algo bellman_ford step 4583 current loss 0.792722, current_train_items 146688.
I0302 19:00:38.597440 22732373614720 run.py:483] Algo bellman_ford step 4584 current loss 1.119231, current_train_items 146720.
I0302 19:00:38.617207 22732373614720 run.py:483] Algo bellman_ford step 4585 current loss 0.389338, current_train_items 146752.
I0302 19:00:38.633460 22732373614720 run.py:483] Algo bellman_ford step 4586 current loss 0.515216, current_train_items 146784.
I0302 19:00:38.655668 22732373614720 run.py:483] Algo bellman_ford step 4587 current loss 0.635451, current_train_items 146816.
I0302 19:00:38.686021 22732373614720 run.py:483] Algo bellman_ford step 4588 current loss 0.763230, current_train_items 146848.
I0302 19:00:38.718063 22732373614720 run.py:483] Algo bellman_ford step 4589 current loss 0.986230, current_train_items 146880.
I0302 19:00:38.737733 22732373614720 run.py:483] Algo bellman_ford step 4590 current loss 0.391945, current_train_items 146912.
I0302 19:00:38.753715 22732373614720 run.py:483] Algo bellman_ford step 4591 current loss 0.561454, current_train_items 146944.
I0302 19:00:38.776147 22732373614720 run.py:483] Algo bellman_ford step 4592 current loss 0.735534, current_train_items 146976.
I0302 19:00:38.807532 22732373614720 run.py:483] Algo bellman_ford step 4593 current loss 0.762237, current_train_items 147008.
I0302 19:00:38.841245 22732373614720 run.py:483] Algo bellman_ford step 4594 current loss 1.023793, current_train_items 147040.
I0302 19:00:38.860768 22732373614720 run.py:483] Algo bellman_ford step 4595 current loss 0.383133, current_train_items 147072.
I0302 19:00:38.876782 22732373614720 run.py:483] Algo bellman_ford step 4596 current loss 0.536400, current_train_items 147104.
I0302 19:00:38.902113 22732373614720 run.py:483] Algo bellman_ford step 4597 current loss 1.010354, current_train_items 147136.
I0302 19:00:38.932218 22732373614720 run.py:483] Algo bellman_ford step 4598 current loss 0.787383, current_train_items 147168.
I0302 19:00:38.964133 22732373614720 run.py:483] Algo bellman_ford step 4599 current loss 0.814429, current_train_items 147200.
I0302 19:00:38.983903 22732373614720 run.py:483] Algo bellman_ford step 4600 current loss 0.341849, current_train_items 147232.
I0302 19:00:38.991764 22732373614720 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0302 19:00:38.991870 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 19:00:39.008685 22732373614720 run.py:483] Algo bellman_ford step 4601 current loss 0.532042, current_train_items 147264.
I0302 19:00:39.032941 22732373614720 run.py:483] Algo bellman_ford step 4602 current loss 0.779664, current_train_items 147296.
I0302 19:00:39.063730 22732373614720 run.py:483] Algo bellman_ford step 4603 current loss 0.968848, current_train_items 147328.
I0302 19:00:39.097788 22732373614720 run.py:483] Algo bellman_ford step 4604 current loss 1.301946, current_train_items 147360.
I0302 19:00:39.117790 22732373614720 run.py:483] Algo bellman_ford step 4605 current loss 0.299587, current_train_items 147392.
I0302 19:00:39.133781 22732373614720 run.py:483] Algo bellman_ford step 4606 current loss 0.730160, current_train_items 147424.
I0302 19:00:39.156665 22732373614720 run.py:483] Algo bellman_ford step 4607 current loss 0.753071, current_train_items 147456.
I0302 19:00:39.188079 22732373614720 run.py:483] Algo bellman_ford step 4608 current loss 0.816251, current_train_items 147488.
I0302 19:00:39.219352 22732373614720 run.py:483] Algo bellman_ford step 4609 current loss 0.766953, current_train_items 147520.
I0302 19:00:39.238932 22732373614720 run.py:483] Algo bellman_ford step 4610 current loss 0.333817, current_train_items 147552.
I0302 19:00:39.254781 22732373614720 run.py:483] Algo bellman_ford step 4611 current loss 0.666063, current_train_items 147584.
I0302 19:00:39.278378 22732373614720 run.py:483] Algo bellman_ford step 4612 current loss 0.812789, current_train_items 147616.
I0302 19:00:39.309100 22732373614720 run.py:483] Algo bellman_ford step 4613 current loss 1.004381, current_train_items 147648.
I0302 19:00:39.343188 22732373614720 run.py:483] Algo bellman_ford step 4614 current loss 1.202035, current_train_items 147680.
I0302 19:00:39.362791 22732373614720 run.py:483] Algo bellman_ford step 4615 current loss 0.390905, current_train_items 147712.
I0302 19:00:39.378914 22732373614720 run.py:483] Algo bellman_ford step 4616 current loss 0.647645, current_train_items 147744.
I0302 19:00:39.402628 22732373614720 run.py:483] Algo bellman_ford step 4617 current loss 0.673536, current_train_items 147776.
I0302 19:00:39.432196 22732373614720 run.py:483] Algo bellman_ford step 4618 current loss 0.661521, current_train_items 147808.
I0302 19:00:39.465409 22732373614720 run.py:483] Algo bellman_ford step 4619 current loss 0.876150, current_train_items 147840.
I0302 19:00:39.484772 22732373614720 run.py:483] Algo bellman_ford step 4620 current loss 0.347614, current_train_items 147872.
I0302 19:00:39.500513 22732373614720 run.py:483] Algo bellman_ford step 4621 current loss 0.501426, current_train_items 147904.
I0302 19:00:39.523439 22732373614720 run.py:483] Algo bellman_ford step 4622 current loss 0.744424, current_train_items 147936.
I0302 19:00:39.554014 22732373614720 run.py:483] Algo bellman_ford step 4623 current loss 0.768386, current_train_items 147968.
I0302 19:00:39.585998 22732373614720 run.py:483] Algo bellman_ford step 4624 current loss 1.014180, current_train_items 148000.
I0302 19:00:39.605688 22732373614720 run.py:483] Algo bellman_ford step 4625 current loss 0.333524, current_train_items 148032.
I0302 19:00:39.621735 22732373614720 run.py:483] Algo bellman_ford step 4626 current loss 0.597983, current_train_items 148064.
I0302 19:00:39.645470 22732373614720 run.py:483] Algo bellman_ford step 4627 current loss 0.736409, current_train_items 148096.
I0302 19:00:39.676854 22732373614720 run.py:483] Algo bellman_ford step 4628 current loss 1.001651, current_train_items 148128.
I0302 19:00:39.711403 22732373614720 run.py:483] Algo bellman_ford step 4629 current loss 0.900007, current_train_items 148160.
I0302 19:00:39.730968 22732373614720 run.py:483] Algo bellman_ford step 4630 current loss 0.327923, current_train_items 148192.
I0302 19:00:39.747145 22732373614720 run.py:483] Algo bellman_ford step 4631 current loss 0.516203, current_train_items 148224.
I0302 19:00:39.771094 22732373614720 run.py:483] Algo bellman_ford step 4632 current loss 0.726302, current_train_items 148256.
I0302 19:00:39.800176 22732373614720 run.py:483] Algo bellman_ford step 4633 current loss 0.702661, current_train_items 148288.
I0302 19:00:39.833804 22732373614720 run.py:483] Algo bellman_ford step 4634 current loss 0.859094, current_train_items 148320.
I0302 19:00:39.853306 22732373614720 run.py:483] Algo bellman_ford step 4635 current loss 0.362414, current_train_items 148352.
I0302 19:00:39.869898 22732373614720 run.py:483] Algo bellman_ford step 4636 current loss 0.603865, current_train_items 148384.
I0302 19:00:39.893665 22732373614720 run.py:483] Algo bellman_ford step 4637 current loss 0.872057, current_train_items 148416.
I0302 19:00:39.925652 22732373614720 run.py:483] Algo bellman_ford step 4638 current loss 0.915215, current_train_items 148448.
I0302 19:00:39.959125 22732373614720 run.py:483] Algo bellman_ford step 4639 current loss 1.000384, current_train_items 148480.
I0302 19:00:39.978725 22732373614720 run.py:483] Algo bellman_ford step 4640 current loss 0.327365, current_train_items 148512.
I0302 19:00:39.994687 22732373614720 run.py:483] Algo bellman_ford step 4641 current loss 0.559556, current_train_items 148544.
I0302 19:00:40.018484 22732373614720 run.py:483] Algo bellman_ford step 4642 current loss 0.767797, current_train_items 148576.
I0302 19:00:40.049437 22732373614720 run.py:483] Algo bellman_ford step 4643 current loss 0.759235, current_train_items 148608.
I0302 19:00:40.083601 22732373614720 run.py:483] Algo bellman_ford step 4644 current loss 0.980030, current_train_items 148640.
I0302 19:00:40.102945 22732373614720 run.py:483] Algo bellman_ford step 4645 current loss 0.445733, current_train_items 148672.
I0302 19:00:40.119085 22732373614720 run.py:483] Algo bellman_ford step 4646 current loss 0.535962, current_train_items 148704.
I0302 19:00:40.141575 22732373614720 run.py:483] Algo bellman_ford step 4647 current loss 0.644625, current_train_items 148736.
I0302 19:00:40.170520 22732373614720 run.py:483] Algo bellman_ford step 4648 current loss 0.770782, current_train_items 148768.
I0302 19:00:40.201870 22732373614720 run.py:483] Algo bellman_ford step 4649 current loss 0.792893, current_train_items 148800.
I0302 19:00:40.221205 22732373614720 run.py:483] Algo bellman_ford step 4650 current loss 0.303092, current_train_items 148832.
I0302 19:00:40.229398 22732373614720 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0302 19:00:40.229506 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 19:00:40.246418 22732373614720 run.py:483] Algo bellman_ford step 4651 current loss 0.557512, current_train_items 148864.
I0302 19:00:40.270445 22732373614720 run.py:483] Algo bellman_ford step 4652 current loss 0.695883, current_train_items 148896.
I0302 19:00:40.302590 22732373614720 run.py:483] Algo bellman_ford step 4653 current loss 0.793340, current_train_items 148928.
I0302 19:00:40.336725 22732373614720 run.py:483] Algo bellman_ford step 4654 current loss 1.017254, current_train_items 148960.
I0302 19:00:40.356690 22732373614720 run.py:483] Algo bellman_ford step 4655 current loss 0.364173, current_train_items 148992.
I0302 19:00:40.372557 22732373614720 run.py:483] Algo bellman_ford step 4656 current loss 0.596702, current_train_items 149024.
I0302 19:00:40.396589 22732373614720 run.py:483] Algo bellman_ford step 4657 current loss 0.684055, current_train_items 149056.
I0302 19:00:40.426952 22732373614720 run.py:483] Algo bellman_ford step 4658 current loss 0.768725, current_train_items 149088.
I0302 19:00:40.459227 22732373614720 run.py:483] Algo bellman_ford step 4659 current loss 0.794254, current_train_items 149120.
I0302 19:00:40.479260 22732373614720 run.py:483] Algo bellman_ford step 4660 current loss 0.358876, current_train_items 149152.
I0302 19:00:40.495563 22732373614720 run.py:483] Algo bellman_ford step 4661 current loss 0.553609, current_train_items 149184.
I0302 19:00:40.519637 22732373614720 run.py:483] Algo bellman_ford step 4662 current loss 0.657475, current_train_items 149216.
I0302 19:00:40.549696 22732373614720 run.py:483] Algo bellman_ford step 4663 current loss 0.698312, current_train_items 149248.
I0302 19:00:40.586109 22732373614720 run.py:483] Algo bellman_ford step 4664 current loss 0.914869, current_train_items 149280.
I0302 19:00:40.605990 22732373614720 run.py:483] Algo bellman_ford step 4665 current loss 0.372361, current_train_items 149312.
I0302 19:00:40.621836 22732373614720 run.py:483] Algo bellman_ford step 4666 current loss 0.502605, current_train_items 149344.
I0302 19:00:40.644622 22732373614720 run.py:483] Algo bellman_ford step 4667 current loss 0.688187, current_train_items 149376.
I0302 19:00:40.675957 22732373614720 run.py:483] Algo bellman_ford step 4668 current loss 0.947082, current_train_items 149408.
I0302 19:00:40.709818 22732373614720 run.py:483] Algo bellman_ford step 4669 current loss 0.917758, current_train_items 149440.
I0302 19:00:40.729674 22732373614720 run.py:483] Algo bellman_ford step 4670 current loss 0.405220, current_train_items 149472.
I0302 19:00:40.745352 22732373614720 run.py:483] Algo bellman_ford step 4671 current loss 0.458394, current_train_items 149504.
I0302 19:00:40.767468 22732373614720 run.py:483] Algo bellman_ford step 4672 current loss 0.718593, current_train_items 149536.
I0302 19:00:40.797094 22732373614720 run.py:483] Algo bellman_ford step 4673 current loss 0.720383, current_train_items 149568.
I0302 19:00:40.830181 22732373614720 run.py:483] Algo bellman_ford step 4674 current loss 0.752905, current_train_items 149600.
I0302 19:00:40.849675 22732373614720 run.py:483] Algo bellman_ford step 4675 current loss 0.310780, current_train_items 149632.
I0302 19:00:40.866292 22732373614720 run.py:483] Algo bellman_ford step 4676 current loss 0.613005, current_train_items 149664.
I0302 19:00:40.889534 22732373614720 run.py:483] Algo bellman_ford step 4677 current loss 0.801382, current_train_items 149696.
I0302 19:00:40.921220 22732373614720 run.py:483] Algo bellman_ford step 4678 current loss 0.856238, current_train_items 149728.
I0302 19:00:40.954720 22732373614720 run.py:483] Algo bellman_ford step 4679 current loss 0.826766, current_train_items 149760.
I0302 19:00:40.974178 22732373614720 run.py:483] Algo bellman_ford step 4680 current loss 0.334005, current_train_items 149792.
I0302 19:00:40.989877 22732373614720 run.py:483] Algo bellman_ford step 4681 current loss 0.503991, current_train_items 149824.
I0302 19:00:41.012904 22732373614720 run.py:483] Algo bellman_ford step 4682 current loss 0.705186, current_train_items 149856.
I0302 19:00:41.043491 22732373614720 run.py:483] Algo bellman_ford step 4683 current loss 0.838973, current_train_items 149888.
I0302 19:00:41.077847 22732373614720 run.py:483] Algo bellman_ford step 4684 current loss 1.042114, current_train_items 149920.
I0302 19:00:41.098161 22732373614720 run.py:483] Algo bellman_ford step 4685 current loss 0.339868, current_train_items 149952.
I0302 19:00:41.113805 22732373614720 run.py:483] Algo bellman_ford step 4686 current loss 0.445312, current_train_items 149984.
I0302 19:00:41.136940 22732373614720 run.py:483] Algo bellman_ford step 4687 current loss 0.768771, current_train_items 150016.
I0302 19:00:41.167715 22732373614720 run.py:483] Algo bellman_ford step 4688 current loss 0.869181, current_train_items 150048.
I0302 19:00:41.202325 22732373614720 run.py:483] Algo bellman_ford step 4689 current loss 0.931216, current_train_items 150080.
I0302 19:00:41.222377 22732373614720 run.py:483] Algo bellman_ford step 4690 current loss 0.332292, current_train_items 150112.
I0302 19:00:41.238459 22732373614720 run.py:483] Algo bellman_ford step 4691 current loss 0.492983, current_train_items 150144.
I0302 19:00:41.261293 22732373614720 run.py:483] Algo bellman_ford step 4692 current loss 0.584020, current_train_items 150176.
I0302 19:00:41.292149 22732373614720 run.py:483] Algo bellman_ford step 4693 current loss 0.830165, current_train_items 150208.
I0302 19:00:41.324406 22732373614720 run.py:483] Algo bellman_ford step 4694 current loss 0.890012, current_train_items 150240.
I0302 19:00:41.344254 22732373614720 run.py:483] Algo bellman_ford step 4695 current loss 0.320333, current_train_items 150272.
I0302 19:00:41.360255 22732373614720 run.py:483] Algo bellman_ford step 4696 current loss 0.542691, current_train_items 150304.
I0302 19:00:41.384045 22732373614720 run.py:483] Algo bellman_ford step 4697 current loss 0.796693, current_train_items 150336.
I0302 19:00:41.414214 22732373614720 run.py:483] Algo bellman_ford step 4698 current loss 0.824639, current_train_items 150368.
I0302 19:00:41.445767 22732373614720 run.py:483] Algo bellman_ford step 4699 current loss 1.012638, current_train_items 150400.
I0302 19:00:41.465793 22732373614720 run.py:483] Algo bellman_ford step 4700 current loss 0.406243, current_train_items 150432.
I0302 19:00:41.473615 22732373614720 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0302 19:00:41.473726 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:00:41.489933 22732373614720 run.py:483] Algo bellman_ford step 4701 current loss 0.513973, current_train_items 150464.
I0302 19:00:41.513850 22732373614720 run.py:483] Algo bellman_ford step 4702 current loss 0.775385, current_train_items 150496.
I0302 19:00:41.546217 22732373614720 run.py:483] Algo bellman_ford step 4703 current loss 0.753389, current_train_items 150528.
I0302 19:00:41.580929 22732373614720 run.py:483] Algo bellman_ford step 4704 current loss 0.957061, current_train_items 150560.
I0302 19:00:41.600904 22732373614720 run.py:483] Algo bellman_ford step 4705 current loss 0.455443, current_train_items 150592.
I0302 19:00:41.616451 22732373614720 run.py:483] Algo bellman_ford step 4706 current loss 0.446600, current_train_items 150624.
I0302 19:00:41.640189 22732373614720 run.py:483] Algo bellman_ford step 4707 current loss 0.756451, current_train_items 150656.
I0302 19:00:41.671921 22732373614720 run.py:483] Algo bellman_ford step 4708 current loss 0.926007, current_train_items 150688.
I0302 19:00:41.704949 22732373614720 run.py:483] Algo bellman_ford step 4709 current loss 0.824152, current_train_items 150720.
I0302 19:00:41.724422 22732373614720 run.py:483] Algo bellman_ford step 4710 current loss 0.399251, current_train_items 150752.
I0302 19:00:41.740468 22732373614720 run.py:483] Algo bellman_ford step 4711 current loss 0.529161, current_train_items 150784.
I0302 19:00:41.764518 22732373614720 run.py:483] Algo bellman_ford step 4712 current loss 0.681981, current_train_items 150816.
I0302 19:00:41.795498 22732373614720 run.py:483] Algo bellman_ford step 4713 current loss 0.730155, current_train_items 150848.
I0302 19:00:41.827346 22732373614720 run.py:483] Algo bellman_ford step 4714 current loss 0.851707, current_train_items 150880.
I0302 19:00:41.846974 22732373614720 run.py:483] Algo bellman_ford step 4715 current loss 0.315160, current_train_items 150912.
I0302 19:00:41.863114 22732373614720 run.py:483] Algo bellman_ford step 4716 current loss 0.678850, current_train_items 150944.
I0302 19:00:41.884925 22732373614720 run.py:483] Algo bellman_ford step 4717 current loss 0.718758, current_train_items 150976.
I0302 19:00:41.915602 22732373614720 run.py:483] Algo bellman_ford step 4718 current loss 0.750048, current_train_items 151008.
I0302 19:00:41.951127 22732373614720 run.py:483] Algo bellman_ford step 4719 current loss 1.057186, current_train_items 151040.
I0302 19:00:41.970439 22732373614720 run.py:483] Algo bellman_ford step 4720 current loss 0.350406, current_train_items 151072.
I0302 19:00:41.986064 22732373614720 run.py:483] Algo bellman_ford step 4721 current loss 0.570300, current_train_items 151104.
I0302 19:00:42.009827 22732373614720 run.py:483] Algo bellman_ford step 4722 current loss 0.693121, current_train_items 151136.
I0302 19:00:42.038852 22732373614720 run.py:483] Algo bellman_ford step 4723 current loss 0.667271, current_train_items 151168.
I0302 19:00:42.072702 22732373614720 run.py:483] Algo bellman_ford step 4724 current loss 0.849132, current_train_items 151200.
I0302 19:00:42.092229 22732373614720 run.py:483] Algo bellman_ford step 4725 current loss 0.383746, current_train_items 151232.
I0302 19:00:42.108428 22732373614720 run.py:483] Algo bellman_ford step 4726 current loss 0.565095, current_train_items 151264.
I0302 19:00:42.132534 22732373614720 run.py:483] Algo bellman_ford step 4727 current loss 0.867608, current_train_items 151296.
I0302 19:00:42.161536 22732373614720 run.py:483] Algo bellman_ford step 4728 current loss 0.690029, current_train_items 151328.
I0302 19:00:42.191087 22732373614720 run.py:483] Algo bellman_ford step 4729 current loss 0.701007, current_train_items 151360.
I0302 19:00:42.210714 22732373614720 run.py:483] Algo bellman_ford step 4730 current loss 0.360050, current_train_items 151392.
I0302 19:00:42.226424 22732373614720 run.py:483] Algo bellman_ford step 4731 current loss 0.446138, current_train_items 151424.
I0302 19:00:42.249094 22732373614720 run.py:483] Algo bellman_ford step 4732 current loss 0.716717, current_train_items 151456.
I0302 19:00:42.279214 22732373614720 run.py:483] Algo bellman_ford step 4733 current loss 0.888526, current_train_items 151488.
I0302 19:00:42.312487 22732373614720 run.py:483] Algo bellman_ford step 4734 current loss 0.923537, current_train_items 151520.
I0302 19:00:42.331936 22732373614720 run.py:483] Algo bellman_ford step 4735 current loss 0.328941, current_train_items 151552.
I0302 19:00:42.348240 22732373614720 run.py:483] Algo bellman_ford step 4736 current loss 0.616969, current_train_items 151584.
I0302 19:00:42.371441 22732373614720 run.py:483] Algo bellman_ford step 4737 current loss 0.809316, current_train_items 151616.
I0302 19:00:42.401431 22732373614720 run.py:483] Algo bellman_ford step 4738 current loss 0.883486, current_train_items 151648.
I0302 19:00:42.433842 22732373614720 run.py:483] Algo bellman_ford step 4739 current loss 1.084210, current_train_items 151680.
I0302 19:00:42.453521 22732373614720 run.py:483] Algo bellman_ford step 4740 current loss 0.369545, current_train_items 151712.
I0302 19:00:42.469253 22732373614720 run.py:483] Algo bellman_ford step 4741 current loss 0.623881, current_train_items 151744.
I0302 19:00:42.491674 22732373614720 run.py:483] Algo bellman_ford step 4742 current loss 0.799788, current_train_items 151776.
I0302 19:00:42.523044 22732373614720 run.py:483] Algo bellman_ford step 4743 current loss 0.970317, current_train_items 151808.
I0302 19:00:42.554761 22732373614720 run.py:483] Algo bellman_ford step 4744 current loss 0.888047, current_train_items 151840.
I0302 19:00:42.574431 22732373614720 run.py:483] Algo bellman_ford step 4745 current loss 0.481307, current_train_items 151872.
I0302 19:00:42.590898 22732373614720 run.py:483] Algo bellman_ford step 4746 current loss 0.615603, current_train_items 151904.
I0302 19:00:42.615589 22732373614720 run.py:483] Algo bellman_ford step 4747 current loss 0.845978, current_train_items 151936.
I0302 19:00:42.646250 22732373614720 run.py:483] Algo bellman_ford step 4748 current loss 0.854564, current_train_items 151968.
I0302 19:00:42.679301 22732373614720 run.py:483] Algo bellman_ford step 4749 current loss 0.935597, current_train_items 152000.
I0302 19:00:42.698911 22732373614720 run.py:483] Algo bellman_ford step 4750 current loss 0.322352, current_train_items 152032.
I0302 19:00:42.707046 22732373614720 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.8779296875, 'score': 0.8779296875, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0302 19:00:42.707165 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.878, val scores are: bellman_ford: 0.878
I0302 19:00:42.723739 22732373614720 run.py:483] Algo bellman_ford step 4751 current loss 0.616846, current_train_items 152064.
I0302 19:00:42.747833 22732373614720 run.py:483] Algo bellman_ford step 4752 current loss 0.793250, current_train_items 152096.
I0302 19:00:42.780387 22732373614720 run.py:483] Algo bellman_ford step 4753 current loss 0.857077, current_train_items 152128.
I0302 19:00:42.813416 22732373614720 run.py:483] Algo bellman_ford step 4754 current loss 0.911013, current_train_items 152160.
I0302 19:00:42.833064 22732373614720 run.py:483] Algo bellman_ford step 4755 current loss 0.386179, current_train_items 152192.
I0302 19:00:42.848778 22732373614720 run.py:483] Algo bellman_ford step 4756 current loss 0.606417, current_train_items 152224.
I0302 19:00:42.872534 22732373614720 run.py:483] Algo bellman_ford step 4757 current loss 0.692707, current_train_items 152256.
I0302 19:00:42.903560 22732373614720 run.py:483] Algo bellman_ford step 4758 current loss 0.766811, current_train_items 152288.
I0302 19:00:42.937250 22732373614720 run.py:483] Algo bellman_ford step 4759 current loss 1.100745, current_train_items 152320.
I0302 19:00:42.956817 22732373614720 run.py:483] Algo bellman_ford step 4760 current loss 0.387068, current_train_items 152352.
I0302 19:00:42.973032 22732373614720 run.py:483] Algo bellman_ford step 4761 current loss 0.510892, current_train_items 152384.
I0302 19:00:42.996386 22732373614720 run.py:483] Algo bellman_ford step 4762 current loss 0.741122, current_train_items 152416.
I0302 19:00:43.026067 22732373614720 run.py:483] Algo bellman_ford step 4763 current loss 0.772099, current_train_items 152448.
I0302 19:00:43.059366 22732373614720 run.py:483] Algo bellman_ford step 4764 current loss 0.949901, current_train_items 152480.
I0302 19:00:43.078742 22732373614720 run.py:483] Algo bellman_ford step 4765 current loss 0.369401, current_train_items 152512.
I0302 19:00:43.094203 22732373614720 run.py:483] Algo bellman_ford step 4766 current loss 0.438652, current_train_items 152544.
I0302 19:00:43.118119 22732373614720 run.py:483] Algo bellman_ford step 4767 current loss 0.686996, current_train_items 152576.
I0302 19:00:43.147522 22732373614720 run.py:483] Algo bellman_ford step 4768 current loss 0.755333, current_train_items 152608.
I0302 19:00:43.179671 22732373614720 run.py:483] Algo bellman_ford step 4769 current loss 0.912479, current_train_items 152640.
I0302 19:00:43.199203 22732373614720 run.py:483] Algo bellman_ford step 4770 current loss 0.385588, current_train_items 152672.
I0302 19:00:43.215043 22732373614720 run.py:483] Algo bellman_ford step 4771 current loss 0.442225, current_train_items 152704.
I0302 19:00:43.236895 22732373614720 run.py:483] Algo bellman_ford step 4772 current loss 0.635655, current_train_items 152736.
I0302 19:00:43.267260 22732373614720 run.py:483] Algo bellman_ford step 4773 current loss 0.922370, current_train_items 152768.
I0302 19:00:43.300785 22732373614720 run.py:483] Algo bellman_ford step 4774 current loss 0.876212, current_train_items 152800.
I0302 19:00:43.320281 22732373614720 run.py:483] Algo bellman_ford step 4775 current loss 0.352391, current_train_items 152832.
I0302 19:00:43.336302 22732373614720 run.py:483] Algo bellman_ford step 4776 current loss 0.526680, current_train_items 152864.
I0302 19:00:43.358989 22732373614720 run.py:483] Algo bellman_ford step 4777 current loss 0.711899, current_train_items 152896.
I0302 19:00:43.389645 22732373614720 run.py:483] Algo bellman_ford step 4778 current loss 0.826645, current_train_items 152928.
I0302 19:00:43.420955 22732373614720 run.py:483] Algo bellman_ford step 4779 current loss 0.779059, current_train_items 152960.
I0302 19:00:43.440584 22732373614720 run.py:483] Algo bellman_ford step 4780 current loss 0.347352, current_train_items 152992.
I0302 19:00:43.456604 22732373614720 run.py:483] Algo bellman_ford step 4781 current loss 0.550281, current_train_items 153024.
I0302 19:00:43.479995 22732373614720 run.py:483] Algo bellman_ford step 4782 current loss 0.643550, current_train_items 153056.
I0302 19:00:43.511166 22732373614720 run.py:483] Algo bellman_ford step 4783 current loss 0.823194, current_train_items 153088.
I0302 19:00:43.544857 22732373614720 run.py:483] Algo bellman_ford step 4784 current loss 0.851997, current_train_items 153120.
I0302 19:00:43.565038 22732373614720 run.py:483] Algo bellman_ford step 4785 current loss 0.480293, current_train_items 153152.
I0302 19:00:43.581433 22732373614720 run.py:483] Algo bellman_ford step 4786 current loss 0.598519, current_train_items 153184.
I0302 19:00:43.604793 22732373614720 run.py:483] Algo bellman_ford step 4787 current loss 0.705484, current_train_items 153216.
I0302 19:00:43.634704 22732373614720 run.py:483] Algo bellman_ford step 4788 current loss 0.738086, current_train_items 153248.
I0302 19:00:43.666434 22732373614720 run.py:483] Algo bellman_ford step 4789 current loss 0.976938, current_train_items 153280.
I0302 19:00:43.685692 22732373614720 run.py:483] Algo bellman_ford step 4790 current loss 0.361604, current_train_items 153312.
I0302 19:00:43.701677 22732373614720 run.py:483] Algo bellman_ford step 4791 current loss 0.515568, current_train_items 153344.
I0302 19:00:43.724697 22732373614720 run.py:483] Algo bellman_ford step 4792 current loss 0.583241, current_train_items 153376.
I0302 19:00:43.754734 22732373614720 run.py:483] Algo bellman_ford step 4793 current loss 0.737455, current_train_items 153408.
I0302 19:00:43.789672 22732373614720 run.py:483] Algo bellman_ford step 4794 current loss 0.843310, current_train_items 153440.
I0302 19:00:43.808890 22732373614720 run.py:483] Algo bellman_ford step 4795 current loss 0.285501, current_train_items 153472.
I0302 19:00:43.824834 22732373614720 run.py:483] Algo bellman_ford step 4796 current loss 0.556316, current_train_items 153504.
I0302 19:00:43.848172 22732373614720 run.py:483] Algo bellman_ford step 4797 current loss 0.798339, current_train_items 153536.
I0302 19:00:43.878034 22732373614720 run.py:483] Algo bellman_ford step 4798 current loss 0.704898, current_train_items 153568.
I0302 19:00:43.910530 22732373614720 run.py:483] Algo bellman_ford step 4799 current loss 0.802034, current_train_items 153600.
I0302 19:00:43.930403 22732373614720 run.py:483] Algo bellman_ford step 4800 current loss 0.311132, current_train_items 153632.
I0302 19:00:43.938422 22732373614720 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0302 19:00:43.938533 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 19:00:43.955165 22732373614720 run.py:483] Algo bellman_ford step 4801 current loss 0.582737, current_train_items 153664.
I0302 19:00:43.979471 22732373614720 run.py:483] Algo bellman_ford step 4802 current loss 0.819522, current_train_items 153696.
I0302 19:00:44.010473 22732373614720 run.py:483] Algo bellman_ford step 4803 current loss 0.760467, current_train_items 153728.
I0302 19:00:44.046807 22732373614720 run.py:483] Algo bellman_ford step 4804 current loss 0.913171, current_train_items 153760.
I0302 19:00:44.066793 22732373614720 run.py:483] Algo bellman_ford step 4805 current loss 0.400250, current_train_items 153792.
I0302 19:00:44.083163 22732373614720 run.py:483] Algo bellman_ford step 4806 current loss 0.614606, current_train_items 153824.
I0302 19:00:44.106580 22732373614720 run.py:483] Algo bellman_ford step 4807 current loss 0.752346, current_train_items 153856.
I0302 19:00:44.137709 22732373614720 run.py:483] Algo bellman_ford step 4808 current loss 0.889897, current_train_items 153888.
I0302 19:00:44.170676 22732373614720 run.py:483] Algo bellman_ford step 4809 current loss 0.841817, current_train_items 153920.
I0302 19:00:44.190587 22732373614720 run.py:483] Algo bellman_ford step 4810 current loss 0.346382, current_train_items 153952.
I0302 19:00:44.206599 22732373614720 run.py:483] Algo bellman_ford step 4811 current loss 0.583627, current_train_items 153984.
I0302 19:00:44.229470 22732373614720 run.py:483] Algo bellman_ford step 4812 current loss 0.723479, current_train_items 154016.
I0302 19:00:44.259517 22732373614720 run.py:483] Algo bellman_ford step 4813 current loss 0.816164, current_train_items 154048.
I0302 19:00:44.293830 22732373614720 run.py:483] Algo bellman_ford step 4814 current loss 0.968236, current_train_items 154080.
I0302 19:00:44.313453 22732373614720 run.py:483] Algo bellman_ford step 4815 current loss 0.363941, current_train_items 154112.
I0302 19:00:44.329487 22732373614720 run.py:483] Algo bellman_ford step 4816 current loss 0.541074, current_train_items 154144.
I0302 19:00:44.353941 22732373614720 run.py:483] Algo bellman_ford step 4817 current loss 0.761928, current_train_items 154176.
I0302 19:00:44.384503 22732373614720 run.py:483] Algo bellman_ford step 4818 current loss 0.766050, current_train_items 154208.
I0302 19:00:44.414270 22732373614720 run.py:483] Algo bellman_ford step 4819 current loss 0.658270, current_train_items 154240.
I0302 19:00:44.434328 22732373614720 run.py:483] Algo bellman_ford step 4820 current loss 0.343484, current_train_items 154272.
I0302 19:00:44.450263 22732373614720 run.py:483] Algo bellman_ford step 4821 current loss 0.557981, current_train_items 154304.
I0302 19:00:44.474049 22732373614720 run.py:483] Algo bellman_ford step 4822 current loss 0.688549, current_train_items 154336.
I0302 19:00:44.504265 22732373614720 run.py:483] Algo bellman_ford step 4823 current loss 0.778312, current_train_items 154368.
I0302 19:00:44.538395 22732373614720 run.py:483] Algo bellman_ford step 4824 current loss 0.935229, current_train_items 154400.
I0302 19:00:44.558332 22732373614720 run.py:483] Algo bellman_ford step 4825 current loss 0.328956, current_train_items 154432.
I0302 19:00:44.574940 22732373614720 run.py:483] Algo bellman_ford step 4826 current loss 0.419339, current_train_items 154464.
I0302 19:00:44.597910 22732373614720 run.py:483] Algo bellman_ford step 4827 current loss 0.617154, current_train_items 154496.
I0302 19:00:44.628644 22732373614720 run.py:483] Algo bellman_ford step 4828 current loss 0.706918, current_train_items 154528.
I0302 19:00:44.662114 22732373614720 run.py:483] Algo bellman_ford step 4829 current loss 0.828089, current_train_items 154560.
I0302 19:00:44.681795 22732373614720 run.py:483] Algo bellman_ford step 4830 current loss 0.301615, current_train_items 154592.
I0302 19:00:44.697994 22732373614720 run.py:483] Algo bellman_ford step 4831 current loss 0.570905, current_train_items 154624.
I0302 19:00:44.721899 22732373614720 run.py:483] Algo bellman_ford step 4832 current loss 0.689180, current_train_items 154656.
I0302 19:00:44.753185 22732373614720 run.py:483] Algo bellman_ford step 4833 current loss 0.901741, current_train_items 154688.
I0302 19:00:44.786195 22732373614720 run.py:483] Algo bellman_ford step 4834 current loss 0.883500, current_train_items 154720.
I0302 19:00:44.805562 22732373614720 run.py:483] Algo bellman_ford step 4835 current loss 0.217819, current_train_items 154752.
I0302 19:00:44.821567 22732373614720 run.py:483] Algo bellman_ford step 4836 current loss 0.523120, current_train_items 154784.
I0302 19:00:44.843822 22732373614720 run.py:483] Algo bellman_ford step 4837 current loss 0.680849, current_train_items 154816.
I0302 19:00:44.874143 22732373614720 run.py:483] Algo bellman_ford step 4838 current loss 0.832199, current_train_items 154848.
I0302 19:00:44.905195 22732373614720 run.py:483] Algo bellman_ford step 4839 current loss 0.855770, current_train_items 154880.
I0302 19:00:44.924615 22732373614720 run.py:483] Algo bellman_ford step 4840 current loss 0.347571, current_train_items 154912.
I0302 19:00:44.940493 22732373614720 run.py:483] Algo bellman_ford step 4841 current loss 0.447681, current_train_items 154944.
I0302 19:00:44.963835 22732373614720 run.py:483] Algo bellman_ford step 4842 current loss 0.620188, current_train_items 154976.
I0302 19:00:44.993669 22732373614720 run.py:483] Algo bellman_ford step 4843 current loss 0.764255, current_train_items 155008.
I0302 19:00:45.028092 22732373614720 run.py:483] Algo bellman_ford step 4844 current loss 1.270437, current_train_items 155040.
I0302 19:00:45.047455 22732373614720 run.py:483] Algo bellman_ford step 4845 current loss 0.367171, current_train_items 155072.
I0302 19:00:45.063770 22732373614720 run.py:483] Algo bellman_ford step 4846 current loss 0.579466, current_train_items 155104.
I0302 19:00:45.086318 22732373614720 run.py:483] Algo bellman_ford step 4847 current loss 0.730546, current_train_items 155136.
I0302 19:00:45.116055 22732373614720 run.py:483] Algo bellman_ford step 4848 current loss 0.865793, current_train_items 155168.
I0302 19:00:45.149049 22732373614720 run.py:483] Algo bellman_ford step 4849 current loss 0.840558, current_train_items 155200.
I0302 19:00:45.168631 22732373614720 run.py:483] Algo bellman_ford step 4850 current loss 0.386521, current_train_items 155232.
I0302 19:00:45.176840 22732373614720 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.935546875, 'score': 0.935546875, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0302 19:00:45.176949 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.936, val scores are: bellman_ford: 0.936
I0302 19:00:45.193842 22732373614720 run.py:483] Algo bellman_ford step 4851 current loss 0.515923, current_train_items 155264.
I0302 19:00:45.216390 22732373614720 run.py:483] Algo bellman_ford step 4852 current loss 0.711791, current_train_items 155296.
I0302 19:00:45.246874 22732373614720 run.py:483] Algo bellman_ford step 4853 current loss 0.732650, current_train_items 155328.
I0302 19:00:45.280221 22732373614720 run.py:483] Algo bellman_ford step 4854 current loss 0.826672, current_train_items 155360.
I0302 19:00:45.300117 22732373614720 run.py:483] Algo bellman_ford step 4855 current loss 0.361887, current_train_items 155392.
I0302 19:00:45.316556 22732373614720 run.py:483] Algo bellman_ford step 4856 current loss 0.573219, current_train_items 155424.
I0302 19:00:45.340787 22732373614720 run.py:483] Algo bellman_ford step 4857 current loss 0.704821, current_train_items 155456.
I0302 19:00:45.371830 22732373614720 run.py:483] Algo bellman_ford step 4858 current loss 0.731625, current_train_items 155488.
I0302 19:00:45.406985 22732373614720 run.py:483] Algo bellman_ford step 4859 current loss 0.911973, current_train_items 155520.
I0302 19:00:45.427014 22732373614720 run.py:483] Algo bellman_ford step 4860 current loss 0.341108, current_train_items 155552.
I0302 19:00:45.443995 22732373614720 run.py:483] Algo bellman_ford step 4861 current loss 0.574013, current_train_items 155584.
I0302 19:00:45.466966 22732373614720 run.py:483] Algo bellman_ford step 4862 current loss 0.751476, current_train_items 155616.
I0302 19:00:45.498111 22732373614720 run.py:483] Algo bellman_ford step 4863 current loss 0.748532, current_train_items 155648.
I0302 19:00:45.530000 22732373614720 run.py:483] Algo bellman_ford step 4864 current loss 0.802722, current_train_items 155680.
I0302 19:00:45.549424 22732373614720 run.py:483] Algo bellman_ford step 4865 current loss 0.312507, current_train_items 155712.
I0302 19:00:45.565178 22732373614720 run.py:483] Algo bellman_ford step 4866 current loss 0.526926, current_train_items 155744.
I0302 19:00:45.588821 22732373614720 run.py:483] Algo bellman_ford step 4867 current loss 0.827757, current_train_items 155776.
I0302 19:00:45.620737 22732373614720 run.py:483] Algo bellman_ford step 4868 current loss 0.837570, current_train_items 155808.
I0302 19:00:45.653000 22732373614720 run.py:483] Algo bellman_ford step 4869 current loss 0.769325, current_train_items 155840.
I0302 19:00:45.673233 22732373614720 run.py:483] Algo bellman_ford step 4870 current loss 0.341854, current_train_items 155872.
I0302 19:00:45.689382 22732373614720 run.py:483] Algo bellman_ford step 4871 current loss 0.539629, current_train_items 155904.
I0302 19:00:45.712340 22732373614720 run.py:483] Algo bellman_ford step 4872 current loss 0.654720, current_train_items 155936.
I0302 19:00:45.740829 22732373614720 run.py:483] Algo bellman_ford step 4873 current loss 0.697168, current_train_items 155968.
I0302 19:00:45.772423 22732373614720 run.py:483] Algo bellman_ford step 4874 current loss 1.057851, current_train_items 156000.
I0302 19:00:45.792239 22732373614720 run.py:483] Algo bellman_ford step 4875 current loss 0.262775, current_train_items 156032.
I0302 19:00:45.808027 22732373614720 run.py:483] Algo bellman_ford step 4876 current loss 0.490617, current_train_items 156064.
I0302 19:00:45.830658 22732373614720 run.py:483] Algo bellman_ford step 4877 current loss 0.591159, current_train_items 156096.
I0302 19:00:45.860238 22732373614720 run.py:483] Algo bellman_ford step 4878 current loss 0.693168, current_train_items 156128.
I0302 19:00:45.893641 22732373614720 run.py:483] Algo bellman_ford step 4879 current loss 0.753697, current_train_items 156160.
I0302 19:00:45.912992 22732373614720 run.py:483] Algo bellman_ford step 4880 current loss 0.374418, current_train_items 156192.
I0302 19:00:45.929007 22732373614720 run.py:483] Algo bellman_ford step 4881 current loss 0.533842, current_train_items 156224.
I0302 19:00:45.953046 22732373614720 run.py:483] Algo bellman_ford step 4882 current loss 0.661084, current_train_items 156256.
I0302 19:00:45.983927 22732373614720 run.py:483] Algo bellman_ford step 4883 current loss 0.790926, current_train_items 156288.
I0302 19:00:46.017873 22732373614720 run.py:483] Algo bellman_ford step 4884 current loss 0.827548, current_train_items 156320.
I0302 19:00:46.037576 22732373614720 run.py:483] Algo bellman_ford step 4885 current loss 0.346481, current_train_items 156352.
I0302 19:00:46.053975 22732373614720 run.py:483] Algo bellman_ford step 4886 current loss 0.625946, current_train_items 156384.
I0302 19:00:46.076940 22732373614720 run.py:483] Algo bellman_ford step 4887 current loss 0.660537, current_train_items 156416.
I0302 19:00:46.107203 22732373614720 run.py:483] Algo bellman_ford step 4888 current loss 0.656287, current_train_items 156448.
I0302 19:00:46.139699 22732373614720 run.py:483] Algo bellman_ford step 4889 current loss 0.753702, current_train_items 156480.
I0302 19:00:46.159734 22732373614720 run.py:483] Algo bellman_ford step 4890 current loss 0.291383, current_train_items 156512.
I0302 19:00:46.175641 22732373614720 run.py:483] Algo bellman_ford step 4891 current loss 0.514416, current_train_items 156544.
I0302 19:00:46.198333 22732373614720 run.py:483] Algo bellman_ford step 4892 current loss 0.671583, current_train_items 156576.
I0302 19:00:46.227606 22732373614720 run.py:483] Algo bellman_ford step 4893 current loss 0.749566, current_train_items 156608.
I0302 19:00:46.259565 22732373614720 run.py:483] Algo bellman_ford step 4894 current loss 0.862344, current_train_items 156640.
I0302 19:00:46.278828 22732373614720 run.py:483] Algo bellman_ford step 4895 current loss 0.330262, current_train_items 156672.
I0302 19:00:46.294979 22732373614720 run.py:483] Algo bellman_ford step 4896 current loss 0.498833, current_train_items 156704.
I0302 19:00:46.317682 22732373614720 run.py:483] Algo bellman_ford step 4897 current loss 0.668155, current_train_items 156736.
I0302 19:00:46.347225 22732373614720 run.py:483] Algo bellman_ford step 4898 current loss 0.735286, current_train_items 156768.
I0302 19:00:46.381106 22732373614720 run.py:483] Algo bellman_ford step 4899 current loss 0.933532, current_train_items 156800.
I0302 19:00:46.401272 22732373614720 run.py:483] Algo bellman_ford step 4900 current loss 0.360721, current_train_items 156832.
I0302 19:00:46.409204 22732373614720 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.939453125, 'score': 0.939453125, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0302 19:00:46.409337 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.938, current avg val score is 0.939, val scores are: bellman_ford: 0.939
I0302 19:00:46.438963 22732373614720 run.py:483] Algo bellman_ford step 4901 current loss 0.497669, current_train_items 156864.
I0302 19:00:46.462600 22732373614720 run.py:483] Algo bellman_ford step 4902 current loss 0.647704, current_train_items 156896.
I0302 19:00:46.495714 22732373614720 run.py:483] Algo bellman_ford step 4903 current loss 0.725311, current_train_items 156928.
I0302 19:00:46.528931 22732373614720 run.py:483] Algo bellman_ford step 4904 current loss 0.754208, current_train_items 156960.
I0302 19:00:46.549165 22732373614720 run.py:483] Algo bellman_ford step 4905 current loss 0.405761, current_train_items 156992.
I0302 19:00:46.564942 22732373614720 run.py:483] Algo bellman_ford step 4906 current loss 0.573997, current_train_items 157024.
I0302 19:00:46.588745 22732373614720 run.py:483] Algo bellman_ford step 4907 current loss 0.725994, current_train_items 157056.
I0302 19:00:46.619251 22732373614720 run.py:483] Algo bellman_ford step 4908 current loss 0.746438, current_train_items 157088.
I0302 19:00:46.651510 22732373614720 run.py:483] Algo bellman_ford step 4909 current loss 0.824574, current_train_items 157120.
I0302 19:00:46.671253 22732373614720 run.py:483] Algo bellman_ford step 4910 current loss 0.354112, current_train_items 157152.
I0302 19:00:46.687490 22732373614720 run.py:483] Algo bellman_ford step 4911 current loss 0.527419, current_train_items 157184.
I0302 19:00:46.711003 22732373614720 run.py:483] Algo bellman_ford step 4912 current loss 0.591696, current_train_items 157216.
I0302 19:00:46.742519 22732373614720 run.py:483] Algo bellman_ford step 4913 current loss 0.832724, current_train_items 157248.
I0302 19:00:46.775300 22732373614720 run.py:483] Algo bellman_ford step 4914 current loss 0.804346, current_train_items 157280.
I0302 19:00:46.794667 22732373614720 run.py:483] Algo bellman_ford step 4915 current loss 0.311363, current_train_items 157312.
I0302 19:00:46.810449 22732373614720 run.py:483] Algo bellman_ford step 4916 current loss 0.574267, current_train_items 157344.
I0302 19:00:46.833506 22732373614720 run.py:483] Algo bellman_ford step 4917 current loss 0.780615, current_train_items 157376.
I0302 19:00:46.862710 22732373614720 run.py:483] Algo bellman_ford step 4918 current loss 0.828529, current_train_items 157408.
I0302 19:00:46.895433 22732373614720 run.py:483] Algo bellman_ford step 4919 current loss 0.940394, current_train_items 157440.
I0302 19:00:46.914808 22732373614720 run.py:483] Algo bellman_ford step 4920 current loss 0.374796, current_train_items 157472.
I0302 19:00:46.930222 22732373614720 run.py:483] Algo bellman_ford step 4921 current loss 0.417589, current_train_items 157504.
I0302 19:00:46.953542 22732373614720 run.py:483] Algo bellman_ford step 4922 current loss 0.625235, current_train_items 157536.
I0302 19:00:46.983908 22732373614720 run.py:483] Algo bellman_ford step 4923 current loss 0.822309, current_train_items 157568.
I0302 19:00:47.016439 22732373614720 run.py:483] Algo bellman_ford step 4924 current loss 1.174332, current_train_items 157600.
I0302 19:00:47.035964 22732373614720 run.py:483] Algo bellman_ford step 4925 current loss 0.298451, current_train_items 157632.
I0302 19:00:47.052087 22732373614720 run.py:483] Algo bellman_ford step 4926 current loss 0.596614, current_train_items 157664.
I0302 19:00:47.075909 22732373614720 run.py:483] Algo bellman_ford step 4927 current loss 0.785097, current_train_items 157696.
I0302 19:00:47.105432 22732373614720 run.py:483] Algo bellman_ford step 4928 current loss 0.717153, current_train_items 157728.
I0302 19:00:47.138169 22732373614720 run.py:483] Algo bellman_ford step 4929 current loss 0.945203, current_train_items 157760.
I0302 19:00:47.157418 22732373614720 run.py:483] Algo bellman_ford step 4930 current loss 0.313995, current_train_items 157792.
I0302 19:00:47.173147 22732373614720 run.py:483] Algo bellman_ford step 4931 current loss 0.572777, current_train_items 157824.
I0302 19:00:47.196042 22732373614720 run.py:483] Algo bellman_ford step 4932 current loss 0.831514, current_train_items 157856.
I0302 19:00:47.225593 22732373614720 run.py:483] Algo bellman_ford step 4933 current loss 0.812148, current_train_items 157888.
I0302 19:00:47.258074 22732373614720 run.py:483] Algo bellman_ford step 4934 current loss 0.971675, current_train_items 157920.
I0302 19:00:47.277597 22732373614720 run.py:483] Algo bellman_ford step 4935 current loss 0.361336, current_train_items 157952.
I0302 19:00:47.293502 22732373614720 run.py:483] Algo bellman_ford step 4936 current loss 0.528323, current_train_items 157984.
I0302 19:00:47.317883 22732373614720 run.py:483] Algo bellman_ford step 4937 current loss 0.733061, current_train_items 158016.
I0302 19:00:47.347886 22732373614720 run.py:483] Algo bellman_ford step 4938 current loss 0.706486, current_train_items 158048.
I0302 19:00:47.382479 22732373614720 run.py:483] Algo bellman_ford step 4939 current loss 0.925691, current_train_items 158080.
I0302 19:00:47.401745 22732373614720 run.py:483] Algo bellman_ford step 4940 current loss 0.334254, current_train_items 158112.
I0302 19:00:47.417353 22732373614720 run.py:483] Algo bellman_ford step 4941 current loss 0.478111, current_train_items 158144.
I0302 19:00:47.440289 22732373614720 run.py:483] Algo bellman_ford step 4942 current loss 0.603831, current_train_items 158176.
I0302 19:00:47.471426 22732373614720 run.py:483] Algo bellman_ford step 4943 current loss 0.718105, current_train_items 158208.
I0302 19:00:47.502556 22732373614720 run.py:483] Algo bellman_ford step 4944 current loss 0.749644, current_train_items 158240.
I0302 19:00:47.521783 22732373614720 run.py:483] Algo bellman_ford step 4945 current loss 0.445667, current_train_items 158272.
I0302 19:00:47.537727 22732373614720 run.py:483] Algo bellman_ford step 4946 current loss 0.610208, current_train_items 158304.
I0302 19:00:47.560388 22732373614720 run.py:483] Algo bellman_ford step 4947 current loss 0.812446, current_train_items 158336.
I0302 19:00:47.590811 22732373614720 run.py:483] Algo bellman_ford step 4948 current loss 0.944835, current_train_items 158368.
I0302 19:00:47.624823 22732373614720 run.py:483] Algo bellman_ford step 4949 current loss 0.958512, current_train_items 158400.
I0302 19:00:47.644263 22732373614720 run.py:483] Algo bellman_ford step 4950 current loss 0.312099, current_train_items 158432.
I0302 19:00:47.652477 22732373614720 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0302 19:00:47.652589 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:00:47.669005 22732373614720 run.py:483] Algo bellman_ford step 4951 current loss 0.540367, current_train_items 158464.
I0302 19:00:47.692470 22732373614720 run.py:483] Algo bellman_ford step 4952 current loss 0.762484, current_train_items 158496.
I0302 19:00:47.723741 22732373614720 run.py:483] Algo bellman_ford step 4953 current loss 0.739510, current_train_items 158528.
I0302 19:00:47.759025 22732373614720 run.py:483] Algo bellman_ford step 4954 current loss 1.021294, current_train_items 158560.
I0302 19:00:47.779141 22732373614720 run.py:483] Algo bellman_ford step 4955 current loss 0.378532, current_train_items 158592.
I0302 19:00:47.795004 22732373614720 run.py:483] Algo bellman_ford step 4956 current loss 0.525236, current_train_items 158624.
I0302 19:00:47.819483 22732373614720 run.py:483] Algo bellman_ford step 4957 current loss 0.766747, current_train_items 158656.
I0302 19:00:47.850044 22732373614720 run.py:483] Algo bellman_ford step 4958 current loss 0.791810, current_train_items 158688.
I0302 19:00:47.880849 22732373614720 run.py:483] Algo bellman_ford step 4959 current loss 0.905631, current_train_items 158720.
I0302 19:00:47.900960 22732373614720 run.py:483] Algo bellman_ford step 4960 current loss 0.327278, current_train_items 158752.
I0302 19:00:47.917305 22732373614720 run.py:483] Algo bellman_ford step 4961 current loss 0.459279, current_train_items 158784.
I0302 19:00:47.940214 22732373614720 run.py:483] Algo bellman_ford step 4962 current loss 0.630146, current_train_items 158816.
I0302 19:00:47.969702 22732373614720 run.py:483] Algo bellman_ford step 4963 current loss 0.651753, current_train_items 158848.
I0302 19:00:48.002707 22732373614720 run.py:483] Algo bellman_ford step 4964 current loss 0.801254, current_train_items 158880.
I0302 19:00:48.022063 22732373614720 run.py:483] Algo bellman_ford step 4965 current loss 0.275266, current_train_items 158912.
I0302 19:00:48.038033 22732373614720 run.py:483] Algo bellman_ford step 4966 current loss 0.475602, current_train_items 158944.
I0302 19:00:48.062359 22732373614720 run.py:483] Algo bellman_ford step 4967 current loss 0.789428, current_train_items 158976.
I0302 19:00:48.093882 22732373614720 run.py:483] Algo bellman_ford step 4968 current loss 0.774632, current_train_items 159008.
I0302 19:00:48.125092 22732373614720 run.py:483] Algo bellman_ford step 4969 current loss 0.827414, current_train_items 159040.
I0302 19:00:48.144908 22732373614720 run.py:483] Algo bellman_ford step 4970 current loss 0.341639, current_train_items 159072.
I0302 19:00:48.160832 22732373614720 run.py:483] Algo bellman_ford step 4971 current loss 0.499775, current_train_items 159104.
I0302 19:00:48.184328 22732373614720 run.py:483] Algo bellman_ford step 4972 current loss 0.662526, current_train_items 159136.
I0302 19:00:48.215402 22732373614720 run.py:483] Algo bellman_ford step 4973 current loss 0.808017, current_train_items 159168.
I0302 19:00:48.247833 22732373614720 run.py:483] Algo bellman_ford step 4974 current loss 0.913381, current_train_items 159200.
I0302 19:00:48.267807 22732373614720 run.py:483] Algo bellman_ford step 4975 current loss 0.367797, current_train_items 159232.
I0302 19:00:48.284297 22732373614720 run.py:483] Algo bellman_ford step 4976 current loss 0.481516, current_train_items 159264.
I0302 19:00:48.308303 22732373614720 run.py:483] Algo bellman_ford step 4977 current loss 0.757128, current_train_items 159296.
I0302 19:00:48.339446 22732373614720 run.py:483] Algo bellman_ford step 4978 current loss 0.828116, current_train_items 159328.
I0302 19:00:48.373249 22732373614720 run.py:483] Algo bellman_ford step 4979 current loss 0.863176, current_train_items 159360.
I0302 19:00:48.392839 22732373614720 run.py:483] Algo bellman_ford step 4980 current loss 0.341479, current_train_items 159392.
I0302 19:00:48.409110 22732373614720 run.py:483] Algo bellman_ford step 4981 current loss 0.654993, current_train_items 159424.
I0302 19:00:48.433524 22732373614720 run.py:483] Algo bellman_ford step 4982 current loss 0.658367, current_train_items 159456.
I0302 19:00:48.462361 22732373614720 run.py:483] Algo bellman_ford step 4983 current loss 0.728635, current_train_items 159488.
I0302 19:00:48.494904 22732373614720 run.py:483] Algo bellman_ford step 4984 current loss 0.899468, current_train_items 159520.
I0302 19:00:48.515057 22732373614720 run.py:483] Algo bellman_ford step 4985 current loss 0.379768, current_train_items 159552.
I0302 19:00:48.531310 22732373614720 run.py:483] Algo bellman_ford step 4986 current loss 0.481565, current_train_items 159584.
I0302 19:00:48.553409 22732373614720 run.py:483] Algo bellman_ford step 4987 current loss 0.627853, current_train_items 159616.
I0302 19:00:48.583142 22732373614720 run.py:483] Algo bellman_ford step 4988 current loss 0.713252, current_train_items 159648.
I0302 19:00:48.618313 22732373614720 run.py:483] Algo bellman_ford step 4989 current loss 0.963172, current_train_items 159680.
I0302 19:00:48.638244 22732373614720 run.py:483] Algo bellman_ford step 4990 current loss 0.343640, current_train_items 159712.
I0302 19:00:48.654044 22732373614720 run.py:483] Algo bellman_ford step 4991 current loss 0.470389, current_train_items 159744.
I0302 19:00:48.677372 22732373614720 run.py:483] Algo bellman_ford step 4992 current loss 0.671293, current_train_items 159776.
I0302 19:00:48.707765 22732373614720 run.py:483] Algo bellman_ford step 4993 current loss 0.734829, current_train_items 159808.
I0302 19:00:48.741215 22732373614720 run.py:483] Algo bellman_ford step 4994 current loss 1.005381, current_train_items 159840.
I0302 19:00:48.760343 22732373614720 run.py:483] Algo bellman_ford step 4995 current loss 0.295540, current_train_items 159872.
I0302 19:00:48.776644 22732373614720 run.py:483] Algo bellman_ford step 4996 current loss 0.496394, current_train_items 159904.
I0302 19:00:48.799883 22732373614720 run.py:483] Algo bellman_ford step 4997 current loss 0.724594, current_train_items 159936.
I0302 19:00:48.830971 22732373614720 run.py:483] Algo bellman_ford step 4998 current loss 0.765814, current_train_items 159968.
I0302 19:00:48.865028 22732373614720 run.py:483] Algo bellman_ford step 4999 current loss 0.923935, current_train_items 160000.
I0302 19:00:48.885072 22732373614720 run.py:483] Algo bellman_ford step 5000 current loss 0.380360, current_train_items 160032.
I0302 19:00:48.892962 22732373614720 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0302 19:00:48.893282 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:00:48.910210 22732373614720 run.py:483] Algo bellman_ford step 5001 current loss 0.551011, current_train_items 160064.
I0302 19:00:48.933611 22732373614720 run.py:483] Algo bellman_ford step 5002 current loss 0.702929, current_train_items 160096.
I0302 19:00:48.963474 22732373614720 run.py:483] Algo bellman_ford step 5003 current loss 0.580154, current_train_items 160128.
I0302 19:00:48.996087 22732373614720 run.py:483] Algo bellman_ford step 5004 current loss 0.860683, current_train_items 160160.
I0302 19:00:49.016042 22732373614720 run.py:483] Algo bellman_ford step 5005 current loss 0.343728, current_train_items 160192.
I0302 19:00:49.031903 22732373614720 run.py:483] Algo bellman_ford step 5006 current loss 0.554225, current_train_items 160224.
I0302 19:00:49.055860 22732373614720 run.py:483] Algo bellman_ford step 5007 current loss 0.772306, current_train_items 160256.
I0302 19:00:49.088133 22732373614720 run.py:483] Algo bellman_ford step 5008 current loss 0.823215, current_train_items 160288.
I0302 19:00:49.119313 22732373614720 run.py:483] Algo bellman_ford step 5009 current loss 0.838210, current_train_items 160320.
I0302 19:00:49.138969 22732373614720 run.py:483] Algo bellman_ford step 5010 current loss 0.366239, current_train_items 160352.
I0302 19:00:49.154712 22732373614720 run.py:483] Algo bellman_ford step 5011 current loss 0.503822, current_train_items 160384.
I0302 19:00:49.178261 22732373614720 run.py:483] Algo bellman_ford step 5012 current loss 0.614405, current_train_items 160416.
I0302 19:00:49.208177 22732373614720 run.py:483] Algo bellman_ford step 5013 current loss 0.793843, current_train_items 160448.
I0302 19:00:49.242297 22732373614720 run.py:483] Algo bellman_ford step 5014 current loss 0.840396, current_train_items 160480.
I0302 19:00:49.262304 22732373614720 run.py:483] Algo bellman_ford step 5015 current loss 0.391720, current_train_items 160512.
I0302 19:00:49.278993 22732373614720 run.py:483] Algo bellman_ford step 5016 current loss 0.585049, current_train_items 160544.
I0302 19:00:49.301964 22732373614720 run.py:483] Algo bellman_ford step 5017 current loss 0.742815, current_train_items 160576.
I0302 19:00:49.332854 22732373614720 run.py:483] Algo bellman_ford step 5018 current loss 0.761687, current_train_items 160608.
I0302 19:00:49.367861 22732373614720 run.py:483] Algo bellman_ford step 5019 current loss 0.970507, current_train_items 160640.
I0302 19:00:49.387239 22732373614720 run.py:483] Algo bellman_ford step 5020 current loss 0.308386, current_train_items 160672.
I0302 19:00:49.403032 22732373614720 run.py:483] Algo bellman_ford step 5021 current loss 0.473265, current_train_items 160704.
I0302 19:00:49.427415 22732373614720 run.py:483] Algo bellman_ford step 5022 current loss 0.736006, current_train_items 160736.
I0302 19:00:49.458090 22732373614720 run.py:483] Algo bellman_ford step 5023 current loss 0.808130, current_train_items 160768.
I0302 19:00:49.491423 22732373614720 run.py:483] Algo bellman_ford step 5024 current loss 0.843242, current_train_items 160800.
I0302 19:00:49.510820 22732373614720 run.py:483] Algo bellman_ford step 5025 current loss 0.330930, current_train_items 160832.
I0302 19:00:49.526380 22732373614720 run.py:483] Algo bellman_ford step 5026 current loss 0.491862, current_train_items 160864.
I0302 19:00:49.548811 22732373614720 run.py:483] Algo bellman_ford step 5027 current loss 0.624138, current_train_items 160896.
I0302 19:00:49.577814 22732373614720 run.py:483] Algo bellman_ford step 5028 current loss 0.652699, current_train_items 160928.
I0302 19:00:49.610470 22732373614720 run.py:483] Algo bellman_ford step 5029 current loss 0.821831, current_train_items 160960.
I0302 19:00:49.630232 22732373614720 run.py:483] Algo bellman_ford step 5030 current loss 0.290154, current_train_items 160992.
I0302 19:00:49.646274 22732373614720 run.py:483] Algo bellman_ford step 5031 current loss 0.536119, current_train_items 161024.
I0302 19:00:49.670085 22732373614720 run.py:483] Algo bellman_ford step 5032 current loss 0.656874, current_train_items 161056.
I0302 19:00:49.699379 22732373614720 run.py:483] Algo bellman_ford step 5033 current loss 0.716666, current_train_items 161088.
I0302 19:00:49.733500 22732373614720 run.py:483] Algo bellman_ford step 5034 current loss 0.892278, current_train_items 161120.
I0302 19:00:49.753331 22732373614720 run.py:483] Algo bellman_ford step 5035 current loss 0.302593, current_train_items 161152.
I0302 19:00:49.769517 22732373614720 run.py:483] Algo bellman_ford step 5036 current loss 0.511097, current_train_items 161184.
I0302 19:00:49.792840 22732373614720 run.py:483] Algo bellman_ford step 5037 current loss 0.732407, current_train_items 161216.
I0302 19:00:49.824709 22732373614720 run.py:483] Algo bellman_ford step 5038 current loss 0.839099, current_train_items 161248.
I0302 19:00:49.857947 22732373614720 run.py:483] Algo bellman_ford step 5039 current loss 0.946856, current_train_items 161280.
I0302 19:00:49.877488 22732373614720 run.py:483] Algo bellman_ford step 5040 current loss 0.314299, current_train_items 161312.
I0302 19:00:49.892945 22732373614720 run.py:483] Algo bellman_ford step 5041 current loss 0.459735, current_train_items 161344.
I0302 19:00:49.917105 22732373614720 run.py:483] Algo bellman_ford step 5042 current loss 0.754156, current_train_items 161376.
I0302 19:00:49.946614 22732373614720 run.py:483] Algo bellman_ford step 5043 current loss 0.665704, current_train_items 161408.
I0302 19:00:49.979315 22732373614720 run.py:483] Algo bellman_ford step 5044 current loss 0.880062, current_train_items 161440.
I0302 19:00:49.998799 22732373614720 run.py:483] Algo bellman_ford step 5045 current loss 0.334529, current_train_items 161472.
I0302 19:00:50.014853 22732373614720 run.py:483] Algo bellman_ford step 5046 current loss 0.598605, current_train_items 161504.
I0302 19:00:50.037338 22732373614720 run.py:483] Algo bellman_ford step 5047 current loss 0.625797, current_train_items 161536.
I0302 19:00:50.067973 22732373614720 run.py:483] Algo bellman_ford step 5048 current loss 0.877729, current_train_items 161568.
I0302 19:00:50.101027 22732373614720 run.py:483] Algo bellman_ford step 5049 current loss 0.833170, current_train_items 161600.
I0302 19:00:50.120699 22732373614720 run.py:483] Algo bellman_ford step 5050 current loss 0.406533, current_train_items 161632.
I0302 19:00:50.128955 22732373614720 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0302 19:00:50.129070 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:00:50.145511 22732373614720 run.py:483] Algo bellman_ford step 5051 current loss 0.593823, current_train_items 161664.
I0302 19:00:50.169891 22732373614720 run.py:483] Algo bellman_ford step 5052 current loss 0.723097, current_train_items 161696.
I0302 19:00:50.201110 22732373614720 run.py:483] Algo bellman_ford step 5053 current loss 0.755053, current_train_items 161728.
I0302 19:00:50.233686 22732373614720 run.py:483] Algo bellman_ford step 5054 current loss 0.833445, current_train_items 161760.
I0302 19:00:50.253769 22732373614720 run.py:483] Algo bellman_ford step 5055 current loss 0.379958, current_train_items 161792.
I0302 19:00:50.270578 22732373614720 run.py:483] Algo bellman_ford step 5056 current loss 0.612279, current_train_items 161824.
I0302 19:00:50.295215 22732373614720 run.py:483] Algo bellman_ford step 5057 current loss 0.740770, current_train_items 161856.
I0302 19:00:50.325272 22732373614720 run.py:483] Algo bellman_ford step 5058 current loss 0.710423, current_train_items 161888.
I0302 19:00:50.358593 22732373614720 run.py:483] Algo bellman_ford step 5059 current loss 0.796661, current_train_items 161920.
I0302 19:00:50.378398 22732373614720 run.py:483] Algo bellman_ford step 5060 current loss 0.333512, current_train_items 161952.
I0302 19:00:50.394404 22732373614720 run.py:483] Algo bellman_ford step 5061 current loss 0.451023, current_train_items 161984.
I0302 19:00:50.416102 22732373614720 run.py:483] Algo bellman_ford step 5062 current loss 0.661927, current_train_items 162016.
I0302 19:00:50.446434 22732373614720 run.py:483] Algo bellman_ford step 5063 current loss 0.798551, current_train_items 162048.
I0302 19:00:50.477619 22732373614720 run.py:483] Algo bellman_ford step 5064 current loss 0.839940, current_train_items 162080.
I0302 19:00:50.496901 22732373614720 run.py:483] Algo bellman_ford step 5065 current loss 0.363379, current_train_items 162112.
I0302 19:00:50.512894 22732373614720 run.py:483] Algo bellman_ford step 5066 current loss 0.560628, current_train_items 162144.
I0302 19:00:50.537318 22732373614720 run.py:483] Algo bellman_ford step 5067 current loss 0.675786, current_train_items 162176.
I0302 19:00:50.567687 22732373614720 run.py:483] Algo bellman_ford step 5068 current loss 0.660646, current_train_items 162208.
I0302 19:00:50.601834 22732373614720 run.py:483] Algo bellman_ford step 5069 current loss 0.993546, current_train_items 162240.
I0302 19:00:50.621599 22732373614720 run.py:483] Algo bellman_ford step 5070 current loss 0.294424, current_train_items 162272.
I0302 19:00:50.638200 22732373614720 run.py:483] Algo bellman_ford step 5071 current loss 0.523848, current_train_items 162304.
I0302 19:00:50.659848 22732373614720 run.py:483] Algo bellman_ford step 5072 current loss 0.637937, current_train_items 162336.
I0302 19:00:50.690992 22732373614720 run.py:483] Algo bellman_ford step 5073 current loss 0.863806, current_train_items 162368.
I0302 19:00:50.721660 22732373614720 run.py:483] Algo bellman_ford step 5074 current loss 0.859751, current_train_items 162400.
I0302 19:00:50.741557 22732373614720 run.py:483] Algo bellman_ford step 5075 current loss 0.343238, current_train_items 162432.
I0302 19:00:50.757575 22732373614720 run.py:483] Algo bellman_ford step 5076 current loss 0.527835, current_train_items 162464.
I0302 19:00:50.780364 22732373614720 run.py:483] Algo bellman_ford step 5077 current loss 0.665232, current_train_items 162496.
I0302 19:00:50.811038 22732373614720 run.py:483] Algo bellman_ford step 5078 current loss 0.854021, current_train_items 162528.
I0302 19:00:50.842930 22732373614720 run.py:483] Algo bellman_ford step 5079 current loss 1.068162, current_train_items 162560.
I0302 19:00:50.862552 22732373614720 run.py:483] Algo bellman_ford step 5080 current loss 0.320675, current_train_items 162592.
I0302 19:00:50.878242 22732373614720 run.py:483] Algo bellman_ford step 5081 current loss 0.429098, current_train_items 162624.
I0302 19:00:50.901221 22732373614720 run.py:483] Algo bellman_ford step 5082 current loss 0.630727, current_train_items 162656.
I0302 19:00:50.932090 22732373614720 run.py:483] Algo bellman_ford step 5083 current loss 0.756280, current_train_items 162688.
I0302 19:00:50.965057 22732373614720 run.py:483] Algo bellman_ford step 5084 current loss 0.908659, current_train_items 162720.
I0302 19:00:50.985113 22732373614720 run.py:483] Algo bellman_ford step 5085 current loss 0.383202, current_train_items 162752.
I0302 19:00:51.001067 22732373614720 run.py:483] Algo bellman_ford step 5086 current loss 0.573627, current_train_items 162784.
I0302 19:00:51.024555 22732373614720 run.py:483] Algo bellman_ford step 5087 current loss 0.706936, current_train_items 162816.
I0302 19:00:51.055188 22732373614720 run.py:483] Algo bellman_ford step 5088 current loss 0.667473, current_train_items 162848.
I0302 19:00:51.089245 22732373614720 run.py:483] Algo bellman_ford step 5089 current loss 0.875968, current_train_items 162880.
I0302 19:00:51.109033 22732373614720 run.py:483] Algo bellman_ford step 5090 current loss 0.301721, current_train_items 162912.
I0302 19:00:51.125344 22732373614720 run.py:483] Algo bellman_ford step 5091 current loss 0.559958, current_train_items 162944.
I0302 19:00:51.147307 22732373614720 run.py:483] Algo bellman_ford step 5092 current loss 0.641329, current_train_items 162976.
I0302 19:00:51.177075 22732373614720 run.py:483] Algo bellman_ford step 5093 current loss 0.727325, current_train_items 163008.
I0302 19:00:51.209012 22732373614720 run.py:483] Algo bellman_ford step 5094 current loss 0.846482, current_train_items 163040.
I0302 19:00:51.228336 22732373614720 run.py:483] Algo bellman_ford step 5095 current loss 0.306347, current_train_items 163072.
I0302 19:00:51.244409 22732373614720 run.py:483] Algo bellman_ford step 5096 current loss 0.569846, current_train_items 163104.
I0302 19:00:51.268181 22732373614720 run.py:483] Algo bellman_ford step 5097 current loss 0.765034, current_train_items 163136.
I0302 19:00:51.299550 22732373614720 run.py:483] Algo bellman_ford step 5098 current loss 0.838348, current_train_items 163168.
I0302 19:00:51.334180 22732373614720 run.py:483] Algo bellman_ford step 5099 current loss 1.026855, current_train_items 163200.
I0302 19:00:51.353930 22732373614720 run.py:483] Algo bellman_ford step 5100 current loss 0.274207, current_train_items 163232.
I0302 19:00:51.361842 22732373614720 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0302 19:00:51.361954 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:00:51.378349 22732373614720 run.py:483] Algo bellman_ford step 5101 current loss 0.547530, current_train_items 163264.
I0302 19:00:51.400249 22732373614720 run.py:483] Algo bellman_ford step 5102 current loss 0.553978, current_train_items 163296.
I0302 19:00:51.431799 22732373614720 run.py:483] Algo bellman_ford step 5103 current loss 0.698816, current_train_items 163328.
I0302 19:00:51.465721 22732373614720 run.py:483] Algo bellman_ford step 5104 current loss 0.869642, current_train_items 163360.
I0302 19:00:51.485814 22732373614720 run.py:483] Algo bellman_ford step 5105 current loss 0.306773, current_train_items 163392.
I0302 19:00:51.502235 22732373614720 run.py:483] Algo bellman_ford step 5106 current loss 0.608693, current_train_items 163424.
I0302 19:00:51.525463 22732373614720 run.py:483] Algo bellman_ford step 5107 current loss 0.650689, current_train_items 163456.
I0302 19:00:51.554648 22732373614720 run.py:483] Algo bellman_ford step 5108 current loss 0.733793, current_train_items 163488.
I0302 19:00:51.588223 22732373614720 run.py:483] Algo bellman_ford step 5109 current loss 0.813160, current_train_items 163520.
I0302 19:00:51.608059 22732373614720 run.py:483] Algo bellman_ford step 5110 current loss 0.351498, current_train_items 163552.
I0302 19:00:51.624121 22732373614720 run.py:483] Algo bellman_ford step 5111 current loss 0.519348, current_train_items 163584.
I0302 19:00:51.646747 22732373614720 run.py:483] Algo bellman_ford step 5112 current loss 0.635567, current_train_items 163616.
I0302 19:00:51.676399 22732373614720 run.py:483] Algo bellman_ford step 5113 current loss 0.670460, current_train_items 163648.
I0302 19:00:51.708919 22732373614720 run.py:483] Algo bellman_ford step 5114 current loss 0.789040, current_train_items 163680.
I0302 19:00:51.728403 22732373614720 run.py:483] Algo bellman_ford step 5115 current loss 0.347386, current_train_items 163712.
I0302 19:00:51.744700 22732373614720 run.py:483] Algo bellman_ford step 5116 current loss 0.541181, current_train_items 163744.
I0302 19:00:51.768516 22732373614720 run.py:483] Algo bellman_ford step 5117 current loss 0.691694, current_train_items 163776.
I0302 19:00:51.798266 22732373614720 run.py:483] Algo bellman_ford step 5118 current loss 0.690797, current_train_items 163808.
I0302 19:00:51.831877 22732373614720 run.py:483] Algo bellman_ford step 5119 current loss 0.832165, current_train_items 163840.
I0302 19:00:51.851511 22732373614720 run.py:483] Algo bellman_ford step 5120 current loss 0.395261, current_train_items 163872.
I0302 19:00:51.867146 22732373614720 run.py:483] Algo bellman_ford step 5121 current loss 0.515871, current_train_items 163904.
I0302 19:00:51.889795 22732373614720 run.py:483] Algo bellman_ford step 5122 current loss 0.689338, current_train_items 163936.
I0302 19:00:51.919377 22732373614720 run.py:483] Algo bellman_ford step 5123 current loss 0.735398, current_train_items 163968.
I0302 19:00:51.952068 22732373614720 run.py:483] Algo bellman_ford step 5124 current loss 0.815719, current_train_items 164000.
I0302 19:00:51.971320 22732373614720 run.py:483] Algo bellman_ford step 5125 current loss 0.353358, current_train_items 164032.
I0302 19:00:51.987221 22732373614720 run.py:483] Algo bellman_ford step 5126 current loss 0.478271, current_train_items 164064.
I0302 19:00:52.010377 22732373614720 run.py:483] Algo bellman_ford step 5127 current loss 0.776082, current_train_items 164096.
I0302 19:00:52.041127 22732373614720 run.py:483] Algo bellman_ford step 5128 current loss 0.765826, current_train_items 164128.
I0302 19:00:52.075389 22732373614720 run.py:483] Algo bellman_ford step 5129 current loss 0.872320, current_train_items 164160.
I0302 19:00:52.094680 22732373614720 run.py:483] Algo bellman_ford step 5130 current loss 0.398449, current_train_items 164192.
I0302 19:00:52.110874 22732373614720 run.py:483] Algo bellman_ford step 5131 current loss 0.564938, current_train_items 164224.
I0302 19:00:52.133463 22732373614720 run.py:483] Algo bellman_ford step 5132 current loss 0.788487, current_train_items 164256.
I0302 19:00:52.164579 22732373614720 run.py:483] Algo bellman_ford step 5133 current loss 0.788572, current_train_items 164288.
I0302 19:00:52.197170 22732373614720 run.py:483] Algo bellman_ford step 5134 current loss 0.785408, current_train_items 164320.
I0302 19:00:52.217100 22732373614720 run.py:483] Algo bellman_ford step 5135 current loss 0.355396, current_train_items 164352.
I0302 19:00:52.233022 22732373614720 run.py:483] Algo bellman_ford step 5136 current loss 0.480042, current_train_items 164384.
I0302 19:00:52.256535 22732373614720 run.py:483] Algo bellman_ford step 5137 current loss 0.711920, current_train_items 164416.
I0302 19:00:52.286702 22732373614720 run.py:483] Algo bellman_ford step 5138 current loss 0.724844, current_train_items 164448.
I0302 19:00:52.320305 22732373614720 run.py:483] Algo bellman_ford step 5139 current loss 0.819594, current_train_items 164480.
I0302 19:00:52.340031 22732373614720 run.py:483] Algo bellman_ford step 5140 current loss 0.371840, current_train_items 164512.
I0302 19:00:52.356371 22732373614720 run.py:483] Algo bellman_ford step 5141 current loss 0.597535, current_train_items 164544.
I0302 19:00:52.380380 22732373614720 run.py:483] Algo bellman_ford step 5142 current loss 0.802093, current_train_items 164576.
I0302 19:00:52.410656 22732373614720 run.py:483] Algo bellman_ford step 5143 current loss 0.900051, current_train_items 164608.
I0302 19:00:52.444078 22732373614720 run.py:483] Algo bellman_ford step 5144 current loss 0.968372, current_train_items 164640.
I0302 19:00:52.463514 22732373614720 run.py:483] Algo bellman_ford step 5145 current loss 0.296898, current_train_items 164672.
I0302 19:00:52.479364 22732373614720 run.py:483] Algo bellman_ford step 5146 current loss 0.622697, current_train_items 164704.
I0302 19:00:52.501196 22732373614720 run.py:483] Algo bellman_ford step 5147 current loss 0.795352, current_train_items 164736.
I0302 19:00:52.530607 22732373614720 run.py:483] Algo bellman_ford step 5148 current loss 0.821240, current_train_items 164768.
I0302 19:00:52.562717 22732373614720 run.py:483] Algo bellman_ford step 5149 current loss 0.878744, current_train_items 164800.
I0302 19:00:52.582411 22732373614720 run.py:483] Algo bellman_ford step 5150 current loss 0.299390, current_train_items 164832.
I0302 19:00:52.590574 22732373614720 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0302 19:00:52.590683 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:00:52.607312 22732373614720 run.py:483] Algo bellman_ford step 5151 current loss 0.617294, current_train_items 164864.
I0302 19:00:52.630669 22732373614720 run.py:483] Algo bellman_ford step 5152 current loss 0.716839, current_train_items 164896.
I0302 19:00:52.660270 22732373614720 run.py:483] Algo bellman_ford step 5153 current loss 0.837131, current_train_items 164928.
I0302 19:00:52.692809 22732373614720 run.py:483] Algo bellman_ford step 5154 current loss 1.030530, current_train_items 164960.
I0302 19:00:52.712761 22732373614720 run.py:483] Algo bellman_ford step 5155 current loss 0.357165, current_train_items 164992.
I0302 19:00:52.728572 22732373614720 run.py:483] Algo bellman_ford step 5156 current loss 0.496527, current_train_items 165024.
I0302 19:00:52.752610 22732373614720 run.py:483] Algo bellman_ford step 5157 current loss 0.691554, current_train_items 165056.
I0302 19:00:52.784496 22732373614720 run.py:483] Algo bellman_ford step 5158 current loss 0.914707, current_train_items 165088.
I0302 19:00:52.816934 22732373614720 run.py:483] Algo bellman_ford step 5159 current loss 1.050245, current_train_items 165120.
I0302 19:00:52.836547 22732373614720 run.py:483] Algo bellman_ford step 5160 current loss 0.302068, current_train_items 165152.
I0302 19:00:52.852509 22732373614720 run.py:483] Algo bellman_ford step 5161 current loss 0.507697, current_train_items 165184.
I0302 19:00:52.876352 22732373614720 run.py:483] Algo bellman_ford step 5162 current loss 0.838892, current_train_items 165216.
I0302 19:00:52.908067 22732373614720 run.py:483] Algo bellman_ford step 5163 current loss 0.783359, current_train_items 165248.
I0302 19:00:52.939850 22732373614720 run.py:483] Algo bellman_ford step 5164 current loss 0.810534, current_train_items 165280.
I0302 19:00:52.959057 22732373614720 run.py:483] Algo bellman_ford step 5165 current loss 0.338724, current_train_items 165312.
I0302 19:00:52.974864 22732373614720 run.py:483] Algo bellman_ford step 5166 current loss 0.526091, current_train_items 165344.
I0302 19:00:52.996854 22732373614720 run.py:483] Algo bellman_ford step 5167 current loss 0.761877, current_train_items 165376.
I0302 19:00:53.027728 22732373614720 run.py:483] Algo bellman_ford step 5168 current loss 1.026552, current_train_items 165408.
I0302 19:00:53.061696 22732373614720 run.py:483] Algo bellman_ford step 5169 current loss 0.979038, current_train_items 165440.
I0302 19:00:53.081421 22732373614720 run.py:483] Algo bellman_ford step 5170 current loss 0.362598, current_train_items 165472.
I0302 19:00:53.097547 22732373614720 run.py:483] Algo bellman_ford step 5171 current loss 0.600465, current_train_items 165504.
I0302 19:00:53.120501 22732373614720 run.py:483] Algo bellman_ford step 5172 current loss 0.756922, current_train_items 165536.
I0302 19:00:53.151951 22732373614720 run.py:483] Algo bellman_ford step 5173 current loss 0.880519, current_train_items 165568.
I0302 19:00:53.184016 22732373614720 run.py:483] Algo bellman_ford step 5174 current loss 0.804342, current_train_items 165600.
I0302 19:00:53.203421 22732373614720 run.py:483] Algo bellman_ford step 5175 current loss 0.288007, current_train_items 165632.
I0302 19:00:53.219589 22732373614720 run.py:483] Algo bellman_ford step 5176 current loss 0.457224, current_train_items 165664.
I0302 19:00:53.242662 22732373614720 run.py:483] Algo bellman_ford step 5177 current loss 0.635101, current_train_items 165696.
I0302 19:00:53.273126 22732373614720 run.py:483] Algo bellman_ford step 5178 current loss 0.759145, current_train_items 165728.
I0302 19:00:53.304816 22732373614720 run.py:483] Algo bellman_ford step 5179 current loss 1.003573, current_train_items 165760.
I0302 19:00:53.323950 22732373614720 run.py:483] Algo bellman_ford step 5180 current loss 0.401483, current_train_items 165792.
I0302 19:00:53.339809 22732373614720 run.py:483] Algo bellman_ford step 5181 current loss 0.489240, current_train_items 165824.
I0302 19:00:53.362773 22732373614720 run.py:483] Algo bellman_ford step 5182 current loss 0.783806, current_train_items 165856.
I0302 19:00:53.393087 22732373614720 run.py:483] Algo bellman_ford step 5183 current loss 0.932585, current_train_items 165888.
I0302 19:00:53.424581 22732373614720 run.py:483] Algo bellman_ford step 5184 current loss 0.880183, current_train_items 165920.
I0302 19:00:53.444276 22732373614720 run.py:483] Algo bellman_ford step 5185 current loss 0.407203, current_train_items 165952.
I0302 19:00:53.460640 22732373614720 run.py:483] Algo bellman_ford step 5186 current loss 0.576682, current_train_items 165984.
I0302 19:00:53.482483 22732373614720 run.py:483] Algo bellman_ford step 5187 current loss 0.651725, current_train_items 166016.
I0302 19:00:53.513544 22732373614720 run.py:483] Algo bellman_ford step 5188 current loss 0.776014, current_train_items 166048.
I0302 19:00:53.545381 22732373614720 run.py:483] Algo bellman_ford step 5189 current loss 0.896946, current_train_items 166080.
I0302 19:00:53.564983 22732373614720 run.py:483] Algo bellman_ford step 5190 current loss 0.387803, current_train_items 166112.
I0302 19:00:53.580960 22732373614720 run.py:483] Algo bellman_ford step 5191 current loss 0.528550, current_train_items 166144.
I0302 19:00:53.604324 22732373614720 run.py:483] Algo bellman_ford step 5192 current loss 0.642551, current_train_items 166176.
I0302 19:00:53.633678 22732373614720 run.py:483] Algo bellman_ford step 5193 current loss 0.718392, current_train_items 166208.
I0302 19:00:53.666524 22732373614720 run.py:483] Algo bellman_ford step 5194 current loss 0.842136, current_train_items 166240.
I0302 19:00:53.685819 22732373614720 run.py:483] Algo bellman_ford step 5195 current loss 0.267191, current_train_items 166272.
I0302 19:00:53.701890 22732373614720 run.py:483] Algo bellman_ford step 5196 current loss 0.567314, current_train_items 166304.
I0302 19:00:53.725338 22732373614720 run.py:483] Algo bellman_ford step 5197 current loss 0.794554, current_train_items 166336.
I0302 19:00:53.756737 22732373614720 run.py:483] Algo bellman_ford step 5198 current loss 0.813340, current_train_items 166368.
I0302 19:00:53.788994 22732373614720 run.py:483] Algo bellman_ford step 5199 current loss 0.814179, current_train_items 166400.
I0302 19:00:53.808681 22732373614720 run.py:483] Algo bellman_ford step 5200 current loss 0.340834, current_train_items 166432.
I0302 19:00:53.816679 22732373614720 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0302 19:00:53.816790 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:00:53.833619 22732373614720 run.py:483] Algo bellman_ford step 5201 current loss 0.521255, current_train_items 166464.
I0302 19:00:53.857852 22732373614720 run.py:483] Algo bellman_ford step 5202 current loss 0.737793, current_train_items 166496.
I0302 19:00:53.889748 22732373614720 run.py:483] Algo bellman_ford step 5203 current loss 0.788944, current_train_items 166528.
I0302 19:00:53.924050 22732373614720 run.py:483] Algo bellman_ford step 5204 current loss 0.733828, current_train_items 166560.
I0302 19:00:53.943783 22732373614720 run.py:483] Algo bellman_ford step 5205 current loss 0.312530, current_train_items 166592.
I0302 19:00:53.959274 22732373614720 run.py:483] Algo bellman_ford step 5206 current loss 0.584263, current_train_items 166624.
I0302 19:00:53.982488 22732373614720 run.py:483] Algo bellman_ford step 5207 current loss 0.694122, current_train_items 166656.
I0302 19:00:54.011821 22732373614720 run.py:483] Algo bellman_ford step 5208 current loss 0.654169, current_train_items 166688.
I0302 19:00:54.046925 22732373614720 run.py:483] Algo bellman_ford step 5209 current loss 0.864734, current_train_items 166720.
I0302 19:00:54.066339 22732373614720 run.py:483] Algo bellman_ford step 5210 current loss 0.393216, current_train_items 166752.
I0302 19:00:54.081833 22732373614720 run.py:483] Algo bellman_ford step 5211 current loss 0.506787, current_train_items 166784.
I0302 19:00:54.105187 22732373614720 run.py:483] Algo bellman_ford step 5212 current loss 0.655223, current_train_items 166816.
I0302 19:00:54.137177 22732373614720 run.py:483] Algo bellman_ford step 5213 current loss 0.842517, current_train_items 166848.
I0302 19:00:54.169430 22732373614720 run.py:483] Algo bellman_ford step 5214 current loss 0.733746, current_train_items 166880.
I0302 19:00:54.188542 22732373614720 run.py:483] Algo bellman_ford step 5215 current loss 0.302140, current_train_items 166912.
I0302 19:00:54.204680 22732373614720 run.py:483] Algo bellman_ford step 5216 current loss 0.504317, current_train_items 166944.
I0302 19:00:54.227651 22732373614720 run.py:483] Algo bellman_ford step 5217 current loss 0.722947, current_train_items 166976.
I0302 19:00:54.257334 22732373614720 run.py:483] Algo bellman_ford step 5218 current loss 0.792299, current_train_items 167008.
I0302 19:00:54.290055 22732373614720 run.py:483] Algo bellman_ford step 5219 current loss 0.827886, current_train_items 167040.
I0302 19:00:54.309500 22732373614720 run.py:483] Algo bellman_ford step 5220 current loss 0.356145, current_train_items 167072.
I0302 19:00:54.325351 22732373614720 run.py:483] Algo bellman_ford step 5221 current loss 0.494548, current_train_items 167104.
I0302 19:00:54.347301 22732373614720 run.py:483] Algo bellman_ford step 5222 current loss 0.675214, current_train_items 167136.
I0302 19:00:54.378441 22732373614720 run.py:483] Algo bellman_ford step 5223 current loss 0.848664, current_train_items 167168.
I0302 19:00:54.410645 22732373614720 run.py:483] Algo bellman_ford step 5224 current loss 0.861270, current_train_items 167200.
I0302 19:00:54.430069 22732373614720 run.py:483] Algo bellman_ford step 5225 current loss 0.401226, current_train_items 167232.
I0302 19:00:54.445846 22732373614720 run.py:483] Algo bellman_ford step 5226 current loss 0.489260, current_train_items 167264.
I0302 19:00:54.469373 22732373614720 run.py:483] Algo bellman_ford step 5227 current loss 0.696031, current_train_items 167296.
I0302 19:00:54.500797 22732373614720 run.py:483] Algo bellman_ford step 5228 current loss 0.769427, current_train_items 167328.
I0302 19:00:54.534744 22732373614720 run.py:483] Algo bellman_ford step 5229 current loss 0.954059, current_train_items 167360.
I0302 19:00:54.554033 22732373614720 run.py:483] Algo bellman_ford step 5230 current loss 0.325360, current_train_items 167392.
I0302 19:00:54.570053 22732373614720 run.py:483] Algo bellman_ford step 5231 current loss 0.539775, current_train_items 167424.
I0302 19:00:54.593403 22732373614720 run.py:483] Algo bellman_ford step 5232 current loss 0.654563, current_train_items 167456.
I0302 19:00:54.623297 22732373614720 run.py:483] Algo bellman_ford step 5233 current loss 0.722568, current_train_items 167488.
I0302 19:00:54.657448 22732373614720 run.py:483] Algo bellman_ford step 5234 current loss 0.859655, current_train_items 167520.
I0302 19:00:54.676614 22732373614720 run.py:483] Algo bellman_ford step 5235 current loss 0.332539, current_train_items 167552.
I0302 19:00:54.692404 22732373614720 run.py:483] Algo bellman_ford step 5236 current loss 0.502445, current_train_items 167584.
I0302 19:00:54.715595 22732373614720 run.py:483] Algo bellman_ford step 5237 current loss 0.592716, current_train_items 167616.
I0302 19:00:54.747399 22732373614720 run.py:483] Algo bellman_ford step 5238 current loss 0.788607, current_train_items 167648.
I0302 19:00:54.781054 22732373614720 run.py:483] Algo bellman_ford step 5239 current loss 0.893242, current_train_items 167680.
I0302 19:00:54.800517 22732373614720 run.py:483] Algo bellman_ford step 5240 current loss 0.376873, current_train_items 167712.
I0302 19:00:54.816488 22732373614720 run.py:483] Algo bellman_ford step 5241 current loss 0.514174, current_train_items 167744.
I0302 19:00:54.839773 22732373614720 run.py:483] Algo bellman_ford step 5242 current loss 0.771119, current_train_items 167776.
I0302 19:00:54.869900 22732373614720 run.py:483] Algo bellman_ford step 5243 current loss 0.721390, current_train_items 167808.
I0302 19:00:54.900297 22732373614720 run.py:483] Algo bellman_ford step 5244 current loss 0.709120, current_train_items 167840.
I0302 19:00:54.919341 22732373614720 run.py:483] Algo bellman_ford step 5245 current loss 0.382098, current_train_items 167872.
I0302 19:00:54.935291 22732373614720 run.py:483] Algo bellman_ford step 5246 current loss 0.483782, current_train_items 167904.
I0302 19:00:54.959060 22732373614720 run.py:483] Algo bellman_ford step 5247 current loss 0.621301, current_train_items 167936.
I0302 19:00:54.989862 22732373614720 run.py:483] Algo bellman_ford step 5248 current loss 0.698838, current_train_items 167968.
I0302 19:00:55.023293 22732373614720 run.py:483] Algo bellman_ford step 5249 current loss 0.818202, current_train_items 168000.
I0302 19:00:55.042766 22732373614720 run.py:483] Algo bellman_ford step 5250 current loss 0.295761, current_train_items 168032.
I0302 19:00:55.051017 22732373614720 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0302 19:00:55.051128 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:00:55.067789 22732373614720 run.py:483] Algo bellman_ford step 5251 current loss 0.469264, current_train_items 168064.
I0302 19:00:55.091963 22732373614720 run.py:483] Algo bellman_ford step 5252 current loss 0.757442, current_train_items 168096.
I0302 19:00:55.123393 22732373614720 run.py:483] Algo bellman_ford step 5253 current loss 0.705738, current_train_items 168128.
I0302 19:00:55.154392 22732373614720 run.py:483] Algo bellman_ford step 5254 current loss 0.670471, current_train_items 168160.
I0302 19:00:55.174496 22732373614720 run.py:483] Algo bellman_ford step 5255 current loss 0.419388, current_train_items 168192.
I0302 19:00:55.190101 22732373614720 run.py:483] Algo bellman_ford step 5256 current loss 0.436422, current_train_items 168224.
I0302 19:00:55.212427 22732373614720 run.py:483] Algo bellman_ford step 5257 current loss 0.667002, current_train_items 168256.
I0302 19:00:55.243520 22732373614720 run.py:483] Algo bellman_ford step 5258 current loss 0.847639, current_train_items 168288.
I0302 19:00:55.274416 22732373614720 run.py:483] Algo bellman_ford step 5259 current loss 0.714189, current_train_items 168320.
I0302 19:00:55.294145 22732373614720 run.py:483] Algo bellman_ford step 5260 current loss 0.328114, current_train_items 168352.
I0302 19:00:55.310276 22732373614720 run.py:483] Algo bellman_ford step 5261 current loss 0.486646, current_train_items 168384.
I0302 19:00:55.332949 22732373614720 run.py:483] Algo bellman_ford step 5262 current loss 0.701145, current_train_items 168416.
I0302 19:00:55.364151 22732373614720 run.py:483] Algo bellman_ford step 5263 current loss 0.800040, current_train_items 168448.
I0302 19:00:55.396316 22732373614720 run.py:483] Algo bellman_ford step 5264 current loss 0.720506, current_train_items 168480.
I0302 19:00:55.415821 22732373614720 run.py:483] Algo bellman_ford step 5265 current loss 0.239621, current_train_items 168512.
I0302 19:00:55.431825 22732373614720 run.py:483] Algo bellman_ford step 5266 current loss 0.576079, current_train_items 168544.
I0302 19:00:55.455724 22732373614720 run.py:483] Algo bellman_ford step 5267 current loss 0.780884, current_train_items 168576.
I0302 19:00:55.485666 22732373614720 run.py:483] Algo bellman_ford step 5268 current loss 0.738267, current_train_items 168608.
I0302 19:00:55.517362 22732373614720 run.py:483] Algo bellman_ford step 5269 current loss 0.820966, current_train_items 168640.
I0302 19:00:55.536995 22732373614720 run.py:483] Algo bellman_ford step 5270 current loss 0.360159, current_train_items 168672.
I0302 19:00:55.552885 22732373614720 run.py:483] Algo bellman_ford step 5271 current loss 0.531348, current_train_items 168704.
I0302 19:00:55.575291 22732373614720 run.py:483] Algo bellman_ford step 5272 current loss 0.697026, current_train_items 168736.
I0302 19:00:55.605368 22732373614720 run.py:483] Algo bellman_ford step 5273 current loss 0.746407, current_train_items 168768.
I0302 19:00:55.637202 22732373614720 run.py:483] Algo bellman_ford step 5274 current loss 0.746642, current_train_items 168800.
I0302 19:00:55.656722 22732373614720 run.py:483] Algo bellman_ford step 5275 current loss 0.385829, current_train_items 168832.
I0302 19:00:55.672718 22732373614720 run.py:483] Algo bellman_ford step 5276 current loss 0.599295, current_train_items 168864.
I0302 19:00:55.695012 22732373614720 run.py:483] Algo bellman_ford step 5277 current loss 0.764105, current_train_items 168896.
I0302 19:00:55.725490 22732373614720 run.py:483] Algo bellman_ford step 5278 current loss 0.746746, current_train_items 168928.
I0302 19:00:55.758904 22732373614720 run.py:483] Algo bellman_ford step 5279 current loss 0.898691, current_train_items 168960.
I0302 19:00:55.778202 22732373614720 run.py:483] Algo bellman_ford step 5280 current loss 0.352176, current_train_items 168992.
I0302 19:00:55.794336 22732373614720 run.py:483] Algo bellman_ford step 5281 current loss 0.622639, current_train_items 169024.
I0302 19:00:55.818445 22732373614720 run.py:483] Algo bellman_ford step 5282 current loss 0.751620, current_train_items 169056.
I0302 19:00:55.849476 22732373614720 run.py:483] Algo bellman_ford step 5283 current loss 0.796480, current_train_items 169088.
I0302 19:00:55.881765 22732373614720 run.py:483] Algo bellman_ford step 5284 current loss 1.147049, current_train_items 169120.
I0302 19:00:55.901394 22732373614720 run.py:483] Algo bellman_ford step 5285 current loss 0.324173, current_train_items 169152.
I0302 19:00:55.917594 22732373614720 run.py:483] Algo bellman_ford step 5286 current loss 0.498014, current_train_items 169184.
I0302 19:00:55.941516 22732373614720 run.py:483] Algo bellman_ford step 5287 current loss 0.673126, current_train_items 169216.
I0302 19:00:55.971846 22732373614720 run.py:483] Algo bellman_ford step 5288 current loss 0.709125, current_train_items 169248.
I0302 19:00:56.005577 22732373614720 run.py:483] Algo bellman_ford step 5289 current loss 0.901074, current_train_items 169280.
I0302 19:00:56.025398 22732373614720 run.py:483] Algo bellman_ford step 5290 current loss 0.369103, current_train_items 169312.
I0302 19:00:56.041885 22732373614720 run.py:483] Algo bellman_ford step 5291 current loss 0.488503, current_train_items 169344.
I0302 19:00:56.064626 22732373614720 run.py:483] Algo bellman_ford step 5292 current loss 0.619064, current_train_items 169376.
I0302 19:00:56.095257 22732373614720 run.py:483] Algo bellman_ford step 5293 current loss 0.706827, current_train_items 169408.
I0302 19:00:56.127197 22732373614720 run.py:483] Algo bellman_ford step 5294 current loss 0.783943, current_train_items 169440.
I0302 19:00:56.146381 22732373614720 run.py:483] Algo bellman_ford step 5295 current loss 0.284789, current_train_items 169472.
I0302 19:00:56.162081 22732373614720 run.py:483] Algo bellman_ford step 5296 current loss 0.495470, current_train_items 169504.
I0302 19:00:56.184528 22732373614720 run.py:483] Algo bellman_ford step 5297 current loss 0.595110, current_train_items 169536.
I0302 19:00:56.215545 22732373614720 run.py:483] Algo bellman_ford step 5298 current loss 0.703307, current_train_items 169568.
I0302 19:00:56.250534 22732373614720 run.py:483] Algo bellman_ford step 5299 current loss 0.896826, current_train_items 169600.
I0302 19:00:56.270116 22732373614720 run.py:483] Algo bellman_ford step 5300 current loss 0.380867, current_train_items 169632.
I0302 19:00:56.278009 22732373614720 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.9091796875, 'score': 0.9091796875, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0302 19:00:56.278119 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.909, val scores are: bellman_ford: 0.909
I0302 19:00:56.294515 22732373614720 run.py:483] Algo bellman_ford step 5301 current loss 0.504517, current_train_items 169664.
I0302 19:00:56.319036 22732373614720 run.py:483] Algo bellman_ford step 5302 current loss 0.722869, current_train_items 169696.
I0302 19:00:56.351181 22732373614720 run.py:483] Algo bellman_ford step 5303 current loss 0.706702, current_train_items 169728.
I0302 19:00:56.385348 22732373614720 run.py:483] Algo bellman_ford step 5304 current loss 0.864226, current_train_items 169760.
I0302 19:00:56.405476 22732373614720 run.py:483] Algo bellman_ford step 5305 current loss 0.328489, current_train_items 169792.
I0302 19:00:56.421442 22732373614720 run.py:483] Algo bellman_ford step 5306 current loss 0.560477, current_train_items 169824.
I0302 19:00:56.445869 22732373614720 run.py:483] Algo bellman_ford step 5307 current loss 0.752497, current_train_items 169856.
I0302 19:00:56.477487 22732373614720 run.py:483] Algo bellman_ford step 5308 current loss 0.818371, current_train_items 169888.
I0302 19:00:56.509541 22732373614720 run.py:483] Algo bellman_ford step 5309 current loss 0.772511, current_train_items 169920.
I0302 19:00:56.529225 22732373614720 run.py:483] Algo bellman_ford step 5310 current loss 0.320683, current_train_items 169952.
I0302 19:00:56.544767 22732373614720 run.py:483] Algo bellman_ford step 5311 current loss 0.401668, current_train_items 169984.
I0302 19:00:56.568052 22732373614720 run.py:483] Algo bellman_ford step 5312 current loss 0.584126, current_train_items 170016.
I0302 19:00:56.598578 22732373614720 run.py:483] Algo bellman_ford step 5313 current loss 0.808355, current_train_items 170048.
I0302 19:00:56.633069 22732373614720 run.py:483] Algo bellman_ford step 5314 current loss 0.864565, current_train_items 170080.
I0302 19:00:56.652702 22732373614720 run.py:483] Algo bellman_ford step 5315 current loss 0.376835, current_train_items 170112.
I0302 19:00:56.668792 22732373614720 run.py:483] Algo bellman_ford step 5316 current loss 0.513254, current_train_items 170144.
I0302 19:00:56.692252 22732373614720 run.py:483] Algo bellman_ford step 5317 current loss 0.634769, current_train_items 170176.
I0302 19:00:56.721344 22732373614720 run.py:483] Algo bellman_ford step 5318 current loss 0.598633, current_train_items 170208.
I0302 19:00:56.755086 22732373614720 run.py:483] Algo bellman_ford step 5319 current loss 0.855139, current_train_items 170240.
I0302 19:00:56.774542 22732373614720 run.py:483] Algo bellman_ford step 5320 current loss 0.348463, current_train_items 170272.
I0302 19:00:56.790513 22732373614720 run.py:483] Algo bellman_ford step 5321 current loss 0.473675, current_train_items 170304.
I0302 19:00:56.814499 22732373614720 run.py:483] Algo bellman_ford step 5322 current loss 0.654960, current_train_items 170336.
I0302 19:00:56.843824 22732373614720 run.py:483] Algo bellman_ford step 5323 current loss 0.638803, current_train_items 170368.
I0302 19:00:56.877745 22732373614720 run.py:483] Algo bellman_ford step 5324 current loss 0.860802, current_train_items 170400.
I0302 19:00:56.897195 22732373614720 run.py:483] Algo bellman_ford step 5325 current loss 0.362001, current_train_items 170432.
I0302 19:00:56.913130 22732373614720 run.py:483] Algo bellman_ford step 5326 current loss 0.579237, current_train_items 170464.
I0302 19:00:56.935756 22732373614720 run.py:483] Algo bellman_ford step 5327 current loss 0.609461, current_train_items 170496.
I0302 19:00:56.967408 22732373614720 run.py:483] Algo bellman_ford step 5328 current loss 0.809983, current_train_items 170528.
I0302 19:00:56.999943 22732373614720 run.py:483] Algo bellman_ford step 5329 current loss 0.911931, current_train_items 170560.
I0302 19:00:57.019801 22732373614720 run.py:483] Algo bellman_ford step 5330 current loss 0.395163, current_train_items 170592.
I0302 19:00:57.035851 22732373614720 run.py:483] Algo bellman_ford step 5331 current loss 0.481235, current_train_items 170624.
I0302 19:00:57.058877 22732373614720 run.py:483] Algo bellman_ford step 5332 current loss 0.634364, current_train_items 170656.
I0302 19:00:57.088125 22732373614720 run.py:483] Algo bellman_ford step 5333 current loss 0.641554, current_train_items 170688.
I0302 19:00:57.120233 22732373614720 run.py:483] Algo bellman_ford step 5334 current loss 0.799737, current_train_items 170720.
I0302 19:00:57.140213 22732373614720 run.py:483] Algo bellman_ford step 5335 current loss 0.396672, current_train_items 170752.
I0302 19:00:57.156487 22732373614720 run.py:483] Algo bellman_ford step 5336 current loss 0.553275, current_train_items 170784.
I0302 19:00:57.179632 22732373614720 run.py:483] Algo bellman_ford step 5337 current loss 0.770379, current_train_items 170816.
I0302 19:00:57.209461 22732373614720 run.py:483] Algo bellman_ford step 5338 current loss 0.762072, current_train_items 170848.
I0302 19:00:57.243015 22732373614720 run.py:483] Algo bellman_ford step 5339 current loss 0.812965, current_train_items 170880.
I0302 19:00:57.262394 22732373614720 run.py:483] Algo bellman_ford step 5340 current loss 0.292769, current_train_items 170912.
I0302 19:00:57.278532 22732373614720 run.py:483] Algo bellman_ford step 5341 current loss 0.570290, current_train_items 170944.
I0302 19:00:57.303750 22732373614720 run.py:483] Algo bellman_ford step 5342 current loss 0.801406, current_train_items 170976.
I0302 19:00:57.334972 22732373614720 run.py:483] Algo bellman_ford step 5343 current loss 0.723930, current_train_items 171008.
I0302 19:00:57.367889 22732373614720 run.py:483] Algo bellman_ford step 5344 current loss 0.844352, current_train_items 171040.
I0302 19:00:57.387444 22732373614720 run.py:483] Algo bellman_ford step 5345 current loss 0.252042, current_train_items 171072.
I0302 19:00:57.402842 22732373614720 run.py:483] Algo bellman_ford step 5346 current loss 0.530914, current_train_items 171104.
I0302 19:00:57.426183 22732373614720 run.py:483] Algo bellman_ford step 5347 current loss 0.661941, current_train_items 171136.
I0302 19:00:57.458008 22732373614720 run.py:483] Algo bellman_ford step 5348 current loss 0.898171, current_train_items 171168.
I0302 19:00:57.491064 22732373614720 run.py:483] Algo bellman_ford step 5349 current loss 0.937376, current_train_items 171200.
I0302 19:00:57.510892 22732373614720 run.py:483] Algo bellman_ford step 5350 current loss 0.333443, current_train_items 171232.
I0302 19:00:57.519129 22732373614720 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.9091796875, 'score': 0.9091796875, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0302 19:00:57.519247 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.909, val scores are: bellman_ford: 0.909
I0302 19:00:57.535508 22732373614720 run.py:483] Algo bellman_ford step 5351 current loss 0.599528, current_train_items 171264.
I0302 19:00:57.559448 22732373614720 run.py:483] Algo bellman_ford step 5352 current loss 0.868807, current_train_items 171296.
I0302 19:00:57.589387 22732373614720 run.py:483] Algo bellman_ford step 5353 current loss 0.925727, current_train_items 171328.
I0302 19:00:57.622527 22732373614720 run.py:483] Algo bellman_ford step 5354 current loss 0.836899, current_train_items 171360.
I0302 19:00:57.642248 22732373614720 run.py:483] Algo bellman_ford step 5355 current loss 0.344872, current_train_items 171392.
I0302 19:00:57.658141 22732373614720 run.py:483] Algo bellman_ford step 5356 current loss 0.510784, current_train_items 171424.
I0302 19:00:57.681134 22732373614720 run.py:483] Algo bellman_ford step 5357 current loss 0.829572, current_train_items 171456.
I0302 19:00:57.710646 22732373614720 run.py:483] Algo bellman_ford step 5358 current loss 0.675853, current_train_items 171488.
I0302 19:00:57.743446 22732373614720 run.py:483] Algo bellman_ford step 5359 current loss 0.914720, current_train_items 171520.
I0302 19:00:57.763280 22732373614720 run.py:483] Algo bellman_ford step 5360 current loss 0.410825, current_train_items 171552.
I0302 19:00:57.779318 22732373614720 run.py:483] Algo bellman_ford step 5361 current loss 0.520141, current_train_items 171584.
I0302 19:00:57.802571 22732373614720 run.py:483] Algo bellman_ford step 5362 current loss 0.804776, current_train_items 171616.
I0302 19:00:57.833194 22732373614720 run.py:483] Algo bellman_ford step 5363 current loss 0.993955, current_train_items 171648.
I0302 19:00:57.865885 22732373614720 run.py:483] Algo bellman_ford step 5364 current loss 1.153741, current_train_items 171680.
I0302 19:00:57.885538 22732373614720 run.py:483] Algo bellman_ford step 5365 current loss 0.352842, current_train_items 171712.
I0302 19:00:57.901583 22732373614720 run.py:483] Algo bellman_ford step 5366 current loss 0.525551, current_train_items 171744.
I0302 19:00:57.924557 22732373614720 run.py:483] Algo bellman_ford step 5367 current loss 0.730062, current_train_items 171776.
I0302 19:00:57.954826 22732373614720 run.py:483] Algo bellman_ford step 5368 current loss 0.757936, current_train_items 171808.
I0302 19:00:57.986717 22732373614720 run.py:483] Algo bellman_ford step 5369 current loss 0.858357, current_train_items 171840.
I0302 19:00:58.006529 22732373614720 run.py:483] Algo bellman_ford step 5370 current loss 0.347075, current_train_items 171872.
I0302 19:00:58.022528 22732373614720 run.py:483] Algo bellman_ford step 5371 current loss 0.505701, current_train_items 171904.
I0302 19:00:58.045129 22732373614720 run.py:483] Algo bellman_ford step 5372 current loss 0.646101, current_train_items 171936.
I0302 19:00:58.075067 22732373614720 run.py:483] Algo bellman_ford step 5373 current loss 0.675468, current_train_items 171968.
I0302 19:00:58.105442 22732373614720 run.py:483] Algo bellman_ford step 5374 current loss 0.738519, current_train_items 172000.
I0302 19:00:58.125261 22732373614720 run.py:483] Algo bellman_ford step 5375 current loss 0.307648, current_train_items 172032.
I0302 19:00:58.141406 22732373614720 run.py:483] Algo bellman_ford step 5376 current loss 0.534783, current_train_items 172064.
I0302 19:00:58.163969 22732373614720 run.py:483] Algo bellman_ford step 5377 current loss 0.744129, current_train_items 172096.
I0302 19:00:58.193279 22732373614720 run.py:483] Algo bellman_ford step 5378 current loss 0.673176, current_train_items 172128.
I0302 19:00:58.223063 22732373614720 run.py:483] Algo bellman_ford step 5379 current loss 0.757362, current_train_items 172160.
I0302 19:00:58.242325 22732373614720 run.py:483] Algo bellman_ford step 5380 current loss 0.436042, current_train_items 172192.
I0302 19:00:58.258560 22732373614720 run.py:483] Algo bellman_ford step 5381 current loss 0.569025, current_train_items 172224.
I0302 19:00:58.282362 22732373614720 run.py:483] Algo bellman_ford step 5382 current loss 0.796484, current_train_items 172256.
I0302 19:00:58.312781 22732373614720 run.py:483] Algo bellman_ford step 5383 current loss 0.792779, current_train_items 172288.
I0302 19:00:58.344330 22732373614720 run.py:483] Algo bellman_ford step 5384 current loss 0.722936, current_train_items 172320.
I0302 19:00:58.364167 22732373614720 run.py:483] Algo bellman_ford step 5385 current loss 0.291750, current_train_items 172352.
I0302 19:00:58.379750 22732373614720 run.py:483] Algo bellman_ford step 5386 current loss 0.558179, current_train_items 172384.
I0302 19:00:58.402125 22732373614720 run.py:483] Algo bellman_ford step 5387 current loss 0.582942, current_train_items 172416.
I0302 19:00:58.431757 22732373614720 run.py:483] Algo bellman_ford step 5388 current loss 0.694384, current_train_items 172448.
I0302 19:00:58.464755 22732373614720 run.py:483] Algo bellman_ford step 5389 current loss 0.863499, current_train_items 172480.
I0302 19:00:58.484500 22732373614720 run.py:483] Algo bellman_ford step 5390 current loss 0.346807, current_train_items 172512.
I0302 19:00:58.500897 22732373614720 run.py:483] Algo bellman_ford step 5391 current loss 0.544506, current_train_items 172544.
I0302 19:00:58.523045 22732373614720 run.py:483] Algo bellman_ford step 5392 current loss 0.602470, current_train_items 172576.
I0302 19:00:58.554871 22732373614720 run.py:483] Algo bellman_ford step 5393 current loss 0.776336, current_train_items 172608.
I0302 19:00:58.588521 22732373614720 run.py:483] Algo bellman_ford step 5394 current loss 0.940604, current_train_items 172640.
I0302 19:00:58.607970 22732373614720 run.py:483] Algo bellman_ford step 5395 current loss 0.383082, current_train_items 172672.
I0302 19:00:58.624421 22732373614720 run.py:483] Algo bellman_ford step 5396 current loss 0.577158, current_train_items 172704.
I0302 19:00:58.647660 22732373614720 run.py:483] Algo bellman_ford step 5397 current loss 0.648173, current_train_items 172736.
I0302 19:00:58.678102 22732373614720 run.py:483] Algo bellman_ford step 5398 current loss 0.681364, current_train_items 172768.
I0302 19:00:58.711702 22732373614720 run.py:483] Algo bellman_ford step 5399 current loss 0.897740, current_train_items 172800.
I0302 19:00:58.731299 22732373614720 run.py:483] Algo bellman_ford step 5400 current loss 0.307608, current_train_items 172832.
I0302 19:00:58.739220 22732373614720 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0302 19:00:58.739326 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:00:58.756404 22732373614720 run.py:483] Algo bellman_ford step 5401 current loss 0.578952, current_train_items 172864.
I0302 19:00:58.780794 22732373614720 run.py:483] Algo bellman_ford step 5402 current loss 0.672468, current_train_items 172896.
I0302 19:00:58.813227 22732373614720 run.py:483] Algo bellman_ford step 5403 current loss 0.837631, current_train_items 172928.
I0302 19:00:58.847312 22732373614720 run.py:483] Algo bellman_ford step 5404 current loss 0.886895, current_train_items 172960.
I0302 19:00:58.867415 22732373614720 run.py:483] Algo bellman_ford step 5405 current loss 0.260781, current_train_items 172992.
I0302 19:00:58.882859 22732373614720 run.py:483] Algo bellman_ford step 5406 current loss 0.450060, current_train_items 173024.
I0302 19:00:58.905867 22732373614720 run.py:483] Algo bellman_ford step 5407 current loss 0.585908, current_train_items 173056.
I0302 19:00:58.934876 22732373614720 run.py:483] Algo bellman_ford step 5408 current loss 0.675379, current_train_items 173088.
I0302 19:00:58.967080 22732373614720 run.py:483] Algo bellman_ford step 5409 current loss 0.739962, current_train_items 173120.
I0302 19:00:58.986649 22732373614720 run.py:483] Algo bellman_ford step 5410 current loss 0.308602, current_train_items 173152.
I0302 19:00:59.003113 22732373614720 run.py:483] Algo bellman_ford step 5411 current loss 0.555211, current_train_items 173184.
I0302 19:00:59.027044 22732373614720 run.py:483] Algo bellman_ford step 5412 current loss 0.706525, current_train_items 173216.
I0302 19:00:59.058199 22732373614720 run.py:483] Algo bellman_ford step 5413 current loss 0.672925, current_train_items 173248.
I0302 19:00:59.091781 22732373614720 run.py:483] Algo bellman_ford step 5414 current loss 0.827965, current_train_items 173280.
I0302 19:00:59.111482 22732373614720 run.py:483] Algo bellman_ford step 5415 current loss 0.264784, current_train_items 173312.
I0302 19:00:59.127750 22732373614720 run.py:483] Algo bellman_ford step 5416 current loss 0.556215, current_train_items 173344.
I0302 19:00:59.151453 22732373614720 run.py:483] Algo bellman_ford step 5417 current loss 0.644049, current_train_items 173376.
I0302 19:00:59.183380 22732373614720 run.py:483] Algo bellman_ford step 5418 current loss 0.692109, current_train_items 173408.
I0302 19:00:59.218249 22732373614720 run.py:483] Algo bellman_ford step 5419 current loss 1.003476, current_train_items 173440.
I0302 19:00:59.238061 22732373614720 run.py:483] Algo bellman_ford step 5420 current loss 0.371395, current_train_items 173472.
I0302 19:00:59.254190 22732373614720 run.py:483] Algo bellman_ford step 5421 current loss 0.509012, current_train_items 173504.
I0302 19:00:59.277036 22732373614720 run.py:483] Algo bellman_ford step 5422 current loss 0.573752, current_train_items 173536.
I0302 19:00:59.306735 22732373614720 run.py:483] Algo bellman_ford step 5423 current loss 0.687604, current_train_items 173568.
I0302 19:00:59.337676 22732373614720 run.py:483] Algo bellman_ford step 5424 current loss 0.790854, current_train_items 173600.
I0302 19:00:59.357604 22732373614720 run.py:483] Algo bellman_ford step 5425 current loss 0.314191, current_train_items 173632.
I0302 19:00:59.373299 22732373614720 run.py:483] Algo bellman_ford step 5426 current loss 0.482279, current_train_items 173664.
I0302 19:00:59.396856 22732373614720 run.py:483] Algo bellman_ford step 5427 current loss 0.825534, current_train_items 173696.
I0302 19:00:59.427527 22732373614720 run.py:483] Algo bellman_ford step 5428 current loss 0.792283, current_train_items 173728.
I0302 19:00:59.461539 22732373614720 run.py:483] Algo bellman_ford step 5429 current loss 0.882147, current_train_items 173760.
I0302 19:00:59.480821 22732373614720 run.py:483] Algo bellman_ford step 5430 current loss 0.287801, current_train_items 173792.
I0302 19:00:59.497215 22732373614720 run.py:483] Algo bellman_ford step 5431 current loss 0.522070, current_train_items 173824.
I0302 19:00:59.520728 22732373614720 run.py:483] Algo bellman_ford step 5432 current loss 0.680738, current_train_items 173856.
I0302 19:00:59.553374 22732373614720 run.py:483] Algo bellman_ford step 5433 current loss 0.931167, current_train_items 173888.
I0302 19:00:59.588137 22732373614720 run.py:483] Algo bellman_ford step 5434 current loss 0.983030, current_train_items 173920.
I0302 19:00:59.607744 22732373614720 run.py:483] Algo bellman_ford step 5435 current loss 0.308234, current_train_items 173952.
I0302 19:00:59.623846 22732373614720 run.py:483] Algo bellman_ford step 5436 current loss 0.515017, current_train_items 173984.
I0302 19:00:59.647139 22732373614720 run.py:483] Algo bellman_ford step 5437 current loss 0.769539, current_train_items 174016.
I0302 19:00:59.676693 22732373614720 run.py:483] Algo bellman_ford step 5438 current loss 0.953084, current_train_items 174048.
I0302 19:00:59.713432 22732373614720 run.py:483] Algo bellman_ford step 5439 current loss 1.111606, current_train_items 174080.
I0302 19:00:59.733046 22732373614720 run.py:483] Algo bellman_ford step 5440 current loss 0.382993, current_train_items 174112.
I0302 19:00:59.748874 22732373614720 run.py:483] Algo bellman_ford step 5441 current loss 0.444890, current_train_items 174144.
I0302 19:00:59.771576 22732373614720 run.py:483] Algo bellman_ford step 5442 current loss 0.597805, current_train_items 174176.
I0302 19:00:59.801720 22732373614720 run.py:483] Algo bellman_ford step 5443 current loss 0.742979, current_train_items 174208.
I0302 19:00:59.835207 22732373614720 run.py:483] Algo bellman_ford step 5444 current loss 0.798796, current_train_items 174240.
I0302 19:00:59.854649 22732373614720 run.py:483] Algo bellman_ford step 5445 current loss 0.270623, current_train_items 174272.
I0302 19:00:59.870645 22732373614720 run.py:483] Algo bellman_ford step 5446 current loss 0.517417, current_train_items 174304.
I0302 19:00:59.894557 22732373614720 run.py:483] Algo bellman_ford step 5447 current loss 0.674361, current_train_items 174336.
I0302 19:00:59.925915 22732373614720 run.py:483] Algo bellman_ford step 5448 current loss 0.833801, current_train_items 174368.
I0302 19:00:59.958997 22732373614720 run.py:483] Algo bellman_ford step 5449 current loss 0.805814, current_train_items 174400.
I0302 19:00:59.978463 22732373614720 run.py:483] Algo bellman_ford step 5450 current loss 0.310892, current_train_items 174432.
I0302 19:00:59.986411 22732373614720 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0302 19:00:59.986523 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:01:00.002911 22732373614720 run.py:483] Algo bellman_ford step 5451 current loss 0.496517, current_train_items 174464.
I0302 19:01:00.026518 22732373614720 run.py:483] Algo bellman_ford step 5452 current loss 0.667791, current_train_items 174496.
I0302 19:01:00.056879 22732373614720 run.py:483] Algo bellman_ford step 5453 current loss 0.663724, current_train_items 174528.
I0302 19:01:00.089779 22732373614720 run.py:483] Algo bellman_ford step 5454 current loss 0.897557, current_train_items 174560.
I0302 19:01:00.109645 22732373614720 run.py:483] Algo bellman_ford step 5455 current loss 0.308361, current_train_items 174592.
I0302 19:01:00.125507 22732373614720 run.py:483] Algo bellman_ford step 5456 current loss 0.486307, current_train_items 174624.
I0302 19:01:00.148955 22732373614720 run.py:483] Algo bellman_ford step 5457 current loss 0.710314, current_train_items 174656.
I0302 19:01:00.179710 22732373614720 run.py:483] Algo bellman_ford step 5458 current loss 0.749772, current_train_items 174688.
I0302 19:01:00.210023 22732373614720 run.py:483] Algo bellman_ford step 5459 current loss 0.772676, current_train_items 174720.
I0302 19:01:00.229550 22732373614720 run.py:483] Algo bellman_ford step 5460 current loss 0.361030, current_train_items 174752.
I0302 19:01:00.245903 22732373614720 run.py:483] Algo bellman_ford step 5461 current loss 0.517901, current_train_items 174784.
I0302 19:01:00.269238 22732373614720 run.py:483] Algo bellman_ford step 5462 current loss 0.612935, current_train_items 174816.
I0302 19:01:00.299377 22732373614720 run.py:483] Algo bellman_ford step 5463 current loss 0.704853, current_train_items 174848.
I0302 19:01:00.332502 22732373614720 run.py:483] Algo bellman_ford step 5464 current loss 0.829440, current_train_items 174880.
I0302 19:01:00.351828 22732373614720 run.py:483] Algo bellman_ford step 5465 current loss 0.361263, current_train_items 174912.
I0302 19:01:00.367649 22732373614720 run.py:483] Algo bellman_ford step 5466 current loss 0.455548, current_train_items 174944.
I0302 19:01:00.390259 22732373614720 run.py:483] Algo bellman_ford step 5467 current loss 0.572870, current_train_items 174976.
I0302 19:01:00.419609 22732373614720 run.py:483] Algo bellman_ford step 5468 current loss 0.590801, current_train_items 175008.
I0302 19:01:00.452376 22732373614720 run.py:483] Algo bellman_ford step 5469 current loss 0.758424, current_train_items 175040.
I0302 19:01:00.472127 22732373614720 run.py:483] Algo bellman_ford step 5470 current loss 0.416177, current_train_items 175072.
I0302 19:01:00.488281 22732373614720 run.py:483] Algo bellman_ford step 5471 current loss 0.650697, current_train_items 175104.
I0302 19:01:00.510714 22732373614720 run.py:483] Algo bellman_ford step 5472 current loss 0.531790, current_train_items 175136.
I0302 19:01:00.540689 22732373614720 run.py:483] Algo bellman_ford step 5473 current loss 0.716738, current_train_items 175168.
I0302 19:01:00.574483 22732373614720 run.py:483] Algo bellman_ford step 5474 current loss 0.902170, current_train_items 175200.
I0302 19:01:00.594240 22732373614720 run.py:483] Algo bellman_ford step 5475 current loss 0.358988, current_train_items 175232.
I0302 19:01:00.610422 22732373614720 run.py:483] Algo bellman_ford step 5476 current loss 0.536179, current_train_items 175264.
I0302 19:01:00.632997 22732373614720 run.py:483] Algo bellman_ford step 5477 current loss 0.630627, current_train_items 175296.
I0302 19:01:00.663734 22732373614720 run.py:483] Algo bellman_ford step 5478 current loss 0.678790, current_train_items 175328.
I0302 19:01:00.696111 22732373614720 run.py:483] Algo bellman_ford step 5479 current loss 0.992587, current_train_items 175360.
I0302 19:01:00.716253 22732373614720 run.py:483] Algo bellman_ford step 5480 current loss 0.389626, current_train_items 175392.
I0302 19:01:00.732847 22732373614720 run.py:483] Algo bellman_ford step 5481 current loss 0.511345, current_train_items 175424.
I0302 19:01:00.757056 22732373614720 run.py:483] Algo bellman_ford step 5482 current loss 0.845178, current_train_items 175456.
I0302 19:01:00.788345 22732373614720 run.py:483] Algo bellman_ford step 5483 current loss 0.809915, current_train_items 175488.
I0302 19:01:00.823179 22732373614720 run.py:483] Algo bellman_ford step 5484 current loss 0.847137, current_train_items 175520.
I0302 19:01:00.842673 22732373614720 run.py:483] Algo bellman_ford step 5485 current loss 0.314604, current_train_items 175552.
I0302 19:01:00.858494 22732373614720 run.py:483] Algo bellman_ford step 5486 current loss 0.493106, current_train_items 175584.
I0302 19:01:00.880755 22732373614720 run.py:483] Algo bellman_ford step 5487 current loss 0.706916, current_train_items 175616.
I0302 19:01:00.912528 22732373614720 run.py:483] Algo bellman_ford step 5488 current loss 0.795931, current_train_items 175648.
I0302 19:01:00.944163 22732373614720 run.py:483] Algo bellman_ford step 5489 current loss 0.692162, current_train_items 175680.
I0302 19:01:00.963852 22732373614720 run.py:483] Algo bellman_ford step 5490 current loss 0.385598, current_train_items 175712.
I0302 19:01:00.979962 22732373614720 run.py:483] Algo bellman_ford step 5491 current loss 0.521000, current_train_items 175744.
I0302 19:01:01.003267 22732373614720 run.py:483] Algo bellman_ford step 5492 current loss 0.776116, current_train_items 175776.
I0302 19:01:01.033859 22732373614720 run.py:483] Algo bellman_ford step 5493 current loss 0.817591, current_train_items 175808.
I0302 19:01:01.066073 22732373614720 run.py:483] Algo bellman_ford step 5494 current loss 0.881893, current_train_items 175840.
I0302 19:01:01.085398 22732373614720 run.py:483] Algo bellman_ford step 5495 current loss 0.349127, current_train_items 175872.
I0302 19:01:01.101655 22732373614720 run.py:483] Algo bellman_ford step 5496 current loss 0.506802, current_train_items 175904.
I0302 19:01:01.124959 22732373614720 run.py:483] Algo bellman_ford step 5497 current loss 0.681376, current_train_items 175936.
I0302 19:01:01.155508 22732373614720 run.py:483] Algo bellman_ford step 5498 current loss 0.631317, current_train_items 175968.
I0302 19:01:01.189172 22732373614720 run.py:483] Algo bellman_ford step 5499 current loss 0.841643, current_train_items 176000.
I0302 19:01:01.209397 22732373614720 run.py:483] Algo bellman_ford step 5500 current loss 0.322748, current_train_items 176032.
I0302 19:01:01.217302 22732373614720 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0302 19:01:01.217410 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:01:01.234503 22732373614720 run.py:483] Algo bellman_ford step 5501 current loss 0.522742, current_train_items 176064.
I0302 19:01:01.258649 22732373614720 run.py:483] Algo bellman_ford step 5502 current loss 0.654311, current_train_items 176096.
I0302 19:01:01.289274 22732373614720 run.py:483] Algo bellman_ford step 5503 current loss 0.682230, current_train_items 176128.
I0302 19:01:01.323371 22732373614720 run.py:483] Algo bellman_ford step 5504 current loss 0.841690, current_train_items 176160.
I0302 19:01:01.343485 22732373614720 run.py:483] Algo bellman_ford step 5505 current loss 0.281031, current_train_items 176192.
I0302 19:01:01.359490 22732373614720 run.py:483] Algo bellman_ford step 5506 current loss 0.500744, current_train_items 176224.
I0302 19:01:01.382547 22732373614720 run.py:483] Algo bellman_ford step 5507 current loss 0.576525, current_train_items 176256.
I0302 19:01:01.412424 22732373614720 run.py:483] Algo bellman_ford step 5508 current loss 0.747315, current_train_items 176288.
I0302 19:01:01.444747 22732373614720 run.py:483] Algo bellman_ford step 5509 current loss 0.789695, current_train_items 176320.
I0302 19:01:01.464467 22732373614720 run.py:483] Algo bellman_ford step 5510 current loss 0.333118, current_train_items 176352.
I0302 19:01:01.480861 22732373614720 run.py:483] Algo bellman_ford step 5511 current loss 0.476660, current_train_items 176384.
I0302 19:01:01.505345 22732373614720 run.py:483] Algo bellman_ford step 5512 current loss 0.887530, current_train_items 176416.
I0302 19:01:01.535317 22732373614720 run.py:483] Algo bellman_ford step 5513 current loss 0.779054, current_train_items 176448.
I0302 19:01:01.566734 22732373614720 run.py:483] Algo bellman_ford step 5514 current loss 0.800589, current_train_items 176480.
I0302 19:01:01.586610 22732373614720 run.py:483] Algo bellman_ford step 5515 current loss 0.342658, current_train_items 176512.
I0302 19:01:01.602334 22732373614720 run.py:483] Algo bellman_ford step 5516 current loss 0.425646, current_train_items 176544.
I0302 19:01:01.626168 22732373614720 run.py:483] Algo bellman_ford step 5517 current loss 0.622743, current_train_items 176576.
I0302 19:01:01.656526 22732373614720 run.py:483] Algo bellman_ford step 5518 current loss 0.714124, current_train_items 176608.
I0302 19:01:01.691045 22732373614720 run.py:483] Algo bellman_ford step 5519 current loss 0.847031, current_train_items 176640.
I0302 19:01:01.710845 22732373614720 run.py:483] Algo bellman_ford step 5520 current loss 0.425836, current_train_items 176672.
I0302 19:01:01.727194 22732373614720 run.py:483] Algo bellman_ford step 5521 current loss 0.548074, current_train_items 176704.
I0302 19:01:01.750187 22732373614720 run.py:483] Algo bellman_ford step 5522 current loss 0.645216, current_train_items 176736.
I0302 19:01:01.781374 22732373614720 run.py:483] Algo bellman_ford step 5523 current loss 0.716755, current_train_items 176768.
I0302 19:01:01.815975 22732373614720 run.py:483] Algo bellman_ford step 5524 current loss 1.145276, current_train_items 176800.
I0302 19:01:01.835385 22732373614720 run.py:483] Algo bellman_ford step 5525 current loss 0.298760, current_train_items 176832.
I0302 19:01:01.851070 22732373614720 run.py:483] Algo bellman_ford step 5526 current loss 0.491894, current_train_items 176864.
I0302 19:01:01.875022 22732373614720 run.py:483] Algo bellman_ford step 5527 current loss 0.663112, current_train_items 176896.
I0302 19:01:01.904712 22732373614720 run.py:483] Algo bellman_ford step 5528 current loss 0.750810, current_train_items 176928.
I0302 19:01:01.936734 22732373614720 run.py:483] Algo bellman_ford step 5529 current loss 0.815758, current_train_items 176960.
I0302 19:01:01.956538 22732373614720 run.py:483] Algo bellman_ford step 5530 current loss 0.352334, current_train_items 176992.
I0302 19:01:01.972622 22732373614720 run.py:483] Algo bellman_ford step 5531 current loss 0.473515, current_train_items 177024.
I0302 19:01:01.996681 22732373614720 run.py:483] Algo bellman_ford step 5532 current loss 0.715309, current_train_items 177056.
I0302 19:01:02.027237 22732373614720 run.py:483] Algo bellman_ford step 5533 current loss 0.803704, current_train_items 177088.
I0302 19:01:02.062239 22732373614720 run.py:483] Algo bellman_ford step 5534 current loss 0.960153, current_train_items 177120.
I0302 19:01:02.081675 22732373614720 run.py:483] Algo bellman_ford step 5535 current loss 0.281976, current_train_items 177152.
I0302 19:01:02.098012 22732373614720 run.py:483] Algo bellman_ford step 5536 current loss 0.602317, current_train_items 177184.
I0302 19:01:02.120911 22732373614720 run.py:483] Algo bellman_ford step 5537 current loss 0.650560, current_train_items 177216.
I0302 19:01:02.151031 22732373614720 run.py:483] Algo bellman_ford step 5538 current loss 0.677062, current_train_items 177248.
I0302 19:01:02.186290 22732373614720 run.py:483] Algo bellman_ford step 5539 current loss 0.972841, current_train_items 177280.
I0302 19:01:02.205930 22732373614720 run.py:483] Algo bellman_ford step 5540 current loss 0.313605, current_train_items 177312.
I0302 19:01:02.222390 22732373614720 run.py:483] Algo bellman_ford step 5541 current loss 0.474478, current_train_items 177344.
I0302 19:01:02.246347 22732373614720 run.py:483] Algo bellman_ford step 5542 current loss 0.667894, current_train_items 177376.
I0302 19:01:02.276945 22732373614720 run.py:483] Algo bellman_ford step 5543 current loss 0.655167, current_train_items 177408.
I0302 19:01:02.309268 22732373614720 run.py:483] Algo bellman_ford step 5544 current loss 0.900867, current_train_items 177440.
I0302 19:01:02.329009 22732373614720 run.py:483] Algo bellman_ford step 5545 current loss 0.295437, current_train_items 177472.
I0302 19:01:02.345529 22732373614720 run.py:483] Algo bellman_ford step 5546 current loss 0.591083, current_train_items 177504.
I0302 19:01:02.368950 22732373614720 run.py:483] Algo bellman_ford step 5547 current loss 0.686158, current_train_items 177536.
I0302 19:01:02.399092 22732373614720 run.py:483] Algo bellman_ford step 5548 current loss 0.673736, current_train_items 177568.
I0302 19:01:02.430973 22732373614720 run.py:483] Algo bellman_ford step 5549 current loss 0.830456, current_train_items 177600.
I0302 19:01:02.450436 22732373614720 run.py:483] Algo bellman_ford step 5550 current loss 0.304789, current_train_items 177632.
I0302 19:01:02.458501 22732373614720 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0302 19:01:02.458611 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:01:02.475258 22732373614720 run.py:483] Algo bellman_ford step 5551 current loss 0.454036, current_train_items 177664.
I0302 19:01:02.499948 22732373614720 run.py:483] Algo bellman_ford step 5552 current loss 0.719181, current_train_items 177696.
I0302 19:01:02.530243 22732373614720 run.py:483] Algo bellman_ford step 5553 current loss 0.828123, current_train_items 177728.
I0302 19:01:02.565558 22732373614720 run.py:483] Algo bellman_ford step 5554 current loss 1.140280, current_train_items 177760.
I0302 19:01:02.585599 22732373614720 run.py:483] Algo bellman_ford step 5555 current loss 0.310001, current_train_items 177792.
I0302 19:01:02.601216 22732373614720 run.py:483] Algo bellman_ford step 5556 current loss 0.474624, current_train_items 177824.
I0302 19:01:02.624594 22732373614720 run.py:483] Algo bellman_ford step 5557 current loss 0.721175, current_train_items 177856.
I0302 19:01:02.654441 22732373614720 run.py:483] Algo bellman_ford step 5558 current loss 0.730127, current_train_items 177888.
I0302 19:01:02.688043 22732373614720 run.py:483] Algo bellman_ford step 5559 current loss 0.821665, current_train_items 177920.
I0302 19:01:02.707863 22732373614720 run.py:483] Algo bellman_ford step 5560 current loss 0.390545, current_train_items 177952.
I0302 19:01:02.723701 22732373614720 run.py:483] Algo bellman_ford step 5561 current loss 0.623399, current_train_items 177984.
I0302 19:01:02.746338 22732373614720 run.py:483] Algo bellman_ford step 5562 current loss 0.672354, current_train_items 178016.
I0302 19:01:02.777112 22732373614720 run.py:483] Algo bellman_ford step 5563 current loss 0.762611, current_train_items 178048.
I0302 19:01:02.810957 22732373614720 run.py:483] Algo bellman_ford step 5564 current loss 0.890185, current_train_items 178080.
I0302 19:01:02.830329 22732373614720 run.py:483] Algo bellman_ford step 5565 current loss 0.350202, current_train_items 178112.
I0302 19:01:02.846302 22732373614720 run.py:483] Algo bellman_ford step 5566 current loss 0.444586, current_train_items 178144.
I0302 19:01:02.869655 22732373614720 run.py:483] Algo bellman_ford step 5567 current loss 0.619578, current_train_items 178176.
I0302 19:01:02.901048 22732373614720 run.py:483] Algo bellman_ford step 5568 current loss 0.801463, current_train_items 178208.
I0302 19:01:02.934436 22732373614720 run.py:483] Algo bellman_ford step 5569 current loss 0.982218, current_train_items 178240.
I0302 19:01:02.954314 22732373614720 run.py:483] Algo bellman_ford step 5570 current loss 0.384102, current_train_items 178272.
I0302 19:01:02.970190 22732373614720 run.py:483] Algo bellman_ford step 5571 current loss 0.488859, current_train_items 178304.
I0302 19:01:02.992774 22732373614720 run.py:483] Algo bellman_ford step 5572 current loss 0.757575, current_train_items 178336.
I0302 19:01:03.023000 22732373614720 run.py:483] Algo bellman_ford step 5573 current loss 0.779906, current_train_items 178368.
I0302 19:01:03.056162 22732373614720 run.py:483] Algo bellman_ford step 5574 current loss 0.861868, current_train_items 178400.
I0302 19:01:03.075752 22732373614720 run.py:483] Algo bellman_ford step 5575 current loss 0.382748, current_train_items 178432.
I0302 19:01:03.091683 22732373614720 run.py:483] Algo bellman_ford step 5576 current loss 0.706124, current_train_items 178464.
I0302 19:01:03.113702 22732373614720 run.py:483] Algo bellman_ford step 5577 current loss 0.766690, current_train_items 178496.
I0302 19:01:03.144040 22732373614720 run.py:483] Algo bellman_ford step 5578 current loss 0.772178, current_train_items 178528.
I0302 19:01:03.178660 22732373614720 run.py:483] Algo bellman_ford step 5579 current loss 0.818886, current_train_items 178560.
I0302 19:01:03.197994 22732373614720 run.py:483] Algo bellman_ford step 5580 current loss 0.365302, current_train_items 178592.
I0302 19:01:03.213573 22732373614720 run.py:483] Algo bellman_ford step 5581 current loss 0.531283, current_train_items 178624.
I0302 19:01:03.237445 22732373614720 run.py:483] Algo bellman_ford step 5582 current loss 0.747468, current_train_items 178656.
I0302 19:01:03.267661 22732373614720 run.py:483] Algo bellman_ford step 5583 current loss 0.740590, current_train_items 178688.
I0302 19:01:03.300607 22732373614720 run.py:483] Algo bellman_ford step 5584 current loss 0.860151, current_train_items 178720.
I0302 19:01:03.320294 22732373614720 run.py:483] Algo bellman_ford step 5585 current loss 0.374945, current_train_items 178752.
I0302 19:01:03.336380 22732373614720 run.py:483] Algo bellman_ford step 5586 current loss 0.481027, current_train_items 178784.
I0302 19:01:03.360895 22732373614720 run.py:483] Algo bellman_ford step 5587 current loss 0.763636, current_train_items 178816.
I0302 19:01:03.391923 22732373614720 run.py:483] Algo bellman_ford step 5588 current loss 0.928700, current_train_items 178848.
I0302 19:01:03.422699 22732373614720 run.py:483] Algo bellman_ford step 5589 current loss 0.836513, current_train_items 178880.
I0302 19:01:03.442397 22732373614720 run.py:483] Algo bellman_ford step 5590 current loss 0.446186, current_train_items 178912.
I0302 19:01:03.458330 22732373614720 run.py:483] Algo bellman_ford step 5591 current loss 0.468316, current_train_items 178944.
I0302 19:01:03.480097 22732373614720 run.py:483] Algo bellman_ford step 5592 current loss 0.690760, current_train_items 178976.
I0302 19:01:03.510443 22732373614720 run.py:483] Algo bellman_ford step 5593 current loss 0.863585, current_train_items 179008.
I0302 19:01:03.544290 22732373614720 run.py:483] Algo bellman_ford step 5594 current loss 0.811066, current_train_items 179040.
I0302 19:01:03.563535 22732373614720 run.py:483] Algo bellman_ford step 5595 current loss 0.379236, current_train_items 179072.
I0302 19:01:03.579361 22732373614720 run.py:483] Algo bellman_ford step 5596 current loss 0.527836, current_train_items 179104.
I0302 19:01:03.602203 22732373614720 run.py:483] Algo bellman_ford step 5597 current loss 0.711716, current_train_items 179136.
I0302 19:01:03.632569 22732373614720 run.py:483] Algo bellman_ford step 5598 current loss 0.853310, current_train_items 179168.
I0302 19:01:03.661700 22732373614720 run.py:483] Algo bellman_ford step 5599 current loss 0.968255, current_train_items 179200.
I0302 19:01:03.681479 22732373614720 run.py:483] Algo bellman_ford step 5600 current loss 0.364609, current_train_items 179232.
I0302 19:01:03.689350 22732373614720 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0302 19:01:03.689458 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 19:01:03.706226 22732373614720 run.py:483] Algo bellman_ford step 5601 current loss 0.553542, current_train_items 179264.
I0302 19:01:03.729970 22732373614720 run.py:483] Algo bellman_ford step 5602 current loss 0.735593, current_train_items 179296.
I0302 19:01:03.760655 22732373614720 run.py:483] Algo bellman_ford step 5603 current loss 0.893957, current_train_items 179328.
I0302 19:01:03.792123 22732373614720 run.py:483] Algo bellman_ford step 5604 current loss 0.812074, current_train_items 179360.
I0302 19:01:03.811997 22732373614720 run.py:483] Algo bellman_ford step 5605 current loss 0.316904, current_train_items 179392.
I0302 19:01:03.827492 22732373614720 run.py:483] Algo bellman_ford step 5606 current loss 0.619516, current_train_items 179424.
I0302 19:01:03.851027 22732373614720 run.py:483] Algo bellman_ford step 5607 current loss 0.694389, current_train_items 179456.
I0302 19:01:03.880115 22732373614720 run.py:483] Algo bellman_ford step 5608 current loss 0.735419, current_train_items 179488.
I0302 19:01:03.913863 22732373614720 run.py:483] Algo bellman_ford step 5609 current loss 0.961171, current_train_items 179520.
I0302 19:01:03.933664 22732373614720 run.py:483] Algo bellman_ford step 5610 current loss 0.332887, current_train_items 179552.
I0302 19:01:03.949753 22732373614720 run.py:483] Algo bellman_ford step 5611 current loss 0.561390, current_train_items 179584.
I0302 19:01:03.972414 22732373614720 run.py:483] Algo bellman_ford step 5612 current loss 0.791449, current_train_items 179616.
I0302 19:01:04.004011 22732373614720 run.py:483] Algo bellman_ford step 5613 current loss 0.766119, current_train_items 179648.
I0302 19:01:04.035674 22732373614720 run.py:483] Algo bellman_ford step 5614 current loss 0.861816, current_train_items 179680.
I0302 19:01:04.055372 22732373614720 run.py:483] Algo bellman_ford step 5615 current loss 0.293423, current_train_items 179712.
I0302 19:01:04.071666 22732373614720 run.py:483] Algo bellman_ford step 5616 current loss 0.497647, current_train_items 179744.
I0302 19:01:04.094845 22732373614720 run.py:483] Algo bellman_ford step 5617 current loss 0.617405, current_train_items 179776.
I0302 19:01:04.124685 22732373614720 run.py:483] Algo bellman_ford step 5618 current loss 0.733872, current_train_items 179808.
I0302 19:01:04.158625 22732373614720 run.py:483] Algo bellman_ford step 5619 current loss 0.818137, current_train_items 179840.
I0302 19:01:04.178349 22732373614720 run.py:483] Algo bellman_ford step 5620 current loss 0.352929, current_train_items 179872.
I0302 19:01:04.194310 22732373614720 run.py:483] Algo bellman_ford step 5621 current loss 0.605094, current_train_items 179904.
I0302 19:01:04.217184 22732373614720 run.py:483] Algo bellman_ford step 5622 current loss 0.629852, current_train_items 179936.
I0302 19:01:04.247166 22732373614720 run.py:483] Algo bellman_ford step 5623 current loss 0.701685, current_train_items 179968.
I0302 19:01:04.280629 22732373614720 run.py:483] Algo bellman_ford step 5624 current loss 0.808854, current_train_items 180000.
I0302 19:01:04.299875 22732373614720 run.py:483] Algo bellman_ford step 5625 current loss 0.272783, current_train_items 180032.
I0302 19:01:04.316287 22732373614720 run.py:483] Algo bellman_ford step 5626 current loss 0.586401, current_train_items 180064.
I0302 19:01:04.339645 22732373614720 run.py:483] Algo bellman_ford step 5627 current loss 0.688307, current_train_items 180096.
I0302 19:01:04.370142 22732373614720 run.py:483] Algo bellman_ford step 5628 current loss 0.723546, current_train_items 180128.
I0302 19:01:04.403735 22732373614720 run.py:483] Algo bellman_ford step 5629 current loss 0.834936, current_train_items 180160.
I0302 19:01:04.422949 22732373614720 run.py:483] Algo bellman_ford step 5630 current loss 0.417795, current_train_items 180192.
I0302 19:01:04.438829 22732373614720 run.py:483] Algo bellman_ford step 5631 current loss 0.429340, current_train_items 180224.
I0302 19:01:04.462444 22732373614720 run.py:483] Algo bellman_ford step 5632 current loss 0.725889, current_train_items 180256.
I0302 19:01:04.492511 22732373614720 run.py:483] Algo bellman_ford step 5633 current loss 0.700242, current_train_items 180288.
I0302 19:01:04.525790 22732373614720 run.py:483] Algo bellman_ford step 5634 current loss 0.846203, current_train_items 180320.
I0302 19:01:04.545167 22732373614720 run.py:483] Algo bellman_ford step 5635 current loss 0.375423, current_train_items 180352.
I0302 19:01:04.561243 22732373614720 run.py:483] Algo bellman_ford step 5636 current loss 0.513004, current_train_items 180384.
I0302 19:01:04.584036 22732373614720 run.py:483] Algo bellman_ford step 5637 current loss 0.683983, current_train_items 180416.
I0302 19:01:04.613288 22732373614720 run.py:483] Algo bellman_ford step 5638 current loss 0.725613, current_train_items 180448.
I0302 19:01:04.644837 22732373614720 run.py:483] Algo bellman_ford step 5639 current loss 0.803444, current_train_items 180480.
I0302 19:01:04.664292 22732373614720 run.py:483] Algo bellman_ford step 5640 current loss 0.324199, current_train_items 180512.
I0302 19:01:04.680054 22732373614720 run.py:483] Algo bellman_ford step 5641 current loss 0.462512, current_train_items 180544.
I0302 19:01:04.702865 22732373614720 run.py:483] Algo bellman_ford step 5642 current loss 0.625207, current_train_items 180576.
I0302 19:01:04.733165 22732373614720 run.py:483] Algo bellman_ford step 5643 current loss 0.802440, current_train_items 180608.
I0302 19:01:04.765572 22732373614720 run.py:483] Algo bellman_ford step 5644 current loss 0.798440, current_train_items 180640.
I0302 19:01:04.784925 22732373614720 run.py:483] Algo bellman_ford step 5645 current loss 0.303969, current_train_items 180672.
I0302 19:01:04.801418 22732373614720 run.py:483] Algo bellman_ford step 5646 current loss 0.532924, current_train_items 180704.
I0302 19:01:04.823931 22732373614720 run.py:483] Algo bellman_ford step 5647 current loss 0.669034, current_train_items 180736.
I0302 19:01:04.853876 22732373614720 run.py:483] Algo bellman_ford step 5648 current loss 0.686703, current_train_items 180768.
I0302 19:01:04.888118 22732373614720 run.py:483] Algo bellman_ford step 5649 current loss 0.835523, current_train_items 180800.
I0302 19:01:04.907371 22732373614720 run.py:483] Algo bellman_ford step 5650 current loss 0.294880, current_train_items 180832.
I0302 19:01:04.915641 22732373614720 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0302 19:01:04.915751 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:01:04.932559 22732373614720 run.py:483] Algo bellman_ford step 5651 current loss 0.513477, current_train_items 180864.
I0302 19:01:04.955653 22732373614720 run.py:483] Algo bellman_ford step 5652 current loss 0.659621, current_train_items 180896.
I0302 19:01:04.988095 22732373614720 run.py:483] Algo bellman_ford step 5653 current loss 0.768141, current_train_items 180928.
I0302 19:01:05.022615 22732373614720 run.py:483] Algo bellman_ford step 5654 current loss 0.862401, current_train_items 180960.
I0302 19:01:05.042437 22732373614720 run.py:483] Algo bellman_ford step 5655 current loss 0.354859, current_train_items 180992.
I0302 19:01:05.058440 22732373614720 run.py:483] Algo bellman_ford step 5656 current loss 0.576726, current_train_items 181024.
I0302 19:01:05.080925 22732373614720 run.py:483] Algo bellman_ford step 5657 current loss 0.698718, current_train_items 181056.
I0302 19:01:05.112446 22732373614720 run.py:483] Algo bellman_ford step 5658 current loss 0.808351, current_train_items 181088.
I0302 19:01:05.145557 22732373614720 run.py:483] Algo bellman_ford step 5659 current loss 0.861591, current_train_items 181120.
I0302 19:01:05.165515 22732373614720 run.py:483] Algo bellman_ford step 5660 current loss 0.334474, current_train_items 181152.
I0302 19:01:05.181237 22732373614720 run.py:483] Algo bellman_ford step 5661 current loss 0.428915, current_train_items 181184.
I0302 19:01:05.203592 22732373614720 run.py:483] Algo bellman_ford step 5662 current loss 0.626793, current_train_items 181216.
I0302 19:01:05.235977 22732373614720 run.py:483] Algo bellman_ford step 5663 current loss 0.827774, current_train_items 181248.
I0302 19:01:05.269289 22732373614720 run.py:483] Algo bellman_ford step 5664 current loss 0.821105, current_train_items 181280.
I0302 19:01:05.288748 22732373614720 run.py:483] Algo bellman_ford step 5665 current loss 0.341662, current_train_items 181312.
I0302 19:01:05.305200 22732373614720 run.py:483] Algo bellman_ford step 5666 current loss 0.499906, current_train_items 181344.
I0302 19:01:05.327938 22732373614720 run.py:483] Algo bellman_ford step 5667 current loss 0.642435, current_train_items 181376.
I0302 19:01:05.359061 22732373614720 run.py:483] Algo bellman_ford step 5668 current loss 0.808156, current_train_items 181408.
I0302 19:01:05.391254 22732373614720 run.py:483] Algo bellman_ford step 5669 current loss 0.757916, current_train_items 181440.
I0302 19:01:05.410864 22732373614720 run.py:483] Algo bellman_ford step 5670 current loss 0.309811, current_train_items 181472.
I0302 19:01:05.426869 22732373614720 run.py:483] Algo bellman_ford step 5671 current loss 0.440972, current_train_items 181504.
I0302 19:01:05.449338 22732373614720 run.py:483] Algo bellman_ford step 5672 current loss 0.623061, current_train_items 181536.
I0302 19:01:05.479475 22732373614720 run.py:483] Algo bellman_ford step 5673 current loss 0.748660, current_train_items 181568.
I0302 19:01:05.511834 22732373614720 run.py:483] Algo bellman_ford step 5674 current loss 0.786959, current_train_items 181600.
I0302 19:01:05.531414 22732373614720 run.py:483] Algo bellman_ford step 5675 current loss 0.306978, current_train_items 181632.
I0302 19:01:05.547308 22732373614720 run.py:483] Algo bellman_ford step 5676 current loss 0.468327, current_train_items 181664.
I0302 19:01:05.570233 22732373614720 run.py:483] Algo bellman_ford step 5677 current loss 0.715465, current_train_items 181696.
I0302 19:01:05.601962 22732373614720 run.py:483] Algo bellman_ford step 5678 current loss 0.837643, current_train_items 181728.
I0302 19:01:05.634915 22732373614720 run.py:483] Algo bellman_ford step 5679 current loss 0.928786, current_train_items 181760.
I0302 19:01:05.654365 22732373614720 run.py:483] Algo bellman_ford step 5680 current loss 0.330780, current_train_items 181792.
I0302 19:01:05.670536 22732373614720 run.py:483] Algo bellman_ford step 5681 current loss 0.454634, current_train_items 181824.
I0302 19:01:05.693938 22732373614720 run.py:483] Algo bellman_ford step 5682 current loss 0.660149, current_train_items 181856.
I0302 19:01:05.724135 22732373614720 run.py:483] Algo bellman_ford step 5683 current loss 0.694506, current_train_items 181888.
I0302 19:01:05.756097 22732373614720 run.py:483] Algo bellman_ford step 5684 current loss 0.676365, current_train_items 181920.
I0302 19:01:05.775742 22732373614720 run.py:483] Algo bellman_ford step 5685 current loss 0.308690, current_train_items 181952.
I0302 19:01:05.791721 22732373614720 run.py:483] Algo bellman_ford step 5686 current loss 0.581410, current_train_items 181984.
I0302 19:01:05.814746 22732373614720 run.py:483] Algo bellman_ford step 5687 current loss 0.717065, current_train_items 182016.
I0302 19:01:05.845638 22732373614720 run.py:483] Algo bellman_ford step 5688 current loss 0.761342, current_train_items 182048.
I0302 19:01:05.878068 22732373614720 run.py:483] Algo bellman_ford step 5689 current loss 0.886362, current_train_items 182080.
I0302 19:01:05.897666 22732373614720 run.py:483] Algo bellman_ford step 5690 current loss 0.324112, current_train_items 182112.
I0302 19:01:05.914294 22732373614720 run.py:483] Algo bellman_ford step 5691 current loss 0.480421, current_train_items 182144.
I0302 19:01:05.937135 22732373614720 run.py:483] Algo bellman_ford step 5692 current loss 0.666883, current_train_items 182176.
I0302 19:01:05.967866 22732373614720 run.py:483] Algo bellman_ford step 5693 current loss 0.716762, current_train_items 182208.
I0302 19:01:05.999984 22732373614720 run.py:483] Algo bellman_ford step 5694 current loss 0.793614, current_train_items 182240.
I0302 19:01:06.019484 22732373614720 run.py:483] Algo bellman_ford step 5695 current loss 0.266375, current_train_items 182272.
I0302 19:01:06.035774 22732373614720 run.py:483] Algo bellman_ford step 5696 current loss 0.615084, current_train_items 182304.
I0302 19:01:06.058663 22732373614720 run.py:483] Algo bellman_ford step 5697 current loss 0.697754, current_train_items 182336.
I0302 19:01:06.090216 22732373614720 run.py:483] Algo bellman_ford step 5698 current loss 0.765920, current_train_items 182368.
I0302 19:01:06.123465 22732373614720 run.py:483] Algo bellman_ford step 5699 current loss 0.895533, current_train_items 182400.
I0302 19:01:06.143085 22732373614720 run.py:483] Algo bellman_ford step 5700 current loss 0.368911, current_train_items 182432.
I0302 19:01:06.150975 22732373614720 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0302 19:01:06.151083 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 19:01:06.167222 22732373614720 run.py:483] Algo bellman_ford step 5701 current loss 0.475887, current_train_items 182464.
I0302 19:01:06.190178 22732373614720 run.py:483] Algo bellman_ford step 5702 current loss 0.620925, current_train_items 182496.
I0302 19:01:06.221207 22732373614720 run.py:483] Algo bellman_ford step 5703 current loss 0.741195, current_train_items 182528.
I0302 19:01:06.255557 22732373614720 run.py:483] Algo bellman_ford step 5704 current loss 0.974469, current_train_items 182560.
I0302 19:01:06.275738 22732373614720 run.py:483] Algo bellman_ford step 5705 current loss 0.331933, current_train_items 182592.
I0302 19:01:06.291351 22732373614720 run.py:483] Algo bellman_ford step 5706 current loss 0.430090, current_train_items 182624.
I0302 19:01:06.315222 22732373614720 run.py:483] Algo bellman_ford step 5707 current loss 0.704950, current_train_items 182656.
I0302 19:01:06.345531 22732373614720 run.py:483] Algo bellman_ford step 5708 current loss 0.731880, current_train_items 182688.
I0302 19:01:06.378981 22732373614720 run.py:483] Algo bellman_ford step 5709 current loss 0.767630, current_train_items 182720.
I0302 19:01:06.398730 22732373614720 run.py:483] Algo bellman_ford step 5710 current loss 0.293505, current_train_items 182752.
I0302 19:01:06.414566 22732373614720 run.py:483] Algo bellman_ford step 5711 current loss 0.663794, current_train_items 182784.
I0302 19:01:06.437282 22732373614720 run.py:483] Algo bellman_ford step 5712 current loss 0.676039, current_train_items 182816.
I0302 19:01:06.467179 22732373614720 run.py:483] Algo bellman_ford step 5713 current loss 0.692800, current_train_items 182848.
I0302 19:01:06.499313 22732373614720 run.py:483] Algo bellman_ford step 5714 current loss 0.806308, current_train_items 182880.
I0302 19:01:06.519264 22732373614720 run.py:483] Algo bellman_ford step 5715 current loss 0.439468, current_train_items 182912.
I0302 19:01:06.535262 22732373614720 run.py:483] Algo bellman_ford step 5716 current loss 0.558260, current_train_items 182944.
I0302 19:01:06.559111 22732373614720 run.py:483] Algo bellman_ford step 5717 current loss 0.666976, current_train_items 182976.
I0302 19:01:06.588934 22732373614720 run.py:483] Algo bellman_ford step 5718 current loss 0.780687, current_train_items 183008.
I0302 19:01:06.622144 22732373614720 run.py:483] Algo bellman_ford step 5719 current loss 0.827726, current_train_items 183040.
I0302 19:01:06.641589 22732373614720 run.py:483] Algo bellman_ford step 5720 current loss 0.289570, current_train_items 183072.
I0302 19:01:06.657550 22732373614720 run.py:483] Algo bellman_ford step 5721 current loss 0.497136, current_train_items 183104.
I0302 19:01:06.680355 22732373614720 run.py:483] Algo bellman_ford step 5722 current loss 0.635732, current_train_items 183136.
I0302 19:01:06.710364 22732373614720 run.py:483] Algo bellman_ford step 5723 current loss 0.700175, current_train_items 183168.
I0302 19:01:06.742733 22732373614720 run.py:483] Algo bellman_ford step 5724 current loss 0.764425, current_train_items 183200.
I0302 19:01:06.762430 22732373614720 run.py:483] Algo bellman_ford step 5725 current loss 0.362332, current_train_items 183232.
I0302 19:01:06.778528 22732373614720 run.py:483] Algo bellman_ford step 5726 current loss 0.490561, current_train_items 183264.
I0302 19:01:06.802246 22732373614720 run.py:483] Algo bellman_ford step 5727 current loss 0.647979, current_train_items 183296.
I0302 19:01:06.831981 22732373614720 run.py:483] Algo bellman_ford step 5728 current loss 0.685872, current_train_items 183328.
I0302 19:01:06.865033 22732373614720 run.py:483] Algo bellman_ford step 5729 current loss 0.775992, current_train_items 183360.
I0302 19:01:06.884776 22732373614720 run.py:483] Algo bellman_ford step 5730 current loss 0.343512, current_train_items 183392.
I0302 19:01:06.901115 22732373614720 run.py:483] Algo bellman_ford step 5731 current loss 0.617733, current_train_items 183424.
I0302 19:01:06.924926 22732373614720 run.py:483] Algo bellman_ford step 5732 current loss 0.751886, current_train_items 183456.
I0302 19:01:06.954771 22732373614720 run.py:483] Algo bellman_ford step 5733 current loss 0.763993, current_train_items 183488.
I0302 19:01:06.988521 22732373614720 run.py:483] Algo bellman_ford step 5734 current loss 0.798825, current_train_items 183520.
I0302 19:01:07.008167 22732373614720 run.py:483] Algo bellman_ford step 5735 current loss 0.297566, current_train_items 183552.
I0302 19:01:07.024480 22732373614720 run.py:483] Algo bellman_ford step 5736 current loss 0.544024, current_train_items 183584.
I0302 19:01:07.047886 22732373614720 run.py:483] Algo bellman_ford step 5737 current loss 0.776158, current_train_items 183616.
I0302 19:01:07.077816 22732373614720 run.py:483] Algo bellman_ford step 5738 current loss 0.698667, current_train_items 183648.
I0302 19:01:07.111472 22732373614720 run.py:483] Algo bellman_ford step 5739 current loss 0.755067, current_train_items 183680.
I0302 19:01:07.131090 22732373614720 run.py:483] Algo bellman_ford step 5740 current loss 0.352378, current_train_items 183712.
I0302 19:01:07.147443 22732373614720 run.py:483] Algo bellman_ford step 5741 current loss 0.479797, current_train_items 183744.
I0302 19:01:07.171773 22732373614720 run.py:483] Algo bellman_ford step 5742 current loss 0.613491, current_train_items 183776.
I0302 19:01:07.203227 22732373614720 run.py:483] Algo bellman_ford step 5743 current loss 0.746133, current_train_items 183808.
I0302 19:01:07.237083 22732373614720 run.py:483] Algo bellman_ford step 5744 current loss 0.905995, current_train_items 183840.
I0302 19:01:07.256713 22732373614720 run.py:483] Algo bellman_ford step 5745 current loss 0.369400, current_train_items 183872.
I0302 19:01:07.273161 22732373614720 run.py:483] Algo bellman_ford step 5746 current loss 0.508653, current_train_items 183904.
I0302 19:01:07.296813 22732373614720 run.py:483] Algo bellman_ford step 5747 current loss 0.678749, current_train_items 183936.
I0302 19:01:07.327727 22732373614720 run.py:483] Algo bellman_ford step 5748 current loss 0.871308, current_train_items 183968.
I0302 19:01:07.360186 22732373614720 run.py:483] Algo bellman_ford step 5749 current loss 0.792519, current_train_items 184000.
I0302 19:01:07.379802 22732373614720 run.py:483] Algo bellman_ford step 5750 current loss 0.354990, current_train_items 184032.
I0302 19:01:07.387961 22732373614720 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0302 19:01:07.388067 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 19:01:07.405105 22732373614720 run.py:483] Algo bellman_ford step 5751 current loss 0.481343, current_train_items 184064.
I0302 19:01:07.428226 22732373614720 run.py:483] Algo bellman_ford step 5752 current loss 0.729615, current_train_items 184096.
I0302 19:01:07.458098 22732373614720 run.py:483] Algo bellman_ford step 5753 current loss 0.688182, current_train_items 184128.
I0302 19:01:07.492915 22732373614720 run.py:483] Algo bellman_ford step 5754 current loss 0.999770, current_train_items 184160.
I0302 19:01:07.513334 22732373614720 run.py:483] Algo bellman_ford step 5755 current loss 0.297869, current_train_items 184192.
I0302 19:01:07.528819 22732373614720 run.py:483] Algo bellman_ford step 5756 current loss 0.450013, current_train_items 184224.
I0302 19:01:07.552470 22732373614720 run.py:483] Algo bellman_ford step 5757 current loss 0.723681, current_train_items 184256.
I0302 19:01:07.584029 22732373614720 run.py:483] Algo bellman_ford step 5758 current loss 0.780287, current_train_items 184288.
I0302 19:01:07.617210 22732373614720 run.py:483] Algo bellman_ford step 5759 current loss 0.818618, current_train_items 184320.
I0302 19:01:07.637206 22732373614720 run.py:483] Algo bellman_ford step 5760 current loss 0.451529, current_train_items 184352.
I0302 19:01:07.653544 22732373614720 run.py:483] Algo bellman_ford step 5761 current loss 0.478690, current_train_items 184384.
I0302 19:01:07.676945 22732373614720 run.py:483] Algo bellman_ford step 5762 current loss 0.567284, current_train_items 184416.
I0302 19:01:07.706668 22732373614720 run.py:483] Algo bellman_ford step 5763 current loss 0.714624, current_train_items 184448.
I0302 19:01:07.740936 22732373614720 run.py:483] Algo bellman_ford step 5764 current loss 0.874986, current_train_items 184480.
I0302 19:01:07.760851 22732373614720 run.py:483] Algo bellman_ford step 5765 current loss 0.387815, current_train_items 184512.
I0302 19:01:07.777093 22732373614720 run.py:483] Algo bellman_ford step 5766 current loss 0.559377, current_train_items 184544.
I0302 19:01:07.800015 22732373614720 run.py:483] Algo bellman_ford step 5767 current loss 0.640367, current_train_items 184576.
I0302 19:01:07.831394 22732373614720 run.py:483] Algo bellman_ford step 5768 current loss 0.794630, current_train_items 184608.
I0302 19:01:07.864274 22732373614720 run.py:483] Algo bellman_ford step 5769 current loss 0.773692, current_train_items 184640.
I0302 19:01:07.884062 22732373614720 run.py:483] Algo bellman_ford step 5770 current loss 0.300658, current_train_items 184672.
I0302 19:01:07.900142 22732373614720 run.py:483] Algo bellman_ford step 5771 current loss 0.463697, current_train_items 184704.
I0302 19:01:07.922271 22732373614720 run.py:483] Algo bellman_ford step 5772 current loss 0.613783, current_train_items 184736.
I0302 19:01:07.952487 22732373614720 run.py:483] Algo bellman_ford step 5773 current loss 0.823938, current_train_items 184768.
I0302 19:01:07.984721 22732373614720 run.py:483] Algo bellman_ford step 5774 current loss 0.692515, current_train_items 184800.
I0302 19:01:08.004334 22732373614720 run.py:483] Algo bellman_ford step 5775 current loss 0.346943, current_train_items 184832.
I0302 19:01:08.020807 22732373614720 run.py:483] Algo bellman_ford step 5776 current loss 0.440835, current_train_items 184864.
I0302 19:01:08.043127 22732373614720 run.py:483] Algo bellman_ford step 5777 current loss 0.616695, current_train_items 184896.
I0302 19:01:08.073740 22732373614720 run.py:483] Algo bellman_ford step 5778 current loss 0.711477, current_train_items 184928.
I0302 19:01:08.107463 22732373614720 run.py:483] Algo bellman_ford step 5779 current loss 0.821449, current_train_items 184960.
I0302 19:01:08.127018 22732373614720 run.py:483] Algo bellman_ford step 5780 current loss 0.284199, current_train_items 184992.
I0302 19:01:08.143046 22732373614720 run.py:483] Algo bellman_ford step 5781 current loss 0.473990, current_train_items 185024.
I0302 19:01:08.166742 22732373614720 run.py:483] Algo bellman_ford step 5782 current loss 0.624660, current_train_items 185056.
I0302 19:01:08.196458 22732373614720 run.py:483] Algo bellman_ford step 5783 current loss 0.600577, current_train_items 185088.
I0302 19:01:08.227553 22732373614720 run.py:483] Algo bellman_ford step 5784 current loss 0.813207, current_train_items 185120.
I0302 19:01:08.247775 22732373614720 run.py:483] Algo bellman_ford step 5785 current loss 0.386654, current_train_items 185152.
I0302 19:01:08.263638 22732373614720 run.py:483] Algo bellman_ford step 5786 current loss 0.478645, current_train_items 185184.
I0302 19:01:08.285417 22732373614720 run.py:483] Algo bellman_ford step 5787 current loss 0.596014, current_train_items 185216.
I0302 19:01:08.316534 22732373614720 run.py:483] Algo bellman_ford step 5788 current loss 0.779101, current_train_items 185248.
I0302 19:01:08.348333 22732373614720 run.py:483] Algo bellman_ford step 5789 current loss 0.960620, current_train_items 185280.
I0302 19:01:08.367981 22732373614720 run.py:483] Algo bellman_ford step 5790 current loss 0.277143, current_train_items 185312.
I0302 19:01:08.384411 22732373614720 run.py:483] Algo bellman_ford step 5791 current loss 0.557440, current_train_items 185344.
I0302 19:01:08.406515 22732373614720 run.py:483] Algo bellman_ford step 5792 current loss 0.636269, current_train_items 185376.
I0302 19:01:08.436789 22732373614720 run.py:483] Algo bellman_ford step 5793 current loss 0.730443, current_train_items 185408.
I0302 19:01:08.469771 22732373614720 run.py:483] Algo bellman_ford step 5794 current loss 0.771133, current_train_items 185440.
I0302 19:01:08.489176 22732373614720 run.py:483] Algo bellman_ford step 5795 current loss 0.333796, current_train_items 185472.
I0302 19:01:08.505119 22732373614720 run.py:483] Algo bellman_ford step 5796 current loss 0.494487, current_train_items 185504.
I0302 19:01:08.528970 22732373614720 run.py:483] Algo bellman_ford step 5797 current loss 0.793388, current_train_items 185536.
I0302 19:01:08.559663 22732373614720 run.py:483] Algo bellman_ford step 5798 current loss 0.672899, current_train_items 185568.
I0302 19:01:08.593321 22732373614720 run.py:483] Algo bellman_ford step 5799 current loss 0.882806, current_train_items 185600.
I0302 19:01:08.613125 22732373614720 run.py:483] Algo bellman_ford step 5800 current loss 0.349604, current_train_items 185632.
I0302 19:01:08.620971 22732373614720 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0302 19:01:08.621084 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:01:08.637966 22732373614720 run.py:483] Algo bellman_ford step 5801 current loss 0.592495, current_train_items 185664.
I0302 19:01:08.660948 22732373614720 run.py:483] Algo bellman_ford step 5802 current loss 0.622170, current_train_items 185696.
I0302 19:01:08.692164 22732373614720 run.py:483] Algo bellman_ford step 5803 current loss 0.748606, current_train_items 185728.
I0302 19:01:08.726808 22732373614720 run.py:483] Algo bellman_ford step 5804 current loss 0.885509, current_train_items 185760.
I0302 19:01:08.746515 22732373614720 run.py:483] Algo bellman_ford step 5805 current loss 0.347097, current_train_items 185792.
I0302 19:01:08.762345 22732373614720 run.py:483] Algo bellman_ford step 5806 current loss 0.425002, current_train_items 185824.
I0302 19:01:08.786136 22732373614720 run.py:483] Algo bellman_ford step 5807 current loss 0.632203, current_train_items 185856.
I0302 19:01:08.816978 22732373614720 run.py:483] Algo bellman_ford step 5808 current loss 0.723787, current_train_items 185888.
I0302 19:01:08.849546 22732373614720 run.py:483] Algo bellman_ford step 5809 current loss 0.736490, current_train_items 185920.
I0302 19:01:08.869080 22732373614720 run.py:483] Algo bellman_ford step 5810 current loss 0.296043, current_train_items 185952.
I0302 19:01:08.884934 22732373614720 run.py:483] Algo bellman_ford step 5811 current loss 0.489013, current_train_items 185984.
I0302 19:01:08.908295 22732373614720 run.py:483] Algo bellman_ford step 5812 current loss 0.618064, current_train_items 186016.
I0302 19:01:08.939832 22732373614720 run.py:483] Algo bellman_ford step 5813 current loss 0.727976, current_train_items 186048.
I0302 19:01:08.973394 22732373614720 run.py:483] Algo bellman_ford step 5814 current loss 0.817095, current_train_items 186080.
I0302 19:01:08.992937 22732373614720 run.py:483] Algo bellman_ford step 5815 current loss 0.341720, current_train_items 186112.
I0302 19:01:09.009101 22732373614720 run.py:483] Algo bellman_ford step 5816 current loss 0.465402, current_train_items 186144.
I0302 19:01:09.032663 22732373614720 run.py:483] Algo bellman_ford step 5817 current loss 0.628423, current_train_items 186176.
I0302 19:01:09.062091 22732373614720 run.py:483] Algo bellman_ford step 5818 current loss 0.617847, current_train_items 186208.
I0302 19:01:09.095301 22732373614720 run.py:483] Algo bellman_ford step 5819 current loss 0.829086, current_train_items 186240.
I0302 19:01:09.114589 22732373614720 run.py:483] Algo bellman_ford step 5820 current loss 0.302298, current_train_items 186272.
I0302 19:01:09.130496 22732373614720 run.py:483] Algo bellman_ford step 5821 current loss 0.466258, current_train_items 186304.
I0302 19:01:09.153874 22732373614720 run.py:483] Algo bellman_ford step 5822 current loss 0.666149, current_train_items 186336.
I0302 19:01:09.184265 22732373614720 run.py:483] Algo bellman_ford step 5823 current loss 0.687772, current_train_items 186368.
I0302 19:01:09.216525 22732373614720 run.py:483] Algo bellman_ford step 5824 current loss 0.874968, current_train_items 186400.
I0302 19:01:09.235868 22732373614720 run.py:483] Algo bellman_ford step 5825 current loss 0.395420, current_train_items 186432.
I0302 19:01:09.251940 22732373614720 run.py:483] Algo bellman_ford step 5826 current loss 0.476174, current_train_items 186464.
I0302 19:01:09.275622 22732373614720 run.py:483] Algo bellman_ford step 5827 current loss 0.717628, current_train_items 186496.
I0302 19:01:09.306698 22732373614720 run.py:483] Algo bellman_ford step 5828 current loss 0.723594, current_train_items 186528.
I0302 19:01:09.340780 22732373614720 run.py:483] Algo bellman_ford step 5829 current loss 0.914581, current_train_items 186560.
I0302 19:01:09.360592 22732373614720 run.py:483] Algo bellman_ford step 5830 current loss 0.383535, current_train_items 186592.
I0302 19:01:09.376871 22732373614720 run.py:483] Algo bellman_ford step 5831 current loss 0.561901, current_train_items 186624.
I0302 19:01:09.399436 22732373614720 run.py:483] Algo bellman_ford step 5832 current loss 0.737469, current_train_items 186656.
I0302 19:01:09.429620 22732373614720 run.py:483] Algo bellman_ford step 5833 current loss 0.888858, current_train_items 186688.
I0302 19:01:09.459300 22732373614720 run.py:483] Algo bellman_ford step 5834 current loss 0.706709, current_train_items 186720.
I0302 19:01:09.478773 22732373614720 run.py:483] Algo bellman_ford step 5835 current loss 0.309580, current_train_items 186752.
I0302 19:01:09.494233 22732373614720 run.py:483] Algo bellman_ford step 5836 current loss 0.504197, current_train_items 186784.
I0302 19:01:09.518671 22732373614720 run.py:483] Algo bellman_ford step 5837 current loss 0.809372, current_train_items 186816.
I0302 19:01:09.547436 22732373614720 run.py:483] Algo bellman_ford step 5838 current loss 0.652810, current_train_items 186848.
I0302 19:01:09.581942 22732373614720 run.py:483] Algo bellman_ford step 5839 current loss 0.928626, current_train_items 186880.
I0302 19:01:09.601562 22732373614720 run.py:483] Algo bellman_ford step 5840 current loss 0.335915, current_train_items 186912.
I0302 19:01:09.617829 22732373614720 run.py:483] Algo bellman_ford step 5841 current loss 0.508835, current_train_items 186944.
I0302 19:01:09.640283 22732373614720 run.py:483] Algo bellman_ford step 5842 current loss 0.677204, current_train_items 186976.
I0302 19:01:09.669639 22732373614720 run.py:483] Algo bellman_ford step 5843 current loss 0.707124, current_train_items 187008.
I0302 19:01:09.701961 22732373614720 run.py:483] Algo bellman_ford step 5844 current loss 0.762453, current_train_items 187040.
I0302 19:01:09.721432 22732373614720 run.py:483] Algo bellman_ford step 5845 current loss 0.378019, current_train_items 187072.
I0302 19:01:09.737560 22732373614720 run.py:483] Algo bellman_ford step 5846 current loss 0.455293, current_train_items 187104.
I0302 19:01:09.761705 22732373614720 run.py:483] Algo bellman_ford step 5847 current loss 0.720973, current_train_items 187136.
I0302 19:01:09.790587 22732373614720 run.py:483] Algo bellman_ford step 5848 current loss 0.689082, current_train_items 187168.
I0302 19:01:09.823785 22732373614720 run.py:483] Algo bellman_ford step 5849 current loss 0.791384, current_train_items 187200.
I0302 19:01:09.842960 22732373614720 run.py:483] Algo bellman_ford step 5850 current loss 0.268397, current_train_items 187232.
I0302 19:01:09.851228 22732373614720 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0302 19:01:09.851338 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:01:09.868176 22732373614720 run.py:483] Algo bellman_ford step 5851 current loss 0.458372, current_train_items 187264.
I0302 19:01:09.891257 22732373614720 run.py:483] Algo bellman_ford step 5852 current loss 0.644013, current_train_items 187296.
I0302 19:01:09.921437 22732373614720 run.py:483] Algo bellman_ford step 5853 current loss 0.610807, current_train_items 187328.
I0302 19:01:09.955400 22732373614720 run.py:483] Algo bellman_ford step 5854 current loss 0.984722, current_train_items 187360.
I0302 19:01:09.975042 22732373614720 run.py:483] Algo bellman_ford step 5855 current loss 0.363396, current_train_items 187392.
I0302 19:01:09.990758 22732373614720 run.py:483] Algo bellman_ford step 5856 current loss 0.583826, current_train_items 187424.
I0302 19:01:10.014443 22732373614720 run.py:483] Algo bellman_ford step 5857 current loss 0.615417, current_train_items 187456.
I0302 19:01:10.043810 22732373614720 run.py:483] Algo bellman_ford step 5858 current loss 0.638968, current_train_items 187488.
I0302 19:01:10.076718 22732373614720 run.py:483] Algo bellman_ford step 5859 current loss 1.004981, current_train_items 187520.
I0302 19:01:10.096480 22732373614720 run.py:483] Algo bellman_ford step 5860 current loss 0.373669, current_train_items 187552.
I0302 19:01:10.112763 22732373614720 run.py:483] Algo bellman_ford step 5861 current loss 0.490355, current_train_items 187584.
I0302 19:01:10.134749 22732373614720 run.py:483] Algo bellman_ford step 5862 current loss 0.623694, current_train_items 187616.
I0302 19:01:10.165788 22732373614720 run.py:483] Algo bellman_ford step 5863 current loss 0.691212, current_train_items 187648.
I0302 19:01:10.199300 22732373614720 run.py:483] Algo bellman_ford step 5864 current loss 0.949015, current_train_items 187680.
I0302 19:01:10.218587 22732373614720 run.py:483] Algo bellman_ford step 5865 current loss 0.367740, current_train_items 187712.
I0302 19:01:10.235076 22732373614720 run.py:483] Algo bellman_ford step 5866 current loss 0.570584, current_train_items 187744.
I0302 19:01:10.258280 22732373614720 run.py:483] Algo bellman_ford step 5867 current loss 0.717276, current_train_items 187776.
I0302 19:01:10.288331 22732373614720 run.py:483] Algo bellman_ford step 5868 current loss 0.813308, current_train_items 187808.
I0302 19:01:10.322909 22732373614720 run.py:483] Algo bellman_ford step 5869 current loss 0.893544, current_train_items 187840.
I0302 19:01:10.342797 22732373614720 run.py:483] Algo bellman_ford step 5870 current loss 0.321125, current_train_items 187872.
I0302 19:01:10.358904 22732373614720 run.py:483] Algo bellman_ford step 5871 current loss 0.476587, current_train_items 187904.
I0302 19:01:10.381220 22732373614720 run.py:483] Algo bellman_ford step 5872 current loss 0.688581, current_train_items 187936.
I0302 19:01:10.411025 22732373614720 run.py:483] Algo bellman_ford step 5873 current loss 0.735297, current_train_items 187968.
I0302 19:01:10.444064 22732373614720 run.py:483] Algo bellman_ford step 5874 current loss 1.025850, current_train_items 188000.
I0302 19:01:10.463928 22732373614720 run.py:483] Algo bellman_ford step 5875 current loss 0.331065, current_train_items 188032.
I0302 19:01:10.479902 22732373614720 run.py:483] Algo bellman_ford step 5876 current loss 0.484450, current_train_items 188064.
I0302 19:01:10.501958 22732373614720 run.py:483] Algo bellman_ford step 5877 current loss 0.686197, current_train_items 188096.
I0302 19:01:10.532136 22732373614720 run.py:483] Algo bellman_ford step 5878 current loss 0.842556, current_train_items 188128.
I0302 19:01:10.565947 22732373614720 run.py:483] Algo bellman_ford step 5879 current loss 1.128711, current_train_items 188160.
I0302 19:01:10.585763 22732373614720 run.py:483] Algo bellman_ford step 5880 current loss 0.382358, current_train_items 188192.
I0302 19:01:10.601683 22732373614720 run.py:483] Algo bellman_ford step 5881 current loss 0.565587, current_train_items 188224.
I0302 19:01:10.625092 22732373614720 run.py:483] Algo bellman_ford step 5882 current loss 0.667900, current_train_items 188256.
I0302 19:01:10.655382 22732373614720 run.py:483] Algo bellman_ford step 5883 current loss 0.681494, current_train_items 188288.
I0302 19:01:10.689007 22732373614720 run.py:483] Algo bellman_ford step 5884 current loss 0.812222, current_train_items 188320.
I0302 19:01:10.709018 22732373614720 run.py:483] Algo bellman_ford step 5885 current loss 0.370888, current_train_items 188352.
I0302 19:01:10.724908 22732373614720 run.py:483] Algo bellman_ford step 5886 current loss 0.599975, current_train_items 188384.
I0302 19:01:10.747529 22732373614720 run.py:483] Algo bellman_ford step 5887 current loss 0.629941, current_train_items 188416.
I0302 19:01:10.777162 22732373614720 run.py:483] Algo bellman_ford step 5888 current loss 0.639097, current_train_items 188448.
I0302 19:01:10.808588 22732373614720 run.py:483] Algo bellman_ford step 5889 current loss 0.802398, current_train_items 188480.
I0302 19:01:10.828631 22732373614720 run.py:483] Algo bellman_ford step 5890 current loss 0.381974, current_train_items 188512.
I0302 19:01:10.844662 22732373614720 run.py:483] Algo bellman_ford step 5891 current loss 0.563847, current_train_items 188544.
I0302 19:01:10.867677 22732373614720 run.py:483] Algo bellman_ford step 5892 current loss 0.678054, current_train_items 188576.
I0302 19:01:10.897025 22732373614720 run.py:483] Algo bellman_ford step 5893 current loss 0.612097, current_train_items 188608.
I0302 19:01:10.927927 22732373614720 run.py:483] Algo bellman_ford step 5894 current loss 0.709937, current_train_items 188640.
I0302 19:01:10.947358 22732373614720 run.py:483] Algo bellman_ford step 5895 current loss 0.304155, current_train_items 188672.
I0302 19:01:10.963364 22732373614720 run.py:483] Algo bellman_ford step 5896 current loss 0.577383, current_train_items 188704.
I0302 19:01:10.986772 22732373614720 run.py:483] Algo bellman_ford step 5897 current loss 0.713314, current_train_items 188736.
I0302 19:01:11.017506 22732373614720 run.py:483] Algo bellman_ford step 5898 current loss 0.824404, current_train_items 188768.
I0302 19:01:11.049854 22732373614720 run.py:483] Algo bellman_ford step 5899 current loss 0.806841, current_train_items 188800.
I0302 19:01:11.069634 22732373614720 run.py:483] Algo bellman_ford step 5900 current loss 0.336882, current_train_items 188832.
I0302 19:01:11.077569 22732373614720 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0302 19:01:11.077679 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0302 19:01:11.093967 22732373614720 run.py:483] Algo bellman_ford step 5901 current loss 0.468406, current_train_items 188864.
I0302 19:01:11.117832 22732373614720 run.py:483] Algo bellman_ford step 5902 current loss 0.665474, current_train_items 188896.
I0302 19:01:11.148889 22732373614720 run.py:483] Algo bellman_ford step 5903 current loss 0.745175, current_train_items 188928.
I0302 19:01:11.180752 22732373614720 run.py:483] Algo bellman_ford step 5904 current loss 0.927316, current_train_items 188960.
I0302 19:01:11.200528 22732373614720 run.py:483] Algo bellman_ford step 5905 current loss 0.338819, current_train_items 188992.
I0302 19:01:11.216548 22732373614720 run.py:483] Algo bellman_ford step 5906 current loss 0.536527, current_train_items 189024.
I0302 19:01:11.239195 22732373614720 run.py:483] Algo bellman_ford step 5907 current loss 0.643697, current_train_items 189056.
I0302 19:01:11.269700 22732373614720 run.py:483] Algo bellman_ford step 5908 current loss 0.655934, current_train_items 189088.
I0302 19:01:11.301397 22732373614720 run.py:483] Algo bellman_ford step 5909 current loss 0.823934, current_train_items 189120.
I0302 19:01:11.320922 22732373614720 run.py:483] Algo bellman_ford step 5910 current loss 0.333697, current_train_items 189152.
I0302 19:01:11.337172 22732373614720 run.py:483] Algo bellman_ford step 5911 current loss 0.626330, current_train_items 189184.
I0302 19:01:11.360416 22732373614720 run.py:483] Algo bellman_ford step 5912 current loss 0.741848, current_train_items 189216.
I0302 19:01:11.389953 22732373614720 run.py:483] Algo bellman_ford step 5913 current loss 0.779872, current_train_items 189248.
I0302 19:01:11.421782 22732373614720 run.py:483] Algo bellman_ford step 5914 current loss 0.746609, current_train_items 189280.
I0302 19:01:11.441487 22732373614720 run.py:483] Algo bellman_ford step 5915 current loss 0.380255, current_train_items 189312.
I0302 19:01:11.457632 22732373614720 run.py:483] Algo bellman_ford step 5916 current loss 0.496610, current_train_items 189344.
I0302 19:01:11.480540 22732373614720 run.py:483] Algo bellman_ford step 5917 current loss 0.618714, current_train_items 189376.
I0302 19:01:11.510057 22732373614720 run.py:483] Algo bellman_ford step 5918 current loss 0.671240, current_train_items 189408.
I0302 19:01:11.543480 22732373614720 run.py:483] Algo bellman_ford step 5919 current loss 0.826902, current_train_items 189440.
I0302 19:01:11.562881 22732373614720 run.py:483] Algo bellman_ford step 5920 current loss 0.312045, current_train_items 189472.
I0302 19:01:11.578839 22732373614720 run.py:483] Algo bellman_ford step 5921 current loss 0.522595, current_train_items 189504.
I0302 19:01:11.601329 22732373614720 run.py:483] Algo bellman_ford step 5922 current loss 0.631476, current_train_items 189536.
I0302 19:01:11.630262 22732373614720 run.py:483] Algo bellman_ford step 5923 current loss 0.654992, current_train_items 189568.
I0302 19:01:11.662755 22732373614720 run.py:483] Algo bellman_ford step 5924 current loss 0.827272, current_train_items 189600.
I0302 19:01:11.681809 22732373614720 run.py:483] Algo bellman_ford step 5925 current loss 0.257458, current_train_items 189632.
I0302 19:01:11.697581 22732373614720 run.py:483] Algo bellman_ford step 5926 current loss 0.560222, current_train_items 189664.
I0302 19:01:11.719728 22732373614720 run.py:483] Algo bellman_ford step 5927 current loss 0.648286, current_train_items 189696.
I0302 19:01:11.750324 22732373614720 run.py:483] Algo bellman_ford step 5928 current loss 0.764745, current_train_items 189728.
I0302 19:01:11.781952 22732373614720 run.py:483] Algo bellman_ford step 5929 current loss 0.795351, current_train_items 189760.
I0302 19:01:11.801179 22732373614720 run.py:483] Algo bellman_ford step 5930 current loss 0.322325, current_train_items 189792.
I0302 19:01:11.817345 22732373614720 run.py:483] Algo bellman_ford step 5931 current loss 0.508272, current_train_items 189824.
I0302 19:01:11.840075 22732373614720 run.py:483] Algo bellman_ford step 5932 current loss 0.721190, current_train_items 189856.
I0302 19:01:11.869148 22732373614720 run.py:483] Algo bellman_ford step 5933 current loss 0.687084, current_train_items 189888.
I0302 19:01:11.902968 22732373614720 run.py:483] Algo bellman_ford step 5934 current loss 0.861223, current_train_items 189920.
I0302 19:01:11.922499 22732373614720 run.py:483] Algo bellman_ford step 5935 current loss 0.372125, current_train_items 189952.
I0302 19:01:11.938104 22732373614720 run.py:483] Algo bellman_ford step 5936 current loss 0.499281, current_train_items 189984.
I0302 19:01:11.960948 22732373614720 run.py:483] Algo bellman_ford step 5937 current loss 0.676115, current_train_items 190016.
I0302 19:01:11.991996 22732373614720 run.py:483] Algo bellman_ford step 5938 current loss 0.727143, current_train_items 190048.
I0302 19:01:12.023101 22732373614720 run.py:483] Algo bellman_ford step 5939 current loss 0.865165, current_train_items 190080.
I0302 19:01:12.042645 22732373614720 run.py:483] Algo bellman_ford step 5940 current loss 0.342570, current_train_items 190112.
I0302 19:01:12.058368 22732373614720 run.py:483] Algo bellman_ford step 5941 current loss 0.495451, current_train_items 190144.
I0302 19:01:12.081538 22732373614720 run.py:483] Algo bellman_ford step 5942 current loss 0.650198, current_train_items 190176.
I0302 19:01:12.112016 22732373614720 run.py:483] Algo bellman_ford step 5943 current loss 0.684940, current_train_items 190208.
I0302 19:01:12.144986 22732373614720 run.py:483] Algo bellman_ford step 5944 current loss 0.745079, current_train_items 190240.
I0302 19:01:12.164637 22732373614720 run.py:483] Algo bellman_ford step 5945 current loss 0.285310, current_train_items 190272.
I0302 19:01:12.180831 22732373614720 run.py:483] Algo bellman_ford step 5946 current loss 0.602290, current_train_items 190304.
I0302 19:01:12.204058 22732373614720 run.py:483] Algo bellman_ford step 5947 current loss 0.661338, current_train_items 190336.
I0302 19:01:12.233162 22732373614720 run.py:483] Algo bellman_ford step 5948 current loss 0.671360, current_train_items 190368.
I0302 19:01:12.265921 22732373614720 run.py:483] Algo bellman_ford step 5949 current loss 0.855620, current_train_items 190400.
I0302 19:01:12.285086 22732373614720 run.py:483] Algo bellman_ford step 5950 current loss 0.263331, current_train_items 190432.
I0302 19:01:12.293428 22732373614720 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.943359375, 'score': 0.943359375, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0302 19:01:12.293539 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.939, current avg val score is 0.943, val scores are: bellman_ford: 0.943
I0302 19:01:12.324172 22732373614720 run.py:483] Algo bellman_ford step 5951 current loss 0.517138, current_train_items 190464.
I0302 19:01:12.347721 22732373614720 run.py:483] Algo bellman_ford step 5952 current loss 0.675737, current_train_items 190496.
I0302 19:01:12.378605 22732373614720 run.py:483] Algo bellman_ford step 5953 current loss 0.813298, current_train_items 190528.
I0302 19:01:12.412109 22732373614720 run.py:483] Algo bellman_ford step 5954 current loss 0.843022, current_train_items 190560.
I0302 19:01:12.432619 22732373614720 run.py:483] Algo bellman_ford step 5955 current loss 0.338337, current_train_items 190592.
I0302 19:01:12.448870 22732373614720 run.py:483] Algo bellman_ford step 5956 current loss 0.499534, current_train_items 190624.
I0302 19:01:12.471113 22732373614720 run.py:483] Algo bellman_ford step 5957 current loss 0.608919, current_train_items 190656.
I0302 19:01:12.501013 22732373614720 run.py:483] Algo bellman_ford step 5958 current loss 0.654679, current_train_items 190688.
I0302 19:01:12.533814 22732373614720 run.py:483] Algo bellman_ford step 5959 current loss 0.772070, current_train_items 190720.
I0302 19:01:12.553963 22732373614720 run.py:483] Algo bellman_ford step 5960 current loss 0.304971, current_train_items 190752.
I0302 19:01:12.570082 22732373614720 run.py:483] Algo bellman_ford step 5961 current loss 0.553566, current_train_items 190784.
I0302 19:01:12.593449 22732373614720 run.py:483] Algo bellman_ford step 5962 current loss 0.599899, current_train_items 190816.
I0302 19:01:12.622022 22732373614720 run.py:483] Algo bellman_ford step 5963 current loss 0.649891, current_train_items 190848.
I0302 19:01:12.654510 22732373614720 run.py:483] Algo bellman_ford step 5964 current loss 0.971430, current_train_items 190880.
I0302 19:01:12.673914 22732373614720 run.py:483] Algo bellman_ford step 5965 current loss 0.309841, current_train_items 190912.
I0302 19:01:12.689892 22732373614720 run.py:483] Algo bellman_ford step 5966 current loss 0.478934, current_train_items 190944.
I0302 19:01:12.713327 22732373614720 run.py:483] Algo bellman_ford step 5967 current loss 0.646435, current_train_items 190976.
I0302 19:01:12.743222 22732373614720 run.py:483] Algo bellman_ford step 5968 current loss 0.728290, current_train_items 191008.
I0302 19:01:12.776814 22732373614720 run.py:483] Algo bellman_ford step 5969 current loss 0.865766, current_train_items 191040.
I0302 19:01:12.796543 22732373614720 run.py:483] Algo bellman_ford step 5970 current loss 0.347583, current_train_items 191072.
I0302 19:01:12.812556 22732373614720 run.py:483] Algo bellman_ford step 5971 current loss 0.531167, current_train_items 191104.
I0302 19:01:12.835006 22732373614720 run.py:483] Algo bellman_ford step 5972 current loss 0.614939, current_train_items 191136.
I0302 19:01:12.865460 22732373614720 run.py:483] Algo bellman_ford step 5973 current loss 0.805745, current_train_items 191168.
I0302 19:01:12.897871 22732373614720 run.py:483] Algo bellman_ford step 5974 current loss 0.917166, current_train_items 191200.
I0302 19:01:12.917463 22732373614720 run.py:483] Algo bellman_ford step 5975 current loss 0.460985, current_train_items 191232.
I0302 19:01:12.933211 22732373614720 run.py:483] Algo bellman_ford step 5976 current loss 0.526305, current_train_items 191264.
I0302 19:01:12.955415 22732373614720 run.py:483] Algo bellman_ford step 5977 current loss 0.670795, current_train_items 191296.
I0302 19:01:12.985707 22732373614720 run.py:483] Algo bellman_ford step 5978 current loss 0.879748, current_train_items 191328.
I0302 19:01:13.017179 22732373614720 run.py:483] Algo bellman_ford step 5979 current loss 0.847000, current_train_items 191360.
I0302 19:01:13.036201 22732373614720 run.py:483] Algo bellman_ford step 5980 current loss 0.284746, current_train_items 191392.
I0302 19:01:13.052808 22732373614720 run.py:483] Algo bellman_ford step 5981 current loss 0.571700, current_train_items 191424.
I0302 19:01:13.075071 22732373614720 run.py:483] Algo bellman_ford step 5982 current loss 0.638959, current_train_items 191456.
I0302 19:01:13.104628 22732373614720 run.py:483] Algo bellman_ford step 5983 current loss 0.675172, current_train_items 191488.
I0302 19:01:13.136634 22732373614720 run.py:483] Algo bellman_ford step 5984 current loss 0.985981, current_train_items 191520.
I0302 19:01:13.156276 22732373614720 run.py:483] Algo bellman_ford step 5985 current loss 0.355766, current_train_items 191552.
I0302 19:01:13.172765 22732373614720 run.py:483] Algo bellman_ford step 5986 current loss 0.502374, current_train_items 191584.
I0302 19:01:13.195381 22732373614720 run.py:483] Algo bellman_ford step 5987 current loss 0.668639, current_train_items 191616.
I0302 19:01:13.224569 22732373614720 run.py:483] Algo bellman_ford step 5988 current loss 0.795649, current_train_items 191648.
I0302 19:01:13.256724 22732373614720 run.py:483] Algo bellman_ford step 5989 current loss 0.871571, current_train_items 191680.
I0302 19:01:13.276329 22732373614720 run.py:483] Algo bellman_ford step 5990 current loss 0.291485, current_train_items 191712.
I0302 19:01:13.291804 22732373614720 run.py:483] Algo bellman_ford step 5991 current loss 0.600043, current_train_items 191744.
I0302 19:01:13.314787 22732373614720 run.py:483] Algo bellman_ford step 5992 current loss 0.822122, current_train_items 191776.
I0302 19:01:13.346228 22732373614720 run.py:483] Algo bellman_ford step 5993 current loss 0.899361, current_train_items 191808.
I0302 19:01:13.378709 22732373614720 run.py:483] Algo bellman_ford step 5994 current loss 0.810139, current_train_items 191840.
I0302 19:01:13.397843 22732373614720 run.py:483] Algo bellman_ford step 5995 current loss 0.327085, current_train_items 191872.
I0302 19:01:13.413807 22732373614720 run.py:483] Algo bellman_ford step 5996 current loss 0.451252, current_train_items 191904.
I0302 19:01:13.437114 22732373614720 run.py:483] Algo bellman_ford step 5997 current loss 0.799373, current_train_items 191936.
I0302 19:01:13.466971 22732373614720 run.py:483] Algo bellman_ford step 5998 current loss 0.906495, current_train_items 191968.
I0302 19:01:13.498620 22732373614720 run.py:483] Algo bellman_ford step 5999 current loss 0.880277, current_train_items 192000.
I0302 19:01:13.518591 22732373614720 run.py:483] Algo bellman_ford step 6000 current loss 0.334750, current_train_items 192032.
I0302 19:01:13.526443 22732373614720 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0302 19:01:13.526552 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.943, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:01:13.543250 22732373614720 run.py:483] Algo bellman_ford step 6001 current loss 0.483401, current_train_items 192064.
I0302 19:01:13.567810 22732373614720 run.py:483] Algo bellman_ford step 6002 current loss 0.679853, current_train_items 192096.
I0302 19:01:13.597317 22732373614720 run.py:483] Algo bellman_ford step 6003 current loss 0.670028, current_train_items 192128.
I0302 19:01:13.629714 22732373614720 run.py:483] Algo bellman_ford step 6004 current loss 0.949289, current_train_items 192160.
I0302 19:01:13.649689 22732373614720 run.py:483] Algo bellman_ford step 6005 current loss 0.266370, current_train_items 192192.
I0302 19:01:13.665582 22732373614720 run.py:483] Algo bellman_ford step 6006 current loss 0.452101, current_train_items 192224.
I0302 19:01:13.690794 22732373614720 run.py:483] Algo bellman_ford step 6007 current loss 0.773127, current_train_items 192256.
I0302 19:01:13.722973 22732373614720 run.py:483] Algo bellman_ford step 6008 current loss 0.929392, current_train_items 192288.
I0302 19:01:13.757421 22732373614720 run.py:483] Algo bellman_ford step 6009 current loss 0.866757, current_train_items 192320.
I0302 19:01:13.777234 22732373614720 run.py:483] Algo bellman_ford step 6010 current loss 0.321652, current_train_items 192352.
I0302 19:01:13.793456 22732373614720 run.py:483] Algo bellman_ford step 6011 current loss 0.505563, current_train_items 192384.
I0302 19:01:13.817137 22732373614720 run.py:483] Algo bellman_ford step 6012 current loss 0.654355, current_train_items 192416.
I0302 19:01:13.847354 22732373614720 run.py:483] Algo bellman_ford step 6013 current loss 0.746747, current_train_items 192448.
I0302 19:01:13.882621 22732373614720 run.py:483] Algo bellman_ford step 6014 current loss 1.016018, current_train_items 192480.
I0302 19:01:13.902118 22732373614720 run.py:483] Algo bellman_ford step 6015 current loss 0.344022, current_train_items 192512.
I0302 19:01:13.918368 22732373614720 run.py:483] Algo bellman_ford step 6016 current loss 0.556973, current_train_items 192544.
I0302 19:01:13.941348 22732373614720 run.py:483] Algo bellman_ford step 6017 current loss 0.625472, current_train_items 192576.
I0302 19:01:13.972539 22732373614720 run.py:483] Algo bellman_ford step 6018 current loss 0.775062, current_train_items 192608.
I0302 19:01:14.005638 22732373614720 run.py:483] Algo bellman_ford step 6019 current loss 0.903553, current_train_items 192640.
I0302 19:01:14.025384 22732373614720 run.py:483] Algo bellman_ford step 6020 current loss 0.328830, current_train_items 192672.
I0302 19:01:14.041718 22732373614720 run.py:483] Algo bellman_ford step 6021 current loss 0.539953, current_train_items 192704.
I0302 19:01:14.065246 22732373614720 run.py:483] Algo bellman_ford step 6022 current loss 0.648164, current_train_items 192736.
I0302 19:01:14.097513 22732373614720 run.py:483] Algo bellman_ford step 6023 current loss 0.928983, current_train_items 192768.
I0302 19:01:14.130669 22732373614720 run.py:483] Algo bellman_ford step 6024 current loss 0.787678, current_train_items 192800.
I0302 19:01:14.150225 22732373614720 run.py:483] Algo bellman_ford step 6025 current loss 0.309194, current_train_items 192832.
I0302 19:01:14.166404 22732373614720 run.py:483] Algo bellman_ford step 6026 current loss 0.688784, current_train_items 192864.
I0302 19:01:14.189582 22732373614720 run.py:483] Algo bellman_ford step 6027 current loss 0.694534, current_train_items 192896.
I0302 19:01:14.219660 22732373614720 run.py:483] Algo bellman_ford step 6028 current loss 0.862411, current_train_items 192928.
I0302 19:01:14.252833 22732373614720 run.py:483] Algo bellman_ford step 6029 current loss 0.883793, current_train_items 192960.
I0302 19:01:14.272496 22732373614720 run.py:483] Algo bellman_ford step 6030 current loss 0.307345, current_train_items 192992.
I0302 19:01:14.288347 22732373614720 run.py:483] Algo bellman_ford step 6031 current loss 0.446374, current_train_items 193024.
I0302 19:01:14.311481 22732373614720 run.py:483] Algo bellman_ford step 6032 current loss 0.520069, current_train_items 193056.
I0302 19:01:14.343402 22732373614720 run.py:483] Algo bellman_ford step 6033 current loss 0.760778, current_train_items 193088.
I0302 19:01:14.376127 22732373614720 run.py:483] Algo bellman_ford step 6034 current loss 0.802540, current_train_items 193120.
I0302 19:01:14.395754 22732373614720 run.py:483] Algo bellman_ford step 6035 current loss 0.302640, current_train_items 193152.
I0302 19:01:14.411644 22732373614720 run.py:483] Algo bellman_ford step 6036 current loss 0.470597, current_train_items 193184.
I0302 19:01:14.434960 22732373614720 run.py:483] Algo bellman_ford step 6037 current loss 0.682941, current_train_items 193216.
I0302 19:01:14.466076 22732373614720 run.py:483] Algo bellman_ford step 6038 current loss 0.783168, current_train_items 193248.
I0302 19:01:14.498021 22732373614720 run.py:483] Algo bellman_ford step 6039 current loss 0.780835, current_train_items 193280.
I0302 19:01:14.517253 22732373614720 run.py:483] Algo bellman_ford step 6040 current loss 0.386820, current_train_items 193312.
I0302 19:01:14.533231 22732373614720 run.py:483] Algo bellman_ford step 6041 current loss 0.508123, current_train_items 193344.
I0302 19:01:14.557034 22732373614720 run.py:483] Algo bellman_ford step 6042 current loss 0.680730, current_train_items 193376.
I0302 19:01:14.587610 22732373614720 run.py:483] Algo bellman_ford step 6043 current loss 0.670732, current_train_items 193408.
I0302 19:01:14.622913 22732373614720 run.py:483] Algo bellman_ford step 6044 current loss 0.857221, current_train_items 193440.
I0302 19:01:14.642615 22732373614720 run.py:483] Algo bellman_ford step 6045 current loss 0.295652, current_train_items 193472.
I0302 19:01:14.658994 22732373614720 run.py:483] Algo bellman_ford step 6046 current loss 0.510339, current_train_items 193504.
I0302 19:01:14.681925 22732373614720 run.py:483] Algo bellman_ford step 6047 current loss 0.689517, current_train_items 193536.
I0302 19:01:14.713725 22732373614720 run.py:483] Algo bellman_ford step 6048 current loss 0.837064, current_train_items 193568.
I0302 19:01:14.747572 22732373614720 run.py:483] Algo bellman_ford step 6049 current loss 0.826796, current_train_items 193600.
I0302 19:01:14.766916 22732373614720 run.py:483] Algo bellman_ford step 6050 current loss 0.300329, current_train_items 193632.
I0302 19:01:14.775112 22732373614720 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.9443359375, 'score': 0.9443359375, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0302 19:01:14.775246 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.943, current avg val score is 0.944, val scores are: bellman_ford: 0.944
I0302 19:01:14.805252 22732373614720 run.py:483] Algo bellman_ford step 6051 current loss 0.478419, current_train_items 193664.
I0302 19:01:14.829379 22732373614720 run.py:483] Algo bellman_ford step 6052 current loss 0.641911, current_train_items 193696.
I0302 19:01:14.860211 22732373614720 run.py:483] Algo bellman_ford step 6053 current loss 0.673856, current_train_items 193728.
I0302 19:01:14.894652 22732373614720 run.py:483] Algo bellman_ford step 6054 current loss 0.878400, current_train_items 193760.
I0302 19:01:14.914852 22732373614720 run.py:483] Algo bellman_ford step 6055 current loss 0.436723, current_train_items 193792.
I0302 19:01:14.930562 22732373614720 run.py:483] Algo bellman_ford step 6056 current loss 0.537263, current_train_items 193824.
I0302 19:01:14.953071 22732373614720 run.py:483] Algo bellman_ford step 6057 current loss 0.671431, current_train_items 193856.
I0302 19:01:14.982944 22732373614720 run.py:483] Algo bellman_ford step 6058 current loss 0.726944, current_train_items 193888.
I0302 19:01:15.013368 22732373614720 run.py:483] Algo bellman_ford step 6059 current loss 0.725573, current_train_items 193920.
I0302 19:01:15.033761 22732373614720 run.py:483] Algo bellman_ford step 6060 current loss 0.309354, current_train_items 193952.
I0302 19:01:15.049981 22732373614720 run.py:483] Algo bellman_ford step 6061 current loss 0.573544, current_train_items 193984.
I0302 19:01:15.071356 22732373614720 run.py:483] Algo bellman_ford step 6062 current loss 0.630276, current_train_items 194016.
I0302 19:01:15.100233 22732373614720 run.py:483] Algo bellman_ford step 6063 current loss 0.763971, current_train_items 194048.
I0302 19:01:15.133464 22732373614720 run.py:483] Algo bellman_ford step 6064 current loss 0.843848, current_train_items 194080.
I0302 19:01:15.153357 22732373614720 run.py:483] Algo bellman_ford step 6065 current loss 0.345042, current_train_items 194112.
I0302 19:01:15.169428 22732373614720 run.py:483] Algo bellman_ford step 6066 current loss 0.554202, current_train_items 194144.
I0302 19:01:15.192950 22732373614720 run.py:483] Algo bellman_ford step 6067 current loss 0.714221, current_train_items 194176.
I0302 19:01:15.224306 22732373614720 run.py:483] Algo bellman_ford step 6068 current loss 0.771340, current_train_items 194208.
I0302 19:01:15.256483 22732373614720 run.py:483] Algo bellman_ford step 6069 current loss 0.898599, current_train_items 194240.
I0302 19:01:15.276281 22732373614720 run.py:483] Algo bellman_ford step 6070 current loss 0.383373, current_train_items 194272.
I0302 19:01:15.292239 22732373614720 run.py:483] Algo bellman_ford step 6071 current loss 0.608739, current_train_items 194304.
I0302 19:01:15.315176 22732373614720 run.py:483] Algo bellman_ford step 6072 current loss 0.712828, current_train_items 194336.
I0302 19:01:15.344717 22732373614720 run.py:483] Algo bellman_ford step 6073 current loss 0.759296, current_train_items 194368.
I0302 19:01:15.377928 22732373614720 run.py:483] Algo bellman_ford step 6074 current loss 0.910162, current_train_items 194400.
I0302 19:01:15.397726 22732373614720 run.py:483] Algo bellman_ford step 6075 current loss 0.293853, current_train_items 194432.
I0302 19:01:15.414020 22732373614720 run.py:483] Algo bellman_ford step 6076 current loss 0.480526, current_train_items 194464.
I0302 19:01:15.435680 22732373614720 run.py:483] Algo bellman_ford step 6077 current loss 0.622781, current_train_items 194496.
I0302 19:01:15.465722 22732373614720 run.py:483] Algo bellman_ford step 6078 current loss 0.736393, current_train_items 194528.
I0302 19:01:15.499282 22732373614720 run.py:483] Algo bellman_ford step 6079 current loss 0.924462, current_train_items 194560.
I0302 19:01:15.518791 22732373614720 run.py:483] Algo bellman_ford step 6080 current loss 0.302231, current_train_items 194592.
I0302 19:01:15.534988 22732373614720 run.py:483] Algo bellman_ford step 6081 current loss 0.454367, current_train_items 194624.
I0302 19:01:15.557534 22732373614720 run.py:483] Algo bellman_ford step 6082 current loss 0.609911, current_train_items 194656.
I0302 19:01:15.589057 22732373614720 run.py:483] Algo bellman_ford step 6083 current loss 0.754912, current_train_items 194688.
I0302 19:01:15.620463 22732373614720 run.py:483] Algo bellman_ford step 6084 current loss 0.725665, current_train_items 194720.
I0302 19:01:15.640569 22732373614720 run.py:483] Algo bellman_ford step 6085 current loss 0.305983, current_train_items 194752.
I0302 19:01:15.656795 22732373614720 run.py:483] Algo bellman_ford step 6086 current loss 0.577541, current_train_items 194784.
I0302 19:01:15.679338 22732373614720 run.py:483] Algo bellman_ford step 6087 current loss 0.730311, current_train_items 194816.
I0302 19:01:15.708828 22732373614720 run.py:483] Algo bellman_ford step 6088 current loss 0.792699, current_train_items 194848.
I0302 19:01:15.742370 22732373614720 run.py:483] Algo bellman_ford step 6089 current loss 1.081430, current_train_items 194880.
I0302 19:01:15.762297 22732373614720 run.py:483] Algo bellman_ford step 6090 current loss 0.413155, current_train_items 194912.
I0302 19:01:15.778443 22732373614720 run.py:483] Algo bellman_ford step 6091 current loss 0.562762, current_train_items 194944.
I0302 19:01:15.800936 22732373614720 run.py:483] Algo bellman_ford step 6092 current loss 0.654660, current_train_items 194976.
I0302 19:01:15.831007 22732373614720 run.py:483] Algo bellman_ford step 6093 current loss 0.666760, current_train_items 195008.
I0302 19:01:15.862640 22732373614720 run.py:483] Algo bellman_ford step 6094 current loss 0.855179, current_train_items 195040.
I0302 19:01:15.882023 22732373614720 run.py:483] Algo bellman_ford step 6095 current loss 0.304777, current_train_items 195072.
I0302 19:01:15.898072 22732373614720 run.py:483] Algo bellman_ford step 6096 current loss 0.416430, current_train_items 195104.
I0302 19:01:15.921466 22732373614720 run.py:483] Algo bellman_ford step 6097 current loss 0.669023, current_train_items 195136.
I0302 19:01:15.950784 22732373614720 run.py:483] Algo bellman_ford step 6098 current loss 0.651146, current_train_items 195168.
I0302 19:01:15.983924 22732373614720 run.py:483] Algo bellman_ford step 6099 current loss 0.834324, current_train_items 195200.
I0302 19:01:16.003562 22732373614720 run.py:483] Algo bellman_ford step 6100 current loss 0.230471, current_train_items 195232.
I0302 19:01:16.011456 22732373614720 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0302 19:01:16.011568 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.944, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:01:16.027696 22732373614720 run.py:483] Algo bellman_ford step 6101 current loss 0.483676, current_train_items 195264.
I0302 19:01:16.052209 22732373614720 run.py:483] Algo bellman_ford step 6102 current loss 0.617926, current_train_items 195296.
I0302 19:01:16.084130 22732373614720 run.py:483] Algo bellman_ford step 6103 current loss 0.795154, current_train_items 195328.
I0302 19:01:16.116737 22732373614720 run.py:483] Algo bellman_ford step 6104 current loss 0.795377, current_train_items 195360.
I0302 19:01:16.136558 22732373614720 run.py:483] Algo bellman_ford step 6105 current loss 0.345715, current_train_items 195392.
I0302 19:01:16.151674 22732373614720 run.py:483] Algo bellman_ford step 6106 current loss 0.579430, current_train_items 195424.
I0302 19:01:16.174739 22732373614720 run.py:483] Algo bellman_ford step 6107 current loss 0.697825, current_train_items 195456.
I0302 19:01:16.205610 22732373614720 run.py:483] Algo bellman_ford step 6108 current loss 0.642085, current_train_items 195488.
I0302 19:01:16.237694 22732373614720 run.py:483] Algo bellman_ford step 6109 current loss 0.778234, current_train_items 195520.
I0302 19:01:16.257491 22732373614720 run.py:483] Algo bellman_ford step 6110 current loss 0.292256, current_train_items 195552.
I0302 19:01:16.273285 22732373614720 run.py:483] Algo bellman_ford step 6111 current loss 0.461468, current_train_items 195584.
I0302 19:01:16.296305 22732373614720 run.py:483] Algo bellman_ford step 6112 current loss 0.730602, current_train_items 195616.
I0302 19:01:16.326084 22732373614720 run.py:483] Algo bellman_ford step 6113 current loss 0.712627, current_train_items 195648.
I0302 19:01:16.358951 22732373614720 run.py:483] Algo bellman_ford step 6114 current loss 0.745213, current_train_items 195680.
I0302 19:01:16.378565 22732373614720 run.py:483] Algo bellman_ford step 6115 current loss 0.310849, current_train_items 195712.
I0302 19:01:16.394479 22732373614720 run.py:483] Algo bellman_ford step 6116 current loss 0.465395, current_train_items 195744.
I0302 19:01:16.417769 22732373614720 run.py:483] Algo bellman_ford step 6117 current loss 0.549622, current_train_items 195776.
I0302 19:01:16.448776 22732373614720 run.py:483] Algo bellman_ford step 6118 current loss 0.701355, current_train_items 195808.
I0302 19:01:16.481001 22732373614720 run.py:483] Algo bellman_ford step 6119 current loss 1.108292, current_train_items 195840.
I0302 19:01:16.500417 22732373614720 run.py:483] Algo bellman_ford step 6120 current loss 0.404183, current_train_items 195872.
I0302 19:01:16.516753 22732373614720 run.py:483] Algo bellman_ford step 6121 current loss 0.509533, current_train_items 195904.
I0302 19:01:16.539564 22732373614720 run.py:483] Algo bellman_ford step 6122 current loss 0.584860, current_train_items 195936.
I0302 19:01:16.570979 22732373614720 run.py:483] Algo bellman_ford step 6123 current loss 0.692663, current_train_items 195968.
I0302 19:01:16.603784 22732373614720 run.py:483] Algo bellman_ford step 6124 current loss 0.967737, current_train_items 196000.
I0302 19:01:16.623354 22732373614720 run.py:483] Algo bellman_ford step 6125 current loss 0.315923, current_train_items 196032.
I0302 19:01:16.638837 22732373614720 run.py:483] Algo bellman_ford step 6126 current loss 0.574498, current_train_items 196064.
I0302 19:01:16.663558 22732373614720 run.py:483] Algo bellman_ford step 6127 current loss 0.838865, current_train_items 196096.
I0302 19:01:16.694372 22732373614720 run.py:483] Algo bellman_ford step 6128 current loss 0.694517, current_train_items 196128.
I0302 19:01:16.726503 22732373614720 run.py:483] Algo bellman_ford step 6129 current loss 0.767734, current_train_items 196160.
I0302 19:01:16.745934 22732373614720 run.py:483] Algo bellman_ford step 6130 current loss 0.401792, current_train_items 196192.
I0302 19:01:16.761736 22732373614720 run.py:483] Algo bellman_ford step 6131 current loss 0.466962, current_train_items 196224.
I0302 19:01:16.785465 22732373614720 run.py:483] Algo bellman_ford step 6132 current loss 0.786686, current_train_items 196256.
I0302 19:01:16.814951 22732373614720 run.py:483] Algo bellman_ford step 6133 current loss 0.672595, current_train_items 196288.
I0302 19:01:16.846703 22732373614720 run.py:483] Algo bellman_ford step 6134 current loss 0.698045, current_train_items 196320.
I0302 19:01:16.866002 22732373614720 run.py:483] Algo bellman_ford step 6135 current loss 0.292681, current_train_items 196352.
I0302 19:01:16.881560 22732373614720 run.py:483] Algo bellman_ford step 6136 current loss 0.472493, current_train_items 196384.
I0302 19:01:16.904699 22732373614720 run.py:483] Algo bellman_ford step 6137 current loss 0.672298, current_train_items 196416.
I0302 19:01:16.936035 22732373614720 run.py:483] Algo bellman_ford step 6138 current loss 0.838632, current_train_items 196448.
I0302 19:01:16.968954 22732373614720 run.py:483] Algo bellman_ford step 6139 current loss 0.857900, current_train_items 196480.
I0302 19:01:16.988850 22732373614720 run.py:483] Algo bellman_ford step 6140 current loss 0.360659, current_train_items 196512.
I0302 19:01:17.004777 22732373614720 run.py:483] Algo bellman_ford step 6141 current loss 0.405059, current_train_items 196544.
I0302 19:01:17.028413 22732373614720 run.py:483] Algo bellman_ford step 6142 current loss 0.636355, current_train_items 196576.
I0302 19:01:17.059147 22732373614720 run.py:483] Algo bellman_ford step 6143 current loss 0.678637, current_train_items 196608.
I0302 19:01:17.091634 22732373614720 run.py:483] Algo bellman_ford step 6144 current loss 0.826415, current_train_items 196640.
I0302 19:01:17.110906 22732373614720 run.py:483] Algo bellman_ford step 6145 current loss 0.256814, current_train_items 196672.
I0302 19:01:17.126909 22732373614720 run.py:483] Algo bellman_ford step 6146 current loss 0.563250, current_train_items 196704.
I0302 19:01:17.149413 22732373614720 run.py:483] Algo bellman_ford step 6147 current loss 0.584573, current_train_items 196736.
I0302 19:01:17.179514 22732373614720 run.py:483] Algo bellman_ford step 6148 current loss 0.787044, current_train_items 196768.
I0302 19:01:17.213203 22732373614720 run.py:483] Algo bellman_ford step 6149 current loss 0.849262, current_train_items 196800.
I0302 19:01:17.232857 22732373614720 run.py:483] Algo bellman_ford step 6150 current loss 0.321487, current_train_items 196832.
I0302 19:01:17.241177 22732373614720 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.9453125, 'score': 0.9453125, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0302 19:01:17.241286 22732373614720 run.py:519] Checkpointing best model, best avg val score was 0.944, current avg val score is 0.945, val scores are: bellman_ford: 0.945
I0302 19:01:17.270373 22732373614720 run.py:483] Algo bellman_ford step 6151 current loss 0.484539, current_train_items 196864.
I0302 19:01:17.292973 22732373614720 run.py:483] Algo bellman_ford step 6152 current loss 0.558592, current_train_items 196896.
I0302 19:01:17.322555 22732373614720 run.py:483] Algo bellman_ford step 6153 current loss 0.660124, current_train_items 196928.
I0302 19:01:17.355894 22732373614720 run.py:483] Algo bellman_ford step 6154 current loss 0.696576, current_train_items 196960.
I0302 19:01:17.375803 22732373614720 run.py:483] Algo bellman_ford step 6155 current loss 0.291365, current_train_items 196992.
I0302 19:01:17.391713 22732373614720 run.py:483] Algo bellman_ford step 6156 current loss 0.503308, current_train_items 197024.
I0302 19:01:17.414662 22732373614720 run.py:483] Algo bellman_ford step 6157 current loss 0.548256, current_train_items 197056.
I0302 19:01:17.444102 22732373614720 run.py:483] Algo bellman_ford step 6158 current loss 0.637855, current_train_items 197088.
I0302 19:01:17.476804 22732373614720 run.py:483] Algo bellman_ford step 6159 current loss 0.712801, current_train_items 197120.
I0302 19:01:17.496543 22732373614720 run.py:483] Algo bellman_ford step 6160 current loss 0.364688, current_train_items 197152.
I0302 19:01:17.513068 22732373614720 run.py:483] Algo bellman_ford step 6161 current loss 0.467423, current_train_items 197184.
I0302 19:01:17.535596 22732373614720 run.py:483] Algo bellman_ford step 6162 current loss 0.664035, current_train_items 197216.
I0302 19:01:17.567201 22732373614720 run.py:483] Algo bellman_ford step 6163 current loss 0.726389, current_train_items 197248.
I0302 19:01:17.600262 22732373614720 run.py:483] Algo bellman_ford step 6164 current loss 0.814226, current_train_items 197280.
I0302 19:01:17.620078 22732373614720 run.py:483] Algo bellman_ford step 6165 current loss 0.263506, current_train_items 197312.
I0302 19:01:17.636217 22732373614720 run.py:483] Algo bellman_ford step 6166 current loss 0.483077, current_train_items 197344.
I0302 19:01:17.659614 22732373614720 run.py:483] Algo bellman_ford step 6167 current loss 0.614772, current_train_items 197376.
I0302 19:01:17.690511 22732373614720 run.py:483] Algo bellman_ford step 6168 current loss 0.691108, current_train_items 197408.
I0302 19:01:17.723874 22732373614720 run.py:483] Algo bellman_ford step 6169 current loss 0.827774, current_train_items 197440.
I0302 19:01:17.743900 22732373614720 run.py:483] Algo bellman_ford step 6170 current loss 0.349668, current_train_items 197472.
I0302 19:01:17.759767 22732373614720 run.py:483] Algo bellman_ford step 6171 current loss 0.424713, current_train_items 197504.
I0302 19:01:17.782808 22732373614720 run.py:483] Algo bellman_ford step 6172 current loss 0.577159, current_train_items 197536.
I0302 19:01:17.814337 22732373614720 run.py:483] Algo bellman_ford step 6173 current loss 0.687057, current_train_items 197568.
I0302 19:01:17.847544 22732373614720 run.py:483] Algo bellman_ford step 6174 current loss 0.839429, current_train_items 197600.
I0302 19:01:17.867124 22732373614720 run.py:483] Algo bellman_ford step 6175 current loss 0.282904, current_train_items 197632.
I0302 19:01:17.883500 22732373614720 run.py:483] Algo bellman_ford step 6176 current loss 0.785061, current_train_items 197664.
I0302 19:01:17.905773 22732373614720 run.py:483] Algo bellman_ford step 6177 current loss 0.636605, current_train_items 197696.
I0302 19:01:17.936056 22732373614720 run.py:483] Algo bellman_ford step 6178 current loss 0.729128, current_train_items 197728.
I0302 19:01:17.969362 22732373614720 run.py:483] Algo bellman_ford step 6179 current loss 1.018484, current_train_items 197760.
I0302 19:01:17.988608 22732373614720 run.py:483] Algo bellman_ford step 6180 current loss 0.278207, current_train_items 197792.
I0302 19:01:18.004825 22732373614720 run.py:483] Algo bellman_ford step 6181 current loss 0.485874, current_train_items 197824.
I0302 19:01:18.027555 22732373614720 run.py:483] Algo bellman_ford step 6182 current loss 0.636759, current_train_items 197856.
I0302 19:01:18.058044 22732373614720 run.py:483] Algo bellman_ford step 6183 current loss 0.657713, current_train_items 197888.
I0302 19:01:18.092877 22732373614720 run.py:483] Algo bellman_ford step 6184 current loss 0.835332, current_train_items 197920.
I0302 19:01:18.112591 22732373614720 run.py:483] Algo bellman_ford step 6185 current loss 0.392745, current_train_items 197952.
I0302 19:01:18.128771 22732373614720 run.py:483] Algo bellman_ford step 6186 current loss 0.534868, current_train_items 197984.
I0302 19:01:18.150767 22732373614720 run.py:483] Algo bellman_ford step 6187 current loss 0.671856, current_train_items 198016.
I0302 19:01:18.180802 22732373614720 run.py:483] Algo bellman_ford step 6188 current loss 0.650399, current_train_items 198048.
I0302 19:01:18.214006 22732373614720 run.py:483] Algo bellman_ford step 6189 current loss 1.038793, current_train_items 198080.
I0302 19:01:18.233741 22732373614720 run.py:483] Algo bellman_ford step 6190 current loss 0.351265, current_train_items 198112.
I0302 19:01:18.250013 22732373614720 run.py:483] Algo bellman_ford step 6191 current loss 0.538480, current_train_items 198144.
I0302 19:01:18.273093 22732373614720 run.py:483] Algo bellman_ford step 6192 current loss 0.717670, current_train_items 198176.
I0302 19:01:18.304481 22732373614720 run.py:483] Algo bellman_ford step 6193 current loss 0.771250, current_train_items 198208.
I0302 19:01:18.336370 22732373614720 run.py:483] Algo bellman_ford step 6194 current loss 0.783384, current_train_items 198240.
I0302 19:01:18.355616 22732373614720 run.py:483] Algo bellman_ford step 6195 current loss 0.349162, current_train_items 198272.
I0302 19:01:18.371118 22732373614720 run.py:483] Algo bellman_ford step 6196 current loss 0.473247, current_train_items 198304.
I0302 19:01:18.394924 22732373614720 run.py:483] Algo bellman_ford step 6197 current loss 0.716302, current_train_items 198336.
I0302 19:01:18.424525 22732373614720 run.py:483] Algo bellman_ford step 6198 current loss 0.675380, current_train_items 198368.
I0302 19:01:18.458241 22732373614720 run.py:483] Algo bellman_ford step 6199 current loss 0.831002, current_train_items 198400.
I0302 19:01:18.478089 22732373614720 run.py:483] Algo bellman_ford step 6200 current loss 0.331436, current_train_items 198432.
I0302 19:01:18.485954 22732373614720 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0302 19:01:18.486066 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:01:18.502793 22732373614720 run.py:483] Algo bellman_ford step 6201 current loss 0.434515, current_train_items 198464.
I0302 19:01:18.526992 22732373614720 run.py:483] Algo bellman_ford step 6202 current loss 0.633402, current_train_items 198496.
I0302 19:01:18.556785 22732373614720 run.py:483] Algo bellman_ford step 6203 current loss 0.611465, current_train_items 198528.
I0302 19:01:18.589216 22732373614720 run.py:483] Algo bellman_ford step 6204 current loss 0.715317, current_train_items 198560.
I0302 19:01:18.609097 22732373614720 run.py:483] Algo bellman_ford step 6205 current loss 0.376953, current_train_items 198592.
I0302 19:01:18.624104 22732373614720 run.py:483] Algo bellman_ford step 6206 current loss 0.465842, current_train_items 198624.
I0302 19:01:18.647204 22732373614720 run.py:483] Algo bellman_ford step 6207 current loss 0.655378, current_train_items 198656.
I0302 19:01:18.677371 22732373614720 run.py:483] Algo bellman_ford step 6208 current loss 0.786072, current_train_items 198688.
I0302 19:01:18.711103 22732373614720 run.py:483] Algo bellman_ford step 6209 current loss 0.898176, current_train_items 198720.
I0302 19:01:18.730390 22732373614720 run.py:483] Algo bellman_ford step 6210 current loss 0.307292, current_train_items 198752.
I0302 19:01:18.746357 22732373614720 run.py:483] Algo bellman_ford step 6211 current loss 0.587654, current_train_items 198784.
I0302 19:01:18.768893 22732373614720 run.py:483] Algo bellman_ford step 6212 current loss 0.751946, current_train_items 198816.
I0302 19:01:18.797644 22732373614720 run.py:483] Algo bellman_ford step 6213 current loss 0.674327, current_train_items 198848.
I0302 19:01:18.832129 22732373614720 run.py:483] Algo bellman_ford step 6214 current loss 0.799985, current_train_items 198880.
I0302 19:01:18.851426 22732373614720 run.py:483] Algo bellman_ford step 6215 current loss 0.294757, current_train_items 198912.
I0302 19:01:18.867407 22732373614720 run.py:483] Algo bellman_ford step 6216 current loss 0.447583, current_train_items 198944.
I0302 19:01:18.889734 22732373614720 run.py:483] Algo bellman_ford step 6217 current loss 0.587694, current_train_items 198976.
I0302 19:01:18.920035 22732373614720 run.py:483] Algo bellman_ford step 6218 current loss 0.670672, current_train_items 199008.
I0302 19:01:18.952073 22732373614720 run.py:483] Algo bellman_ford step 6219 current loss 0.848829, current_train_items 199040.
I0302 19:01:18.971646 22732373614720 run.py:483] Algo bellman_ford step 6220 current loss 0.301884, current_train_items 199072.
I0302 19:01:18.987087 22732373614720 run.py:483] Algo bellman_ford step 6221 current loss 0.500628, current_train_items 199104.
I0302 19:01:19.010732 22732373614720 run.py:483] Algo bellman_ford step 6222 current loss 0.703790, current_train_items 199136.
I0302 19:01:19.039826 22732373614720 run.py:483] Algo bellman_ford step 6223 current loss 0.652484, current_train_items 199168.
I0302 19:01:19.073334 22732373614720 run.py:483] Algo bellman_ford step 6224 current loss 0.910337, current_train_items 199200.
I0302 19:01:19.092735 22732373614720 run.py:483] Algo bellman_ford step 6225 current loss 0.370129, current_train_items 199232.
I0302 19:01:19.108634 22732373614720 run.py:483] Algo bellman_ford step 6226 current loss 0.515200, current_train_items 199264.
I0302 19:01:19.130793 22732373614720 run.py:483] Algo bellman_ford step 6227 current loss 0.683567, current_train_items 199296.
I0302 19:01:19.160044 22732373614720 run.py:483] Algo bellman_ford step 6228 current loss 0.683691, current_train_items 199328.
I0302 19:01:19.191067 22732373614720 run.py:483] Algo bellman_ford step 6229 current loss 0.739975, current_train_items 199360.
I0302 19:01:19.210440 22732373614720 run.py:483] Algo bellman_ford step 6230 current loss 0.306379, current_train_items 199392.
I0302 19:01:19.226488 22732373614720 run.py:483] Algo bellman_ford step 6231 current loss 0.532045, current_train_items 199424.
I0302 19:01:19.249825 22732373614720 run.py:483] Algo bellman_ford step 6232 current loss 0.660610, current_train_items 199456.
I0302 19:01:19.279084 22732373614720 run.py:483] Algo bellman_ford step 6233 current loss 0.728679, current_train_items 199488.
I0302 19:01:19.311431 22732373614720 run.py:483] Algo bellman_ford step 6234 current loss 0.765776, current_train_items 199520.
I0302 19:01:19.330740 22732373614720 run.py:483] Algo bellman_ford step 6235 current loss 0.218639, current_train_items 199552.
I0302 19:01:19.346779 22732373614720 run.py:483] Algo bellman_ford step 6236 current loss 0.421507, current_train_items 199584.
I0302 19:01:19.369047 22732373614720 run.py:483] Algo bellman_ford step 6237 current loss 0.667934, current_train_items 199616.
I0302 19:01:19.399885 22732373614720 run.py:483] Algo bellman_ford step 6238 current loss 0.708486, current_train_items 199648.
I0302 19:01:19.430862 22732373614720 run.py:483] Algo bellman_ford step 6239 current loss 0.966059, current_train_items 199680.
I0302 19:01:19.450212 22732373614720 run.py:483] Algo bellman_ford step 6240 current loss 0.315454, current_train_items 199712.
I0302 19:01:19.466423 22732373614720 run.py:483] Algo bellman_ford step 6241 current loss 0.611788, current_train_items 199744.
I0302 19:01:19.489841 22732373614720 run.py:483] Algo bellman_ford step 6242 current loss 0.766210, current_train_items 199776.
I0302 19:01:19.520701 22732373614720 run.py:483] Algo bellman_ford step 6243 current loss 0.789516, current_train_items 199808.
I0302 19:01:19.552104 22732373614720 run.py:483] Algo bellman_ford step 6244 current loss 0.732149, current_train_items 199840.
I0302 19:01:19.571897 22732373614720 run.py:483] Algo bellman_ford step 6245 current loss 0.357474, current_train_items 199872.
I0302 19:01:19.587876 22732373614720 run.py:483] Algo bellman_ford step 6246 current loss 0.582809, current_train_items 199904.
I0302 19:01:19.610330 22732373614720 run.py:483] Algo bellman_ford step 6247 current loss 0.523771, current_train_items 199936.
I0302 19:01:19.641637 22732373614720 run.py:483] Algo bellman_ford step 6248 current loss 0.685114, current_train_items 199968.
I0302 19:01:19.673198 22732373614720 run.py:483] Algo bellman_ford step 6249 current loss 0.779219, current_train_items 200000.
I0302 19:01:19.692462 22732373614720 run.py:483] Algo bellman_ford step 6250 current loss 0.302683, current_train_items 200032.
I0302 19:01:19.700697 22732373614720 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0302 19:01:19.700807 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:01:19.717727 22732373614720 run.py:483] Algo bellman_ford step 6251 current loss 0.561649, current_train_items 200064.
I0302 19:01:19.741145 22732373614720 run.py:483] Algo bellman_ford step 6252 current loss 0.620386, current_train_items 200096.
I0302 19:01:19.772691 22732373614720 run.py:483] Algo bellman_ford step 6253 current loss 0.745544, current_train_items 200128.
I0302 19:01:19.804742 22732373614720 run.py:483] Algo bellman_ford step 6254 current loss 0.810008, current_train_items 200160.
I0302 19:01:19.824368 22732373614720 run.py:483] Algo bellman_ford step 6255 current loss 0.302832, current_train_items 200192.
I0302 19:01:19.839960 22732373614720 run.py:483] Algo bellman_ford step 6256 current loss 0.426635, current_train_items 200224.
I0302 19:01:19.863491 22732373614720 run.py:483] Algo bellman_ford step 6257 current loss 0.677141, current_train_items 200256.
I0302 19:01:19.893218 22732373614720 run.py:483] Algo bellman_ford step 6258 current loss 0.777269, current_train_items 200288.
I0302 19:01:19.925062 22732373614720 run.py:483] Algo bellman_ford step 6259 current loss 0.993213, current_train_items 200320.
I0302 19:01:19.945055 22732373614720 run.py:483] Algo bellman_ford step 6260 current loss 0.441501, current_train_items 200352.
I0302 19:01:19.961365 22732373614720 run.py:483] Algo bellman_ford step 6261 current loss 0.494006, current_train_items 200384.
I0302 19:01:19.984001 22732373614720 run.py:483] Algo bellman_ford step 6262 current loss 0.679531, current_train_items 200416.
I0302 19:01:20.014626 22732373614720 run.py:483] Algo bellman_ford step 6263 current loss 0.728036, current_train_items 200448.
I0302 19:01:20.047657 22732373614720 run.py:483] Algo bellman_ford step 6264 current loss 0.816491, current_train_items 200480.
I0302 19:01:20.067192 22732373614720 run.py:483] Algo bellman_ford step 6265 current loss 0.294942, current_train_items 200512.
I0302 19:01:20.083938 22732373614720 run.py:483] Algo bellman_ford step 6266 current loss 0.616617, current_train_items 200544.
I0302 19:01:20.107859 22732373614720 run.py:483] Algo bellman_ford step 6267 current loss 0.698185, current_train_items 200576.
I0302 19:01:20.137681 22732373614720 run.py:483] Algo bellman_ford step 6268 current loss 0.671169, current_train_items 200608.
I0302 19:01:20.167741 22732373614720 run.py:483] Algo bellman_ford step 6269 current loss 0.688235, current_train_items 200640.
I0302 19:01:20.187331 22732373614720 run.py:483] Algo bellman_ford step 6270 current loss 0.296043, current_train_items 200672.
I0302 19:01:20.203611 22732373614720 run.py:483] Algo bellman_ford step 6271 current loss 0.486632, current_train_items 200704.
I0302 19:01:20.226237 22732373614720 run.py:483] Algo bellman_ford step 6272 current loss 0.627820, current_train_items 200736.
I0302 19:01:20.255351 22732373614720 run.py:483] Algo bellman_ford step 6273 current loss 0.640283, current_train_items 200768.
I0302 19:01:20.289263 22732373614720 run.py:483] Algo bellman_ford step 6274 current loss 0.843127, current_train_items 200800.
I0302 19:01:20.309030 22732373614720 run.py:483] Algo bellman_ford step 6275 current loss 0.355723, current_train_items 200832.
I0302 19:01:20.325345 22732373614720 run.py:483] Algo bellman_ford step 6276 current loss 0.523413, current_train_items 200864.
I0302 19:01:20.348026 22732373614720 run.py:483] Algo bellman_ford step 6277 current loss 0.656340, current_train_items 200896.
I0302 19:01:20.377709 22732373614720 run.py:483] Algo bellman_ford step 6278 current loss 0.688788, current_train_items 200928.
I0302 19:01:20.411782 22732373614720 run.py:483] Algo bellman_ford step 6279 current loss 0.894116, current_train_items 200960.
I0302 19:01:20.431634 22732373614720 run.py:483] Algo bellman_ford step 6280 current loss 0.324446, current_train_items 200992.
I0302 19:01:20.447559 22732373614720 run.py:483] Algo bellman_ford step 6281 current loss 0.447794, current_train_items 201024.
I0302 19:01:20.470790 22732373614720 run.py:483] Algo bellman_ford step 6282 current loss 0.791806, current_train_items 201056.
I0302 19:01:20.501199 22732373614720 run.py:483] Algo bellman_ford step 6283 current loss 0.699102, current_train_items 201088.
I0302 19:01:20.534815 22732373614720 run.py:483] Algo bellman_ford step 6284 current loss 0.837459, current_train_items 201120.
I0302 19:01:20.554471 22732373614720 run.py:483] Algo bellman_ford step 6285 current loss 0.325085, current_train_items 201152.
I0302 19:01:20.570696 22732373614720 run.py:483] Algo bellman_ford step 6286 current loss 0.497117, current_train_items 201184.
I0302 19:01:20.593449 22732373614720 run.py:483] Algo bellman_ford step 6287 current loss 0.623140, current_train_items 201216.
I0302 19:01:20.623941 22732373614720 run.py:483] Algo bellman_ford step 6288 current loss 0.761172, current_train_items 201248.
I0302 19:01:20.656750 22732373614720 run.py:483] Algo bellman_ford step 6289 current loss 0.913076, current_train_items 201280.
I0302 19:01:20.676249 22732373614720 run.py:483] Algo bellman_ford step 6290 current loss 0.352595, current_train_items 201312.
I0302 19:01:20.692651 22732373614720 run.py:483] Algo bellman_ford step 6291 current loss 0.496289, current_train_items 201344.
I0302 19:01:20.715257 22732373614720 run.py:483] Algo bellman_ford step 6292 current loss 0.605471, current_train_items 201376.
I0302 19:01:20.746340 22732373614720 run.py:483] Algo bellman_ford step 6293 current loss 0.894714, current_train_items 201408.
I0302 19:01:20.780618 22732373614720 run.py:483] Algo bellman_ford step 6294 current loss 0.888616, current_train_items 201440.
I0302 19:01:20.800033 22732373614720 run.py:483] Algo bellman_ford step 6295 current loss 0.337006, current_train_items 201472.
I0302 19:01:20.816704 22732373614720 run.py:483] Algo bellman_ford step 6296 current loss 0.527573, current_train_items 201504.
I0302 19:01:20.839254 22732373614720 run.py:483] Algo bellman_ford step 6297 current loss 0.564791, current_train_items 201536.
I0302 19:01:20.869580 22732373614720 run.py:483] Algo bellman_ford step 6298 current loss 0.767564, current_train_items 201568.
I0302 19:01:20.901704 22732373614720 run.py:483] Algo bellman_ford step 6299 current loss 0.714394, current_train_items 201600.
I0302 19:01:20.921226 22732373614720 run.py:483] Algo bellman_ford step 6300 current loss 0.344381, current_train_items 201632.
I0302 19:01:20.929300 22732373614720 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0302 19:01:20.929411 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:01:20.946165 22732373614720 run.py:483] Algo bellman_ford step 6301 current loss 0.604600, current_train_items 201664.
I0302 19:01:20.969193 22732373614720 run.py:483] Algo bellman_ford step 6302 current loss 0.656471, current_train_items 201696.
I0302 19:01:20.999516 22732373614720 run.py:483] Algo bellman_ford step 6303 current loss 0.649405, current_train_items 201728.
I0302 19:01:21.030953 22732373614720 run.py:483] Algo bellman_ford step 6304 current loss 0.692063, current_train_items 201760.
I0302 19:01:21.050851 22732373614720 run.py:483] Algo bellman_ford step 6305 current loss 0.329235, current_train_items 201792.
I0302 19:01:21.066338 22732373614720 run.py:483] Algo bellman_ford step 6306 current loss 0.502745, current_train_items 201824.
I0302 19:01:21.089565 22732373614720 run.py:483] Algo bellman_ford step 6307 current loss 0.644877, current_train_items 201856.
I0302 19:01:21.120589 22732373614720 run.py:483] Algo bellman_ford step 6308 current loss 0.743432, current_train_items 201888.
I0302 19:01:21.152725 22732373614720 run.py:483] Algo bellman_ford step 6309 current loss 0.918507, current_train_items 201920.
I0302 19:01:21.172128 22732373614720 run.py:483] Algo bellman_ford step 6310 current loss 0.357575, current_train_items 201952.
I0302 19:01:21.188017 22732373614720 run.py:483] Algo bellman_ford step 6311 current loss 0.479800, current_train_items 201984.
I0302 19:01:21.209881 22732373614720 run.py:483] Algo bellman_ford step 6312 current loss 0.627546, current_train_items 202016.
I0302 19:01:21.241130 22732373614720 run.py:483] Algo bellman_ford step 6313 current loss 0.874704, current_train_items 202048.
I0302 19:01:21.274573 22732373614720 run.py:483] Algo bellman_ford step 6314 current loss 0.940255, current_train_items 202080.
I0302 19:01:21.294309 22732373614720 run.py:483] Algo bellman_ford step 6315 current loss 0.335070, current_train_items 202112.
I0302 19:01:21.310191 22732373614720 run.py:483] Algo bellman_ford step 6316 current loss 0.478604, current_train_items 202144.
I0302 19:01:21.333258 22732373614720 run.py:483] Algo bellman_ford step 6317 current loss 0.675572, current_train_items 202176.
I0302 19:01:21.362726 22732373614720 run.py:483] Algo bellman_ford step 6318 current loss 0.606921, current_train_items 202208.
I0302 19:01:21.395114 22732373614720 run.py:483] Algo bellman_ford step 6319 current loss 0.689370, current_train_items 202240.
I0302 19:01:21.414764 22732373614720 run.py:483] Algo bellman_ford step 6320 current loss 0.357188, current_train_items 202272.
I0302 19:01:21.431054 22732373614720 run.py:483] Algo bellman_ford step 6321 current loss 0.501652, current_train_items 202304.
I0302 19:01:21.454928 22732373614720 run.py:483] Algo bellman_ford step 6322 current loss 0.633291, current_train_items 202336.
I0302 19:01:21.486196 22732373614720 run.py:483] Algo bellman_ford step 6323 current loss 0.836179, current_train_items 202368.
I0302 19:01:21.519734 22732373614720 run.py:483] Algo bellman_ford step 6324 current loss 0.826555, current_train_items 202400.
I0302 19:01:21.539250 22732373614720 run.py:483] Algo bellman_ford step 6325 current loss 0.362252, current_train_items 202432.
I0302 19:01:21.554900 22732373614720 run.py:483] Algo bellman_ford step 6326 current loss 0.518178, current_train_items 202464.
I0302 19:01:21.579171 22732373614720 run.py:483] Algo bellman_ford step 6327 current loss 0.642477, current_train_items 202496.
I0302 19:01:21.609519 22732373614720 run.py:483] Algo bellman_ford step 6328 current loss 0.737402, current_train_items 202528.
I0302 19:01:21.642899 22732373614720 run.py:483] Algo bellman_ford step 6329 current loss 0.748253, current_train_items 202560.
I0302 19:01:21.662310 22732373614720 run.py:483] Algo bellman_ford step 6330 current loss 0.295420, current_train_items 202592.
I0302 19:01:21.678347 22732373614720 run.py:483] Algo bellman_ford step 6331 current loss 0.551355, current_train_items 202624.
I0302 19:01:21.700970 22732373614720 run.py:483] Algo bellman_ford step 6332 current loss 0.616610, current_train_items 202656.
I0302 19:01:21.731280 22732373614720 run.py:483] Algo bellman_ford step 6333 current loss 0.714054, current_train_items 202688.
I0302 19:01:21.763530 22732373614720 run.py:483] Algo bellman_ford step 6334 current loss 0.900779, current_train_items 202720.
I0302 19:01:21.783151 22732373614720 run.py:483] Algo bellman_ford step 6335 current loss 0.297747, current_train_items 202752.
I0302 19:01:21.799322 22732373614720 run.py:483] Algo bellman_ford step 6336 current loss 0.468004, current_train_items 202784.
I0302 19:01:21.823013 22732373614720 run.py:483] Algo bellman_ford step 6337 current loss 0.662331, current_train_items 202816.
I0302 19:01:21.854132 22732373614720 run.py:483] Algo bellman_ford step 6338 current loss 0.748936, current_train_items 202848.
I0302 19:01:21.886441 22732373614720 run.py:483] Algo bellman_ford step 6339 current loss 0.821873, current_train_items 202880.
I0302 19:01:21.905941 22732373614720 run.py:483] Algo bellman_ford step 6340 current loss 0.363662, current_train_items 202912.
I0302 19:01:21.921569 22732373614720 run.py:483] Algo bellman_ford step 6341 current loss 0.525317, current_train_items 202944.
I0302 19:01:21.945878 22732373614720 run.py:483] Algo bellman_ford step 6342 current loss 0.743463, current_train_items 202976.
I0302 19:01:21.975847 22732373614720 run.py:483] Algo bellman_ford step 6343 current loss 0.705054, current_train_items 203008.
I0302 19:01:22.006433 22732373614720 run.py:483] Algo bellman_ford step 6344 current loss 0.761322, current_train_items 203040.
I0302 19:01:22.025747 22732373614720 run.py:483] Algo bellman_ford step 6345 current loss 0.323083, current_train_items 203072.
I0302 19:01:22.042122 22732373614720 run.py:483] Algo bellman_ford step 6346 current loss 0.590287, current_train_items 203104.
I0302 19:01:22.065361 22732373614720 run.py:483] Algo bellman_ford step 6347 current loss 0.663802, current_train_items 203136.
I0302 19:01:22.095130 22732373614720 run.py:483] Algo bellman_ford step 6348 current loss 0.736114, current_train_items 203168.
I0302 19:01:22.129902 22732373614720 run.py:483] Algo bellman_ford step 6349 current loss 0.770340, current_train_items 203200.
I0302 19:01:22.149576 22732373614720 run.py:483] Algo bellman_ford step 6350 current loss 0.354561, current_train_items 203232.
I0302 19:01:22.157646 22732373614720 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0302 19:01:22.157757 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:01:22.174691 22732373614720 run.py:483] Algo bellman_ford step 6351 current loss 0.529320, current_train_items 203264.
I0302 19:01:22.197835 22732373614720 run.py:483] Algo bellman_ford step 6352 current loss 0.712393, current_train_items 203296.
I0302 19:01:22.228919 22732373614720 run.py:483] Algo bellman_ford step 6353 current loss 0.663840, current_train_items 203328.
I0302 19:01:22.261845 22732373614720 run.py:483] Algo bellman_ford step 6354 current loss 0.894926, current_train_items 203360.
I0302 19:01:22.281997 22732373614720 run.py:483] Algo bellman_ford step 6355 current loss 0.357994, current_train_items 203392.
I0302 19:01:22.297779 22732373614720 run.py:483] Algo bellman_ford step 6356 current loss 0.600551, current_train_items 203424.
I0302 19:01:22.321518 22732373614720 run.py:483] Algo bellman_ford step 6357 current loss 0.697691, current_train_items 203456.
I0302 19:01:22.351930 22732373614720 run.py:483] Algo bellman_ford step 6358 current loss 0.778220, current_train_items 203488.
I0302 19:01:22.385224 22732373614720 run.py:483] Algo bellman_ford step 6359 current loss 0.814924, current_train_items 203520.
I0302 19:01:22.405383 22732373614720 run.py:483] Algo bellman_ford step 6360 current loss 0.276169, current_train_items 203552.
I0302 19:01:22.421731 22732373614720 run.py:483] Algo bellman_ford step 6361 current loss 0.503284, current_train_items 203584.
I0302 19:01:22.444161 22732373614720 run.py:483] Algo bellman_ford step 6362 current loss 0.671387, current_train_items 203616.
I0302 19:01:22.474397 22732373614720 run.py:483] Algo bellman_ford step 6363 current loss 0.628783, current_train_items 203648.
I0302 19:01:22.506637 22732373614720 run.py:483] Algo bellman_ford step 6364 current loss 0.842757, current_train_items 203680.
I0302 19:01:22.526270 22732373614720 run.py:483] Algo bellman_ford step 6365 current loss 0.302702, current_train_items 203712.
I0302 19:01:22.542428 22732373614720 run.py:483] Algo bellman_ford step 6366 current loss 0.465660, current_train_items 203744.
I0302 19:01:22.565109 22732373614720 run.py:483] Algo bellman_ford step 6367 current loss 0.678422, current_train_items 203776.
I0302 19:01:22.596737 22732373614720 run.py:483] Algo bellman_ford step 6368 current loss 0.840061, current_train_items 203808.
I0302 19:01:22.627974 22732373614720 run.py:483] Algo bellman_ford step 6369 current loss 0.754731, current_train_items 203840.
I0302 19:01:22.647960 22732373614720 run.py:483] Algo bellman_ford step 6370 current loss 0.311211, current_train_items 203872.
I0302 19:01:22.664305 22732373614720 run.py:483] Algo bellman_ford step 6371 current loss 0.520933, current_train_items 203904.
I0302 19:01:22.688064 22732373614720 run.py:483] Algo bellman_ford step 6372 current loss 0.709333, current_train_items 203936.
I0302 19:01:22.717947 22732373614720 run.py:483] Algo bellman_ford step 6373 current loss 0.754305, current_train_items 203968.
I0302 19:01:22.750622 22732373614720 run.py:483] Algo bellman_ford step 6374 current loss 0.745266, current_train_items 204000.
I0302 19:01:22.770450 22732373614720 run.py:483] Algo bellman_ford step 6375 current loss 0.446078, current_train_items 204032.
I0302 19:01:22.786289 22732373614720 run.py:483] Algo bellman_ford step 6376 current loss 0.537958, current_train_items 204064.
I0302 19:01:22.810258 22732373614720 run.py:483] Algo bellman_ford step 6377 current loss 0.752744, current_train_items 204096.
I0302 19:01:22.840687 22732373614720 run.py:483] Algo bellman_ford step 6378 current loss 0.728343, current_train_items 204128.
I0302 19:01:22.875543 22732373614720 run.py:483] Algo bellman_ford step 6379 current loss 0.977501, current_train_items 204160.
I0302 19:01:22.895051 22732373614720 run.py:483] Algo bellman_ford step 6380 current loss 0.313240, current_train_items 204192.
I0302 19:01:22.911001 22732373614720 run.py:483] Algo bellman_ford step 6381 current loss 0.533818, current_train_items 204224.
I0302 19:01:22.934220 22732373614720 run.py:483] Algo bellman_ford step 6382 current loss 0.636705, current_train_items 204256.
I0302 19:01:22.963772 22732373614720 run.py:483] Algo bellman_ford step 6383 current loss 0.872195, current_train_items 204288.
I0302 19:01:22.996893 22732373614720 run.py:483] Algo bellman_ford step 6384 current loss 0.877724, current_train_items 204320.
I0302 19:01:23.016859 22732373614720 run.py:483] Algo bellman_ford step 6385 current loss 0.364159, current_train_items 204352.
I0302 19:01:23.032481 22732373614720 run.py:483] Algo bellman_ford step 6386 current loss 0.552263, current_train_items 204384.
I0302 19:01:23.055818 22732373614720 run.py:483] Algo bellman_ford step 6387 current loss 0.571159, current_train_items 204416.
I0302 19:01:23.085726 22732373614720 run.py:483] Algo bellman_ford step 6388 current loss 0.621620, current_train_items 204448.
I0302 19:01:23.118059 22732373614720 run.py:483] Algo bellman_ford step 6389 current loss 0.756661, current_train_items 204480.
I0302 19:01:23.137868 22732373614720 run.py:483] Algo bellman_ford step 6390 current loss 0.339418, current_train_items 204512.
I0302 19:01:23.154137 22732373614720 run.py:483] Algo bellman_ford step 6391 current loss 0.460051, current_train_items 204544.
I0302 19:01:23.177069 22732373614720 run.py:483] Algo bellman_ford step 6392 current loss 0.647694, current_train_items 204576.
I0302 19:01:23.206068 22732373614720 run.py:483] Algo bellman_ford step 6393 current loss 0.625054, current_train_items 204608.
I0302 19:01:23.239293 22732373614720 run.py:483] Algo bellman_ford step 6394 current loss 0.830461, current_train_items 204640.
I0302 19:01:23.259064 22732373614720 run.py:483] Algo bellman_ford step 6395 current loss 0.313189, current_train_items 204672.
I0302 19:01:23.274989 22732373614720 run.py:483] Algo bellman_ford step 6396 current loss 0.453241, current_train_items 204704.
I0302 19:01:23.297712 22732373614720 run.py:483] Algo bellman_ford step 6397 current loss 0.757564, current_train_items 204736.
I0302 19:01:23.327247 22732373614720 run.py:483] Algo bellman_ford step 6398 current loss 0.742916, current_train_items 204768.
I0302 19:01:23.358332 22732373614720 run.py:483] Algo bellman_ford step 6399 current loss 0.790404, current_train_items 204800.
I0302 19:01:23.378198 22732373614720 run.py:483] Algo bellman_ford step 6400 current loss 0.334658, current_train_items 204832.
I0302 19:01:23.386137 22732373614720 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0302 19:01:23.386259 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0302 19:01:23.402190 22732373614720 run.py:483] Algo bellman_ford step 6401 current loss 0.493167, current_train_items 204864.
I0302 19:01:23.425144 22732373614720 run.py:483] Algo bellman_ford step 6402 current loss 0.642518, current_train_items 204896.
I0302 19:01:23.455799 22732373614720 run.py:483] Algo bellman_ford step 6403 current loss 0.738821, current_train_items 204928.
I0302 19:01:23.488859 22732373614720 run.py:483] Algo bellman_ford step 6404 current loss 0.810614, current_train_items 204960.
I0302 19:01:23.508449 22732373614720 run.py:483] Algo bellman_ford step 6405 current loss 0.322033, current_train_items 204992.
I0302 19:01:23.523837 22732373614720 run.py:483] Algo bellman_ford step 6406 current loss 0.513136, current_train_items 205024.
I0302 19:01:23.546404 22732373614720 run.py:483] Algo bellman_ford step 6407 current loss 0.602435, current_train_items 205056.
I0302 19:01:23.576522 22732373614720 run.py:483] Algo bellman_ford step 6408 current loss 0.696376, current_train_items 205088.
I0302 19:01:23.610358 22732373614720 run.py:483] Algo bellman_ford step 6409 current loss 0.795290, current_train_items 205120.
I0302 19:01:23.629612 22732373614720 run.py:483] Algo bellman_ford step 6410 current loss 0.292818, current_train_items 205152.
I0302 19:01:23.645247 22732373614720 run.py:483] Algo bellman_ford step 6411 current loss 0.459302, current_train_items 205184.
I0302 19:01:23.667346 22732373614720 run.py:483] Algo bellman_ford step 6412 current loss 0.606747, current_train_items 205216.
I0302 19:01:23.697601 22732373614720 run.py:483] Algo bellman_ford step 6413 current loss 0.824450, current_train_items 205248.
I0302 19:01:23.730248 22732373614720 run.py:483] Algo bellman_ford step 6414 current loss 0.663799, current_train_items 205280.
I0302 19:01:23.749548 22732373614720 run.py:483] Algo bellman_ford step 6415 current loss 0.248785, current_train_items 205312.
I0302 19:01:23.765393 22732373614720 run.py:483] Algo bellman_ford step 6416 current loss 0.414967, current_train_items 205344.
I0302 19:01:23.788973 22732373614720 run.py:483] Algo bellman_ford step 6417 current loss 0.646947, current_train_items 205376.
I0302 19:01:23.820136 22732373614720 run.py:483] Algo bellman_ford step 6418 current loss 0.731839, current_train_items 205408.
I0302 19:01:23.853299 22732373614720 run.py:483] Algo bellman_ford step 6419 current loss 0.941216, current_train_items 205440.
I0302 19:01:23.872814 22732373614720 run.py:483] Algo bellman_ford step 6420 current loss 0.324119, current_train_items 205472.
I0302 19:01:23.888885 22732373614720 run.py:483] Algo bellman_ford step 6421 current loss 0.497076, current_train_items 205504.
I0302 19:01:23.911374 22732373614720 run.py:483] Algo bellman_ford step 6422 current loss 0.661596, current_train_items 205536.
I0302 19:01:23.941370 22732373614720 run.py:483] Algo bellman_ford step 6423 current loss 0.697792, current_train_items 205568.
I0302 19:01:23.973489 22732373614720 run.py:483] Algo bellman_ford step 6424 current loss 0.864139, current_train_items 205600.
I0302 19:01:23.992694 22732373614720 run.py:483] Algo bellman_ford step 6425 current loss 0.331457, current_train_items 205632.
I0302 19:01:24.008985 22732373614720 run.py:483] Algo bellman_ford step 6426 current loss 0.584338, current_train_items 205664.
I0302 19:01:24.031970 22732373614720 run.py:483] Algo bellman_ford step 6427 current loss 0.675937, current_train_items 205696.
I0302 19:01:24.062556 22732373614720 run.py:483] Algo bellman_ford step 6428 current loss 0.778890, current_train_items 205728.
I0302 19:01:24.097228 22732373614720 run.py:483] Algo bellman_ford step 6429 current loss 1.026528, current_train_items 205760.
I0302 19:01:24.116593 22732373614720 run.py:483] Algo bellman_ford step 6430 current loss 0.338919, current_train_items 205792.
I0302 19:01:24.131973 22732373614720 run.py:483] Algo bellman_ford step 6431 current loss 0.527234, current_train_items 205824.
I0302 19:01:24.155076 22732373614720 run.py:483] Algo bellman_ford step 6432 current loss 0.806219, current_train_items 205856.
I0302 19:01:24.185656 22732373614720 run.py:483] Algo bellman_ford step 6433 current loss 0.628431, current_train_items 205888.
I0302 19:01:24.217436 22732373614720 run.py:483] Algo bellman_ford step 6434 current loss 0.866144, current_train_items 205920.
I0302 19:01:24.236679 22732373614720 run.py:483] Algo bellman_ford step 6435 current loss 0.342227, current_train_items 205952.
I0302 19:01:24.252518 22732373614720 run.py:483] Algo bellman_ford step 6436 current loss 0.599666, current_train_items 205984.
I0302 19:01:24.275017 22732373614720 run.py:483] Algo bellman_ford step 6437 current loss 0.681657, current_train_items 206016.
I0302 19:01:24.306216 22732373614720 run.py:483] Algo bellman_ford step 6438 current loss 0.812696, current_train_items 206048.
I0302 19:01:24.339394 22732373614720 run.py:483] Algo bellman_ford step 6439 current loss 0.774598, current_train_items 206080.
I0302 19:01:24.358617 22732373614720 run.py:483] Algo bellman_ford step 6440 current loss 0.281484, current_train_items 206112.
I0302 19:01:24.374829 22732373614720 run.py:483] Algo bellman_ford step 6441 current loss 0.586294, current_train_items 206144.
I0302 19:01:24.396697 22732373614720 run.py:483] Algo bellman_ford step 6442 current loss 0.628877, current_train_items 206176.
I0302 19:01:24.425746 22732373614720 run.py:483] Algo bellman_ford step 6443 current loss 0.655950, current_train_items 206208.
I0302 19:01:24.458687 22732373614720 run.py:483] Algo bellman_ford step 6444 current loss 0.733296, current_train_items 206240.
I0302 19:01:24.477955 22732373614720 run.py:483] Algo bellman_ford step 6445 current loss 0.321405, current_train_items 206272.
I0302 19:01:24.493598 22732373614720 run.py:483] Algo bellman_ford step 6446 current loss 0.572852, current_train_items 206304.
I0302 19:01:24.517377 22732373614720 run.py:483] Algo bellman_ford step 6447 current loss 0.718411, current_train_items 206336.
I0302 19:01:24.547669 22732373614720 run.py:483] Algo bellman_ford step 6448 current loss 0.880813, current_train_items 206368.
I0302 19:01:24.580691 22732373614720 run.py:483] Algo bellman_ford step 6449 current loss 0.914216, current_train_items 206400.
I0302 19:01:24.600095 22732373614720 run.py:483] Algo bellman_ford step 6450 current loss 0.331808, current_train_items 206432.
I0302 19:01:24.608117 22732373614720 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0302 19:01:24.608235 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:01:24.624872 22732373614720 run.py:483] Algo bellman_ford step 6451 current loss 0.481865, current_train_items 206464.
I0302 19:01:24.649335 22732373614720 run.py:483] Algo bellman_ford step 6452 current loss 0.804567, current_train_items 206496.
I0302 19:01:24.678974 22732373614720 run.py:483] Algo bellman_ford step 6453 current loss 0.769863, current_train_items 206528.
I0302 19:01:24.709961 22732373614720 run.py:483] Algo bellman_ford step 6454 current loss 0.759686, current_train_items 206560.
I0302 19:01:24.729567 22732373614720 run.py:483] Algo bellman_ford step 6455 current loss 0.313401, current_train_items 206592.
I0302 19:01:24.745281 22732373614720 run.py:483] Algo bellman_ford step 6456 current loss 0.433918, current_train_items 206624.
I0302 19:01:24.767357 22732373614720 run.py:483] Algo bellman_ford step 6457 current loss 0.702693, current_train_items 206656.
I0302 19:01:24.797350 22732373614720 run.py:483] Algo bellman_ford step 6458 current loss 0.661658, current_train_items 206688.
I0302 19:01:24.828421 22732373614720 run.py:483] Algo bellman_ford step 6459 current loss 0.838660, current_train_items 206720.
I0302 19:01:24.848034 22732373614720 run.py:483] Algo bellman_ford step 6460 current loss 0.346383, current_train_items 206752.
I0302 19:01:24.864103 22732373614720 run.py:483] Algo bellman_ford step 6461 current loss 0.473884, current_train_items 206784.
I0302 19:01:24.886532 22732373614720 run.py:483] Algo bellman_ford step 6462 current loss 0.686254, current_train_items 206816.
I0302 19:01:24.916216 22732373614720 run.py:483] Algo bellman_ford step 6463 current loss 0.672015, current_train_items 206848.
I0302 19:01:24.950646 22732373614720 run.py:483] Algo bellman_ford step 6464 current loss 0.852959, current_train_items 206880.
I0302 19:01:24.970068 22732373614720 run.py:483] Algo bellman_ford step 6465 current loss 0.252984, current_train_items 206912.
I0302 19:01:24.986296 22732373614720 run.py:483] Algo bellman_ford step 6466 current loss 0.481636, current_train_items 206944.
I0302 19:01:25.010577 22732373614720 run.py:483] Algo bellman_ford step 6467 current loss 0.796282, current_train_items 206976.
I0302 19:01:25.042026 22732373614720 run.py:483] Algo bellman_ford step 6468 current loss 0.792877, current_train_items 207008.
I0302 19:01:25.076035 22732373614720 run.py:483] Algo bellman_ford step 6469 current loss 0.789810, current_train_items 207040.
I0302 19:01:25.095963 22732373614720 run.py:483] Algo bellman_ford step 6470 current loss 0.330588, current_train_items 207072.
I0302 19:01:25.112503 22732373614720 run.py:483] Algo bellman_ford step 6471 current loss 0.461261, current_train_items 207104.
I0302 19:01:25.134559 22732373614720 run.py:483] Algo bellman_ford step 6472 current loss 0.567917, current_train_items 207136.
I0302 19:01:25.164796 22732373614720 run.py:483] Algo bellman_ford step 6473 current loss 0.818538, current_train_items 207168.
I0302 19:01:25.196798 22732373614720 run.py:483] Algo bellman_ford step 6474 current loss 0.751977, current_train_items 207200.
I0302 19:01:25.216582 22732373614720 run.py:483] Algo bellman_ford step 6475 current loss 0.429829, current_train_items 207232.
I0302 19:01:25.232760 22732373614720 run.py:483] Algo bellman_ford step 6476 current loss 0.593382, current_train_items 207264.
I0302 19:01:25.254615 22732373614720 run.py:483] Algo bellman_ford step 6477 current loss 0.656066, current_train_items 207296.
I0302 19:01:25.285042 22732373614720 run.py:483] Algo bellman_ford step 6478 current loss 0.787122, current_train_items 207328.
I0302 19:01:25.319401 22732373614720 run.py:483] Algo bellman_ford step 6479 current loss 0.923615, current_train_items 207360.
I0302 19:01:25.338948 22732373614720 run.py:483] Algo bellman_ford step 6480 current loss 0.396577, current_train_items 207392.
I0302 19:01:25.354604 22732373614720 run.py:483] Algo bellman_ford step 6481 current loss 0.500162, current_train_items 207424.
I0302 19:01:25.377284 22732373614720 run.py:483] Algo bellman_ford step 6482 current loss 0.668407, current_train_items 207456.
I0302 19:01:25.408187 22732373614720 run.py:483] Algo bellman_ford step 6483 current loss 0.772753, current_train_items 207488.
I0302 19:01:25.441794 22732373614720 run.py:483] Algo bellman_ford step 6484 current loss 0.786548, current_train_items 207520.
I0302 19:01:25.461690 22732373614720 run.py:483] Algo bellman_ford step 6485 current loss 0.376568, current_train_items 207552.
I0302 19:01:25.478024 22732373614720 run.py:483] Algo bellman_ford step 6486 current loss 0.458840, current_train_items 207584.
I0302 19:01:25.502111 22732373614720 run.py:483] Algo bellman_ford step 6487 current loss 0.790828, current_train_items 207616.
I0302 19:01:25.531072 22732373614720 run.py:483] Algo bellman_ford step 6488 current loss 0.821240, current_train_items 207648.
I0302 19:01:25.564185 22732373614720 run.py:483] Algo bellman_ford step 6489 current loss 0.826615, current_train_items 207680.
I0302 19:01:25.584047 22732373614720 run.py:483] Algo bellman_ford step 6490 current loss 0.328377, current_train_items 207712.
I0302 19:01:25.600056 22732373614720 run.py:483] Algo bellman_ford step 6491 current loss 0.449250, current_train_items 207744.
I0302 19:01:25.622182 22732373614720 run.py:483] Algo bellman_ford step 6492 current loss 0.719775, current_train_items 207776.
I0302 19:01:25.652258 22732373614720 run.py:483] Algo bellman_ford step 6493 current loss 0.784722, current_train_items 207808.
I0302 19:01:25.684865 22732373614720 run.py:483] Algo bellman_ford step 6494 current loss 1.025942, current_train_items 207840.
I0302 19:01:25.704032 22732373614720 run.py:483] Algo bellman_ford step 6495 current loss 0.255891, current_train_items 207872.
I0302 19:01:25.719684 22732373614720 run.py:483] Algo bellman_ford step 6496 current loss 0.507265, current_train_items 207904.
I0302 19:01:25.743657 22732373614720 run.py:483] Algo bellman_ford step 6497 current loss 0.788372, current_train_items 207936.
I0302 19:01:25.772948 22732373614720 run.py:483] Algo bellman_ford step 6498 current loss 0.709880, current_train_items 207968.
I0302 19:01:25.807120 22732373614720 run.py:483] Algo bellman_ford step 6499 current loss 0.988317, current_train_items 208000.
I0302 19:01:25.826804 22732373614720 run.py:483] Algo bellman_ford step 6500 current loss 0.505607, current_train_items 208032.
I0302 19:01:25.834657 22732373614720 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0302 19:01:25.834767 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:01:25.851318 22732373614720 run.py:483] Algo bellman_ford step 6501 current loss 0.479973, current_train_items 208064.
I0302 19:01:25.875541 22732373614720 run.py:483] Algo bellman_ford step 6502 current loss 0.809185, current_train_items 208096.
I0302 19:01:25.905871 22732373614720 run.py:483] Algo bellman_ford step 6503 current loss 0.912726, current_train_items 208128.
I0302 19:01:25.940388 22732373614720 run.py:483] Algo bellman_ford step 6504 current loss 0.909800, current_train_items 208160.
I0302 19:01:25.960209 22732373614720 run.py:483] Algo bellman_ford step 6505 current loss 0.381488, current_train_items 208192.
I0302 19:01:25.975261 22732373614720 run.py:483] Algo bellman_ford step 6506 current loss 0.365360, current_train_items 208224.
I0302 19:01:25.998479 22732373614720 run.py:483] Algo bellman_ford step 6507 current loss 0.706960, current_train_items 208256.
I0302 19:01:26.030016 22732373614720 run.py:483] Algo bellman_ford step 6508 current loss 0.691041, current_train_items 208288.
I0302 19:01:26.060731 22732373614720 run.py:483] Algo bellman_ford step 6509 current loss 0.835083, current_train_items 208320.
I0302 19:01:26.080337 22732373614720 run.py:483] Algo bellman_ford step 6510 current loss 0.361092, current_train_items 208352.
I0302 19:01:26.096333 22732373614720 run.py:483] Algo bellman_ford step 6511 current loss 0.471626, current_train_items 208384.
I0302 19:01:26.120210 22732373614720 run.py:483] Algo bellman_ford step 6512 current loss 0.691422, current_train_items 208416.
I0302 19:01:26.150225 22732373614720 run.py:483] Algo bellman_ford step 6513 current loss 0.685948, current_train_items 208448.
I0302 19:01:26.184232 22732373614720 run.py:483] Algo bellman_ford step 6514 current loss 0.840407, current_train_items 208480.
I0302 19:01:26.203351 22732373614720 run.py:483] Algo bellman_ford step 6515 current loss 0.329032, current_train_items 208512.
I0302 19:01:26.219075 22732373614720 run.py:483] Algo bellman_ford step 6516 current loss 0.538891, current_train_items 208544.
I0302 19:01:26.242349 22732373614720 run.py:483] Algo bellman_ford step 6517 current loss 0.613186, current_train_items 208576.
I0302 19:01:26.272487 22732373614720 run.py:483] Algo bellman_ford step 6518 current loss 0.608777, current_train_items 208608.
I0302 19:01:26.304582 22732373614720 run.py:483] Algo bellman_ford step 6519 current loss 0.752094, current_train_items 208640.
I0302 19:01:26.323810 22732373614720 run.py:483] Algo bellman_ford step 6520 current loss 0.347698, current_train_items 208672.
I0302 19:01:26.339848 22732373614720 run.py:483] Algo bellman_ford step 6521 current loss 0.480784, current_train_items 208704.
I0302 19:01:26.363592 22732373614720 run.py:483] Algo bellman_ford step 6522 current loss 0.733675, current_train_items 208736.
I0302 19:01:26.393473 22732373614720 run.py:483] Algo bellman_ford step 6523 current loss 0.739016, current_train_items 208768.
I0302 19:01:26.424357 22732373614720 run.py:483] Algo bellman_ford step 6524 current loss 0.693312, current_train_items 208800.
I0302 19:01:26.443917 22732373614720 run.py:483] Algo bellman_ford step 6525 current loss 0.289042, current_train_items 208832.
I0302 19:01:26.459641 22732373614720 run.py:483] Algo bellman_ford step 6526 current loss 0.459517, current_train_items 208864.
I0302 19:01:26.482268 22732373614720 run.py:483] Algo bellman_ford step 6527 current loss 0.583230, current_train_items 208896.
I0302 19:01:26.511853 22732373614720 run.py:483] Algo bellman_ford step 6528 current loss 0.650295, current_train_items 208928.
I0302 19:01:26.544924 22732373614720 run.py:483] Algo bellman_ford step 6529 current loss 0.840939, current_train_items 208960.
I0302 19:01:26.564251 22732373614720 run.py:483] Algo bellman_ford step 6530 current loss 0.325454, current_train_items 208992.
I0302 19:01:26.580284 22732373614720 run.py:483] Algo bellman_ford step 6531 current loss 0.529695, current_train_items 209024.
I0302 19:01:26.604853 22732373614720 run.py:483] Algo bellman_ford step 6532 current loss 0.652127, current_train_items 209056.
I0302 19:01:26.634502 22732373614720 run.py:483] Algo bellman_ford step 6533 current loss 0.791107, current_train_items 209088.
I0302 19:01:26.667382 22732373614720 run.py:483] Algo bellman_ford step 6534 current loss 1.007843, current_train_items 209120.
I0302 19:01:26.686681 22732373614720 run.py:483] Algo bellman_ford step 6535 current loss 0.316534, current_train_items 209152.
I0302 19:01:26.702725 22732373614720 run.py:483] Algo bellman_ford step 6536 current loss 0.498735, current_train_items 209184.
I0302 19:01:26.725607 22732373614720 run.py:483] Algo bellman_ford step 6537 current loss 0.645467, current_train_items 209216.
I0302 19:01:26.755096 22732373614720 run.py:483] Algo bellman_ford step 6538 current loss 0.753724, current_train_items 209248.
I0302 19:01:26.788052 22732373614720 run.py:483] Algo bellman_ford step 6539 current loss 0.697428, current_train_items 209280.
I0302 19:01:26.807413 22732373614720 run.py:483] Algo bellman_ford step 6540 current loss 0.351758, current_train_items 209312.
I0302 19:01:26.823304 22732373614720 run.py:483] Algo bellman_ford step 6541 current loss 0.604503, current_train_items 209344.
I0302 19:01:26.846055 22732373614720 run.py:483] Algo bellman_ford step 6542 current loss 0.652135, current_train_items 209376.
I0302 19:01:26.876628 22732373614720 run.py:483] Algo bellman_ford step 6543 current loss 0.741357, current_train_items 209408.
I0302 19:01:26.911269 22732373614720 run.py:483] Algo bellman_ford step 6544 current loss 0.832001, current_train_items 209440.
I0302 19:01:26.930667 22732373614720 run.py:483] Algo bellman_ford step 6545 current loss 0.311874, current_train_items 209472.
I0302 19:01:26.946429 22732373614720 run.py:483] Algo bellman_ford step 6546 current loss 0.464875, current_train_items 209504.
I0302 19:01:26.968825 22732373614720 run.py:483] Algo bellman_ford step 6547 current loss 0.563521, current_train_items 209536.
I0302 19:01:26.999998 22732373614720 run.py:483] Algo bellman_ford step 6548 current loss 0.768631, current_train_items 209568.
I0302 19:01:27.033819 22732373614720 run.py:483] Algo bellman_ford step 6549 current loss 0.777286, current_train_items 209600.
I0302 19:01:27.053101 22732373614720 run.py:483] Algo bellman_ford step 6550 current loss 0.274714, current_train_items 209632.
I0302 19:01:27.061356 22732373614720 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0302 19:01:27.061464 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:01:27.078325 22732373614720 run.py:483] Algo bellman_ford step 6551 current loss 0.510967, current_train_items 209664.
I0302 19:01:27.101846 22732373614720 run.py:483] Algo bellman_ford step 6552 current loss 0.588337, current_train_items 209696.
I0302 19:01:27.133388 22732373614720 run.py:483] Algo bellman_ford step 6553 current loss 0.712879, current_train_items 209728.
I0302 19:01:27.168737 22732373614720 run.py:483] Algo bellman_ford step 6554 current loss 1.000347, current_train_items 209760.
I0302 19:01:27.188974 22732373614720 run.py:483] Algo bellman_ford step 6555 current loss 0.327484, current_train_items 209792.
I0302 19:01:27.204555 22732373614720 run.py:483] Algo bellman_ford step 6556 current loss 0.517511, current_train_items 209824.
I0302 19:01:27.228096 22732373614720 run.py:483] Algo bellman_ford step 6557 current loss 0.688781, current_train_items 209856.
I0302 19:01:27.259013 22732373614720 run.py:483] Algo bellman_ford step 6558 current loss 0.749059, current_train_items 209888.
I0302 19:01:27.292989 22732373614720 run.py:483] Algo bellman_ford step 6559 current loss 0.871645, current_train_items 209920.
I0302 19:01:27.313315 22732373614720 run.py:483] Algo bellman_ford step 6560 current loss 0.367277, current_train_items 209952.
I0302 19:01:27.328878 22732373614720 run.py:483] Algo bellman_ford step 6561 current loss 0.555676, current_train_items 209984.
I0302 19:01:27.350897 22732373614720 run.py:483] Algo bellman_ford step 6562 current loss 0.661773, current_train_items 210016.
I0302 19:01:27.381238 22732373614720 run.py:483] Algo bellman_ford step 6563 current loss 0.884779, current_train_items 210048.
I0302 19:01:27.411853 22732373614720 run.py:483] Algo bellman_ford step 6564 current loss 0.789438, current_train_items 210080.
I0302 19:01:27.431832 22732373614720 run.py:483] Algo bellman_ford step 6565 current loss 0.353538, current_train_items 210112.
I0302 19:01:27.447486 22732373614720 run.py:483] Algo bellman_ford step 6566 current loss 0.499474, current_train_items 210144.
I0302 19:01:27.470821 22732373614720 run.py:483] Algo bellman_ford step 6567 current loss 0.718250, current_train_items 210176.
I0302 19:01:27.499586 22732373614720 run.py:483] Algo bellman_ford step 6568 current loss 0.877568, current_train_items 210208.
I0302 19:01:27.532567 22732373614720 run.py:483] Algo bellman_ford step 6569 current loss 0.784461, current_train_items 210240.
I0302 19:01:27.552567 22732373614720 run.py:483] Algo bellman_ford step 6570 current loss 0.330655, current_train_items 210272.
I0302 19:01:27.568637 22732373614720 run.py:483] Algo bellman_ford step 6571 current loss 0.576650, current_train_items 210304.
I0302 19:01:27.591501 22732373614720 run.py:483] Algo bellman_ford step 6572 current loss 0.598974, current_train_items 210336.
I0302 19:01:27.621350 22732373614720 run.py:483] Algo bellman_ford step 6573 current loss 0.639273, current_train_items 210368.
I0302 19:01:27.653606 22732373614720 run.py:483] Algo bellman_ford step 6574 current loss 1.002191, current_train_items 210400.
I0302 19:01:27.673643 22732373614720 run.py:483] Algo bellman_ford step 6575 current loss 0.354605, current_train_items 210432.
I0302 19:01:27.689836 22732373614720 run.py:483] Algo bellman_ford step 6576 current loss 0.467033, current_train_items 210464.
I0302 19:01:27.712381 22732373614720 run.py:483] Algo bellman_ford step 6577 current loss 0.573972, current_train_items 210496.
I0302 19:01:27.742515 22732373614720 run.py:483] Algo bellman_ford step 6578 current loss 0.708220, current_train_items 210528.
I0302 19:01:27.775371 22732373614720 run.py:483] Algo bellman_ford step 6579 current loss 0.799594, current_train_items 210560.
I0302 19:01:27.794949 22732373614720 run.py:483] Algo bellman_ford step 6580 current loss 0.361379, current_train_items 210592.
I0302 19:01:27.810941 22732373614720 run.py:483] Algo bellman_ford step 6581 current loss 0.492739, current_train_items 210624.
I0302 19:01:27.834319 22732373614720 run.py:483] Algo bellman_ford step 6582 current loss 0.722088, current_train_items 210656.
I0302 19:01:27.865955 22732373614720 run.py:483] Algo bellman_ford step 6583 current loss 0.767370, current_train_items 210688.
I0302 19:01:27.898483 22732373614720 run.py:483] Algo bellman_ford step 6584 current loss 0.782790, current_train_items 210720.
I0302 19:01:27.918190 22732373614720 run.py:483] Algo bellman_ford step 6585 current loss 0.314552, current_train_items 210752.
I0302 19:01:27.933763 22732373614720 run.py:483] Algo bellman_ford step 6586 current loss 0.540582, current_train_items 210784.
I0302 19:01:27.955566 22732373614720 run.py:483] Algo bellman_ford step 6587 current loss 0.640185, current_train_items 210816.
I0302 19:01:27.985272 22732373614720 run.py:483] Algo bellman_ford step 6588 current loss 0.765365, current_train_items 210848.
I0302 19:01:28.016695 22732373614720 run.py:483] Algo bellman_ford step 6589 current loss 0.714783, current_train_items 210880.
I0302 19:01:28.036404 22732373614720 run.py:483] Algo bellman_ford step 6590 current loss 0.372992, current_train_items 210912.
I0302 19:01:28.052184 22732373614720 run.py:483] Algo bellman_ford step 6591 current loss 0.554068, current_train_items 210944.
I0302 19:01:28.075316 22732373614720 run.py:483] Algo bellman_ford step 6592 current loss 0.756005, current_train_items 210976.
I0302 19:01:28.105674 22732373614720 run.py:483] Algo bellman_ford step 6593 current loss 0.829206, current_train_items 211008.
I0302 19:01:28.137497 22732373614720 run.py:483] Algo bellman_ford step 6594 current loss 1.015243, current_train_items 211040.
I0302 19:01:28.157044 22732373614720 run.py:483] Algo bellman_ford step 6595 current loss 0.306574, current_train_items 211072.
I0302 19:01:28.172832 22732373614720 run.py:483] Algo bellman_ford step 6596 current loss 0.456668, current_train_items 211104.
I0302 19:01:28.196532 22732373614720 run.py:483] Algo bellman_ford step 6597 current loss 0.682238, current_train_items 211136.
I0302 19:01:28.227655 22732373614720 run.py:483] Algo bellman_ford step 6598 current loss 0.740043, current_train_items 211168.
I0302 19:01:28.257421 22732373614720 run.py:483] Algo bellman_ford step 6599 current loss 0.810486, current_train_items 211200.
I0302 19:01:28.277398 22732373614720 run.py:483] Algo bellman_ford step 6600 current loss 0.286630, current_train_items 211232.
I0302 19:01:28.285269 22732373614720 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.8876953125, 'score': 0.8876953125, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0302 19:01:28.285379 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.888, val scores are: bellman_ford: 0.888
I0302 19:01:28.301902 22732373614720 run.py:483] Algo bellman_ford step 6601 current loss 0.499102, current_train_items 211264.
I0302 19:01:28.324134 22732373614720 run.py:483] Algo bellman_ford step 6602 current loss 0.622945, current_train_items 211296.
I0302 19:01:28.354770 22732373614720 run.py:483] Algo bellman_ford step 6603 current loss 0.728485, current_train_items 211328.
I0302 19:01:28.387801 22732373614720 run.py:483] Algo bellman_ford step 6604 current loss 0.766011, current_train_items 211360.
I0302 19:01:28.407399 22732373614720 run.py:483] Algo bellman_ford step 6605 current loss 0.302703, current_train_items 211392.
I0302 19:01:28.422874 22732373614720 run.py:483] Algo bellman_ford step 6606 current loss 0.439646, current_train_items 211424.
I0302 19:01:28.445856 22732373614720 run.py:483] Algo bellman_ford step 6607 current loss 0.720671, current_train_items 211456.
I0302 19:01:28.476393 22732373614720 run.py:483] Algo bellman_ford step 6608 current loss 0.577757, current_train_items 211488.
I0302 19:01:28.507472 22732373614720 run.py:483] Algo bellman_ford step 6609 current loss 0.687863, current_train_items 211520.
I0302 19:01:28.526925 22732373614720 run.py:483] Algo bellman_ford step 6610 current loss 0.315521, current_train_items 211552.
I0302 19:01:28.542706 22732373614720 run.py:483] Algo bellman_ford step 6611 current loss 0.460275, current_train_items 211584.
I0302 19:01:28.564625 22732373614720 run.py:483] Algo bellman_ford step 6612 current loss 0.535125, current_train_items 211616.
I0302 19:01:28.594935 22732373614720 run.py:483] Algo bellman_ford step 6613 current loss 0.690011, current_train_items 211648.
I0302 19:01:28.629970 22732373614720 run.py:483] Algo bellman_ford step 6614 current loss 0.922735, current_train_items 211680.
I0302 19:01:28.649343 22732373614720 run.py:483] Algo bellman_ford step 6615 current loss 0.342398, current_train_items 211712.
I0302 19:01:28.665098 22732373614720 run.py:483] Algo bellman_ford step 6616 current loss 0.427540, current_train_items 211744.
I0302 19:01:28.688451 22732373614720 run.py:483] Algo bellman_ford step 6617 current loss 0.646756, current_train_items 211776.
I0302 19:01:28.718771 22732373614720 run.py:483] Algo bellman_ford step 6618 current loss 0.680499, current_train_items 211808.
I0302 19:01:28.751929 22732373614720 run.py:483] Algo bellman_ford step 6619 current loss 0.777413, current_train_items 211840.
I0302 19:01:28.771056 22732373614720 run.py:483] Algo bellman_ford step 6620 current loss 0.356246, current_train_items 211872.
I0302 19:01:28.786536 22732373614720 run.py:483] Algo bellman_ford step 6621 current loss 0.467114, current_train_items 211904.
I0302 19:01:28.808297 22732373614720 run.py:483] Algo bellman_ford step 6622 current loss 0.753768, current_train_items 211936.
I0302 19:01:28.838970 22732373614720 run.py:483] Algo bellman_ford step 6623 current loss 0.762231, current_train_items 211968.
I0302 19:01:28.870246 22732373614720 run.py:483] Algo bellman_ford step 6624 current loss 0.815393, current_train_items 212000.
I0302 19:01:28.889509 22732373614720 run.py:483] Algo bellman_ford step 6625 current loss 0.401766, current_train_items 212032.
I0302 19:01:28.905548 22732373614720 run.py:483] Algo bellman_ford step 6626 current loss 0.575463, current_train_items 212064.
I0302 19:01:28.928602 22732373614720 run.py:483] Algo bellman_ford step 6627 current loss 0.756616, current_train_items 212096.
I0302 19:01:28.957631 22732373614720 run.py:483] Algo bellman_ford step 6628 current loss 0.729042, current_train_items 212128.
I0302 19:01:28.988581 22732373614720 run.py:483] Algo bellman_ford step 6629 current loss 0.887382, current_train_items 212160.
I0302 19:01:29.007703 22732373614720 run.py:483] Algo bellman_ford step 6630 current loss 0.373138, current_train_items 212192.
I0302 19:01:29.023680 22732373614720 run.py:483] Algo bellman_ford step 6631 current loss 0.569814, current_train_items 212224.
I0302 19:01:29.047376 22732373614720 run.py:483] Algo bellman_ford step 6632 current loss 0.695579, current_train_items 212256.
I0302 19:01:29.078252 22732373614720 run.py:483] Algo bellman_ford step 6633 current loss 0.792031, current_train_items 212288.
I0302 19:01:29.109825 22732373614720 run.py:483] Algo bellman_ford step 6634 current loss 0.861729, current_train_items 212320.
I0302 19:01:29.129132 22732373614720 run.py:483] Algo bellman_ford step 6635 current loss 0.487171, current_train_items 212352.
I0302 19:01:29.145148 22732373614720 run.py:483] Algo bellman_ford step 6636 current loss 0.687291, current_train_items 212384.
I0302 19:01:29.168080 22732373614720 run.py:483] Algo bellman_ford step 6637 current loss 0.738825, current_train_items 212416.
I0302 19:01:29.198041 22732373614720 run.py:483] Algo bellman_ford step 6638 current loss 0.799157, current_train_items 212448.
I0302 19:01:29.230996 22732373614720 run.py:483] Algo bellman_ford step 6639 current loss 0.971106, current_train_items 212480.
I0302 19:01:29.250107 22732373614720 run.py:483] Algo bellman_ford step 6640 current loss 0.435816, current_train_items 212512.
I0302 19:01:29.266206 22732373614720 run.py:483] Algo bellman_ford step 6641 current loss 0.577960, current_train_items 212544.
I0302 19:01:29.289828 22732373614720 run.py:483] Algo bellman_ford step 6642 current loss 0.762815, current_train_items 212576.
I0302 19:01:29.319361 22732373614720 run.py:483] Algo bellman_ford step 6643 current loss 0.773079, current_train_items 212608.
I0302 19:01:29.352499 22732373614720 run.py:483] Algo bellman_ford step 6644 current loss 0.864097, current_train_items 212640.
I0302 19:01:29.371756 22732373614720 run.py:483] Algo bellman_ford step 6645 current loss 0.429342, current_train_items 212672.
I0302 19:01:29.387533 22732373614720 run.py:483] Algo bellman_ford step 6646 current loss 0.595648, current_train_items 212704.
I0302 19:01:29.410424 22732373614720 run.py:483] Algo bellman_ford step 6647 current loss 0.901813, current_train_items 212736.
I0302 19:01:29.440893 22732373614720 run.py:483] Algo bellman_ford step 6648 current loss 0.825506, current_train_items 212768.
I0302 19:01:29.474292 22732373614720 run.py:483] Algo bellman_ford step 6649 current loss 0.800278, current_train_items 212800.
I0302 19:01:29.493541 22732373614720 run.py:483] Algo bellman_ford step 6650 current loss 0.400805, current_train_items 212832.
I0302 19:01:29.501760 22732373614720 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0302 19:01:29.501871 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 19:01:29.518894 22732373614720 run.py:483] Algo bellman_ford step 6651 current loss 0.645544, current_train_items 212864.
I0302 19:01:29.542946 22732373614720 run.py:483] Algo bellman_ford step 6652 current loss 0.868094, current_train_items 212896.
I0302 19:01:29.574296 22732373614720 run.py:483] Algo bellman_ford step 6653 current loss 0.954589, current_train_items 212928.
I0302 19:01:29.603361 22732373614720 run.py:483] Algo bellman_ford step 6654 current loss 0.960688, current_train_items 212960.
I0302 19:01:29.623074 22732373614720 run.py:483] Algo bellman_ford step 6655 current loss 0.423675, current_train_items 212992.
I0302 19:01:29.638574 22732373614720 run.py:483] Algo bellman_ford step 6656 current loss 0.664199, current_train_items 213024.
I0302 19:01:29.661212 22732373614720 run.py:483] Algo bellman_ford step 6657 current loss 0.639504, current_train_items 213056.
I0302 19:01:29.692030 22732373614720 run.py:483] Algo bellman_ford step 6658 current loss 0.831726, current_train_items 213088.
I0302 19:01:29.726457 22732373614720 run.py:483] Algo bellman_ford step 6659 current loss 0.984600, current_train_items 213120.
I0302 19:01:29.746446 22732373614720 run.py:483] Algo bellman_ford step 6660 current loss 0.505424, current_train_items 213152.
I0302 19:01:29.762712 22732373614720 run.py:483] Algo bellman_ford step 6661 current loss 0.588868, current_train_items 213184.
I0302 19:01:29.786909 22732373614720 run.py:483] Algo bellman_ford step 6662 current loss 0.724459, current_train_items 213216.
I0302 19:01:29.815999 22732373614720 run.py:483] Algo bellman_ford step 6663 current loss 0.762256, current_train_items 213248.
I0302 19:01:29.848616 22732373614720 run.py:483] Algo bellman_ford step 6664 current loss 0.863869, current_train_items 213280.
I0302 19:01:29.868167 22732373614720 run.py:483] Algo bellman_ford step 6665 current loss 0.386958, current_train_items 213312.
I0302 19:01:29.884105 22732373614720 run.py:483] Algo bellman_ford step 6666 current loss 0.620976, current_train_items 213344.
I0302 19:01:29.908849 22732373614720 run.py:483] Algo bellman_ford step 6667 current loss 0.745535, current_train_items 213376.
I0302 19:01:29.940296 22732373614720 run.py:483] Algo bellman_ford step 6668 current loss 0.772332, current_train_items 213408.
I0302 19:01:29.974656 22732373614720 run.py:483] Algo bellman_ford step 6669 current loss 0.925370, current_train_items 213440.
I0302 19:01:29.994870 22732373614720 run.py:483] Algo bellman_ford step 6670 current loss 0.496006, current_train_items 213472.
I0302 19:01:30.010794 22732373614720 run.py:483] Algo bellman_ford step 6671 current loss 0.608357, current_train_items 213504.
I0302 19:01:30.033654 22732373614720 run.py:483] Algo bellman_ford step 6672 current loss 0.756830, current_train_items 213536.
I0302 19:01:30.064271 22732373614720 run.py:483] Algo bellman_ford step 6673 current loss 0.771472, current_train_items 213568.
I0302 19:01:30.095808 22732373614720 run.py:483] Algo bellman_ford step 6674 current loss 0.864847, current_train_items 213600.
I0302 19:01:30.115471 22732373614720 run.py:483] Algo bellman_ford step 6675 current loss 0.474513, current_train_items 213632.
I0302 19:01:30.131020 22732373614720 run.py:483] Algo bellman_ford step 6676 current loss 0.528646, current_train_items 213664.
I0302 19:01:30.154595 22732373614720 run.py:483] Algo bellman_ford step 6677 current loss 0.784735, current_train_items 213696.
I0302 19:01:30.186037 22732373614720 run.py:483] Algo bellman_ford step 6678 current loss 0.979149, current_train_items 213728.
I0302 19:01:30.220728 22732373614720 run.py:483] Algo bellman_ford step 6679 current loss 1.150914, current_train_items 213760.
I0302 19:01:30.240246 22732373614720 run.py:483] Algo bellman_ford step 6680 current loss 0.345058, current_train_items 213792.
I0302 19:01:30.256252 22732373614720 run.py:483] Algo bellman_ford step 6681 current loss 0.574183, current_train_items 213824.
I0302 19:01:30.280680 22732373614720 run.py:483] Algo bellman_ford step 6682 current loss 0.731161, current_train_items 213856.
I0302 19:01:30.310620 22732373614720 run.py:483] Algo bellman_ford step 6683 current loss 0.777172, current_train_items 213888.
I0302 19:01:30.345900 22732373614720 run.py:483] Algo bellman_ford step 6684 current loss 1.060704, current_train_items 213920.
I0302 19:01:30.365792 22732373614720 run.py:483] Algo bellman_ford step 6685 current loss 0.381872, current_train_items 213952.
I0302 19:01:30.381632 22732373614720 run.py:483] Algo bellman_ford step 6686 current loss 0.612432, current_train_items 213984.
I0302 19:01:30.404348 22732373614720 run.py:483] Algo bellman_ford step 6687 current loss 0.680122, current_train_items 214016.
I0302 19:01:30.435094 22732373614720 run.py:483] Algo bellman_ford step 6688 current loss 0.792637, current_train_items 214048.
I0302 19:01:30.467618 22732373614720 run.py:483] Algo bellman_ford step 6689 current loss 0.896542, current_train_items 214080.
I0302 19:01:30.487395 22732373614720 run.py:483] Algo bellman_ford step 6690 current loss 0.391316, current_train_items 214112.
I0302 19:01:30.502999 22732373614720 run.py:483] Algo bellman_ford step 6691 current loss 0.502275, current_train_items 214144.
I0302 19:01:30.526696 22732373614720 run.py:483] Algo bellman_ford step 6692 current loss 0.804428, current_train_items 214176.
I0302 19:01:30.556988 22732373614720 run.py:483] Algo bellman_ford step 6693 current loss 0.728216, current_train_items 214208.
I0302 19:01:30.590703 22732373614720 run.py:483] Algo bellman_ford step 6694 current loss 0.831374, current_train_items 214240.
I0302 19:01:30.609910 22732373614720 run.py:483] Algo bellman_ford step 6695 current loss 0.345302, current_train_items 214272.
I0302 19:01:30.625915 22732373614720 run.py:483] Algo bellman_ford step 6696 current loss 0.596552, current_train_items 214304.
I0302 19:01:30.649139 22732373614720 run.py:483] Algo bellman_ford step 6697 current loss 0.746268, current_train_items 214336.
I0302 19:01:30.680202 22732373614720 run.py:483] Algo bellman_ford step 6698 current loss 0.759205, current_train_items 214368.
I0302 19:01:30.714062 22732373614720 run.py:483] Algo bellman_ford step 6699 current loss 0.861975, current_train_items 214400.
I0302 19:01:30.733545 22732373614720 run.py:483] Algo bellman_ford step 6700 current loss 0.498869, current_train_items 214432.
I0302 19:01:30.741481 22732373614720 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0302 19:01:30.741595 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:01:30.758712 22732373614720 run.py:483] Algo bellman_ford step 6701 current loss 0.610301, current_train_items 214464.
I0302 19:01:30.782494 22732373614720 run.py:483] Algo bellman_ford step 6702 current loss 0.718421, current_train_items 214496.
I0302 19:01:30.813803 22732373614720 run.py:483] Algo bellman_ford step 6703 current loss 0.799868, current_train_items 214528.
I0302 19:01:30.846406 22732373614720 run.py:483] Algo bellman_ford step 6704 current loss 0.834515, current_train_items 214560.
I0302 19:01:30.866548 22732373614720 run.py:483] Algo bellman_ford step 6705 current loss 0.341803, current_train_items 214592.
I0302 19:01:30.882359 22732373614720 run.py:483] Algo bellman_ford step 6706 current loss 0.586682, current_train_items 214624.
I0302 19:01:30.906621 22732373614720 run.py:483] Algo bellman_ford step 6707 current loss 0.715510, current_train_items 214656.
I0302 19:01:30.937243 22732373614720 run.py:483] Algo bellman_ford step 6708 current loss 0.700606, current_train_items 214688.
I0302 19:01:30.971109 22732373614720 run.py:483] Algo bellman_ford step 6709 current loss 0.885974, current_train_items 214720.
I0302 19:01:30.990613 22732373614720 run.py:483] Algo bellman_ford step 6710 current loss 0.485058, current_train_items 214752.
I0302 19:01:31.006791 22732373614720 run.py:483] Algo bellman_ford step 6711 current loss 0.626405, current_train_items 214784.
I0302 19:01:31.029216 22732373614720 run.py:483] Algo bellman_ford step 6712 current loss 0.731314, current_train_items 214816.
I0302 19:01:31.061138 22732373614720 run.py:483] Algo bellman_ford step 6713 current loss 0.864470, current_train_items 214848.
I0302 19:01:31.093705 22732373614720 run.py:483] Algo bellman_ford step 6714 current loss 0.757087, current_train_items 214880.
I0302 19:01:31.113033 22732373614720 run.py:483] Algo bellman_ford step 6715 current loss 0.476015, current_train_items 214912.
I0302 19:01:31.129047 22732373614720 run.py:483] Algo bellman_ford step 6716 current loss 0.535153, current_train_items 214944.
I0302 19:01:31.152252 22732373614720 run.py:483] Algo bellman_ford step 6717 current loss 0.665013, current_train_items 214976.
I0302 19:01:31.183190 22732373614720 run.py:483] Algo bellman_ford step 6718 current loss 0.658328, current_train_items 215008.
I0302 19:01:31.217706 22732373614720 run.py:483] Algo bellman_ford step 6719 current loss 0.813141, current_train_items 215040.
I0302 19:01:31.236784 22732373614720 run.py:483] Algo bellman_ford step 6720 current loss 0.347674, current_train_items 215072.
I0302 19:01:31.252764 22732373614720 run.py:483] Algo bellman_ford step 6721 current loss 0.577037, current_train_items 215104.
I0302 19:01:31.275095 22732373614720 run.py:483] Algo bellman_ford step 6722 current loss 0.662698, current_train_items 215136.
I0302 19:01:31.306473 22732373614720 run.py:483] Algo bellman_ford step 6723 current loss 0.789426, current_train_items 215168.
I0302 19:01:31.338515 22732373614720 run.py:483] Algo bellman_ford step 6724 current loss 0.771708, current_train_items 215200.
I0302 19:01:31.358026 22732373614720 run.py:483] Algo bellman_ford step 6725 current loss 0.326336, current_train_items 215232.
I0302 19:01:31.374008 22732373614720 run.py:483] Algo bellman_ford step 6726 current loss 0.589814, current_train_items 215264.
I0302 19:01:31.396547 22732373614720 run.py:483] Algo bellman_ford step 6727 current loss 0.618966, current_train_items 215296.
I0302 19:01:31.426975 22732373614720 run.py:483] Algo bellman_ford step 6728 current loss 0.692387, current_train_items 215328.
I0302 19:01:31.458362 22732373614720 run.py:483] Algo bellman_ford step 6729 current loss 0.793573, current_train_items 215360.
I0302 19:01:31.477776 22732373614720 run.py:483] Algo bellman_ford step 6730 current loss 0.410725, current_train_items 215392.
I0302 19:01:31.493530 22732373614720 run.py:483] Algo bellman_ford step 6731 current loss 0.533991, current_train_items 215424.
I0302 19:01:31.517024 22732373614720 run.py:483] Algo bellman_ford step 6732 current loss 0.806800, current_train_items 215456.
I0302 19:01:31.545536 22732373614720 run.py:483] Algo bellman_ford step 6733 current loss 0.748551, current_train_items 215488.
I0302 19:01:31.576295 22732373614720 run.py:483] Algo bellman_ford step 6734 current loss 0.817544, current_train_items 215520.
I0302 19:01:31.595618 22732373614720 run.py:483] Algo bellman_ford step 6735 current loss 0.376655, current_train_items 215552.
I0302 19:01:31.611701 22732373614720 run.py:483] Algo bellman_ford step 6736 current loss 0.604935, current_train_items 215584.
I0302 19:01:31.635462 22732373614720 run.py:483] Algo bellman_ford step 6737 current loss 0.676929, current_train_items 215616.
I0302 19:01:31.664894 22732373614720 run.py:483] Algo bellman_ford step 6738 current loss 0.710079, current_train_items 215648.
I0302 19:01:31.698889 22732373614720 run.py:483] Algo bellman_ford step 6739 current loss 0.921914, current_train_items 215680.
I0302 19:01:31.718202 22732373614720 run.py:483] Algo bellman_ford step 6740 current loss 0.380898, current_train_items 215712.
I0302 19:01:31.734276 22732373614720 run.py:483] Algo bellman_ford step 6741 current loss 0.518876, current_train_items 215744.
I0302 19:01:31.757940 22732373614720 run.py:483] Algo bellman_ford step 6742 current loss 0.661404, current_train_items 215776.
I0302 19:01:31.787226 22732373614720 run.py:483] Algo bellman_ford step 6743 current loss 0.681438, current_train_items 215808.
I0302 19:01:31.820974 22732373614720 run.py:483] Algo bellman_ford step 6744 current loss 0.934467, current_train_items 215840.
I0302 19:01:31.840319 22732373614720 run.py:483] Algo bellman_ford step 6745 current loss 0.403422, current_train_items 215872.
I0302 19:01:31.856493 22732373614720 run.py:483] Algo bellman_ford step 6746 current loss 0.641414, current_train_items 215904.
I0302 19:01:31.879545 22732373614720 run.py:483] Algo bellman_ford step 6747 current loss 0.657463, current_train_items 215936.
I0302 19:01:31.909588 22732373614720 run.py:483] Algo bellman_ford step 6748 current loss 0.677200, current_train_items 215968.
I0302 19:01:31.942846 22732373614720 run.py:483] Algo bellman_ford step 6749 current loss 0.983695, current_train_items 216000.
I0302 19:01:31.962083 22732373614720 run.py:483] Algo bellman_ford step 6750 current loss 0.467786, current_train_items 216032.
I0302 19:01:31.970310 22732373614720 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.8876953125, 'score': 0.8876953125, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0302 19:01:31.970421 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.888, val scores are: bellman_ford: 0.888
I0302 19:01:31.987162 22732373614720 run.py:483] Algo bellman_ford step 6751 current loss 0.553893, current_train_items 216064.
I0302 19:01:32.011609 22732373614720 run.py:483] Algo bellman_ford step 6752 current loss 0.815766, current_train_items 216096.
I0302 19:01:32.043632 22732373614720 run.py:483] Algo bellman_ford step 6753 current loss 0.850832, current_train_items 216128.
I0302 19:01:32.077896 22732373614720 run.py:483] Algo bellman_ford step 6754 current loss 0.827362, current_train_items 216160.
I0302 19:01:32.097810 22732373614720 run.py:483] Algo bellman_ford step 6755 current loss 0.375485, current_train_items 216192.
I0302 19:01:32.113997 22732373614720 run.py:483] Algo bellman_ford step 6756 current loss 0.634971, current_train_items 216224.
I0302 19:01:32.137150 22732373614720 run.py:483] Algo bellman_ford step 6757 current loss 0.623315, current_train_items 216256.
I0302 19:01:32.168181 22732373614720 run.py:483] Algo bellman_ford step 6758 current loss 0.884156, current_train_items 216288.
I0302 19:01:32.202499 22732373614720 run.py:483] Algo bellman_ford step 6759 current loss 0.968384, current_train_items 216320.
I0302 19:01:32.222479 22732373614720 run.py:483] Algo bellman_ford step 6760 current loss 0.375518, current_train_items 216352.
I0302 19:01:32.238772 22732373614720 run.py:483] Algo bellman_ford step 6761 current loss 0.463688, current_train_items 216384.
I0302 19:01:32.261793 22732373614720 run.py:483] Algo bellman_ford step 6762 current loss 0.638025, current_train_items 216416.
I0302 19:01:32.292760 22732373614720 run.py:483] Algo bellman_ford step 6763 current loss 0.778788, current_train_items 216448.
I0302 19:01:32.325045 22732373614720 run.py:483] Algo bellman_ford step 6764 current loss 0.886432, current_train_items 216480.
I0302 19:01:32.344522 22732373614720 run.py:483] Algo bellman_ford step 6765 current loss 0.357869, current_train_items 216512.
I0302 19:01:32.360800 22732373614720 run.py:483] Algo bellman_ford step 6766 current loss 0.508378, current_train_items 216544.
I0302 19:01:32.384073 22732373614720 run.py:483] Algo bellman_ford step 6767 current loss 0.605886, current_train_items 216576.
I0302 19:01:32.413663 22732373614720 run.py:483] Algo bellman_ford step 6768 current loss 0.667643, current_train_items 216608.
I0302 19:01:32.446435 22732373614720 run.py:483] Algo bellman_ford step 6769 current loss 0.880895, current_train_items 216640.
I0302 19:01:32.466391 22732373614720 run.py:483] Algo bellman_ford step 6770 current loss 0.350256, current_train_items 216672.
I0302 19:01:32.482302 22732373614720 run.py:483] Algo bellman_ford step 6771 current loss 0.537872, current_train_items 216704.
I0302 19:01:32.504418 22732373614720 run.py:483] Algo bellman_ford step 6772 current loss 0.601941, current_train_items 216736.
I0302 19:01:32.535069 22732373614720 run.py:483] Algo bellman_ford step 6773 current loss 0.720055, current_train_items 216768.
I0302 19:01:32.566596 22732373614720 run.py:483] Algo bellman_ford step 6774 current loss 0.748935, current_train_items 216800.
I0302 19:01:32.586509 22732373614720 run.py:483] Algo bellman_ford step 6775 current loss 0.347218, current_train_items 216832.
I0302 19:01:32.602564 22732373614720 run.py:483] Algo bellman_ford step 6776 current loss 0.502849, current_train_items 216864.
I0302 19:01:32.625137 22732373614720 run.py:483] Algo bellman_ford step 6777 current loss 0.672924, current_train_items 216896.
I0302 19:01:32.657098 22732373614720 run.py:483] Algo bellman_ford step 6778 current loss 0.749907, current_train_items 216928.
I0302 19:01:32.689759 22732373614720 run.py:483] Algo bellman_ford step 6779 current loss 0.697409, current_train_items 216960.
I0302 19:01:32.709078 22732373614720 run.py:483] Algo bellman_ford step 6780 current loss 0.394801, current_train_items 216992.
I0302 19:01:32.724879 22732373614720 run.py:483] Algo bellman_ford step 6781 current loss 0.570566, current_train_items 217024.
I0302 19:01:32.747704 22732373614720 run.py:483] Algo bellman_ford step 6782 current loss 0.725981, current_train_items 217056.
I0302 19:01:32.779275 22732373614720 run.py:483] Algo bellman_ford step 6783 current loss 0.768301, current_train_items 217088.
I0302 19:01:32.811761 22732373614720 run.py:483] Algo bellman_ford step 6784 current loss 1.028651, current_train_items 217120.
I0302 19:01:32.831457 22732373614720 run.py:483] Algo bellman_ford step 6785 current loss 0.342615, current_train_items 217152.
I0302 19:01:32.847221 22732373614720 run.py:483] Algo bellman_ford step 6786 current loss 0.502785, current_train_items 217184.
I0302 19:01:32.869651 22732373614720 run.py:483] Algo bellman_ford step 6787 current loss 0.630016, current_train_items 217216.
I0302 19:01:32.898924 22732373614720 run.py:483] Algo bellman_ford step 6788 current loss 0.679178, current_train_items 217248.
I0302 19:01:32.932479 22732373614720 run.py:483] Algo bellman_ford step 6789 current loss 0.865245, current_train_items 217280.
I0302 19:01:32.952505 22732373614720 run.py:483] Algo bellman_ford step 6790 current loss 0.483346, current_train_items 217312.
I0302 19:01:32.968410 22732373614720 run.py:483] Algo bellman_ford step 6791 current loss 0.512544, current_train_items 217344.
I0302 19:01:32.991505 22732373614720 run.py:483] Algo bellman_ford step 6792 current loss 0.597327, current_train_items 217376.
I0302 19:01:33.021124 22732373614720 run.py:483] Algo bellman_ford step 6793 current loss 0.682530, current_train_items 217408.
I0302 19:01:33.054067 22732373614720 run.py:483] Algo bellman_ford step 6794 current loss 0.910694, current_train_items 217440.
I0302 19:01:33.073642 22732373614720 run.py:483] Algo bellman_ford step 6795 current loss 0.363110, current_train_items 217472.
I0302 19:01:33.089417 22732373614720 run.py:483] Algo bellman_ford step 6796 current loss 0.502073, current_train_items 217504.
I0302 19:01:33.113294 22732373614720 run.py:483] Algo bellman_ford step 6797 current loss 0.739000, current_train_items 217536.
I0302 19:01:33.143163 22732373614720 run.py:483] Algo bellman_ford step 6798 current loss 0.764165, current_train_items 217568.
I0302 19:01:33.176075 22732373614720 run.py:483] Algo bellman_ford step 6799 current loss 0.861371, current_train_items 217600.
I0302 19:01:33.195995 22732373614720 run.py:483] Algo bellman_ford step 6800 current loss 0.398165, current_train_items 217632.
I0302 19:01:33.203787 22732373614720 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0302 19:01:33.203902 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:01:33.220267 22732373614720 run.py:483] Algo bellman_ford step 6801 current loss 0.511897, current_train_items 217664.
I0302 19:01:33.243863 22732373614720 run.py:483] Algo bellman_ford step 6802 current loss 0.737164, current_train_items 217696.
I0302 19:01:33.276060 22732373614720 run.py:483] Algo bellman_ford step 6803 current loss 0.864091, current_train_items 217728.
I0302 19:01:33.309651 22732373614720 run.py:483] Algo bellman_ford step 6804 current loss 0.892655, current_train_items 217760.
I0302 19:01:33.329822 22732373614720 run.py:483] Algo bellman_ford step 6805 current loss 0.329153, current_train_items 217792.
I0302 19:01:33.345683 22732373614720 run.py:483] Algo bellman_ford step 6806 current loss 0.405525, current_train_items 217824.
I0302 19:01:33.369314 22732373614720 run.py:483] Algo bellman_ford step 6807 current loss 0.734977, current_train_items 217856.
I0302 19:01:33.398684 22732373614720 run.py:483] Algo bellman_ford step 6808 current loss 0.697848, current_train_items 217888.
I0302 19:01:33.431457 22732373614720 run.py:483] Algo bellman_ford step 6809 current loss 0.837031, current_train_items 217920.
I0302 19:01:33.451290 22732373614720 run.py:483] Algo bellman_ford step 6810 current loss 0.420956, current_train_items 217952.
I0302 19:01:33.466803 22732373614720 run.py:483] Algo bellman_ford step 6811 current loss 0.448242, current_train_items 217984.
I0302 19:01:33.491039 22732373614720 run.py:483] Algo bellman_ford step 6812 current loss 0.717836, current_train_items 218016.
I0302 19:01:33.521031 22732373614720 run.py:483] Algo bellman_ford step 6813 current loss 0.736225, current_train_items 218048.
I0302 19:01:33.552218 22732373614720 run.py:483] Algo bellman_ford step 6814 current loss 0.825574, current_train_items 218080.
I0302 19:01:33.571593 22732373614720 run.py:483] Algo bellman_ford step 6815 current loss 0.388893, current_train_items 218112.
I0302 19:01:33.587543 22732373614720 run.py:483] Algo bellman_ford step 6816 current loss 0.475721, current_train_items 218144.
I0302 19:01:33.609968 22732373614720 run.py:483] Algo bellman_ford step 6817 current loss 0.727373, current_train_items 218176.
I0302 19:01:33.640872 22732373614720 run.py:483] Algo bellman_ford step 6818 current loss 0.770111, current_train_items 218208.
I0302 19:01:33.675873 22732373614720 run.py:483] Algo bellman_ford step 6819 current loss 1.074696, current_train_items 218240.
I0302 19:01:33.695767 22732373614720 run.py:483] Algo bellman_ford step 6820 current loss 0.366220, current_train_items 218272.
I0302 19:01:33.711649 22732373614720 run.py:483] Algo bellman_ford step 6821 current loss 0.513352, current_train_items 218304.
I0302 19:01:33.734278 22732373614720 run.py:483] Algo bellman_ford step 6822 current loss 0.718091, current_train_items 218336.
I0302 19:01:33.764935 22732373614720 run.py:483] Algo bellman_ford step 6823 current loss 0.860015, current_train_items 218368.
I0302 19:01:33.799023 22732373614720 run.py:483] Algo bellman_ford step 6824 current loss 0.892635, current_train_items 218400.
I0302 19:01:33.818815 22732373614720 run.py:483] Algo bellman_ford step 6825 current loss 0.334422, current_train_items 218432.
I0302 19:01:33.834981 22732373614720 run.py:483] Algo bellman_ford step 6826 current loss 0.512505, current_train_items 218464.
I0302 19:01:33.858457 22732373614720 run.py:483] Algo bellman_ford step 6827 current loss 0.639960, current_train_items 218496.
I0302 19:01:33.888523 22732373614720 run.py:483] Algo bellman_ford step 6828 current loss 0.664564, current_train_items 218528.
I0302 19:01:33.923820 22732373614720 run.py:483] Algo bellman_ford step 6829 current loss 0.867228, current_train_items 218560.
I0302 19:01:33.943363 22732373614720 run.py:483] Algo bellman_ford step 6830 current loss 0.388496, current_train_items 218592.
I0302 19:01:33.959323 22732373614720 run.py:483] Algo bellman_ford step 6831 current loss 0.461088, current_train_items 218624.
I0302 19:01:33.984372 22732373614720 run.py:483] Algo bellman_ford step 6832 current loss 0.705890, current_train_items 218656.
I0302 19:01:34.014814 22732373614720 run.py:483] Algo bellman_ford step 6833 current loss 0.706132, current_train_items 218688.
I0302 19:01:34.047912 22732373614720 run.py:483] Algo bellman_ford step 6834 current loss 0.858182, current_train_items 218720.
I0302 19:01:34.067546 22732373614720 run.py:483] Algo bellman_ford step 6835 current loss 0.408717, current_train_items 218752.
I0302 19:01:34.083718 22732373614720 run.py:483] Algo bellman_ford step 6836 current loss 0.694746, current_train_items 218784.
I0302 19:01:34.106914 22732373614720 run.py:483] Algo bellman_ford step 6837 current loss 0.775788, current_train_items 218816.
I0302 19:01:34.138303 22732373614720 run.py:483] Algo bellman_ford step 6838 current loss 0.886599, current_train_items 218848.
I0302 19:01:34.169493 22732373614720 run.py:483] Algo bellman_ford step 6839 current loss 0.993200, current_train_items 218880.
I0302 19:01:34.188733 22732373614720 run.py:483] Algo bellman_ford step 6840 current loss 0.370095, current_train_items 218912.
I0302 19:01:34.205085 22732373614720 run.py:483] Algo bellman_ford step 6841 current loss 0.635486, current_train_items 218944.
I0302 19:01:34.229113 22732373614720 run.py:483] Algo bellman_ford step 6842 current loss 0.665435, current_train_items 218976.
I0302 19:01:34.258927 22732373614720 run.py:483] Algo bellman_ford step 6843 current loss 0.612569, current_train_items 219008.
I0302 19:01:34.291372 22732373614720 run.py:483] Algo bellman_ford step 6844 current loss 0.908022, current_train_items 219040.
I0302 19:01:34.310537 22732373614720 run.py:483] Algo bellman_ford step 6845 current loss 0.357033, current_train_items 219072.
I0302 19:01:34.327011 22732373614720 run.py:483] Algo bellman_ford step 6846 current loss 0.477060, current_train_items 219104.
I0302 19:01:34.351054 22732373614720 run.py:483] Algo bellman_ford step 6847 current loss 0.686183, current_train_items 219136.
I0302 19:01:34.381977 22732373614720 run.py:483] Algo bellman_ford step 6848 current loss 0.788187, current_train_items 219168.
I0302 19:01:34.415846 22732373614720 run.py:483] Algo bellman_ford step 6849 current loss 0.836172, current_train_items 219200.
I0302 19:01:34.435129 22732373614720 run.py:483] Algo bellman_ford step 6850 current loss 0.424364, current_train_items 219232.
I0302 19:01:34.443319 22732373614720 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0302 19:01:34.443428 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 19:01:34.460307 22732373614720 run.py:483] Algo bellman_ford step 6851 current loss 0.554271, current_train_items 219264.
I0302 19:01:34.484415 22732373614720 run.py:483] Algo bellman_ford step 6852 current loss 0.779310, current_train_items 219296.
I0302 19:01:34.515613 22732373614720 run.py:483] Algo bellman_ford step 6853 current loss 0.777018, current_train_items 219328.
I0302 19:01:34.549213 22732373614720 run.py:483] Algo bellman_ford step 6854 current loss 0.851555, current_train_items 219360.
I0302 19:01:34.569333 22732373614720 run.py:483] Algo bellman_ford step 6855 current loss 0.405747, current_train_items 219392.
I0302 19:01:34.584842 22732373614720 run.py:483] Algo bellman_ford step 6856 current loss 0.526395, current_train_items 219424.
I0302 19:01:34.607911 22732373614720 run.py:483] Algo bellman_ford step 6857 current loss 0.774086, current_train_items 219456.
I0302 19:01:34.637962 22732373614720 run.py:483] Algo bellman_ford step 6858 current loss 0.730595, current_train_items 219488.
I0302 19:01:34.671182 22732373614720 run.py:483] Algo bellman_ford step 6859 current loss 0.810920, current_train_items 219520.
I0302 19:01:34.690928 22732373614720 run.py:483] Algo bellman_ford step 6860 current loss 0.416212, current_train_items 219552.
I0302 19:01:34.706901 22732373614720 run.py:483] Algo bellman_ford step 6861 current loss 0.532966, current_train_items 219584.
I0302 19:01:34.729484 22732373614720 run.py:483] Algo bellman_ford step 6862 current loss 0.620504, current_train_items 219616.
I0302 19:01:34.759197 22732373614720 run.py:483] Algo bellman_ford step 6863 current loss 0.672128, current_train_items 219648.
I0302 19:01:34.790611 22732373614720 run.py:483] Algo bellman_ford step 6864 current loss 0.805636, current_train_items 219680.
I0302 19:01:34.810178 22732373614720 run.py:483] Algo bellman_ford step 6865 current loss 0.335721, current_train_items 219712.
I0302 19:01:34.826184 22732373614720 run.py:483] Algo bellman_ford step 6866 current loss 0.482810, current_train_items 219744.
I0302 19:01:34.850564 22732373614720 run.py:483] Algo bellman_ford step 6867 current loss 0.729925, current_train_items 219776.
I0302 19:01:34.881399 22732373614720 run.py:483] Algo bellman_ford step 6868 current loss 0.715215, current_train_items 219808.
I0302 19:01:34.916718 22732373614720 run.py:483] Algo bellman_ford step 6869 current loss 0.923409, current_train_items 219840.
I0302 19:01:34.936547 22732373614720 run.py:483] Algo bellman_ford step 6870 current loss 0.378174, current_train_items 219872.
I0302 19:01:34.952816 22732373614720 run.py:483] Algo bellman_ford step 6871 current loss 0.462429, current_train_items 219904.
I0302 19:01:34.975669 22732373614720 run.py:483] Algo bellman_ford step 6872 current loss 0.647181, current_train_items 219936.
I0302 19:01:35.005140 22732373614720 run.py:483] Algo bellman_ford step 6873 current loss 0.672324, current_train_items 219968.
I0302 19:01:35.037972 22732373614720 run.py:483] Algo bellman_ford step 6874 current loss 0.847105, current_train_items 220000.
I0302 19:01:35.057676 22732373614720 run.py:483] Algo bellman_ford step 6875 current loss 0.404019, current_train_items 220032.
I0302 19:01:35.073383 22732373614720 run.py:483] Algo bellman_ford step 6876 current loss 0.561773, current_train_items 220064.
I0302 19:01:35.096281 22732373614720 run.py:483] Algo bellman_ford step 6877 current loss 0.683715, current_train_items 220096.
I0302 19:01:35.126680 22732373614720 run.py:483] Algo bellman_ford step 6878 current loss 0.674379, current_train_items 220128.
I0302 19:01:35.160450 22732373614720 run.py:483] Algo bellman_ford step 6879 current loss 0.977558, current_train_items 220160.
I0302 19:01:35.179872 22732373614720 run.py:483] Algo bellman_ford step 6880 current loss 0.379207, current_train_items 220192.
I0302 19:01:35.195587 22732373614720 run.py:483] Algo bellman_ford step 6881 current loss 0.598633, current_train_items 220224.
I0302 19:01:35.219785 22732373614720 run.py:483] Algo bellman_ford step 6882 current loss 0.746098, current_train_items 220256.
I0302 19:01:35.249473 22732373614720 run.py:483] Algo bellman_ford step 6883 current loss 0.660855, current_train_items 220288.
I0302 19:01:35.282056 22732373614720 run.py:483] Algo bellman_ford step 6884 current loss 0.789892, current_train_items 220320.
I0302 19:01:35.302052 22732373614720 run.py:483] Algo bellman_ford step 6885 current loss 0.372395, current_train_items 220352.
I0302 19:01:35.318273 22732373614720 run.py:483] Algo bellman_ford step 6886 current loss 0.514734, current_train_items 220384.
I0302 19:01:35.341827 22732373614720 run.py:483] Algo bellman_ford step 6887 current loss 0.821582, current_train_items 220416.
I0302 19:01:35.373392 22732373614720 run.py:483] Algo bellman_ford step 6888 current loss 1.015890, current_train_items 220448.
I0302 19:01:35.406558 22732373614720 run.py:483] Algo bellman_ford step 6889 current loss 1.002566, current_train_items 220480.
I0302 19:01:35.426268 22732373614720 run.py:483] Algo bellman_ford step 6890 current loss 0.445473, current_train_items 220512.
I0302 19:01:35.442593 22732373614720 run.py:483] Algo bellman_ford step 6891 current loss 0.588821, current_train_items 220544.
I0302 19:01:35.465946 22732373614720 run.py:483] Algo bellman_ford step 6892 current loss 0.789052, current_train_items 220576.
I0302 19:01:35.496918 22732373614720 run.py:483] Algo bellman_ford step 6893 current loss 0.865930, current_train_items 220608.
I0302 19:01:35.529062 22732373614720 run.py:483] Algo bellman_ford step 6894 current loss 1.175821, current_train_items 220640.
I0302 19:01:35.548656 22732373614720 run.py:483] Algo bellman_ford step 6895 current loss 0.368326, current_train_items 220672.
I0302 19:01:35.564494 22732373614720 run.py:483] Algo bellman_ford step 6896 current loss 0.535784, current_train_items 220704.
I0302 19:01:35.588444 22732373614720 run.py:483] Algo bellman_ford step 6897 current loss 0.791779, current_train_items 220736.
I0302 19:01:35.618148 22732373614720 run.py:483] Algo bellman_ford step 6898 current loss 0.557606, current_train_items 220768.
I0302 19:01:35.651960 22732373614720 run.py:483] Algo bellman_ford step 6899 current loss 0.882372, current_train_items 220800.
I0302 19:01:35.672063 22732373614720 run.py:483] Algo bellman_ford step 6900 current loss 0.467084, current_train_items 220832.
I0302 19:01:35.679971 22732373614720 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0302 19:01:35.680081 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:01:35.696346 22732373614720 run.py:483] Algo bellman_ford step 6901 current loss 0.497810, current_train_items 220864.
I0302 19:01:35.719610 22732373614720 run.py:483] Algo bellman_ford step 6902 current loss 0.758641, current_train_items 220896.
I0302 19:01:35.751268 22732373614720 run.py:483] Algo bellman_ford step 6903 current loss 0.771446, current_train_items 220928.
I0302 19:01:35.785963 22732373614720 run.py:483] Algo bellman_ford step 6904 current loss 0.776827, current_train_items 220960.
I0302 19:01:35.805519 22732373614720 run.py:483] Algo bellman_ford step 6905 current loss 0.431723, current_train_items 220992.
I0302 19:01:35.820793 22732373614720 run.py:483] Algo bellman_ford step 6906 current loss 0.471267, current_train_items 221024.
I0302 19:01:35.843994 22732373614720 run.py:483] Algo bellman_ford step 6907 current loss 0.686169, current_train_items 221056.
I0302 19:01:35.875721 22732373614720 run.py:483] Algo bellman_ford step 6908 current loss 0.909650, current_train_items 221088.
I0302 19:01:35.909554 22732373614720 run.py:483] Algo bellman_ford step 6909 current loss 0.969638, current_train_items 221120.
I0302 19:01:35.929113 22732373614720 run.py:483] Algo bellman_ford step 6910 current loss 0.358238, current_train_items 221152.
I0302 19:01:35.945680 22732373614720 run.py:483] Algo bellman_ford step 6911 current loss 0.579441, current_train_items 221184.
I0302 19:01:35.967775 22732373614720 run.py:483] Algo bellman_ford step 6912 current loss 0.671216, current_train_items 221216.
I0302 19:01:35.999163 22732373614720 run.py:483] Algo bellman_ford step 6913 current loss 0.672794, current_train_items 221248.
I0302 19:01:36.033381 22732373614720 run.py:483] Algo bellman_ford step 6914 current loss 0.887003, current_train_items 221280.
I0302 19:01:36.052559 22732373614720 run.py:483] Algo bellman_ford step 6915 current loss 0.497864, current_train_items 221312.
I0302 19:01:36.068100 22732373614720 run.py:483] Algo bellman_ford step 6916 current loss 0.467858, current_train_items 221344.
I0302 19:01:36.091984 22732373614720 run.py:483] Algo bellman_ford step 6917 current loss 0.688827, current_train_items 221376.
I0302 19:01:36.122896 22732373614720 run.py:483] Algo bellman_ford step 6918 current loss 0.737168, current_train_items 221408.
I0302 19:01:36.157292 22732373614720 run.py:483] Algo bellman_ford step 6919 current loss 0.776152, current_train_items 221440.
I0302 19:01:36.176783 22732373614720 run.py:483] Algo bellman_ford step 6920 current loss 0.356065, current_train_items 221472.
I0302 19:01:36.192651 22732373614720 run.py:483] Algo bellman_ford step 6921 current loss 0.485582, current_train_items 221504.
I0302 19:01:36.215143 22732373614720 run.py:483] Algo bellman_ford step 6922 current loss 0.658359, current_train_items 221536.
I0302 19:01:36.245994 22732373614720 run.py:483] Algo bellman_ford step 6923 current loss 0.752403, current_train_items 221568.
I0302 19:01:36.279197 22732373614720 run.py:483] Algo bellman_ford step 6924 current loss 0.778878, current_train_items 221600.
I0302 19:01:36.298631 22732373614720 run.py:483] Algo bellman_ford step 6925 current loss 0.337696, current_train_items 221632.
I0302 19:01:36.314398 22732373614720 run.py:483] Algo bellman_ford step 6926 current loss 0.415506, current_train_items 221664.
I0302 19:01:36.337779 22732373614720 run.py:483] Algo bellman_ford step 6927 current loss 0.718175, current_train_items 221696.
I0302 19:01:36.368340 22732373614720 run.py:483] Algo bellman_ford step 6928 current loss 0.731594, current_train_items 221728.
I0302 19:01:36.400752 22732373614720 run.py:483] Algo bellman_ford step 6929 current loss 0.831938, current_train_items 221760.
I0302 19:01:36.420215 22732373614720 run.py:483] Algo bellman_ford step 6930 current loss 0.346931, current_train_items 221792.
I0302 19:01:36.436320 22732373614720 run.py:483] Algo bellman_ford step 6931 current loss 0.511596, current_train_items 221824.
I0302 19:01:36.459104 22732373614720 run.py:483] Algo bellman_ford step 6932 current loss 0.641741, current_train_items 221856.
I0302 19:01:36.489557 22732373614720 run.py:483] Algo bellman_ford step 6933 current loss 0.726470, current_train_items 221888.
I0302 19:01:36.523317 22732373614720 run.py:483] Algo bellman_ford step 6934 current loss 0.923334, current_train_items 221920.
I0302 19:01:36.542699 22732373614720 run.py:483] Algo bellman_ford step 6935 current loss 0.373456, current_train_items 221952.
I0302 19:01:36.558449 22732373614720 run.py:483] Algo bellman_ford step 6936 current loss 0.507761, current_train_items 221984.
I0302 19:01:36.581355 22732373614720 run.py:483] Algo bellman_ford step 6937 current loss 0.625090, current_train_items 222016.
I0302 19:01:36.611931 22732373614720 run.py:483] Algo bellman_ford step 6938 current loss 0.667459, current_train_items 222048.
I0302 19:01:36.643741 22732373614720 run.py:483] Algo bellman_ford step 6939 current loss 0.701027, current_train_items 222080.
I0302 19:01:36.663177 22732373614720 run.py:483] Algo bellman_ford step 6940 current loss 0.385369, current_train_items 222112.
I0302 19:01:36.679518 22732373614720 run.py:483] Algo bellman_ford step 6941 current loss 0.632333, current_train_items 222144.
I0302 19:01:36.703052 22732373614720 run.py:483] Algo bellman_ford step 6942 current loss 0.716048, current_train_items 222176.
I0302 19:01:36.733623 22732373614720 run.py:483] Algo bellman_ford step 6943 current loss 0.684754, current_train_items 222208.
I0302 19:01:36.767197 22732373614720 run.py:483] Algo bellman_ford step 6944 current loss 0.939847, current_train_items 222240.
I0302 19:01:36.786523 22732373614720 run.py:483] Algo bellman_ford step 6945 current loss 0.374244, current_train_items 222272.
I0302 19:01:36.802700 22732373614720 run.py:483] Algo bellman_ford step 6946 current loss 0.587451, current_train_items 222304.
I0302 19:01:36.825848 22732373614720 run.py:483] Algo bellman_ford step 6947 current loss 0.742014, current_train_items 222336.
I0302 19:01:36.854501 22732373614720 run.py:483] Algo bellman_ford step 6948 current loss 0.671367, current_train_items 222368.
I0302 19:01:36.888295 22732373614720 run.py:483] Algo bellman_ford step 6949 current loss 0.899434, current_train_items 222400.
I0302 19:01:36.907556 22732373614720 run.py:483] Algo bellman_ford step 6950 current loss 0.421807, current_train_items 222432.
I0302 19:01:36.915498 22732373614720 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0302 19:01:36.915609 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:01:36.932468 22732373614720 run.py:483] Algo bellman_ford step 6951 current loss 0.604404, current_train_items 222464.
I0302 19:01:36.956722 22732373614720 run.py:483] Algo bellman_ford step 6952 current loss 0.802598, current_train_items 222496.
I0302 19:01:36.987131 22732373614720 run.py:483] Algo bellman_ford step 6953 current loss 0.762424, current_train_items 222528.
I0302 19:01:37.021259 22732373614720 run.py:483] Algo bellman_ford step 6954 current loss 0.819648, current_train_items 222560.
I0302 19:01:37.041276 22732373614720 run.py:483] Algo bellman_ford step 6955 current loss 0.393220, current_train_items 222592.
I0302 19:01:37.057070 22732373614720 run.py:483] Algo bellman_ford step 6956 current loss 0.486741, current_train_items 222624.
I0302 19:01:37.081794 22732373614720 run.py:483] Algo bellman_ford step 6957 current loss 0.713572, current_train_items 222656.
I0302 19:01:37.111954 22732373614720 run.py:483] Algo bellman_ford step 6958 current loss 0.791173, current_train_items 222688.
I0302 19:01:37.143284 22732373614720 run.py:483] Algo bellman_ford step 6959 current loss 0.850457, current_train_items 222720.
I0302 19:01:37.163056 22732373614720 run.py:483] Algo bellman_ford step 6960 current loss 0.374678, current_train_items 222752.
I0302 19:01:37.179375 22732373614720 run.py:483] Algo bellman_ford step 6961 current loss 0.548854, current_train_items 222784.
I0302 19:01:37.201678 22732373614720 run.py:483] Algo bellman_ford step 6962 current loss 0.714945, current_train_items 222816.
I0302 19:01:37.232022 22732373614720 run.py:483] Algo bellman_ford step 6963 current loss 0.852418, current_train_items 222848.
I0302 19:01:37.266430 22732373614720 run.py:483] Algo bellman_ford step 6964 current loss 0.781663, current_train_items 222880.
I0302 19:01:37.285872 22732373614720 run.py:483] Algo bellman_ford step 6965 current loss 0.398840, current_train_items 222912.
I0302 19:01:37.301876 22732373614720 run.py:483] Algo bellman_ford step 6966 current loss 0.590600, current_train_items 222944.
I0302 19:01:37.325046 22732373614720 run.py:483] Algo bellman_ford step 6967 current loss 0.694634, current_train_items 222976.
I0302 19:01:37.354873 22732373614720 run.py:483] Algo bellman_ford step 6968 current loss 0.795262, current_train_items 223008.
I0302 19:01:37.387617 22732373614720 run.py:483] Algo bellman_ford step 6969 current loss 1.005183, current_train_items 223040.
I0302 19:01:37.407142 22732373614720 run.py:483] Algo bellman_ford step 6970 current loss 0.356186, current_train_items 223072.
I0302 19:01:37.423665 22732373614720 run.py:483] Algo bellman_ford step 6971 current loss 0.425330, current_train_items 223104.
I0302 19:01:37.445930 22732373614720 run.py:483] Algo bellman_ford step 6972 current loss 0.723728, current_train_items 223136.
I0302 19:01:37.475966 22732373614720 run.py:483] Algo bellman_ford step 6973 current loss 0.740334, current_train_items 223168.
I0302 19:01:37.509698 22732373614720 run.py:483] Algo bellman_ford step 6974 current loss 0.838642, current_train_items 223200.
I0302 19:01:37.529356 22732373614720 run.py:483] Algo bellman_ford step 6975 current loss 0.401690, current_train_items 223232.
I0302 19:01:37.545636 22732373614720 run.py:483] Algo bellman_ford step 6976 current loss 0.507079, current_train_items 223264.
I0302 19:01:37.568550 22732373614720 run.py:483] Algo bellman_ford step 6977 current loss 0.674269, current_train_items 223296.
I0302 19:01:37.597964 22732373614720 run.py:483] Algo bellman_ford step 6978 current loss 0.718238, current_train_items 223328.
I0302 19:01:37.631520 22732373614720 run.py:483] Algo bellman_ford step 6979 current loss 0.776044, current_train_items 223360.
I0302 19:01:37.650991 22732373614720 run.py:483] Algo bellman_ford step 6980 current loss 0.398855, current_train_items 223392.
I0302 19:01:37.667648 22732373614720 run.py:483] Algo bellman_ford step 6981 current loss 0.517704, current_train_items 223424.
I0302 19:01:37.689922 22732373614720 run.py:483] Algo bellman_ford step 6982 current loss 0.559961, current_train_items 223456.
I0302 19:01:37.721233 22732373614720 run.py:483] Algo bellman_ford step 6983 current loss 0.751286, current_train_items 223488.
I0302 19:01:37.754295 22732373614720 run.py:483] Algo bellman_ford step 6984 current loss 0.763852, current_train_items 223520.
I0302 19:01:37.773990 22732373614720 run.py:483] Algo bellman_ford step 6985 current loss 0.310403, current_train_items 223552.
I0302 19:01:37.789965 22732373614720 run.py:483] Algo bellman_ford step 6986 current loss 0.467187, current_train_items 223584.
I0302 19:01:37.812736 22732373614720 run.py:483] Algo bellman_ford step 6987 current loss 0.736594, current_train_items 223616.
I0302 19:01:37.843428 22732373614720 run.py:483] Algo bellman_ford step 6988 current loss 0.807145, current_train_items 223648.
I0302 19:01:37.878135 22732373614720 run.py:483] Algo bellman_ford step 6989 current loss 0.984643, current_train_items 223680.
I0302 19:01:37.898082 22732373614720 run.py:483] Algo bellman_ford step 6990 current loss 0.366980, current_train_items 223712.
I0302 19:01:37.914091 22732373614720 run.py:483] Algo bellman_ford step 6991 current loss 0.576200, current_train_items 223744.
I0302 19:01:37.936908 22732373614720 run.py:483] Algo bellman_ford step 6992 current loss 0.733991, current_train_items 223776.
I0302 19:01:37.968380 22732373614720 run.py:483] Algo bellman_ford step 6993 current loss 0.774207, current_train_items 223808.
I0302 19:01:38.003993 22732373614720 run.py:483] Algo bellman_ford step 6994 current loss 0.816108, current_train_items 223840.
I0302 19:01:38.023534 22732373614720 run.py:483] Algo bellman_ford step 6995 current loss 0.422751, current_train_items 223872.
I0302 19:01:38.039847 22732373614720 run.py:483] Algo bellman_ford step 6996 current loss 0.473722, current_train_items 223904.
I0302 19:01:38.061466 22732373614720 run.py:483] Algo bellman_ford step 6997 current loss 0.631210, current_train_items 223936.
I0302 19:01:38.091294 22732373614720 run.py:483] Algo bellman_ford step 6998 current loss 0.657141, current_train_items 223968.
I0302 19:01:38.124323 22732373614720 run.py:483] Algo bellman_ford step 6999 current loss 0.909455, current_train_items 224000.
I0302 19:01:38.144069 22732373614720 run.py:483] Algo bellman_ford step 7000 current loss 0.373619, current_train_items 224032.
I0302 19:01:38.152001 22732373614720 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.880859375, 'score': 0.880859375, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0302 19:01:38.152112 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.881, val scores are: bellman_ford: 0.881
I0302 19:01:38.168558 22732373614720 run.py:483] Algo bellman_ford step 7001 current loss 0.508879, current_train_items 224064.
I0302 19:01:38.191457 22732373614720 run.py:483] Algo bellman_ford step 7002 current loss 0.665611, current_train_items 224096.
I0302 19:01:38.220833 22732373614720 run.py:483] Algo bellman_ford step 7003 current loss 0.717133, current_train_items 224128.
I0302 19:01:38.253012 22732373614720 run.py:483] Algo bellman_ford step 7004 current loss 0.872131, current_train_items 224160.
I0302 19:01:38.272782 22732373614720 run.py:483] Algo bellman_ford step 7005 current loss 0.318988, current_train_items 224192.
I0302 19:01:38.288663 22732373614720 run.py:483] Algo bellman_ford step 7006 current loss 0.536780, current_train_items 224224.
I0302 19:01:38.312402 22732373614720 run.py:483] Algo bellman_ford step 7007 current loss 0.683996, current_train_items 224256.
I0302 19:01:38.341255 22732373614720 run.py:483] Algo bellman_ford step 7008 current loss 0.736983, current_train_items 224288.
I0302 19:01:38.376399 22732373614720 run.py:483] Algo bellman_ford step 7009 current loss 0.999896, current_train_items 224320.
I0302 19:01:38.395905 22732373614720 run.py:483] Algo bellman_ford step 7010 current loss 0.374523, current_train_items 224352.
I0302 19:01:38.412099 22732373614720 run.py:483] Algo bellman_ford step 7011 current loss 0.519907, current_train_items 224384.
I0302 19:01:38.434894 22732373614720 run.py:483] Algo bellman_ford step 7012 current loss 0.666882, current_train_items 224416.
I0302 19:01:38.464756 22732373614720 run.py:483] Algo bellman_ford step 7013 current loss 0.683250, current_train_items 224448.
I0302 19:01:38.497912 22732373614720 run.py:483] Algo bellman_ford step 7014 current loss 0.735776, current_train_items 224480.
I0302 19:01:38.517508 22732373614720 run.py:483] Algo bellman_ford step 7015 current loss 0.318832, current_train_items 224512.
I0302 19:01:38.533429 22732373614720 run.py:483] Algo bellman_ford step 7016 current loss 0.451969, current_train_items 224544.
I0302 19:01:38.556603 22732373614720 run.py:483] Algo bellman_ford step 7017 current loss 0.568971, current_train_items 224576.
I0302 19:01:38.586195 22732373614720 run.py:483] Algo bellman_ford step 7018 current loss 0.651480, current_train_items 224608.
I0302 19:01:38.618395 22732373614720 run.py:483] Algo bellman_ford step 7019 current loss 0.700318, current_train_items 224640.
I0302 19:01:38.637601 22732373614720 run.py:483] Algo bellman_ford step 7020 current loss 0.360614, current_train_items 224672.
I0302 19:01:38.653600 22732373614720 run.py:483] Algo bellman_ford step 7021 current loss 0.622606, current_train_items 224704.
I0302 19:01:38.676247 22732373614720 run.py:483] Algo bellman_ford step 7022 current loss 0.608963, current_train_items 224736.
I0302 19:01:38.705770 22732373614720 run.py:483] Algo bellman_ford step 7023 current loss 0.604249, current_train_items 224768.
I0302 19:01:38.739208 22732373614720 run.py:483] Algo bellman_ford step 7024 current loss 0.823511, current_train_items 224800.
I0302 19:01:38.758739 22732373614720 run.py:483] Algo bellman_ford step 7025 current loss 0.445488, current_train_items 224832.
I0302 19:01:38.775223 22732373614720 run.py:483] Algo bellman_ford step 7026 current loss 0.513694, current_train_items 224864.
I0302 19:01:38.799551 22732373614720 run.py:483] Algo bellman_ford step 7027 current loss 0.802788, current_train_items 224896.
I0302 19:01:38.831763 22732373614720 run.py:483] Algo bellman_ford step 7028 current loss 0.882028, current_train_items 224928.
I0302 19:01:38.863134 22732373614720 run.py:483] Algo bellman_ford step 7029 current loss 0.789330, current_train_items 224960.
I0302 19:01:38.882547 22732373614720 run.py:483] Algo bellman_ford step 7030 current loss 0.410545, current_train_items 224992.
I0302 19:01:38.898755 22732373614720 run.py:483] Algo bellman_ford step 7031 current loss 0.479434, current_train_items 225024.
I0302 19:01:38.921569 22732373614720 run.py:483] Algo bellman_ford step 7032 current loss 0.626428, current_train_items 225056.
I0302 19:01:38.951502 22732373614720 run.py:483] Algo bellman_ford step 7033 current loss 0.701470, current_train_items 225088.
I0302 19:01:38.986089 22732373614720 run.py:483] Algo bellman_ford step 7034 current loss 0.920160, current_train_items 225120.
I0302 19:01:39.005790 22732373614720 run.py:483] Algo bellman_ford step 7035 current loss 0.294435, current_train_items 225152.
I0302 19:01:39.021879 22732373614720 run.py:483] Algo bellman_ford step 7036 current loss 0.494907, current_train_items 225184.
I0302 19:01:39.044800 22732373614720 run.py:483] Algo bellman_ford step 7037 current loss 0.613849, current_train_items 225216.
I0302 19:01:39.075762 22732373614720 run.py:483] Algo bellman_ford step 7038 current loss 0.708312, current_train_items 225248.
I0302 19:01:39.108138 22732373614720 run.py:483] Algo bellman_ford step 7039 current loss 0.771103, current_train_items 225280.
I0302 19:01:39.127620 22732373614720 run.py:483] Algo bellman_ford step 7040 current loss 0.337866, current_train_items 225312.
I0302 19:01:39.143708 22732373614720 run.py:483] Algo bellman_ford step 7041 current loss 0.506146, current_train_items 225344.
I0302 19:01:39.166926 22732373614720 run.py:483] Algo bellman_ford step 7042 current loss 0.684823, current_train_items 225376.
I0302 19:01:39.197547 22732373614720 run.py:483] Algo bellman_ford step 7043 current loss 0.795841, current_train_items 225408.
I0302 19:01:39.230558 22732373614720 run.py:483] Algo bellman_ford step 7044 current loss 0.778787, current_train_items 225440.
I0302 19:01:39.249844 22732373614720 run.py:483] Algo bellman_ford step 7045 current loss 0.372508, current_train_items 225472.
I0302 19:01:39.266007 22732373614720 run.py:483] Algo bellman_ford step 7046 current loss 0.619098, current_train_items 225504.
I0302 19:01:39.288557 22732373614720 run.py:483] Algo bellman_ford step 7047 current loss 0.738793, current_train_items 225536.
I0302 19:01:39.319384 22732373614720 run.py:483] Algo bellman_ford step 7048 current loss 0.792268, current_train_items 225568.
I0302 19:01:39.351608 22732373614720 run.py:483] Algo bellman_ford step 7049 current loss 0.864412, current_train_items 225600.
I0302 19:01:39.370965 22732373614720 run.py:483] Algo bellman_ford step 7050 current loss 0.357968, current_train_items 225632.
I0302 19:01:39.379258 22732373614720 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0302 19:01:39.379367 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:01:39.395551 22732373614720 run.py:483] Algo bellman_ford step 7051 current loss 0.506672, current_train_items 225664.
I0302 19:01:39.419708 22732373614720 run.py:483] Algo bellman_ford step 7052 current loss 0.750087, current_train_items 225696.
I0302 19:01:39.451938 22732373614720 run.py:483] Algo bellman_ford step 7053 current loss 0.877257, current_train_items 225728.
I0302 19:01:39.486570 22732373614720 run.py:483] Algo bellman_ford step 7054 current loss 0.864634, current_train_items 225760.
I0302 19:01:39.506783 22732373614720 run.py:483] Algo bellman_ford step 7055 current loss 0.377510, current_train_items 225792.
I0302 19:01:39.522639 22732373614720 run.py:483] Algo bellman_ford step 7056 current loss 0.514345, current_train_items 225824.
I0302 19:01:39.546462 22732373614720 run.py:483] Algo bellman_ford step 7057 current loss 0.705059, current_train_items 225856.
I0302 19:01:39.576970 22732373614720 run.py:483] Algo bellman_ford step 7058 current loss 0.753982, current_train_items 225888.
I0302 19:01:39.611649 22732373614720 run.py:483] Algo bellman_ford step 7059 current loss 0.962291, current_train_items 225920.
I0302 19:01:39.631257 22732373614720 run.py:483] Algo bellman_ford step 7060 current loss 0.356275, current_train_items 225952.
I0302 19:01:39.647390 22732373614720 run.py:483] Algo bellman_ford step 7061 current loss 0.559399, current_train_items 225984.
I0302 19:01:39.671576 22732373614720 run.py:483] Algo bellman_ford step 7062 current loss 0.655632, current_train_items 226016.
I0302 19:01:39.699817 22732373614720 run.py:483] Algo bellman_ford step 7063 current loss 0.646215, current_train_items 226048.
I0302 19:01:39.732821 22732373614720 run.py:483] Algo bellman_ford step 7064 current loss 0.879792, current_train_items 226080.
I0302 19:01:39.752216 22732373614720 run.py:483] Algo bellman_ford step 7065 current loss 0.377181, current_train_items 226112.
I0302 19:01:39.768642 22732373614720 run.py:483] Algo bellman_ford step 7066 current loss 0.487235, current_train_items 226144.
I0302 19:01:39.791481 22732373614720 run.py:483] Algo bellman_ford step 7067 current loss 0.613129, current_train_items 226176.
I0302 19:01:39.823714 22732373614720 run.py:483] Algo bellman_ford step 7068 current loss 0.751647, current_train_items 226208.
I0302 19:01:39.858657 22732373614720 run.py:483] Algo bellman_ford step 7069 current loss 1.001569, current_train_items 226240.
I0302 19:01:39.878854 22732373614720 run.py:483] Algo bellman_ford step 7070 current loss 0.366238, current_train_items 226272.
I0302 19:01:39.894902 22732373614720 run.py:483] Algo bellman_ford step 7071 current loss 0.644760, current_train_items 226304.
I0302 19:01:39.916376 22732373614720 run.py:483] Algo bellman_ford step 7072 current loss 0.665045, current_train_items 226336.
I0302 19:01:39.946012 22732373614720 run.py:483] Algo bellman_ford step 7073 current loss 0.671553, current_train_items 226368.
I0302 19:01:39.978545 22732373614720 run.py:483] Algo bellman_ford step 7074 current loss 0.826651, current_train_items 226400.
I0302 19:01:39.998473 22732373614720 run.py:483] Algo bellman_ford step 7075 current loss 0.312071, current_train_items 226432.
I0302 19:01:40.014040 22732373614720 run.py:483] Algo bellman_ford step 7076 current loss 0.479997, current_train_items 226464.
I0302 19:01:40.036613 22732373614720 run.py:483] Algo bellman_ford step 7077 current loss 0.727059, current_train_items 226496.
I0302 19:01:40.067744 22732373614720 run.py:483] Algo bellman_ford step 7078 current loss 0.709125, current_train_items 226528.
I0302 19:01:40.101556 22732373614720 run.py:483] Algo bellman_ford step 7079 current loss 0.917191, current_train_items 226560.
I0302 19:01:40.120821 22732373614720 run.py:483] Algo bellman_ford step 7080 current loss 0.334387, current_train_items 226592.
I0302 19:01:40.136609 22732373614720 run.py:483] Algo bellman_ford step 7081 current loss 0.480993, current_train_items 226624.
I0302 19:01:40.159625 22732373614720 run.py:483] Algo bellman_ford step 7082 current loss 0.696325, current_train_items 226656.
I0302 19:01:40.190117 22732373614720 run.py:483] Algo bellman_ford step 7083 current loss 0.748145, current_train_items 226688.
I0302 19:01:40.222567 22732373614720 run.py:483] Algo bellman_ford step 7084 current loss 0.940230, current_train_items 226720.
I0302 19:01:40.242341 22732373614720 run.py:483] Algo bellman_ford step 7085 current loss 0.391109, current_train_items 226752.
I0302 19:01:40.258299 22732373614720 run.py:483] Algo bellman_ford step 7086 current loss 0.508907, current_train_items 226784.
I0302 19:01:40.280261 22732373614720 run.py:483] Algo bellman_ford step 7087 current loss 0.613384, current_train_items 226816.
I0302 19:01:40.310695 22732373614720 run.py:483] Algo bellman_ford step 7088 current loss 0.735034, current_train_items 226848.
I0302 19:01:40.343432 22732373614720 run.py:483] Algo bellman_ford step 7089 current loss 0.779193, current_train_items 226880.
I0302 19:01:40.363458 22732373614720 run.py:483] Algo bellman_ford step 7090 current loss 0.353137, current_train_items 226912.
I0302 19:01:40.379293 22732373614720 run.py:483] Algo bellman_ford step 7091 current loss 0.478611, current_train_items 226944.
I0302 19:01:40.402317 22732373614720 run.py:483] Algo bellman_ford step 7092 current loss 0.684246, current_train_items 226976.
I0302 19:01:40.433574 22732373614720 run.py:483] Algo bellman_ford step 7093 current loss 0.819804, current_train_items 227008.
I0302 19:01:40.464883 22732373614720 run.py:483] Algo bellman_ford step 7094 current loss 0.873232, current_train_items 227040.
I0302 19:01:40.484746 22732373614720 run.py:483] Algo bellman_ford step 7095 current loss 0.386651, current_train_items 227072.
I0302 19:01:40.500846 22732373614720 run.py:483] Algo bellman_ford step 7096 current loss 0.525513, current_train_items 227104.
I0302 19:01:40.522741 22732373614720 run.py:483] Algo bellman_ford step 7097 current loss 0.579225, current_train_items 227136.
I0302 19:01:40.553955 22732373614720 run.py:483] Algo bellman_ford step 7098 current loss 0.943016, current_train_items 227168.
I0302 19:01:40.586198 22732373614720 run.py:483] Algo bellman_ford step 7099 current loss 0.794681, current_train_items 227200.
I0302 19:01:40.606313 22732373614720 run.py:483] Algo bellman_ford step 7100 current loss 0.395993, current_train_items 227232.
I0302 19:01:40.614212 22732373614720 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0302 19:01:40.614322 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 19:01:40.630896 22732373614720 run.py:483] Algo bellman_ford step 7101 current loss 0.535975, current_train_items 227264.
I0302 19:01:40.654123 22732373614720 run.py:483] Algo bellman_ford step 7102 current loss 0.676413, current_train_items 227296.
I0302 19:01:40.685317 22732373614720 run.py:483] Algo bellman_ford step 7103 current loss 0.777791, current_train_items 227328.
I0302 19:01:40.720400 22732373614720 run.py:483] Algo bellman_ford step 7104 current loss 0.974572, current_train_items 227360.
I0302 19:01:40.740232 22732373614720 run.py:483] Algo bellman_ford step 7105 current loss 0.402740, current_train_items 227392.
I0302 19:01:40.756141 22732373614720 run.py:483] Algo bellman_ford step 7106 current loss 0.503430, current_train_items 227424.
I0302 19:01:40.779632 22732373614720 run.py:483] Algo bellman_ford step 7107 current loss 0.691812, current_train_items 227456.
I0302 19:01:40.809838 22732373614720 run.py:483] Algo bellman_ford step 7108 current loss 0.686935, current_train_items 227488.
I0302 19:01:40.844801 22732373614720 run.py:483] Algo bellman_ford step 7109 current loss 0.977480, current_train_items 227520.
I0302 19:01:40.864431 22732373614720 run.py:483] Algo bellman_ford step 7110 current loss 0.470430, current_train_items 227552.
I0302 19:01:40.880631 22732373614720 run.py:483] Algo bellman_ford step 7111 current loss 0.538554, current_train_items 227584.
I0302 19:01:40.903620 22732373614720 run.py:483] Algo bellman_ford step 7112 current loss 0.690286, current_train_items 227616.
I0302 19:01:40.934917 22732373614720 run.py:483] Algo bellman_ford step 7113 current loss 0.869960, current_train_items 227648.
I0302 19:01:40.968299 22732373614720 run.py:483] Algo bellman_ford step 7114 current loss 0.970606, current_train_items 227680.
I0302 19:01:40.987656 22732373614720 run.py:483] Algo bellman_ford step 7115 current loss 0.402626, current_train_items 227712.
I0302 19:01:41.003776 22732373614720 run.py:483] Algo bellman_ford step 7116 current loss 0.683498, current_train_items 227744.
I0302 19:01:41.027139 22732373614720 run.py:483] Algo bellman_ford step 7117 current loss 0.646490, current_train_items 227776.
I0302 19:01:41.057974 22732373614720 run.py:483] Algo bellman_ford step 7118 current loss 0.916873, current_train_items 227808.
I0302 19:01:41.090954 22732373614720 run.py:483] Algo bellman_ford step 7119 current loss 1.000848, current_train_items 227840.
I0302 19:01:41.110180 22732373614720 run.py:483] Algo bellman_ford step 7120 current loss 0.389045, current_train_items 227872.
I0302 19:01:41.126290 22732373614720 run.py:483] Algo bellman_ford step 7121 current loss 0.610163, current_train_items 227904.
I0302 19:01:41.149242 22732373614720 run.py:483] Algo bellman_ford step 7122 current loss 0.715257, current_train_items 227936.
I0302 19:01:41.179045 22732373614720 run.py:483] Algo bellman_ford step 7123 current loss 0.730001, current_train_items 227968.
I0302 19:01:41.212224 22732373614720 run.py:483] Algo bellman_ford step 7124 current loss 1.072900, current_train_items 228000.
I0302 19:01:41.231605 22732373614720 run.py:483] Algo bellman_ford step 7125 current loss 0.373972, current_train_items 228032.
I0302 19:01:41.247220 22732373614720 run.py:483] Algo bellman_ford step 7126 current loss 0.506199, current_train_items 228064.
I0302 19:01:41.269536 22732373614720 run.py:483] Algo bellman_ford step 7127 current loss 0.629201, current_train_items 228096.
I0302 19:01:41.299172 22732373614720 run.py:483] Algo bellman_ford step 7128 current loss 0.715714, current_train_items 228128.
I0302 19:01:41.333312 22732373614720 run.py:483] Algo bellman_ford step 7129 current loss 0.909616, current_train_items 228160.
I0302 19:01:41.352819 22732373614720 run.py:483] Algo bellman_ford step 7130 current loss 0.372996, current_train_items 228192.
I0302 19:01:41.369076 22732373614720 run.py:483] Algo bellman_ford step 7131 current loss 0.488416, current_train_items 228224.
I0302 19:01:41.392564 22732373614720 run.py:483] Algo bellman_ford step 7132 current loss 0.631834, current_train_items 228256.
I0302 19:01:41.422981 22732373614720 run.py:483] Algo bellman_ford step 7133 current loss 0.786249, current_train_items 228288.
I0302 19:01:41.455349 22732373614720 run.py:483] Algo bellman_ford step 7134 current loss 0.793481, current_train_items 228320.
I0302 19:01:41.474477 22732373614720 run.py:483] Algo bellman_ford step 7135 current loss 0.333863, current_train_items 228352.
I0302 19:01:41.490062 22732373614720 run.py:483] Algo bellman_ford step 7136 current loss 0.460577, current_train_items 228384.
I0302 19:01:41.512287 22732373614720 run.py:483] Algo bellman_ford step 7137 current loss 0.671701, current_train_items 228416.
I0302 19:01:41.541076 22732373614720 run.py:483] Algo bellman_ford step 7138 current loss 0.665003, current_train_items 228448.
I0302 19:01:41.576256 22732373614720 run.py:483] Algo bellman_ford step 7139 current loss 0.954169, current_train_items 228480.
I0302 19:01:41.595545 22732373614720 run.py:483] Algo bellman_ford step 7140 current loss 0.365571, current_train_items 228512.
I0302 19:01:41.611251 22732373614720 run.py:483] Algo bellman_ford step 7141 current loss 0.564408, current_train_items 228544.
I0302 19:01:41.634596 22732373614720 run.py:483] Algo bellman_ford step 7142 current loss 0.668084, current_train_items 228576.
I0302 19:01:41.663073 22732373614720 run.py:483] Algo bellman_ford step 7143 current loss 0.616573, current_train_items 228608.
I0302 19:01:41.695281 22732373614720 run.py:483] Algo bellman_ford step 7144 current loss 0.748828, current_train_items 228640.
I0302 19:01:41.714585 22732373614720 run.py:483] Algo bellman_ford step 7145 current loss 0.347610, current_train_items 228672.
I0302 19:01:41.730855 22732373614720 run.py:483] Algo bellman_ford step 7146 current loss 0.544655, current_train_items 228704.
I0302 19:01:41.754245 22732373614720 run.py:483] Algo bellman_ford step 7147 current loss 0.777593, current_train_items 228736.
I0302 19:01:41.783479 22732373614720 run.py:483] Algo bellman_ford step 7148 current loss 0.696410, current_train_items 228768.
I0302 19:01:41.813957 22732373614720 run.py:483] Algo bellman_ford step 7149 current loss 0.805616, current_train_items 228800.
I0302 19:01:41.833034 22732373614720 run.py:483] Algo bellman_ford step 7150 current loss 0.341364, current_train_items 228832.
I0302 19:01:41.841353 22732373614720 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0302 19:01:41.841463 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:01:41.858563 22732373614720 run.py:483] Algo bellman_ford step 7151 current loss 0.522373, current_train_items 228864.
I0302 19:01:41.883041 22732373614720 run.py:483] Algo bellman_ford step 7152 current loss 0.793894, current_train_items 228896.
I0302 19:01:41.914438 22732373614720 run.py:483] Algo bellman_ford step 7153 current loss 0.820136, current_train_items 228928.
I0302 19:01:41.947205 22732373614720 run.py:483] Algo bellman_ford step 7154 current loss 0.733602, current_train_items 228960.
I0302 19:01:41.967617 22732373614720 run.py:483] Algo bellman_ford step 7155 current loss 0.433427, current_train_items 228992.
I0302 19:01:41.983662 22732373614720 run.py:483] Algo bellman_ford step 7156 current loss 0.596565, current_train_items 229024.
I0302 19:01:42.006958 22732373614720 run.py:483] Algo bellman_ford step 7157 current loss 0.848229, current_train_items 229056.
I0302 19:01:42.037151 22732373614720 run.py:483] Algo bellman_ford step 7158 current loss 0.963067, current_train_items 229088.
I0302 19:01:42.070708 22732373614720 run.py:483] Algo bellman_ford step 7159 current loss 1.433128, current_train_items 229120.
I0302 19:01:42.090955 22732373614720 run.py:483] Algo bellman_ford step 7160 current loss 0.306383, current_train_items 229152.
I0302 19:01:42.107475 22732373614720 run.py:483] Algo bellman_ford step 7161 current loss 0.727564, current_train_items 229184.
W0302 19:01:42.121912 22732373614720 samplers.py:155] Increasing hint lengh from 10 to 11
I0302 19:01:48.780128 22732373614720 run.py:483] Algo bellman_ford step 7162 current loss 0.765091, current_train_items 229216.
I0302 19:01:48.811986 22732373614720 run.py:483] Algo bellman_ford step 7163 current loss 0.785469, current_train_items 229248.
I0302 19:01:48.846641 22732373614720 run.py:483] Algo bellman_ford step 7164 current loss 1.090828, current_train_items 229280.
I0302 19:01:48.866978 22732373614720 run.py:483] Algo bellman_ford step 7165 current loss 0.399339, current_train_items 229312.
I0302 19:01:48.883192 22732373614720 run.py:483] Algo bellman_ford step 7166 current loss 0.585206, current_train_items 229344.
I0302 19:01:48.906435 22732373614720 run.py:483] Algo bellman_ford step 7167 current loss 0.856924, current_train_items 229376.
I0302 19:01:48.938054 22732373614720 run.py:483] Algo bellman_ford step 7168 current loss 0.799621, current_train_items 229408.
I0302 19:01:48.968717 22732373614720 run.py:483] Algo bellman_ford step 7169 current loss 0.939448, current_train_items 229440.
I0302 19:01:48.988933 22732373614720 run.py:483] Algo bellman_ford step 7170 current loss 0.405915, current_train_items 229472.
I0302 19:01:49.005291 22732373614720 run.py:483] Algo bellman_ford step 7171 current loss 0.573373, current_train_items 229504.
I0302 19:01:49.028210 22732373614720 run.py:483] Algo bellman_ford step 7172 current loss 0.741893, current_train_items 229536.
I0302 19:01:49.059162 22732373614720 run.py:483] Algo bellman_ford step 7173 current loss 0.714825, current_train_items 229568.
I0302 19:01:49.092858 22732373614720 run.py:483] Algo bellman_ford step 7174 current loss 0.963246, current_train_items 229600.
I0302 19:01:49.112817 22732373614720 run.py:483] Algo bellman_ford step 7175 current loss 0.378763, current_train_items 229632.
I0302 19:01:49.128604 22732373614720 run.py:483] Algo bellman_ford step 7176 current loss 0.587099, current_train_items 229664.
I0302 19:01:49.151808 22732373614720 run.py:483] Algo bellman_ford step 7177 current loss 0.761155, current_train_items 229696.
I0302 19:01:49.184498 22732373614720 run.py:483] Algo bellman_ford step 7178 current loss 0.781880, current_train_items 229728.
I0302 19:01:49.218936 22732373614720 run.py:483] Algo bellman_ford step 7179 current loss 0.840867, current_train_items 229760.
I0302 19:01:49.238435 22732373614720 run.py:483] Algo bellman_ford step 7180 current loss 0.325374, current_train_items 229792.
I0302 19:01:49.254415 22732373614720 run.py:483] Algo bellman_ford step 7181 current loss 0.565368, current_train_items 229824.
I0302 19:01:49.278538 22732373614720 run.py:483] Algo bellman_ford step 7182 current loss 0.753032, current_train_items 229856.
I0302 19:01:49.308808 22732373614720 run.py:483] Algo bellman_ford step 7183 current loss 0.819430, current_train_items 229888.
I0302 19:01:49.345047 22732373614720 run.py:483] Algo bellman_ford step 7184 current loss 1.005409, current_train_items 229920.
I0302 19:01:49.364935 22732373614720 run.py:483] Algo bellman_ford step 7185 current loss 0.379584, current_train_items 229952.
I0302 19:01:49.381076 22732373614720 run.py:483] Algo bellman_ford step 7186 current loss 0.665441, current_train_items 229984.
I0302 19:01:49.405526 22732373614720 run.py:483] Algo bellman_ford step 7187 current loss 0.757452, current_train_items 230016.
I0302 19:01:49.436800 22732373614720 run.py:483] Algo bellman_ford step 7188 current loss 0.992359, current_train_items 230048.
I0302 19:01:49.471529 22732373614720 run.py:483] Algo bellman_ford step 7189 current loss 1.116747, current_train_items 230080.
I0302 19:01:49.491250 22732373614720 run.py:483] Algo bellman_ford step 7190 current loss 0.352343, current_train_items 230112.
I0302 19:01:49.507686 22732373614720 run.py:483] Algo bellman_ford step 7191 current loss 0.501190, current_train_items 230144.
I0302 19:01:49.530737 22732373614720 run.py:483] Algo bellman_ford step 7192 current loss 0.737929, current_train_items 230176.
I0302 19:01:49.562134 22732373614720 run.py:483] Algo bellman_ford step 7193 current loss 0.699920, current_train_items 230208.
I0302 19:01:49.594891 22732373614720 run.py:483] Algo bellman_ford step 7194 current loss 0.878315, current_train_items 230240.
I0302 19:01:49.614433 22732373614720 run.py:483] Algo bellman_ford step 7195 current loss 0.343297, current_train_items 230272.
I0302 19:01:49.630237 22732373614720 run.py:483] Algo bellman_ford step 7196 current loss 0.565375, current_train_items 230304.
I0302 19:01:49.653502 22732373614720 run.py:483] Algo bellman_ford step 7197 current loss 0.632384, current_train_items 230336.
I0302 19:01:49.685008 22732373614720 run.py:483] Algo bellman_ford step 7198 current loss 0.741260, current_train_items 230368.
I0302 19:01:49.718221 22732373614720 run.py:483] Algo bellman_ford step 7199 current loss 0.928689, current_train_items 230400.
I0302 19:01:49.738197 22732373614720 run.py:483] Algo bellman_ford step 7200 current loss 0.377897, current_train_items 230432.
I0302 19:01:49.747651 22732373614720 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0302 19:01:49.747765 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:01:49.764557 22732373614720 run.py:483] Algo bellman_ford step 7201 current loss 0.598320, current_train_items 230464.
I0302 19:01:49.788142 22732373614720 run.py:483] Algo bellman_ford step 7202 current loss 0.645659, current_train_items 230496.
I0302 19:01:49.820342 22732373614720 run.py:483] Algo bellman_ford step 7203 current loss 0.745118, current_train_items 230528.
I0302 19:01:49.853759 22732373614720 run.py:483] Algo bellman_ford step 7204 current loss 0.827187, current_train_items 230560.
I0302 19:01:49.873914 22732373614720 run.py:483] Algo bellman_ford step 7205 current loss 0.428328, current_train_items 230592.
I0302 19:01:49.889539 22732373614720 run.py:483] Algo bellman_ford step 7206 current loss 0.562342, current_train_items 230624.
I0302 19:01:49.913434 22732373614720 run.py:483] Algo bellman_ford step 7207 current loss 0.730308, current_train_items 230656.
I0302 19:01:49.944045 22732373614720 run.py:483] Algo bellman_ford step 7208 current loss 0.737383, current_train_items 230688.
I0302 19:01:49.978268 22732373614720 run.py:483] Algo bellman_ford step 7209 current loss 0.942947, current_train_items 230720.
I0302 19:01:49.997872 22732373614720 run.py:483] Algo bellman_ford step 7210 current loss 0.354778, current_train_items 230752.
I0302 19:01:50.013782 22732373614720 run.py:483] Algo bellman_ford step 7211 current loss 0.492133, current_train_items 230784.
I0302 19:01:50.037169 22732373614720 run.py:483] Algo bellman_ford step 7212 current loss 0.621999, current_train_items 230816.
I0302 19:01:50.068042 22732373614720 run.py:483] Algo bellman_ford step 7213 current loss 0.659660, current_train_items 230848.
I0302 19:01:50.099884 22732373614720 run.py:483] Algo bellman_ford step 7214 current loss 0.878569, current_train_items 230880.
I0302 19:01:50.119729 22732373614720 run.py:483] Algo bellman_ford step 7215 current loss 0.448036, current_train_items 230912.
I0302 19:01:50.135892 22732373614720 run.py:483] Algo bellman_ford step 7216 current loss 0.628124, current_train_items 230944.
I0302 19:01:50.159825 22732373614720 run.py:483] Algo bellman_ford step 7217 current loss 0.666647, current_train_items 230976.
I0302 19:01:50.190829 22732373614720 run.py:483] Algo bellman_ford step 7218 current loss 0.742045, current_train_items 231008.
I0302 19:01:50.223499 22732373614720 run.py:483] Algo bellman_ford step 7219 current loss 0.818632, current_train_items 231040.
I0302 19:01:50.243187 22732373614720 run.py:483] Algo bellman_ford step 7220 current loss 0.429879, current_train_items 231072.
I0302 19:01:50.259370 22732373614720 run.py:483] Algo bellman_ford step 7221 current loss 0.604845, current_train_items 231104.
I0302 19:01:50.283947 22732373614720 run.py:483] Algo bellman_ford step 7222 current loss 0.756896, current_train_items 231136.
I0302 19:01:50.315016 22732373614720 run.py:483] Algo bellman_ford step 7223 current loss 0.834540, current_train_items 231168.
I0302 19:01:50.348999 22732373614720 run.py:483] Algo bellman_ford step 7224 current loss 0.908302, current_train_items 231200.
I0302 19:01:50.368889 22732373614720 run.py:483] Algo bellman_ford step 7225 current loss 0.312469, current_train_items 231232.
I0302 19:01:50.384998 22732373614720 run.py:483] Algo bellman_ford step 7226 current loss 0.448080, current_train_items 231264.
I0302 19:01:50.409649 22732373614720 run.py:483] Algo bellman_ford step 7227 current loss 0.736587, current_train_items 231296.
I0302 19:01:50.440525 22732373614720 run.py:483] Algo bellman_ford step 7228 current loss 0.647302, current_train_items 231328.
I0302 19:01:50.471688 22732373614720 run.py:483] Algo bellman_ford step 7229 current loss 0.958889, current_train_items 231360.
I0302 19:01:50.491373 22732373614720 run.py:483] Algo bellman_ford step 7230 current loss 0.365571, current_train_items 231392.
I0302 19:01:50.507506 22732373614720 run.py:483] Algo bellman_ford step 7231 current loss 0.493740, current_train_items 231424.
I0302 19:01:50.528703 22732373614720 run.py:483] Algo bellman_ford step 7232 current loss 0.554413, current_train_items 231456.
I0302 19:01:50.560436 22732373614720 run.py:483] Algo bellman_ford step 7233 current loss 0.711384, current_train_items 231488.
I0302 19:01:50.592621 22732373614720 run.py:483] Algo bellman_ford step 7234 current loss 0.775094, current_train_items 231520.
I0302 19:01:50.612131 22732373614720 run.py:483] Algo bellman_ford step 7235 current loss 0.352950, current_train_items 231552.
I0302 19:01:50.628415 22732373614720 run.py:483] Algo bellman_ford step 7236 current loss 0.615731, current_train_items 231584.
I0302 19:01:50.652621 22732373614720 run.py:483] Algo bellman_ford step 7237 current loss 0.601259, current_train_items 231616.
I0302 19:01:50.685173 22732373614720 run.py:483] Algo bellman_ford step 7238 current loss 0.746852, current_train_items 231648.
I0302 19:01:50.718207 22732373614720 run.py:483] Algo bellman_ford step 7239 current loss 0.739656, current_train_items 231680.
I0302 19:01:50.738043 22732373614720 run.py:483] Algo bellman_ford step 7240 current loss 0.415832, current_train_items 231712.
I0302 19:01:50.753942 22732373614720 run.py:483] Algo bellman_ford step 7241 current loss 0.505150, current_train_items 231744.
I0302 19:01:50.778125 22732373614720 run.py:483] Algo bellman_ford step 7242 current loss 0.801409, current_train_items 231776.
I0302 19:01:50.809327 22732373614720 run.py:483] Algo bellman_ford step 7243 current loss 0.734236, current_train_items 231808.
I0302 19:01:50.844102 22732373614720 run.py:483] Algo bellman_ford step 7244 current loss 0.784584, current_train_items 231840.
I0302 19:01:50.863430 22732373614720 run.py:483] Algo bellman_ford step 7245 current loss 0.366572, current_train_items 231872.
I0302 19:01:50.879369 22732373614720 run.py:483] Algo bellman_ford step 7246 current loss 0.465780, current_train_items 231904.
I0302 19:01:50.903614 22732373614720 run.py:483] Algo bellman_ford step 7247 current loss 0.632735, current_train_items 231936.
I0302 19:01:50.935843 22732373614720 run.py:483] Algo bellman_ford step 7248 current loss 0.761890, current_train_items 231968.
I0302 19:01:50.969887 22732373614720 run.py:483] Algo bellman_ford step 7249 current loss 0.739749, current_train_items 232000.
I0302 19:01:50.989743 22732373614720 run.py:483] Algo bellman_ford step 7250 current loss 0.427908, current_train_items 232032.
I0302 19:01:50.998018 22732373614720 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0302 19:01:50.998128 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:01:51.014948 22732373614720 run.py:483] Algo bellman_ford step 7251 current loss 0.463458, current_train_items 232064.
I0302 19:01:51.038460 22732373614720 run.py:483] Algo bellman_ford step 7252 current loss 0.612931, current_train_items 232096.
I0302 19:01:51.068852 22732373614720 run.py:483] Algo bellman_ford step 7253 current loss 0.645028, current_train_items 232128.
I0302 19:01:51.102131 22732373614720 run.py:483] Algo bellman_ford step 7254 current loss 0.734539, current_train_items 232160.
I0302 19:01:51.122088 22732373614720 run.py:483] Algo bellman_ford step 7255 current loss 0.391083, current_train_items 232192.
I0302 19:01:51.137634 22732373614720 run.py:483] Algo bellman_ford step 7256 current loss 0.495094, current_train_items 232224.
I0302 19:01:51.160903 22732373614720 run.py:483] Algo bellman_ford step 7257 current loss 0.655862, current_train_items 232256.
I0302 19:01:51.194207 22732373614720 run.py:483] Algo bellman_ford step 7258 current loss 0.799905, current_train_items 232288.
I0302 19:01:51.227995 22732373614720 run.py:483] Algo bellman_ford step 7259 current loss 0.867075, current_train_items 232320.
I0302 19:01:51.248357 22732373614720 run.py:483] Algo bellman_ford step 7260 current loss 0.423911, current_train_items 232352.
I0302 19:01:51.264513 22732373614720 run.py:483] Algo bellman_ford step 7261 current loss 0.593793, current_train_items 232384.
I0302 19:01:51.288088 22732373614720 run.py:483] Algo bellman_ford step 7262 current loss 0.633414, current_train_items 232416.
I0302 19:01:51.319471 22732373614720 run.py:483] Algo bellman_ford step 7263 current loss 0.872199, current_train_items 232448.
I0302 19:01:51.352665 22732373614720 run.py:483] Algo bellman_ford step 7264 current loss 0.925557, current_train_items 232480.
I0302 19:01:51.372138 22732373614720 run.py:483] Algo bellman_ford step 7265 current loss 0.349874, current_train_items 232512.
I0302 19:01:51.388122 22732373614720 run.py:483] Algo bellman_ford step 7266 current loss 0.616170, current_train_items 232544.
I0302 19:01:51.411166 22732373614720 run.py:483] Algo bellman_ford step 7267 current loss 0.836859, current_train_items 232576.
I0302 19:01:51.442259 22732373614720 run.py:483] Algo bellman_ford step 7268 current loss 0.774149, current_train_items 232608.
I0302 19:01:51.474528 22732373614720 run.py:483] Algo bellman_ford step 7269 current loss 0.986822, current_train_items 232640.
I0302 19:01:51.494559 22732373614720 run.py:483] Algo bellman_ford step 7270 current loss 0.354105, current_train_items 232672.
I0302 19:01:51.510897 22732373614720 run.py:483] Algo bellman_ford step 7271 current loss 0.639922, current_train_items 232704.
I0302 19:01:51.534316 22732373614720 run.py:483] Algo bellman_ford step 7272 current loss 0.647999, current_train_items 232736.
I0302 19:01:51.565125 22732373614720 run.py:483] Algo bellman_ford step 7273 current loss 0.705175, current_train_items 232768.
I0302 19:01:51.597930 22732373614720 run.py:483] Algo bellman_ford step 7274 current loss 0.844577, current_train_items 232800.
I0302 19:01:51.618006 22732373614720 run.py:483] Algo bellman_ford step 7275 current loss 0.350296, current_train_items 232832.
I0302 19:01:51.633963 22732373614720 run.py:483] Algo bellman_ford step 7276 current loss 0.506724, current_train_items 232864.
I0302 19:01:51.657141 22732373614720 run.py:483] Algo bellman_ford step 7277 current loss 0.845677, current_train_items 232896.
I0302 19:01:51.689013 22732373614720 run.py:483] Algo bellman_ford step 7278 current loss 1.411143, current_train_items 232928.
I0302 19:01:51.722213 22732373614720 run.py:483] Algo bellman_ford step 7279 current loss 1.470213, current_train_items 232960.
I0302 19:01:51.741820 22732373614720 run.py:483] Algo bellman_ford step 7280 current loss 0.360029, current_train_items 232992.
I0302 19:01:51.758326 22732373614720 run.py:483] Algo bellman_ford step 7281 current loss 0.575469, current_train_items 233024.
I0302 19:01:51.781388 22732373614720 run.py:483] Algo bellman_ford step 7282 current loss 0.756840, current_train_items 233056.
I0302 19:01:51.812880 22732373614720 run.py:483] Algo bellman_ford step 7283 current loss 0.852605, current_train_items 233088.
I0302 19:01:51.845318 22732373614720 run.py:483] Algo bellman_ford step 7284 current loss 0.937293, current_train_items 233120.
I0302 19:01:51.865535 22732373614720 run.py:483] Algo bellman_ford step 7285 current loss 0.354558, current_train_items 233152.
I0302 19:01:51.882641 22732373614720 run.py:483] Algo bellman_ford step 7286 current loss 0.928905, current_train_items 233184.
I0302 19:01:51.905747 22732373614720 run.py:483] Algo bellman_ford step 7287 current loss 0.872918, current_train_items 233216.
I0302 19:01:51.936923 22732373614720 run.py:483] Algo bellman_ford step 7288 current loss 0.866280, current_train_items 233248.
I0302 19:01:51.969290 22732373614720 run.py:483] Algo bellman_ford step 7289 current loss 0.822054, current_train_items 233280.
I0302 19:01:51.989272 22732373614720 run.py:483] Algo bellman_ford step 7290 current loss 0.395152, current_train_items 233312.
I0302 19:01:52.005402 22732373614720 run.py:483] Algo bellman_ford step 7291 current loss 0.644073, current_train_items 233344.
I0302 19:01:52.027710 22732373614720 run.py:483] Algo bellman_ford step 7292 current loss 0.698217, current_train_items 233376.
I0302 19:01:52.059109 22732373614720 run.py:483] Algo bellman_ford step 7293 current loss 0.968770, current_train_items 233408.
I0302 19:01:52.090775 22732373614720 run.py:483] Algo bellman_ford step 7294 current loss 1.104623, current_train_items 233440.
I0302 19:01:52.110402 22732373614720 run.py:483] Algo bellman_ford step 7295 current loss 0.410918, current_train_items 233472.
I0302 19:01:52.126494 22732373614720 run.py:483] Algo bellman_ford step 7296 current loss 0.509122, current_train_items 233504.
I0302 19:01:52.149738 22732373614720 run.py:483] Algo bellman_ford step 7297 current loss 0.686849, current_train_items 233536.
I0302 19:01:52.181034 22732373614720 run.py:483] Algo bellman_ford step 7298 current loss 0.725436, current_train_items 233568.
I0302 19:01:52.213847 22732373614720 run.py:483] Algo bellman_ford step 7299 current loss 0.839201, current_train_items 233600.
I0302 19:01:52.233624 22732373614720 run.py:483] Algo bellman_ford step 7300 current loss 0.445359, current_train_items 233632.
I0302 19:01:52.241594 22732373614720 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.87890625, 'score': 0.87890625, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0302 19:01:52.241705 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.879, val scores are: bellman_ford: 0.879
I0302 19:01:52.258787 22732373614720 run.py:483] Algo bellman_ford step 7301 current loss 0.524077, current_train_items 233664.
I0302 19:01:52.282621 22732373614720 run.py:483] Algo bellman_ford step 7302 current loss 0.647932, current_train_items 233696.
I0302 19:01:52.313739 22732373614720 run.py:483] Algo bellman_ford step 7303 current loss 0.721021, current_train_items 233728.
I0302 19:01:52.347653 22732373614720 run.py:483] Algo bellman_ford step 7304 current loss 0.866638, current_train_items 233760.
I0302 19:01:52.367637 22732373614720 run.py:483] Algo bellman_ford step 7305 current loss 0.357790, current_train_items 233792.
I0302 19:01:52.383380 22732373614720 run.py:483] Algo bellman_ford step 7306 current loss 0.428195, current_train_items 233824.
I0302 19:01:52.406683 22732373614720 run.py:483] Algo bellman_ford step 7307 current loss 0.701889, current_train_items 233856.
I0302 19:01:52.436978 22732373614720 run.py:483] Algo bellman_ford step 7308 current loss 0.694351, current_train_items 233888.
I0302 19:01:52.471166 22732373614720 run.py:483] Algo bellman_ford step 7309 current loss 1.018798, current_train_items 233920.
I0302 19:01:52.490814 22732373614720 run.py:483] Algo bellman_ford step 7310 current loss 0.330250, current_train_items 233952.
I0302 19:01:52.506726 22732373614720 run.py:483] Algo bellman_ford step 7311 current loss 0.527328, current_train_items 233984.
I0302 19:01:52.531769 22732373614720 run.py:483] Algo bellman_ford step 7312 current loss 0.772555, current_train_items 234016.
I0302 19:01:52.562867 22732373614720 run.py:483] Algo bellman_ford step 7313 current loss 0.696151, current_train_items 234048.
I0302 19:01:52.597128 22732373614720 run.py:483] Algo bellman_ford step 7314 current loss 0.815794, current_train_items 234080.
I0302 19:01:52.616850 22732373614720 run.py:483] Algo bellman_ford step 7315 current loss 0.414080, current_train_items 234112.
I0302 19:01:52.632772 22732373614720 run.py:483] Algo bellman_ford step 7316 current loss 0.546920, current_train_items 234144.
I0302 19:01:52.657299 22732373614720 run.py:483] Algo bellman_ford step 7317 current loss 0.731455, current_train_items 234176.
I0302 19:01:52.688469 22732373614720 run.py:483] Algo bellman_ford step 7318 current loss 0.733590, current_train_items 234208.
I0302 19:01:52.723152 22732373614720 run.py:483] Algo bellman_ford step 7319 current loss 0.931295, current_train_items 234240.
I0302 19:01:52.743013 22732373614720 run.py:483] Algo bellman_ford step 7320 current loss 0.336573, current_train_items 234272.
I0302 19:01:52.759611 22732373614720 run.py:483] Algo bellman_ford step 7321 current loss 0.527178, current_train_items 234304.
I0302 19:01:52.784299 22732373614720 run.py:483] Algo bellman_ford step 7322 current loss 0.720435, current_train_items 234336.
I0302 19:01:52.814751 22732373614720 run.py:483] Algo bellman_ford step 7323 current loss 0.706816, current_train_items 234368.
I0302 19:01:52.849304 22732373614720 run.py:483] Algo bellman_ford step 7324 current loss 0.870173, current_train_items 234400.
I0302 19:01:52.869010 22732373614720 run.py:483] Algo bellman_ford step 7325 current loss 0.419327, current_train_items 234432.
I0302 19:01:52.885453 22732373614720 run.py:483] Algo bellman_ford step 7326 current loss 0.615677, current_train_items 234464.
I0302 19:01:52.909346 22732373614720 run.py:483] Algo bellman_ford step 7327 current loss 0.663153, current_train_items 234496.
I0302 19:01:52.939924 22732373614720 run.py:483] Algo bellman_ford step 7328 current loss 0.741065, current_train_items 234528.
I0302 19:01:52.973401 22732373614720 run.py:483] Algo bellman_ford step 7329 current loss 0.738208, current_train_items 234560.
I0302 19:01:52.993033 22732373614720 run.py:483] Algo bellman_ford step 7330 current loss 0.381209, current_train_items 234592.
I0302 19:01:53.009421 22732373614720 run.py:483] Algo bellman_ford step 7331 current loss 0.595570, current_train_items 234624.
I0302 19:01:53.034430 22732373614720 run.py:483] Algo bellman_ford step 7332 current loss 0.653147, current_train_items 234656.
I0302 19:01:53.066165 22732373614720 run.py:483] Algo bellman_ford step 7333 current loss 0.783602, current_train_items 234688.
I0302 19:01:53.098140 22732373614720 run.py:483] Algo bellman_ford step 7334 current loss 0.819969, current_train_items 234720.
I0302 19:01:53.117836 22732373614720 run.py:483] Algo bellman_ford step 7335 current loss 0.376375, current_train_items 234752.
I0302 19:01:53.133608 22732373614720 run.py:483] Algo bellman_ford step 7336 current loss 0.487453, current_train_items 234784.
I0302 19:01:53.157191 22732373614720 run.py:483] Algo bellman_ford step 7337 current loss 0.634869, current_train_items 234816.
I0302 19:01:53.188605 22732373614720 run.py:483] Algo bellman_ford step 7338 current loss 0.653728, current_train_items 234848.
I0302 19:01:53.223352 22732373614720 run.py:483] Algo bellman_ford step 7339 current loss 0.835270, current_train_items 234880.
I0302 19:01:53.242888 22732373614720 run.py:483] Algo bellman_ford step 7340 current loss 0.339286, current_train_items 234912.
I0302 19:01:53.258759 22732373614720 run.py:483] Algo bellman_ford step 7341 current loss 0.572550, current_train_items 234944.
I0302 19:01:53.283067 22732373614720 run.py:483] Algo bellman_ford step 7342 current loss 0.751223, current_train_items 234976.
I0302 19:01:53.314658 22732373614720 run.py:483] Algo bellman_ford step 7343 current loss 0.698162, current_train_items 235008.
I0302 19:01:53.347447 22732373614720 run.py:483] Algo bellman_ford step 7344 current loss 0.846553, current_train_items 235040.
I0302 19:01:53.367137 22732373614720 run.py:483] Algo bellman_ford step 7345 current loss 0.379115, current_train_items 235072.
I0302 19:01:53.382845 22732373614720 run.py:483] Algo bellman_ford step 7346 current loss 0.465529, current_train_items 235104.
I0302 19:01:53.406776 22732373614720 run.py:483] Algo bellman_ford step 7347 current loss 0.731745, current_train_items 235136.
I0302 19:01:53.438277 22732373614720 run.py:483] Algo bellman_ford step 7348 current loss 0.676657, current_train_items 235168.
I0302 19:01:53.468010 22732373614720 run.py:483] Algo bellman_ford step 7349 current loss 0.635989, current_train_items 235200.
I0302 19:01:53.487394 22732373614720 run.py:483] Algo bellman_ford step 7350 current loss 0.362330, current_train_items 235232.
I0302 19:01:53.495572 22732373614720 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0302 19:01:53.495714 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 19:01:53.512871 22732373614720 run.py:483] Algo bellman_ford step 7351 current loss 0.573902, current_train_items 235264.
I0302 19:01:53.537644 22732373614720 run.py:483] Algo bellman_ford step 7352 current loss 0.653496, current_train_items 235296.
I0302 19:01:53.569895 22732373614720 run.py:483] Algo bellman_ford step 7353 current loss 0.733419, current_train_items 235328.
I0302 19:01:53.602522 22732373614720 run.py:483] Algo bellman_ford step 7354 current loss 0.772456, current_train_items 235360.
I0302 19:01:53.622876 22732373614720 run.py:483] Algo bellman_ford step 7355 current loss 0.341683, current_train_items 235392.
I0302 19:01:53.638544 22732373614720 run.py:483] Algo bellman_ford step 7356 current loss 0.429910, current_train_items 235424.
I0302 19:01:53.663637 22732373614720 run.py:483] Algo bellman_ford step 7357 current loss 0.643687, current_train_items 235456.
I0302 19:01:53.694638 22732373614720 run.py:483] Algo bellman_ford step 7358 current loss 0.760628, current_train_items 235488.
I0302 19:01:53.727750 22732373614720 run.py:483] Algo bellman_ford step 7359 current loss 0.730314, current_train_items 235520.
I0302 19:01:53.748169 22732373614720 run.py:483] Algo bellman_ford step 7360 current loss 0.404106, current_train_items 235552.
I0302 19:01:53.764963 22732373614720 run.py:483] Algo bellman_ford step 7361 current loss 0.585010, current_train_items 235584.
I0302 19:01:53.787252 22732373614720 run.py:483] Algo bellman_ford step 7362 current loss 0.658698, current_train_items 235616.
I0302 19:01:53.820569 22732373614720 run.py:483] Algo bellman_ford step 7363 current loss 0.850673, current_train_items 235648.
I0302 19:01:53.851738 22732373614720 run.py:483] Algo bellman_ford step 7364 current loss 0.853346, current_train_items 235680.
I0302 19:01:53.871517 22732373614720 run.py:483] Algo bellman_ford step 7365 current loss 0.384600, current_train_items 235712.
I0302 19:01:53.887644 22732373614720 run.py:483] Algo bellman_ford step 7366 current loss 0.496778, current_train_items 235744.
I0302 19:01:53.911921 22732373614720 run.py:483] Algo bellman_ford step 7367 current loss 0.577660, current_train_items 235776.
I0302 19:01:53.944993 22732373614720 run.py:483] Algo bellman_ford step 7368 current loss 0.787724, current_train_items 235808.
I0302 19:01:53.977036 22732373614720 run.py:483] Algo bellman_ford step 7369 current loss 0.737678, current_train_items 235840.
I0302 19:01:53.996935 22732373614720 run.py:483] Algo bellman_ford step 7370 current loss 0.318270, current_train_items 235872.
I0302 19:01:54.013322 22732373614720 run.py:483] Algo bellman_ford step 7371 current loss 0.650008, current_train_items 235904.
I0302 19:01:54.035670 22732373614720 run.py:483] Algo bellman_ford step 7372 current loss 0.667505, current_train_items 235936.
I0302 19:01:54.067064 22732373614720 run.py:483] Algo bellman_ford step 7373 current loss 0.653406, current_train_items 235968.
I0302 19:01:54.100082 22732373614720 run.py:483] Algo bellman_ford step 7374 current loss 0.870342, current_train_items 236000.
I0302 19:01:54.120193 22732373614720 run.py:483] Algo bellman_ford step 7375 current loss 0.351616, current_train_items 236032.
I0302 19:01:54.137020 22732373614720 run.py:483] Algo bellman_ford step 7376 current loss 0.486601, current_train_items 236064.
I0302 19:01:54.161042 22732373614720 run.py:483] Algo bellman_ford step 7377 current loss 0.672390, current_train_items 236096.
I0302 19:01:54.192338 22732373614720 run.py:483] Algo bellman_ford step 7378 current loss 0.699446, current_train_items 236128.
I0302 19:01:54.224198 22732373614720 run.py:483] Algo bellman_ford step 7379 current loss 0.874916, current_train_items 236160.
I0302 19:01:54.243714 22732373614720 run.py:483] Algo bellman_ford step 7380 current loss 0.386320, current_train_items 236192.
I0302 19:01:54.259540 22732373614720 run.py:483] Algo bellman_ford step 7381 current loss 0.452725, current_train_items 236224.
I0302 19:01:54.284080 22732373614720 run.py:483] Algo bellman_ford step 7382 current loss 0.827954, current_train_items 236256.
I0302 19:01:54.314905 22732373614720 run.py:483] Algo bellman_ford step 7383 current loss 0.757515, current_train_items 236288.
I0302 19:01:54.346869 22732373614720 run.py:483] Algo bellman_ford step 7384 current loss 0.749135, current_train_items 236320.
I0302 19:01:54.366527 22732373614720 run.py:483] Algo bellman_ford step 7385 current loss 0.308700, current_train_items 236352.
I0302 19:01:54.382788 22732373614720 run.py:483] Algo bellman_ford step 7386 current loss 0.517621, current_train_items 236384.
I0302 19:01:54.406576 22732373614720 run.py:483] Algo bellman_ford step 7387 current loss 0.620994, current_train_items 236416.
I0302 19:01:54.438786 22732373614720 run.py:483] Algo bellman_ford step 7388 current loss 0.754196, current_train_items 236448.
I0302 19:01:54.470886 22732373614720 run.py:483] Algo bellman_ford step 7389 current loss 0.822775, current_train_items 236480.
I0302 19:01:54.490910 22732373614720 run.py:483] Algo bellman_ford step 7390 current loss 0.350291, current_train_items 236512.
I0302 19:01:54.507385 22732373614720 run.py:483] Algo bellman_ford step 7391 current loss 0.556186, current_train_items 236544.
I0302 19:01:54.530550 22732373614720 run.py:483] Algo bellman_ford step 7392 current loss 0.641661, current_train_items 236576.
I0302 19:01:54.561152 22732373614720 run.py:483] Algo bellman_ford step 7393 current loss 0.629240, current_train_items 236608.
I0302 19:01:54.594720 22732373614720 run.py:483] Algo bellman_ford step 7394 current loss 0.800091, current_train_items 236640.
I0302 19:01:54.614293 22732373614720 run.py:483] Algo bellman_ford step 7395 current loss 0.397407, current_train_items 236672.
I0302 19:01:54.630708 22732373614720 run.py:483] Algo bellman_ford step 7396 current loss 0.588027, current_train_items 236704.
I0302 19:01:54.655109 22732373614720 run.py:483] Algo bellman_ford step 7397 current loss 0.680643, current_train_items 236736.
I0302 19:01:54.685915 22732373614720 run.py:483] Algo bellman_ford step 7398 current loss 0.617540, current_train_items 236768.
I0302 19:01:54.717735 22732373614720 run.py:483] Algo bellman_ford step 7399 current loss 0.794532, current_train_items 236800.
I0302 19:01:54.737719 22732373614720 run.py:483] Algo bellman_ford step 7400 current loss 0.367629, current_train_items 236832.
I0302 19:01:54.745529 22732373614720 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0302 19:01:54.745642 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:01:54.762692 22732373614720 run.py:483] Algo bellman_ford step 7401 current loss 0.467941, current_train_items 236864.
I0302 19:01:54.787434 22732373614720 run.py:483] Algo bellman_ford step 7402 current loss 0.605790, current_train_items 236896.
I0302 19:01:54.819515 22732373614720 run.py:483] Algo bellman_ford step 7403 current loss 0.747538, current_train_items 236928.
I0302 19:01:54.853183 22732373614720 run.py:483] Algo bellman_ford step 7404 current loss 0.785760, current_train_items 236960.
I0302 19:01:54.873012 22732373614720 run.py:483] Algo bellman_ford step 7405 current loss 0.399741, current_train_items 236992.
I0302 19:01:54.888371 22732373614720 run.py:483] Algo bellman_ford step 7406 current loss 0.451943, current_train_items 237024.
I0302 19:01:54.911924 22732373614720 run.py:483] Algo bellman_ford step 7407 current loss 0.570387, current_train_items 237056.
I0302 19:01:54.943617 22732373614720 run.py:483] Algo bellman_ford step 7408 current loss 0.743601, current_train_items 237088.
I0302 19:01:54.975224 22732373614720 run.py:483] Algo bellman_ford step 7409 current loss 0.932859, current_train_items 237120.
I0302 19:01:54.994722 22732373614720 run.py:483] Algo bellman_ford step 7410 current loss 0.331745, current_train_items 237152.
I0302 19:01:55.011246 22732373614720 run.py:483] Algo bellman_ford step 7411 current loss 0.536459, current_train_items 237184.
I0302 19:01:55.035787 22732373614720 run.py:483] Algo bellman_ford step 7412 current loss 0.769380, current_train_items 237216.
I0302 19:01:55.067276 22732373614720 run.py:483] Algo bellman_ford step 7413 current loss 0.819001, current_train_items 237248.
I0302 19:01:55.098583 22732373614720 run.py:483] Algo bellman_ford step 7414 current loss 0.805067, current_train_items 237280.
I0302 19:01:55.117971 22732373614720 run.py:483] Algo bellman_ford step 7415 current loss 0.406527, current_train_items 237312.
I0302 19:01:55.133654 22732373614720 run.py:483] Algo bellman_ford step 7416 current loss 0.475716, current_train_items 237344.
I0302 19:01:55.157126 22732373614720 run.py:483] Algo bellman_ford step 7417 current loss 0.671238, current_train_items 237376.
I0302 19:01:55.187734 22732373614720 run.py:483] Algo bellman_ford step 7418 current loss 0.741047, current_train_items 237408.
I0302 19:01:55.219714 22732373614720 run.py:483] Algo bellman_ford step 7419 current loss 0.791841, current_train_items 237440.
I0302 19:01:55.239093 22732373614720 run.py:483] Algo bellman_ford step 7420 current loss 0.366655, current_train_items 237472.
I0302 19:01:55.254896 22732373614720 run.py:483] Algo bellman_ford step 7421 current loss 0.531215, current_train_items 237504.
I0302 19:01:55.278195 22732373614720 run.py:483] Algo bellman_ford step 7422 current loss 0.584498, current_train_items 237536.
I0302 19:01:55.308979 22732373614720 run.py:483] Algo bellman_ford step 7423 current loss 0.697050, current_train_items 237568.
I0302 19:01:55.341663 22732373614720 run.py:483] Algo bellman_ford step 7424 current loss 0.743837, current_train_items 237600.
I0302 19:01:55.361083 22732373614720 run.py:483] Algo bellman_ford step 7425 current loss 0.359146, current_train_items 237632.
I0302 19:01:55.377050 22732373614720 run.py:483] Algo bellman_ford step 7426 current loss 0.427821, current_train_items 237664.
I0302 19:01:55.401180 22732373614720 run.py:483] Algo bellman_ford step 7427 current loss 0.667793, current_train_items 237696.
I0302 19:01:55.431562 22732373614720 run.py:483] Algo bellman_ford step 7428 current loss 0.713052, current_train_items 237728.
I0302 19:01:55.464948 22732373614720 run.py:483] Algo bellman_ford step 7429 current loss 1.083954, current_train_items 237760.
I0302 19:01:55.484749 22732373614720 run.py:483] Algo bellman_ford step 7430 current loss 0.315373, current_train_items 237792.
I0302 19:01:55.500514 22732373614720 run.py:483] Algo bellman_ford step 7431 current loss 0.484136, current_train_items 237824.
I0302 19:01:55.524588 22732373614720 run.py:483] Algo bellman_ford step 7432 current loss 0.710941, current_train_items 237856.
I0302 19:01:55.556216 22732373614720 run.py:483] Algo bellman_ford step 7433 current loss 0.721079, current_train_items 237888.
I0302 19:01:55.586916 22732373614720 run.py:483] Algo bellman_ford step 7434 current loss 0.823657, current_train_items 237920.
I0302 19:01:55.606466 22732373614720 run.py:483] Algo bellman_ford step 7435 current loss 0.320185, current_train_items 237952.
I0302 19:01:55.622549 22732373614720 run.py:483] Algo bellman_ford step 7436 current loss 0.583379, current_train_items 237984.
I0302 19:01:55.647164 22732373614720 run.py:483] Algo bellman_ford step 7437 current loss 0.767073, current_train_items 238016.
I0302 19:01:55.677628 22732373614720 run.py:483] Algo bellman_ford step 7438 current loss 0.861098, current_train_items 238048.
I0302 19:01:55.711402 22732373614720 run.py:483] Algo bellman_ford step 7439 current loss 0.917191, current_train_items 238080.
I0302 19:01:55.730870 22732373614720 run.py:483] Algo bellman_ford step 7440 current loss 0.433970, current_train_items 238112.
I0302 19:01:55.747208 22732373614720 run.py:483] Algo bellman_ford step 7441 current loss 0.636887, current_train_items 238144.
I0302 19:01:55.770119 22732373614720 run.py:483] Algo bellman_ford step 7442 current loss 0.641410, current_train_items 238176.
I0302 19:01:55.802061 22732373614720 run.py:483] Algo bellman_ford step 7443 current loss 0.920001, current_train_items 238208.
I0302 19:01:55.835782 22732373614720 run.py:483] Algo bellman_ford step 7444 current loss 0.941790, current_train_items 238240.
I0302 19:01:55.855698 22732373614720 run.py:483] Algo bellman_ford step 7445 current loss 0.360884, current_train_items 238272.
I0302 19:01:55.871780 22732373614720 run.py:483] Algo bellman_ford step 7446 current loss 0.543385, current_train_items 238304.
I0302 19:01:55.895195 22732373614720 run.py:483] Algo bellman_ford step 7447 current loss 0.726871, current_train_items 238336.
I0302 19:01:55.925880 22732373614720 run.py:483] Algo bellman_ford step 7448 current loss 0.704843, current_train_items 238368.
I0302 19:01:55.960427 22732373614720 run.py:483] Algo bellman_ford step 7449 current loss 0.897124, current_train_items 238400.
I0302 19:01:55.979940 22732373614720 run.py:483] Algo bellman_ford step 7450 current loss 0.394495, current_train_items 238432.
I0302 19:01:55.988332 22732373614720 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0302 19:01:55.988442 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 19:01:56.005301 22732373614720 run.py:483] Algo bellman_ford step 7451 current loss 0.491030, current_train_items 238464.
I0302 19:01:56.030475 22732373614720 run.py:483] Algo bellman_ford step 7452 current loss 0.688058, current_train_items 238496.
I0302 19:01:56.061462 22732373614720 run.py:483] Algo bellman_ford step 7453 current loss 0.652554, current_train_items 238528.
I0302 19:01:56.094260 22732373614720 run.py:483] Algo bellman_ford step 7454 current loss 1.014441, current_train_items 238560.
I0302 19:01:56.114387 22732373614720 run.py:483] Algo bellman_ford step 7455 current loss 0.356999, current_train_items 238592.
I0302 19:01:56.130717 22732373614720 run.py:483] Algo bellman_ford step 7456 current loss 0.456423, current_train_items 238624.
I0302 19:01:56.154278 22732373614720 run.py:483] Algo bellman_ford step 7457 current loss 0.630584, current_train_items 238656.
I0302 19:01:56.186311 22732373614720 run.py:483] Algo bellman_ford step 7458 current loss 0.781659, current_train_items 238688.
I0302 19:01:56.219364 22732373614720 run.py:483] Algo bellman_ford step 7459 current loss 0.929812, current_train_items 238720.
I0302 19:01:56.239177 22732373614720 run.py:483] Algo bellman_ford step 7460 current loss 0.330459, current_train_items 238752.
I0302 19:01:56.255983 22732373614720 run.py:483] Algo bellman_ford step 7461 current loss 0.534167, current_train_items 238784.
I0302 19:01:56.278854 22732373614720 run.py:483] Algo bellman_ford step 7462 current loss 0.654930, current_train_items 238816.
I0302 19:01:56.309489 22732373614720 run.py:483] Algo bellman_ford step 7463 current loss 0.692290, current_train_items 238848.
I0302 19:01:56.342431 22732373614720 run.py:483] Algo bellman_ford step 7464 current loss 0.934481, current_train_items 238880.
I0302 19:01:56.362066 22732373614720 run.py:483] Algo bellman_ford step 7465 current loss 0.366286, current_train_items 238912.
I0302 19:01:56.378032 22732373614720 run.py:483] Algo bellman_ford step 7466 current loss 0.422903, current_train_items 238944.
I0302 19:01:56.401490 22732373614720 run.py:483] Algo bellman_ford step 7467 current loss 0.629894, current_train_items 238976.
I0302 19:01:56.433810 22732373614720 run.py:483] Algo bellman_ford step 7468 current loss 0.710527, current_train_items 239008.
I0302 19:01:56.465482 22732373614720 run.py:483] Algo bellman_ford step 7469 current loss 0.816508, current_train_items 239040.
I0302 19:01:56.485741 22732373614720 run.py:483] Algo bellman_ford step 7470 current loss 0.311330, current_train_items 239072.
I0302 19:01:56.501964 22732373614720 run.py:483] Algo bellman_ford step 7471 current loss 0.452881, current_train_items 239104.
I0302 19:01:56.524520 22732373614720 run.py:483] Algo bellman_ford step 7472 current loss 0.784314, current_train_items 239136.
I0302 19:01:56.556126 22732373614720 run.py:483] Algo bellman_ford step 7473 current loss 0.807451, current_train_items 239168.
I0302 19:01:56.590457 22732373614720 run.py:483] Algo bellman_ford step 7474 current loss 0.856107, current_train_items 239200.
I0302 19:01:56.610669 22732373614720 run.py:483] Algo bellman_ford step 7475 current loss 0.319498, current_train_items 239232.
I0302 19:01:56.627034 22732373614720 run.py:483] Algo bellman_ford step 7476 current loss 0.550581, current_train_items 239264.
I0302 19:01:56.649396 22732373614720 run.py:483] Algo bellman_ford step 7477 current loss 0.662615, current_train_items 239296.
I0302 19:01:56.681083 22732373614720 run.py:483] Algo bellman_ford step 7478 current loss 0.698352, current_train_items 239328.
I0302 19:01:56.717071 22732373614720 run.py:483] Algo bellman_ford step 7479 current loss 0.944745, current_train_items 239360.
I0302 19:01:56.736431 22732373614720 run.py:483] Algo bellman_ford step 7480 current loss 0.336095, current_train_items 239392.
I0302 19:01:56.752806 22732373614720 run.py:483] Algo bellman_ford step 7481 current loss 0.517256, current_train_items 239424.
I0302 19:01:56.777512 22732373614720 run.py:483] Algo bellman_ford step 7482 current loss 0.744678, current_train_items 239456.
I0302 19:01:56.809605 22732373614720 run.py:483] Algo bellman_ford step 7483 current loss 0.818977, current_train_items 239488.
I0302 19:01:56.843621 22732373614720 run.py:483] Algo bellman_ford step 7484 current loss 0.990211, current_train_items 239520.
I0302 19:01:56.863531 22732373614720 run.py:483] Algo bellman_ford step 7485 current loss 0.334381, current_train_items 239552.
I0302 19:01:56.879757 22732373614720 run.py:483] Algo bellman_ford step 7486 current loss 0.566147, current_train_items 239584.
I0302 19:01:56.902358 22732373614720 run.py:483] Algo bellman_ford step 7487 current loss 0.625933, current_train_items 239616.
I0302 19:01:56.934189 22732373614720 run.py:483] Algo bellman_ford step 7488 current loss 0.733263, current_train_items 239648.
I0302 19:01:56.969380 22732373614720 run.py:483] Algo bellman_ford step 7489 current loss 0.872968, current_train_items 239680.
I0302 19:01:56.989012 22732373614720 run.py:483] Algo bellman_ford step 7490 current loss 0.373303, current_train_items 239712.
I0302 19:01:57.004910 22732373614720 run.py:483] Algo bellman_ford step 7491 current loss 0.538109, current_train_items 239744.
I0302 19:01:57.027569 22732373614720 run.py:483] Algo bellman_ford step 7492 current loss 0.554304, current_train_items 239776.
I0302 19:01:57.058613 22732373614720 run.py:483] Algo bellman_ford step 7493 current loss 0.679104, current_train_items 239808.
I0302 19:01:57.091327 22732373614720 run.py:483] Algo bellman_ford step 7494 current loss 0.781604, current_train_items 239840.
I0302 19:01:57.111210 22732373614720 run.py:483] Algo bellman_ford step 7495 current loss 0.421357, current_train_items 239872.
I0302 19:01:57.127557 22732373614720 run.py:483] Algo bellman_ford step 7496 current loss 0.480327, current_train_items 239904.
I0302 19:01:57.151301 22732373614720 run.py:483] Algo bellman_ford step 7497 current loss 0.571924, current_train_items 239936.
I0302 19:01:57.181812 22732373614720 run.py:483] Algo bellman_ford step 7498 current loss 0.727659, current_train_items 239968.
I0302 19:01:57.214302 22732373614720 run.py:483] Algo bellman_ford step 7499 current loss 0.714875, current_train_items 240000.
I0302 19:01:57.234086 22732373614720 run.py:483] Algo bellman_ford step 7500 current loss 0.369721, current_train_items 240032.
I0302 19:01:57.242001 22732373614720 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0302 19:01:57.242112 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:01:57.258770 22732373614720 run.py:483] Algo bellman_ford step 7501 current loss 0.425681, current_train_items 240064.
I0302 19:01:57.282599 22732373614720 run.py:483] Algo bellman_ford step 7502 current loss 0.636013, current_train_items 240096.
I0302 19:01:57.314052 22732373614720 run.py:483] Algo bellman_ford step 7503 current loss 0.684233, current_train_items 240128.
I0302 19:01:57.347085 22732373614720 run.py:483] Algo bellman_ford step 7504 current loss 0.748408, current_train_items 240160.
I0302 19:01:57.367045 22732373614720 run.py:483] Algo bellman_ford step 7505 current loss 0.350372, current_train_items 240192.
I0302 19:01:57.382789 22732373614720 run.py:483] Algo bellman_ford step 7506 current loss 0.499257, current_train_items 240224.
I0302 19:01:57.405826 22732373614720 run.py:483] Algo bellman_ford step 7507 current loss 0.564582, current_train_items 240256.
I0302 19:01:57.436404 22732373614720 run.py:483] Algo bellman_ford step 7508 current loss 0.697815, current_train_items 240288.
I0302 19:01:57.468767 22732373614720 run.py:483] Algo bellman_ford step 7509 current loss 0.727014, current_train_items 240320.
I0302 19:01:57.488285 22732373614720 run.py:483] Algo bellman_ford step 7510 current loss 0.364567, current_train_items 240352.
I0302 19:01:57.504648 22732373614720 run.py:483] Algo bellman_ford step 7511 current loss 0.580674, current_train_items 240384.
I0302 19:01:57.526880 22732373614720 run.py:483] Algo bellman_ford step 7512 current loss 0.685768, current_train_items 240416.
I0302 19:01:57.557415 22732373614720 run.py:483] Algo bellman_ford step 7513 current loss 0.676906, current_train_items 240448.
I0302 19:01:57.589062 22732373614720 run.py:483] Algo bellman_ford step 7514 current loss 0.868111, current_train_items 240480.
I0302 19:01:57.608706 22732373614720 run.py:483] Algo bellman_ford step 7515 current loss 0.351445, current_train_items 240512.
I0302 19:01:57.624130 22732373614720 run.py:483] Algo bellman_ford step 7516 current loss 0.566827, current_train_items 240544.
I0302 19:01:57.647958 22732373614720 run.py:483] Algo bellman_ford step 7517 current loss 0.640543, current_train_items 240576.
I0302 19:01:57.679235 22732373614720 run.py:483] Algo bellman_ford step 7518 current loss 0.745707, current_train_items 240608.
I0302 19:01:57.712682 22732373614720 run.py:483] Algo bellman_ford step 7519 current loss 0.973448, current_train_items 240640.
I0302 19:01:57.732490 22732373614720 run.py:483] Algo bellman_ford step 7520 current loss 0.314070, current_train_items 240672.
I0302 19:01:57.748603 22732373614720 run.py:483] Algo bellman_ford step 7521 current loss 0.458668, current_train_items 240704.
I0302 19:01:57.771874 22732373614720 run.py:483] Algo bellman_ford step 7522 current loss 0.646989, current_train_items 240736.
I0302 19:01:57.803093 22732373614720 run.py:483] Algo bellman_ford step 7523 current loss 0.721576, current_train_items 240768.
I0302 19:01:57.833772 22732373614720 run.py:483] Algo bellman_ford step 7524 current loss 0.720865, current_train_items 240800.
I0302 19:01:57.853440 22732373614720 run.py:483] Algo bellman_ford step 7525 current loss 0.282407, current_train_items 240832.
I0302 19:01:57.869436 22732373614720 run.py:483] Algo bellman_ford step 7526 current loss 0.515948, current_train_items 240864.
I0302 19:01:57.893634 22732373614720 run.py:483] Algo bellman_ford step 7527 current loss 0.720347, current_train_items 240896.
I0302 19:01:57.924136 22732373614720 run.py:483] Algo bellman_ford step 7528 current loss 0.676130, current_train_items 240928.
I0302 19:01:57.957119 22732373614720 run.py:483] Algo bellman_ford step 7529 current loss 0.752157, current_train_items 240960.
I0302 19:01:57.976656 22732373614720 run.py:483] Algo bellman_ford step 7530 current loss 0.340863, current_train_items 240992.
I0302 19:01:57.992607 22732373614720 run.py:483] Algo bellman_ford step 7531 current loss 0.516015, current_train_items 241024.
I0302 19:01:58.015454 22732373614720 run.py:483] Algo bellman_ford step 7532 current loss 0.657737, current_train_items 241056.
I0302 19:01:58.048013 22732373614720 run.py:483] Algo bellman_ford step 7533 current loss 0.735093, current_train_items 241088.
I0302 19:01:58.081840 22732373614720 run.py:483] Algo bellman_ford step 7534 current loss 0.848557, current_train_items 241120.
I0302 19:01:58.101437 22732373614720 run.py:483] Algo bellman_ford step 7535 current loss 0.325947, current_train_items 241152.
I0302 19:01:58.117639 22732373614720 run.py:483] Algo bellman_ford step 7536 current loss 0.498171, current_train_items 241184.
I0302 19:01:58.141119 22732373614720 run.py:483] Algo bellman_ford step 7537 current loss 0.701382, current_train_items 241216.
I0302 19:01:58.172723 22732373614720 run.py:483] Algo bellman_ford step 7538 current loss 0.858328, current_train_items 241248.
I0302 19:01:58.205440 22732373614720 run.py:483] Algo bellman_ford step 7539 current loss 0.769105, current_train_items 241280.
I0302 19:01:58.225532 22732373614720 run.py:483] Algo bellman_ford step 7540 current loss 0.312507, current_train_items 241312.
I0302 19:01:58.242019 22732373614720 run.py:483] Algo bellman_ford step 7541 current loss 0.565802, current_train_items 241344.
I0302 19:01:58.266681 22732373614720 run.py:483] Algo bellman_ford step 7542 current loss 0.617653, current_train_items 241376.
I0302 19:01:58.299102 22732373614720 run.py:483] Algo bellman_ford step 7543 current loss 0.703444, current_train_items 241408.
I0302 19:01:58.331889 22732373614720 run.py:483] Algo bellman_ford step 7544 current loss 0.717095, current_train_items 241440.
I0302 19:01:58.351233 22732373614720 run.py:483] Algo bellman_ford step 7545 current loss 0.347683, current_train_items 241472.
I0302 19:01:58.367192 22732373614720 run.py:483] Algo bellman_ford step 7546 current loss 0.643401, current_train_items 241504.
I0302 19:01:58.391428 22732373614720 run.py:483] Algo bellman_ford step 7547 current loss 0.740551, current_train_items 241536.
I0302 19:01:58.423108 22732373614720 run.py:483] Algo bellman_ford step 7548 current loss 0.736277, current_train_items 241568.
I0302 19:01:58.456100 22732373614720 run.py:483] Algo bellman_ford step 7549 current loss 0.817073, current_train_items 241600.
I0302 19:01:58.475576 22732373614720 run.py:483] Algo bellman_ford step 7550 current loss 0.323996, current_train_items 241632.
I0302 19:01:58.483699 22732373614720 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.935546875, 'score': 0.935546875, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0302 19:01:58.483809 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.936, val scores are: bellman_ford: 0.936
I0302 19:01:58.500714 22732373614720 run.py:483] Algo bellman_ford step 7551 current loss 0.527903, current_train_items 241664.
I0302 19:01:58.524621 22732373614720 run.py:483] Algo bellman_ford step 7552 current loss 0.625152, current_train_items 241696.
I0302 19:01:58.556197 22732373614720 run.py:483] Algo bellman_ford step 7553 current loss 0.704780, current_train_items 241728.
I0302 19:01:58.589517 22732373614720 run.py:483] Algo bellman_ford step 7554 current loss 0.859396, current_train_items 241760.
I0302 19:01:58.609437 22732373614720 run.py:483] Algo bellman_ford step 7555 current loss 0.339970, current_train_items 241792.
I0302 19:01:58.625378 22732373614720 run.py:483] Algo bellman_ford step 7556 current loss 0.567113, current_train_items 241824.
I0302 19:01:58.648962 22732373614720 run.py:483] Algo bellman_ford step 7557 current loss 0.644892, current_train_items 241856.
I0302 19:01:58.681168 22732373614720 run.py:483] Algo bellman_ford step 7558 current loss 0.824995, current_train_items 241888.
I0302 19:01:58.712924 22732373614720 run.py:483] Algo bellman_ford step 7559 current loss 0.826752, current_train_items 241920.
I0302 19:01:58.732628 22732373614720 run.py:483] Algo bellman_ford step 7560 current loss 0.353441, current_train_items 241952.
I0302 19:01:58.749573 22732373614720 run.py:483] Algo bellman_ford step 7561 current loss 0.605506, current_train_items 241984.
I0302 19:01:58.772341 22732373614720 run.py:483] Algo bellman_ford step 7562 current loss 0.711648, current_train_items 242016.
I0302 19:01:58.803782 22732373614720 run.py:483] Algo bellman_ford step 7563 current loss 0.851034, current_train_items 242048.
I0302 19:01:58.837042 22732373614720 run.py:483] Algo bellman_ford step 7564 current loss 0.904419, current_train_items 242080.
I0302 19:01:58.856373 22732373614720 run.py:483] Algo bellman_ford step 7565 current loss 0.458498, current_train_items 242112.
I0302 19:01:58.872064 22732373614720 run.py:483] Algo bellman_ford step 7566 current loss 0.551069, current_train_items 242144.
I0302 19:01:58.895596 22732373614720 run.py:483] Algo bellman_ford step 7567 current loss 0.710008, current_train_items 242176.
I0302 19:01:58.926010 22732373614720 run.py:483] Algo bellman_ford step 7568 current loss 0.837514, current_train_items 242208.
I0302 19:01:58.957981 22732373614720 run.py:483] Algo bellman_ford step 7569 current loss 0.835562, current_train_items 242240.
I0302 19:01:58.977819 22732373614720 run.py:483] Algo bellman_ford step 7570 current loss 0.462689, current_train_items 242272.
I0302 19:01:58.994097 22732373614720 run.py:483] Algo bellman_ford step 7571 current loss 0.507155, current_train_items 242304.
I0302 19:01:59.017290 22732373614720 run.py:483] Algo bellman_ford step 7572 current loss 0.791332, current_train_items 242336.
I0302 19:01:59.047243 22732373614720 run.py:483] Algo bellman_ford step 7573 current loss 0.634739, current_train_items 242368.
I0302 19:01:59.077620 22732373614720 run.py:483] Algo bellman_ford step 7574 current loss 0.723524, current_train_items 242400.
I0302 19:01:59.097502 22732373614720 run.py:483] Algo bellman_ford step 7575 current loss 0.333765, current_train_items 242432.
I0302 19:01:59.114229 22732373614720 run.py:483] Algo bellman_ford step 7576 current loss 0.472974, current_train_items 242464.
I0302 19:01:59.136451 22732373614720 run.py:483] Algo bellman_ford step 7577 current loss 0.600868, current_train_items 242496.
I0302 19:01:59.167943 22732373614720 run.py:483] Algo bellman_ford step 7578 current loss 0.672228, current_train_items 242528.
I0302 19:01:59.202463 22732373614720 run.py:483] Algo bellman_ford step 7579 current loss 0.902888, current_train_items 242560.
I0302 19:01:59.222007 22732373614720 run.py:483] Algo bellman_ford step 7580 current loss 0.351794, current_train_items 242592.
I0302 19:01:59.238414 22732373614720 run.py:483] Algo bellman_ford step 7581 current loss 0.615734, current_train_items 242624.
I0302 19:01:59.262025 22732373614720 run.py:483] Algo bellman_ford step 7582 current loss 0.658790, current_train_items 242656.
I0302 19:01:59.292777 22732373614720 run.py:483] Algo bellman_ford step 7583 current loss 0.719031, current_train_items 242688.
I0302 19:01:59.325190 22732373614720 run.py:483] Algo bellman_ford step 7584 current loss 0.782785, current_train_items 242720.
I0302 19:01:59.344925 22732373614720 run.py:483] Algo bellman_ford step 7585 current loss 0.304928, current_train_items 242752.
I0302 19:01:59.360752 22732373614720 run.py:483] Algo bellman_ford step 7586 current loss 0.503994, current_train_items 242784.
I0302 19:01:59.384198 22732373614720 run.py:483] Algo bellman_ford step 7587 current loss 0.831568, current_train_items 242816.
I0302 19:01:59.413232 22732373614720 run.py:483] Algo bellman_ford step 7588 current loss 0.716690, current_train_items 242848.
I0302 19:01:59.446468 22732373614720 run.py:483] Algo bellman_ford step 7589 current loss 1.051563, current_train_items 242880.
I0302 19:01:59.465996 22732373614720 run.py:483] Algo bellman_ford step 7590 current loss 0.327243, current_train_items 242912.
I0302 19:01:59.481938 22732373614720 run.py:483] Algo bellman_ford step 7591 current loss 0.478497, current_train_items 242944.
I0302 19:01:59.506135 22732373614720 run.py:483] Algo bellman_ford step 7592 current loss 0.722673, current_train_items 242976.
I0302 19:01:59.535830 22732373614720 run.py:483] Algo bellman_ford step 7593 current loss 0.817968, current_train_items 243008.
I0302 19:01:59.568996 22732373614720 run.py:483] Algo bellman_ford step 7594 current loss 0.835211, current_train_items 243040.
I0302 19:01:59.588476 22732373614720 run.py:483] Algo bellman_ford step 7595 current loss 0.321843, current_train_items 243072.
I0302 19:01:59.604397 22732373614720 run.py:483] Algo bellman_ford step 7596 current loss 0.506003, current_train_items 243104.
I0302 19:01:59.628257 22732373614720 run.py:483] Algo bellman_ford step 7597 current loss 0.749234, current_train_items 243136.
I0302 19:01:59.660326 22732373614720 run.py:483] Algo bellman_ford step 7598 current loss 0.717775, current_train_items 243168.
I0302 19:01:59.690805 22732373614720 run.py:483] Algo bellman_ford step 7599 current loss 1.055019, current_train_items 243200.
I0302 19:01:59.710511 22732373614720 run.py:483] Algo bellman_ford step 7600 current loss 0.334595, current_train_items 243232.
I0302 19:01:59.718310 22732373614720 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0302 19:01:59.718420 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 19:01:59.735319 22732373614720 run.py:483] Algo bellman_ford step 7601 current loss 0.531855, current_train_items 243264.
I0302 19:01:59.760694 22732373614720 run.py:483] Algo bellman_ford step 7602 current loss 0.787233, current_train_items 243296.
I0302 19:01:59.791813 22732373614720 run.py:483] Algo bellman_ford step 7603 current loss 0.679006, current_train_items 243328.
I0302 19:01:59.825984 22732373614720 run.py:483] Algo bellman_ford step 7604 current loss 0.870842, current_train_items 243360.
I0302 19:01:59.845973 22732373614720 run.py:483] Algo bellman_ford step 7605 current loss 0.404600, current_train_items 243392.
I0302 19:01:59.861118 22732373614720 run.py:483] Algo bellman_ford step 7606 current loss 0.581925, current_train_items 243424.
I0302 19:01:59.883939 22732373614720 run.py:483] Algo bellman_ford step 7607 current loss 0.640220, current_train_items 243456.
I0302 19:01:59.915579 22732373614720 run.py:483] Algo bellman_ford step 7608 current loss 0.814111, current_train_items 243488.
I0302 19:01:59.949240 22732373614720 run.py:483] Algo bellman_ford step 7609 current loss 0.918146, current_train_items 243520.
I0302 19:01:59.968520 22732373614720 run.py:483] Algo bellman_ford step 7610 current loss 0.315998, current_train_items 243552.
I0302 19:01:59.985012 22732373614720 run.py:483] Algo bellman_ford step 7611 current loss 0.558754, current_train_items 243584.
I0302 19:02:00.008374 22732373614720 run.py:483] Algo bellman_ford step 7612 current loss 0.718141, current_train_items 243616.
I0302 19:02:00.037595 22732373614720 run.py:483] Algo bellman_ford step 7613 current loss 0.699701, current_train_items 243648.
I0302 19:02:00.069619 22732373614720 run.py:483] Algo bellman_ford step 7614 current loss 1.201466, current_train_items 243680.
I0302 19:02:00.088921 22732373614720 run.py:483] Algo bellman_ford step 7615 current loss 0.347186, current_train_items 243712.
I0302 19:02:00.105234 22732373614720 run.py:483] Algo bellman_ford step 7616 current loss 0.525016, current_train_items 243744.
I0302 19:02:00.128346 22732373614720 run.py:483] Algo bellman_ford step 7617 current loss 0.635645, current_train_items 243776.
I0302 19:02:00.158858 22732373614720 run.py:483] Algo bellman_ford step 7618 current loss 0.731183, current_train_items 243808.
I0302 19:02:00.191827 22732373614720 run.py:483] Algo bellman_ford step 7619 current loss 0.905353, current_train_items 243840.
I0302 19:02:00.211091 22732373614720 run.py:483] Algo bellman_ford step 7620 current loss 0.370735, current_train_items 243872.
I0302 19:02:00.227237 22732373614720 run.py:483] Algo bellman_ford step 7621 current loss 0.528896, current_train_items 243904.
I0302 19:02:00.250891 22732373614720 run.py:483] Algo bellman_ford step 7622 current loss 0.586447, current_train_items 243936.
I0302 19:02:00.283496 22732373614720 run.py:483] Algo bellman_ford step 7623 current loss 0.751313, current_train_items 243968.
I0302 19:02:00.315852 22732373614720 run.py:483] Algo bellman_ford step 7624 current loss 0.834744, current_train_items 244000.
I0302 19:02:00.335500 22732373614720 run.py:483] Algo bellman_ford step 7625 current loss 0.354588, current_train_items 244032.
I0302 19:02:00.350886 22732373614720 run.py:483] Algo bellman_ford step 7626 current loss 0.462755, current_train_items 244064.
I0302 19:02:00.375171 22732373614720 run.py:483] Algo bellman_ford step 7627 current loss 0.767706, current_train_items 244096.
I0302 19:02:00.406807 22732373614720 run.py:483] Algo bellman_ford step 7628 current loss 0.740386, current_train_items 244128.
I0302 19:02:00.438702 22732373614720 run.py:483] Algo bellman_ford step 7629 current loss 0.802285, current_train_items 244160.
I0302 19:02:00.458342 22732373614720 run.py:483] Algo bellman_ford step 7630 current loss 0.329719, current_train_items 244192.
I0302 19:02:00.474636 22732373614720 run.py:483] Algo bellman_ford step 7631 current loss 0.679930, current_train_items 244224.
I0302 19:02:00.497732 22732373614720 run.py:483] Algo bellman_ford step 7632 current loss 0.770147, current_train_items 244256.
I0302 19:02:00.529250 22732373614720 run.py:483] Algo bellman_ford step 7633 current loss 0.789863, current_train_items 244288.
I0302 19:02:00.562405 22732373614720 run.py:483] Algo bellman_ford step 7634 current loss 0.770089, current_train_items 244320.
I0302 19:02:00.581727 22732373614720 run.py:483] Algo bellman_ford step 7635 current loss 0.306226, current_train_items 244352.
I0302 19:02:00.597515 22732373614720 run.py:483] Algo bellman_ford step 7636 current loss 0.566207, current_train_items 244384.
I0302 19:02:00.620692 22732373614720 run.py:483] Algo bellman_ford step 7637 current loss 0.691079, current_train_items 244416.
I0302 19:02:00.650371 22732373614720 run.py:483] Algo bellman_ford step 7638 current loss 0.658704, current_train_items 244448.
I0302 19:02:00.685681 22732373614720 run.py:483] Algo bellman_ford step 7639 current loss 0.814966, current_train_items 244480.
I0302 19:02:00.704769 22732373614720 run.py:483] Algo bellman_ford step 7640 current loss 0.329031, current_train_items 244512.
I0302 19:02:00.720519 22732373614720 run.py:483] Algo bellman_ford step 7641 current loss 0.580135, current_train_items 244544.
I0302 19:02:00.742757 22732373614720 run.py:483] Algo bellman_ford step 7642 current loss 0.642428, current_train_items 244576.
I0302 19:02:00.776108 22732373614720 run.py:483] Algo bellman_ford step 7643 current loss 0.776667, current_train_items 244608.
I0302 19:02:00.810469 22732373614720 run.py:483] Algo bellman_ford step 7644 current loss 0.849828, current_train_items 244640.
I0302 19:02:00.829857 22732373614720 run.py:483] Algo bellman_ford step 7645 current loss 0.311424, current_train_items 244672.
I0302 19:02:00.845895 22732373614720 run.py:483] Algo bellman_ford step 7646 current loss 0.586406, current_train_items 244704.
I0302 19:02:00.869984 22732373614720 run.py:483] Algo bellman_ford step 7647 current loss 0.642220, current_train_items 244736.
I0302 19:02:00.902739 22732373614720 run.py:483] Algo bellman_ford step 7648 current loss 0.731291, current_train_items 244768.
I0302 19:02:00.937404 22732373614720 run.py:483] Algo bellman_ford step 7649 current loss 0.980236, current_train_items 244800.
I0302 19:02:00.956793 22732373614720 run.py:483] Algo bellman_ford step 7650 current loss 0.362810, current_train_items 244832.
I0302 19:02:00.965079 22732373614720 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0302 19:02:00.965201 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:02:00.981669 22732373614720 run.py:483] Algo bellman_ford step 7651 current loss 0.413668, current_train_items 244864.
I0302 19:02:01.006544 22732373614720 run.py:483] Algo bellman_ford step 7652 current loss 0.692063, current_train_items 244896.
I0302 19:02:01.037931 22732373614720 run.py:483] Algo bellman_ford step 7653 current loss 0.689166, current_train_items 244928.
I0302 19:02:01.071239 22732373614720 run.py:483] Algo bellman_ford step 7654 current loss 0.896650, current_train_items 244960.
I0302 19:02:01.091276 22732373614720 run.py:483] Algo bellman_ford step 7655 current loss 0.303187, current_train_items 244992.
I0302 19:02:01.107122 22732373614720 run.py:483] Algo bellman_ford step 7656 current loss 0.493599, current_train_items 245024.
I0302 19:02:01.130773 22732373614720 run.py:483] Algo bellman_ford step 7657 current loss 0.708207, current_train_items 245056.
I0302 19:02:01.162585 22732373614720 run.py:483] Algo bellman_ford step 7658 current loss 0.683991, current_train_items 245088.
I0302 19:02:01.196903 22732373614720 run.py:483] Algo bellman_ford step 7659 current loss 1.203508, current_train_items 245120.
I0302 19:02:01.216639 22732373614720 run.py:483] Algo bellman_ford step 7660 current loss 0.373124, current_train_items 245152.
I0302 19:02:01.232826 22732373614720 run.py:483] Algo bellman_ford step 7661 current loss 0.437045, current_train_items 245184.
I0302 19:02:01.255799 22732373614720 run.py:483] Algo bellman_ford step 7662 current loss 0.621090, current_train_items 245216.
I0302 19:02:01.286403 22732373614720 run.py:483] Algo bellman_ford step 7663 current loss 0.759082, current_train_items 245248.
I0302 19:02:01.320115 22732373614720 run.py:483] Algo bellman_ford step 7664 current loss 0.854265, current_train_items 245280.
I0302 19:02:01.339542 22732373614720 run.py:483] Algo bellman_ford step 7665 current loss 0.360367, current_train_items 245312.
I0302 19:02:01.355762 22732373614720 run.py:483] Algo bellman_ford step 7666 current loss 0.538998, current_train_items 245344.
I0302 19:02:01.378976 22732373614720 run.py:483] Algo bellman_ford step 7667 current loss 0.758321, current_train_items 245376.
I0302 19:02:01.412206 22732373614720 run.py:483] Algo bellman_ford step 7668 current loss 0.781203, current_train_items 245408.
I0302 19:02:01.446103 22732373614720 run.py:483] Algo bellman_ford step 7669 current loss 0.849269, current_train_items 245440.
I0302 19:02:01.465610 22732373614720 run.py:483] Algo bellman_ford step 7670 current loss 0.351882, current_train_items 245472.
I0302 19:02:01.482043 22732373614720 run.py:483] Algo bellman_ford step 7671 current loss 0.479667, current_train_items 245504.
I0302 19:02:01.504938 22732373614720 run.py:483] Algo bellman_ford step 7672 current loss 0.645315, current_train_items 245536.
I0302 19:02:01.536599 22732373614720 run.py:483] Algo bellman_ford step 7673 current loss 0.795820, current_train_items 245568.
I0302 19:02:01.570079 22732373614720 run.py:483] Algo bellman_ford step 7674 current loss 0.912493, current_train_items 245600.
I0302 19:02:01.589963 22732373614720 run.py:483] Algo bellman_ford step 7675 current loss 0.367073, current_train_items 245632.
I0302 19:02:01.606536 22732373614720 run.py:483] Algo bellman_ford step 7676 current loss 0.563690, current_train_items 245664.
I0302 19:02:01.630413 22732373614720 run.py:483] Algo bellman_ford step 7677 current loss 0.663242, current_train_items 245696.
I0302 19:02:01.661195 22732373614720 run.py:483] Algo bellman_ford step 7678 current loss 0.641831, current_train_items 245728.
I0302 19:02:01.695842 22732373614720 run.py:483] Algo bellman_ford step 7679 current loss 0.886168, current_train_items 245760.
I0302 19:02:01.715133 22732373614720 run.py:483] Algo bellman_ford step 7680 current loss 0.362349, current_train_items 245792.
I0302 19:02:01.731412 22732373614720 run.py:483] Algo bellman_ford step 7681 current loss 0.683846, current_train_items 245824.
I0302 19:02:01.754477 22732373614720 run.py:483] Algo bellman_ford step 7682 current loss 0.621149, current_train_items 245856.
I0302 19:02:01.785903 22732373614720 run.py:483] Algo bellman_ford step 7683 current loss 0.710983, current_train_items 245888.
I0302 19:02:01.820068 22732373614720 run.py:483] Algo bellman_ford step 7684 current loss 0.786702, current_train_items 245920.
I0302 19:02:01.839977 22732373614720 run.py:483] Algo bellman_ford step 7685 current loss 0.302322, current_train_items 245952.
I0302 19:02:01.856398 22732373614720 run.py:483] Algo bellman_ford step 7686 current loss 0.623750, current_train_items 245984.
I0302 19:02:01.878863 22732373614720 run.py:483] Algo bellman_ford step 7687 current loss 0.664259, current_train_items 246016.
I0302 19:02:01.911275 22732373614720 run.py:483] Algo bellman_ford step 7688 current loss 0.777231, current_train_items 246048.
I0302 19:02:01.944757 22732373614720 run.py:483] Algo bellman_ford step 7689 current loss 0.841319, current_train_items 246080.
I0302 19:02:01.964684 22732373614720 run.py:483] Algo bellman_ford step 7690 current loss 0.303620, current_train_items 246112.
I0302 19:02:01.980939 22732373614720 run.py:483] Algo bellman_ford step 7691 current loss 0.530131, current_train_items 246144.
I0302 19:02:02.004030 22732373614720 run.py:483] Algo bellman_ford step 7692 current loss 0.655238, current_train_items 246176.
I0302 19:02:02.037132 22732373614720 run.py:483] Algo bellman_ford step 7693 current loss 0.789291, current_train_items 246208.
I0302 19:02:02.069904 22732373614720 run.py:483] Algo bellman_ford step 7694 current loss 0.834515, current_train_items 246240.
I0302 19:02:02.089688 22732373614720 run.py:483] Algo bellman_ford step 7695 current loss 0.357545, current_train_items 246272.
I0302 19:02:02.106038 22732373614720 run.py:483] Algo bellman_ford step 7696 current loss 0.510666, current_train_items 246304.
I0302 19:02:02.130324 22732373614720 run.py:483] Algo bellman_ford step 7697 current loss 0.668100, current_train_items 246336.
I0302 19:02:02.162642 22732373614720 run.py:483] Algo bellman_ford step 7698 current loss 0.776302, current_train_items 246368.
I0302 19:02:02.197488 22732373614720 run.py:483] Algo bellman_ford step 7699 current loss 0.922625, current_train_items 246400.
I0302 19:02:02.217663 22732373614720 run.py:483] Algo bellman_ford step 7700 current loss 0.424880, current_train_items 246432.
I0302 19:02:02.225406 22732373614720 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.8720703125, 'score': 0.8720703125, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0302 19:02:02.225518 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.872, val scores are: bellman_ford: 0.872
I0302 19:02:02.242712 22732373614720 run.py:483] Algo bellman_ford step 7701 current loss 0.638028, current_train_items 246464.
I0302 19:02:02.266768 22732373614720 run.py:483] Algo bellman_ford step 7702 current loss 0.891391, current_train_items 246496.
I0302 19:02:02.297235 22732373614720 run.py:483] Algo bellman_ford step 7703 current loss 0.718981, current_train_items 246528.
I0302 19:02:02.329902 22732373614720 run.py:483] Algo bellman_ford step 7704 current loss 0.980388, current_train_items 246560.
I0302 19:02:02.349773 22732373614720 run.py:483] Algo bellman_ford step 7705 current loss 0.385888, current_train_items 246592.
I0302 19:02:02.365486 22732373614720 run.py:483] Algo bellman_ford step 7706 current loss 0.504000, current_train_items 246624.
I0302 19:02:02.390007 22732373614720 run.py:483] Algo bellman_ford step 7707 current loss 0.663051, current_train_items 246656.
I0302 19:02:02.420279 22732373614720 run.py:483] Algo bellman_ford step 7708 current loss 0.709599, current_train_items 246688.
I0302 19:02:02.452200 22732373614720 run.py:483] Algo bellman_ford step 7709 current loss 0.876131, current_train_items 246720.
I0302 19:02:02.471715 22732373614720 run.py:483] Algo bellman_ford step 7710 current loss 0.395814, current_train_items 246752.
I0302 19:02:02.487953 22732373614720 run.py:483] Algo bellman_ford step 7711 current loss 0.497578, current_train_items 246784.
I0302 19:02:02.511315 22732373614720 run.py:483] Algo bellman_ford step 7712 current loss 0.648456, current_train_items 246816.
I0302 19:02:02.541596 22732373614720 run.py:483] Algo bellman_ford step 7713 current loss 0.779903, current_train_items 246848.
I0302 19:02:02.574185 22732373614720 run.py:483] Algo bellman_ford step 7714 current loss 0.837627, current_train_items 246880.
I0302 19:02:02.593593 22732373614720 run.py:483] Algo bellman_ford step 7715 current loss 0.385251, current_train_items 246912.
I0302 19:02:02.609386 22732373614720 run.py:483] Algo bellman_ford step 7716 current loss 0.480495, current_train_items 246944.
I0302 19:02:02.633010 22732373614720 run.py:483] Algo bellman_ford step 7717 current loss 0.623297, current_train_items 246976.
I0302 19:02:02.663834 22732373614720 run.py:483] Algo bellman_ford step 7718 current loss 0.684694, current_train_items 247008.
I0302 19:02:02.695686 22732373614720 run.py:483] Algo bellman_ford step 7719 current loss 0.671852, current_train_items 247040.
I0302 19:02:02.715386 22732373614720 run.py:483] Algo bellman_ford step 7720 current loss 0.389957, current_train_items 247072.
I0302 19:02:02.731285 22732373614720 run.py:483] Algo bellman_ford step 7721 current loss 0.462189, current_train_items 247104.
I0302 19:02:02.755479 22732373614720 run.py:483] Algo bellman_ford step 7722 current loss 0.655497, current_train_items 247136.
I0302 19:02:02.787601 22732373614720 run.py:483] Algo bellman_ford step 7723 current loss 0.686541, current_train_items 247168.
I0302 19:02:02.821105 22732373614720 run.py:483] Algo bellman_ford step 7724 current loss 0.742452, current_train_items 247200.
I0302 19:02:02.840739 22732373614720 run.py:483] Algo bellman_ford step 7725 current loss 0.340493, current_train_items 247232.
I0302 19:02:02.856803 22732373614720 run.py:483] Algo bellman_ford step 7726 current loss 0.540919, current_train_items 247264.
I0302 19:02:02.879047 22732373614720 run.py:483] Algo bellman_ford step 7727 current loss 0.740772, current_train_items 247296.
I0302 19:02:02.910135 22732373614720 run.py:483] Algo bellman_ford step 7728 current loss 0.707051, current_train_items 247328.
I0302 19:02:02.942086 22732373614720 run.py:483] Algo bellman_ford step 7729 current loss 0.730230, current_train_items 247360.
I0302 19:02:02.961525 22732373614720 run.py:483] Algo bellman_ford step 7730 current loss 0.372407, current_train_items 247392.
I0302 19:02:02.977303 22732373614720 run.py:483] Algo bellman_ford step 7731 current loss 0.521618, current_train_items 247424.
I0302 19:02:03.000221 22732373614720 run.py:483] Algo bellman_ford step 7732 current loss 0.576829, current_train_items 247456.
I0302 19:02:03.031419 22732373614720 run.py:483] Algo bellman_ford step 7733 current loss 0.770856, current_train_items 247488.
I0302 19:02:03.062631 22732373614720 run.py:483] Algo bellman_ford step 7734 current loss 0.837176, current_train_items 247520.
I0302 19:02:03.081974 22732373614720 run.py:483] Algo bellman_ford step 7735 current loss 0.312431, current_train_items 247552.
I0302 19:02:03.098126 22732373614720 run.py:483] Algo bellman_ford step 7736 current loss 0.611893, current_train_items 247584.
I0302 19:02:03.121541 22732373614720 run.py:483] Algo bellman_ford step 7737 current loss 0.580035, current_train_items 247616.
I0302 19:02:03.152398 22732373614720 run.py:483] Algo bellman_ford step 7738 current loss 0.798334, current_train_items 247648.
I0302 19:02:03.185933 22732373614720 run.py:483] Algo bellman_ford step 7739 current loss 0.790113, current_train_items 247680.
I0302 19:02:03.205282 22732373614720 run.py:483] Algo bellman_ford step 7740 current loss 0.335168, current_train_items 247712.
I0302 19:02:03.221520 22732373614720 run.py:483] Algo bellman_ford step 7741 current loss 0.456394, current_train_items 247744.
I0302 19:02:03.246162 22732373614720 run.py:483] Algo bellman_ford step 7742 current loss 0.737286, current_train_items 247776.
I0302 19:02:03.278202 22732373614720 run.py:483] Algo bellman_ford step 7743 current loss 0.961823, current_train_items 247808.
I0302 19:02:03.311277 22732373614720 run.py:483] Algo bellman_ford step 7744 current loss 1.247501, current_train_items 247840.
I0302 19:02:03.330888 22732373614720 run.py:483] Algo bellman_ford step 7745 current loss 0.350704, current_train_items 247872.
I0302 19:02:03.346859 22732373614720 run.py:483] Algo bellman_ford step 7746 current loss 0.436323, current_train_items 247904.
I0302 19:02:03.370047 22732373614720 run.py:483] Algo bellman_ford step 7747 current loss 0.765427, current_train_items 247936.
I0302 19:02:03.402684 22732373614720 run.py:483] Algo bellman_ford step 7748 current loss 0.822019, current_train_items 247968.
I0302 19:02:03.436243 22732373614720 run.py:483] Algo bellman_ford step 7749 current loss 0.798737, current_train_items 248000.
I0302 19:02:03.456000 22732373614720 run.py:483] Algo bellman_ford step 7750 current loss 0.414259, current_train_items 248032.
I0302 19:02:03.464145 22732373614720 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0302 19:02:03.464264 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:02:03.481276 22732373614720 run.py:483] Algo bellman_ford step 7751 current loss 0.568835, current_train_items 248064.
I0302 19:02:03.505307 22732373614720 run.py:483] Algo bellman_ford step 7752 current loss 0.721189, current_train_items 248096.
I0302 19:02:03.536799 22732373614720 run.py:483] Algo bellman_ford step 7753 current loss 0.646716, current_train_items 248128.
I0302 19:02:03.571923 22732373614720 run.py:483] Algo bellman_ford step 7754 current loss 0.922693, current_train_items 248160.
I0302 19:02:03.591978 22732373614720 run.py:483] Algo bellman_ford step 7755 current loss 0.334976, current_train_items 248192.
I0302 19:02:03.607285 22732373614720 run.py:483] Algo bellman_ford step 7756 current loss 0.519949, current_train_items 248224.
I0302 19:02:03.630782 22732373614720 run.py:483] Algo bellman_ford step 7757 current loss 0.687568, current_train_items 248256.
I0302 19:02:03.660555 22732373614720 run.py:483] Algo bellman_ford step 7758 current loss 0.774832, current_train_items 248288.
I0302 19:02:03.693058 22732373614720 run.py:483] Algo bellman_ford step 7759 current loss 1.015641, current_train_items 248320.
I0302 19:02:03.713275 22732373614720 run.py:483] Algo bellman_ford step 7760 current loss 0.353330, current_train_items 248352.
I0302 19:02:03.729402 22732373614720 run.py:483] Algo bellman_ford step 7761 current loss 0.679808, current_train_items 248384.
I0302 19:02:03.753311 22732373614720 run.py:483] Algo bellman_ford step 7762 current loss 0.614734, current_train_items 248416.
I0302 19:02:03.784350 22732373614720 run.py:483] Algo bellman_ford step 7763 current loss 0.830799, current_train_items 248448.
I0302 19:02:03.816165 22732373614720 run.py:483] Algo bellman_ford step 7764 current loss 0.930408, current_train_items 248480.
I0302 19:02:03.835912 22732373614720 run.py:483] Algo bellman_ford step 7765 current loss 0.375832, current_train_items 248512.
I0302 19:02:03.851937 22732373614720 run.py:483] Algo bellman_ford step 7766 current loss 0.523401, current_train_items 248544.
I0302 19:02:03.875607 22732373614720 run.py:483] Algo bellman_ford step 7767 current loss 0.565619, current_train_items 248576.
I0302 19:02:03.906886 22732373614720 run.py:483] Algo bellman_ford step 7768 current loss 0.678922, current_train_items 248608.
I0302 19:02:03.939872 22732373614720 run.py:483] Algo bellman_ford step 7769 current loss 0.884116, current_train_items 248640.
I0302 19:02:03.959800 22732373614720 run.py:483] Algo bellman_ford step 7770 current loss 0.367023, current_train_items 248672.
I0302 19:02:03.976140 22732373614720 run.py:483] Algo bellman_ford step 7771 current loss 0.487512, current_train_items 248704.
I0302 19:02:03.999267 22732373614720 run.py:483] Algo bellman_ford step 7772 current loss 0.611106, current_train_items 248736.
I0302 19:02:04.030803 22732373614720 run.py:483] Algo bellman_ford step 7773 current loss 0.747752, current_train_items 248768.
I0302 19:02:04.063817 22732373614720 run.py:483] Algo bellman_ford step 7774 current loss 0.778991, current_train_items 248800.
I0302 19:02:04.084033 22732373614720 run.py:483] Algo bellman_ford step 7775 current loss 0.331257, current_train_items 248832.
I0302 19:02:04.100311 22732373614720 run.py:483] Algo bellman_ford step 7776 current loss 0.655629, current_train_items 248864.
I0302 19:02:04.124816 22732373614720 run.py:483] Algo bellman_ford step 7777 current loss 0.745824, current_train_items 248896.
I0302 19:02:04.156641 22732373614720 run.py:483] Algo bellman_ford step 7778 current loss 0.705717, current_train_items 248928.
I0302 19:02:04.191308 22732373614720 run.py:483] Algo bellman_ford step 7779 current loss 0.798488, current_train_items 248960.
I0302 19:02:04.210748 22732373614720 run.py:483] Algo bellman_ford step 7780 current loss 0.325205, current_train_items 248992.
I0302 19:02:04.227072 22732373614720 run.py:483] Algo bellman_ford step 7781 current loss 0.573195, current_train_items 249024.
I0302 19:02:04.250768 22732373614720 run.py:483] Algo bellman_ford step 7782 current loss 0.617335, current_train_items 249056.
I0302 19:02:04.282739 22732373614720 run.py:483] Algo bellman_ford step 7783 current loss 0.731739, current_train_items 249088.
I0302 19:02:04.316212 22732373614720 run.py:483] Algo bellman_ford step 7784 current loss 0.924428, current_train_items 249120.
I0302 19:02:04.336364 22732373614720 run.py:483] Algo bellman_ford step 7785 current loss 0.359072, current_train_items 249152.
I0302 19:02:04.352722 22732373614720 run.py:483] Algo bellman_ford step 7786 current loss 0.511553, current_train_items 249184.
I0302 19:02:04.375731 22732373614720 run.py:483] Algo bellman_ford step 7787 current loss 0.602140, current_train_items 249216.
I0302 19:02:04.406778 22732373614720 run.py:483] Algo bellman_ford step 7788 current loss 0.665771, current_train_items 249248.
I0302 19:02:04.437981 22732373614720 run.py:483] Algo bellman_ford step 7789 current loss 0.760263, current_train_items 249280.
I0302 19:02:04.457868 22732373614720 run.py:483] Algo bellman_ford step 7790 current loss 0.384700, current_train_items 249312.
I0302 19:02:04.473545 22732373614720 run.py:483] Algo bellman_ford step 7791 current loss 0.489242, current_train_items 249344.
I0302 19:02:04.496883 22732373614720 run.py:483] Algo bellman_ford step 7792 current loss 0.673559, current_train_items 249376.
I0302 19:02:04.527692 22732373614720 run.py:483] Algo bellman_ford step 7793 current loss 0.746320, current_train_items 249408.
I0302 19:02:04.560684 22732373614720 run.py:483] Algo bellman_ford step 7794 current loss 0.772998, current_train_items 249440.
I0302 19:02:04.580515 22732373614720 run.py:483] Algo bellman_ford step 7795 current loss 0.364982, current_train_items 249472.
I0302 19:02:04.596219 22732373614720 run.py:483] Algo bellman_ford step 7796 current loss 0.558848, current_train_items 249504.
I0302 19:02:04.618934 22732373614720 run.py:483] Algo bellman_ford step 7797 current loss 0.644780, current_train_items 249536.
I0302 19:02:04.649582 22732373614720 run.py:483] Algo bellman_ford step 7798 current loss 0.608275, current_train_items 249568.
I0302 19:02:04.683881 22732373614720 run.py:483] Algo bellman_ford step 7799 current loss 0.761168, current_train_items 249600.
I0302 19:02:04.703930 22732373614720 run.py:483] Algo bellman_ford step 7800 current loss 0.392678, current_train_items 249632.
I0302 19:02:04.711519 22732373614720 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0302 19:02:04.711628 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:02:04.728365 22732373614720 run.py:483] Algo bellman_ford step 7801 current loss 0.636945, current_train_items 249664.
I0302 19:02:04.752484 22732373614720 run.py:483] Algo bellman_ford step 7802 current loss 0.688742, current_train_items 249696.
I0302 19:02:04.784800 22732373614720 run.py:483] Algo bellman_ford step 7803 current loss 0.801211, current_train_items 249728.
I0302 19:02:04.820391 22732373614720 run.py:483] Algo bellman_ford step 7804 current loss 0.865080, current_train_items 249760.
I0302 19:02:04.840498 22732373614720 run.py:483] Algo bellman_ford step 7805 current loss 0.319479, current_train_items 249792.
I0302 19:02:04.856500 22732373614720 run.py:483] Algo bellman_ford step 7806 current loss 0.507290, current_train_items 249824.
I0302 19:02:04.879095 22732373614720 run.py:483] Algo bellman_ford step 7807 current loss 0.549948, current_train_items 249856.
I0302 19:02:04.909081 22732373614720 run.py:483] Algo bellman_ford step 7808 current loss 0.624420, current_train_items 249888.
I0302 19:02:04.942458 22732373614720 run.py:483] Algo bellman_ford step 7809 current loss 0.746850, current_train_items 249920.
I0302 19:02:04.962040 22732373614720 run.py:483] Algo bellman_ford step 7810 current loss 0.419486, current_train_items 249952.
I0302 19:02:04.978470 22732373614720 run.py:483] Algo bellman_ford step 7811 current loss 0.603762, current_train_items 249984.
I0302 19:02:05.002468 22732373614720 run.py:483] Algo bellman_ford step 7812 current loss 0.672187, current_train_items 250016.
I0302 19:02:05.034828 22732373614720 run.py:483] Algo bellman_ford step 7813 current loss 0.778774, current_train_items 250048.
I0302 19:02:05.068190 22732373614720 run.py:483] Algo bellman_ford step 7814 current loss 0.842865, current_train_items 250080.
I0302 19:02:05.087477 22732373614720 run.py:483] Algo bellman_ford step 7815 current loss 0.372901, current_train_items 250112.
I0302 19:02:05.103300 22732373614720 run.py:483] Algo bellman_ford step 7816 current loss 0.516338, current_train_items 250144.
I0302 19:02:05.127411 22732373614720 run.py:483] Algo bellman_ford step 7817 current loss 0.668145, current_train_items 250176.
I0302 19:02:05.157200 22732373614720 run.py:483] Algo bellman_ford step 7818 current loss 0.674296, current_train_items 250208.
I0302 19:02:05.189263 22732373614720 run.py:483] Algo bellman_ford step 7819 current loss 0.790702, current_train_items 250240.
I0302 19:02:05.208789 22732373614720 run.py:483] Algo bellman_ford step 7820 current loss 0.392946, current_train_items 250272.
I0302 19:02:05.225170 22732373614720 run.py:483] Algo bellman_ford step 7821 current loss 0.689909, current_train_items 250304.
I0302 19:02:05.250046 22732373614720 run.py:483] Algo bellman_ford step 7822 current loss 0.700088, current_train_items 250336.
I0302 19:02:05.282750 22732373614720 run.py:483] Algo bellman_ford step 7823 current loss 0.719346, current_train_items 250368.
I0302 19:02:05.315043 22732373614720 run.py:483] Algo bellman_ford step 7824 current loss 0.826257, current_train_items 250400.
I0302 19:02:05.334551 22732373614720 run.py:483] Algo bellman_ford step 7825 current loss 0.335321, current_train_items 250432.
I0302 19:02:05.351002 22732373614720 run.py:483] Algo bellman_ford step 7826 current loss 0.653332, current_train_items 250464.
I0302 19:02:05.375266 22732373614720 run.py:483] Algo bellman_ford step 7827 current loss 0.764394, current_train_items 250496.
I0302 19:02:05.406964 22732373614720 run.py:483] Algo bellman_ford step 7828 current loss 0.842048, current_train_items 250528.
I0302 19:02:05.440054 22732373614720 run.py:483] Algo bellman_ford step 7829 current loss 1.023705, current_train_items 250560.
I0302 19:02:05.459739 22732373614720 run.py:483] Algo bellman_ford step 7830 current loss 0.303034, current_train_items 250592.
I0302 19:02:05.475438 22732373614720 run.py:483] Algo bellman_ford step 7831 current loss 0.523492, current_train_items 250624.
I0302 19:02:05.499173 22732373614720 run.py:483] Algo bellman_ford step 7832 current loss 0.680377, current_train_items 250656.
I0302 19:02:05.531047 22732373614720 run.py:483] Algo bellman_ford step 7833 current loss 0.731242, current_train_items 250688.
I0302 19:02:05.562263 22732373614720 run.py:483] Algo bellman_ford step 7834 current loss 0.858505, current_train_items 250720.
I0302 19:02:05.581752 22732373614720 run.py:483] Algo bellman_ford step 7835 current loss 0.345060, current_train_items 250752.
I0302 19:02:05.598145 22732373614720 run.py:483] Algo bellman_ford step 7836 current loss 0.571161, current_train_items 250784.
I0302 19:02:05.621682 22732373614720 run.py:483] Algo bellman_ford step 7837 current loss 0.588287, current_train_items 250816.
I0302 19:02:05.654214 22732373614720 run.py:483] Algo bellman_ford step 7838 current loss 0.783583, current_train_items 250848.
I0302 19:02:05.688498 22732373614720 run.py:483] Algo bellman_ford step 7839 current loss 1.048735, current_train_items 250880.
I0302 19:02:05.708111 22732373614720 run.py:483] Algo bellman_ford step 7840 current loss 0.300168, current_train_items 250912.
I0302 19:02:05.724202 22732373614720 run.py:483] Algo bellman_ford step 7841 current loss 0.504954, current_train_items 250944.
I0302 19:02:05.747494 22732373614720 run.py:483] Algo bellman_ford step 7842 current loss 0.738395, current_train_items 250976.
I0302 19:02:05.780055 22732373614720 run.py:483] Algo bellman_ford step 7843 current loss 0.730789, current_train_items 251008.
I0302 19:02:05.814506 22732373614720 run.py:483] Algo bellman_ford step 7844 current loss 0.816611, current_train_items 251040.
I0302 19:02:05.833854 22732373614720 run.py:483] Algo bellman_ford step 7845 current loss 0.473655, current_train_items 251072.
I0302 19:02:05.849784 22732373614720 run.py:483] Algo bellman_ford step 7846 current loss 0.581843, current_train_items 251104.
I0302 19:02:05.873764 22732373614720 run.py:483] Algo bellman_ford step 7847 current loss 0.657036, current_train_items 251136.
I0302 19:02:05.905386 22732373614720 run.py:483] Algo bellman_ford step 7848 current loss 0.742517, current_train_items 251168.
I0302 19:02:05.938949 22732373614720 run.py:483] Algo bellman_ford step 7849 current loss 0.856635, current_train_items 251200.
I0302 19:02:05.958649 22732373614720 run.py:483] Algo bellman_ford step 7850 current loss 0.354785, current_train_items 251232.
I0302 19:02:05.966921 22732373614720 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0302 19:02:05.967033 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:05.983703 22732373614720 run.py:483] Algo bellman_ford step 7851 current loss 0.561140, current_train_items 251264.
I0302 19:02:06.007280 22732373614720 run.py:483] Algo bellman_ford step 7852 current loss 0.589836, current_train_items 251296.
I0302 19:02:06.039976 22732373614720 run.py:483] Algo bellman_ford step 7853 current loss 0.702665, current_train_items 251328.
I0302 19:02:06.075429 22732373614720 run.py:483] Algo bellman_ford step 7854 current loss 0.831842, current_train_items 251360.
I0302 19:02:06.095535 22732373614720 run.py:483] Algo bellman_ford step 7855 current loss 0.398819, current_train_items 251392.
I0302 19:02:06.111860 22732373614720 run.py:483] Algo bellman_ford step 7856 current loss 0.628314, current_train_items 251424.
I0302 19:02:06.135635 22732373614720 run.py:483] Algo bellman_ford step 7857 current loss 0.687442, current_train_items 251456.
I0302 19:02:06.166613 22732373614720 run.py:483] Algo bellman_ford step 7858 current loss 0.758132, current_train_items 251488.
I0302 19:02:06.201083 22732373614720 run.py:483] Algo bellman_ford step 7859 current loss 1.049247, current_train_items 251520.
I0302 19:02:06.220854 22732373614720 run.py:483] Algo bellman_ford step 7860 current loss 0.358476, current_train_items 251552.
I0302 19:02:06.236842 22732373614720 run.py:483] Algo bellman_ford step 7861 current loss 0.432477, current_train_items 251584.
I0302 19:02:06.260218 22732373614720 run.py:483] Algo bellman_ford step 7862 current loss 0.787439, current_train_items 251616.
I0302 19:02:06.291266 22732373614720 run.py:483] Algo bellman_ford step 7863 current loss 0.744867, current_train_items 251648.
I0302 19:02:06.323205 22732373614720 run.py:483] Algo bellman_ford step 7864 current loss 0.816344, current_train_items 251680.
I0302 19:02:06.342822 22732373614720 run.py:483] Algo bellman_ford step 7865 current loss 0.359647, current_train_items 251712.
I0302 19:02:06.359214 22732373614720 run.py:483] Algo bellman_ford step 7866 current loss 0.604164, current_train_items 251744.
I0302 19:02:06.383344 22732373614720 run.py:483] Algo bellman_ford step 7867 current loss 0.587825, current_train_items 251776.
I0302 19:02:06.414074 22732373614720 run.py:483] Algo bellman_ford step 7868 current loss 0.742999, current_train_items 251808.
I0302 19:02:06.446507 22732373614720 run.py:483] Algo bellman_ford step 7869 current loss 0.789099, current_train_items 251840.
I0302 19:02:06.466299 22732373614720 run.py:483] Algo bellman_ford step 7870 current loss 0.367312, current_train_items 251872.
I0302 19:02:06.482568 22732373614720 run.py:483] Algo bellman_ford step 7871 current loss 0.462329, current_train_items 251904.
I0302 19:02:06.505990 22732373614720 run.py:483] Algo bellman_ford step 7872 current loss 0.635190, current_train_items 251936.
I0302 19:02:06.536509 22732373614720 run.py:483] Algo bellman_ford step 7873 current loss 0.734977, current_train_items 251968.
I0302 19:02:06.569142 22732373614720 run.py:483] Algo bellman_ford step 7874 current loss 0.731294, current_train_items 252000.
I0302 19:02:06.588913 22732373614720 run.py:483] Algo bellman_ford step 7875 current loss 0.363594, current_train_items 252032.
I0302 19:02:06.604587 22732373614720 run.py:483] Algo bellman_ford step 7876 current loss 0.466037, current_train_items 252064.
I0302 19:02:06.627489 22732373614720 run.py:483] Algo bellman_ford step 7877 current loss 0.651339, current_train_items 252096.
I0302 19:02:06.658311 22732373614720 run.py:483] Algo bellman_ford step 7878 current loss 0.701502, current_train_items 252128.
I0302 19:02:06.691387 22732373614720 run.py:483] Algo bellman_ford step 7879 current loss 0.819903, current_train_items 252160.
I0302 19:02:06.710633 22732373614720 run.py:483] Algo bellman_ford step 7880 current loss 0.336669, current_train_items 252192.
I0302 19:02:06.726215 22732373614720 run.py:483] Algo bellman_ford step 7881 current loss 0.445353, current_train_items 252224.
I0302 19:02:06.749143 22732373614720 run.py:483] Algo bellman_ford step 7882 current loss 0.621836, current_train_items 252256.
I0302 19:02:06.780519 22732373614720 run.py:483] Algo bellman_ford step 7883 current loss 0.673117, current_train_items 252288.
I0302 19:02:06.813808 22732373614720 run.py:483] Algo bellman_ford step 7884 current loss 0.864799, current_train_items 252320.
I0302 19:02:06.833733 22732373614720 run.py:483] Algo bellman_ford step 7885 current loss 0.352668, current_train_items 252352.
I0302 19:02:06.849874 22732373614720 run.py:483] Algo bellman_ford step 7886 current loss 0.490729, current_train_items 252384.
I0302 19:02:06.872359 22732373614720 run.py:483] Algo bellman_ford step 7887 current loss 0.627295, current_train_items 252416.
I0302 19:02:06.901875 22732373614720 run.py:483] Algo bellman_ford step 7888 current loss 0.711142, current_train_items 252448.
I0302 19:02:06.934481 22732373614720 run.py:483] Algo bellman_ford step 7889 current loss 0.711230, current_train_items 252480.
I0302 19:02:06.954241 22732373614720 run.py:483] Algo bellman_ford step 7890 current loss 0.320559, current_train_items 252512.
I0302 19:02:06.970161 22732373614720 run.py:483] Algo bellman_ford step 7891 current loss 0.495943, current_train_items 252544.
I0302 19:02:06.993601 22732373614720 run.py:483] Algo bellman_ford step 7892 current loss 0.606596, current_train_items 252576.
I0302 19:02:07.025109 22732373614720 run.py:483] Algo bellman_ford step 7893 current loss 0.730298, current_train_items 252608.
I0302 19:02:07.057482 22732373614720 run.py:483] Algo bellman_ford step 7894 current loss 0.753225, current_train_items 252640.
I0302 19:02:07.076832 22732373614720 run.py:483] Algo bellman_ford step 7895 current loss 0.297050, current_train_items 252672.
I0302 19:02:07.092595 22732373614720 run.py:483] Algo bellman_ford step 7896 current loss 0.508093, current_train_items 252704.
I0302 19:02:07.116606 22732373614720 run.py:483] Algo bellman_ford step 7897 current loss 0.686503, current_train_items 252736.
I0302 19:02:07.148362 22732373614720 run.py:483] Algo bellman_ford step 7898 current loss 0.681029, current_train_items 252768.
I0302 19:02:07.181624 22732373614720 run.py:483] Algo bellman_ford step 7899 current loss 0.848112, current_train_items 252800.
I0302 19:02:07.201436 22732373614720 run.py:483] Algo bellman_ford step 7900 current loss 0.341606, current_train_items 252832.
I0302 19:02:07.209285 22732373614720 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0302 19:02:07.209397 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 19:02:07.226109 22732373614720 run.py:483] Algo bellman_ford step 7901 current loss 0.499099, current_train_items 252864.
I0302 19:02:07.250720 22732373614720 run.py:483] Algo bellman_ford step 7902 current loss 0.638004, current_train_items 252896.
I0302 19:02:07.283250 22732373614720 run.py:483] Algo bellman_ford step 7903 current loss 0.754182, current_train_items 252928.
I0302 19:02:07.318997 22732373614720 run.py:483] Algo bellman_ford step 7904 current loss 0.969722, current_train_items 252960.
I0302 19:02:07.338929 22732373614720 run.py:483] Algo bellman_ford step 7905 current loss 0.368782, current_train_items 252992.
I0302 19:02:07.355263 22732373614720 run.py:483] Algo bellman_ford step 7906 current loss 0.587614, current_train_items 253024.
I0302 19:02:07.379225 22732373614720 run.py:483] Algo bellman_ford step 7907 current loss 0.646754, current_train_items 253056.
I0302 19:02:07.409998 22732373614720 run.py:483] Algo bellman_ford step 7908 current loss 0.699054, current_train_items 253088.
I0302 19:02:07.440931 22732373614720 run.py:483] Algo bellman_ford step 7909 current loss 0.802650, current_train_items 253120.
I0302 19:02:07.460719 22732373614720 run.py:483] Algo bellman_ford step 7910 current loss 0.334902, current_train_items 253152.
I0302 19:02:07.477337 22732373614720 run.py:483] Algo bellman_ford step 7911 current loss 0.574670, current_train_items 253184.
I0302 19:02:07.501398 22732373614720 run.py:483] Algo bellman_ford step 7912 current loss 0.785774, current_train_items 253216.
I0302 19:02:07.532956 22732373614720 run.py:483] Algo bellman_ford step 7913 current loss 0.703336, current_train_items 253248.
I0302 19:02:07.566450 22732373614720 run.py:483] Algo bellman_ford step 7914 current loss 0.814076, current_train_items 253280.
I0302 19:02:07.586180 22732373614720 run.py:483] Algo bellman_ford step 7915 current loss 0.321947, current_train_items 253312.
I0302 19:02:07.602421 22732373614720 run.py:483] Algo bellman_ford step 7916 current loss 0.597700, current_train_items 253344.
I0302 19:02:07.625751 22732373614720 run.py:483] Algo bellman_ford step 7917 current loss 0.652645, current_train_items 253376.
I0302 19:02:07.657175 22732373614720 run.py:483] Algo bellman_ford step 7918 current loss 0.671726, current_train_items 253408.
I0302 19:02:07.692006 22732373614720 run.py:483] Algo bellman_ford step 7919 current loss 0.781148, current_train_items 253440.
I0302 19:02:07.711576 22732373614720 run.py:483] Algo bellman_ford step 7920 current loss 0.316568, current_train_items 253472.
I0302 19:02:07.727901 22732373614720 run.py:483] Algo bellman_ford step 7921 current loss 0.494480, current_train_items 253504.
I0302 19:02:07.751836 22732373614720 run.py:483] Algo bellman_ford step 7922 current loss 0.660899, current_train_items 253536.
I0302 19:02:07.782925 22732373614720 run.py:483] Algo bellman_ford step 7923 current loss 0.582453, current_train_items 253568.
I0302 19:02:07.816762 22732373614720 run.py:483] Algo bellman_ford step 7924 current loss 0.899066, current_train_items 253600.
I0302 19:02:07.836261 22732373614720 run.py:483] Algo bellman_ford step 7925 current loss 0.397772, current_train_items 253632.
I0302 19:02:07.852411 22732373614720 run.py:483] Algo bellman_ford step 7926 current loss 0.544840, current_train_items 253664.
I0302 19:02:07.875739 22732373614720 run.py:483] Algo bellman_ford step 7927 current loss 0.698243, current_train_items 253696.
I0302 19:02:07.907902 22732373614720 run.py:483] Algo bellman_ford step 7928 current loss 0.917088, current_train_items 253728.
I0302 19:02:07.940696 22732373614720 run.py:483] Algo bellman_ford step 7929 current loss 0.820654, current_train_items 253760.
I0302 19:02:07.960170 22732373614720 run.py:483] Algo bellman_ford step 7930 current loss 0.407883, current_train_items 253792.
I0302 19:02:07.976289 22732373614720 run.py:483] Algo bellman_ford step 7931 current loss 0.482270, current_train_items 253824.
I0302 19:02:07.998996 22732373614720 run.py:483] Algo bellman_ford step 7932 current loss 0.581168, current_train_items 253856.
I0302 19:02:08.030480 22732373614720 run.py:483] Algo bellman_ford step 7933 current loss 0.711560, current_train_items 253888.
I0302 19:02:08.064228 22732373614720 run.py:483] Algo bellman_ford step 7934 current loss 0.805426, current_train_items 253920.
I0302 19:02:08.083817 22732373614720 run.py:483] Algo bellman_ford step 7935 current loss 0.343181, current_train_items 253952.
I0302 19:02:08.099518 22732373614720 run.py:483] Algo bellman_ford step 7936 current loss 0.484810, current_train_items 253984.
I0302 19:02:08.123708 22732373614720 run.py:483] Algo bellman_ford step 7937 current loss 0.605556, current_train_items 254016.
I0302 19:02:08.154978 22732373614720 run.py:483] Algo bellman_ford step 7938 current loss 0.761918, current_train_items 254048.
I0302 19:02:08.188708 22732373614720 run.py:483] Algo bellman_ford step 7939 current loss 0.955819, current_train_items 254080.
I0302 19:02:08.208271 22732373614720 run.py:483] Algo bellman_ford step 7940 current loss 0.351470, current_train_items 254112.
I0302 19:02:08.223763 22732373614720 run.py:483] Algo bellman_ford step 7941 current loss 0.508396, current_train_items 254144.
I0302 19:02:08.248006 22732373614720 run.py:483] Algo bellman_ford step 7942 current loss 0.663257, current_train_items 254176.
I0302 19:02:08.280795 22732373614720 run.py:483] Algo bellman_ford step 7943 current loss 0.870782, current_train_items 254208.
I0302 19:02:08.313525 22732373614720 run.py:483] Algo bellman_ford step 7944 current loss 0.956798, current_train_items 254240.
I0302 19:02:08.333261 22732373614720 run.py:483] Algo bellman_ford step 7945 current loss 0.316804, current_train_items 254272.
I0302 19:02:08.348762 22732373614720 run.py:483] Algo bellman_ford step 7946 current loss 0.488500, current_train_items 254304.
I0302 19:02:08.371504 22732373614720 run.py:483] Algo bellman_ford step 7947 current loss 0.604376, current_train_items 254336.
I0302 19:02:08.401365 22732373614720 run.py:483] Algo bellman_ford step 7948 current loss 0.678162, current_train_items 254368.
I0302 19:02:08.435178 22732373614720 run.py:483] Algo bellman_ford step 7949 current loss 1.067024, current_train_items 254400.
I0302 19:02:08.454575 22732373614720 run.py:483] Algo bellman_ford step 7950 current loss 0.375740, current_train_items 254432.
I0302 19:02:08.462666 22732373614720 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.8798828125, 'score': 0.8798828125, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0302 19:02:08.462778 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.880, val scores are: bellman_ford: 0.880
I0302 19:02:08.479470 22732373614720 run.py:483] Algo bellman_ford step 7951 current loss 0.574282, current_train_items 254464.
I0302 19:02:08.503924 22732373614720 run.py:483] Algo bellman_ford step 7952 current loss 0.605513, current_train_items 254496.
I0302 19:02:08.536533 22732373614720 run.py:483] Algo bellman_ford step 7953 current loss 0.764249, current_train_items 254528.
I0302 19:02:08.569829 22732373614720 run.py:483] Algo bellman_ford step 7954 current loss 0.713676, current_train_items 254560.
I0302 19:02:08.590236 22732373614720 run.py:483] Algo bellman_ford step 7955 current loss 0.393969, current_train_items 254592.
I0302 19:02:08.606224 22732373614720 run.py:483] Algo bellman_ford step 7956 current loss 0.613076, current_train_items 254624.
I0302 19:02:08.630229 22732373614720 run.py:483] Algo bellman_ford step 7957 current loss 0.589943, current_train_items 254656.
I0302 19:02:08.661348 22732373614720 run.py:483] Algo bellman_ford step 7958 current loss 0.731860, current_train_items 254688.
I0302 19:02:08.694523 22732373614720 run.py:483] Algo bellman_ford step 7959 current loss 0.724789, current_train_items 254720.
I0302 19:02:08.714767 22732373614720 run.py:483] Algo bellman_ford step 7960 current loss 0.308451, current_train_items 254752.
I0302 19:02:08.731362 22732373614720 run.py:483] Algo bellman_ford step 7961 current loss 0.483442, current_train_items 254784.
I0302 19:02:08.754598 22732373614720 run.py:483] Algo bellman_ford step 7962 current loss 0.630305, current_train_items 254816.
I0302 19:02:08.785187 22732373614720 run.py:483] Algo bellman_ford step 7963 current loss 0.672404, current_train_items 254848.
I0302 19:02:08.819046 22732373614720 run.py:483] Algo bellman_ford step 7964 current loss 0.790009, current_train_items 254880.
I0302 19:02:08.838669 22732373614720 run.py:483] Algo bellman_ford step 7965 current loss 0.380517, current_train_items 254912.
I0302 19:02:08.855104 22732373614720 run.py:483] Algo bellman_ford step 7966 current loss 0.604520, current_train_items 254944.
I0302 19:02:08.879824 22732373614720 run.py:483] Algo bellman_ford step 7967 current loss 0.747947, current_train_items 254976.
I0302 19:02:08.911231 22732373614720 run.py:483] Algo bellman_ford step 7968 current loss 0.638849, current_train_items 255008.
I0302 19:02:08.943140 22732373614720 run.py:483] Algo bellman_ford step 7969 current loss 0.775474, current_train_items 255040.
I0302 19:02:08.963145 22732373614720 run.py:483] Algo bellman_ford step 7970 current loss 0.429807, current_train_items 255072.
I0302 19:02:08.979299 22732373614720 run.py:483] Algo bellman_ford step 7971 current loss 0.682797, current_train_items 255104.
I0302 19:02:09.003541 22732373614720 run.py:483] Algo bellman_ford step 7972 current loss 0.649853, current_train_items 255136.
I0302 19:02:09.034393 22732373614720 run.py:483] Algo bellman_ford step 7973 current loss 0.608887, current_train_items 255168.
I0302 19:02:09.069368 22732373614720 run.py:483] Algo bellman_ford step 7974 current loss 0.770334, current_train_items 255200.
I0302 19:02:09.089527 22732373614720 run.py:483] Algo bellman_ford step 7975 current loss 0.410505, current_train_items 255232.
I0302 19:02:09.105877 22732373614720 run.py:483] Algo bellman_ford step 7976 current loss 0.495423, current_train_items 255264.
I0302 19:02:09.129543 22732373614720 run.py:483] Algo bellman_ford step 7977 current loss 0.754441, current_train_items 255296.
I0302 19:02:09.160932 22732373614720 run.py:483] Algo bellman_ford step 7978 current loss 0.739778, current_train_items 255328.
I0302 19:02:09.192460 22732373614720 run.py:483] Algo bellman_ford step 7979 current loss 0.710908, current_train_items 255360.
I0302 19:02:09.212313 22732373614720 run.py:483] Algo bellman_ford step 7980 current loss 0.335772, current_train_items 255392.
I0302 19:02:09.228208 22732373614720 run.py:483] Algo bellman_ford step 7981 current loss 0.558425, current_train_items 255424.
I0302 19:02:09.251146 22732373614720 run.py:483] Algo bellman_ford step 7982 current loss 0.581071, current_train_items 255456.
I0302 19:02:09.281736 22732373614720 run.py:483] Algo bellman_ford step 7983 current loss 0.692478, current_train_items 255488.
I0302 19:02:09.315693 22732373614720 run.py:483] Algo bellman_ford step 7984 current loss 0.843239, current_train_items 255520.
I0302 19:02:09.335781 22732373614720 run.py:483] Algo bellman_ford step 7985 current loss 0.313294, current_train_items 255552.
I0302 19:02:09.352151 22732373614720 run.py:483] Algo bellman_ford step 7986 current loss 0.572698, current_train_items 255584.
I0302 19:02:09.375038 22732373614720 run.py:483] Algo bellman_ford step 7987 current loss 0.561382, current_train_items 255616.
I0302 19:02:09.406821 22732373614720 run.py:483] Algo bellman_ford step 7988 current loss 0.639321, current_train_items 255648.
I0302 19:02:09.439375 22732373614720 run.py:483] Algo bellman_ford step 7989 current loss 0.955864, current_train_items 255680.
I0302 19:02:09.459207 22732373614720 run.py:483] Algo bellman_ford step 7990 current loss 0.411550, current_train_items 255712.
I0302 19:02:09.475478 22732373614720 run.py:483] Algo bellman_ford step 7991 current loss 0.677482, current_train_items 255744.
I0302 19:02:09.498994 22732373614720 run.py:483] Algo bellman_ford step 7992 current loss 0.709373, current_train_items 255776.
I0302 19:02:09.532329 22732373614720 run.py:483] Algo bellman_ford step 7993 current loss 0.799978, current_train_items 255808.
I0302 19:02:09.566921 22732373614720 run.py:483] Algo bellman_ford step 7994 current loss 0.807306, current_train_items 255840.
I0302 19:02:09.586744 22732373614720 run.py:483] Algo bellman_ford step 7995 current loss 0.373499, current_train_items 255872.
I0302 19:02:09.602398 22732373614720 run.py:483] Algo bellman_ford step 7996 current loss 0.445886, current_train_items 255904.
I0302 19:02:09.625852 22732373614720 run.py:483] Algo bellman_ford step 7997 current loss 0.725321, current_train_items 255936.
I0302 19:02:09.656905 22732373614720 run.py:483] Algo bellman_ford step 7998 current loss 0.687499, current_train_items 255968.
I0302 19:02:09.688223 22732373614720 run.py:483] Algo bellman_ford step 7999 current loss 0.906444, current_train_items 256000.
I0302 19:02:09.708374 22732373614720 run.py:483] Algo bellman_ford step 8000 current loss 0.400149, current_train_items 256032.
I0302 19:02:09.716392 22732373614720 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.8740234375, 'score': 0.8740234375, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0302 19:02:09.716501 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.874, val scores are: bellman_ford: 0.874
I0302 19:02:09.733502 22732373614720 run.py:483] Algo bellman_ford step 8001 current loss 0.567557, current_train_items 256064.
I0302 19:02:09.756835 22732373614720 run.py:483] Algo bellman_ford step 8002 current loss 0.707750, current_train_items 256096.
I0302 19:02:09.789096 22732373614720 run.py:483] Algo bellman_ford step 8003 current loss 0.856308, current_train_items 256128.
I0302 19:02:09.821336 22732373614720 run.py:483] Algo bellman_ford step 8004 current loss 0.724018, current_train_items 256160.
I0302 19:02:09.841492 22732373614720 run.py:483] Algo bellman_ford step 8005 current loss 0.311297, current_train_items 256192.
I0302 19:02:09.857393 22732373614720 run.py:483] Algo bellman_ford step 8006 current loss 0.488838, current_train_items 256224.
I0302 19:02:09.881386 22732373614720 run.py:483] Algo bellman_ford step 8007 current loss 0.800569, current_train_items 256256.
I0302 19:02:09.913989 22732373614720 run.py:483] Algo bellman_ford step 8008 current loss 0.806123, current_train_items 256288.
I0302 19:02:09.947489 22732373614720 run.py:483] Algo bellman_ford step 8009 current loss 0.924453, current_train_items 256320.
I0302 19:02:09.966873 22732373614720 run.py:483] Algo bellman_ford step 8010 current loss 0.350604, current_train_items 256352.
I0302 19:02:09.982723 22732373614720 run.py:483] Algo bellman_ford step 8011 current loss 0.535879, current_train_items 256384.
I0302 19:02:10.007139 22732373614720 run.py:483] Algo bellman_ford step 8012 current loss 0.610275, current_train_items 256416.
I0302 19:02:10.038298 22732373614720 run.py:483] Algo bellman_ford step 8013 current loss 0.748874, current_train_items 256448.
I0302 19:02:10.069757 22732373614720 run.py:483] Algo bellman_ford step 8014 current loss 0.710310, current_train_items 256480.
I0302 19:02:10.089697 22732373614720 run.py:483] Algo bellman_ford step 8015 current loss 0.409107, current_train_items 256512.
I0302 19:02:10.106446 22732373614720 run.py:483] Algo bellman_ford step 8016 current loss 0.517888, current_train_items 256544.
I0302 19:02:10.129529 22732373614720 run.py:483] Algo bellman_ford step 8017 current loss 0.604230, current_train_items 256576.
I0302 19:02:10.160839 22732373614720 run.py:483] Algo bellman_ford step 8018 current loss 0.628707, current_train_items 256608.
I0302 19:02:10.192581 22732373614720 run.py:483] Algo bellman_ford step 8019 current loss 0.748704, current_train_items 256640.
I0302 19:02:10.212116 22732373614720 run.py:483] Algo bellman_ford step 8020 current loss 0.410741, current_train_items 256672.
I0302 19:02:10.228336 22732373614720 run.py:483] Algo bellman_ford step 8021 current loss 0.507577, current_train_items 256704.
I0302 19:02:10.251630 22732373614720 run.py:483] Algo bellman_ford step 8022 current loss 0.606687, current_train_items 256736.
I0302 19:02:10.282137 22732373614720 run.py:483] Algo bellman_ford step 8023 current loss 0.762632, current_train_items 256768.
I0302 19:02:10.316720 22732373614720 run.py:483] Algo bellman_ford step 8024 current loss 0.818148, current_train_items 256800.
I0302 19:02:10.336252 22732373614720 run.py:483] Algo bellman_ford step 8025 current loss 0.428047, current_train_items 256832.
I0302 19:02:10.352637 22732373614720 run.py:483] Algo bellman_ford step 8026 current loss 0.547411, current_train_items 256864.
I0302 19:02:10.376549 22732373614720 run.py:483] Algo bellman_ford step 8027 current loss 0.710628, current_train_items 256896.
I0302 19:02:10.406798 22732373614720 run.py:483] Algo bellman_ford step 8028 current loss 0.689470, current_train_items 256928.
I0302 19:02:10.439865 22732373614720 run.py:483] Algo bellman_ford step 8029 current loss 0.782970, current_train_items 256960.
I0302 19:02:10.459424 22732373614720 run.py:483] Algo bellman_ford step 8030 current loss 0.321904, current_train_items 256992.
I0302 19:02:10.475325 22732373614720 run.py:483] Algo bellman_ford step 8031 current loss 0.419932, current_train_items 257024.
I0302 19:02:10.499215 22732373614720 run.py:483] Algo bellman_ford step 8032 current loss 0.764962, current_train_items 257056.
I0302 19:02:10.530930 22732373614720 run.py:483] Algo bellman_ford step 8033 current loss 0.679718, current_train_items 257088.
I0302 19:02:10.565067 22732373614720 run.py:483] Algo bellman_ford step 8034 current loss 0.750531, current_train_items 257120.
I0302 19:02:10.584770 22732373614720 run.py:483] Algo bellman_ford step 8035 current loss 0.344951, current_train_items 257152.
I0302 19:02:10.601219 22732373614720 run.py:483] Algo bellman_ford step 8036 current loss 0.546675, current_train_items 257184.
I0302 19:02:10.625048 22732373614720 run.py:483] Algo bellman_ford step 8037 current loss 0.603676, current_train_items 257216.
I0302 19:02:10.655772 22732373614720 run.py:483] Algo bellman_ford step 8038 current loss 0.676243, current_train_items 257248.
I0302 19:02:10.690319 22732373614720 run.py:483] Algo bellman_ford step 8039 current loss 0.892400, current_train_items 257280.
I0302 19:02:10.709928 22732373614720 run.py:483] Algo bellman_ford step 8040 current loss 0.357919, current_train_items 257312.
I0302 19:02:10.726012 22732373614720 run.py:483] Algo bellman_ford step 8041 current loss 0.603637, current_train_items 257344.
I0302 19:02:10.749856 22732373614720 run.py:483] Algo bellman_ford step 8042 current loss 0.726982, current_train_items 257376.
I0302 19:02:10.781463 22732373614720 run.py:483] Algo bellman_ford step 8043 current loss 0.703895, current_train_items 257408.
I0302 19:02:10.814766 22732373614720 run.py:483] Algo bellman_ford step 8044 current loss 0.889584, current_train_items 257440.
I0302 19:02:10.834613 22732373614720 run.py:483] Algo bellman_ford step 8045 current loss 0.301995, current_train_items 257472.
I0302 19:02:10.851367 22732373614720 run.py:483] Algo bellman_ford step 8046 current loss 0.527800, current_train_items 257504.
I0302 19:02:10.874735 22732373614720 run.py:483] Algo bellman_ford step 8047 current loss 0.609875, current_train_items 257536.
I0302 19:02:10.906657 22732373614720 run.py:483] Algo bellman_ford step 8048 current loss 0.662868, current_train_items 257568.
I0302 19:02:10.938552 22732373614720 run.py:483] Algo bellman_ford step 8049 current loss 0.668679, current_train_items 257600.
I0302 19:02:10.958369 22732373614720 run.py:483] Algo bellman_ford step 8050 current loss 0.359887, current_train_items 257632.
I0302 19:02:10.966651 22732373614720 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0302 19:02:10.966761 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:02:10.983601 22732373614720 run.py:483] Algo bellman_ford step 8051 current loss 0.497354, current_train_items 257664.
I0302 19:02:11.007276 22732373614720 run.py:483] Algo bellman_ford step 8052 current loss 0.556209, current_train_items 257696.
I0302 19:02:11.038356 22732373614720 run.py:483] Algo bellman_ford step 8053 current loss 0.670153, current_train_items 257728.
I0302 19:02:11.072911 22732373614720 run.py:483] Algo bellman_ford step 8054 current loss 0.981335, current_train_items 257760.
I0302 19:02:11.092808 22732373614720 run.py:483] Algo bellman_ford step 8055 current loss 0.483915, current_train_items 257792.
I0302 19:02:11.107953 22732373614720 run.py:483] Algo bellman_ford step 8056 current loss 0.427781, current_train_items 257824.
I0302 19:02:11.130129 22732373614720 run.py:483] Algo bellman_ford step 8057 current loss 0.507615, current_train_items 257856.
I0302 19:02:11.162474 22732373614720 run.py:483] Algo bellman_ford step 8058 current loss 0.760695, current_train_items 257888.
I0302 19:02:11.197664 22732373614720 run.py:483] Algo bellman_ford step 8059 current loss 0.804567, current_train_items 257920.
I0302 19:02:11.217576 22732373614720 run.py:483] Algo bellman_ford step 8060 current loss 0.354314, current_train_items 257952.
I0302 19:02:11.233492 22732373614720 run.py:483] Algo bellman_ford step 8061 current loss 0.542817, current_train_items 257984.
I0302 19:02:11.258013 22732373614720 run.py:483] Algo bellman_ford step 8062 current loss 0.693799, current_train_items 258016.
I0302 19:02:11.289309 22732373614720 run.py:483] Algo bellman_ford step 8063 current loss 0.664515, current_train_items 258048.
I0302 19:02:11.321418 22732373614720 run.py:483] Algo bellman_ford step 8064 current loss 0.880343, current_train_items 258080.
I0302 19:02:11.341148 22732373614720 run.py:483] Algo bellman_ford step 8065 current loss 0.334211, current_train_items 258112.
I0302 19:02:11.356956 22732373614720 run.py:483] Algo bellman_ford step 8066 current loss 0.545492, current_train_items 258144.
I0302 19:02:11.380347 22732373614720 run.py:483] Algo bellman_ford step 8067 current loss 0.647713, current_train_items 258176.
I0302 19:02:11.411338 22732373614720 run.py:483] Algo bellman_ford step 8068 current loss 0.723903, current_train_items 258208.
I0302 19:02:11.443202 22732373614720 run.py:483] Algo bellman_ford step 8069 current loss 0.819916, current_train_items 258240.
I0302 19:02:11.463077 22732373614720 run.py:483] Algo bellman_ford step 8070 current loss 0.386270, current_train_items 258272.
I0302 19:02:11.480044 22732373614720 run.py:483] Algo bellman_ford step 8071 current loss 0.571234, current_train_items 258304.
I0302 19:02:11.503356 22732373614720 run.py:483] Algo bellman_ford step 8072 current loss 0.732704, current_train_items 258336.
I0302 19:02:11.534781 22732373614720 run.py:483] Algo bellman_ford step 8073 current loss 0.728933, current_train_items 258368.
I0302 19:02:11.568542 22732373614720 run.py:483] Algo bellman_ford step 8074 current loss 0.901640, current_train_items 258400.
I0302 19:02:11.588247 22732373614720 run.py:483] Algo bellman_ford step 8075 current loss 0.344483, current_train_items 258432.
I0302 19:02:11.604406 22732373614720 run.py:483] Algo bellman_ford step 8076 current loss 0.484117, current_train_items 258464.
I0302 19:02:11.627339 22732373614720 run.py:483] Algo bellman_ford step 8077 current loss 0.551001, current_train_items 258496.
I0302 19:02:11.658356 22732373614720 run.py:483] Algo bellman_ford step 8078 current loss 0.796595, current_train_items 258528.
I0302 19:02:11.689772 22732373614720 run.py:483] Algo bellman_ford step 8079 current loss 0.775496, current_train_items 258560.
I0302 19:02:11.709244 22732373614720 run.py:483] Algo bellman_ford step 8080 current loss 0.426108, current_train_items 258592.
I0302 19:02:11.725416 22732373614720 run.py:483] Algo bellman_ford step 8081 current loss 0.528533, current_train_items 258624.
I0302 19:02:11.748854 22732373614720 run.py:483] Algo bellman_ford step 8082 current loss 0.647078, current_train_items 258656.
I0302 19:02:11.780006 22732373614720 run.py:483] Algo bellman_ford step 8083 current loss 0.766681, current_train_items 258688.
I0302 19:02:11.816137 22732373614720 run.py:483] Algo bellman_ford step 8084 current loss 0.882082, current_train_items 258720.
I0302 19:02:11.836000 22732373614720 run.py:483] Algo bellman_ford step 8085 current loss 0.396825, current_train_items 258752.
I0302 19:02:11.852542 22732373614720 run.py:483] Algo bellman_ford step 8086 current loss 0.526667, current_train_items 258784.
I0302 19:02:11.875586 22732373614720 run.py:483] Algo bellman_ford step 8087 current loss 0.665039, current_train_items 258816.
I0302 19:02:11.908198 22732373614720 run.py:483] Algo bellman_ford step 8088 current loss 0.717591, current_train_items 258848.
I0302 19:02:11.939740 22732373614720 run.py:483] Algo bellman_ford step 8089 current loss 0.782582, current_train_items 258880.
I0302 19:02:11.959574 22732373614720 run.py:483] Algo bellman_ford step 8090 current loss 0.380383, current_train_items 258912.
I0302 19:02:11.975201 22732373614720 run.py:483] Algo bellman_ford step 8091 current loss 0.434268, current_train_items 258944.
I0302 19:02:11.998786 22732373614720 run.py:483] Algo bellman_ford step 8092 current loss 0.635755, current_train_items 258976.
I0302 19:02:12.029771 22732373614720 run.py:483] Algo bellman_ford step 8093 current loss 0.688842, current_train_items 259008.
I0302 19:02:12.063745 22732373614720 run.py:483] Algo bellman_ford step 8094 current loss 0.840649, current_train_items 259040.
I0302 19:02:12.083503 22732373614720 run.py:483] Algo bellman_ford step 8095 current loss 0.387825, current_train_items 259072.
I0302 19:02:12.099380 22732373614720 run.py:483] Algo bellman_ford step 8096 current loss 0.524515, current_train_items 259104.
I0302 19:02:12.123846 22732373614720 run.py:483] Algo bellman_ford step 8097 current loss 0.644413, current_train_items 259136.
I0302 19:02:12.155840 22732373614720 run.py:483] Algo bellman_ford step 8098 current loss 0.735578, current_train_items 259168.
I0302 19:02:12.188746 22732373614720 run.py:483] Algo bellman_ford step 8099 current loss 0.816843, current_train_items 259200.
I0302 19:02:12.208262 22732373614720 run.py:483] Algo bellman_ford step 8100 current loss 0.327074, current_train_items 259232.
I0302 19:02:12.216295 22732373614720 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.8818359375, 'score': 0.8818359375, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0302 19:02:12.216406 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.882, val scores are: bellman_ford: 0.882
I0302 19:02:12.233280 22732373614720 run.py:483] Algo bellman_ford step 8101 current loss 0.470244, current_train_items 259264.
I0302 19:02:12.258498 22732373614720 run.py:483] Algo bellman_ford step 8102 current loss 0.856941, current_train_items 259296.
I0302 19:02:12.289978 22732373614720 run.py:483] Algo bellman_ford step 8103 current loss 0.675897, current_train_items 259328.
I0302 19:02:12.323773 22732373614720 run.py:483] Algo bellman_ford step 8104 current loss 0.936446, current_train_items 259360.
I0302 19:02:12.344136 22732373614720 run.py:483] Algo bellman_ford step 8105 current loss 0.319318, current_train_items 259392.
I0302 19:02:12.360087 22732373614720 run.py:483] Algo bellman_ford step 8106 current loss 0.507431, current_train_items 259424.
I0302 19:02:12.383244 22732373614720 run.py:483] Algo bellman_ford step 8107 current loss 0.722957, current_train_items 259456.
I0302 19:02:12.413845 22732373614720 run.py:483] Algo bellman_ford step 8108 current loss 0.902318, current_train_items 259488.
I0302 19:02:12.447726 22732373614720 run.py:483] Algo bellman_ford step 8109 current loss 0.810053, current_train_items 259520.
I0302 19:02:12.467718 22732373614720 run.py:483] Algo bellman_ford step 8110 current loss 0.349608, current_train_items 259552.
I0302 19:02:12.483357 22732373614720 run.py:483] Algo bellman_ford step 8111 current loss 0.601213, current_train_items 259584.
I0302 19:02:12.506781 22732373614720 run.py:483] Algo bellman_ford step 8112 current loss 0.622923, current_train_items 259616.
I0302 19:02:12.536873 22732373614720 run.py:483] Algo bellman_ford step 8113 current loss 0.652766, current_train_items 259648.
I0302 19:02:12.572247 22732373614720 run.py:483] Algo bellman_ford step 8114 current loss 0.789808, current_train_items 259680.
I0302 19:02:12.591974 22732373614720 run.py:483] Algo bellman_ford step 8115 current loss 0.375250, current_train_items 259712.
I0302 19:02:12.608165 22732373614720 run.py:483] Algo bellman_ford step 8116 current loss 0.487532, current_train_items 259744.
I0302 19:02:12.632103 22732373614720 run.py:483] Algo bellman_ford step 8117 current loss 0.651190, current_train_items 259776.
I0302 19:02:12.663997 22732373614720 run.py:483] Algo bellman_ford step 8118 current loss 0.743521, current_train_items 259808.
I0302 19:02:12.696952 22732373614720 run.py:483] Algo bellman_ford step 8119 current loss 0.818049, current_train_items 259840.
I0302 19:02:12.716705 22732373614720 run.py:483] Algo bellman_ford step 8120 current loss 0.345459, current_train_items 259872.
I0302 19:02:12.732892 22732373614720 run.py:483] Algo bellman_ford step 8121 current loss 0.502780, current_train_items 259904.
I0302 19:02:12.756589 22732373614720 run.py:483] Algo bellman_ford step 8122 current loss 0.685188, current_train_items 259936.
I0302 19:02:12.789029 22732373614720 run.py:483] Algo bellman_ford step 8123 current loss 0.724221, current_train_items 259968.
I0302 19:02:12.820549 22732373614720 run.py:483] Algo bellman_ford step 8124 current loss 0.771128, current_train_items 260000.
I0302 19:02:12.840174 22732373614720 run.py:483] Algo bellman_ford step 8125 current loss 0.319658, current_train_items 260032.
I0302 19:02:12.856546 22732373614720 run.py:483] Algo bellman_ford step 8126 current loss 0.783086, current_train_items 260064.
I0302 19:02:12.880920 22732373614720 run.py:483] Algo bellman_ford step 8127 current loss 0.751188, current_train_items 260096.
I0302 19:02:12.912877 22732373614720 run.py:483] Algo bellman_ford step 8128 current loss 0.781163, current_train_items 260128.
I0302 19:02:12.948623 22732373614720 run.py:483] Algo bellman_ford step 8129 current loss 1.035617, current_train_items 260160.
I0302 19:02:12.968590 22732373614720 run.py:483] Algo bellman_ford step 8130 current loss 0.339222, current_train_items 260192.
I0302 19:02:12.984417 22732373614720 run.py:483] Algo bellman_ford step 8131 current loss 0.458617, current_train_items 260224.
I0302 19:02:13.008222 22732373614720 run.py:483] Algo bellman_ford step 8132 current loss 0.735574, current_train_items 260256.
I0302 19:02:13.039506 22732373614720 run.py:483] Algo bellman_ford step 8133 current loss 0.731460, current_train_items 260288.
I0302 19:02:13.072892 22732373614720 run.py:483] Algo bellman_ford step 8134 current loss 0.960692, current_train_items 260320.
I0302 19:02:13.092387 22732373614720 run.py:483] Algo bellman_ford step 8135 current loss 0.361236, current_train_items 260352.
I0302 19:02:13.108013 22732373614720 run.py:483] Algo bellman_ford step 8136 current loss 0.530126, current_train_items 260384.
I0302 19:02:13.131385 22732373614720 run.py:483] Algo bellman_ford step 8137 current loss 0.668854, current_train_items 260416.
I0302 19:02:13.162807 22732373614720 run.py:483] Algo bellman_ford step 8138 current loss 0.690889, current_train_items 260448.
I0302 19:02:13.197721 22732373614720 run.py:483] Algo bellman_ford step 8139 current loss 1.012713, current_train_items 260480.
I0302 19:02:13.217563 22732373614720 run.py:483] Algo bellman_ford step 8140 current loss 0.398539, current_train_items 260512.
I0302 19:02:13.233392 22732373614720 run.py:483] Algo bellman_ford step 8141 current loss 0.636178, current_train_items 260544.
I0302 19:02:13.257486 22732373614720 run.py:483] Algo bellman_ford step 8142 current loss 0.671144, current_train_items 260576.
I0302 19:02:13.289264 22732373614720 run.py:483] Algo bellman_ford step 8143 current loss 0.741129, current_train_items 260608.
I0302 19:02:13.323209 22732373614720 run.py:483] Algo bellman_ford step 8144 current loss 0.856718, current_train_items 260640.
I0302 19:02:13.342800 22732373614720 run.py:483] Algo bellman_ford step 8145 current loss 0.453375, current_train_items 260672.
I0302 19:02:13.359005 22732373614720 run.py:483] Algo bellman_ford step 8146 current loss 0.516950, current_train_items 260704.
I0302 19:02:13.382602 22732373614720 run.py:483] Algo bellman_ford step 8147 current loss 0.610987, current_train_items 260736.
I0302 19:02:13.413915 22732373614720 run.py:483] Algo bellman_ford step 8148 current loss 0.803394, current_train_items 260768.
I0302 19:02:13.446647 22732373614720 run.py:483] Algo bellman_ford step 8149 current loss 0.902733, current_train_items 260800.
I0302 19:02:13.466227 22732373614720 run.py:483] Algo bellman_ford step 8150 current loss 0.398219, current_train_items 260832.
I0302 19:02:13.474086 22732373614720 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.935546875, 'score': 0.935546875, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0302 19:02:13.474201 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.936, val scores are: bellman_ford: 0.936
I0302 19:02:13.490773 22732373614720 run.py:483] Algo bellman_ford step 8151 current loss 0.443246, current_train_items 260864.
I0302 19:02:13.514710 22732373614720 run.py:483] Algo bellman_ford step 8152 current loss 0.703605, current_train_items 260896.
I0302 19:02:13.545537 22732373614720 run.py:483] Algo bellman_ford step 8153 current loss 0.786187, current_train_items 260928.
I0302 19:02:13.580497 22732373614720 run.py:483] Algo bellman_ford step 8154 current loss 0.914631, current_train_items 260960.
I0302 19:02:13.600401 22732373614720 run.py:483] Algo bellman_ford step 8155 current loss 0.331540, current_train_items 260992.
I0302 19:02:13.615654 22732373614720 run.py:483] Algo bellman_ford step 8156 current loss 0.434215, current_train_items 261024.
I0302 19:02:13.639259 22732373614720 run.py:483] Algo bellman_ford step 8157 current loss 0.604522, current_train_items 261056.
I0302 19:02:13.670016 22732373614720 run.py:483] Algo bellman_ford step 8158 current loss 0.745742, current_train_items 261088.
I0302 19:02:13.702766 22732373614720 run.py:483] Algo bellman_ford step 8159 current loss 0.875691, current_train_items 261120.
I0302 19:02:13.722379 22732373614720 run.py:483] Algo bellman_ford step 8160 current loss 0.361786, current_train_items 261152.
I0302 19:02:13.738533 22732373614720 run.py:483] Algo bellman_ford step 8161 current loss 0.533615, current_train_items 261184.
I0302 19:02:13.761377 22732373614720 run.py:483] Algo bellman_ford step 8162 current loss 0.803814, current_train_items 261216.
I0302 19:02:13.792776 22732373614720 run.py:483] Algo bellman_ford step 8163 current loss 0.804650, current_train_items 261248.
I0302 19:02:13.822815 22732373614720 run.py:483] Algo bellman_ford step 8164 current loss 0.662011, current_train_items 261280.
I0302 19:02:13.842107 22732373614720 run.py:483] Algo bellman_ford step 8165 current loss 0.281911, current_train_items 261312.
I0302 19:02:13.858192 22732373614720 run.py:483] Algo bellman_ford step 8166 current loss 0.581125, current_train_items 261344.
I0302 19:02:13.881547 22732373614720 run.py:483] Algo bellman_ford step 8167 current loss 0.707749, current_train_items 261376.
I0302 19:02:13.912687 22732373614720 run.py:483] Algo bellman_ford step 8168 current loss 0.690065, current_train_items 261408.
I0302 19:02:13.946793 22732373614720 run.py:483] Algo bellman_ford step 8169 current loss 0.853712, current_train_items 261440.
I0302 19:02:13.966486 22732373614720 run.py:483] Algo bellman_ford step 8170 current loss 0.345936, current_train_items 261472.
I0302 19:02:13.982707 22732373614720 run.py:483] Algo bellman_ford step 8171 current loss 0.479418, current_train_items 261504.
I0302 19:02:14.006567 22732373614720 run.py:483] Algo bellman_ford step 8172 current loss 0.696798, current_train_items 261536.
I0302 19:02:14.038124 22732373614720 run.py:483] Algo bellman_ford step 8173 current loss 0.678599, current_train_items 261568.
I0302 19:02:14.070206 22732373614720 run.py:483] Algo bellman_ford step 8174 current loss 0.799365, current_train_items 261600.
I0302 19:02:14.089949 22732373614720 run.py:483] Algo bellman_ford step 8175 current loss 0.353107, current_train_items 261632.
I0302 19:02:14.106510 22732373614720 run.py:483] Algo bellman_ford step 8176 current loss 0.543011, current_train_items 261664.
I0302 19:02:14.129628 22732373614720 run.py:483] Algo bellman_ford step 8177 current loss 0.583493, current_train_items 261696.
I0302 19:02:14.161607 22732373614720 run.py:483] Algo bellman_ford step 8178 current loss 0.709441, current_train_items 261728.
I0302 19:02:14.192122 22732373614720 run.py:483] Algo bellman_ford step 8179 current loss 0.656026, current_train_items 261760.
I0302 19:02:14.211556 22732373614720 run.py:483] Algo bellman_ford step 8180 current loss 0.338335, current_train_items 261792.
I0302 19:02:14.227618 22732373614720 run.py:483] Algo bellman_ford step 8181 current loss 0.443686, current_train_items 261824.
I0302 19:02:14.250823 22732373614720 run.py:483] Algo bellman_ford step 8182 current loss 0.587837, current_train_items 261856.
I0302 19:02:14.282456 22732373614720 run.py:483] Algo bellman_ford step 8183 current loss 0.731466, current_train_items 261888.
I0302 19:02:14.317460 22732373614720 run.py:483] Algo bellman_ford step 8184 current loss 1.055060, current_train_items 261920.
I0302 19:02:14.337385 22732373614720 run.py:483] Algo bellman_ford step 8185 current loss 0.388166, current_train_items 261952.
I0302 19:02:14.353392 22732373614720 run.py:483] Algo bellman_ford step 8186 current loss 0.571249, current_train_items 261984.
I0302 19:02:14.376777 22732373614720 run.py:483] Algo bellman_ford step 8187 current loss 0.702630, current_train_items 262016.
I0302 19:02:14.407998 22732373614720 run.py:483] Algo bellman_ford step 8188 current loss 0.633207, current_train_items 262048.
I0302 19:02:14.440478 22732373614720 run.py:483] Algo bellman_ford step 8189 current loss 0.863190, current_train_items 262080.
I0302 19:02:14.460350 22732373614720 run.py:483] Algo bellman_ford step 8190 current loss 0.364622, current_train_items 262112.
I0302 19:02:14.476842 22732373614720 run.py:483] Algo bellman_ford step 8191 current loss 0.581738, current_train_items 262144.
I0302 19:02:14.500456 22732373614720 run.py:483] Algo bellman_ford step 8192 current loss 0.735527, current_train_items 262176.
I0302 19:02:14.531413 22732373614720 run.py:483] Algo bellman_ford step 8193 current loss 0.734299, current_train_items 262208.
I0302 19:02:14.564248 22732373614720 run.py:483] Algo bellman_ford step 8194 current loss 0.849875, current_train_items 262240.
I0302 19:02:14.583702 22732373614720 run.py:483] Algo bellman_ford step 8195 current loss 0.370914, current_train_items 262272.
I0302 19:02:14.599808 22732373614720 run.py:483] Algo bellman_ford step 8196 current loss 0.435898, current_train_items 262304.
I0302 19:02:14.622479 22732373614720 run.py:483] Algo bellman_ford step 8197 current loss 0.633664, current_train_items 262336.
I0302 19:02:14.653557 22732373614720 run.py:483] Algo bellman_ford step 8198 current loss 0.725403, current_train_items 262368.
I0302 19:02:14.686183 22732373614720 run.py:483] Algo bellman_ford step 8199 current loss 0.834253, current_train_items 262400.
I0302 19:02:14.706107 22732373614720 run.py:483] Algo bellman_ford step 8200 current loss 0.312932, current_train_items 262432.
I0302 19:02:14.714143 22732373614720 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0302 19:02:14.714261 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:02:14.730644 22732373614720 run.py:483] Algo bellman_ford step 8201 current loss 0.401321, current_train_items 262464.
I0302 19:02:14.755080 22732373614720 run.py:483] Algo bellman_ford step 8202 current loss 0.587646, current_train_items 262496.
I0302 19:02:14.786367 22732373614720 run.py:483] Algo bellman_ford step 8203 current loss 0.726245, current_train_items 262528.
I0302 19:02:14.820846 22732373614720 run.py:483] Algo bellman_ford step 8204 current loss 0.849088, current_train_items 262560.
I0302 19:02:14.840873 22732373614720 run.py:483] Algo bellman_ford step 8205 current loss 0.314840, current_train_items 262592.
I0302 19:02:14.857131 22732373614720 run.py:483] Algo bellman_ford step 8206 current loss 0.443288, current_train_items 262624.
I0302 19:02:14.881458 22732373614720 run.py:483] Algo bellman_ford step 8207 current loss 0.651688, current_train_items 262656.
I0302 19:02:14.912758 22732373614720 run.py:483] Algo bellman_ford step 8208 current loss 0.679071, current_train_items 262688.
I0302 19:02:14.943470 22732373614720 run.py:483] Algo bellman_ford step 8209 current loss 0.741257, current_train_items 262720.
I0302 19:02:14.963059 22732373614720 run.py:483] Algo bellman_ford step 8210 current loss 0.316247, current_train_items 262752.
I0302 19:02:14.978904 22732373614720 run.py:483] Algo bellman_ford step 8211 current loss 0.439144, current_train_items 262784.
I0302 19:02:15.001736 22732373614720 run.py:483] Algo bellman_ford step 8212 current loss 0.609064, current_train_items 262816.
I0302 19:02:15.032704 22732373614720 run.py:483] Algo bellman_ford step 8213 current loss 0.715843, current_train_items 262848.
I0302 19:02:15.065699 22732373614720 run.py:483] Algo bellman_ford step 8214 current loss 0.960613, current_train_items 262880.
I0302 19:02:15.085187 22732373614720 run.py:483] Algo bellman_ford step 8215 current loss 0.324970, current_train_items 262912.
I0302 19:02:15.100728 22732373614720 run.py:483] Algo bellman_ford step 8216 current loss 0.505600, current_train_items 262944.
I0302 19:02:15.123404 22732373614720 run.py:483] Algo bellman_ford step 8217 current loss 0.680975, current_train_items 262976.
I0302 19:02:15.155814 22732373614720 run.py:483] Algo bellman_ford step 8218 current loss 0.683969, current_train_items 263008.
I0302 19:02:15.190057 22732373614720 run.py:483] Algo bellman_ford step 8219 current loss 0.810425, current_train_items 263040.
I0302 19:02:15.209474 22732373614720 run.py:483] Algo bellman_ford step 8220 current loss 0.299575, current_train_items 263072.
I0302 19:02:15.225302 22732373614720 run.py:483] Algo bellman_ford step 8221 current loss 0.529667, current_train_items 263104.
I0302 19:02:15.249492 22732373614720 run.py:483] Algo bellman_ford step 8222 current loss 0.743112, current_train_items 263136.
I0302 19:02:15.281399 22732373614720 run.py:483] Algo bellman_ford step 8223 current loss 0.662412, current_train_items 263168.
I0302 19:02:15.312218 22732373614720 run.py:483] Algo bellman_ford step 8224 current loss 0.676093, current_train_items 263200.
I0302 19:02:15.332137 22732373614720 run.py:483] Algo bellman_ford step 8225 current loss 0.301470, current_train_items 263232.
I0302 19:02:15.348375 22732373614720 run.py:483] Algo bellman_ford step 8226 current loss 0.478214, current_train_items 263264.
I0302 19:02:15.372648 22732373614720 run.py:483] Algo bellman_ford step 8227 current loss 0.713159, current_train_items 263296.
I0302 19:02:15.402869 22732373614720 run.py:483] Algo bellman_ford step 8228 current loss 0.659318, current_train_items 263328.
I0302 19:02:15.436138 22732373614720 run.py:483] Algo bellman_ford step 8229 current loss 0.807453, current_train_items 263360.
I0302 19:02:15.455948 22732373614720 run.py:483] Algo bellman_ford step 8230 current loss 0.316287, current_train_items 263392.
I0302 19:02:15.471995 22732373614720 run.py:483] Algo bellman_ford step 8231 current loss 0.614001, current_train_items 263424.
I0302 19:02:15.496464 22732373614720 run.py:483] Algo bellman_ford step 8232 current loss 0.654082, current_train_items 263456.
I0302 19:02:15.527744 22732373614720 run.py:483] Algo bellman_ford step 8233 current loss 0.660026, current_train_items 263488.
I0302 19:02:15.560418 22732373614720 run.py:483] Algo bellman_ford step 8234 current loss 0.780453, current_train_items 263520.
I0302 19:02:15.579994 22732373614720 run.py:483] Algo bellman_ford step 8235 current loss 0.373192, current_train_items 263552.
I0302 19:02:15.596406 22732373614720 run.py:483] Algo bellman_ford step 8236 current loss 0.523006, current_train_items 263584.
I0302 19:02:15.620822 22732373614720 run.py:483] Algo bellman_ford step 8237 current loss 0.772848, current_train_items 263616.
I0302 19:02:15.652753 22732373614720 run.py:483] Algo bellman_ford step 8238 current loss 0.738949, current_train_items 263648.
I0302 19:02:15.685502 22732373614720 run.py:483] Algo bellman_ford step 8239 current loss 0.844930, current_train_items 263680.
I0302 19:02:15.705224 22732373614720 run.py:483] Algo bellman_ford step 8240 current loss 0.377526, current_train_items 263712.
I0302 19:02:15.721320 22732373614720 run.py:483] Algo bellman_ford step 8241 current loss 0.474604, current_train_items 263744.
I0302 19:02:15.744244 22732373614720 run.py:483] Algo bellman_ford step 8242 current loss 0.610583, current_train_items 263776.
I0302 19:02:15.775348 22732373614720 run.py:483] Algo bellman_ford step 8243 current loss 0.635734, current_train_items 263808.
I0302 19:02:15.810269 22732373614720 run.py:483] Algo bellman_ford step 8244 current loss 0.830857, current_train_items 263840.
I0302 19:02:15.830034 22732373614720 run.py:483] Algo bellman_ford step 8245 current loss 0.334559, current_train_items 263872.
I0302 19:02:15.846240 22732373614720 run.py:483] Algo bellman_ford step 8246 current loss 0.522971, current_train_items 263904.
I0302 19:02:15.868747 22732373614720 run.py:483] Algo bellman_ford step 8247 current loss 0.566743, current_train_items 263936.
I0302 19:02:15.899425 22732373614720 run.py:483] Algo bellman_ford step 8248 current loss 0.627232, current_train_items 263968.
I0302 19:02:15.932763 22732373614720 run.py:483] Algo bellman_ford step 8249 current loss 0.837197, current_train_items 264000.
I0302 19:02:15.952244 22732373614720 run.py:483] Algo bellman_ford step 8250 current loss 0.246830, current_train_items 264032.
I0302 19:02:15.960610 22732373614720 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0302 19:02:15.960722 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:02:15.977485 22732373614720 run.py:483] Algo bellman_ford step 8251 current loss 0.554810, current_train_items 264064.
I0302 19:02:16.001353 22732373614720 run.py:483] Algo bellman_ford step 8252 current loss 0.721249, current_train_items 264096.
I0302 19:02:16.033989 22732373614720 run.py:483] Algo bellman_ford step 8253 current loss 0.858461, current_train_items 264128.
I0302 19:02:16.067223 22732373614720 run.py:483] Algo bellman_ford step 8254 current loss 0.722087, current_train_items 264160.
I0302 19:02:16.087054 22732373614720 run.py:483] Algo bellman_ford step 8255 current loss 0.452076, current_train_items 264192.
I0302 19:02:16.103255 22732373614720 run.py:483] Algo bellman_ford step 8256 current loss 0.523518, current_train_items 264224.
I0302 19:02:16.127070 22732373614720 run.py:483] Algo bellman_ford step 8257 current loss 0.671601, current_train_items 264256.
I0302 19:02:16.158731 22732373614720 run.py:483] Algo bellman_ford step 8258 current loss 0.703617, current_train_items 264288.
I0302 19:02:16.193418 22732373614720 run.py:483] Algo bellman_ford step 8259 current loss 0.879180, current_train_items 264320.
I0302 19:02:16.213044 22732373614720 run.py:483] Algo bellman_ford step 8260 current loss 0.376535, current_train_items 264352.
I0302 19:02:16.229277 22732373614720 run.py:483] Algo bellman_ford step 8261 current loss 0.496240, current_train_items 264384.
I0302 19:02:16.253196 22732373614720 run.py:483] Algo bellman_ford step 8262 current loss 0.666949, current_train_items 264416.
I0302 19:02:16.285109 22732373614720 run.py:483] Algo bellman_ford step 8263 current loss 0.730453, current_train_items 264448.
I0302 19:02:16.319336 22732373614720 run.py:483] Algo bellman_ford step 8264 current loss 0.918761, current_train_items 264480.
I0302 19:02:16.338790 22732373614720 run.py:483] Algo bellman_ford step 8265 current loss 0.296378, current_train_items 264512.
I0302 19:02:16.354402 22732373614720 run.py:483] Algo bellman_ford step 8266 current loss 0.562719, current_train_items 264544.
I0302 19:02:16.377648 22732373614720 run.py:483] Algo bellman_ford step 8267 current loss 0.658729, current_train_items 264576.
I0302 19:02:16.409371 22732373614720 run.py:483] Algo bellman_ford step 8268 current loss 0.836931, current_train_items 264608.
I0302 19:02:16.441373 22732373614720 run.py:483] Algo bellman_ford step 8269 current loss 0.790980, current_train_items 264640.
I0302 19:02:16.461274 22732373614720 run.py:483] Algo bellman_ford step 8270 current loss 0.303880, current_train_items 264672.
I0302 19:02:16.476766 22732373614720 run.py:483] Algo bellman_ford step 8271 current loss 0.387820, current_train_items 264704.
I0302 19:02:16.499711 22732373614720 run.py:483] Algo bellman_ford step 8272 current loss 0.659018, current_train_items 264736.
I0302 19:02:16.530330 22732373614720 run.py:483] Algo bellman_ford step 8273 current loss 0.720337, current_train_items 264768.
I0302 19:02:16.564389 22732373614720 run.py:483] Algo bellman_ford step 8274 current loss 0.843791, current_train_items 264800.
I0302 19:02:16.584383 22732373614720 run.py:483] Algo bellman_ford step 8275 current loss 0.347415, current_train_items 264832.
I0302 19:02:16.601058 22732373614720 run.py:483] Algo bellman_ford step 8276 current loss 0.549417, current_train_items 264864.
I0302 19:02:16.624881 22732373614720 run.py:483] Algo bellman_ford step 8277 current loss 0.859454, current_train_items 264896.
I0302 19:02:16.655950 22732373614720 run.py:483] Algo bellman_ford step 8278 current loss 0.723656, current_train_items 264928.
I0302 19:02:16.688400 22732373614720 run.py:483] Algo bellman_ford step 8279 current loss 0.786246, current_train_items 264960.
I0302 19:02:16.708055 22732373614720 run.py:483] Algo bellman_ford step 8280 current loss 0.338932, current_train_items 264992.
I0302 19:02:16.724530 22732373614720 run.py:483] Algo bellman_ford step 8281 current loss 0.575142, current_train_items 265024.
I0302 19:02:16.747728 22732373614720 run.py:483] Algo bellman_ford step 8282 current loss 0.776820, current_train_items 265056.
I0302 19:02:16.778423 22732373614720 run.py:483] Algo bellman_ford step 8283 current loss 0.752826, current_train_items 265088.
I0302 19:02:16.810454 22732373614720 run.py:483] Algo bellman_ford step 8284 current loss 0.859605, current_train_items 265120.
I0302 19:02:16.830188 22732373614720 run.py:483] Algo bellman_ford step 8285 current loss 0.331366, current_train_items 265152.
I0302 19:02:16.846532 22732373614720 run.py:483] Algo bellman_ford step 8286 current loss 0.492955, current_train_items 265184.
I0302 19:02:16.869604 22732373614720 run.py:483] Algo bellman_ford step 8287 current loss 0.582012, current_train_items 265216.
I0302 19:02:16.902240 22732373614720 run.py:483] Algo bellman_ford step 8288 current loss 0.752948, current_train_items 265248.
I0302 19:02:16.935658 22732373614720 run.py:483] Algo bellman_ford step 8289 current loss 0.754704, current_train_items 265280.
I0302 19:02:16.955087 22732373614720 run.py:483] Algo bellman_ford step 8290 current loss 0.379690, current_train_items 265312.
I0302 19:02:16.971295 22732373614720 run.py:483] Algo bellman_ford step 8291 current loss 0.535192, current_train_items 265344.
I0302 19:02:16.995176 22732373614720 run.py:483] Algo bellman_ford step 8292 current loss 0.705472, current_train_items 265376.
I0302 19:02:17.025386 22732373614720 run.py:483] Algo bellman_ford step 8293 current loss 0.721647, current_train_items 265408.
I0302 19:02:17.060134 22732373614720 run.py:483] Algo bellman_ford step 8294 current loss 1.019647, current_train_items 265440.
I0302 19:02:17.079780 22732373614720 run.py:483] Algo bellman_ford step 8295 current loss 0.330911, current_train_items 265472.
I0302 19:02:17.096014 22732373614720 run.py:483] Algo bellman_ford step 8296 current loss 0.566050, current_train_items 265504.
I0302 19:02:17.120508 22732373614720 run.py:483] Algo bellman_ford step 8297 current loss 0.769500, current_train_items 265536.
I0302 19:02:17.151728 22732373614720 run.py:483] Algo bellman_ford step 8298 current loss 0.823644, current_train_items 265568.
I0302 19:02:17.184075 22732373614720 run.py:483] Algo bellman_ford step 8299 current loss 1.274448, current_train_items 265600.
I0302 19:02:17.203798 22732373614720 run.py:483] Algo bellman_ford step 8300 current loss 0.297641, current_train_items 265632.
I0302 19:02:17.211715 22732373614720 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0302 19:02:17.211824 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:02:17.228219 22732373614720 run.py:483] Algo bellman_ford step 8301 current loss 0.612076, current_train_items 265664.
I0302 19:02:17.253619 22732373614720 run.py:483] Algo bellman_ford step 8302 current loss 0.843947, current_train_items 265696.
I0302 19:02:17.285564 22732373614720 run.py:483] Algo bellman_ford step 8303 current loss 0.762349, current_train_items 265728.
I0302 19:02:17.318760 22732373614720 run.py:483] Algo bellman_ford step 8304 current loss 0.769391, current_train_items 265760.
I0302 19:02:17.338718 22732373614720 run.py:483] Algo bellman_ford step 8305 current loss 0.339638, current_train_items 265792.
I0302 19:02:17.354246 22732373614720 run.py:483] Algo bellman_ford step 8306 current loss 0.590335, current_train_items 265824.
I0302 19:02:17.377743 22732373614720 run.py:483] Algo bellman_ford step 8307 current loss 0.627465, current_train_items 265856.
I0302 19:02:17.406693 22732373614720 run.py:483] Algo bellman_ford step 8308 current loss 0.623847, current_train_items 265888.
I0302 19:02:17.441703 22732373614720 run.py:483] Algo bellman_ford step 8309 current loss 0.907528, current_train_items 265920.
I0302 19:02:17.461075 22732373614720 run.py:483] Algo bellman_ford step 8310 current loss 0.316091, current_train_items 265952.
I0302 19:02:17.476937 22732373614720 run.py:483] Algo bellman_ford step 8311 current loss 0.469333, current_train_items 265984.
I0302 19:02:17.500339 22732373614720 run.py:483] Algo bellman_ford step 8312 current loss 0.667576, current_train_items 266016.
I0302 19:02:17.530939 22732373614720 run.py:483] Algo bellman_ford step 8313 current loss 0.682466, current_train_items 266048.
I0302 19:02:17.561982 22732373614720 run.py:483] Algo bellman_ford step 8314 current loss 0.780540, current_train_items 266080.
I0302 19:02:17.581324 22732373614720 run.py:483] Algo bellman_ford step 8315 current loss 0.294795, current_train_items 266112.
I0302 19:02:17.597547 22732373614720 run.py:483] Algo bellman_ford step 8316 current loss 0.549640, current_train_items 266144.
I0302 19:02:17.620525 22732373614720 run.py:483] Algo bellman_ford step 8317 current loss 0.680696, current_train_items 266176.
I0302 19:02:17.650399 22732373614720 run.py:483] Algo bellman_ford step 8318 current loss 0.636827, current_train_items 266208.
I0302 19:02:17.683434 22732373614720 run.py:483] Algo bellman_ford step 8319 current loss 0.860200, current_train_items 266240.
I0302 19:02:17.703086 22732373614720 run.py:483] Algo bellman_ford step 8320 current loss 0.286901, current_train_items 266272.
I0302 19:02:17.719011 22732373614720 run.py:483] Algo bellman_ford step 8321 current loss 0.482023, current_train_items 266304.
I0302 19:02:17.742909 22732373614720 run.py:483] Algo bellman_ford step 8322 current loss 0.656459, current_train_items 266336.
I0302 19:02:17.773659 22732373614720 run.py:483] Algo bellman_ford step 8323 current loss 0.758158, current_train_items 266368.
I0302 19:02:17.808004 22732373614720 run.py:483] Algo bellman_ford step 8324 current loss 1.086931, current_train_items 266400.
I0302 19:02:17.827303 22732373614720 run.py:483] Algo bellman_ford step 8325 current loss 0.400074, current_train_items 266432.
I0302 19:02:17.842759 22732373614720 run.py:483] Algo bellman_ford step 8326 current loss 0.586561, current_train_items 266464.
I0302 19:02:17.866060 22732373614720 run.py:483] Algo bellman_ford step 8327 current loss 0.634293, current_train_items 266496.
I0302 19:02:17.896288 22732373614720 run.py:483] Algo bellman_ford step 8328 current loss 0.765794, current_train_items 266528.
I0302 19:02:17.929859 22732373614720 run.py:483] Algo bellman_ford step 8329 current loss 0.946263, current_train_items 266560.
I0302 19:02:17.949271 22732373614720 run.py:483] Algo bellman_ford step 8330 current loss 0.373745, current_train_items 266592.
I0302 19:02:17.965110 22732373614720 run.py:483] Algo bellman_ford step 8331 current loss 0.629965, current_train_items 266624.
I0302 19:02:17.987804 22732373614720 run.py:483] Algo bellman_ford step 8332 current loss 0.752562, current_train_items 266656.
I0302 19:02:18.019210 22732373614720 run.py:483] Algo bellman_ford step 8333 current loss 0.972540, current_train_items 266688.
I0302 19:02:18.051640 22732373614720 run.py:483] Algo bellman_ford step 8334 current loss 0.969787, current_train_items 266720.
I0302 19:02:18.070960 22732373614720 run.py:483] Algo bellman_ford step 8335 current loss 0.382144, current_train_items 266752.
I0302 19:02:18.086868 22732373614720 run.py:483] Algo bellman_ford step 8336 current loss 0.518256, current_train_items 266784.
I0302 19:02:18.111085 22732373614720 run.py:483] Algo bellman_ford step 8337 current loss 0.762453, current_train_items 266816.
I0302 19:02:18.141935 22732373614720 run.py:483] Algo bellman_ford step 8338 current loss 0.827248, current_train_items 266848.
I0302 19:02:18.173794 22732373614720 run.py:483] Algo bellman_ford step 8339 current loss 1.075645, current_train_items 266880.
I0302 19:02:18.193285 22732373614720 run.py:483] Algo bellman_ford step 8340 current loss 0.370349, current_train_items 266912.
I0302 19:02:18.209125 22732373614720 run.py:483] Algo bellman_ford step 8341 current loss 0.434939, current_train_items 266944.
I0302 19:02:18.231577 22732373614720 run.py:483] Algo bellman_ford step 8342 current loss 0.599216, current_train_items 266976.
I0302 19:02:18.262171 22732373614720 run.py:483] Algo bellman_ford step 8343 current loss 0.636145, current_train_items 267008.
I0302 19:02:18.295892 22732373614720 run.py:483] Algo bellman_ford step 8344 current loss 0.869170, current_train_items 267040.
I0302 19:02:18.315216 22732373614720 run.py:483] Algo bellman_ford step 8345 current loss 0.362978, current_train_items 267072.
I0302 19:02:18.330969 22732373614720 run.py:483] Algo bellman_ford step 8346 current loss 0.486885, current_train_items 267104.
I0302 19:02:18.354855 22732373614720 run.py:483] Algo bellman_ford step 8347 current loss 0.591884, current_train_items 267136.
I0302 19:02:18.386440 22732373614720 run.py:483] Algo bellman_ford step 8348 current loss 0.697854, current_train_items 267168.
I0302 19:02:18.419937 22732373614720 run.py:483] Algo bellman_ford step 8349 current loss 0.902015, current_train_items 267200.
I0302 19:02:18.439622 22732373614720 run.py:483] Algo bellman_ford step 8350 current loss 0.314458, current_train_items 267232.
I0302 19:02:18.447572 22732373614720 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0302 19:02:18.447685 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:02:18.464478 22732373614720 run.py:483] Algo bellman_ford step 8351 current loss 0.458986, current_train_items 267264.
I0302 19:02:18.488855 22732373614720 run.py:483] Algo bellman_ford step 8352 current loss 0.660272, current_train_items 267296.
I0302 19:02:18.520243 22732373614720 run.py:483] Algo bellman_ford step 8353 current loss 0.680398, current_train_items 267328.
I0302 19:02:18.555078 22732373614720 run.py:483] Algo bellman_ford step 8354 current loss 0.974661, current_train_items 267360.
I0302 19:02:18.574741 22732373614720 run.py:483] Algo bellman_ford step 8355 current loss 0.346222, current_train_items 267392.
I0302 19:02:18.590298 22732373614720 run.py:483] Algo bellman_ford step 8356 current loss 0.554250, current_train_items 267424.
I0302 19:02:18.613325 22732373614720 run.py:483] Algo bellman_ford step 8357 current loss 0.590572, current_train_items 267456.
I0302 19:02:18.644562 22732373614720 run.py:483] Algo bellman_ford step 8358 current loss 0.693459, current_train_items 267488.
I0302 19:02:18.676542 22732373614720 run.py:483] Algo bellman_ford step 8359 current loss 0.879439, current_train_items 267520.
I0302 19:02:18.696249 22732373614720 run.py:483] Algo bellman_ford step 8360 current loss 0.353306, current_train_items 267552.
I0302 19:02:18.712022 22732373614720 run.py:483] Algo bellman_ford step 8361 current loss 0.521611, current_train_items 267584.
I0302 19:02:18.735479 22732373614720 run.py:483] Algo bellman_ford step 8362 current loss 0.663699, current_train_items 267616.
I0302 19:02:18.766048 22732373614720 run.py:483] Algo bellman_ford step 8363 current loss 0.660156, current_train_items 267648.
I0302 19:02:18.799289 22732373614720 run.py:483] Algo bellman_ford step 8364 current loss 0.876204, current_train_items 267680.
I0302 19:02:18.818683 22732373614720 run.py:483] Algo bellman_ford step 8365 current loss 0.351745, current_train_items 267712.
I0302 19:02:18.834977 22732373614720 run.py:483] Algo bellman_ford step 8366 current loss 0.452637, current_train_items 267744.
I0302 19:02:18.857759 22732373614720 run.py:483] Algo bellman_ford step 8367 current loss 0.601008, current_train_items 267776.
I0302 19:02:18.888952 22732373614720 run.py:483] Algo bellman_ford step 8368 current loss 0.761864, current_train_items 267808.
I0302 19:02:18.924029 22732373614720 run.py:483] Algo bellman_ford step 8369 current loss 0.957834, current_train_items 267840.
I0302 19:02:18.943986 22732373614720 run.py:483] Algo bellman_ford step 8370 current loss 0.268115, current_train_items 267872.
I0302 19:02:18.960381 22732373614720 run.py:483] Algo bellman_ford step 8371 current loss 0.616914, current_train_items 267904.
I0302 19:02:18.982941 22732373614720 run.py:483] Algo bellman_ford step 8372 current loss 0.689567, current_train_items 267936.
I0302 19:02:19.013645 22732373614720 run.py:483] Algo bellman_ford step 8373 current loss 0.645240, current_train_items 267968.
I0302 19:02:19.047195 22732373614720 run.py:483] Algo bellman_ford step 8374 current loss 0.831353, current_train_items 268000.
I0302 19:02:19.067215 22732373614720 run.py:483] Algo bellman_ford step 8375 current loss 0.302981, current_train_items 268032.
I0302 19:02:19.082998 22732373614720 run.py:483] Algo bellman_ford step 8376 current loss 0.459783, current_train_items 268064.
I0302 19:02:19.105599 22732373614720 run.py:483] Algo bellman_ford step 8377 current loss 0.658972, current_train_items 268096.
I0302 19:02:19.136695 22732373614720 run.py:483] Algo bellman_ford step 8378 current loss 0.672961, current_train_items 268128.
I0302 19:02:19.169952 22732373614720 run.py:483] Algo bellman_ford step 8379 current loss 0.949429, current_train_items 268160.
I0302 19:02:19.189527 22732373614720 run.py:483] Algo bellman_ford step 8380 current loss 0.369066, current_train_items 268192.
I0302 19:02:19.205673 22732373614720 run.py:483] Algo bellman_ford step 8381 current loss 0.532150, current_train_items 268224.
I0302 19:02:19.228221 22732373614720 run.py:483] Algo bellman_ford step 8382 current loss 0.477663, current_train_items 268256.
I0302 19:02:19.259848 22732373614720 run.py:483] Algo bellman_ford step 8383 current loss 0.774342, current_train_items 268288.
I0302 19:02:19.291407 22732373614720 run.py:483] Algo bellman_ford step 8384 current loss 0.756076, current_train_items 268320.
I0302 19:02:19.311223 22732373614720 run.py:483] Algo bellman_ford step 8385 current loss 0.381559, current_train_items 268352.
I0302 19:02:19.326934 22732373614720 run.py:483] Algo bellman_ford step 8386 current loss 0.435386, current_train_items 268384.
I0302 19:02:19.349748 22732373614720 run.py:483] Algo bellman_ford step 8387 current loss 0.563713, current_train_items 268416.
I0302 19:02:19.380908 22732373614720 run.py:483] Algo bellman_ford step 8388 current loss 0.762062, current_train_items 268448.
I0302 19:02:19.413621 22732373614720 run.py:483] Algo bellman_ford step 8389 current loss 0.915065, current_train_items 268480.
I0302 19:02:19.433670 22732373614720 run.py:483] Algo bellman_ford step 8390 current loss 0.388895, current_train_items 268512.
I0302 19:02:19.449920 22732373614720 run.py:483] Algo bellman_ford step 8391 current loss 0.484307, current_train_items 268544.
I0302 19:02:19.472433 22732373614720 run.py:483] Algo bellman_ford step 8392 current loss 0.596246, current_train_items 268576.
I0302 19:02:19.504738 22732373614720 run.py:483] Algo bellman_ford step 8393 current loss 0.765597, current_train_items 268608.
I0302 19:02:19.536840 22732373614720 run.py:483] Algo bellman_ford step 8394 current loss 0.811504, current_train_items 268640.
I0302 19:02:19.556534 22732373614720 run.py:483] Algo bellman_ford step 8395 current loss 0.407802, current_train_items 268672.
I0302 19:02:19.572684 22732373614720 run.py:483] Algo bellman_ford step 8396 current loss 0.569287, current_train_items 268704.
I0302 19:02:19.595840 22732373614720 run.py:483] Algo bellman_ford step 8397 current loss 0.628770, current_train_items 268736.
I0302 19:02:19.627253 22732373614720 run.py:483] Algo bellman_ford step 8398 current loss 0.756071, current_train_items 268768.
I0302 19:02:19.661310 22732373614720 run.py:483] Algo bellman_ford step 8399 current loss 0.991424, current_train_items 268800.
I0302 19:02:19.681101 22732373614720 run.py:483] Algo bellman_ford step 8400 current loss 0.396157, current_train_items 268832.
I0302 19:02:19.688983 22732373614720 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.8779296875, 'score': 0.8779296875, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0302 19:02:19.689095 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.878, val scores are: bellman_ford: 0.878
I0302 19:02:19.706262 22732373614720 run.py:483] Algo bellman_ford step 8401 current loss 0.505899, current_train_items 268864.
I0302 19:02:19.730982 22732373614720 run.py:483] Algo bellman_ford step 8402 current loss 0.680464, current_train_items 268896.
I0302 19:02:19.763071 22732373614720 run.py:483] Algo bellman_ford step 8403 current loss 0.787145, current_train_items 268928.
I0302 19:02:19.799122 22732373614720 run.py:483] Algo bellman_ford step 8404 current loss 0.945610, current_train_items 268960.
I0302 19:02:19.819409 22732373614720 run.py:483] Algo bellman_ford step 8405 current loss 0.376704, current_train_items 268992.
I0302 19:02:19.835360 22732373614720 run.py:483] Algo bellman_ford step 8406 current loss 0.568663, current_train_items 269024.
I0302 19:02:19.858706 22732373614720 run.py:483] Algo bellman_ford step 8407 current loss 0.684953, current_train_items 269056.
I0302 19:02:19.889625 22732373614720 run.py:483] Algo bellman_ford step 8408 current loss 0.763119, current_train_items 269088.
I0302 19:02:19.924243 22732373614720 run.py:483] Algo bellman_ford step 8409 current loss 0.859135, current_train_items 269120.
I0302 19:02:19.943839 22732373614720 run.py:483] Algo bellman_ford step 8410 current loss 0.283847, current_train_items 269152.
I0302 19:02:19.959964 22732373614720 run.py:483] Algo bellman_ford step 8411 current loss 0.524399, current_train_items 269184.
I0302 19:02:19.984066 22732373614720 run.py:483] Algo bellman_ford step 8412 current loss 0.607059, current_train_items 269216.
I0302 19:02:20.016517 22732373614720 run.py:483] Algo bellman_ford step 8413 current loss 0.792769, current_train_items 269248.
I0302 19:02:20.050426 22732373614720 run.py:483] Algo bellman_ford step 8414 current loss 0.817460, current_train_items 269280.
I0302 19:02:20.070502 22732373614720 run.py:483] Algo bellman_ford step 8415 current loss 0.382626, current_train_items 269312.
I0302 19:02:20.086670 22732373614720 run.py:483] Algo bellman_ford step 8416 current loss 0.471385, current_train_items 269344.
I0302 19:02:20.110118 22732373614720 run.py:483] Algo bellman_ford step 8417 current loss 0.744614, current_train_items 269376.
I0302 19:02:20.142220 22732373614720 run.py:483] Algo bellman_ford step 8418 current loss 0.732648, current_train_items 269408.
I0302 19:02:20.178974 22732373614720 run.py:483] Algo bellman_ford step 8419 current loss 0.934047, current_train_items 269440.
I0302 19:02:20.198854 22732373614720 run.py:483] Algo bellman_ford step 8420 current loss 0.299284, current_train_items 269472.
I0302 19:02:20.215045 22732373614720 run.py:483] Algo bellman_ford step 8421 current loss 0.532728, current_train_items 269504.
I0302 19:02:20.239973 22732373614720 run.py:483] Algo bellman_ford step 8422 current loss 0.585616, current_train_items 269536.
I0302 19:02:20.271692 22732373614720 run.py:483] Algo bellman_ford step 8423 current loss 0.640104, current_train_items 269568.
I0302 19:02:20.306101 22732373614720 run.py:483] Algo bellman_ford step 8424 current loss 0.859103, current_train_items 269600.
I0302 19:02:20.325481 22732373614720 run.py:483] Algo bellman_ford step 8425 current loss 0.320038, current_train_items 269632.
I0302 19:02:20.341164 22732373614720 run.py:483] Algo bellman_ford step 8426 current loss 0.517320, current_train_items 269664.
I0302 19:02:20.364702 22732373614720 run.py:483] Algo bellman_ford step 8427 current loss 0.623013, current_train_items 269696.
I0302 19:02:20.396016 22732373614720 run.py:483] Algo bellman_ford step 8428 current loss 0.690661, current_train_items 269728.
I0302 19:02:20.429214 22732373614720 run.py:483] Algo bellman_ford step 8429 current loss 0.874843, current_train_items 269760.
I0302 19:02:20.448706 22732373614720 run.py:483] Algo bellman_ford step 8430 current loss 0.365889, current_train_items 269792.
I0302 19:02:20.465250 22732373614720 run.py:483] Algo bellman_ford step 8431 current loss 0.626808, current_train_items 269824.
I0302 19:02:20.488612 22732373614720 run.py:483] Algo bellman_ford step 8432 current loss 0.524222, current_train_items 269856.
I0302 19:02:20.521122 22732373614720 run.py:483] Algo bellman_ford step 8433 current loss 0.882514, current_train_items 269888.
I0302 19:02:20.553272 22732373614720 run.py:483] Algo bellman_ford step 8434 current loss 0.782749, current_train_items 269920.
I0302 19:02:20.572485 22732373614720 run.py:483] Algo bellman_ford step 8435 current loss 0.318615, current_train_items 269952.
I0302 19:02:20.588140 22732373614720 run.py:483] Algo bellman_ford step 8436 current loss 0.572591, current_train_items 269984.
I0302 19:02:20.611447 22732373614720 run.py:483] Algo bellman_ford step 8437 current loss 0.706866, current_train_items 270016.
I0302 19:02:20.642608 22732373614720 run.py:483] Algo bellman_ford step 8438 current loss 0.783741, current_train_items 270048.
I0302 19:02:20.674500 22732373614720 run.py:483] Algo bellman_ford step 8439 current loss 0.870868, current_train_items 270080.
I0302 19:02:20.694104 22732373614720 run.py:483] Algo bellman_ford step 8440 current loss 0.363500, current_train_items 270112.
I0302 19:02:20.710234 22732373614720 run.py:483] Algo bellman_ford step 8441 current loss 0.614282, current_train_items 270144.
I0302 19:02:20.734193 22732373614720 run.py:483] Algo bellman_ford step 8442 current loss 0.738200, current_train_items 270176.
I0302 19:02:20.765135 22732373614720 run.py:483] Algo bellman_ford step 8443 current loss 0.826307, current_train_items 270208.
I0302 19:02:20.798841 22732373614720 run.py:483] Algo bellman_ford step 8444 current loss 1.010446, current_train_items 270240.
I0302 19:02:20.818081 22732373614720 run.py:483] Algo bellman_ford step 8445 current loss 0.395526, current_train_items 270272.
I0302 19:02:20.833861 22732373614720 run.py:483] Algo bellman_ford step 8446 current loss 0.504825, current_train_items 270304.
I0302 19:02:20.857206 22732373614720 run.py:483] Algo bellman_ford step 8447 current loss 0.597630, current_train_items 270336.
I0302 19:02:20.888228 22732373614720 run.py:483] Algo bellman_ford step 8448 current loss 0.695034, current_train_items 270368.
I0302 19:02:20.921845 22732373614720 run.py:483] Algo bellman_ford step 8449 current loss 0.845525, current_train_items 270400.
I0302 19:02:20.941509 22732373614720 run.py:483] Algo bellman_ford step 8450 current loss 0.352471, current_train_items 270432.
I0302 19:02:20.949702 22732373614720 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0302 19:02:20.949812 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 19:02:20.966084 22732373614720 run.py:483] Algo bellman_ford step 8451 current loss 0.532842, current_train_items 270464.
I0302 19:02:20.990548 22732373614720 run.py:483] Algo bellman_ford step 8452 current loss 0.602668, current_train_items 270496.
I0302 19:02:21.023063 22732373614720 run.py:483] Algo bellman_ford step 8453 current loss 0.702111, current_train_items 270528.
I0302 19:02:21.057370 22732373614720 run.py:483] Algo bellman_ford step 8454 current loss 0.744793, current_train_items 270560.
I0302 19:02:21.077829 22732373614720 run.py:483] Algo bellman_ford step 8455 current loss 0.342681, current_train_items 270592.
I0302 19:02:21.093894 22732373614720 run.py:483] Algo bellman_ford step 8456 current loss 0.546463, current_train_items 270624.
I0302 19:02:21.118244 22732373614720 run.py:483] Algo bellman_ford step 8457 current loss 0.668152, current_train_items 270656.
I0302 19:02:21.148260 22732373614720 run.py:483] Algo bellman_ford step 8458 current loss 0.727508, current_train_items 270688.
I0302 19:02:21.179397 22732373614720 run.py:483] Algo bellman_ford step 8459 current loss 0.761420, current_train_items 270720.
I0302 19:02:21.199368 22732373614720 run.py:483] Algo bellman_ford step 8460 current loss 0.370094, current_train_items 270752.
I0302 19:02:21.215297 22732373614720 run.py:483] Algo bellman_ford step 8461 current loss 0.459439, current_train_items 270784.
I0302 19:02:21.237760 22732373614720 run.py:483] Algo bellman_ford step 8462 current loss 0.619162, current_train_items 270816.
I0302 19:02:21.268206 22732373614720 run.py:483] Algo bellman_ford step 8463 current loss 0.657454, current_train_items 270848.
I0302 19:02:21.301956 22732373614720 run.py:483] Algo bellman_ford step 8464 current loss 0.857701, current_train_items 270880.
I0302 19:02:21.321510 22732373614720 run.py:483] Algo bellman_ford step 8465 current loss 0.327391, current_train_items 270912.
I0302 19:02:21.338078 22732373614720 run.py:483] Algo bellman_ford step 8466 current loss 0.549734, current_train_items 270944.
I0302 19:02:21.361098 22732373614720 run.py:483] Algo bellman_ford step 8467 current loss 0.608943, current_train_items 270976.
I0302 19:02:21.391872 22732373614720 run.py:483] Algo bellman_ford step 8468 current loss 0.815151, current_train_items 271008.
I0302 19:02:21.424572 22732373614720 run.py:483] Algo bellman_ford step 8469 current loss 0.796423, current_train_items 271040.
I0302 19:02:21.444716 22732373614720 run.py:483] Algo bellman_ford step 8470 current loss 0.459232, current_train_items 271072.
I0302 19:02:21.461394 22732373614720 run.py:483] Algo bellman_ford step 8471 current loss 0.661076, current_train_items 271104.
I0302 19:02:21.484506 22732373614720 run.py:483] Algo bellman_ford step 8472 current loss 0.603212, current_train_items 271136.
I0302 19:02:21.513503 22732373614720 run.py:483] Algo bellman_ford step 8473 current loss 0.624020, current_train_items 271168.
I0302 19:02:21.547109 22732373614720 run.py:483] Algo bellman_ford step 8474 current loss 0.798801, current_train_items 271200.
I0302 19:02:21.567312 22732373614720 run.py:483] Algo bellman_ford step 8475 current loss 0.337274, current_train_items 271232.
I0302 19:02:21.583734 22732373614720 run.py:483] Algo bellman_ford step 8476 current loss 0.516294, current_train_items 271264.
I0302 19:02:21.608277 22732373614720 run.py:483] Algo bellman_ford step 8477 current loss 0.766963, current_train_items 271296.
I0302 19:02:21.639782 22732373614720 run.py:483] Algo bellman_ford step 8478 current loss 0.692476, current_train_items 271328.
I0302 19:02:21.674216 22732373614720 run.py:483] Algo bellman_ford step 8479 current loss 0.774012, current_train_items 271360.
I0302 19:02:21.693817 22732373614720 run.py:483] Algo bellman_ford step 8480 current loss 0.398775, current_train_items 271392.
I0302 19:02:21.710142 22732373614720 run.py:483] Algo bellman_ford step 8481 current loss 0.662538, current_train_items 271424.
I0302 19:02:21.734132 22732373614720 run.py:483] Algo bellman_ford step 8482 current loss 0.820473, current_train_items 271456.
I0302 19:02:21.765357 22732373614720 run.py:483] Algo bellman_ford step 8483 current loss 0.852664, current_train_items 271488.
I0302 19:02:21.797750 22732373614720 run.py:483] Algo bellman_ford step 8484 current loss 0.741382, current_train_items 271520.
I0302 19:02:21.817868 22732373614720 run.py:483] Algo bellman_ford step 8485 current loss 0.421226, current_train_items 271552.
I0302 19:02:21.833732 22732373614720 run.py:483] Algo bellman_ford step 8486 current loss 0.389213, current_train_items 271584.
I0302 19:02:21.857078 22732373614720 run.py:483] Algo bellman_ford step 8487 current loss 0.646202, current_train_items 271616.
I0302 19:02:21.888981 22732373614720 run.py:483] Algo bellman_ford step 8488 current loss 0.694447, current_train_items 271648.
I0302 19:02:21.922547 22732373614720 run.py:483] Algo bellman_ford step 8489 current loss 0.802066, current_train_items 271680.
I0302 19:02:21.942722 22732373614720 run.py:483] Algo bellman_ford step 8490 current loss 0.356270, current_train_items 271712.
I0302 19:02:21.958666 22732373614720 run.py:483] Algo bellman_ford step 8491 current loss 0.534978, current_train_items 271744.
I0302 19:02:21.982660 22732373614720 run.py:483] Algo bellman_ford step 8492 current loss 0.787991, current_train_items 271776.
I0302 19:02:22.013598 22732373614720 run.py:483] Algo bellman_ford step 8493 current loss 0.719053, current_train_items 271808.
I0302 19:02:22.045608 22732373614720 run.py:483] Algo bellman_ford step 8494 current loss 0.870401, current_train_items 271840.
I0302 19:02:22.065719 22732373614720 run.py:483] Algo bellman_ford step 8495 current loss 0.310548, current_train_items 271872.
I0302 19:02:22.082143 22732373614720 run.py:483] Algo bellman_ford step 8496 current loss 0.596489, current_train_items 271904.
I0302 19:02:22.105812 22732373614720 run.py:483] Algo bellman_ford step 8497 current loss 0.553379, current_train_items 271936.
I0302 19:02:22.137234 22732373614720 run.py:483] Algo bellman_ford step 8498 current loss 0.725936, current_train_items 271968.
I0302 19:02:22.169030 22732373614720 run.py:483] Algo bellman_ford step 8499 current loss 0.745652, current_train_items 272000.
I0302 19:02:22.188946 22732373614720 run.py:483] Algo bellman_ford step 8500 current loss 0.343555, current_train_items 272032.
I0302 19:02:22.196715 22732373614720 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0302 19:02:22.196857 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:02:22.213932 22732373614720 run.py:483] Algo bellman_ford step 8501 current loss 0.486101, current_train_items 272064.
I0302 19:02:22.238546 22732373614720 run.py:483] Algo bellman_ford step 8502 current loss 0.700497, current_train_items 272096.
I0302 19:02:22.269076 22732373614720 run.py:483] Algo bellman_ford step 8503 current loss 0.799528, current_train_items 272128.
I0302 19:02:22.301568 22732373614720 run.py:483] Algo bellman_ford step 8504 current loss 0.907780, current_train_items 272160.
I0302 19:02:22.321336 22732373614720 run.py:483] Algo bellman_ford step 8505 current loss 0.357646, current_train_items 272192.
I0302 19:02:22.336433 22732373614720 run.py:483] Algo bellman_ford step 8506 current loss 0.448322, current_train_items 272224.
I0302 19:02:22.358419 22732373614720 run.py:483] Algo bellman_ford step 8507 current loss 0.586079, current_train_items 272256.
I0302 19:02:22.389836 22732373614720 run.py:483] Algo bellman_ford step 8508 current loss 0.759297, current_train_items 272288.
I0302 19:02:22.424045 22732373614720 run.py:483] Algo bellman_ford step 8509 current loss 0.831496, current_train_items 272320.
I0302 19:02:22.443285 22732373614720 run.py:483] Algo bellman_ford step 8510 current loss 0.396752, current_train_items 272352.
I0302 19:02:22.458687 22732373614720 run.py:483] Algo bellman_ford step 8511 current loss 0.569469, current_train_items 272384.
I0302 19:02:22.482774 22732373614720 run.py:483] Algo bellman_ford step 8512 current loss 0.819808, current_train_items 272416.
I0302 19:02:22.513254 22732373614720 run.py:483] Algo bellman_ford step 8513 current loss 0.856767, current_train_items 272448.
I0302 19:02:22.546774 22732373614720 run.py:483] Algo bellman_ford step 8514 current loss 0.854090, current_train_items 272480.
I0302 19:02:22.566123 22732373614720 run.py:483] Algo bellman_ford step 8515 current loss 0.351136, current_train_items 272512.
I0302 19:02:22.582091 22732373614720 run.py:483] Algo bellman_ford step 8516 current loss 0.480476, current_train_items 272544.
I0302 19:02:22.605372 22732373614720 run.py:483] Algo bellman_ford step 8517 current loss 0.669704, current_train_items 272576.
I0302 19:02:22.637762 22732373614720 run.py:483] Algo bellman_ford step 8518 current loss 0.866827, current_train_items 272608.
I0302 19:02:22.669725 22732373614720 run.py:483] Algo bellman_ford step 8519 current loss 1.048960, current_train_items 272640.
I0302 19:02:22.689420 22732373614720 run.py:483] Algo bellman_ford step 8520 current loss 0.329803, current_train_items 272672.
I0302 19:02:22.705171 22732373614720 run.py:483] Algo bellman_ford step 8521 current loss 0.571794, current_train_items 272704.
I0302 19:02:22.728197 22732373614720 run.py:483] Algo bellman_ford step 8522 current loss 0.589898, current_train_items 272736.
I0302 19:02:22.758865 22732373614720 run.py:483] Algo bellman_ford step 8523 current loss 0.793386, current_train_items 272768.
I0302 19:02:22.792508 22732373614720 run.py:483] Algo bellman_ford step 8524 current loss 0.856807, current_train_items 272800.
I0302 19:02:22.812039 22732373614720 run.py:483] Algo bellman_ford step 8525 current loss 0.444924, current_train_items 272832.
I0302 19:02:22.828016 22732373614720 run.py:483] Algo bellman_ford step 8526 current loss 0.687367, current_train_items 272864.
I0302 19:02:22.851236 22732373614720 run.py:483] Algo bellman_ford step 8527 current loss 0.883088, current_train_items 272896.
I0302 19:02:22.881657 22732373614720 run.py:483] Algo bellman_ford step 8528 current loss 0.732890, current_train_items 272928.
I0302 19:02:22.915130 22732373614720 run.py:483] Algo bellman_ford step 8529 current loss 0.814145, current_train_items 272960.
I0302 19:02:22.934730 22732373614720 run.py:483] Algo bellman_ford step 8530 current loss 0.282515, current_train_items 272992.
I0302 19:02:22.950736 22732373614720 run.py:483] Algo bellman_ford step 8531 current loss 0.543778, current_train_items 273024.
I0302 19:02:22.972779 22732373614720 run.py:483] Algo bellman_ford step 8532 current loss 0.643324, current_train_items 273056.
I0302 19:02:23.003100 22732373614720 run.py:483] Algo bellman_ford step 8533 current loss 0.882864, current_train_items 273088.
I0302 19:02:23.034965 22732373614720 run.py:483] Algo bellman_ford step 8534 current loss 1.116440, current_train_items 273120.
I0302 19:02:23.054423 22732373614720 run.py:483] Algo bellman_ford step 8535 current loss 0.273519, current_train_items 273152.
I0302 19:02:23.070233 22732373614720 run.py:483] Algo bellman_ford step 8536 current loss 0.640566, current_train_items 273184.
I0302 19:02:23.093319 22732373614720 run.py:483] Algo bellman_ford step 8537 current loss 0.816695, current_train_items 273216.
I0302 19:02:23.124188 22732373614720 run.py:483] Algo bellman_ford step 8538 current loss 0.799745, current_train_items 273248.
I0302 19:02:23.158706 22732373614720 run.py:483] Algo bellman_ford step 8539 current loss 1.159230, current_train_items 273280.
I0302 19:02:23.178279 22732373614720 run.py:483] Algo bellman_ford step 8540 current loss 0.331033, current_train_items 273312.
I0302 19:02:23.194090 22732373614720 run.py:483] Algo bellman_ford step 8541 current loss 0.592982, current_train_items 273344.
I0302 19:02:23.217650 22732373614720 run.py:483] Algo bellman_ford step 8542 current loss 0.727780, current_train_items 273376.
I0302 19:02:23.249142 22732373614720 run.py:483] Algo bellman_ford step 8543 current loss 0.739779, current_train_items 273408.
I0302 19:02:23.281751 22732373614720 run.py:483] Algo bellman_ford step 8544 current loss 0.988787, current_train_items 273440.
I0302 19:02:23.300882 22732373614720 run.py:483] Algo bellman_ford step 8545 current loss 0.337074, current_train_items 273472.
I0302 19:02:23.316368 22732373614720 run.py:483] Algo bellman_ford step 8546 current loss 0.450302, current_train_items 273504.
I0302 19:02:23.340059 22732373614720 run.py:483] Algo bellman_ford step 8547 current loss 0.695038, current_train_items 273536.
I0302 19:02:23.370636 22732373614720 run.py:483] Algo bellman_ford step 8548 current loss 0.718863, current_train_items 273568.
I0302 19:02:23.404048 22732373614720 run.py:483] Algo bellman_ford step 8549 current loss 0.892145, current_train_items 273600.
I0302 19:02:23.423242 22732373614720 run.py:483] Algo bellman_ford step 8550 current loss 0.341758, current_train_items 273632.
I0302 19:02:23.431341 22732373614720 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0302 19:02:23.431453 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:02:23.448502 22732373614720 run.py:483] Algo bellman_ford step 8551 current loss 0.541075, current_train_items 273664.
I0302 19:02:23.472826 22732373614720 run.py:483] Algo bellman_ford step 8552 current loss 0.659815, current_train_items 273696.
I0302 19:02:23.504365 22732373614720 run.py:483] Algo bellman_ford step 8553 current loss 0.689977, current_train_items 273728.
I0302 19:02:23.535975 22732373614720 run.py:483] Algo bellman_ford step 8554 current loss 0.690659, current_train_items 273760.
I0302 19:02:23.556309 22732373614720 run.py:483] Algo bellman_ford step 8555 current loss 0.436814, current_train_items 273792.
I0302 19:02:23.571997 22732373614720 run.py:483] Algo bellman_ford step 8556 current loss 0.577360, current_train_items 273824.
I0302 19:02:23.595006 22732373614720 run.py:483] Algo bellman_ford step 8557 current loss 0.663312, current_train_items 273856.
I0302 19:02:23.626323 22732373614720 run.py:483] Algo bellman_ford step 8558 current loss 0.736786, current_train_items 273888.
I0302 19:02:23.660511 22732373614720 run.py:483] Algo bellman_ford step 8559 current loss 0.812750, current_train_items 273920.
I0302 19:02:23.680145 22732373614720 run.py:483] Algo bellman_ford step 8560 current loss 0.320415, current_train_items 273952.
I0302 19:02:23.696625 22732373614720 run.py:483] Algo bellman_ford step 8561 current loss 0.483986, current_train_items 273984.
I0302 19:02:23.719770 22732373614720 run.py:483] Algo bellman_ford step 8562 current loss 0.716453, current_train_items 274016.
I0302 19:02:23.751935 22732373614720 run.py:483] Algo bellman_ford step 8563 current loss 0.781296, current_train_items 274048.
I0302 19:02:23.783751 22732373614720 run.py:483] Algo bellman_ford step 8564 current loss 0.918696, current_train_items 274080.
I0302 19:02:23.802971 22732373614720 run.py:483] Algo bellman_ford step 8565 current loss 0.322134, current_train_items 274112.
I0302 19:02:23.819542 22732373614720 run.py:483] Algo bellman_ford step 8566 current loss 0.502695, current_train_items 274144.
I0302 19:02:23.844313 22732373614720 run.py:483] Algo bellman_ford step 8567 current loss 0.656087, current_train_items 274176.
I0302 19:02:23.875591 22732373614720 run.py:483] Algo bellman_ford step 8568 current loss 0.725328, current_train_items 274208.
I0302 19:02:23.909197 22732373614720 run.py:483] Algo bellman_ford step 8569 current loss 0.833045, current_train_items 274240.
I0302 19:02:23.929226 22732373614720 run.py:483] Algo bellman_ford step 8570 current loss 0.383310, current_train_items 274272.
I0302 19:02:23.945187 22732373614720 run.py:483] Algo bellman_ford step 8571 current loss 0.692512, current_train_items 274304.
I0302 19:02:23.967737 22732373614720 run.py:483] Algo bellman_ford step 8572 current loss 0.599402, current_train_items 274336.
I0302 19:02:23.998457 22732373614720 run.py:483] Algo bellman_ford step 8573 current loss 0.677429, current_train_items 274368.
I0302 19:02:24.029862 22732373614720 run.py:483] Algo bellman_ford step 8574 current loss 0.784107, current_train_items 274400.
I0302 19:02:24.049510 22732373614720 run.py:483] Algo bellman_ford step 8575 current loss 0.399603, current_train_items 274432.
I0302 19:02:24.065529 22732373614720 run.py:483] Algo bellman_ford step 8576 current loss 0.510257, current_train_items 274464.
I0302 19:02:24.088693 22732373614720 run.py:483] Algo bellman_ford step 8577 current loss 0.629533, current_train_items 274496.
I0302 19:02:24.120317 22732373614720 run.py:483] Algo bellman_ford step 8578 current loss 0.718280, current_train_items 274528.
I0302 19:02:24.153582 22732373614720 run.py:483] Algo bellman_ford step 8579 current loss 0.792874, current_train_items 274560.
I0302 19:02:24.173433 22732373614720 run.py:483] Algo bellman_ford step 8580 current loss 0.378123, current_train_items 274592.
I0302 19:02:24.189630 22732373614720 run.py:483] Algo bellman_ford step 8581 current loss 0.521374, current_train_items 274624.
I0302 19:02:24.212078 22732373614720 run.py:483] Algo bellman_ford step 8582 current loss 0.663870, current_train_items 274656.
I0302 19:02:24.243322 22732373614720 run.py:483] Algo bellman_ford step 8583 current loss 0.738921, current_train_items 274688.
I0302 19:02:24.277281 22732373614720 run.py:483] Algo bellman_ford step 8584 current loss 0.868995, current_train_items 274720.
I0302 19:02:24.296868 22732373614720 run.py:483] Algo bellman_ford step 8585 current loss 0.343622, current_train_items 274752.
I0302 19:02:24.312959 22732373614720 run.py:483] Algo bellman_ford step 8586 current loss 0.492697, current_train_items 274784.
I0302 19:02:24.337263 22732373614720 run.py:483] Algo bellman_ford step 8587 current loss 0.748427, current_train_items 274816.
I0302 19:02:24.368602 22732373614720 run.py:483] Algo bellman_ford step 8588 current loss 0.680247, current_train_items 274848.
I0302 19:02:24.400915 22732373614720 run.py:483] Algo bellman_ford step 8589 current loss 1.019563, current_train_items 274880.
I0302 19:02:24.420544 22732373614720 run.py:483] Algo bellman_ford step 8590 current loss 0.369701, current_train_items 274912.
I0302 19:02:24.435959 22732373614720 run.py:483] Algo bellman_ford step 8591 current loss 0.484238, current_train_items 274944.
I0302 19:02:24.459088 22732373614720 run.py:483] Algo bellman_ford step 8592 current loss 0.757735, current_train_items 274976.
I0302 19:02:24.490820 22732373614720 run.py:483] Algo bellman_ford step 8593 current loss 0.888776, current_train_items 275008.
I0302 19:02:24.524023 22732373614720 run.py:483] Algo bellman_ford step 8594 current loss 0.994046, current_train_items 275040.
I0302 19:02:24.543393 22732373614720 run.py:483] Algo bellman_ford step 8595 current loss 0.320737, current_train_items 275072.
I0302 19:02:24.559170 22732373614720 run.py:483] Algo bellman_ford step 8596 current loss 0.488021, current_train_items 275104.
I0302 19:02:24.582511 22732373614720 run.py:483] Algo bellman_ford step 8597 current loss 0.569496, current_train_items 275136.
I0302 19:02:24.612264 22732373614720 run.py:483] Algo bellman_ford step 8598 current loss 0.649348, current_train_items 275168.
I0302 19:02:24.646371 22732373614720 run.py:483] Algo bellman_ford step 8599 current loss 1.092481, current_train_items 275200.
I0302 19:02:24.666068 22732373614720 run.py:483] Algo bellman_ford step 8600 current loss 0.392562, current_train_items 275232.
I0302 19:02:24.674148 22732373614720 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0302 19:02:24.674268 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:02:24.691152 22732373614720 run.py:483] Algo bellman_ford step 8601 current loss 0.551834, current_train_items 275264.
I0302 19:02:24.715934 22732373614720 run.py:483] Algo bellman_ford step 8602 current loss 0.735110, current_train_items 275296.
I0302 19:02:24.748555 22732373614720 run.py:483] Algo bellman_ford step 8603 current loss 0.715061, current_train_items 275328.
I0302 19:02:24.782973 22732373614720 run.py:483] Algo bellman_ford step 8604 current loss 0.800932, current_train_items 275360.
I0302 19:02:24.802992 22732373614720 run.py:483] Algo bellman_ford step 8605 current loss 0.344389, current_train_items 275392.
I0302 19:02:24.818215 22732373614720 run.py:483] Algo bellman_ford step 8606 current loss 0.454611, current_train_items 275424.
I0302 19:02:24.841421 22732373614720 run.py:483] Algo bellman_ford step 8607 current loss 0.647805, current_train_items 275456.
I0302 19:02:24.872726 22732373614720 run.py:483] Algo bellman_ford step 8608 current loss 0.696520, current_train_items 275488.
I0302 19:02:24.907370 22732373614720 run.py:483] Algo bellman_ford step 8609 current loss 1.020726, current_train_items 275520.
I0302 19:02:24.927199 22732373614720 run.py:483] Algo bellman_ford step 8610 current loss 0.340076, current_train_items 275552.
I0302 19:02:24.943233 22732373614720 run.py:483] Algo bellman_ford step 8611 current loss 0.500729, current_train_items 275584.
I0302 19:02:24.967138 22732373614720 run.py:483] Algo bellman_ford step 8612 current loss 0.703188, current_train_items 275616.
I0302 19:02:24.998508 22732373614720 run.py:483] Algo bellman_ford step 8613 current loss 0.728094, current_train_items 275648.
I0302 19:02:25.030110 22732373614720 run.py:483] Algo bellman_ford step 8614 current loss 1.017027, current_train_items 275680.
I0302 19:02:25.049663 22732373614720 run.py:483] Algo bellman_ford step 8615 current loss 0.325245, current_train_items 275712.
I0302 19:02:25.065792 22732373614720 run.py:483] Algo bellman_ford step 8616 current loss 0.479268, current_train_items 275744.
I0302 19:02:25.089514 22732373614720 run.py:483] Algo bellman_ford step 8617 current loss 0.604310, current_train_items 275776.
I0302 19:02:25.121454 22732373614720 run.py:483] Algo bellman_ford step 8618 current loss 0.753873, current_train_items 275808.
I0302 19:02:25.154574 22732373614720 run.py:483] Algo bellman_ford step 8619 current loss 0.959990, current_train_items 275840.
I0302 19:02:25.174387 22732373614720 run.py:483] Algo bellman_ford step 8620 current loss 0.317994, current_train_items 275872.
I0302 19:02:25.190670 22732373614720 run.py:483] Algo bellman_ford step 8621 current loss 0.509998, current_train_items 275904.
I0302 19:02:25.215002 22732373614720 run.py:483] Algo bellman_ford step 8622 current loss 0.712429, current_train_items 275936.
I0302 19:02:25.247037 22732373614720 run.py:483] Algo bellman_ford step 8623 current loss 0.657083, current_train_items 275968.
I0302 19:02:25.280364 22732373614720 run.py:483] Algo bellman_ford step 8624 current loss 0.677472, current_train_items 276000.
I0302 19:02:25.300060 22732373614720 run.py:483] Algo bellman_ford step 8625 current loss 0.352474, current_train_items 276032.
I0302 19:02:25.316130 22732373614720 run.py:483] Algo bellman_ford step 8626 current loss 0.468224, current_train_items 276064.
I0302 19:02:25.340054 22732373614720 run.py:483] Algo bellman_ford step 8627 current loss 0.580706, current_train_items 276096.
I0302 19:02:25.372630 22732373614720 run.py:483] Algo bellman_ford step 8628 current loss 0.848198, current_train_items 276128.
I0302 19:02:25.405135 22732373614720 run.py:483] Algo bellman_ford step 8629 current loss 0.874015, current_train_items 276160.
I0302 19:02:25.424934 22732373614720 run.py:483] Algo bellman_ford step 8630 current loss 0.357745, current_train_items 276192.
I0302 19:02:25.441035 22732373614720 run.py:483] Algo bellman_ford step 8631 current loss 0.491405, current_train_items 276224.
I0302 19:02:25.464495 22732373614720 run.py:483] Algo bellman_ford step 8632 current loss 0.783530, current_train_items 276256.
I0302 19:02:25.495551 22732373614720 run.py:483] Algo bellman_ford step 8633 current loss 0.621593, current_train_items 276288.
I0302 19:02:25.528713 22732373614720 run.py:483] Algo bellman_ford step 8634 current loss 0.833201, current_train_items 276320.
I0302 19:02:25.548500 22732373614720 run.py:483] Algo bellman_ford step 8635 current loss 0.366739, current_train_items 276352.
I0302 19:02:25.564425 22732373614720 run.py:483] Algo bellman_ford step 8636 current loss 0.491949, current_train_items 276384.
I0302 19:02:25.588778 22732373614720 run.py:483] Algo bellman_ford step 8637 current loss 0.616442, current_train_items 276416.
I0302 19:02:25.619477 22732373614720 run.py:483] Algo bellman_ford step 8638 current loss 0.603335, current_train_items 276448.
I0302 19:02:25.653057 22732373614720 run.py:483] Algo bellman_ford step 8639 current loss 0.774980, current_train_items 276480.
I0302 19:02:25.672714 22732373614720 run.py:483] Algo bellman_ford step 8640 current loss 0.350625, current_train_items 276512.
I0302 19:02:25.689048 22732373614720 run.py:483] Algo bellman_ford step 8641 current loss 0.562416, current_train_items 276544.
I0302 19:02:25.712786 22732373614720 run.py:483] Algo bellman_ford step 8642 current loss 0.552469, current_train_items 276576.
I0302 19:02:25.744711 22732373614720 run.py:483] Algo bellman_ford step 8643 current loss 0.696592, current_train_items 276608.
I0302 19:02:25.777930 22732373614720 run.py:483] Algo bellman_ford step 8644 current loss 1.035645, current_train_items 276640.
I0302 19:02:25.797420 22732373614720 run.py:483] Algo bellman_ford step 8645 current loss 0.417132, current_train_items 276672.
I0302 19:02:25.813406 22732373614720 run.py:483] Algo bellman_ford step 8646 current loss 0.442660, current_train_items 276704.
I0302 19:02:25.837107 22732373614720 run.py:483] Algo bellman_ford step 8647 current loss 0.691145, current_train_items 276736.
I0302 19:02:25.869563 22732373614720 run.py:483] Algo bellman_ford step 8648 current loss 0.695303, current_train_items 276768.
I0302 19:02:25.902465 22732373614720 run.py:483] Algo bellman_ford step 8649 current loss 0.797905, current_train_items 276800.
I0302 19:02:25.921766 22732373614720 run.py:483] Algo bellman_ford step 8650 current loss 0.470689, current_train_items 276832.
I0302 19:02:25.929775 22732373614720 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0302 19:02:25.929886 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:02:25.946799 22732373614720 run.py:483] Algo bellman_ford step 8651 current loss 0.437715, current_train_items 276864.
I0302 19:02:25.970763 22732373614720 run.py:483] Algo bellman_ford step 8652 current loss 0.615431, current_train_items 276896.
I0302 19:02:26.002742 22732373614720 run.py:483] Algo bellman_ford step 8653 current loss 0.723128, current_train_items 276928.
I0302 19:02:26.037113 22732373614720 run.py:483] Algo bellman_ford step 8654 current loss 0.905106, current_train_items 276960.
I0302 19:02:26.057266 22732373614720 run.py:483] Algo bellman_ford step 8655 current loss 0.296274, current_train_items 276992.
I0302 19:02:26.072857 22732373614720 run.py:483] Algo bellman_ford step 8656 current loss 0.499246, current_train_items 277024.
I0302 19:02:26.097425 22732373614720 run.py:483] Algo bellman_ford step 8657 current loss 0.710217, current_train_items 277056.
I0302 19:02:26.128933 22732373614720 run.py:483] Algo bellman_ford step 8658 current loss 0.689495, current_train_items 277088.
I0302 19:02:26.161775 22732373614720 run.py:483] Algo bellman_ford step 8659 current loss 0.779118, current_train_items 277120.
I0302 19:02:26.181860 22732373614720 run.py:483] Algo bellman_ford step 8660 current loss 0.331623, current_train_items 277152.
I0302 19:02:26.197650 22732373614720 run.py:483] Algo bellman_ford step 8661 current loss 0.454482, current_train_items 277184.
I0302 19:02:26.220486 22732373614720 run.py:483] Algo bellman_ford step 8662 current loss 0.604813, current_train_items 277216.
I0302 19:02:26.251099 22732373614720 run.py:483] Algo bellman_ford step 8663 current loss 0.700830, current_train_items 277248.
I0302 19:02:26.285564 22732373614720 run.py:483] Algo bellman_ford step 8664 current loss 0.767130, current_train_items 277280.
I0302 19:02:26.305107 22732373614720 run.py:483] Algo bellman_ford step 8665 current loss 0.361795, current_train_items 277312.
I0302 19:02:26.320575 22732373614720 run.py:483] Algo bellman_ford step 8666 current loss 0.394745, current_train_items 277344.
I0302 19:02:26.344464 22732373614720 run.py:483] Algo bellman_ford step 8667 current loss 0.587758, current_train_items 277376.
I0302 19:02:26.374936 22732373614720 run.py:483] Algo bellman_ford step 8668 current loss 0.694471, current_train_items 277408.
I0302 19:02:26.407343 22732373614720 run.py:483] Algo bellman_ford step 8669 current loss 0.852568, current_train_items 277440.
I0302 19:02:26.426984 22732373614720 run.py:483] Algo bellman_ford step 8670 current loss 0.347862, current_train_items 277472.
I0302 19:02:26.443212 22732373614720 run.py:483] Algo bellman_ford step 8671 current loss 0.540504, current_train_items 277504.
I0302 19:02:26.465995 22732373614720 run.py:483] Algo bellman_ford step 8672 current loss 0.647725, current_train_items 277536.
I0302 19:02:26.496773 22732373614720 run.py:483] Algo bellman_ford step 8673 current loss 0.709336, current_train_items 277568.
I0302 19:02:26.527960 22732373614720 run.py:483] Algo bellman_ford step 8674 current loss 0.885372, current_train_items 277600.
I0302 19:02:26.547506 22732373614720 run.py:483] Algo bellman_ford step 8675 current loss 0.370883, current_train_items 277632.
I0302 19:02:26.563544 22732373614720 run.py:483] Algo bellman_ford step 8676 current loss 0.589953, current_train_items 277664.
I0302 19:02:26.587116 22732373614720 run.py:483] Algo bellman_ford step 8677 current loss 0.717771, current_train_items 277696.
I0302 19:02:26.618628 22732373614720 run.py:483] Algo bellman_ford step 8678 current loss 0.712259, current_train_items 277728.
I0302 19:02:26.651129 22732373614720 run.py:483] Algo bellman_ford step 8679 current loss 0.842280, current_train_items 277760.
I0302 19:02:26.670312 22732373614720 run.py:483] Algo bellman_ford step 8680 current loss 0.377825, current_train_items 277792.
I0302 19:02:26.686549 22732373614720 run.py:483] Algo bellman_ford step 8681 current loss 0.588995, current_train_items 277824.
I0302 19:02:26.710309 22732373614720 run.py:483] Algo bellman_ford step 8682 current loss 0.643023, current_train_items 277856.
I0302 19:02:26.741306 22732373614720 run.py:483] Algo bellman_ford step 8683 current loss 0.641537, current_train_items 277888.
I0302 19:02:26.773687 22732373614720 run.py:483] Algo bellman_ford step 8684 current loss 0.857640, current_train_items 277920.
I0302 19:02:26.793609 22732373614720 run.py:483] Algo bellman_ford step 8685 current loss 0.358032, current_train_items 277952.
I0302 19:02:26.809456 22732373614720 run.py:483] Algo bellman_ford step 8686 current loss 0.547752, current_train_items 277984.
I0302 19:02:26.833658 22732373614720 run.py:483] Algo bellman_ford step 8687 current loss 0.749388, current_train_items 278016.
I0302 19:02:26.865140 22732373614720 run.py:483] Algo bellman_ford step 8688 current loss 0.748467, current_train_items 278048.
I0302 19:02:26.897259 22732373614720 run.py:483] Algo bellman_ford step 8689 current loss 0.796449, current_train_items 278080.
I0302 19:02:26.916751 22732373614720 run.py:483] Algo bellman_ford step 8690 current loss 0.294400, current_train_items 278112.
I0302 19:02:26.933120 22732373614720 run.py:483] Algo bellman_ford step 8691 current loss 0.484004, current_train_items 278144.
I0302 19:02:26.956446 22732373614720 run.py:483] Algo bellman_ford step 8692 current loss 0.655504, current_train_items 278176.
I0302 19:02:26.987536 22732373614720 run.py:483] Algo bellman_ford step 8693 current loss 0.686511, current_train_items 278208.
I0302 19:02:27.020367 22732373614720 run.py:483] Algo bellman_ford step 8694 current loss 0.903178, current_train_items 278240.
I0302 19:02:27.039804 22732373614720 run.py:483] Algo bellman_ford step 8695 current loss 0.329570, current_train_items 278272.
I0302 19:02:27.055651 22732373614720 run.py:483] Algo bellman_ford step 8696 current loss 0.474176, current_train_items 278304.
I0302 19:02:27.079352 22732373614720 run.py:483] Algo bellman_ford step 8697 current loss 0.636223, current_train_items 278336.
I0302 19:02:27.110931 22732373614720 run.py:483] Algo bellman_ford step 8698 current loss 0.664474, current_train_items 278368.
I0302 19:02:27.145642 22732373614720 run.py:483] Algo bellman_ford step 8699 current loss 0.782545, current_train_items 278400.
I0302 19:02:27.165410 22732373614720 run.py:483] Algo bellman_ford step 8700 current loss 0.310982, current_train_items 278432.
I0302 19:02:27.173409 22732373614720 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0302 19:02:27.173519 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:02:27.190512 22732373614720 run.py:483] Algo bellman_ford step 8701 current loss 0.570488, current_train_items 278464.
I0302 19:02:27.214464 22732373614720 run.py:483] Algo bellman_ford step 8702 current loss 0.667420, current_train_items 278496.
I0302 19:02:27.247145 22732373614720 run.py:483] Algo bellman_ford step 8703 current loss 0.803019, current_train_items 278528.
I0302 19:02:27.282152 22732373614720 run.py:483] Algo bellman_ford step 8704 current loss 0.816940, current_train_items 278560.
I0302 19:02:27.302722 22732373614720 run.py:483] Algo bellman_ford step 8705 current loss 0.331433, current_train_items 278592.
I0302 19:02:27.318356 22732373614720 run.py:483] Algo bellman_ford step 8706 current loss 0.537162, current_train_items 278624.
I0302 19:02:27.342006 22732373614720 run.py:483] Algo bellman_ford step 8707 current loss 0.649858, current_train_items 278656.
I0302 19:02:27.374199 22732373614720 run.py:483] Algo bellman_ford step 8708 current loss 0.749439, current_train_items 278688.
I0302 19:02:27.407471 22732373614720 run.py:483] Algo bellman_ford step 8709 current loss 0.913906, current_train_items 278720.
I0302 19:02:27.426915 22732373614720 run.py:483] Algo bellman_ford step 8710 current loss 0.357190, current_train_items 278752.
I0302 19:02:27.443019 22732373614720 run.py:483] Algo bellman_ford step 8711 current loss 0.574637, current_train_items 278784.
I0302 19:02:27.466541 22732373614720 run.py:483] Algo bellman_ford step 8712 current loss 0.659295, current_train_items 278816.
I0302 19:02:27.499865 22732373614720 run.py:483] Algo bellman_ford step 8713 current loss 0.861046, current_train_items 278848.
I0302 19:02:27.531831 22732373614720 run.py:483] Algo bellman_ford step 8714 current loss 0.775967, current_train_items 278880.
I0302 19:02:27.551213 22732373614720 run.py:483] Algo bellman_ford step 8715 current loss 0.329890, current_train_items 278912.
I0302 19:02:27.567508 22732373614720 run.py:483] Algo bellman_ford step 8716 current loss 0.525498, current_train_items 278944.
I0302 19:02:27.590780 22732373614720 run.py:483] Algo bellman_ford step 8717 current loss 0.634975, current_train_items 278976.
I0302 19:02:27.621279 22732373614720 run.py:483] Algo bellman_ford step 8718 current loss 0.634701, current_train_items 279008.
I0302 19:02:27.653183 22732373614720 run.py:483] Algo bellman_ford step 8719 current loss 0.905519, current_train_items 279040.
I0302 19:02:27.673019 22732373614720 run.py:483] Algo bellman_ford step 8720 current loss 0.394555, current_train_items 279072.
I0302 19:02:27.689311 22732373614720 run.py:483] Algo bellman_ford step 8721 current loss 0.554992, current_train_items 279104.
I0302 19:02:27.713629 22732373614720 run.py:483] Algo bellman_ford step 8722 current loss 0.629897, current_train_items 279136.
I0302 19:02:27.744378 22732373614720 run.py:483] Algo bellman_ford step 8723 current loss 0.651346, current_train_items 279168.
I0302 19:02:27.776587 22732373614720 run.py:483] Algo bellman_ford step 8724 current loss 0.817101, current_train_items 279200.
I0302 19:02:27.796413 22732373614720 run.py:483] Algo bellman_ford step 8725 current loss 0.371117, current_train_items 279232.
I0302 19:02:27.812512 22732373614720 run.py:483] Algo bellman_ford step 8726 current loss 0.611004, current_train_items 279264.
I0302 19:02:27.836469 22732373614720 run.py:483] Algo bellman_ford step 8727 current loss 0.622674, current_train_items 279296.
I0302 19:02:27.867490 22732373614720 run.py:483] Algo bellman_ford step 8728 current loss 0.650552, current_train_items 279328.
I0302 19:02:27.900100 22732373614720 run.py:483] Algo bellman_ford step 8729 current loss 0.848643, current_train_items 279360.
I0302 19:02:27.919777 22732373614720 run.py:483] Algo bellman_ford step 8730 current loss 0.346409, current_train_items 279392.
I0302 19:02:27.935259 22732373614720 run.py:483] Algo bellman_ford step 8731 current loss 0.401238, current_train_items 279424.
I0302 19:02:27.958862 22732373614720 run.py:483] Algo bellman_ford step 8732 current loss 0.665532, current_train_items 279456.
I0302 19:02:27.990464 22732373614720 run.py:483] Algo bellman_ford step 8733 current loss 0.692517, current_train_items 279488.
I0302 19:02:28.025062 22732373614720 run.py:483] Algo bellman_ford step 8734 current loss 0.702394, current_train_items 279520.
I0302 19:02:28.044572 22732373614720 run.py:483] Algo bellman_ford step 8735 current loss 0.427303, current_train_items 279552.
I0302 19:02:28.060433 22732373614720 run.py:483] Algo bellman_ford step 8736 current loss 0.535868, current_train_items 279584.
I0302 19:02:28.084949 22732373614720 run.py:483] Algo bellman_ford step 8737 current loss 0.599540, current_train_items 279616.
I0302 19:02:28.115742 22732373614720 run.py:483] Algo bellman_ford step 8738 current loss 0.700111, current_train_items 279648.
I0302 19:02:28.149583 22732373614720 run.py:483] Algo bellman_ford step 8739 current loss 0.974746, current_train_items 279680.
I0302 19:02:28.169439 22732373614720 run.py:483] Algo bellman_ford step 8740 current loss 0.409090, current_train_items 279712.
I0302 19:02:28.185270 22732373614720 run.py:483] Algo bellman_ford step 8741 current loss 0.449420, current_train_items 279744.
I0302 19:02:28.209232 22732373614720 run.py:483] Algo bellman_ford step 8742 current loss 0.756038, current_train_items 279776.
I0302 19:02:28.241172 22732373614720 run.py:483] Algo bellman_ford step 8743 current loss 0.738026, current_train_items 279808.
I0302 19:02:28.274682 22732373614720 run.py:483] Algo bellman_ford step 8744 current loss 0.821918, current_train_items 279840.
I0302 19:02:28.294565 22732373614720 run.py:483] Algo bellman_ford step 8745 current loss 0.333823, current_train_items 279872.
I0302 19:02:28.310874 22732373614720 run.py:483] Algo bellman_ford step 8746 current loss 0.598541, current_train_items 279904.
I0302 19:02:28.334252 22732373614720 run.py:483] Algo bellman_ford step 8747 current loss 0.797722, current_train_items 279936.
I0302 19:02:28.366874 22732373614720 run.py:483] Algo bellman_ford step 8748 current loss 0.835486, current_train_items 279968.
I0302 19:02:28.401715 22732373614720 run.py:483] Algo bellman_ford step 8749 current loss 1.030020, current_train_items 280000.
I0302 19:02:28.421463 22732373614720 run.py:483] Algo bellman_ford step 8750 current loss 0.362143, current_train_items 280032.
I0302 19:02:28.429558 22732373614720 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.8916015625, 'score': 0.8916015625, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0302 19:02:28.429670 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.892, val scores are: bellman_ford: 0.892
I0302 19:02:28.446643 22732373614720 run.py:483] Algo bellman_ford step 8751 current loss 0.431747, current_train_items 280064.
I0302 19:02:28.470475 22732373614720 run.py:483] Algo bellman_ford step 8752 current loss 0.650520, current_train_items 280096.
I0302 19:02:28.502827 22732373614720 run.py:483] Algo bellman_ford step 8753 current loss 0.795752, current_train_items 280128.
I0302 19:02:28.536909 22732373614720 run.py:483] Algo bellman_ford step 8754 current loss 0.815514, current_train_items 280160.
I0302 19:02:28.557176 22732373614720 run.py:483] Algo bellman_ford step 8755 current loss 0.361746, current_train_items 280192.
I0302 19:02:28.572896 22732373614720 run.py:483] Algo bellman_ford step 8756 current loss 0.526801, current_train_items 280224.
I0302 19:02:28.597909 22732373614720 run.py:483] Algo bellman_ford step 8757 current loss 0.737885, current_train_items 280256.
I0302 19:02:28.629731 22732373614720 run.py:483] Algo bellman_ford step 8758 current loss 0.678517, current_train_items 280288.
I0302 19:02:28.663252 22732373614720 run.py:483] Algo bellman_ford step 8759 current loss 0.843524, current_train_items 280320.
I0302 19:02:28.683459 22732373614720 run.py:483] Algo bellman_ford step 8760 current loss 0.370360, current_train_items 280352.
I0302 19:02:28.699744 22732373614720 run.py:483] Algo bellman_ford step 8761 current loss 0.580610, current_train_items 280384.
I0302 19:02:28.723858 22732373614720 run.py:483] Algo bellman_ford step 8762 current loss 0.639730, current_train_items 280416.
I0302 19:02:28.754721 22732373614720 run.py:483] Algo bellman_ford step 8763 current loss 0.639077, current_train_items 280448.
I0302 19:02:28.789793 22732373614720 run.py:483] Algo bellman_ford step 8764 current loss 0.859296, current_train_items 280480.
I0302 19:02:28.809140 22732373614720 run.py:483] Algo bellman_ford step 8765 current loss 0.348010, current_train_items 280512.
I0302 19:02:28.825594 22732373614720 run.py:483] Algo bellman_ford step 8766 current loss 0.494501, current_train_items 280544.
I0302 19:02:28.848604 22732373614720 run.py:483] Algo bellman_ford step 8767 current loss 0.539082, current_train_items 280576.
I0302 19:02:28.879121 22732373614720 run.py:483] Algo bellman_ford step 8768 current loss 0.662106, current_train_items 280608.
I0302 19:02:28.910775 22732373614720 run.py:483] Algo bellman_ford step 8769 current loss 0.682495, current_train_items 280640.
I0302 19:02:28.931018 22732373614720 run.py:483] Algo bellman_ford step 8770 current loss 0.327913, current_train_items 280672.
I0302 19:02:28.947734 22732373614720 run.py:483] Algo bellman_ford step 8771 current loss 0.538265, current_train_items 280704.
I0302 19:02:28.971443 22732373614720 run.py:483] Algo bellman_ford step 8772 current loss 0.682370, current_train_items 280736.
I0302 19:02:29.003166 22732373614720 run.py:483] Algo bellman_ford step 8773 current loss 0.876111, current_train_items 280768.
I0302 19:02:29.035827 22732373614720 run.py:483] Algo bellman_ford step 8774 current loss 0.845232, current_train_items 280800.
I0302 19:02:29.056009 22732373614720 run.py:483] Algo bellman_ford step 8775 current loss 0.406335, current_train_items 280832.
I0302 19:02:29.072150 22732373614720 run.py:483] Algo bellman_ford step 8776 current loss 0.508830, current_train_items 280864.
I0302 19:02:29.095207 22732373614720 run.py:483] Algo bellman_ford step 8777 current loss 0.623316, current_train_items 280896.
I0302 19:02:29.126462 22732373614720 run.py:483] Algo bellman_ford step 8778 current loss 0.596163, current_train_items 280928.
I0302 19:02:29.160493 22732373614720 run.py:483] Algo bellman_ford step 8779 current loss 0.711200, current_train_items 280960.
I0302 19:02:29.179977 22732373614720 run.py:483] Algo bellman_ford step 8780 current loss 0.372346, current_train_items 280992.
I0302 19:02:29.195940 22732373614720 run.py:483] Algo bellman_ford step 8781 current loss 0.632585, current_train_items 281024.
I0302 19:02:29.219368 22732373614720 run.py:483] Algo bellman_ford step 8782 current loss 0.638771, current_train_items 281056.
I0302 19:02:29.251272 22732373614720 run.py:483] Algo bellman_ford step 8783 current loss 0.741266, current_train_items 281088.
I0302 19:02:29.285024 22732373614720 run.py:483] Algo bellman_ford step 8784 current loss 0.877894, current_train_items 281120.
I0302 19:02:29.304828 22732373614720 run.py:483] Algo bellman_ford step 8785 current loss 0.348123, current_train_items 281152.
I0302 19:02:29.320922 22732373614720 run.py:483] Algo bellman_ford step 8786 current loss 0.495035, current_train_items 281184.
I0302 19:02:29.345181 22732373614720 run.py:483] Algo bellman_ford step 8787 current loss 0.680687, current_train_items 281216.
I0302 19:02:29.376314 22732373614720 run.py:483] Algo bellman_ford step 8788 current loss 0.781067, current_train_items 281248.
I0302 19:02:29.409638 22732373614720 run.py:483] Algo bellman_ford step 8789 current loss 0.974789, current_train_items 281280.
I0302 19:02:29.429902 22732373614720 run.py:483] Algo bellman_ford step 8790 current loss 0.372563, current_train_items 281312.
I0302 19:02:29.446561 22732373614720 run.py:483] Algo bellman_ford step 8791 current loss 0.627526, current_train_items 281344.
I0302 19:02:29.469850 22732373614720 run.py:483] Algo bellman_ford step 8792 current loss 0.695834, current_train_items 281376.
I0302 19:02:29.501231 22732373614720 run.py:483] Algo bellman_ford step 8793 current loss 0.693692, current_train_items 281408.
I0302 19:02:29.535055 22732373614720 run.py:483] Algo bellman_ford step 8794 current loss 0.957076, current_train_items 281440.
I0302 19:02:29.554808 22732373614720 run.py:483] Algo bellman_ford step 8795 current loss 0.340542, current_train_items 281472.
I0302 19:02:29.570899 22732373614720 run.py:483] Algo bellman_ford step 8796 current loss 0.515095, current_train_items 281504.
I0302 19:02:29.594530 22732373614720 run.py:483] Algo bellman_ford step 8797 current loss 0.634061, current_train_items 281536.
I0302 19:02:29.626345 22732373614720 run.py:483] Algo bellman_ford step 8798 current loss 0.776112, current_train_items 281568.
I0302 19:02:29.661044 22732373614720 run.py:483] Algo bellman_ford step 8799 current loss 0.886641, current_train_items 281600.
I0302 19:02:29.681190 22732373614720 run.py:483] Algo bellman_ford step 8800 current loss 0.304806, current_train_items 281632.
I0302 19:02:29.689109 22732373614720 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0302 19:02:29.689231 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 19:02:29.705732 22732373614720 run.py:483] Algo bellman_ford step 8801 current loss 0.496000, current_train_items 281664.
I0302 19:02:29.729791 22732373614720 run.py:483] Algo bellman_ford step 8802 current loss 0.678460, current_train_items 281696.
I0302 19:02:29.762423 22732373614720 run.py:483] Algo bellman_ford step 8803 current loss 0.734569, current_train_items 281728.
I0302 19:02:29.795837 22732373614720 run.py:483] Algo bellman_ford step 8804 current loss 0.775112, current_train_items 281760.
I0302 19:02:29.815657 22732373614720 run.py:483] Algo bellman_ford step 8805 current loss 0.346827, current_train_items 281792.
I0302 19:02:29.831365 22732373614720 run.py:483] Algo bellman_ford step 8806 current loss 0.479041, current_train_items 281824.
I0302 19:02:29.855475 22732373614720 run.py:483] Algo bellman_ford step 8807 current loss 0.658311, current_train_items 281856.
I0302 19:02:29.886203 22732373614720 run.py:483] Algo bellman_ford step 8808 current loss 0.752874, current_train_items 281888.
I0302 19:02:29.921362 22732373614720 run.py:483] Algo bellman_ford step 8809 current loss 0.863021, current_train_items 281920.
I0302 19:02:29.940593 22732373614720 run.py:483] Algo bellman_ford step 8810 current loss 0.448859, current_train_items 281952.
I0302 19:02:29.956781 22732373614720 run.py:483] Algo bellman_ford step 8811 current loss 0.547623, current_train_items 281984.
I0302 19:02:29.980211 22732373614720 run.py:483] Algo bellman_ford step 8812 current loss 0.617585, current_train_items 282016.
I0302 19:02:30.011924 22732373614720 run.py:483] Algo bellman_ford step 8813 current loss 0.667618, current_train_items 282048.
I0302 19:02:30.044901 22732373614720 run.py:483] Algo bellman_ford step 8814 current loss 0.818430, current_train_items 282080.
I0302 19:02:30.064165 22732373614720 run.py:483] Algo bellman_ford step 8815 current loss 0.371528, current_train_items 282112.
I0302 19:02:30.080195 22732373614720 run.py:483] Algo bellman_ford step 8816 current loss 0.531269, current_train_items 282144.
I0302 19:02:30.103389 22732373614720 run.py:483] Algo bellman_ford step 8817 current loss 0.550792, current_train_items 282176.
I0302 19:02:30.133707 22732373614720 run.py:483] Algo bellman_ford step 8818 current loss 0.698654, current_train_items 282208.
I0302 19:02:30.170410 22732373614720 run.py:483] Algo bellman_ford step 8819 current loss 0.878343, current_train_items 282240.
I0302 19:02:30.189845 22732373614720 run.py:483] Algo bellman_ford step 8820 current loss 0.378951, current_train_items 282272.
I0302 19:02:30.205677 22732373614720 run.py:483] Algo bellman_ford step 8821 current loss 0.491174, current_train_items 282304.
I0302 19:02:30.228825 22732373614720 run.py:483] Algo bellman_ford step 8822 current loss 0.596566, current_train_items 282336.
I0302 19:02:30.260529 22732373614720 run.py:483] Algo bellman_ford step 8823 current loss 0.750822, current_train_items 282368.
I0302 19:02:30.292586 22732373614720 run.py:483] Algo bellman_ford step 8824 current loss 0.742079, current_train_items 282400.
I0302 19:02:30.311893 22732373614720 run.py:483] Algo bellman_ford step 8825 current loss 0.317772, current_train_items 282432.
I0302 19:02:30.328099 22732373614720 run.py:483] Algo bellman_ford step 8826 current loss 0.529380, current_train_items 282464.
I0302 19:02:30.351589 22732373614720 run.py:483] Algo bellman_ford step 8827 current loss 0.649156, current_train_items 282496.
I0302 19:02:30.382910 22732373614720 run.py:483] Algo bellman_ford step 8828 current loss 0.714791, current_train_items 282528.
I0302 19:02:30.415468 22732373614720 run.py:483] Algo bellman_ford step 8829 current loss 0.884682, current_train_items 282560.
I0302 19:02:30.434807 22732373614720 run.py:483] Algo bellman_ford step 8830 current loss 0.359830, current_train_items 282592.
I0302 19:02:30.450541 22732373614720 run.py:483] Algo bellman_ford step 8831 current loss 0.531847, current_train_items 282624.
I0302 19:02:30.474467 22732373614720 run.py:483] Algo bellman_ford step 8832 current loss 0.717077, current_train_items 282656.
I0302 19:02:30.504217 22732373614720 run.py:483] Algo bellman_ford step 8833 current loss 0.631811, current_train_items 282688.
I0302 19:02:30.535878 22732373614720 run.py:483] Algo bellman_ford step 8834 current loss 0.764882, current_train_items 282720.
I0302 19:02:30.555839 22732373614720 run.py:483] Algo bellman_ford step 8835 current loss 0.329306, current_train_items 282752.
I0302 19:02:30.571471 22732373614720 run.py:483] Algo bellman_ford step 8836 current loss 0.500380, current_train_items 282784.
I0302 19:02:30.595377 22732373614720 run.py:483] Algo bellman_ford step 8837 current loss 0.646225, current_train_items 282816.
I0302 19:02:30.626593 22732373614720 run.py:483] Algo bellman_ford step 8838 current loss 0.732466, current_train_items 282848.
I0302 19:02:30.659799 22732373614720 run.py:483] Algo bellman_ford step 8839 current loss 0.794127, current_train_items 282880.
I0302 19:02:30.679405 22732373614720 run.py:483] Algo bellman_ford step 8840 current loss 0.440813, current_train_items 282912.
I0302 19:02:30.695597 22732373614720 run.py:483] Algo bellman_ford step 8841 current loss 0.565384, current_train_items 282944.
I0302 19:02:30.719130 22732373614720 run.py:483] Algo bellman_ford step 8842 current loss 0.656981, current_train_items 282976.
I0302 19:02:30.749981 22732373614720 run.py:483] Algo bellman_ford step 8843 current loss 0.767875, current_train_items 283008.
I0302 19:02:30.784748 22732373614720 run.py:483] Algo bellman_ford step 8844 current loss 0.905793, current_train_items 283040.
I0302 19:02:30.804149 22732373614720 run.py:483] Algo bellman_ford step 8845 current loss 0.408744, current_train_items 283072.
I0302 19:02:30.819987 22732373614720 run.py:483] Algo bellman_ford step 8846 current loss 0.433368, current_train_items 283104.
I0302 19:02:30.843247 22732373614720 run.py:483] Algo bellman_ford step 8847 current loss 0.615834, current_train_items 283136.
I0302 19:02:30.873055 22732373614720 run.py:483] Algo bellman_ford step 8848 current loss 0.654779, current_train_items 283168.
I0302 19:02:30.906173 22732373614720 run.py:483] Algo bellman_ford step 8849 current loss 0.758966, current_train_items 283200.
I0302 19:02:30.925237 22732373614720 run.py:483] Algo bellman_ford step 8850 current loss 0.361240, current_train_items 283232.
I0302 19:02:30.933413 22732373614720 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0302 19:02:30.933522 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:02:30.950533 22732373614720 run.py:483] Algo bellman_ford step 8851 current loss 0.474582, current_train_items 283264.
I0302 19:02:30.975038 22732373614720 run.py:483] Algo bellman_ford step 8852 current loss 0.590351, current_train_items 283296.
I0302 19:02:31.006770 22732373614720 run.py:483] Algo bellman_ford step 8853 current loss 0.688731, current_train_items 283328.
I0302 19:02:31.040376 22732373614720 run.py:483] Algo bellman_ford step 8854 current loss 0.843892, current_train_items 283360.
I0302 19:02:31.060947 22732373614720 run.py:483] Algo bellman_ford step 8855 current loss 0.361716, current_train_items 283392.
I0302 19:02:31.076816 22732373614720 run.py:483] Algo bellman_ford step 8856 current loss 0.453061, current_train_items 283424.
I0302 19:02:31.100440 22732373614720 run.py:483] Algo bellman_ford step 8857 current loss 0.635770, current_train_items 283456.
I0302 19:02:31.132560 22732373614720 run.py:483] Algo bellman_ford step 8858 current loss 0.757173, current_train_items 283488.
I0302 19:02:31.164255 22732373614720 run.py:483] Algo bellman_ford step 8859 current loss 0.783278, current_train_items 283520.
I0302 19:02:31.184459 22732373614720 run.py:483] Algo bellman_ford step 8860 current loss 0.458901, current_train_items 283552.
I0302 19:02:31.200865 22732373614720 run.py:483] Algo bellman_ford step 8861 current loss 0.516555, current_train_items 283584.
I0302 19:02:31.224682 22732373614720 run.py:483] Algo bellman_ford step 8862 current loss 0.668840, current_train_items 283616.
I0302 19:02:31.255191 22732373614720 run.py:483] Algo bellman_ford step 8863 current loss 0.630958, current_train_items 283648.
I0302 19:02:31.287457 22732373614720 run.py:483] Algo bellman_ford step 8864 current loss 0.804728, current_train_items 283680.
I0302 19:02:31.307628 22732373614720 run.py:483] Algo bellman_ford step 8865 current loss 0.345814, current_train_items 283712.
I0302 19:02:31.323602 22732373614720 run.py:483] Algo bellman_ford step 8866 current loss 0.490259, current_train_items 283744.
I0302 19:02:31.346403 22732373614720 run.py:483] Algo bellman_ford step 8867 current loss 0.620814, current_train_items 283776.
I0302 19:02:31.378107 22732373614720 run.py:483] Algo bellman_ford step 8868 current loss 0.714819, current_train_items 283808.
I0302 19:02:31.410952 22732373614720 run.py:483] Algo bellman_ford step 8869 current loss 0.747058, current_train_items 283840.
I0302 19:02:31.431106 22732373614720 run.py:483] Algo bellman_ford step 8870 current loss 0.355673, current_train_items 283872.
I0302 19:02:31.447204 22732373614720 run.py:483] Algo bellman_ford step 8871 current loss 0.474515, current_train_items 283904.
I0302 19:02:31.471061 22732373614720 run.py:483] Algo bellman_ford step 8872 current loss 0.626714, current_train_items 283936.
I0302 19:02:31.502420 22732373614720 run.py:483] Algo bellman_ford step 8873 current loss 0.637543, current_train_items 283968.
I0302 19:02:31.536720 22732373614720 run.py:483] Algo bellman_ford step 8874 current loss 0.841322, current_train_items 284000.
I0302 19:02:31.556941 22732373614720 run.py:483] Algo bellman_ford step 8875 current loss 0.375179, current_train_items 284032.
I0302 19:02:31.573264 22732373614720 run.py:483] Algo bellman_ford step 8876 current loss 0.457480, current_train_items 284064.
I0302 19:02:31.596224 22732373614720 run.py:483] Algo bellman_ford step 8877 current loss 0.569813, current_train_items 284096.
I0302 19:02:31.627408 22732373614720 run.py:483] Algo bellman_ford step 8878 current loss 0.670316, current_train_items 284128.
I0302 19:02:31.661324 22732373614720 run.py:483] Algo bellman_ford step 8879 current loss 0.831695, current_train_items 284160.
I0302 19:02:31.681008 22732373614720 run.py:483] Algo bellman_ford step 8880 current loss 0.348481, current_train_items 284192.
I0302 19:02:31.697033 22732373614720 run.py:483] Algo bellman_ford step 8881 current loss 0.488118, current_train_items 284224.
I0302 19:02:31.720087 22732373614720 run.py:483] Algo bellman_ford step 8882 current loss 0.569088, current_train_items 284256.
I0302 19:02:31.752492 22732373614720 run.py:483] Algo bellman_ford step 8883 current loss 0.700728, current_train_items 284288.
I0302 19:02:31.786030 22732373614720 run.py:483] Algo bellman_ford step 8884 current loss 0.798569, current_train_items 284320.
I0302 19:02:31.806290 22732373614720 run.py:483] Algo bellman_ford step 8885 current loss 0.376634, current_train_items 284352.
I0302 19:02:31.822883 22732373614720 run.py:483] Algo bellman_ford step 8886 current loss 0.496628, current_train_items 284384.
I0302 19:02:31.846689 22732373614720 run.py:483] Algo bellman_ford step 8887 current loss 0.608160, current_train_items 284416.
I0302 19:02:31.877354 22732373614720 run.py:483] Algo bellman_ford step 8888 current loss 0.766888, current_train_items 284448.
I0302 19:02:31.911071 22732373614720 run.py:483] Algo bellman_ford step 8889 current loss 0.906918, current_train_items 284480.
I0302 19:02:31.930763 22732373614720 run.py:483] Algo bellman_ford step 8890 current loss 0.349786, current_train_items 284512.
I0302 19:02:31.946635 22732373614720 run.py:483] Algo bellman_ford step 8891 current loss 0.526681, current_train_items 284544.
I0302 19:02:31.970229 22732373614720 run.py:483] Algo bellman_ford step 8892 current loss 0.577990, current_train_items 284576.
I0302 19:02:32.002856 22732373614720 run.py:483] Algo bellman_ford step 8893 current loss 0.770723, current_train_items 284608.
I0302 19:02:32.036276 22732373614720 run.py:483] Algo bellman_ford step 8894 current loss 0.878248, current_train_items 284640.
I0302 19:02:32.055898 22732373614720 run.py:483] Algo bellman_ford step 8895 current loss 0.376862, current_train_items 284672.
I0302 19:02:32.071816 22732373614720 run.py:483] Algo bellman_ford step 8896 current loss 0.552656, current_train_items 284704.
I0302 19:02:32.094514 22732373614720 run.py:483] Algo bellman_ford step 8897 current loss 0.547601, current_train_items 284736.
I0302 19:02:32.126724 22732373614720 run.py:483] Algo bellman_ford step 8898 current loss 0.729563, current_train_items 284768.
I0302 19:02:32.161545 22732373614720 run.py:483] Algo bellman_ford step 8899 current loss 0.760721, current_train_items 284800.
I0302 19:02:32.181504 22732373614720 run.py:483] Algo bellman_ford step 8900 current loss 0.319502, current_train_items 284832.
I0302 19:02:32.189391 22732373614720 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.9365234375, 'score': 0.9365234375, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0302 19:02:32.189500 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.937, val scores are: bellman_ford: 0.937
I0302 19:02:32.206250 22732373614720 run.py:483] Algo bellman_ford step 8901 current loss 0.462075, current_train_items 284864.
I0302 19:02:32.230209 22732373614720 run.py:483] Algo bellman_ford step 8902 current loss 0.626203, current_train_items 284896.
I0302 19:02:32.260880 22732373614720 run.py:483] Algo bellman_ford step 8903 current loss 0.691869, current_train_items 284928.
I0302 19:02:32.296175 22732373614720 run.py:483] Algo bellman_ford step 8904 current loss 0.861455, current_train_items 284960.
I0302 19:02:32.316232 22732373614720 run.py:483] Algo bellman_ford step 8905 current loss 0.359551, current_train_items 284992.
I0302 19:02:32.332042 22732373614720 run.py:483] Algo bellman_ford step 8906 current loss 0.451642, current_train_items 285024.
I0302 19:02:32.355808 22732373614720 run.py:483] Algo bellman_ford step 8907 current loss 0.549162, current_train_items 285056.
I0302 19:02:32.386610 22732373614720 run.py:483] Algo bellman_ford step 8908 current loss 0.657560, current_train_items 285088.
I0302 19:02:32.418389 22732373614720 run.py:483] Algo bellman_ford step 8909 current loss 0.767438, current_train_items 285120.
I0302 19:02:32.438248 22732373614720 run.py:483] Algo bellman_ford step 8910 current loss 0.388450, current_train_items 285152.
I0302 19:02:32.454073 22732373614720 run.py:483] Algo bellman_ford step 8911 current loss 0.444834, current_train_items 285184.
I0302 19:02:32.477987 22732373614720 run.py:483] Algo bellman_ford step 8912 current loss 0.644008, current_train_items 285216.
I0302 19:02:32.508200 22732373614720 run.py:483] Algo bellman_ford step 8913 current loss 0.741671, current_train_items 285248.
I0302 19:02:32.542416 22732373614720 run.py:483] Algo bellman_ford step 8914 current loss 1.038465, current_train_items 285280.
I0302 19:02:32.562319 22732373614720 run.py:483] Algo bellman_ford step 8915 current loss 0.349301, current_train_items 285312.
I0302 19:02:32.577978 22732373614720 run.py:483] Algo bellman_ford step 8916 current loss 0.472905, current_train_items 285344.
I0302 19:02:32.601905 22732373614720 run.py:483] Algo bellman_ford step 8917 current loss 0.679046, current_train_items 285376.
I0302 19:02:32.633145 22732373614720 run.py:483] Algo bellman_ford step 8918 current loss 0.648781, current_train_items 285408.
I0302 19:02:32.665931 22732373614720 run.py:483] Algo bellman_ford step 8919 current loss 0.809820, current_train_items 285440.
I0302 19:02:32.685676 22732373614720 run.py:483] Algo bellman_ford step 8920 current loss 0.364714, current_train_items 285472.
I0302 19:02:32.701837 22732373614720 run.py:483] Algo bellman_ford step 8921 current loss 0.543352, current_train_items 285504.
I0302 19:02:32.726480 22732373614720 run.py:483] Algo bellman_ford step 8922 current loss 0.613470, current_train_items 285536.
I0302 19:02:32.757282 22732373614720 run.py:483] Algo bellman_ford step 8923 current loss 0.608102, current_train_items 285568.
I0302 19:02:32.791272 22732373614720 run.py:483] Algo bellman_ford step 8924 current loss 0.817096, current_train_items 285600.
I0302 19:02:32.810640 22732373614720 run.py:483] Algo bellman_ford step 8925 current loss 0.303848, current_train_items 285632.
I0302 19:02:32.826350 22732373614720 run.py:483] Algo bellman_ford step 8926 current loss 0.723381, current_train_items 285664.
I0302 19:02:32.850375 22732373614720 run.py:483] Algo bellman_ford step 8927 current loss 0.663290, current_train_items 285696.
I0302 19:02:32.880652 22732373614720 run.py:483] Algo bellman_ford step 8928 current loss 0.633875, current_train_items 285728.
I0302 19:02:32.914127 22732373614720 run.py:483] Algo bellman_ford step 8929 current loss 0.833616, current_train_items 285760.
I0302 19:02:32.934163 22732373614720 run.py:483] Algo bellman_ford step 8930 current loss 0.330879, current_train_items 285792.
I0302 19:02:32.950767 22732373614720 run.py:483] Algo bellman_ford step 8931 current loss 0.527433, current_train_items 285824.
I0302 19:02:32.974187 22732373614720 run.py:483] Algo bellman_ford step 8932 current loss 0.588376, current_train_items 285856.
I0302 19:02:33.006910 22732373614720 run.py:483] Algo bellman_ford step 8933 current loss 0.743697, current_train_items 285888.
I0302 19:02:33.040917 22732373614720 run.py:483] Algo bellman_ford step 8934 current loss 0.796572, current_train_items 285920.
I0302 19:02:33.060659 22732373614720 run.py:483] Algo bellman_ford step 8935 current loss 0.317778, current_train_items 285952.
I0302 19:02:33.076471 22732373614720 run.py:483] Algo bellman_ford step 8936 current loss 0.487201, current_train_items 285984.
I0302 19:02:33.100314 22732373614720 run.py:483] Algo bellman_ford step 8937 current loss 0.646595, current_train_items 286016.
I0302 19:02:33.131313 22732373614720 run.py:483] Algo bellman_ford step 8938 current loss 0.863525, current_train_items 286048.
I0302 19:02:33.163573 22732373614720 run.py:483] Algo bellman_ford step 8939 current loss 0.849413, current_train_items 286080.
I0302 19:02:33.183082 22732373614720 run.py:483] Algo bellman_ford step 8940 current loss 0.354226, current_train_items 286112.
I0302 19:02:33.199011 22732373614720 run.py:483] Algo bellman_ford step 8941 current loss 0.470950, current_train_items 286144.
I0302 19:02:33.223215 22732373614720 run.py:483] Algo bellman_ford step 8942 current loss 0.670153, current_train_items 286176.
I0302 19:02:33.255654 22732373614720 run.py:483] Algo bellman_ford step 8943 current loss 0.770013, current_train_items 286208.
I0302 19:02:33.288970 22732373614720 run.py:483] Algo bellman_ford step 8944 current loss 0.809933, current_train_items 286240.
I0302 19:02:33.308583 22732373614720 run.py:483] Algo bellman_ford step 8945 current loss 0.345737, current_train_items 286272.
I0302 19:02:33.324596 22732373614720 run.py:483] Algo bellman_ford step 8946 current loss 0.474092, current_train_items 286304.
I0302 19:02:33.347491 22732373614720 run.py:483] Algo bellman_ford step 8947 current loss 0.625532, current_train_items 286336.
I0302 19:02:33.379018 22732373614720 run.py:483] Algo bellman_ford step 8948 current loss 0.818135, current_train_items 286368.
I0302 19:02:33.411678 22732373614720 run.py:483] Algo bellman_ford step 8949 current loss 0.769701, current_train_items 286400.
I0302 19:02:33.430899 22732373614720 run.py:483] Algo bellman_ford step 8950 current loss 0.378492, current_train_items 286432.
I0302 19:02:33.438888 22732373614720 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0302 19:02:33.438995 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:02:33.455968 22732373614720 run.py:483] Algo bellman_ford step 8951 current loss 0.513206, current_train_items 286464.
I0302 19:02:33.479430 22732373614720 run.py:483] Algo bellman_ford step 8952 current loss 0.589582, current_train_items 286496.
I0302 19:02:33.513548 22732373614720 run.py:483] Algo bellman_ford step 8953 current loss 0.791526, current_train_items 286528.
I0302 19:02:33.548195 22732373614720 run.py:483] Algo bellman_ford step 8954 current loss 0.783908, current_train_items 286560.
I0302 19:02:33.568034 22732373614720 run.py:483] Algo bellman_ford step 8955 current loss 0.372429, current_train_items 286592.
I0302 19:02:33.584211 22732373614720 run.py:483] Algo bellman_ford step 8956 current loss 0.633254, current_train_items 286624.
I0302 19:02:33.607955 22732373614720 run.py:483] Algo bellman_ford step 8957 current loss 0.705075, current_train_items 286656.
I0302 19:02:33.638941 22732373614720 run.py:483] Algo bellman_ford step 8958 current loss 0.726844, current_train_items 286688.
I0302 19:02:33.673033 22732373614720 run.py:483] Algo bellman_ford step 8959 current loss 0.875870, current_train_items 286720.
I0302 19:02:33.692862 22732373614720 run.py:483] Algo bellman_ford step 8960 current loss 0.384738, current_train_items 286752.
I0302 19:02:33.708378 22732373614720 run.py:483] Algo bellman_ford step 8961 current loss 0.464823, current_train_items 286784.
I0302 19:02:33.732349 22732373614720 run.py:483] Algo bellman_ford step 8962 current loss 0.640778, current_train_items 286816.
I0302 19:02:33.763432 22732373614720 run.py:483] Algo bellman_ford step 8963 current loss 0.729003, current_train_items 286848.
I0302 19:02:33.795092 22732373614720 run.py:483] Algo bellman_ford step 8964 current loss 0.899624, current_train_items 286880.
I0302 19:02:33.814467 22732373614720 run.py:483] Algo bellman_ford step 8965 current loss 0.390172, current_train_items 286912.
I0302 19:02:33.830603 22732373614720 run.py:483] Algo bellman_ford step 8966 current loss 0.513507, current_train_items 286944.
I0302 19:02:33.854021 22732373614720 run.py:483] Algo bellman_ford step 8967 current loss 0.711088, current_train_items 286976.
I0302 19:02:33.885382 22732373614720 run.py:483] Algo bellman_ford step 8968 current loss 0.680859, current_train_items 287008.
I0302 19:02:33.918112 22732373614720 run.py:483] Algo bellman_ford step 8969 current loss 0.769929, current_train_items 287040.
I0302 19:02:33.937659 22732373614720 run.py:483] Algo bellman_ford step 8970 current loss 0.339943, current_train_items 287072.
I0302 19:02:33.953615 22732373614720 run.py:483] Algo bellman_ford step 8971 current loss 0.454876, current_train_items 287104.
I0302 19:02:33.976876 22732373614720 run.py:483] Algo bellman_ford step 8972 current loss 0.663314, current_train_items 287136.
I0302 19:02:34.007115 22732373614720 run.py:483] Algo bellman_ford step 8973 current loss 0.735892, current_train_items 287168.
I0302 19:02:34.039335 22732373614720 run.py:483] Algo bellman_ford step 8974 current loss 0.827432, current_train_items 287200.
I0302 19:02:34.059148 22732373614720 run.py:483] Algo bellman_ford step 8975 current loss 0.361121, current_train_items 287232.
I0302 19:02:34.075041 22732373614720 run.py:483] Algo bellman_ford step 8976 current loss 0.471039, current_train_items 287264.
I0302 19:02:34.097191 22732373614720 run.py:483] Algo bellman_ford step 8977 current loss 0.593344, current_train_items 287296.
I0302 19:02:34.128648 22732373614720 run.py:483] Algo bellman_ford step 8978 current loss 0.700148, current_train_items 287328.
I0302 19:02:34.160639 22732373614720 run.py:483] Algo bellman_ford step 8979 current loss 0.874981, current_train_items 287360.
I0302 19:02:34.180035 22732373614720 run.py:483] Algo bellman_ford step 8980 current loss 0.299559, current_train_items 287392.
I0302 19:02:34.196101 22732373614720 run.py:483] Algo bellman_ford step 8981 current loss 0.501667, current_train_items 287424.
I0302 19:02:34.219429 22732373614720 run.py:483] Algo bellman_ford step 8982 current loss 0.645710, current_train_items 287456.
I0302 19:02:34.250850 22732373614720 run.py:483] Algo bellman_ford step 8983 current loss 0.699240, current_train_items 287488.
I0302 19:02:34.283586 22732373614720 run.py:483] Algo bellman_ford step 8984 current loss 0.709529, current_train_items 287520.
I0302 19:02:34.303232 22732373614720 run.py:483] Algo bellman_ford step 8985 current loss 0.287318, current_train_items 287552.
I0302 19:02:34.319236 22732373614720 run.py:483] Algo bellman_ford step 8986 current loss 0.460703, current_train_items 287584.
I0302 19:02:34.341594 22732373614720 run.py:483] Algo bellman_ford step 8987 current loss 0.593821, current_train_items 287616.
I0302 19:02:34.371182 22732373614720 run.py:483] Algo bellman_ford step 8988 current loss 0.690496, current_train_items 287648.
I0302 19:02:34.406511 22732373614720 run.py:483] Algo bellman_ford step 8989 current loss 0.835273, current_train_items 287680.
I0302 19:02:34.426208 22732373614720 run.py:483] Algo bellman_ford step 8990 current loss 0.319976, current_train_items 287712.
I0302 19:02:34.441914 22732373614720 run.py:483] Algo bellman_ford step 8991 current loss 0.470467, current_train_items 287744.
I0302 19:02:34.464513 22732373614720 run.py:483] Algo bellman_ford step 8992 current loss 0.619966, current_train_items 287776.
I0302 19:02:34.495352 22732373614720 run.py:483] Algo bellman_ford step 8993 current loss 0.688323, current_train_items 287808.
I0302 19:02:34.527879 22732373614720 run.py:483] Algo bellman_ford step 8994 current loss 0.908260, current_train_items 287840.
I0302 19:02:34.546788 22732373614720 run.py:483] Algo bellman_ford step 8995 current loss 0.294810, current_train_items 287872.
I0302 19:02:34.562762 22732373614720 run.py:483] Algo bellman_ford step 8996 current loss 0.459467, current_train_items 287904.
I0302 19:02:34.586643 22732373614720 run.py:483] Algo bellman_ford step 8997 current loss 0.698292, current_train_items 287936.
I0302 19:02:34.619277 22732373614720 run.py:483] Algo bellman_ford step 8998 current loss 0.801207, current_train_items 287968.
I0302 19:02:34.651089 22732373614720 run.py:483] Algo bellman_ford step 8999 current loss 0.818573, current_train_items 288000.
I0302 19:02:34.670832 22732373614720 run.py:483] Algo bellman_ford step 9000 current loss 0.337851, current_train_items 288032.
I0302 19:02:34.678826 22732373614720 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0302 19:02:34.678935 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:34.695738 22732373614720 run.py:483] Algo bellman_ford step 9001 current loss 0.518313, current_train_items 288064.
I0302 19:02:34.719105 22732373614720 run.py:483] Algo bellman_ford step 9002 current loss 0.694885, current_train_items 288096.
I0302 19:02:34.749999 22732373614720 run.py:483] Algo bellman_ford step 9003 current loss 0.895818, current_train_items 288128.
I0302 19:02:34.783722 22732373614720 run.py:483] Algo bellman_ford step 9004 current loss 0.807200, current_train_items 288160.
I0302 19:02:34.803619 22732373614720 run.py:483] Algo bellman_ford step 9005 current loss 0.321579, current_train_items 288192.
I0302 19:02:34.819491 22732373614720 run.py:483] Algo bellman_ford step 9006 current loss 0.477356, current_train_items 288224.
I0302 19:02:34.842307 22732373614720 run.py:483] Algo bellman_ford step 9007 current loss 0.702144, current_train_items 288256.
I0302 19:02:34.874269 22732373614720 run.py:483] Algo bellman_ford step 9008 current loss 0.797274, current_train_items 288288.
I0302 19:02:34.907525 22732373614720 run.py:483] Algo bellman_ford step 9009 current loss 0.964041, current_train_items 288320.
I0302 19:02:34.927071 22732373614720 run.py:483] Algo bellman_ford step 9010 current loss 0.332192, current_train_items 288352.
I0302 19:02:34.943141 22732373614720 run.py:483] Algo bellman_ford step 9011 current loss 0.451274, current_train_items 288384.
I0302 19:02:34.966380 22732373614720 run.py:483] Algo bellman_ford step 9012 current loss 0.589137, current_train_items 288416.
I0302 19:02:34.996823 22732373614720 run.py:483] Algo bellman_ford step 9013 current loss 0.674423, current_train_items 288448.
I0302 19:02:35.031965 22732373614720 run.py:483] Algo bellman_ford step 9014 current loss 0.918514, current_train_items 288480.
I0302 19:02:35.051314 22732373614720 run.py:483] Algo bellman_ford step 9015 current loss 0.311770, current_train_items 288512.
I0302 19:02:35.067481 22732373614720 run.py:483] Algo bellman_ford step 9016 current loss 0.509017, current_train_items 288544.
I0302 19:02:35.091884 22732373614720 run.py:483] Algo bellman_ford step 9017 current loss 0.683544, current_train_items 288576.
I0302 19:02:35.123329 22732373614720 run.py:483] Algo bellman_ford step 9018 current loss 0.641769, current_train_items 288608.
I0302 19:02:35.157694 22732373614720 run.py:483] Algo bellman_ford step 9019 current loss 0.768960, current_train_items 288640.
I0302 19:02:35.176981 22732373614720 run.py:483] Algo bellman_ford step 9020 current loss 0.399946, current_train_items 288672.
I0302 19:02:35.192975 22732373614720 run.py:483] Algo bellman_ford step 9021 current loss 0.541070, current_train_items 288704.
I0302 19:02:35.217602 22732373614720 run.py:483] Algo bellman_ford step 9022 current loss 0.647122, current_train_items 288736.
I0302 19:02:35.250010 22732373614720 run.py:483] Algo bellman_ford step 9023 current loss 0.825442, current_train_items 288768.
I0302 19:02:35.283364 22732373614720 run.py:483] Algo bellman_ford step 9024 current loss 1.008508, current_train_items 288800.
I0302 19:02:35.302825 22732373614720 run.py:483] Algo bellman_ford step 9025 current loss 0.362885, current_train_items 288832.
I0302 19:02:35.318617 22732373614720 run.py:483] Algo bellman_ford step 9026 current loss 0.407436, current_train_items 288864.
I0302 19:02:35.342442 22732373614720 run.py:483] Algo bellman_ford step 9027 current loss 0.649987, current_train_items 288896.
I0302 19:02:35.373921 22732373614720 run.py:483] Algo bellman_ford step 9028 current loss 0.683662, current_train_items 288928.
I0302 19:02:35.407104 22732373614720 run.py:483] Algo bellman_ford step 9029 current loss 0.858249, current_train_items 288960.
I0302 19:02:35.426889 22732373614720 run.py:483] Algo bellman_ford step 9030 current loss 0.359363, current_train_items 288992.
I0302 19:02:35.442825 22732373614720 run.py:483] Algo bellman_ford step 9031 current loss 0.467320, current_train_items 289024.
I0302 19:02:35.466854 22732373614720 run.py:483] Algo bellman_ford step 9032 current loss 0.756551, current_train_items 289056.
I0302 19:02:35.498551 22732373614720 run.py:483] Algo bellman_ford step 9033 current loss 0.738234, current_train_items 289088.
I0302 19:02:35.534520 22732373614720 run.py:483] Algo bellman_ford step 9034 current loss 0.896184, current_train_items 289120.
I0302 19:02:35.553843 22732373614720 run.py:483] Algo bellman_ford step 9035 current loss 0.363257, current_train_items 289152.
I0302 19:02:35.569633 22732373614720 run.py:483] Algo bellman_ford step 9036 current loss 0.640499, current_train_items 289184.
I0302 19:02:35.594031 22732373614720 run.py:483] Algo bellman_ford step 9037 current loss 0.648401, current_train_items 289216.
I0302 19:02:35.625323 22732373614720 run.py:483] Algo bellman_ford step 9038 current loss 0.725636, current_train_items 289248.
I0302 19:02:35.658315 22732373614720 run.py:483] Algo bellman_ford step 9039 current loss 0.869955, current_train_items 289280.
I0302 19:02:35.677864 22732373614720 run.py:483] Algo bellman_ford step 9040 current loss 0.358641, current_train_items 289312.
I0302 19:02:35.694136 22732373614720 run.py:483] Algo bellman_ford step 9041 current loss 0.467327, current_train_items 289344.
I0302 19:02:35.717144 22732373614720 run.py:483] Algo bellman_ford step 9042 current loss 0.578748, current_train_items 289376.
I0302 19:02:35.748145 22732373614720 run.py:483] Algo bellman_ford step 9043 current loss 0.703829, current_train_items 289408.
I0302 19:02:35.782238 22732373614720 run.py:483] Algo bellman_ford step 9044 current loss 0.876609, current_train_items 289440.
I0302 19:02:35.801795 22732373614720 run.py:483] Algo bellman_ford step 9045 current loss 0.385317, current_train_items 289472.
I0302 19:02:35.818140 22732373614720 run.py:483] Algo bellman_ford step 9046 current loss 0.505850, current_train_items 289504.
I0302 19:02:35.840577 22732373614720 run.py:483] Algo bellman_ford step 9047 current loss 0.789753, current_train_items 289536.
I0302 19:02:35.872208 22732373614720 run.py:483] Algo bellman_ford step 9048 current loss 0.789563, current_train_items 289568.
I0302 19:02:35.905307 22732373614720 run.py:483] Algo bellman_ford step 9049 current loss 0.776522, current_train_items 289600.
I0302 19:02:35.924585 22732373614720 run.py:483] Algo bellman_ford step 9050 current loss 0.328733, current_train_items 289632.
I0302 19:02:35.932666 22732373614720 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0302 19:02:35.932778 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:02:35.949502 22732373614720 run.py:483] Algo bellman_ford step 9051 current loss 0.459508, current_train_items 289664.
I0302 19:02:35.973870 22732373614720 run.py:483] Algo bellman_ford step 9052 current loss 0.748697, current_train_items 289696.
I0302 19:02:36.006619 22732373614720 run.py:483] Algo bellman_ford step 9053 current loss 0.773908, current_train_items 289728.
I0302 19:02:36.039075 22732373614720 run.py:483] Algo bellman_ford step 9054 current loss 0.904389, current_train_items 289760.
I0302 19:02:36.059333 22732373614720 run.py:483] Algo bellman_ford step 9055 current loss 0.338758, current_train_items 289792.
I0302 19:02:36.075055 22732373614720 run.py:483] Algo bellman_ford step 9056 current loss 0.507454, current_train_items 289824.
I0302 19:02:36.098277 22732373614720 run.py:483] Algo bellman_ford step 9057 current loss 0.665364, current_train_items 289856.
I0302 19:02:36.128253 22732373614720 run.py:483] Algo bellman_ford step 9058 current loss 0.711729, current_train_items 289888.
I0302 19:02:36.159822 22732373614720 run.py:483] Algo bellman_ford step 9059 current loss 0.730214, current_train_items 289920.
I0302 19:02:36.179709 22732373614720 run.py:483] Algo bellman_ford step 9060 current loss 0.382617, current_train_items 289952.
I0302 19:02:36.196375 22732373614720 run.py:483] Algo bellman_ford step 9061 current loss 0.514858, current_train_items 289984.
I0302 19:02:36.220949 22732373614720 run.py:483] Algo bellman_ford step 9062 current loss 0.687493, current_train_items 290016.
I0302 19:02:36.251268 22732373614720 run.py:483] Algo bellman_ford step 9063 current loss 0.628712, current_train_items 290048.
I0302 19:02:36.285373 22732373614720 run.py:483] Algo bellman_ford step 9064 current loss 0.851628, current_train_items 290080.
I0302 19:02:36.305191 22732373614720 run.py:483] Algo bellman_ford step 9065 current loss 0.361007, current_train_items 290112.
I0302 19:02:36.321233 22732373614720 run.py:483] Algo bellman_ford step 9066 current loss 0.539521, current_train_items 290144.
I0302 19:02:36.343205 22732373614720 run.py:483] Algo bellman_ford step 9067 current loss 0.548886, current_train_items 290176.
I0302 19:02:36.374951 22732373614720 run.py:483] Algo bellman_ford step 9068 current loss 0.706479, current_train_items 290208.
I0302 19:02:36.408968 22732373614720 run.py:483] Algo bellman_ford step 9069 current loss 0.829723, current_train_items 290240.
I0302 19:02:36.429135 22732373614720 run.py:483] Algo bellman_ford step 9070 current loss 0.308120, current_train_items 290272.
I0302 19:02:36.445606 22732373614720 run.py:483] Algo bellman_ford step 9071 current loss 0.534411, current_train_items 290304.
I0302 19:02:36.469872 22732373614720 run.py:483] Algo bellman_ford step 9072 current loss 0.704317, current_train_items 290336.
I0302 19:02:36.501612 22732373614720 run.py:483] Algo bellman_ford step 9073 current loss 0.748671, current_train_items 290368.
I0302 19:02:36.534772 22732373614720 run.py:483] Algo bellman_ford step 9074 current loss 0.841158, current_train_items 290400.
I0302 19:02:36.554528 22732373614720 run.py:483] Algo bellman_ford step 9075 current loss 0.289952, current_train_items 290432.
I0302 19:02:36.570725 22732373614720 run.py:483] Algo bellman_ford step 9076 current loss 0.631322, current_train_items 290464.
I0302 19:02:36.594432 22732373614720 run.py:483] Algo bellman_ford step 9077 current loss 0.741591, current_train_items 290496.
I0302 19:02:36.625684 22732373614720 run.py:483] Algo bellman_ford step 9078 current loss 0.698602, current_train_items 290528.
I0302 19:02:36.659812 22732373614720 run.py:483] Algo bellman_ford step 9079 current loss 0.984899, current_train_items 290560.
I0302 19:02:36.679552 22732373614720 run.py:483] Algo bellman_ford step 9080 current loss 0.345569, current_train_items 290592.
I0302 19:02:36.695539 22732373614720 run.py:483] Algo bellman_ford step 9081 current loss 0.509751, current_train_items 290624.
I0302 19:02:36.718823 22732373614720 run.py:483] Algo bellman_ford step 9082 current loss 0.618238, current_train_items 290656.
I0302 19:02:36.749773 22732373614720 run.py:483] Algo bellman_ford step 9083 current loss 0.702622, current_train_items 290688.
I0302 19:02:36.782400 22732373614720 run.py:483] Algo bellman_ford step 9084 current loss 0.729820, current_train_items 290720.
I0302 19:02:36.802248 22732373614720 run.py:483] Algo bellman_ford step 9085 current loss 0.358209, current_train_items 290752.
I0302 19:02:36.818437 22732373614720 run.py:483] Algo bellman_ford step 9086 current loss 0.459478, current_train_items 290784.
I0302 19:02:36.842389 22732373614720 run.py:483] Algo bellman_ford step 9087 current loss 0.671258, current_train_items 290816.
I0302 19:02:36.873136 22732373614720 run.py:483] Algo bellman_ford step 9088 current loss 0.599776, current_train_items 290848.
I0302 19:02:36.908019 22732373614720 run.py:483] Algo bellman_ford step 9089 current loss 0.836786, current_train_items 290880.
I0302 19:02:36.927961 22732373614720 run.py:483] Algo bellman_ford step 9090 current loss 0.302871, current_train_items 290912.
I0302 19:02:36.944036 22732373614720 run.py:483] Algo bellman_ford step 9091 current loss 0.519059, current_train_items 290944.
I0302 19:02:36.966931 22732373614720 run.py:483] Algo bellman_ford step 9092 current loss 0.557305, current_train_items 290976.
I0302 19:02:36.996311 22732373614720 run.py:483] Algo bellman_ford step 9093 current loss 0.590944, current_train_items 291008.
I0302 19:02:37.032123 22732373614720 run.py:483] Algo bellman_ford step 9094 current loss 0.893661, current_train_items 291040.
I0302 19:02:37.051990 22732373614720 run.py:483] Algo bellman_ford step 9095 current loss 0.446213, current_train_items 291072.
I0302 19:02:37.067959 22732373614720 run.py:483] Algo bellman_ford step 9096 current loss 0.515756, current_train_items 291104.
I0302 19:02:37.091233 22732373614720 run.py:483] Algo bellman_ford step 9097 current loss 0.640889, current_train_items 291136.
I0302 19:02:37.122761 22732373614720 run.py:483] Algo bellman_ford step 9098 current loss 0.702286, current_train_items 291168.
I0302 19:02:37.155575 22732373614720 run.py:483] Algo bellman_ford step 9099 current loss 0.721610, current_train_items 291200.
I0302 19:02:37.175595 22732373614720 run.py:483] Algo bellman_ford step 9100 current loss 0.288441, current_train_items 291232.
I0302 19:02:37.183448 22732373614720 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0302 19:02:37.183562 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:02:37.200340 22732373614720 run.py:483] Algo bellman_ford step 9101 current loss 0.445511, current_train_items 291264.
I0302 19:02:37.225493 22732373614720 run.py:483] Algo bellman_ford step 9102 current loss 0.674708, current_train_items 291296.
I0302 19:02:37.257582 22732373614720 run.py:483] Algo bellman_ford step 9103 current loss 0.634809, current_train_items 291328.
I0302 19:02:37.292962 22732373614720 run.py:483] Algo bellman_ford step 9104 current loss 0.774550, current_train_items 291360.
I0302 19:02:37.312895 22732373614720 run.py:483] Algo bellman_ford step 9105 current loss 0.410459, current_train_items 291392.
I0302 19:02:37.328813 22732373614720 run.py:483] Algo bellman_ford step 9106 current loss 0.584578, current_train_items 291424.
I0302 19:02:37.352828 22732373614720 run.py:483] Algo bellman_ford step 9107 current loss 0.652846, current_train_items 291456.
I0302 19:02:37.384516 22732373614720 run.py:483] Algo bellman_ford step 9108 current loss 0.690664, current_train_items 291488.
I0302 19:02:37.417990 22732373614720 run.py:483] Algo bellman_ford step 9109 current loss 0.815315, current_train_items 291520.
I0302 19:02:37.437728 22732373614720 run.py:483] Algo bellman_ford step 9110 current loss 0.347960, current_train_items 291552.
I0302 19:02:37.453557 22732373614720 run.py:483] Algo bellman_ford step 9111 current loss 0.446619, current_train_items 291584.
I0302 19:02:37.476592 22732373614720 run.py:483] Algo bellman_ford step 9112 current loss 0.741705, current_train_items 291616.
I0302 19:02:37.509277 22732373614720 run.py:483] Algo bellman_ford step 9113 current loss 0.660028, current_train_items 291648.
I0302 19:02:37.543923 22732373614720 run.py:483] Algo bellman_ford step 9114 current loss 0.851800, current_train_items 291680.
I0302 19:02:37.563956 22732373614720 run.py:483] Algo bellman_ford step 9115 current loss 0.403455, current_train_items 291712.
I0302 19:02:37.580328 22732373614720 run.py:483] Algo bellman_ford step 9116 current loss 0.462118, current_train_items 291744.
I0302 19:02:37.603980 22732373614720 run.py:483] Algo bellman_ford step 9117 current loss 0.653680, current_train_items 291776.
I0302 19:02:37.634860 22732373614720 run.py:483] Algo bellman_ford step 9118 current loss 0.728762, current_train_items 291808.
I0302 19:02:37.669628 22732373614720 run.py:483] Algo bellman_ford step 9119 current loss 0.887682, current_train_items 291840.
I0302 19:02:37.689315 22732373614720 run.py:483] Algo bellman_ford step 9120 current loss 0.291004, current_train_items 291872.
I0302 19:02:37.705317 22732373614720 run.py:483] Algo bellman_ford step 9121 current loss 0.484219, current_train_items 291904.
I0302 19:02:37.728819 22732373614720 run.py:483] Algo bellman_ford step 9122 current loss 0.660304, current_train_items 291936.
I0302 19:02:37.762087 22732373614720 run.py:483] Algo bellman_ford step 9123 current loss 0.802933, current_train_items 291968.
I0302 19:02:37.797344 22732373614720 run.py:483] Algo bellman_ford step 9124 current loss 0.802872, current_train_items 292000.
I0302 19:02:37.816832 22732373614720 run.py:483] Algo bellman_ford step 9125 current loss 0.313402, current_train_items 292032.
I0302 19:02:37.832641 22732373614720 run.py:483] Algo bellman_ford step 9126 current loss 0.467692, current_train_items 292064.
I0302 19:02:37.855701 22732373614720 run.py:483] Algo bellman_ford step 9127 current loss 0.607218, current_train_items 292096.
I0302 19:02:37.887882 22732373614720 run.py:483] Algo bellman_ford step 9128 current loss 0.726125, current_train_items 292128.
I0302 19:02:37.920612 22732373614720 run.py:483] Algo bellman_ford step 9129 current loss 0.932729, current_train_items 292160.
I0302 19:02:37.940367 22732373614720 run.py:483] Algo bellman_ford step 9130 current loss 0.321090, current_train_items 292192.
I0302 19:02:37.956437 22732373614720 run.py:483] Algo bellman_ford step 9131 current loss 0.529432, current_train_items 292224.
I0302 19:02:37.980806 22732373614720 run.py:483] Algo bellman_ford step 9132 current loss 0.649444, current_train_items 292256.
I0302 19:02:38.012665 22732373614720 run.py:483] Algo bellman_ford step 9133 current loss 0.758191, current_train_items 292288.
I0302 19:02:38.045043 22732373614720 run.py:483] Algo bellman_ford step 9134 current loss 0.834043, current_train_items 292320.
I0302 19:02:38.064821 22732373614720 run.py:483] Algo bellman_ford step 9135 current loss 0.323252, current_train_items 292352.
I0302 19:02:38.080468 22732373614720 run.py:483] Algo bellman_ford step 9136 current loss 0.478943, current_train_items 292384.
I0302 19:02:38.103437 22732373614720 run.py:483] Algo bellman_ford step 9137 current loss 0.625214, current_train_items 292416.
I0302 19:02:38.134628 22732373614720 run.py:483] Algo bellman_ford step 9138 current loss 0.704912, current_train_items 292448.
I0302 19:02:38.166927 22732373614720 run.py:483] Algo bellman_ford step 9139 current loss 0.731972, current_train_items 292480.
I0302 19:02:38.186422 22732373614720 run.py:483] Algo bellman_ford step 9140 current loss 0.324281, current_train_items 292512.
I0302 19:02:38.202400 22732373614720 run.py:483] Algo bellman_ford step 9141 current loss 0.546976, current_train_items 292544.
I0302 19:02:38.225886 22732373614720 run.py:483] Algo bellman_ford step 9142 current loss 0.555810, current_train_items 292576.
I0302 19:02:38.256782 22732373614720 run.py:483] Algo bellman_ford step 9143 current loss 0.635255, current_train_items 292608.
I0302 19:02:38.291236 22732373614720 run.py:483] Algo bellman_ford step 9144 current loss 0.745619, current_train_items 292640.
I0302 19:02:38.310588 22732373614720 run.py:483] Algo bellman_ford step 9145 current loss 0.343220, current_train_items 292672.
I0302 19:02:38.326777 22732373614720 run.py:483] Algo bellman_ford step 9146 current loss 0.475827, current_train_items 292704.
I0302 19:02:38.350604 22732373614720 run.py:483] Algo bellman_ford step 9147 current loss 0.742714, current_train_items 292736.
I0302 19:02:38.381954 22732373614720 run.py:483] Algo bellman_ford step 9148 current loss 0.946481, current_train_items 292768.
I0302 19:02:38.412192 22732373614720 run.py:483] Algo bellman_ford step 9149 current loss 0.734482, current_train_items 292800.
I0302 19:02:38.431687 22732373614720 run.py:483] Algo bellman_ford step 9150 current loss 0.363284, current_train_items 292832.
I0302 19:02:38.439741 22732373614720 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0302 19:02:38.439850 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 19:02:38.456711 22732373614720 run.py:483] Algo bellman_ford step 9151 current loss 0.590191, current_train_items 292864.
I0302 19:02:38.480794 22732373614720 run.py:483] Algo bellman_ford step 9152 current loss 0.537174, current_train_items 292896.
I0302 19:02:38.513091 22732373614720 run.py:483] Algo bellman_ford step 9153 current loss 0.677583, current_train_items 292928.
I0302 19:02:38.544851 22732373614720 run.py:483] Algo bellman_ford step 9154 current loss 0.813250, current_train_items 292960.
I0302 19:02:38.564983 22732373614720 run.py:483] Algo bellman_ford step 9155 current loss 0.320473, current_train_items 292992.
I0302 19:02:38.581490 22732373614720 run.py:483] Algo bellman_ford step 9156 current loss 0.657394, current_train_items 293024.
I0302 19:02:38.606138 22732373614720 run.py:483] Algo bellman_ford step 9157 current loss 0.641873, current_train_items 293056.
I0302 19:02:38.637073 22732373614720 run.py:483] Algo bellman_ford step 9158 current loss 0.680252, current_train_items 293088.
I0302 19:02:38.671325 22732373614720 run.py:483] Algo bellman_ford step 9159 current loss 0.868946, current_train_items 293120.
I0302 19:02:38.691410 22732373614720 run.py:483] Algo bellman_ford step 9160 current loss 0.329763, current_train_items 293152.
I0302 19:02:38.707818 22732373614720 run.py:483] Algo bellman_ford step 9161 current loss 0.487903, current_train_items 293184.
I0302 19:02:38.731323 22732373614720 run.py:483] Algo bellman_ford step 9162 current loss 0.656252, current_train_items 293216.
I0302 19:02:38.762212 22732373614720 run.py:483] Algo bellman_ford step 9163 current loss 0.671755, current_train_items 293248.
I0302 19:02:38.797108 22732373614720 run.py:483] Algo bellman_ford step 9164 current loss 0.770285, current_train_items 293280.
I0302 19:02:38.816700 22732373614720 run.py:483] Algo bellman_ford step 9165 current loss 0.424293, current_train_items 293312.
I0302 19:02:38.832915 22732373614720 run.py:483] Algo bellman_ford step 9166 current loss 0.480467, current_train_items 293344.
I0302 19:02:38.856628 22732373614720 run.py:483] Algo bellman_ford step 9167 current loss 0.644434, current_train_items 293376.
I0302 19:02:38.888750 22732373614720 run.py:483] Algo bellman_ford step 9168 current loss 0.665524, current_train_items 293408.
I0302 19:02:38.921737 22732373614720 run.py:483] Algo bellman_ford step 9169 current loss 0.766531, current_train_items 293440.
I0302 19:02:38.941899 22732373614720 run.py:483] Algo bellman_ford step 9170 current loss 0.312980, current_train_items 293472.
I0302 19:02:38.957983 22732373614720 run.py:483] Algo bellman_ford step 9171 current loss 0.505557, current_train_items 293504.
I0302 19:02:38.982844 22732373614720 run.py:483] Algo bellman_ford step 9172 current loss 0.828344, current_train_items 293536.
I0302 19:02:39.014466 22732373614720 run.py:483] Algo bellman_ford step 9173 current loss 0.705494, current_train_items 293568.
I0302 19:02:39.048729 22732373614720 run.py:483] Algo bellman_ford step 9174 current loss 0.927331, current_train_items 293600.
I0302 19:02:39.068509 22732373614720 run.py:483] Algo bellman_ford step 9175 current loss 0.300292, current_train_items 293632.
I0302 19:02:39.084958 22732373614720 run.py:483] Algo bellman_ford step 9176 current loss 0.387865, current_train_items 293664.
I0302 19:02:39.107500 22732373614720 run.py:483] Algo bellman_ford step 9177 current loss 0.625716, current_train_items 293696.
I0302 19:02:39.137541 22732373614720 run.py:483] Algo bellman_ford step 9178 current loss 0.716452, current_train_items 293728.
I0302 19:02:39.172506 22732373614720 run.py:483] Algo bellman_ford step 9179 current loss 0.886313, current_train_items 293760.
I0302 19:02:39.192036 22732373614720 run.py:483] Algo bellman_ford step 9180 current loss 0.395218, current_train_items 293792.
I0302 19:02:39.208173 22732373614720 run.py:483] Algo bellman_ford step 9181 current loss 0.588486, current_train_items 293824.
I0302 19:02:39.232840 22732373614720 run.py:483] Algo bellman_ford step 9182 current loss 0.734639, current_train_items 293856.
I0302 19:02:39.263082 22732373614720 run.py:483] Algo bellman_ford step 9183 current loss 0.696665, current_train_items 293888.
I0302 19:02:39.295414 22732373614720 run.py:483] Algo bellman_ford step 9184 current loss 0.959572, current_train_items 293920.
I0302 19:02:39.315540 22732373614720 run.py:483] Algo bellman_ford step 9185 current loss 0.346177, current_train_items 293952.
I0302 19:02:39.331975 22732373614720 run.py:483] Algo bellman_ford step 9186 current loss 0.525200, current_train_items 293984.
I0302 19:02:39.355935 22732373614720 run.py:483] Algo bellman_ford step 9187 current loss 0.675377, current_train_items 294016.
I0302 19:02:39.388748 22732373614720 run.py:483] Algo bellman_ford step 9188 current loss 0.722217, current_train_items 294048.
I0302 19:02:39.424291 22732373614720 run.py:483] Algo bellman_ford step 9189 current loss 0.897650, current_train_items 294080.
I0302 19:02:39.444166 22732373614720 run.py:483] Algo bellman_ford step 9190 current loss 0.494953, current_train_items 294112.
I0302 19:02:39.460097 22732373614720 run.py:483] Algo bellman_ford step 9191 current loss 0.657415, current_train_items 294144.
I0302 19:02:39.484032 22732373614720 run.py:483] Algo bellman_ford step 9192 current loss 0.597836, current_train_items 294176.
I0302 19:02:39.515848 22732373614720 run.py:483] Algo bellman_ford step 9193 current loss 0.729343, current_train_items 294208.
I0302 19:02:39.548162 22732373614720 run.py:483] Algo bellman_ford step 9194 current loss 0.760645, current_train_items 294240.
I0302 19:02:39.567984 22732373614720 run.py:483] Algo bellman_ford step 9195 current loss 0.398367, current_train_items 294272.
I0302 19:02:39.584560 22732373614720 run.py:483] Algo bellman_ford step 9196 current loss 0.611782, current_train_items 294304.
I0302 19:02:39.608440 22732373614720 run.py:483] Algo bellman_ford step 9197 current loss 0.892256, current_train_items 294336.
I0302 19:02:39.639316 22732373614720 run.py:483] Algo bellman_ford step 9198 current loss 0.785377, current_train_items 294368.
I0302 19:02:39.671583 22732373614720 run.py:483] Algo bellman_ford step 9199 current loss 0.828177, current_train_items 294400.
I0302 19:02:39.691621 22732373614720 run.py:483] Algo bellman_ford step 9200 current loss 0.316514, current_train_items 294432.
I0302 19:02:39.699658 22732373614720 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0302 19:02:39.699769 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 19:02:39.716619 22732373614720 run.py:483] Algo bellman_ford step 9201 current loss 0.498770, current_train_items 294464.
I0302 19:02:39.740536 22732373614720 run.py:483] Algo bellman_ford step 9202 current loss 0.674811, current_train_items 294496.
I0302 19:02:39.773355 22732373614720 run.py:483] Algo bellman_ford step 9203 current loss 0.734845, current_train_items 294528.
I0302 19:02:39.807610 22732373614720 run.py:483] Algo bellman_ford step 9204 current loss 0.729046, current_train_items 294560.
I0302 19:02:39.827762 22732373614720 run.py:483] Algo bellman_ford step 9205 current loss 0.314690, current_train_items 294592.
I0302 19:02:39.843817 22732373614720 run.py:483] Algo bellman_ford step 9206 current loss 0.481554, current_train_items 294624.
I0302 19:02:39.867352 22732373614720 run.py:483] Algo bellman_ford step 9207 current loss 0.610126, current_train_items 294656.
I0302 19:02:39.898597 22732373614720 run.py:483] Algo bellman_ford step 9208 current loss 0.683108, current_train_items 294688.
I0302 19:02:39.929422 22732373614720 run.py:483] Algo bellman_ford step 9209 current loss 0.755695, current_train_items 294720.
I0302 19:02:39.948765 22732373614720 run.py:483] Algo bellman_ford step 9210 current loss 0.322399, current_train_items 294752.
I0302 19:02:39.965397 22732373614720 run.py:483] Algo bellman_ford step 9211 current loss 0.557540, current_train_items 294784.
I0302 19:02:39.988625 22732373614720 run.py:483] Algo bellman_ford step 9212 current loss 0.610730, current_train_items 294816.
I0302 19:02:40.019544 22732373614720 run.py:483] Algo bellman_ford step 9213 current loss 0.698701, current_train_items 294848.
I0302 19:02:40.051716 22732373614720 run.py:483] Algo bellman_ford step 9214 current loss 0.769682, current_train_items 294880.
I0302 19:02:40.071063 22732373614720 run.py:483] Algo bellman_ford step 9215 current loss 0.422259, current_train_items 294912.
I0302 19:02:40.087287 22732373614720 run.py:483] Algo bellman_ford step 9216 current loss 0.573030, current_train_items 294944.
I0302 19:02:40.110357 22732373614720 run.py:483] Algo bellman_ford step 9217 current loss 0.642464, current_train_items 294976.
I0302 19:02:40.140971 22732373614720 run.py:483] Algo bellman_ford step 9218 current loss 0.689127, current_train_items 295008.
I0302 19:02:40.173603 22732373614720 run.py:483] Algo bellman_ford step 9219 current loss 0.808058, current_train_items 295040.
I0302 19:02:40.193092 22732373614720 run.py:483] Algo bellman_ford step 9220 current loss 0.329356, current_train_items 295072.
I0302 19:02:40.209023 22732373614720 run.py:483] Algo bellman_ford step 9221 current loss 0.428979, current_train_items 295104.
I0302 19:02:40.232823 22732373614720 run.py:483] Algo bellman_ford step 9222 current loss 0.661546, current_train_items 295136.
I0302 19:02:40.263561 22732373614720 run.py:483] Algo bellman_ford step 9223 current loss 0.701124, current_train_items 295168.
I0302 19:02:40.296559 22732373614720 run.py:483] Algo bellman_ford step 9224 current loss 0.757543, current_train_items 295200.
I0302 19:02:40.316104 22732373614720 run.py:483] Algo bellman_ford step 9225 current loss 0.372728, current_train_items 295232.
I0302 19:02:40.332039 22732373614720 run.py:483] Algo bellman_ford step 9226 current loss 0.500039, current_train_items 295264.
I0302 19:02:40.355860 22732373614720 run.py:483] Algo bellman_ford step 9227 current loss 0.702947, current_train_items 295296.
I0302 19:02:40.387098 22732373614720 run.py:483] Algo bellman_ford step 9228 current loss 0.733834, current_train_items 295328.
I0302 19:02:40.417991 22732373614720 run.py:483] Algo bellman_ford step 9229 current loss 0.866207, current_train_items 295360.
I0302 19:02:40.437358 22732373614720 run.py:483] Algo bellman_ford step 9230 current loss 0.349249, current_train_items 295392.
I0302 19:02:40.453764 22732373614720 run.py:483] Algo bellman_ford step 9231 current loss 0.503858, current_train_items 295424.
I0302 19:02:40.477518 22732373614720 run.py:483] Algo bellman_ford step 9232 current loss 0.746513, current_train_items 295456.
I0302 19:02:40.510842 22732373614720 run.py:483] Algo bellman_ford step 9233 current loss 0.724853, current_train_items 295488.
I0302 19:02:40.544436 22732373614720 run.py:483] Algo bellman_ford step 9234 current loss 0.848680, current_train_items 295520.
I0302 19:02:40.563625 22732373614720 run.py:483] Algo bellman_ford step 9235 current loss 0.389924, current_train_items 295552.
I0302 19:02:40.579818 22732373614720 run.py:483] Algo bellman_ford step 9236 current loss 0.536137, current_train_items 295584.
I0302 19:02:40.603270 22732373614720 run.py:483] Algo bellman_ford step 9237 current loss 0.672759, current_train_items 295616.
I0302 19:02:40.635790 22732373614720 run.py:483] Algo bellman_ford step 9238 current loss 0.833549, current_train_items 295648.
I0302 19:02:40.669383 22732373614720 run.py:483] Algo bellman_ford step 9239 current loss 0.820835, current_train_items 295680.
I0302 19:02:40.689035 22732373614720 run.py:483] Algo bellman_ford step 9240 current loss 0.284091, current_train_items 295712.
I0302 19:02:40.705521 22732373614720 run.py:483] Algo bellman_ford step 9241 current loss 0.578475, current_train_items 295744.
I0302 19:02:40.729492 22732373614720 run.py:483] Algo bellman_ford step 9242 current loss 0.684744, current_train_items 295776.
I0302 19:02:40.760907 22732373614720 run.py:483] Algo bellman_ford step 9243 current loss 0.623060, current_train_items 295808.
I0302 19:02:40.792778 22732373614720 run.py:483] Algo bellman_ford step 9244 current loss 0.774976, current_train_items 295840.
I0302 19:02:40.812272 22732373614720 run.py:483] Algo bellman_ford step 9245 current loss 0.322473, current_train_items 295872.
I0302 19:02:40.828375 22732373614720 run.py:483] Algo bellman_ford step 9246 current loss 0.506468, current_train_items 295904.
I0302 19:02:40.851102 22732373614720 run.py:483] Algo bellman_ford step 9247 current loss 0.615523, current_train_items 295936.
I0302 19:02:40.882391 22732373614720 run.py:483] Algo bellman_ford step 9248 current loss 0.822288, current_train_items 295968.
I0302 19:02:40.915933 22732373614720 run.py:483] Algo bellman_ford step 9249 current loss 0.861538, current_train_items 296000.
I0302 19:02:40.935525 22732373614720 run.py:483] Algo bellman_ford step 9250 current loss 0.317807, current_train_items 296032.
I0302 19:02:40.943677 22732373614720 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0302 19:02:40.943788 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:02:40.960476 22732373614720 run.py:483] Algo bellman_ford step 9251 current loss 0.504832, current_train_items 296064.
I0302 19:02:40.984640 22732373614720 run.py:483] Algo bellman_ford step 9252 current loss 0.595251, current_train_items 296096.
I0302 19:02:41.015513 22732373614720 run.py:483] Algo bellman_ford step 9253 current loss 0.614739, current_train_items 296128.
I0302 19:02:41.047172 22732373614720 run.py:483] Algo bellman_ford step 9254 current loss 0.800124, current_train_items 296160.
I0302 19:02:41.066657 22732373614720 run.py:483] Algo bellman_ford step 9255 current loss 0.393789, current_train_items 296192.
I0302 19:02:41.082375 22732373614720 run.py:483] Algo bellman_ford step 9256 current loss 0.404109, current_train_items 296224.
I0302 19:02:41.105911 22732373614720 run.py:483] Algo bellman_ford step 9257 current loss 0.686603, current_train_items 296256.
I0302 19:02:41.137164 22732373614720 run.py:483] Algo bellman_ford step 9258 current loss 0.763829, current_train_items 296288.
I0302 19:02:41.170343 22732373614720 run.py:483] Algo bellman_ford step 9259 current loss 0.758532, current_train_items 296320.
I0302 19:02:41.190336 22732373614720 run.py:483] Algo bellman_ford step 9260 current loss 0.368509, current_train_items 296352.
I0302 19:02:41.207112 22732373614720 run.py:483] Algo bellman_ford step 9261 current loss 0.612844, current_train_items 296384.
I0302 19:02:41.229121 22732373614720 run.py:483] Algo bellman_ford step 9262 current loss 0.603219, current_train_items 296416.
I0302 19:02:41.260315 22732373614720 run.py:483] Algo bellman_ford step 9263 current loss 0.676083, current_train_items 296448.
I0302 19:02:41.293451 22732373614720 run.py:483] Algo bellman_ford step 9264 current loss 0.730532, current_train_items 296480.
I0302 19:02:41.312852 22732373614720 run.py:483] Algo bellman_ford step 9265 current loss 0.338232, current_train_items 296512.
I0302 19:02:41.328860 22732373614720 run.py:483] Algo bellman_ford step 9266 current loss 0.466440, current_train_items 296544.
I0302 19:02:41.350876 22732373614720 run.py:483] Algo bellman_ford step 9267 current loss 0.595651, current_train_items 296576.
I0302 19:02:41.381915 22732373614720 run.py:483] Algo bellman_ford step 9268 current loss 0.659606, current_train_items 296608.
I0302 19:02:41.414638 22732373614720 run.py:483] Algo bellman_ford step 9269 current loss 0.836756, current_train_items 296640.
I0302 19:02:41.434436 22732373614720 run.py:483] Algo bellman_ford step 9270 current loss 0.372170, current_train_items 296672.
I0302 19:02:41.450422 22732373614720 run.py:483] Algo bellman_ford step 9271 current loss 0.508418, current_train_items 296704.
I0302 19:02:41.473223 22732373614720 run.py:483] Algo bellman_ford step 9272 current loss 0.601006, current_train_items 296736.
I0302 19:02:41.505773 22732373614720 run.py:483] Algo bellman_ford step 9273 current loss 0.795971, current_train_items 296768.
I0302 19:02:41.539655 22732373614720 run.py:483] Algo bellman_ford step 9274 current loss 0.837851, current_train_items 296800.
I0302 19:02:41.559701 22732373614720 run.py:483] Algo bellman_ford step 9275 current loss 0.331357, current_train_items 296832.
I0302 19:02:41.575850 22732373614720 run.py:483] Algo bellman_ford step 9276 current loss 0.535014, current_train_items 296864.
I0302 19:02:41.598890 22732373614720 run.py:483] Algo bellman_ford step 9277 current loss 0.619022, current_train_items 296896.
I0302 19:02:41.630759 22732373614720 run.py:483] Algo bellman_ford step 9278 current loss 0.709085, current_train_items 296928.
I0302 19:02:41.663757 22732373614720 run.py:483] Algo bellman_ford step 9279 current loss 1.017289, current_train_items 296960.
I0302 19:02:41.683043 22732373614720 run.py:483] Algo bellman_ford step 9280 current loss 0.381347, current_train_items 296992.
I0302 19:02:41.699184 22732373614720 run.py:483] Algo bellman_ford step 9281 current loss 0.473402, current_train_items 297024.
I0302 19:02:41.722249 22732373614720 run.py:483] Algo bellman_ford step 9282 current loss 0.620526, current_train_items 297056.
I0302 19:02:41.751785 22732373614720 run.py:483] Algo bellman_ford step 9283 current loss 0.613551, current_train_items 297088.
I0302 19:02:41.783891 22732373614720 run.py:483] Algo bellman_ford step 9284 current loss 0.777127, current_train_items 297120.
I0302 19:02:41.803495 22732373614720 run.py:483] Algo bellman_ford step 9285 current loss 0.264700, current_train_items 297152.
I0302 19:02:41.818982 22732373614720 run.py:483] Algo bellman_ford step 9286 current loss 0.476092, current_train_items 297184.
I0302 19:02:41.843237 22732373614720 run.py:483] Algo bellman_ford step 9287 current loss 0.761553, current_train_items 297216.
I0302 19:02:41.873527 22732373614720 run.py:483] Algo bellman_ford step 9288 current loss 0.769179, current_train_items 297248.
I0302 19:02:41.908535 22732373614720 run.py:483] Algo bellman_ford step 9289 current loss 0.913301, current_train_items 297280.
I0302 19:02:41.928148 22732373614720 run.py:483] Algo bellman_ford step 9290 current loss 0.392143, current_train_items 297312.
I0302 19:02:41.944246 22732373614720 run.py:483] Algo bellman_ford step 9291 current loss 0.507731, current_train_items 297344.
I0302 19:02:41.967867 22732373614720 run.py:483] Algo bellman_ford step 9292 current loss 0.730187, current_train_items 297376.
I0302 19:02:41.998067 22732373614720 run.py:483] Algo bellman_ford step 9293 current loss 0.629467, current_train_items 297408.
I0302 19:02:42.031801 22732373614720 run.py:483] Algo bellman_ford step 9294 current loss 0.868447, current_train_items 297440.
I0302 19:02:42.051458 22732373614720 run.py:483] Algo bellman_ford step 9295 current loss 0.318625, current_train_items 297472.
I0302 19:02:42.067545 22732373614720 run.py:483] Algo bellman_ford step 9296 current loss 0.431182, current_train_items 297504.
I0302 19:02:42.090825 22732373614720 run.py:483] Algo bellman_ford step 9297 current loss 0.624948, current_train_items 297536.
I0302 19:02:42.122394 22732373614720 run.py:483] Algo bellman_ford step 9298 current loss 0.704132, current_train_items 297568.
I0302 19:02:42.155684 22732373614720 run.py:483] Algo bellman_ford step 9299 current loss 0.866350, current_train_items 297600.
I0302 19:02:42.176062 22732373614720 run.py:483] Algo bellman_ford step 9300 current loss 0.438279, current_train_items 297632.
I0302 19:02:42.183901 22732373614720 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0302 19:02:42.184012 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 19:02:42.200992 22732373614720 run.py:483] Algo bellman_ford step 9301 current loss 0.455214, current_train_items 297664.
I0302 19:02:42.225268 22732373614720 run.py:483] Algo bellman_ford step 9302 current loss 0.563238, current_train_items 297696.
I0302 19:02:42.257188 22732373614720 run.py:483] Algo bellman_ford step 9303 current loss 0.677682, current_train_items 297728.
I0302 19:02:42.292028 22732373614720 run.py:483] Algo bellman_ford step 9304 current loss 0.849801, current_train_items 297760.
I0302 19:02:42.312056 22732373614720 run.py:483] Algo bellman_ford step 9305 current loss 0.319617, current_train_items 297792.
I0302 19:02:42.327664 22732373614720 run.py:483] Algo bellman_ford step 9306 current loss 0.464585, current_train_items 297824.
I0302 19:02:42.351684 22732373614720 run.py:483] Algo bellman_ford step 9307 current loss 0.693075, current_train_items 297856.
I0302 19:02:42.383064 22732373614720 run.py:483] Algo bellman_ford step 9308 current loss 0.702791, current_train_items 297888.
I0302 19:02:42.415860 22732373614720 run.py:483] Algo bellman_ford step 9309 current loss 0.805335, current_train_items 297920.
I0302 19:02:42.435376 22732373614720 run.py:483] Algo bellman_ford step 9310 current loss 0.338614, current_train_items 297952.
I0302 19:02:42.451335 22732373614720 run.py:483] Algo bellman_ford step 9311 current loss 0.492522, current_train_items 297984.
I0302 19:02:42.475101 22732373614720 run.py:483] Algo bellman_ford step 9312 current loss 0.697463, current_train_items 298016.
I0302 19:02:42.505481 22732373614720 run.py:483] Algo bellman_ford step 9313 current loss 0.709244, current_train_items 298048.
I0302 19:02:42.538830 22732373614720 run.py:483] Algo bellman_ford step 9314 current loss 0.805473, current_train_items 298080.
I0302 19:02:42.558402 22732373614720 run.py:483] Algo bellman_ford step 9315 current loss 0.366151, current_train_items 298112.
I0302 19:02:42.574061 22732373614720 run.py:483] Algo bellman_ford step 9316 current loss 0.497067, current_train_items 298144.
I0302 19:02:42.597644 22732373614720 run.py:483] Algo bellman_ford step 9317 current loss 0.697622, current_train_items 298176.
I0302 19:02:42.629634 22732373614720 run.py:483] Algo bellman_ford step 9318 current loss 0.876379, current_train_items 298208.
I0302 19:02:42.662852 22732373614720 run.py:483] Algo bellman_ford step 9319 current loss 0.831835, current_train_items 298240.
I0302 19:02:42.682524 22732373614720 run.py:483] Algo bellman_ford step 9320 current loss 0.399246, current_train_items 298272.
I0302 19:02:42.698532 22732373614720 run.py:483] Algo bellman_ford step 9321 current loss 0.522363, current_train_items 298304.
I0302 19:02:42.720626 22732373614720 run.py:483] Algo bellman_ford step 9322 current loss 0.604804, current_train_items 298336.
I0302 19:02:42.750955 22732373614720 run.py:483] Algo bellman_ford step 9323 current loss 0.707109, current_train_items 298368.
I0302 19:02:42.786648 22732373614720 run.py:483] Algo bellman_ford step 9324 current loss 0.916356, current_train_items 298400.
I0302 19:02:42.806194 22732373614720 run.py:483] Algo bellman_ford step 9325 current loss 0.343510, current_train_items 298432.
I0302 19:02:42.822063 22732373614720 run.py:483] Algo bellman_ford step 9326 current loss 0.571201, current_train_items 298464.
I0302 19:02:42.846178 22732373614720 run.py:483] Algo bellman_ford step 9327 current loss 0.690877, current_train_items 298496.
I0302 19:02:42.878136 22732373614720 run.py:483] Algo bellman_ford step 9328 current loss 0.744141, current_train_items 298528.
I0302 19:02:42.908437 22732373614720 run.py:483] Algo bellman_ford step 9329 current loss 0.795490, current_train_items 298560.
I0302 19:02:42.928189 22732373614720 run.py:483] Algo bellman_ford step 9330 current loss 0.384643, current_train_items 298592.
I0302 19:02:42.944270 22732373614720 run.py:483] Algo bellman_ford step 9331 current loss 0.597824, current_train_items 298624.
I0302 19:02:42.967520 22732373614720 run.py:483] Algo bellman_ford step 9332 current loss 0.625598, current_train_items 298656.
I0302 19:02:42.998955 22732373614720 run.py:483] Algo bellman_ford step 9333 current loss 0.705640, current_train_items 298688.
I0302 19:02:43.031514 22732373614720 run.py:483] Algo bellman_ford step 9334 current loss 0.835945, current_train_items 298720.
I0302 19:02:43.051275 22732373614720 run.py:483] Algo bellman_ford step 9335 current loss 0.268677, current_train_items 298752.
I0302 19:02:43.067305 22732373614720 run.py:483] Algo bellman_ford step 9336 current loss 0.497694, current_train_items 298784.
I0302 19:02:43.091490 22732373614720 run.py:483] Algo bellman_ford step 9337 current loss 0.762922, current_train_items 298816.
I0302 19:02:43.122550 22732373614720 run.py:483] Algo bellman_ford step 9338 current loss 0.714492, current_train_items 298848.
I0302 19:02:43.155832 22732373614720 run.py:483] Algo bellman_ford step 9339 current loss 0.802080, current_train_items 298880.
I0302 19:02:43.175556 22732373614720 run.py:483] Algo bellman_ford step 9340 current loss 0.335771, current_train_items 298912.
I0302 19:02:43.191192 22732373614720 run.py:483] Algo bellman_ford step 9341 current loss 0.487762, current_train_items 298944.
I0302 19:02:43.214874 22732373614720 run.py:483] Algo bellman_ford step 9342 current loss 0.585539, current_train_items 298976.
I0302 19:02:43.246124 22732373614720 run.py:483] Algo bellman_ford step 9343 current loss 0.702613, current_train_items 299008.
I0302 19:02:43.280124 22732373614720 run.py:483] Algo bellman_ford step 9344 current loss 0.774157, current_train_items 299040.
I0302 19:02:43.299764 22732373614720 run.py:483] Algo bellman_ford step 9345 current loss 0.353345, current_train_items 299072.
I0302 19:02:43.315695 22732373614720 run.py:483] Algo bellman_ford step 9346 current loss 0.536449, current_train_items 299104.
I0302 19:02:43.339433 22732373614720 run.py:483] Algo bellman_ford step 9347 current loss 0.622584, current_train_items 299136.
I0302 19:02:43.371253 22732373614720 run.py:483] Algo bellman_ford step 9348 current loss 0.699501, current_train_items 299168.
I0302 19:02:43.403769 22732373614720 run.py:483] Algo bellman_ford step 9349 current loss 0.721832, current_train_items 299200.
I0302 19:02:43.423177 22732373614720 run.py:483] Algo bellman_ford step 9350 current loss 0.420932, current_train_items 299232.
I0302 19:02:43.431324 22732373614720 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0302 19:02:43.431623 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 19:02:43.448545 22732373614720 run.py:483] Algo bellman_ford step 9351 current loss 0.483640, current_train_items 299264.
I0302 19:02:43.473111 22732373614720 run.py:483] Algo bellman_ford step 9352 current loss 0.762016, current_train_items 299296.
I0302 19:02:43.505664 22732373614720 run.py:483] Algo bellman_ford step 9353 current loss 0.692472, current_train_items 299328.
I0302 19:02:43.541211 22732373614720 run.py:483] Algo bellman_ford step 9354 current loss 0.797640, current_train_items 299360.
I0302 19:02:43.561396 22732373614720 run.py:483] Algo bellman_ford step 9355 current loss 0.347554, current_train_items 299392.
I0302 19:02:43.576814 22732373614720 run.py:483] Algo bellman_ford step 9356 current loss 0.427274, current_train_items 299424.
I0302 19:02:43.600764 22732373614720 run.py:483] Algo bellman_ford step 9357 current loss 0.673899, current_train_items 299456.
I0302 19:02:43.632905 22732373614720 run.py:483] Algo bellman_ford step 9358 current loss 0.694480, current_train_items 299488.
I0302 19:02:43.665859 22732373614720 run.py:483] Algo bellman_ford step 9359 current loss 0.709603, current_train_items 299520.
I0302 19:02:43.686253 22732373614720 run.py:483] Algo bellman_ford step 9360 current loss 0.336729, current_train_items 299552.
I0302 19:02:43.702321 22732373614720 run.py:483] Algo bellman_ford step 9361 current loss 0.487302, current_train_items 299584.
I0302 19:02:43.725571 22732373614720 run.py:483] Algo bellman_ford step 9362 current loss 0.546043, current_train_items 299616.
I0302 19:02:43.758198 22732373614720 run.py:483] Algo bellman_ford step 9363 current loss 0.673308, current_train_items 299648.
I0302 19:02:43.792746 22732373614720 run.py:483] Algo bellman_ford step 9364 current loss 0.779505, current_train_items 299680.
I0302 19:02:43.812065 22732373614720 run.py:483] Algo bellman_ford step 9365 current loss 0.364849, current_train_items 299712.
I0302 19:02:43.828550 22732373614720 run.py:483] Algo bellman_ford step 9366 current loss 0.561603, current_train_items 299744.
I0302 19:02:43.853736 22732373614720 run.py:483] Algo bellman_ford step 9367 current loss 0.786534, current_train_items 299776.
I0302 19:02:43.886362 22732373614720 run.py:483] Algo bellman_ford step 9368 current loss 0.672223, current_train_items 299808.
I0302 19:02:43.919372 22732373614720 run.py:483] Algo bellman_ford step 9369 current loss 0.772693, current_train_items 299840.
I0302 19:02:43.939875 22732373614720 run.py:483] Algo bellman_ford step 9370 current loss 0.367417, current_train_items 299872.
I0302 19:02:43.956314 22732373614720 run.py:483] Algo bellman_ford step 9371 current loss 0.554785, current_train_items 299904.
I0302 19:02:43.979684 22732373614720 run.py:483] Algo bellman_ford step 9372 current loss 0.665822, current_train_items 299936.
I0302 19:02:44.012011 22732373614720 run.py:483] Algo bellman_ford step 9373 current loss 0.672343, current_train_items 299968.
I0302 19:02:44.045594 22732373614720 run.py:483] Algo bellman_ford step 9374 current loss 0.664447, current_train_items 300000.
I0302 19:02:44.065689 22732373614720 run.py:483] Algo bellman_ford step 9375 current loss 0.354278, current_train_items 300032.
I0302 19:02:44.081441 22732373614720 run.py:483] Algo bellman_ford step 9376 current loss 0.553680, current_train_items 300064.
I0302 19:02:44.104818 22732373614720 run.py:483] Algo bellman_ford step 9377 current loss 0.539475, current_train_items 300096.
I0302 19:02:44.135602 22732373614720 run.py:483] Algo bellman_ford step 9378 current loss 0.697443, current_train_items 300128.
I0302 19:02:44.168715 22732373614720 run.py:483] Algo bellman_ford step 9379 current loss 0.843888, current_train_items 300160.
I0302 19:02:44.188451 22732373614720 run.py:483] Algo bellman_ford step 9380 current loss 0.311884, current_train_items 300192.
I0302 19:02:44.204030 22732373614720 run.py:483] Algo bellman_ford step 9381 current loss 0.482755, current_train_items 300224.
I0302 19:02:44.227779 22732373614720 run.py:483] Algo bellman_ford step 9382 current loss 0.606630, current_train_items 300256.
I0302 19:02:44.258193 22732373614720 run.py:483] Algo bellman_ford step 9383 current loss 0.586197, current_train_items 300288.
I0302 19:02:44.291699 22732373614720 run.py:483] Algo bellman_ford step 9384 current loss 0.794779, current_train_items 300320.
I0302 19:02:44.311659 22732373614720 run.py:483] Algo bellman_ford step 9385 current loss 0.354004, current_train_items 300352.
I0302 19:02:44.328259 22732373614720 run.py:483] Algo bellman_ford step 9386 current loss 0.615471, current_train_items 300384.
I0302 19:02:44.351795 22732373614720 run.py:483] Algo bellman_ford step 9387 current loss 0.736625, current_train_items 300416.
I0302 19:02:44.382925 22732373614720 run.py:483] Algo bellman_ford step 9388 current loss 0.726430, current_train_items 300448.
I0302 19:02:44.418271 22732373614720 run.py:483] Algo bellman_ford step 9389 current loss 0.726846, current_train_items 300480.
I0302 19:02:44.438263 22732373614720 run.py:483] Algo bellman_ford step 9390 current loss 0.365688, current_train_items 300512.
I0302 19:02:44.454597 22732373614720 run.py:483] Algo bellman_ford step 9391 current loss 0.436189, current_train_items 300544.
I0302 19:02:44.477893 22732373614720 run.py:483] Algo bellman_ford step 9392 current loss 0.638357, current_train_items 300576.
I0302 19:02:44.508984 22732373614720 run.py:483] Algo bellman_ford step 9393 current loss 0.682128, current_train_items 300608.
I0302 19:02:44.541979 22732373614720 run.py:483] Algo bellman_ford step 9394 current loss 0.767309, current_train_items 300640.
I0302 19:02:44.561594 22732373614720 run.py:483] Algo bellman_ford step 9395 current loss 0.368133, current_train_items 300672.
I0302 19:02:44.577825 22732373614720 run.py:483] Algo bellman_ford step 9396 current loss 0.543748, current_train_items 300704.
I0302 19:02:44.602315 22732373614720 run.py:483] Algo bellman_ford step 9397 current loss 0.648255, current_train_items 300736.
I0302 19:02:44.633557 22732373614720 run.py:483] Algo bellman_ford step 9398 current loss 0.659677, current_train_items 300768.
I0302 19:02:44.667006 22732373614720 run.py:483] Algo bellman_ford step 9399 current loss 0.737314, current_train_items 300800.
I0302 19:02:44.686948 22732373614720 run.py:483] Algo bellman_ford step 9400 current loss 0.306579, current_train_items 300832.
I0302 19:02:44.694984 22732373614720 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0302 19:02:44.695094 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 19:02:44.711846 22732373614720 run.py:483] Algo bellman_ford step 9401 current loss 0.549138, current_train_items 300864.
I0302 19:02:44.736441 22732373614720 run.py:483] Algo bellman_ford step 9402 current loss 0.743614, current_train_items 300896.
I0302 19:02:44.767613 22732373614720 run.py:483] Algo bellman_ford step 9403 current loss 0.693724, current_train_items 300928.
I0302 19:02:44.802033 22732373614720 run.py:483] Algo bellman_ford step 9404 current loss 0.788162, current_train_items 300960.
I0302 19:02:44.821820 22732373614720 run.py:483] Algo bellman_ford step 9405 current loss 0.347226, current_train_items 300992.
I0302 19:02:44.837431 22732373614720 run.py:483] Algo bellman_ford step 9406 current loss 0.507681, current_train_items 301024.
I0302 19:02:44.862118 22732373614720 run.py:483] Algo bellman_ford step 9407 current loss 0.656473, current_train_items 301056.
I0302 19:02:44.893006 22732373614720 run.py:483] Algo bellman_ford step 9408 current loss 0.677288, current_train_items 301088.
I0302 19:02:44.927800 22732373614720 run.py:483] Algo bellman_ford step 9409 current loss 0.847412, current_train_items 301120.
I0302 19:02:44.947201 22732373614720 run.py:483] Algo bellman_ford step 9410 current loss 0.370168, current_train_items 301152.
I0302 19:02:44.963201 22732373614720 run.py:483] Algo bellman_ford step 9411 current loss 0.574941, current_train_items 301184.
I0302 19:02:44.987397 22732373614720 run.py:483] Algo bellman_ford step 9412 current loss 0.603433, current_train_items 301216.
I0302 19:02:45.018797 22732373614720 run.py:483] Algo bellman_ford step 9413 current loss 0.620925, current_train_items 301248.
I0302 19:02:45.051279 22732373614720 run.py:483] Algo bellman_ford step 9414 current loss 0.811495, current_train_items 301280.
I0302 19:02:45.070740 22732373614720 run.py:483] Algo bellman_ford step 9415 current loss 0.410675, current_train_items 301312.
I0302 19:02:45.087041 22732373614720 run.py:483] Algo bellman_ford step 9416 current loss 0.440098, current_train_items 301344.
I0302 19:02:45.111185 22732373614720 run.py:483] Algo bellman_ford step 9417 current loss 0.657226, current_train_items 301376.
I0302 19:02:45.141659 22732373614720 run.py:483] Algo bellman_ford step 9418 current loss 0.662767, current_train_items 301408.
I0302 19:02:45.172814 22732373614720 run.py:483] Algo bellman_ford step 9419 current loss 0.676712, current_train_items 301440.
I0302 19:02:45.192061 22732373614720 run.py:483] Algo bellman_ford step 9420 current loss 0.396519, current_train_items 301472.
I0302 19:02:45.208369 22732373614720 run.py:483] Algo bellman_ford step 9421 current loss 0.425882, current_train_items 301504.
I0302 19:02:45.232168 22732373614720 run.py:483] Algo bellman_ford step 9422 current loss 0.647401, current_train_items 301536.
I0302 19:02:45.263221 22732373614720 run.py:483] Algo bellman_ford step 9423 current loss 0.769567, current_train_items 301568.
I0302 19:02:45.296266 22732373614720 run.py:483] Algo bellman_ford step 9424 current loss 1.040761, current_train_items 301600.
I0302 19:02:45.315806 22732373614720 run.py:483] Algo bellman_ford step 9425 current loss 0.386059, current_train_items 301632.
I0302 19:02:45.331477 22732373614720 run.py:483] Algo bellman_ford step 9426 current loss 0.449744, current_train_items 301664.
I0302 19:02:45.354391 22732373614720 run.py:483] Algo bellman_ford step 9427 current loss 0.605098, current_train_items 301696.
I0302 19:02:45.386164 22732373614720 run.py:483] Algo bellman_ford step 9428 current loss 0.786667, current_train_items 301728.
I0302 19:02:45.416602 22732373614720 run.py:483] Algo bellman_ford step 9429 current loss 1.040125, current_train_items 301760.
I0302 19:02:45.435914 22732373614720 run.py:483] Algo bellman_ford step 9430 current loss 0.320676, current_train_items 301792.
I0302 19:02:45.451900 22732373614720 run.py:483] Algo bellman_ford step 9431 current loss 0.541472, current_train_items 301824.
I0302 19:02:45.474998 22732373614720 run.py:483] Algo bellman_ford step 9432 current loss 0.622415, current_train_items 301856.
I0302 19:02:45.505303 22732373614720 run.py:483] Algo bellman_ford step 9433 current loss 0.686642, current_train_items 301888.
I0302 19:02:45.538807 22732373614720 run.py:483] Algo bellman_ford step 9434 current loss 0.930210, current_train_items 301920.
I0302 19:02:45.558385 22732373614720 run.py:483] Algo bellman_ford step 9435 current loss 0.329401, current_train_items 301952.
I0302 19:02:45.574620 22732373614720 run.py:483] Algo bellman_ford step 9436 current loss 0.465470, current_train_items 301984.
I0302 19:02:45.598009 22732373614720 run.py:483] Algo bellman_ford step 9437 current loss 0.588697, current_train_items 302016.
I0302 19:02:45.629470 22732373614720 run.py:483] Algo bellman_ford step 9438 current loss 0.737701, current_train_items 302048.
I0302 19:02:45.661587 22732373614720 run.py:483] Algo bellman_ford step 9439 current loss 0.801157, current_train_items 302080.
I0302 19:02:45.681192 22732373614720 run.py:483] Algo bellman_ford step 9440 current loss 0.370597, current_train_items 302112.
I0302 19:02:45.697744 22732373614720 run.py:483] Algo bellman_ford step 9441 current loss 0.480867, current_train_items 302144.
I0302 19:02:45.721294 22732373614720 run.py:483] Algo bellman_ford step 9442 current loss 0.695156, current_train_items 302176.
I0302 19:02:45.753120 22732373614720 run.py:483] Algo bellman_ford step 9443 current loss 0.624742, current_train_items 302208.
I0302 19:02:45.788563 22732373614720 run.py:483] Algo bellman_ford step 9444 current loss 0.766625, current_train_items 302240.
I0302 19:02:45.808013 22732373614720 run.py:483] Algo bellman_ford step 9445 current loss 0.378769, current_train_items 302272.
I0302 19:02:45.823782 22732373614720 run.py:483] Algo bellman_ford step 9446 current loss 0.414127, current_train_items 302304.
I0302 19:02:45.846982 22732373614720 run.py:483] Algo bellman_ford step 9447 current loss 0.632983, current_train_items 302336.
I0302 19:02:45.877819 22732373614720 run.py:483] Algo bellman_ford step 9448 current loss 0.717573, current_train_items 302368.
I0302 19:02:45.909645 22732373614720 run.py:483] Algo bellman_ford step 9449 current loss 0.704267, current_train_items 302400.
I0302 19:02:45.929081 22732373614720 run.py:483] Algo bellman_ford step 9450 current loss 0.378143, current_train_items 302432.
I0302 19:02:45.937362 22732373614720 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0302 19:02:45.937475 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:02:45.954414 22732373614720 run.py:483] Algo bellman_ford step 9451 current loss 0.464553, current_train_items 302464.
I0302 19:02:45.977455 22732373614720 run.py:483] Algo bellman_ford step 9452 current loss 0.586403, current_train_items 302496.
I0302 19:02:46.009532 22732373614720 run.py:483] Algo bellman_ford step 9453 current loss 0.636764, current_train_items 302528.
I0302 19:02:46.043605 22732373614720 run.py:483] Algo bellman_ford step 9454 current loss 0.850186, current_train_items 302560.
I0302 19:02:46.063673 22732373614720 run.py:483] Algo bellman_ford step 9455 current loss 0.368462, current_train_items 302592.
I0302 19:02:46.079494 22732373614720 run.py:483] Algo bellman_ford step 9456 current loss 0.473420, current_train_items 302624.
I0302 19:02:46.103286 22732373614720 run.py:483] Algo bellman_ford step 9457 current loss 0.515269, current_train_items 302656.
I0302 19:02:46.134603 22732373614720 run.py:483] Algo bellman_ford step 9458 current loss 0.633298, current_train_items 302688.
I0302 19:02:46.166585 22732373614720 run.py:483] Algo bellman_ford step 9459 current loss 0.876746, current_train_items 302720.
I0302 19:02:46.186856 22732373614720 run.py:483] Algo bellman_ford step 9460 current loss 0.336813, current_train_items 302752.
I0302 19:02:46.203202 22732373614720 run.py:483] Algo bellman_ford step 9461 current loss 0.506247, current_train_items 302784.
I0302 19:02:46.225953 22732373614720 run.py:483] Algo bellman_ford step 9462 current loss 0.532731, current_train_items 302816.
I0302 19:02:46.258277 22732373614720 run.py:483] Algo bellman_ford step 9463 current loss 0.642521, current_train_items 302848.
I0302 19:02:46.291062 22732373614720 run.py:483] Algo bellman_ford step 9464 current loss 0.669381, current_train_items 302880.
I0302 19:02:46.310642 22732373614720 run.py:483] Algo bellman_ford step 9465 current loss 0.425362, current_train_items 302912.
I0302 19:02:46.326954 22732373614720 run.py:483] Algo bellman_ford step 9466 current loss 0.544534, current_train_items 302944.
I0302 19:02:46.352047 22732373614720 run.py:483] Algo bellman_ford step 9467 current loss 0.658778, current_train_items 302976.
I0302 19:02:46.384281 22732373614720 run.py:483] Algo bellman_ford step 9468 current loss 0.642339, current_train_items 303008.
I0302 19:02:46.418220 22732373614720 run.py:483] Algo bellman_ford step 9469 current loss 0.805917, current_train_items 303040.
I0302 19:02:46.438306 22732373614720 run.py:483] Algo bellman_ford step 9470 current loss 0.360671, current_train_items 303072.
I0302 19:02:46.454191 22732373614720 run.py:483] Algo bellman_ford step 9471 current loss 0.543536, current_train_items 303104.
I0302 19:02:46.477609 22732373614720 run.py:483] Algo bellman_ford step 9472 current loss 0.580678, current_train_items 303136.
I0302 19:02:46.509417 22732373614720 run.py:483] Algo bellman_ford step 9473 current loss 0.698168, current_train_items 303168.
I0302 19:02:46.541751 22732373614720 run.py:483] Algo bellman_ford step 9474 current loss 0.821510, current_train_items 303200.
I0302 19:02:46.561704 22732373614720 run.py:483] Algo bellman_ford step 9475 current loss 0.309355, current_train_items 303232.
I0302 19:02:46.578018 22732373614720 run.py:483] Algo bellman_ford step 9476 current loss 0.485274, current_train_items 303264.
I0302 19:02:46.601237 22732373614720 run.py:483] Algo bellman_ford step 9477 current loss 0.561313, current_train_items 303296.
I0302 19:02:46.631819 22732373614720 run.py:483] Algo bellman_ford step 9478 current loss 0.660620, current_train_items 303328.
I0302 19:02:46.664342 22732373614720 run.py:483] Algo bellman_ford step 9479 current loss 0.776609, current_train_items 303360.
I0302 19:02:46.684134 22732373614720 run.py:483] Algo bellman_ford step 9480 current loss 0.411458, current_train_items 303392.
I0302 19:02:46.700050 22732373614720 run.py:483] Algo bellman_ford step 9481 current loss 0.525361, current_train_items 303424.
I0302 19:02:46.725026 22732373614720 run.py:483] Algo bellman_ford step 9482 current loss 0.794383, current_train_items 303456.
I0302 19:02:46.755212 22732373614720 run.py:483] Algo bellman_ford step 9483 current loss 0.692182, current_train_items 303488.
I0302 19:02:46.787478 22732373614720 run.py:483] Algo bellman_ford step 9484 current loss 0.832118, current_train_items 303520.
I0302 19:02:46.807487 22732373614720 run.py:483] Algo bellman_ford step 9485 current loss 0.354192, current_train_items 303552.
I0302 19:02:46.823993 22732373614720 run.py:483] Algo bellman_ford step 9486 current loss 0.519563, current_train_items 303584.
I0302 19:02:46.848663 22732373614720 run.py:483] Algo bellman_ford step 9487 current loss 0.623192, current_train_items 303616.
I0302 19:02:46.880994 22732373614720 run.py:483] Algo bellman_ford step 9488 current loss 0.650999, current_train_items 303648.
I0302 19:02:46.913552 22732373614720 run.py:483] Algo bellman_ford step 9489 current loss 0.766088, current_train_items 303680.
I0302 19:02:46.933390 22732373614720 run.py:483] Algo bellman_ford step 9490 current loss 0.380768, current_train_items 303712.
I0302 19:02:46.949510 22732373614720 run.py:483] Algo bellman_ford step 9491 current loss 0.511442, current_train_items 303744.
I0302 19:02:46.973566 22732373614720 run.py:483] Algo bellman_ford step 9492 current loss 0.738075, current_train_items 303776.
I0302 19:02:47.003984 22732373614720 run.py:483] Algo bellman_ford step 9493 current loss 0.603312, current_train_items 303808.
I0302 19:02:47.037028 22732373614720 run.py:483] Algo bellman_ford step 9494 current loss 0.854341, current_train_items 303840.
I0302 19:02:47.056829 22732373614720 run.py:483] Algo bellman_ford step 9495 current loss 0.368657, current_train_items 303872.
I0302 19:02:47.073428 22732373614720 run.py:483] Algo bellman_ford step 9496 current loss 0.530350, current_train_items 303904.
I0302 19:02:47.097095 22732373614720 run.py:483] Algo bellman_ford step 9497 current loss 0.622474, current_train_items 303936.
I0302 19:02:47.127336 22732373614720 run.py:483] Algo bellman_ford step 9498 current loss 0.669676, current_train_items 303968.
I0302 19:02:47.161118 22732373614720 run.py:483] Algo bellman_ford step 9499 current loss 0.829311, current_train_items 304000.
I0302 19:02:47.181307 22732373614720 run.py:483] Algo bellman_ford step 9500 current loss 0.270201, current_train_items 304032.
I0302 19:02:47.189319 22732373614720 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0302 19:02:47.189430 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:47.205922 22732373614720 run.py:483] Algo bellman_ford step 9501 current loss 0.441927, current_train_items 304064.
I0302 19:02:47.230623 22732373614720 run.py:483] Algo bellman_ford step 9502 current loss 0.647792, current_train_items 304096.
I0302 19:02:47.262258 22732373614720 run.py:483] Algo bellman_ford step 9503 current loss 0.718651, current_train_items 304128.
I0302 19:02:47.295271 22732373614720 run.py:483] Algo bellman_ford step 9504 current loss 0.722492, current_train_items 304160.
I0302 19:02:47.315558 22732373614720 run.py:483] Algo bellman_ford step 9505 current loss 0.346012, current_train_items 304192.
I0302 19:02:47.331677 22732373614720 run.py:483] Algo bellman_ford step 9506 current loss 0.572751, current_train_items 304224.
I0302 19:02:47.356300 22732373614720 run.py:483] Algo bellman_ford step 9507 current loss 0.571667, current_train_items 304256.
I0302 19:02:47.386594 22732373614720 run.py:483] Algo bellman_ford step 9508 current loss 0.669524, current_train_items 304288.
I0302 19:02:47.421695 22732373614720 run.py:483] Algo bellman_ford step 9509 current loss 0.854434, current_train_items 304320.
I0302 19:02:47.441847 22732373614720 run.py:483] Algo bellman_ford step 9510 current loss 0.359275, current_train_items 304352.
I0302 19:02:47.458513 22732373614720 run.py:483] Algo bellman_ford step 9511 current loss 0.634874, current_train_items 304384.
I0302 19:02:47.482454 22732373614720 run.py:483] Algo bellman_ford step 9512 current loss 0.730450, current_train_items 304416.
I0302 19:02:47.514066 22732373614720 run.py:483] Algo bellman_ford step 9513 current loss 0.804673, current_train_items 304448.
I0302 19:02:47.546658 22732373614720 run.py:483] Algo bellman_ford step 9514 current loss 0.756345, current_train_items 304480.
I0302 19:02:47.566267 22732373614720 run.py:483] Algo bellman_ford step 9515 current loss 0.290267, current_train_items 304512.
I0302 19:02:47.581994 22732373614720 run.py:483] Algo bellman_ford step 9516 current loss 0.495114, current_train_items 304544.
I0302 19:02:47.606134 22732373614720 run.py:483] Algo bellman_ford step 9517 current loss 0.770246, current_train_items 304576.
I0302 19:02:47.637413 22732373614720 run.py:483] Algo bellman_ford step 9518 current loss 0.877114, current_train_items 304608.
I0302 19:02:47.669385 22732373614720 run.py:483] Algo bellman_ford step 9519 current loss 0.895852, current_train_items 304640.
I0302 19:02:47.689140 22732373614720 run.py:483] Algo bellman_ford step 9520 current loss 0.365614, current_train_items 304672.
I0302 19:02:47.704632 22732373614720 run.py:483] Algo bellman_ford step 9521 current loss 0.446911, current_train_items 304704.
I0302 19:02:47.727787 22732373614720 run.py:483] Algo bellman_ford step 9522 current loss 0.608015, current_train_items 304736.
I0302 19:02:47.757844 22732373614720 run.py:483] Algo bellman_ford step 9523 current loss 0.642448, current_train_items 304768.
I0302 19:02:47.790217 22732373614720 run.py:483] Algo bellman_ford step 9524 current loss 0.745645, current_train_items 304800.
I0302 19:02:47.809586 22732373614720 run.py:483] Algo bellman_ford step 9525 current loss 0.278082, current_train_items 304832.
I0302 19:02:47.825661 22732373614720 run.py:483] Algo bellman_ford step 9526 current loss 0.478253, current_train_items 304864.
I0302 19:02:47.849877 22732373614720 run.py:483] Algo bellman_ford step 9527 current loss 0.702085, current_train_items 304896.
I0302 19:02:47.882179 22732373614720 run.py:483] Algo bellman_ford step 9528 current loss 0.705376, current_train_items 304928.
I0302 19:02:47.916588 22732373614720 run.py:483] Algo bellman_ford step 9529 current loss 0.825973, current_train_items 304960.
I0302 19:02:47.936553 22732373614720 run.py:483] Algo bellman_ford step 9530 current loss 0.311025, current_train_items 304992.
I0302 19:02:47.952549 22732373614720 run.py:483] Algo bellman_ford step 9531 current loss 0.466083, current_train_items 305024.
I0302 19:02:47.975752 22732373614720 run.py:483] Algo bellman_ford step 9532 current loss 0.573483, current_train_items 305056.
I0302 19:02:48.008488 22732373614720 run.py:483] Algo bellman_ford step 9533 current loss 0.752233, current_train_items 305088.
I0302 19:02:48.041409 22732373614720 run.py:483] Algo bellman_ford step 9534 current loss 0.731785, current_train_items 305120.
I0302 19:02:48.061280 22732373614720 run.py:483] Algo bellman_ford step 9535 current loss 0.390854, current_train_items 305152.
I0302 19:02:48.077366 22732373614720 run.py:483] Algo bellman_ford step 9536 current loss 0.600526, current_train_items 305184.
I0302 19:02:48.101261 22732373614720 run.py:483] Algo bellman_ford step 9537 current loss 0.644908, current_train_items 305216.
I0302 19:02:48.133787 22732373614720 run.py:483] Algo bellman_ford step 9538 current loss 0.669059, current_train_items 305248.
I0302 19:02:48.164828 22732373614720 run.py:483] Algo bellman_ford step 9539 current loss 0.737457, current_train_items 305280.
I0302 19:02:48.184500 22732373614720 run.py:483] Algo bellman_ford step 9540 current loss 0.354320, current_train_items 305312.
I0302 19:02:48.200357 22732373614720 run.py:483] Algo bellman_ford step 9541 current loss 0.474339, current_train_items 305344.
I0302 19:02:48.223626 22732373614720 run.py:483] Algo bellman_ford step 9542 current loss 0.702698, current_train_items 305376.
I0302 19:02:48.256530 22732373614720 run.py:483] Algo bellman_ford step 9543 current loss 0.949057, current_train_items 305408.
I0302 19:02:48.290225 22732373614720 run.py:483] Algo bellman_ford step 9544 current loss 0.886992, current_train_items 305440.
I0302 19:02:48.310470 22732373614720 run.py:483] Algo bellman_ford step 9545 current loss 0.356145, current_train_items 305472.
I0302 19:02:48.326018 22732373614720 run.py:483] Algo bellman_ford step 9546 current loss 0.552802, current_train_items 305504.
I0302 19:02:48.347653 22732373614720 run.py:483] Algo bellman_ford step 9547 current loss 0.562581, current_train_items 305536.
I0302 19:02:48.378589 22732373614720 run.py:483] Algo bellman_ford step 9548 current loss 0.869909, current_train_items 305568.
I0302 19:02:48.411954 22732373614720 run.py:483] Algo bellman_ford step 9549 current loss 0.963547, current_train_items 305600.
I0302 19:02:48.431633 22732373614720 run.py:483] Algo bellman_ford step 9550 current loss 0.285030, current_train_items 305632.
I0302 19:02:48.439761 22732373614720 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0302 19:02:48.439870 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 19:02:48.456729 22732373614720 run.py:483] Algo bellman_ford step 9551 current loss 0.524716, current_train_items 305664.
I0302 19:02:48.481065 22732373614720 run.py:483] Algo bellman_ford step 9552 current loss 0.816956, current_train_items 305696.
I0302 19:02:48.514575 22732373614720 run.py:483] Algo bellman_ford step 9553 current loss 0.696639, current_train_items 305728.
I0302 19:02:48.546923 22732373614720 run.py:483] Algo bellman_ford step 9554 current loss 0.690196, current_train_items 305760.
I0302 19:02:48.566827 22732373614720 run.py:483] Algo bellman_ford step 9555 current loss 0.376671, current_train_items 305792.
I0302 19:02:48.582218 22732373614720 run.py:483] Algo bellman_ford step 9556 current loss 0.511822, current_train_items 305824.
I0302 19:02:48.606468 22732373614720 run.py:483] Algo bellman_ford step 9557 current loss 0.648108, current_train_items 305856.
I0302 19:02:48.637324 22732373614720 run.py:483] Algo bellman_ford step 9558 current loss 0.716971, current_train_items 305888.
I0302 19:02:48.669926 22732373614720 run.py:483] Algo bellman_ford step 9559 current loss 0.967938, current_train_items 305920.
I0302 19:02:48.689806 22732373614720 run.py:483] Algo bellman_ford step 9560 current loss 0.380321, current_train_items 305952.
I0302 19:02:48.706113 22732373614720 run.py:483] Algo bellman_ford step 9561 current loss 0.507005, current_train_items 305984.
I0302 19:02:48.729085 22732373614720 run.py:483] Algo bellman_ford step 9562 current loss 0.615702, current_train_items 306016.
I0302 19:02:48.760085 22732373614720 run.py:483] Algo bellman_ford step 9563 current loss 0.675720, current_train_items 306048.
I0302 19:02:48.792367 22732373614720 run.py:483] Algo bellman_ford step 9564 current loss 0.719582, current_train_items 306080.
I0302 19:02:48.812003 22732373614720 run.py:483] Algo bellman_ford step 9565 current loss 0.376863, current_train_items 306112.
I0302 19:02:48.828633 22732373614720 run.py:483] Algo bellman_ford step 9566 current loss 0.510094, current_train_items 306144.
I0302 19:02:48.852998 22732373614720 run.py:483] Algo bellman_ford step 9567 current loss 0.817163, current_train_items 306176.
I0302 19:02:48.884797 22732373614720 run.py:483] Algo bellman_ford step 9568 current loss 0.778196, current_train_items 306208.
I0302 19:02:48.917482 22732373614720 run.py:483] Algo bellman_ford step 9569 current loss 0.803551, current_train_items 306240.
I0302 19:02:48.937469 22732373614720 run.py:483] Algo bellman_ford step 9570 current loss 0.366205, current_train_items 306272.
I0302 19:02:48.953631 22732373614720 run.py:483] Algo bellman_ford step 9571 current loss 0.481325, current_train_items 306304.
I0302 19:02:48.977899 22732373614720 run.py:483] Algo bellman_ford step 9572 current loss 0.754280, current_train_items 306336.
I0302 19:02:49.008403 22732373614720 run.py:483] Algo bellman_ford step 9573 current loss 0.634238, current_train_items 306368.
I0302 19:02:49.042215 22732373614720 run.py:483] Algo bellman_ford step 9574 current loss 0.961386, current_train_items 306400.
I0302 19:02:49.062285 22732373614720 run.py:483] Algo bellman_ford step 9575 current loss 0.355667, current_train_items 306432.
I0302 19:02:49.079173 22732373614720 run.py:483] Algo bellman_ford step 9576 current loss 0.490217, current_train_items 306464.
I0302 19:02:49.103492 22732373614720 run.py:483] Algo bellman_ford step 9577 current loss 0.704665, current_train_items 306496.
I0302 19:02:49.134255 22732373614720 run.py:483] Algo bellman_ford step 9578 current loss 0.602104, current_train_items 306528.
I0302 19:02:49.167121 22732373614720 run.py:483] Algo bellman_ford step 9579 current loss 0.698777, current_train_items 306560.
I0302 19:02:49.186912 22732373614720 run.py:483] Algo bellman_ford step 9580 current loss 0.325220, current_train_items 306592.
I0302 19:02:49.202781 22732373614720 run.py:483] Algo bellman_ford step 9581 current loss 0.515966, current_train_items 306624.
I0302 19:02:49.227563 22732373614720 run.py:483] Algo bellman_ford step 9582 current loss 0.609182, current_train_items 306656.
I0302 19:02:49.260231 22732373614720 run.py:483] Algo bellman_ford step 9583 current loss 0.705182, current_train_items 306688.
I0302 19:02:49.293997 22732373614720 run.py:483] Algo bellman_ford step 9584 current loss 0.719795, current_train_items 306720.
I0302 19:02:49.313755 22732373614720 run.py:483] Algo bellman_ford step 9585 current loss 0.363805, current_train_items 306752.
I0302 19:02:49.329982 22732373614720 run.py:483] Algo bellman_ford step 9586 current loss 0.470655, current_train_items 306784.
I0302 19:02:49.353619 22732373614720 run.py:483] Algo bellman_ford step 9587 current loss 0.667295, current_train_items 306816.
I0302 19:02:49.385280 22732373614720 run.py:483] Algo bellman_ford step 9588 current loss 0.718103, current_train_items 306848.
I0302 19:02:49.419666 22732373614720 run.py:483] Algo bellman_ford step 9589 current loss 0.844417, current_train_items 306880.
I0302 19:02:49.439411 22732373614720 run.py:483] Algo bellman_ford step 9590 current loss 0.304685, current_train_items 306912.
I0302 19:02:49.455507 22732373614720 run.py:483] Algo bellman_ford step 9591 current loss 0.430744, current_train_items 306944.
I0302 19:02:49.478872 22732373614720 run.py:483] Algo bellman_ford step 9592 current loss 0.673922, current_train_items 306976.
I0302 19:02:49.510365 22732373614720 run.py:483] Algo bellman_ford step 9593 current loss 0.653626, current_train_items 307008.
I0302 19:02:49.542937 22732373614720 run.py:483] Algo bellman_ford step 9594 current loss 0.869048, current_train_items 307040.
I0302 19:02:49.562745 22732373614720 run.py:483] Algo bellman_ford step 9595 current loss 0.362255, current_train_items 307072.
I0302 19:02:49.579164 22732373614720 run.py:483] Algo bellman_ford step 9596 current loss 0.527326, current_train_items 307104.
I0302 19:02:49.603674 22732373614720 run.py:483] Algo bellman_ford step 9597 current loss 0.595028, current_train_items 307136.
I0302 19:02:49.633808 22732373614720 run.py:483] Algo bellman_ford step 9598 current loss 0.556405, current_train_items 307168.
I0302 19:02:49.666844 22732373614720 run.py:483] Algo bellman_ford step 9599 current loss 0.762680, current_train_items 307200.
I0302 19:02:49.686597 22732373614720 run.py:483] Algo bellman_ford step 9600 current loss 0.337196, current_train_items 307232.
I0302 19:02:49.694660 22732373614720 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0302 19:02:49.694770 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:02:49.711893 22732373614720 run.py:483] Algo bellman_ford step 9601 current loss 0.558689, current_train_items 307264.
I0302 19:02:49.736752 22732373614720 run.py:483] Algo bellman_ford step 9602 current loss 0.662589, current_train_items 307296.
I0302 19:02:49.767715 22732373614720 run.py:483] Algo bellman_ford step 9603 current loss 0.659948, current_train_items 307328.
I0302 19:02:49.802782 22732373614720 run.py:483] Algo bellman_ford step 9604 current loss 0.818732, current_train_items 307360.
I0302 19:02:49.822691 22732373614720 run.py:483] Algo bellman_ford step 9605 current loss 0.366695, current_train_items 307392.
I0302 19:02:49.838324 22732373614720 run.py:483] Algo bellman_ford step 9606 current loss 0.542964, current_train_items 307424.
I0302 19:02:49.862164 22732373614720 run.py:483] Algo bellman_ford step 9607 current loss 0.677671, current_train_items 307456.
I0302 19:02:49.893090 22732373614720 run.py:483] Algo bellman_ford step 9608 current loss 0.797437, current_train_items 307488.
I0302 19:02:49.926213 22732373614720 run.py:483] Algo bellman_ford step 9609 current loss 0.976131, current_train_items 307520.
I0302 19:02:49.945732 22732373614720 run.py:483] Algo bellman_ford step 9610 current loss 0.324746, current_train_items 307552.
I0302 19:02:49.961992 22732373614720 run.py:483] Algo bellman_ford step 9611 current loss 0.532065, current_train_items 307584.
I0302 19:02:49.986152 22732373614720 run.py:483] Algo bellman_ford step 9612 current loss 0.611356, current_train_items 307616.
I0302 19:02:50.017664 22732373614720 run.py:483] Algo bellman_ford step 9613 current loss 0.751195, current_train_items 307648.
I0302 19:02:50.050706 22732373614720 run.py:483] Algo bellman_ford step 9614 current loss 0.761679, current_train_items 307680.
I0302 19:02:50.070717 22732373614720 run.py:483] Algo bellman_ford step 9615 current loss 0.372842, current_train_items 307712.
I0302 19:02:50.086774 22732373614720 run.py:483] Algo bellman_ford step 9616 current loss 0.630257, current_train_items 307744.
I0302 19:02:50.110713 22732373614720 run.py:483] Algo bellman_ford step 9617 current loss 0.654444, current_train_items 307776.
I0302 19:02:50.141663 22732373614720 run.py:483] Algo bellman_ford step 9618 current loss 0.628262, current_train_items 307808.
I0302 19:02:50.175029 22732373614720 run.py:483] Algo bellman_ford step 9619 current loss 0.832191, current_train_items 307840.
I0302 19:02:50.194807 22732373614720 run.py:483] Algo bellman_ford step 9620 current loss 0.363397, current_train_items 307872.
I0302 19:02:50.210832 22732373614720 run.py:483] Algo bellman_ford step 9621 current loss 0.424619, current_train_items 307904.
I0302 19:02:50.234557 22732373614720 run.py:483] Algo bellman_ford step 9622 current loss 0.631336, current_train_items 307936.
I0302 19:02:50.266008 22732373614720 run.py:483] Algo bellman_ford step 9623 current loss 0.655305, current_train_items 307968.
I0302 19:02:50.299680 22732373614720 run.py:483] Algo bellman_ford step 9624 current loss 0.852888, current_train_items 308000.
I0302 19:02:50.319386 22732373614720 run.py:483] Algo bellman_ford step 9625 current loss 0.352772, current_train_items 308032.
I0302 19:02:50.335319 22732373614720 run.py:483] Algo bellman_ford step 9626 current loss 0.527027, current_train_items 308064.
I0302 19:02:50.358983 22732373614720 run.py:483] Algo bellman_ford step 9627 current loss 0.580130, current_train_items 308096.
I0302 19:02:50.389861 22732373614720 run.py:483] Algo bellman_ford step 9628 current loss 0.633977, current_train_items 308128.
I0302 19:02:50.424234 22732373614720 run.py:483] Algo bellman_ford step 9629 current loss 0.851957, current_train_items 308160.
I0302 19:02:50.443720 22732373614720 run.py:483] Algo bellman_ford step 9630 current loss 0.355445, current_train_items 308192.
I0302 19:02:50.460028 22732373614720 run.py:483] Algo bellman_ford step 9631 current loss 0.470385, current_train_items 308224.
I0302 19:02:50.484612 22732373614720 run.py:483] Algo bellman_ford step 9632 current loss 0.659313, current_train_items 308256.
I0302 19:02:50.515169 22732373614720 run.py:483] Algo bellman_ford step 9633 current loss 0.619165, current_train_items 308288.
I0302 19:02:50.549055 22732373614720 run.py:483] Algo bellman_ford step 9634 current loss 0.839249, current_train_items 308320.
I0302 19:02:50.568624 22732373614720 run.py:483] Algo bellman_ford step 9635 current loss 0.308376, current_train_items 308352.
I0302 19:02:50.584619 22732373614720 run.py:483] Algo bellman_ford step 9636 current loss 0.478241, current_train_items 308384.
I0302 19:02:50.608417 22732373614720 run.py:483] Algo bellman_ford step 9637 current loss 0.640230, current_train_items 308416.
I0302 19:02:50.640298 22732373614720 run.py:483] Algo bellman_ford step 9638 current loss 0.737384, current_train_items 308448.
I0302 19:02:50.673221 22732373614720 run.py:483] Algo bellman_ford step 9639 current loss 0.800851, current_train_items 308480.
I0302 19:02:50.692914 22732373614720 run.py:483] Algo bellman_ford step 9640 current loss 0.270999, current_train_items 308512.
I0302 19:02:50.708628 22732373614720 run.py:483] Algo bellman_ford step 9641 current loss 0.397536, current_train_items 308544.
I0302 19:02:50.732533 22732373614720 run.py:483] Algo bellman_ford step 9642 current loss 0.660759, current_train_items 308576.
I0302 19:02:50.765373 22732373614720 run.py:483] Algo bellman_ford step 9643 current loss 0.725197, current_train_items 308608.
I0302 19:02:50.796886 22732373614720 run.py:483] Algo bellman_ford step 9644 current loss 0.857751, current_train_items 308640.
I0302 19:02:50.816138 22732373614720 run.py:483] Algo bellman_ford step 9645 current loss 0.315183, current_train_items 308672.
I0302 19:02:50.832263 22732373614720 run.py:483] Algo bellman_ford step 9646 current loss 0.570696, current_train_items 308704.
I0302 19:02:50.855550 22732373614720 run.py:483] Algo bellman_ford step 9647 current loss 0.662525, current_train_items 308736.
I0302 19:02:50.887776 22732373614720 run.py:483] Algo bellman_ford step 9648 current loss 0.734132, current_train_items 308768.
I0302 19:02:50.921609 22732373614720 run.py:483] Algo bellman_ford step 9649 current loss 0.870209, current_train_items 308800.
I0302 19:02:50.941050 22732373614720 run.py:483] Algo bellman_ford step 9650 current loss 0.361097, current_train_items 308832.
I0302 19:02:50.949389 22732373614720 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0302 19:02:50.949522 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:02:50.966229 22732373614720 run.py:483] Algo bellman_ford step 9651 current loss 0.454372, current_train_items 308864.
I0302 19:02:50.991802 22732373614720 run.py:483] Algo bellman_ford step 9652 current loss 0.686827, current_train_items 308896.
I0302 19:02:51.023601 22732373614720 run.py:483] Algo bellman_ford step 9653 current loss 0.735709, current_train_items 308928.
I0302 19:02:51.059167 22732373614720 run.py:483] Algo bellman_ford step 9654 current loss 0.858183, current_train_items 308960.
I0302 19:02:51.079213 22732373614720 run.py:483] Algo bellman_ford step 9655 current loss 0.396210, current_train_items 308992.
I0302 19:02:51.095116 22732373614720 run.py:483] Algo bellman_ford step 9656 current loss 0.549383, current_train_items 309024.
I0302 19:02:51.118857 22732373614720 run.py:483] Algo bellman_ford step 9657 current loss 0.680072, current_train_items 309056.
I0302 19:02:51.150898 22732373614720 run.py:483] Algo bellman_ford step 9658 current loss 0.731197, current_train_items 309088.
I0302 19:02:51.182729 22732373614720 run.py:483] Algo bellman_ford step 9659 current loss 0.791340, current_train_items 309120.
I0302 19:02:51.202435 22732373614720 run.py:483] Algo bellman_ford step 9660 current loss 0.327399, current_train_items 309152.
I0302 19:02:51.218985 22732373614720 run.py:483] Algo bellman_ford step 9661 current loss 0.495614, current_train_items 309184.
I0302 19:02:51.242339 22732373614720 run.py:483] Algo bellman_ford step 9662 current loss 0.641768, current_train_items 309216.
I0302 19:02:51.273548 22732373614720 run.py:483] Algo bellman_ford step 9663 current loss 0.633248, current_train_items 309248.
I0302 19:02:51.307167 22732373614720 run.py:483] Algo bellman_ford step 9664 current loss 0.851746, current_train_items 309280.
I0302 19:02:51.327187 22732373614720 run.py:483] Algo bellman_ford step 9665 current loss 0.386390, current_train_items 309312.
I0302 19:02:51.343275 22732373614720 run.py:483] Algo bellman_ford step 9666 current loss 0.487276, current_train_items 309344.
I0302 19:02:51.366292 22732373614720 run.py:483] Algo bellman_ford step 9667 current loss 0.674579, current_train_items 309376.
I0302 19:02:51.397056 22732373614720 run.py:483] Algo bellman_ford step 9668 current loss 0.600491, current_train_items 309408.
I0302 19:02:51.430034 22732373614720 run.py:483] Algo bellman_ford step 9669 current loss 0.765268, current_train_items 309440.
I0302 19:02:51.449882 22732373614720 run.py:483] Algo bellman_ford step 9670 current loss 0.313218, current_train_items 309472.
I0302 19:02:51.465940 22732373614720 run.py:483] Algo bellman_ford step 9671 current loss 0.433242, current_train_items 309504.
I0302 19:02:51.489870 22732373614720 run.py:483] Algo bellman_ford step 9672 current loss 0.621811, current_train_items 309536.
I0302 19:02:51.520715 22732373614720 run.py:483] Algo bellman_ford step 9673 current loss 0.614245, current_train_items 309568.
I0302 19:02:51.554054 22732373614720 run.py:483] Algo bellman_ford step 9674 current loss 0.765019, current_train_items 309600.
I0302 19:02:51.573969 22732373614720 run.py:483] Algo bellman_ford step 9675 current loss 0.322025, current_train_items 309632.
I0302 19:02:51.590414 22732373614720 run.py:483] Algo bellman_ford step 9676 current loss 0.564525, current_train_items 309664.
I0302 19:02:51.614328 22732373614720 run.py:483] Algo bellman_ford step 9677 current loss 0.613887, current_train_items 309696.
I0302 19:02:51.644480 22732373614720 run.py:483] Algo bellman_ford step 9678 current loss 0.647708, current_train_items 309728.
I0302 19:02:51.678168 22732373614720 run.py:483] Algo bellman_ford step 9679 current loss 0.787674, current_train_items 309760.
I0302 19:02:51.697857 22732373614720 run.py:483] Algo bellman_ford step 9680 current loss 0.298802, current_train_items 309792.
I0302 19:02:51.714070 22732373614720 run.py:483] Algo bellman_ford step 9681 current loss 0.489301, current_train_items 309824.
I0302 19:02:51.737743 22732373614720 run.py:483] Algo bellman_ford step 9682 current loss 0.559972, current_train_items 309856.
I0302 19:02:51.768490 22732373614720 run.py:483] Algo bellman_ford step 9683 current loss 0.604476, current_train_items 309888.
I0302 19:02:51.800634 22732373614720 run.py:483] Algo bellman_ford step 9684 current loss 0.781089, current_train_items 309920.
I0302 19:02:51.820451 22732373614720 run.py:483] Algo bellman_ford step 9685 current loss 0.343937, current_train_items 309952.
I0302 19:02:51.836393 22732373614720 run.py:483] Algo bellman_ford step 9686 current loss 0.569618, current_train_items 309984.
I0302 19:02:51.860078 22732373614720 run.py:483] Algo bellman_ford step 9687 current loss 0.653777, current_train_items 310016.
I0302 19:02:51.890833 22732373614720 run.py:483] Algo bellman_ford step 9688 current loss 0.815290, current_train_items 310048.
I0302 19:02:51.923994 22732373614720 run.py:483] Algo bellman_ford step 9689 current loss 0.788432, current_train_items 310080.
I0302 19:02:51.943948 22732373614720 run.py:483] Algo bellman_ford step 9690 current loss 0.314584, current_train_items 310112.
I0302 19:02:51.959709 22732373614720 run.py:483] Algo bellman_ford step 9691 current loss 0.459705, current_train_items 310144.
I0302 19:02:51.983035 22732373614720 run.py:483] Algo bellman_ford step 9692 current loss 0.530026, current_train_items 310176.
I0302 19:02:52.013801 22732373614720 run.py:483] Algo bellman_ford step 9693 current loss 0.693113, current_train_items 310208.
I0302 19:02:52.047165 22732373614720 run.py:483] Algo bellman_ford step 9694 current loss 0.763305, current_train_items 310240.
I0302 19:02:52.066622 22732373614720 run.py:483] Algo bellman_ford step 9695 current loss 0.396545, current_train_items 310272.
I0302 19:02:52.082675 22732373614720 run.py:483] Algo bellman_ford step 9696 current loss 0.590001, current_train_items 310304.
I0302 19:02:52.105684 22732373614720 run.py:483] Algo bellman_ford step 9697 current loss 0.529404, current_train_items 310336.
I0302 19:02:52.139321 22732373614720 run.py:483] Algo bellman_ford step 9698 current loss 0.762390, current_train_items 310368.
I0302 19:02:52.174317 22732373614720 run.py:483] Algo bellman_ford step 9699 current loss 0.818334, current_train_items 310400.
I0302 19:02:52.194318 22732373614720 run.py:483] Algo bellman_ford step 9700 current loss 0.334473, current_train_items 310432.
I0302 19:02:52.202282 22732373614720 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0302 19:02:52.202392 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 19:02:52.219051 22732373614720 run.py:483] Algo bellman_ford step 9701 current loss 0.594523, current_train_items 310464.
I0302 19:02:52.243692 22732373614720 run.py:483] Algo bellman_ford step 9702 current loss 0.657874, current_train_items 310496.
I0302 19:02:52.276012 22732373614720 run.py:483] Algo bellman_ford step 9703 current loss 0.707936, current_train_items 310528.
I0302 19:02:52.308864 22732373614720 run.py:483] Algo bellman_ford step 9704 current loss 0.770855, current_train_items 310560.
I0302 19:02:52.328917 22732373614720 run.py:483] Algo bellman_ford step 9705 current loss 0.280054, current_train_items 310592.
I0302 19:02:52.344649 22732373614720 run.py:483] Algo bellman_ford step 9706 current loss 0.479928, current_train_items 310624.
I0302 19:02:52.369024 22732373614720 run.py:483] Algo bellman_ford step 9707 current loss 0.752872, current_train_items 310656.
I0302 19:02:52.400667 22732373614720 run.py:483] Algo bellman_ford step 9708 current loss 0.713131, current_train_items 310688.
I0302 19:02:52.431905 22732373614720 run.py:483] Algo bellman_ford step 9709 current loss 0.740800, current_train_items 310720.
I0302 19:02:52.451395 22732373614720 run.py:483] Algo bellman_ford step 9710 current loss 0.299098, current_train_items 310752.
I0302 19:02:52.467288 22732373614720 run.py:483] Algo bellman_ford step 9711 current loss 0.501876, current_train_items 310784.
I0302 19:02:52.490803 22732373614720 run.py:483] Algo bellman_ford step 9712 current loss 0.705890, current_train_items 310816.
I0302 19:02:52.522390 22732373614720 run.py:483] Algo bellman_ford step 9713 current loss 0.604818, current_train_items 310848.
I0302 19:02:52.556064 22732373614720 run.py:483] Algo bellman_ford step 9714 current loss 0.839903, current_train_items 310880.
I0302 19:02:52.575397 22732373614720 run.py:483] Algo bellman_ford step 9715 current loss 0.361852, current_train_items 310912.
I0302 19:02:52.591759 22732373614720 run.py:483] Algo bellman_ford step 9716 current loss 0.457510, current_train_items 310944.
I0302 19:02:52.614055 22732373614720 run.py:483] Algo bellman_ford step 9717 current loss 0.633203, current_train_items 310976.
I0302 19:02:52.645593 22732373614720 run.py:483] Algo bellman_ford step 9718 current loss 0.719584, current_train_items 311008.
I0302 19:02:52.676196 22732373614720 run.py:483] Algo bellman_ford step 9719 current loss 0.704796, current_train_items 311040.
I0302 19:02:52.695532 22732373614720 run.py:483] Algo bellman_ford step 9720 current loss 0.350524, current_train_items 311072.
I0302 19:02:52.711106 22732373614720 run.py:483] Algo bellman_ford step 9721 current loss 0.447303, current_train_items 311104.
I0302 19:02:52.734198 22732373614720 run.py:483] Algo bellman_ford step 9722 current loss 0.693215, current_train_items 311136.
I0302 19:02:52.765655 22732373614720 run.py:483] Algo bellman_ford step 9723 current loss 0.738799, current_train_items 311168.
I0302 19:02:52.798572 22732373614720 run.py:483] Algo bellman_ford step 9724 current loss 0.676856, current_train_items 311200.
I0302 19:02:52.818266 22732373614720 run.py:483] Algo bellman_ford step 9725 current loss 0.304564, current_train_items 311232.
I0302 19:02:52.834079 22732373614720 run.py:483] Algo bellman_ford step 9726 current loss 0.530246, current_train_items 311264.
I0302 19:02:52.856942 22732373614720 run.py:483] Algo bellman_ford step 9727 current loss 0.582585, current_train_items 311296.
I0302 19:02:52.888534 22732373614720 run.py:483] Algo bellman_ford step 9728 current loss 0.762092, current_train_items 311328.
I0302 19:02:52.921977 22732373614720 run.py:483] Algo bellman_ford step 9729 current loss 0.893789, current_train_items 311360.
I0302 19:02:52.941528 22732373614720 run.py:483] Algo bellman_ford step 9730 current loss 0.361862, current_train_items 311392.
I0302 19:02:52.957775 22732373614720 run.py:483] Algo bellman_ford step 9731 current loss 0.530103, current_train_items 311424.
I0302 19:02:52.980893 22732373614720 run.py:483] Algo bellman_ford step 9732 current loss 0.771945, current_train_items 311456.
I0302 19:02:53.011739 22732373614720 run.py:483] Algo bellman_ford step 9733 current loss 0.724065, current_train_items 311488.
I0302 19:02:53.043662 22732373614720 run.py:483] Algo bellman_ford step 9734 current loss 0.721362, current_train_items 311520.
I0302 19:02:53.063379 22732373614720 run.py:483] Algo bellman_ford step 9735 current loss 0.358371, current_train_items 311552.
I0302 19:02:53.079415 22732373614720 run.py:483] Algo bellman_ford step 9736 current loss 0.561302, current_train_items 311584.
I0302 19:02:53.102738 22732373614720 run.py:483] Algo bellman_ford step 9737 current loss 0.634078, current_train_items 311616.
I0302 19:02:53.135815 22732373614720 run.py:483] Algo bellman_ford step 9738 current loss 0.708383, current_train_items 311648.
I0302 19:02:53.169856 22732373614720 run.py:483] Algo bellman_ford step 9739 current loss 0.800888, current_train_items 311680.
I0302 19:02:53.189528 22732373614720 run.py:483] Algo bellman_ford step 9740 current loss 0.315477, current_train_items 311712.
I0302 19:02:53.206319 22732373614720 run.py:483] Algo bellman_ford step 9741 current loss 0.489357, current_train_items 311744.
I0302 19:02:53.231262 22732373614720 run.py:483] Algo bellman_ford step 9742 current loss 0.775818, current_train_items 311776.
I0302 19:02:53.263358 22732373614720 run.py:483] Algo bellman_ford step 9743 current loss 0.775580, current_train_items 311808.
I0302 19:02:53.296671 22732373614720 run.py:483] Algo bellman_ford step 9744 current loss 0.831701, current_train_items 311840.
I0302 19:02:53.316254 22732373614720 run.py:483] Algo bellman_ford step 9745 current loss 0.297717, current_train_items 311872.
I0302 19:02:53.332988 22732373614720 run.py:483] Algo bellman_ford step 9746 current loss 0.471672, current_train_items 311904.
I0302 19:02:53.356151 22732373614720 run.py:483] Algo bellman_ford step 9747 current loss 0.595076, current_train_items 311936.
I0302 19:02:53.385367 22732373614720 run.py:483] Algo bellman_ford step 9748 current loss 0.598334, current_train_items 311968.
I0302 19:02:53.417107 22732373614720 run.py:483] Algo bellman_ford step 9749 current loss 0.704711, current_train_items 312000.
I0302 19:02:53.436514 22732373614720 run.py:483] Algo bellman_ford step 9750 current loss 0.355514, current_train_items 312032.
I0302 19:02:53.444524 22732373614720 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0302 19:02:53.444638 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:02:53.461222 22732373614720 run.py:483] Algo bellman_ford step 9751 current loss 0.530460, current_train_items 312064.
I0302 19:02:53.486599 22732373614720 run.py:483] Algo bellman_ford step 9752 current loss 0.709136, current_train_items 312096.
I0302 19:02:53.517381 22732373614720 run.py:483] Algo bellman_ford step 9753 current loss 0.766897, current_train_items 312128.
I0302 19:02:53.550280 22732373614720 run.py:483] Algo bellman_ford step 9754 current loss 0.823746, current_train_items 312160.
I0302 19:02:53.570141 22732373614720 run.py:483] Algo bellman_ford step 9755 current loss 0.347288, current_train_items 312192.
I0302 19:02:53.586605 22732373614720 run.py:483] Algo bellman_ford step 9756 current loss 0.586060, current_train_items 312224.
I0302 19:02:53.609403 22732373614720 run.py:483] Algo bellman_ford step 9757 current loss 0.558148, current_train_items 312256.
I0302 19:02:53.641102 22732373614720 run.py:483] Algo bellman_ford step 9758 current loss 0.762442, current_train_items 312288.
I0302 19:02:53.673735 22732373614720 run.py:483] Algo bellman_ford step 9759 current loss 0.820818, current_train_items 312320.
I0302 19:02:53.693795 22732373614720 run.py:483] Algo bellman_ford step 9760 current loss 0.307666, current_train_items 312352.
I0302 19:02:53.710315 22732373614720 run.py:483] Algo bellman_ford step 9761 current loss 0.592100, current_train_items 312384.
I0302 19:02:53.734203 22732373614720 run.py:483] Algo bellman_ford step 9762 current loss 0.722534, current_train_items 312416.
I0302 19:02:53.766180 22732373614720 run.py:483] Algo bellman_ford step 9763 current loss 0.707699, current_train_items 312448.
I0302 19:02:53.796524 22732373614720 run.py:483] Algo bellman_ford step 9764 current loss 0.682753, current_train_items 312480.
I0302 19:02:53.815990 22732373614720 run.py:483] Algo bellman_ford step 9765 current loss 0.301268, current_train_items 312512.
I0302 19:02:53.831743 22732373614720 run.py:483] Algo bellman_ford step 9766 current loss 0.477726, current_train_items 312544.
I0302 19:02:53.855783 22732373614720 run.py:483] Algo bellman_ford step 9767 current loss 0.688353, current_train_items 312576.
I0302 19:02:53.886665 22732373614720 run.py:483] Algo bellman_ford step 9768 current loss 0.746513, current_train_items 312608.
I0302 19:02:53.920108 22732373614720 run.py:483] Algo bellman_ford step 9769 current loss 0.823522, current_train_items 312640.
I0302 19:02:53.940314 22732373614720 run.py:483] Algo bellman_ford step 9770 current loss 0.392104, current_train_items 312672.
I0302 19:02:53.956007 22732373614720 run.py:483] Algo bellman_ford step 9771 current loss 0.599972, current_train_items 312704.
I0302 19:02:53.978551 22732373614720 run.py:483] Algo bellman_ford step 9772 current loss 0.661140, current_train_items 312736.
I0302 19:02:54.010818 22732373614720 run.py:483] Algo bellman_ford step 9773 current loss 0.775989, current_train_items 312768.
I0302 19:02:54.043817 22732373614720 run.py:483] Algo bellman_ford step 9774 current loss 0.901942, current_train_items 312800.
I0302 19:02:54.063434 22732373614720 run.py:483] Algo bellman_ford step 9775 current loss 0.328861, current_train_items 312832.
I0302 19:02:54.079308 22732373614720 run.py:483] Algo bellman_ford step 9776 current loss 0.470144, current_train_items 312864.
I0302 19:02:54.103147 22732373614720 run.py:483] Algo bellman_ford step 9777 current loss 0.607253, current_train_items 312896.
I0302 19:02:54.134906 22732373614720 run.py:483] Algo bellman_ford step 9778 current loss 0.718495, current_train_items 312928.
I0302 19:02:54.168177 22732373614720 run.py:483] Algo bellman_ford step 9779 current loss 0.740005, current_train_items 312960.
I0302 19:02:54.187512 22732373614720 run.py:483] Algo bellman_ford step 9780 current loss 0.353212, current_train_items 312992.
I0302 19:02:54.203784 22732373614720 run.py:483] Algo bellman_ford step 9781 current loss 0.542549, current_train_items 313024.
I0302 19:02:54.226831 22732373614720 run.py:483] Algo bellman_ford step 9782 current loss 0.676036, current_train_items 313056.
I0302 19:02:54.260028 22732373614720 run.py:483] Algo bellman_ford step 9783 current loss 0.735776, current_train_items 313088.
I0302 19:02:54.291639 22732373614720 run.py:483] Algo bellman_ford step 9784 current loss 0.720946, current_train_items 313120.
I0302 19:02:54.311454 22732373614720 run.py:483] Algo bellman_ford step 9785 current loss 0.303899, current_train_items 313152.
I0302 19:02:54.327926 22732373614720 run.py:483] Algo bellman_ford step 9786 current loss 0.597699, current_train_items 313184.
I0302 19:02:54.350878 22732373614720 run.py:483] Algo bellman_ford step 9787 current loss 0.680880, current_train_items 313216.
I0302 19:02:54.381310 22732373614720 run.py:483] Algo bellman_ford step 9788 current loss 0.671920, current_train_items 313248.
I0302 19:02:54.414687 22732373614720 run.py:483] Algo bellman_ford step 9789 current loss 0.764055, current_train_items 313280.
I0302 19:02:54.434259 22732373614720 run.py:483] Algo bellman_ford step 9790 current loss 0.423569, current_train_items 313312.
I0302 19:02:54.450166 22732373614720 run.py:483] Algo bellman_ford step 9791 current loss 0.475264, current_train_items 313344.
I0302 19:02:54.472896 22732373614720 run.py:483] Algo bellman_ford step 9792 current loss 0.636595, current_train_items 313376.
I0302 19:02:54.503930 22732373614720 run.py:483] Algo bellman_ford step 9793 current loss 0.731870, current_train_items 313408.
I0302 19:02:54.534813 22732373614720 run.py:483] Algo bellman_ford step 9794 current loss 0.745989, current_train_items 313440.
I0302 19:02:54.554390 22732373614720 run.py:483] Algo bellman_ford step 9795 current loss 0.366628, current_train_items 313472.
I0302 19:02:54.570860 22732373614720 run.py:483] Algo bellman_ford step 9796 current loss 0.467858, current_train_items 313504.
I0302 19:02:54.594316 22732373614720 run.py:483] Algo bellman_ford step 9797 current loss 0.618012, current_train_items 313536.
I0302 19:02:54.624793 22732373614720 run.py:483] Algo bellman_ford step 9798 current loss 0.690027, current_train_items 313568.
I0302 19:02:54.656925 22732373614720 run.py:483] Algo bellman_ford step 9799 current loss 0.734451, current_train_items 313600.
I0302 19:02:54.676957 22732373614720 run.py:483] Algo bellman_ford step 9800 current loss 0.396356, current_train_items 313632.
I0302 19:02:54.684955 22732373614720 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0302 19:02:54.685063 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:02:54.701565 22732373614720 run.py:483] Algo bellman_ford step 9801 current loss 0.508887, current_train_items 313664.
I0302 19:02:54.726083 22732373614720 run.py:483] Algo bellman_ford step 9802 current loss 0.617257, current_train_items 313696.
I0302 19:02:54.758562 22732373614720 run.py:483] Algo bellman_ford step 9803 current loss 0.804460, current_train_items 313728.
I0302 19:02:54.791166 22732373614720 run.py:483] Algo bellman_ford step 9804 current loss 0.714780, current_train_items 313760.
I0302 19:02:54.811364 22732373614720 run.py:483] Algo bellman_ford step 9805 current loss 0.328563, current_train_items 313792.
I0302 19:02:54.827411 22732373614720 run.py:483] Algo bellman_ford step 9806 current loss 0.610874, current_train_items 313824.
I0302 19:02:54.850330 22732373614720 run.py:483] Algo bellman_ford step 9807 current loss 0.642317, current_train_items 313856.
I0302 19:02:54.881769 22732373614720 run.py:483] Algo bellman_ford step 9808 current loss 0.662605, current_train_items 313888.
I0302 19:02:54.916585 22732373614720 run.py:483] Algo bellman_ford step 9809 current loss 0.888840, current_train_items 313920.
I0302 19:02:54.936397 22732373614720 run.py:483] Algo bellman_ford step 9810 current loss 0.294055, current_train_items 313952.
I0302 19:02:54.952169 22732373614720 run.py:483] Algo bellman_ford step 9811 current loss 0.516689, current_train_items 313984.
I0302 19:02:54.976764 22732373614720 run.py:483] Algo bellman_ford step 9812 current loss 0.774533, current_train_items 314016.
I0302 19:02:55.006067 22732373614720 run.py:483] Algo bellman_ford step 9813 current loss 0.644368, current_train_items 314048.
I0302 19:02:55.039482 22732373614720 run.py:483] Algo bellman_ford step 9814 current loss 0.712489, current_train_items 314080.
I0302 19:02:55.059199 22732373614720 run.py:483] Algo bellman_ford step 9815 current loss 0.322492, current_train_items 314112.
I0302 19:02:55.075580 22732373614720 run.py:483] Algo bellman_ford step 9816 current loss 0.536917, current_train_items 314144.
I0302 19:02:55.098885 22732373614720 run.py:483] Algo bellman_ford step 9817 current loss 0.651193, current_train_items 314176.
I0302 19:02:55.131460 22732373614720 run.py:483] Algo bellman_ford step 9818 current loss 0.798085, current_train_items 314208.
I0302 19:02:55.164068 22732373614720 run.py:483] Algo bellman_ford step 9819 current loss 0.775000, current_train_items 314240.
I0302 19:02:55.183676 22732373614720 run.py:483] Algo bellman_ford step 9820 current loss 0.361168, current_train_items 314272.
I0302 19:02:55.199652 22732373614720 run.py:483] Algo bellman_ford step 9821 current loss 0.589173, current_train_items 314304.
I0302 19:02:55.223808 22732373614720 run.py:483] Algo bellman_ford step 9822 current loss 0.647311, current_train_items 314336.
I0302 19:02:55.254823 22732373614720 run.py:483] Algo bellman_ford step 9823 current loss 0.659514, current_train_items 314368.
I0302 19:02:55.287714 22732373614720 run.py:483] Algo bellman_ford step 9824 current loss 0.896608, current_train_items 314400.
I0302 19:02:55.307605 22732373614720 run.py:483] Algo bellman_ford step 9825 current loss 0.313818, current_train_items 314432.
I0302 19:02:55.323444 22732373614720 run.py:483] Algo bellman_ford step 9826 current loss 0.550800, current_train_items 314464.
I0302 19:02:55.346824 22732373614720 run.py:483] Algo bellman_ford step 9827 current loss 0.604862, current_train_items 314496.
I0302 19:02:55.377135 22732373614720 run.py:483] Algo bellman_ford step 9828 current loss 0.689296, current_train_items 314528.
I0302 19:02:55.411361 22732373614720 run.py:483] Algo bellman_ford step 9829 current loss 0.799104, current_train_items 314560.
I0302 19:02:55.430843 22732373614720 run.py:483] Algo bellman_ford step 9830 current loss 0.283556, current_train_items 314592.
I0302 19:02:55.447500 22732373614720 run.py:483] Algo bellman_ford step 9831 current loss 0.513179, current_train_items 314624.
I0302 19:02:55.471283 22732373614720 run.py:483] Algo bellman_ford step 9832 current loss 0.589423, current_train_items 314656.
I0302 19:02:55.503306 22732373614720 run.py:483] Algo bellman_ford step 9833 current loss 0.673208, current_train_items 314688.
I0302 19:02:55.535126 22732373614720 run.py:483] Algo bellman_ford step 9834 current loss 0.736041, current_train_items 314720.
I0302 19:02:55.554958 22732373614720 run.py:483] Algo bellman_ford step 9835 current loss 0.335838, current_train_items 314752.
I0302 19:02:55.571023 22732373614720 run.py:483] Algo bellman_ford step 9836 current loss 0.532382, current_train_items 314784.
I0302 19:02:55.594375 22732373614720 run.py:483] Algo bellman_ford step 9837 current loss 0.727367, current_train_items 314816.
I0302 19:02:55.626637 22732373614720 run.py:483] Algo bellman_ford step 9838 current loss 0.785433, current_train_items 314848.
I0302 19:02:55.661224 22732373614720 run.py:483] Algo bellman_ford step 9839 current loss 0.818712, current_train_items 314880.
I0302 19:02:55.680592 22732373614720 run.py:483] Algo bellman_ford step 9840 current loss 0.329863, current_train_items 314912.
I0302 19:02:55.696671 22732373614720 run.py:483] Algo bellman_ford step 9841 current loss 0.509355, current_train_items 314944.
I0302 19:02:55.720738 22732373614720 run.py:483] Algo bellman_ford step 9842 current loss 0.612420, current_train_items 314976.
I0302 19:02:55.751922 22732373614720 run.py:483] Algo bellman_ford step 9843 current loss 0.669606, current_train_items 315008.
I0302 19:02:55.783130 22732373614720 run.py:483] Algo bellman_ford step 9844 current loss 0.719741, current_train_items 315040.
I0302 19:02:55.802837 22732373614720 run.py:483] Algo bellman_ford step 9845 current loss 0.313765, current_train_items 315072.
I0302 19:02:55.819178 22732373614720 run.py:483] Algo bellman_ford step 9846 current loss 0.502628, current_train_items 315104.
I0302 19:02:55.843358 22732373614720 run.py:483] Algo bellman_ford step 9847 current loss 0.672216, current_train_items 315136.
I0302 19:02:55.874466 22732373614720 run.py:483] Algo bellman_ford step 9848 current loss 0.668185, current_train_items 315168.
I0302 19:02:55.907501 22732373614720 run.py:483] Algo bellman_ford step 9849 current loss 0.738127, current_train_items 315200.
I0302 19:02:55.927218 22732373614720 run.py:483] Algo bellman_ford step 9850 current loss 0.315625, current_train_items 315232.
I0302 19:02:55.935578 22732373614720 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0302 19:02:55.935687 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0302 19:02:55.953102 22732373614720 run.py:483] Algo bellman_ford step 9851 current loss 0.539171, current_train_items 315264.
I0302 19:02:55.978320 22732373614720 run.py:483] Algo bellman_ford step 9852 current loss 0.821233, current_train_items 315296.
I0302 19:02:56.009534 22732373614720 run.py:483] Algo bellman_ford step 9853 current loss 0.699120, current_train_items 315328.
I0302 19:02:56.043323 22732373614720 run.py:483] Algo bellman_ford step 9854 current loss 0.907025, current_train_items 315360.
I0302 19:02:56.063093 22732373614720 run.py:483] Algo bellman_ford step 9855 current loss 0.368487, current_train_items 315392.
I0302 19:02:56.078902 22732373614720 run.py:483] Algo bellman_ford step 9856 current loss 0.530672, current_train_items 315424.
I0302 19:02:56.102777 22732373614720 run.py:483] Algo bellman_ford step 9857 current loss 0.665888, current_train_items 315456.
I0302 19:02:56.133392 22732373614720 run.py:483] Algo bellman_ford step 9858 current loss 0.711216, current_train_items 315488.
I0302 19:02:56.166165 22732373614720 run.py:483] Algo bellman_ford step 9859 current loss 0.850967, current_train_items 315520.
I0302 19:02:56.186165 22732373614720 run.py:483] Algo bellman_ford step 9860 current loss 0.328647, current_train_items 315552.
I0302 19:02:56.202373 22732373614720 run.py:483] Algo bellman_ford step 9861 current loss 0.476141, current_train_items 315584.
I0302 19:02:56.226099 22732373614720 run.py:483] Algo bellman_ford step 9862 current loss 0.674082, current_train_items 315616.
I0302 19:02:56.257600 22732373614720 run.py:483] Algo bellman_ford step 9863 current loss 0.723681, current_train_items 315648.
I0302 19:02:56.290371 22732373614720 run.py:483] Algo bellman_ford step 9864 current loss 0.800669, current_train_items 315680.
I0302 19:02:56.309997 22732373614720 run.py:483] Algo bellman_ford step 9865 current loss 0.297486, current_train_items 315712.
I0302 19:02:56.325945 22732373614720 run.py:483] Algo bellman_ford step 9866 current loss 0.517015, current_train_items 315744.
I0302 19:02:56.349419 22732373614720 run.py:483] Algo bellman_ford step 9867 current loss 0.662359, current_train_items 315776.
I0302 19:02:56.381206 22732373614720 run.py:483] Algo bellman_ford step 9868 current loss 0.678861, current_train_items 315808.
I0302 19:02:56.414598 22732373614720 run.py:483] Algo bellman_ford step 9869 current loss 0.802925, current_train_items 315840.
I0302 19:02:56.434536 22732373614720 run.py:483] Algo bellman_ford step 9870 current loss 0.342200, current_train_items 315872.
I0302 19:02:56.451236 22732373614720 run.py:483] Algo bellman_ford step 9871 current loss 0.506132, current_train_items 315904.
I0302 19:02:56.475088 22732373614720 run.py:483] Algo bellman_ford step 9872 current loss 0.607774, current_train_items 315936.
I0302 19:02:56.506638 22732373614720 run.py:483] Algo bellman_ford step 9873 current loss 0.630762, current_train_items 315968.
I0302 19:02:56.538494 22732373614720 run.py:483] Algo bellman_ford step 9874 current loss 0.755246, current_train_items 316000.
I0302 19:02:56.558215 22732373614720 run.py:483] Algo bellman_ford step 9875 current loss 0.315730, current_train_items 316032.
I0302 19:02:56.574264 22732373614720 run.py:483] Algo bellman_ford step 9876 current loss 0.476907, current_train_items 316064.
I0302 19:02:56.596005 22732373614720 run.py:483] Algo bellman_ford step 9877 current loss 0.647797, current_train_items 316096.
I0302 19:02:56.626929 22732373614720 run.py:483] Algo bellman_ford step 9878 current loss 0.677488, current_train_items 316128.
I0302 19:02:56.659410 22732373614720 run.py:483] Algo bellman_ford step 9879 current loss 0.705917, current_train_items 316160.
I0302 19:02:56.678833 22732373614720 run.py:483] Algo bellman_ford step 9880 current loss 0.342290, current_train_items 316192.
I0302 19:02:56.695069 22732373614720 run.py:483] Algo bellman_ford step 9881 current loss 0.482147, current_train_items 316224.
I0302 19:02:56.718308 22732373614720 run.py:483] Algo bellman_ford step 9882 current loss 0.555882, current_train_items 316256.
I0302 19:02:56.749698 22732373614720 run.py:483] Algo bellman_ford step 9883 current loss 0.694031, current_train_items 316288.
I0302 19:02:56.782769 22732373614720 run.py:483] Algo bellman_ford step 9884 current loss 0.730918, current_train_items 316320.
I0302 19:02:56.802731 22732373614720 run.py:483] Algo bellman_ford step 9885 current loss 0.350930, current_train_items 316352.
I0302 19:02:56.818722 22732373614720 run.py:483] Algo bellman_ford step 9886 current loss 0.498459, current_train_items 316384.
I0302 19:02:56.841932 22732373614720 run.py:483] Algo bellman_ford step 9887 current loss 0.595287, current_train_items 316416.
I0302 19:02:56.872832 22732373614720 run.py:483] Algo bellman_ford step 9888 current loss 0.681736, current_train_items 316448.
I0302 19:02:56.904788 22732373614720 run.py:483] Algo bellman_ford step 9889 current loss 0.776600, current_train_items 316480.
I0302 19:02:56.924688 22732373614720 run.py:483] Algo bellman_ford step 9890 current loss 0.321483, current_train_items 316512.
I0302 19:02:56.940770 22732373614720 run.py:483] Algo bellman_ford step 9891 current loss 0.533003, current_train_items 316544.
I0302 19:02:56.963272 22732373614720 run.py:483] Algo bellman_ford step 9892 current loss 0.737379, current_train_items 316576.
I0302 19:02:56.995250 22732373614720 run.py:483] Algo bellman_ford step 9893 current loss 0.698897, current_train_items 316608.
I0302 19:02:57.029311 22732373614720 run.py:483] Algo bellman_ford step 9894 current loss 0.780877, current_train_items 316640.
I0302 19:02:57.048912 22732373614720 run.py:483] Algo bellman_ford step 9895 current loss 0.298452, current_train_items 316672.
I0302 19:02:57.064804 22732373614720 run.py:483] Algo bellman_ford step 9896 current loss 0.474998, current_train_items 316704.
I0302 19:02:57.088667 22732373614720 run.py:483] Algo bellman_ford step 9897 current loss 0.778911, current_train_items 316736.
I0302 19:02:57.119919 22732373614720 run.py:483] Algo bellman_ford step 9898 current loss 0.815247, current_train_items 316768.
I0302 19:02:57.154638 22732373614720 run.py:483] Algo bellman_ford step 9899 current loss 1.153995, current_train_items 316800.
I0302 19:02:57.174630 22732373614720 run.py:483] Algo bellman_ford step 9900 current loss 0.314272, current_train_items 316832.
I0302 19:02:57.182369 22732373614720 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0302 19:02:57.182480 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 19:02:57.199305 22732373614720 run.py:483] Algo bellman_ford step 9901 current loss 0.463417, current_train_items 316864.
I0302 19:02:57.223415 22732373614720 run.py:483] Algo bellman_ford step 9902 current loss 0.679691, current_train_items 316896.
I0302 19:02:57.254879 22732373614720 run.py:483] Algo bellman_ford step 9903 current loss 0.642968, current_train_items 316928.
I0302 19:02:57.288665 22732373614720 run.py:483] Algo bellman_ford step 9904 current loss 0.741390, current_train_items 316960.
I0302 19:02:57.308681 22732373614720 run.py:483] Algo bellman_ford step 9905 current loss 0.320010, current_train_items 316992.
I0302 19:02:57.324926 22732373614720 run.py:483] Algo bellman_ford step 9906 current loss 0.451344, current_train_items 317024.
I0302 19:02:57.349261 22732373614720 run.py:483] Algo bellman_ford step 9907 current loss 0.848206, current_train_items 317056.
I0302 19:02:57.380725 22732373614720 run.py:483] Algo bellman_ford step 9908 current loss 0.711070, current_train_items 317088.
I0302 19:02:57.412235 22732373614720 run.py:483] Algo bellman_ford step 9909 current loss 0.805865, current_train_items 317120.
I0302 19:02:57.431987 22732373614720 run.py:483] Algo bellman_ford step 9910 current loss 0.310574, current_train_items 317152.
I0302 19:02:57.448449 22732373614720 run.py:483] Algo bellman_ford step 9911 current loss 0.544803, current_train_items 317184.
I0302 19:02:57.472188 22732373614720 run.py:483] Algo bellman_ford step 9912 current loss 0.674410, current_train_items 317216.
I0302 19:02:57.504299 22732373614720 run.py:483] Algo bellman_ford step 9913 current loss 0.696553, current_train_items 317248.
I0302 19:02:57.536963 22732373614720 run.py:483] Algo bellman_ford step 9914 current loss 0.852375, current_train_items 317280.
I0302 19:02:57.557081 22732373614720 run.py:483] Algo bellman_ford step 9915 current loss 0.363851, current_train_items 317312.
I0302 19:02:57.573424 22732373614720 run.py:483] Algo bellman_ford step 9916 current loss 0.549656, current_train_items 317344.
I0302 19:02:57.597482 22732373614720 run.py:483] Algo bellman_ford step 9917 current loss 0.670106, current_train_items 317376.
I0302 19:02:57.629132 22732373614720 run.py:483] Algo bellman_ford step 9918 current loss 0.733499, current_train_items 317408.
I0302 19:02:57.661620 22732373614720 run.py:483] Algo bellman_ford step 9919 current loss 0.823514, current_train_items 317440.
I0302 19:02:57.681280 22732373614720 run.py:483] Algo bellman_ford step 9920 current loss 0.323800, current_train_items 317472.
I0302 19:02:57.697678 22732373614720 run.py:483] Algo bellman_ford step 9921 current loss 0.547193, current_train_items 317504.
I0302 19:02:57.721397 22732373614720 run.py:483] Algo bellman_ford step 9922 current loss 0.633815, current_train_items 317536.
I0302 19:02:57.751302 22732373614720 run.py:483] Algo bellman_ford step 9923 current loss 0.588756, current_train_items 317568.
I0302 19:02:57.786591 22732373614720 run.py:483] Algo bellman_ford step 9924 current loss 0.846768, current_train_items 317600.
I0302 19:02:57.806193 22732373614720 run.py:483] Algo bellman_ford step 9925 current loss 0.327210, current_train_items 317632.
I0302 19:02:57.822142 22732373614720 run.py:483] Algo bellman_ford step 9926 current loss 0.495536, current_train_items 317664.
I0302 19:02:57.846517 22732373614720 run.py:483] Algo bellman_ford step 9927 current loss 0.614479, current_train_items 317696.
I0302 19:02:57.878924 22732373614720 run.py:483] Algo bellman_ford step 9928 current loss 0.702797, current_train_items 317728.
I0302 19:02:57.909939 22732373614720 run.py:483] Algo bellman_ford step 9929 current loss 0.736809, current_train_items 317760.
I0302 19:02:57.929610 22732373614720 run.py:483] Algo bellman_ford step 9930 current loss 0.342851, current_train_items 317792.
I0302 19:02:57.945101 22732373614720 run.py:483] Algo bellman_ford step 9931 current loss 0.472555, current_train_items 317824.
I0302 19:02:57.968999 22732373614720 run.py:483] Algo bellman_ford step 9932 current loss 0.607504, current_train_items 317856.
I0302 19:02:58.000072 22732373614720 run.py:483] Algo bellman_ford step 9933 current loss 0.686780, current_train_items 317888.
I0302 19:02:58.032043 22732373614720 run.py:483] Algo bellman_ford step 9934 current loss 0.924028, current_train_items 317920.
I0302 19:02:58.051736 22732373614720 run.py:483] Algo bellman_ford step 9935 current loss 0.323225, current_train_items 317952.
I0302 19:02:58.067494 22732373614720 run.py:483] Algo bellman_ford step 9936 current loss 0.534074, current_train_items 317984.
I0302 19:02:58.090896 22732373614720 run.py:483] Algo bellman_ford step 9937 current loss 0.631335, current_train_items 318016.
I0302 19:02:58.121865 22732373614720 run.py:483] Algo bellman_ford step 9938 current loss 0.656767, current_train_items 318048.
I0302 19:02:58.155394 22732373614720 run.py:483] Algo bellman_ford step 9939 current loss 0.883160, current_train_items 318080.
I0302 19:02:58.175220 22732373614720 run.py:483] Algo bellman_ford step 9940 current loss 0.353109, current_train_items 318112.
I0302 19:02:58.191068 22732373614720 run.py:483] Algo bellman_ford step 9941 current loss 0.445449, current_train_items 318144.
I0302 19:02:58.214649 22732373614720 run.py:483] Algo bellman_ford step 9942 current loss 0.731758, current_train_items 318176.
I0302 19:02:58.246419 22732373614720 run.py:483] Algo bellman_ford step 9943 current loss 0.691719, current_train_items 318208.
I0302 19:02:58.277841 22732373614720 run.py:483] Algo bellman_ford step 9944 current loss 0.697966, current_train_items 318240.
I0302 19:02:58.297418 22732373614720 run.py:483] Algo bellman_ford step 9945 current loss 0.386939, current_train_items 318272.
I0302 19:02:58.313115 22732373614720 run.py:483] Algo bellman_ford step 9946 current loss 0.546129, current_train_items 318304.
I0302 19:02:58.336907 22732373614720 run.py:483] Algo bellman_ford step 9947 current loss 0.686883, current_train_items 318336.
I0302 19:02:58.368423 22732373614720 run.py:483] Algo bellman_ford step 9948 current loss 0.758170, current_train_items 318368.
I0302 19:02:58.402731 22732373614720 run.py:483] Algo bellman_ford step 9949 current loss 0.882190, current_train_items 318400.
I0302 19:02:58.421942 22732373614720 run.py:483] Algo bellman_ford step 9950 current loss 0.326692, current_train_items 318432.
I0302 19:02:58.430227 22732373614720 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.884765625, 'score': 0.884765625, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0302 19:02:58.430336 22732373614720 run.py:522] Not saving new best model, best avg val score was 0.945, current avg val score is 0.885, val scores are: bellman_ford: 0.885
I0302 19:02:58.447365 22732373614720 run.py:483] Algo bellman_ford step 9951 current loss 0.515800, current_train_items 318464.
I0302 19:02:58.471783 22732373614720 run.py:483] Algo bellman_ford step 9952 current loss 0.579655, current_train_items 318496.
I0302 19:02:58.504452 22732373614720 run.py:483] Algo bellman_ford step 9953 current loss 0.726930, current_train_items 318528.
I0302 19:02:58.539268 22732373614720 run.py:483] Algo bellman_ford step 9954 current loss 0.702625, current_train_items 318560.
I0302 19:02:58.558952 22732373614720 run.py:483] Algo bellman_ford step 9955 current loss 0.339423, current_train_items 318592.
I0302 19:02:58.575141 22732373614720 run.py:483] Algo bellman_ford step 9956 current loss 0.562347, current_train_items 318624.
I0302 19:02:58.598973 22732373614720 run.py:483] Algo bellman_ford step 9957 current loss 0.606014, current_train_items 318656.
I0302 19:02:58.630586 22732373614720 run.py:483] Algo bellman_ford step 9958 current loss 0.659772, current_train_items 318688.
I0302 19:02:58.664955 22732373614720 run.py:483] Algo bellman_ford step 9959 current loss 0.771748, current_train_items 318720.
I0302 19:02:58.684800 22732373614720 run.py:483] Algo bellman_ford step 9960 current loss 0.353562, current_train_items 318752.
I0302 19:02:58.700436 22732373614720 run.py:483] Algo bellman_ford step 9961 current loss 0.479757, current_train_items 318784.
I0302 19:02:58.724910 22732373614720 run.py:483] Algo bellman_ford step 9962 current loss 0.713586, current_train_items 318816.
I0302 19:02:58.756189 22732373614720 run.py:483] Algo bellman_ford step 9963 current loss 0.727115, current_train_items 318848.
I0302 19:02:58.788674 22732373614720 run.py:483] Algo bellman_ford step 9964 current loss 0.795413, current_train_items 318880.
I0302 19:02:58.808198 22732373614720 run.py:483] Algo bellman_ford step 9965 current loss 0.368709, current_train_items 318912.
I0302 19:02:58.824554 22732373614720 run.py:483] Algo bellman_ford step 9966 current loss 0.548039, current_train_items 318944.
I0302 19:02:58.848447 22732373614720 run.py:483] Algo bellman_ford step 9967 current loss 0.608892, current_train_items 318976.
I0302 19:02:58.879576 22732373614720 run.py:483] Algo bellman_ford step 9968 current loss 0.655577, current_train_items 319008.
I0302 19:02:58.912746 22732373614720 run.py:483] Algo bellman_ford step 9969 current loss 0.664579, current_train_items 319040.
I0302 19:02:58.932822 22732373614720 run.py:483] Algo bellman_ford step 9970 current loss 0.328444, current_train_items 319072.
I0302 19:02:58.948673 22732373614720 run.py:483] Algo bellman_ford step 9971 current loss 0.568533, current_train_items 319104.
I0302 19:02:58.971690 22732373614720 run.py:483] Algo bellman_ford step 9972 current loss 0.648414, current_train_items 319136.
I0302 19:02:59.002083 22732373614720 run.py:483] Algo bellman_ford step 9973 current loss 0.702174, current_train_items 319168.
I0302 19:02:59.035071 22732373614720 run.py:483] Algo bellman_ford step 9974 current loss 0.709324, current_train_items 319200.
I0302 19:02:59.054936 22732373614720 run.py:483] Algo bellman_ford step 9975 current loss 0.349694, current_train_items 319232.
I0302 19:02:59.071278 22732373614720 run.py:483] Algo bellman_ford step 9976 current loss 0.500240, current_train_items 319264.
I0302 19:02:59.095019 22732373614720 run.py:483] Algo bellman_ford step 9977 current loss 0.641662, current_train_items 319296.
I0302 19:02:59.126118 22732373614720 run.py:483] Algo bellman_ford step 9978 current loss 0.657953, current_train_items 319328.
I0302 19:02:59.158230 22732373614720 run.py:483] Algo bellman_ford step 9979 current loss 0.751384, current_train_items 319360.
I0302 19:02:59.177689 22732373614720 run.py:483] Algo bellman_ford step 9980 current loss 0.343164, current_train_items 319392.
I0302 19:02:59.193642 22732373614720 run.py:483] Algo bellman_ford step 9981 current loss 0.542130, current_train_items 319424.
I0302 19:02:59.218021 22732373614720 run.py:483] Algo bellman_ford step 9982 current loss 0.638852, current_train_items 319456.
I0302 19:02:59.250629 22732373614720 run.py:483] Algo bellman_ford step 9983 current loss 0.748100, current_train_items 319488.
I0302 19:02:59.284433 22732373614720 run.py:483] Algo bellman_ford step 9984 current loss 0.812307, current_train_items 319520.
I0302 19:02:59.304392 22732373614720 run.py:483] Algo bellman_ford step 9985 current loss 0.371330, current_train_items 319552.
I0302 19:02:59.320187 22732373614720 run.py:483] Algo bellman_ford step 9986 current loss 0.382450, current_train_items 319584.
I0302 19:02:59.344449 22732373614720 run.py:483] Algo bellman_ford step 9987 current loss 0.678298, current_train_items 319616.
I0302 19:02:59.375712 22732373614720 run.py:483] Algo bellman_ford step 9988 current loss 0.700215, current_train_items 319648.
I0302 19:02:59.408898 22732373614720 run.py:483] Algo bellman_ford step 9989 current loss 0.776635, current_train_items 319680.
I0302 19:02:59.428821 22732373614720 run.py:483] Algo bellman_ford step 9990 current loss 0.285734, current_train_items 319712.
I0302 19:02:59.445297 22732373614720 run.py:483] Algo bellman_ford step 9991 current loss 0.429551, current_train_items 319744.
I0302 19:02:59.468623 22732373614720 run.py:483] Algo bellman_ford step 9992 current loss 0.686224, current_train_items 319776.
I0302 19:02:59.499301 22732373614720 run.py:483] Algo bellman_ford step 9993 current loss 0.629732, current_train_items 319808.
I0302 19:02:59.531488 22732373614720 run.py:483] Algo bellman_ford step 9994 current loss 0.659074, current_train_items 319840.
I0302 19:02:59.551204 22732373614720 run.py:483] Algo bellman_ford step 9995 current loss 0.344773, current_train_items 319872.
I0302 19:02:59.567320 22732373614720 run.py:483] Algo bellman_ford step 9996 current loss 0.481056, current_train_items 319904.
I0302 19:02:59.590985 22732373614720 run.py:483] Algo bellman_ford step 9997 current loss 0.504433, current_train_items 319936.
I0302 19:02:59.622428 22732373614720 run.py:483] Algo bellman_ford step 9998 current loss 0.613235, current_train_items 319968.
I0302 19:02:59.653717 22732373614720 run.py:483] Algo bellman_ford step 9999 current loss 0.987652, current_train_items 320000.
I0302 19:02:59.659745 22732373614720 run.py:527] Restoring best model from checkpoint...
I0302 19:03:02.319672 22732373614720 run.py:542] (test) algo bellman_ford : {'pi': 0.17431640625, 'score': 0.17431640625, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0302 19:03:02.319929 22732373614720 run.py:544] Done!

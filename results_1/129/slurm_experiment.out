Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-04 19:24:58.438645: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-04 19:24:58.438932: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-04 19:24:58.482576: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-04 19:25:19.187130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0304 19:26:25.666237 22502662377600 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0304 19:26:25.670744 22502662377600 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0304 19:26:27.024782 22502662377600 run.py:307] Creating samplers for algo bellman_ford
W0304 19:26:27.025218 22502662377600 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.025491 22502662377600 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.246064 22502662377600 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.246305 22502662377600 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.496490 22502662377600 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.496753 22502662377600 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.840482 22502662377600 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.840718 22502662377600 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:28.264185 22502662377600 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:28.264425 22502662377600 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:28.797014 22502662377600 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0304 19:26:28.797263 22502662377600 samplers.py:112] Creating a dataset with 64 samples.
I0304 19:26:28.835606 22502662377600 run.py:166] Dataset not found in ./datasets_1/129/CLRS30_v1.0.0. Downloading...
I0304 19:26:44.558896 22502662377600 dataset_info.py:482] Load dataset info from ./datasets_1/129/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:26:44.561404 22502662377600 dataset_info.py:482] Load dataset info from ./datasets_1/129/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:26:44.562184 22502662377600 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/129/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0304 19:26:44.562264 22502662377600 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/129/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:27:00.643079 22502662377600 run.py:483] Algo bellman_ford step 0 current loss 9.629043, current_train_items 32.
I0304 19:27:03.422875 22502662377600 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.5400390625, 'score': 0.5400390625, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0304 19:27:03.423147 22502662377600 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.540, val scores are: bellman_ford: 0.540
I0304 19:27:13.460952 22502662377600 run.py:483] Algo bellman_ford step 1 current loss 7.549677, current_train_items 64.
I0304 19:27:24.582232 22502662377600 run.py:483] Algo bellman_ford step 2 current loss 6.795609, current_train_items 96.
I0304 19:27:35.321085 22502662377600 run.py:483] Algo bellman_ford step 3 current loss 5.277297, current_train_items 128.
I0304 19:27:45.336000 22502662377600 run.py:483] Algo bellman_ford step 4 current loss 4.651158, current_train_items 160.
I0304 19:27:45.354270 22502662377600 run.py:483] Algo bellman_ford step 5 current loss 1.735378, current_train_items 192.
I0304 19:27:45.370987 22502662377600 run.py:483] Algo bellman_ford step 6 current loss 2.314405, current_train_items 224.
I0304 19:27:45.393609 22502662377600 run.py:483] Algo bellman_ford step 7 current loss 2.723876, current_train_items 256.
I0304 19:27:45.423625 22502662377600 run.py:483] Algo bellman_ford step 8 current loss 2.953767, current_train_items 288.
I0304 19:27:45.455877 22502662377600 run.py:483] Algo bellman_ford step 9 current loss 3.102663, current_train_items 320.
I0304 19:27:45.473432 22502662377600 run.py:483] Algo bellman_ford step 10 current loss 1.416905, current_train_items 352.
I0304 19:27:45.489845 22502662377600 run.py:483] Algo bellman_ford step 11 current loss 2.005887, current_train_items 384.
I0304 19:27:45.512059 22502662377600 run.py:483] Algo bellman_ford step 12 current loss 2.038630, current_train_items 416.
I0304 19:27:45.543380 22502662377600 run.py:483] Algo bellman_ford step 13 current loss 2.580727, current_train_items 448.
I0304 19:27:45.571739 22502662377600 run.py:483] Algo bellman_ford step 14 current loss 2.348853, current_train_items 480.
I0304 19:27:45.589726 22502662377600 run.py:483] Algo bellman_ford step 15 current loss 1.100875, current_train_items 512.
I0304 19:27:45.605627 22502662377600 run.py:483] Algo bellman_ford step 16 current loss 1.635840, current_train_items 544.
I0304 19:27:45.630441 22502662377600 run.py:483] Algo bellman_ford step 17 current loss 2.384583, current_train_items 576.
I0304 19:27:45.659769 22502662377600 run.py:483] Algo bellman_ford step 18 current loss 2.178896, current_train_items 608.
I0304 19:27:45.692445 22502662377600 run.py:483] Algo bellman_ford step 19 current loss 2.424563, current_train_items 640.
I0304 19:27:45.709758 22502662377600 run.py:483] Algo bellman_ford step 20 current loss 0.940506, current_train_items 672.
I0304 19:27:45.725391 22502662377600 run.py:483] Algo bellman_ford step 21 current loss 1.235779, current_train_items 704.
I0304 19:27:45.749223 22502662377600 run.py:483] Algo bellman_ford step 22 current loss 1.941523, current_train_items 736.
I0304 19:27:45.778706 22502662377600 run.py:483] Algo bellman_ford step 23 current loss 1.995398, current_train_items 768.
I0304 19:27:45.810034 22502662377600 run.py:483] Algo bellman_ford step 24 current loss 2.283510, current_train_items 800.
I0304 19:27:45.827916 22502662377600 run.py:483] Algo bellman_ford step 25 current loss 0.827296, current_train_items 832.
I0304 19:27:45.844079 22502662377600 run.py:483] Algo bellman_ford step 26 current loss 1.282484, current_train_items 864.
I0304 19:27:45.868243 22502662377600 run.py:483] Algo bellman_ford step 27 current loss 1.686667, current_train_items 896.
I0304 19:27:45.898447 22502662377600 run.py:483] Algo bellman_ford step 28 current loss 1.805564, current_train_items 928.
I0304 19:27:45.930981 22502662377600 run.py:483] Algo bellman_ford step 29 current loss 2.073081, current_train_items 960.
I0304 19:27:45.949083 22502662377600 run.py:483] Algo bellman_ford step 30 current loss 0.710996, current_train_items 992.
I0304 19:27:45.964345 22502662377600 run.py:483] Algo bellman_ford step 31 current loss 1.036241, current_train_items 1024.
I0304 19:27:45.987435 22502662377600 run.py:483] Algo bellman_ford step 32 current loss 1.531344, current_train_items 1056.
I0304 19:27:46.017938 22502662377600 run.py:483] Algo bellman_ford step 33 current loss 1.722577, current_train_items 1088.
I0304 19:27:46.049265 22502662377600 run.py:483] Algo bellman_ford step 34 current loss 1.861714, current_train_items 1120.
I0304 19:27:46.066979 22502662377600 run.py:483] Algo bellman_ford step 35 current loss 0.754630, current_train_items 1152.
I0304 19:27:46.082660 22502662377600 run.py:483] Algo bellman_ford step 36 current loss 0.908137, current_train_items 1184.
I0304 19:27:46.106134 22502662377600 run.py:483] Algo bellman_ford step 37 current loss 1.446703, current_train_items 1216.
I0304 19:27:46.135702 22502662377600 run.py:483] Algo bellman_ford step 38 current loss 1.400554, current_train_items 1248.
W0304 19:27:46.158490 22502662377600 samplers.py:155] Increasing hint lengh from 9 to 11
I0304 19:27:52.841450 22502662377600 run.py:483] Algo bellman_ford step 39 current loss 2.026995, current_train_items 1280.
I0304 19:27:52.861354 22502662377600 run.py:483] Algo bellman_ford step 40 current loss 0.711837, current_train_items 1312.
I0304 19:27:52.877761 22502662377600 run.py:483] Algo bellman_ford step 41 current loss 0.920590, current_train_items 1344.
I0304 19:27:52.901077 22502662377600 run.py:483] Algo bellman_ford step 42 current loss 1.187297, current_train_items 1376.
I0304 19:27:52.931702 22502662377600 run.py:483] Algo bellman_ford step 43 current loss 1.424445, current_train_items 1408.
I0304 19:27:52.963825 22502662377600 run.py:483] Algo bellman_ford step 44 current loss 1.650654, current_train_items 1440.
I0304 19:27:52.982978 22502662377600 run.py:483] Algo bellman_ford step 45 current loss 0.588807, current_train_items 1472.
I0304 19:27:52.999373 22502662377600 run.py:483] Algo bellman_ford step 46 current loss 0.947125, current_train_items 1504.
I0304 19:27:53.021663 22502662377600 run.py:483] Algo bellman_ford step 47 current loss 1.173746, current_train_items 1536.
I0304 19:27:53.048786 22502662377600 run.py:483] Algo bellman_ford step 48 current loss 1.139366, current_train_items 1568.
I0304 19:27:53.078076 22502662377600 run.py:483] Algo bellman_ford step 49 current loss 1.429623, current_train_items 1600.
I0304 19:27:53.096531 22502662377600 run.py:483] Algo bellman_ford step 50 current loss 0.525651, current_train_items 1632.
I0304 19:27:53.106171 22502662377600 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.8291015625, 'score': 0.8291015625, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0304 19:27:53.106280 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.540, current avg val score is 0.829, val scores are: bellman_ford: 0.829
I0304 19:27:53.135383 22502662377600 run.py:483] Algo bellman_ford step 51 current loss 0.778413, current_train_items 1664.
I0304 19:27:53.158511 22502662377600 run.py:483] Algo bellman_ford step 52 current loss 1.221114, current_train_items 1696.
I0304 19:27:53.188183 22502662377600 run.py:483] Algo bellman_ford step 53 current loss 1.150459, current_train_items 1728.
I0304 19:27:53.220613 22502662377600 run.py:483] Algo bellman_ford step 54 current loss 1.406643, current_train_items 1760.
I0304 19:27:53.239856 22502662377600 run.py:483] Algo bellman_ford step 55 current loss 0.569024, current_train_items 1792.
I0304 19:27:53.255813 22502662377600 run.py:483] Algo bellman_ford step 56 current loss 0.688982, current_train_items 1824.
I0304 19:27:53.278669 22502662377600 run.py:483] Algo bellman_ford step 57 current loss 1.064821, current_train_items 1856.
I0304 19:27:53.306990 22502662377600 run.py:483] Algo bellman_ford step 58 current loss 0.937464, current_train_items 1888.
I0304 19:27:53.339949 22502662377600 run.py:483] Algo bellman_ford step 59 current loss 1.250473, current_train_items 1920.
I0304 19:27:53.358939 22502662377600 run.py:483] Algo bellman_ford step 60 current loss 0.476899, current_train_items 1952.
W0304 19:27:53.368035 22502662377600 samplers.py:155] Increasing hint lengh from 6 to 7
I0304 19:27:59.826211 22502662377600 run.py:483] Algo bellman_ford step 61 current loss 0.667431, current_train_items 1984.
I0304 19:27:59.850729 22502662377600 run.py:483] Algo bellman_ford step 62 current loss 0.991660, current_train_items 2016.
I0304 19:27:59.880391 22502662377600 run.py:483] Algo bellman_ford step 63 current loss 1.173768, current_train_items 2048.
I0304 19:27:59.914480 22502662377600 run.py:483] Algo bellman_ford step 64 current loss 1.276457, current_train_items 2080.
I0304 19:27:59.934252 22502662377600 run.py:483] Algo bellman_ford step 65 current loss 0.432346, current_train_items 2112.
I0304 19:27:59.950301 22502662377600 run.py:483] Algo bellman_ford step 66 current loss 0.588850, current_train_items 2144.
I0304 19:27:59.975164 22502662377600 run.py:483] Algo bellman_ford step 67 current loss 1.097897, current_train_items 2176.
I0304 19:28:00.003940 22502662377600 run.py:483] Algo bellman_ford step 68 current loss 0.939868, current_train_items 2208.
I0304 19:28:00.036396 22502662377600 run.py:483] Algo bellman_ford step 69 current loss 1.119385, current_train_items 2240.
I0304 19:28:00.055022 22502662377600 run.py:483] Algo bellman_ford step 70 current loss 0.408689, current_train_items 2272.
I0304 19:28:00.071378 22502662377600 run.py:483] Algo bellman_ford step 71 current loss 0.750952, current_train_items 2304.
I0304 19:28:00.095026 22502662377600 run.py:483] Algo bellman_ford step 72 current loss 1.016228, current_train_items 2336.
I0304 19:28:00.125190 22502662377600 run.py:483] Algo bellman_ford step 73 current loss 0.951526, current_train_items 2368.
I0304 19:28:00.157897 22502662377600 run.py:483] Algo bellman_ford step 74 current loss 1.092398, current_train_items 2400.
I0304 19:28:00.176717 22502662377600 run.py:483] Algo bellman_ford step 75 current loss 0.305297, current_train_items 2432.
I0304 19:28:00.193363 22502662377600 run.py:483] Algo bellman_ford step 76 current loss 0.602636, current_train_items 2464.
I0304 19:28:00.216489 22502662377600 run.py:483] Algo bellman_ford step 77 current loss 0.867330, current_train_items 2496.
I0304 19:28:00.245082 22502662377600 run.py:483] Algo bellman_ford step 78 current loss 0.935060, current_train_items 2528.
I0304 19:28:00.274950 22502662377600 run.py:483] Algo bellman_ford step 79 current loss 1.020597, current_train_items 2560.
I0304 19:28:00.294130 22502662377600 run.py:483] Algo bellman_ford step 80 current loss 0.340859, current_train_items 2592.
I0304 19:28:00.310199 22502662377600 run.py:483] Algo bellman_ford step 81 current loss 0.516744, current_train_items 2624.
I0304 19:28:00.333737 22502662377600 run.py:483] Algo bellman_ford step 82 current loss 0.835887, current_train_items 2656.
I0304 19:28:00.363079 22502662377600 run.py:483] Algo bellman_ford step 83 current loss 0.882273, current_train_items 2688.
I0304 19:28:00.393422 22502662377600 run.py:483] Algo bellman_ford step 84 current loss 0.933554, current_train_items 2720.
I0304 19:28:00.412231 22502662377600 run.py:483] Algo bellman_ford step 85 current loss 0.354502, current_train_items 2752.
I0304 19:28:00.428495 22502662377600 run.py:483] Algo bellman_ford step 86 current loss 0.500351, current_train_items 2784.
I0304 19:28:00.453255 22502662377600 run.py:483] Algo bellman_ford step 87 current loss 0.894805, current_train_items 2816.
I0304 19:28:00.483217 22502662377600 run.py:483] Algo bellman_ford step 88 current loss 0.782704, current_train_items 2848.
I0304 19:28:00.515829 22502662377600 run.py:483] Algo bellman_ford step 89 current loss 0.960137, current_train_items 2880.
I0304 19:28:00.534780 22502662377600 run.py:483] Algo bellman_ford step 90 current loss 0.332763, current_train_items 2912.
I0304 19:28:00.550970 22502662377600 run.py:483] Algo bellman_ford step 91 current loss 0.598467, current_train_items 2944.
I0304 19:28:00.574037 22502662377600 run.py:483] Algo bellman_ford step 92 current loss 0.757716, current_train_items 2976.
I0304 19:28:00.604449 22502662377600 run.py:483] Algo bellman_ford step 93 current loss 0.846561, current_train_items 3008.
I0304 19:28:00.635606 22502662377600 run.py:483] Algo bellman_ford step 94 current loss 0.870583, current_train_items 3040.
I0304 19:28:00.654884 22502662377600 run.py:483] Algo bellman_ford step 95 current loss 0.311091, current_train_items 3072.
I0304 19:28:00.670747 22502662377600 run.py:483] Algo bellman_ford step 96 current loss 0.444256, current_train_items 3104.
I0304 19:28:00.694453 22502662377600 run.py:483] Algo bellman_ford step 97 current loss 0.651296, current_train_items 3136.
I0304 19:28:00.724375 22502662377600 run.py:483] Algo bellman_ford step 98 current loss 0.758721, current_train_items 3168.
I0304 19:28:00.756334 22502662377600 run.py:483] Algo bellman_ford step 99 current loss 0.962523, current_train_items 3200.
I0304 19:28:00.774987 22502662377600 run.py:483] Algo bellman_ford step 100 current loss 0.314148, current_train_items 3232.
I0304 19:28:00.784802 22502662377600 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.8701171875, 'score': 0.8701171875, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0304 19:28:00.784915 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.829, current avg val score is 0.870, val scores are: bellman_ford: 0.870
I0304 19:28:00.814095 22502662377600 run.py:483] Algo bellman_ford step 101 current loss 0.505963, current_train_items 3264.
I0304 19:28:00.837494 22502662377600 run.py:483] Algo bellman_ford step 102 current loss 0.583448, current_train_items 3296.
I0304 19:28:00.866651 22502662377600 run.py:483] Algo bellman_ford step 103 current loss 0.760664, current_train_items 3328.
I0304 19:28:00.901273 22502662377600 run.py:483] Algo bellman_ford step 104 current loss 0.942759, current_train_items 3360.
I0304 19:28:00.920330 22502662377600 run.py:483] Algo bellman_ford step 105 current loss 0.256253, current_train_items 3392.
I0304 19:28:00.936414 22502662377600 run.py:483] Algo bellman_ford step 106 current loss 0.448707, current_train_items 3424.
I0304 19:28:00.960050 22502662377600 run.py:483] Algo bellman_ford step 107 current loss 0.608393, current_train_items 3456.
I0304 19:28:00.988922 22502662377600 run.py:483] Algo bellman_ford step 108 current loss 0.834610, current_train_items 3488.
I0304 19:28:01.019362 22502662377600 run.py:483] Algo bellman_ford step 109 current loss 0.781803, current_train_items 3520.
I0304 19:28:01.038090 22502662377600 run.py:483] Algo bellman_ford step 110 current loss 0.265222, current_train_items 3552.
I0304 19:28:01.054071 22502662377600 run.py:483] Algo bellman_ford step 111 current loss 0.390767, current_train_items 3584.
I0304 19:28:01.077146 22502662377600 run.py:483] Algo bellman_ford step 112 current loss 0.539252, current_train_items 3616.
I0304 19:28:01.106989 22502662377600 run.py:483] Algo bellman_ford step 113 current loss 0.709518, current_train_items 3648.
I0304 19:28:01.137939 22502662377600 run.py:483] Algo bellman_ford step 114 current loss 0.773631, current_train_items 3680.
I0304 19:28:01.156314 22502662377600 run.py:483] Algo bellman_ford step 115 current loss 0.236699, current_train_items 3712.
I0304 19:28:01.172624 22502662377600 run.py:483] Algo bellman_ford step 116 current loss 0.483807, current_train_items 3744.
I0304 19:28:01.196841 22502662377600 run.py:483] Algo bellman_ford step 117 current loss 0.936386, current_train_items 3776.
I0304 19:28:01.225906 22502662377600 run.py:483] Algo bellman_ford step 118 current loss 0.688905, current_train_items 3808.
I0304 19:28:01.255520 22502662377600 run.py:483] Algo bellman_ford step 119 current loss 0.688452, current_train_items 3840.
I0304 19:28:01.274341 22502662377600 run.py:483] Algo bellman_ford step 120 current loss 0.232633, current_train_items 3872.
I0304 19:28:01.290788 22502662377600 run.py:483] Algo bellman_ford step 121 current loss 0.432459, current_train_items 3904.
I0304 19:28:01.314688 22502662377600 run.py:483] Algo bellman_ford step 122 current loss 0.591806, current_train_items 3936.
I0304 19:28:01.344874 22502662377600 run.py:483] Algo bellman_ford step 123 current loss 0.704705, current_train_items 3968.
I0304 19:28:01.380294 22502662377600 run.py:483] Algo bellman_ford step 124 current loss 1.104645, current_train_items 4000.
I0304 19:28:01.399212 22502662377600 run.py:483] Algo bellman_ford step 125 current loss 0.313579, current_train_items 4032.
I0304 19:28:01.415622 22502662377600 run.py:483] Algo bellman_ford step 126 current loss 0.480581, current_train_items 4064.
I0304 19:28:01.439398 22502662377600 run.py:483] Algo bellman_ford step 127 current loss 0.653731, current_train_items 4096.
I0304 19:28:01.468585 22502662377600 run.py:483] Algo bellman_ford step 128 current loss 0.780107, current_train_items 4128.
I0304 19:28:01.500913 22502662377600 run.py:483] Algo bellman_ford step 129 current loss 0.884129, current_train_items 4160.
I0304 19:28:01.519360 22502662377600 run.py:483] Algo bellman_ford step 130 current loss 0.270035, current_train_items 4192.
I0304 19:28:01.535362 22502662377600 run.py:483] Algo bellman_ford step 131 current loss 0.353579, current_train_items 4224.
I0304 19:28:01.559214 22502662377600 run.py:483] Algo bellman_ford step 132 current loss 0.659610, current_train_items 4256.
I0304 19:28:01.588722 22502662377600 run.py:483] Algo bellman_ford step 133 current loss 0.662090, current_train_items 4288.
I0304 19:28:01.620105 22502662377600 run.py:483] Algo bellman_ford step 134 current loss 0.699575, current_train_items 4320.
I0304 19:28:01.638279 22502662377600 run.py:483] Algo bellman_ford step 135 current loss 0.199292, current_train_items 4352.
I0304 19:28:01.654255 22502662377600 run.py:483] Algo bellman_ford step 136 current loss 0.385831, current_train_items 4384.
I0304 19:28:01.678139 22502662377600 run.py:483] Algo bellman_ford step 137 current loss 0.547780, current_train_items 4416.
I0304 19:28:01.707646 22502662377600 run.py:483] Algo bellman_ford step 138 current loss 0.767644, current_train_items 4448.
I0304 19:28:01.740254 22502662377600 run.py:483] Algo bellman_ford step 139 current loss 0.819408, current_train_items 4480.
I0304 19:28:01.758704 22502662377600 run.py:483] Algo bellman_ford step 140 current loss 0.191042, current_train_items 4512.
I0304 19:28:01.775097 22502662377600 run.py:483] Algo bellman_ford step 141 current loss 0.446295, current_train_items 4544.
I0304 19:28:01.798624 22502662377600 run.py:483] Algo bellman_ford step 142 current loss 0.621992, current_train_items 4576.
I0304 19:28:01.828817 22502662377600 run.py:483] Algo bellman_ford step 143 current loss 0.708001, current_train_items 4608.
I0304 19:28:01.860683 22502662377600 run.py:483] Algo bellman_ford step 144 current loss 0.692144, current_train_items 4640.
I0304 19:28:01.879482 22502662377600 run.py:483] Algo bellman_ford step 145 current loss 0.188104, current_train_items 4672.
I0304 19:28:01.895431 22502662377600 run.py:483] Algo bellman_ford step 146 current loss 0.387933, current_train_items 4704.
I0304 19:28:01.918548 22502662377600 run.py:483] Algo bellman_ford step 147 current loss 0.472786, current_train_items 4736.
I0304 19:28:01.948002 22502662377600 run.py:483] Algo bellman_ford step 148 current loss 0.647193, current_train_items 4768.
I0304 19:28:01.978291 22502662377600 run.py:483] Algo bellman_ford step 149 current loss 0.753162, current_train_items 4800.
I0304 19:28:01.996964 22502662377600 run.py:483] Algo bellman_ford step 150 current loss 0.229227, current_train_items 4832.
I0304 19:28:02.004986 22502662377600 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0304 19:28:02.005097 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.870, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0304 19:28:02.033784 22502662377600 run.py:483] Algo bellman_ford step 151 current loss 0.326418, current_train_items 4864.
I0304 19:28:02.057694 22502662377600 run.py:483] Algo bellman_ford step 152 current loss 0.518119, current_train_items 4896.
I0304 19:28:02.087108 22502662377600 run.py:483] Algo bellman_ford step 153 current loss 0.644543, current_train_items 4928.
I0304 19:28:02.123219 22502662377600 run.py:483] Algo bellman_ford step 154 current loss 0.962805, current_train_items 4960.
I0304 19:28:02.142577 22502662377600 run.py:483] Algo bellman_ford step 155 current loss 0.234136, current_train_items 4992.
I0304 19:28:02.158919 22502662377600 run.py:483] Algo bellman_ford step 156 current loss 0.408569, current_train_items 5024.
I0304 19:28:02.182118 22502662377600 run.py:483] Algo bellman_ford step 157 current loss 0.567353, current_train_items 5056.
I0304 19:28:02.210386 22502662377600 run.py:483] Algo bellman_ford step 158 current loss 0.557240, current_train_items 5088.
I0304 19:28:02.242972 22502662377600 run.py:483] Algo bellman_ford step 159 current loss 0.572436, current_train_items 5120.
I0304 19:28:02.261971 22502662377600 run.py:483] Algo bellman_ford step 160 current loss 0.192130, current_train_items 5152.
I0304 19:28:02.278336 22502662377600 run.py:483] Algo bellman_ford step 161 current loss 0.311686, current_train_items 5184.
I0304 19:28:02.301218 22502662377600 run.py:483] Algo bellman_ford step 162 current loss 0.365787, current_train_items 5216.
I0304 19:28:02.330725 22502662377600 run.py:483] Algo bellman_ford step 163 current loss 0.569655, current_train_items 5248.
I0304 19:28:02.361744 22502662377600 run.py:483] Algo bellman_ford step 164 current loss 0.639404, current_train_items 5280.
I0304 19:28:02.380833 22502662377600 run.py:483] Algo bellman_ford step 165 current loss 0.171790, current_train_items 5312.
I0304 19:28:02.397310 22502662377600 run.py:483] Algo bellman_ford step 166 current loss 0.438522, current_train_items 5344.
I0304 19:28:02.420352 22502662377600 run.py:483] Algo bellman_ford step 167 current loss 0.678815, current_train_items 5376.
I0304 19:28:02.450356 22502662377600 run.py:483] Algo bellman_ford step 168 current loss 0.556287, current_train_items 5408.
I0304 19:28:02.482352 22502662377600 run.py:483] Algo bellman_ford step 169 current loss 0.747731, current_train_items 5440.
I0304 19:28:02.501377 22502662377600 run.py:483] Algo bellman_ford step 170 current loss 0.199591, current_train_items 5472.
I0304 19:28:02.517625 22502662377600 run.py:483] Algo bellman_ford step 171 current loss 0.299360, current_train_items 5504.
I0304 19:28:02.540003 22502662377600 run.py:483] Algo bellman_ford step 172 current loss 0.511190, current_train_items 5536.
I0304 19:28:02.570239 22502662377600 run.py:483] Algo bellman_ford step 173 current loss 0.697439, current_train_items 5568.
I0304 19:28:02.605575 22502662377600 run.py:483] Algo bellman_ford step 174 current loss 0.850203, current_train_items 5600.
I0304 19:28:02.624096 22502662377600 run.py:483] Algo bellman_ford step 175 current loss 0.222918, current_train_items 5632.
I0304 19:28:02.640240 22502662377600 run.py:483] Algo bellman_ford step 176 current loss 0.326130, current_train_items 5664.
I0304 19:28:02.664344 22502662377600 run.py:483] Algo bellman_ford step 177 current loss 0.564612, current_train_items 5696.
I0304 19:28:02.692395 22502662377600 run.py:483] Algo bellman_ford step 178 current loss 0.419523, current_train_items 5728.
I0304 19:28:02.722960 22502662377600 run.py:483] Algo bellman_ford step 179 current loss 0.746088, current_train_items 5760.
I0304 19:28:02.741970 22502662377600 run.py:483] Algo bellman_ford step 180 current loss 0.221817, current_train_items 5792.
I0304 19:28:02.758260 22502662377600 run.py:483] Algo bellman_ford step 181 current loss 0.328919, current_train_items 5824.
I0304 19:28:02.782830 22502662377600 run.py:483] Algo bellman_ford step 182 current loss 0.529806, current_train_items 5856.
I0304 19:28:02.812207 22502662377600 run.py:483] Algo bellman_ford step 183 current loss 0.535898, current_train_items 5888.
I0304 19:28:02.842423 22502662377600 run.py:483] Algo bellman_ford step 184 current loss 0.540180, current_train_items 5920.
I0304 19:28:02.861019 22502662377600 run.py:483] Algo bellman_ford step 185 current loss 0.133871, current_train_items 5952.
I0304 19:28:02.877500 22502662377600 run.py:483] Algo bellman_ford step 186 current loss 0.292259, current_train_items 5984.
I0304 19:28:02.900335 22502662377600 run.py:483] Algo bellman_ford step 187 current loss 0.337036, current_train_items 6016.
I0304 19:28:02.929523 22502662377600 run.py:483] Algo bellman_ford step 188 current loss 0.569441, current_train_items 6048.
I0304 19:28:02.963128 22502662377600 run.py:483] Algo bellman_ford step 189 current loss 0.630253, current_train_items 6080.
I0304 19:28:02.981947 22502662377600 run.py:483] Algo bellman_ford step 190 current loss 0.147476, current_train_items 6112.
I0304 19:28:02.998508 22502662377600 run.py:483] Algo bellman_ford step 191 current loss 0.337935, current_train_items 6144.
I0304 19:28:03.022496 22502662377600 run.py:483] Algo bellman_ford step 192 current loss 0.571384, current_train_items 6176.
I0304 19:28:03.051768 22502662377600 run.py:483] Algo bellman_ford step 193 current loss 0.434635, current_train_items 6208.
I0304 19:28:03.083070 22502662377600 run.py:483] Algo bellman_ford step 194 current loss 0.526377, current_train_items 6240.
I0304 19:28:03.102414 22502662377600 run.py:483] Algo bellman_ford step 195 current loss 0.140175, current_train_items 6272.
I0304 19:28:03.118680 22502662377600 run.py:483] Algo bellman_ford step 196 current loss 0.292251, current_train_items 6304.
I0304 19:28:03.141752 22502662377600 run.py:483] Algo bellman_ford step 197 current loss 0.389234, current_train_items 6336.
I0304 19:28:03.171693 22502662377600 run.py:483] Algo bellman_ford step 198 current loss 0.631655, current_train_items 6368.
I0304 19:28:03.204193 22502662377600 run.py:483] Algo bellman_ford step 199 current loss 0.602780, current_train_items 6400.
I0304 19:28:03.223131 22502662377600 run.py:483] Algo bellman_ford step 200 current loss 0.145045, current_train_items 6432.
I0304 19:28:03.231351 22502662377600 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0304 19:28:03.231456 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.915, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0304 19:28:03.260929 22502662377600 run.py:483] Algo bellman_ford step 201 current loss 0.249413, current_train_items 6464.
I0304 19:28:03.284889 22502662377600 run.py:483] Algo bellman_ford step 202 current loss 0.446351, current_train_items 6496.
I0304 19:28:03.316860 22502662377600 run.py:483] Algo bellman_ford step 203 current loss 0.575821, current_train_items 6528.
I0304 19:28:03.352183 22502662377600 run.py:483] Algo bellman_ford step 204 current loss 0.664991, current_train_items 6560.
I0304 19:28:03.371392 22502662377600 run.py:483] Algo bellman_ford step 205 current loss 0.169082, current_train_items 6592.
I0304 19:28:03.386699 22502662377600 run.py:483] Algo bellman_ford step 206 current loss 0.282592, current_train_items 6624.
I0304 19:28:03.410832 22502662377600 run.py:483] Algo bellman_ford step 207 current loss 0.880471, current_train_items 6656.
I0304 19:28:03.440786 22502662377600 run.py:483] Algo bellman_ford step 208 current loss 0.904560, current_train_items 6688.
I0304 19:28:03.471964 22502662377600 run.py:483] Algo bellman_ford step 209 current loss 0.644483, current_train_items 6720.
I0304 19:28:03.490873 22502662377600 run.py:483] Algo bellman_ford step 210 current loss 0.263262, current_train_items 6752.
I0304 19:28:03.507081 22502662377600 run.py:483] Algo bellman_ford step 211 current loss 0.404849, current_train_items 6784.
I0304 19:28:03.530935 22502662377600 run.py:483] Algo bellman_ford step 212 current loss 0.722163, current_train_items 6816.
I0304 19:28:03.560996 22502662377600 run.py:483] Algo bellman_ford step 213 current loss 0.672421, current_train_items 6848.
I0304 19:28:03.589530 22502662377600 run.py:483] Algo bellman_ford step 214 current loss 0.424535, current_train_items 6880.
I0304 19:28:03.608308 22502662377600 run.py:483] Algo bellman_ford step 215 current loss 0.168806, current_train_items 6912.
I0304 19:28:03.624247 22502662377600 run.py:483] Algo bellman_ford step 216 current loss 0.531154, current_train_items 6944.
I0304 19:28:03.647454 22502662377600 run.py:483] Algo bellman_ford step 217 current loss 0.815266, current_train_items 6976.
I0304 19:28:03.676561 22502662377600 run.py:483] Algo bellman_ford step 218 current loss 0.670850, current_train_items 7008.
I0304 19:28:03.707324 22502662377600 run.py:483] Algo bellman_ford step 219 current loss 0.558175, current_train_items 7040.
I0304 19:28:03.726103 22502662377600 run.py:483] Algo bellman_ford step 220 current loss 0.162269, current_train_items 7072.
I0304 19:28:03.742340 22502662377600 run.py:483] Algo bellman_ford step 221 current loss 0.350266, current_train_items 7104.
I0304 19:28:03.766455 22502662377600 run.py:483] Algo bellman_ford step 222 current loss 0.626497, current_train_items 7136.
I0304 19:28:03.795280 22502662377600 run.py:483] Algo bellman_ford step 223 current loss 0.767252, current_train_items 7168.
I0304 19:28:03.827142 22502662377600 run.py:483] Algo bellman_ford step 224 current loss 0.693847, current_train_items 7200.
I0304 19:28:03.845723 22502662377600 run.py:483] Algo bellman_ford step 225 current loss 0.155268, current_train_items 7232.
I0304 19:28:03.861991 22502662377600 run.py:483] Algo bellman_ford step 226 current loss 0.296101, current_train_items 7264.
I0304 19:28:03.885859 22502662377600 run.py:483] Algo bellman_ford step 227 current loss 0.572804, current_train_items 7296.
I0304 19:28:03.915669 22502662377600 run.py:483] Algo bellman_ford step 228 current loss 0.527991, current_train_items 7328.
I0304 19:28:03.948389 22502662377600 run.py:483] Algo bellman_ford step 229 current loss 0.586167, current_train_items 7360.
I0304 19:28:03.966727 22502662377600 run.py:483] Algo bellman_ford step 230 current loss 0.114602, current_train_items 7392.
I0304 19:28:03.982842 22502662377600 run.py:483] Algo bellman_ford step 231 current loss 0.236058, current_train_items 7424.
I0304 19:28:04.006804 22502662377600 run.py:483] Algo bellman_ford step 232 current loss 0.457392, current_train_items 7456.
I0304 19:28:04.037034 22502662377600 run.py:483] Algo bellman_ford step 233 current loss 0.470299, current_train_items 7488.
I0304 19:28:04.070294 22502662377600 run.py:483] Algo bellman_ford step 234 current loss 0.532320, current_train_items 7520.
I0304 19:28:04.089130 22502662377600 run.py:483] Algo bellman_ford step 235 current loss 0.146600, current_train_items 7552.
I0304 19:28:04.104942 22502662377600 run.py:483] Algo bellman_ford step 236 current loss 0.238977, current_train_items 7584.
I0304 19:28:04.128070 22502662377600 run.py:483] Algo bellman_ford step 237 current loss 0.370672, current_train_items 7616.
I0304 19:28:04.157806 22502662377600 run.py:483] Algo bellman_ford step 238 current loss 0.418785, current_train_items 7648.
I0304 19:28:04.189482 22502662377600 run.py:483] Algo bellman_ford step 239 current loss 0.434422, current_train_items 7680.
I0304 19:28:04.208106 22502662377600 run.py:483] Algo bellman_ford step 240 current loss 0.107277, current_train_items 7712.
I0304 19:28:04.224402 22502662377600 run.py:483] Algo bellman_ford step 241 current loss 0.250295, current_train_items 7744.
I0304 19:28:04.247993 22502662377600 run.py:483] Algo bellman_ford step 242 current loss 0.377634, current_train_items 7776.
I0304 19:28:04.277255 22502662377600 run.py:483] Algo bellman_ford step 243 current loss 0.446144, current_train_items 7808.
I0304 19:28:04.306967 22502662377600 run.py:483] Algo bellman_ford step 244 current loss 0.403574, current_train_items 7840.
I0304 19:28:04.325698 22502662377600 run.py:483] Algo bellman_ford step 245 current loss 0.131257, current_train_items 7872.
I0304 19:28:04.341659 22502662377600 run.py:483] Algo bellman_ford step 246 current loss 0.222290, current_train_items 7904.
I0304 19:28:04.364283 22502662377600 run.py:483] Algo bellman_ford step 247 current loss 0.355668, current_train_items 7936.
I0304 19:28:04.394733 22502662377600 run.py:483] Algo bellman_ford step 248 current loss 0.490084, current_train_items 7968.
I0304 19:28:04.427057 22502662377600 run.py:483] Algo bellman_ford step 249 current loss 0.480764, current_train_items 8000.
I0304 19:28:04.445232 22502662377600 run.py:483] Algo bellman_ford step 250 current loss 0.147128, current_train_items 8032.
I0304 19:28:04.453568 22502662377600 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.8876953125, 'score': 0.8876953125, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0304 19:28:04.453672 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.917, current avg val score is 0.888, val scores are: bellman_ford: 0.888
I0304 19:28:04.470685 22502662377600 run.py:483] Algo bellman_ford step 251 current loss 0.357774, current_train_items 8064.
I0304 19:28:04.495036 22502662377600 run.py:483] Algo bellman_ford step 252 current loss 0.350462, current_train_items 8096.
I0304 19:28:04.525761 22502662377600 run.py:483] Algo bellman_ford step 253 current loss 0.405484, current_train_items 8128.
I0304 19:28:04.559125 22502662377600 run.py:483] Algo bellman_ford step 254 current loss 0.597467, current_train_items 8160.
I0304 19:28:04.578345 22502662377600 run.py:483] Algo bellman_ford step 255 current loss 0.247777, current_train_items 8192.
I0304 19:28:04.594113 22502662377600 run.py:483] Algo bellman_ford step 256 current loss 0.233513, current_train_items 8224.
I0304 19:28:04.616751 22502662377600 run.py:483] Algo bellman_ford step 257 current loss 0.272947, current_train_items 8256.
I0304 19:28:04.646282 22502662377600 run.py:483] Algo bellman_ford step 258 current loss 0.387269, current_train_items 8288.
I0304 19:28:04.676888 22502662377600 run.py:483] Algo bellman_ford step 259 current loss 0.449140, current_train_items 8320.
I0304 19:28:04.695781 22502662377600 run.py:483] Algo bellman_ford step 260 current loss 0.113821, current_train_items 8352.
I0304 19:28:04.711847 22502662377600 run.py:483] Algo bellman_ford step 261 current loss 0.195167, current_train_items 8384.
I0304 19:28:04.734455 22502662377600 run.py:483] Algo bellman_ford step 262 current loss 0.293392, current_train_items 8416.
I0304 19:28:04.763146 22502662377600 run.py:483] Algo bellman_ford step 263 current loss 0.380469, current_train_items 8448.
I0304 19:28:04.793860 22502662377600 run.py:483] Algo bellman_ford step 264 current loss 0.355649, current_train_items 8480.
I0304 19:28:04.812383 22502662377600 run.py:483] Algo bellman_ford step 265 current loss 0.103421, current_train_items 8512.
I0304 19:28:04.828618 22502662377600 run.py:483] Algo bellman_ford step 266 current loss 0.233471, current_train_items 8544.
I0304 19:28:04.852816 22502662377600 run.py:483] Algo bellman_ford step 267 current loss 0.407142, current_train_items 8576.
I0304 19:28:04.883219 22502662377600 run.py:483] Algo bellman_ford step 268 current loss 0.516125, current_train_items 8608.
I0304 19:28:04.913331 22502662377600 run.py:483] Algo bellman_ford step 269 current loss 0.415705, current_train_items 8640.
I0304 19:28:04.932413 22502662377600 run.py:483] Algo bellman_ford step 270 current loss 0.091429, current_train_items 8672.
I0304 19:28:04.948084 22502662377600 run.py:483] Algo bellman_ford step 271 current loss 0.173289, current_train_items 8704.
I0304 19:28:04.972071 22502662377600 run.py:483] Algo bellman_ford step 272 current loss 0.436921, current_train_items 8736.
I0304 19:28:05.001166 22502662377600 run.py:483] Algo bellman_ford step 273 current loss 0.415032, current_train_items 8768.
I0304 19:28:05.031878 22502662377600 run.py:483] Algo bellman_ford step 274 current loss 0.497304, current_train_items 8800.
I0304 19:28:05.051139 22502662377600 run.py:483] Algo bellman_ford step 275 current loss 0.136881, current_train_items 8832.
I0304 19:28:05.067378 22502662377600 run.py:483] Algo bellman_ford step 276 current loss 0.281439, current_train_items 8864.
I0304 19:28:05.091474 22502662377600 run.py:483] Algo bellman_ford step 277 current loss 0.411617, current_train_items 8896.
I0304 19:28:05.122024 22502662377600 run.py:483] Algo bellman_ford step 278 current loss 0.444687, current_train_items 8928.
I0304 19:28:05.154103 22502662377600 run.py:483] Algo bellman_ford step 279 current loss 0.463470, current_train_items 8960.
I0304 19:28:05.172703 22502662377600 run.py:483] Algo bellman_ford step 280 current loss 0.127452, current_train_items 8992.
I0304 19:28:05.189109 22502662377600 run.py:483] Algo bellman_ford step 281 current loss 0.235357, current_train_items 9024.
I0304 19:28:05.213061 22502662377600 run.py:483] Algo bellman_ford step 282 current loss 0.342912, current_train_items 9056.
I0304 19:28:05.242439 22502662377600 run.py:483] Algo bellman_ford step 283 current loss 0.464529, current_train_items 9088.
I0304 19:28:05.275496 22502662377600 run.py:483] Algo bellman_ford step 284 current loss 0.647042, current_train_items 9120.
I0304 19:28:05.294420 22502662377600 run.py:483] Algo bellman_ford step 285 current loss 0.108155, current_train_items 9152.
I0304 19:28:05.310902 22502662377600 run.py:483] Algo bellman_ford step 286 current loss 0.276390, current_train_items 9184.
I0304 19:28:05.333806 22502662377600 run.py:483] Algo bellman_ford step 287 current loss 0.430246, current_train_items 9216.
I0304 19:28:05.363783 22502662377600 run.py:483] Algo bellman_ford step 288 current loss 0.493160, current_train_items 9248.
I0304 19:28:05.396550 22502662377600 run.py:483] Algo bellman_ford step 289 current loss 0.513438, current_train_items 9280.
I0304 19:28:05.415752 22502662377600 run.py:483] Algo bellman_ford step 290 current loss 0.091115, current_train_items 9312.
I0304 19:28:05.431883 22502662377600 run.py:483] Algo bellman_ford step 291 current loss 0.202633, current_train_items 9344.
I0304 19:28:05.454975 22502662377600 run.py:483] Algo bellman_ford step 292 current loss 0.300743, current_train_items 9376.
I0304 19:28:05.483948 22502662377600 run.py:483] Algo bellman_ford step 293 current loss 0.390101, current_train_items 9408.
I0304 19:28:05.515552 22502662377600 run.py:483] Algo bellman_ford step 294 current loss 0.524422, current_train_items 9440.
I0304 19:28:05.533924 22502662377600 run.py:483] Algo bellman_ford step 295 current loss 0.108358, current_train_items 9472.
I0304 19:28:05.549741 22502662377600 run.py:483] Algo bellman_ford step 296 current loss 0.167830, current_train_items 9504.
I0304 19:28:05.573770 22502662377600 run.py:483] Algo bellman_ford step 297 current loss 0.300113, current_train_items 9536.
I0304 19:28:05.604386 22502662377600 run.py:483] Algo bellman_ford step 298 current loss 0.492139, current_train_items 9568.
I0304 19:28:05.635376 22502662377600 run.py:483] Algo bellman_ford step 299 current loss 0.468562, current_train_items 9600.
I0304 19:28:05.654300 22502662377600 run.py:483] Algo bellman_ford step 300 current loss 0.131011, current_train_items 9632.
I0304 19:28:05.662077 22502662377600 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0304 19:28:05.662182 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.917, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0304 19:28:05.692683 22502662377600 run.py:483] Algo bellman_ford step 301 current loss 0.228418, current_train_items 9664.
I0304 19:28:05.715946 22502662377600 run.py:483] Algo bellman_ford step 302 current loss 0.271853, current_train_items 9696.
I0304 19:28:05.744967 22502662377600 run.py:483] Algo bellman_ford step 303 current loss 0.248474, current_train_items 9728.
I0304 19:28:05.778172 22502662377600 run.py:483] Algo bellman_ford step 304 current loss 0.543103, current_train_items 9760.
I0304 19:28:05.797471 22502662377600 run.py:483] Algo bellman_ford step 305 current loss 0.101997, current_train_items 9792.
I0304 19:28:05.813980 22502662377600 run.py:483] Algo bellman_ford step 306 current loss 0.271679, current_train_items 9824.
I0304 19:28:05.838574 22502662377600 run.py:483] Algo bellman_ford step 307 current loss 0.356458, current_train_items 9856.
I0304 19:28:05.868668 22502662377600 run.py:483] Algo bellman_ford step 308 current loss 0.457315, current_train_items 9888.
I0304 19:28:05.900656 22502662377600 run.py:483] Algo bellman_ford step 309 current loss 0.482480, current_train_items 9920.
I0304 19:28:05.919583 22502662377600 run.py:483] Algo bellman_ford step 310 current loss 0.115048, current_train_items 9952.
I0304 19:28:05.935626 22502662377600 run.py:483] Algo bellman_ford step 311 current loss 0.166449, current_train_items 9984.
I0304 19:28:05.959192 22502662377600 run.py:483] Algo bellman_ford step 312 current loss 0.259708, current_train_items 10016.
I0304 19:28:05.989546 22502662377600 run.py:483] Algo bellman_ford step 313 current loss 0.385556, current_train_items 10048.
I0304 19:28:06.021579 22502662377600 run.py:483] Algo bellman_ford step 314 current loss 0.462216, current_train_items 10080.
I0304 19:28:06.040351 22502662377600 run.py:483] Algo bellman_ford step 315 current loss 0.106852, current_train_items 10112.
I0304 19:28:06.056521 22502662377600 run.py:483] Algo bellman_ford step 316 current loss 0.170994, current_train_items 10144.
I0304 19:28:06.079624 22502662377600 run.py:483] Algo bellman_ford step 317 current loss 0.265367, current_train_items 10176.
I0304 19:28:06.108202 22502662377600 run.py:483] Algo bellman_ford step 318 current loss 0.322075, current_train_items 10208.
I0304 19:28:06.138853 22502662377600 run.py:483] Algo bellman_ford step 319 current loss 0.459992, current_train_items 10240.
I0304 19:28:06.157647 22502662377600 run.py:483] Algo bellman_ford step 320 current loss 0.074735, current_train_items 10272.
I0304 19:28:06.173788 22502662377600 run.py:483] Algo bellman_ford step 321 current loss 0.193551, current_train_items 10304.
I0304 19:28:06.197288 22502662377600 run.py:483] Algo bellman_ford step 322 current loss 0.344421, current_train_items 10336.
I0304 19:28:06.225830 22502662377600 run.py:483] Algo bellman_ford step 323 current loss 0.283765, current_train_items 10368.
I0304 19:28:06.257688 22502662377600 run.py:483] Algo bellman_ford step 324 current loss 0.486145, current_train_items 10400.
I0304 19:28:06.276288 22502662377600 run.py:483] Algo bellman_ford step 325 current loss 0.120015, current_train_items 10432.
I0304 19:28:06.292106 22502662377600 run.py:483] Algo bellman_ford step 326 current loss 0.128108, current_train_items 10464.
I0304 19:28:06.315720 22502662377600 run.py:483] Algo bellman_ford step 327 current loss 0.278383, current_train_items 10496.
I0304 19:28:06.346135 22502662377600 run.py:483] Algo bellman_ford step 328 current loss 0.341112, current_train_items 10528.
I0304 19:28:06.378236 22502662377600 run.py:483] Algo bellman_ford step 329 current loss 0.477246, current_train_items 10560.
I0304 19:28:06.397186 22502662377600 run.py:483] Algo bellman_ford step 330 current loss 0.085972, current_train_items 10592.
I0304 19:28:06.413224 22502662377600 run.py:483] Algo bellman_ford step 331 current loss 0.174307, current_train_items 10624.
I0304 19:28:06.436947 22502662377600 run.py:483] Algo bellman_ford step 332 current loss 0.315226, current_train_items 10656.
I0304 19:28:06.467291 22502662377600 run.py:483] Algo bellman_ford step 333 current loss 0.332556, current_train_items 10688.
I0304 19:28:06.500302 22502662377600 run.py:483] Algo bellman_ford step 334 current loss 0.423671, current_train_items 10720.
I0304 19:28:06.518913 22502662377600 run.py:483] Algo bellman_ford step 335 current loss 0.087641, current_train_items 10752.
I0304 19:28:06.535433 22502662377600 run.py:483] Algo bellman_ford step 336 current loss 0.205353, current_train_items 10784.
I0304 19:28:06.560201 22502662377600 run.py:483] Algo bellman_ford step 337 current loss 0.358318, current_train_items 10816.
I0304 19:28:06.590147 22502662377600 run.py:483] Algo bellman_ford step 338 current loss 0.288620, current_train_items 10848.
I0304 19:28:06.623108 22502662377600 run.py:483] Algo bellman_ford step 339 current loss 0.420298, current_train_items 10880.
I0304 19:28:06.641917 22502662377600 run.py:483] Algo bellman_ford step 340 current loss 0.091060, current_train_items 10912.
I0304 19:28:06.657924 22502662377600 run.py:483] Algo bellman_ford step 341 current loss 0.186008, current_train_items 10944.
I0304 19:28:06.681147 22502662377600 run.py:483] Algo bellman_ford step 342 current loss 0.373281, current_train_items 10976.
I0304 19:28:06.709650 22502662377600 run.py:483] Algo bellman_ford step 343 current loss 0.287891, current_train_items 11008.
I0304 19:28:06.741981 22502662377600 run.py:483] Algo bellman_ford step 344 current loss 0.403766, current_train_items 11040.
I0304 19:28:06.761029 22502662377600 run.py:483] Algo bellman_ford step 345 current loss 0.091614, current_train_items 11072.
I0304 19:28:06.777162 22502662377600 run.py:483] Algo bellman_ford step 346 current loss 0.188916, current_train_items 11104.
I0304 19:28:06.801845 22502662377600 run.py:483] Algo bellman_ford step 347 current loss 0.385030, current_train_items 11136.
I0304 19:28:06.831401 22502662377600 run.py:483] Algo bellman_ford step 348 current loss 0.404345, current_train_items 11168.
I0304 19:28:06.864295 22502662377600 run.py:483] Algo bellman_ford step 349 current loss 0.405063, current_train_items 11200.
I0304 19:28:06.883337 22502662377600 run.py:483] Algo bellman_ford step 350 current loss 0.098778, current_train_items 11232.
I0304 19:28:06.891493 22502662377600 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0304 19:28:06.891598 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.929, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0304 19:28:06.920682 22502662377600 run.py:483] Algo bellman_ford step 351 current loss 0.169580, current_train_items 11264.
I0304 19:28:06.944620 22502662377600 run.py:483] Algo bellman_ford step 352 current loss 0.345779, current_train_items 11296.
I0304 19:28:06.974879 22502662377600 run.py:483] Algo bellman_ford step 353 current loss 0.371166, current_train_items 11328.
I0304 19:28:07.008967 22502662377600 run.py:483] Algo bellman_ford step 354 current loss 0.443912, current_train_items 11360.
I0304 19:28:07.028009 22502662377600 run.py:483] Algo bellman_ford step 355 current loss 0.089619, current_train_items 11392.
I0304 19:28:07.043807 22502662377600 run.py:483] Algo bellman_ford step 356 current loss 0.191852, current_train_items 11424.
I0304 19:28:07.066934 22502662377600 run.py:483] Algo bellman_ford step 357 current loss 0.272659, current_train_items 11456.
I0304 19:28:07.097946 22502662377600 run.py:483] Algo bellman_ford step 358 current loss 0.408691, current_train_items 11488.
I0304 19:28:07.131211 22502662377600 run.py:483] Algo bellman_ford step 359 current loss 0.504913, current_train_items 11520.
I0304 19:28:07.150372 22502662377600 run.py:483] Algo bellman_ford step 360 current loss 0.110902, current_train_items 11552.
I0304 19:28:07.166919 22502662377600 run.py:483] Algo bellman_ford step 361 current loss 0.271908, current_train_items 11584.
I0304 19:28:07.189690 22502662377600 run.py:483] Algo bellman_ford step 362 current loss 0.276074, current_train_items 11616.
I0304 19:28:07.219264 22502662377600 run.py:483] Algo bellman_ford step 363 current loss 0.372865, current_train_items 11648.
I0304 19:28:07.249815 22502662377600 run.py:483] Algo bellman_ford step 364 current loss 0.389916, current_train_items 11680.
I0304 19:28:07.268206 22502662377600 run.py:483] Algo bellman_ford step 365 current loss 0.110391, current_train_items 11712.
I0304 19:28:07.284266 22502662377600 run.py:483] Algo bellman_ford step 366 current loss 0.162151, current_train_items 11744.
I0304 19:28:07.306929 22502662377600 run.py:483] Algo bellman_ford step 367 current loss 0.221858, current_train_items 11776.
I0304 19:28:07.337378 22502662377600 run.py:483] Algo bellman_ford step 368 current loss 0.376144, current_train_items 11808.
I0304 19:28:07.367559 22502662377600 run.py:483] Algo bellman_ford step 369 current loss 0.265841, current_train_items 11840.
I0304 19:28:07.386447 22502662377600 run.py:483] Algo bellman_ford step 370 current loss 0.059189, current_train_items 11872.
I0304 19:28:07.402428 22502662377600 run.py:483] Algo bellman_ford step 371 current loss 0.157725, current_train_items 11904.
I0304 19:28:07.425352 22502662377600 run.py:483] Algo bellman_ford step 372 current loss 0.281783, current_train_items 11936.
I0304 19:28:07.455887 22502662377600 run.py:483] Algo bellman_ford step 373 current loss 0.421750, current_train_items 11968.
I0304 19:28:07.489431 22502662377600 run.py:483] Algo bellman_ford step 374 current loss 0.468830, current_train_items 12000.
I0304 19:28:07.508638 22502662377600 run.py:483] Algo bellman_ford step 375 current loss 0.071067, current_train_items 12032.
I0304 19:28:07.524374 22502662377600 run.py:483] Algo bellman_ford step 376 current loss 0.144075, current_train_items 12064.
I0304 19:28:07.548521 22502662377600 run.py:483] Algo bellman_ford step 377 current loss 0.243484, current_train_items 12096.
I0304 19:28:07.577970 22502662377600 run.py:483] Algo bellman_ford step 378 current loss 0.235088, current_train_items 12128.
I0304 19:28:07.611082 22502662377600 run.py:483] Algo bellman_ford step 379 current loss 0.460318, current_train_items 12160.
I0304 19:28:07.629767 22502662377600 run.py:483] Algo bellman_ford step 380 current loss 0.062468, current_train_items 12192.
I0304 19:28:07.646126 22502662377600 run.py:483] Algo bellman_ford step 381 current loss 0.170078, current_train_items 12224.
I0304 19:28:07.669966 22502662377600 run.py:483] Algo bellman_ford step 382 current loss 0.254277, current_train_items 12256.
I0304 19:28:07.699773 22502662377600 run.py:483] Algo bellman_ford step 383 current loss 0.419795, current_train_items 12288.
I0304 19:28:07.732809 22502662377600 run.py:483] Algo bellman_ford step 384 current loss 0.364181, current_train_items 12320.
I0304 19:28:07.752053 22502662377600 run.py:483] Algo bellman_ford step 385 current loss 0.072421, current_train_items 12352.
I0304 19:28:07.768475 22502662377600 run.py:483] Algo bellman_ford step 386 current loss 0.109773, current_train_items 12384.
I0304 19:28:07.791502 22502662377600 run.py:483] Algo bellman_ford step 387 current loss 0.268207, current_train_items 12416.
I0304 19:28:07.821069 22502662377600 run.py:483] Algo bellman_ford step 388 current loss 0.283312, current_train_items 12448.
I0304 19:28:07.854046 22502662377600 run.py:483] Algo bellman_ford step 389 current loss 0.414421, current_train_items 12480.
I0304 19:28:07.873138 22502662377600 run.py:483] Algo bellman_ford step 390 current loss 0.093686, current_train_items 12512.
I0304 19:28:07.889256 22502662377600 run.py:483] Algo bellman_ford step 391 current loss 0.184495, current_train_items 12544.
I0304 19:28:07.912753 22502662377600 run.py:483] Algo bellman_ford step 392 current loss 0.233181, current_train_items 12576.
I0304 19:28:07.944193 22502662377600 run.py:483] Algo bellman_ford step 393 current loss 0.490646, current_train_items 12608.
I0304 19:28:07.976196 22502662377600 run.py:483] Algo bellman_ford step 394 current loss 0.711348, current_train_items 12640.
I0304 19:28:07.994530 22502662377600 run.py:483] Algo bellman_ford step 395 current loss 0.073302, current_train_items 12672.
I0304 19:28:08.010729 22502662377600 run.py:483] Algo bellman_ford step 396 current loss 0.209656, current_train_items 12704.
I0304 19:28:08.034667 22502662377600 run.py:483] Algo bellman_ford step 397 current loss 0.400248, current_train_items 12736.
I0304 19:28:08.065428 22502662377600 run.py:483] Algo bellman_ford step 398 current loss 0.353085, current_train_items 12768.
I0304 19:28:08.095664 22502662377600 run.py:483] Algo bellman_ford step 399 current loss 0.362474, current_train_items 12800.
I0304 19:28:08.114656 22502662377600 run.py:483] Algo bellman_ford step 400 current loss 0.051490, current_train_items 12832.
I0304 19:28:08.122539 22502662377600 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0304 19:28:08.122668 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.932, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0304 19:28:08.139566 22502662377600 run.py:483] Algo bellman_ford step 401 current loss 0.188005, current_train_items 12864.
I0304 19:28:08.163546 22502662377600 run.py:483] Algo bellman_ford step 402 current loss 0.311962, current_train_items 12896.
I0304 19:28:08.193568 22502662377600 run.py:483] Algo bellman_ford step 403 current loss 0.357241, current_train_items 12928.
I0304 19:28:08.224479 22502662377600 run.py:483] Algo bellman_ford step 404 current loss 0.411666, current_train_items 12960.
I0304 19:28:08.243343 22502662377600 run.py:483] Algo bellman_ford step 405 current loss 0.116362, current_train_items 12992.
I0304 19:28:08.258627 22502662377600 run.py:483] Algo bellman_ford step 406 current loss 0.124011, current_train_items 13024.
I0304 19:28:08.282017 22502662377600 run.py:483] Algo bellman_ford step 407 current loss 0.294480, current_train_items 13056.
I0304 19:28:08.312794 22502662377600 run.py:483] Algo bellman_ford step 408 current loss 0.359242, current_train_items 13088.
I0304 19:28:08.344371 22502662377600 run.py:483] Algo bellman_ford step 409 current loss 0.408984, current_train_items 13120.
I0304 19:28:08.362822 22502662377600 run.py:483] Algo bellman_ford step 410 current loss 0.098327, current_train_items 13152.
I0304 19:28:08.378610 22502662377600 run.py:483] Algo bellman_ford step 411 current loss 0.123992, current_train_items 13184.
I0304 19:28:08.402130 22502662377600 run.py:483] Algo bellman_ford step 412 current loss 0.240377, current_train_items 13216.
I0304 19:28:08.432035 22502662377600 run.py:483] Algo bellman_ford step 413 current loss 0.283950, current_train_items 13248.
I0304 19:28:08.464141 22502662377600 run.py:483] Algo bellman_ford step 414 current loss 0.416716, current_train_items 13280.
I0304 19:28:08.482664 22502662377600 run.py:483] Algo bellman_ford step 415 current loss 0.105706, current_train_items 13312.
I0304 19:28:08.498454 22502662377600 run.py:483] Algo bellman_ford step 416 current loss 0.117167, current_train_items 13344.
I0304 19:28:08.522506 22502662377600 run.py:483] Algo bellman_ford step 417 current loss 0.285116, current_train_items 13376.
I0304 19:28:08.553994 22502662377600 run.py:483] Algo bellman_ford step 418 current loss 0.348599, current_train_items 13408.
I0304 19:28:08.585540 22502662377600 run.py:483] Algo bellman_ford step 419 current loss 0.427916, current_train_items 13440.
I0304 19:28:08.603930 22502662377600 run.py:483] Algo bellman_ford step 420 current loss 0.071635, current_train_items 13472.
I0304 19:28:08.619789 22502662377600 run.py:483] Algo bellman_ford step 421 current loss 0.085697, current_train_items 13504.
I0304 19:28:08.643589 22502662377600 run.py:483] Algo bellman_ford step 422 current loss 0.226866, current_train_items 13536.
I0304 19:28:08.672130 22502662377600 run.py:483] Algo bellman_ford step 423 current loss 0.291920, current_train_items 13568.
I0304 19:28:08.703734 22502662377600 run.py:483] Algo bellman_ford step 424 current loss 0.344533, current_train_items 13600.
I0304 19:28:08.722518 22502662377600 run.py:483] Algo bellman_ford step 425 current loss 0.115213, current_train_items 13632.
I0304 19:28:08.738142 22502662377600 run.py:483] Algo bellman_ford step 426 current loss 0.087367, current_train_items 13664.
I0304 19:28:08.760997 22502662377600 run.py:483] Algo bellman_ford step 427 current loss 0.276020, current_train_items 13696.
I0304 19:28:08.790035 22502662377600 run.py:483] Algo bellman_ford step 428 current loss 0.240289, current_train_items 13728.
I0304 19:28:08.821767 22502662377600 run.py:483] Algo bellman_ford step 429 current loss 0.321128, current_train_items 13760.
I0304 19:28:08.839981 22502662377600 run.py:483] Algo bellman_ford step 430 current loss 0.074777, current_train_items 13792.
I0304 19:28:08.855918 22502662377600 run.py:483] Algo bellman_ford step 431 current loss 0.139633, current_train_items 13824.
I0304 19:28:08.880027 22502662377600 run.py:483] Algo bellman_ford step 432 current loss 0.237054, current_train_items 13856.
I0304 19:28:08.908334 22502662377600 run.py:483] Algo bellman_ford step 433 current loss 0.293601, current_train_items 13888.
I0304 19:28:08.939900 22502662377600 run.py:483] Algo bellman_ford step 434 current loss 0.480290, current_train_items 13920.
I0304 19:28:08.957886 22502662377600 run.py:483] Algo bellman_ford step 435 current loss 0.083316, current_train_items 13952.
I0304 19:28:08.974087 22502662377600 run.py:483] Algo bellman_ford step 436 current loss 0.120035, current_train_items 13984.
I0304 19:28:08.996855 22502662377600 run.py:483] Algo bellman_ford step 437 current loss 0.196944, current_train_items 14016.
I0304 19:28:09.025701 22502662377600 run.py:483] Algo bellman_ford step 438 current loss 0.325687, current_train_items 14048.
I0304 19:28:09.057624 22502662377600 run.py:483] Algo bellman_ford step 439 current loss 0.449747, current_train_items 14080.
I0304 19:28:09.075974 22502662377600 run.py:483] Algo bellman_ford step 440 current loss 0.059584, current_train_items 14112.
I0304 19:28:09.091747 22502662377600 run.py:483] Algo bellman_ford step 441 current loss 0.177356, current_train_items 14144.
I0304 19:28:09.114307 22502662377600 run.py:483] Algo bellman_ford step 442 current loss 0.304983, current_train_items 14176.
I0304 19:28:09.143680 22502662377600 run.py:483] Algo bellman_ford step 443 current loss 0.350770, current_train_items 14208.
I0304 19:28:09.174644 22502662377600 run.py:483] Algo bellman_ford step 444 current loss 0.325073, current_train_items 14240.
I0304 19:28:09.193100 22502662377600 run.py:483] Algo bellman_ford step 445 current loss 0.073358, current_train_items 14272.
I0304 19:28:09.210067 22502662377600 run.py:483] Algo bellman_ford step 446 current loss 0.236020, current_train_items 14304.
I0304 19:28:09.233649 22502662377600 run.py:483] Algo bellman_ford step 447 current loss 0.281196, current_train_items 14336.
I0304 19:28:09.261408 22502662377600 run.py:483] Algo bellman_ford step 448 current loss 0.276280, current_train_items 14368.
I0304 19:28:09.291121 22502662377600 run.py:483] Algo bellman_ford step 449 current loss 0.284499, current_train_items 14400.
I0304 19:28:09.309583 22502662377600 run.py:483] Algo bellman_ford step 450 current loss 0.069772, current_train_items 14432.
I0304 19:28:09.317742 22502662377600 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0304 19:28:09.317843 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.932, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0304 19:28:09.334800 22502662377600 run.py:483] Algo bellman_ford step 451 current loss 0.144170, current_train_items 14464.
I0304 19:28:09.358241 22502662377600 run.py:483] Algo bellman_ford step 452 current loss 0.314402, current_train_items 14496.
I0304 19:28:09.389347 22502662377600 run.py:483] Algo bellman_ford step 453 current loss 0.430882, current_train_items 14528.
I0304 19:28:09.421678 22502662377600 run.py:483] Algo bellman_ford step 454 current loss 0.354546, current_train_items 14560.
I0304 19:28:09.440320 22502662377600 run.py:483] Algo bellman_ford step 455 current loss 0.093829, current_train_items 14592.
I0304 19:28:09.456276 22502662377600 run.py:483] Algo bellman_ford step 456 current loss 0.412456, current_train_items 14624.
I0304 19:28:09.479267 22502662377600 run.py:483] Algo bellman_ford step 457 current loss 0.560040, current_train_items 14656.
I0304 19:28:09.508662 22502662377600 run.py:483] Algo bellman_ford step 458 current loss 0.396578, current_train_items 14688.
I0304 19:28:09.539419 22502662377600 run.py:483] Algo bellman_ford step 459 current loss 0.397248, current_train_items 14720.
I0304 19:28:09.558327 22502662377600 run.py:483] Algo bellman_ford step 460 current loss 0.047338, current_train_items 14752.
I0304 19:28:09.574807 22502662377600 run.py:483] Algo bellman_ford step 461 current loss 0.191499, current_train_items 14784.
I0304 19:28:09.598493 22502662377600 run.py:483] Algo bellman_ford step 462 current loss 0.628707, current_train_items 14816.
I0304 19:28:09.628323 22502662377600 run.py:483] Algo bellman_ford step 463 current loss 0.896928, current_train_items 14848.
I0304 19:28:09.661452 22502662377600 run.py:483] Algo bellman_ford step 464 current loss 0.894671, current_train_items 14880.
I0304 19:28:09.680351 22502662377600 run.py:483] Algo bellman_ford step 465 current loss 0.064243, current_train_items 14912.
I0304 19:28:09.696052 22502662377600 run.py:483] Algo bellman_ford step 466 current loss 0.134789, current_train_items 14944.
I0304 19:28:09.720305 22502662377600 run.py:483] Algo bellman_ford step 467 current loss 0.328223, current_train_items 14976.
I0304 19:28:09.748902 22502662377600 run.py:483] Algo bellman_ford step 468 current loss 0.235859, current_train_items 15008.
I0304 19:28:09.782275 22502662377600 run.py:483] Algo bellman_ford step 469 current loss 0.384058, current_train_items 15040.
I0304 19:28:09.801116 22502662377600 run.py:483] Algo bellman_ford step 470 current loss 0.081513, current_train_items 15072.
I0304 19:28:09.817199 22502662377600 run.py:483] Algo bellman_ford step 471 current loss 0.124953, current_train_items 15104.
I0304 19:28:09.841059 22502662377600 run.py:483] Algo bellman_ford step 472 current loss 0.256792, current_train_items 15136.
I0304 19:28:09.871251 22502662377600 run.py:483] Algo bellman_ford step 473 current loss 0.249726, current_train_items 15168.
I0304 19:28:09.904214 22502662377600 run.py:483] Algo bellman_ford step 474 current loss 0.419425, current_train_items 15200.
I0304 19:28:09.923487 22502662377600 run.py:483] Algo bellman_ford step 475 current loss 0.078758, current_train_items 15232.
I0304 19:28:09.939352 22502662377600 run.py:483] Algo bellman_ford step 476 current loss 0.191989, current_train_items 15264.
I0304 19:28:09.963246 22502662377600 run.py:483] Algo bellman_ford step 477 current loss 0.239480, current_train_items 15296.
I0304 19:28:09.991488 22502662377600 run.py:483] Algo bellman_ford step 478 current loss 0.175677, current_train_items 15328.
I0304 19:28:10.024376 22502662377600 run.py:483] Algo bellman_ford step 479 current loss 0.494067, current_train_items 15360.
I0304 19:28:10.043023 22502662377600 run.py:483] Algo bellman_ford step 480 current loss 0.063955, current_train_items 15392.
I0304 19:28:10.059100 22502662377600 run.py:483] Algo bellman_ford step 481 current loss 0.118074, current_train_items 15424.
I0304 19:28:10.082040 22502662377600 run.py:483] Algo bellman_ford step 482 current loss 0.247828, current_train_items 15456.
I0304 19:28:10.111880 22502662377600 run.py:483] Algo bellman_ford step 483 current loss 0.236497, current_train_items 15488.
I0304 19:28:10.144320 22502662377600 run.py:483] Algo bellman_ford step 484 current loss 0.332387, current_train_items 15520.
I0304 19:28:10.163398 22502662377600 run.py:483] Algo bellman_ford step 485 current loss 0.056121, current_train_items 15552.
I0304 19:28:10.179571 22502662377600 run.py:483] Algo bellman_ford step 486 current loss 0.121830, current_train_items 15584.
I0304 19:28:10.203357 22502662377600 run.py:483] Algo bellman_ford step 487 current loss 0.276105, current_train_items 15616.
I0304 19:28:10.232518 22502662377600 run.py:483] Algo bellman_ford step 488 current loss 0.235813, current_train_items 15648.
I0304 19:28:10.263866 22502662377600 run.py:483] Algo bellman_ford step 489 current loss 0.299231, current_train_items 15680.
I0304 19:28:10.282871 22502662377600 run.py:483] Algo bellman_ford step 490 current loss 0.049162, current_train_items 15712.
I0304 19:28:10.299250 22502662377600 run.py:483] Algo bellman_ford step 491 current loss 0.113797, current_train_items 15744.
I0304 19:28:10.322425 22502662377600 run.py:483] Algo bellman_ford step 492 current loss 0.171748, current_train_items 15776.
I0304 19:28:10.351929 22502662377600 run.py:483] Algo bellman_ford step 493 current loss 0.210877, current_train_items 15808.
I0304 19:28:10.385245 22502662377600 run.py:483] Algo bellman_ford step 494 current loss 0.344983, current_train_items 15840.
I0304 19:28:10.403743 22502662377600 run.py:483] Algo bellman_ford step 495 current loss 0.039820, current_train_items 15872.
I0304 19:28:10.419617 22502662377600 run.py:483] Algo bellman_ford step 496 current loss 0.119773, current_train_items 15904.
I0304 19:28:10.444409 22502662377600 run.py:483] Algo bellman_ford step 497 current loss 0.310608, current_train_items 15936.
I0304 19:28:10.474809 22502662377600 run.py:483] Algo bellman_ford step 498 current loss 0.291503, current_train_items 15968.
I0304 19:28:10.506627 22502662377600 run.py:483] Algo bellman_ford step 499 current loss 0.274446, current_train_items 16000.
I0304 19:28:10.525604 22502662377600 run.py:483] Algo bellman_ford step 500 current loss 0.057885, current_train_items 16032.
I0304 19:28:10.533239 22502662377600 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.939453125, 'score': 0.939453125, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0304 19:28:10.533344 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.932, current avg val score is 0.939, val scores are: bellman_ford: 0.939
I0304 19:28:10.563856 22502662377600 run.py:483] Algo bellman_ford step 501 current loss 0.075435, current_train_items 16064.
I0304 19:28:10.588122 22502662377600 run.py:483] Algo bellman_ford step 502 current loss 0.266488, current_train_items 16096.
I0304 19:28:10.619338 22502662377600 run.py:483] Algo bellman_ford step 503 current loss 0.424027, current_train_items 16128.
I0304 19:28:10.654267 22502662377600 run.py:483] Algo bellman_ford step 504 current loss 0.503036, current_train_items 16160.
I0304 19:28:10.673518 22502662377600 run.py:483] Algo bellman_ford step 505 current loss 0.049112, current_train_items 16192.
I0304 19:28:10.688804 22502662377600 run.py:483] Algo bellman_ford step 506 current loss 0.099226, current_train_items 16224.
I0304 19:28:10.711485 22502662377600 run.py:483] Algo bellman_ford step 507 current loss 0.142238, current_train_items 16256.
I0304 19:28:10.741826 22502662377600 run.py:483] Algo bellman_ford step 508 current loss 0.235707, current_train_items 16288.
I0304 19:28:10.774931 22502662377600 run.py:483] Algo bellman_ford step 509 current loss 0.427232, current_train_items 16320.
I0304 19:28:10.793879 22502662377600 run.py:483] Algo bellman_ford step 510 current loss 0.073856, current_train_items 16352.
I0304 19:28:10.810089 22502662377600 run.py:483] Algo bellman_ford step 511 current loss 0.202061, current_train_items 16384.
I0304 19:28:10.833694 22502662377600 run.py:483] Algo bellman_ford step 512 current loss 0.229601, current_train_items 16416.
I0304 19:28:10.863858 22502662377600 run.py:483] Algo bellman_ford step 513 current loss 0.279376, current_train_items 16448.
I0304 19:28:10.896272 22502662377600 run.py:483] Algo bellman_ford step 514 current loss 0.364895, current_train_items 16480.
I0304 19:28:10.914976 22502662377600 run.py:483] Algo bellman_ford step 515 current loss 0.038807, current_train_items 16512.
I0304 19:28:10.930753 22502662377600 run.py:483] Algo bellman_ford step 516 current loss 0.111881, current_train_items 16544.
I0304 19:28:10.954895 22502662377600 run.py:483] Algo bellman_ford step 517 current loss 0.163171, current_train_items 16576.
I0304 19:28:10.985777 22502662377600 run.py:483] Algo bellman_ford step 518 current loss 0.333435, current_train_items 16608.
I0304 19:28:11.017663 22502662377600 run.py:483] Algo bellman_ford step 519 current loss 0.428133, current_train_items 16640.
I0304 19:28:11.036142 22502662377600 run.py:483] Algo bellman_ford step 520 current loss 0.089550, current_train_items 16672.
I0304 19:28:11.052514 22502662377600 run.py:483] Algo bellman_ford step 521 current loss 0.160871, current_train_items 16704.
I0304 19:28:11.075768 22502662377600 run.py:483] Algo bellman_ford step 522 current loss 0.236981, current_train_items 16736.
I0304 19:28:11.106256 22502662377600 run.py:483] Algo bellman_ford step 523 current loss 0.283648, current_train_items 16768.
I0304 19:28:11.137869 22502662377600 run.py:483] Algo bellman_ford step 524 current loss 0.334446, current_train_items 16800.
I0304 19:28:11.156626 22502662377600 run.py:483] Algo bellman_ford step 525 current loss 0.048858, current_train_items 16832.
I0304 19:28:11.172899 22502662377600 run.py:483] Algo bellman_ford step 526 current loss 0.197799, current_train_items 16864.
I0304 19:28:11.196636 22502662377600 run.py:483] Algo bellman_ford step 527 current loss 0.379754, current_train_items 16896.
I0304 19:28:11.225437 22502662377600 run.py:483] Algo bellman_ford step 528 current loss 0.358139, current_train_items 16928.
I0304 19:28:11.258187 22502662377600 run.py:483] Algo bellman_ford step 529 current loss 0.366338, current_train_items 16960.
I0304 19:28:11.276917 22502662377600 run.py:483] Algo bellman_ford step 530 current loss 0.043811, current_train_items 16992.
I0304 19:28:11.292956 22502662377600 run.py:483] Algo bellman_ford step 531 current loss 0.147017, current_train_items 17024.
I0304 19:28:11.315676 22502662377600 run.py:483] Algo bellman_ford step 532 current loss 0.193968, current_train_items 17056.
I0304 19:28:11.345746 22502662377600 run.py:483] Algo bellman_ford step 533 current loss 0.357067, current_train_items 17088.
I0304 19:28:11.378338 22502662377600 run.py:483] Algo bellman_ford step 534 current loss 0.443499, current_train_items 17120.
I0304 19:28:11.396947 22502662377600 run.py:483] Algo bellman_ford step 535 current loss 0.055081, current_train_items 17152.
I0304 19:28:11.412611 22502662377600 run.py:483] Algo bellman_ford step 536 current loss 0.103510, current_train_items 17184.
I0304 19:28:11.435366 22502662377600 run.py:483] Algo bellman_ford step 537 current loss 0.201757, current_train_items 17216.
I0304 19:28:11.464212 22502662377600 run.py:483] Algo bellman_ford step 538 current loss 0.221381, current_train_items 17248.
I0304 19:28:11.495113 22502662377600 run.py:483] Algo bellman_ford step 539 current loss 0.317095, current_train_items 17280.
I0304 19:28:11.513817 22502662377600 run.py:483] Algo bellman_ford step 540 current loss 0.058070, current_train_items 17312.
I0304 19:28:11.529366 22502662377600 run.py:483] Algo bellman_ford step 541 current loss 0.068174, current_train_items 17344.
I0304 19:28:11.553162 22502662377600 run.py:483] Algo bellman_ford step 542 current loss 0.337480, current_train_items 17376.
I0304 19:28:11.583301 22502662377600 run.py:483] Algo bellman_ford step 543 current loss 0.399261, current_train_items 17408.
I0304 19:28:11.616243 22502662377600 run.py:483] Algo bellman_ford step 544 current loss 0.420730, current_train_items 17440.
I0304 19:28:11.635166 22502662377600 run.py:483] Algo bellman_ford step 545 current loss 0.055783, current_train_items 17472.
I0304 19:28:11.651266 22502662377600 run.py:483] Algo bellman_ford step 546 current loss 0.185068, current_train_items 17504.
I0304 19:28:11.674656 22502662377600 run.py:483] Algo bellman_ford step 547 current loss 0.291160, current_train_items 17536.
I0304 19:28:11.704801 22502662377600 run.py:483] Algo bellman_ford step 548 current loss 0.421722, current_train_items 17568.
I0304 19:28:11.734579 22502662377600 run.py:483] Algo bellman_ford step 549 current loss 0.269324, current_train_items 17600.
I0304 19:28:11.753412 22502662377600 run.py:483] Algo bellman_ford step 550 current loss 0.072018, current_train_items 17632.
I0304 19:28:11.761595 22502662377600 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0304 19:28:11.761698 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0304 19:28:11.778368 22502662377600 run.py:483] Algo bellman_ford step 551 current loss 0.091261, current_train_items 17664.
I0304 19:28:11.801801 22502662377600 run.py:483] Algo bellman_ford step 552 current loss 0.327532, current_train_items 17696.
I0304 19:28:11.833621 22502662377600 run.py:483] Algo bellman_ford step 553 current loss 0.621843, current_train_items 17728.
I0304 19:28:11.865705 22502662377600 run.py:483] Algo bellman_ford step 554 current loss 0.477179, current_train_items 17760.
I0304 19:28:11.884798 22502662377600 run.py:483] Algo bellman_ford step 555 current loss 0.098202, current_train_items 17792.
I0304 19:28:11.900457 22502662377600 run.py:483] Algo bellman_ford step 556 current loss 0.222393, current_train_items 17824.
I0304 19:28:11.925158 22502662377600 run.py:483] Algo bellman_ford step 557 current loss 0.493297, current_train_items 17856.
I0304 19:28:11.954212 22502662377600 run.py:483] Algo bellman_ford step 558 current loss 0.495101, current_train_items 17888.
I0304 19:28:11.987840 22502662377600 run.py:483] Algo bellman_ford step 559 current loss 0.419717, current_train_items 17920.
I0304 19:28:12.007194 22502662377600 run.py:483] Algo bellman_ford step 560 current loss 0.119653, current_train_items 17952.
I0304 19:28:12.023585 22502662377600 run.py:483] Algo bellman_ford step 561 current loss 0.127104, current_train_items 17984.
I0304 19:28:12.047241 22502662377600 run.py:483] Algo bellman_ford step 562 current loss 0.275962, current_train_items 18016.
I0304 19:28:12.076252 22502662377600 run.py:483] Algo bellman_ford step 563 current loss 0.227300, current_train_items 18048.
I0304 19:28:12.106545 22502662377600 run.py:483] Algo bellman_ford step 564 current loss 0.258412, current_train_items 18080.
I0304 19:28:12.125182 22502662377600 run.py:483] Algo bellman_ford step 565 current loss 0.044006, current_train_items 18112.
I0304 19:28:12.141381 22502662377600 run.py:483] Algo bellman_ford step 566 current loss 0.140937, current_train_items 18144.
I0304 19:28:12.165784 22502662377600 run.py:483] Algo bellman_ford step 567 current loss 0.263728, current_train_items 18176.
I0304 19:28:12.195660 22502662377600 run.py:483] Algo bellman_ford step 568 current loss 0.262425, current_train_items 18208.
I0304 19:28:12.227064 22502662377600 run.py:483] Algo bellman_ford step 569 current loss 0.370823, current_train_items 18240.
I0304 19:28:12.245985 22502662377600 run.py:483] Algo bellman_ford step 570 current loss 0.217051, current_train_items 18272.
I0304 19:28:12.261904 22502662377600 run.py:483] Algo bellman_ford step 571 current loss 0.186034, current_train_items 18304.
I0304 19:28:12.285798 22502662377600 run.py:483] Algo bellman_ford step 572 current loss 0.242541, current_train_items 18336.
I0304 19:28:12.316722 22502662377600 run.py:483] Algo bellman_ford step 573 current loss 0.255318, current_train_items 18368.
I0304 19:28:12.347927 22502662377600 run.py:483] Algo bellman_ford step 574 current loss 0.314578, current_train_items 18400.
I0304 19:28:12.367307 22502662377600 run.py:483] Algo bellman_ford step 575 current loss 0.064462, current_train_items 18432.
I0304 19:28:12.383204 22502662377600 run.py:483] Algo bellman_ford step 576 current loss 0.098439, current_train_items 18464.
I0304 19:28:12.406638 22502662377600 run.py:483] Algo bellman_ford step 577 current loss 0.212358, current_train_items 18496.
I0304 19:28:12.436980 22502662377600 run.py:483] Algo bellman_ford step 578 current loss 0.364812, current_train_items 18528.
I0304 19:28:12.470245 22502662377600 run.py:483] Algo bellman_ford step 579 current loss 0.470995, current_train_items 18560.
I0304 19:28:12.488985 22502662377600 run.py:483] Algo bellman_ford step 580 current loss 0.063457, current_train_items 18592.
I0304 19:28:12.504930 22502662377600 run.py:483] Algo bellman_ford step 581 current loss 0.136760, current_train_items 18624.
I0304 19:28:12.527305 22502662377600 run.py:483] Algo bellman_ford step 582 current loss 0.238881, current_train_items 18656.
I0304 19:28:12.555750 22502662377600 run.py:483] Algo bellman_ford step 583 current loss 0.191320, current_train_items 18688.
I0304 19:28:12.587711 22502662377600 run.py:483] Algo bellman_ford step 584 current loss 0.259459, current_train_items 18720.
I0304 19:28:12.606477 22502662377600 run.py:483] Algo bellman_ford step 585 current loss 0.039644, current_train_items 18752.
I0304 19:28:12.622662 22502662377600 run.py:483] Algo bellman_ford step 586 current loss 0.104947, current_train_items 18784.
I0304 19:28:12.644898 22502662377600 run.py:483] Algo bellman_ford step 587 current loss 0.074503, current_train_items 18816.
I0304 19:28:12.674016 22502662377600 run.py:483] Algo bellman_ford step 588 current loss 0.246076, current_train_items 18848.
I0304 19:28:12.707211 22502662377600 run.py:483] Algo bellman_ford step 589 current loss 0.402073, current_train_items 18880.
I0304 19:28:12.726633 22502662377600 run.py:483] Algo bellman_ford step 590 current loss 0.053058, current_train_items 18912.
I0304 19:28:12.742742 22502662377600 run.py:483] Algo bellman_ford step 591 current loss 0.120741, current_train_items 18944.
I0304 19:28:12.766819 22502662377600 run.py:483] Algo bellman_ford step 592 current loss 0.275929, current_train_items 18976.
I0304 19:28:12.796364 22502662377600 run.py:483] Algo bellman_ford step 593 current loss 0.249813, current_train_items 19008.
I0304 19:28:12.828268 22502662377600 run.py:483] Algo bellman_ford step 594 current loss 0.319539, current_train_items 19040.
I0304 19:28:12.846786 22502662377600 run.py:483] Algo bellman_ford step 595 current loss 0.043793, current_train_items 19072.
I0304 19:28:12.862861 22502662377600 run.py:483] Algo bellman_ford step 596 current loss 0.075087, current_train_items 19104.
I0304 19:28:12.887282 22502662377600 run.py:483] Algo bellman_ford step 597 current loss 0.224971, current_train_items 19136.
I0304 19:28:12.917210 22502662377600 run.py:483] Algo bellman_ford step 598 current loss 0.209495, current_train_items 19168.
I0304 19:28:12.949277 22502662377600 run.py:483] Algo bellman_ford step 599 current loss 0.297117, current_train_items 19200.
I0304 19:28:12.968330 22502662377600 run.py:483] Algo bellman_ford step 600 current loss 0.096193, current_train_items 19232.
I0304 19:28:12.976096 22502662377600 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.947265625, 'score': 0.947265625, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0304 19:28:12.976200 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.939, current avg val score is 0.947, val scores are: bellman_ford: 0.947
I0304 19:28:13.005554 22502662377600 run.py:483] Algo bellman_ford step 601 current loss 0.083572, current_train_items 19264.
I0304 19:28:13.029472 22502662377600 run.py:483] Algo bellman_ford step 602 current loss 0.226192, current_train_items 19296.
I0304 19:28:13.059127 22502662377600 run.py:483] Algo bellman_ford step 603 current loss 0.257859, current_train_items 19328.
I0304 19:28:13.090714 22502662377600 run.py:483] Algo bellman_ford step 604 current loss 0.271188, current_train_items 19360.
I0304 19:28:13.110191 22502662377600 run.py:483] Algo bellman_ford step 605 current loss 0.050405, current_train_items 19392.
I0304 19:28:13.125654 22502662377600 run.py:483] Algo bellman_ford step 606 current loss 0.089723, current_train_items 19424.
I0304 19:28:13.150119 22502662377600 run.py:483] Algo bellman_ford step 607 current loss 0.227437, current_train_items 19456.
I0304 19:28:13.179033 22502662377600 run.py:483] Algo bellman_ford step 608 current loss 0.214770, current_train_items 19488.
I0304 19:28:13.211273 22502662377600 run.py:483] Algo bellman_ford step 609 current loss 0.265771, current_train_items 19520.
I0304 19:28:13.230034 22502662377600 run.py:483] Algo bellman_ford step 610 current loss 0.053571, current_train_items 19552.
I0304 19:28:13.246326 22502662377600 run.py:483] Algo bellman_ford step 611 current loss 0.074939, current_train_items 19584.
I0304 19:28:13.270028 22502662377600 run.py:483] Algo bellman_ford step 612 current loss 0.174760, current_train_items 19616.
I0304 19:28:13.298194 22502662377600 run.py:483] Algo bellman_ford step 613 current loss 0.196008, current_train_items 19648.
I0304 19:28:13.329885 22502662377600 run.py:483] Algo bellman_ford step 614 current loss 0.244903, current_train_items 19680.
I0304 19:28:13.348341 22502662377600 run.py:483] Algo bellman_ford step 615 current loss 0.026516, current_train_items 19712.
I0304 19:28:13.364674 22502662377600 run.py:483] Algo bellman_ford step 616 current loss 0.084464, current_train_items 19744.
I0304 19:28:13.387783 22502662377600 run.py:483] Algo bellman_ford step 617 current loss 0.160275, current_train_items 19776.
I0304 19:28:13.416717 22502662377600 run.py:483] Algo bellman_ford step 618 current loss 0.207692, current_train_items 19808.
I0304 19:28:13.448859 22502662377600 run.py:483] Algo bellman_ford step 619 current loss 0.251516, current_train_items 19840.
I0304 19:28:13.467705 22502662377600 run.py:483] Algo bellman_ford step 620 current loss 0.081975, current_train_items 19872.
I0304 19:28:13.484273 22502662377600 run.py:483] Algo bellman_ford step 621 current loss 0.198098, current_train_items 19904.
I0304 19:28:13.508289 22502662377600 run.py:483] Algo bellman_ford step 622 current loss 0.226588, current_train_items 19936.
I0304 19:28:13.537502 22502662377600 run.py:483] Algo bellman_ford step 623 current loss 0.253184, current_train_items 19968.
I0304 19:28:13.568794 22502662377600 run.py:483] Algo bellman_ford step 624 current loss 0.414964, current_train_items 20000.
I0304 19:28:13.587192 22502662377600 run.py:483] Algo bellman_ford step 625 current loss 0.045468, current_train_items 20032.
I0304 19:28:13.603067 22502662377600 run.py:483] Algo bellman_ford step 626 current loss 0.062423, current_train_items 20064.
I0304 19:28:13.627564 22502662377600 run.py:483] Algo bellman_ford step 627 current loss 0.161244, current_train_items 20096.
I0304 19:28:13.658200 22502662377600 run.py:483] Algo bellman_ford step 628 current loss 0.231094, current_train_items 20128.
I0304 19:28:13.690883 22502662377600 run.py:483] Algo bellman_ford step 629 current loss 0.369024, current_train_items 20160.
I0304 19:28:13.709451 22502662377600 run.py:483] Algo bellman_ford step 630 current loss 0.034128, current_train_items 20192.
I0304 19:28:13.725389 22502662377600 run.py:483] Algo bellman_ford step 631 current loss 0.084310, current_train_items 20224.
I0304 19:28:13.748646 22502662377600 run.py:483] Algo bellman_ford step 632 current loss 0.240203, current_train_items 20256.
I0304 19:28:13.778055 22502662377600 run.py:483] Algo bellman_ford step 633 current loss 0.282002, current_train_items 20288.
I0304 19:28:13.809791 22502662377600 run.py:483] Algo bellman_ford step 634 current loss 0.331729, current_train_items 20320.
I0304 19:28:13.828454 22502662377600 run.py:483] Algo bellman_ford step 635 current loss 0.080209, current_train_items 20352.
I0304 19:28:13.844473 22502662377600 run.py:483] Algo bellman_ford step 636 current loss 0.103657, current_train_items 20384.
I0304 19:28:13.867985 22502662377600 run.py:483] Algo bellman_ford step 637 current loss 0.294931, current_train_items 20416.
I0304 19:28:13.898439 22502662377600 run.py:483] Algo bellman_ford step 638 current loss 0.368250, current_train_items 20448.
I0304 19:28:13.927994 22502662377600 run.py:483] Algo bellman_ford step 639 current loss 0.271602, current_train_items 20480.
I0304 19:28:13.946681 22502662377600 run.py:483] Algo bellman_ford step 640 current loss 0.074663, current_train_items 20512.
I0304 19:28:13.962592 22502662377600 run.py:483] Algo bellman_ford step 641 current loss 0.272765, current_train_items 20544.
I0304 19:28:13.985099 22502662377600 run.py:483] Algo bellman_ford step 642 current loss 0.742730, current_train_items 20576.
I0304 19:28:14.014099 22502662377600 run.py:483] Algo bellman_ford step 643 current loss 0.642096, current_train_items 20608.
I0304 19:28:14.046200 22502662377600 run.py:483] Algo bellman_ford step 644 current loss 0.500534, current_train_items 20640.
I0304 19:28:14.064737 22502662377600 run.py:483] Algo bellman_ford step 645 current loss 0.046493, current_train_items 20672.
I0304 19:28:14.081701 22502662377600 run.py:483] Algo bellman_ford step 646 current loss 0.178592, current_train_items 20704.
I0304 19:28:14.104739 22502662377600 run.py:483] Algo bellman_ford step 647 current loss 0.258011, current_train_items 20736.
I0304 19:28:14.135657 22502662377600 run.py:483] Algo bellman_ford step 648 current loss 0.428012, current_train_items 20768.
I0304 19:28:14.168099 22502662377600 run.py:483] Algo bellman_ford step 649 current loss 0.268584, current_train_items 20800.
I0304 19:28:14.187025 22502662377600 run.py:483] Algo bellman_ford step 650 current loss 0.069270, current_train_items 20832.
I0304 19:28:14.195037 22502662377600 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0304 19:28:14.195142 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.947, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0304 19:28:14.212083 22502662377600 run.py:483] Algo bellman_ford step 651 current loss 0.195515, current_train_items 20864.
I0304 19:28:14.236052 22502662377600 run.py:483] Algo bellman_ford step 652 current loss 0.264347, current_train_items 20896.
I0304 19:28:14.264833 22502662377600 run.py:483] Algo bellman_ford step 653 current loss 0.199337, current_train_items 20928.
I0304 19:28:14.296481 22502662377600 run.py:483] Algo bellman_ford step 654 current loss 0.246088, current_train_items 20960.
I0304 19:28:14.315439 22502662377600 run.py:483] Algo bellman_ford step 655 current loss 0.034522, current_train_items 20992.
I0304 19:28:14.331195 22502662377600 run.py:483] Algo bellman_ford step 656 current loss 0.129045, current_train_items 21024.
I0304 19:28:14.355414 22502662377600 run.py:483] Algo bellman_ford step 657 current loss 0.240464, current_train_items 21056.
I0304 19:28:14.385406 22502662377600 run.py:483] Algo bellman_ford step 658 current loss 0.227007, current_train_items 21088.
I0304 19:28:14.416710 22502662377600 run.py:483] Algo bellman_ford step 659 current loss 0.286407, current_train_items 21120.
I0304 19:28:14.435854 22502662377600 run.py:483] Algo bellman_ford step 660 current loss 0.053915, current_train_items 21152.
I0304 19:28:14.451951 22502662377600 run.py:483] Algo bellman_ford step 661 current loss 0.107738, current_train_items 21184.
I0304 19:28:14.474958 22502662377600 run.py:483] Algo bellman_ford step 662 current loss 0.120469, current_train_items 21216.
I0304 19:28:14.504298 22502662377600 run.py:483] Algo bellman_ford step 663 current loss 0.193042, current_train_items 21248.
I0304 19:28:14.538586 22502662377600 run.py:483] Algo bellman_ford step 664 current loss 0.314435, current_train_items 21280.
I0304 19:28:14.557288 22502662377600 run.py:483] Algo bellman_ford step 665 current loss 0.058344, current_train_items 21312.
I0304 19:28:14.573369 22502662377600 run.py:483] Algo bellman_ford step 666 current loss 0.130780, current_train_items 21344.
I0304 19:28:14.597901 22502662377600 run.py:483] Algo bellman_ford step 667 current loss 0.244109, current_train_items 21376.
I0304 19:28:14.627311 22502662377600 run.py:483] Algo bellman_ford step 668 current loss 0.247484, current_train_items 21408.
I0304 19:28:14.658681 22502662377600 run.py:483] Algo bellman_ford step 669 current loss 0.257070, current_train_items 21440.
I0304 19:28:14.677499 22502662377600 run.py:483] Algo bellman_ford step 670 current loss 0.037921, current_train_items 21472.
I0304 19:28:14.693425 22502662377600 run.py:483] Algo bellman_ford step 671 current loss 0.134499, current_train_items 21504.
I0304 19:28:14.717370 22502662377600 run.py:483] Algo bellman_ford step 672 current loss 0.248864, current_train_items 21536.
I0304 19:28:14.746555 22502662377600 run.py:483] Algo bellman_ford step 673 current loss 0.207048, current_train_items 21568.
I0304 19:28:14.779544 22502662377600 run.py:483] Algo bellman_ford step 674 current loss 0.286146, current_train_items 21600.
I0304 19:28:14.798770 22502662377600 run.py:483] Algo bellman_ford step 675 current loss 0.047998, current_train_items 21632.
I0304 19:28:14.814404 22502662377600 run.py:483] Algo bellman_ford step 676 current loss 0.090715, current_train_items 21664.
I0304 19:28:14.837577 22502662377600 run.py:483] Algo bellman_ford step 677 current loss 0.134879, current_train_items 21696.
I0304 19:28:14.868102 22502662377600 run.py:483] Algo bellman_ford step 678 current loss 0.274939, current_train_items 21728.
I0304 19:28:14.900556 22502662377600 run.py:483] Algo bellman_ford step 679 current loss 0.372047, current_train_items 21760.
I0304 19:28:14.919471 22502662377600 run.py:483] Algo bellman_ford step 680 current loss 0.082838, current_train_items 21792.
I0304 19:28:14.935469 22502662377600 run.py:483] Algo bellman_ford step 681 current loss 0.081830, current_train_items 21824.
I0304 19:28:14.959004 22502662377600 run.py:483] Algo bellman_ford step 682 current loss 0.195271, current_train_items 21856.
I0304 19:28:14.988027 22502662377600 run.py:483] Algo bellman_ford step 683 current loss 0.191773, current_train_items 21888.
I0304 19:28:15.021213 22502662377600 run.py:483] Algo bellman_ford step 684 current loss 0.280443, current_train_items 21920.
I0304 19:28:15.040192 22502662377600 run.py:483] Algo bellman_ford step 685 current loss 0.058478, current_train_items 21952.
I0304 19:28:15.056137 22502662377600 run.py:483] Algo bellman_ford step 686 current loss 0.099583, current_train_items 21984.
I0304 19:28:15.079717 22502662377600 run.py:483] Algo bellman_ford step 687 current loss 0.136037, current_train_items 22016.
I0304 19:28:15.107873 22502662377600 run.py:483] Algo bellman_ford step 688 current loss 0.137641, current_train_items 22048.
I0304 19:28:15.141754 22502662377600 run.py:483] Algo bellman_ford step 689 current loss 0.303998, current_train_items 22080.
I0304 19:28:15.160253 22502662377600 run.py:483] Algo bellman_ford step 690 current loss 0.050459, current_train_items 22112.
I0304 19:28:15.176841 22502662377600 run.py:483] Algo bellman_ford step 691 current loss 0.171181, current_train_items 22144.
I0304 19:28:15.201203 22502662377600 run.py:483] Algo bellman_ford step 692 current loss 0.175304, current_train_items 22176.
I0304 19:28:15.230277 22502662377600 run.py:483] Algo bellman_ford step 693 current loss 0.286446, current_train_items 22208.
I0304 19:28:15.263694 22502662377600 run.py:483] Algo bellman_ford step 694 current loss 0.356297, current_train_items 22240.
I0304 19:28:15.282294 22502662377600 run.py:483] Algo bellman_ford step 695 current loss 0.043820, current_train_items 22272.
I0304 19:28:15.298114 22502662377600 run.py:483] Algo bellman_ford step 696 current loss 0.073166, current_train_items 22304.
I0304 19:28:15.320331 22502662377600 run.py:483] Algo bellman_ford step 697 current loss 0.217367, current_train_items 22336.
I0304 19:28:15.350228 22502662377600 run.py:483] Algo bellman_ford step 698 current loss 0.264943, current_train_items 22368.
I0304 19:28:15.383913 22502662377600 run.py:483] Algo bellman_ford step 699 current loss 0.302212, current_train_items 22400.
I0304 19:28:15.402774 22502662377600 run.py:483] Algo bellman_ford step 700 current loss 0.026659, current_train_items 22432.
I0304 19:28:15.410860 22502662377600 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.947265625, 'score': 0.947265625, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0304 19:28:15.410964 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.947, current avg val score is 0.947, val scores are: bellman_ford: 0.947
I0304 19:28:15.427403 22502662377600 run.py:483] Algo bellman_ford step 701 current loss 0.074525, current_train_items 22464.
I0304 19:28:15.451250 22502662377600 run.py:483] Algo bellman_ford step 702 current loss 0.222634, current_train_items 22496.
I0304 19:28:15.479866 22502662377600 run.py:483] Algo bellman_ford step 703 current loss 0.186259, current_train_items 22528.
I0304 19:28:15.511081 22502662377600 run.py:483] Algo bellman_ford step 704 current loss 0.289320, current_train_items 22560.
I0304 19:28:15.529963 22502662377600 run.py:483] Algo bellman_ford step 705 current loss 0.052009, current_train_items 22592.
I0304 19:28:15.545766 22502662377600 run.py:483] Algo bellman_ford step 706 current loss 0.072590, current_train_items 22624.
I0304 19:28:15.569416 22502662377600 run.py:483] Algo bellman_ford step 707 current loss 0.206577, current_train_items 22656.
I0304 19:28:15.599280 22502662377600 run.py:483] Algo bellman_ford step 708 current loss 0.202532, current_train_items 22688.
I0304 19:28:15.632076 22502662377600 run.py:483] Algo bellman_ford step 709 current loss 0.284391, current_train_items 22720.
I0304 19:28:15.650818 22502662377600 run.py:483] Algo bellman_ford step 710 current loss 0.041472, current_train_items 22752.
I0304 19:28:15.666963 22502662377600 run.py:483] Algo bellman_ford step 711 current loss 0.122536, current_train_items 22784.
I0304 19:28:15.691224 22502662377600 run.py:483] Algo bellman_ford step 712 current loss 0.350333, current_train_items 22816.
I0304 19:28:15.721408 22502662377600 run.py:483] Algo bellman_ford step 713 current loss 0.397795, current_train_items 22848.
I0304 19:28:15.751673 22502662377600 run.py:483] Algo bellman_ford step 714 current loss 0.283728, current_train_items 22880.
I0304 19:28:15.770239 22502662377600 run.py:483] Algo bellman_ford step 715 current loss 0.034046, current_train_items 22912.
I0304 19:28:15.786659 22502662377600 run.py:483] Algo bellman_ford step 716 current loss 0.180346, current_train_items 22944.
I0304 19:28:15.811758 22502662377600 run.py:483] Algo bellman_ford step 717 current loss 0.289361, current_train_items 22976.
I0304 19:28:15.840634 22502662377600 run.py:483] Algo bellman_ford step 718 current loss 0.158669, current_train_items 23008.
I0304 19:28:15.872591 22502662377600 run.py:483] Algo bellman_ford step 719 current loss 0.340186, current_train_items 23040.
I0304 19:28:15.891289 22502662377600 run.py:483] Algo bellman_ford step 720 current loss 0.068208, current_train_items 23072.
I0304 19:28:15.907821 22502662377600 run.py:483] Algo bellman_ford step 721 current loss 0.204890, current_train_items 23104.
I0304 19:28:15.933570 22502662377600 run.py:483] Algo bellman_ford step 722 current loss 0.136128, current_train_items 23136.
I0304 19:28:15.963015 22502662377600 run.py:483] Algo bellman_ford step 723 current loss 0.208708, current_train_items 23168.
I0304 19:28:15.994573 22502662377600 run.py:483] Algo bellman_ford step 724 current loss 0.326561, current_train_items 23200.
I0304 19:28:16.014014 22502662377600 run.py:483] Algo bellman_ford step 725 current loss 0.096968, current_train_items 23232.
I0304 19:28:16.030920 22502662377600 run.py:483] Algo bellman_ford step 726 current loss 0.123956, current_train_items 23264.
I0304 19:28:16.055029 22502662377600 run.py:483] Algo bellman_ford step 727 current loss 0.276063, current_train_items 23296.
I0304 19:28:16.084358 22502662377600 run.py:483] Algo bellman_ford step 728 current loss 0.279576, current_train_items 23328.
I0304 19:28:16.116315 22502662377600 run.py:483] Algo bellman_ford step 729 current loss 0.391841, current_train_items 23360.
I0304 19:28:16.134953 22502662377600 run.py:483] Algo bellman_ford step 730 current loss 0.026106, current_train_items 23392.
I0304 19:28:16.151110 22502662377600 run.py:483] Algo bellman_ford step 731 current loss 0.159963, current_train_items 23424.
I0304 19:28:16.174399 22502662377600 run.py:483] Algo bellman_ford step 732 current loss 0.201913, current_train_items 23456.
I0304 19:28:16.204157 22502662377600 run.py:483] Algo bellman_ford step 733 current loss 0.204792, current_train_items 23488.
I0304 19:28:16.237053 22502662377600 run.py:483] Algo bellman_ford step 734 current loss 0.271873, current_train_items 23520.
I0304 19:28:16.255925 22502662377600 run.py:483] Algo bellman_ford step 735 current loss 0.050799, current_train_items 23552.
I0304 19:28:16.272390 22502662377600 run.py:483] Algo bellman_ford step 736 current loss 0.083876, current_train_items 23584.
I0304 19:28:16.295525 22502662377600 run.py:483] Algo bellman_ford step 737 current loss 0.188941, current_train_items 23616.
I0304 19:28:16.325003 22502662377600 run.py:483] Algo bellman_ford step 738 current loss 0.281744, current_train_items 23648.
I0304 19:28:16.355013 22502662377600 run.py:483] Algo bellman_ford step 739 current loss 0.221754, current_train_items 23680.
I0304 19:28:16.373544 22502662377600 run.py:483] Algo bellman_ford step 740 current loss 0.037430, current_train_items 23712.
I0304 19:28:16.389753 22502662377600 run.py:483] Algo bellman_ford step 741 current loss 0.083300, current_train_items 23744.
I0304 19:28:16.413106 22502662377600 run.py:483] Algo bellman_ford step 742 current loss 0.235721, current_train_items 23776.
I0304 19:28:16.442025 22502662377600 run.py:483] Algo bellman_ford step 743 current loss 0.218170, current_train_items 23808.
I0304 19:28:16.476084 22502662377600 run.py:483] Algo bellman_ford step 744 current loss 0.296921, current_train_items 23840.
I0304 19:28:16.494665 22502662377600 run.py:483] Algo bellman_ford step 745 current loss 0.031512, current_train_items 23872.
I0304 19:28:16.510733 22502662377600 run.py:483] Algo bellman_ford step 746 current loss 0.193509, current_train_items 23904.
I0304 19:28:16.533927 22502662377600 run.py:483] Algo bellman_ford step 747 current loss 0.304480, current_train_items 23936.
I0304 19:28:16.564145 22502662377600 run.py:483] Algo bellman_ford step 748 current loss 0.475386, current_train_items 23968.
I0304 19:28:16.595918 22502662377600 run.py:483] Algo bellman_ford step 749 current loss 0.369675, current_train_items 24000.
I0304 19:28:16.614544 22502662377600 run.py:483] Algo bellman_ford step 750 current loss 0.027541, current_train_items 24032.
I0304 19:28:16.622743 22502662377600 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.94140625, 'score': 0.94140625, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0304 19:28:16.622852 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.947, current avg val score is 0.941, val scores are: bellman_ford: 0.941
I0304 19:28:16.639845 22502662377600 run.py:483] Algo bellman_ford step 751 current loss 0.102022, current_train_items 24064.
I0304 19:28:16.663436 22502662377600 run.py:483] Algo bellman_ford step 752 current loss 0.205844, current_train_items 24096.
I0304 19:28:16.692940 22502662377600 run.py:483] Algo bellman_ford step 753 current loss 0.285927, current_train_items 24128.
I0304 19:28:16.726068 22502662377600 run.py:483] Algo bellman_ford step 754 current loss 0.346445, current_train_items 24160.
I0304 19:28:16.745248 22502662377600 run.py:483] Algo bellman_ford step 755 current loss 0.099270, current_train_items 24192.
I0304 19:28:16.760738 22502662377600 run.py:483] Algo bellman_ford step 756 current loss 0.119049, current_train_items 24224.
I0304 19:28:16.785559 22502662377600 run.py:483] Algo bellman_ford step 757 current loss 0.233834, current_train_items 24256.
I0304 19:28:16.814672 22502662377600 run.py:483] Algo bellman_ford step 758 current loss 0.286450, current_train_items 24288.
I0304 19:28:16.846267 22502662377600 run.py:483] Algo bellman_ford step 759 current loss 0.311593, current_train_items 24320.
I0304 19:28:16.865344 22502662377600 run.py:483] Algo bellman_ford step 760 current loss 0.026115, current_train_items 24352.
I0304 19:28:16.881555 22502662377600 run.py:483] Algo bellman_ford step 761 current loss 0.128480, current_train_items 24384.
I0304 19:28:16.904970 22502662377600 run.py:483] Algo bellman_ford step 762 current loss 0.236771, current_train_items 24416.
I0304 19:28:16.934012 22502662377600 run.py:483] Algo bellman_ford step 763 current loss 0.200706, current_train_items 24448.
I0304 19:28:16.966427 22502662377600 run.py:483] Algo bellman_ford step 764 current loss 0.327902, current_train_items 24480.
I0304 19:28:16.985423 22502662377600 run.py:483] Algo bellman_ford step 765 current loss 0.044518, current_train_items 24512.
I0304 19:28:17.002277 22502662377600 run.py:483] Algo bellman_ford step 766 current loss 0.136887, current_train_items 24544.
I0304 19:28:17.026019 22502662377600 run.py:483] Algo bellman_ford step 767 current loss 0.262115, current_train_items 24576.
I0304 19:28:17.055468 22502662377600 run.py:483] Algo bellman_ford step 768 current loss 0.365739, current_train_items 24608.
I0304 19:28:17.088645 22502662377600 run.py:483] Algo bellman_ford step 769 current loss 0.384849, current_train_items 24640.
I0304 19:28:17.108029 22502662377600 run.py:483] Algo bellman_ford step 770 current loss 0.034184, current_train_items 24672.
I0304 19:28:17.124365 22502662377600 run.py:483] Algo bellman_ford step 771 current loss 0.139228, current_train_items 24704.
I0304 19:28:17.147331 22502662377600 run.py:483] Algo bellman_ford step 772 current loss 0.261361, current_train_items 24736.
I0304 19:28:17.176616 22502662377600 run.py:483] Algo bellman_ford step 773 current loss 0.135471, current_train_items 24768.
I0304 19:28:17.209336 22502662377600 run.py:483] Algo bellman_ford step 774 current loss 0.315141, current_train_items 24800.
I0304 19:28:17.228493 22502662377600 run.py:483] Algo bellman_ford step 775 current loss 0.034668, current_train_items 24832.
I0304 19:28:17.245239 22502662377600 run.py:483] Algo bellman_ford step 776 current loss 0.067158, current_train_items 24864.
I0304 19:28:17.269102 22502662377600 run.py:483] Algo bellman_ford step 777 current loss 0.112451, current_train_items 24896.
I0304 19:28:17.297846 22502662377600 run.py:483] Algo bellman_ford step 778 current loss 0.178319, current_train_items 24928.
I0304 19:28:17.329617 22502662377600 run.py:483] Algo bellman_ford step 779 current loss 0.256286, current_train_items 24960.
I0304 19:28:17.348273 22502662377600 run.py:483] Algo bellman_ford step 780 current loss 0.066286, current_train_items 24992.
I0304 19:28:17.364634 22502662377600 run.py:483] Algo bellman_ford step 781 current loss 0.120986, current_train_items 25024.
I0304 19:28:17.387940 22502662377600 run.py:483] Algo bellman_ford step 782 current loss 0.113607, current_train_items 25056.
I0304 19:28:17.417308 22502662377600 run.py:483] Algo bellman_ford step 783 current loss 0.246346, current_train_items 25088.
I0304 19:28:17.450372 22502662377600 run.py:483] Algo bellman_ford step 784 current loss 0.356316, current_train_items 25120.
I0304 19:28:17.469433 22502662377600 run.py:483] Algo bellman_ford step 785 current loss 0.029609, current_train_items 25152.
I0304 19:28:17.485883 22502662377600 run.py:483] Algo bellman_ford step 786 current loss 0.103855, current_train_items 25184.
I0304 19:28:17.509006 22502662377600 run.py:483] Algo bellman_ford step 787 current loss 0.162159, current_train_items 25216.
I0304 19:28:17.538988 22502662377600 run.py:483] Algo bellman_ford step 788 current loss 0.225545, current_train_items 25248.
I0304 19:28:17.568948 22502662377600 run.py:483] Algo bellman_ford step 789 current loss 0.330900, current_train_items 25280.
I0304 19:28:17.588117 22502662377600 run.py:483] Algo bellman_ford step 790 current loss 0.041532, current_train_items 25312.
I0304 19:28:17.604680 22502662377600 run.py:483] Algo bellman_ford step 791 current loss 0.181415, current_train_items 25344.
I0304 19:28:17.626966 22502662377600 run.py:483] Algo bellman_ford step 792 current loss 0.138626, current_train_items 25376.
I0304 19:28:17.656344 22502662377600 run.py:483] Algo bellman_ford step 793 current loss 0.205208, current_train_items 25408.
I0304 19:28:17.687003 22502662377600 run.py:483] Algo bellman_ford step 794 current loss 0.252615, current_train_items 25440.
I0304 19:28:17.705677 22502662377600 run.py:483] Algo bellman_ford step 795 current loss 0.035648, current_train_items 25472.
I0304 19:28:17.721501 22502662377600 run.py:483] Algo bellman_ford step 796 current loss 0.066361, current_train_items 25504.
I0304 19:28:17.745826 22502662377600 run.py:483] Algo bellman_ford step 797 current loss 0.190641, current_train_items 25536.
I0304 19:28:17.774484 22502662377600 run.py:483] Algo bellman_ford step 798 current loss 0.157974, current_train_items 25568.
I0304 19:28:17.805722 22502662377600 run.py:483] Algo bellman_ford step 799 current loss 0.174705, current_train_items 25600.
I0304 19:28:17.824406 22502662377600 run.py:483] Algo bellman_ford step 800 current loss 0.027351, current_train_items 25632.
I0304 19:28:17.832260 22502662377600 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0304 19:28:17.832364 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.947, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:28:17.861998 22502662377600 run.py:483] Algo bellman_ford step 801 current loss 0.077749, current_train_items 25664.
I0304 19:28:17.886804 22502662377600 run.py:483] Algo bellman_ford step 802 current loss 0.215017, current_train_items 25696.
I0304 19:28:17.917108 22502662377600 run.py:483] Algo bellman_ford step 803 current loss 0.198469, current_train_items 25728.
I0304 19:28:17.949320 22502662377600 run.py:483] Algo bellman_ford step 804 current loss 0.235651, current_train_items 25760.
I0304 19:28:17.968991 22502662377600 run.py:483] Algo bellman_ford step 805 current loss 0.051296, current_train_items 25792.
I0304 19:28:17.984812 22502662377600 run.py:483] Algo bellman_ford step 806 current loss 0.094556, current_train_items 25824.
I0304 19:28:18.009417 22502662377600 run.py:483] Algo bellman_ford step 807 current loss 0.208831, current_train_items 25856.
I0304 19:28:18.038971 22502662377600 run.py:483] Algo bellman_ford step 808 current loss 0.189130, current_train_items 25888.
I0304 19:28:18.070009 22502662377600 run.py:483] Algo bellman_ford step 809 current loss 0.206483, current_train_items 25920.
I0304 19:28:18.088888 22502662377600 run.py:483] Algo bellman_ford step 810 current loss 0.046128, current_train_items 25952.
I0304 19:28:18.105619 22502662377600 run.py:483] Algo bellman_ford step 811 current loss 0.132886, current_train_items 25984.
I0304 19:28:18.128715 22502662377600 run.py:483] Algo bellman_ford step 812 current loss 0.133523, current_train_items 26016.
I0304 19:28:18.157808 22502662377600 run.py:483] Algo bellman_ford step 813 current loss 0.168044, current_train_items 26048.
I0304 19:28:18.189840 22502662377600 run.py:483] Algo bellman_ford step 814 current loss 0.215778, current_train_items 26080.
I0304 19:28:18.208838 22502662377600 run.py:483] Algo bellman_ford step 815 current loss 0.056037, current_train_items 26112.
I0304 19:28:18.225234 22502662377600 run.py:483] Algo bellman_ford step 816 current loss 0.086942, current_train_items 26144.
I0304 19:28:18.249230 22502662377600 run.py:483] Algo bellman_ford step 817 current loss 0.137493, current_train_items 26176.
I0304 19:28:18.279859 22502662377600 run.py:483] Algo bellman_ford step 818 current loss 0.220749, current_train_items 26208.
I0304 19:28:18.313801 22502662377600 run.py:483] Algo bellman_ford step 819 current loss 0.221972, current_train_items 26240.
I0304 19:28:18.332516 22502662377600 run.py:483] Algo bellman_ford step 820 current loss 0.040471, current_train_items 26272.
I0304 19:28:18.348432 22502662377600 run.py:483] Algo bellman_ford step 821 current loss 0.064083, current_train_items 26304.
I0304 19:28:18.371924 22502662377600 run.py:483] Algo bellman_ford step 822 current loss 0.130792, current_train_items 26336.
I0304 19:28:18.401799 22502662377600 run.py:483] Algo bellman_ford step 823 current loss 0.292630, current_train_items 26368.
I0304 19:28:18.432721 22502662377600 run.py:483] Algo bellman_ford step 824 current loss 0.336827, current_train_items 26400.
I0304 19:28:18.451133 22502662377600 run.py:483] Algo bellman_ford step 825 current loss 0.056493, current_train_items 26432.
I0304 19:28:18.467665 22502662377600 run.py:483] Algo bellman_ford step 826 current loss 0.254722, current_train_items 26464.
I0304 19:28:18.492256 22502662377600 run.py:483] Algo bellman_ford step 827 current loss 0.399667, current_train_items 26496.
I0304 19:28:18.521993 22502662377600 run.py:483] Algo bellman_ford step 828 current loss 0.399371, current_train_items 26528.
I0304 19:28:18.553586 22502662377600 run.py:483] Algo bellman_ford step 829 current loss 0.218790, current_train_items 26560.
I0304 19:28:18.572061 22502662377600 run.py:483] Algo bellman_ford step 830 current loss 0.018025, current_train_items 26592.
I0304 19:28:18.588052 22502662377600 run.py:483] Algo bellman_ford step 831 current loss 0.116875, current_train_items 26624.
I0304 19:28:18.612037 22502662377600 run.py:483] Algo bellman_ford step 832 current loss 0.415346, current_train_items 26656.
I0304 19:28:18.641844 22502662377600 run.py:483] Algo bellman_ford step 833 current loss 0.375737, current_train_items 26688.
I0304 19:28:18.672762 22502662377600 run.py:483] Algo bellman_ford step 834 current loss 0.292054, current_train_items 26720.
I0304 19:28:18.691662 22502662377600 run.py:483] Algo bellman_ford step 835 current loss 0.058519, current_train_items 26752.
I0304 19:28:18.707663 22502662377600 run.py:483] Algo bellman_ford step 836 current loss 0.228212, current_train_items 26784.
I0304 19:28:18.731837 22502662377600 run.py:483] Algo bellman_ford step 837 current loss 0.484773, current_train_items 26816.
I0304 19:28:18.761483 22502662377600 run.py:483] Algo bellman_ford step 838 current loss 0.332003, current_train_items 26848.
I0304 19:28:18.794320 22502662377600 run.py:483] Algo bellman_ford step 839 current loss 0.411929, current_train_items 26880.
I0304 19:28:18.812725 22502662377600 run.py:483] Algo bellman_ford step 840 current loss 0.102770, current_train_items 26912.
I0304 19:28:18.828902 22502662377600 run.py:483] Algo bellman_ford step 841 current loss 0.153538, current_train_items 26944.
I0304 19:28:18.852782 22502662377600 run.py:483] Algo bellman_ford step 842 current loss 0.270235, current_train_items 26976.
I0304 19:28:18.881278 22502662377600 run.py:483] Algo bellman_ford step 843 current loss 0.374828, current_train_items 27008.
I0304 19:28:18.912327 22502662377600 run.py:483] Algo bellman_ford step 844 current loss 0.233868, current_train_items 27040.
I0304 19:28:18.931040 22502662377600 run.py:483] Algo bellman_ford step 845 current loss 0.032663, current_train_items 27072.
I0304 19:28:18.947339 22502662377600 run.py:483] Algo bellman_ford step 846 current loss 0.110033, current_train_items 27104.
I0304 19:28:18.972187 22502662377600 run.py:483] Algo bellman_ford step 847 current loss 0.420500, current_train_items 27136.
I0304 19:28:19.001266 22502662377600 run.py:483] Algo bellman_ford step 848 current loss 0.282036, current_train_items 27168.
I0304 19:28:19.033943 22502662377600 run.py:483] Algo bellman_ford step 849 current loss 0.370935, current_train_items 27200.
I0304 19:28:19.052649 22502662377600 run.py:483] Algo bellman_ford step 850 current loss 0.039905, current_train_items 27232.
I0304 19:28:19.060761 22502662377600 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0304 19:28:19.060863 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0304 19:28:19.077255 22502662377600 run.py:483] Algo bellman_ford step 851 current loss 0.068296, current_train_items 27264.
I0304 19:28:19.101181 22502662377600 run.py:483] Algo bellman_ford step 852 current loss 0.324592, current_train_items 27296.
I0304 19:28:19.132277 22502662377600 run.py:483] Algo bellman_ford step 853 current loss 0.412114, current_train_items 27328.
I0304 19:28:19.164121 22502662377600 run.py:483] Algo bellman_ford step 854 current loss 0.363883, current_train_items 27360.
I0304 19:28:19.182992 22502662377600 run.py:483] Algo bellman_ford step 855 current loss 0.056151, current_train_items 27392.
I0304 19:28:19.199032 22502662377600 run.py:483] Algo bellman_ford step 856 current loss 0.077106, current_train_items 27424.
I0304 19:28:19.221816 22502662377600 run.py:483] Algo bellman_ford step 857 current loss 0.249998, current_train_items 27456.
I0304 19:28:19.252452 22502662377600 run.py:483] Algo bellman_ford step 858 current loss 0.292103, current_train_items 27488.
I0304 19:28:19.283868 22502662377600 run.py:483] Algo bellman_ford step 859 current loss 0.249353, current_train_items 27520.
I0304 19:28:19.303050 22502662377600 run.py:483] Algo bellman_ford step 860 current loss 0.035541, current_train_items 27552.
I0304 19:28:19.319530 22502662377600 run.py:483] Algo bellman_ford step 861 current loss 0.223169, current_train_items 27584.
I0304 19:28:19.342827 22502662377600 run.py:483] Algo bellman_ford step 862 current loss 0.382802, current_train_items 27616.
I0304 19:28:19.372390 22502662377600 run.py:483] Algo bellman_ford step 863 current loss 0.506880, current_train_items 27648.
I0304 19:28:19.403718 22502662377600 run.py:483] Algo bellman_ford step 864 current loss 0.421046, current_train_items 27680.
I0304 19:28:19.422069 22502662377600 run.py:483] Algo bellman_ford step 865 current loss 0.036263, current_train_items 27712.
I0304 19:28:19.437865 22502662377600 run.py:483] Algo bellman_ford step 866 current loss 0.159054, current_train_items 27744.
I0304 19:28:19.462641 22502662377600 run.py:483] Algo bellman_ford step 867 current loss 0.605606, current_train_items 27776.
I0304 19:28:19.491925 22502662377600 run.py:483] Algo bellman_ford step 868 current loss 0.431902, current_train_items 27808.
I0304 19:28:19.524960 22502662377600 run.py:483] Algo bellman_ford step 869 current loss 0.467454, current_train_items 27840.
I0304 19:28:19.543857 22502662377600 run.py:483] Algo bellman_ford step 870 current loss 0.056897, current_train_items 27872.
I0304 19:28:19.560301 22502662377600 run.py:483] Algo bellman_ford step 871 current loss 0.096201, current_train_items 27904.
I0304 19:28:19.583009 22502662377600 run.py:483] Algo bellman_ford step 872 current loss 0.150759, current_train_items 27936.
I0304 19:28:19.613245 22502662377600 run.py:483] Algo bellman_ford step 873 current loss 0.308801, current_train_items 27968.
I0304 19:28:19.644372 22502662377600 run.py:483] Algo bellman_ford step 874 current loss 0.321170, current_train_items 28000.
I0304 19:28:19.663638 22502662377600 run.py:483] Algo bellman_ford step 875 current loss 0.025904, current_train_items 28032.
I0304 19:28:19.680111 22502662377600 run.py:483] Algo bellman_ford step 876 current loss 0.111102, current_train_items 28064.
I0304 19:28:19.705308 22502662377600 run.py:483] Algo bellman_ford step 877 current loss 0.312256, current_train_items 28096.
I0304 19:28:19.734231 22502662377600 run.py:483] Algo bellman_ford step 878 current loss 0.198091, current_train_items 28128.
I0304 19:28:19.767366 22502662377600 run.py:483] Algo bellman_ford step 879 current loss 0.251287, current_train_items 28160.
I0304 19:28:19.785938 22502662377600 run.py:483] Algo bellman_ford step 880 current loss 0.022088, current_train_items 28192.
I0304 19:28:19.801772 22502662377600 run.py:483] Algo bellman_ford step 881 current loss 0.046701, current_train_items 28224.
I0304 19:28:19.824633 22502662377600 run.py:483] Algo bellman_ford step 882 current loss 0.131086, current_train_items 28256.
I0304 19:28:19.854611 22502662377600 run.py:483] Algo bellman_ford step 883 current loss 0.188144, current_train_items 28288.
I0304 19:28:19.887489 22502662377600 run.py:483] Algo bellman_ford step 884 current loss 0.256375, current_train_items 28320.
I0304 19:28:19.906597 22502662377600 run.py:483] Algo bellman_ford step 885 current loss 0.038778, current_train_items 28352.
I0304 19:28:19.922673 22502662377600 run.py:483] Algo bellman_ford step 886 current loss 0.156274, current_train_items 28384.
I0304 19:28:19.946255 22502662377600 run.py:483] Algo bellman_ford step 887 current loss 0.181100, current_train_items 28416.
I0304 19:28:19.975253 22502662377600 run.py:483] Algo bellman_ford step 888 current loss 0.162915, current_train_items 28448.
I0304 19:28:20.008551 22502662377600 run.py:483] Algo bellman_ford step 889 current loss 0.240688, current_train_items 28480.
I0304 19:28:20.027808 22502662377600 run.py:483] Algo bellman_ford step 890 current loss 0.040381, current_train_items 28512.
I0304 19:28:20.044123 22502662377600 run.py:483] Algo bellman_ford step 891 current loss 0.083631, current_train_items 28544.
I0304 19:28:20.067914 22502662377600 run.py:483] Algo bellman_ford step 892 current loss 0.208825, current_train_items 28576.
I0304 19:28:20.098680 22502662377600 run.py:483] Algo bellman_ford step 893 current loss 0.217027, current_train_items 28608.
I0304 19:28:20.130580 22502662377600 run.py:483] Algo bellman_ford step 894 current loss 0.211012, current_train_items 28640.
I0304 19:28:20.149314 22502662377600 run.py:483] Algo bellman_ford step 895 current loss 0.060759, current_train_items 28672.
I0304 19:28:20.165313 22502662377600 run.py:483] Algo bellman_ford step 896 current loss 0.064293, current_train_items 28704.
I0304 19:28:20.189431 22502662377600 run.py:483] Algo bellman_ford step 897 current loss 0.213462, current_train_items 28736.
I0304 19:28:20.219664 22502662377600 run.py:483] Algo bellman_ford step 898 current loss 0.171986, current_train_items 28768.
I0304 19:28:20.253731 22502662377600 run.py:483] Algo bellman_ford step 899 current loss 0.274461, current_train_items 28800.
I0304 19:28:20.272864 22502662377600 run.py:483] Algo bellman_ford step 900 current loss 0.037337, current_train_items 28832.
I0304 19:28:20.280469 22502662377600 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0304 19:28:20.280572 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:28:20.297760 22502662377600 run.py:483] Algo bellman_ford step 901 current loss 0.116993, current_train_items 28864.
I0304 19:28:20.322516 22502662377600 run.py:483] Algo bellman_ford step 902 current loss 0.203035, current_train_items 28896.
I0304 19:28:20.353568 22502662377600 run.py:483] Algo bellman_ford step 903 current loss 0.175632, current_train_items 28928.
I0304 19:28:20.386156 22502662377600 run.py:483] Algo bellman_ford step 904 current loss 0.350987, current_train_items 28960.
I0304 19:28:20.405139 22502662377600 run.py:483] Algo bellman_ford step 905 current loss 0.018253, current_train_items 28992.
I0304 19:28:20.421419 22502662377600 run.py:483] Algo bellman_ford step 906 current loss 0.089216, current_train_items 29024.
I0304 19:28:20.445607 22502662377600 run.py:483] Algo bellman_ford step 907 current loss 0.133851, current_train_items 29056.
I0304 19:28:20.476377 22502662377600 run.py:483] Algo bellman_ford step 908 current loss 0.160669, current_train_items 29088.
I0304 19:28:20.508971 22502662377600 run.py:483] Algo bellman_ford step 909 current loss 0.250490, current_train_items 29120.
I0304 19:28:20.527493 22502662377600 run.py:483] Algo bellman_ford step 910 current loss 0.040429, current_train_items 29152.
I0304 19:28:20.544538 22502662377600 run.py:483] Algo bellman_ford step 911 current loss 0.147732, current_train_items 29184.
I0304 19:28:20.568675 22502662377600 run.py:483] Algo bellman_ford step 912 current loss 0.242764, current_train_items 29216.
I0304 19:28:20.596653 22502662377600 run.py:483] Algo bellman_ford step 913 current loss 0.189444, current_train_items 29248.
I0304 19:28:20.629653 22502662377600 run.py:483] Algo bellman_ford step 914 current loss 0.330558, current_train_items 29280.
I0304 19:28:20.648172 22502662377600 run.py:483] Algo bellman_ford step 915 current loss 0.023389, current_train_items 29312.
I0304 19:28:20.664628 22502662377600 run.py:483] Algo bellman_ford step 916 current loss 0.084064, current_train_items 29344.
I0304 19:28:20.688597 22502662377600 run.py:483] Algo bellman_ford step 917 current loss 0.389441, current_train_items 29376.
I0304 19:28:20.718860 22502662377600 run.py:483] Algo bellman_ford step 918 current loss 0.277264, current_train_items 29408.
I0304 19:28:20.752226 22502662377600 run.py:483] Algo bellman_ford step 919 current loss 0.312621, current_train_items 29440.
I0304 19:28:20.771066 22502662377600 run.py:483] Algo bellman_ford step 920 current loss 0.071223, current_train_items 29472.
I0304 19:28:20.787622 22502662377600 run.py:483] Algo bellman_ford step 921 current loss 0.241421, current_train_items 29504.
I0304 19:28:20.811697 22502662377600 run.py:483] Algo bellman_ford step 922 current loss 0.276107, current_train_items 29536.
I0304 19:28:20.841004 22502662377600 run.py:483] Algo bellman_ford step 923 current loss 0.197297, current_train_items 29568.
I0304 19:28:20.872779 22502662377600 run.py:483] Algo bellman_ford step 924 current loss 0.215842, current_train_items 29600.
I0304 19:28:20.891884 22502662377600 run.py:483] Algo bellman_ford step 925 current loss 0.043874, current_train_items 29632.
I0304 19:28:20.907844 22502662377600 run.py:483] Algo bellman_ford step 926 current loss 0.071457, current_train_items 29664.
I0304 19:28:20.931806 22502662377600 run.py:483] Algo bellman_ford step 927 current loss 0.117284, current_train_items 29696.
I0304 19:28:20.962365 22502662377600 run.py:483] Algo bellman_ford step 928 current loss 0.160514, current_train_items 29728.
I0304 19:28:20.993255 22502662377600 run.py:483] Algo bellman_ford step 929 current loss 0.187057, current_train_items 29760.
I0304 19:28:21.012150 22502662377600 run.py:483] Algo bellman_ford step 930 current loss 0.033087, current_train_items 29792.
I0304 19:28:21.028491 22502662377600 run.py:483] Algo bellman_ford step 931 current loss 0.081614, current_train_items 29824.
I0304 19:28:21.052084 22502662377600 run.py:483] Algo bellman_ford step 932 current loss 0.137664, current_train_items 29856.
I0304 19:28:21.081146 22502662377600 run.py:483] Algo bellman_ford step 933 current loss 0.143067, current_train_items 29888.
I0304 19:28:21.114266 22502662377600 run.py:483] Algo bellman_ford step 934 current loss 0.224332, current_train_items 29920.
I0304 19:28:21.133153 22502662377600 run.py:483] Algo bellman_ford step 935 current loss 0.033734, current_train_items 29952.
I0304 19:28:21.149212 22502662377600 run.py:483] Algo bellman_ford step 936 current loss 0.128835, current_train_items 29984.
I0304 19:28:21.172298 22502662377600 run.py:483] Algo bellman_ford step 937 current loss 0.126909, current_train_items 30016.
I0304 19:28:21.201638 22502662377600 run.py:483] Algo bellman_ford step 938 current loss 0.198375, current_train_items 30048.
I0304 19:28:21.234094 22502662377600 run.py:483] Algo bellman_ford step 939 current loss 0.339105, current_train_items 30080.
I0304 19:28:21.252688 22502662377600 run.py:483] Algo bellman_ford step 940 current loss 0.031374, current_train_items 30112.
I0304 19:28:21.268969 22502662377600 run.py:483] Algo bellman_ford step 941 current loss 0.089873, current_train_items 30144.
I0304 19:28:21.293401 22502662377600 run.py:483] Algo bellman_ford step 942 current loss 0.148696, current_train_items 30176.
I0304 19:28:21.323446 22502662377600 run.py:483] Algo bellman_ford step 943 current loss 0.130096, current_train_items 30208.
I0304 19:28:21.356082 22502662377600 run.py:483] Algo bellman_ford step 944 current loss 0.236740, current_train_items 30240.
I0304 19:28:21.374833 22502662377600 run.py:483] Algo bellman_ford step 945 current loss 0.023297, current_train_items 30272.
I0304 19:28:21.390578 22502662377600 run.py:483] Algo bellman_ford step 946 current loss 0.074230, current_train_items 30304.
I0304 19:28:21.415178 22502662377600 run.py:483] Algo bellman_ford step 947 current loss 0.205896, current_train_items 30336.
I0304 19:28:21.444436 22502662377600 run.py:483] Algo bellman_ford step 948 current loss 0.165631, current_train_items 30368.
I0304 19:28:21.476157 22502662377600 run.py:483] Algo bellman_ford step 949 current loss 0.230103, current_train_items 30400.
I0304 19:28:21.494997 22502662377600 run.py:483] Algo bellman_ford step 950 current loss 0.028343, current_train_items 30432.
I0304 19:28:21.502984 22502662377600 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.94921875, 'score': 0.94921875, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0304 19:28:21.503089 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.949, val scores are: bellman_ford: 0.949
I0304 19:28:21.519839 22502662377600 run.py:483] Algo bellman_ford step 951 current loss 0.119367, current_train_items 30464.
I0304 19:28:21.543745 22502662377600 run.py:483] Algo bellman_ford step 952 current loss 0.134053, current_train_items 30496.
I0304 19:28:21.573865 22502662377600 run.py:483] Algo bellman_ford step 953 current loss 0.242635, current_train_items 30528.
I0304 19:28:21.603926 22502662377600 run.py:483] Algo bellman_ford step 954 current loss 0.186263, current_train_items 30560.
I0304 19:28:21.623020 22502662377600 run.py:483] Algo bellman_ford step 955 current loss 0.029309, current_train_items 30592.
I0304 19:28:21.639001 22502662377600 run.py:483] Algo bellman_ford step 956 current loss 0.050318, current_train_items 30624.
I0304 19:28:21.663052 22502662377600 run.py:483] Algo bellman_ford step 957 current loss 0.137626, current_train_items 30656.
I0304 19:28:21.691407 22502662377600 run.py:483] Algo bellman_ford step 958 current loss 0.187117, current_train_items 30688.
I0304 19:28:21.723822 22502662377600 run.py:483] Algo bellman_ford step 959 current loss 0.214990, current_train_items 30720.
I0304 19:28:21.742809 22502662377600 run.py:483] Algo bellman_ford step 960 current loss 0.047891, current_train_items 30752.
I0304 19:28:21.759867 22502662377600 run.py:483] Algo bellman_ford step 961 current loss 0.219595, current_train_items 30784.
I0304 19:28:21.781893 22502662377600 run.py:483] Algo bellman_ford step 962 current loss 0.163525, current_train_items 30816.
I0304 19:28:21.809797 22502662377600 run.py:483] Algo bellman_ford step 963 current loss 0.148498, current_train_items 30848.
I0304 19:28:21.842336 22502662377600 run.py:483] Algo bellman_ford step 964 current loss 0.294107, current_train_items 30880.
I0304 19:28:21.861201 22502662377600 run.py:483] Algo bellman_ford step 965 current loss 0.144198, current_train_items 30912.
I0304 19:28:21.877236 22502662377600 run.py:483] Algo bellman_ford step 966 current loss 0.088455, current_train_items 30944.
I0304 19:28:21.901548 22502662377600 run.py:483] Algo bellman_ford step 967 current loss 0.170324, current_train_items 30976.
I0304 19:28:21.931889 22502662377600 run.py:483] Algo bellman_ford step 968 current loss 0.146086, current_train_items 31008.
I0304 19:28:21.963721 22502662377600 run.py:483] Algo bellman_ford step 969 current loss 0.308779, current_train_items 31040.
I0304 19:28:21.982318 22502662377600 run.py:483] Algo bellman_ford step 970 current loss 0.028682, current_train_items 31072.
I0304 19:28:21.998712 22502662377600 run.py:483] Algo bellman_ford step 971 current loss 0.126488, current_train_items 31104.
I0304 19:28:22.022839 22502662377600 run.py:483] Algo bellman_ford step 972 current loss 0.205187, current_train_items 31136.
I0304 19:28:22.053329 22502662377600 run.py:483] Algo bellman_ford step 973 current loss 0.156283, current_train_items 31168.
I0304 19:28:22.086992 22502662377600 run.py:483] Algo bellman_ford step 974 current loss 0.339898, current_train_items 31200.
I0304 19:28:22.105977 22502662377600 run.py:483] Algo bellman_ford step 975 current loss 0.073520, current_train_items 31232.
I0304 19:28:22.121979 22502662377600 run.py:483] Algo bellman_ford step 976 current loss 0.049840, current_train_items 31264.
I0304 19:28:22.145492 22502662377600 run.py:483] Algo bellman_ford step 977 current loss 0.106904, current_train_items 31296.
I0304 19:28:22.174727 22502662377600 run.py:483] Algo bellman_ford step 978 current loss 0.168878, current_train_items 31328.
I0304 19:28:22.206219 22502662377600 run.py:483] Algo bellman_ford step 979 current loss 0.320018, current_train_items 31360.
I0304 19:28:22.224877 22502662377600 run.py:483] Algo bellman_ford step 980 current loss 0.050642, current_train_items 31392.
I0304 19:28:22.241137 22502662377600 run.py:483] Algo bellman_ford step 981 current loss 0.139674, current_train_items 31424.
I0304 19:28:22.264645 22502662377600 run.py:483] Algo bellman_ford step 982 current loss 0.167741, current_train_items 31456.
I0304 19:28:22.293378 22502662377600 run.py:483] Algo bellman_ford step 983 current loss 0.205693, current_train_items 31488.
I0304 19:28:22.326101 22502662377600 run.py:483] Algo bellman_ford step 984 current loss 0.342313, current_train_items 31520.
I0304 19:28:22.345144 22502662377600 run.py:483] Algo bellman_ford step 985 current loss 0.055059, current_train_items 31552.
I0304 19:28:22.361731 22502662377600 run.py:483] Algo bellman_ford step 986 current loss 0.101681, current_train_items 31584.
I0304 19:28:22.384904 22502662377600 run.py:483] Algo bellman_ford step 987 current loss 0.106254, current_train_items 31616.
I0304 19:28:22.414505 22502662377600 run.py:483] Algo bellman_ford step 988 current loss 0.219929, current_train_items 31648.
I0304 19:28:22.445609 22502662377600 run.py:483] Algo bellman_ford step 989 current loss 0.219932, current_train_items 31680.
I0304 19:28:22.464254 22502662377600 run.py:483] Algo bellman_ford step 990 current loss 0.028794, current_train_items 31712.
I0304 19:28:22.480243 22502662377600 run.py:483] Algo bellman_ford step 991 current loss 0.045038, current_train_items 31744.
I0304 19:28:22.503781 22502662377600 run.py:483] Algo bellman_ford step 992 current loss 0.162125, current_train_items 31776.
I0304 19:28:22.531980 22502662377600 run.py:483] Algo bellman_ford step 993 current loss 0.201842, current_train_items 31808.
I0304 19:28:22.566415 22502662377600 run.py:483] Algo bellman_ford step 994 current loss 0.334521, current_train_items 31840.
I0304 19:28:22.585012 22502662377600 run.py:483] Algo bellman_ford step 995 current loss 0.033263, current_train_items 31872.
I0304 19:28:22.600969 22502662377600 run.py:483] Algo bellman_ford step 996 current loss 0.046555, current_train_items 31904.
I0304 19:28:22.624475 22502662377600 run.py:483] Algo bellman_ford step 997 current loss 0.162845, current_train_items 31936.
I0304 19:28:22.654553 22502662377600 run.py:483] Algo bellman_ford step 998 current loss 0.135103, current_train_items 31968.
I0304 19:28:22.686113 22502662377600 run.py:483] Algo bellman_ford step 999 current loss 0.212865, current_train_items 32000.
I0304 19:28:22.704842 22502662377600 run.py:483] Algo bellman_ford step 1000 current loss 0.017544, current_train_items 32032.
I0304 19:28:22.712717 22502662377600 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.9619140625, 'score': 0.9619140625, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0304 19:28:22.712821 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.962, val scores are: bellman_ford: 0.962
I0304 19:28:22.729630 22502662377600 run.py:483] Algo bellman_ford step 1001 current loss 0.094347, current_train_items 32064.
I0304 19:28:22.753879 22502662377600 run.py:483] Algo bellman_ford step 1002 current loss 0.160100, current_train_items 32096.
I0304 19:28:22.782396 22502662377600 run.py:483] Algo bellman_ford step 1003 current loss 0.151341, current_train_items 32128.
I0304 19:28:22.817215 22502662377600 run.py:483] Algo bellman_ford step 1004 current loss 0.293894, current_train_items 32160.
I0304 19:28:22.836655 22502662377600 run.py:483] Algo bellman_ford step 1005 current loss 0.051791, current_train_items 32192.
I0304 19:28:22.852348 22502662377600 run.py:483] Algo bellman_ford step 1006 current loss 0.076755, current_train_items 32224.
I0304 19:28:22.877219 22502662377600 run.py:483] Algo bellman_ford step 1007 current loss 0.140082, current_train_items 32256.
I0304 19:28:22.906314 22502662377600 run.py:483] Algo bellman_ford step 1008 current loss 0.170881, current_train_items 32288.
I0304 19:28:22.939552 22502662377600 run.py:483] Algo bellman_ford step 1009 current loss 0.220554, current_train_items 32320.
I0304 19:28:22.958073 22502662377600 run.py:483] Algo bellman_ford step 1010 current loss 0.029796, current_train_items 32352.
I0304 19:28:22.974352 22502662377600 run.py:483] Algo bellman_ford step 1011 current loss 0.068266, current_train_items 32384.
I0304 19:28:22.999231 22502662377600 run.py:483] Algo bellman_ford step 1012 current loss 0.166087, current_train_items 32416.
I0304 19:28:23.029144 22502662377600 run.py:483] Algo bellman_ford step 1013 current loss 0.220014, current_train_items 32448.
I0304 19:28:23.061861 22502662377600 run.py:483] Algo bellman_ford step 1014 current loss 0.173760, current_train_items 32480.
I0304 19:28:23.080850 22502662377600 run.py:483] Algo bellman_ford step 1015 current loss 0.028815, current_train_items 32512.
I0304 19:28:23.097220 22502662377600 run.py:483] Algo bellman_ford step 1016 current loss 0.082166, current_train_items 32544.
I0304 19:28:23.121189 22502662377600 run.py:483] Algo bellman_ford step 1017 current loss 0.168185, current_train_items 32576.
I0304 19:28:23.151140 22502662377600 run.py:483] Algo bellman_ford step 1018 current loss 0.151961, current_train_items 32608.
I0304 19:28:23.185923 22502662377600 run.py:483] Algo bellman_ford step 1019 current loss 0.357120, current_train_items 32640.
I0304 19:28:23.204362 22502662377600 run.py:483] Algo bellman_ford step 1020 current loss 0.025738, current_train_items 32672.
I0304 19:28:23.220635 22502662377600 run.py:483] Algo bellman_ford step 1021 current loss 0.068855, current_train_items 32704.
I0304 19:28:23.243893 22502662377600 run.py:483] Algo bellman_ford step 1022 current loss 0.155045, current_train_items 32736.
I0304 19:28:23.272741 22502662377600 run.py:483] Algo bellman_ford step 1023 current loss 0.214539, current_train_items 32768.
I0304 19:28:23.303273 22502662377600 run.py:483] Algo bellman_ford step 1024 current loss 0.173899, current_train_items 32800.
I0304 19:28:23.322245 22502662377600 run.py:483] Algo bellman_ford step 1025 current loss 0.041211, current_train_items 32832.
I0304 19:28:23.338995 22502662377600 run.py:483] Algo bellman_ford step 1026 current loss 0.175496, current_train_items 32864.
I0304 19:28:23.362825 22502662377600 run.py:483] Algo bellman_ford step 1027 current loss 0.219057, current_train_items 32896.
I0304 19:28:23.393175 22502662377600 run.py:483] Algo bellman_ford step 1028 current loss 0.228496, current_train_items 32928.
I0304 19:28:23.424715 22502662377600 run.py:483] Algo bellman_ford step 1029 current loss 0.262428, current_train_items 32960.
I0304 19:28:23.443502 22502662377600 run.py:483] Algo bellman_ford step 1030 current loss 0.048338, current_train_items 32992.
I0304 19:28:23.459391 22502662377600 run.py:483] Algo bellman_ford step 1031 current loss 0.095100, current_train_items 33024.
I0304 19:28:23.482603 22502662377600 run.py:483] Algo bellman_ford step 1032 current loss 0.226522, current_train_items 33056.
I0304 19:28:23.511585 22502662377600 run.py:483] Algo bellman_ford step 1033 current loss 0.385324, current_train_items 33088.
I0304 19:28:23.544106 22502662377600 run.py:483] Algo bellman_ford step 1034 current loss 0.363659, current_train_items 33120.
I0304 19:28:23.562906 22502662377600 run.py:483] Algo bellman_ford step 1035 current loss 0.089899, current_train_items 33152.
I0304 19:28:23.578963 22502662377600 run.py:483] Algo bellman_ford step 1036 current loss 0.092898, current_train_items 33184.
I0304 19:28:23.603403 22502662377600 run.py:483] Algo bellman_ford step 1037 current loss 0.206240, current_train_items 33216.
I0304 19:28:23.633501 22502662377600 run.py:483] Algo bellman_ford step 1038 current loss 0.298556, current_train_items 33248.
I0304 19:28:23.667315 22502662377600 run.py:483] Algo bellman_ford step 1039 current loss 0.314213, current_train_items 33280.
I0304 19:28:23.686063 22502662377600 run.py:483] Algo bellman_ford step 1040 current loss 0.027388, current_train_items 33312.
I0304 19:28:23.703106 22502662377600 run.py:483] Algo bellman_ford step 1041 current loss 0.196782, current_train_items 33344.
I0304 19:28:23.727235 22502662377600 run.py:483] Algo bellman_ford step 1042 current loss 0.200408, current_train_items 33376.
I0304 19:28:23.756640 22502662377600 run.py:483] Algo bellman_ford step 1043 current loss 0.185301, current_train_items 33408.
I0304 19:28:23.788609 22502662377600 run.py:483] Algo bellman_ford step 1044 current loss 0.176353, current_train_items 33440.
I0304 19:28:23.807445 22502662377600 run.py:483] Algo bellman_ford step 1045 current loss 0.020983, current_train_items 33472.
I0304 19:28:23.823220 22502662377600 run.py:483] Algo bellman_ford step 1046 current loss 0.048428, current_train_items 33504.
I0304 19:28:23.847434 22502662377600 run.py:483] Algo bellman_ford step 1047 current loss 0.179610, current_train_items 33536.
I0304 19:28:23.877946 22502662377600 run.py:483] Algo bellman_ford step 1048 current loss 0.192767, current_train_items 33568.
I0304 19:28:23.910863 22502662377600 run.py:483] Algo bellman_ford step 1049 current loss 0.244810, current_train_items 33600.
I0304 19:28:23.929748 22502662377600 run.py:483] Algo bellman_ford step 1050 current loss 0.091605, current_train_items 33632.
I0304 19:28:23.937893 22502662377600 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.951171875, 'score': 0.951171875, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0304 19:28:23.938000 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.951, val scores are: bellman_ford: 0.951
I0304 19:28:23.954392 22502662377600 run.py:483] Algo bellman_ford step 1051 current loss 0.084442, current_train_items 33664.
I0304 19:28:23.978842 22502662377600 run.py:483] Algo bellman_ford step 1052 current loss 0.152564, current_train_items 33696.
I0304 19:28:24.007752 22502662377600 run.py:483] Algo bellman_ford step 1053 current loss 0.155646, current_train_items 33728.
I0304 19:28:24.040219 22502662377600 run.py:483] Algo bellman_ford step 1054 current loss 0.222275, current_train_items 33760.
I0304 19:28:24.059003 22502662377600 run.py:483] Algo bellman_ford step 1055 current loss 0.038883, current_train_items 33792.
I0304 19:28:24.075310 22502662377600 run.py:483] Algo bellman_ford step 1056 current loss 0.077927, current_train_items 33824.
I0304 19:28:24.099290 22502662377600 run.py:483] Algo bellman_ford step 1057 current loss 0.215732, current_train_items 33856.
I0304 19:28:24.129632 22502662377600 run.py:483] Algo bellman_ford step 1058 current loss 0.188363, current_train_items 33888.
I0304 19:28:24.159361 22502662377600 run.py:483] Algo bellman_ford step 1059 current loss 0.152186, current_train_items 33920.
I0304 19:28:24.178164 22502662377600 run.py:483] Algo bellman_ford step 1060 current loss 0.019451, current_train_items 33952.
I0304 19:28:24.194355 22502662377600 run.py:483] Algo bellman_ford step 1061 current loss 0.052665, current_train_items 33984.
I0304 19:28:24.216328 22502662377600 run.py:483] Algo bellman_ford step 1062 current loss 0.080463, current_train_items 34016.
I0304 19:28:24.245064 22502662377600 run.py:483] Algo bellman_ford step 1063 current loss 0.190366, current_train_items 34048.
I0304 19:28:24.277726 22502662377600 run.py:483] Algo bellman_ford step 1064 current loss 0.228919, current_train_items 34080.
I0304 19:28:24.296041 22502662377600 run.py:483] Algo bellman_ford step 1065 current loss 0.025488, current_train_items 34112.
I0304 19:28:24.312110 22502662377600 run.py:483] Algo bellman_ford step 1066 current loss 0.041305, current_train_items 34144.
I0304 19:28:24.335881 22502662377600 run.py:483] Algo bellman_ford step 1067 current loss 0.137793, current_train_items 34176.
I0304 19:28:24.365684 22502662377600 run.py:483] Algo bellman_ford step 1068 current loss 0.126310, current_train_items 34208.
I0304 19:28:24.399068 22502662377600 run.py:483] Algo bellman_ford step 1069 current loss 0.202946, current_train_items 34240.
I0304 19:28:24.417658 22502662377600 run.py:483] Algo bellman_ford step 1070 current loss 0.014430, current_train_items 34272.
I0304 19:28:24.434244 22502662377600 run.py:483] Algo bellman_ford step 1071 current loss 0.127242, current_train_items 34304.
I0304 19:28:24.457536 22502662377600 run.py:483] Algo bellman_ford step 1072 current loss 0.148360, current_train_items 34336.
I0304 19:28:24.487003 22502662377600 run.py:483] Algo bellman_ford step 1073 current loss 0.192539, current_train_items 34368.
I0304 19:28:24.518019 22502662377600 run.py:483] Algo bellman_ford step 1074 current loss 0.236772, current_train_items 34400.
I0304 19:28:24.536596 22502662377600 run.py:483] Algo bellman_ford step 1075 current loss 0.017208, current_train_items 34432.
I0304 19:28:24.553060 22502662377600 run.py:483] Algo bellman_ford step 1076 current loss 0.126156, current_train_items 34464.
I0304 19:28:24.576796 22502662377600 run.py:483] Algo bellman_ford step 1077 current loss 0.098628, current_train_items 34496.
I0304 19:28:24.606587 22502662377600 run.py:483] Algo bellman_ford step 1078 current loss 0.164736, current_train_items 34528.
I0304 19:28:24.637829 22502662377600 run.py:483] Algo bellman_ford step 1079 current loss 0.150811, current_train_items 34560.
I0304 19:28:24.656713 22502662377600 run.py:483] Algo bellman_ford step 1080 current loss 0.024062, current_train_items 34592.
I0304 19:28:24.673402 22502662377600 run.py:483] Algo bellman_ford step 1081 current loss 0.069763, current_train_items 34624.
I0304 19:28:24.696304 22502662377600 run.py:483] Algo bellman_ford step 1082 current loss 0.114432, current_train_items 34656.
I0304 19:28:24.725148 22502662377600 run.py:483] Algo bellman_ford step 1083 current loss 0.159100, current_train_items 34688.
I0304 19:28:24.757787 22502662377600 run.py:483] Algo bellman_ford step 1084 current loss 0.258444, current_train_items 34720.
I0304 19:28:24.776500 22502662377600 run.py:483] Algo bellman_ford step 1085 current loss 0.022674, current_train_items 34752.
I0304 19:28:24.792911 22502662377600 run.py:483] Algo bellman_ford step 1086 current loss 0.051711, current_train_items 34784.
I0304 19:28:24.816268 22502662377600 run.py:483] Algo bellman_ford step 1087 current loss 0.098669, current_train_items 34816.
I0304 19:28:24.844693 22502662377600 run.py:483] Algo bellman_ford step 1088 current loss 0.087046, current_train_items 34848.
I0304 19:28:24.875043 22502662377600 run.py:483] Algo bellman_ford step 1089 current loss 0.125481, current_train_items 34880.
I0304 19:28:24.893699 22502662377600 run.py:483] Algo bellman_ford step 1090 current loss 0.028602, current_train_items 34912.
I0304 19:28:24.910261 22502662377600 run.py:483] Algo bellman_ford step 1091 current loss 0.075526, current_train_items 34944.
I0304 19:28:24.933071 22502662377600 run.py:483] Algo bellman_ford step 1092 current loss 0.128160, current_train_items 34976.
I0304 19:28:24.963388 22502662377600 run.py:483] Algo bellman_ford step 1093 current loss 0.183764, current_train_items 35008.
I0304 19:28:24.996179 22502662377600 run.py:483] Algo bellman_ford step 1094 current loss 0.180175, current_train_items 35040.
I0304 19:28:25.014448 22502662377600 run.py:483] Algo bellman_ford step 1095 current loss 0.021189, current_train_items 35072.
I0304 19:28:25.030440 22502662377600 run.py:483] Algo bellman_ford step 1096 current loss 0.090987, current_train_items 35104.
I0304 19:28:25.054123 22502662377600 run.py:483] Algo bellman_ford step 1097 current loss 0.185518, current_train_items 35136.
I0304 19:28:25.083747 22502662377600 run.py:483] Algo bellman_ford step 1098 current loss 0.232712, current_train_items 35168.
I0304 19:28:25.115122 22502662377600 run.py:483] Algo bellman_ford step 1099 current loss 0.183095, current_train_items 35200.
I0304 19:28:25.133805 22502662377600 run.py:483] Algo bellman_ford step 1100 current loss 0.026076, current_train_items 35232.
I0304 19:28:25.141580 22502662377600 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0304 19:28:25.141685 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:28:25.158424 22502662377600 run.py:483] Algo bellman_ford step 1101 current loss 0.087287, current_train_items 35264.
I0304 19:28:25.182023 22502662377600 run.py:483] Algo bellman_ford step 1102 current loss 0.136402, current_train_items 35296.
I0304 19:28:25.212874 22502662377600 run.py:483] Algo bellman_ford step 1103 current loss 0.232181, current_train_items 35328.
I0304 19:28:25.247346 22502662377600 run.py:483] Algo bellman_ford step 1104 current loss 0.333083, current_train_items 35360.
I0304 19:28:25.266171 22502662377600 run.py:483] Algo bellman_ford step 1105 current loss 0.017752, current_train_items 35392.
I0304 19:28:25.282022 22502662377600 run.py:483] Algo bellman_ford step 1106 current loss 0.047721, current_train_items 35424.
I0304 19:28:25.305896 22502662377600 run.py:483] Algo bellman_ford step 1107 current loss 0.182844, current_train_items 35456.
I0304 19:28:25.334791 22502662377600 run.py:483] Algo bellman_ford step 1108 current loss 0.169842, current_train_items 35488.
I0304 19:28:25.365083 22502662377600 run.py:483] Algo bellman_ford step 1109 current loss 0.141527, current_train_items 35520.
I0304 19:28:25.383737 22502662377600 run.py:483] Algo bellman_ford step 1110 current loss 0.046282, current_train_items 35552.
I0304 19:28:25.399976 22502662377600 run.py:483] Algo bellman_ford step 1111 current loss 0.073625, current_train_items 35584.
I0304 19:28:25.423744 22502662377600 run.py:483] Algo bellman_ford step 1112 current loss 0.171166, current_train_items 35616.
I0304 19:28:25.451639 22502662377600 run.py:483] Algo bellman_ford step 1113 current loss 0.145474, current_train_items 35648.
I0304 19:28:25.485121 22502662377600 run.py:483] Algo bellman_ford step 1114 current loss 0.185664, current_train_items 35680.
I0304 19:28:25.503612 22502662377600 run.py:483] Algo bellman_ford step 1115 current loss 0.023148, current_train_items 35712.
I0304 19:28:25.519382 22502662377600 run.py:483] Algo bellman_ford step 1116 current loss 0.056509, current_train_items 35744.
I0304 19:28:25.543751 22502662377600 run.py:483] Algo bellman_ford step 1117 current loss 0.173317, current_train_items 35776.
I0304 19:28:25.573902 22502662377600 run.py:483] Algo bellman_ford step 1118 current loss 0.099452, current_train_items 35808.
I0304 19:28:25.606541 22502662377600 run.py:483] Algo bellman_ford step 1119 current loss 0.282715, current_train_items 35840.
I0304 19:28:25.625101 22502662377600 run.py:483] Algo bellman_ford step 1120 current loss 0.059307, current_train_items 35872.
I0304 19:28:25.640731 22502662377600 run.py:483] Algo bellman_ford step 1121 current loss 0.063754, current_train_items 35904.
I0304 19:28:25.664736 22502662377600 run.py:483] Algo bellman_ford step 1122 current loss 0.126217, current_train_items 35936.
I0304 19:28:25.693139 22502662377600 run.py:483] Algo bellman_ford step 1123 current loss 0.153435, current_train_items 35968.
I0304 19:28:25.726903 22502662377600 run.py:483] Algo bellman_ford step 1124 current loss 0.260121, current_train_items 36000.
I0304 19:28:25.745749 22502662377600 run.py:483] Algo bellman_ford step 1125 current loss 0.021847, current_train_items 36032.
I0304 19:28:25.761495 22502662377600 run.py:483] Algo bellman_ford step 1126 current loss 0.075518, current_train_items 36064.
I0304 19:28:25.785274 22502662377600 run.py:483] Algo bellman_ford step 1127 current loss 0.162389, current_train_items 36096.
I0304 19:28:25.814967 22502662377600 run.py:483] Algo bellman_ford step 1128 current loss 0.212293, current_train_items 36128.
I0304 19:28:25.844778 22502662377600 run.py:483] Algo bellman_ford step 1129 current loss 0.186120, current_train_items 36160.
I0304 19:28:25.863378 22502662377600 run.py:483] Algo bellman_ford step 1130 current loss 0.022260, current_train_items 36192.
I0304 19:28:25.879496 22502662377600 run.py:483] Algo bellman_ford step 1131 current loss 0.047164, current_train_items 36224.
I0304 19:28:25.903249 22502662377600 run.py:483] Algo bellman_ford step 1132 current loss 0.188139, current_train_items 36256.
I0304 19:28:25.933052 22502662377600 run.py:483] Algo bellman_ford step 1133 current loss 0.208828, current_train_items 36288.
I0304 19:28:25.964210 22502662377600 run.py:483] Algo bellman_ford step 1134 current loss 0.175795, current_train_items 36320.
I0304 19:28:25.982712 22502662377600 run.py:483] Algo bellman_ford step 1135 current loss 0.050157, current_train_items 36352.
I0304 19:28:25.998886 22502662377600 run.py:483] Algo bellman_ford step 1136 current loss 0.054294, current_train_items 36384.
I0304 19:28:26.021678 22502662377600 run.py:483] Algo bellman_ford step 1137 current loss 0.103413, current_train_items 36416.
I0304 19:28:26.052439 22502662377600 run.py:483] Algo bellman_ford step 1138 current loss 0.141137, current_train_items 36448.
I0304 19:28:26.083932 22502662377600 run.py:483] Algo bellman_ford step 1139 current loss 0.208015, current_train_items 36480.
I0304 19:28:26.102741 22502662377600 run.py:483] Algo bellman_ford step 1140 current loss 0.021503, current_train_items 36512.
I0304 19:28:26.119112 22502662377600 run.py:483] Algo bellman_ford step 1141 current loss 0.050877, current_train_items 36544.
I0304 19:28:26.141746 22502662377600 run.py:483] Algo bellman_ford step 1142 current loss 0.156454, current_train_items 36576.
I0304 19:28:26.170635 22502662377600 run.py:483] Algo bellman_ford step 1143 current loss 0.217603, current_train_items 36608.
I0304 19:28:26.202156 22502662377600 run.py:483] Algo bellman_ford step 1144 current loss 0.159457, current_train_items 36640.
I0304 19:28:26.221116 22502662377600 run.py:483] Algo bellman_ford step 1145 current loss 0.044667, current_train_items 36672.
I0304 19:28:26.237300 22502662377600 run.py:483] Algo bellman_ford step 1146 current loss 0.071413, current_train_items 36704.
I0304 19:28:26.260937 22502662377600 run.py:483] Algo bellman_ford step 1147 current loss 0.109445, current_train_items 36736.
I0304 19:28:26.290705 22502662377600 run.py:483] Algo bellman_ford step 1148 current loss 0.157175, current_train_items 36768.
I0304 19:28:26.320555 22502662377600 run.py:483] Algo bellman_ford step 1149 current loss 0.108641, current_train_items 36800.
I0304 19:28:26.339261 22502662377600 run.py:483] Algo bellman_ford step 1150 current loss 0.049665, current_train_items 36832.
I0304 19:28:26.347248 22502662377600 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.947265625, 'score': 0.947265625, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0304 19:28:26.347351 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.947, val scores are: bellman_ford: 0.947
I0304 19:28:26.363900 22502662377600 run.py:483] Algo bellman_ford step 1151 current loss 0.056885, current_train_items 36864.
I0304 19:28:26.388314 22502662377600 run.py:483] Algo bellman_ford step 1152 current loss 0.309678, current_train_items 36896.
I0304 19:28:26.419483 22502662377600 run.py:483] Algo bellman_ford step 1153 current loss 0.285153, current_train_items 36928.
W0304 19:28:26.440375 22502662377600 samplers.py:155] Increasing hint lengh from 11 to 12
I0304 19:28:33.171549 22502662377600 run.py:483] Algo bellman_ford step 1154 current loss 0.303234, current_train_items 36960.
I0304 19:28:33.192209 22502662377600 run.py:483] Algo bellman_ford step 1155 current loss 0.022658, current_train_items 36992.
I0304 19:28:33.208209 22502662377600 run.py:483] Algo bellman_ford step 1156 current loss 0.108394, current_train_items 37024.
I0304 19:28:33.231689 22502662377600 run.py:483] Algo bellman_ford step 1157 current loss 0.296027, current_train_items 37056.
I0304 19:28:33.261115 22502662377600 run.py:483] Algo bellman_ford step 1158 current loss 0.320218, current_train_items 37088.
I0304 19:28:33.291625 22502662377600 run.py:483] Algo bellman_ford step 1159 current loss 0.281430, current_train_items 37120.
I0304 19:28:33.311295 22502662377600 run.py:483] Algo bellman_ford step 1160 current loss 0.065864, current_train_items 37152.
I0304 19:28:33.327735 22502662377600 run.py:483] Algo bellman_ford step 1161 current loss 0.213448, current_train_items 37184.
I0304 19:28:33.350130 22502662377600 run.py:483] Algo bellman_ford step 1162 current loss 0.276743, current_train_items 37216.
I0304 19:28:33.379553 22502662377600 run.py:483] Algo bellman_ford step 1163 current loss 0.346122, current_train_items 37248.
I0304 19:28:33.411902 22502662377600 run.py:483] Algo bellman_ford step 1164 current loss 0.351762, current_train_items 37280.
I0304 19:28:33.431163 22502662377600 run.py:483] Algo bellman_ford step 1165 current loss 0.020868, current_train_items 37312.
I0304 19:28:33.447353 22502662377600 run.py:483] Algo bellman_ford step 1166 current loss 0.079432, current_train_items 37344.
I0304 19:28:33.470961 22502662377600 run.py:483] Algo bellman_ford step 1167 current loss 0.318514, current_train_items 37376.
I0304 19:28:33.500648 22502662377600 run.py:483] Algo bellman_ford step 1168 current loss 0.303342, current_train_items 37408.
I0304 19:28:33.534651 22502662377600 run.py:483] Algo bellman_ford step 1169 current loss 0.342010, current_train_items 37440.
I0304 19:28:33.554016 22502662377600 run.py:483] Algo bellman_ford step 1170 current loss 0.048465, current_train_items 37472.
I0304 19:28:33.570849 22502662377600 run.py:483] Algo bellman_ford step 1171 current loss 0.129496, current_train_items 37504.
I0304 19:28:33.594137 22502662377600 run.py:483] Algo bellman_ford step 1172 current loss 0.326573, current_train_items 37536.
I0304 19:28:33.624352 22502662377600 run.py:483] Algo bellman_ford step 1173 current loss 0.363663, current_train_items 37568.
I0304 19:28:33.656652 22502662377600 run.py:483] Algo bellman_ford step 1174 current loss 0.210679, current_train_items 37600.
I0304 19:28:33.676351 22502662377600 run.py:483] Algo bellman_ford step 1175 current loss 0.021238, current_train_items 37632.
I0304 19:28:33.692761 22502662377600 run.py:483] Algo bellman_ford step 1176 current loss 0.086569, current_train_items 37664.
I0304 19:28:33.715391 22502662377600 run.py:483] Algo bellman_ford step 1177 current loss 0.372130, current_train_items 37696.
I0304 19:28:33.744335 22502662377600 run.py:483] Algo bellman_ford step 1178 current loss 0.374572, current_train_items 37728.
I0304 19:28:33.778283 22502662377600 run.py:483] Algo bellman_ford step 1179 current loss 0.401248, current_train_items 37760.
I0304 19:28:33.797016 22502662377600 run.py:483] Algo bellman_ford step 1180 current loss 0.030770, current_train_items 37792.
I0304 19:28:33.813580 22502662377600 run.py:483] Algo bellman_ford step 1181 current loss 0.194433, current_train_items 37824.
I0304 19:28:33.836981 22502662377600 run.py:483] Algo bellman_ford step 1182 current loss 0.205935, current_train_items 37856.
I0304 19:28:33.865920 22502662377600 run.py:483] Algo bellman_ford step 1183 current loss 0.201586, current_train_items 37888.
I0304 19:28:33.898072 22502662377600 run.py:483] Algo bellman_ford step 1184 current loss 0.240649, current_train_items 37920.
I0304 19:28:33.917843 22502662377600 run.py:483] Algo bellman_ford step 1185 current loss 0.028939, current_train_items 37952.
I0304 19:28:33.933964 22502662377600 run.py:483] Algo bellman_ford step 1186 current loss 0.085828, current_train_items 37984.
I0304 19:28:33.957224 22502662377600 run.py:483] Algo bellman_ford step 1187 current loss 0.301737, current_train_items 38016.
I0304 19:28:33.986375 22502662377600 run.py:483] Algo bellman_ford step 1188 current loss 0.175273, current_train_items 38048.
I0304 19:28:34.017308 22502662377600 run.py:483] Algo bellman_ford step 1189 current loss 0.220856, current_train_items 38080.
I0304 19:28:34.036651 22502662377600 run.py:483] Algo bellman_ford step 1190 current loss 0.033629, current_train_items 38112.
I0304 19:28:34.053022 22502662377600 run.py:483] Algo bellman_ford step 1191 current loss 0.166004, current_train_items 38144.
I0304 19:28:34.077362 22502662377600 run.py:483] Algo bellman_ford step 1192 current loss 0.218309, current_train_items 38176.
I0304 19:28:34.107088 22502662377600 run.py:483] Algo bellman_ford step 1193 current loss 0.165440, current_train_items 38208.
I0304 19:28:34.139246 22502662377600 run.py:483] Algo bellman_ford step 1194 current loss 0.210588, current_train_items 38240.
I0304 19:28:34.158322 22502662377600 run.py:483] Algo bellman_ford step 1195 current loss 0.032463, current_train_items 38272.
I0304 19:28:34.174602 22502662377600 run.py:483] Algo bellman_ford step 1196 current loss 0.092552, current_train_items 38304.
I0304 19:28:34.197656 22502662377600 run.py:483] Algo bellman_ford step 1197 current loss 0.137265, current_train_items 38336.
I0304 19:28:34.228416 22502662377600 run.py:483] Algo bellman_ford step 1198 current loss 0.177473, current_train_items 38368.
I0304 19:28:34.262112 22502662377600 run.py:483] Algo bellman_ford step 1199 current loss 0.281427, current_train_items 38400.
I0304 19:28:34.281617 22502662377600 run.py:483] Algo bellman_ford step 1200 current loss 0.037583, current_train_items 38432.
I0304 19:28:34.291231 22502662377600 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0304 19:28:34.291340 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:28:34.308244 22502662377600 run.py:483] Algo bellman_ford step 1201 current loss 0.083288, current_train_items 38464.
I0304 19:28:34.331854 22502662377600 run.py:483] Algo bellman_ford step 1202 current loss 0.087831, current_train_items 38496.
I0304 19:28:34.362533 22502662377600 run.py:483] Algo bellman_ford step 1203 current loss 0.163245, current_train_items 38528.
I0304 19:28:34.396041 22502662377600 run.py:483] Algo bellman_ford step 1204 current loss 0.182106, current_train_items 38560.
I0304 19:28:34.415474 22502662377600 run.py:483] Algo bellman_ford step 1205 current loss 0.017630, current_train_items 38592.
I0304 19:28:34.431332 22502662377600 run.py:483] Algo bellman_ford step 1206 current loss 0.059983, current_train_items 38624.
I0304 19:28:34.454349 22502662377600 run.py:483] Algo bellman_ford step 1207 current loss 0.137357, current_train_items 38656.
I0304 19:28:34.484616 22502662377600 run.py:483] Algo bellman_ford step 1208 current loss 0.141253, current_train_items 38688.
I0304 19:28:34.518050 22502662377600 run.py:483] Algo bellman_ford step 1209 current loss 0.139797, current_train_items 38720.
I0304 19:28:34.537301 22502662377600 run.py:483] Algo bellman_ford step 1210 current loss 0.015525, current_train_items 38752.
I0304 19:28:34.553523 22502662377600 run.py:483] Algo bellman_ford step 1211 current loss 0.098958, current_train_items 38784.
I0304 19:28:34.576153 22502662377600 run.py:483] Algo bellman_ford step 1212 current loss 0.136436, current_train_items 38816.
I0304 19:28:34.604602 22502662377600 run.py:483] Algo bellman_ford step 1213 current loss 0.108653, current_train_items 38848.
I0304 19:28:34.636962 22502662377600 run.py:483] Algo bellman_ford step 1214 current loss 0.212417, current_train_items 38880.
I0304 19:28:34.655991 22502662377600 run.py:483] Algo bellman_ford step 1215 current loss 0.029909, current_train_items 38912.
I0304 19:28:34.671787 22502662377600 run.py:483] Algo bellman_ford step 1216 current loss 0.047642, current_train_items 38944.
I0304 19:28:34.694907 22502662377600 run.py:483] Algo bellman_ford step 1217 current loss 0.112977, current_train_items 38976.
I0304 19:28:34.725393 22502662377600 run.py:483] Algo bellman_ford step 1218 current loss 0.271461, current_train_items 39008.
I0304 19:28:34.757888 22502662377600 run.py:483] Algo bellman_ford step 1219 current loss 0.260665, current_train_items 39040.
I0304 19:28:34.776909 22502662377600 run.py:483] Algo bellman_ford step 1220 current loss 0.015710, current_train_items 39072.
I0304 19:28:34.792850 22502662377600 run.py:483] Algo bellman_ford step 1221 current loss 0.041739, current_train_items 39104.
I0304 19:28:34.817028 22502662377600 run.py:483] Algo bellman_ford step 1222 current loss 0.171822, current_train_items 39136.
I0304 19:28:34.847491 22502662377600 run.py:483] Algo bellman_ford step 1223 current loss 0.169686, current_train_items 39168.
I0304 19:28:34.879384 22502662377600 run.py:483] Algo bellman_ford step 1224 current loss 0.151677, current_train_items 39200.
I0304 19:28:34.898133 22502662377600 run.py:483] Algo bellman_ford step 1225 current loss 0.013941, current_train_items 39232.
I0304 19:28:34.914506 22502662377600 run.py:483] Algo bellman_ford step 1226 current loss 0.058648, current_train_items 39264.
I0304 19:28:34.938595 22502662377600 run.py:483] Algo bellman_ford step 1227 current loss 0.189238, current_train_items 39296.
I0304 19:28:34.967649 22502662377600 run.py:483] Algo bellman_ford step 1228 current loss 0.153936, current_train_items 39328.
I0304 19:28:35.001704 22502662377600 run.py:483] Algo bellman_ford step 1229 current loss 0.187961, current_train_items 39360.
I0304 19:28:35.020807 22502662377600 run.py:483] Algo bellman_ford step 1230 current loss 0.032304, current_train_items 39392.
I0304 19:28:35.037069 22502662377600 run.py:483] Algo bellman_ford step 1231 current loss 0.108167, current_train_items 39424.
I0304 19:28:35.061196 22502662377600 run.py:483] Algo bellman_ford step 1232 current loss 0.244780, current_train_items 39456.
I0304 19:28:35.090707 22502662377600 run.py:483] Algo bellman_ford step 1233 current loss 0.292320, current_train_items 39488.
I0304 19:28:35.121494 22502662377600 run.py:483] Algo bellman_ford step 1234 current loss 0.247067, current_train_items 39520.
I0304 19:28:35.140278 22502662377600 run.py:483] Algo bellman_ford step 1235 current loss 0.018532, current_train_items 39552.
I0304 19:28:35.155927 22502662377600 run.py:483] Algo bellman_ford step 1236 current loss 0.086895, current_train_items 39584.
I0304 19:28:35.179085 22502662377600 run.py:483] Algo bellman_ford step 1237 current loss 0.170381, current_train_items 39616.
I0304 19:28:35.208608 22502662377600 run.py:483] Algo bellman_ford step 1238 current loss 0.245609, current_train_items 39648.
I0304 19:28:35.242081 22502662377600 run.py:483] Algo bellman_ford step 1239 current loss 0.294425, current_train_items 39680.
I0304 19:28:35.261172 22502662377600 run.py:483] Algo bellman_ford step 1240 current loss 0.038641, current_train_items 39712.
I0304 19:28:35.277309 22502662377600 run.py:483] Algo bellman_ford step 1241 current loss 0.129460, current_train_items 39744.
I0304 19:28:35.301060 22502662377600 run.py:483] Algo bellman_ford step 1242 current loss 0.173976, current_train_items 39776.
I0304 19:28:35.330497 22502662377600 run.py:483] Algo bellman_ford step 1243 current loss 0.142849, current_train_items 39808.
I0304 19:28:35.363231 22502662377600 run.py:483] Algo bellman_ford step 1244 current loss 0.218840, current_train_items 39840.
I0304 19:28:35.382106 22502662377600 run.py:483] Algo bellman_ford step 1245 current loss 0.021184, current_train_items 39872.
I0304 19:28:35.398348 22502662377600 run.py:483] Algo bellman_ford step 1246 current loss 0.063185, current_train_items 39904.
I0304 19:28:35.421183 22502662377600 run.py:483] Algo bellman_ford step 1247 current loss 0.154148, current_train_items 39936.
I0304 19:28:35.451195 22502662377600 run.py:483] Algo bellman_ford step 1248 current loss 0.237228, current_train_items 39968.
I0304 19:28:35.482989 22502662377600 run.py:483] Algo bellman_ford step 1249 current loss 0.244692, current_train_items 40000.
I0304 19:28:35.502041 22502662377600 run.py:483] Algo bellman_ford step 1250 current loss 0.024579, current_train_items 40032.
I0304 19:28:35.510242 22502662377600 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0304 19:28:35.510350 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0304 19:28:35.527244 22502662377600 run.py:483] Algo bellman_ford step 1251 current loss 0.100094, current_train_items 40064.
I0304 19:28:35.551117 22502662377600 run.py:483] Algo bellman_ford step 1252 current loss 0.150276, current_train_items 40096.
I0304 19:28:35.579493 22502662377600 run.py:483] Algo bellman_ford step 1253 current loss 0.117503, current_train_items 40128.
I0304 19:28:35.609585 22502662377600 run.py:483] Algo bellman_ford step 1254 current loss 0.168975, current_train_items 40160.
I0304 19:28:35.628823 22502662377600 run.py:483] Algo bellman_ford step 1255 current loss 0.020290, current_train_items 40192.
I0304 19:28:35.645043 22502662377600 run.py:483] Algo bellman_ford step 1256 current loss 0.072613, current_train_items 40224.
I0304 19:28:35.668152 22502662377600 run.py:483] Algo bellman_ford step 1257 current loss 0.102105, current_train_items 40256.
I0304 19:28:35.697172 22502662377600 run.py:483] Algo bellman_ford step 1258 current loss 0.110115, current_train_items 40288.
I0304 19:28:35.728857 22502662377600 run.py:483] Algo bellman_ford step 1259 current loss 0.167992, current_train_items 40320.
I0304 19:28:35.748128 22502662377600 run.py:483] Algo bellman_ford step 1260 current loss 0.095492, current_train_items 40352.
I0304 19:28:35.764943 22502662377600 run.py:483] Algo bellman_ford step 1261 current loss 0.083292, current_train_items 40384.
I0304 19:28:35.788452 22502662377600 run.py:483] Algo bellman_ford step 1262 current loss 0.132488, current_train_items 40416.
I0304 19:28:35.815366 22502662377600 run.py:483] Algo bellman_ford step 1263 current loss 0.133117, current_train_items 40448.
I0304 19:28:35.849713 22502662377600 run.py:483] Algo bellman_ford step 1264 current loss 0.297421, current_train_items 40480.
I0304 19:28:35.868566 22502662377600 run.py:483] Algo bellman_ford step 1265 current loss 0.015580, current_train_items 40512.
I0304 19:28:35.884702 22502662377600 run.py:483] Algo bellman_ford step 1266 current loss 0.100409, current_train_items 40544.
I0304 19:28:35.907444 22502662377600 run.py:483] Algo bellman_ford step 1267 current loss 0.129632, current_train_items 40576.
I0304 19:28:35.936364 22502662377600 run.py:483] Algo bellman_ford step 1268 current loss 0.126680, current_train_items 40608.
I0304 19:28:35.969099 22502662377600 run.py:483] Algo bellman_ford step 1269 current loss 0.138121, current_train_items 40640.
I0304 19:28:35.988452 22502662377600 run.py:483] Algo bellman_ford step 1270 current loss 0.020740, current_train_items 40672.
I0304 19:28:36.004543 22502662377600 run.py:483] Algo bellman_ford step 1271 current loss 0.056958, current_train_items 40704.
I0304 19:28:36.026870 22502662377600 run.py:483] Algo bellman_ford step 1272 current loss 0.165605, current_train_items 40736.
I0304 19:28:36.055190 22502662377600 run.py:483] Algo bellman_ford step 1273 current loss 0.107728, current_train_items 40768.
I0304 19:28:36.087820 22502662377600 run.py:483] Algo bellman_ford step 1274 current loss 0.173664, current_train_items 40800.
I0304 19:28:36.107153 22502662377600 run.py:483] Algo bellman_ford step 1275 current loss 0.019249, current_train_items 40832.
I0304 19:28:36.123498 22502662377600 run.py:483] Algo bellman_ford step 1276 current loss 0.063326, current_train_items 40864.
I0304 19:28:36.145616 22502662377600 run.py:483] Algo bellman_ford step 1277 current loss 0.069528, current_train_items 40896.
I0304 19:28:36.175006 22502662377600 run.py:483] Algo bellman_ford step 1278 current loss 0.097743, current_train_items 40928.
I0304 19:28:36.206800 22502662377600 run.py:483] Algo bellman_ford step 1279 current loss 0.161104, current_train_items 40960.
I0304 19:28:36.225759 22502662377600 run.py:483] Algo bellman_ford step 1280 current loss 0.016603, current_train_items 40992.
I0304 19:28:36.241737 22502662377600 run.py:483] Algo bellman_ford step 1281 current loss 0.077200, current_train_items 41024.
I0304 19:28:36.265215 22502662377600 run.py:483] Algo bellman_ford step 1282 current loss 0.134519, current_train_items 41056.
I0304 19:28:36.295194 22502662377600 run.py:483] Algo bellman_ford step 1283 current loss 0.173258, current_train_items 41088.
I0304 19:28:36.326553 22502662377600 run.py:483] Algo bellman_ford step 1284 current loss 0.264941, current_train_items 41120.
I0304 19:28:36.345914 22502662377600 run.py:483] Algo bellman_ford step 1285 current loss 0.129951, current_train_items 41152.
I0304 19:28:36.362112 22502662377600 run.py:483] Algo bellman_ford step 1286 current loss 0.058461, current_train_items 41184.
I0304 19:28:36.386485 22502662377600 run.py:483] Algo bellman_ford step 1287 current loss 0.188298, current_train_items 41216.
I0304 19:28:36.416056 22502662377600 run.py:483] Algo bellman_ford step 1288 current loss 0.154490, current_train_items 41248.
I0304 19:28:36.448477 22502662377600 run.py:483] Algo bellman_ford step 1289 current loss 0.243460, current_train_items 41280.
I0304 19:28:36.468205 22502662377600 run.py:483] Algo bellman_ford step 1290 current loss 0.019127, current_train_items 41312.
I0304 19:28:36.484506 22502662377600 run.py:483] Algo bellman_ford step 1291 current loss 0.066193, current_train_items 41344.
I0304 19:28:36.507280 22502662377600 run.py:483] Algo bellman_ford step 1292 current loss 0.123683, current_train_items 41376.
I0304 19:28:36.536650 22502662377600 run.py:483] Algo bellman_ford step 1293 current loss 0.112153, current_train_items 41408.
I0304 19:28:36.570384 22502662377600 run.py:483] Algo bellman_ford step 1294 current loss 0.195423, current_train_items 41440.
I0304 19:28:36.589365 22502662377600 run.py:483] Algo bellman_ford step 1295 current loss 0.019590, current_train_items 41472.
I0304 19:28:36.605517 22502662377600 run.py:483] Algo bellman_ford step 1296 current loss 0.089614, current_train_items 41504.
I0304 19:28:36.629271 22502662377600 run.py:483] Algo bellman_ford step 1297 current loss 0.182426, current_train_items 41536.
I0304 19:28:36.657286 22502662377600 run.py:483] Algo bellman_ford step 1298 current loss 0.158752, current_train_items 41568.
I0304 19:28:36.687621 22502662377600 run.py:483] Algo bellman_ford step 1299 current loss 0.196593, current_train_items 41600.
I0304 19:28:36.706685 22502662377600 run.py:483] Algo bellman_ford step 1300 current loss 0.016322, current_train_items 41632.
I0304 19:28:36.714275 22502662377600 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.9580078125, 'score': 0.9580078125, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0304 19:28:36.714382 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.958, val scores are: bellman_ford: 0.958
I0304 19:28:36.731058 22502662377600 run.py:483] Algo bellman_ford step 1301 current loss 0.077662, current_train_items 41664.
I0304 19:28:36.755687 22502662377600 run.py:483] Algo bellman_ford step 1302 current loss 0.178645, current_train_items 41696.
I0304 19:28:36.786184 22502662377600 run.py:483] Algo bellman_ford step 1303 current loss 0.166387, current_train_items 41728.
I0304 19:28:36.817785 22502662377600 run.py:483] Algo bellman_ford step 1304 current loss 0.150043, current_train_items 41760.
I0304 19:28:36.836970 22502662377600 run.py:483] Algo bellman_ford step 1305 current loss 0.016866, current_train_items 41792.
I0304 19:28:36.852880 22502662377600 run.py:483] Algo bellman_ford step 1306 current loss 0.042394, current_train_items 41824.
I0304 19:28:36.876176 22502662377600 run.py:483] Algo bellman_ford step 1307 current loss 0.107992, current_train_items 41856.
I0304 19:28:36.905411 22502662377600 run.py:483] Algo bellman_ford step 1308 current loss 0.153531, current_train_items 41888.
I0304 19:28:36.937250 22502662377600 run.py:483] Algo bellman_ford step 1309 current loss 0.189842, current_train_items 41920.
I0304 19:28:36.956610 22502662377600 run.py:483] Algo bellman_ford step 1310 current loss 0.023952, current_train_items 41952.
I0304 19:28:36.972765 22502662377600 run.py:483] Algo bellman_ford step 1311 current loss 0.089143, current_train_items 41984.
I0304 19:28:36.996187 22502662377600 run.py:483] Algo bellman_ford step 1312 current loss 0.158989, current_train_items 42016.
I0304 19:28:37.026740 22502662377600 run.py:483] Algo bellman_ford step 1313 current loss 0.199114, current_train_items 42048.
I0304 19:28:37.059454 22502662377600 run.py:483] Algo bellman_ford step 1314 current loss 0.159960, current_train_items 42080.
I0304 19:28:37.078378 22502662377600 run.py:483] Algo bellman_ford step 1315 current loss 0.012739, current_train_items 42112.
I0304 19:28:37.094281 22502662377600 run.py:483] Algo bellman_ford step 1316 current loss 0.072826, current_train_items 42144.
I0304 19:28:37.117774 22502662377600 run.py:483] Algo bellman_ford step 1317 current loss 0.245899, current_train_items 42176.
I0304 19:28:37.147479 22502662377600 run.py:483] Algo bellman_ford step 1318 current loss 0.090653, current_train_items 42208.
I0304 19:28:37.179518 22502662377600 run.py:483] Algo bellman_ford step 1319 current loss 0.201334, current_train_items 42240.
I0304 19:28:37.198583 22502662377600 run.py:483] Algo bellman_ford step 1320 current loss 0.027653, current_train_items 42272.
I0304 19:28:37.214726 22502662377600 run.py:483] Algo bellman_ford step 1321 current loss 0.058937, current_train_items 42304.
I0304 19:28:37.238632 22502662377600 run.py:483] Algo bellman_ford step 1322 current loss 0.172491, current_train_items 42336.
I0304 19:28:37.268874 22502662377600 run.py:483] Algo bellman_ford step 1323 current loss 0.130477, current_train_items 42368.
I0304 19:28:37.300908 22502662377600 run.py:483] Algo bellman_ford step 1324 current loss 0.224373, current_train_items 42400.
I0304 19:28:37.320164 22502662377600 run.py:483] Algo bellman_ford step 1325 current loss 0.110656, current_train_items 42432.
I0304 19:28:37.336056 22502662377600 run.py:483] Algo bellman_ford step 1326 current loss 0.054578, current_train_items 42464.
I0304 19:28:37.358838 22502662377600 run.py:483] Algo bellman_ford step 1327 current loss 0.088768, current_train_items 42496.
I0304 19:28:37.388459 22502662377600 run.py:483] Algo bellman_ford step 1328 current loss 0.173557, current_train_items 42528.
I0304 19:28:37.421633 22502662377600 run.py:483] Algo bellman_ford step 1329 current loss 0.156871, current_train_items 42560.
I0304 19:28:37.440998 22502662377600 run.py:483] Algo bellman_ford step 1330 current loss 0.034379, current_train_items 42592.
I0304 19:28:37.456976 22502662377600 run.py:483] Algo bellman_ford step 1331 current loss 0.056800, current_train_items 42624.
I0304 19:28:37.479937 22502662377600 run.py:483] Algo bellman_ford step 1332 current loss 0.169634, current_train_items 42656.
I0304 19:28:37.509409 22502662377600 run.py:483] Algo bellman_ford step 1333 current loss 0.227850, current_train_items 42688.
I0304 19:28:37.540984 22502662377600 run.py:483] Algo bellman_ford step 1334 current loss 0.178552, current_train_items 42720.
I0304 19:28:37.560282 22502662377600 run.py:483] Algo bellman_ford step 1335 current loss 0.017700, current_train_items 42752.
I0304 19:28:37.576128 22502662377600 run.py:483] Algo bellman_ford step 1336 current loss 0.075312, current_train_items 42784.
I0304 19:28:37.600779 22502662377600 run.py:483] Algo bellman_ford step 1337 current loss 0.092236, current_train_items 42816.
I0304 19:28:37.630241 22502662377600 run.py:483] Algo bellman_ford step 1338 current loss 0.137528, current_train_items 42848.
I0304 19:28:37.664860 22502662377600 run.py:483] Algo bellman_ford step 1339 current loss 0.203091, current_train_items 42880.
I0304 19:28:37.683607 22502662377600 run.py:483] Algo bellman_ford step 1340 current loss 0.040626, current_train_items 42912.
I0304 19:28:37.699691 22502662377600 run.py:483] Algo bellman_ford step 1341 current loss 0.048153, current_train_items 42944.
I0304 19:28:37.723234 22502662377600 run.py:483] Algo bellman_ford step 1342 current loss 0.125731, current_train_items 42976.
I0304 19:28:37.752769 22502662377600 run.py:483] Algo bellman_ford step 1343 current loss 0.130579, current_train_items 43008.
I0304 19:28:37.784983 22502662377600 run.py:483] Algo bellman_ford step 1344 current loss 0.151384, current_train_items 43040.
I0304 19:28:37.804132 22502662377600 run.py:483] Algo bellman_ford step 1345 current loss 0.016008, current_train_items 43072.
I0304 19:28:37.820373 22502662377600 run.py:483] Algo bellman_ford step 1346 current loss 0.084055, current_train_items 43104.
I0304 19:28:37.845314 22502662377600 run.py:483] Algo bellman_ford step 1347 current loss 0.153276, current_train_items 43136.
I0304 19:28:37.873620 22502662377600 run.py:483] Algo bellman_ford step 1348 current loss 0.159944, current_train_items 43168.
I0304 19:28:37.904839 22502662377600 run.py:483] Algo bellman_ford step 1349 current loss 0.163945, current_train_items 43200.
I0304 19:28:37.924280 22502662377600 run.py:483] Algo bellman_ford step 1350 current loss 0.039534, current_train_items 43232.
I0304 19:28:37.932559 22502662377600 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0304 19:28:37.932665 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0304 19:28:37.949661 22502662377600 run.py:483] Algo bellman_ford step 1351 current loss 0.097119, current_train_items 43264.
I0304 19:28:37.974508 22502662377600 run.py:483] Algo bellman_ford step 1352 current loss 0.199725, current_train_items 43296.
I0304 19:28:38.003094 22502662377600 run.py:483] Algo bellman_ford step 1353 current loss 0.294477, current_train_items 43328.
I0304 19:28:38.037789 22502662377600 run.py:483] Algo bellman_ford step 1354 current loss 0.216231, current_train_items 43360.
I0304 19:28:38.057556 22502662377600 run.py:483] Algo bellman_ford step 1355 current loss 0.028763, current_train_items 43392.
I0304 19:28:38.073315 22502662377600 run.py:483] Algo bellman_ford step 1356 current loss 0.184303, current_train_items 43424.
I0304 19:28:38.096363 22502662377600 run.py:483] Algo bellman_ford step 1357 current loss 0.251636, current_train_items 43456.
I0304 19:28:38.125862 22502662377600 run.py:483] Algo bellman_ford step 1358 current loss 0.410194, current_train_items 43488.
I0304 19:28:38.159628 22502662377600 run.py:483] Algo bellman_ford step 1359 current loss 0.488661, current_train_items 43520.
I0304 19:28:38.179087 22502662377600 run.py:483] Algo bellman_ford step 1360 current loss 0.019756, current_train_items 43552.
I0304 19:28:38.196019 22502662377600 run.py:483] Algo bellman_ford step 1361 current loss 0.159912, current_train_items 43584.
I0304 19:28:38.220271 22502662377600 run.py:483] Algo bellman_ford step 1362 current loss 0.168187, current_train_items 43616.
I0304 19:28:38.250180 22502662377600 run.py:483] Algo bellman_ford step 1363 current loss 0.183068, current_train_items 43648.
I0304 19:28:38.283870 22502662377600 run.py:483] Algo bellman_ford step 1364 current loss 0.173645, current_train_items 43680.
I0304 19:28:38.303254 22502662377600 run.py:483] Algo bellman_ford step 1365 current loss 0.015738, current_train_items 43712.
I0304 19:28:38.319135 22502662377600 run.py:483] Algo bellman_ford step 1366 current loss 0.094358, current_train_items 43744.
I0304 19:28:38.342097 22502662377600 run.py:483] Algo bellman_ford step 1367 current loss 0.210826, current_train_items 43776.
I0304 19:28:38.369945 22502662377600 run.py:483] Algo bellman_ford step 1368 current loss 0.154344, current_train_items 43808.
I0304 19:28:38.402592 22502662377600 run.py:483] Algo bellman_ford step 1369 current loss 0.206278, current_train_items 43840.
I0304 19:28:38.422147 22502662377600 run.py:483] Algo bellman_ford step 1370 current loss 0.020815, current_train_items 43872.
I0304 19:28:38.438690 22502662377600 run.py:483] Algo bellman_ford step 1371 current loss 0.145724, current_train_items 43904.
I0304 19:28:38.462247 22502662377600 run.py:483] Algo bellman_ford step 1372 current loss 0.292292, current_train_items 43936.
I0304 19:28:38.493249 22502662377600 run.py:483] Algo bellman_ford step 1373 current loss 0.461521, current_train_items 43968.
I0304 19:28:38.526566 22502662377600 run.py:483] Algo bellman_ford step 1374 current loss 0.206506, current_train_items 44000.
I0304 19:28:38.545821 22502662377600 run.py:483] Algo bellman_ford step 1375 current loss 0.044578, current_train_items 44032.
I0304 19:28:38.561593 22502662377600 run.py:483] Algo bellman_ford step 1376 current loss 0.131191, current_train_items 44064.
I0304 19:28:38.584897 22502662377600 run.py:483] Algo bellman_ford step 1377 current loss 0.349698, current_train_items 44096.
I0304 19:28:38.615357 22502662377600 run.py:483] Algo bellman_ford step 1378 current loss 0.397580, current_train_items 44128.
I0304 19:28:38.648453 22502662377600 run.py:483] Algo bellman_ford step 1379 current loss 0.336298, current_train_items 44160.
I0304 19:28:38.667892 22502662377600 run.py:483] Algo bellman_ford step 1380 current loss 0.048606, current_train_items 44192.
I0304 19:28:38.684311 22502662377600 run.py:483] Algo bellman_ford step 1381 current loss 0.060003, current_train_items 44224.
I0304 19:28:38.707728 22502662377600 run.py:483] Algo bellman_ford step 1382 current loss 0.133630, current_train_items 44256.
I0304 19:28:38.737446 22502662377600 run.py:483] Algo bellman_ford step 1383 current loss 0.194556, current_train_items 44288.
I0304 19:28:38.767504 22502662377600 run.py:483] Algo bellman_ford step 1384 current loss 0.174199, current_train_items 44320.
I0304 19:28:38.786970 22502662377600 run.py:483] Algo bellman_ford step 1385 current loss 0.014127, current_train_items 44352.
I0304 19:28:38.802826 22502662377600 run.py:483] Algo bellman_ford step 1386 current loss 0.037471, current_train_items 44384.
I0304 19:28:38.825357 22502662377600 run.py:483] Algo bellman_ford step 1387 current loss 0.134670, current_train_items 44416.
I0304 19:28:38.855581 22502662377600 run.py:483] Algo bellman_ford step 1388 current loss 0.180228, current_train_items 44448.
I0304 19:28:38.887855 22502662377600 run.py:483] Algo bellman_ford step 1389 current loss 0.232422, current_train_items 44480.
I0304 19:28:38.907453 22502662377600 run.py:483] Algo bellman_ford step 1390 current loss 0.034444, current_train_items 44512.
I0304 19:28:38.923355 22502662377600 run.py:483] Algo bellman_ford step 1391 current loss 0.082781, current_train_items 44544.
I0304 19:28:38.947514 22502662377600 run.py:483] Algo bellman_ford step 1392 current loss 0.189080, current_train_items 44576.
I0304 19:28:38.976677 22502662377600 run.py:483] Algo bellman_ford step 1393 current loss 0.237276, current_train_items 44608.
I0304 19:28:39.008993 22502662377600 run.py:483] Algo bellman_ford step 1394 current loss 0.207363, current_train_items 44640.
I0304 19:28:39.028026 22502662377600 run.py:483] Algo bellman_ford step 1395 current loss 0.011113, current_train_items 44672.
I0304 19:28:39.043882 22502662377600 run.py:483] Algo bellman_ford step 1396 current loss 0.052967, current_train_items 44704.
I0304 19:28:39.067780 22502662377600 run.py:483] Algo bellman_ford step 1397 current loss 0.228067, current_train_items 44736.
I0304 19:28:39.098657 22502662377600 run.py:483] Algo bellman_ford step 1398 current loss 0.236667, current_train_items 44768.
I0304 19:28:39.130128 22502662377600 run.py:483] Algo bellman_ford step 1399 current loss 0.223273, current_train_items 44800.
I0304 19:28:39.149814 22502662377600 run.py:483] Algo bellman_ford step 1400 current loss 0.028415, current_train_items 44832.
I0304 19:28:39.157440 22502662377600 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0304 19:28:39.157552 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:28:39.173993 22502662377600 run.py:483] Algo bellman_ford step 1401 current loss 0.108738, current_train_items 44864.
I0304 19:28:39.198668 22502662377600 run.py:483] Algo bellman_ford step 1402 current loss 0.325745, current_train_items 44896.
I0304 19:28:39.229390 22502662377600 run.py:483] Algo bellman_ford step 1403 current loss 0.242694, current_train_items 44928.
I0304 19:28:39.263013 22502662377600 run.py:483] Algo bellman_ford step 1404 current loss 0.197977, current_train_items 44960.
I0304 19:28:39.282631 22502662377600 run.py:483] Algo bellman_ford step 1405 current loss 0.025363, current_train_items 44992.
I0304 19:28:39.298851 22502662377600 run.py:483] Algo bellman_ford step 1406 current loss 0.125669, current_train_items 45024.
I0304 19:28:39.322514 22502662377600 run.py:483] Algo bellman_ford step 1407 current loss 0.257015, current_train_items 45056.
I0304 19:28:39.352603 22502662377600 run.py:483] Algo bellman_ford step 1408 current loss 0.258747, current_train_items 45088.
I0304 19:28:39.384922 22502662377600 run.py:483] Algo bellman_ford step 1409 current loss 0.212177, current_train_items 45120.
I0304 19:28:39.403925 22502662377600 run.py:483] Algo bellman_ford step 1410 current loss 0.023353, current_train_items 45152.
I0304 19:28:39.420480 22502662377600 run.py:483] Algo bellman_ford step 1411 current loss 0.089875, current_train_items 45184.
I0304 19:28:39.444126 22502662377600 run.py:483] Algo bellman_ford step 1412 current loss 0.210140, current_train_items 45216.
I0304 19:28:39.471968 22502662377600 run.py:483] Algo bellman_ford step 1413 current loss 0.136657, current_train_items 45248.
I0304 19:28:39.503825 22502662377600 run.py:483] Algo bellman_ford step 1414 current loss 0.182119, current_train_items 45280.
I0304 19:28:39.523287 22502662377600 run.py:483] Algo bellman_ford step 1415 current loss 0.020657, current_train_items 45312.
I0304 19:28:39.539872 22502662377600 run.py:483] Algo bellman_ford step 1416 current loss 0.063109, current_train_items 45344.
I0304 19:28:39.563033 22502662377600 run.py:483] Algo bellman_ford step 1417 current loss 0.106243, current_train_items 45376.
I0304 19:28:39.591942 22502662377600 run.py:483] Algo bellman_ford step 1418 current loss 0.294278, current_train_items 45408.
I0304 19:28:39.624991 22502662377600 run.py:483] Algo bellman_ford step 1419 current loss 0.335712, current_train_items 45440.
I0304 19:28:39.644496 22502662377600 run.py:483] Algo bellman_ford step 1420 current loss 0.075923, current_train_items 45472.
I0304 19:28:39.660667 22502662377600 run.py:483] Algo bellman_ford step 1421 current loss 0.096267, current_train_items 45504.
I0304 19:28:39.684656 22502662377600 run.py:483] Algo bellman_ford step 1422 current loss 0.166422, current_train_items 45536.
I0304 19:28:39.715411 22502662377600 run.py:483] Algo bellman_ford step 1423 current loss 0.193886, current_train_items 45568.
I0304 19:28:39.747266 22502662377600 run.py:483] Algo bellman_ford step 1424 current loss 0.113442, current_train_items 45600.
I0304 19:28:39.766455 22502662377600 run.py:483] Algo bellman_ford step 1425 current loss 0.022323, current_train_items 45632.
I0304 19:28:39.783085 22502662377600 run.py:483] Algo bellman_ford step 1426 current loss 0.136283, current_train_items 45664.
I0304 19:28:39.807796 22502662377600 run.py:483] Algo bellman_ford step 1427 current loss 0.199059, current_train_items 45696.
I0304 19:28:39.837630 22502662377600 run.py:483] Algo bellman_ford step 1428 current loss 0.204891, current_train_items 45728.
I0304 19:28:39.868726 22502662377600 run.py:483] Algo bellman_ford step 1429 current loss 0.137674, current_train_items 45760.
I0304 19:28:39.887809 22502662377600 run.py:483] Algo bellman_ford step 1430 current loss 0.016794, current_train_items 45792.
I0304 19:28:39.903642 22502662377600 run.py:483] Algo bellman_ford step 1431 current loss 0.073537, current_train_items 45824.
I0304 19:28:39.928582 22502662377600 run.py:483] Algo bellman_ford step 1432 current loss 0.298777, current_train_items 45856.
I0304 19:28:39.957023 22502662377600 run.py:483] Algo bellman_ford step 1433 current loss 0.178355, current_train_items 45888.
I0304 19:28:39.988122 22502662377600 run.py:483] Algo bellman_ford step 1434 current loss 0.145483, current_train_items 45920.
I0304 19:28:40.006978 22502662377600 run.py:483] Algo bellman_ford step 1435 current loss 0.051768, current_train_items 45952.
I0304 19:28:40.023020 22502662377600 run.py:483] Algo bellman_ford step 1436 current loss 0.085335, current_train_items 45984.
I0304 19:28:40.046294 22502662377600 run.py:483] Algo bellman_ford step 1437 current loss 0.263273, current_train_items 46016.
I0304 19:28:40.075266 22502662377600 run.py:483] Algo bellman_ford step 1438 current loss 0.138519, current_train_items 46048.
I0304 19:28:40.109047 22502662377600 run.py:483] Algo bellman_ford step 1439 current loss 0.224777, current_train_items 46080.
I0304 19:28:40.128306 22502662377600 run.py:483] Algo bellman_ford step 1440 current loss 0.014796, current_train_items 46112.
I0304 19:28:40.144564 22502662377600 run.py:483] Algo bellman_ford step 1441 current loss 0.114501, current_train_items 46144.
I0304 19:28:40.167577 22502662377600 run.py:483] Algo bellman_ford step 1442 current loss 0.155807, current_train_items 46176.
I0304 19:28:40.197317 22502662377600 run.py:483] Algo bellman_ford step 1443 current loss 0.163016, current_train_items 46208.
I0304 19:28:40.233824 22502662377600 run.py:483] Algo bellman_ford step 1444 current loss 0.285356, current_train_items 46240.
I0304 19:28:40.253033 22502662377600 run.py:483] Algo bellman_ford step 1445 current loss 0.022511, current_train_items 46272.
I0304 19:28:40.268586 22502662377600 run.py:483] Algo bellman_ford step 1446 current loss 0.137247, current_train_items 46304.
I0304 19:28:40.292521 22502662377600 run.py:483] Algo bellman_ford step 1447 current loss 0.145663, current_train_items 46336.
I0304 19:28:40.321969 22502662377600 run.py:483] Algo bellman_ford step 1448 current loss 0.176287, current_train_items 46368.
I0304 19:28:40.353374 22502662377600 run.py:483] Algo bellman_ford step 1449 current loss 0.216657, current_train_items 46400.
I0304 19:28:40.372952 22502662377600 run.py:483] Algo bellman_ford step 1450 current loss 0.027866, current_train_items 46432.
I0304 19:28:40.381047 22502662377600 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0304 19:28:40.381154 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0304 19:28:40.397441 22502662377600 run.py:483] Algo bellman_ford step 1451 current loss 0.062030, current_train_items 46464.
I0304 19:28:40.420867 22502662377600 run.py:483] Algo bellman_ford step 1452 current loss 0.096414, current_train_items 46496.
I0304 19:28:40.451701 22502662377600 run.py:483] Algo bellman_ford step 1453 current loss 0.149157, current_train_items 46528.
I0304 19:28:40.485849 22502662377600 run.py:483] Algo bellman_ford step 1454 current loss 0.215465, current_train_items 46560.
I0304 19:28:40.505057 22502662377600 run.py:483] Algo bellman_ford step 1455 current loss 0.032866, current_train_items 46592.
I0304 19:28:40.520922 22502662377600 run.py:483] Algo bellman_ford step 1456 current loss 0.060955, current_train_items 46624.
I0304 19:28:40.543308 22502662377600 run.py:483] Algo bellman_ford step 1457 current loss 0.060042, current_train_items 46656.
I0304 19:28:40.573328 22502662377600 run.py:483] Algo bellman_ford step 1458 current loss 0.182690, current_train_items 46688.
I0304 19:28:40.605357 22502662377600 run.py:483] Algo bellman_ford step 1459 current loss 0.209616, current_train_items 46720.
I0304 19:28:40.625006 22502662377600 run.py:483] Algo bellman_ford step 1460 current loss 0.034174, current_train_items 46752.
I0304 19:28:40.640771 22502662377600 run.py:483] Algo bellman_ford step 1461 current loss 0.043821, current_train_items 46784.
I0304 19:28:40.663412 22502662377600 run.py:483] Algo bellman_ford step 1462 current loss 0.125494, current_train_items 46816.
I0304 19:28:40.692563 22502662377600 run.py:483] Algo bellman_ford step 1463 current loss 0.172927, current_train_items 46848.
I0304 19:28:40.723490 22502662377600 run.py:483] Algo bellman_ford step 1464 current loss 0.143700, current_train_items 46880.
I0304 19:28:40.742382 22502662377600 run.py:483] Algo bellman_ford step 1465 current loss 0.064810, current_train_items 46912.
I0304 19:28:40.758820 22502662377600 run.py:483] Algo bellman_ford step 1466 current loss 0.046312, current_train_items 46944.
I0304 19:28:40.782000 22502662377600 run.py:483] Algo bellman_ford step 1467 current loss 0.088547, current_train_items 46976.
I0304 19:28:40.809825 22502662377600 run.py:483] Algo bellman_ford step 1468 current loss 0.109211, current_train_items 47008.
I0304 19:28:40.840668 22502662377600 run.py:483] Algo bellman_ford step 1469 current loss 0.170781, current_train_items 47040.
I0304 19:28:40.859925 22502662377600 run.py:483] Algo bellman_ford step 1470 current loss 0.028955, current_train_items 47072.
I0304 19:28:40.876487 22502662377600 run.py:483] Algo bellman_ford step 1471 current loss 0.074071, current_train_items 47104.
I0304 19:28:40.899800 22502662377600 run.py:483] Algo bellman_ford step 1472 current loss 0.110050, current_train_items 47136.
I0304 19:28:40.929292 22502662377600 run.py:483] Algo bellman_ford step 1473 current loss 0.181477, current_train_items 47168.
I0304 19:28:40.962150 22502662377600 run.py:483] Algo bellman_ford step 1474 current loss 0.195085, current_train_items 47200.
I0304 19:28:40.981470 22502662377600 run.py:483] Algo bellman_ford step 1475 current loss 0.048618, current_train_items 47232.
I0304 19:28:40.997571 22502662377600 run.py:483] Algo bellman_ford step 1476 current loss 0.046822, current_train_items 47264.
I0304 19:28:41.020496 22502662377600 run.py:483] Algo bellman_ford step 1477 current loss 0.110529, current_train_items 47296.
I0304 19:28:41.049845 22502662377600 run.py:483] Algo bellman_ford step 1478 current loss 0.261380, current_train_items 47328.
I0304 19:28:41.083365 22502662377600 run.py:483] Algo bellman_ford step 1479 current loss 0.404077, current_train_items 47360.
I0304 19:28:41.101943 22502662377600 run.py:483] Algo bellman_ford step 1480 current loss 0.017958, current_train_items 47392.
I0304 19:28:41.117770 22502662377600 run.py:483] Algo bellman_ford step 1481 current loss 0.049927, current_train_items 47424.
I0304 19:28:41.140347 22502662377600 run.py:483] Algo bellman_ford step 1482 current loss 0.127276, current_train_items 47456.
I0304 19:28:41.170237 22502662377600 run.py:483] Algo bellman_ford step 1483 current loss 0.167254, current_train_items 47488.
I0304 19:28:41.200771 22502662377600 run.py:483] Algo bellman_ford step 1484 current loss 0.152410, current_train_items 47520.
I0304 19:28:41.220026 22502662377600 run.py:483] Algo bellman_ford step 1485 current loss 0.029713, current_train_items 47552.
I0304 19:28:41.235784 22502662377600 run.py:483] Algo bellman_ford step 1486 current loss 0.060483, current_train_items 47584.
I0304 19:28:41.259593 22502662377600 run.py:483] Algo bellman_ford step 1487 current loss 0.241731, current_train_items 47616.
I0304 19:28:41.287226 22502662377600 run.py:483] Algo bellman_ford step 1488 current loss 0.225690, current_train_items 47648.
I0304 19:28:41.320169 22502662377600 run.py:483] Algo bellman_ford step 1489 current loss 0.247704, current_train_items 47680.
I0304 19:28:41.339315 22502662377600 run.py:483] Algo bellman_ford step 1490 current loss 0.018930, current_train_items 47712.
I0304 19:28:41.355551 22502662377600 run.py:483] Algo bellman_ford step 1491 current loss 0.083736, current_train_items 47744.
I0304 19:28:41.378897 22502662377600 run.py:483] Algo bellman_ford step 1492 current loss 0.126279, current_train_items 47776.
I0304 19:28:41.408242 22502662377600 run.py:483] Algo bellman_ford step 1493 current loss 0.239413, current_train_items 47808.
I0304 19:28:41.441861 22502662377600 run.py:483] Algo bellman_ford step 1494 current loss 0.252952, current_train_items 47840.
I0304 19:28:41.460702 22502662377600 run.py:483] Algo bellman_ford step 1495 current loss 0.021088, current_train_items 47872.
I0304 19:28:41.477327 22502662377600 run.py:483] Algo bellman_ford step 1496 current loss 0.127801, current_train_items 47904.
I0304 19:28:41.501271 22502662377600 run.py:483] Algo bellman_ford step 1497 current loss 0.176313, current_train_items 47936.
I0304 19:28:41.531327 22502662377600 run.py:483] Algo bellman_ford step 1498 current loss 0.257744, current_train_items 47968.
I0304 19:28:41.565111 22502662377600 run.py:483] Algo bellman_ford step 1499 current loss 0.214676, current_train_items 48000.
I0304 19:28:41.584454 22502662377600 run.py:483] Algo bellman_ford step 1500 current loss 0.024978, current_train_items 48032.
I0304 19:28:41.592323 22502662377600 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.9580078125, 'score': 0.9580078125, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0304 19:28:41.592428 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.958, val scores are: bellman_ford: 0.958
I0304 19:28:41.609289 22502662377600 run.py:483] Algo bellman_ford step 1501 current loss 0.097724, current_train_items 48064.
I0304 19:28:41.633842 22502662377600 run.py:483] Algo bellman_ford step 1502 current loss 0.168394, current_train_items 48096.
I0304 19:28:41.663561 22502662377600 run.py:483] Algo bellman_ford step 1503 current loss 0.194779, current_train_items 48128.
I0304 19:28:41.696075 22502662377600 run.py:483] Algo bellman_ford step 1504 current loss 0.171364, current_train_items 48160.
I0304 19:28:41.715162 22502662377600 run.py:483] Algo bellman_ford step 1505 current loss 0.028702, current_train_items 48192.
I0304 19:28:41.730904 22502662377600 run.py:483] Algo bellman_ford step 1506 current loss 0.118102, current_train_items 48224.
I0304 19:28:41.753949 22502662377600 run.py:483] Algo bellman_ford step 1507 current loss 0.171672, current_train_items 48256.
I0304 19:28:41.783574 22502662377600 run.py:483] Algo bellman_ford step 1508 current loss 0.333626, current_train_items 48288.
I0304 19:28:41.816073 22502662377600 run.py:483] Algo bellman_ford step 1509 current loss 0.243360, current_train_items 48320.
I0304 19:28:41.835294 22502662377600 run.py:483] Algo bellman_ford step 1510 current loss 0.029725, current_train_items 48352.
I0304 19:28:41.851540 22502662377600 run.py:483] Algo bellman_ford step 1511 current loss 0.069168, current_train_items 48384.
I0304 19:28:41.875402 22502662377600 run.py:483] Algo bellman_ford step 1512 current loss 0.142149, current_train_items 48416.
I0304 19:28:41.903696 22502662377600 run.py:483] Algo bellman_ford step 1513 current loss 0.134570, current_train_items 48448.
I0304 19:28:41.937691 22502662377600 run.py:483] Algo bellman_ford step 1514 current loss 0.267362, current_train_items 48480.
I0304 19:28:41.956803 22502662377600 run.py:483] Algo bellman_ford step 1515 current loss 0.022167, current_train_items 48512.
I0304 19:28:41.972549 22502662377600 run.py:483] Algo bellman_ford step 1516 current loss 0.055765, current_train_items 48544.
I0304 19:28:41.996185 22502662377600 run.py:483] Algo bellman_ford step 1517 current loss 0.217896, current_train_items 48576.
I0304 19:28:42.026236 22502662377600 run.py:483] Algo bellman_ford step 1518 current loss 0.226694, current_train_items 48608.
I0304 19:28:42.057934 22502662377600 run.py:483] Algo bellman_ford step 1519 current loss 0.150974, current_train_items 48640.
I0304 19:28:42.077010 22502662377600 run.py:483] Algo bellman_ford step 1520 current loss 0.016234, current_train_items 48672.
I0304 19:28:42.092945 22502662377600 run.py:483] Algo bellman_ford step 1521 current loss 0.115318, current_train_items 48704.
I0304 19:28:42.117083 22502662377600 run.py:483] Algo bellman_ford step 1522 current loss 0.221465, current_train_items 48736.
I0304 19:28:42.146700 22502662377600 run.py:483] Algo bellman_ford step 1523 current loss 0.321517, current_train_items 48768.
I0304 19:28:42.179008 22502662377600 run.py:483] Algo bellman_ford step 1524 current loss 0.218046, current_train_items 48800.
I0304 19:28:42.198400 22502662377600 run.py:483] Algo bellman_ford step 1525 current loss 0.030089, current_train_items 48832.
I0304 19:28:42.214311 22502662377600 run.py:483] Algo bellman_ford step 1526 current loss 0.092502, current_train_items 48864.
I0304 19:28:42.238655 22502662377600 run.py:483] Algo bellman_ford step 1527 current loss 0.447814, current_train_items 48896.
I0304 19:28:42.268095 22502662377600 run.py:483] Algo bellman_ford step 1528 current loss 0.736443, current_train_items 48928.
I0304 19:28:42.300718 22502662377600 run.py:483] Algo bellman_ford step 1529 current loss 0.578410, current_train_items 48960.
I0304 19:28:42.319809 22502662377600 run.py:483] Algo bellman_ford step 1530 current loss 0.078668, current_train_items 48992.
I0304 19:28:42.335710 22502662377600 run.py:483] Algo bellman_ford step 1531 current loss 0.085607, current_train_items 49024.
I0304 19:28:42.358867 22502662377600 run.py:483] Algo bellman_ford step 1532 current loss 0.133832, current_train_items 49056.
I0304 19:28:42.389318 22502662377600 run.py:483] Algo bellman_ford step 1533 current loss 0.296119, current_train_items 49088.
I0304 19:28:42.422521 22502662377600 run.py:483] Algo bellman_ford step 1534 current loss 0.515382, current_train_items 49120.
I0304 19:28:42.441844 22502662377600 run.py:483] Algo bellman_ford step 1535 current loss 0.056796, current_train_items 49152.
I0304 19:28:42.458270 22502662377600 run.py:483] Algo bellman_ford step 1536 current loss 0.074597, current_train_items 49184.
I0304 19:28:42.482300 22502662377600 run.py:483] Algo bellman_ford step 1537 current loss 0.149685, current_train_items 49216.
I0304 19:28:42.512491 22502662377600 run.py:483] Algo bellman_ford step 1538 current loss 0.170844, current_train_items 49248.
I0304 19:28:42.547511 22502662377600 run.py:483] Algo bellman_ford step 1539 current loss 0.399061, current_train_items 49280.
I0304 19:28:42.566980 22502662377600 run.py:483] Algo bellman_ford step 1540 current loss 0.044852, current_train_items 49312.
I0304 19:28:42.582822 22502662377600 run.py:483] Algo bellman_ford step 1541 current loss 0.124181, current_train_items 49344.
I0304 19:28:42.606289 22502662377600 run.py:483] Algo bellman_ford step 1542 current loss 0.168616, current_train_items 49376.
I0304 19:28:42.635498 22502662377600 run.py:483] Algo bellman_ford step 1543 current loss 0.150405, current_train_items 49408.
I0304 19:28:42.667801 22502662377600 run.py:483] Algo bellman_ford step 1544 current loss 0.292934, current_train_items 49440.
I0304 19:28:42.687115 22502662377600 run.py:483] Algo bellman_ford step 1545 current loss 0.034430, current_train_items 49472.
I0304 19:28:42.703187 22502662377600 run.py:483] Algo bellman_ford step 1546 current loss 0.101016, current_train_items 49504.
I0304 19:28:42.727102 22502662377600 run.py:483] Algo bellman_ford step 1547 current loss 0.140199, current_train_items 49536.
I0304 19:28:42.757823 22502662377600 run.py:483] Algo bellman_ford step 1548 current loss 0.144471, current_train_items 49568.
I0304 19:28:42.791985 22502662377600 run.py:483] Algo bellman_ford step 1549 current loss 0.275176, current_train_items 49600.
I0304 19:28:42.811149 22502662377600 run.py:483] Algo bellman_ford step 1550 current loss 0.053563, current_train_items 49632.
I0304 19:28:42.819229 22502662377600 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0304 19:28:42.819334 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.969, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:28:42.849101 22502662377600 run.py:483] Algo bellman_ford step 1551 current loss 0.083869, current_train_items 49664.
I0304 19:28:42.872673 22502662377600 run.py:483] Algo bellman_ford step 1552 current loss 0.082111, current_train_items 49696.
I0304 19:28:42.902653 22502662377600 run.py:483] Algo bellman_ford step 1553 current loss 0.145964, current_train_items 49728.
I0304 19:28:42.934177 22502662377600 run.py:483] Algo bellman_ford step 1554 current loss 0.204190, current_train_items 49760.
I0304 19:28:42.953702 22502662377600 run.py:483] Algo bellman_ford step 1555 current loss 0.014236, current_train_items 49792.
I0304 19:28:42.969516 22502662377600 run.py:483] Algo bellman_ford step 1556 current loss 0.076069, current_train_items 49824.
I0304 19:28:42.993405 22502662377600 run.py:483] Algo bellman_ford step 1557 current loss 0.179221, current_train_items 49856.
I0304 19:28:43.023603 22502662377600 run.py:483] Algo bellman_ford step 1558 current loss 0.131962, current_train_items 49888.
I0304 19:28:43.056527 22502662377600 run.py:483] Algo bellman_ford step 1559 current loss 0.181992, current_train_items 49920.
I0304 19:28:43.076412 22502662377600 run.py:483] Algo bellman_ford step 1560 current loss 0.054155, current_train_items 49952.
I0304 19:28:43.092787 22502662377600 run.py:483] Algo bellman_ford step 1561 current loss 0.043673, current_train_items 49984.
I0304 19:28:43.116747 22502662377600 run.py:483] Algo bellman_ford step 1562 current loss 0.133129, current_train_items 50016.
I0304 19:28:43.146985 22502662377600 run.py:483] Algo bellman_ford step 1563 current loss 0.132691, current_train_items 50048.
I0304 19:28:43.180314 22502662377600 run.py:483] Algo bellman_ford step 1564 current loss 0.163347, current_train_items 50080.
I0304 19:28:43.199186 22502662377600 run.py:483] Algo bellman_ford step 1565 current loss 0.016007, current_train_items 50112.
I0304 19:28:43.215473 22502662377600 run.py:483] Algo bellman_ford step 1566 current loss 0.048124, current_train_items 50144.
I0304 19:28:43.238934 22502662377600 run.py:483] Algo bellman_ford step 1567 current loss 0.117566, current_train_items 50176.
I0304 19:28:43.268228 22502662377600 run.py:483] Algo bellman_ford step 1568 current loss 0.218951, current_train_items 50208.
I0304 19:28:43.302330 22502662377600 run.py:483] Algo bellman_ford step 1569 current loss 0.209320, current_train_items 50240.
I0304 19:28:43.321578 22502662377600 run.py:483] Algo bellman_ford step 1570 current loss 0.018525, current_train_items 50272.
I0304 19:28:43.338022 22502662377600 run.py:483] Algo bellman_ford step 1571 current loss 0.049576, current_train_items 50304.
I0304 19:28:43.362210 22502662377600 run.py:483] Algo bellman_ford step 1572 current loss 0.158256, current_train_items 50336.
I0304 19:28:43.391622 22502662377600 run.py:483] Algo bellman_ford step 1573 current loss 0.194482, current_train_items 50368.
I0304 19:28:43.422313 22502662377600 run.py:483] Algo bellman_ford step 1574 current loss 0.297852, current_train_items 50400.
I0304 19:28:43.442291 22502662377600 run.py:483] Algo bellman_ford step 1575 current loss 0.058479, current_train_items 50432.
I0304 19:28:43.458682 22502662377600 run.py:483] Algo bellman_ford step 1576 current loss 0.091984, current_train_items 50464.
I0304 19:28:43.481113 22502662377600 run.py:483] Algo bellman_ford step 1577 current loss 0.156786, current_train_items 50496.
I0304 19:28:43.511619 22502662377600 run.py:483] Algo bellman_ford step 1578 current loss 0.397447, current_train_items 50528.
I0304 19:28:43.544013 22502662377600 run.py:483] Algo bellman_ford step 1579 current loss 0.275697, current_train_items 50560.
I0304 19:28:43.563130 22502662377600 run.py:483] Algo bellman_ford step 1580 current loss 0.042298, current_train_items 50592.
I0304 19:28:43.578994 22502662377600 run.py:483] Algo bellman_ford step 1581 current loss 0.108437, current_train_items 50624.
I0304 19:28:43.601931 22502662377600 run.py:483] Algo bellman_ford step 1582 current loss 0.115820, current_train_items 50656.
I0304 19:28:43.631344 22502662377600 run.py:483] Algo bellman_ford step 1583 current loss 0.174308, current_train_items 50688.
I0304 19:28:43.664645 22502662377600 run.py:483] Algo bellman_ford step 1584 current loss 0.280038, current_train_items 50720.
I0304 19:28:43.684161 22502662377600 run.py:483] Algo bellman_ford step 1585 current loss 0.048241, current_train_items 50752.
I0304 19:28:43.699843 22502662377600 run.py:483] Algo bellman_ford step 1586 current loss 0.120064, current_train_items 50784.
I0304 19:28:43.723544 22502662377600 run.py:483] Algo bellman_ford step 1587 current loss 0.109204, current_train_items 50816.
I0304 19:28:43.752832 22502662377600 run.py:483] Algo bellman_ford step 1588 current loss 0.198032, current_train_items 50848.
I0304 19:28:43.785061 22502662377600 run.py:483] Algo bellman_ford step 1589 current loss 0.177287, current_train_items 50880.
I0304 19:28:43.804656 22502662377600 run.py:483] Algo bellman_ford step 1590 current loss 0.028474, current_train_items 50912.
I0304 19:28:43.820817 22502662377600 run.py:483] Algo bellman_ford step 1591 current loss 0.049480, current_train_items 50944.
I0304 19:28:43.843160 22502662377600 run.py:483] Algo bellman_ford step 1592 current loss 0.103124, current_train_items 50976.
I0304 19:28:43.873705 22502662377600 run.py:483] Algo bellman_ford step 1593 current loss 0.138886, current_train_items 51008.
I0304 19:28:43.907300 22502662377600 run.py:483] Algo bellman_ford step 1594 current loss 0.176850, current_train_items 51040.
I0304 19:28:43.926903 22502662377600 run.py:483] Algo bellman_ford step 1595 current loss 0.042420, current_train_items 51072.
I0304 19:28:43.942376 22502662377600 run.py:483] Algo bellman_ford step 1596 current loss 0.057182, current_train_items 51104.
I0304 19:28:43.966327 22502662377600 run.py:483] Algo bellman_ford step 1597 current loss 0.131979, current_train_items 51136.
I0304 19:28:43.994785 22502662377600 run.py:483] Algo bellman_ford step 1598 current loss 0.117119, current_train_items 51168.
I0304 19:28:44.027748 22502662377600 run.py:483] Algo bellman_ford step 1599 current loss 0.199884, current_train_items 51200.
I0304 19:28:44.047093 22502662377600 run.py:483] Algo bellman_ford step 1600 current loss 0.026226, current_train_items 51232.
I0304 19:28:44.054602 22502662377600 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0304 19:28:44.054705 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.977, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:28:44.071280 22502662377600 run.py:483] Algo bellman_ford step 1601 current loss 0.040340, current_train_items 51264.
I0304 19:28:44.096115 22502662377600 run.py:483] Algo bellman_ford step 1602 current loss 0.094497, current_train_items 51296.
I0304 19:28:44.126696 22502662377600 run.py:483] Algo bellman_ford step 1603 current loss 0.224127, current_train_items 51328.
I0304 19:28:44.160275 22502662377600 run.py:483] Algo bellman_ford step 1604 current loss 0.208437, current_train_items 51360.
I0304 19:28:44.180042 22502662377600 run.py:483] Algo bellman_ford step 1605 current loss 0.064600, current_train_items 51392.
I0304 19:28:44.195568 22502662377600 run.py:483] Algo bellman_ford step 1606 current loss 0.075758, current_train_items 51424.
I0304 19:28:44.219117 22502662377600 run.py:483] Algo bellman_ford step 1607 current loss 0.098326, current_train_items 51456.
I0304 19:28:44.249619 22502662377600 run.py:483] Algo bellman_ford step 1608 current loss 0.183779, current_train_items 51488.
I0304 19:28:44.283115 22502662377600 run.py:483] Algo bellman_ford step 1609 current loss 0.271388, current_train_items 51520.
I0304 19:28:44.302383 22502662377600 run.py:483] Algo bellman_ford step 1610 current loss 0.034608, current_train_items 51552.
I0304 19:28:44.318819 22502662377600 run.py:483] Algo bellman_ford step 1611 current loss 0.106092, current_train_items 51584.
I0304 19:28:44.342482 22502662377600 run.py:483] Algo bellman_ford step 1612 current loss 0.107425, current_train_items 51616.
I0304 19:28:44.372516 22502662377600 run.py:483] Algo bellman_ford step 1613 current loss 0.201009, current_train_items 51648.
I0304 19:28:44.404069 22502662377600 run.py:483] Algo bellman_ford step 1614 current loss 0.159610, current_train_items 51680.
I0304 19:28:44.423308 22502662377600 run.py:483] Algo bellman_ford step 1615 current loss 0.021825, current_train_items 51712.
I0304 19:28:44.439674 22502662377600 run.py:483] Algo bellman_ford step 1616 current loss 0.073521, current_train_items 51744.
I0304 19:28:44.463360 22502662377600 run.py:483] Algo bellman_ford step 1617 current loss 0.209186, current_train_items 51776.
I0304 19:28:44.492443 22502662377600 run.py:483] Algo bellman_ford step 1618 current loss 0.112991, current_train_items 51808.
I0304 19:28:44.526940 22502662377600 run.py:483] Algo bellman_ford step 1619 current loss 0.228937, current_train_items 51840.
I0304 19:28:44.546437 22502662377600 run.py:483] Algo bellman_ford step 1620 current loss 0.028050, current_train_items 51872.
I0304 19:28:44.562657 22502662377600 run.py:483] Algo bellman_ford step 1621 current loss 0.076718, current_train_items 51904.
I0304 19:28:44.586488 22502662377600 run.py:483] Algo bellman_ford step 1622 current loss 0.203817, current_train_items 51936.
I0304 19:28:44.616647 22502662377600 run.py:483] Algo bellman_ford step 1623 current loss 0.162276, current_train_items 51968.
I0304 19:28:44.649072 22502662377600 run.py:483] Algo bellman_ford step 1624 current loss 0.158459, current_train_items 52000.
I0304 19:28:44.668479 22502662377600 run.py:483] Algo bellman_ford step 1625 current loss 0.022518, current_train_items 52032.
I0304 19:28:44.684954 22502662377600 run.py:483] Algo bellman_ford step 1626 current loss 0.065208, current_train_items 52064.
I0304 19:28:44.708539 22502662377600 run.py:483] Algo bellman_ford step 1627 current loss 0.152553, current_train_items 52096.
I0304 19:28:44.739890 22502662377600 run.py:483] Algo bellman_ford step 1628 current loss 0.167882, current_train_items 52128.
I0304 19:28:44.772095 22502662377600 run.py:483] Algo bellman_ford step 1629 current loss 0.161255, current_train_items 52160.
I0304 19:28:44.791187 22502662377600 run.py:483] Algo bellman_ford step 1630 current loss 0.025113, current_train_items 52192.
I0304 19:28:44.808000 22502662377600 run.py:483] Algo bellman_ford step 1631 current loss 0.104402, current_train_items 52224.
I0304 19:28:44.832226 22502662377600 run.py:483] Algo bellman_ford step 1632 current loss 0.176697, current_train_items 52256.
I0304 19:28:44.861385 22502662377600 run.py:483] Algo bellman_ford step 1633 current loss 0.112875, current_train_items 52288.
I0304 19:28:44.894967 22502662377600 run.py:483] Algo bellman_ford step 1634 current loss 0.165872, current_train_items 52320.
I0304 19:28:44.913853 22502662377600 run.py:483] Algo bellman_ford step 1635 current loss 0.037502, current_train_items 52352.
I0304 19:28:44.930034 22502662377600 run.py:483] Algo bellman_ford step 1636 current loss 0.060134, current_train_items 52384.
I0304 19:28:44.954154 22502662377600 run.py:483] Algo bellman_ford step 1637 current loss 0.155805, current_train_items 52416.
I0304 19:28:44.984925 22502662377600 run.py:483] Algo bellman_ford step 1638 current loss 0.243543, current_train_items 52448.
I0304 19:28:45.019897 22502662377600 run.py:483] Algo bellman_ford step 1639 current loss 0.197808, current_train_items 52480.
I0304 19:28:45.039127 22502662377600 run.py:483] Algo bellman_ford step 1640 current loss 0.050597, current_train_items 52512.
I0304 19:28:45.054918 22502662377600 run.py:483] Algo bellman_ford step 1641 current loss 0.063716, current_train_items 52544.
I0304 19:28:45.078655 22502662377600 run.py:483] Algo bellman_ford step 1642 current loss 0.156293, current_train_items 52576.
I0304 19:28:45.108368 22502662377600 run.py:483] Algo bellman_ford step 1643 current loss 0.238047, current_train_items 52608.
I0304 19:28:45.140309 22502662377600 run.py:483] Algo bellman_ford step 1644 current loss 0.350733, current_train_items 52640.
I0304 19:28:45.159554 22502662377600 run.py:483] Algo bellman_ford step 1645 current loss 0.054101, current_train_items 52672.
I0304 19:28:45.175716 22502662377600 run.py:483] Algo bellman_ford step 1646 current loss 0.036174, current_train_items 52704.
I0304 19:28:45.200380 22502662377600 run.py:483] Algo bellman_ford step 1647 current loss 0.109272, current_train_items 52736.
I0304 19:28:45.229212 22502662377600 run.py:483] Algo bellman_ford step 1648 current loss 0.073743, current_train_items 52768.
I0304 19:28:45.263056 22502662377600 run.py:483] Algo bellman_ford step 1649 current loss 0.197157, current_train_items 52800.
I0304 19:28:45.282193 22502662377600 run.py:483] Algo bellman_ford step 1650 current loss 0.025808, current_train_items 52832.
I0304 19:28:45.290531 22502662377600 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0304 19:28:45.290633 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.977, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:28:45.320968 22502662377600 run.py:483] Algo bellman_ford step 1651 current loss 0.094649, current_train_items 52864.
I0304 19:28:45.345512 22502662377600 run.py:483] Algo bellman_ford step 1652 current loss 0.116134, current_train_items 52896.
I0304 19:28:45.375565 22502662377600 run.py:483] Algo bellman_ford step 1653 current loss 0.102836, current_train_items 52928.
I0304 19:28:45.407384 22502662377600 run.py:483] Algo bellman_ford step 1654 current loss 0.160811, current_train_items 52960.
I0304 19:28:45.426810 22502662377600 run.py:483] Algo bellman_ford step 1655 current loss 0.020342, current_train_items 52992.
I0304 19:28:45.442737 22502662377600 run.py:483] Algo bellman_ford step 1656 current loss 0.046254, current_train_items 53024.
I0304 19:28:45.466036 22502662377600 run.py:483] Algo bellman_ford step 1657 current loss 0.072277, current_train_items 53056.
I0304 19:28:45.494500 22502662377600 run.py:483] Algo bellman_ford step 1658 current loss 0.080872, current_train_items 53088.
I0304 19:28:45.525619 22502662377600 run.py:483] Algo bellman_ford step 1659 current loss 0.160819, current_train_items 53120.
I0304 19:28:45.544900 22502662377600 run.py:483] Algo bellman_ford step 1660 current loss 0.021534, current_train_items 53152.
I0304 19:28:45.561231 22502662377600 run.py:483] Algo bellman_ford step 1661 current loss 0.118026, current_train_items 53184.
I0304 19:28:45.583830 22502662377600 run.py:483] Algo bellman_ford step 1662 current loss 0.108248, current_train_items 53216.
I0304 19:28:45.614805 22502662377600 run.py:483] Algo bellman_ford step 1663 current loss 0.155278, current_train_items 53248.
I0304 19:28:45.648628 22502662377600 run.py:483] Algo bellman_ford step 1664 current loss 0.168614, current_train_items 53280.
I0304 19:28:45.667838 22502662377600 run.py:483] Algo bellman_ford step 1665 current loss 0.039937, current_train_items 53312.
I0304 19:28:45.683997 22502662377600 run.py:483] Algo bellman_ford step 1666 current loss 0.056622, current_train_items 53344.
I0304 19:28:45.706968 22502662377600 run.py:483] Algo bellman_ford step 1667 current loss 0.179165, current_train_items 53376.
I0304 19:28:45.735510 22502662377600 run.py:483] Algo bellman_ford step 1668 current loss 0.213730, current_train_items 53408.
I0304 19:28:45.766606 22502662377600 run.py:483] Algo bellman_ford step 1669 current loss 0.295378, current_train_items 53440.
I0304 19:28:45.786118 22502662377600 run.py:483] Algo bellman_ford step 1670 current loss 0.018008, current_train_items 53472.
I0304 19:28:45.802191 22502662377600 run.py:483] Algo bellman_ford step 1671 current loss 0.075459, current_train_items 53504.
I0304 19:28:45.824955 22502662377600 run.py:483] Algo bellman_ford step 1672 current loss 0.122726, current_train_items 53536.
I0304 19:28:45.855515 22502662377600 run.py:483] Algo bellman_ford step 1673 current loss 0.267334, current_train_items 53568.
I0304 19:28:45.888232 22502662377600 run.py:483] Algo bellman_ford step 1674 current loss 0.198019, current_train_items 53600.
I0304 19:28:45.907637 22502662377600 run.py:483] Algo bellman_ford step 1675 current loss 0.032325, current_train_items 53632.
I0304 19:28:45.923948 22502662377600 run.py:483] Algo bellman_ford step 1676 current loss 0.146567, current_train_items 53664.
I0304 19:28:45.946730 22502662377600 run.py:483] Algo bellman_ford step 1677 current loss 0.115241, current_train_items 53696.
I0304 19:28:45.976052 22502662377600 run.py:483] Algo bellman_ford step 1678 current loss 0.136325, current_train_items 53728.
I0304 19:28:46.009323 22502662377600 run.py:483] Algo bellman_ford step 1679 current loss 0.190027, current_train_items 53760.
I0304 19:28:46.028315 22502662377600 run.py:483] Algo bellman_ford step 1680 current loss 0.038741, current_train_items 53792.
I0304 19:28:46.044251 22502662377600 run.py:483] Algo bellman_ford step 1681 current loss 0.054048, current_train_items 53824.
I0304 19:28:46.067085 22502662377600 run.py:483] Algo bellman_ford step 1682 current loss 0.229152, current_train_items 53856.
I0304 19:28:46.095627 22502662377600 run.py:483] Algo bellman_ford step 1683 current loss 0.167926, current_train_items 53888.
I0304 19:28:46.128070 22502662377600 run.py:483] Algo bellman_ford step 1684 current loss 0.144065, current_train_items 53920.
I0304 19:28:46.147747 22502662377600 run.py:483] Algo bellman_ford step 1685 current loss 0.047884, current_train_items 53952.
I0304 19:28:46.164087 22502662377600 run.py:483] Algo bellman_ford step 1686 current loss 0.103560, current_train_items 53984.
I0304 19:28:46.187304 22502662377600 run.py:483] Algo bellman_ford step 1687 current loss 0.099854, current_train_items 54016.
I0304 19:28:46.216696 22502662377600 run.py:483] Algo bellman_ford step 1688 current loss 0.147063, current_train_items 54048.
I0304 19:28:46.251147 22502662377600 run.py:483] Algo bellman_ford step 1689 current loss 0.190964, current_train_items 54080.
I0304 19:28:46.270388 22502662377600 run.py:483] Algo bellman_ford step 1690 current loss 0.012755, current_train_items 54112.
I0304 19:28:46.287081 22502662377600 run.py:483] Algo bellman_ford step 1691 current loss 0.098599, current_train_items 54144.
I0304 19:28:46.310673 22502662377600 run.py:483] Algo bellman_ford step 1692 current loss 0.094349, current_train_items 54176.
I0304 19:28:46.341193 22502662377600 run.py:483] Algo bellman_ford step 1693 current loss 0.160342, current_train_items 54208.
I0304 19:28:46.373918 22502662377600 run.py:483] Algo bellman_ford step 1694 current loss 0.188510, current_train_items 54240.
I0304 19:28:46.392882 22502662377600 run.py:483] Algo bellman_ford step 1695 current loss 0.016867, current_train_items 54272.
I0304 19:28:46.409316 22502662377600 run.py:483] Algo bellman_ford step 1696 current loss 0.050554, current_train_items 54304.
I0304 19:28:46.430989 22502662377600 run.py:483] Algo bellman_ford step 1697 current loss 0.076980, current_train_items 54336.
I0304 19:28:46.459300 22502662377600 run.py:483] Algo bellman_ford step 1698 current loss 0.116064, current_train_items 54368.
I0304 19:28:46.492871 22502662377600 run.py:483] Algo bellman_ford step 1699 current loss 0.169560, current_train_items 54400.
I0304 19:28:46.512408 22502662377600 run.py:483] Algo bellman_ford step 1700 current loss 0.027377, current_train_items 54432.
I0304 19:28:46.520227 22502662377600 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.95703125, 'score': 0.95703125, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0304 19:28:46.520330 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.957, val scores are: bellman_ford: 0.957
I0304 19:28:46.537052 22502662377600 run.py:483] Algo bellman_ford step 1701 current loss 0.090379, current_train_items 54464.
I0304 19:28:46.560389 22502662377600 run.py:483] Algo bellman_ford step 1702 current loss 0.094902, current_train_items 54496.
I0304 19:28:46.590317 22502662377600 run.py:483] Algo bellman_ford step 1703 current loss 0.108042, current_train_items 54528.
I0304 19:28:46.622658 22502662377600 run.py:483] Algo bellman_ford step 1704 current loss 0.187836, current_train_items 54560.
I0304 19:28:46.642396 22502662377600 run.py:483] Algo bellman_ford step 1705 current loss 0.014885, current_train_items 54592.
I0304 19:28:46.657921 22502662377600 run.py:483] Algo bellman_ford step 1706 current loss 0.022292, current_train_items 54624.
I0304 19:28:46.681164 22502662377600 run.py:483] Algo bellman_ford step 1707 current loss 0.099489, current_train_items 54656.
I0304 19:28:46.710505 22502662377600 run.py:483] Algo bellman_ford step 1708 current loss 0.148768, current_train_items 54688.
I0304 19:28:46.744488 22502662377600 run.py:483] Algo bellman_ford step 1709 current loss 0.153169, current_train_items 54720.
I0304 19:28:46.763472 22502662377600 run.py:483] Algo bellman_ford step 1710 current loss 0.039648, current_train_items 54752.
I0304 19:28:46.779805 22502662377600 run.py:483] Algo bellman_ford step 1711 current loss 0.051862, current_train_items 54784.
I0304 19:28:46.803298 22502662377600 run.py:483] Algo bellman_ford step 1712 current loss 0.110586, current_train_items 54816.
I0304 19:28:46.832232 22502662377600 run.py:483] Algo bellman_ford step 1713 current loss 0.162976, current_train_items 54848.
I0304 19:28:46.864357 22502662377600 run.py:483] Algo bellman_ford step 1714 current loss 0.152188, current_train_items 54880.
I0304 19:28:46.883435 22502662377600 run.py:483] Algo bellman_ford step 1715 current loss 0.022741, current_train_items 54912.
I0304 19:28:46.899097 22502662377600 run.py:483] Algo bellman_ford step 1716 current loss 0.043462, current_train_items 54944.
I0304 19:28:46.922824 22502662377600 run.py:483] Algo bellman_ford step 1717 current loss 0.141551, current_train_items 54976.
I0304 19:28:46.950937 22502662377600 run.py:483] Algo bellman_ford step 1718 current loss 0.149655, current_train_items 55008.
I0304 19:28:46.983400 22502662377600 run.py:483] Algo bellman_ford step 1719 current loss 0.136456, current_train_items 55040.
I0304 19:28:47.002305 22502662377600 run.py:483] Algo bellman_ford step 1720 current loss 0.071147, current_train_items 55072.
I0304 19:28:47.018366 22502662377600 run.py:483] Algo bellman_ford step 1721 current loss 0.105636, current_train_items 55104.
I0304 19:28:47.041396 22502662377600 run.py:483] Algo bellman_ford step 1722 current loss 0.147029, current_train_items 55136.
I0304 19:28:47.070656 22502662377600 run.py:483] Algo bellman_ford step 1723 current loss 0.152288, current_train_items 55168.
I0304 19:28:47.102699 22502662377600 run.py:483] Algo bellman_ford step 1724 current loss 0.199985, current_train_items 55200.
I0304 19:28:47.121439 22502662377600 run.py:483] Algo bellman_ford step 1725 current loss 0.019486, current_train_items 55232.
I0304 19:28:47.137670 22502662377600 run.py:483] Algo bellman_ford step 1726 current loss 0.110040, current_train_items 55264.
I0304 19:28:47.162524 22502662377600 run.py:483] Algo bellman_ford step 1727 current loss 0.121606, current_train_items 55296.
I0304 19:28:47.191874 22502662377600 run.py:483] Algo bellman_ford step 1728 current loss 0.128300, current_train_items 55328.
I0304 19:28:47.224315 22502662377600 run.py:483] Algo bellman_ford step 1729 current loss 0.155033, current_train_items 55360.
I0304 19:28:47.243321 22502662377600 run.py:483] Algo bellman_ford step 1730 current loss 0.022223, current_train_items 55392.
I0304 19:28:47.259670 22502662377600 run.py:483] Algo bellman_ford step 1731 current loss 0.046345, current_train_items 55424.
I0304 19:28:47.283179 22502662377600 run.py:483] Algo bellman_ford step 1732 current loss 0.145245, current_train_items 55456.
I0304 19:28:47.312471 22502662377600 run.py:483] Algo bellman_ford step 1733 current loss 0.121882, current_train_items 55488.
I0304 19:28:47.342643 22502662377600 run.py:483] Algo bellman_ford step 1734 current loss 0.163861, current_train_items 55520.
I0304 19:28:47.361875 22502662377600 run.py:483] Algo bellman_ford step 1735 current loss 0.029154, current_train_items 55552.
I0304 19:28:47.378129 22502662377600 run.py:483] Algo bellman_ford step 1736 current loss 0.075673, current_train_items 55584.
I0304 19:28:47.401628 22502662377600 run.py:483] Algo bellman_ford step 1737 current loss 0.171061, current_train_items 55616.
I0304 19:28:47.431419 22502662377600 run.py:483] Algo bellman_ford step 1738 current loss 0.179159, current_train_items 55648.
I0304 19:28:47.464382 22502662377600 run.py:483] Algo bellman_ford step 1739 current loss 0.150463, current_train_items 55680.
I0304 19:28:47.483138 22502662377600 run.py:483] Algo bellman_ford step 1740 current loss 0.027940, current_train_items 55712.
I0304 19:28:47.499347 22502662377600 run.py:483] Algo bellman_ford step 1741 current loss 0.112348, current_train_items 55744.
I0304 19:28:47.522878 22502662377600 run.py:483] Algo bellman_ford step 1742 current loss 0.118639, current_train_items 55776.
I0304 19:28:47.553340 22502662377600 run.py:483] Algo bellman_ford step 1743 current loss 0.106576, current_train_items 55808.
I0304 19:28:47.584721 22502662377600 run.py:483] Algo bellman_ford step 1744 current loss 0.156768, current_train_items 55840.
I0304 19:28:47.603647 22502662377600 run.py:483] Algo bellman_ford step 1745 current loss 0.011345, current_train_items 55872.
I0304 19:28:47.619865 22502662377600 run.py:483] Algo bellman_ford step 1746 current loss 0.090790, current_train_items 55904.
I0304 19:28:47.642872 22502662377600 run.py:483] Algo bellman_ford step 1747 current loss 0.139946, current_train_items 55936.
I0304 19:28:47.672167 22502662377600 run.py:483] Algo bellman_ford step 1748 current loss 0.133863, current_train_items 55968.
I0304 19:28:47.703026 22502662377600 run.py:483] Algo bellman_ford step 1749 current loss 0.110072, current_train_items 56000.
I0304 19:28:47.722148 22502662377600 run.py:483] Algo bellman_ford step 1750 current loss 0.026783, current_train_items 56032.
I0304 19:28:47.730358 22502662377600 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0304 19:28:47.730471 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:28:47.747264 22502662377600 run.py:483] Algo bellman_ford step 1751 current loss 0.063089, current_train_items 56064.
I0304 19:28:47.770667 22502662377600 run.py:483] Algo bellman_ford step 1752 current loss 0.088342, current_train_items 56096.
I0304 19:28:47.801912 22502662377600 run.py:483] Algo bellman_ford step 1753 current loss 0.170107, current_train_items 56128.
I0304 19:28:47.835930 22502662377600 run.py:483] Algo bellman_ford step 1754 current loss 0.217854, current_train_items 56160.
I0304 19:28:47.855277 22502662377600 run.py:483] Algo bellman_ford step 1755 current loss 0.014035, current_train_items 56192.
I0304 19:28:47.871365 22502662377600 run.py:483] Algo bellman_ford step 1756 current loss 0.068924, current_train_items 56224.
I0304 19:28:47.895603 22502662377600 run.py:483] Algo bellman_ford step 1757 current loss 0.173325, current_train_items 56256.
I0304 19:28:47.926244 22502662377600 run.py:483] Algo bellman_ford step 1758 current loss 0.332448, current_train_items 56288.
I0304 19:28:47.956521 22502662377600 run.py:483] Algo bellman_ford step 1759 current loss 0.157626, current_train_items 56320.
I0304 19:28:47.975913 22502662377600 run.py:483] Algo bellman_ford step 1760 current loss 0.024384, current_train_items 56352.
I0304 19:28:47.992097 22502662377600 run.py:483] Algo bellman_ford step 1761 current loss 0.064570, current_train_items 56384.
I0304 19:28:48.015937 22502662377600 run.py:483] Algo bellman_ford step 1762 current loss 0.146715, current_train_items 56416.
I0304 19:28:48.047376 22502662377600 run.py:483] Algo bellman_ford step 1763 current loss 0.133041, current_train_items 56448.
I0304 19:28:48.081708 22502662377600 run.py:483] Algo bellman_ford step 1764 current loss 0.259374, current_train_items 56480.
I0304 19:28:48.100896 22502662377600 run.py:483] Algo bellman_ford step 1765 current loss 0.023689, current_train_items 56512.
I0304 19:28:48.116909 22502662377600 run.py:483] Algo bellman_ford step 1766 current loss 0.026591, current_train_items 56544.
I0304 19:28:48.139873 22502662377600 run.py:483] Algo bellman_ford step 1767 current loss 0.108427, current_train_items 56576.
I0304 19:28:48.169352 22502662377600 run.py:483] Algo bellman_ford step 1768 current loss 0.162531, current_train_items 56608.
I0304 19:28:48.200544 22502662377600 run.py:483] Algo bellman_ford step 1769 current loss 0.214170, current_train_items 56640.
I0304 19:28:48.220065 22502662377600 run.py:483] Algo bellman_ford step 1770 current loss 0.037953, current_train_items 56672.
I0304 19:28:48.236434 22502662377600 run.py:483] Algo bellman_ford step 1771 current loss 0.074953, current_train_items 56704.
I0304 19:28:48.260199 22502662377600 run.py:483] Algo bellman_ford step 1772 current loss 0.129515, current_train_items 56736.
I0304 19:28:48.290486 22502662377600 run.py:483] Algo bellman_ford step 1773 current loss 0.121967, current_train_items 56768.
I0304 19:28:48.321766 22502662377600 run.py:483] Algo bellman_ford step 1774 current loss 0.183198, current_train_items 56800.
I0304 19:28:48.341001 22502662377600 run.py:483] Algo bellman_ford step 1775 current loss 0.015354, current_train_items 56832.
I0304 19:28:48.357385 22502662377600 run.py:483] Algo bellman_ford step 1776 current loss 0.061520, current_train_items 56864.
I0304 19:28:48.380766 22502662377600 run.py:483] Algo bellman_ford step 1777 current loss 0.151783, current_train_items 56896.
I0304 19:28:48.410419 22502662377600 run.py:483] Algo bellman_ford step 1778 current loss 0.135440, current_train_items 56928.
I0304 19:28:48.441047 22502662377600 run.py:483] Algo bellman_ford step 1779 current loss 0.108820, current_train_items 56960.
I0304 19:28:48.459910 22502662377600 run.py:483] Algo bellman_ford step 1780 current loss 0.011442, current_train_items 56992.
I0304 19:28:48.475852 22502662377600 run.py:483] Algo bellman_ford step 1781 current loss 0.059233, current_train_items 57024.
I0304 19:28:48.498977 22502662377600 run.py:483] Algo bellman_ford step 1782 current loss 0.120132, current_train_items 57056.
I0304 19:28:48.529082 22502662377600 run.py:483] Algo bellman_ford step 1783 current loss 0.190134, current_train_items 57088.
I0304 19:28:48.560750 22502662377600 run.py:483] Algo bellman_ford step 1784 current loss 0.181860, current_train_items 57120.
I0304 19:28:48.580104 22502662377600 run.py:483] Algo bellman_ford step 1785 current loss 0.012220, current_train_items 57152.
I0304 19:28:48.596436 22502662377600 run.py:483] Algo bellman_ford step 1786 current loss 0.057162, current_train_items 57184.
I0304 19:28:48.618971 22502662377600 run.py:483] Algo bellman_ford step 1787 current loss 0.145907, current_train_items 57216.
I0304 19:28:48.647914 22502662377600 run.py:483] Algo bellman_ford step 1788 current loss 0.121638, current_train_items 57248.
I0304 19:28:48.680666 22502662377600 run.py:483] Algo bellman_ford step 1789 current loss 0.204234, current_train_items 57280.
I0304 19:28:48.699746 22502662377600 run.py:483] Algo bellman_ford step 1790 current loss 0.032884, current_train_items 57312.
I0304 19:28:48.716480 22502662377600 run.py:483] Algo bellman_ford step 1791 current loss 0.086291, current_train_items 57344.
I0304 19:28:48.740051 22502662377600 run.py:483] Algo bellman_ford step 1792 current loss 0.091293, current_train_items 57376.
I0304 19:28:48.768295 22502662377600 run.py:483] Algo bellman_ford step 1793 current loss 0.158765, current_train_items 57408.
I0304 19:28:48.800284 22502662377600 run.py:483] Algo bellman_ford step 1794 current loss 0.176252, current_train_items 57440.
I0304 19:28:48.819297 22502662377600 run.py:483] Algo bellman_ford step 1795 current loss 0.031109, current_train_items 57472.
I0304 19:28:48.835369 22502662377600 run.py:483] Algo bellman_ford step 1796 current loss 0.069222, current_train_items 57504.
I0304 19:28:48.859396 22502662377600 run.py:483] Algo bellman_ford step 1797 current loss 0.136049, current_train_items 57536.
I0304 19:28:48.889481 22502662377600 run.py:483] Algo bellman_ford step 1798 current loss 0.239016, current_train_items 57568.
I0304 19:28:48.922299 22502662377600 run.py:483] Algo bellman_ford step 1799 current loss 0.171611, current_train_items 57600.
I0304 19:28:48.941774 22502662377600 run.py:483] Algo bellman_ford step 1800 current loss 0.027201, current_train_items 57632.
I0304 19:28:48.949670 22502662377600 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0304 19:28:48.949772 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0304 19:28:48.966245 22502662377600 run.py:483] Algo bellman_ford step 1801 current loss 0.050186, current_train_items 57664.
I0304 19:28:48.990669 22502662377600 run.py:483] Algo bellman_ford step 1802 current loss 0.217312, current_train_items 57696.
I0304 19:28:49.019781 22502662377600 run.py:483] Algo bellman_ford step 1803 current loss 0.183719, current_train_items 57728.
I0304 19:28:49.051255 22502662377600 run.py:483] Algo bellman_ford step 1804 current loss 0.317648, current_train_items 57760.
I0304 19:28:49.070418 22502662377600 run.py:483] Algo bellman_ford step 1805 current loss 0.015814, current_train_items 57792.
I0304 19:28:49.086121 22502662377600 run.py:483] Algo bellman_ford step 1806 current loss 0.037058, current_train_items 57824.
I0304 19:28:49.109273 22502662377600 run.py:483] Algo bellman_ford step 1807 current loss 0.103273, current_train_items 57856.
I0304 19:28:49.137749 22502662377600 run.py:483] Algo bellman_ford step 1808 current loss 0.150528, current_train_items 57888.
I0304 19:28:49.170913 22502662377600 run.py:483] Algo bellman_ford step 1809 current loss 0.250422, current_train_items 57920.
I0304 19:28:49.189760 22502662377600 run.py:483] Algo bellman_ford step 1810 current loss 0.023077, current_train_items 57952.
I0304 19:28:49.205965 22502662377600 run.py:483] Algo bellman_ford step 1811 current loss 0.070559, current_train_items 57984.
I0304 19:28:49.229559 22502662377600 run.py:483] Algo bellman_ford step 1812 current loss 0.147506, current_train_items 58016.
I0304 19:28:49.257004 22502662377600 run.py:483] Algo bellman_ford step 1813 current loss 0.149404, current_train_items 58048.
I0304 19:28:49.289167 22502662377600 run.py:483] Algo bellman_ford step 1814 current loss 0.157834, current_train_items 58080.
I0304 19:28:49.308138 22502662377600 run.py:483] Algo bellman_ford step 1815 current loss 0.031116, current_train_items 58112.
I0304 19:28:49.324677 22502662377600 run.py:483] Algo bellman_ford step 1816 current loss 0.140719, current_train_items 58144.
I0304 19:28:49.347567 22502662377600 run.py:483] Algo bellman_ford step 1817 current loss 0.117337, current_train_items 58176.
I0304 19:28:49.376827 22502662377600 run.py:483] Algo bellman_ford step 1818 current loss 0.141933, current_train_items 58208.
I0304 19:28:49.409219 22502662377600 run.py:483] Algo bellman_ford step 1819 current loss 0.157712, current_train_items 58240.
I0304 19:28:49.428153 22502662377600 run.py:483] Algo bellman_ford step 1820 current loss 0.055801, current_train_items 58272.
I0304 19:28:49.444687 22502662377600 run.py:483] Algo bellman_ford step 1821 current loss 0.083550, current_train_items 58304.
I0304 19:28:49.469959 22502662377600 run.py:483] Algo bellman_ford step 1822 current loss 0.122045, current_train_items 58336.
I0304 19:28:49.499879 22502662377600 run.py:483] Algo bellman_ford step 1823 current loss 0.132022, current_train_items 58368.
I0304 19:28:49.530793 22502662377600 run.py:483] Algo bellman_ford step 1824 current loss 0.214749, current_train_items 58400.
I0304 19:28:49.549587 22502662377600 run.py:483] Algo bellman_ford step 1825 current loss 0.011232, current_train_items 58432.
I0304 19:28:49.565825 22502662377600 run.py:483] Algo bellman_ford step 1826 current loss 0.079246, current_train_items 58464.
I0304 19:28:49.589097 22502662377600 run.py:483] Algo bellman_ford step 1827 current loss 0.078096, current_train_items 58496.
I0304 19:28:49.619880 22502662377600 run.py:483] Algo bellman_ford step 1828 current loss 0.138402, current_train_items 58528.
I0304 19:28:49.654475 22502662377600 run.py:483] Algo bellman_ford step 1829 current loss 0.186302, current_train_items 58560.
I0304 19:28:49.673480 22502662377600 run.py:483] Algo bellman_ford step 1830 current loss 0.020182, current_train_items 58592.
I0304 19:28:49.689827 22502662377600 run.py:483] Algo bellman_ford step 1831 current loss 0.101005, current_train_items 58624.
I0304 19:28:49.714731 22502662377600 run.py:483] Algo bellman_ford step 1832 current loss 0.099111, current_train_items 58656.
I0304 19:28:49.744332 22502662377600 run.py:483] Algo bellman_ford step 1833 current loss 0.118661, current_train_items 58688.
I0304 19:28:49.775909 22502662377600 run.py:483] Algo bellman_ford step 1834 current loss 0.110326, current_train_items 58720.
I0304 19:28:49.794758 22502662377600 run.py:483] Algo bellman_ford step 1835 current loss 0.011486, current_train_items 58752.
I0304 19:28:49.811046 22502662377600 run.py:483] Algo bellman_ford step 1836 current loss 0.051698, current_train_items 58784.
I0304 19:28:49.835591 22502662377600 run.py:483] Algo bellman_ford step 1837 current loss 0.119878, current_train_items 58816.
I0304 19:28:49.864612 22502662377600 run.py:483] Algo bellman_ford step 1838 current loss 0.128692, current_train_items 58848.
I0304 19:28:49.897360 22502662377600 run.py:483] Algo bellman_ford step 1839 current loss 0.176035, current_train_items 58880.
I0304 19:28:49.916225 22502662377600 run.py:483] Algo bellman_ford step 1840 current loss 0.013573, current_train_items 58912.
I0304 19:28:49.932412 22502662377600 run.py:483] Algo bellman_ford step 1841 current loss 0.055045, current_train_items 58944.
I0304 19:28:49.956288 22502662377600 run.py:483] Algo bellman_ford step 1842 current loss 0.075834, current_train_items 58976.
I0304 19:28:49.986489 22502662377600 run.py:483] Algo bellman_ford step 1843 current loss 0.110179, current_train_items 59008.
I0304 19:28:50.018811 22502662377600 run.py:483] Algo bellman_ford step 1844 current loss 0.109032, current_train_items 59040.
I0304 19:28:50.037854 22502662377600 run.py:483] Algo bellman_ford step 1845 current loss 0.015910, current_train_items 59072.
I0304 19:28:50.053853 22502662377600 run.py:483] Algo bellman_ford step 1846 current loss 0.033392, current_train_items 59104.
I0304 19:28:50.077554 22502662377600 run.py:483] Algo bellman_ford step 1847 current loss 0.099862, current_train_items 59136.
I0304 19:28:50.107172 22502662377600 run.py:483] Algo bellman_ford step 1848 current loss 0.189576, current_train_items 59168.
I0304 19:28:50.140747 22502662377600 run.py:483] Algo bellman_ford step 1849 current loss 0.162221, current_train_items 59200.
I0304 19:28:50.159909 22502662377600 run.py:483] Algo bellman_ford step 1850 current loss 0.024663, current_train_items 59232.
I0304 19:28:50.168039 22502662377600 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0304 19:28:50.168145 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:28:50.185122 22502662377600 run.py:483] Algo bellman_ford step 1851 current loss 0.054362, current_train_items 59264.
I0304 19:28:50.209473 22502662377600 run.py:483] Algo bellman_ford step 1852 current loss 0.158358, current_train_items 59296.
I0304 19:28:50.239511 22502662377600 run.py:483] Algo bellman_ford step 1853 current loss 0.166997, current_train_items 59328.
I0304 19:28:50.272662 22502662377600 run.py:483] Algo bellman_ford step 1854 current loss 0.228933, current_train_items 59360.
I0304 19:28:50.291966 22502662377600 run.py:483] Algo bellman_ford step 1855 current loss 0.027624, current_train_items 59392.
I0304 19:28:50.307354 22502662377600 run.py:483] Algo bellman_ford step 1856 current loss 0.042303, current_train_items 59424.
I0304 19:28:50.331121 22502662377600 run.py:483] Algo bellman_ford step 1857 current loss 0.130385, current_train_items 59456.
I0304 19:28:50.359655 22502662377600 run.py:483] Algo bellman_ford step 1858 current loss 0.093526, current_train_items 59488.
I0304 19:28:50.392378 22502662377600 run.py:483] Algo bellman_ford step 1859 current loss 0.222007, current_train_items 59520.
I0304 19:28:50.411859 22502662377600 run.py:483] Algo bellman_ford step 1860 current loss 0.022979, current_train_items 59552.
I0304 19:28:50.428489 22502662377600 run.py:483] Algo bellman_ford step 1861 current loss 0.059942, current_train_items 59584.
I0304 19:28:50.451569 22502662377600 run.py:483] Algo bellman_ford step 1862 current loss 0.129304, current_train_items 59616.
I0304 19:28:50.481406 22502662377600 run.py:483] Algo bellman_ford step 1863 current loss 0.126421, current_train_items 59648.
I0304 19:28:50.514603 22502662377600 run.py:483] Algo bellman_ford step 1864 current loss 0.251313, current_train_items 59680.
I0304 19:28:50.533615 22502662377600 run.py:483] Algo bellman_ford step 1865 current loss 0.020342, current_train_items 59712.
I0304 19:28:50.549777 22502662377600 run.py:483] Algo bellman_ford step 1866 current loss 0.043025, current_train_items 59744.
I0304 19:28:50.574395 22502662377600 run.py:483] Algo bellman_ford step 1867 current loss 0.078666, current_train_items 59776.
I0304 19:28:50.604097 22502662377600 run.py:483] Algo bellman_ford step 1868 current loss 0.118324, current_train_items 59808.
I0304 19:28:50.636206 22502662377600 run.py:483] Algo bellman_ford step 1869 current loss 0.153378, current_train_items 59840.
I0304 19:28:50.655665 22502662377600 run.py:483] Algo bellman_ford step 1870 current loss 0.011676, current_train_items 59872.
I0304 19:28:50.671894 22502662377600 run.py:483] Algo bellman_ford step 1871 current loss 0.026630, current_train_items 59904.
I0304 19:28:50.694484 22502662377600 run.py:483] Algo bellman_ford step 1872 current loss 0.051924, current_train_items 59936.
I0304 19:28:50.724096 22502662377600 run.py:483] Algo bellman_ford step 1873 current loss 0.109563, current_train_items 59968.
I0304 19:28:50.756108 22502662377600 run.py:483] Algo bellman_ford step 1874 current loss 0.126049, current_train_items 60000.
I0304 19:28:50.775546 22502662377600 run.py:483] Algo bellman_ford step 1875 current loss 0.036929, current_train_items 60032.
I0304 19:28:50.791656 22502662377600 run.py:483] Algo bellman_ford step 1876 current loss 0.026068, current_train_items 60064.
I0304 19:28:50.815035 22502662377600 run.py:483] Algo bellman_ford step 1877 current loss 0.131036, current_train_items 60096.
I0304 19:28:50.843818 22502662377600 run.py:483] Algo bellman_ford step 1878 current loss 0.082869, current_train_items 60128.
I0304 19:28:50.878959 22502662377600 run.py:483] Algo bellman_ford step 1879 current loss 0.255780, current_train_items 60160.
I0304 19:28:50.897957 22502662377600 run.py:483] Algo bellman_ford step 1880 current loss 0.015155, current_train_items 60192.
I0304 19:28:50.914415 22502662377600 run.py:483] Algo bellman_ford step 1881 current loss 0.046794, current_train_items 60224.
I0304 19:28:50.937384 22502662377600 run.py:483] Algo bellman_ford step 1882 current loss 0.098653, current_train_items 60256.
I0304 19:28:50.967419 22502662377600 run.py:483] Algo bellman_ford step 1883 current loss 0.132227, current_train_items 60288.
I0304 19:28:50.999852 22502662377600 run.py:483] Algo bellman_ford step 1884 current loss 0.206495, current_train_items 60320.
I0304 19:28:51.018842 22502662377600 run.py:483] Algo bellman_ford step 1885 current loss 0.041020, current_train_items 60352.
I0304 19:28:51.035376 22502662377600 run.py:483] Algo bellman_ford step 1886 current loss 0.071209, current_train_items 60384.
I0304 19:28:51.058540 22502662377600 run.py:483] Algo bellman_ford step 1887 current loss 0.121392, current_train_items 60416.
I0304 19:28:51.087150 22502662377600 run.py:483] Algo bellman_ford step 1888 current loss 0.114641, current_train_items 60448.
I0304 19:28:51.121293 22502662377600 run.py:483] Algo bellman_ford step 1889 current loss 0.200009, current_train_items 60480.
I0304 19:28:51.140384 22502662377600 run.py:483] Algo bellman_ford step 1890 current loss 0.010083, current_train_items 60512.
I0304 19:28:51.156777 22502662377600 run.py:483] Algo bellman_ford step 1891 current loss 0.089808, current_train_items 60544.
I0304 19:28:51.179271 22502662377600 run.py:483] Algo bellman_ford step 1892 current loss 0.085312, current_train_items 60576.
I0304 19:28:51.209339 22502662377600 run.py:483] Algo bellman_ford step 1893 current loss 0.129838, current_train_items 60608.
I0304 19:28:51.244705 22502662377600 run.py:483] Algo bellman_ford step 1894 current loss 0.158783, current_train_items 60640.
I0304 19:28:51.263630 22502662377600 run.py:483] Algo bellman_ford step 1895 current loss 0.032287, current_train_items 60672.
I0304 19:28:51.279684 22502662377600 run.py:483] Algo bellman_ford step 1896 current loss 0.076059, current_train_items 60704.
I0304 19:28:51.303117 22502662377600 run.py:483] Algo bellman_ford step 1897 current loss 0.246730, current_train_items 60736.
I0304 19:28:51.332909 22502662377600 run.py:483] Algo bellman_ford step 1898 current loss 0.307770, current_train_items 60768.
I0304 19:28:51.365838 22502662377600 run.py:483] Algo bellman_ford step 1899 current loss 0.355286, current_train_items 60800.
I0304 19:28:51.385104 22502662377600 run.py:483] Algo bellman_ford step 1900 current loss 0.019074, current_train_items 60832.
I0304 19:28:51.393081 22502662377600 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0304 19:28:51.393186 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:28:51.409785 22502662377600 run.py:483] Algo bellman_ford step 1901 current loss 0.033429, current_train_items 60864.
I0304 19:28:51.433125 22502662377600 run.py:483] Algo bellman_ford step 1902 current loss 0.081986, current_train_items 60896.
I0304 19:28:51.461253 22502662377600 run.py:483] Algo bellman_ford step 1903 current loss 0.117484, current_train_items 60928.
I0304 19:28:51.493703 22502662377600 run.py:483] Algo bellman_ford step 1904 current loss 0.163806, current_train_items 60960.
I0304 19:28:51.513010 22502662377600 run.py:483] Algo bellman_ford step 1905 current loss 0.019650, current_train_items 60992.
I0304 19:28:51.528709 22502662377600 run.py:483] Algo bellman_ford step 1906 current loss 0.054395, current_train_items 61024.
I0304 19:28:51.551565 22502662377600 run.py:483] Algo bellman_ford step 1907 current loss 0.080215, current_train_items 61056.
I0304 19:28:51.580827 22502662377600 run.py:483] Algo bellman_ford step 1908 current loss 0.154537, current_train_items 61088.
I0304 19:28:51.612711 22502662377600 run.py:483] Algo bellman_ford step 1909 current loss 0.199061, current_train_items 61120.
I0304 19:28:51.631960 22502662377600 run.py:483] Algo bellman_ford step 1910 current loss 0.049399, current_train_items 61152.
I0304 19:28:51.648246 22502662377600 run.py:483] Algo bellman_ford step 1911 current loss 0.123497, current_train_items 61184.
I0304 19:28:51.672185 22502662377600 run.py:483] Algo bellman_ford step 1912 current loss 0.122545, current_train_items 61216.
I0304 19:28:51.701309 22502662377600 run.py:483] Algo bellman_ford step 1913 current loss 0.116405, current_train_items 61248.
I0304 19:28:51.734554 22502662377600 run.py:483] Algo bellman_ford step 1914 current loss 0.138532, current_train_items 61280.
I0304 19:28:51.753378 22502662377600 run.py:483] Algo bellman_ford step 1915 current loss 0.011351, current_train_items 61312.
I0304 19:28:51.769611 22502662377600 run.py:483] Algo bellman_ford step 1916 current loss 0.024386, current_train_items 61344.
I0304 19:28:51.793192 22502662377600 run.py:483] Algo bellman_ford step 1917 current loss 0.085879, current_train_items 61376.
I0304 19:28:51.823189 22502662377600 run.py:483] Algo bellman_ford step 1918 current loss 0.093187, current_train_items 61408.
I0304 19:28:51.856804 22502662377600 run.py:483] Algo bellman_ford step 1919 current loss 0.173210, current_train_items 61440.
I0304 19:28:51.875660 22502662377600 run.py:483] Algo bellman_ford step 1920 current loss 0.027046, current_train_items 61472.
I0304 19:28:51.891927 22502662377600 run.py:483] Algo bellman_ford step 1921 current loss 0.058731, current_train_items 61504.
I0304 19:28:51.915020 22502662377600 run.py:483] Algo bellman_ford step 1922 current loss 0.081169, current_train_items 61536.
I0304 19:28:51.943566 22502662377600 run.py:483] Algo bellman_ford step 1923 current loss 0.086191, current_train_items 61568.
I0304 19:28:51.976001 22502662377600 run.py:483] Algo bellman_ford step 1924 current loss 0.143516, current_train_items 61600.
I0304 19:28:51.994788 22502662377600 run.py:483] Algo bellman_ford step 1925 current loss 0.012690, current_train_items 61632.
I0304 19:28:52.011124 22502662377600 run.py:483] Algo bellman_ford step 1926 current loss 0.109455, current_train_items 61664.
I0304 19:28:52.033450 22502662377600 run.py:483] Algo bellman_ford step 1927 current loss 0.063176, current_train_items 61696.
I0304 19:28:52.062535 22502662377600 run.py:483] Algo bellman_ford step 1928 current loss 0.116039, current_train_items 61728.
I0304 19:28:52.093381 22502662377600 run.py:483] Algo bellman_ford step 1929 current loss 0.157096, current_train_items 61760.
I0304 19:28:52.112522 22502662377600 run.py:483] Algo bellman_ford step 1930 current loss 0.039088, current_train_items 61792.
I0304 19:28:52.128113 22502662377600 run.py:483] Algo bellman_ford step 1931 current loss 0.054488, current_train_items 61824.
I0304 19:28:52.152493 22502662377600 run.py:483] Algo bellman_ford step 1932 current loss 0.179776, current_train_items 61856.
I0304 19:28:52.181826 22502662377600 run.py:483] Algo bellman_ford step 1933 current loss 0.228001, current_train_items 61888.
I0304 19:28:52.212470 22502662377600 run.py:483] Algo bellman_ford step 1934 current loss 0.119347, current_train_items 61920.
I0304 19:28:52.231324 22502662377600 run.py:483] Algo bellman_ford step 1935 current loss 0.016906, current_train_items 61952.
I0304 19:28:52.247532 22502662377600 run.py:483] Algo bellman_ford step 1936 current loss 0.103945, current_train_items 61984.
I0304 19:28:52.271601 22502662377600 run.py:483] Algo bellman_ford step 1937 current loss 0.245139, current_train_items 62016.
I0304 19:28:52.300337 22502662377600 run.py:483] Algo bellman_ford step 1938 current loss 0.212564, current_train_items 62048.
I0304 19:28:52.333926 22502662377600 run.py:483] Algo bellman_ford step 1939 current loss 0.204454, current_train_items 62080.
I0304 19:28:52.353072 22502662377600 run.py:483] Algo bellman_ford step 1940 current loss 0.035266, current_train_items 62112.
I0304 19:28:52.369451 22502662377600 run.py:483] Algo bellman_ford step 1941 current loss 0.051468, current_train_items 62144.
I0304 19:28:52.392444 22502662377600 run.py:483] Algo bellman_ford step 1942 current loss 0.165024, current_train_items 62176.
I0304 19:28:52.420921 22502662377600 run.py:483] Algo bellman_ford step 1943 current loss 0.127155, current_train_items 62208.
I0304 19:28:52.453531 22502662377600 run.py:483] Algo bellman_ford step 1944 current loss 0.166277, current_train_items 62240.
I0304 19:28:52.472537 22502662377600 run.py:483] Algo bellman_ford step 1945 current loss 0.055802, current_train_items 62272.
I0304 19:28:52.488786 22502662377600 run.py:483] Algo bellman_ford step 1946 current loss 0.135888, current_train_items 62304.
I0304 19:28:52.512284 22502662377600 run.py:483] Algo bellman_ford step 1947 current loss 0.148205, current_train_items 62336.
I0304 19:28:52.542730 22502662377600 run.py:483] Algo bellman_ford step 1948 current loss 0.200784, current_train_items 62368.
I0304 19:28:52.574487 22502662377600 run.py:483] Algo bellman_ford step 1949 current loss 0.194621, current_train_items 62400.
I0304 19:28:52.593509 22502662377600 run.py:483] Algo bellman_ford step 1950 current loss 0.041770, current_train_items 62432.
I0304 19:28:52.601491 22502662377600 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0304 19:28:52.601598 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0304 19:28:52.618647 22502662377600 run.py:483] Algo bellman_ford step 1951 current loss 0.133307, current_train_items 62464.
I0304 19:28:52.642945 22502662377600 run.py:483] Algo bellman_ford step 1952 current loss 0.170896, current_train_items 62496.
I0304 19:28:52.674315 22502662377600 run.py:483] Algo bellman_ford step 1953 current loss 0.156598, current_train_items 62528.
I0304 19:28:52.706640 22502662377600 run.py:483] Algo bellman_ford step 1954 current loss 0.160117, current_train_items 62560.
I0304 19:28:52.725940 22502662377600 run.py:483] Algo bellman_ford step 1955 current loss 0.035604, current_train_items 62592.
I0304 19:28:52.741316 22502662377600 run.py:483] Algo bellman_ford step 1956 current loss 0.017359, current_train_items 62624.
I0304 19:28:52.764917 22502662377600 run.py:483] Algo bellman_ford step 1957 current loss 0.144018, current_train_items 62656.
I0304 19:28:52.795094 22502662377600 run.py:483] Algo bellman_ford step 1958 current loss 0.131824, current_train_items 62688.
I0304 19:28:52.828079 22502662377600 run.py:483] Algo bellman_ford step 1959 current loss 0.132588, current_train_items 62720.
I0304 19:28:52.847392 22502662377600 run.py:483] Algo bellman_ford step 1960 current loss 0.016001, current_train_items 62752.
I0304 19:28:52.863844 22502662377600 run.py:483] Algo bellman_ford step 1961 current loss 0.069165, current_train_items 62784.
I0304 19:28:52.886621 22502662377600 run.py:483] Algo bellman_ford step 1962 current loss 0.109832, current_train_items 62816.
I0304 19:28:52.915048 22502662377600 run.py:483] Algo bellman_ford step 1963 current loss 0.113612, current_train_items 62848.
I0304 19:28:52.949964 22502662377600 run.py:483] Algo bellman_ford step 1964 current loss 0.199380, current_train_items 62880.
I0304 19:28:52.969100 22502662377600 run.py:483] Algo bellman_ford step 1965 current loss 0.012366, current_train_items 62912.
I0304 19:28:52.985122 22502662377600 run.py:483] Algo bellman_ford step 1966 current loss 0.060784, current_train_items 62944.
I0304 19:28:53.008473 22502662377600 run.py:483] Algo bellman_ford step 1967 current loss 0.074468, current_train_items 62976.
I0304 19:28:53.037386 22502662377600 run.py:483] Algo bellman_ford step 1968 current loss 0.115967, current_train_items 63008.
I0304 19:28:53.069106 22502662377600 run.py:483] Algo bellman_ford step 1969 current loss 0.143614, current_train_items 63040.
I0304 19:28:53.088357 22502662377600 run.py:483] Algo bellman_ford step 1970 current loss 0.009290, current_train_items 63072.
I0304 19:28:53.104290 22502662377600 run.py:483] Algo bellman_ford step 1971 current loss 0.069865, current_train_items 63104.
I0304 19:28:53.126740 22502662377600 run.py:483] Algo bellman_ford step 1972 current loss 0.104910, current_train_items 63136.
I0304 19:28:53.156034 22502662377600 run.py:483] Algo bellman_ford step 1973 current loss 0.197111, current_train_items 63168.
I0304 19:28:53.191530 22502662377600 run.py:483] Algo bellman_ford step 1974 current loss 0.273690, current_train_items 63200.
I0304 19:28:53.210873 22502662377600 run.py:483] Algo bellman_ford step 1975 current loss 0.019299, current_train_items 63232.
I0304 19:28:53.226747 22502662377600 run.py:483] Algo bellman_ford step 1976 current loss 0.109187, current_train_items 63264.
I0304 19:28:53.248773 22502662377600 run.py:483] Algo bellman_ford step 1977 current loss 0.112597, current_train_items 63296.
I0304 19:28:53.277644 22502662377600 run.py:483] Algo bellman_ford step 1978 current loss 0.190386, current_train_items 63328.
I0304 19:28:53.310847 22502662377600 run.py:483] Algo bellman_ford step 1979 current loss 0.179458, current_train_items 63360.
I0304 19:28:53.329794 22502662377600 run.py:483] Algo bellman_ford step 1980 current loss 0.015263, current_train_items 63392.
I0304 19:28:53.346122 22502662377600 run.py:483] Algo bellman_ford step 1981 current loss 0.035475, current_train_items 63424.
I0304 19:28:53.369299 22502662377600 run.py:483] Algo bellman_ford step 1982 current loss 0.130045, current_train_items 63456.
I0304 19:28:53.397481 22502662377600 run.py:483] Algo bellman_ford step 1983 current loss 0.089975, current_train_items 63488.
I0304 19:28:53.432680 22502662377600 run.py:483] Algo bellman_ford step 1984 current loss 0.176709, current_train_items 63520.
I0304 19:28:53.451810 22502662377600 run.py:483] Algo bellman_ford step 1985 current loss 0.014605, current_train_items 63552.
I0304 19:28:53.468024 22502662377600 run.py:483] Algo bellman_ford step 1986 current loss 0.046467, current_train_items 63584.
I0304 19:28:53.490100 22502662377600 run.py:483] Algo bellman_ford step 1987 current loss 0.090786, current_train_items 63616.
I0304 19:28:53.518575 22502662377600 run.py:483] Algo bellman_ford step 1988 current loss 0.134156, current_train_items 63648.
I0304 19:28:53.551877 22502662377600 run.py:483] Algo bellman_ford step 1989 current loss 0.193002, current_train_items 63680.
I0304 19:28:53.570937 22502662377600 run.py:483] Algo bellman_ford step 1990 current loss 0.021532, current_train_items 63712.
I0304 19:28:53.587158 22502662377600 run.py:483] Algo bellman_ford step 1991 current loss 0.056163, current_train_items 63744.
I0304 19:28:53.610270 22502662377600 run.py:483] Algo bellman_ford step 1992 current loss 0.089528, current_train_items 63776.
I0304 19:28:53.639577 22502662377600 run.py:483] Algo bellman_ford step 1993 current loss 0.099383, current_train_items 63808.
I0304 19:28:53.667875 22502662377600 run.py:483] Algo bellman_ford step 1994 current loss 0.093158, current_train_items 63840.
I0304 19:28:53.686867 22502662377600 run.py:483] Algo bellman_ford step 1995 current loss 0.037695, current_train_items 63872.
I0304 19:28:53.703565 22502662377600 run.py:483] Algo bellman_ford step 1996 current loss 0.062529, current_train_items 63904.
I0304 19:28:53.726574 22502662377600 run.py:483] Algo bellman_ford step 1997 current loss 0.097899, current_train_items 63936.
I0304 19:28:53.757320 22502662377600 run.py:483] Algo bellman_ford step 1998 current loss 0.137857, current_train_items 63968.
I0304 19:28:53.788813 22502662377600 run.py:483] Algo bellman_ford step 1999 current loss 0.139916, current_train_items 64000.
I0304 19:28:53.808021 22502662377600 run.py:483] Algo bellman_ford step 2000 current loss 0.016118, current_train_items 64032.
I0304 19:28:53.815874 22502662377600 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0304 19:28:53.815979 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:28:53.832412 22502662377600 run.py:483] Algo bellman_ford step 2001 current loss 0.050858, current_train_items 64064.
I0304 19:28:53.857227 22502662377600 run.py:483] Algo bellman_ford step 2002 current loss 0.136702, current_train_items 64096.
I0304 19:28:53.887148 22502662377600 run.py:483] Algo bellman_ford step 2003 current loss 0.084969, current_train_items 64128.
I0304 19:28:53.919998 22502662377600 run.py:483] Algo bellman_ford step 2004 current loss 0.136101, current_train_items 64160.
I0304 19:28:53.939360 22502662377600 run.py:483] Algo bellman_ford step 2005 current loss 0.029157, current_train_items 64192.
I0304 19:28:53.955434 22502662377600 run.py:483] Algo bellman_ford step 2006 current loss 0.078767, current_train_items 64224.
I0304 19:28:53.978780 22502662377600 run.py:483] Algo bellman_ford step 2007 current loss 0.073653, current_train_items 64256.
I0304 19:28:54.009210 22502662377600 run.py:483] Algo bellman_ford step 2008 current loss 0.153480, current_train_items 64288.
I0304 19:28:54.041178 22502662377600 run.py:483] Algo bellman_ford step 2009 current loss 0.110561, current_train_items 64320.
I0304 19:28:54.060765 22502662377600 run.py:483] Algo bellman_ford step 2010 current loss 0.013232, current_train_items 64352.
I0304 19:28:54.077042 22502662377600 run.py:483] Algo bellman_ford step 2011 current loss 0.064205, current_train_items 64384.
I0304 19:28:54.100636 22502662377600 run.py:483] Algo bellman_ford step 2012 current loss 0.157035, current_train_items 64416.
I0304 19:28:54.129886 22502662377600 run.py:483] Algo bellman_ford step 2013 current loss 0.218045, current_train_items 64448.
I0304 19:28:54.162492 22502662377600 run.py:483] Algo bellman_ford step 2014 current loss 0.219560, current_train_items 64480.
I0304 19:28:54.181306 22502662377600 run.py:483] Algo bellman_ford step 2015 current loss 0.020314, current_train_items 64512.
I0304 19:28:54.197927 22502662377600 run.py:483] Algo bellman_ford step 2016 current loss 0.058319, current_train_items 64544.
I0304 19:28:54.221723 22502662377600 run.py:483] Algo bellman_ford step 2017 current loss 0.187610, current_train_items 64576.
I0304 19:28:54.249924 22502662377600 run.py:483] Algo bellman_ford step 2018 current loss 0.126445, current_train_items 64608.
I0304 19:28:54.283821 22502662377600 run.py:483] Algo bellman_ford step 2019 current loss 0.241467, current_train_items 64640.
I0304 19:28:54.302943 22502662377600 run.py:483] Algo bellman_ford step 2020 current loss 0.017583, current_train_items 64672.
I0304 19:28:54.319118 22502662377600 run.py:483] Algo bellman_ford step 2021 current loss 0.038444, current_train_items 64704.
I0304 19:28:54.342926 22502662377600 run.py:483] Algo bellman_ford step 2022 current loss 0.122338, current_train_items 64736.
I0304 19:28:54.372716 22502662377600 run.py:483] Algo bellman_ford step 2023 current loss 0.123591, current_train_items 64768.
I0304 19:28:54.408122 22502662377600 run.py:483] Algo bellman_ford step 2024 current loss 0.180819, current_train_items 64800.
I0304 19:28:54.427565 22502662377600 run.py:483] Algo bellman_ford step 2025 current loss 0.022969, current_train_items 64832.
I0304 19:28:54.443428 22502662377600 run.py:483] Algo bellman_ford step 2026 current loss 0.077433, current_train_items 64864.
I0304 19:28:54.467194 22502662377600 run.py:483] Algo bellman_ford step 2027 current loss 0.101703, current_train_items 64896.
I0304 19:28:54.496687 22502662377600 run.py:483] Algo bellman_ford step 2028 current loss 0.087759, current_train_items 64928.
I0304 19:28:54.529062 22502662377600 run.py:483] Algo bellman_ford step 2029 current loss 0.167610, current_train_items 64960.
I0304 19:28:54.548539 22502662377600 run.py:483] Algo bellman_ford step 2030 current loss 0.061002, current_train_items 64992.
I0304 19:28:54.564973 22502662377600 run.py:483] Algo bellman_ford step 2031 current loss 0.093756, current_train_items 65024.
I0304 19:28:54.587872 22502662377600 run.py:483] Algo bellman_ford step 2032 current loss 0.106685, current_train_items 65056.
I0304 19:28:54.618545 22502662377600 run.py:483] Algo bellman_ford step 2033 current loss 0.164992, current_train_items 65088.
I0304 19:28:54.650093 22502662377600 run.py:483] Algo bellman_ford step 2034 current loss 0.128994, current_train_items 65120.
I0304 19:28:54.669108 22502662377600 run.py:483] Algo bellman_ford step 2035 current loss 0.018946, current_train_items 65152.
I0304 19:28:54.684954 22502662377600 run.py:483] Algo bellman_ford step 2036 current loss 0.018451, current_train_items 65184.
I0304 19:28:54.707785 22502662377600 run.py:483] Algo bellman_ford step 2037 current loss 0.078418, current_train_items 65216.
I0304 19:28:54.737775 22502662377600 run.py:483] Algo bellman_ford step 2038 current loss 0.153681, current_train_items 65248.
I0304 19:28:54.770503 22502662377600 run.py:483] Algo bellman_ford step 2039 current loss 0.113389, current_train_items 65280.
I0304 19:28:54.789927 22502662377600 run.py:483] Algo bellman_ford step 2040 current loss 0.025226, current_train_items 65312.
I0304 19:28:54.805898 22502662377600 run.py:483] Algo bellman_ford step 2041 current loss 0.142367, current_train_items 65344.
I0304 19:28:54.830455 22502662377600 run.py:483] Algo bellman_ford step 2042 current loss 0.194519, current_train_items 65376.
I0304 19:28:54.860338 22502662377600 run.py:483] Algo bellman_ford step 2043 current loss 0.138315, current_train_items 65408.
I0304 19:28:54.887964 22502662377600 run.py:483] Algo bellman_ford step 2044 current loss 0.072182, current_train_items 65440.
I0304 19:28:54.907323 22502662377600 run.py:483] Algo bellman_ford step 2045 current loss 0.009995, current_train_items 65472.
I0304 19:28:54.923969 22502662377600 run.py:483] Algo bellman_ford step 2046 current loss 0.045814, current_train_items 65504.
I0304 19:28:54.947702 22502662377600 run.py:483] Algo bellman_ford step 2047 current loss 0.206266, current_train_items 65536.
I0304 19:28:54.977058 22502662377600 run.py:483] Algo bellman_ford step 2048 current loss 0.100158, current_train_items 65568.
I0304 19:28:55.007385 22502662377600 run.py:483] Algo bellman_ford step 2049 current loss 0.110250, current_train_items 65600.
I0304 19:28:55.026671 22502662377600 run.py:483] Algo bellman_ford step 2050 current loss 0.029341, current_train_items 65632.
I0304 19:28:55.035159 22502662377600 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0304 19:28:55.035281 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:28:55.052623 22502662377600 run.py:483] Algo bellman_ford step 2051 current loss 0.057793, current_train_items 65664.
I0304 19:28:55.076676 22502662377600 run.py:483] Algo bellman_ford step 2052 current loss 0.063381, current_train_items 65696.
I0304 19:28:55.106035 22502662377600 run.py:483] Algo bellman_ford step 2053 current loss 0.093606, current_train_items 65728.
I0304 19:28:55.141453 22502662377600 run.py:483] Algo bellman_ford step 2054 current loss 0.174119, current_train_items 65760.
I0304 19:28:55.160778 22502662377600 run.py:483] Algo bellman_ford step 2055 current loss 0.010606, current_train_items 65792.
I0304 19:28:55.176388 22502662377600 run.py:483] Algo bellman_ford step 2056 current loss 0.028519, current_train_items 65824.
I0304 19:28:55.199822 22502662377600 run.py:483] Algo bellman_ford step 2057 current loss 0.090869, current_train_items 65856.
I0304 19:28:55.229180 22502662377600 run.py:483] Algo bellman_ford step 2058 current loss 0.124607, current_train_items 65888.
I0304 19:28:55.262275 22502662377600 run.py:483] Algo bellman_ford step 2059 current loss 0.183129, current_train_items 65920.
I0304 19:28:55.281778 22502662377600 run.py:483] Algo bellman_ford step 2060 current loss 0.026837, current_train_items 65952.
I0304 19:28:55.298193 22502662377600 run.py:483] Algo bellman_ford step 2061 current loss 0.104528, current_train_items 65984.
I0304 19:28:55.321548 22502662377600 run.py:483] Algo bellman_ford step 2062 current loss 0.219497, current_train_items 66016.
I0304 19:28:55.351229 22502662377600 run.py:483] Algo bellman_ford step 2063 current loss 0.294981, current_train_items 66048.
I0304 19:28:55.385653 22502662377600 run.py:483] Algo bellman_ford step 2064 current loss 0.256510, current_train_items 66080.
I0304 19:28:55.404994 22502662377600 run.py:483] Algo bellman_ford step 2065 current loss 0.017571, current_train_items 66112.
I0304 19:28:55.421213 22502662377600 run.py:483] Algo bellman_ford step 2066 current loss 0.040128, current_train_items 66144.
I0304 19:28:55.444890 22502662377600 run.py:483] Algo bellman_ford step 2067 current loss 0.087028, current_train_items 66176.
I0304 19:28:55.475354 22502662377600 run.py:483] Algo bellman_ford step 2068 current loss 0.128668, current_train_items 66208.
I0304 19:28:55.507912 22502662377600 run.py:483] Algo bellman_ford step 2069 current loss 0.122973, current_train_items 66240.
I0304 19:28:55.527720 22502662377600 run.py:483] Algo bellman_ford step 2070 current loss 0.022266, current_train_items 66272.
I0304 19:28:55.544095 22502662377600 run.py:483] Algo bellman_ford step 2071 current loss 0.063967, current_train_items 66304.
I0304 19:28:55.567487 22502662377600 run.py:483] Algo bellman_ford step 2072 current loss 0.217554, current_train_items 66336.
I0304 19:28:55.597647 22502662377600 run.py:483] Algo bellman_ford step 2073 current loss 0.236348, current_train_items 66368.
I0304 19:28:55.630548 22502662377600 run.py:483] Algo bellman_ford step 2074 current loss 0.203027, current_train_items 66400.
I0304 19:28:55.649843 22502662377600 run.py:483] Algo bellman_ford step 2075 current loss 0.038511, current_train_items 66432.
I0304 19:28:55.666563 22502662377600 run.py:483] Algo bellman_ford step 2076 current loss 0.064986, current_train_items 66464.
I0304 19:28:55.690553 22502662377600 run.py:483] Algo bellman_ford step 2077 current loss 0.092975, current_train_items 66496.
I0304 19:28:55.718291 22502662377600 run.py:483] Algo bellman_ford step 2078 current loss 0.088318, current_train_items 66528.
I0304 19:28:55.750167 22502662377600 run.py:483] Algo bellman_ford step 2079 current loss 0.158774, current_train_items 66560.
I0304 19:28:55.769181 22502662377600 run.py:483] Algo bellman_ford step 2080 current loss 0.014158, current_train_items 66592.
I0304 19:28:55.785396 22502662377600 run.py:483] Algo bellman_ford step 2081 current loss 0.047968, current_train_items 66624.
I0304 19:28:55.809147 22502662377600 run.py:483] Algo bellman_ford step 2082 current loss 0.098611, current_train_items 66656.
I0304 19:28:55.840271 22502662377600 run.py:483] Algo bellman_ford step 2083 current loss 0.127136, current_train_items 66688.
I0304 19:28:55.873423 22502662377600 run.py:483] Algo bellman_ford step 2084 current loss 0.151770, current_train_items 66720.
I0304 19:28:55.893347 22502662377600 run.py:483] Algo bellman_ford step 2085 current loss 0.013209, current_train_items 66752.
I0304 19:28:55.909518 22502662377600 run.py:483] Algo bellman_ford step 2086 current loss 0.094618, current_train_items 66784.
I0304 19:28:55.932979 22502662377600 run.py:483] Algo bellman_ford step 2087 current loss 0.155342, current_train_items 66816.
I0304 19:28:55.960741 22502662377600 run.py:483] Algo bellman_ford step 2088 current loss 0.064400, current_train_items 66848.
I0304 19:28:55.993518 22502662377600 run.py:483] Algo bellman_ford step 2089 current loss 0.216204, current_train_items 66880.
I0304 19:28:56.012852 22502662377600 run.py:483] Algo bellman_ford step 2090 current loss 0.018924, current_train_items 66912.
I0304 19:28:56.029162 22502662377600 run.py:483] Algo bellman_ford step 2091 current loss 0.048023, current_train_items 66944.
I0304 19:28:56.052436 22502662377600 run.py:483] Algo bellman_ford step 2092 current loss 0.147972, current_train_items 66976.
I0304 19:28:56.082114 22502662377600 run.py:483] Algo bellman_ford step 2093 current loss 0.115200, current_train_items 67008.
I0304 19:28:56.114610 22502662377600 run.py:483] Algo bellman_ford step 2094 current loss 0.280909, current_train_items 67040.
I0304 19:28:56.133866 22502662377600 run.py:483] Algo bellman_ford step 2095 current loss 0.034603, current_train_items 67072.
I0304 19:28:56.150072 22502662377600 run.py:483] Algo bellman_ford step 2096 current loss 0.071903, current_train_items 67104.
I0304 19:28:56.173392 22502662377600 run.py:483] Algo bellman_ford step 2097 current loss 0.100804, current_train_items 67136.
I0304 19:28:56.202303 22502662377600 run.py:483] Algo bellman_ford step 2098 current loss 0.096316, current_train_items 67168.
I0304 19:28:56.234492 22502662377600 run.py:483] Algo bellman_ford step 2099 current loss 0.208179, current_train_items 67200.
I0304 19:28:56.254224 22502662377600 run.py:483] Algo bellman_ford step 2100 current loss 0.014251, current_train_items 67232.
I0304 19:28:56.262093 22502662377600 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0304 19:28:56.262197 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:28:56.278759 22502662377600 run.py:483] Algo bellman_ford step 2101 current loss 0.092527, current_train_items 67264.
I0304 19:28:56.302326 22502662377600 run.py:483] Algo bellman_ford step 2102 current loss 0.103798, current_train_items 67296.
I0304 19:28:56.332473 22502662377600 run.py:483] Algo bellman_ford step 2103 current loss 0.099890, current_train_items 67328.
I0304 19:28:56.364532 22502662377600 run.py:483] Algo bellman_ford step 2104 current loss 0.133707, current_train_items 67360.
I0304 19:28:56.383876 22502662377600 run.py:483] Algo bellman_ford step 2105 current loss 0.019961, current_train_items 67392.
I0304 19:28:56.399036 22502662377600 run.py:483] Algo bellman_ford step 2106 current loss 0.021130, current_train_items 67424.
I0304 19:28:56.422849 22502662377600 run.py:483] Algo bellman_ford step 2107 current loss 0.118795, current_train_items 67456.
I0304 19:28:56.453573 22502662377600 run.py:483] Algo bellman_ford step 2108 current loss 0.134578, current_train_items 67488.
I0304 19:28:56.484635 22502662377600 run.py:483] Algo bellman_ford step 2109 current loss 0.093691, current_train_items 67520.
I0304 19:28:56.504190 22502662377600 run.py:483] Algo bellman_ford step 2110 current loss 0.042524, current_train_items 67552.
I0304 19:28:56.520276 22502662377600 run.py:483] Algo bellman_ford step 2111 current loss 0.040960, current_train_items 67584.
I0304 19:28:56.542955 22502662377600 run.py:483] Algo bellman_ford step 2112 current loss 0.112772, current_train_items 67616.
I0304 19:28:56.571555 22502662377600 run.py:483] Algo bellman_ford step 2113 current loss 0.093937, current_train_items 67648.
I0304 19:28:56.602701 22502662377600 run.py:483] Algo bellman_ford step 2114 current loss 0.128802, current_train_items 67680.
I0304 19:28:56.621480 22502662377600 run.py:483] Algo bellman_ford step 2115 current loss 0.034728, current_train_items 67712.
I0304 19:28:56.637568 22502662377600 run.py:483] Algo bellman_ford step 2116 current loss 0.064641, current_train_items 67744.
I0304 19:28:56.661361 22502662377600 run.py:483] Algo bellman_ford step 2117 current loss 0.133116, current_train_items 67776.
I0304 19:28:56.691337 22502662377600 run.py:483] Algo bellman_ford step 2118 current loss 0.147334, current_train_items 67808.
I0304 19:28:56.722704 22502662377600 run.py:483] Algo bellman_ford step 2119 current loss 0.120895, current_train_items 67840.
I0304 19:28:56.741551 22502662377600 run.py:483] Algo bellman_ford step 2120 current loss 0.020960, current_train_items 67872.
I0304 19:28:56.757378 22502662377600 run.py:483] Algo bellman_ford step 2121 current loss 0.066480, current_train_items 67904.
I0304 19:28:56.780556 22502662377600 run.py:483] Algo bellman_ford step 2122 current loss 0.116066, current_train_items 67936.
I0304 19:28:56.810618 22502662377600 run.py:483] Algo bellman_ford step 2123 current loss 0.169216, current_train_items 67968.
I0304 19:28:56.839283 22502662377600 run.py:483] Algo bellman_ford step 2124 current loss 0.178784, current_train_items 68000.
I0304 19:28:56.858429 22502662377600 run.py:483] Algo bellman_ford step 2125 current loss 0.017671, current_train_items 68032.
I0304 19:28:56.874971 22502662377600 run.py:483] Algo bellman_ford step 2126 current loss 0.116533, current_train_items 68064.
I0304 19:28:56.899648 22502662377600 run.py:483] Algo bellman_ford step 2127 current loss 0.167373, current_train_items 68096.
I0304 19:28:56.928728 22502662377600 run.py:483] Algo bellman_ford step 2128 current loss 0.187415, current_train_items 68128.
I0304 19:28:56.960115 22502662377600 run.py:483] Algo bellman_ford step 2129 current loss 0.136035, current_train_items 68160.
I0304 19:28:56.979068 22502662377600 run.py:483] Algo bellman_ford step 2130 current loss 0.023290, current_train_items 68192.
I0304 19:28:56.995775 22502662377600 run.py:483] Algo bellman_ford step 2131 current loss 0.086079, current_train_items 68224.
I0304 19:28:57.019719 22502662377600 run.py:483] Algo bellman_ford step 2132 current loss 0.145107, current_train_items 68256.
I0304 19:28:57.048634 22502662377600 run.py:483] Algo bellman_ford step 2133 current loss 0.167066, current_train_items 68288.
I0304 19:28:57.082117 22502662377600 run.py:483] Algo bellman_ford step 2134 current loss 0.155360, current_train_items 68320.
I0304 19:28:57.100990 22502662377600 run.py:483] Algo bellman_ford step 2135 current loss 0.016232, current_train_items 68352.
I0304 19:28:57.117365 22502662377600 run.py:483] Algo bellman_ford step 2136 current loss 0.063381, current_train_items 68384.
I0304 19:28:57.140496 22502662377600 run.py:483] Algo bellman_ford step 2137 current loss 0.061292, current_train_items 68416.
I0304 19:28:57.170070 22502662377600 run.py:483] Algo bellman_ford step 2138 current loss 0.175678, current_train_items 68448.
I0304 19:28:57.205231 22502662377600 run.py:483] Algo bellman_ford step 2139 current loss 0.212804, current_train_items 68480.
I0304 19:28:57.224121 22502662377600 run.py:483] Algo bellman_ford step 2140 current loss 0.070149, current_train_items 68512.
I0304 19:28:57.240077 22502662377600 run.py:483] Algo bellman_ford step 2141 current loss 0.062619, current_train_items 68544.
I0304 19:28:57.261654 22502662377600 run.py:483] Algo bellman_ford step 2142 current loss 0.076096, current_train_items 68576.
I0304 19:28:57.290911 22502662377600 run.py:483] Algo bellman_ford step 2143 current loss 0.173961, current_train_items 68608.
I0304 19:28:57.323385 22502662377600 run.py:483] Algo bellman_ford step 2144 current loss 0.291368, current_train_items 68640.
I0304 19:28:57.342278 22502662377600 run.py:483] Algo bellman_ford step 2145 current loss 0.042744, current_train_items 68672.
I0304 19:28:57.359085 22502662377600 run.py:483] Algo bellman_ford step 2146 current loss 0.055415, current_train_items 68704.
I0304 19:28:57.382873 22502662377600 run.py:483] Algo bellman_ford step 2147 current loss 0.120792, current_train_items 68736.
I0304 19:28:57.412739 22502662377600 run.py:483] Algo bellman_ford step 2148 current loss 0.173810, current_train_items 68768.
I0304 19:28:57.443937 22502662377600 run.py:483] Algo bellman_ford step 2149 current loss 0.171848, current_train_items 68800.
I0304 19:28:57.462950 22502662377600 run.py:483] Algo bellman_ford step 2150 current loss 0.022018, current_train_items 68832.
I0304 19:28:57.471065 22502662377600 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0304 19:28:57.471169 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:28:57.487998 22502662377600 run.py:483] Algo bellman_ford step 2151 current loss 0.032826, current_train_items 68864.
I0304 19:28:57.511805 22502662377600 run.py:483] Algo bellman_ford step 2152 current loss 0.097450, current_train_items 68896.
I0304 19:28:57.541173 22502662377600 run.py:483] Algo bellman_ford step 2153 current loss 0.174343, current_train_items 68928.
I0304 19:28:57.573408 22502662377600 run.py:483] Algo bellman_ford step 2154 current loss 0.146819, current_train_items 68960.
I0304 19:28:57.592489 22502662377600 run.py:483] Algo bellman_ford step 2155 current loss 0.010617, current_train_items 68992.
I0304 19:28:57.608240 22502662377600 run.py:483] Algo bellman_ford step 2156 current loss 0.035592, current_train_items 69024.
I0304 19:28:57.631950 22502662377600 run.py:483] Algo bellman_ford step 2157 current loss 0.105969, current_train_items 69056.
I0304 19:28:57.662384 22502662377600 run.py:483] Algo bellman_ford step 2158 current loss 0.155119, current_train_items 69088.
I0304 19:28:57.695115 22502662377600 run.py:483] Algo bellman_ford step 2159 current loss 0.189997, current_train_items 69120.
I0304 19:28:57.714733 22502662377600 run.py:483] Algo bellman_ford step 2160 current loss 0.034097, current_train_items 69152.
I0304 19:28:57.730881 22502662377600 run.py:483] Algo bellman_ford step 2161 current loss 0.034657, current_train_items 69184.
I0304 19:28:57.754143 22502662377600 run.py:483] Algo bellman_ford step 2162 current loss 0.181634, current_train_items 69216.
I0304 19:28:57.782274 22502662377600 run.py:483] Algo bellman_ford step 2163 current loss 0.117505, current_train_items 69248.
I0304 19:28:57.815676 22502662377600 run.py:483] Algo bellman_ford step 2164 current loss 0.174116, current_train_items 69280.
I0304 19:28:57.834649 22502662377600 run.py:483] Algo bellman_ford step 2165 current loss 0.022021, current_train_items 69312.
I0304 19:28:57.851286 22502662377600 run.py:483] Algo bellman_ford step 2166 current loss 0.082562, current_train_items 69344.
I0304 19:28:57.874559 22502662377600 run.py:483] Algo bellman_ford step 2167 current loss 0.101015, current_train_items 69376.
I0304 19:28:57.903039 22502662377600 run.py:483] Algo bellman_ford step 2168 current loss 0.130516, current_train_items 69408.
I0304 19:28:57.935562 22502662377600 run.py:483] Algo bellman_ford step 2169 current loss 0.123843, current_train_items 69440.
I0304 19:28:57.954778 22502662377600 run.py:483] Algo bellman_ford step 2170 current loss 0.032465, current_train_items 69472.
I0304 19:28:57.970927 22502662377600 run.py:483] Algo bellman_ford step 2171 current loss 0.042439, current_train_items 69504.
I0304 19:28:57.993819 22502662377600 run.py:483] Algo bellman_ford step 2172 current loss 0.118648, current_train_items 69536.
I0304 19:28:58.024285 22502662377600 run.py:483] Algo bellman_ford step 2173 current loss 0.184611, current_train_items 69568.
I0304 19:28:58.054902 22502662377600 run.py:483] Algo bellman_ford step 2174 current loss 0.199468, current_train_items 69600.
I0304 19:28:58.074182 22502662377600 run.py:483] Algo bellman_ford step 2175 current loss 0.012541, current_train_items 69632.
I0304 19:28:58.089987 22502662377600 run.py:483] Algo bellman_ford step 2176 current loss 0.030731, current_train_items 69664.
I0304 19:28:58.113680 22502662377600 run.py:483] Algo bellman_ford step 2177 current loss 0.113308, current_train_items 69696.
I0304 19:28:58.142505 22502662377600 run.py:483] Algo bellman_ford step 2178 current loss 0.114781, current_train_items 69728.
I0304 19:28:58.175634 22502662377600 run.py:483] Algo bellman_ford step 2179 current loss 0.117272, current_train_items 69760.
I0304 19:28:58.194317 22502662377600 run.py:483] Algo bellman_ford step 2180 current loss 0.015752, current_train_items 69792.
I0304 19:28:58.210552 22502662377600 run.py:483] Algo bellman_ford step 2181 current loss 0.047129, current_train_items 69824.
I0304 19:28:58.233874 22502662377600 run.py:483] Algo bellman_ford step 2182 current loss 0.060509, current_train_items 69856.
I0304 19:28:58.262439 22502662377600 run.py:483] Algo bellman_ford step 2183 current loss 0.167186, current_train_items 69888.
I0304 19:28:58.296048 22502662377600 run.py:483] Algo bellman_ford step 2184 current loss 0.172914, current_train_items 69920.
I0304 19:28:58.315444 22502662377600 run.py:483] Algo bellman_ford step 2185 current loss 0.020181, current_train_items 69952.
I0304 19:28:58.331139 22502662377600 run.py:483] Algo bellman_ford step 2186 current loss 0.025173, current_train_items 69984.
I0304 19:28:58.354176 22502662377600 run.py:483] Algo bellman_ford step 2187 current loss 0.116274, current_train_items 70016.
I0304 19:28:58.383666 22502662377600 run.py:483] Algo bellman_ford step 2188 current loss 0.106241, current_train_items 70048.
W0304 19:28:58.407915 22502662377600 samplers.py:155] Increasing hint lengh from 12 to 13
I0304 19:29:05.156321 22502662377600 run.py:483] Algo bellman_ford step 2189 current loss 0.197454, current_train_items 70080.
I0304 19:29:05.177168 22502662377600 run.py:483] Algo bellman_ford step 2190 current loss 0.014354, current_train_items 70112.
I0304 19:29:05.193721 22502662377600 run.py:483] Algo bellman_ford step 2191 current loss 0.105592, current_train_items 70144.
I0304 19:29:05.217790 22502662377600 run.py:483] Algo bellman_ford step 2192 current loss 0.293013, current_train_items 70176.
W0304 19:29:05.239327 22502662377600 samplers.py:155] Increasing hint lengh from 10 to 12
I0304 19:29:12.173346 22502662377600 run.py:483] Algo bellman_ford step 2193 current loss 0.360969, current_train_items 70208.
I0304 19:29:12.206149 22502662377600 run.py:483] Algo bellman_ford step 2194 current loss 0.196856, current_train_items 70240.
I0304 19:29:12.226628 22502662377600 run.py:483] Algo bellman_ford step 2195 current loss 0.032289, current_train_items 70272.
I0304 19:29:12.243218 22502662377600 run.py:483] Algo bellman_ford step 2196 current loss 0.044884, current_train_items 70304.
I0304 19:29:12.266647 22502662377600 run.py:483] Algo bellman_ford step 2197 current loss 0.086912, current_train_items 70336.
I0304 19:29:12.295629 22502662377600 run.py:483] Algo bellman_ford step 2198 current loss 0.144710, current_train_items 70368.
I0304 19:29:12.328742 22502662377600 run.py:483] Algo bellman_ford step 2199 current loss 0.133715, current_train_items 70400.
I0304 19:29:12.348607 22502662377600 run.py:483] Algo bellman_ford step 2200 current loss 0.015440, current_train_items 70432.
I0304 19:29:12.357605 22502662377600 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0304 19:29:12.357709 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.955, val scores are: bellman_ford: 0.955
I0304 19:29:12.374741 22502662377600 run.py:483] Algo bellman_ford step 2201 current loss 0.111031, current_train_items 70464.
I0304 19:29:12.397880 22502662377600 run.py:483] Algo bellman_ford step 2202 current loss 0.131899, current_train_items 70496.
I0304 19:29:12.428230 22502662377600 run.py:483] Algo bellman_ford step 2203 current loss 0.248111, current_train_items 70528.
I0304 19:29:12.462382 22502662377600 run.py:483] Algo bellman_ford step 2204 current loss 0.188777, current_train_items 70560.
I0304 19:29:12.482704 22502662377600 run.py:483] Algo bellman_ford step 2205 current loss 0.011769, current_train_items 70592.
I0304 19:29:12.498836 22502662377600 run.py:483] Algo bellman_ford step 2206 current loss 0.063305, current_train_items 70624.
I0304 19:29:12.521939 22502662377600 run.py:483] Algo bellman_ford step 2207 current loss 0.180438, current_train_items 70656.
I0304 19:29:12.552296 22502662377600 run.py:483] Algo bellman_ford step 2208 current loss 0.312701, current_train_items 70688.
I0304 19:29:12.586556 22502662377600 run.py:483] Algo bellman_ford step 2209 current loss 0.228001, current_train_items 70720.
I0304 19:29:12.606213 22502662377600 run.py:483] Algo bellman_ford step 2210 current loss 0.041736, current_train_items 70752.
I0304 19:29:12.622428 22502662377600 run.py:483] Algo bellman_ford step 2211 current loss 0.047317, current_train_items 70784.
I0304 19:29:12.646389 22502662377600 run.py:483] Algo bellman_ford step 2212 current loss 0.175685, current_train_items 70816.
I0304 19:29:12.677251 22502662377600 run.py:483] Algo bellman_ford step 2213 current loss 0.343783, current_train_items 70848.
I0304 19:29:12.711664 22502662377600 run.py:483] Algo bellman_ford step 2214 current loss 0.240237, current_train_items 70880.
I0304 19:29:12.731255 22502662377600 run.py:483] Algo bellman_ford step 2215 current loss 0.042446, current_train_items 70912.
I0304 19:29:12.747361 22502662377600 run.py:483] Algo bellman_ford step 2216 current loss 0.031493, current_train_items 70944.
I0304 19:29:12.769490 22502662377600 run.py:483] Algo bellman_ford step 2217 current loss 0.124009, current_train_items 70976.
I0304 19:29:12.800596 22502662377600 run.py:483] Algo bellman_ford step 2218 current loss 0.418669, current_train_items 71008.
I0304 19:29:12.832860 22502662377600 run.py:483] Algo bellman_ford step 2219 current loss 0.368705, current_train_items 71040.
I0304 19:29:12.852253 22502662377600 run.py:483] Algo bellman_ford step 2220 current loss 0.042555, current_train_items 71072.
I0304 19:29:12.868322 22502662377600 run.py:483] Algo bellman_ford step 2221 current loss 0.138006, current_train_items 71104.
I0304 19:29:12.891271 22502662377600 run.py:483] Algo bellman_ford step 2222 current loss 0.105464, current_train_items 71136.
I0304 19:29:12.922031 22502662377600 run.py:483] Algo bellman_ford step 2223 current loss 0.136266, current_train_items 71168.
I0304 19:29:12.956287 22502662377600 run.py:483] Algo bellman_ford step 2224 current loss 0.256180, current_train_items 71200.
I0304 19:29:12.976345 22502662377600 run.py:483] Algo bellman_ford step 2225 current loss 0.035717, current_train_items 71232.
I0304 19:29:12.992592 22502662377600 run.py:483] Algo bellman_ford step 2226 current loss 0.057994, current_train_items 71264.
I0304 19:29:13.016091 22502662377600 run.py:483] Algo bellman_ford step 2227 current loss 0.087094, current_train_items 71296.
I0304 19:29:13.046777 22502662377600 run.py:483] Algo bellman_ford step 2228 current loss 0.135498, current_train_items 71328.
I0304 19:29:13.077887 22502662377600 run.py:483] Algo bellman_ford step 2229 current loss 0.099045, current_train_items 71360.
I0304 19:29:13.097685 22502662377600 run.py:483] Algo bellman_ford step 2230 current loss 0.009321, current_train_items 71392.
I0304 19:29:13.114047 22502662377600 run.py:483] Algo bellman_ford step 2231 current loss 0.044494, current_train_items 71424.
I0304 19:29:13.137290 22502662377600 run.py:483] Algo bellman_ford step 2232 current loss 0.084207, current_train_items 71456.
I0304 19:29:13.167177 22502662377600 run.py:483] Algo bellman_ford step 2233 current loss 0.127431, current_train_items 71488.
I0304 19:29:13.199653 22502662377600 run.py:483] Algo bellman_ford step 2234 current loss 0.124019, current_train_items 71520.
I0304 19:29:13.219115 22502662377600 run.py:483] Algo bellman_ford step 2235 current loss 0.017692, current_train_items 71552.
I0304 19:29:13.235481 22502662377600 run.py:483] Algo bellman_ford step 2236 current loss 0.058275, current_train_items 71584.
I0304 19:29:13.259945 22502662377600 run.py:483] Algo bellman_ford step 2237 current loss 0.111787, current_train_items 71616.
I0304 19:29:13.290424 22502662377600 run.py:483] Algo bellman_ford step 2238 current loss 0.101215, current_train_items 71648.
I0304 19:29:13.322605 22502662377600 run.py:483] Algo bellman_ford step 2239 current loss 0.116844, current_train_items 71680.
I0304 19:29:13.342210 22502662377600 run.py:483] Algo bellman_ford step 2240 current loss 0.031440, current_train_items 71712.
I0304 19:29:13.358794 22502662377600 run.py:483] Algo bellman_ford step 2241 current loss 0.041755, current_train_items 71744.
I0304 19:29:13.383408 22502662377600 run.py:483] Algo bellman_ford step 2242 current loss 0.080105, current_train_items 71776.
I0304 19:29:13.413990 22502662377600 run.py:483] Algo bellman_ford step 2243 current loss 0.132207, current_train_items 71808.
I0304 19:29:13.445287 22502662377600 run.py:483] Algo bellman_ford step 2244 current loss 0.107863, current_train_items 71840.
I0304 19:29:13.464850 22502662377600 run.py:483] Algo bellman_ford step 2245 current loss 0.028648, current_train_items 71872.
I0304 19:29:13.481274 22502662377600 run.py:483] Algo bellman_ford step 2246 current loss 0.061272, current_train_items 71904.
I0304 19:29:13.504041 22502662377600 run.py:483] Algo bellman_ford step 2247 current loss 0.083638, current_train_items 71936.
I0304 19:29:13.535495 22502662377600 run.py:483] Algo bellman_ford step 2248 current loss 0.102199, current_train_items 71968.
I0304 19:29:13.569173 22502662377600 run.py:483] Algo bellman_ford step 2249 current loss 0.114338, current_train_items 72000.
I0304 19:29:13.588858 22502662377600 run.py:483] Algo bellman_ford step 2250 current loss 0.023104, current_train_items 72032.
I0304 19:29:13.597636 22502662377600 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0304 19:29:13.597741 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:29:13.614443 22502662377600 run.py:483] Algo bellman_ford step 2251 current loss 0.057966, current_train_items 72064.
I0304 19:29:13.637482 22502662377600 run.py:483] Algo bellman_ford step 2252 current loss 0.109255, current_train_items 72096.
I0304 19:29:13.669497 22502662377600 run.py:483] Algo bellman_ford step 2253 current loss 0.316575, current_train_items 72128.
I0304 19:29:13.703828 22502662377600 run.py:483] Algo bellman_ford step 2254 current loss 0.214806, current_train_items 72160.
I0304 19:29:13.723956 22502662377600 run.py:483] Algo bellman_ford step 2255 current loss 0.026298, current_train_items 72192.
I0304 19:29:13.740006 22502662377600 run.py:483] Algo bellman_ford step 2256 current loss 0.060823, current_train_items 72224.
I0304 19:29:13.763311 22502662377600 run.py:483] Algo bellman_ford step 2257 current loss 0.188617, current_train_items 72256.
I0304 19:29:13.792750 22502662377600 run.py:483] Algo bellman_ford step 2258 current loss 0.199229, current_train_items 72288.
I0304 19:29:13.827254 22502662377600 run.py:483] Algo bellman_ford step 2259 current loss 0.267821, current_train_items 72320.
I0304 19:29:13.847541 22502662377600 run.py:483] Algo bellman_ford step 2260 current loss 0.026914, current_train_items 72352.
I0304 19:29:13.864226 22502662377600 run.py:483] Algo bellman_ford step 2261 current loss 0.071194, current_train_items 72384.
I0304 19:29:13.888977 22502662377600 run.py:483] Algo bellman_ford step 2262 current loss 0.163117, current_train_items 72416.
I0304 19:29:13.918521 22502662377600 run.py:483] Algo bellman_ford step 2263 current loss 0.079940, current_train_items 72448.
I0304 19:29:13.951839 22502662377600 run.py:483] Algo bellman_ford step 2264 current loss 0.187835, current_train_items 72480.
I0304 19:29:13.971580 22502662377600 run.py:483] Algo bellman_ford step 2265 current loss 0.051948, current_train_items 72512.
I0304 19:29:13.987971 22502662377600 run.py:483] Algo bellman_ford step 2266 current loss 0.046587, current_train_items 72544.
I0304 19:29:14.012585 22502662377600 run.py:483] Algo bellman_ford step 2267 current loss 0.080548, current_train_items 72576.
I0304 19:29:14.043115 22502662377600 run.py:483] Algo bellman_ford step 2268 current loss 0.142474, current_train_items 72608.
I0304 19:29:14.075965 22502662377600 run.py:483] Algo bellman_ford step 2269 current loss 0.152068, current_train_items 72640.
I0304 19:29:14.095963 22502662377600 run.py:483] Algo bellman_ford step 2270 current loss 0.020070, current_train_items 72672.
I0304 19:29:14.112001 22502662377600 run.py:483] Algo bellman_ford step 2271 current loss 0.027289, current_train_items 72704.
I0304 19:29:14.134223 22502662377600 run.py:483] Algo bellman_ford step 2272 current loss 0.084519, current_train_items 72736.
I0304 19:29:14.165869 22502662377600 run.py:483] Algo bellman_ford step 2273 current loss 0.094564, current_train_items 72768.
I0304 19:29:14.201480 22502662377600 run.py:483] Algo bellman_ford step 2274 current loss 0.183664, current_train_items 72800.
I0304 19:29:14.221338 22502662377600 run.py:483] Algo bellman_ford step 2275 current loss 0.012112, current_train_items 72832.
I0304 19:29:14.237403 22502662377600 run.py:483] Algo bellman_ford step 2276 current loss 0.055379, current_train_items 72864.
I0304 19:29:14.259104 22502662377600 run.py:483] Algo bellman_ford step 2277 current loss 0.051696, current_train_items 72896.
I0304 19:29:14.288730 22502662377600 run.py:483] Algo bellman_ford step 2278 current loss 0.090262, current_train_items 72928.
I0304 19:29:14.323247 22502662377600 run.py:483] Algo bellman_ford step 2279 current loss 0.129009, current_train_items 72960.
I0304 19:29:14.342852 22502662377600 run.py:483] Algo bellman_ford step 2280 current loss 0.009891, current_train_items 72992.
I0304 19:29:14.358714 22502662377600 run.py:483] Algo bellman_ford step 2281 current loss 0.089501, current_train_items 73024.
I0304 19:29:14.382711 22502662377600 run.py:483] Algo bellman_ford step 2282 current loss 0.085260, current_train_items 73056.
I0304 19:29:14.415600 22502662377600 run.py:483] Algo bellman_ford step 2283 current loss 0.191303, current_train_items 73088.
I0304 19:29:14.450496 22502662377600 run.py:483] Algo bellman_ford step 2284 current loss 0.136978, current_train_items 73120.
I0304 19:29:14.470225 22502662377600 run.py:483] Algo bellman_ford step 2285 current loss 0.018622, current_train_items 73152.
I0304 19:29:14.486502 22502662377600 run.py:483] Algo bellman_ford step 2286 current loss 0.046722, current_train_items 73184.
I0304 19:29:14.509587 22502662377600 run.py:483] Algo bellman_ford step 2287 current loss 0.095742, current_train_items 73216.
I0304 19:29:14.539409 22502662377600 run.py:483] Algo bellman_ford step 2288 current loss 0.088548, current_train_items 73248.
I0304 19:29:14.575447 22502662377600 run.py:483] Algo bellman_ford step 2289 current loss 0.156707, current_train_items 73280.
I0304 19:29:14.595547 22502662377600 run.py:483] Algo bellman_ford step 2290 current loss 0.012397, current_train_items 73312.
I0304 19:29:14.611181 22502662377600 run.py:483] Algo bellman_ford step 2291 current loss 0.038617, current_train_items 73344.
I0304 19:29:14.633831 22502662377600 run.py:483] Algo bellman_ford step 2292 current loss 0.100071, current_train_items 73376.
I0304 19:29:14.664657 22502662377600 run.py:483] Algo bellman_ford step 2293 current loss 0.132131, current_train_items 73408.
I0304 19:29:14.698394 22502662377600 run.py:483] Algo bellman_ford step 2294 current loss 0.186934, current_train_items 73440.
I0304 19:29:14.717726 22502662377600 run.py:483] Algo bellman_ford step 2295 current loss 0.021135, current_train_items 73472.
I0304 19:29:14.733829 22502662377600 run.py:483] Algo bellman_ford step 2296 current loss 0.066844, current_train_items 73504.
I0304 19:29:14.756507 22502662377600 run.py:483] Algo bellman_ford step 2297 current loss 0.042135, current_train_items 73536.
I0304 19:29:14.786120 22502662377600 run.py:483] Algo bellman_ford step 2298 current loss 0.126005, current_train_items 73568.
I0304 19:29:14.820030 22502662377600 run.py:483] Algo bellman_ford step 2299 current loss 0.170669, current_train_items 73600.
I0304 19:29:14.839927 22502662377600 run.py:483] Algo bellman_ford step 2300 current loss 0.011810, current_train_items 73632.
I0304 19:29:14.847931 22502662377600 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0304 19:29:14.848036 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:29:14.864837 22502662377600 run.py:483] Algo bellman_ford step 2301 current loss 0.060090, current_train_items 73664.
I0304 19:29:14.889423 22502662377600 run.py:483] Algo bellman_ford step 2302 current loss 0.098003, current_train_items 73696.
I0304 19:29:14.921877 22502662377600 run.py:483] Algo bellman_ford step 2303 current loss 0.118114, current_train_items 73728.
I0304 19:29:14.956305 22502662377600 run.py:483] Algo bellman_ford step 2304 current loss 0.147322, current_train_items 73760.
I0304 19:29:14.977027 22502662377600 run.py:483] Algo bellman_ford step 2305 current loss 0.075268, current_train_items 73792.
I0304 19:29:14.993470 22502662377600 run.py:483] Algo bellman_ford step 2306 current loss 0.093090, current_train_items 73824.
I0304 19:29:15.016774 22502662377600 run.py:483] Algo bellman_ford step 2307 current loss 0.172692, current_train_items 73856.
I0304 19:29:15.046339 22502662377600 run.py:483] Algo bellman_ford step 2308 current loss 0.103168, current_train_items 73888.
I0304 19:29:15.080974 22502662377600 run.py:483] Algo bellman_ford step 2309 current loss 0.121039, current_train_items 73920.
I0304 19:29:15.100742 22502662377600 run.py:483] Algo bellman_ford step 2310 current loss 0.022737, current_train_items 73952.
I0304 19:29:15.117018 22502662377600 run.py:483] Algo bellman_ford step 2311 current loss 0.050278, current_train_items 73984.
I0304 19:29:15.140710 22502662377600 run.py:483] Algo bellman_ford step 2312 current loss 0.103354, current_train_items 74016.
I0304 19:29:15.171344 22502662377600 run.py:483] Algo bellman_ford step 2313 current loss 0.093954, current_train_items 74048.
I0304 19:29:15.205295 22502662377600 run.py:483] Algo bellman_ford step 2314 current loss 0.114192, current_train_items 74080.
I0304 19:29:15.225029 22502662377600 run.py:483] Algo bellman_ford step 2315 current loss 0.052057, current_train_items 74112.
I0304 19:29:15.241104 22502662377600 run.py:483] Algo bellman_ford step 2316 current loss 0.096076, current_train_items 74144.
I0304 19:29:15.264054 22502662377600 run.py:483] Algo bellman_ford step 2317 current loss 0.118826, current_train_items 74176.
I0304 19:29:15.295252 22502662377600 run.py:483] Algo bellman_ford step 2318 current loss 0.112675, current_train_items 74208.
I0304 19:29:15.329688 22502662377600 run.py:483] Algo bellman_ford step 2319 current loss 0.157689, current_train_items 74240.
I0304 19:29:15.349334 22502662377600 run.py:483] Algo bellman_ford step 2320 current loss 0.019053, current_train_items 74272.
I0304 19:29:15.365272 22502662377600 run.py:483] Algo bellman_ford step 2321 current loss 0.021241, current_train_items 74304.
I0304 19:29:15.389275 22502662377600 run.py:483] Algo bellman_ford step 2322 current loss 0.125596, current_train_items 74336.
I0304 19:29:15.419350 22502662377600 run.py:483] Algo bellman_ford step 2323 current loss 0.131942, current_train_items 74368.
I0304 19:29:15.453334 22502662377600 run.py:483] Algo bellman_ford step 2324 current loss 0.198533, current_train_items 74400.
I0304 19:29:15.473348 22502662377600 run.py:483] Algo bellman_ford step 2325 current loss 0.024569, current_train_items 74432.
I0304 19:29:15.489375 22502662377600 run.py:483] Algo bellman_ford step 2326 current loss 0.071151, current_train_items 74464.
I0304 19:29:15.512185 22502662377600 run.py:483] Algo bellman_ford step 2327 current loss 0.197169, current_train_items 74496.
I0304 19:29:15.541533 22502662377600 run.py:483] Algo bellman_ford step 2328 current loss 0.128874, current_train_items 74528.
I0304 19:29:15.573921 22502662377600 run.py:483] Algo bellman_ford step 2329 current loss 0.140217, current_train_items 74560.
I0304 19:29:15.593612 22502662377600 run.py:483] Algo bellman_ford step 2330 current loss 0.027176, current_train_items 74592.
I0304 19:29:15.609347 22502662377600 run.py:483] Algo bellman_ford step 2331 current loss 0.072279, current_train_items 74624.
I0304 19:29:15.633532 22502662377600 run.py:483] Algo bellman_ford step 2332 current loss 0.262615, current_train_items 74656.
I0304 19:29:15.664765 22502662377600 run.py:483] Algo bellman_ford step 2333 current loss 0.191178, current_train_items 74688.
I0304 19:29:15.699656 22502662377600 run.py:483] Algo bellman_ford step 2334 current loss 0.186099, current_train_items 74720.
I0304 19:29:15.719454 22502662377600 run.py:483] Algo bellman_ford step 2335 current loss 0.018338, current_train_items 74752.
I0304 19:29:15.736117 22502662377600 run.py:483] Algo bellman_ford step 2336 current loss 0.093914, current_train_items 74784.
I0304 19:29:15.760128 22502662377600 run.py:483] Algo bellman_ford step 2337 current loss 0.149416, current_train_items 74816.
I0304 19:29:15.790187 22502662377600 run.py:483] Algo bellman_ford step 2338 current loss 0.130520, current_train_items 74848.
I0304 19:29:15.823057 22502662377600 run.py:483] Algo bellman_ford step 2339 current loss 0.143261, current_train_items 74880.
I0304 19:29:15.843118 22502662377600 run.py:483] Algo bellman_ford step 2340 current loss 0.014402, current_train_items 74912.
I0304 19:29:15.859423 22502662377600 run.py:483] Algo bellman_ford step 2341 current loss 0.037739, current_train_items 74944.
I0304 19:29:15.882331 22502662377600 run.py:483] Algo bellman_ford step 2342 current loss 0.203073, current_train_items 74976.
I0304 19:29:15.911919 22502662377600 run.py:483] Algo bellman_ford step 2343 current loss 0.266578, current_train_items 75008.
I0304 19:29:15.943806 22502662377600 run.py:483] Algo bellman_ford step 2344 current loss 0.215265, current_train_items 75040.
I0304 19:29:15.963386 22502662377600 run.py:483] Algo bellman_ford step 2345 current loss 0.035499, current_train_items 75072.
I0304 19:29:15.979273 22502662377600 run.py:483] Algo bellman_ford step 2346 current loss 0.073481, current_train_items 75104.
I0304 19:29:16.002745 22502662377600 run.py:483] Algo bellman_ford step 2347 current loss 0.057507, current_train_items 75136.
I0304 19:29:16.032375 22502662377600 run.py:483] Algo bellman_ford step 2348 current loss 0.082435, current_train_items 75168.
I0304 19:29:16.064671 22502662377600 run.py:483] Algo bellman_ford step 2349 current loss 0.136470, current_train_items 75200.
I0304 19:29:16.084428 22502662377600 run.py:483] Algo bellman_ford step 2350 current loss 0.011981, current_train_items 75232.
I0304 19:29:16.092890 22502662377600 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0304 19:29:16.092994 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:29:16.109722 22502662377600 run.py:483] Algo bellman_ford step 2351 current loss 0.041935, current_train_items 75264.
I0304 19:29:16.133695 22502662377600 run.py:483] Algo bellman_ford step 2352 current loss 0.111816, current_train_items 75296.
I0304 19:29:16.163394 22502662377600 run.py:483] Algo bellman_ford step 2353 current loss 0.112527, current_train_items 75328.
I0304 19:29:16.198968 22502662377600 run.py:483] Algo bellman_ford step 2354 current loss 0.156470, current_train_items 75360.
I0304 19:29:16.219160 22502662377600 run.py:483] Algo bellman_ford step 2355 current loss 0.063096, current_train_items 75392.
I0304 19:29:16.234615 22502662377600 run.py:483] Algo bellman_ford step 2356 current loss 0.036144, current_train_items 75424.
I0304 19:29:16.258347 22502662377600 run.py:483] Algo bellman_ford step 2357 current loss 0.130614, current_train_items 75456.
I0304 19:29:16.287852 22502662377600 run.py:483] Algo bellman_ford step 2358 current loss 0.116162, current_train_items 75488.
I0304 19:29:16.323160 22502662377600 run.py:483] Algo bellman_ford step 2359 current loss 0.132340, current_train_items 75520.
I0304 19:29:16.343318 22502662377600 run.py:483] Algo bellman_ford step 2360 current loss 0.022291, current_train_items 75552.
I0304 19:29:16.360219 22502662377600 run.py:483] Algo bellman_ford step 2361 current loss 0.100396, current_train_items 75584.
I0304 19:29:16.384801 22502662377600 run.py:483] Algo bellman_ford step 2362 current loss 0.111007, current_train_items 75616.
I0304 19:29:16.414733 22502662377600 run.py:483] Algo bellman_ford step 2363 current loss 0.090280, current_train_items 75648.
I0304 19:29:16.447995 22502662377600 run.py:483] Algo bellman_ford step 2364 current loss 0.146983, current_train_items 75680.
I0304 19:29:16.468150 22502662377600 run.py:483] Algo bellman_ford step 2365 current loss 0.015899, current_train_items 75712.
I0304 19:29:16.484389 22502662377600 run.py:483] Algo bellman_ford step 2366 current loss 0.043062, current_train_items 75744.
I0304 19:29:16.508021 22502662377600 run.py:483] Algo bellman_ford step 2367 current loss 0.141114, current_train_items 75776.
I0304 19:29:16.539583 22502662377600 run.py:483] Algo bellman_ford step 2368 current loss 0.101366, current_train_items 75808.
I0304 19:29:16.574338 22502662377600 run.py:483] Algo bellman_ford step 2369 current loss 0.125033, current_train_items 75840.
I0304 19:29:16.594794 22502662377600 run.py:483] Algo bellman_ford step 2370 current loss 0.012078, current_train_items 75872.
I0304 19:29:16.610891 22502662377600 run.py:483] Algo bellman_ford step 2371 current loss 0.109467, current_train_items 75904.
I0304 19:29:16.634053 22502662377600 run.py:483] Algo bellman_ford step 2372 current loss 0.133259, current_train_items 75936.
I0304 19:29:16.663920 22502662377600 run.py:483] Algo bellman_ford step 2373 current loss 0.132076, current_train_items 75968.
I0304 19:29:16.696715 22502662377600 run.py:483] Algo bellman_ford step 2374 current loss 0.118949, current_train_items 76000.
I0304 19:29:16.716673 22502662377600 run.py:483] Algo bellman_ford step 2375 current loss 0.015523, current_train_items 76032.
I0304 19:29:16.732983 22502662377600 run.py:483] Algo bellman_ford step 2376 current loss 0.059129, current_train_items 76064.
I0304 19:29:16.756368 22502662377600 run.py:483] Algo bellman_ford step 2377 current loss 0.234003, current_train_items 76096.
I0304 19:29:16.788017 22502662377600 run.py:483] Algo bellman_ford step 2378 current loss 0.261218, current_train_items 76128.
I0304 19:29:16.823253 22502662377600 run.py:483] Algo bellman_ford step 2379 current loss 0.224419, current_train_items 76160.
I0304 19:29:16.843049 22502662377600 run.py:483] Algo bellman_ford step 2380 current loss 0.025651, current_train_items 76192.
I0304 19:29:16.859661 22502662377600 run.py:483] Algo bellman_ford step 2381 current loss 0.079988, current_train_items 76224.
I0304 19:29:16.883806 22502662377600 run.py:483] Algo bellman_ford step 2382 current loss 0.162997, current_train_items 76256.
I0304 19:29:16.913324 22502662377600 run.py:483] Algo bellman_ford step 2383 current loss 0.110003, current_train_items 76288.
I0304 19:29:16.947297 22502662377600 run.py:483] Algo bellman_ford step 2384 current loss 0.161197, current_train_items 76320.
I0304 19:29:16.967396 22502662377600 run.py:483] Algo bellman_ford step 2385 current loss 0.020742, current_train_items 76352.
I0304 19:29:16.983355 22502662377600 run.py:483] Algo bellman_ford step 2386 current loss 0.041029, current_train_items 76384.
I0304 19:29:17.006232 22502662377600 run.py:483] Algo bellman_ford step 2387 current loss 0.093463, current_train_items 76416.
I0304 19:29:17.036826 22502662377600 run.py:483] Algo bellman_ford step 2388 current loss 0.131800, current_train_items 76448.
I0304 19:29:17.072129 22502662377600 run.py:483] Algo bellman_ford step 2389 current loss 0.140489, current_train_items 76480.
I0304 19:29:17.092506 22502662377600 run.py:483] Algo bellman_ford step 2390 current loss 0.016641, current_train_items 76512.
I0304 19:29:17.108286 22502662377600 run.py:483] Algo bellman_ford step 2391 current loss 0.066153, current_train_items 76544.
I0304 19:29:17.130399 22502662377600 run.py:483] Algo bellman_ford step 2392 current loss 0.106412, current_train_items 76576.
I0304 19:29:17.160609 22502662377600 run.py:483] Algo bellman_ford step 2393 current loss 0.147570, current_train_items 76608.
I0304 19:29:17.196990 22502662377600 run.py:483] Algo bellman_ford step 2394 current loss 0.157366, current_train_items 76640.
I0304 19:29:17.216755 22502662377600 run.py:483] Algo bellman_ford step 2395 current loss 0.040675, current_train_items 76672.
I0304 19:29:17.232849 22502662377600 run.py:483] Algo bellman_ford step 2396 current loss 0.042592, current_train_items 76704.
I0304 19:29:17.255073 22502662377600 run.py:483] Algo bellman_ford step 2397 current loss 0.177191, current_train_items 76736.
I0304 19:29:17.284839 22502662377600 run.py:483] Algo bellman_ford step 2398 current loss 0.245927, current_train_items 76768.
I0304 19:29:17.319783 22502662377600 run.py:483] Algo bellman_ford step 2399 current loss 0.193531, current_train_items 76800.
I0304 19:29:17.340194 22502662377600 run.py:483] Algo bellman_ford step 2400 current loss 0.010144, current_train_items 76832.
I0304 19:29:17.348138 22502662377600 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0304 19:29:17.348242 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:29:17.364618 22502662377600 run.py:483] Algo bellman_ford step 2401 current loss 0.032272, current_train_items 76864.
I0304 19:29:17.388445 22502662377600 run.py:483] Algo bellman_ford step 2402 current loss 0.094219, current_train_items 76896.
I0304 19:29:17.419898 22502662377600 run.py:483] Algo bellman_ford step 2403 current loss 0.160787, current_train_items 76928.
I0304 19:29:17.457524 22502662377600 run.py:483] Algo bellman_ford step 2404 current loss 0.152390, current_train_items 76960.
I0304 19:29:17.477501 22502662377600 run.py:483] Algo bellman_ford step 2405 current loss 0.034322, current_train_items 76992.
I0304 19:29:17.493425 22502662377600 run.py:483] Algo bellman_ford step 2406 current loss 0.062969, current_train_items 77024.
I0304 19:29:17.517139 22502662377600 run.py:483] Algo bellman_ford step 2407 current loss 0.062134, current_train_items 77056.
I0304 19:29:17.546810 22502662377600 run.py:483] Algo bellman_ford step 2408 current loss 0.114420, current_train_items 77088.
I0304 19:29:17.579852 22502662377600 run.py:483] Algo bellman_ford step 2409 current loss 0.094520, current_train_items 77120.
I0304 19:29:17.599175 22502662377600 run.py:483] Algo bellman_ford step 2410 current loss 0.006996, current_train_items 77152.
I0304 19:29:17.615369 22502662377600 run.py:483] Algo bellman_ford step 2411 current loss 0.030551, current_train_items 77184.
I0304 19:29:17.640053 22502662377600 run.py:483] Algo bellman_ford step 2412 current loss 0.125404, current_train_items 77216.
I0304 19:29:17.671582 22502662377600 run.py:483] Algo bellman_ford step 2413 current loss 0.102618, current_train_items 77248.
I0304 19:29:17.706380 22502662377600 run.py:483] Algo bellman_ford step 2414 current loss 0.164271, current_train_items 77280.
I0304 19:29:17.726423 22502662377600 run.py:483] Algo bellman_ford step 2415 current loss 0.011188, current_train_items 77312.
I0304 19:29:17.741965 22502662377600 run.py:483] Algo bellman_ford step 2416 current loss 0.038806, current_train_items 77344.
I0304 19:29:17.765817 22502662377600 run.py:483] Algo bellman_ford step 2417 current loss 0.085945, current_train_items 77376.
I0304 19:29:17.797433 22502662377600 run.py:483] Algo bellman_ford step 2418 current loss 0.167625, current_train_items 77408.
I0304 19:29:17.829491 22502662377600 run.py:483] Algo bellman_ford step 2419 current loss 0.098860, current_train_items 77440.
I0304 19:29:17.849354 22502662377600 run.py:483] Algo bellman_ford step 2420 current loss 0.035908, current_train_items 77472.
I0304 19:29:17.865210 22502662377600 run.py:483] Algo bellman_ford step 2421 current loss 0.041961, current_train_items 77504.
I0304 19:29:17.889774 22502662377600 run.py:483] Algo bellman_ford step 2422 current loss 0.137851, current_train_items 77536.
I0304 19:29:17.920524 22502662377600 run.py:483] Algo bellman_ford step 2423 current loss 0.104280, current_train_items 77568.
I0304 19:29:17.955215 22502662377600 run.py:483] Algo bellman_ford step 2424 current loss 0.153062, current_train_items 77600.
I0304 19:29:17.975044 22502662377600 run.py:483] Algo bellman_ford step 2425 current loss 0.034228, current_train_items 77632.
I0304 19:29:17.990988 22502662377600 run.py:483] Algo bellman_ford step 2426 current loss 0.070235, current_train_items 77664.
I0304 19:29:18.015858 22502662377600 run.py:483] Algo bellman_ford step 2427 current loss 0.180862, current_train_items 77696.
I0304 19:29:18.045403 22502662377600 run.py:483] Algo bellman_ford step 2428 current loss 0.122377, current_train_items 77728.
I0304 19:29:18.078902 22502662377600 run.py:483] Algo bellman_ford step 2429 current loss 0.108107, current_train_items 77760.
I0304 19:29:18.098642 22502662377600 run.py:483] Algo bellman_ford step 2430 current loss 0.047806, current_train_items 77792.
I0304 19:29:18.114925 22502662377600 run.py:483] Algo bellman_ford step 2431 current loss 0.055662, current_train_items 77824.
I0304 19:29:18.139345 22502662377600 run.py:483] Algo bellman_ford step 2432 current loss 0.105080, current_train_items 77856.
I0304 19:29:18.170089 22502662377600 run.py:483] Algo bellman_ford step 2433 current loss 0.171800, current_train_items 77888.
I0304 19:29:18.203991 22502662377600 run.py:483] Algo bellman_ford step 2434 current loss 0.153151, current_train_items 77920.
I0304 19:29:18.223424 22502662377600 run.py:483] Algo bellman_ford step 2435 current loss 0.015938, current_train_items 77952.
I0304 19:29:18.239726 22502662377600 run.py:483] Algo bellman_ford step 2436 current loss 0.071228, current_train_items 77984.
I0304 19:29:18.263128 22502662377600 run.py:483] Algo bellman_ford step 2437 current loss 0.098776, current_train_items 78016.
I0304 19:29:18.292845 22502662377600 run.py:483] Algo bellman_ford step 2438 current loss 0.100054, current_train_items 78048.
I0304 19:29:18.325829 22502662377600 run.py:483] Algo bellman_ford step 2439 current loss 0.118242, current_train_items 78080.
I0304 19:29:18.345623 22502662377600 run.py:483] Algo bellman_ford step 2440 current loss 0.023676, current_train_items 78112.
I0304 19:29:18.361706 22502662377600 run.py:483] Algo bellman_ford step 2441 current loss 0.065157, current_train_items 78144.
I0304 19:29:18.386458 22502662377600 run.py:483] Algo bellman_ford step 2442 current loss 0.106761, current_train_items 78176.
I0304 19:29:18.417174 22502662377600 run.py:483] Algo bellman_ford step 2443 current loss 0.109004, current_train_items 78208.
I0304 19:29:18.452258 22502662377600 run.py:483] Algo bellman_ford step 2444 current loss 0.129137, current_train_items 78240.
I0304 19:29:18.472064 22502662377600 run.py:483] Algo bellman_ford step 2445 current loss 0.027092, current_train_items 78272.
I0304 19:29:18.488179 22502662377600 run.py:483] Algo bellman_ford step 2446 current loss 0.046209, current_train_items 78304.
I0304 19:29:18.512339 22502662377600 run.py:483] Algo bellman_ford step 2447 current loss 0.127465, current_train_items 78336.
I0304 19:29:18.541818 22502662377600 run.py:483] Algo bellman_ford step 2448 current loss 0.093591, current_train_items 78368.
I0304 19:29:18.574983 22502662377600 run.py:483] Algo bellman_ford step 2449 current loss 0.129903, current_train_items 78400.
I0304 19:29:18.594672 22502662377600 run.py:483] Algo bellman_ford step 2450 current loss 0.043677, current_train_items 78432.
I0304 19:29:18.602956 22502662377600 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0304 19:29:18.603059 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:29:18.619726 22502662377600 run.py:483] Algo bellman_ford step 2451 current loss 0.048050, current_train_items 78464.
I0304 19:29:18.643712 22502662377600 run.py:483] Algo bellman_ford step 2452 current loss 0.097278, current_train_items 78496.
I0304 19:29:18.673877 22502662377600 run.py:483] Algo bellman_ford step 2453 current loss 0.098332, current_train_items 78528.
I0304 19:29:18.708729 22502662377600 run.py:483] Algo bellman_ford step 2454 current loss 0.134720, current_train_items 78560.
I0304 19:29:18.728542 22502662377600 run.py:483] Algo bellman_ford step 2455 current loss 0.020311, current_train_items 78592.
I0304 19:29:18.744377 22502662377600 run.py:483] Algo bellman_ford step 2456 current loss 0.038575, current_train_items 78624.
I0304 19:29:18.767095 22502662377600 run.py:483] Algo bellman_ford step 2457 current loss 0.049753, current_train_items 78656.
I0304 19:29:18.797685 22502662377600 run.py:483] Algo bellman_ford step 2458 current loss 0.108699, current_train_items 78688.
I0304 19:29:18.832729 22502662377600 run.py:483] Algo bellman_ford step 2459 current loss 0.184581, current_train_items 78720.
I0304 19:29:18.852680 22502662377600 run.py:483] Algo bellman_ford step 2460 current loss 0.016343, current_train_items 78752.
I0304 19:29:18.869382 22502662377600 run.py:483] Algo bellman_ford step 2461 current loss 0.055001, current_train_items 78784.
I0304 19:29:18.892212 22502662377600 run.py:483] Algo bellman_ford step 2462 current loss 0.087134, current_train_items 78816.
I0304 19:29:18.924053 22502662377600 run.py:483] Algo bellman_ford step 2463 current loss 0.121752, current_train_items 78848.
I0304 19:29:18.958098 22502662377600 run.py:483] Algo bellman_ford step 2464 current loss 0.101070, current_train_items 78880.
I0304 19:29:18.977406 22502662377600 run.py:483] Algo bellman_ford step 2465 current loss 0.008035, current_train_items 78912.
I0304 19:29:18.993540 22502662377600 run.py:483] Algo bellman_ford step 2466 current loss 0.064599, current_train_items 78944.
I0304 19:29:19.017452 22502662377600 run.py:483] Algo bellman_ford step 2467 current loss 0.091780, current_train_items 78976.
I0304 19:29:19.048599 22502662377600 run.py:483] Algo bellman_ford step 2468 current loss 0.183262, current_train_items 79008.
I0304 19:29:19.083516 22502662377600 run.py:483] Algo bellman_ford step 2469 current loss 0.195289, current_train_items 79040.
I0304 19:29:19.103512 22502662377600 run.py:483] Algo bellman_ford step 2470 current loss 0.044612, current_train_items 79072.
I0304 19:29:19.119653 22502662377600 run.py:483] Algo bellman_ford step 2471 current loss 0.078121, current_train_items 79104.
I0304 19:29:19.143566 22502662377600 run.py:483] Algo bellman_ford step 2472 current loss 0.141411, current_train_items 79136.
I0304 19:29:19.175104 22502662377600 run.py:483] Algo bellman_ford step 2473 current loss 0.219773, current_train_items 79168.
I0304 19:29:19.208200 22502662377600 run.py:483] Algo bellman_ford step 2474 current loss 0.120244, current_train_items 79200.
I0304 19:29:19.228090 22502662377600 run.py:483] Algo bellman_ford step 2475 current loss 0.024963, current_train_items 79232.
I0304 19:29:19.244335 22502662377600 run.py:483] Algo bellman_ford step 2476 current loss 0.049832, current_train_items 79264.
I0304 19:29:19.266400 22502662377600 run.py:483] Algo bellman_ford step 2477 current loss 0.073345, current_train_items 79296.
I0304 19:29:19.296072 22502662377600 run.py:483] Algo bellman_ford step 2478 current loss 0.237973, current_train_items 79328.
I0304 19:29:19.331122 22502662377600 run.py:483] Algo bellman_ford step 2479 current loss 0.239071, current_train_items 79360.
I0304 19:29:19.350420 22502662377600 run.py:483] Algo bellman_ford step 2480 current loss 0.028725, current_train_items 79392.
I0304 19:29:19.366209 22502662377600 run.py:483] Algo bellman_ford step 2481 current loss 0.067887, current_train_items 79424.
I0304 19:29:19.390398 22502662377600 run.py:483] Algo bellman_ford step 2482 current loss 0.125136, current_train_items 79456.
I0304 19:29:19.421188 22502662377600 run.py:483] Algo bellman_ford step 2483 current loss 0.233495, current_train_items 79488.
I0304 19:29:19.456474 22502662377600 run.py:483] Algo bellman_ford step 2484 current loss 0.229996, current_train_items 79520.
I0304 19:29:19.476545 22502662377600 run.py:483] Algo bellman_ford step 2485 current loss 0.024180, current_train_items 79552.
I0304 19:29:19.492868 22502662377600 run.py:483] Algo bellman_ford step 2486 current loss 0.061680, current_train_items 79584.
I0304 19:29:19.517103 22502662377600 run.py:483] Algo bellman_ford step 2487 current loss 0.146313, current_train_items 79616.
I0304 19:29:19.544928 22502662377600 run.py:483] Algo bellman_ford step 2488 current loss 0.065350, current_train_items 79648.
I0304 19:29:19.579177 22502662377600 run.py:483] Algo bellman_ford step 2489 current loss 0.147343, current_train_items 79680.
I0304 19:29:19.598989 22502662377600 run.py:483] Algo bellman_ford step 2490 current loss 0.042716, current_train_items 79712.
I0304 19:29:19.615459 22502662377600 run.py:483] Algo bellman_ford step 2491 current loss 0.060025, current_train_items 79744.
I0304 19:29:19.638496 22502662377600 run.py:483] Algo bellman_ford step 2492 current loss 0.165138, current_train_items 79776.
I0304 19:29:19.668795 22502662377600 run.py:483] Algo bellman_ford step 2493 current loss 0.118878, current_train_items 79808.
I0304 19:29:19.701595 22502662377600 run.py:483] Algo bellman_ford step 2494 current loss 0.129437, current_train_items 79840.
I0304 19:29:19.721357 22502662377600 run.py:483] Algo bellman_ford step 2495 current loss 0.049604, current_train_items 79872.
I0304 19:29:19.737954 22502662377600 run.py:483] Algo bellman_ford step 2496 current loss 0.029693, current_train_items 79904.
I0304 19:29:19.760934 22502662377600 run.py:483] Algo bellman_ford step 2497 current loss 0.083841, current_train_items 79936.
I0304 19:29:19.790449 22502662377600 run.py:483] Algo bellman_ford step 2498 current loss 0.211042, current_train_items 79968.
I0304 19:29:19.824068 22502662377600 run.py:483] Algo bellman_ford step 2499 current loss 0.191048, current_train_items 80000.
I0304 19:29:19.843876 22502662377600 run.py:483] Algo bellman_ford step 2500 current loss 0.028455, current_train_items 80032.
I0304 19:29:19.851888 22502662377600 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0304 19:29:19.851993 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:19.868925 22502662377600 run.py:483] Algo bellman_ford step 2501 current loss 0.066034, current_train_items 80064.
I0304 19:29:19.893645 22502662377600 run.py:483] Algo bellman_ford step 2502 current loss 0.164890, current_train_items 80096.
I0304 19:29:19.923403 22502662377600 run.py:483] Algo bellman_ford step 2503 current loss 0.098538, current_train_items 80128.
I0304 19:29:19.958593 22502662377600 run.py:483] Algo bellman_ford step 2504 current loss 0.161270, current_train_items 80160.
I0304 19:29:19.978830 22502662377600 run.py:483] Algo bellman_ford step 2505 current loss 0.017157, current_train_items 80192.
I0304 19:29:19.994742 22502662377600 run.py:483] Algo bellman_ford step 2506 current loss 0.048599, current_train_items 80224.
I0304 19:29:20.018926 22502662377600 run.py:483] Algo bellman_ford step 2507 current loss 0.072484, current_train_items 80256.
I0304 19:29:20.048421 22502662377600 run.py:483] Algo bellman_ford step 2508 current loss 0.116183, current_train_items 80288.
I0304 19:29:20.083576 22502662377600 run.py:483] Algo bellman_ford step 2509 current loss 0.185608, current_train_items 80320.
I0304 19:29:20.103046 22502662377600 run.py:483] Algo bellman_ford step 2510 current loss 0.031160, current_train_items 80352.
I0304 19:29:20.119451 22502662377600 run.py:483] Algo bellman_ford step 2511 current loss 0.049845, current_train_items 80384.
I0304 19:29:20.143298 22502662377600 run.py:483] Algo bellman_ford step 2512 current loss 0.126222, current_train_items 80416.
I0304 19:29:20.172998 22502662377600 run.py:483] Algo bellman_ford step 2513 current loss 0.145913, current_train_items 80448.
I0304 19:29:20.207823 22502662377600 run.py:483] Algo bellman_ford step 2514 current loss 0.135580, current_train_items 80480.
I0304 19:29:20.227167 22502662377600 run.py:483] Algo bellman_ford step 2515 current loss 0.028404, current_train_items 80512.
I0304 19:29:20.243412 22502662377600 run.py:483] Algo bellman_ford step 2516 current loss 0.047940, current_train_items 80544.
I0304 19:29:20.267142 22502662377600 run.py:483] Algo bellman_ford step 2517 current loss 0.182453, current_train_items 80576.
I0304 19:29:20.296689 22502662377600 run.py:483] Algo bellman_ford step 2518 current loss 0.103878, current_train_items 80608.
I0304 19:29:20.331138 22502662377600 run.py:483] Algo bellman_ford step 2519 current loss 0.193448, current_train_items 80640.
I0304 19:29:20.350472 22502662377600 run.py:483] Algo bellman_ford step 2520 current loss 0.023148, current_train_items 80672.
I0304 19:29:20.366768 22502662377600 run.py:483] Algo bellman_ford step 2521 current loss 0.106054, current_train_items 80704.
I0304 19:29:20.388995 22502662377600 run.py:483] Algo bellman_ford step 2522 current loss 0.049511, current_train_items 80736.
I0304 19:29:20.418490 22502662377600 run.py:483] Algo bellman_ford step 2523 current loss 0.061456, current_train_items 80768.
I0304 19:29:20.451522 22502662377600 run.py:483] Algo bellman_ford step 2524 current loss 0.101029, current_train_items 80800.
I0304 19:29:20.471197 22502662377600 run.py:483] Algo bellman_ford step 2525 current loss 0.011553, current_train_items 80832.
I0304 19:29:20.487359 22502662377600 run.py:483] Algo bellman_ford step 2526 current loss 0.061289, current_train_items 80864.
I0304 19:29:20.510859 22502662377600 run.py:483] Algo bellman_ford step 2527 current loss 0.130561, current_train_items 80896.
I0304 19:29:20.540248 22502662377600 run.py:483] Algo bellman_ford step 2528 current loss 0.078806, current_train_items 80928.
I0304 19:29:20.574859 22502662377600 run.py:483] Algo bellman_ford step 2529 current loss 0.211729, current_train_items 80960.
I0304 19:29:20.594543 22502662377600 run.py:483] Algo bellman_ford step 2530 current loss 0.014244, current_train_items 80992.
I0304 19:29:20.610289 22502662377600 run.py:483] Algo bellman_ford step 2531 current loss 0.080490, current_train_items 81024.
I0304 19:29:20.633847 22502662377600 run.py:483] Algo bellman_ford step 2532 current loss 0.142363, current_train_items 81056.
I0304 19:29:20.664058 22502662377600 run.py:483] Algo bellman_ford step 2533 current loss 0.105279, current_train_items 81088.
I0304 19:29:20.697217 22502662377600 run.py:483] Algo bellman_ford step 2534 current loss 0.125017, current_train_items 81120.
I0304 19:29:20.716792 22502662377600 run.py:483] Algo bellman_ford step 2535 current loss 0.016586, current_train_items 81152.
I0304 19:29:20.732655 22502662377600 run.py:483] Algo bellman_ford step 2536 current loss 0.092281, current_train_items 81184.
W0304 19:29:20.749232 22502662377600 samplers.py:155] Increasing hint lengh from 9 to 10
I0304 19:29:27.457187 22502662377600 run.py:483] Algo bellman_ford step 2537 current loss 0.178038, current_train_items 81216.
I0304 19:29:27.489944 22502662377600 run.py:483] Algo bellman_ford step 2538 current loss 0.133754, current_train_items 81248.
I0304 19:29:27.524687 22502662377600 run.py:483] Algo bellman_ford step 2539 current loss 0.166648, current_train_items 81280.
I0304 19:29:27.545228 22502662377600 run.py:483] Algo bellman_ford step 2540 current loss 0.054683, current_train_items 81312.
I0304 19:29:27.561354 22502662377600 run.py:483] Algo bellman_ford step 2541 current loss 0.035811, current_train_items 81344.
I0304 19:29:27.585384 22502662377600 run.py:483] Algo bellman_ford step 2542 current loss 0.303728, current_train_items 81376.
I0304 19:29:27.615184 22502662377600 run.py:483] Algo bellman_ford step 2543 current loss 0.244108, current_train_items 81408.
I0304 19:29:27.649246 22502662377600 run.py:483] Algo bellman_ford step 2544 current loss 0.184678, current_train_items 81440.
I0304 19:29:27.669526 22502662377600 run.py:483] Algo bellman_ford step 2545 current loss 0.019837, current_train_items 81472.
I0304 19:29:27.685624 22502662377600 run.py:483] Algo bellman_ford step 2546 current loss 0.072430, current_train_items 81504.
I0304 19:29:27.709753 22502662377600 run.py:483] Algo bellman_ford step 2547 current loss 0.137288, current_train_items 81536.
I0304 19:29:27.741652 22502662377600 run.py:483] Algo bellman_ford step 2548 current loss 0.148031, current_train_items 81568.
I0304 19:29:27.774736 22502662377600 run.py:483] Algo bellman_ford step 2549 current loss 0.129587, current_train_items 81600.
I0304 19:29:27.794741 22502662377600 run.py:483] Algo bellman_ford step 2550 current loss 0.015269, current_train_items 81632.
I0304 19:29:27.804804 22502662377600 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0304 19:29:27.804914 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.978, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:29:27.822052 22502662377600 run.py:483] Algo bellman_ford step 2551 current loss 0.038086, current_train_items 81664.
I0304 19:29:27.846695 22502662377600 run.py:483] Algo bellman_ford step 2552 current loss 0.133471, current_train_items 81696.
I0304 19:29:27.878929 22502662377600 run.py:483] Algo bellman_ford step 2553 current loss 0.267561, current_train_items 81728.
I0304 19:29:27.913157 22502662377600 run.py:483] Algo bellman_ford step 2554 current loss 0.263948, current_train_items 81760.
I0304 19:29:27.933182 22502662377600 run.py:483] Algo bellman_ford step 2555 current loss 0.013112, current_train_items 81792.
I0304 19:29:27.949441 22502662377600 run.py:483] Algo bellman_ford step 2556 current loss 0.066862, current_train_items 81824.
I0304 19:29:27.973590 22502662377600 run.py:483] Algo bellman_ford step 2557 current loss 0.072549, current_train_items 81856.
I0304 19:29:28.005377 22502662377600 run.py:483] Algo bellman_ford step 2558 current loss 0.129087, current_train_items 81888.
I0304 19:29:28.040625 22502662377600 run.py:483] Algo bellman_ford step 2559 current loss 0.163582, current_train_items 81920.
I0304 19:29:28.060457 22502662377600 run.py:483] Algo bellman_ford step 2560 current loss 0.035196, current_train_items 81952.
I0304 19:29:28.077165 22502662377600 run.py:483] Algo bellman_ford step 2561 current loss 0.048956, current_train_items 81984.
I0304 19:29:28.100607 22502662377600 run.py:483] Algo bellman_ford step 2562 current loss 0.116509, current_train_items 82016.
I0304 19:29:28.132918 22502662377600 run.py:483] Algo bellman_ford step 2563 current loss 0.169361, current_train_items 82048.
I0304 19:29:28.166926 22502662377600 run.py:483] Algo bellman_ford step 2564 current loss 0.113008, current_train_items 82080.
I0304 19:29:28.186253 22502662377600 run.py:483] Algo bellman_ford step 2565 current loss 0.009681, current_train_items 82112.
I0304 19:29:28.202656 22502662377600 run.py:483] Algo bellman_ford step 2566 current loss 0.059991, current_train_items 82144.
I0304 19:29:28.226116 22502662377600 run.py:483] Algo bellman_ford step 2567 current loss 0.085109, current_train_items 82176.
I0304 19:29:28.257998 22502662377600 run.py:483] Algo bellman_ford step 2568 current loss 0.125264, current_train_items 82208.
I0304 19:29:28.291093 22502662377600 run.py:483] Algo bellman_ford step 2569 current loss 0.100430, current_train_items 82240.
I0304 19:29:28.310852 22502662377600 run.py:483] Algo bellman_ford step 2570 current loss 0.013241, current_train_items 82272.
I0304 19:29:28.327094 22502662377600 run.py:483] Algo bellman_ford step 2571 current loss 0.044724, current_train_items 82304.
I0304 19:29:28.349500 22502662377600 run.py:483] Algo bellman_ford step 2572 current loss 0.082136, current_train_items 82336.
I0304 19:29:28.380443 22502662377600 run.py:483] Algo bellman_ford step 2573 current loss 0.107165, current_train_items 82368.
I0304 19:29:28.415480 22502662377600 run.py:483] Algo bellman_ford step 2574 current loss 0.259086, current_train_items 82400.
I0304 19:29:28.435519 22502662377600 run.py:483] Algo bellman_ford step 2575 current loss 0.028056, current_train_items 82432.
I0304 19:29:28.451506 22502662377600 run.py:483] Algo bellman_ford step 2576 current loss 0.029891, current_train_items 82464.
I0304 19:29:28.474378 22502662377600 run.py:483] Algo bellman_ford step 2577 current loss 0.048779, current_train_items 82496.
I0304 19:29:28.506046 22502662377600 run.py:483] Algo bellman_ford step 2578 current loss 0.112837, current_train_items 82528.
I0304 19:29:28.538635 22502662377600 run.py:483] Algo bellman_ford step 2579 current loss 0.159895, current_train_items 82560.
I0304 19:29:28.558218 22502662377600 run.py:483] Algo bellman_ford step 2580 current loss 0.015380, current_train_items 82592.
I0304 19:29:28.574502 22502662377600 run.py:483] Algo bellman_ford step 2581 current loss 0.028206, current_train_items 82624.
I0304 19:29:28.598031 22502662377600 run.py:483] Algo bellman_ford step 2582 current loss 0.092191, current_train_items 82656.
I0304 19:29:28.628664 22502662377600 run.py:483] Algo bellman_ford step 2583 current loss 0.084837, current_train_items 82688.
I0304 19:29:28.663392 22502662377600 run.py:483] Algo bellman_ford step 2584 current loss 0.110074, current_train_items 82720.
I0304 19:29:28.683182 22502662377600 run.py:483] Algo bellman_ford step 2585 current loss 0.011001, current_train_items 82752.
I0304 19:29:28.699516 22502662377600 run.py:483] Algo bellman_ford step 2586 current loss 0.069312, current_train_items 82784.
I0304 19:29:28.722809 22502662377600 run.py:483] Algo bellman_ford step 2587 current loss 0.064106, current_train_items 82816.
I0304 19:29:28.753374 22502662377600 run.py:483] Algo bellman_ford step 2588 current loss 0.068866, current_train_items 82848.
I0304 19:29:28.787529 22502662377600 run.py:483] Algo bellman_ford step 2589 current loss 0.151876, current_train_items 82880.
I0304 19:29:28.807348 22502662377600 run.py:483] Algo bellman_ford step 2590 current loss 0.014380, current_train_items 82912.
I0304 19:29:28.823878 22502662377600 run.py:483] Algo bellman_ford step 2591 current loss 0.036560, current_train_items 82944.
I0304 19:29:28.847973 22502662377600 run.py:483] Algo bellman_ford step 2592 current loss 0.111033, current_train_items 82976.
I0304 19:29:28.879952 22502662377600 run.py:483] Algo bellman_ford step 2593 current loss 0.180271, current_train_items 83008.
I0304 19:29:28.911798 22502662377600 run.py:483] Algo bellman_ford step 2594 current loss 0.148621, current_train_items 83040.
I0304 19:29:28.931133 22502662377600 run.py:483] Algo bellman_ford step 2595 current loss 0.029420, current_train_items 83072.
I0304 19:29:28.947642 22502662377600 run.py:483] Algo bellman_ford step 2596 current loss 0.071225, current_train_items 83104.
I0304 19:29:28.972220 22502662377600 run.py:483] Algo bellman_ford step 2597 current loss 0.072126, current_train_items 83136.
I0304 19:29:29.003304 22502662377600 run.py:483] Algo bellman_ford step 2598 current loss 0.081643, current_train_items 83168.
I0304 19:29:29.036569 22502662377600 run.py:483] Algo bellman_ford step 2599 current loss 0.085468, current_train_items 83200.
I0304 19:29:29.056382 22502662377600 run.py:483] Algo bellman_ford step 2600 current loss 0.011118, current_train_items 83232.
I0304 19:29:29.064616 22502662377600 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0304 19:29:29.064721 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.978, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:29:29.094650 22502662377600 run.py:483] Algo bellman_ford step 2601 current loss 0.024639, current_train_items 83264.
I0304 19:29:29.118612 22502662377600 run.py:483] Algo bellman_ford step 2602 current loss 0.056282, current_train_items 83296.
I0304 19:29:29.148829 22502662377600 run.py:483] Algo bellman_ford step 2603 current loss 0.063169, current_train_items 83328.
I0304 19:29:29.183442 22502662377600 run.py:483] Algo bellman_ford step 2604 current loss 0.096935, current_train_items 83360.
I0304 19:29:29.203686 22502662377600 run.py:483] Algo bellman_ford step 2605 current loss 0.025008, current_train_items 83392.
I0304 19:29:29.219570 22502662377600 run.py:483] Algo bellman_ford step 2606 current loss 0.031041, current_train_items 83424.
I0304 19:29:29.243760 22502662377600 run.py:483] Algo bellman_ford step 2607 current loss 0.088664, current_train_items 83456.
I0304 19:29:29.275732 22502662377600 run.py:483] Algo bellman_ford step 2608 current loss 0.131560, current_train_items 83488.
I0304 19:29:29.310161 22502662377600 run.py:483] Algo bellman_ford step 2609 current loss 0.145254, current_train_items 83520.
I0304 19:29:29.329918 22502662377600 run.py:483] Algo bellman_ford step 2610 current loss 0.034570, current_train_items 83552.
I0304 19:29:29.346042 22502662377600 run.py:483] Algo bellman_ford step 2611 current loss 0.030902, current_train_items 83584.
I0304 19:29:29.369817 22502662377600 run.py:483] Algo bellman_ford step 2612 current loss 0.048667, current_train_items 83616.
I0304 19:29:29.400650 22502662377600 run.py:483] Algo bellman_ford step 2613 current loss 0.116742, current_train_items 83648.
I0304 19:29:29.433506 22502662377600 run.py:483] Algo bellman_ford step 2614 current loss 0.105137, current_train_items 83680.
I0304 19:29:29.453604 22502662377600 run.py:483] Algo bellman_ford step 2615 current loss 0.012799, current_train_items 83712.
I0304 19:29:29.469472 22502662377600 run.py:483] Algo bellman_ford step 2616 current loss 0.055401, current_train_items 83744.
I0304 19:29:29.494557 22502662377600 run.py:483] Algo bellman_ford step 2617 current loss 0.124940, current_train_items 83776.
I0304 19:29:29.526718 22502662377600 run.py:483] Algo bellman_ford step 2618 current loss 0.100280, current_train_items 83808.
I0304 19:29:29.558918 22502662377600 run.py:483] Algo bellman_ford step 2619 current loss 0.122538, current_train_items 83840.
I0304 19:29:29.578971 22502662377600 run.py:483] Algo bellman_ford step 2620 current loss 0.018290, current_train_items 83872.
I0304 19:29:29.595132 22502662377600 run.py:483] Algo bellman_ford step 2621 current loss 0.033887, current_train_items 83904.
I0304 19:29:29.619366 22502662377600 run.py:483] Algo bellman_ford step 2622 current loss 0.124206, current_train_items 83936.
I0304 19:29:29.650190 22502662377600 run.py:483] Algo bellman_ford step 2623 current loss 0.119032, current_train_items 83968.
I0304 19:29:29.683178 22502662377600 run.py:483] Algo bellman_ford step 2624 current loss 0.111650, current_train_items 84000.
I0304 19:29:29.703171 22502662377600 run.py:483] Algo bellman_ford step 2625 current loss 0.011665, current_train_items 84032.
I0304 19:29:29.719574 22502662377600 run.py:483] Algo bellman_ford step 2626 current loss 0.021140, current_train_items 84064.
I0304 19:29:29.743475 22502662377600 run.py:483] Algo bellman_ford step 2627 current loss 0.078075, current_train_items 84096.
I0304 19:29:29.774951 22502662377600 run.py:483] Algo bellman_ford step 2628 current loss 0.127602, current_train_items 84128.
I0304 19:29:29.808564 22502662377600 run.py:483] Algo bellman_ford step 2629 current loss 0.142042, current_train_items 84160.
I0304 19:29:29.828468 22502662377600 run.py:483] Algo bellman_ford step 2630 current loss 0.015348, current_train_items 84192.
I0304 19:29:29.844625 22502662377600 run.py:483] Algo bellman_ford step 2631 current loss 0.059269, current_train_items 84224.
I0304 19:29:29.868104 22502662377600 run.py:483] Algo bellman_ford step 2632 current loss 0.084230, current_train_items 84256.
I0304 19:29:29.898361 22502662377600 run.py:483] Algo bellman_ford step 2633 current loss 0.066152, current_train_items 84288.
I0304 19:29:29.933917 22502662377600 run.py:483] Algo bellman_ford step 2634 current loss 0.157288, current_train_items 84320.
I0304 19:29:29.953900 22502662377600 run.py:483] Algo bellman_ford step 2635 current loss 0.014186, current_train_items 84352.
I0304 19:29:29.969913 22502662377600 run.py:483] Algo bellman_ford step 2636 current loss 0.068229, current_train_items 84384.
I0304 19:29:29.994210 22502662377600 run.py:483] Algo bellman_ford step 2637 current loss 0.139099, current_train_items 84416.
I0304 19:29:30.026047 22502662377600 run.py:483] Algo bellman_ford step 2638 current loss 0.088331, current_train_items 84448.
I0304 19:29:30.060422 22502662377600 run.py:483] Algo bellman_ford step 2639 current loss 0.128878, current_train_items 84480.
I0304 19:29:30.080041 22502662377600 run.py:483] Algo bellman_ford step 2640 current loss 0.009432, current_train_items 84512.
I0304 19:29:30.096126 22502662377600 run.py:483] Algo bellman_ford step 2641 current loss 0.031445, current_train_items 84544.
I0304 19:29:30.119109 22502662377600 run.py:483] Algo bellman_ford step 2642 current loss 0.076054, current_train_items 84576.
I0304 19:29:30.150058 22502662377600 run.py:483] Algo bellman_ford step 2643 current loss 0.120405, current_train_items 84608.
I0304 19:29:30.185332 22502662377600 run.py:483] Algo bellman_ford step 2644 current loss 0.128562, current_train_items 84640.
I0304 19:29:30.205169 22502662377600 run.py:483] Algo bellman_ford step 2645 current loss 0.017659, current_train_items 84672.
I0304 19:29:30.221185 22502662377600 run.py:483] Algo bellman_ford step 2646 current loss 0.063115, current_train_items 84704.
I0304 19:29:30.244715 22502662377600 run.py:483] Algo bellman_ford step 2647 current loss 0.112195, current_train_items 84736.
I0304 19:29:30.276250 22502662377600 run.py:483] Algo bellman_ford step 2648 current loss 0.089231, current_train_items 84768.
I0304 19:29:30.312380 22502662377600 run.py:483] Algo bellman_ford step 2649 current loss 0.166200, current_train_items 84800.
I0304 19:29:30.332457 22502662377600 run.py:483] Algo bellman_ford step 2650 current loss 0.015506, current_train_items 84832.
I0304 19:29:30.340544 22502662377600 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0304 19:29:30.340650 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:29:30.357152 22502662377600 run.py:483] Algo bellman_ford step 2651 current loss 0.047194, current_train_items 84864.
I0304 19:29:30.381053 22502662377600 run.py:483] Algo bellman_ford step 2652 current loss 0.100386, current_train_items 84896.
I0304 19:29:30.413559 22502662377600 run.py:483] Algo bellman_ford step 2653 current loss 0.123670, current_train_items 84928.
I0304 19:29:30.447617 22502662377600 run.py:483] Algo bellman_ford step 2654 current loss 0.084579, current_train_items 84960.
I0304 19:29:30.467616 22502662377600 run.py:483] Algo bellman_ford step 2655 current loss 0.010392, current_train_items 84992.
I0304 19:29:30.483542 22502662377600 run.py:483] Algo bellman_ford step 2656 current loss 0.065527, current_train_items 85024.
I0304 19:29:30.507367 22502662377600 run.py:483] Algo bellman_ford step 2657 current loss 0.053084, current_train_items 85056.
I0304 19:29:30.537459 22502662377600 run.py:483] Algo bellman_ford step 2658 current loss 0.061743, current_train_items 85088.
I0304 19:29:30.573507 22502662377600 run.py:483] Algo bellman_ford step 2659 current loss 0.140945, current_train_items 85120.
I0304 19:29:30.593592 22502662377600 run.py:483] Algo bellman_ford step 2660 current loss 0.006985, current_train_items 85152.
I0304 19:29:30.610367 22502662377600 run.py:483] Algo bellman_ford step 2661 current loss 0.036054, current_train_items 85184.
I0304 19:29:30.633644 22502662377600 run.py:483] Algo bellman_ford step 2662 current loss 0.093829, current_train_items 85216.
I0304 19:29:30.664967 22502662377600 run.py:483] Algo bellman_ford step 2663 current loss 0.094993, current_train_items 85248.
I0304 19:29:30.701103 22502662377600 run.py:483] Algo bellman_ford step 2664 current loss 0.161389, current_train_items 85280.
I0304 19:29:30.720660 22502662377600 run.py:483] Algo bellman_ford step 2665 current loss 0.011548, current_train_items 85312.
I0304 19:29:30.736934 22502662377600 run.py:483] Algo bellman_ford step 2666 current loss 0.066803, current_train_items 85344.
I0304 19:29:30.761493 22502662377600 run.py:483] Algo bellman_ford step 2667 current loss 0.082656, current_train_items 85376.
I0304 19:29:30.793040 22502662377600 run.py:483] Algo bellman_ford step 2668 current loss 0.150010, current_train_items 85408.
I0304 19:29:30.826167 22502662377600 run.py:483] Algo bellman_ford step 2669 current loss 0.107267, current_train_items 85440.
I0304 19:29:30.846003 22502662377600 run.py:483] Algo bellman_ford step 2670 current loss 0.015010, current_train_items 85472.
I0304 19:29:30.862467 22502662377600 run.py:483] Algo bellman_ford step 2671 current loss 0.023941, current_train_items 85504.
I0304 19:29:30.886370 22502662377600 run.py:483] Algo bellman_ford step 2672 current loss 0.067035, current_train_items 85536.
I0304 19:29:30.919130 22502662377600 run.py:483] Algo bellman_ford step 2673 current loss 0.124167, current_train_items 85568.
I0304 19:29:30.954506 22502662377600 run.py:483] Algo bellman_ford step 2674 current loss 0.123044, current_train_items 85600.
I0304 19:29:30.974685 22502662377600 run.py:483] Algo bellman_ford step 2675 current loss 0.010369, current_train_items 85632.
I0304 19:29:30.990886 22502662377600 run.py:483] Algo bellman_ford step 2676 current loss 0.058053, current_train_items 85664.
I0304 19:29:31.015346 22502662377600 run.py:483] Algo bellman_ford step 2677 current loss 0.087865, current_train_items 85696.
I0304 19:29:31.044968 22502662377600 run.py:483] Algo bellman_ford step 2678 current loss 0.048943, current_train_items 85728.
I0304 19:29:31.077554 22502662377600 run.py:483] Algo bellman_ford step 2679 current loss 0.069881, current_train_items 85760.
I0304 19:29:31.096891 22502662377600 run.py:483] Algo bellman_ford step 2680 current loss 0.008547, current_train_items 85792.
I0304 19:29:31.113133 22502662377600 run.py:483] Algo bellman_ford step 2681 current loss 0.043264, current_train_items 85824.
I0304 19:29:31.136897 22502662377600 run.py:483] Algo bellman_ford step 2682 current loss 0.091895, current_train_items 85856.
I0304 19:29:31.169470 22502662377600 run.py:483] Algo bellman_ford step 2683 current loss 0.096738, current_train_items 85888.
I0304 19:29:31.203333 22502662377600 run.py:483] Algo bellman_ford step 2684 current loss 0.104892, current_train_items 85920.
I0304 19:29:31.223432 22502662377600 run.py:483] Algo bellman_ford step 2685 current loss 0.009957, current_train_items 85952.
I0304 19:29:31.239560 22502662377600 run.py:483] Algo bellman_ford step 2686 current loss 0.058415, current_train_items 85984.
I0304 19:29:31.263301 22502662377600 run.py:483] Algo bellman_ford step 2687 current loss 0.057217, current_train_items 86016.
I0304 19:29:31.294431 22502662377600 run.py:483] Algo bellman_ford step 2688 current loss 0.126904, current_train_items 86048.
I0304 19:29:31.327085 22502662377600 run.py:483] Algo bellman_ford step 2689 current loss 0.106782, current_train_items 86080.
I0304 19:29:31.347223 22502662377600 run.py:483] Algo bellman_ford step 2690 current loss 0.025889, current_train_items 86112.
I0304 19:29:31.363503 22502662377600 run.py:483] Algo bellman_ford step 2691 current loss 0.026823, current_train_items 86144.
I0304 19:29:31.388216 22502662377600 run.py:483] Algo bellman_ford step 2692 current loss 0.072433, current_train_items 86176.
I0304 19:29:31.420825 22502662377600 run.py:483] Algo bellman_ford step 2693 current loss 0.100488, current_train_items 86208.
I0304 19:29:31.454719 22502662377600 run.py:483] Algo bellman_ford step 2694 current loss 0.136238, current_train_items 86240.
I0304 19:29:31.474376 22502662377600 run.py:483] Algo bellman_ford step 2695 current loss 0.011291, current_train_items 86272.
I0304 19:29:31.490371 22502662377600 run.py:483] Algo bellman_ford step 2696 current loss 0.025581, current_train_items 86304.
I0304 19:29:31.515058 22502662377600 run.py:483] Algo bellman_ford step 2697 current loss 0.125307, current_train_items 86336.
I0304 19:29:31.545787 22502662377600 run.py:483] Algo bellman_ford step 2698 current loss 0.126468, current_train_items 86368.
I0304 19:29:31.578169 22502662377600 run.py:483] Algo bellman_ford step 2699 current loss 0.151467, current_train_items 86400.
I0304 19:29:31.597879 22502662377600 run.py:483] Algo bellman_ford step 2700 current loss 0.026370, current_train_items 86432.
I0304 19:29:31.605919 22502662377600 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0304 19:29:31.606023 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:29:31.622727 22502662377600 run.py:483] Algo bellman_ford step 2701 current loss 0.047451, current_train_items 86464.
I0304 19:29:31.647054 22502662377600 run.py:483] Algo bellman_ford step 2702 current loss 0.074357, current_train_items 86496.
I0304 19:29:31.680290 22502662377600 run.py:483] Algo bellman_ford step 2703 current loss 0.096600, current_train_items 86528.
I0304 19:29:31.716518 22502662377600 run.py:483] Algo bellman_ford step 2704 current loss 0.149444, current_train_items 86560.
I0304 19:29:31.736255 22502662377600 run.py:483] Algo bellman_ford step 2705 current loss 0.016336, current_train_items 86592.
I0304 19:29:31.752093 22502662377600 run.py:483] Algo bellman_ford step 2706 current loss 0.047893, current_train_items 86624.
I0304 19:29:31.776072 22502662377600 run.py:483] Algo bellman_ford step 2707 current loss 0.086374, current_train_items 86656.
I0304 19:29:31.806042 22502662377600 run.py:483] Algo bellman_ford step 2708 current loss 0.131642, current_train_items 86688.
I0304 19:29:31.837901 22502662377600 run.py:483] Algo bellman_ford step 2709 current loss 0.126119, current_train_items 86720.
I0304 19:29:31.857686 22502662377600 run.py:483] Algo bellman_ford step 2710 current loss 0.012265, current_train_items 86752.
I0304 19:29:31.874360 22502662377600 run.py:483] Algo bellman_ford step 2711 current loss 0.052691, current_train_items 86784.
I0304 19:29:31.898678 22502662377600 run.py:483] Algo bellman_ford step 2712 current loss 0.101626, current_train_items 86816.
I0304 19:29:31.930442 22502662377600 run.py:483] Algo bellman_ford step 2713 current loss 0.143065, current_train_items 86848.
I0304 19:29:31.966206 22502662377600 run.py:483] Algo bellman_ford step 2714 current loss 0.175819, current_train_items 86880.
I0304 19:29:31.985977 22502662377600 run.py:483] Algo bellman_ford step 2715 current loss 0.045267, current_train_items 86912.
I0304 19:29:32.001687 22502662377600 run.py:483] Algo bellman_ford step 2716 current loss 0.025021, current_train_items 86944.
I0304 19:29:32.026477 22502662377600 run.py:483] Algo bellman_ford step 2717 current loss 0.211814, current_train_items 86976.
I0304 19:29:32.056311 22502662377600 run.py:483] Algo bellman_ford step 2718 current loss 0.173606, current_train_items 87008.
I0304 19:29:32.089852 22502662377600 run.py:483] Algo bellman_ford step 2719 current loss 0.160625, current_train_items 87040.
I0304 19:29:32.109336 22502662377600 run.py:483] Algo bellman_ford step 2720 current loss 0.008200, current_train_items 87072.
I0304 19:29:32.125389 22502662377600 run.py:483] Algo bellman_ford step 2721 current loss 0.113076, current_train_items 87104.
I0304 19:29:32.149207 22502662377600 run.py:483] Algo bellman_ford step 2722 current loss 0.114003, current_train_items 87136.
I0304 19:29:32.181232 22502662377600 run.py:483] Algo bellman_ford step 2723 current loss 0.160526, current_train_items 87168.
I0304 19:29:32.214710 22502662377600 run.py:483] Algo bellman_ford step 2724 current loss 0.094714, current_train_items 87200.
I0304 19:29:32.233993 22502662377600 run.py:483] Algo bellman_ford step 2725 current loss 0.010499, current_train_items 87232.
I0304 19:29:32.250083 22502662377600 run.py:483] Algo bellman_ford step 2726 current loss 0.095832, current_train_items 87264.
I0304 19:29:32.274159 22502662377600 run.py:483] Algo bellman_ford step 2727 current loss 0.178241, current_train_items 87296.
I0304 19:29:32.305332 22502662377600 run.py:483] Algo bellman_ford step 2728 current loss 0.192149, current_train_items 87328.
I0304 19:29:32.338787 22502662377600 run.py:483] Algo bellman_ford step 2729 current loss 0.143306, current_train_items 87360.
I0304 19:29:32.358345 22502662377600 run.py:483] Algo bellman_ford step 2730 current loss 0.012730, current_train_items 87392.
I0304 19:29:32.374473 22502662377600 run.py:483] Algo bellman_ford step 2731 current loss 0.079579, current_train_items 87424.
I0304 19:29:32.397268 22502662377600 run.py:483] Algo bellman_ford step 2732 current loss 0.108118, current_train_items 87456.
I0304 19:29:32.427486 22502662377600 run.py:483] Algo bellman_ford step 2733 current loss 0.138606, current_train_items 87488.
I0304 19:29:32.461415 22502662377600 run.py:483] Algo bellman_ford step 2734 current loss 0.144852, current_train_items 87520.
I0304 19:29:32.481301 22502662377600 run.py:483] Algo bellman_ford step 2735 current loss 0.110600, current_train_items 87552.
I0304 19:29:32.496976 22502662377600 run.py:483] Algo bellman_ford step 2736 current loss 0.101851, current_train_items 87584.
I0304 19:29:32.520528 22502662377600 run.py:483] Algo bellman_ford step 2737 current loss 0.103085, current_train_items 87616.
I0304 19:29:32.550731 22502662377600 run.py:483] Algo bellman_ford step 2738 current loss 0.112733, current_train_items 87648.
I0304 19:29:32.583499 22502662377600 run.py:483] Algo bellman_ford step 2739 current loss 0.144870, current_train_items 87680.
I0304 19:29:32.603147 22502662377600 run.py:483] Algo bellman_ford step 2740 current loss 0.007899, current_train_items 87712.
I0304 19:29:32.619070 22502662377600 run.py:483] Algo bellman_ford step 2741 current loss 0.042250, current_train_items 87744.
I0304 19:29:32.642694 22502662377600 run.py:483] Algo bellman_ford step 2742 current loss 0.090327, current_train_items 87776.
I0304 19:29:32.675245 22502662377600 run.py:483] Algo bellman_ford step 2743 current loss 0.181014, current_train_items 87808.
I0304 19:29:32.708429 22502662377600 run.py:483] Algo bellman_ford step 2744 current loss 0.133163, current_train_items 87840.
I0304 19:29:32.727816 22502662377600 run.py:483] Algo bellman_ford step 2745 current loss 0.011078, current_train_items 87872.
I0304 19:29:32.743696 22502662377600 run.py:483] Algo bellman_ford step 2746 current loss 0.048259, current_train_items 87904.
I0304 19:29:32.767304 22502662377600 run.py:483] Algo bellman_ford step 2747 current loss 0.105679, current_train_items 87936.
I0304 19:29:32.799028 22502662377600 run.py:483] Algo bellman_ford step 2748 current loss 0.096878, current_train_items 87968.
I0304 19:29:32.832415 22502662377600 run.py:483] Algo bellman_ford step 2749 current loss 0.122692, current_train_items 88000.
I0304 19:29:32.851927 22502662377600 run.py:483] Algo bellman_ford step 2750 current loss 0.016404, current_train_items 88032.
I0304 19:29:32.859963 22502662377600 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0304 19:29:32.860068 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:29:32.877073 22502662377600 run.py:483] Algo bellman_ford step 2751 current loss 0.045146, current_train_items 88064.
I0304 19:29:32.901255 22502662377600 run.py:483] Algo bellman_ford step 2752 current loss 0.146676, current_train_items 88096.
I0304 19:29:32.933270 22502662377600 run.py:483] Algo bellman_ford step 2753 current loss 0.176231, current_train_items 88128.
I0304 19:29:32.966089 22502662377600 run.py:483] Algo bellman_ford step 2754 current loss 0.145285, current_train_items 88160.
I0304 19:29:32.985859 22502662377600 run.py:483] Algo bellman_ford step 2755 current loss 0.014629, current_train_items 88192.
I0304 19:29:33.001979 22502662377600 run.py:483] Algo bellman_ford step 2756 current loss 0.044763, current_train_items 88224.
I0304 19:29:33.025210 22502662377600 run.py:483] Algo bellman_ford step 2757 current loss 0.081714, current_train_items 88256.
I0304 19:29:33.057184 22502662377600 run.py:483] Algo bellman_ford step 2758 current loss 0.132617, current_train_items 88288.
I0304 19:29:33.093403 22502662377600 run.py:483] Algo bellman_ford step 2759 current loss 0.198732, current_train_items 88320.
I0304 19:29:33.113050 22502662377600 run.py:483] Algo bellman_ford step 2760 current loss 0.020943, current_train_items 88352.
I0304 19:29:33.129727 22502662377600 run.py:483] Algo bellman_ford step 2761 current loss 0.125704, current_train_items 88384.
I0304 19:29:33.154120 22502662377600 run.py:483] Algo bellman_ford step 2762 current loss 0.355012, current_train_items 88416.
I0304 19:29:33.185404 22502662377600 run.py:483] Algo bellman_ford step 2763 current loss 0.404132, current_train_items 88448.
I0304 19:29:33.218728 22502662377600 run.py:483] Algo bellman_ford step 2764 current loss 0.331942, current_train_items 88480.
I0304 19:29:33.238551 22502662377600 run.py:483] Algo bellman_ford step 2765 current loss 0.022183, current_train_items 88512.
I0304 19:29:33.254938 22502662377600 run.py:483] Algo bellman_ford step 2766 current loss 0.095298, current_train_items 88544.
I0304 19:29:33.278744 22502662377600 run.py:483] Algo bellman_ford step 2767 current loss 0.103334, current_train_items 88576.
I0304 19:29:33.310875 22502662377600 run.py:483] Algo bellman_ford step 2768 current loss 0.221229, current_train_items 88608.
I0304 19:29:33.344424 22502662377600 run.py:483] Algo bellman_ford step 2769 current loss 0.151152, current_train_items 88640.
I0304 19:29:33.364159 22502662377600 run.py:483] Algo bellman_ford step 2770 current loss 0.014296, current_train_items 88672.
I0304 19:29:33.380629 22502662377600 run.py:483] Algo bellman_ford step 2771 current loss 0.059107, current_train_items 88704.
I0304 19:29:33.404771 22502662377600 run.py:483] Algo bellman_ford step 2772 current loss 0.088389, current_train_items 88736.
I0304 19:29:33.435163 22502662377600 run.py:483] Algo bellman_ford step 2773 current loss 0.091152, current_train_items 88768.
I0304 19:29:33.468764 22502662377600 run.py:483] Algo bellman_ford step 2774 current loss 0.088641, current_train_items 88800.
I0304 19:29:33.488423 22502662377600 run.py:483] Algo bellman_ford step 2775 current loss 0.010157, current_train_items 88832.
I0304 19:29:33.505155 22502662377600 run.py:483] Algo bellman_ford step 2776 current loss 0.119770, current_train_items 88864.
I0304 19:29:33.528682 22502662377600 run.py:483] Algo bellman_ford step 2777 current loss 0.070996, current_train_items 88896.
I0304 19:29:33.559266 22502662377600 run.py:483] Algo bellman_ford step 2778 current loss 0.143754, current_train_items 88928.
I0304 19:29:33.593529 22502662377600 run.py:483] Algo bellman_ford step 2779 current loss 0.165284, current_train_items 88960.
I0304 19:29:33.613348 22502662377600 run.py:483] Algo bellman_ford step 2780 current loss 0.011774, current_train_items 88992.
I0304 19:29:33.629522 22502662377600 run.py:483] Algo bellman_ford step 2781 current loss 0.065705, current_train_items 89024.
I0304 19:29:33.653581 22502662377600 run.py:483] Algo bellman_ford step 2782 current loss 0.223292, current_train_items 89056.
I0304 19:29:33.684648 22502662377600 run.py:483] Algo bellman_ford step 2783 current loss 0.143987, current_train_items 89088.
I0304 19:29:33.719219 22502662377600 run.py:483] Algo bellman_ford step 2784 current loss 0.170202, current_train_items 89120.
I0304 19:29:33.739082 22502662377600 run.py:483] Algo bellman_ford step 2785 current loss 0.018183, current_train_items 89152.
I0304 19:29:33.755177 22502662377600 run.py:483] Algo bellman_ford step 2786 current loss 0.042712, current_train_items 89184.
I0304 19:29:33.777643 22502662377600 run.py:483] Algo bellman_ford step 2787 current loss 0.065691, current_train_items 89216.
I0304 19:29:33.808162 22502662377600 run.py:483] Algo bellman_ford step 2788 current loss 0.232760, current_train_items 89248.
I0304 19:29:33.839081 22502662377600 run.py:483] Algo bellman_ford step 2789 current loss 0.206746, current_train_items 89280.
I0304 19:29:33.859029 22502662377600 run.py:483] Algo bellman_ford step 2790 current loss 0.022524, current_train_items 89312.
I0304 19:29:33.875371 22502662377600 run.py:483] Algo bellman_ford step 2791 current loss 0.051339, current_train_items 89344.
I0304 19:29:33.898879 22502662377600 run.py:483] Algo bellman_ford step 2792 current loss 0.074502, current_train_items 89376.
I0304 19:29:33.929420 22502662377600 run.py:483] Algo bellman_ford step 2793 current loss 0.144286, current_train_items 89408.
I0304 19:29:33.965254 22502662377600 run.py:483] Algo bellman_ford step 2794 current loss 0.153223, current_train_items 89440.
I0304 19:29:33.985087 22502662377600 run.py:483] Algo bellman_ford step 2795 current loss 0.013630, current_train_items 89472.
I0304 19:29:34.001716 22502662377600 run.py:483] Algo bellman_ford step 2796 current loss 0.050721, current_train_items 89504.
I0304 19:29:34.024987 22502662377600 run.py:483] Algo bellman_ford step 2797 current loss 0.147262, current_train_items 89536.
I0304 19:29:34.056684 22502662377600 run.py:483] Algo bellman_ford step 2798 current loss 0.163356, current_train_items 89568.
I0304 19:29:34.090415 22502662377600 run.py:483] Algo bellman_ford step 2799 current loss 0.195275, current_train_items 89600.
I0304 19:29:34.110423 22502662377600 run.py:483] Algo bellman_ford step 2800 current loss 0.010356, current_train_items 89632.
I0304 19:29:34.118477 22502662377600 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0304 19:29:34.118583 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:29:34.135386 22502662377600 run.py:483] Algo bellman_ford step 2801 current loss 0.037997, current_train_items 89664.
I0304 19:29:34.160302 22502662377600 run.py:483] Algo bellman_ford step 2802 current loss 0.092769, current_train_items 89696.
I0304 19:29:34.192499 22502662377600 run.py:483] Algo bellman_ford step 2803 current loss 0.099297, current_train_items 89728.
I0304 19:29:34.226907 22502662377600 run.py:483] Algo bellman_ford step 2804 current loss 0.090115, current_train_items 89760.
I0304 19:29:34.246988 22502662377600 run.py:483] Algo bellman_ford step 2805 current loss 0.017967, current_train_items 89792.
I0304 19:29:34.263336 22502662377600 run.py:483] Algo bellman_ford step 2806 current loss 0.062733, current_train_items 89824.
I0304 19:29:34.286933 22502662377600 run.py:483] Algo bellman_ford step 2807 current loss 0.076526, current_train_items 89856.
I0304 19:29:34.319017 22502662377600 run.py:483] Algo bellman_ford step 2808 current loss 0.217763, current_train_items 89888.
I0304 19:29:34.353795 22502662377600 run.py:483] Algo bellman_ford step 2809 current loss 0.173200, current_train_items 89920.
I0304 19:29:34.373584 22502662377600 run.py:483] Algo bellman_ford step 2810 current loss 0.005820, current_train_items 89952.
I0304 19:29:34.390202 22502662377600 run.py:483] Algo bellman_ford step 2811 current loss 0.075392, current_train_items 89984.
I0304 19:29:34.413240 22502662377600 run.py:483] Algo bellman_ford step 2812 current loss 0.096596, current_train_items 90016.
I0304 19:29:34.445531 22502662377600 run.py:483] Algo bellman_ford step 2813 current loss 0.164879, current_train_items 90048.
I0304 19:29:34.478788 22502662377600 run.py:483] Algo bellman_ford step 2814 current loss 0.151012, current_train_items 90080.
I0304 19:29:34.498254 22502662377600 run.py:483] Algo bellman_ford step 2815 current loss 0.021710, current_train_items 90112.
I0304 19:29:34.514601 22502662377600 run.py:483] Algo bellman_ford step 2816 current loss 0.046683, current_train_items 90144.
I0304 19:29:34.538790 22502662377600 run.py:483] Algo bellman_ford step 2817 current loss 0.070510, current_train_items 90176.
I0304 19:29:34.566995 22502662377600 run.py:483] Algo bellman_ford step 2818 current loss 0.072919, current_train_items 90208.
I0304 19:29:34.600827 22502662377600 run.py:483] Algo bellman_ford step 2819 current loss 0.137067, current_train_items 90240.
I0304 19:29:34.620419 22502662377600 run.py:483] Algo bellman_ford step 2820 current loss 0.060652, current_train_items 90272.
I0304 19:29:34.636496 22502662377600 run.py:483] Algo bellman_ford step 2821 current loss 0.058628, current_train_items 90304.
I0304 19:29:34.660019 22502662377600 run.py:483] Algo bellman_ford step 2822 current loss 0.095824, current_train_items 90336.
I0304 19:29:34.690958 22502662377600 run.py:483] Algo bellman_ford step 2823 current loss 0.149299, current_train_items 90368.
I0304 19:29:34.727225 22502662377600 run.py:483] Algo bellman_ford step 2824 current loss 0.193799, current_train_items 90400.
I0304 19:29:34.747088 22502662377600 run.py:483] Algo bellman_ford step 2825 current loss 0.012437, current_train_items 90432.
I0304 19:29:34.763000 22502662377600 run.py:483] Algo bellman_ford step 2826 current loss 0.076889, current_train_items 90464.
I0304 19:29:34.786980 22502662377600 run.py:483] Algo bellman_ford step 2827 current loss 0.125460, current_train_items 90496.
I0304 19:29:34.818501 22502662377600 run.py:483] Algo bellman_ford step 2828 current loss 0.107611, current_train_items 90528.
I0304 19:29:34.850756 22502662377600 run.py:483] Algo bellman_ford step 2829 current loss 0.108903, current_train_items 90560.
I0304 19:29:34.870833 22502662377600 run.py:483] Algo bellman_ford step 2830 current loss 0.017651, current_train_items 90592.
I0304 19:29:34.887353 22502662377600 run.py:483] Algo bellman_ford step 2831 current loss 0.082817, current_train_items 90624.
I0304 19:29:34.910979 22502662377600 run.py:483] Algo bellman_ford step 2832 current loss 0.088081, current_train_items 90656.
I0304 19:29:34.941553 22502662377600 run.py:483] Algo bellman_ford step 2833 current loss 0.073789, current_train_items 90688.
I0304 19:29:34.975106 22502662377600 run.py:483] Algo bellman_ford step 2834 current loss 0.180821, current_train_items 90720.
I0304 19:29:34.995012 22502662377600 run.py:483] Algo bellman_ford step 2835 current loss 0.099043, current_train_items 90752.
I0304 19:29:35.010764 22502662377600 run.py:483] Algo bellman_ford step 2836 current loss 0.059870, current_train_items 90784.
I0304 19:29:35.034533 22502662377600 run.py:483] Algo bellman_ford step 2837 current loss 0.065954, current_train_items 90816.
I0304 19:29:35.065745 22502662377600 run.py:483] Algo bellman_ford step 2838 current loss 0.106459, current_train_items 90848.
I0304 19:29:35.101862 22502662377600 run.py:483] Algo bellman_ford step 2839 current loss 0.162524, current_train_items 90880.
I0304 19:29:35.121632 22502662377600 run.py:483] Algo bellman_ford step 2840 current loss 0.029513, current_train_items 90912.
I0304 19:29:35.137491 22502662377600 run.py:483] Algo bellman_ford step 2841 current loss 0.045506, current_train_items 90944.
I0304 19:29:35.161336 22502662377600 run.py:483] Algo bellman_ford step 2842 current loss 0.135084, current_train_items 90976.
I0304 19:29:35.193528 22502662377600 run.py:483] Algo bellman_ford step 2843 current loss 0.139418, current_train_items 91008.
I0304 19:29:35.228244 22502662377600 run.py:483] Algo bellman_ford step 2844 current loss 0.133933, current_train_items 91040.
I0304 19:29:35.247951 22502662377600 run.py:483] Algo bellman_ford step 2845 current loss 0.011342, current_train_items 91072.
I0304 19:29:35.264541 22502662377600 run.py:483] Algo bellman_ford step 2846 current loss 0.042404, current_train_items 91104.
I0304 19:29:35.288328 22502662377600 run.py:483] Algo bellman_ford step 2847 current loss 0.072602, current_train_items 91136.
I0304 19:29:35.319241 22502662377600 run.py:483] Algo bellman_ford step 2848 current loss 0.105822, current_train_items 91168.
I0304 19:29:35.354557 22502662377600 run.py:483] Algo bellman_ford step 2849 current loss 0.200642, current_train_items 91200.
I0304 19:29:35.374166 22502662377600 run.py:483] Algo bellman_ford step 2850 current loss 0.014759, current_train_items 91232.
I0304 19:29:35.382580 22502662377600 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0304 19:29:35.382683 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:29:35.399002 22502662377600 run.py:483] Algo bellman_ford step 2851 current loss 0.025294, current_train_items 91264.
I0304 19:29:35.422914 22502662377600 run.py:483] Algo bellman_ford step 2852 current loss 0.077925, current_train_items 91296.
I0304 19:29:35.455117 22502662377600 run.py:483] Algo bellman_ford step 2853 current loss 0.093704, current_train_items 91328.
I0304 19:29:35.489041 22502662377600 run.py:483] Algo bellman_ford step 2854 current loss 0.157195, current_train_items 91360.
I0304 19:29:35.508967 22502662377600 run.py:483] Algo bellman_ford step 2855 current loss 0.028221, current_train_items 91392.
I0304 19:29:35.524923 22502662377600 run.py:483] Algo bellman_ford step 2856 current loss 0.046583, current_train_items 91424.
I0304 19:29:35.549244 22502662377600 run.py:483] Algo bellman_ford step 2857 current loss 0.118238, current_train_items 91456.
I0304 19:29:35.578885 22502662377600 run.py:483] Algo bellman_ford step 2858 current loss 0.072704, current_train_items 91488.
I0304 19:29:35.611290 22502662377600 run.py:483] Algo bellman_ford step 2859 current loss 0.100298, current_train_items 91520.
I0304 19:29:35.631743 22502662377600 run.py:483] Algo bellman_ford step 2860 current loss 0.020594, current_train_items 91552.
I0304 19:29:35.648130 22502662377600 run.py:483] Algo bellman_ford step 2861 current loss 0.050718, current_train_items 91584.
I0304 19:29:35.671966 22502662377600 run.py:483] Algo bellman_ford step 2862 current loss 0.106497, current_train_items 91616.
I0304 19:29:35.703843 22502662377600 run.py:483] Algo bellman_ford step 2863 current loss 0.119118, current_train_items 91648.
I0304 19:29:35.735139 22502662377600 run.py:483] Algo bellman_ford step 2864 current loss 0.088635, current_train_items 91680.
I0304 19:29:35.755023 22502662377600 run.py:483] Algo bellman_ford step 2865 current loss 0.036835, current_train_items 91712.
I0304 19:29:35.771296 22502662377600 run.py:483] Algo bellman_ford step 2866 current loss 0.044346, current_train_items 91744.
I0304 19:29:35.795201 22502662377600 run.py:483] Algo bellman_ford step 2867 current loss 0.108805, current_train_items 91776.
I0304 19:29:35.825727 22502662377600 run.py:483] Algo bellman_ford step 2868 current loss 0.081788, current_train_items 91808.
I0304 19:29:35.860234 22502662377600 run.py:483] Algo bellman_ford step 2869 current loss 0.111000, current_train_items 91840.
I0304 19:29:35.880301 22502662377600 run.py:483] Algo bellman_ford step 2870 current loss 0.009827, current_train_items 91872.
I0304 19:29:35.896636 22502662377600 run.py:483] Algo bellman_ford step 2871 current loss 0.049111, current_train_items 91904.
I0304 19:29:35.920610 22502662377600 run.py:483] Algo bellman_ford step 2872 current loss 0.096042, current_train_items 91936.
I0304 19:29:35.952612 22502662377600 run.py:483] Algo bellman_ford step 2873 current loss 0.091494, current_train_items 91968.
I0304 19:29:35.985489 22502662377600 run.py:483] Algo bellman_ford step 2874 current loss 0.124135, current_train_items 92000.
I0304 19:29:36.005576 22502662377600 run.py:483] Algo bellman_ford step 2875 current loss 0.017618, current_train_items 92032.
I0304 19:29:36.021852 22502662377600 run.py:483] Algo bellman_ford step 2876 current loss 0.026877, current_train_items 92064.
I0304 19:29:36.045010 22502662377600 run.py:483] Algo bellman_ford step 2877 current loss 0.050017, current_train_items 92096.
I0304 19:29:36.077724 22502662377600 run.py:483] Algo bellman_ford step 2878 current loss 0.136156, current_train_items 92128.
I0304 19:29:36.110609 22502662377600 run.py:483] Algo bellman_ford step 2879 current loss 0.103470, current_train_items 92160.
I0304 19:29:36.130556 22502662377600 run.py:483] Algo bellman_ford step 2880 current loss 0.064161, current_train_items 92192.
I0304 19:29:36.146697 22502662377600 run.py:483] Algo bellman_ford step 2881 current loss 0.021586, current_train_items 92224.
I0304 19:29:36.170545 22502662377600 run.py:483] Algo bellman_ford step 2882 current loss 0.062740, current_train_items 92256.
I0304 19:29:36.202103 22502662377600 run.py:483] Algo bellman_ford step 2883 current loss 0.081622, current_train_items 92288.
I0304 19:29:36.233986 22502662377600 run.py:483] Algo bellman_ford step 2884 current loss 0.162508, current_train_items 92320.
I0304 19:29:36.254024 22502662377600 run.py:483] Algo bellman_ford step 2885 current loss 0.011979, current_train_items 92352.
I0304 19:29:36.270205 22502662377600 run.py:483] Algo bellman_ford step 2886 current loss 0.046626, current_train_items 92384.
I0304 19:29:36.294110 22502662377600 run.py:483] Algo bellman_ford step 2887 current loss 0.078047, current_train_items 92416.
I0304 19:29:36.324575 22502662377600 run.py:483] Algo bellman_ford step 2888 current loss 0.079719, current_train_items 92448.
I0304 19:29:36.358672 22502662377600 run.py:483] Algo bellman_ford step 2889 current loss 0.165828, current_train_items 92480.
I0304 19:29:36.379168 22502662377600 run.py:483] Algo bellman_ford step 2890 current loss 0.014474, current_train_items 92512.
I0304 19:29:36.395385 22502662377600 run.py:483] Algo bellman_ford step 2891 current loss 0.032565, current_train_items 92544.
I0304 19:29:36.420181 22502662377600 run.py:483] Algo bellman_ford step 2892 current loss 0.093922, current_train_items 92576.
I0304 19:29:36.450419 22502662377600 run.py:483] Algo bellman_ford step 2893 current loss 0.089128, current_train_items 92608.
I0304 19:29:36.484332 22502662377600 run.py:483] Algo bellman_ford step 2894 current loss 0.104296, current_train_items 92640.
I0304 19:29:36.504029 22502662377600 run.py:483] Algo bellman_ford step 2895 current loss 0.017608, current_train_items 92672.
I0304 19:29:36.520748 22502662377600 run.py:483] Algo bellman_ford step 2896 current loss 0.079558, current_train_items 92704.
I0304 19:29:36.543533 22502662377600 run.py:483] Algo bellman_ford step 2897 current loss 0.081495, current_train_items 92736.
I0304 19:29:36.574285 22502662377600 run.py:483] Algo bellman_ford step 2898 current loss 0.076293, current_train_items 92768.
I0304 19:29:36.605724 22502662377600 run.py:483] Algo bellman_ford step 2899 current loss 0.122135, current_train_items 92800.
I0304 19:29:36.625522 22502662377600 run.py:483] Algo bellman_ford step 2900 current loss 0.003437, current_train_items 92832.
I0304 19:29:36.633204 22502662377600 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0304 19:29:36.633309 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:29:36.649717 22502662377600 run.py:483] Algo bellman_ford step 2901 current loss 0.021783, current_train_items 92864.
I0304 19:29:36.673516 22502662377600 run.py:483] Algo bellman_ford step 2902 current loss 0.089934, current_train_items 92896.
I0304 19:29:36.704791 22502662377600 run.py:483] Algo bellman_ford step 2903 current loss 0.112932, current_train_items 92928.
I0304 19:29:36.739205 22502662377600 run.py:483] Algo bellman_ford step 2904 current loss 0.160660, current_train_items 92960.
I0304 19:29:36.759310 22502662377600 run.py:483] Algo bellman_ford step 2905 current loss 0.032961, current_train_items 92992.
I0304 19:29:36.775248 22502662377600 run.py:483] Algo bellman_ford step 2906 current loss 0.071041, current_train_items 93024.
I0304 19:29:36.799450 22502662377600 run.py:483] Algo bellman_ford step 2907 current loss 0.140442, current_train_items 93056.
I0304 19:29:36.832193 22502662377600 run.py:483] Algo bellman_ford step 2908 current loss 0.151145, current_train_items 93088.
I0304 19:29:36.866570 22502662377600 run.py:483] Algo bellman_ford step 2909 current loss 0.166828, current_train_items 93120.
I0304 19:29:36.886597 22502662377600 run.py:483] Algo bellman_ford step 2910 current loss 0.016838, current_train_items 93152.
I0304 19:29:36.903271 22502662377600 run.py:483] Algo bellman_ford step 2911 current loss 0.068434, current_train_items 93184.
I0304 19:29:36.927587 22502662377600 run.py:483] Algo bellman_ford step 2912 current loss 0.092229, current_train_items 93216.
I0304 19:29:36.958453 22502662377600 run.py:483] Algo bellman_ford step 2913 current loss 0.094598, current_train_items 93248.
I0304 19:29:36.993524 22502662377600 run.py:483] Algo bellman_ford step 2914 current loss 0.134697, current_train_items 93280.
I0304 19:29:37.012986 22502662377600 run.py:483] Algo bellman_ford step 2915 current loss 0.010680, current_train_items 93312.
I0304 19:29:37.029156 22502662377600 run.py:483] Algo bellman_ford step 2916 current loss 0.032268, current_train_items 93344.
I0304 19:29:37.052497 22502662377600 run.py:483] Algo bellman_ford step 2917 current loss 0.136001, current_train_items 93376.
I0304 19:29:37.084753 22502662377600 run.py:483] Algo bellman_ford step 2918 current loss 0.151716, current_train_items 93408.
I0304 19:29:37.120341 22502662377600 run.py:483] Algo bellman_ford step 2919 current loss 0.152024, current_train_items 93440.
I0304 19:29:37.140115 22502662377600 run.py:483] Algo bellman_ford step 2920 current loss 0.015906, current_train_items 93472.
I0304 19:29:37.156353 22502662377600 run.py:483] Algo bellman_ford step 2921 current loss 0.086938, current_train_items 93504.
I0304 19:29:37.180808 22502662377600 run.py:483] Algo bellman_ford step 2922 current loss 0.178411, current_train_items 93536.
I0304 19:29:37.211206 22502662377600 run.py:483] Algo bellman_ford step 2923 current loss 0.110933, current_train_items 93568.
I0304 19:29:37.246765 22502662377600 run.py:483] Algo bellman_ford step 2924 current loss 0.194785, current_train_items 93600.
I0304 19:29:37.266697 22502662377600 run.py:483] Algo bellman_ford step 2925 current loss 0.042663, current_train_items 93632.
I0304 19:29:37.282656 22502662377600 run.py:483] Algo bellman_ford step 2926 current loss 0.127346, current_train_items 93664.
I0304 19:29:37.307260 22502662377600 run.py:483] Algo bellman_ford step 2927 current loss 0.225436, current_train_items 93696.
I0304 19:29:37.339160 22502662377600 run.py:483] Algo bellman_ford step 2928 current loss 0.270328, current_train_items 93728.
I0304 19:29:37.373209 22502662377600 run.py:483] Algo bellman_ford step 2929 current loss 0.220982, current_train_items 93760.
I0304 19:29:37.392831 22502662377600 run.py:483] Algo bellman_ford step 2930 current loss 0.009821, current_train_items 93792.
I0304 19:29:37.408847 22502662377600 run.py:483] Algo bellman_ford step 2931 current loss 0.055694, current_train_items 93824.
I0304 19:29:37.434155 22502662377600 run.py:483] Algo bellman_ford step 2932 current loss 0.352270, current_train_items 93856.
I0304 19:29:37.464424 22502662377600 run.py:483] Algo bellman_ford step 2933 current loss 0.276775, current_train_items 93888.
I0304 19:29:37.497706 22502662377600 run.py:483] Algo bellman_ford step 2934 current loss 0.282909, current_train_items 93920.
I0304 19:29:37.517515 22502662377600 run.py:483] Algo bellman_ford step 2935 current loss 0.014254, current_train_items 93952.
I0304 19:29:37.533587 22502662377600 run.py:483] Algo bellman_ford step 2936 current loss 0.034437, current_train_items 93984.
I0304 19:29:37.557623 22502662377600 run.py:483] Algo bellman_ford step 2937 current loss 0.099325, current_train_items 94016.
I0304 19:29:37.587092 22502662377600 run.py:483] Algo bellman_ford step 2938 current loss 0.053373, current_train_items 94048.
I0304 19:29:37.621191 22502662377600 run.py:483] Algo bellman_ford step 2939 current loss 0.203186, current_train_items 94080.
I0304 19:29:37.640646 22502662377600 run.py:483] Algo bellman_ford step 2940 current loss 0.062356, current_train_items 94112.
I0304 19:29:37.657267 22502662377600 run.py:483] Algo bellman_ford step 2941 current loss 0.060611, current_train_items 94144.
I0304 19:29:37.681775 22502662377600 run.py:483] Algo bellman_ford step 2942 current loss 0.147528, current_train_items 94176.
I0304 19:29:37.713852 22502662377600 run.py:483] Algo bellman_ford step 2943 current loss 0.215147, current_train_items 94208.
I0304 19:29:37.749112 22502662377600 run.py:483] Algo bellman_ford step 2944 current loss 0.184780, current_train_items 94240.
I0304 19:29:37.768811 22502662377600 run.py:483] Algo bellman_ford step 2945 current loss 0.014274, current_train_items 94272.
I0304 19:29:37.785259 22502662377600 run.py:483] Algo bellman_ford step 2946 current loss 0.039541, current_train_items 94304.
I0304 19:29:37.809262 22502662377600 run.py:483] Algo bellman_ford step 2947 current loss 0.132695, current_train_items 94336.
I0304 19:29:37.840155 22502662377600 run.py:483] Algo bellman_ford step 2948 current loss 0.168470, current_train_items 94368.
I0304 19:29:37.873551 22502662377600 run.py:483] Algo bellman_ford step 2949 current loss 0.110020, current_train_items 94400.
I0304 19:29:37.893036 22502662377600 run.py:483] Algo bellman_ford step 2950 current loss 0.016370, current_train_items 94432.
I0304 19:29:37.901119 22502662377600 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.9501953125, 'score': 0.9501953125, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0304 19:29:37.901224 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.950, val scores are: bellman_ford: 0.950
I0304 19:29:37.918042 22502662377600 run.py:483] Algo bellman_ford step 2951 current loss 0.055105, current_train_items 94464.
I0304 19:29:37.941320 22502662377600 run.py:483] Algo bellman_ford step 2952 current loss 0.076656, current_train_items 94496.
I0304 19:29:37.972189 22502662377600 run.py:483] Algo bellman_ford step 2953 current loss 0.110592, current_train_items 94528.
I0304 19:29:38.006488 22502662377600 run.py:483] Algo bellman_ford step 2954 current loss 0.128598, current_train_items 94560.
I0304 19:29:38.026248 22502662377600 run.py:483] Algo bellman_ford step 2955 current loss 0.006712, current_train_items 94592.
I0304 19:29:38.041919 22502662377600 run.py:483] Algo bellman_ford step 2956 current loss 0.102483, current_train_items 94624.
I0304 19:29:38.066750 22502662377600 run.py:483] Algo bellman_ford step 2957 current loss 0.118911, current_train_items 94656.
I0304 19:29:38.099331 22502662377600 run.py:483] Algo bellman_ford step 2958 current loss 0.095392, current_train_items 94688.
I0304 19:29:38.133627 22502662377600 run.py:483] Algo bellman_ford step 2959 current loss 0.143293, current_train_items 94720.
I0304 19:29:38.153714 22502662377600 run.py:483] Algo bellman_ford step 2960 current loss 0.038368, current_train_items 94752.
I0304 19:29:38.170169 22502662377600 run.py:483] Algo bellman_ford step 2961 current loss 0.030941, current_train_items 94784.
I0304 19:29:38.193764 22502662377600 run.py:483] Algo bellman_ford step 2962 current loss 0.100552, current_train_items 94816.
I0304 19:29:38.223857 22502662377600 run.py:483] Algo bellman_ford step 2963 current loss 0.092355, current_train_items 94848.
I0304 19:29:38.258864 22502662377600 run.py:483] Algo bellman_ford step 2964 current loss 0.135217, current_train_items 94880.
I0304 19:29:38.278270 22502662377600 run.py:483] Algo bellman_ford step 2965 current loss 0.010118, current_train_items 94912.
I0304 19:29:38.294513 22502662377600 run.py:483] Algo bellman_ford step 2966 current loss 0.036489, current_train_items 94944.
I0304 19:29:38.318352 22502662377600 run.py:483] Algo bellman_ford step 2967 current loss 0.093584, current_train_items 94976.
I0304 19:29:38.349341 22502662377600 run.py:483] Algo bellman_ford step 2968 current loss 0.099045, current_train_items 95008.
I0304 19:29:38.383810 22502662377600 run.py:483] Algo bellman_ford step 2969 current loss 0.136059, current_train_items 95040.
I0304 19:29:38.404212 22502662377600 run.py:483] Algo bellman_ford step 2970 current loss 0.016393, current_train_items 95072.
I0304 19:29:38.420259 22502662377600 run.py:483] Algo bellman_ford step 2971 current loss 0.048687, current_train_items 95104.
I0304 19:29:38.443930 22502662377600 run.py:483] Algo bellman_ford step 2972 current loss 0.070554, current_train_items 95136.
I0304 19:29:38.474262 22502662377600 run.py:483] Algo bellman_ford step 2973 current loss 0.079743, current_train_items 95168.
I0304 19:29:38.510931 22502662377600 run.py:483] Algo bellman_ford step 2974 current loss 0.163934, current_train_items 95200.
I0304 19:29:38.531160 22502662377600 run.py:483] Algo bellman_ford step 2975 current loss 0.015338, current_train_items 95232.
I0304 19:29:38.548063 22502662377600 run.py:483] Algo bellman_ford step 2976 current loss 0.051487, current_train_items 95264.
I0304 19:29:38.571902 22502662377600 run.py:483] Algo bellman_ford step 2977 current loss 0.074043, current_train_items 95296.
I0304 19:29:38.602731 22502662377600 run.py:483] Algo bellman_ford step 2978 current loss 0.110377, current_train_items 95328.
I0304 19:29:38.637929 22502662377600 run.py:483] Algo bellman_ford step 2979 current loss 0.180744, current_train_items 95360.
I0304 19:29:38.657874 22502662377600 run.py:483] Algo bellman_ford step 2980 current loss 0.010773, current_train_items 95392.
I0304 19:29:38.673886 22502662377600 run.py:483] Algo bellman_ford step 2981 current loss 0.036601, current_train_items 95424.
I0304 19:29:38.698661 22502662377600 run.py:483] Algo bellman_ford step 2982 current loss 0.088310, current_train_items 95456.
I0304 19:29:38.731679 22502662377600 run.py:483] Algo bellman_ford step 2983 current loss 0.105239, current_train_items 95488.
I0304 19:29:38.765853 22502662377600 run.py:483] Algo bellman_ford step 2984 current loss 0.130904, current_train_items 95520.
I0304 19:29:38.785703 22502662377600 run.py:483] Algo bellman_ford step 2985 current loss 0.010191, current_train_items 95552.
I0304 19:29:38.802385 22502662377600 run.py:483] Algo bellman_ford step 2986 current loss 0.069838, current_train_items 95584.
I0304 19:29:38.827218 22502662377600 run.py:483] Algo bellman_ford step 2987 current loss 0.073222, current_train_items 95616.
I0304 19:29:38.858646 22502662377600 run.py:483] Algo bellman_ford step 2988 current loss 0.075338, current_train_items 95648.
I0304 19:29:38.893357 22502662377600 run.py:483] Algo bellman_ford step 2989 current loss 0.107139, current_train_items 95680.
I0304 19:29:38.913144 22502662377600 run.py:483] Algo bellman_ford step 2990 current loss 0.011619, current_train_items 95712.
I0304 19:29:38.929985 22502662377600 run.py:483] Algo bellman_ford step 2991 current loss 0.030779, current_train_items 95744.
I0304 19:29:38.954383 22502662377600 run.py:483] Algo bellman_ford step 2992 current loss 0.104476, current_train_items 95776.
I0304 19:29:38.986608 22502662377600 run.py:483] Algo bellman_ford step 2993 current loss 0.146806, current_train_items 95808.
I0304 19:29:39.022139 22502662377600 run.py:483] Algo bellman_ford step 2994 current loss 0.166697, current_train_items 95840.
I0304 19:29:39.041798 22502662377600 run.py:483] Algo bellman_ford step 2995 current loss 0.022989, current_train_items 95872.
I0304 19:29:39.058107 22502662377600 run.py:483] Algo bellman_ford step 2996 current loss 0.070416, current_train_items 95904.
I0304 19:29:39.083368 22502662377600 run.py:483] Algo bellman_ford step 2997 current loss 0.295040, current_train_items 95936.
I0304 19:29:39.115456 22502662377600 run.py:483] Algo bellman_ford step 2998 current loss 0.368755, current_train_items 95968.
I0304 19:29:39.148558 22502662377600 run.py:483] Algo bellman_ford step 2999 current loss 0.295380, current_train_items 96000.
I0304 19:29:39.168338 22502662377600 run.py:483] Algo bellman_ford step 3000 current loss 0.006950, current_train_items 96032.
I0304 19:29:39.176101 22502662377600 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0304 19:29:39.176203 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:29:39.192797 22502662377600 run.py:483] Algo bellman_ford step 3001 current loss 0.045061, current_train_items 96064.
I0304 19:29:39.216545 22502662377600 run.py:483] Algo bellman_ford step 3002 current loss 0.081180, current_train_items 96096.
I0304 19:29:39.248124 22502662377600 run.py:483] Algo bellman_ford step 3003 current loss 0.374155, current_train_items 96128.
I0304 19:29:39.281274 22502662377600 run.py:483] Algo bellman_ford step 3004 current loss 0.221087, current_train_items 96160.
I0304 19:29:39.300923 22502662377600 run.py:483] Algo bellman_ford step 3005 current loss 0.023792, current_train_items 96192.
I0304 19:29:39.316707 22502662377600 run.py:483] Algo bellman_ford step 3006 current loss 0.052434, current_train_items 96224.
I0304 19:29:39.341393 22502662377600 run.py:483] Algo bellman_ford step 3007 current loss 0.124883, current_train_items 96256.
I0304 19:29:39.373146 22502662377600 run.py:483] Algo bellman_ford step 3008 current loss 0.098080, current_train_items 96288.
I0304 19:29:39.409225 22502662377600 run.py:483] Algo bellman_ford step 3009 current loss 0.142598, current_train_items 96320.
I0304 19:29:39.428978 22502662377600 run.py:483] Algo bellman_ford step 3010 current loss 0.013688, current_train_items 96352.
I0304 19:29:39.445220 22502662377600 run.py:483] Algo bellman_ford step 3011 current loss 0.045459, current_train_items 96384.
I0304 19:29:39.469392 22502662377600 run.py:483] Algo bellman_ford step 3012 current loss 0.156229, current_train_items 96416.
I0304 19:29:39.500162 22502662377600 run.py:483] Algo bellman_ford step 3013 current loss 0.138612, current_train_items 96448.
I0304 19:29:39.535080 22502662377600 run.py:483] Algo bellman_ford step 3014 current loss 0.185362, current_train_items 96480.
I0304 19:29:39.554602 22502662377600 run.py:483] Algo bellman_ford step 3015 current loss 0.028214, current_train_items 96512.
I0304 19:29:39.571148 22502662377600 run.py:483] Algo bellman_ford step 3016 current loss 0.043211, current_train_items 96544.
I0304 19:29:39.594717 22502662377600 run.py:483] Algo bellman_ford step 3017 current loss 0.077222, current_train_items 96576.
I0304 19:29:39.625130 22502662377600 run.py:483] Algo bellman_ford step 3018 current loss 0.066004, current_train_items 96608.
I0304 19:29:39.657391 22502662377600 run.py:483] Algo bellman_ford step 3019 current loss 0.126037, current_train_items 96640.
I0304 19:29:39.676767 22502662377600 run.py:483] Algo bellman_ford step 3020 current loss 0.008538, current_train_items 96672.
I0304 19:29:39.692510 22502662377600 run.py:483] Algo bellman_ford step 3021 current loss 0.058223, current_train_items 96704.
I0304 19:29:39.717810 22502662377600 run.py:483] Algo bellman_ford step 3022 current loss 0.102904, current_train_items 96736.
I0304 19:29:39.749210 22502662377600 run.py:483] Algo bellman_ford step 3023 current loss 0.103679, current_train_items 96768.
I0304 19:29:39.782840 22502662377600 run.py:483] Algo bellman_ford step 3024 current loss 0.132466, current_train_items 96800.
I0304 19:29:39.802582 22502662377600 run.py:483] Algo bellman_ford step 3025 current loss 0.009077, current_train_items 96832.
I0304 19:29:39.818603 22502662377600 run.py:483] Algo bellman_ford step 3026 current loss 0.029726, current_train_items 96864.
I0304 19:29:39.841303 22502662377600 run.py:483] Algo bellman_ford step 3027 current loss 0.071572, current_train_items 96896.
I0304 19:29:39.872082 22502662377600 run.py:483] Algo bellman_ford step 3028 current loss 0.110939, current_train_items 96928.
I0304 19:29:39.904683 22502662377600 run.py:483] Algo bellman_ford step 3029 current loss 0.096827, current_train_items 96960.
I0304 19:29:39.924235 22502662377600 run.py:483] Algo bellman_ford step 3030 current loss 0.009598, current_train_items 96992.
I0304 19:29:39.940119 22502662377600 run.py:483] Algo bellman_ford step 3031 current loss 0.111046, current_train_items 97024.
I0304 19:29:39.964430 22502662377600 run.py:483] Algo bellman_ford step 3032 current loss 0.105217, current_train_items 97056.
I0304 19:29:39.996173 22502662377600 run.py:483] Algo bellman_ford step 3033 current loss 0.141140, current_train_items 97088.
I0304 19:29:40.030511 22502662377600 run.py:483] Algo bellman_ford step 3034 current loss 0.126268, current_train_items 97120.
I0304 19:29:40.050111 22502662377600 run.py:483] Algo bellman_ford step 3035 current loss 0.081871, current_train_items 97152.
I0304 19:29:40.066565 22502662377600 run.py:483] Algo bellman_ford step 3036 current loss 0.047679, current_train_items 97184.
I0304 19:29:40.090302 22502662377600 run.py:483] Algo bellman_ford step 3037 current loss 0.051039, current_train_items 97216.
I0304 19:29:40.121579 22502662377600 run.py:483] Algo bellman_ford step 3038 current loss 0.100759, current_train_items 97248.
I0304 19:29:40.153864 22502662377600 run.py:483] Algo bellman_ford step 3039 current loss 0.169495, current_train_items 97280.
I0304 19:29:40.173384 22502662377600 run.py:483] Algo bellman_ford step 3040 current loss 0.008072, current_train_items 97312.
I0304 19:29:40.189447 22502662377600 run.py:483] Algo bellman_ford step 3041 current loss 0.032796, current_train_items 97344.
I0304 19:29:40.213136 22502662377600 run.py:483] Algo bellman_ford step 3042 current loss 0.052319, current_train_items 97376.
I0304 19:29:40.244345 22502662377600 run.py:483] Algo bellman_ford step 3043 current loss 0.108917, current_train_items 97408.
I0304 19:29:40.276919 22502662377600 run.py:483] Algo bellman_ford step 3044 current loss 0.178341, current_train_items 97440.
I0304 19:29:40.296474 22502662377600 run.py:483] Algo bellman_ford step 3045 current loss 0.113118, current_train_items 97472.
I0304 19:29:40.312632 22502662377600 run.py:483] Algo bellman_ford step 3046 current loss 0.052645, current_train_items 97504.
I0304 19:29:40.336082 22502662377600 run.py:483] Algo bellman_ford step 3047 current loss 0.115236, current_train_items 97536.
I0304 19:29:40.367679 22502662377600 run.py:483] Algo bellman_ford step 3048 current loss 0.104307, current_train_items 97568.
I0304 19:29:40.400842 22502662377600 run.py:483] Algo bellman_ford step 3049 current loss 0.130831, current_train_items 97600.
I0304 19:29:40.420451 22502662377600 run.py:483] Algo bellman_ford step 3050 current loss 0.011149, current_train_items 97632.
I0304 19:29:40.428584 22502662377600 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0304 19:29:40.428689 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:29:40.445312 22502662377600 run.py:483] Algo bellman_ford step 3051 current loss 0.029198, current_train_items 97664.
I0304 19:29:40.469511 22502662377600 run.py:483] Algo bellman_ford step 3052 current loss 0.107523, current_train_items 97696.
I0304 19:29:40.501686 22502662377600 run.py:483] Algo bellman_ford step 3053 current loss 0.073204, current_train_items 97728.
I0304 19:29:40.533976 22502662377600 run.py:483] Algo bellman_ford step 3054 current loss 0.098909, current_train_items 97760.
I0304 19:29:40.553975 22502662377600 run.py:483] Algo bellman_ford step 3055 current loss 0.007429, current_train_items 97792.
I0304 19:29:40.569741 22502662377600 run.py:483] Algo bellman_ford step 3056 current loss 0.085082, current_train_items 97824.
I0304 19:29:40.593726 22502662377600 run.py:483] Algo bellman_ford step 3057 current loss 0.085926, current_train_items 97856.
I0304 19:29:40.625221 22502662377600 run.py:483] Algo bellman_ford step 3058 current loss 0.066593, current_train_items 97888.
I0304 19:29:40.657969 22502662377600 run.py:483] Algo bellman_ford step 3059 current loss 0.143175, current_train_items 97920.
I0304 19:29:40.678098 22502662377600 run.py:483] Algo bellman_ford step 3060 current loss 0.006776, current_train_items 97952.
I0304 19:29:40.694552 22502662377600 run.py:483] Algo bellman_ford step 3061 current loss 0.091431, current_train_items 97984.
I0304 19:29:40.717643 22502662377600 run.py:483] Algo bellman_ford step 3062 current loss 0.065461, current_train_items 98016.
I0304 19:29:40.749868 22502662377600 run.py:483] Algo bellman_ford step 3063 current loss 0.154256, current_train_items 98048.
I0304 19:29:40.784814 22502662377600 run.py:483] Algo bellman_ford step 3064 current loss 0.163827, current_train_items 98080.
I0304 19:29:40.804429 22502662377600 run.py:483] Algo bellman_ford step 3065 current loss 0.011434, current_train_items 98112.
I0304 19:29:40.820580 22502662377600 run.py:483] Algo bellman_ford step 3066 current loss 0.063737, current_train_items 98144.
I0304 19:29:40.845284 22502662377600 run.py:483] Algo bellman_ford step 3067 current loss 0.059653, current_train_items 98176.
I0304 19:29:40.876187 22502662377600 run.py:483] Algo bellman_ford step 3068 current loss 0.088473, current_train_items 98208.
I0304 19:29:40.912418 22502662377600 run.py:483] Algo bellman_ford step 3069 current loss 0.121032, current_train_items 98240.
I0304 19:29:40.932794 22502662377600 run.py:483] Algo bellman_ford step 3070 current loss 0.006086, current_train_items 98272.
I0304 19:29:40.949113 22502662377600 run.py:483] Algo bellman_ford step 3071 current loss 0.029915, current_train_items 98304.
I0304 19:29:40.972246 22502662377600 run.py:483] Algo bellman_ford step 3072 current loss 0.096158, current_train_items 98336.
I0304 19:29:41.003163 22502662377600 run.py:483] Algo bellman_ford step 3073 current loss 0.081256, current_train_items 98368.
I0304 19:29:41.038809 22502662377600 run.py:483] Algo bellman_ford step 3074 current loss 0.180606, current_train_items 98400.
I0304 19:29:41.058911 22502662377600 run.py:483] Algo bellman_ford step 3075 current loss 0.007703, current_train_items 98432.
I0304 19:29:41.075533 22502662377600 run.py:483] Algo bellman_ford step 3076 current loss 0.057886, current_train_items 98464.
I0304 19:29:41.099558 22502662377600 run.py:483] Algo bellman_ford step 3077 current loss 0.082066, current_train_items 98496.
I0304 19:29:41.131490 22502662377600 run.py:483] Algo bellman_ford step 3078 current loss 0.095568, current_train_items 98528.
I0304 19:29:41.163715 22502662377600 run.py:483] Algo bellman_ford step 3079 current loss 0.133176, current_train_items 98560.
I0304 19:29:41.183620 22502662377600 run.py:483] Algo bellman_ford step 3080 current loss 0.090970, current_train_items 98592.
I0304 19:29:41.199997 22502662377600 run.py:483] Algo bellman_ford step 3081 current loss 0.054568, current_train_items 98624.
I0304 19:29:41.223676 22502662377600 run.py:483] Algo bellman_ford step 3082 current loss 0.069423, current_train_items 98656.
I0304 19:29:41.255005 22502662377600 run.py:483] Algo bellman_ford step 3083 current loss 0.094369, current_train_items 98688.
I0304 19:29:41.289431 22502662377600 run.py:483] Algo bellman_ford step 3084 current loss 0.126421, current_train_items 98720.
I0304 19:29:41.309420 22502662377600 run.py:483] Algo bellman_ford step 3085 current loss 0.014816, current_train_items 98752.
I0304 19:29:41.325888 22502662377600 run.py:483] Algo bellman_ford step 3086 current loss 0.058861, current_train_items 98784.
I0304 19:29:41.348274 22502662377600 run.py:483] Algo bellman_ford step 3087 current loss 0.069364, current_train_items 98816.
I0304 19:29:41.379570 22502662377600 run.py:483] Algo bellman_ford step 3088 current loss 0.111080, current_train_items 98848.
I0304 19:29:41.414974 22502662377600 run.py:483] Algo bellman_ford step 3089 current loss 0.157895, current_train_items 98880.
I0304 19:29:41.434991 22502662377600 run.py:483] Algo bellman_ford step 3090 current loss 0.009609, current_train_items 98912.
I0304 19:29:41.451094 22502662377600 run.py:483] Algo bellman_ford step 3091 current loss 0.070727, current_train_items 98944.
I0304 19:29:41.473691 22502662377600 run.py:483] Algo bellman_ford step 3092 current loss 0.152527, current_train_items 98976.
I0304 19:29:41.505002 22502662377600 run.py:483] Algo bellman_ford step 3093 current loss 0.238595, current_train_items 99008.
I0304 19:29:41.538873 22502662377600 run.py:483] Algo bellman_ford step 3094 current loss 0.210726, current_train_items 99040.
I0304 19:29:41.558500 22502662377600 run.py:483] Algo bellman_ford step 3095 current loss 0.009220, current_train_items 99072.
I0304 19:29:41.575187 22502662377600 run.py:483] Algo bellman_ford step 3096 current loss 0.076670, current_train_items 99104.
I0304 19:29:41.600520 22502662377600 run.py:483] Algo bellman_ford step 3097 current loss 0.256529, current_train_items 99136.
I0304 19:29:41.631454 22502662377600 run.py:483] Algo bellman_ford step 3098 current loss 0.167824, current_train_items 99168.
I0304 19:29:41.664733 22502662377600 run.py:483] Algo bellman_ford step 3099 current loss 0.152908, current_train_items 99200.
I0304 19:29:41.684989 22502662377600 run.py:483] Algo bellman_ford step 3100 current loss 0.008142, current_train_items 99232.
I0304 19:29:41.692574 22502662377600 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0304 19:29:41.692676 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:29:41.710353 22502662377600 run.py:483] Algo bellman_ford step 3101 current loss 0.118058, current_train_items 99264.
I0304 19:29:41.735413 22502662377600 run.py:483] Algo bellman_ford step 3102 current loss 0.156443, current_train_items 99296.
I0304 19:29:41.767749 22502662377600 run.py:483] Algo bellman_ford step 3103 current loss 0.113773, current_train_items 99328.
I0304 19:29:41.802997 22502662377600 run.py:483] Algo bellman_ford step 3104 current loss 0.092197, current_train_items 99360.
I0304 19:29:41.823381 22502662377600 run.py:483] Algo bellman_ford step 3105 current loss 0.018524, current_train_items 99392.
I0304 19:29:41.839141 22502662377600 run.py:483] Algo bellman_ford step 3106 current loss 0.010226, current_train_items 99424.
I0304 19:29:41.863069 22502662377600 run.py:483] Algo bellman_ford step 3107 current loss 0.052103, current_train_items 99456.
I0304 19:29:41.895406 22502662377600 run.py:483] Algo bellman_ford step 3108 current loss 0.081223, current_train_items 99488.
I0304 19:29:41.928131 22502662377600 run.py:483] Algo bellman_ford step 3109 current loss 0.089256, current_train_items 99520.
I0304 19:29:41.948206 22502662377600 run.py:483] Algo bellman_ford step 3110 current loss 0.008400, current_train_items 99552.
I0304 19:29:41.964474 22502662377600 run.py:483] Algo bellman_ford step 3111 current loss 0.040973, current_train_items 99584.
I0304 19:29:41.988358 22502662377600 run.py:483] Algo bellman_ford step 3112 current loss 0.101043, current_train_items 99616.
I0304 19:29:42.019711 22502662377600 run.py:483] Algo bellman_ford step 3113 current loss 0.089957, current_train_items 99648.
I0304 19:29:42.054293 22502662377600 run.py:483] Algo bellman_ford step 3114 current loss 0.129550, current_train_items 99680.
I0304 19:29:42.073957 22502662377600 run.py:483] Algo bellman_ford step 3115 current loss 0.009052, current_train_items 99712.
I0304 19:29:42.090332 22502662377600 run.py:483] Algo bellman_ford step 3116 current loss 0.035341, current_train_items 99744.
I0304 19:29:42.114415 22502662377600 run.py:483] Algo bellman_ford step 3117 current loss 0.105517, current_train_items 99776.
I0304 19:29:42.146517 22502662377600 run.py:483] Algo bellman_ford step 3118 current loss 0.186868, current_train_items 99808.
I0304 19:29:42.179984 22502662377600 run.py:483] Algo bellman_ford step 3119 current loss 0.128034, current_train_items 99840.
I0304 19:29:42.199729 22502662377600 run.py:483] Algo bellman_ford step 3120 current loss 0.007633, current_train_items 99872.
I0304 19:29:42.215951 22502662377600 run.py:483] Algo bellman_ford step 3121 current loss 0.049828, current_train_items 99904.
I0304 19:29:42.240203 22502662377600 run.py:483] Algo bellman_ford step 3122 current loss 0.050858, current_train_items 99936.
I0304 19:29:42.271976 22502662377600 run.py:483] Algo bellman_ford step 3123 current loss 0.095682, current_train_items 99968.
I0304 19:29:42.306133 22502662377600 run.py:483] Algo bellman_ford step 3124 current loss 0.072372, current_train_items 100000.
I0304 19:29:42.325994 22502662377600 run.py:483] Algo bellman_ford step 3125 current loss 0.014690, current_train_items 100032.
I0304 19:29:42.342231 22502662377600 run.py:483] Algo bellman_ford step 3126 current loss 0.052991, current_train_items 100064.
I0304 19:29:42.365471 22502662377600 run.py:483] Algo bellman_ford step 3127 current loss 0.136688, current_train_items 100096.
I0304 19:29:42.397747 22502662377600 run.py:483] Algo bellman_ford step 3128 current loss 0.114028, current_train_items 100128.
I0304 19:29:42.430326 22502662377600 run.py:483] Algo bellman_ford step 3129 current loss 0.098700, current_train_items 100160.
I0304 19:29:42.450373 22502662377600 run.py:483] Algo bellman_ford step 3130 current loss 0.017284, current_train_items 100192.
I0304 19:29:42.466538 22502662377600 run.py:483] Algo bellman_ford step 3131 current loss 0.028137, current_train_items 100224.
I0304 19:29:42.490675 22502662377600 run.py:483] Algo bellman_ford step 3132 current loss 0.075837, current_train_items 100256.
I0304 19:29:42.521128 22502662377600 run.py:483] Algo bellman_ford step 3133 current loss 0.138390, current_train_items 100288.
I0304 19:29:42.556959 22502662377600 run.py:483] Algo bellman_ford step 3134 current loss 0.167728, current_train_items 100320.
I0304 19:29:42.576517 22502662377600 run.py:483] Algo bellman_ford step 3135 current loss 0.030838, current_train_items 100352.
I0304 19:29:42.592595 22502662377600 run.py:483] Algo bellman_ford step 3136 current loss 0.023678, current_train_items 100384.
I0304 19:29:42.616952 22502662377600 run.py:483] Algo bellman_ford step 3137 current loss 0.123012, current_train_items 100416.
I0304 19:29:42.647276 22502662377600 run.py:483] Algo bellman_ford step 3138 current loss 0.084548, current_train_items 100448.
I0304 19:29:42.681475 22502662377600 run.py:483] Algo bellman_ford step 3139 current loss 0.140449, current_train_items 100480.
I0304 19:29:42.701125 22502662377600 run.py:483] Algo bellman_ford step 3140 current loss 0.013847, current_train_items 100512.
I0304 19:29:42.717836 22502662377600 run.py:483] Algo bellman_ford step 3141 current loss 0.076937, current_train_items 100544.
I0304 19:29:42.740909 22502662377600 run.py:483] Algo bellman_ford step 3142 current loss 0.077931, current_train_items 100576.
I0304 19:29:42.773459 22502662377600 run.py:483] Algo bellman_ford step 3143 current loss 0.199597, current_train_items 100608.
I0304 19:29:42.806931 22502662377600 run.py:483] Algo bellman_ford step 3144 current loss 0.183485, current_train_items 100640.
I0304 19:29:42.826910 22502662377600 run.py:483] Algo bellman_ford step 3145 current loss 0.027642, current_train_items 100672.
I0304 19:29:42.842991 22502662377600 run.py:483] Algo bellman_ford step 3146 current loss 0.031245, current_train_items 100704.
I0304 19:29:42.866236 22502662377600 run.py:483] Algo bellman_ford step 3147 current loss 0.044158, current_train_items 100736.
I0304 19:29:42.895697 22502662377600 run.py:483] Algo bellman_ford step 3148 current loss 0.050669, current_train_items 100768.
I0304 19:29:42.931522 22502662377600 run.py:483] Algo bellman_ford step 3149 current loss 0.208868, current_train_items 100800.
I0304 19:29:42.951162 22502662377600 run.py:483] Algo bellman_ford step 3150 current loss 0.072098, current_train_items 100832.
I0304 19:29:42.959573 22502662377600 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0304 19:29:42.959682 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:29:42.976560 22502662377600 run.py:483] Algo bellman_ford step 3151 current loss 0.064921, current_train_items 100864.
I0304 19:29:43.002111 22502662377600 run.py:483] Algo bellman_ford step 3152 current loss 0.147495, current_train_items 100896.
I0304 19:29:43.033936 22502662377600 run.py:483] Algo bellman_ford step 3153 current loss 0.076539, current_train_items 100928.
I0304 19:29:43.068555 22502662377600 run.py:483] Algo bellman_ford step 3154 current loss 0.118940, current_train_items 100960.
I0304 19:29:43.088640 22502662377600 run.py:483] Algo bellman_ford step 3155 current loss 0.029044, current_train_items 100992.
I0304 19:29:43.104417 22502662377600 run.py:483] Algo bellman_ford step 3156 current loss 0.042492, current_train_items 101024.
I0304 19:29:43.129070 22502662377600 run.py:483] Algo bellman_ford step 3157 current loss 0.185040, current_train_items 101056.
I0304 19:29:43.159355 22502662377600 run.py:483] Algo bellman_ford step 3158 current loss 0.207245, current_train_items 101088.
I0304 19:29:43.192498 22502662377600 run.py:483] Algo bellman_ford step 3159 current loss 0.143988, current_train_items 101120.
I0304 19:29:43.212609 22502662377600 run.py:483] Algo bellman_ford step 3160 current loss 0.014289, current_train_items 101152.
I0304 19:29:43.228605 22502662377600 run.py:483] Algo bellman_ford step 3161 current loss 0.019897, current_train_items 101184.
I0304 19:29:43.252453 22502662377600 run.py:483] Algo bellman_ford step 3162 current loss 0.063788, current_train_items 101216.
I0304 19:29:43.282712 22502662377600 run.py:483] Algo bellman_ford step 3163 current loss 0.083997, current_train_items 101248.
I0304 19:29:43.317566 22502662377600 run.py:483] Algo bellman_ford step 3164 current loss 0.163420, current_train_items 101280.
I0304 19:29:43.337248 22502662377600 run.py:483] Algo bellman_ford step 3165 current loss 0.017789, current_train_items 101312.
I0304 19:29:43.354120 22502662377600 run.py:483] Algo bellman_ford step 3166 current loss 0.077240, current_train_items 101344.
I0304 19:29:43.378400 22502662377600 run.py:483] Algo bellman_ford step 3167 current loss 0.084202, current_train_items 101376.
I0304 19:29:43.409756 22502662377600 run.py:483] Algo bellman_ford step 3168 current loss 0.097923, current_train_items 101408.
I0304 19:29:43.442793 22502662377600 run.py:483] Algo bellman_ford step 3169 current loss 0.133484, current_train_items 101440.
I0304 19:29:43.462844 22502662377600 run.py:483] Algo bellman_ford step 3170 current loss 0.014395, current_train_items 101472.
I0304 19:29:43.479489 22502662377600 run.py:483] Algo bellman_ford step 3171 current loss 0.144821, current_train_items 101504.
I0304 19:29:43.503537 22502662377600 run.py:483] Algo bellman_ford step 3172 current loss 0.078970, current_train_items 101536.
I0304 19:29:43.535336 22502662377600 run.py:483] Algo bellman_ford step 3173 current loss 0.107982, current_train_items 101568.
I0304 19:29:43.567215 22502662377600 run.py:483] Algo bellman_ford step 3174 current loss 0.128390, current_train_items 101600.
I0304 19:29:43.587036 22502662377600 run.py:483] Algo bellman_ford step 3175 current loss 0.007620, current_train_items 101632.
I0304 19:29:43.603335 22502662377600 run.py:483] Algo bellman_ford step 3176 current loss 0.054011, current_train_items 101664.
I0304 19:29:43.626801 22502662377600 run.py:483] Algo bellman_ford step 3177 current loss 0.067177, current_train_items 101696.
I0304 19:29:43.656105 22502662377600 run.py:483] Algo bellman_ford step 3178 current loss 0.065991, current_train_items 101728.
I0304 19:29:43.690794 22502662377600 run.py:483] Algo bellman_ford step 3179 current loss 0.153241, current_train_items 101760.
I0304 19:29:43.710390 22502662377600 run.py:483] Algo bellman_ford step 3180 current loss 0.013724, current_train_items 101792.
I0304 19:29:43.726758 22502662377600 run.py:483] Algo bellman_ford step 3181 current loss 0.027034, current_train_items 101824.
I0304 19:29:43.751030 22502662377600 run.py:483] Algo bellman_ford step 3182 current loss 0.052621, current_train_items 101856.
I0304 19:29:43.782796 22502662377600 run.py:483] Algo bellman_ford step 3183 current loss 0.061200, current_train_items 101888.
I0304 19:29:43.819513 22502662377600 run.py:483] Algo bellman_ford step 3184 current loss 0.162520, current_train_items 101920.
I0304 19:29:43.839617 22502662377600 run.py:483] Algo bellman_ford step 3185 current loss 0.023053, current_train_items 101952.
I0304 19:29:43.855808 22502662377600 run.py:483] Algo bellman_ford step 3186 current loss 0.024480, current_train_items 101984.
I0304 19:29:43.880647 22502662377600 run.py:483] Algo bellman_ford step 3187 current loss 0.189936, current_train_items 102016.
I0304 19:29:43.911509 22502662377600 run.py:483] Algo bellman_ford step 3188 current loss 0.126764, current_train_items 102048.
I0304 19:29:43.945184 22502662377600 run.py:483] Algo bellman_ford step 3189 current loss 0.082067, current_train_items 102080.
I0304 19:29:43.965425 22502662377600 run.py:483] Algo bellman_ford step 3190 current loss 0.041108, current_train_items 102112.
I0304 19:29:43.981827 22502662377600 run.py:483] Algo bellman_ford step 3191 current loss 0.039896, current_train_items 102144.
I0304 19:29:44.006045 22502662377600 run.py:483] Algo bellman_ford step 3192 current loss 0.114678, current_train_items 102176.
I0304 19:29:44.037024 22502662377600 run.py:483] Algo bellman_ford step 3193 current loss 0.110529, current_train_items 102208.
I0304 19:29:44.071205 22502662377600 run.py:483] Algo bellman_ford step 3194 current loss 0.099358, current_train_items 102240.
I0304 19:29:44.090888 22502662377600 run.py:483] Algo bellman_ford step 3195 current loss 0.015094, current_train_items 102272.
I0304 19:29:44.106737 22502662377600 run.py:483] Algo bellman_ford step 3196 current loss 0.026553, current_train_items 102304.
I0304 19:29:44.130474 22502662377600 run.py:483] Algo bellman_ford step 3197 current loss 0.191562, current_train_items 102336.
I0304 19:29:44.162137 22502662377600 run.py:483] Algo bellman_ford step 3198 current loss 0.164267, current_train_items 102368.
I0304 19:29:44.196193 22502662377600 run.py:483] Algo bellman_ford step 3199 current loss 0.177985, current_train_items 102400.
I0304 19:29:44.216240 22502662377600 run.py:483] Algo bellman_ford step 3200 current loss 0.038582, current_train_items 102432.
I0304 19:29:44.224177 22502662377600 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.9404296875, 'score': 0.9404296875, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0304 19:29:44.224281 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.940, val scores are: bellman_ford: 0.940
I0304 19:29:44.240604 22502662377600 run.py:483] Algo bellman_ford step 3201 current loss 0.052962, current_train_items 102464.
I0304 19:29:44.264496 22502662377600 run.py:483] Algo bellman_ford step 3202 current loss 0.163564, current_train_items 102496.
I0304 19:29:44.297443 22502662377600 run.py:483] Algo bellman_ford step 3203 current loss 0.633533, current_train_items 102528.
I0304 19:29:44.331682 22502662377600 run.py:483] Algo bellman_ford step 3204 current loss 0.401879, current_train_items 102560.
I0304 19:29:44.351586 22502662377600 run.py:483] Algo bellman_ford step 3205 current loss 0.027623, current_train_items 102592.
I0304 19:29:44.367069 22502662377600 run.py:483] Algo bellman_ford step 3206 current loss 0.058782, current_train_items 102624.
I0304 19:29:44.389661 22502662377600 run.py:483] Algo bellman_ford step 3207 current loss 0.078020, current_train_items 102656.
I0304 19:29:44.422415 22502662377600 run.py:483] Algo bellman_ford step 3208 current loss 0.149556, current_train_items 102688.
I0304 19:29:44.456686 22502662377600 run.py:483] Algo bellman_ford step 3209 current loss 0.309971, current_train_items 102720.
I0304 19:29:44.476172 22502662377600 run.py:483] Algo bellman_ford step 3210 current loss 0.075943, current_train_items 102752.
I0304 19:29:44.491966 22502662377600 run.py:483] Algo bellman_ford step 3211 current loss 0.028908, current_train_items 102784.
I0304 19:29:44.515837 22502662377600 run.py:483] Algo bellman_ford step 3212 current loss 0.069039, current_train_items 102816.
I0304 19:29:44.546858 22502662377600 run.py:483] Algo bellman_ford step 3213 current loss 0.095787, current_train_items 102848.
I0304 19:29:44.580373 22502662377600 run.py:483] Algo bellman_ford step 3214 current loss 0.129032, current_train_items 102880.
I0304 19:29:44.600126 22502662377600 run.py:483] Algo bellman_ford step 3215 current loss 0.033567, current_train_items 102912.
I0304 19:29:44.616173 22502662377600 run.py:483] Algo bellman_ford step 3216 current loss 0.068112, current_train_items 102944.
I0304 19:29:44.640680 22502662377600 run.py:483] Algo bellman_ford step 3217 current loss 0.071773, current_train_items 102976.
I0304 19:29:44.670812 22502662377600 run.py:483] Algo bellman_ford step 3218 current loss 0.101690, current_train_items 103008.
I0304 19:29:44.703993 22502662377600 run.py:483] Algo bellman_ford step 3219 current loss 0.081198, current_train_items 103040.
I0304 19:29:44.723591 22502662377600 run.py:483] Algo bellman_ford step 3220 current loss 0.020613, current_train_items 103072.
I0304 19:29:44.740121 22502662377600 run.py:483] Algo bellman_ford step 3221 current loss 0.050474, current_train_items 103104.
I0304 19:29:44.764881 22502662377600 run.py:483] Algo bellman_ford step 3222 current loss 0.128778, current_train_items 103136.
I0304 19:29:44.796188 22502662377600 run.py:483] Algo bellman_ford step 3223 current loss 0.116457, current_train_items 103168.
I0304 19:29:44.828077 22502662377600 run.py:483] Algo bellman_ford step 3224 current loss 0.125227, current_train_items 103200.
I0304 19:29:44.847524 22502662377600 run.py:483] Algo bellman_ford step 3225 current loss 0.017100, current_train_items 103232.
I0304 19:29:44.862952 22502662377600 run.py:483] Algo bellman_ford step 3226 current loss 0.023510, current_train_items 103264.
I0304 19:29:44.887072 22502662377600 run.py:483] Algo bellman_ford step 3227 current loss 0.052619, current_train_items 103296.
I0304 19:29:44.919104 22502662377600 run.py:483] Algo bellman_ford step 3228 current loss 0.107810, current_train_items 103328.
I0304 19:29:44.956449 22502662377600 run.py:483] Algo bellman_ford step 3229 current loss 0.162806, current_train_items 103360.
I0304 19:29:44.975964 22502662377600 run.py:483] Algo bellman_ford step 3230 current loss 0.019542, current_train_items 103392.
I0304 19:29:44.992164 22502662377600 run.py:483] Algo bellman_ford step 3231 current loss 0.120849, current_train_items 103424.
I0304 19:29:45.015289 22502662377600 run.py:483] Algo bellman_ford step 3232 current loss 0.138834, current_train_items 103456.
I0304 19:29:45.046410 22502662377600 run.py:483] Algo bellman_ford step 3233 current loss 0.120310, current_train_items 103488.
I0304 19:29:45.081345 22502662377600 run.py:483] Algo bellman_ford step 3234 current loss 0.106045, current_train_items 103520.
I0304 19:29:45.101026 22502662377600 run.py:483] Algo bellman_ford step 3235 current loss 0.061754, current_train_items 103552.
I0304 19:29:45.117326 22502662377600 run.py:483] Algo bellman_ford step 3236 current loss 0.086420, current_train_items 103584.
I0304 19:29:45.140325 22502662377600 run.py:483] Algo bellman_ford step 3237 current loss 0.102686, current_train_items 103616.
I0304 19:29:45.172705 22502662377600 run.py:483] Algo bellman_ford step 3238 current loss 0.101603, current_train_items 103648.
I0304 19:29:45.206072 22502662377600 run.py:483] Algo bellman_ford step 3239 current loss 0.093248, current_train_items 103680.
I0304 19:29:45.225378 22502662377600 run.py:483] Algo bellman_ford step 3240 current loss 0.031972, current_train_items 103712.
I0304 19:29:45.241653 22502662377600 run.py:483] Algo bellman_ford step 3241 current loss 0.052239, current_train_items 103744.
I0304 19:29:45.265468 22502662377600 run.py:483] Algo bellman_ford step 3242 current loss 0.067844, current_train_items 103776.
I0304 19:29:45.295476 22502662377600 run.py:483] Algo bellman_ford step 3243 current loss 0.084555, current_train_items 103808.
I0304 19:29:45.329795 22502662377600 run.py:483] Algo bellman_ford step 3244 current loss 0.144833, current_train_items 103840.
I0304 19:29:45.349272 22502662377600 run.py:483] Algo bellman_ford step 3245 current loss 0.009725, current_train_items 103872.
I0304 19:29:45.365767 22502662377600 run.py:483] Algo bellman_ford step 3246 current loss 0.025991, current_train_items 103904.
I0304 19:29:45.389231 22502662377600 run.py:483] Algo bellman_ford step 3247 current loss 0.078935, current_train_items 103936.
I0304 19:29:45.420888 22502662377600 run.py:483] Algo bellman_ford step 3248 current loss 0.070317, current_train_items 103968.
I0304 19:29:45.453472 22502662377600 run.py:483] Algo bellman_ford step 3249 current loss 0.076852, current_train_items 104000.
I0304 19:29:45.473330 22502662377600 run.py:483] Algo bellman_ford step 3250 current loss 0.023452, current_train_items 104032.
I0304 19:29:45.482037 22502662377600 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0304 19:29:45.482142 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:29:45.498837 22502662377600 run.py:483] Algo bellman_ford step 3251 current loss 0.082099, current_train_items 104064.
I0304 19:29:45.521379 22502662377600 run.py:483] Algo bellman_ford step 3252 current loss 0.063106, current_train_items 104096.
I0304 19:29:45.554069 22502662377600 run.py:483] Algo bellman_ford step 3253 current loss 0.117680, current_train_items 104128.
I0304 19:29:45.588538 22502662377600 run.py:483] Algo bellman_ford step 3254 current loss 0.124603, current_train_items 104160.
I0304 19:29:45.608648 22502662377600 run.py:483] Algo bellman_ford step 3255 current loss 0.056007, current_train_items 104192.
I0304 19:29:45.624190 22502662377600 run.py:483] Algo bellman_ford step 3256 current loss 0.035520, current_train_items 104224.
I0304 19:29:45.647434 22502662377600 run.py:483] Algo bellman_ford step 3257 current loss 0.043471, current_train_items 104256.
I0304 19:29:45.679823 22502662377600 run.py:483] Algo bellman_ford step 3258 current loss 0.137295, current_train_items 104288.
I0304 19:29:45.715034 22502662377600 run.py:483] Algo bellman_ford step 3259 current loss 0.105781, current_train_items 104320.
I0304 19:29:45.734686 22502662377600 run.py:483] Algo bellman_ford step 3260 current loss 0.009744, current_train_items 104352.
I0304 19:29:45.750908 22502662377600 run.py:483] Algo bellman_ford step 3261 current loss 0.014959, current_train_items 104384.
I0304 19:29:45.774100 22502662377600 run.py:483] Algo bellman_ford step 3262 current loss 0.060310, current_train_items 104416.
I0304 19:29:45.805457 22502662377600 run.py:483] Algo bellman_ford step 3263 current loss 0.089367, current_train_items 104448.
I0304 19:29:45.839579 22502662377600 run.py:483] Algo bellman_ford step 3264 current loss 0.120480, current_train_items 104480.
I0304 19:29:45.859536 22502662377600 run.py:483] Algo bellman_ford step 3265 current loss 0.018450, current_train_items 104512.
I0304 19:29:45.875834 22502662377600 run.py:483] Algo bellman_ford step 3266 current loss 0.025925, current_train_items 104544.
I0304 19:29:45.899328 22502662377600 run.py:483] Algo bellman_ford step 3267 current loss 0.054751, current_train_items 104576.
I0304 19:29:45.928773 22502662377600 run.py:483] Algo bellman_ford step 3268 current loss 0.067669, current_train_items 104608.
I0304 19:29:45.964558 22502662377600 run.py:483] Algo bellman_ford step 3269 current loss 0.141106, current_train_items 104640.
I0304 19:29:45.984370 22502662377600 run.py:483] Algo bellman_ford step 3270 current loss 0.010799, current_train_items 104672.
I0304 19:29:46.000967 22502662377600 run.py:483] Algo bellman_ford step 3271 current loss 0.047960, current_train_items 104704.
I0304 19:29:46.024393 22502662377600 run.py:483] Algo bellman_ford step 3272 current loss 0.036372, current_train_items 104736.
I0304 19:29:46.054285 22502662377600 run.py:483] Algo bellman_ford step 3273 current loss 0.083605, current_train_items 104768.
I0304 19:29:46.086157 22502662377600 run.py:483] Algo bellman_ford step 3274 current loss 0.059317, current_train_items 104800.
I0304 19:29:46.106264 22502662377600 run.py:483] Algo bellman_ford step 3275 current loss 0.009670, current_train_items 104832.
I0304 19:29:46.122283 22502662377600 run.py:483] Algo bellman_ford step 3276 current loss 0.033752, current_train_items 104864.
I0304 19:29:46.145367 22502662377600 run.py:483] Algo bellman_ford step 3277 current loss 0.044499, current_train_items 104896.
I0304 19:29:46.177356 22502662377600 run.py:483] Algo bellman_ford step 3278 current loss 0.107844, current_train_items 104928.
I0304 19:29:46.211806 22502662377600 run.py:483] Algo bellman_ford step 3279 current loss 0.128104, current_train_items 104960.
I0304 19:29:46.231298 22502662377600 run.py:483] Algo bellman_ford step 3280 current loss 0.013873, current_train_items 104992.
I0304 19:29:46.247280 22502662377600 run.py:483] Algo bellman_ford step 3281 current loss 0.025007, current_train_items 105024.
I0304 19:29:46.270703 22502662377600 run.py:483] Algo bellman_ford step 3282 current loss 0.075786, current_train_items 105056.
I0304 19:29:46.302499 22502662377600 run.py:483] Algo bellman_ford step 3283 current loss 0.137861, current_train_items 105088.
I0304 19:29:46.336894 22502662377600 run.py:483] Algo bellman_ford step 3284 current loss 0.134698, current_train_items 105120.
I0304 19:29:46.356793 22502662377600 run.py:483] Algo bellman_ford step 3285 current loss 0.010261, current_train_items 105152.
I0304 19:29:46.373351 22502662377600 run.py:483] Algo bellman_ford step 3286 current loss 0.020383, current_train_items 105184.
I0304 19:29:46.396961 22502662377600 run.py:483] Algo bellman_ford step 3287 current loss 0.077065, current_train_items 105216.
I0304 19:29:46.427025 22502662377600 run.py:483] Algo bellman_ford step 3288 current loss 0.054161, current_train_items 105248.
I0304 19:29:46.459026 22502662377600 run.py:483] Algo bellman_ford step 3289 current loss 0.099161, current_train_items 105280.
I0304 19:29:46.478956 22502662377600 run.py:483] Algo bellman_ford step 3290 current loss 0.021657, current_train_items 105312.
I0304 19:29:46.495580 22502662377600 run.py:483] Algo bellman_ford step 3291 current loss 0.042287, current_train_items 105344.
I0304 19:29:46.519052 22502662377600 run.py:483] Algo bellman_ford step 3292 current loss 0.074960, current_train_items 105376.
I0304 19:29:46.550470 22502662377600 run.py:483] Algo bellman_ford step 3293 current loss 0.104586, current_train_items 105408.
I0304 19:29:46.584218 22502662377600 run.py:483] Algo bellman_ford step 3294 current loss 0.095544, current_train_items 105440.
I0304 19:29:46.604040 22502662377600 run.py:483] Algo bellman_ford step 3295 current loss 0.014062, current_train_items 105472.
I0304 19:29:46.619859 22502662377600 run.py:483] Algo bellman_ford step 3296 current loss 0.012011, current_train_items 105504.
I0304 19:29:46.644366 22502662377600 run.py:483] Algo bellman_ford step 3297 current loss 0.106975, current_train_items 105536.
I0304 19:29:46.676883 22502662377600 run.py:483] Algo bellman_ford step 3298 current loss 0.148124, current_train_items 105568.
I0304 19:29:46.711711 22502662377600 run.py:483] Algo bellman_ford step 3299 current loss 0.151049, current_train_items 105600.
I0304 19:29:46.731416 22502662377600 run.py:483] Algo bellman_ford step 3300 current loss 0.021493, current_train_items 105632.
I0304 19:29:46.739350 22502662377600 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0304 19:29:46.739454 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:29:46.756072 22502662377600 run.py:483] Algo bellman_ford step 3301 current loss 0.041945, current_train_items 105664.
I0304 19:29:46.780177 22502662377600 run.py:483] Algo bellman_ford step 3302 current loss 0.145715, current_train_items 105696.
I0304 19:29:46.810666 22502662377600 run.py:483] Algo bellman_ford step 3303 current loss 0.075148, current_train_items 105728.
I0304 19:29:46.843091 22502662377600 run.py:483] Algo bellman_ford step 3304 current loss 0.096825, current_train_items 105760.
I0304 19:29:46.862901 22502662377600 run.py:483] Algo bellman_ford step 3305 current loss 0.007300, current_train_items 105792.
I0304 19:29:46.878947 22502662377600 run.py:483] Algo bellman_ford step 3306 current loss 0.036053, current_train_items 105824.
I0304 19:29:46.903310 22502662377600 run.py:483] Algo bellman_ford step 3307 current loss 0.059190, current_train_items 105856.
I0304 19:29:46.934409 22502662377600 run.py:483] Algo bellman_ford step 3308 current loss 0.050563, current_train_items 105888.
I0304 19:29:46.969966 22502662377600 run.py:483] Algo bellman_ford step 3309 current loss 0.153394, current_train_items 105920.
I0304 19:29:46.989844 22502662377600 run.py:483] Algo bellman_ford step 3310 current loss 0.032386, current_train_items 105952.
I0304 19:29:47.006328 22502662377600 run.py:483] Algo bellman_ford step 3311 current loss 0.148588, current_train_items 105984.
I0304 19:29:47.030157 22502662377600 run.py:483] Algo bellman_ford step 3312 current loss 0.122908, current_train_items 106016.
I0304 19:29:47.062052 22502662377600 run.py:483] Algo bellman_ford step 3313 current loss 0.146113, current_train_items 106048.
I0304 19:29:47.095156 22502662377600 run.py:483] Algo bellman_ford step 3314 current loss 0.116622, current_train_items 106080.
I0304 19:29:47.114999 22502662377600 run.py:483] Algo bellman_ford step 3315 current loss 0.006225, current_train_items 106112.
I0304 19:29:47.131546 22502662377600 run.py:483] Algo bellman_ford step 3316 current loss 0.020623, current_train_items 106144.
I0304 19:29:47.154826 22502662377600 run.py:483] Algo bellman_ford step 3317 current loss 0.116453, current_train_items 106176.
I0304 19:29:47.185971 22502662377600 run.py:483] Algo bellman_ford step 3318 current loss 0.197918, current_train_items 106208.
I0304 19:29:47.220474 22502662377600 run.py:483] Algo bellman_ford step 3319 current loss 0.246383, current_train_items 106240.
I0304 19:29:47.240381 22502662377600 run.py:483] Algo bellman_ford step 3320 current loss 0.013580, current_train_items 106272.
I0304 19:29:47.256608 22502662377600 run.py:483] Algo bellman_ford step 3321 current loss 0.037887, current_train_items 106304.
I0304 19:29:47.281430 22502662377600 run.py:483] Algo bellman_ford step 3322 current loss 0.105413, current_train_items 106336.
I0304 19:29:47.313405 22502662377600 run.py:483] Algo bellman_ford step 3323 current loss 0.123786, current_train_items 106368.
I0304 19:29:47.346609 22502662377600 run.py:483] Algo bellman_ford step 3324 current loss 0.092071, current_train_items 106400.
I0304 19:29:47.366398 22502662377600 run.py:483] Algo bellman_ford step 3325 current loss 0.009496, current_train_items 106432.
I0304 19:29:47.382672 22502662377600 run.py:483] Algo bellman_ford step 3326 current loss 0.060198, current_train_items 106464.
I0304 19:29:47.406172 22502662377600 run.py:483] Algo bellman_ford step 3327 current loss 0.105502, current_train_items 106496.
I0304 19:29:47.436843 22502662377600 run.py:483] Algo bellman_ford step 3328 current loss 0.120175, current_train_items 106528.
I0304 19:29:47.470496 22502662377600 run.py:483] Algo bellman_ford step 3329 current loss 0.150675, current_train_items 106560.
I0304 19:29:47.490288 22502662377600 run.py:483] Algo bellman_ford step 3330 current loss 0.025079, current_train_items 106592.
I0304 19:29:47.506349 22502662377600 run.py:483] Algo bellman_ford step 3331 current loss 0.034593, current_train_items 106624.
I0304 19:29:47.530253 22502662377600 run.py:483] Algo bellman_ford step 3332 current loss 0.064836, current_train_items 106656.
I0304 19:29:47.560854 22502662377600 run.py:483] Algo bellman_ford step 3333 current loss 0.116599, current_train_items 106688.
I0304 19:29:47.593204 22502662377600 run.py:483] Algo bellman_ford step 3334 current loss 0.160994, current_train_items 106720.
I0304 19:29:47.612969 22502662377600 run.py:483] Algo bellman_ford step 3335 current loss 0.010799, current_train_items 106752.
I0304 19:29:47.628704 22502662377600 run.py:483] Algo bellman_ford step 3336 current loss 0.031348, current_train_items 106784.
I0304 19:29:47.653315 22502662377600 run.py:483] Algo bellman_ford step 3337 current loss 0.094531, current_train_items 106816.
I0304 19:29:47.685887 22502662377600 run.py:483] Algo bellman_ford step 3338 current loss 0.139267, current_train_items 106848.
I0304 19:29:47.721207 22502662377600 run.py:483] Algo bellman_ford step 3339 current loss 0.105077, current_train_items 106880.
I0304 19:29:47.741020 22502662377600 run.py:483] Algo bellman_ford step 3340 current loss 0.008986, current_train_items 106912.
I0304 19:29:47.757582 22502662377600 run.py:483] Algo bellman_ford step 3341 current loss 0.045722, current_train_items 106944.
I0304 19:29:47.781684 22502662377600 run.py:483] Algo bellman_ford step 3342 current loss 0.113315, current_train_items 106976.
I0304 19:29:47.813935 22502662377600 run.py:483] Algo bellman_ford step 3343 current loss 0.098437, current_train_items 107008.
I0304 19:29:47.846018 22502662377600 run.py:483] Algo bellman_ford step 3344 current loss 0.076009, current_train_items 107040.
I0304 19:29:47.865866 22502662377600 run.py:483] Algo bellman_ford step 3345 current loss 0.011369, current_train_items 107072.
I0304 19:29:47.881808 22502662377600 run.py:483] Algo bellman_ford step 3346 current loss 0.047193, current_train_items 107104.
I0304 19:29:47.905665 22502662377600 run.py:483] Algo bellman_ford step 3347 current loss 0.133569, current_train_items 107136.
I0304 19:29:47.936906 22502662377600 run.py:483] Algo bellman_ford step 3348 current loss 0.114978, current_train_items 107168.
I0304 19:29:47.968794 22502662377600 run.py:483] Algo bellman_ford step 3349 current loss 0.125875, current_train_items 107200.
I0304 19:29:47.988922 22502662377600 run.py:483] Algo bellman_ford step 3350 current loss 0.035996, current_train_items 107232.
I0304 19:29:47.997160 22502662377600 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0304 19:29:47.997266 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:29:48.014476 22502662377600 run.py:483] Algo bellman_ford step 3351 current loss 0.073611, current_train_items 107264.
I0304 19:29:48.039082 22502662377600 run.py:483] Algo bellman_ford step 3352 current loss 0.156206, current_train_items 107296.
I0304 19:29:48.071161 22502662377600 run.py:483] Algo bellman_ford step 3353 current loss 0.105048, current_train_items 107328.
I0304 19:29:48.105594 22502662377600 run.py:483] Algo bellman_ford step 3354 current loss 0.136080, current_train_items 107360.
I0304 19:29:48.125112 22502662377600 run.py:483] Algo bellman_ford step 3355 current loss 0.005857, current_train_items 107392.
I0304 19:29:48.140771 22502662377600 run.py:483] Algo bellman_ford step 3356 current loss 0.170001, current_train_items 107424.
I0304 19:29:48.164742 22502662377600 run.py:483] Algo bellman_ford step 3357 current loss 0.149745, current_train_items 107456.
I0304 19:29:48.195633 22502662377600 run.py:483] Algo bellman_ford step 3358 current loss 0.136006, current_train_items 107488.
I0304 19:29:48.230912 22502662377600 run.py:483] Algo bellman_ford step 3359 current loss 0.120352, current_train_items 107520.
I0304 19:29:48.250650 22502662377600 run.py:483] Algo bellman_ford step 3360 current loss 0.009086, current_train_items 107552.
I0304 19:29:48.266709 22502662377600 run.py:483] Algo bellman_ford step 3361 current loss 0.050749, current_train_items 107584.
I0304 19:29:48.290634 22502662377600 run.py:483] Algo bellman_ford step 3362 current loss 0.137907, current_train_items 107616.
I0304 19:29:48.321467 22502662377600 run.py:483] Algo bellman_ford step 3363 current loss 0.118802, current_train_items 107648.
I0304 19:29:48.354391 22502662377600 run.py:483] Algo bellman_ford step 3364 current loss 0.132206, current_train_items 107680.
I0304 19:29:48.373952 22502662377600 run.py:483] Algo bellman_ford step 3365 current loss 0.025874, current_train_items 107712.
I0304 19:29:48.389801 22502662377600 run.py:483] Algo bellman_ford step 3366 current loss 0.029530, current_train_items 107744.
I0304 19:29:48.414244 22502662377600 run.py:483] Algo bellman_ford step 3367 current loss 0.142236, current_train_items 107776.
I0304 19:29:48.444156 22502662377600 run.py:483] Algo bellman_ford step 3368 current loss 0.119327, current_train_items 107808.
I0304 19:29:48.476661 22502662377600 run.py:483] Algo bellman_ford step 3369 current loss 0.098608, current_train_items 107840.
I0304 19:29:48.496674 22502662377600 run.py:483] Algo bellman_ford step 3370 current loss 0.012117, current_train_items 107872.
I0304 19:29:48.513236 22502662377600 run.py:483] Algo bellman_ford step 3371 current loss 0.097633, current_train_items 107904.
I0304 19:29:48.536688 22502662377600 run.py:483] Algo bellman_ford step 3372 current loss 0.219934, current_train_items 107936.
I0304 19:29:48.569292 22502662377600 run.py:483] Algo bellman_ford step 3373 current loss 0.176029, current_train_items 107968.
I0304 19:29:48.602487 22502662377600 run.py:483] Algo bellman_ford step 3374 current loss 0.182057, current_train_items 108000.
I0304 19:29:48.622184 22502662377600 run.py:483] Algo bellman_ford step 3375 current loss 0.021680, current_train_items 108032.
I0304 19:29:48.638115 22502662377600 run.py:483] Algo bellman_ford step 3376 current loss 0.025190, current_train_items 108064.
I0304 19:29:48.661298 22502662377600 run.py:483] Algo bellman_ford step 3377 current loss 0.170447, current_train_items 108096.
I0304 19:29:48.691590 22502662377600 run.py:483] Algo bellman_ford step 3378 current loss 0.103958, current_train_items 108128.
I0304 19:29:48.726344 22502662377600 run.py:483] Algo bellman_ford step 3379 current loss 0.168509, current_train_items 108160.
I0304 19:29:48.745814 22502662377600 run.py:483] Algo bellman_ford step 3380 current loss 0.023081, current_train_items 108192.
I0304 19:29:48.762229 22502662377600 run.py:483] Algo bellman_ford step 3381 current loss 0.073805, current_train_items 108224.
I0304 19:29:48.786888 22502662377600 run.py:483] Algo bellman_ford step 3382 current loss 0.068390, current_train_items 108256.
I0304 19:29:48.817036 22502662377600 run.py:483] Algo bellman_ford step 3383 current loss 0.195469, current_train_items 108288.
I0304 19:29:48.852244 22502662377600 run.py:483] Algo bellman_ford step 3384 current loss 0.195188, current_train_items 108320.
I0304 19:29:48.872113 22502662377600 run.py:483] Algo bellman_ford step 3385 current loss 0.011340, current_train_items 108352.
I0304 19:29:48.888643 22502662377600 run.py:483] Algo bellman_ford step 3386 current loss 0.031970, current_train_items 108384.
I0304 19:29:48.912245 22502662377600 run.py:483] Algo bellman_ford step 3387 current loss 0.087634, current_train_items 108416.
I0304 19:29:48.942851 22502662377600 run.py:483] Algo bellman_ford step 3388 current loss 0.105051, current_train_items 108448.
I0304 19:29:48.974960 22502662377600 run.py:483] Algo bellman_ford step 3389 current loss 0.169628, current_train_items 108480.
I0304 19:29:48.995000 22502662377600 run.py:483] Algo bellman_ford step 3390 current loss 0.042997, current_train_items 108512.
I0304 19:29:49.011272 22502662377600 run.py:483] Algo bellman_ford step 3391 current loss 0.031324, current_train_items 108544.
I0304 19:29:49.034796 22502662377600 run.py:483] Algo bellman_ford step 3392 current loss 0.092901, current_train_items 108576.
I0304 19:29:49.065128 22502662377600 run.py:483] Algo bellman_ford step 3393 current loss 0.055380, current_train_items 108608.
I0304 19:29:49.096895 22502662377600 run.py:483] Algo bellman_ford step 3394 current loss 0.053262, current_train_items 108640.
I0304 19:29:49.116486 22502662377600 run.py:483] Algo bellman_ford step 3395 current loss 0.012454, current_train_items 108672.
I0304 19:29:49.132804 22502662377600 run.py:483] Algo bellman_ford step 3396 current loss 0.030837, current_train_items 108704.
I0304 19:29:49.156541 22502662377600 run.py:483] Algo bellman_ford step 3397 current loss 0.044111, current_train_items 108736.
I0304 19:29:49.188045 22502662377600 run.py:483] Algo bellman_ford step 3398 current loss 0.126053, current_train_items 108768.
I0304 19:29:49.220785 22502662377600 run.py:483] Algo bellman_ford step 3399 current loss 0.108661, current_train_items 108800.
I0304 19:29:49.240691 22502662377600 run.py:483] Algo bellman_ford step 3400 current loss 0.009379, current_train_items 108832.
I0304 19:29:49.248677 22502662377600 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0304 19:29:49.248782 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:29:49.265805 22502662377600 run.py:483] Algo bellman_ford step 3401 current loss 0.039805, current_train_items 108864.
I0304 19:29:49.290087 22502662377600 run.py:483] Algo bellman_ford step 3402 current loss 0.072362, current_train_items 108896.
I0304 19:29:49.321638 22502662377600 run.py:483] Algo bellman_ford step 3403 current loss 0.092258, current_train_items 108928.
I0304 19:29:49.356786 22502662377600 run.py:483] Algo bellman_ford step 3404 current loss 0.154534, current_train_items 108960.
I0304 19:29:49.376674 22502662377600 run.py:483] Algo bellman_ford step 3405 current loss 0.007633, current_train_items 108992.
I0304 19:29:49.392182 22502662377600 run.py:483] Algo bellman_ford step 3406 current loss 0.040548, current_train_items 109024.
I0304 19:29:49.416309 22502662377600 run.py:483] Algo bellman_ford step 3407 current loss 0.078042, current_train_items 109056.
I0304 19:29:49.449002 22502662377600 run.py:483] Algo bellman_ford step 3408 current loss 0.094645, current_train_items 109088.
I0304 19:29:49.482037 22502662377600 run.py:483] Algo bellman_ford step 3409 current loss 0.099811, current_train_items 109120.
I0304 19:29:49.501735 22502662377600 run.py:483] Algo bellman_ford step 3410 current loss 0.004165, current_train_items 109152.
I0304 19:29:49.518178 22502662377600 run.py:483] Algo bellman_ford step 3411 current loss 0.047454, current_train_items 109184.
I0304 19:29:49.543121 22502662377600 run.py:483] Algo bellman_ford step 3412 current loss 0.097736, current_train_items 109216.
I0304 19:29:49.575635 22502662377600 run.py:483] Algo bellman_ford step 3413 current loss 0.174116, current_train_items 109248.
I0304 19:29:49.611847 22502662377600 run.py:483] Algo bellman_ford step 3414 current loss 0.216194, current_train_items 109280.
I0304 19:29:49.631453 22502662377600 run.py:483] Algo bellman_ford step 3415 current loss 0.017996, current_train_items 109312.
I0304 19:29:49.647705 22502662377600 run.py:483] Algo bellman_ford step 3416 current loss 0.038223, current_train_items 109344.
I0304 19:29:49.672026 22502662377600 run.py:483] Algo bellman_ford step 3417 current loss 0.144392, current_train_items 109376.
I0304 19:29:49.703400 22502662377600 run.py:483] Algo bellman_ford step 3418 current loss 0.190476, current_train_items 109408.
I0304 19:29:49.736185 22502662377600 run.py:483] Algo bellman_ford step 3419 current loss 0.151974, current_train_items 109440.
I0304 19:29:49.756211 22502662377600 run.py:483] Algo bellman_ford step 3420 current loss 0.026695, current_train_items 109472.
I0304 19:29:49.772188 22502662377600 run.py:483] Algo bellman_ford step 3421 current loss 0.063163, current_train_items 109504.
I0304 19:29:49.796097 22502662377600 run.py:483] Algo bellman_ford step 3422 current loss 0.099097, current_train_items 109536.
I0304 19:29:49.827669 22502662377600 run.py:483] Algo bellman_ford step 3423 current loss 0.107433, current_train_items 109568.
I0304 19:29:49.860761 22502662377600 run.py:483] Algo bellman_ford step 3424 current loss 0.174081, current_train_items 109600.
I0304 19:29:49.880336 22502662377600 run.py:483] Algo bellman_ford step 3425 current loss 0.026984, current_train_items 109632.
I0304 19:29:49.896997 22502662377600 run.py:483] Algo bellman_ford step 3426 current loss 0.049332, current_train_items 109664.
I0304 19:29:49.920782 22502662377600 run.py:483] Algo bellman_ford step 3427 current loss 0.071231, current_train_items 109696.
I0304 19:29:49.952097 22502662377600 run.py:483] Algo bellman_ford step 3428 current loss 0.082119, current_train_items 109728.
I0304 19:29:49.982936 22502662377600 run.py:483] Algo bellman_ford step 3429 current loss 0.078939, current_train_items 109760.
I0304 19:29:50.002481 22502662377600 run.py:483] Algo bellman_ford step 3430 current loss 0.017184, current_train_items 109792.
I0304 19:29:50.019035 22502662377600 run.py:483] Algo bellman_ford step 3431 current loss 0.019768, current_train_items 109824.
I0304 19:29:50.042339 22502662377600 run.py:483] Algo bellman_ford step 3432 current loss 0.062867, current_train_items 109856.
I0304 19:29:50.075415 22502662377600 run.py:483] Algo bellman_ford step 3433 current loss 0.132381, current_train_items 109888.
I0304 19:29:50.107891 22502662377600 run.py:483] Algo bellman_ford step 3434 current loss 0.121767, current_train_items 109920.
I0304 19:29:50.127422 22502662377600 run.py:483] Algo bellman_ford step 3435 current loss 0.014054, current_train_items 109952.
I0304 19:29:50.143760 22502662377600 run.py:483] Algo bellman_ford step 3436 current loss 0.026246, current_train_items 109984.
I0304 19:29:50.167813 22502662377600 run.py:483] Algo bellman_ford step 3437 current loss 0.082358, current_train_items 110016.
I0304 19:29:50.197934 22502662377600 run.py:483] Algo bellman_ford step 3438 current loss 0.123566, current_train_items 110048.
I0304 19:29:50.229829 22502662377600 run.py:483] Algo bellman_ford step 3439 current loss 0.098232, current_train_items 110080.
I0304 19:29:50.249473 22502662377600 run.py:483] Algo bellman_ford step 3440 current loss 0.067210, current_train_items 110112.
I0304 19:29:50.266014 22502662377600 run.py:483] Algo bellman_ford step 3441 current loss 0.066132, current_train_items 110144.
I0304 19:29:50.289575 22502662377600 run.py:483] Algo bellman_ford step 3442 current loss 0.053198, current_train_items 110176.
I0304 19:29:50.321206 22502662377600 run.py:483] Algo bellman_ford step 3443 current loss 0.108436, current_train_items 110208.
I0304 19:29:50.355885 22502662377600 run.py:483] Algo bellman_ford step 3444 current loss 0.117546, current_train_items 110240.
I0304 19:29:50.375311 22502662377600 run.py:483] Algo bellman_ford step 3445 current loss 0.008454, current_train_items 110272.
I0304 19:29:50.391860 22502662377600 run.py:483] Algo bellman_ford step 3446 current loss 0.041022, current_train_items 110304.
I0304 19:29:50.415944 22502662377600 run.py:483] Algo bellman_ford step 3447 current loss 0.085195, current_train_items 110336.
I0304 19:29:50.446718 22502662377600 run.py:483] Algo bellman_ford step 3448 current loss 0.071729, current_train_items 110368.
I0304 19:29:50.480371 22502662377600 run.py:483] Algo bellman_ford step 3449 current loss 0.126172, current_train_items 110400.
I0304 19:29:50.499881 22502662377600 run.py:483] Algo bellman_ford step 3450 current loss 0.034702, current_train_items 110432.
I0304 19:29:50.508192 22502662377600 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0304 19:29:50.508300 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:29:50.524947 22502662377600 run.py:483] Algo bellman_ford step 3451 current loss 0.050898, current_train_items 110464.
I0304 19:29:50.549738 22502662377600 run.py:483] Algo bellman_ford step 3452 current loss 0.101386, current_train_items 110496.
I0304 19:29:50.579811 22502662377600 run.py:483] Algo bellman_ford step 3453 current loss 0.117003, current_train_items 110528.
I0304 19:29:50.615998 22502662377600 run.py:483] Algo bellman_ford step 3454 current loss 0.195628, current_train_items 110560.
I0304 19:29:50.635947 22502662377600 run.py:483] Algo bellman_ford step 3455 current loss 0.010967, current_train_items 110592.
I0304 19:29:50.651472 22502662377600 run.py:483] Algo bellman_ford step 3456 current loss 0.066703, current_train_items 110624.
I0304 19:29:50.675686 22502662377600 run.py:483] Algo bellman_ford step 3457 current loss 0.088466, current_train_items 110656.
I0304 19:29:50.708409 22502662377600 run.py:483] Algo bellman_ford step 3458 current loss 0.115981, current_train_items 110688.
I0304 19:29:50.741258 22502662377600 run.py:483] Algo bellman_ford step 3459 current loss 0.085642, current_train_items 110720.
I0304 19:29:50.761532 22502662377600 run.py:483] Algo bellman_ford step 3460 current loss 0.068441, current_train_items 110752.
I0304 19:29:50.777972 22502662377600 run.py:483] Algo bellman_ford step 3461 current loss 0.060109, current_train_items 110784.
I0304 19:29:50.801865 22502662377600 run.py:483] Algo bellman_ford step 3462 current loss 0.115829, current_train_items 110816.
I0304 19:29:50.832975 22502662377600 run.py:483] Algo bellman_ford step 3463 current loss 0.105527, current_train_items 110848.
I0304 19:29:50.865749 22502662377600 run.py:483] Algo bellman_ford step 3464 current loss 0.121928, current_train_items 110880.
I0304 19:29:50.885459 22502662377600 run.py:483] Algo bellman_ford step 3465 current loss 0.047344, current_train_items 110912.
I0304 19:29:50.902076 22502662377600 run.py:483] Algo bellman_ford step 3466 current loss 0.036080, current_train_items 110944.
I0304 19:29:50.926749 22502662377600 run.py:483] Algo bellman_ford step 3467 current loss 0.114930, current_train_items 110976.
I0304 19:29:50.959259 22502662377600 run.py:483] Algo bellman_ford step 3468 current loss 0.117763, current_train_items 111008.
I0304 19:29:50.995130 22502662377600 run.py:483] Algo bellman_ford step 3469 current loss 0.135292, current_train_items 111040.
I0304 19:29:51.015097 22502662377600 run.py:483] Algo bellman_ford step 3470 current loss 0.042448, current_train_items 111072.
I0304 19:29:51.031923 22502662377600 run.py:483] Algo bellman_ford step 3471 current loss 0.041689, current_train_items 111104.
I0304 19:29:51.055346 22502662377600 run.py:483] Algo bellman_ford step 3472 current loss 0.047599, current_train_items 111136.
I0304 19:29:51.088025 22502662377600 run.py:483] Algo bellman_ford step 3473 current loss 0.100738, current_train_items 111168.
I0304 19:29:51.121920 22502662377600 run.py:483] Algo bellman_ford step 3474 current loss 0.119159, current_train_items 111200.
I0304 19:29:51.142154 22502662377600 run.py:483] Algo bellman_ford step 3475 current loss 0.018270, current_train_items 111232.
I0304 19:29:51.158293 22502662377600 run.py:483] Algo bellman_ford step 3476 current loss 0.047223, current_train_items 111264.
I0304 19:29:51.181251 22502662377600 run.py:483] Algo bellman_ford step 3477 current loss 0.129354, current_train_items 111296.
I0304 19:29:51.211516 22502662377600 run.py:483] Algo bellman_ford step 3478 current loss 0.095083, current_train_items 111328.
I0304 19:29:51.246565 22502662377600 run.py:483] Algo bellman_ford step 3479 current loss 0.144728, current_train_items 111360.
I0304 19:29:51.266265 22502662377600 run.py:483] Algo bellman_ford step 3480 current loss 0.012436, current_train_items 111392.
I0304 19:29:51.282266 22502662377600 run.py:483] Algo bellman_ford step 3481 current loss 0.044243, current_train_items 111424.
I0304 19:29:51.305844 22502662377600 run.py:483] Algo bellman_ford step 3482 current loss 0.047115, current_train_items 111456.
I0304 19:29:51.336186 22502662377600 run.py:483] Algo bellman_ford step 3483 current loss 0.100414, current_train_items 111488.
I0304 19:29:51.369679 22502662377600 run.py:483] Algo bellman_ford step 3484 current loss 0.108800, current_train_items 111520.
I0304 19:29:51.389602 22502662377600 run.py:483] Algo bellman_ford step 3485 current loss 0.012127, current_train_items 111552.
I0304 19:29:51.406044 22502662377600 run.py:483] Algo bellman_ford step 3486 current loss 0.041219, current_train_items 111584.
I0304 19:29:51.430446 22502662377600 run.py:483] Algo bellman_ford step 3487 current loss 0.093947, current_train_items 111616.
I0304 19:29:51.461271 22502662377600 run.py:483] Algo bellman_ford step 3488 current loss 0.093243, current_train_items 111648.
I0304 19:29:51.493667 22502662377600 run.py:483] Algo bellman_ford step 3489 current loss 0.111651, current_train_items 111680.
I0304 19:29:51.513797 22502662377600 run.py:483] Algo bellman_ford step 3490 current loss 0.006560, current_train_items 111712.
I0304 19:29:51.529705 22502662377600 run.py:483] Algo bellman_ford step 3491 current loss 0.038228, current_train_items 111744.
I0304 19:29:51.553311 22502662377600 run.py:483] Algo bellman_ford step 3492 current loss 0.081967, current_train_items 111776.
I0304 19:29:51.585843 22502662377600 run.py:483] Algo bellman_ford step 3493 current loss 0.125946, current_train_items 111808.
I0304 19:29:51.619197 22502662377600 run.py:483] Algo bellman_ford step 3494 current loss 0.097752, current_train_items 111840.
I0304 19:29:51.638742 22502662377600 run.py:483] Algo bellman_ford step 3495 current loss 0.005213, current_train_items 111872.
I0304 19:29:51.654953 22502662377600 run.py:483] Algo bellman_ford step 3496 current loss 0.058928, current_train_items 111904.
I0304 19:29:51.678401 22502662377600 run.py:483] Algo bellman_ford step 3497 current loss 0.065307, current_train_items 111936.
I0304 19:29:51.709629 22502662377600 run.py:483] Algo bellman_ford step 3498 current loss 0.079421, current_train_items 111968.
I0304 19:29:51.744168 22502662377600 run.py:483] Algo bellman_ford step 3499 current loss 0.144885, current_train_items 112000.
I0304 19:29:51.764193 22502662377600 run.py:483] Algo bellman_ford step 3500 current loss 0.013963, current_train_items 112032.
I0304 19:29:51.772005 22502662377600 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.9560546875, 'score': 0.9560546875, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0304 19:29:51.772111 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.956, val scores are: bellman_ford: 0.956
I0304 19:29:51.788315 22502662377600 run.py:483] Algo bellman_ford step 3501 current loss 0.042963, current_train_items 112064.
I0304 19:29:51.812441 22502662377600 run.py:483] Algo bellman_ford step 3502 current loss 0.108661, current_train_items 112096.
I0304 19:29:51.844326 22502662377600 run.py:483] Algo bellman_ford step 3503 current loss 0.079737, current_train_items 112128.
I0304 19:29:51.880225 22502662377600 run.py:483] Algo bellman_ford step 3504 current loss 0.129397, current_train_items 112160.
I0304 19:29:51.900237 22502662377600 run.py:483] Algo bellman_ford step 3505 current loss 0.080149, current_train_items 112192.
I0304 19:29:51.915456 22502662377600 run.py:483] Algo bellman_ford step 3506 current loss 0.047013, current_train_items 112224.
I0304 19:29:51.939763 22502662377600 run.py:483] Algo bellman_ford step 3507 current loss 0.077043, current_train_items 112256.
I0304 19:29:51.970130 22502662377600 run.py:483] Algo bellman_ford step 3508 current loss 0.121974, current_train_items 112288.
I0304 19:29:52.000822 22502662377600 run.py:483] Algo bellman_ford step 3509 current loss 0.085978, current_train_items 112320.
I0304 19:29:52.020326 22502662377600 run.py:483] Algo bellman_ford step 3510 current loss 0.021252, current_train_items 112352.
I0304 19:29:52.036299 22502662377600 run.py:483] Algo bellman_ford step 3511 current loss 0.034825, current_train_items 112384.
I0304 19:29:52.059619 22502662377600 run.py:483] Algo bellman_ford step 3512 current loss 0.115151, current_train_items 112416.
I0304 19:29:52.089730 22502662377600 run.py:483] Algo bellman_ford step 3513 current loss 0.088447, current_train_items 112448.
I0304 19:29:52.122280 22502662377600 run.py:483] Algo bellman_ford step 3514 current loss 0.100919, current_train_items 112480.
I0304 19:29:52.141481 22502662377600 run.py:483] Algo bellman_ford step 3515 current loss 0.011311, current_train_items 112512.
I0304 19:29:52.157254 22502662377600 run.py:483] Algo bellman_ford step 3516 current loss 0.034346, current_train_items 112544.
I0304 19:29:52.181054 22502662377600 run.py:483] Algo bellman_ford step 3517 current loss 0.154958, current_train_items 112576.
I0304 19:29:52.212715 22502662377600 run.py:483] Algo bellman_ford step 3518 current loss 0.261525, current_train_items 112608.
I0304 19:29:52.249128 22502662377600 run.py:483] Algo bellman_ford step 3519 current loss 0.246605, current_train_items 112640.
I0304 19:29:52.268752 22502662377600 run.py:483] Algo bellman_ford step 3520 current loss 0.042941, current_train_items 112672.
I0304 19:29:52.284720 22502662377600 run.py:483] Algo bellman_ford step 3521 current loss 0.079579, current_train_items 112704.
I0304 19:29:52.307506 22502662377600 run.py:483] Algo bellman_ford step 3522 current loss 0.093713, current_train_items 112736.
I0304 19:29:52.338684 22502662377600 run.py:483] Algo bellman_ford step 3523 current loss 0.251294, current_train_items 112768.
I0304 19:29:52.372012 22502662377600 run.py:483] Algo bellman_ford step 3524 current loss 0.286197, current_train_items 112800.
I0304 19:29:52.391866 22502662377600 run.py:483] Algo bellman_ford step 3525 current loss 0.049011, current_train_items 112832.
I0304 19:29:52.407809 22502662377600 run.py:483] Algo bellman_ford step 3526 current loss 0.039963, current_train_items 112864.
I0304 19:29:52.431762 22502662377600 run.py:483] Algo bellman_ford step 3527 current loss 0.081595, current_train_items 112896.
I0304 19:29:52.464299 22502662377600 run.py:483] Algo bellman_ford step 3528 current loss 0.151182, current_train_items 112928.
I0304 19:29:52.496605 22502662377600 run.py:483] Algo bellman_ford step 3529 current loss 0.146895, current_train_items 112960.
I0304 19:29:52.516247 22502662377600 run.py:483] Algo bellman_ford step 3530 current loss 0.024957, current_train_items 112992.
I0304 19:29:52.532623 22502662377600 run.py:483] Algo bellman_ford step 3531 current loss 0.022971, current_train_items 113024.
I0304 19:29:52.557585 22502662377600 run.py:483] Algo bellman_ford step 3532 current loss 0.074723, current_train_items 113056.
I0304 19:29:52.591912 22502662377600 run.py:483] Algo bellman_ford step 3533 current loss 0.221696, current_train_items 113088.
I0304 19:29:52.626219 22502662377600 run.py:483] Algo bellman_ford step 3534 current loss 0.209684, current_train_items 113120.
I0304 19:29:52.645833 22502662377600 run.py:483] Algo bellman_ford step 3535 current loss 0.038013, current_train_items 113152.
I0304 19:29:52.662234 22502662377600 run.py:483] Algo bellman_ford step 3536 current loss 0.054385, current_train_items 113184.
I0304 19:29:52.685927 22502662377600 run.py:483] Algo bellman_ford step 3537 current loss 0.099702, current_train_items 113216.
I0304 19:29:52.717754 22502662377600 run.py:483] Algo bellman_ford step 3538 current loss 0.204273, current_train_items 113248.
I0304 19:29:52.753481 22502662377600 run.py:483] Algo bellman_ford step 3539 current loss 0.159416, current_train_items 113280.
I0304 19:29:52.773002 22502662377600 run.py:483] Algo bellman_ford step 3540 current loss 0.014699, current_train_items 113312.
I0304 19:29:52.789001 22502662377600 run.py:483] Algo bellman_ford step 3541 current loss 0.045529, current_train_items 113344.
I0304 19:29:52.813302 22502662377600 run.py:483] Algo bellman_ford step 3542 current loss 0.102092, current_train_items 113376.
I0304 19:29:52.844094 22502662377600 run.py:483] Algo bellman_ford step 3543 current loss 0.095233, current_train_items 113408.
I0304 19:29:52.877324 22502662377600 run.py:483] Algo bellman_ford step 3544 current loss 0.131063, current_train_items 113440.
I0304 19:29:52.896789 22502662377600 run.py:483] Algo bellman_ford step 3545 current loss 0.012817, current_train_items 113472.
I0304 19:29:52.912710 22502662377600 run.py:483] Algo bellman_ford step 3546 current loss 0.022396, current_train_items 113504.
I0304 19:29:52.937133 22502662377600 run.py:483] Algo bellman_ford step 3547 current loss 0.149547, current_train_items 113536.
I0304 19:29:52.966899 22502662377600 run.py:483] Algo bellman_ford step 3548 current loss 0.146729, current_train_items 113568.
I0304 19:29:53.000382 22502662377600 run.py:483] Algo bellman_ford step 3549 current loss 0.150209, current_train_items 113600.
I0304 19:29:53.020294 22502662377600 run.py:483] Algo bellman_ford step 3550 current loss 0.027011, current_train_items 113632.
I0304 19:29:53.028279 22502662377600 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0304 19:29:53.028383 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:29:53.045580 22502662377600 run.py:483] Algo bellman_ford step 3551 current loss 0.072570, current_train_items 113664.
I0304 19:29:53.071700 22502662377600 run.py:483] Algo bellman_ford step 3552 current loss 0.114086, current_train_items 113696.
I0304 19:29:53.103112 22502662377600 run.py:483] Algo bellman_ford step 3553 current loss 0.063134, current_train_items 113728.
I0304 19:29:53.137732 22502662377600 run.py:483] Algo bellman_ford step 3554 current loss 0.141867, current_train_items 113760.
I0304 19:29:53.158023 22502662377600 run.py:483] Algo bellman_ford step 3555 current loss 0.016552, current_train_items 113792.
I0304 19:29:53.174027 22502662377600 run.py:483] Algo bellman_ford step 3556 current loss 0.027355, current_train_items 113824.
I0304 19:29:53.198018 22502662377600 run.py:483] Algo bellman_ford step 3557 current loss 0.110840, current_train_items 113856.
I0304 19:29:53.230050 22502662377600 run.py:483] Algo bellman_ford step 3558 current loss 0.119786, current_train_items 113888.
I0304 19:29:53.263496 22502662377600 run.py:483] Algo bellman_ford step 3559 current loss 0.086392, current_train_items 113920.
I0304 19:29:53.283498 22502662377600 run.py:483] Algo bellman_ford step 3560 current loss 0.011784, current_train_items 113952.
I0304 19:29:53.299807 22502662377600 run.py:483] Algo bellman_ford step 3561 current loss 0.037483, current_train_items 113984.
I0304 19:29:53.324951 22502662377600 run.py:483] Algo bellman_ford step 3562 current loss 0.137697, current_train_items 114016.
I0304 19:29:53.355627 22502662377600 run.py:483] Algo bellman_ford step 3563 current loss 0.089416, current_train_items 114048.
I0304 19:29:53.392360 22502662377600 run.py:483] Algo bellman_ford step 3564 current loss 0.127414, current_train_items 114080.
I0304 19:29:53.412208 22502662377600 run.py:483] Algo bellman_ford step 3565 current loss 0.007569, current_train_items 114112.
I0304 19:29:53.428602 22502662377600 run.py:483] Algo bellman_ford step 3566 current loss 0.052017, current_train_items 114144.
I0304 19:29:53.452919 22502662377600 run.py:483] Algo bellman_ford step 3567 current loss 0.108278, current_train_items 114176.
I0304 19:29:53.484429 22502662377600 run.py:483] Algo bellman_ford step 3568 current loss 0.117409, current_train_items 114208.
I0304 19:29:53.518746 22502662377600 run.py:483] Algo bellman_ford step 3569 current loss 0.122464, current_train_items 114240.
I0304 19:29:53.538653 22502662377600 run.py:483] Algo bellman_ford step 3570 current loss 0.008057, current_train_items 114272.
I0304 19:29:53.554611 22502662377600 run.py:483] Algo bellman_ford step 3571 current loss 0.032022, current_train_items 114304.
I0304 19:29:53.578170 22502662377600 run.py:483] Algo bellman_ford step 3572 current loss 0.105709, current_train_items 114336.
I0304 19:29:53.608972 22502662377600 run.py:483] Algo bellman_ford step 3573 current loss 0.105293, current_train_items 114368.
I0304 19:29:53.644735 22502662377600 run.py:483] Algo bellman_ford step 3574 current loss 0.172589, current_train_items 114400.
I0304 19:29:53.664742 22502662377600 run.py:483] Algo bellman_ford step 3575 current loss 0.012394, current_train_items 114432.
I0304 19:29:53.681094 22502662377600 run.py:483] Algo bellman_ford step 3576 current loss 0.033536, current_train_items 114464.
I0304 19:29:53.704664 22502662377600 run.py:483] Algo bellman_ford step 3577 current loss 0.141965, current_train_items 114496.
I0304 19:29:53.736050 22502662377600 run.py:483] Algo bellman_ford step 3578 current loss 0.123012, current_train_items 114528.
I0304 19:29:53.769067 22502662377600 run.py:483] Algo bellman_ford step 3579 current loss 0.142803, current_train_items 114560.
I0304 19:29:53.789001 22502662377600 run.py:483] Algo bellman_ford step 3580 current loss 0.009573, current_train_items 114592.
I0304 19:29:53.805308 22502662377600 run.py:483] Algo bellman_ford step 3581 current loss 0.150647, current_train_items 114624.
I0304 19:29:53.830908 22502662377600 run.py:483] Algo bellman_ford step 3582 current loss 0.088368, current_train_items 114656.
I0304 19:29:53.863082 22502662377600 run.py:483] Algo bellman_ford step 3583 current loss 0.103113, current_train_items 114688.
I0304 19:29:53.897674 22502662377600 run.py:483] Algo bellman_ford step 3584 current loss 0.092528, current_train_items 114720.
I0304 19:29:53.917542 22502662377600 run.py:483] Algo bellman_ford step 3585 current loss 0.006424, current_train_items 114752.
I0304 19:29:53.933857 22502662377600 run.py:483] Algo bellman_ford step 3586 current loss 0.094086, current_train_items 114784.
I0304 19:29:53.957402 22502662377600 run.py:483] Algo bellman_ford step 3587 current loss 0.135478, current_train_items 114816.
I0304 19:29:53.989634 22502662377600 run.py:483] Algo bellman_ford step 3588 current loss 0.117698, current_train_items 114848.
I0304 19:29:54.024468 22502662377600 run.py:483] Algo bellman_ford step 3589 current loss 0.126629, current_train_items 114880.
I0304 19:29:54.044383 22502662377600 run.py:483] Algo bellman_ford step 3590 current loss 0.007450, current_train_items 114912.
I0304 19:29:54.060459 22502662377600 run.py:483] Algo bellman_ford step 3591 current loss 0.019813, current_train_items 114944.
I0304 19:29:54.085733 22502662377600 run.py:483] Algo bellman_ford step 3592 current loss 0.081580, current_train_items 114976.
I0304 19:29:54.118537 22502662377600 run.py:483] Algo bellman_ford step 3593 current loss 0.153771, current_train_items 115008.
I0304 19:29:54.153338 22502662377600 run.py:483] Algo bellman_ford step 3594 current loss 0.118062, current_train_items 115040.
I0304 19:29:54.173280 22502662377600 run.py:483] Algo bellman_ford step 3595 current loss 0.013478, current_train_items 115072.
I0304 19:29:54.189804 22502662377600 run.py:483] Algo bellman_ford step 3596 current loss 0.072645, current_train_items 115104.
I0304 19:29:54.213332 22502662377600 run.py:483] Algo bellman_ford step 3597 current loss 0.052981, current_train_items 115136.
I0304 19:29:54.243889 22502662377600 run.py:483] Algo bellman_ford step 3598 current loss 0.074172, current_train_items 115168.
I0304 19:29:54.278459 22502662377600 run.py:483] Algo bellman_ford step 3599 current loss 0.092406, current_train_items 115200.
I0304 19:29:54.298444 22502662377600 run.py:483] Algo bellman_ford step 3600 current loss 0.008862, current_train_items 115232.
I0304 19:29:54.306435 22502662377600 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0304 19:29:54.306550 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:29:54.323711 22502662377600 run.py:483] Algo bellman_ford step 3601 current loss 0.027181, current_train_items 115264.
I0304 19:29:54.348284 22502662377600 run.py:483] Algo bellman_ford step 3602 current loss 0.049295, current_train_items 115296.
I0304 19:29:54.379759 22502662377600 run.py:483] Algo bellman_ford step 3603 current loss 0.063330, current_train_items 115328.
I0304 19:29:54.416870 22502662377600 run.py:483] Algo bellman_ford step 3604 current loss 0.152736, current_train_items 115360.
I0304 19:29:54.436713 22502662377600 run.py:483] Algo bellman_ford step 3605 current loss 0.022128, current_train_items 115392.
I0304 19:29:54.452992 22502662377600 run.py:483] Algo bellman_ford step 3606 current loss 0.054029, current_train_items 115424.
I0304 19:29:54.475802 22502662377600 run.py:483] Algo bellman_ford step 3607 current loss 0.024629, current_train_items 115456.
I0304 19:29:54.506492 22502662377600 run.py:483] Algo bellman_ford step 3608 current loss 0.072305, current_train_items 115488.
I0304 19:29:54.538619 22502662377600 run.py:483] Algo bellman_ford step 3609 current loss 0.071806, current_train_items 115520.
I0304 19:29:54.558377 22502662377600 run.py:483] Algo bellman_ford step 3610 current loss 0.009710, current_train_items 115552.
I0304 19:29:54.574827 22502662377600 run.py:483] Algo bellman_ford step 3611 current loss 0.023284, current_train_items 115584.
I0304 19:29:54.599827 22502662377600 run.py:483] Algo bellman_ford step 3612 current loss 0.092712, current_train_items 115616.
I0304 19:29:54.630403 22502662377600 run.py:483] Algo bellman_ford step 3613 current loss 0.088129, current_train_items 115648.
I0304 19:29:54.663091 22502662377600 run.py:483] Algo bellman_ford step 3614 current loss 0.117950, current_train_items 115680.
I0304 19:29:54.682518 22502662377600 run.py:483] Algo bellman_ford step 3615 current loss 0.036245, current_train_items 115712.
I0304 19:29:54.699380 22502662377600 run.py:483] Algo bellman_ford step 3616 current loss 0.060029, current_train_items 115744.
I0304 19:29:54.722910 22502662377600 run.py:483] Algo bellman_ford step 3617 current loss 0.078810, current_train_items 115776.
I0304 19:29:54.754595 22502662377600 run.py:483] Algo bellman_ford step 3618 current loss 0.113822, current_train_items 115808.
I0304 19:29:54.785554 22502662377600 run.py:483] Algo bellman_ford step 3619 current loss 0.082623, current_train_items 115840.
I0304 19:29:54.805299 22502662377600 run.py:483] Algo bellman_ford step 3620 current loss 0.022364, current_train_items 115872.
I0304 19:29:54.821622 22502662377600 run.py:483] Algo bellman_ford step 3621 current loss 0.056010, current_train_items 115904.
I0304 19:29:54.845191 22502662377600 run.py:483] Algo bellman_ford step 3622 current loss 0.067876, current_train_items 115936.
I0304 19:29:54.876911 22502662377600 run.py:483] Algo bellman_ford step 3623 current loss 0.149988, current_train_items 115968.
I0304 19:29:54.909899 22502662377600 run.py:483] Algo bellman_ford step 3624 current loss 0.138285, current_train_items 116000.
I0304 19:29:54.929308 22502662377600 run.py:483] Algo bellman_ford step 3625 current loss 0.021536, current_train_items 116032.
I0304 19:29:54.945656 22502662377600 run.py:483] Algo bellman_ford step 3626 current loss 0.053336, current_train_items 116064.
I0304 19:29:54.969940 22502662377600 run.py:483] Algo bellman_ford step 3627 current loss 0.121388, current_train_items 116096.
I0304 19:29:55.001719 22502662377600 run.py:483] Algo bellman_ford step 3628 current loss 0.180521, current_train_items 116128.
I0304 19:29:55.034514 22502662377600 run.py:483] Algo bellman_ford step 3629 current loss 0.111028, current_train_items 116160.
I0304 19:29:55.054358 22502662377600 run.py:483] Algo bellman_ford step 3630 current loss 0.043161, current_train_items 116192.
I0304 19:29:55.070452 22502662377600 run.py:483] Algo bellman_ford step 3631 current loss 0.050261, current_train_items 116224.
I0304 19:29:55.094342 22502662377600 run.py:483] Algo bellman_ford step 3632 current loss 0.159471, current_train_items 116256.
I0304 19:29:55.125766 22502662377600 run.py:483] Algo bellman_ford step 3633 current loss 0.186995, current_train_items 116288.
I0304 19:29:55.158481 22502662377600 run.py:483] Algo bellman_ford step 3634 current loss 0.215702, current_train_items 116320.
I0304 19:29:55.177977 22502662377600 run.py:483] Algo bellman_ford step 3635 current loss 0.009197, current_train_items 116352.
I0304 19:29:55.194092 22502662377600 run.py:483] Algo bellman_ford step 3636 current loss 0.050436, current_train_items 116384.
I0304 19:29:55.217005 22502662377600 run.py:483] Algo bellman_ford step 3637 current loss 0.101775, current_train_items 116416.
I0304 19:29:55.248600 22502662377600 run.py:483] Algo bellman_ford step 3638 current loss 0.072154, current_train_items 116448.
I0304 19:29:55.281326 22502662377600 run.py:483] Algo bellman_ford step 3639 current loss 0.101512, current_train_items 116480.
I0304 19:29:55.300698 22502662377600 run.py:483] Algo bellman_ford step 3640 current loss 0.015297, current_train_items 116512.
I0304 19:29:55.316473 22502662377600 run.py:483] Algo bellman_ford step 3641 current loss 0.061544, current_train_items 116544.
I0304 19:29:55.340783 22502662377600 run.py:483] Algo bellman_ford step 3642 current loss 0.184477, current_train_items 116576.
I0304 19:29:55.374677 22502662377600 run.py:483] Algo bellman_ford step 3643 current loss 0.090068, current_train_items 116608.
I0304 19:29:55.408933 22502662377600 run.py:483] Algo bellman_ford step 3644 current loss 0.114487, current_train_items 116640.
I0304 19:29:55.428965 22502662377600 run.py:483] Algo bellman_ford step 3645 current loss 0.007487, current_train_items 116672.
I0304 19:29:55.445180 22502662377600 run.py:483] Algo bellman_ford step 3646 current loss 0.060485, current_train_items 116704.
I0304 19:29:55.470617 22502662377600 run.py:483] Algo bellman_ford step 3647 current loss 0.100130, current_train_items 116736.
I0304 19:29:55.501199 22502662377600 run.py:483] Algo bellman_ford step 3648 current loss 0.068529, current_train_items 116768.
I0304 19:29:55.535272 22502662377600 run.py:483] Algo bellman_ford step 3649 current loss 0.106213, current_train_items 116800.
I0304 19:29:55.554918 22502662377600 run.py:483] Algo bellman_ford step 3650 current loss 0.004700, current_train_items 116832.
I0304 19:29:55.562914 22502662377600 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0304 19:29:55.563018 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:29:55.580007 22502662377600 run.py:483] Algo bellman_ford step 3651 current loss 0.034763, current_train_items 116864.
I0304 19:29:55.603287 22502662377600 run.py:483] Algo bellman_ford step 3652 current loss 0.087005, current_train_items 116896.
I0304 19:29:55.634144 22502662377600 run.py:483] Algo bellman_ford step 3653 current loss 0.061893, current_train_items 116928.
I0304 19:29:55.667366 22502662377600 run.py:483] Algo bellman_ford step 3654 current loss 0.124631, current_train_items 116960.
I0304 19:29:55.686870 22502662377600 run.py:483] Algo bellman_ford step 3655 current loss 0.010302, current_train_items 116992.
I0304 19:29:55.702220 22502662377600 run.py:483] Algo bellman_ford step 3656 current loss 0.035947, current_train_items 117024.
I0304 19:29:55.726590 22502662377600 run.py:483] Algo bellman_ford step 3657 current loss 0.087764, current_train_items 117056.
I0304 19:29:55.757259 22502662377600 run.py:483] Algo bellman_ford step 3658 current loss 0.107989, current_train_items 117088.
I0304 19:29:55.790926 22502662377600 run.py:483] Algo bellman_ford step 3659 current loss 0.153685, current_train_items 117120.
I0304 19:29:55.810882 22502662377600 run.py:483] Algo bellman_ford step 3660 current loss 0.014518, current_train_items 117152.
I0304 19:29:55.827684 22502662377600 run.py:483] Algo bellman_ford step 3661 current loss 0.057271, current_train_items 117184.
I0304 19:29:55.850670 22502662377600 run.py:483] Algo bellman_ford step 3662 current loss 0.059221, current_train_items 117216.
I0304 19:29:55.882686 22502662377600 run.py:483] Algo bellman_ford step 3663 current loss 0.109657, current_train_items 117248.
I0304 19:29:55.917562 22502662377600 run.py:483] Algo bellman_ford step 3664 current loss 0.157235, current_train_items 117280.
I0304 19:29:55.937138 22502662377600 run.py:483] Algo bellman_ford step 3665 current loss 0.048040, current_train_items 117312.
I0304 19:29:55.953063 22502662377600 run.py:483] Algo bellman_ford step 3666 current loss 0.043581, current_train_items 117344.
I0304 19:29:55.977072 22502662377600 run.py:483] Algo bellman_ford step 3667 current loss 0.088612, current_train_items 117376.
I0304 19:29:56.008676 22502662377600 run.py:483] Algo bellman_ford step 3668 current loss 0.100756, current_train_items 117408.
I0304 19:29:56.043729 22502662377600 run.py:483] Algo bellman_ford step 3669 current loss 0.100201, current_train_items 117440.
I0304 19:29:56.063534 22502662377600 run.py:483] Algo bellman_ford step 3670 current loss 0.014021, current_train_items 117472.
I0304 19:29:56.079994 22502662377600 run.py:483] Algo bellman_ford step 3671 current loss 0.074895, current_train_items 117504.
I0304 19:29:56.103200 22502662377600 run.py:483] Algo bellman_ford step 3672 current loss 0.091502, current_train_items 117536.
I0304 19:29:56.134277 22502662377600 run.py:483] Algo bellman_ford step 3673 current loss 0.065276, current_train_items 117568.
I0304 19:29:56.168958 22502662377600 run.py:483] Algo bellman_ford step 3674 current loss 0.146153, current_train_items 117600.
I0304 19:29:56.188577 22502662377600 run.py:483] Algo bellman_ford step 3675 current loss 0.024857, current_train_items 117632.
I0304 19:29:56.204488 22502662377600 run.py:483] Algo bellman_ford step 3676 current loss 0.028102, current_train_items 117664.
I0304 19:29:56.227881 22502662377600 run.py:483] Algo bellman_ford step 3677 current loss 0.052878, current_train_items 117696.
I0304 19:29:56.259271 22502662377600 run.py:483] Algo bellman_ford step 3678 current loss 0.094562, current_train_items 117728.
I0304 19:29:56.292782 22502662377600 run.py:483] Algo bellman_ford step 3679 current loss 0.099182, current_train_items 117760.
I0304 19:29:56.312147 22502662377600 run.py:483] Algo bellman_ford step 3680 current loss 0.004457, current_train_items 117792.
I0304 19:29:56.328580 22502662377600 run.py:483] Algo bellman_ford step 3681 current loss 0.029822, current_train_items 117824.
I0304 19:29:56.352872 22502662377600 run.py:483] Algo bellman_ford step 3682 current loss 0.113318, current_train_items 117856.
I0304 19:29:56.383518 22502662377600 run.py:483] Algo bellman_ford step 3683 current loss 0.099235, current_train_items 117888.
I0304 19:29:56.417159 22502662377600 run.py:483] Algo bellman_ford step 3684 current loss 0.122364, current_train_items 117920.
I0304 19:29:56.436817 22502662377600 run.py:483] Algo bellman_ford step 3685 current loss 0.006387, current_train_items 117952.
I0304 19:29:56.452987 22502662377600 run.py:483] Algo bellman_ford step 3686 current loss 0.100288, current_train_items 117984.
I0304 19:29:56.476287 22502662377600 run.py:483] Algo bellman_ford step 3687 current loss 0.093468, current_train_items 118016.
I0304 19:29:56.508765 22502662377600 run.py:483] Algo bellman_ford step 3688 current loss 0.099748, current_train_items 118048.
I0304 19:29:56.544065 22502662377600 run.py:483] Algo bellman_ford step 3689 current loss 0.104863, current_train_items 118080.
I0304 19:29:56.563924 22502662377600 run.py:483] Algo bellman_ford step 3690 current loss 0.006451, current_train_items 118112.
I0304 19:29:56.580211 22502662377600 run.py:483] Algo bellman_ford step 3691 current loss 0.029179, current_train_items 118144.
I0304 19:29:56.602311 22502662377600 run.py:483] Algo bellman_ford step 3692 current loss 0.081319, current_train_items 118176.
I0304 19:29:56.634897 22502662377600 run.py:483] Algo bellman_ford step 3693 current loss 0.098921, current_train_items 118208.
I0304 19:29:56.667497 22502662377600 run.py:483] Algo bellman_ford step 3694 current loss 0.068392, current_train_items 118240.
I0304 19:29:56.687131 22502662377600 run.py:483] Algo bellman_ford step 3695 current loss 0.013603, current_train_items 118272.
I0304 19:29:56.703894 22502662377600 run.py:483] Algo bellman_ford step 3696 current loss 0.090009, current_train_items 118304.
I0304 19:29:56.727948 22502662377600 run.py:483] Algo bellman_ford step 3697 current loss 0.146619, current_train_items 118336.
I0304 19:29:56.759760 22502662377600 run.py:483] Algo bellman_ford step 3698 current loss 0.193597, current_train_items 118368.
I0304 19:29:56.791773 22502662377600 run.py:483] Algo bellman_ford step 3699 current loss 0.170507, current_train_items 118400.
I0304 19:29:56.811617 22502662377600 run.py:483] Algo bellman_ford step 3700 current loss 0.075370, current_train_items 118432.
I0304 19:29:56.819502 22502662377600 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0304 19:29:56.819606 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:29:56.836140 22502662377600 run.py:483] Algo bellman_ford step 3701 current loss 0.026216, current_train_items 118464.
I0304 19:29:56.860816 22502662377600 run.py:483] Algo bellman_ford step 3702 current loss 0.105746, current_train_items 118496.
I0304 19:29:56.893807 22502662377600 run.py:483] Algo bellman_ford step 3703 current loss 0.099286, current_train_items 118528.
I0304 19:29:56.927816 22502662377600 run.py:483] Algo bellman_ford step 3704 current loss 0.095908, current_train_items 118560.
I0304 19:29:56.947714 22502662377600 run.py:483] Algo bellman_ford step 3705 current loss 0.005887, current_train_items 118592.
I0304 19:29:56.963899 22502662377600 run.py:483] Algo bellman_ford step 3706 current loss 0.072170, current_train_items 118624.
I0304 19:29:56.988005 22502662377600 run.py:483] Algo bellman_ford step 3707 current loss 0.113373, current_train_items 118656.
I0304 19:29:57.018403 22502662377600 run.py:483] Algo bellman_ford step 3708 current loss 0.076033, current_train_items 118688.
I0304 19:29:57.052528 22502662377600 run.py:483] Algo bellman_ford step 3709 current loss 0.141054, current_train_items 118720.
I0304 19:29:57.072391 22502662377600 run.py:483] Algo bellman_ford step 3710 current loss 0.018195, current_train_items 118752.
I0304 19:29:57.088762 22502662377600 run.py:483] Algo bellman_ford step 3711 current loss 0.043533, current_train_items 118784.
I0304 19:29:57.112999 22502662377600 run.py:483] Algo bellman_ford step 3712 current loss 0.062475, current_train_items 118816.
I0304 19:29:57.145996 22502662377600 run.py:483] Algo bellman_ford step 3713 current loss 0.119417, current_train_items 118848.
I0304 19:29:57.180836 22502662377600 run.py:483] Algo bellman_ford step 3714 current loss 0.161418, current_train_items 118880.
I0304 19:29:57.200272 22502662377600 run.py:483] Algo bellman_ford step 3715 current loss 0.007090, current_train_items 118912.
I0304 19:29:57.216370 22502662377600 run.py:483] Algo bellman_ford step 3716 current loss 0.014795, current_train_items 118944.
I0304 19:29:57.240231 22502662377600 run.py:483] Algo bellman_ford step 3717 current loss 0.080746, current_train_items 118976.
I0304 19:29:57.271872 22502662377600 run.py:483] Algo bellman_ford step 3718 current loss 0.118692, current_train_items 119008.
I0304 19:29:57.304577 22502662377600 run.py:483] Algo bellman_ford step 3719 current loss 0.071344, current_train_items 119040.
I0304 19:29:57.324525 22502662377600 run.py:483] Algo bellman_ford step 3720 current loss 0.007408, current_train_items 119072.
I0304 19:29:57.340874 22502662377600 run.py:483] Algo bellman_ford step 3721 current loss 0.060876, current_train_items 119104.
I0304 19:29:57.364573 22502662377600 run.py:483] Algo bellman_ford step 3722 current loss 0.039785, current_train_items 119136.
I0304 19:29:57.395623 22502662377600 run.py:483] Algo bellman_ford step 3723 current loss 0.098496, current_train_items 119168.
I0304 19:29:57.429737 22502662377600 run.py:483] Algo bellman_ford step 3724 current loss 0.120555, current_train_items 119200.
I0304 19:29:57.449423 22502662377600 run.py:483] Algo bellman_ford step 3725 current loss 0.039889, current_train_items 119232.
I0304 19:29:57.465956 22502662377600 run.py:483] Algo bellman_ford step 3726 current loss 0.032870, current_train_items 119264.
I0304 19:29:57.490431 22502662377600 run.py:483] Algo bellman_ford step 3727 current loss 0.084690, current_train_items 119296.
I0304 19:29:57.522102 22502662377600 run.py:483] Algo bellman_ford step 3728 current loss 0.114189, current_train_items 119328.
I0304 19:29:57.555904 22502662377600 run.py:483] Algo bellman_ford step 3729 current loss 0.098167, current_train_items 119360.
I0304 19:29:57.575662 22502662377600 run.py:483] Algo bellman_ford step 3730 current loss 0.010017, current_train_items 119392.
I0304 19:29:57.591646 22502662377600 run.py:483] Algo bellman_ford step 3731 current loss 0.076243, current_train_items 119424.
I0304 19:29:57.615979 22502662377600 run.py:483] Algo bellman_ford step 3732 current loss 0.087138, current_train_items 119456.
I0304 19:29:57.645715 22502662377600 run.py:483] Algo bellman_ford step 3733 current loss 0.080126, current_train_items 119488.
I0304 19:29:57.679987 22502662377600 run.py:483] Algo bellman_ford step 3734 current loss 0.206688, current_train_items 119520.
I0304 19:29:57.700105 22502662377600 run.py:483] Algo bellman_ford step 3735 current loss 0.015237, current_train_items 119552.
I0304 19:29:57.716141 22502662377600 run.py:483] Algo bellman_ford step 3736 current loss 0.053636, current_train_items 119584.
I0304 19:29:57.739499 22502662377600 run.py:483] Algo bellman_ford step 3737 current loss 0.077217, current_train_items 119616.
I0304 19:29:57.771168 22502662377600 run.py:483] Algo bellman_ford step 3738 current loss 0.090122, current_train_items 119648.
I0304 19:29:57.804540 22502662377600 run.py:483] Algo bellman_ford step 3739 current loss 0.087656, current_train_items 119680.
I0304 19:29:57.824565 22502662377600 run.py:483] Algo bellman_ford step 3740 current loss 0.012620, current_train_items 119712.
I0304 19:29:57.840555 22502662377600 run.py:483] Algo bellman_ford step 3741 current loss 0.036179, current_train_items 119744.
I0304 19:29:57.863991 22502662377600 run.py:483] Algo bellman_ford step 3742 current loss 0.126401, current_train_items 119776.
I0304 19:29:57.895058 22502662377600 run.py:483] Algo bellman_ford step 3743 current loss 0.248061, current_train_items 119808.
I0304 19:29:57.927157 22502662377600 run.py:483] Algo bellman_ford step 3744 current loss 0.132281, current_train_items 119840.
I0304 19:29:57.946655 22502662377600 run.py:483] Algo bellman_ford step 3745 current loss 0.025272, current_train_items 119872.
I0304 19:29:57.962536 22502662377600 run.py:483] Algo bellman_ford step 3746 current loss 0.074292, current_train_items 119904.
I0304 19:29:57.987006 22502662377600 run.py:483] Algo bellman_ford step 3747 current loss 0.163329, current_train_items 119936.
I0304 19:29:58.019017 22502662377600 run.py:483] Algo bellman_ford step 3748 current loss 0.281492, current_train_items 119968.
I0304 19:29:58.054288 22502662377600 run.py:483] Algo bellman_ford step 3749 current loss 0.361234, current_train_items 120000.
I0304 19:29:58.074172 22502662377600 run.py:483] Algo bellman_ford step 3750 current loss 0.017186, current_train_items 120032.
I0304 19:29:58.082411 22502662377600 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.9599609375, 'score': 0.9599609375, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0304 19:29:58.082557 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.960, val scores are: bellman_ford: 0.960
I0304 19:29:58.099570 22502662377600 run.py:483] Algo bellman_ford step 3751 current loss 0.044820, current_train_items 120064.
I0304 19:29:58.124070 22502662377600 run.py:483] Algo bellman_ford step 3752 current loss 0.090235, current_train_items 120096.
I0304 19:29:58.155444 22502662377600 run.py:483] Algo bellman_ford step 3753 current loss 0.176309, current_train_items 120128.
I0304 19:29:58.191206 22502662377600 run.py:483] Algo bellman_ford step 3754 current loss 0.196752, current_train_items 120160.
I0304 19:29:58.210797 22502662377600 run.py:483] Algo bellman_ford step 3755 current loss 0.014451, current_train_items 120192.
I0304 19:29:58.226968 22502662377600 run.py:483] Algo bellman_ford step 3756 current loss 0.045531, current_train_items 120224.
I0304 19:29:58.251074 22502662377600 run.py:483] Algo bellman_ford step 3757 current loss 0.075945, current_train_items 120256.
I0304 19:29:58.282515 22502662377600 run.py:483] Algo bellman_ford step 3758 current loss 0.141510, current_train_items 120288.
I0304 19:29:58.314457 22502662377600 run.py:483] Algo bellman_ford step 3759 current loss 0.138630, current_train_items 120320.
I0304 19:29:58.334431 22502662377600 run.py:483] Algo bellman_ford step 3760 current loss 0.004391, current_train_items 120352.
I0304 19:29:58.350713 22502662377600 run.py:483] Algo bellman_ford step 3761 current loss 0.012974, current_train_items 120384.
I0304 19:29:58.373550 22502662377600 run.py:483] Algo bellman_ford step 3762 current loss 0.024363, current_train_items 120416.
I0304 19:29:58.404281 22502662377600 run.py:483] Algo bellman_ford step 3763 current loss 0.053493, current_train_items 120448.
I0304 19:29:58.438234 22502662377600 run.py:483] Algo bellman_ford step 3764 current loss 0.082544, current_train_items 120480.
I0304 19:29:58.457588 22502662377600 run.py:483] Algo bellman_ford step 3765 current loss 0.003659, current_train_items 120512.
I0304 19:29:58.474417 22502662377600 run.py:483] Algo bellman_ford step 3766 current loss 0.026015, current_train_items 120544.
I0304 19:29:58.498608 22502662377600 run.py:483] Algo bellman_ford step 3767 current loss 0.126680, current_train_items 120576.
I0304 19:29:58.530451 22502662377600 run.py:483] Algo bellman_ford step 3768 current loss 0.092775, current_train_items 120608.
I0304 19:29:58.562864 22502662377600 run.py:483] Algo bellman_ford step 3769 current loss 0.079187, current_train_items 120640.
I0304 19:29:58.583061 22502662377600 run.py:483] Algo bellman_ford step 3770 current loss 0.006377, current_train_items 120672.
I0304 19:29:58.599443 22502662377600 run.py:483] Algo bellman_ford step 3771 current loss 0.076019, current_train_items 120704.
I0304 19:29:58.622146 22502662377600 run.py:483] Algo bellman_ford step 3772 current loss 0.103303, current_train_items 120736.
I0304 19:29:58.652672 22502662377600 run.py:483] Algo bellman_ford step 3773 current loss 0.074461, current_train_items 120768.
I0304 19:29:58.683342 22502662377600 run.py:483] Algo bellman_ford step 3774 current loss 0.092196, current_train_items 120800.
I0304 19:29:58.703147 22502662377600 run.py:483] Algo bellman_ford step 3775 current loss 0.006540, current_train_items 120832.
I0304 19:29:58.719793 22502662377600 run.py:483] Algo bellman_ford step 3776 current loss 0.060434, current_train_items 120864.
I0304 19:29:58.742659 22502662377600 run.py:483] Algo bellman_ford step 3777 current loss 0.071144, current_train_items 120896.
I0304 19:29:58.774493 22502662377600 run.py:483] Algo bellman_ford step 3778 current loss 0.080249, current_train_items 120928.
I0304 19:29:58.808156 22502662377600 run.py:483] Algo bellman_ford step 3779 current loss 0.108600, current_train_items 120960.
I0304 19:29:58.827687 22502662377600 run.py:483] Algo bellman_ford step 3780 current loss 0.009704, current_train_items 120992.
I0304 19:29:58.843979 22502662377600 run.py:483] Algo bellman_ford step 3781 current loss 0.045020, current_train_items 121024.
I0304 19:29:58.867608 22502662377600 run.py:483] Algo bellman_ford step 3782 current loss 0.082467, current_train_items 121056.
I0304 19:29:58.897294 22502662377600 run.py:483] Algo bellman_ford step 3783 current loss 0.079613, current_train_items 121088.
I0304 19:29:58.933722 22502662377600 run.py:483] Algo bellman_ford step 3784 current loss 0.120965, current_train_items 121120.
I0304 19:29:58.953580 22502662377600 run.py:483] Algo bellman_ford step 3785 current loss 0.012798, current_train_items 121152.
I0304 19:29:58.969770 22502662377600 run.py:483] Algo bellman_ford step 3786 current loss 0.027300, current_train_items 121184.
I0304 19:29:58.993688 22502662377600 run.py:483] Algo bellman_ford step 3787 current loss 0.142802, current_train_items 121216.
I0304 19:29:59.024699 22502662377600 run.py:483] Algo bellman_ford step 3788 current loss 0.120368, current_train_items 121248.
I0304 19:29:59.058772 22502662377600 run.py:483] Algo bellman_ford step 3789 current loss 0.154100, current_train_items 121280.
I0304 19:29:59.078659 22502662377600 run.py:483] Algo bellman_ford step 3790 current loss 0.011401, current_train_items 121312.
I0304 19:29:59.094982 22502662377600 run.py:483] Algo bellman_ford step 3791 current loss 0.027229, current_train_items 121344.
I0304 19:29:59.118046 22502662377600 run.py:483] Algo bellman_ford step 3792 current loss 0.067009, current_train_items 121376.
I0304 19:29:59.150067 22502662377600 run.py:483] Algo bellman_ford step 3793 current loss 0.162329, current_train_items 121408.
I0304 19:29:59.183192 22502662377600 run.py:483] Algo bellman_ford step 3794 current loss 0.078917, current_train_items 121440.
I0304 19:29:59.203014 22502662377600 run.py:483] Algo bellman_ford step 3795 current loss 0.025814, current_train_items 121472.
I0304 19:29:59.219454 22502662377600 run.py:483] Algo bellman_ford step 3796 current loss 0.050011, current_train_items 121504.
I0304 19:29:59.243558 22502662377600 run.py:483] Algo bellman_ford step 3797 current loss 0.052020, current_train_items 121536.
I0304 19:29:59.275639 22502662377600 run.py:483] Algo bellman_ford step 3798 current loss 0.119536, current_train_items 121568.
I0304 19:29:59.308314 22502662377600 run.py:483] Algo bellman_ford step 3799 current loss 0.143936, current_train_items 121600.
I0304 19:29:59.328280 22502662377600 run.py:483] Algo bellman_ford step 3800 current loss 0.012943, current_train_items 121632.
I0304 19:29:59.336080 22502662377600 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.96484375, 'score': 0.96484375, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0304 19:29:59.336184 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.965, val scores are: bellman_ford: 0.965
I0304 19:29:59.352921 22502662377600 run.py:483] Algo bellman_ford step 3801 current loss 0.036824, current_train_items 121664.
I0304 19:29:59.378022 22502662377600 run.py:483] Algo bellman_ford step 3802 current loss 0.064067, current_train_items 121696.
I0304 19:29:59.409536 22502662377600 run.py:483] Algo bellman_ford step 3803 current loss 0.073445, current_train_items 121728.
I0304 19:29:59.441653 22502662377600 run.py:483] Algo bellman_ford step 3804 current loss 0.094966, current_train_items 121760.
I0304 19:29:59.461656 22502662377600 run.py:483] Algo bellman_ford step 3805 current loss 0.011146, current_train_items 121792.
I0304 19:29:59.477623 22502662377600 run.py:483] Algo bellman_ford step 3806 current loss 0.070834, current_train_items 121824.
I0304 19:29:59.501408 22502662377600 run.py:483] Algo bellman_ford step 3807 current loss 0.093937, current_train_items 121856.
I0304 19:29:59.532728 22502662377600 run.py:483] Algo bellman_ford step 3808 current loss 0.093827, current_train_items 121888.
I0304 19:29:59.568040 22502662377600 run.py:483] Algo bellman_ford step 3809 current loss 0.101106, current_train_items 121920.
I0304 19:29:59.587482 22502662377600 run.py:483] Algo bellman_ford step 3810 current loss 0.050939, current_train_items 121952.
I0304 19:29:59.603980 22502662377600 run.py:483] Algo bellman_ford step 3811 current loss 0.020547, current_train_items 121984.
I0304 19:29:59.627808 22502662377600 run.py:483] Algo bellman_ford step 3812 current loss 0.059198, current_train_items 122016.
I0304 19:29:59.661490 22502662377600 run.py:483] Algo bellman_ford step 3813 current loss 0.094050, current_train_items 122048.
I0304 19:29:59.696942 22502662377600 run.py:483] Algo bellman_ford step 3814 current loss 0.150508, current_train_items 122080.
I0304 19:29:59.717040 22502662377600 run.py:483] Algo bellman_ford step 3815 current loss 0.050372, current_train_items 122112.
I0304 19:29:59.733604 22502662377600 run.py:483] Algo bellman_ford step 3816 current loss 0.086157, current_train_items 122144.
I0304 19:29:59.757647 22502662377600 run.py:483] Algo bellman_ford step 3817 current loss 0.117122, current_train_items 122176.
I0304 19:29:59.789118 22502662377600 run.py:483] Algo bellman_ford step 3818 current loss 0.101101, current_train_items 122208.
I0304 19:29:59.823049 22502662377600 run.py:483] Algo bellman_ford step 3819 current loss 0.135744, current_train_items 122240.
I0304 19:29:59.842585 22502662377600 run.py:483] Algo bellman_ford step 3820 current loss 0.004300, current_train_items 122272.
I0304 19:29:59.858392 22502662377600 run.py:483] Algo bellman_ford step 3821 current loss 0.024833, current_train_items 122304.
I0304 19:29:59.882100 22502662377600 run.py:483] Algo bellman_ford step 3822 current loss 0.073874, current_train_items 122336.
I0304 19:29:59.913559 22502662377600 run.py:483] Algo bellman_ford step 3823 current loss 0.105770, current_train_items 122368.
I0304 19:29:59.947910 22502662377600 run.py:483] Algo bellman_ford step 3824 current loss 0.129824, current_train_items 122400.
I0304 19:29:59.967474 22502662377600 run.py:483] Algo bellman_ford step 3825 current loss 0.049429, current_train_items 122432.
I0304 19:29:59.983331 22502662377600 run.py:483] Algo bellman_ford step 3826 current loss 0.105581, current_train_items 122464.
I0304 19:30:00.007944 22502662377600 run.py:483] Algo bellman_ford step 3827 current loss 0.110842, current_train_items 122496.
I0304 19:30:00.039541 22502662377600 run.py:483] Algo bellman_ford step 3828 current loss 0.129559, current_train_items 122528.
I0304 19:30:00.071853 22502662377600 run.py:483] Algo bellman_ford step 3829 current loss 0.097323, current_train_items 122560.
I0304 19:30:00.091406 22502662377600 run.py:483] Algo bellman_ford step 3830 current loss 0.005506, current_train_items 122592.
I0304 19:30:00.107701 22502662377600 run.py:483] Algo bellman_ford step 3831 current loss 0.043365, current_train_items 122624.
I0304 19:30:00.130597 22502662377600 run.py:483] Algo bellman_ford step 3832 current loss 0.095540, current_train_items 122656.
I0304 19:30:00.162775 22502662377600 run.py:483] Algo bellman_ford step 3833 current loss 0.121140, current_train_items 122688.
I0304 19:30:00.195834 22502662377600 run.py:483] Algo bellman_ford step 3834 current loss 0.091042, current_train_items 122720.
I0304 19:30:00.215317 22502662377600 run.py:483] Algo bellman_ford step 3835 current loss 0.021925, current_train_items 122752.
I0304 19:30:00.230933 22502662377600 run.py:483] Algo bellman_ford step 3836 current loss 0.021319, current_train_items 122784.
I0304 19:30:00.256113 22502662377600 run.py:483] Algo bellman_ford step 3837 current loss 0.265175, current_train_items 122816.
I0304 19:30:00.287578 22502662377600 run.py:483] Algo bellman_ford step 3838 current loss 0.244799, current_train_items 122848.
I0304 19:30:00.320995 22502662377600 run.py:483] Algo bellman_ford step 3839 current loss 0.112874, current_train_items 122880.
I0304 19:30:00.340660 22502662377600 run.py:483] Algo bellman_ford step 3840 current loss 0.024772, current_train_items 122912.
I0304 19:30:00.356439 22502662377600 run.py:483] Algo bellman_ford step 3841 current loss 0.022873, current_train_items 122944.
I0304 19:30:00.379390 22502662377600 run.py:483] Algo bellman_ford step 3842 current loss 0.066617, current_train_items 122976.
I0304 19:30:00.410757 22502662377600 run.py:483] Algo bellman_ford step 3843 current loss 0.087363, current_train_items 123008.
I0304 19:30:00.445708 22502662377600 run.py:483] Algo bellman_ford step 3844 current loss 0.185972, current_train_items 123040.
I0304 19:30:00.465422 22502662377600 run.py:483] Algo bellman_ford step 3845 current loss 0.012692, current_train_items 123072.
I0304 19:30:00.481526 22502662377600 run.py:483] Algo bellman_ford step 3846 current loss 0.066411, current_train_items 123104.
I0304 19:30:00.506962 22502662377600 run.py:483] Algo bellman_ford step 3847 current loss 0.308746, current_train_items 123136.
I0304 19:30:00.537792 22502662377600 run.py:483] Algo bellman_ford step 3848 current loss 0.145479, current_train_items 123168.
I0304 19:30:00.571515 22502662377600 run.py:483] Algo bellman_ford step 3849 current loss 0.110170, current_train_items 123200.
I0304 19:30:00.591555 22502662377600 run.py:483] Algo bellman_ford step 3850 current loss 0.019947, current_train_items 123232.
I0304 19:30:00.599667 22502662377600 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0304 19:30:00.599770 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:30:00.616079 22502662377600 run.py:483] Algo bellman_ford step 3851 current loss 0.018693, current_train_items 123264.
I0304 19:30:00.639207 22502662377600 run.py:483] Algo bellman_ford step 3852 current loss 0.087102, current_train_items 123296.
I0304 19:30:00.671688 22502662377600 run.py:483] Algo bellman_ford step 3853 current loss 0.149322, current_train_items 123328.
I0304 19:30:00.707606 22502662377600 run.py:483] Algo bellman_ford step 3854 current loss 0.125370, current_train_items 123360.
I0304 19:30:00.727413 22502662377600 run.py:483] Algo bellman_ford step 3855 current loss 0.018349, current_train_items 123392.
I0304 19:30:00.743065 22502662377600 run.py:483] Algo bellman_ford step 3856 current loss 0.012918, current_train_items 123424.
I0304 19:30:00.765784 22502662377600 run.py:483] Algo bellman_ford step 3857 current loss 0.105594, current_train_items 123456.
I0304 19:30:00.797438 22502662377600 run.py:483] Algo bellman_ford step 3858 current loss 0.364493, current_train_items 123488.
I0304 19:30:00.833274 22502662377600 run.py:483] Algo bellman_ford step 3859 current loss 0.364440, current_train_items 123520.
I0304 19:30:00.853024 22502662377600 run.py:483] Algo bellman_ford step 3860 current loss 0.038356, current_train_items 123552.
I0304 19:30:00.869496 22502662377600 run.py:483] Algo bellman_ford step 3861 current loss 0.089783, current_train_items 123584.
I0304 19:30:00.892931 22502662377600 run.py:483] Algo bellman_ford step 3862 current loss 0.072420, current_train_items 123616.
I0304 19:30:00.924128 22502662377600 run.py:483] Algo bellman_ford step 3863 current loss 0.101050, current_train_items 123648.
I0304 19:30:00.959871 22502662377600 run.py:483] Algo bellman_ford step 3864 current loss 0.171234, current_train_items 123680.
I0304 19:30:00.979086 22502662377600 run.py:483] Algo bellman_ford step 3865 current loss 0.014385, current_train_items 123712.
I0304 19:30:00.995322 22502662377600 run.py:483] Algo bellman_ford step 3866 current loss 0.041707, current_train_items 123744.
I0304 19:30:01.019010 22502662377600 run.py:483] Algo bellman_ford step 3867 current loss 0.064419, current_train_items 123776.
I0304 19:30:01.049726 22502662377600 run.py:483] Algo bellman_ford step 3868 current loss 0.057193, current_train_items 123808.
I0304 19:30:01.082796 22502662377600 run.py:483] Algo bellman_ford step 3869 current loss 0.105822, current_train_items 123840.
I0304 19:30:01.102831 22502662377600 run.py:483] Algo bellman_ford step 3870 current loss 0.016822, current_train_items 123872.
I0304 19:30:01.118658 22502662377600 run.py:483] Algo bellman_ford step 3871 current loss 0.042072, current_train_items 123904.
I0304 19:30:01.142854 22502662377600 run.py:483] Algo bellman_ford step 3872 current loss 0.058911, current_train_items 123936.
I0304 19:30:01.172578 22502662377600 run.py:483] Algo bellman_ford step 3873 current loss 0.057752, current_train_items 123968.
I0304 19:30:01.205233 22502662377600 run.py:483] Algo bellman_ford step 3874 current loss 0.073353, current_train_items 124000.
I0304 19:30:01.225242 22502662377600 run.py:483] Algo bellman_ford step 3875 current loss 0.010064, current_train_items 124032.
I0304 19:30:01.241786 22502662377600 run.py:483] Algo bellman_ford step 3876 current loss 0.049775, current_train_items 124064.
I0304 19:30:01.266079 22502662377600 run.py:483] Algo bellman_ford step 3877 current loss 0.100900, current_train_items 124096.
I0304 19:30:01.297770 22502662377600 run.py:483] Algo bellman_ford step 3878 current loss 0.083957, current_train_items 124128.
I0304 19:30:01.331125 22502662377600 run.py:483] Algo bellman_ford step 3879 current loss 0.121749, current_train_items 124160.
I0304 19:30:01.350964 22502662377600 run.py:483] Algo bellman_ford step 3880 current loss 0.061665, current_train_items 124192.
I0304 19:30:01.366912 22502662377600 run.py:483] Algo bellman_ford step 3881 current loss 0.028800, current_train_items 124224.
I0304 19:30:01.390863 22502662377600 run.py:483] Algo bellman_ford step 3882 current loss 0.073061, current_train_items 124256.
I0304 19:30:01.421977 22502662377600 run.py:483] Algo bellman_ford step 3883 current loss 0.093001, current_train_items 124288.
I0304 19:30:01.454260 22502662377600 run.py:483] Algo bellman_ford step 3884 current loss 0.158984, current_train_items 124320.
I0304 19:30:01.474304 22502662377600 run.py:483] Algo bellman_ford step 3885 current loss 0.012567, current_train_items 124352.
I0304 19:30:01.490974 22502662377600 run.py:483] Algo bellman_ford step 3886 current loss 0.054348, current_train_items 124384.
I0304 19:30:01.514286 22502662377600 run.py:483] Algo bellman_ford step 3887 current loss 0.128263, current_train_items 124416.
I0304 19:30:01.545271 22502662377600 run.py:483] Algo bellman_ford step 3888 current loss 0.090310, current_train_items 124448.
I0304 19:30:01.580174 22502662377600 run.py:483] Algo bellman_ford step 3889 current loss 0.098747, current_train_items 124480.
I0304 19:30:01.599948 22502662377600 run.py:483] Algo bellman_ford step 3890 current loss 0.010575, current_train_items 124512.
I0304 19:30:01.616246 22502662377600 run.py:483] Algo bellman_ford step 3891 current loss 0.052706, current_train_items 124544.
I0304 19:30:01.640385 22502662377600 run.py:483] Algo bellman_ford step 3892 current loss 0.081013, current_train_items 124576.
I0304 19:30:01.670050 22502662377600 run.py:483] Algo bellman_ford step 3893 current loss 0.044254, current_train_items 124608.
I0304 19:30:01.704332 22502662377600 run.py:483] Algo bellman_ford step 3894 current loss 0.088261, current_train_items 124640.
I0304 19:30:01.723840 22502662377600 run.py:483] Algo bellman_ford step 3895 current loss 0.008044, current_train_items 124672.
I0304 19:30:01.740261 22502662377600 run.py:483] Algo bellman_ford step 3896 current loss 0.041951, current_train_items 124704.
I0304 19:30:01.765059 22502662377600 run.py:483] Algo bellman_ford step 3897 current loss 0.067212, current_train_items 124736.
I0304 19:30:01.796693 22502662377600 run.py:483] Algo bellman_ford step 3898 current loss 0.054778, current_train_items 124768.
I0304 19:30:01.829687 22502662377600 run.py:483] Algo bellman_ford step 3899 current loss 0.144685, current_train_items 124800.
I0304 19:30:01.849832 22502662377600 run.py:483] Algo bellman_ford step 3900 current loss 0.008299, current_train_items 124832.
I0304 19:30:01.857770 22502662377600 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0304 19:30:01.857874 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:30:01.874875 22502662377600 run.py:483] Algo bellman_ford step 3901 current loss 0.043628, current_train_items 124864.
I0304 19:30:01.899115 22502662377600 run.py:483] Algo bellman_ford step 3902 current loss 0.125680, current_train_items 124896.
I0304 19:30:01.931146 22502662377600 run.py:483] Algo bellman_ford step 3903 current loss 0.130745, current_train_items 124928.
I0304 19:30:01.965372 22502662377600 run.py:483] Algo bellman_ford step 3904 current loss 0.100577, current_train_items 124960.
I0304 19:30:01.985382 22502662377600 run.py:483] Algo bellman_ford step 3905 current loss 0.014872, current_train_items 124992.
I0304 19:30:02.001184 22502662377600 run.py:483] Algo bellman_ford step 3906 current loss 0.065601, current_train_items 125024.
I0304 19:30:02.024239 22502662377600 run.py:483] Algo bellman_ford step 3907 current loss 0.064122, current_train_items 125056.
I0304 19:30:02.056077 22502662377600 run.py:483] Algo bellman_ford step 3908 current loss 0.094622, current_train_items 125088.
I0304 19:30:02.090393 22502662377600 run.py:483] Algo bellman_ford step 3909 current loss 0.117664, current_train_items 125120.
I0304 19:30:02.110311 22502662377600 run.py:483] Algo bellman_ford step 3910 current loss 0.011048, current_train_items 125152.
I0304 19:30:02.126242 22502662377600 run.py:483] Algo bellman_ford step 3911 current loss 0.035731, current_train_items 125184.
I0304 19:30:02.150609 22502662377600 run.py:483] Algo bellman_ford step 3912 current loss 0.109680, current_train_items 125216.
I0304 19:30:02.181723 22502662377600 run.py:483] Algo bellman_ford step 3913 current loss 0.097770, current_train_items 125248.
I0304 19:30:02.212683 22502662377600 run.py:483] Algo bellman_ford step 3914 current loss 0.100067, current_train_items 125280.
I0304 19:30:02.232376 22502662377600 run.py:483] Algo bellman_ford step 3915 current loss 0.072494, current_train_items 125312.
I0304 19:30:02.248394 22502662377600 run.py:483] Algo bellman_ford step 3916 current loss 0.035136, current_train_items 125344.
I0304 19:30:02.273746 22502662377600 run.py:483] Algo bellman_ford step 3917 current loss 0.232726, current_train_items 125376.
I0304 19:30:02.304255 22502662377600 run.py:483] Algo bellman_ford step 3918 current loss 0.203755, current_train_items 125408.
I0304 19:30:02.338051 22502662377600 run.py:483] Algo bellman_ford step 3919 current loss 0.192059, current_train_items 125440.
I0304 19:30:02.358019 22502662377600 run.py:483] Algo bellman_ford step 3920 current loss 0.006210, current_train_items 125472.
I0304 19:30:02.374390 22502662377600 run.py:483] Algo bellman_ford step 3921 current loss 0.028319, current_train_items 125504.
I0304 19:30:02.399241 22502662377600 run.py:483] Algo bellman_ford step 3922 current loss 0.076213, current_train_items 125536.
I0304 19:30:02.431307 22502662377600 run.py:483] Algo bellman_ford step 3923 current loss 0.131375, current_train_items 125568.
I0304 19:30:02.467701 22502662377600 run.py:483] Algo bellman_ford step 3924 current loss 0.135352, current_train_items 125600.
I0304 19:30:02.487597 22502662377600 run.py:483] Algo bellman_ford step 3925 current loss 0.008089, current_train_items 125632.
I0304 19:30:02.503845 22502662377600 run.py:483] Algo bellman_ford step 3926 current loss 0.059279, current_train_items 125664.
I0304 19:30:02.528229 22502662377600 run.py:483] Algo bellman_ford step 3927 current loss 0.090684, current_train_items 125696.
I0304 19:30:02.559394 22502662377600 run.py:483] Algo bellman_ford step 3928 current loss 0.098373, current_train_items 125728.
I0304 19:30:02.593053 22502662377600 run.py:483] Algo bellman_ford step 3929 current loss 0.085705, current_train_items 125760.
I0304 19:30:02.612554 22502662377600 run.py:483] Algo bellman_ford step 3930 current loss 0.007826, current_train_items 125792.
I0304 19:30:02.628487 22502662377600 run.py:483] Algo bellman_ford step 3931 current loss 0.039742, current_train_items 125824.
I0304 19:30:02.652494 22502662377600 run.py:483] Algo bellman_ford step 3932 current loss 0.082868, current_train_items 125856.
I0304 19:30:02.685347 22502662377600 run.py:483] Algo bellman_ford step 3933 current loss 0.086250, current_train_items 125888.
I0304 19:30:02.719282 22502662377600 run.py:483] Algo bellman_ford step 3934 current loss 0.093306, current_train_items 125920.
I0304 19:30:02.738938 22502662377600 run.py:483] Algo bellman_ford step 3935 current loss 0.047614, current_train_items 125952.
I0304 19:30:02.755376 22502662377600 run.py:483] Algo bellman_ford step 3936 current loss 0.025080, current_train_items 125984.
I0304 19:30:02.779397 22502662377600 run.py:483] Algo bellman_ford step 3937 current loss 0.058866, current_train_items 126016.
I0304 19:30:02.810969 22502662377600 run.py:483] Algo bellman_ford step 3938 current loss 0.119828, current_train_items 126048.
I0304 19:30:02.846029 22502662377600 run.py:483] Algo bellman_ford step 3939 current loss 0.140525, current_train_items 126080.
I0304 19:30:02.865840 22502662377600 run.py:483] Algo bellman_ford step 3940 current loss 0.005006, current_train_items 126112.
I0304 19:30:02.881880 22502662377600 run.py:483] Algo bellman_ford step 3941 current loss 0.052313, current_train_items 126144.
I0304 19:30:02.906158 22502662377600 run.py:483] Algo bellman_ford step 3942 current loss 0.130472, current_train_items 126176.
I0304 19:30:02.936482 22502662377600 run.py:483] Algo bellman_ford step 3943 current loss 0.041519, current_train_items 126208.
I0304 19:30:02.971215 22502662377600 run.py:483] Algo bellman_ford step 3944 current loss 0.178577, current_train_items 126240.
I0304 19:30:02.991165 22502662377600 run.py:483] Algo bellman_ford step 3945 current loss 0.055324, current_train_items 126272.
I0304 19:30:03.007542 22502662377600 run.py:483] Algo bellman_ford step 3946 current loss 0.029315, current_train_items 126304.
I0304 19:30:03.031152 22502662377600 run.py:483] Algo bellman_ford step 3947 current loss 0.079807, current_train_items 126336.
I0304 19:30:03.061023 22502662377600 run.py:483] Algo bellman_ford step 3948 current loss 0.099358, current_train_items 126368.
I0304 19:30:03.095236 22502662377600 run.py:483] Algo bellman_ford step 3949 current loss 0.082536, current_train_items 126400.
I0304 19:30:03.115245 22502662377600 run.py:483] Algo bellman_ford step 3950 current loss 0.008550, current_train_items 126432.
I0304 19:30:03.123274 22502662377600 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0304 19:30:03.123378 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.986, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:30:03.140392 22502662377600 run.py:483] Algo bellman_ford step 3951 current loss 0.041840, current_train_items 126464.
I0304 19:30:03.164890 22502662377600 run.py:483] Algo bellman_ford step 3952 current loss 0.062816, current_train_items 126496.
I0304 19:30:03.195606 22502662377600 run.py:483] Algo bellman_ford step 3953 current loss 0.075858, current_train_items 126528.
I0304 19:30:03.227848 22502662377600 run.py:483] Algo bellman_ford step 3954 current loss 0.064966, current_train_items 126560.
I0304 19:30:03.247373 22502662377600 run.py:483] Algo bellman_ford step 3955 current loss 0.009657, current_train_items 126592.
I0304 19:30:03.263139 22502662377600 run.py:483] Algo bellman_ford step 3956 current loss 0.067937, current_train_items 126624.
I0304 19:30:03.287499 22502662377600 run.py:483] Algo bellman_ford step 3957 current loss 0.103842, current_train_items 126656.
I0304 19:30:03.319218 22502662377600 run.py:483] Algo bellman_ford step 3958 current loss 0.094912, current_train_items 126688.
I0304 19:30:03.354885 22502662377600 run.py:483] Algo bellman_ford step 3959 current loss 0.102964, current_train_items 126720.
I0304 19:30:03.374763 22502662377600 run.py:483] Algo bellman_ford step 3960 current loss 0.014285, current_train_items 126752.
I0304 19:30:03.391037 22502662377600 run.py:483] Algo bellman_ford step 3961 current loss 0.045010, current_train_items 126784.
I0304 19:30:03.413376 22502662377600 run.py:483] Algo bellman_ford step 3962 current loss 0.042218, current_train_items 126816.
I0304 19:30:03.444911 22502662377600 run.py:483] Algo bellman_ford step 3963 current loss 0.077791, current_train_items 126848.
I0304 19:30:03.478809 22502662377600 run.py:483] Algo bellman_ford step 3964 current loss 0.144049, current_train_items 126880.
I0304 19:30:03.498325 22502662377600 run.py:483] Algo bellman_ford step 3965 current loss 0.009003, current_train_items 126912.
I0304 19:30:03.514420 22502662377600 run.py:483] Algo bellman_ford step 3966 current loss 0.044345, current_train_items 126944.
I0304 19:30:03.539531 22502662377600 run.py:483] Algo bellman_ford step 3967 current loss 0.075770, current_train_items 126976.
I0304 19:30:03.570765 22502662377600 run.py:483] Algo bellman_ford step 3968 current loss 0.094193, current_train_items 127008.
I0304 19:30:03.603619 22502662377600 run.py:483] Algo bellman_ford step 3969 current loss 0.091786, current_train_items 127040.
I0304 19:30:03.623495 22502662377600 run.py:483] Algo bellman_ford step 3970 current loss 0.005941, current_train_items 127072.
I0304 19:30:03.639643 22502662377600 run.py:483] Algo bellman_ford step 3971 current loss 0.021359, current_train_items 127104.
I0304 19:30:03.663812 22502662377600 run.py:483] Algo bellman_ford step 3972 current loss 0.057603, current_train_items 127136.
I0304 19:30:03.695775 22502662377600 run.py:483] Algo bellman_ford step 3973 current loss 0.110868, current_train_items 127168.
I0304 19:30:03.727790 22502662377600 run.py:483] Algo bellman_ford step 3974 current loss 0.121278, current_train_items 127200.
I0304 19:30:03.747833 22502662377600 run.py:483] Algo bellman_ford step 3975 current loss 0.004413, current_train_items 127232.
I0304 19:30:03.764329 22502662377600 run.py:483] Algo bellman_ford step 3976 current loss 0.048445, current_train_items 127264.
I0304 19:30:03.787608 22502662377600 run.py:483] Algo bellman_ford step 3977 current loss 0.050517, current_train_items 127296.
I0304 19:30:03.819472 22502662377600 run.py:483] Algo bellman_ford step 3978 current loss 0.098372, current_train_items 127328.
I0304 19:30:03.852727 22502662377600 run.py:483] Algo bellman_ford step 3979 current loss 0.070578, current_train_items 127360.
I0304 19:30:03.872145 22502662377600 run.py:483] Algo bellman_ford step 3980 current loss 0.003756, current_train_items 127392.
I0304 19:30:03.888326 22502662377600 run.py:483] Algo bellman_ford step 3981 current loss 0.042171, current_train_items 127424.
I0304 19:30:03.911612 22502662377600 run.py:483] Algo bellman_ford step 3982 current loss 0.092514, current_train_items 127456.
I0304 19:30:03.943137 22502662377600 run.py:483] Algo bellman_ford step 3983 current loss 0.155236, current_train_items 127488.
I0304 19:30:03.975570 22502662377600 run.py:483] Algo bellman_ford step 3984 current loss 0.128518, current_train_items 127520.
I0304 19:30:03.995368 22502662377600 run.py:483] Algo bellman_ford step 3985 current loss 0.025417, current_train_items 127552.
I0304 19:30:04.011749 22502662377600 run.py:483] Algo bellman_ford step 3986 current loss 0.054266, current_train_items 127584.
I0304 19:30:04.034998 22502662377600 run.py:483] Algo bellman_ford step 3987 current loss 0.116846, current_train_items 127616.
I0304 19:30:04.066515 22502662377600 run.py:483] Algo bellman_ford step 3988 current loss 0.126526, current_train_items 127648.
I0304 19:30:04.100537 22502662377600 run.py:483] Algo bellman_ford step 3989 current loss 0.142088, current_train_items 127680.
I0304 19:30:04.120543 22502662377600 run.py:483] Algo bellman_ford step 3990 current loss 0.027097, current_train_items 127712.
I0304 19:30:04.137074 22502662377600 run.py:483] Algo bellman_ford step 3991 current loss 0.046850, current_train_items 127744.
I0304 19:30:04.159664 22502662377600 run.py:483] Algo bellman_ford step 3992 current loss 0.052089, current_train_items 127776.
I0304 19:30:04.190515 22502662377600 run.py:483] Algo bellman_ford step 3993 current loss 0.114950, current_train_items 127808.
I0304 19:30:04.224370 22502662377600 run.py:483] Algo bellman_ford step 3994 current loss 0.171346, current_train_items 127840.
I0304 19:30:04.243839 22502662377600 run.py:483] Algo bellman_ford step 3995 current loss 0.030134, current_train_items 127872.
I0304 19:30:04.260119 22502662377600 run.py:483] Algo bellman_ford step 3996 current loss 0.031602, current_train_items 127904.
I0304 19:30:04.284115 22502662377600 run.py:483] Algo bellman_ford step 3997 current loss 0.058907, current_train_items 127936.
I0304 19:30:04.314734 22502662377600 run.py:483] Algo bellman_ford step 3998 current loss 0.064401, current_train_items 127968.
I0304 19:30:04.349348 22502662377600 run.py:483] Algo bellman_ford step 3999 current loss 0.094898, current_train_items 128000.
I0304 19:30:04.369255 22502662377600 run.py:483] Algo bellman_ford step 4000 current loss 0.011351, current_train_items 128032.
I0304 19:30:04.377325 22502662377600 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0304 19:30:04.377429 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.986, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:30:04.406893 22502662377600 run.py:483] Algo bellman_ford step 4001 current loss 0.026333, current_train_items 128064.
I0304 19:30:04.431176 22502662377600 run.py:483] Algo bellman_ford step 4002 current loss 0.066771, current_train_items 128096.
I0304 19:30:04.462455 22502662377600 run.py:483] Algo bellman_ford step 4003 current loss 0.062581, current_train_items 128128.
I0304 19:30:04.495864 22502662377600 run.py:483] Algo bellman_ford step 4004 current loss 0.112473, current_train_items 128160.
I0304 19:30:04.516083 22502662377600 run.py:483] Algo bellman_ford step 4005 current loss 0.006082, current_train_items 128192.
I0304 19:30:04.532133 22502662377600 run.py:483] Algo bellman_ford step 4006 current loss 0.060677, current_train_items 128224.
I0304 19:30:04.556109 22502662377600 run.py:483] Algo bellman_ford step 4007 current loss 0.112419, current_train_items 128256.
I0304 19:30:04.588146 22502662377600 run.py:483] Algo bellman_ford step 4008 current loss 0.120845, current_train_items 128288.
I0304 19:30:04.620380 22502662377600 run.py:483] Algo bellman_ford step 4009 current loss 0.097824, current_train_items 128320.
I0304 19:30:04.640382 22502662377600 run.py:483] Algo bellman_ford step 4010 current loss 0.008357, current_train_items 128352.
I0304 19:30:04.656555 22502662377600 run.py:483] Algo bellman_ford step 4011 current loss 0.062717, current_train_items 128384.
I0304 19:30:04.679471 22502662377600 run.py:483] Algo bellman_ford step 4012 current loss 0.055890, current_train_items 128416.
I0304 19:30:04.711095 22502662377600 run.py:483] Algo bellman_ford step 4013 current loss 0.132135, current_train_items 128448.
I0304 19:30:04.745007 22502662377600 run.py:483] Algo bellman_ford step 4014 current loss 0.172374, current_train_items 128480.
I0304 19:30:04.764699 22502662377600 run.py:483] Algo bellman_ford step 4015 current loss 0.007546, current_train_items 128512.
I0304 19:30:04.780947 22502662377600 run.py:483] Algo bellman_ford step 4016 current loss 0.095470, current_train_items 128544.
I0304 19:30:04.804784 22502662377600 run.py:483] Algo bellman_ford step 4017 current loss 0.099831, current_train_items 128576.
I0304 19:30:04.836012 22502662377600 run.py:483] Algo bellman_ford step 4018 current loss 0.178331, current_train_items 128608.
I0304 19:30:04.868856 22502662377600 run.py:483] Algo bellman_ford step 4019 current loss 0.114617, current_train_items 128640.
I0304 19:30:04.888407 22502662377600 run.py:483] Algo bellman_ford step 4020 current loss 0.006433, current_train_items 128672.
I0304 19:30:04.904443 22502662377600 run.py:483] Algo bellman_ford step 4021 current loss 0.080290, current_train_items 128704.
I0304 19:30:04.929204 22502662377600 run.py:483] Algo bellman_ford step 4022 current loss 0.175047, current_train_items 128736.
I0304 19:30:04.959542 22502662377600 run.py:483] Algo bellman_ford step 4023 current loss 0.141774, current_train_items 128768.
I0304 19:30:04.995653 22502662377600 run.py:483] Algo bellman_ford step 4024 current loss 0.213170, current_train_items 128800.
I0304 19:30:05.015253 22502662377600 run.py:483] Algo bellman_ford step 4025 current loss 0.007542, current_train_items 128832.
I0304 19:30:05.031644 22502662377600 run.py:483] Algo bellman_ford step 4026 current loss 0.040904, current_train_items 128864.
I0304 19:30:05.055190 22502662377600 run.py:483] Algo bellman_ford step 4027 current loss 0.076795, current_train_items 128896.
I0304 19:30:05.085448 22502662377600 run.py:483] Algo bellman_ford step 4028 current loss 0.115612, current_train_items 128928.
I0304 19:30:05.118255 22502662377600 run.py:483] Algo bellman_ford step 4029 current loss 0.108810, current_train_items 128960.
I0304 19:30:05.137618 22502662377600 run.py:483] Algo bellman_ford step 4030 current loss 0.004862, current_train_items 128992.
I0304 19:30:05.153668 22502662377600 run.py:483] Algo bellman_ford step 4031 current loss 0.031252, current_train_items 129024.
I0304 19:30:05.177591 22502662377600 run.py:483] Algo bellman_ford step 4032 current loss 0.086242, current_train_items 129056.
I0304 19:30:05.209662 22502662377600 run.py:483] Algo bellman_ford step 4033 current loss 0.123162, current_train_items 129088.
I0304 19:30:05.242754 22502662377600 run.py:483] Algo bellman_ford step 4034 current loss 0.133018, current_train_items 129120.
I0304 19:30:05.262320 22502662377600 run.py:483] Algo bellman_ford step 4035 current loss 0.004645, current_train_items 129152.
I0304 19:30:05.278311 22502662377600 run.py:483] Algo bellman_ford step 4036 current loss 0.048198, current_train_items 129184.
I0304 19:30:05.302130 22502662377600 run.py:483] Algo bellman_ford step 4037 current loss 0.051447, current_train_items 129216.
I0304 19:30:05.333904 22502662377600 run.py:483] Algo bellman_ford step 4038 current loss 0.102216, current_train_items 129248.
I0304 19:30:05.367834 22502662377600 run.py:483] Algo bellman_ford step 4039 current loss 0.094005, current_train_items 129280.
I0304 19:30:05.387385 22502662377600 run.py:483] Algo bellman_ford step 4040 current loss 0.005710, current_train_items 129312.
I0304 19:30:05.404028 22502662377600 run.py:483] Algo bellman_ford step 4041 current loss 0.032269, current_train_items 129344.
I0304 19:30:05.427328 22502662377600 run.py:483] Algo bellman_ford step 4042 current loss 0.085745, current_train_items 129376.
I0304 19:30:05.459656 22502662377600 run.py:483] Algo bellman_ford step 4043 current loss 0.109768, current_train_items 129408.
I0304 19:30:05.492033 22502662377600 run.py:483] Algo bellman_ford step 4044 current loss 0.091103, current_train_items 129440.
I0304 19:30:05.511533 22502662377600 run.py:483] Algo bellman_ford step 4045 current loss 0.009653, current_train_items 129472.
I0304 19:30:05.528037 22502662377600 run.py:483] Algo bellman_ford step 4046 current loss 0.036778, current_train_items 129504.
I0304 19:30:05.551545 22502662377600 run.py:483] Algo bellman_ford step 4047 current loss 0.087051, current_train_items 129536.
I0304 19:30:05.581732 22502662377600 run.py:483] Algo bellman_ford step 4048 current loss 0.060139, current_train_items 129568.
I0304 19:30:05.616342 22502662377600 run.py:483] Algo bellman_ford step 4049 current loss 0.086060, current_train_items 129600.
I0304 19:30:05.635756 22502662377600 run.py:483] Algo bellman_ford step 4050 current loss 0.016224, current_train_items 129632.
I0304 19:30:05.643750 22502662377600 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0304 19:30:05.643856 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:05.660840 22502662377600 run.py:483] Algo bellman_ford step 4051 current loss 0.020698, current_train_items 129664.
I0304 19:30:05.684357 22502662377600 run.py:483] Algo bellman_ford step 4052 current loss 0.056718, current_train_items 129696.
I0304 19:30:05.715554 22502662377600 run.py:483] Algo bellman_ford step 4053 current loss 0.071629, current_train_items 129728.
I0304 19:30:05.751437 22502662377600 run.py:483] Algo bellman_ford step 4054 current loss 0.139519, current_train_items 129760.
I0304 19:30:05.771333 22502662377600 run.py:483] Algo bellman_ford step 4055 current loss 0.019841, current_train_items 129792.
I0304 19:30:05.787022 22502662377600 run.py:483] Algo bellman_ford step 4056 current loss 0.041995, current_train_items 129824.
I0304 19:30:05.810663 22502662377600 run.py:483] Algo bellman_ford step 4057 current loss 0.066105, current_train_items 129856.
I0304 19:30:05.840868 22502662377600 run.py:483] Algo bellman_ford step 4058 current loss 0.079144, current_train_items 129888.
I0304 19:30:05.873105 22502662377600 run.py:483] Algo bellman_ford step 4059 current loss 0.102485, current_train_items 129920.
I0304 19:30:05.892966 22502662377600 run.py:483] Algo bellman_ford step 4060 current loss 0.016320, current_train_items 129952.
I0304 19:30:05.909472 22502662377600 run.py:483] Algo bellman_ford step 4061 current loss 0.032081, current_train_items 129984.
I0304 19:30:05.931985 22502662377600 run.py:483] Algo bellman_ford step 4062 current loss 0.078147, current_train_items 130016.
I0304 19:30:05.962039 22502662377600 run.py:483] Algo bellman_ford step 4063 current loss 0.079031, current_train_items 130048.
I0304 19:30:05.997131 22502662377600 run.py:483] Algo bellman_ford step 4064 current loss 0.171389, current_train_items 130080.
I0304 19:30:06.016546 22502662377600 run.py:483] Algo bellman_ford step 4065 current loss 0.009888, current_train_items 130112.
I0304 19:30:06.032262 22502662377600 run.py:483] Algo bellman_ford step 4066 current loss 0.210613, current_train_items 130144.
I0304 19:30:06.057118 22502662377600 run.py:483] Algo bellman_ford step 4067 current loss 0.184286, current_train_items 130176.
I0304 19:30:06.087873 22502662377600 run.py:483] Algo bellman_ford step 4068 current loss 0.123553, current_train_items 130208.
I0304 19:30:06.122545 22502662377600 run.py:483] Algo bellman_ford step 4069 current loss 0.135072, current_train_items 130240.
I0304 19:30:06.142305 22502662377600 run.py:483] Algo bellman_ford step 4070 current loss 0.008371, current_train_items 130272.
I0304 19:30:06.158696 22502662377600 run.py:483] Algo bellman_ford step 4071 current loss 0.086710, current_train_items 130304.
I0304 19:30:06.181935 22502662377600 run.py:483] Algo bellman_ford step 4072 current loss 0.225341, current_train_items 130336.
I0304 19:30:06.211623 22502662377600 run.py:483] Algo bellman_ford step 4073 current loss 0.164030, current_train_items 130368.
I0304 19:30:06.244384 22502662377600 run.py:483] Algo bellman_ford step 4074 current loss 0.175267, current_train_items 130400.
I0304 19:30:06.264291 22502662377600 run.py:483] Algo bellman_ford step 4075 current loss 0.015766, current_train_items 130432.
I0304 19:30:06.281007 22502662377600 run.py:483] Algo bellman_ford step 4076 current loss 0.059116, current_train_items 130464.
I0304 19:30:06.305116 22502662377600 run.py:483] Algo bellman_ford step 4077 current loss 0.123056, current_train_items 130496.
I0304 19:30:06.335439 22502662377600 run.py:483] Algo bellman_ford step 4078 current loss 0.087911, current_train_items 130528.
I0304 19:30:06.368263 22502662377600 run.py:483] Algo bellman_ford step 4079 current loss 0.124301, current_train_items 130560.
I0304 19:30:06.387558 22502662377600 run.py:483] Algo bellman_ford step 4080 current loss 0.006325, current_train_items 130592.
I0304 19:30:06.403984 22502662377600 run.py:483] Algo bellman_ford step 4081 current loss 0.026648, current_train_items 130624.
I0304 19:30:06.428420 22502662377600 run.py:483] Algo bellman_ford step 4082 current loss 0.112655, current_train_items 130656.
I0304 19:30:06.461129 22502662377600 run.py:483] Algo bellman_ford step 4083 current loss 0.117864, current_train_items 130688.
I0304 19:30:06.492443 22502662377600 run.py:483] Algo bellman_ford step 4084 current loss 0.072540, current_train_items 130720.
I0304 19:30:06.512528 22502662377600 run.py:483] Algo bellman_ford step 4085 current loss 0.012233, current_train_items 130752.
I0304 19:30:06.528883 22502662377600 run.py:483] Algo bellman_ford step 4086 current loss 0.034334, current_train_items 130784.
I0304 19:30:06.551848 22502662377600 run.py:483] Algo bellman_ford step 4087 current loss 0.076423, current_train_items 130816.
I0304 19:30:06.582646 22502662377600 run.py:483] Algo bellman_ford step 4088 current loss 0.070322, current_train_items 130848.
I0304 19:30:06.614497 22502662377600 run.py:483] Algo bellman_ford step 4089 current loss 0.068887, current_train_items 130880.
I0304 19:30:06.634107 22502662377600 run.py:483] Algo bellman_ford step 4090 current loss 0.007502, current_train_items 130912.
I0304 19:30:06.650294 22502662377600 run.py:483] Algo bellman_ford step 4091 current loss 0.023152, current_train_items 130944.
I0304 19:30:06.674341 22502662377600 run.py:483] Algo bellman_ford step 4092 current loss 0.047918, current_train_items 130976.
I0304 19:30:06.705920 22502662377600 run.py:483] Algo bellman_ford step 4093 current loss 0.084474, current_train_items 131008.
I0304 19:30:06.739580 22502662377600 run.py:483] Algo bellman_ford step 4094 current loss 0.087444, current_train_items 131040.
I0304 19:30:06.758997 22502662377600 run.py:483] Algo bellman_ford step 4095 current loss 0.007854, current_train_items 131072.
I0304 19:30:06.775519 22502662377600 run.py:483] Algo bellman_ford step 4096 current loss 0.040839, current_train_items 131104.
I0304 19:30:06.799753 22502662377600 run.py:483] Algo bellman_ford step 4097 current loss 0.093227, current_train_items 131136.
I0304 19:30:06.830913 22502662377600 run.py:483] Algo bellman_ford step 4098 current loss 0.065327, current_train_items 131168.
I0304 19:30:06.865295 22502662377600 run.py:483] Algo bellman_ford step 4099 current loss 0.130440, current_train_items 131200.
I0304 19:30:06.885428 22502662377600 run.py:483] Algo bellman_ford step 4100 current loss 0.007358, current_train_items 131232.
I0304 19:30:06.893328 22502662377600 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0304 19:30:06.893433 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:30:06.909728 22502662377600 run.py:483] Algo bellman_ford step 4101 current loss 0.010887, current_train_items 131264.
I0304 19:30:06.932544 22502662377600 run.py:483] Algo bellman_ford step 4102 current loss 0.055203, current_train_items 131296.
I0304 19:30:06.965156 22502662377600 run.py:483] Algo bellman_ford step 4103 current loss 0.096364, current_train_items 131328.
I0304 19:30:07.000794 22502662377600 run.py:483] Algo bellman_ford step 4104 current loss 0.112619, current_train_items 131360.
I0304 19:30:07.020614 22502662377600 run.py:483] Algo bellman_ford step 4105 current loss 0.010367, current_train_items 131392.
I0304 19:30:07.036729 22502662377600 run.py:483] Algo bellman_ford step 4106 current loss 0.026878, current_train_items 131424.
I0304 19:30:07.061234 22502662377600 run.py:483] Algo bellman_ford step 4107 current loss 0.050005, current_train_items 131456.
I0304 19:30:07.092572 22502662377600 run.py:483] Algo bellman_ford step 4108 current loss 0.110374, current_train_items 131488.
I0304 19:30:07.128676 22502662377600 run.py:483] Algo bellman_ford step 4109 current loss 0.176177, current_train_items 131520.
I0304 19:30:07.148238 22502662377600 run.py:483] Algo bellman_ford step 4110 current loss 0.020261, current_train_items 131552.
I0304 19:30:07.164888 22502662377600 run.py:483] Algo bellman_ford step 4111 current loss 0.040747, current_train_items 131584.
I0304 19:30:07.188157 22502662377600 run.py:483] Algo bellman_ford step 4112 current loss 0.100304, current_train_items 131616.
I0304 19:30:07.220356 22502662377600 run.py:483] Algo bellman_ford step 4113 current loss 0.126387, current_train_items 131648.
I0304 19:30:07.255187 22502662377600 run.py:483] Algo bellman_ford step 4114 current loss 0.087651, current_train_items 131680.
I0304 19:30:07.274977 22502662377600 run.py:483] Algo bellman_ford step 4115 current loss 0.006459, current_train_items 131712.
I0304 19:30:07.291468 22502662377600 run.py:483] Algo bellman_ford step 4116 current loss 0.038217, current_train_items 131744.
I0304 19:30:07.316534 22502662377600 run.py:483] Algo bellman_ford step 4117 current loss 0.103469, current_train_items 131776.
I0304 19:30:07.347432 22502662377600 run.py:483] Algo bellman_ford step 4118 current loss 0.052995, current_train_items 131808.
I0304 19:30:07.381615 22502662377600 run.py:483] Algo bellman_ford step 4119 current loss 0.102718, current_train_items 131840.
I0304 19:30:07.401324 22502662377600 run.py:483] Algo bellman_ford step 4120 current loss 0.027247, current_train_items 131872.
I0304 19:30:07.417573 22502662377600 run.py:483] Algo bellman_ford step 4121 current loss 0.018261, current_train_items 131904.
I0304 19:30:07.442646 22502662377600 run.py:483] Algo bellman_ford step 4122 current loss 0.086522, current_train_items 131936.
I0304 19:30:07.474935 22502662377600 run.py:483] Algo bellman_ford step 4123 current loss 0.120141, current_train_items 131968.
I0304 19:30:07.508917 22502662377600 run.py:483] Algo bellman_ford step 4124 current loss 0.119334, current_train_items 132000.
I0304 19:30:07.528585 22502662377600 run.py:483] Algo bellman_ford step 4125 current loss 0.006956, current_train_items 132032.
I0304 19:30:07.544789 22502662377600 run.py:483] Algo bellman_ford step 4126 current loss 0.062389, current_train_items 132064.
I0304 19:30:07.569758 22502662377600 run.py:483] Algo bellman_ford step 4127 current loss 0.116877, current_train_items 132096.
I0304 19:30:07.601033 22502662377600 run.py:483] Algo bellman_ford step 4128 current loss 0.107980, current_train_items 132128.
I0304 19:30:07.633644 22502662377600 run.py:483] Algo bellman_ford step 4129 current loss 0.222179, current_train_items 132160.
I0304 19:30:07.653443 22502662377600 run.py:483] Algo bellman_ford step 4130 current loss 0.007423, current_train_items 132192.
I0304 19:30:07.669918 22502662377600 run.py:483] Algo bellman_ford step 4131 current loss 0.029436, current_train_items 132224.
I0304 19:30:07.693967 22502662377600 run.py:483] Algo bellman_ford step 4132 current loss 0.091698, current_train_items 132256.
I0304 19:30:07.726423 22502662377600 run.py:483] Algo bellman_ford step 4133 current loss 0.091916, current_train_items 132288.
I0304 19:30:07.761690 22502662377600 run.py:483] Algo bellman_ford step 4134 current loss 0.075957, current_train_items 132320.
I0304 19:30:07.781522 22502662377600 run.py:483] Algo bellman_ford step 4135 current loss 0.012439, current_train_items 132352.
I0304 19:30:07.797647 22502662377600 run.py:483] Algo bellman_ford step 4136 current loss 0.052627, current_train_items 132384.
I0304 19:30:07.821069 22502662377600 run.py:483] Algo bellman_ford step 4137 current loss 0.036982, current_train_items 132416.
I0304 19:30:07.852275 22502662377600 run.py:483] Algo bellman_ford step 4138 current loss 0.079334, current_train_items 132448.
I0304 19:30:07.886914 22502662377600 run.py:483] Algo bellman_ford step 4139 current loss 0.134784, current_train_items 132480.
I0304 19:30:07.906451 22502662377600 run.py:483] Algo bellman_ford step 4140 current loss 0.011844, current_train_items 132512.
I0304 19:30:07.922280 22502662377600 run.py:483] Algo bellman_ford step 4141 current loss 0.022674, current_train_items 132544.
I0304 19:30:07.947539 22502662377600 run.py:483] Algo bellman_ford step 4142 current loss 0.114881, current_train_items 132576.
I0304 19:30:07.978390 22502662377600 run.py:483] Algo bellman_ford step 4143 current loss 0.106328, current_train_items 132608.
I0304 19:30:08.011844 22502662377600 run.py:483] Algo bellman_ford step 4144 current loss 0.125164, current_train_items 132640.
I0304 19:30:08.031755 22502662377600 run.py:483] Algo bellman_ford step 4145 current loss 0.009015, current_train_items 132672.
I0304 19:30:08.047656 22502662377600 run.py:483] Algo bellman_ford step 4146 current loss 0.027410, current_train_items 132704.
I0304 19:30:08.071514 22502662377600 run.py:483] Algo bellman_ford step 4147 current loss 0.120505, current_train_items 132736.
I0304 19:30:08.101618 22502662377600 run.py:483] Algo bellman_ford step 4148 current loss 0.041279, current_train_items 132768.
I0304 19:30:08.134435 22502662377600 run.py:483] Algo bellman_ford step 4149 current loss 0.121213, current_train_items 132800.
I0304 19:30:08.154157 22502662377600 run.py:483] Algo bellman_ford step 4150 current loss 0.008558, current_train_items 132832.
I0304 19:30:08.162029 22502662377600 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0304 19:30:08.162134 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:30:08.178880 22502662377600 run.py:483] Algo bellman_ford step 4151 current loss 0.054519, current_train_items 132864.
I0304 19:30:08.203642 22502662377600 run.py:483] Algo bellman_ford step 4152 current loss 0.167522, current_train_items 132896.
I0304 19:30:08.235299 22502662377600 run.py:483] Algo bellman_ford step 4153 current loss 0.137289, current_train_items 132928.
I0304 19:30:08.270379 22502662377600 run.py:483] Algo bellman_ford step 4154 current loss 0.213721, current_train_items 132960.
I0304 19:30:08.290401 22502662377600 run.py:483] Algo bellman_ford step 4155 current loss 0.031142, current_train_items 132992.
I0304 19:30:08.306638 22502662377600 run.py:483] Algo bellman_ford step 4156 current loss 0.066755, current_train_items 133024.
I0304 19:30:08.329621 22502662377600 run.py:483] Algo bellman_ford step 4157 current loss 0.076585, current_train_items 133056.
I0304 19:30:08.360336 22502662377600 run.py:483] Algo bellman_ford step 4158 current loss 0.094815, current_train_items 133088.
I0304 19:30:08.394271 22502662377600 run.py:483] Algo bellman_ford step 4159 current loss 0.126850, current_train_items 133120.
I0304 19:30:08.414331 22502662377600 run.py:483] Algo bellman_ford step 4160 current loss 0.008235, current_train_items 133152.
I0304 19:30:08.430695 22502662377600 run.py:483] Algo bellman_ford step 4161 current loss 0.058922, current_train_items 133184.
I0304 19:30:08.452918 22502662377600 run.py:483] Algo bellman_ford step 4162 current loss 0.058043, current_train_items 133216.
I0304 19:30:08.484441 22502662377600 run.py:483] Algo bellman_ford step 4163 current loss 0.108709, current_train_items 133248.
I0304 19:30:08.518414 22502662377600 run.py:483] Algo bellman_ford step 4164 current loss 0.139187, current_train_items 133280.
I0304 19:30:08.538067 22502662377600 run.py:483] Algo bellman_ford step 4165 current loss 0.011254, current_train_items 133312.
I0304 19:30:08.554158 22502662377600 run.py:483] Algo bellman_ford step 4166 current loss 0.024296, current_train_items 133344.
I0304 19:30:08.579049 22502662377600 run.py:483] Algo bellman_ford step 4167 current loss 0.214526, current_train_items 133376.
I0304 19:30:08.609958 22502662377600 run.py:483] Algo bellman_ford step 4168 current loss 0.106043, current_train_items 133408.
I0304 19:30:08.642425 22502662377600 run.py:483] Algo bellman_ford step 4169 current loss 0.142082, current_train_items 133440.
I0304 19:30:08.662233 22502662377600 run.py:483] Algo bellman_ford step 4170 current loss 0.003228, current_train_items 133472.
I0304 19:30:08.678115 22502662377600 run.py:483] Algo bellman_ford step 4171 current loss 0.024975, current_train_items 133504.
I0304 19:30:08.701412 22502662377600 run.py:483] Algo bellman_ford step 4172 current loss 0.037541, current_train_items 133536.
I0304 19:30:08.731901 22502662377600 run.py:483] Algo bellman_ford step 4173 current loss 0.075805, current_train_items 133568.
I0304 19:30:08.767335 22502662377600 run.py:483] Algo bellman_ford step 4174 current loss 0.114140, current_train_items 133600.
I0304 19:30:08.787025 22502662377600 run.py:483] Algo bellman_ford step 4175 current loss 0.004524, current_train_items 133632.
I0304 19:30:08.803840 22502662377600 run.py:483] Algo bellman_ford step 4176 current loss 0.026861, current_train_items 133664.
I0304 19:30:08.827649 22502662377600 run.py:483] Algo bellman_ford step 4177 current loss 0.062553, current_train_items 133696.
I0304 19:30:08.858917 22502662377600 run.py:483] Algo bellman_ford step 4178 current loss 0.110018, current_train_items 133728.
I0304 19:30:08.893156 22502662377600 run.py:483] Algo bellman_ford step 4179 current loss 0.118358, current_train_items 133760.
I0304 19:30:08.912676 22502662377600 run.py:483] Algo bellman_ford step 4180 current loss 0.006312, current_train_items 133792.
I0304 19:30:08.929242 22502662377600 run.py:483] Algo bellman_ford step 4181 current loss 0.042248, current_train_items 133824.
I0304 19:30:08.953936 22502662377600 run.py:483] Algo bellman_ford step 4182 current loss 0.106750, current_train_items 133856.
I0304 19:30:08.983592 22502662377600 run.py:483] Algo bellman_ford step 4183 current loss 0.097880, current_train_items 133888.
I0304 19:30:09.017664 22502662377600 run.py:483] Algo bellman_ford step 4184 current loss 0.127032, current_train_items 133920.
I0304 19:30:09.037494 22502662377600 run.py:483] Algo bellman_ford step 4185 current loss 0.012394, current_train_items 133952.
I0304 19:30:09.053772 22502662377600 run.py:483] Algo bellman_ford step 4186 current loss 0.043148, current_train_items 133984.
I0304 19:30:09.077215 22502662377600 run.py:483] Algo bellman_ford step 4187 current loss 0.071544, current_train_items 134016.
I0304 19:30:09.107960 22502662377600 run.py:483] Algo bellman_ford step 4188 current loss 0.099435, current_train_items 134048.
I0304 19:30:09.140852 22502662377600 run.py:483] Algo bellman_ford step 4189 current loss 0.108238, current_train_items 134080.
I0304 19:30:09.160870 22502662377600 run.py:483] Algo bellman_ford step 4190 current loss 0.014416, current_train_items 134112.
I0304 19:30:09.177546 22502662377600 run.py:483] Algo bellman_ford step 4191 current loss 0.026348, current_train_items 134144.
I0304 19:30:09.198804 22502662377600 run.py:483] Algo bellman_ford step 4192 current loss 0.029447, current_train_items 134176.
I0304 19:30:09.230484 22502662377600 run.py:483] Algo bellman_ford step 4193 current loss 0.073784, current_train_items 134208.
I0304 19:30:09.266372 22502662377600 run.py:483] Algo bellman_ford step 4194 current loss 0.203675, current_train_items 134240.
I0304 19:30:09.285806 22502662377600 run.py:483] Algo bellman_ford step 4195 current loss 0.041661, current_train_items 134272.
I0304 19:30:09.301899 22502662377600 run.py:483] Algo bellman_ford step 4196 current loss 0.059794, current_train_items 134304.
I0304 19:30:09.325997 22502662377600 run.py:483] Algo bellman_ford step 4197 current loss 0.063210, current_train_items 134336.
I0304 19:30:09.356643 22502662377600 run.py:483] Algo bellman_ford step 4198 current loss 0.141315, current_train_items 134368.
I0304 19:30:09.390184 22502662377600 run.py:483] Algo bellman_ford step 4199 current loss 0.205918, current_train_items 134400.
I0304 19:30:09.409760 22502662377600 run.py:483] Algo bellman_ford step 4200 current loss 0.009311, current_train_items 134432.
I0304 19:30:09.417266 22502662377600 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0304 19:30:09.417370 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:30:09.434369 22502662377600 run.py:483] Algo bellman_ford step 4201 current loss 0.065497, current_train_items 134464.
I0304 19:30:09.457864 22502662377600 run.py:483] Algo bellman_ford step 4202 current loss 0.109389, current_train_items 134496.
I0304 19:30:09.489259 22502662377600 run.py:483] Algo bellman_ford step 4203 current loss 0.108109, current_train_items 134528.
I0304 19:30:09.522834 22502662377600 run.py:483] Algo bellman_ford step 4204 current loss 0.134381, current_train_items 134560.
I0304 19:30:09.542562 22502662377600 run.py:483] Algo bellman_ford step 4205 current loss 0.015911, current_train_items 134592.
I0304 19:30:09.558580 22502662377600 run.py:483] Algo bellman_ford step 4206 current loss 0.055351, current_train_items 134624.
I0304 19:30:09.582634 22502662377600 run.py:483] Algo bellman_ford step 4207 current loss 0.052042, current_train_items 134656.
I0304 19:30:09.614751 22502662377600 run.py:483] Algo bellman_ford step 4208 current loss 0.105799, current_train_items 134688.
I0304 19:30:09.649575 22502662377600 run.py:483] Algo bellman_ford step 4209 current loss 0.154716, current_train_items 134720.
I0304 19:30:09.669041 22502662377600 run.py:483] Algo bellman_ford step 4210 current loss 0.018810, current_train_items 134752.
I0304 19:30:09.685153 22502662377600 run.py:483] Algo bellman_ford step 4211 current loss 0.079140, current_train_items 134784.
I0304 19:30:09.709197 22502662377600 run.py:483] Algo bellman_ford step 4212 current loss 0.055129, current_train_items 134816.
I0304 19:30:09.740621 22502662377600 run.py:483] Algo bellman_ford step 4213 current loss 0.103410, current_train_items 134848.
I0304 19:30:09.774028 22502662377600 run.py:483] Algo bellman_ford step 4214 current loss 0.134060, current_train_items 134880.
I0304 19:30:09.793495 22502662377600 run.py:483] Algo bellman_ford step 4215 current loss 0.017486, current_train_items 134912.
I0304 19:30:09.809735 22502662377600 run.py:483] Algo bellman_ford step 4216 current loss 0.033712, current_train_items 134944.
I0304 19:30:09.833157 22502662377600 run.py:483] Algo bellman_ford step 4217 current loss 0.042962, current_train_items 134976.
I0304 19:30:09.863902 22502662377600 run.py:483] Algo bellman_ford step 4218 current loss 0.125516, current_train_items 135008.
I0304 19:30:09.897277 22502662377600 run.py:483] Algo bellman_ford step 4219 current loss 0.085929, current_train_items 135040.
I0304 19:30:09.916938 22502662377600 run.py:483] Algo bellman_ford step 4220 current loss 0.008058, current_train_items 135072.
I0304 19:30:09.933279 22502662377600 run.py:483] Algo bellman_ford step 4221 current loss 0.044543, current_train_items 135104.
I0304 19:30:09.957511 22502662377600 run.py:483] Algo bellman_ford step 4222 current loss 0.078604, current_train_items 135136.
I0304 19:30:09.989374 22502662377600 run.py:483] Algo bellman_ford step 4223 current loss 0.077006, current_train_items 135168.
I0304 19:30:10.025021 22502662377600 run.py:483] Algo bellman_ford step 4224 current loss 0.077250, current_train_items 135200.
I0304 19:30:10.044388 22502662377600 run.py:483] Algo bellman_ford step 4225 current loss 0.005739, current_train_items 135232.
I0304 19:30:10.060324 22502662377600 run.py:483] Algo bellman_ford step 4226 current loss 0.043285, current_train_items 135264.
I0304 19:30:10.083686 22502662377600 run.py:483] Algo bellman_ford step 4227 current loss 0.125531, current_train_items 135296.
I0304 19:30:10.113584 22502662377600 run.py:483] Algo bellman_ford step 4228 current loss 0.099029, current_train_items 135328.
I0304 19:30:10.145374 22502662377600 run.py:483] Algo bellman_ford step 4229 current loss 0.090180, current_train_items 135360.
I0304 19:30:10.164785 22502662377600 run.py:483] Algo bellman_ford step 4230 current loss 0.006895, current_train_items 135392.
I0304 19:30:10.180738 22502662377600 run.py:483] Algo bellman_ford step 4231 current loss 0.032153, current_train_items 135424.
I0304 19:30:10.204570 22502662377600 run.py:483] Algo bellman_ford step 4232 current loss 0.096109, current_train_items 135456.
I0304 19:30:10.236087 22502662377600 run.py:483] Algo bellman_ford step 4233 current loss 0.112349, current_train_items 135488.
I0304 19:30:10.269140 22502662377600 run.py:483] Algo bellman_ford step 4234 current loss 0.095250, current_train_items 135520.
I0304 19:30:10.288742 22502662377600 run.py:483] Algo bellman_ford step 4235 current loss 0.007350, current_train_items 135552.
I0304 19:30:10.305459 22502662377600 run.py:483] Algo bellman_ford step 4236 current loss 0.064987, current_train_items 135584.
I0304 19:30:10.329262 22502662377600 run.py:483] Algo bellman_ford step 4237 current loss 0.139896, current_train_items 135616.
I0304 19:30:10.360664 22502662377600 run.py:483] Algo bellman_ford step 4238 current loss 0.132721, current_train_items 135648.
I0304 19:30:10.396215 22502662377600 run.py:483] Algo bellman_ford step 4239 current loss 0.148428, current_train_items 135680.
I0304 19:30:10.415863 22502662377600 run.py:483] Algo bellman_ford step 4240 current loss 0.030532, current_train_items 135712.
I0304 19:30:10.431886 22502662377600 run.py:483] Algo bellman_ford step 4241 current loss 0.017342, current_train_items 135744.
I0304 19:30:10.455217 22502662377600 run.py:483] Algo bellman_ford step 4242 current loss 0.111325, current_train_items 135776.
I0304 19:30:10.487532 22502662377600 run.py:483] Algo bellman_ford step 4243 current loss 0.140763, current_train_items 135808.
I0304 19:30:10.520589 22502662377600 run.py:483] Algo bellman_ford step 4244 current loss 0.102475, current_train_items 135840.
I0304 19:30:10.540038 22502662377600 run.py:483] Algo bellman_ford step 4245 current loss 0.006928, current_train_items 135872.
I0304 19:30:10.555848 22502662377600 run.py:483] Algo bellman_ford step 4246 current loss 0.017238, current_train_items 135904.
I0304 19:30:10.581013 22502662377600 run.py:483] Algo bellman_ford step 4247 current loss 0.079274, current_train_items 135936.
I0304 19:30:10.612578 22502662377600 run.py:483] Algo bellman_ford step 4248 current loss 0.121421, current_train_items 135968.
I0304 19:30:10.648062 22502662377600 run.py:483] Algo bellman_ford step 4249 current loss 0.099475, current_train_items 136000.
I0304 19:30:10.667662 22502662377600 run.py:483] Algo bellman_ford step 4250 current loss 0.010219, current_train_items 136032.
I0304 19:30:10.675737 22502662377600 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0304 19:30:10.675840 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:10.692336 22502662377600 run.py:483] Algo bellman_ford step 4251 current loss 0.048300, current_train_items 136064.
I0304 19:30:10.716440 22502662377600 run.py:483] Algo bellman_ford step 4252 current loss 0.050678, current_train_items 136096.
I0304 19:30:10.747875 22502662377600 run.py:483] Algo bellman_ford step 4253 current loss 0.152198, current_train_items 136128.
I0304 19:30:10.781805 22502662377600 run.py:483] Algo bellman_ford step 4254 current loss 0.114147, current_train_items 136160.
I0304 19:30:10.801368 22502662377600 run.py:483] Algo bellman_ford step 4255 current loss 0.008459, current_train_items 136192.
I0304 19:30:10.817123 22502662377600 run.py:483] Algo bellman_ford step 4256 current loss 0.029510, current_train_items 136224.
I0304 19:30:10.840355 22502662377600 run.py:483] Algo bellman_ford step 4257 current loss 0.047537, current_train_items 136256.
I0304 19:30:10.871601 22502662377600 run.py:483] Algo bellman_ford step 4258 current loss 0.103461, current_train_items 136288.
I0304 19:30:10.905756 22502662377600 run.py:483] Algo bellman_ford step 4259 current loss 0.094080, current_train_items 136320.
I0304 19:30:10.925616 22502662377600 run.py:483] Algo bellman_ford step 4260 current loss 0.007232, current_train_items 136352.
I0304 19:30:10.942265 22502662377600 run.py:483] Algo bellman_ford step 4261 current loss 0.045613, current_train_items 136384.
I0304 19:30:10.965605 22502662377600 run.py:483] Algo bellman_ford step 4262 current loss 0.072352, current_train_items 136416.
I0304 19:30:10.996815 22502662377600 run.py:483] Algo bellman_ford step 4263 current loss 0.076172, current_train_items 136448.
I0304 19:30:11.030633 22502662377600 run.py:483] Algo bellman_ford step 4264 current loss 0.093979, current_train_items 136480.
I0304 19:30:11.050385 22502662377600 run.py:483] Algo bellman_ford step 4265 current loss 0.017208, current_train_items 136512.
I0304 19:30:11.066534 22502662377600 run.py:483] Algo bellman_ford step 4266 current loss 0.043918, current_train_items 136544.
I0304 19:30:11.090188 22502662377600 run.py:483] Algo bellman_ford step 4267 current loss 0.035709, current_train_items 136576.
I0304 19:30:11.120854 22502662377600 run.py:483] Algo bellman_ford step 4268 current loss 0.066812, current_train_items 136608.
I0304 19:30:11.153908 22502662377600 run.py:483] Algo bellman_ford step 4269 current loss 0.068916, current_train_items 136640.
I0304 19:30:11.174125 22502662377600 run.py:483] Algo bellman_ford step 4270 current loss 0.005806, current_train_items 136672.
I0304 19:30:11.190355 22502662377600 run.py:483] Algo bellman_ford step 4271 current loss 0.037082, current_train_items 136704.
I0304 19:30:11.212701 22502662377600 run.py:483] Algo bellman_ford step 4272 current loss 0.033923, current_train_items 136736.
I0304 19:30:11.243798 22502662377600 run.py:483] Algo bellman_ford step 4273 current loss 0.126067, current_train_items 136768.
I0304 19:30:11.277033 22502662377600 run.py:483] Algo bellman_ford step 4274 current loss 0.111482, current_train_items 136800.
I0304 19:30:11.297202 22502662377600 run.py:483] Algo bellman_ford step 4275 current loss 0.005642, current_train_items 136832.
I0304 19:30:11.313558 22502662377600 run.py:483] Algo bellman_ford step 4276 current loss 0.026930, current_train_items 136864.
I0304 19:30:11.336714 22502662377600 run.py:483] Algo bellman_ford step 4277 current loss 0.102111, current_train_items 136896.
I0304 19:30:11.369668 22502662377600 run.py:483] Algo bellman_ford step 4278 current loss 0.124140, current_train_items 136928.
I0304 19:30:11.403625 22502662377600 run.py:483] Algo bellman_ford step 4279 current loss 0.108122, current_train_items 136960.
I0304 19:30:11.423426 22502662377600 run.py:483] Algo bellman_ford step 4280 current loss 0.007937, current_train_items 136992.
I0304 19:30:11.439455 22502662377600 run.py:483] Algo bellman_ford step 4281 current loss 0.039337, current_train_items 137024.
I0304 19:30:11.463210 22502662377600 run.py:483] Algo bellman_ford step 4282 current loss 0.039969, current_train_items 137056.
I0304 19:30:11.493390 22502662377600 run.py:483] Algo bellman_ford step 4283 current loss 0.055243, current_train_items 137088.
I0304 19:30:11.529135 22502662377600 run.py:483] Algo bellman_ford step 4284 current loss 0.129498, current_train_items 137120.
I0304 19:30:11.548804 22502662377600 run.py:483] Algo bellman_ford step 4285 current loss 0.004837, current_train_items 137152.
I0304 19:30:11.564917 22502662377600 run.py:483] Algo bellman_ford step 4286 current loss 0.009742, current_train_items 137184.
I0304 19:30:11.588205 22502662377600 run.py:483] Algo bellman_ford step 4287 current loss 0.069920, current_train_items 137216.
I0304 19:30:11.621469 22502662377600 run.py:483] Algo bellman_ford step 4288 current loss 0.059340, current_train_items 137248.
I0304 19:30:11.655773 22502662377600 run.py:483] Algo bellman_ford step 4289 current loss 0.121470, current_train_items 137280.
I0304 19:30:11.675544 22502662377600 run.py:483] Algo bellman_ford step 4290 current loss 0.010893, current_train_items 137312.
I0304 19:30:11.692178 22502662377600 run.py:483] Algo bellman_ford step 4291 current loss 0.032913, current_train_items 137344.
I0304 19:30:11.716931 22502662377600 run.py:483] Algo bellman_ford step 4292 current loss 0.065964, current_train_items 137376.
I0304 19:30:11.749904 22502662377600 run.py:483] Algo bellman_ford step 4293 current loss 0.094083, current_train_items 137408.
I0304 19:30:11.785729 22502662377600 run.py:483] Algo bellman_ford step 4294 current loss 0.090649, current_train_items 137440.
I0304 19:30:11.805357 22502662377600 run.py:483] Algo bellman_ford step 4295 current loss 0.007020, current_train_items 137472.
I0304 19:30:11.821316 22502662377600 run.py:483] Algo bellman_ford step 4296 current loss 0.020441, current_train_items 137504.
I0304 19:30:11.844497 22502662377600 run.py:483] Algo bellman_ford step 4297 current loss 0.070223, current_train_items 137536.
I0304 19:30:11.875960 22502662377600 run.py:483] Algo bellman_ford step 4298 current loss 0.067486, current_train_items 137568.
I0304 19:30:11.906213 22502662377600 run.py:483] Algo bellman_ford step 4299 current loss 0.060715, current_train_items 137600.
I0304 19:30:11.926193 22502662377600 run.py:483] Algo bellman_ford step 4300 current loss 0.004615, current_train_items 137632.
I0304 19:30:11.934057 22502662377600 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0304 19:30:11.934160 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:30:11.950695 22502662377600 run.py:483] Algo bellman_ford step 4301 current loss 0.023644, current_train_items 137664.
I0304 19:30:11.974978 22502662377600 run.py:483] Algo bellman_ford step 4302 current loss 0.051703, current_train_items 137696.
I0304 19:30:12.007850 22502662377600 run.py:483] Algo bellman_ford step 4303 current loss 0.202571, current_train_items 137728.
I0304 19:30:12.044287 22502662377600 run.py:483] Algo bellman_ford step 4304 current loss 0.088753, current_train_items 137760.
I0304 19:30:12.064551 22502662377600 run.py:483] Algo bellman_ford step 4305 current loss 0.004871, current_train_items 137792.
I0304 19:30:12.080550 22502662377600 run.py:483] Algo bellman_ford step 4306 current loss 0.039005, current_train_items 137824.
I0304 19:30:12.105972 22502662377600 run.py:483] Algo bellman_ford step 4307 current loss 0.068019, current_train_items 137856.
I0304 19:30:12.138137 22502662377600 run.py:483] Algo bellman_ford step 4308 current loss 0.123763, current_train_items 137888.
I0304 19:30:12.174484 22502662377600 run.py:483] Algo bellman_ford step 4309 current loss 0.129716, current_train_items 137920.
I0304 19:30:12.194535 22502662377600 run.py:483] Algo bellman_ford step 4310 current loss 0.015598, current_train_items 137952.
I0304 19:30:12.210620 22502662377600 run.py:483] Algo bellman_ford step 4311 current loss 0.032711, current_train_items 137984.
I0304 19:30:12.233437 22502662377600 run.py:483] Algo bellman_ford step 4312 current loss 0.062108, current_train_items 138016.
I0304 19:30:12.265328 22502662377600 run.py:483] Algo bellman_ford step 4313 current loss 0.064042, current_train_items 138048.
I0304 19:30:12.299790 22502662377600 run.py:483] Algo bellman_ford step 4314 current loss 0.130697, current_train_items 138080.
I0304 19:30:12.319643 22502662377600 run.py:483] Algo bellman_ford step 4315 current loss 0.014160, current_train_items 138112.
I0304 19:30:12.335620 22502662377600 run.py:483] Algo bellman_ford step 4316 current loss 0.070135, current_train_items 138144.
I0304 19:30:12.360238 22502662377600 run.py:483] Algo bellman_ford step 4317 current loss 0.053210, current_train_items 138176.
I0304 19:30:12.390871 22502662377600 run.py:483] Algo bellman_ford step 4318 current loss 0.101334, current_train_items 138208.
I0304 19:30:12.424286 22502662377600 run.py:483] Algo bellman_ford step 4319 current loss 0.085877, current_train_items 138240.
I0304 19:30:12.443967 22502662377600 run.py:483] Algo bellman_ford step 4320 current loss 0.007404, current_train_items 138272.
I0304 19:30:12.460124 22502662377600 run.py:483] Algo bellman_ford step 4321 current loss 0.095305, current_train_items 138304.
I0304 19:30:12.484235 22502662377600 run.py:483] Algo bellman_ford step 4322 current loss 0.094743, current_train_items 138336.
I0304 19:30:12.514622 22502662377600 run.py:483] Algo bellman_ford step 4323 current loss 0.093600, current_train_items 138368.
I0304 19:30:12.549213 22502662377600 run.py:483] Algo bellman_ford step 4324 current loss 0.117495, current_train_items 138400.
I0304 19:30:12.569086 22502662377600 run.py:483] Algo bellman_ford step 4325 current loss 0.003970, current_train_items 138432.
I0304 19:30:12.585377 22502662377600 run.py:483] Algo bellman_ford step 4326 current loss 0.023410, current_train_items 138464.
I0304 19:30:12.609586 22502662377600 run.py:483] Algo bellman_ford step 4327 current loss 0.089749, current_train_items 138496.
I0304 19:30:12.642423 22502662377600 run.py:483] Algo bellman_ford step 4328 current loss 0.129968, current_train_items 138528.
I0304 19:30:12.675410 22502662377600 run.py:483] Algo bellman_ford step 4329 current loss 0.103798, current_train_items 138560.
I0304 19:30:12.695553 22502662377600 run.py:483] Algo bellman_ford step 4330 current loss 0.008571, current_train_items 138592.
I0304 19:30:12.712148 22502662377600 run.py:483] Algo bellman_ford step 4331 current loss 0.028179, current_train_items 138624.
I0304 19:30:12.736888 22502662377600 run.py:483] Algo bellman_ford step 4332 current loss 0.104882, current_train_items 138656.
I0304 19:30:12.767696 22502662377600 run.py:483] Algo bellman_ford step 4333 current loss 0.078907, current_train_items 138688.
I0304 19:30:12.800115 22502662377600 run.py:483] Algo bellman_ford step 4334 current loss 0.123754, current_train_items 138720.
I0304 19:30:12.819825 22502662377600 run.py:483] Algo bellman_ford step 4335 current loss 0.007758, current_train_items 138752.
I0304 19:30:12.835978 22502662377600 run.py:483] Algo bellman_ford step 4336 current loss 0.047251, current_train_items 138784.
I0304 19:30:12.860374 22502662377600 run.py:483] Algo bellman_ford step 4337 current loss 0.104355, current_train_items 138816.
I0304 19:30:12.892932 22502662377600 run.py:483] Algo bellman_ford step 4338 current loss 0.077784, current_train_items 138848.
I0304 19:30:12.926233 22502662377600 run.py:483] Algo bellman_ford step 4339 current loss 0.100138, current_train_items 138880.
I0304 19:30:12.946197 22502662377600 run.py:483] Algo bellman_ford step 4340 current loss 0.035230, current_train_items 138912.
I0304 19:30:12.962157 22502662377600 run.py:483] Algo bellman_ford step 4341 current loss 0.009865, current_train_items 138944.
I0304 19:30:12.985994 22502662377600 run.py:483] Algo bellman_ford step 4342 current loss 0.056252, current_train_items 138976.
I0304 19:30:13.017641 22502662377600 run.py:483] Algo bellman_ford step 4343 current loss 0.092161, current_train_items 139008.
I0304 19:30:13.052510 22502662377600 run.py:483] Algo bellman_ford step 4344 current loss 0.087405, current_train_items 139040.
I0304 19:30:13.072568 22502662377600 run.py:483] Algo bellman_ford step 4345 current loss 0.013345, current_train_items 139072.
I0304 19:30:13.089205 22502662377600 run.py:483] Algo bellman_ford step 4346 current loss 0.018947, current_train_items 139104.
I0304 19:30:13.113469 22502662377600 run.py:483] Algo bellman_ford step 4347 current loss 0.080284, current_train_items 139136.
I0304 19:30:13.145126 22502662377600 run.py:483] Algo bellman_ford step 4348 current loss 0.048314, current_train_items 139168.
I0304 19:30:13.178093 22502662377600 run.py:483] Algo bellman_ford step 4349 current loss 0.141197, current_train_items 139200.
I0304 19:30:13.197783 22502662377600 run.py:483] Algo bellman_ford step 4350 current loss 0.013102, current_train_items 139232.
I0304 19:30:13.205911 22502662377600 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0304 19:30:13.206017 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:13.223165 22502662377600 run.py:483] Algo bellman_ford step 4351 current loss 0.058819, current_train_items 139264.
I0304 19:30:13.247914 22502662377600 run.py:483] Algo bellman_ford step 4352 current loss 0.056796, current_train_items 139296.
I0304 19:30:13.278984 22502662377600 run.py:483] Algo bellman_ford step 4353 current loss 0.068988, current_train_items 139328.
I0304 19:30:13.313507 22502662377600 run.py:483] Algo bellman_ford step 4354 current loss 0.086972, current_train_items 139360.
I0304 19:30:13.333215 22502662377600 run.py:483] Algo bellman_ford step 4355 current loss 0.006045, current_train_items 139392.
I0304 19:30:13.349152 22502662377600 run.py:483] Algo bellman_ford step 4356 current loss 0.029525, current_train_items 139424.
I0304 19:30:13.373374 22502662377600 run.py:483] Algo bellman_ford step 4357 current loss 0.074685, current_train_items 139456.
I0304 19:30:13.402871 22502662377600 run.py:483] Algo bellman_ford step 4358 current loss 0.082867, current_train_items 139488.
I0304 19:30:13.435567 22502662377600 run.py:483] Algo bellman_ford step 4359 current loss 0.065913, current_train_items 139520.
I0304 19:30:13.455298 22502662377600 run.py:483] Algo bellman_ford step 4360 current loss 0.004952, current_train_items 139552.
I0304 19:30:13.471960 22502662377600 run.py:483] Algo bellman_ford step 4361 current loss 0.085378, current_train_items 139584.
I0304 19:30:13.494648 22502662377600 run.py:483] Algo bellman_ford step 4362 current loss 0.096210, current_train_items 139616.
I0304 19:30:13.526786 22502662377600 run.py:483] Algo bellman_ford step 4363 current loss 0.144169, current_train_items 139648.
I0304 19:30:13.562137 22502662377600 run.py:483] Algo bellman_ford step 4364 current loss 0.142333, current_train_items 139680.
I0304 19:30:13.581480 22502662377600 run.py:483] Algo bellman_ford step 4365 current loss 0.012685, current_train_items 139712.
I0304 19:30:13.597624 22502662377600 run.py:483] Algo bellman_ford step 4366 current loss 0.035628, current_train_items 139744.
I0304 19:30:13.621080 22502662377600 run.py:483] Algo bellman_ford step 4367 current loss 0.240508, current_train_items 139776.
I0304 19:30:13.651393 22502662377600 run.py:483] Algo bellman_ford step 4368 current loss 0.293468, current_train_items 139808.
I0304 19:30:13.685518 22502662377600 run.py:483] Algo bellman_ford step 4369 current loss 0.255242, current_train_items 139840.
I0304 19:30:13.705162 22502662377600 run.py:483] Algo bellman_ford step 4370 current loss 0.008353, current_train_items 139872.
I0304 19:30:13.721201 22502662377600 run.py:483] Algo bellman_ford step 4371 current loss 0.069697, current_train_items 139904.
I0304 19:30:13.744208 22502662377600 run.py:483] Algo bellman_ford step 4372 current loss 0.050151, current_train_items 139936.
I0304 19:30:13.774781 22502662377600 run.py:483] Algo bellman_ford step 4373 current loss 0.094550, current_train_items 139968.
I0304 19:30:13.808027 22502662377600 run.py:483] Algo bellman_ford step 4374 current loss 0.173119, current_train_items 140000.
I0304 19:30:13.828118 22502662377600 run.py:483] Algo bellman_ford step 4375 current loss 0.008948, current_train_items 140032.
I0304 19:30:13.844215 22502662377600 run.py:483] Algo bellman_ford step 4376 current loss 0.057935, current_train_items 140064.
I0304 19:30:13.867390 22502662377600 run.py:483] Algo bellman_ford step 4377 current loss 0.066303, current_train_items 140096.
I0304 19:30:13.897695 22502662377600 run.py:483] Algo bellman_ford step 4378 current loss 0.093873, current_train_items 140128.
I0304 19:30:13.931300 22502662377600 run.py:483] Algo bellman_ford step 4379 current loss 0.087410, current_train_items 140160.
I0304 19:30:13.950547 22502662377600 run.py:483] Algo bellman_ford step 4380 current loss 0.006616, current_train_items 140192.
I0304 19:30:13.966568 22502662377600 run.py:483] Algo bellman_ford step 4381 current loss 0.058552, current_train_items 140224.
I0304 19:30:13.990512 22502662377600 run.py:483] Algo bellman_ford step 4382 current loss 0.091798, current_train_items 140256.
I0304 19:30:14.021408 22502662377600 run.py:483] Algo bellman_ford step 4383 current loss 0.085149, current_train_items 140288.
I0304 19:30:14.055253 22502662377600 run.py:483] Algo bellman_ford step 4384 current loss 0.079754, current_train_items 140320.
I0304 19:30:14.075183 22502662377600 run.py:483] Algo bellman_ford step 4385 current loss 0.028126, current_train_items 140352.
I0304 19:30:14.091103 22502662377600 run.py:483] Algo bellman_ford step 4386 current loss 0.027266, current_train_items 140384.
I0304 19:30:14.114707 22502662377600 run.py:483] Algo bellman_ford step 4387 current loss 0.104719, current_train_items 140416.
I0304 19:30:14.145816 22502662377600 run.py:483] Algo bellman_ford step 4388 current loss 0.080948, current_train_items 140448.
I0304 19:30:14.178008 22502662377600 run.py:483] Algo bellman_ford step 4389 current loss 0.084905, current_train_items 140480.
I0304 19:30:14.197762 22502662377600 run.py:483] Algo bellman_ford step 4390 current loss 0.014391, current_train_items 140512.
I0304 19:30:14.213783 22502662377600 run.py:483] Algo bellman_ford step 4391 current loss 0.015953, current_train_items 140544.
I0304 19:30:14.237053 22502662377600 run.py:483] Algo bellman_ford step 4392 current loss 0.117960, current_train_items 140576.
I0304 19:30:14.268047 22502662377600 run.py:483] Algo bellman_ford step 4393 current loss 0.100781, current_train_items 140608.
I0304 19:30:14.302419 22502662377600 run.py:483] Algo bellman_ford step 4394 current loss 0.159844, current_train_items 140640.
I0304 19:30:14.321855 22502662377600 run.py:483] Algo bellman_ford step 4395 current loss 0.010061, current_train_items 140672.
I0304 19:30:14.338258 22502662377600 run.py:483] Algo bellman_ford step 4396 current loss 0.079111, current_train_items 140704.
I0304 19:30:14.362354 22502662377600 run.py:483] Algo bellman_ford step 4397 current loss 0.138987, current_train_items 140736.
I0304 19:30:14.393502 22502662377600 run.py:483] Algo bellman_ford step 4398 current loss 0.136499, current_train_items 140768.
I0304 19:30:14.426470 22502662377600 run.py:483] Algo bellman_ford step 4399 current loss 0.195292, current_train_items 140800.
I0304 19:30:14.446196 22502662377600 run.py:483] Algo bellman_ford step 4400 current loss 0.006320, current_train_items 140832.
I0304 19:30:14.454129 22502662377600 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0304 19:30:14.454234 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:30:14.471213 22502662377600 run.py:483] Algo bellman_ford step 4401 current loss 0.034696, current_train_items 140864.
I0304 19:30:14.495529 22502662377600 run.py:483] Algo bellman_ford step 4402 current loss 0.182755, current_train_items 140896.
I0304 19:30:14.526806 22502662377600 run.py:483] Algo bellman_ford step 4403 current loss 0.208711, current_train_items 140928.
I0304 19:30:14.565263 22502662377600 run.py:483] Algo bellman_ford step 4404 current loss 0.618270, current_train_items 140960.
I0304 19:30:14.585116 22502662377600 run.py:483] Algo bellman_ford step 4405 current loss 0.011250, current_train_items 140992.
I0304 19:30:14.601151 22502662377600 run.py:483] Algo bellman_ford step 4406 current loss 0.097467, current_train_items 141024.
I0304 19:30:14.624918 22502662377600 run.py:483] Algo bellman_ford step 4407 current loss 0.064771, current_train_items 141056.
I0304 19:30:14.656028 22502662377600 run.py:483] Algo bellman_ford step 4408 current loss 0.067929, current_train_items 141088.
I0304 19:30:14.691316 22502662377600 run.py:483] Algo bellman_ford step 4409 current loss 0.171768, current_train_items 141120.
I0304 19:30:14.711094 22502662377600 run.py:483] Algo bellman_ford step 4410 current loss 0.008197, current_train_items 141152.
I0304 19:30:14.727484 22502662377600 run.py:483] Algo bellman_ford step 4411 current loss 0.098425, current_train_items 141184.
I0304 19:30:14.751333 22502662377600 run.py:483] Algo bellman_ford step 4412 current loss 0.118131, current_train_items 141216.
I0304 19:30:14.783838 22502662377600 run.py:483] Algo bellman_ford step 4413 current loss 0.305277, current_train_items 141248.
I0304 19:30:14.817064 22502662377600 run.py:483] Algo bellman_ford step 4414 current loss 0.077539, current_train_items 141280.
I0304 19:30:14.836795 22502662377600 run.py:483] Algo bellman_ford step 4415 current loss 0.008230, current_train_items 141312.
I0304 19:30:14.853480 22502662377600 run.py:483] Algo bellman_ford step 4416 current loss 0.024144, current_train_items 141344.
I0304 19:30:14.877756 22502662377600 run.py:483] Algo bellman_ford step 4417 current loss 0.270526, current_train_items 141376.
I0304 19:30:14.908903 22502662377600 run.py:483] Algo bellman_ford step 4418 current loss 0.243541, current_train_items 141408.
I0304 19:30:14.942374 22502662377600 run.py:483] Algo bellman_ford step 4419 current loss 0.205173, current_train_items 141440.
I0304 19:30:14.961856 22502662377600 run.py:483] Algo bellman_ford step 4420 current loss 0.014976, current_train_items 141472.
I0304 19:30:14.978026 22502662377600 run.py:483] Algo bellman_ford step 4421 current loss 0.041328, current_train_items 141504.
I0304 19:30:15.001894 22502662377600 run.py:483] Algo bellman_ford step 4422 current loss 0.078556, current_train_items 141536.
I0304 19:30:15.032986 22502662377600 run.py:483] Algo bellman_ford step 4423 current loss 0.052548, current_train_items 141568.
I0304 19:30:15.067478 22502662377600 run.py:483] Algo bellman_ford step 4424 current loss 0.081666, current_train_items 141600.
I0304 19:30:15.087772 22502662377600 run.py:483] Algo bellman_ford step 4425 current loss 0.013570, current_train_items 141632.
I0304 19:30:15.103288 22502662377600 run.py:483] Algo bellman_ford step 4426 current loss 0.040921, current_train_items 141664.
I0304 19:30:15.127430 22502662377600 run.py:483] Algo bellman_ford step 4427 current loss 0.084475, current_train_items 141696.
I0304 19:30:15.158695 22502662377600 run.py:483] Algo bellman_ford step 4428 current loss 0.050587, current_train_items 141728.
I0304 19:30:15.192406 22502662377600 run.py:483] Algo bellman_ford step 4429 current loss 0.061803, current_train_items 141760.
I0304 19:30:15.212176 22502662377600 run.py:483] Algo bellman_ford step 4430 current loss 0.007308, current_train_items 141792.
I0304 19:30:15.227914 22502662377600 run.py:483] Algo bellman_ford step 4431 current loss 0.048809, current_train_items 141824.
I0304 19:30:15.251920 22502662377600 run.py:483] Algo bellman_ford step 4432 current loss 0.114443, current_train_items 141856.
I0304 19:30:15.281985 22502662377600 run.py:483] Algo bellman_ford step 4433 current loss 0.204421, current_train_items 141888.
I0304 19:30:15.315538 22502662377600 run.py:483] Algo bellman_ford step 4434 current loss 0.314072, current_train_items 141920.
I0304 19:30:15.335489 22502662377600 run.py:483] Algo bellman_ford step 4435 current loss 0.009271, current_train_items 141952.
I0304 19:30:15.351686 22502662377600 run.py:483] Algo bellman_ford step 4436 current loss 0.056746, current_train_items 141984.
I0304 19:30:15.376033 22502662377600 run.py:483] Algo bellman_ford step 4437 current loss 0.058497, current_train_items 142016.
I0304 19:30:15.406299 22502662377600 run.py:483] Algo bellman_ford step 4438 current loss 0.152579, current_train_items 142048.
I0304 19:30:15.441034 22502662377600 run.py:483] Algo bellman_ford step 4439 current loss 0.402225, current_train_items 142080.
I0304 19:30:15.460702 22502662377600 run.py:483] Algo bellman_ford step 4440 current loss 0.009812, current_train_items 142112.
I0304 19:30:15.477183 22502662377600 run.py:483] Algo bellman_ford step 4441 current loss 0.174821, current_train_items 142144.
I0304 19:30:15.500607 22502662377600 run.py:483] Algo bellman_ford step 4442 current loss 0.081746, current_train_items 142176.
I0304 19:30:15.531693 22502662377600 run.py:483] Algo bellman_ford step 4443 current loss 0.068554, current_train_items 142208.
I0304 19:30:15.566438 22502662377600 run.py:483] Algo bellman_ford step 4444 current loss 0.119621, current_train_items 142240.
I0304 19:30:15.586058 22502662377600 run.py:483] Algo bellman_ford step 4445 current loss 0.037006, current_train_items 142272.
I0304 19:30:15.601972 22502662377600 run.py:483] Algo bellman_ford step 4446 current loss 0.044411, current_train_items 142304.
I0304 19:30:15.626088 22502662377600 run.py:483] Algo bellman_ford step 4447 current loss 0.073638, current_train_items 142336.
I0304 19:30:15.657797 22502662377600 run.py:483] Algo bellman_ford step 4448 current loss 0.104217, current_train_items 142368.
I0304 19:30:15.691440 22502662377600 run.py:483] Algo bellman_ford step 4449 current loss 0.158073, current_train_items 142400.
I0304 19:30:15.711640 22502662377600 run.py:483] Algo bellman_ford step 4450 current loss 0.060618, current_train_items 142432.
I0304 19:30:15.720021 22502662377600 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0304 19:30:15.720128 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:30:15.736212 22502662377600 run.py:483] Algo bellman_ford step 4451 current loss 0.018301, current_train_items 142464.
I0304 19:30:15.760617 22502662377600 run.py:483] Algo bellman_ford step 4452 current loss 0.056137, current_train_items 142496.
I0304 19:30:15.790706 22502662377600 run.py:483] Algo bellman_ford step 4453 current loss 0.073300, current_train_items 142528.
I0304 19:30:15.823613 22502662377600 run.py:483] Algo bellman_ford step 4454 current loss 0.151801, current_train_items 142560.
I0304 19:30:15.843235 22502662377600 run.py:483] Algo bellman_ford step 4455 current loss 0.013308, current_train_items 142592.
I0304 19:30:15.858919 22502662377600 run.py:483] Algo bellman_ford step 4456 current loss 0.069759, current_train_items 142624.
I0304 19:30:15.882755 22502662377600 run.py:483] Algo bellman_ford step 4457 current loss 0.085255, current_train_items 142656.
I0304 19:30:15.912394 22502662377600 run.py:483] Algo bellman_ford step 4458 current loss 0.077717, current_train_items 142688.
I0304 19:30:15.949203 22502662377600 run.py:483] Algo bellman_ford step 4459 current loss 0.113689, current_train_items 142720.
I0304 19:30:15.969506 22502662377600 run.py:483] Algo bellman_ford step 4460 current loss 0.027864, current_train_items 142752.
I0304 19:30:15.985902 22502662377600 run.py:483] Algo bellman_ford step 4461 current loss 0.062880, current_train_items 142784.
I0304 19:30:16.010760 22502662377600 run.py:483] Algo bellman_ford step 4462 current loss 0.124435, current_train_items 142816.
I0304 19:30:16.040477 22502662377600 run.py:483] Algo bellman_ford step 4463 current loss 0.059794, current_train_items 142848.
I0304 19:30:16.074206 22502662377600 run.py:483] Algo bellman_ford step 4464 current loss 0.104031, current_train_items 142880.
I0304 19:30:16.094215 22502662377600 run.py:483] Algo bellman_ford step 4465 current loss 0.005691, current_train_items 142912.
I0304 19:30:16.110489 22502662377600 run.py:483] Algo bellman_ford step 4466 current loss 0.030398, current_train_items 142944.
I0304 19:30:16.134172 22502662377600 run.py:483] Algo bellman_ford step 4467 current loss 0.116456, current_train_items 142976.
I0304 19:30:16.167018 22502662377600 run.py:483] Algo bellman_ford step 4468 current loss 0.137645, current_train_items 143008.
I0304 19:30:16.201514 22502662377600 run.py:483] Algo bellman_ford step 4469 current loss 0.136118, current_train_items 143040.
I0304 19:30:16.221416 22502662377600 run.py:483] Algo bellman_ford step 4470 current loss 0.012289, current_train_items 143072.
I0304 19:30:16.237519 22502662377600 run.py:483] Algo bellman_ford step 4471 current loss 0.029355, current_train_items 143104.
I0304 19:30:16.260783 22502662377600 run.py:483] Algo bellman_ford step 4472 current loss 0.130228, current_train_items 143136.
I0304 19:30:16.291792 22502662377600 run.py:483] Algo bellman_ford step 4473 current loss 0.209559, current_train_items 143168.
I0304 19:30:16.328077 22502662377600 run.py:483] Algo bellman_ford step 4474 current loss 0.232271, current_train_items 143200.
I0304 19:30:16.348006 22502662377600 run.py:483] Algo bellman_ford step 4475 current loss 0.020430, current_train_items 143232.
I0304 19:30:16.364098 22502662377600 run.py:483] Algo bellman_ford step 4476 current loss 0.035497, current_train_items 143264.
I0304 19:30:16.387245 22502662377600 run.py:483] Algo bellman_ford step 4477 current loss 0.077663, current_train_items 143296.
I0304 19:30:16.418055 22502662377600 run.py:483] Algo bellman_ford step 4478 current loss 0.153487, current_train_items 143328.
I0304 19:30:16.453603 22502662377600 run.py:483] Algo bellman_ford step 4479 current loss 0.258916, current_train_items 143360.
I0304 19:30:16.473453 22502662377600 run.py:483] Algo bellman_ford step 4480 current loss 0.010001, current_train_items 143392.
I0304 19:30:16.489605 22502662377600 run.py:483] Algo bellman_ford step 4481 current loss 0.053153, current_train_items 143424.
I0304 19:30:16.513819 22502662377600 run.py:483] Algo bellman_ford step 4482 current loss 0.068889, current_train_items 143456.
I0304 19:30:16.545698 22502662377600 run.py:483] Algo bellman_ford step 4483 current loss 0.105378, current_train_items 143488.
I0304 19:30:16.577406 22502662377600 run.py:483] Algo bellman_ford step 4484 current loss 0.136928, current_train_items 143520.
I0304 19:30:16.597367 22502662377600 run.py:483] Algo bellman_ford step 4485 current loss 0.015326, current_train_items 143552.
I0304 19:30:16.613842 22502662377600 run.py:483] Algo bellman_ford step 4486 current loss 0.063177, current_train_items 143584.
I0304 19:30:16.638631 22502662377600 run.py:483] Algo bellman_ford step 4487 current loss 0.117823, current_train_items 143616.
I0304 19:30:16.670413 22502662377600 run.py:483] Algo bellman_ford step 4488 current loss 0.065959, current_train_items 143648.
I0304 19:30:16.703363 22502662377600 run.py:483] Algo bellman_ford step 4489 current loss 0.100473, current_train_items 143680.
I0304 19:30:16.723360 22502662377600 run.py:483] Algo bellman_ford step 4490 current loss 0.026918, current_train_items 143712.
I0304 19:30:16.739205 22502662377600 run.py:483] Algo bellman_ford step 4491 current loss 0.054441, current_train_items 143744.
I0304 19:30:16.763792 22502662377600 run.py:483] Algo bellman_ford step 4492 current loss 0.129624, current_train_items 143776.
I0304 19:30:16.795144 22502662377600 run.py:483] Algo bellman_ford step 4493 current loss 0.096766, current_train_items 143808.
I0304 19:30:16.829432 22502662377600 run.py:483] Algo bellman_ford step 4494 current loss 0.094019, current_train_items 143840.
I0304 19:30:16.848944 22502662377600 run.py:483] Algo bellman_ford step 4495 current loss 0.059631, current_train_items 143872.
I0304 19:30:16.865436 22502662377600 run.py:483] Algo bellman_ford step 4496 current loss 0.043782, current_train_items 143904.
I0304 19:30:16.889441 22502662377600 run.py:483] Algo bellman_ford step 4497 current loss 0.115379, current_train_items 143936.
I0304 19:30:16.919184 22502662377600 run.py:483] Algo bellman_ford step 4498 current loss 0.117392, current_train_items 143968.
I0304 19:30:16.950966 22502662377600 run.py:483] Algo bellman_ford step 4499 current loss 0.066329, current_train_items 144000.
I0304 19:30:16.970904 22502662377600 run.py:483] Algo bellman_ford step 4500 current loss 0.010359, current_train_items 144032.
I0304 19:30:16.978924 22502662377600 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0304 19:30:16.979029 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:16.995774 22502662377600 run.py:483] Algo bellman_ford step 4501 current loss 0.024157, current_train_items 144064.
I0304 19:30:17.021281 22502662377600 run.py:483] Algo bellman_ford step 4502 current loss 0.056118, current_train_items 144096.
I0304 19:30:17.053032 22502662377600 run.py:483] Algo bellman_ford step 4503 current loss 0.105303, current_train_items 144128.
I0304 19:30:17.087843 22502662377600 run.py:483] Algo bellman_ford step 4504 current loss 0.070260, current_train_items 144160.
I0304 19:30:17.108014 22502662377600 run.py:483] Algo bellman_ford step 4505 current loss 0.008817, current_train_items 144192.
I0304 19:30:17.124140 22502662377600 run.py:483] Algo bellman_ford step 4506 current loss 0.030765, current_train_items 144224.
I0304 19:30:17.147546 22502662377600 run.py:483] Algo bellman_ford step 4507 current loss 0.077733, current_train_items 144256.
I0304 19:30:17.177764 22502662377600 run.py:483] Algo bellman_ford step 4508 current loss 0.095574, current_train_items 144288.
I0304 19:30:17.210972 22502662377600 run.py:483] Algo bellman_ford step 4509 current loss 0.098984, current_train_items 144320.
I0304 19:30:17.230917 22502662377600 run.py:483] Algo bellman_ford step 4510 current loss 0.055076, current_train_items 144352.
I0304 19:30:17.247251 22502662377600 run.py:483] Algo bellman_ford step 4511 current loss 0.107875, current_train_items 144384.
I0304 19:30:17.271569 22502662377600 run.py:483] Algo bellman_ford step 4512 current loss 0.071053, current_train_items 144416.
I0304 19:30:17.302799 22502662377600 run.py:483] Algo bellman_ford step 4513 current loss 0.079599, current_train_items 144448.
I0304 19:30:17.335931 22502662377600 run.py:483] Algo bellman_ford step 4514 current loss 0.058609, current_train_items 144480.
I0304 19:30:17.355451 22502662377600 run.py:483] Algo bellman_ford step 4515 current loss 0.004283, current_train_items 144512.
I0304 19:30:17.371945 22502662377600 run.py:483] Algo bellman_ford step 4516 current loss 0.034361, current_train_items 144544.
I0304 19:30:17.395266 22502662377600 run.py:483] Algo bellman_ford step 4517 current loss 0.096231, current_train_items 144576.
I0304 19:30:17.427582 22502662377600 run.py:483] Algo bellman_ford step 4518 current loss 0.065986, current_train_items 144608.
I0304 19:30:17.464710 22502662377600 run.py:483] Algo bellman_ford step 4519 current loss 0.108902, current_train_items 144640.
I0304 19:30:17.484584 22502662377600 run.py:483] Algo bellman_ford step 4520 current loss 0.006012, current_train_items 144672.
I0304 19:30:17.500971 22502662377600 run.py:483] Algo bellman_ford step 4521 current loss 0.012207, current_train_items 144704.
I0304 19:30:17.524683 22502662377600 run.py:483] Algo bellman_ford step 4522 current loss 0.036781, current_train_items 144736.
I0304 19:30:17.557790 22502662377600 run.py:483] Algo bellman_ford step 4523 current loss 0.094645, current_train_items 144768.
I0304 19:30:17.591512 22502662377600 run.py:483] Algo bellman_ford step 4524 current loss 0.064912, current_train_items 144800.
I0304 19:30:17.611154 22502662377600 run.py:483] Algo bellman_ford step 4525 current loss 0.005224, current_train_items 144832.
I0304 19:30:17.627496 22502662377600 run.py:483] Algo bellman_ford step 4526 current loss 0.027965, current_train_items 144864.
I0304 19:30:17.652385 22502662377600 run.py:483] Algo bellman_ford step 4527 current loss 0.123680, current_train_items 144896.
I0304 19:30:17.683895 22502662377600 run.py:483] Algo bellman_ford step 4528 current loss 0.077095, current_train_items 144928.
I0304 19:30:17.717792 22502662377600 run.py:483] Algo bellman_ford step 4529 current loss 0.101163, current_train_items 144960.
I0304 19:30:17.737409 22502662377600 run.py:483] Algo bellman_ford step 4530 current loss 0.054703, current_train_items 144992.
I0304 19:30:17.753355 22502662377600 run.py:483] Algo bellman_ford step 4531 current loss 0.049785, current_train_items 145024.
I0304 19:30:17.776325 22502662377600 run.py:483] Algo bellman_ford step 4532 current loss 0.047183, current_train_items 145056.
I0304 19:30:17.806076 22502662377600 run.py:483] Algo bellman_ford step 4533 current loss 0.063400, current_train_items 145088.
I0304 19:30:17.841001 22502662377600 run.py:483] Algo bellman_ford step 4534 current loss 0.176321, current_train_items 145120.
I0304 19:30:17.860611 22502662377600 run.py:483] Algo bellman_ford step 4535 current loss 0.012246, current_train_items 145152.
I0304 19:30:17.876780 22502662377600 run.py:483] Algo bellman_ford step 4536 current loss 0.046674, current_train_items 145184.
I0304 19:30:17.900807 22502662377600 run.py:483] Algo bellman_ford step 4537 current loss 0.116955, current_train_items 145216.
I0304 19:30:17.931739 22502662377600 run.py:483] Algo bellman_ford step 4538 current loss 0.115635, current_train_items 145248.
I0304 19:30:17.967293 22502662377600 run.py:483] Algo bellman_ford step 4539 current loss 0.198119, current_train_items 145280.
I0304 19:30:17.986976 22502662377600 run.py:483] Algo bellman_ford step 4540 current loss 0.028605, current_train_items 145312.
I0304 19:30:18.003061 22502662377600 run.py:483] Algo bellman_ford step 4541 current loss 0.048287, current_train_items 145344.
I0304 19:30:18.026067 22502662377600 run.py:483] Algo bellman_ford step 4542 current loss 0.070135, current_train_items 145376.
I0304 19:30:18.057021 22502662377600 run.py:483] Algo bellman_ford step 4543 current loss 0.066981, current_train_items 145408.
I0304 19:30:18.091800 22502662377600 run.py:483] Algo bellman_ford step 4544 current loss 0.147334, current_train_items 145440.
I0304 19:30:18.111669 22502662377600 run.py:483] Algo bellman_ford step 4545 current loss 0.008309, current_train_items 145472.
I0304 19:30:18.127709 22502662377600 run.py:483] Algo bellman_ford step 4546 current loss 0.052959, current_train_items 145504.
I0304 19:30:18.151287 22502662377600 run.py:483] Algo bellman_ford step 4547 current loss 0.086982, current_train_items 145536.
I0304 19:30:18.181344 22502662377600 run.py:483] Algo bellman_ford step 4548 current loss 0.103370, current_train_items 145568.
I0304 19:30:18.216202 22502662377600 run.py:483] Algo bellman_ford step 4549 current loss 0.125636, current_train_items 145600.
I0304 19:30:18.236065 22502662377600 run.py:483] Algo bellman_ford step 4550 current loss 0.006967, current_train_items 145632.
I0304 19:30:18.244067 22502662377600 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0304 19:30:18.244172 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:30:18.261231 22502662377600 run.py:483] Algo bellman_ford step 4551 current loss 0.060203, current_train_items 145664.
I0304 19:30:18.285449 22502662377600 run.py:483] Algo bellman_ford step 4552 current loss 0.121418, current_train_items 145696.
I0304 19:30:18.317998 22502662377600 run.py:483] Algo bellman_ford step 4553 current loss 0.275362, current_train_items 145728.
I0304 19:30:18.352587 22502662377600 run.py:483] Algo bellman_ford step 4554 current loss 0.257759, current_train_items 145760.
I0304 19:30:18.372710 22502662377600 run.py:483] Algo bellman_ford step 4555 current loss 0.015037, current_train_items 145792.
I0304 19:30:18.388115 22502662377600 run.py:483] Algo bellman_ford step 4556 current loss 0.067422, current_train_items 145824.
I0304 19:30:18.411145 22502662377600 run.py:483] Algo bellman_ford step 4557 current loss 0.067575, current_train_items 145856.
I0304 19:30:18.443758 22502662377600 run.py:483] Algo bellman_ford step 4558 current loss 0.078091, current_train_items 145888.
I0304 19:30:18.479145 22502662377600 run.py:483] Algo bellman_ford step 4559 current loss 0.122238, current_train_items 145920.
I0304 19:30:18.498933 22502662377600 run.py:483] Algo bellman_ford step 4560 current loss 0.012259, current_train_items 145952.
I0304 19:30:18.515443 22502662377600 run.py:483] Algo bellman_ford step 4561 current loss 0.044049, current_train_items 145984.
I0304 19:30:18.538661 22502662377600 run.py:483] Algo bellman_ford step 4562 current loss 0.066522, current_train_items 146016.
I0304 19:30:18.569303 22502662377600 run.py:483] Algo bellman_ford step 4563 current loss 0.097353, current_train_items 146048.
I0304 19:30:18.604194 22502662377600 run.py:483] Algo bellman_ford step 4564 current loss 0.131693, current_train_items 146080.
I0304 19:30:18.623862 22502662377600 run.py:483] Algo bellman_ford step 4565 current loss 0.018455, current_train_items 146112.
I0304 19:30:18.640079 22502662377600 run.py:483] Algo bellman_ford step 4566 current loss 0.047281, current_train_items 146144.
I0304 19:30:18.664263 22502662377600 run.py:483] Algo bellman_ford step 4567 current loss 0.053297, current_train_items 146176.
I0304 19:30:18.696484 22502662377600 run.py:483] Algo bellman_ford step 4568 current loss 0.117084, current_train_items 146208.
I0304 19:30:18.729470 22502662377600 run.py:483] Algo bellman_ford step 4569 current loss 0.144467, current_train_items 146240.
I0304 19:30:18.749128 22502662377600 run.py:483] Algo bellman_ford step 4570 current loss 0.006109, current_train_items 146272.
I0304 19:30:18.765214 22502662377600 run.py:483] Algo bellman_ford step 4571 current loss 0.021825, current_train_items 146304.
I0304 19:30:18.789410 22502662377600 run.py:483] Algo bellman_ford step 4572 current loss 0.144408, current_train_items 146336.
I0304 19:30:18.819952 22502662377600 run.py:483] Algo bellman_ford step 4573 current loss 0.059080, current_train_items 146368.
I0304 19:30:18.852368 22502662377600 run.py:483] Algo bellman_ford step 4574 current loss 0.097283, current_train_items 146400.
I0304 19:30:18.872131 22502662377600 run.py:483] Algo bellman_ford step 4575 current loss 0.008562, current_train_items 146432.
I0304 19:30:18.888370 22502662377600 run.py:483] Algo bellman_ford step 4576 current loss 0.031890, current_train_items 146464.
I0304 19:30:18.911922 22502662377600 run.py:483] Algo bellman_ford step 4577 current loss 0.061172, current_train_items 146496.
I0304 19:30:18.943431 22502662377600 run.py:483] Algo bellman_ford step 4578 current loss 0.056399, current_train_items 146528.
I0304 19:30:18.977748 22502662377600 run.py:483] Algo bellman_ford step 4579 current loss 0.102994, current_train_items 146560.
I0304 19:30:18.997382 22502662377600 run.py:483] Algo bellman_ford step 4580 current loss 0.024271, current_train_items 146592.
I0304 19:30:19.014038 22502662377600 run.py:483] Algo bellman_ford step 4581 current loss 0.044083, current_train_items 146624.
I0304 19:30:19.037926 22502662377600 run.py:483] Algo bellman_ford step 4582 current loss 0.084447, current_train_items 146656.
I0304 19:30:19.069633 22502662377600 run.py:483] Algo bellman_ford step 4583 current loss 0.069023, current_train_items 146688.
I0304 19:30:19.103872 22502662377600 run.py:483] Algo bellman_ford step 4584 current loss 0.129862, current_train_items 146720.
I0304 19:30:19.123878 22502662377600 run.py:483] Algo bellman_ford step 4585 current loss 0.023089, current_train_items 146752.
I0304 19:30:19.140446 22502662377600 run.py:483] Algo bellman_ford step 4586 current loss 0.050796, current_train_items 146784.
I0304 19:30:19.163244 22502662377600 run.py:483] Algo bellman_ford step 4587 current loss 0.071162, current_train_items 146816.
I0304 19:30:19.194566 22502662377600 run.py:483] Algo bellman_ford step 4588 current loss 0.157807, current_train_items 146848.
I0304 19:30:19.227420 22502662377600 run.py:483] Algo bellman_ford step 4589 current loss 0.157613, current_train_items 146880.
I0304 19:30:19.247252 22502662377600 run.py:483] Algo bellman_ford step 4590 current loss 0.015604, current_train_items 146912.
I0304 19:30:19.263503 22502662377600 run.py:483] Algo bellman_ford step 4591 current loss 0.029749, current_train_items 146944.
I0304 19:30:19.286549 22502662377600 run.py:483] Algo bellman_ford step 4592 current loss 0.054288, current_train_items 146976.
I0304 19:30:19.318490 22502662377600 run.py:483] Algo bellman_ford step 4593 current loss 0.059836, current_train_items 147008.
I0304 19:30:19.353134 22502662377600 run.py:483] Algo bellman_ford step 4594 current loss 0.101527, current_train_items 147040.
I0304 19:30:19.372793 22502662377600 run.py:483] Algo bellman_ford step 4595 current loss 0.011112, current_train_items 147072.
I0304 19:30:19.388922 22502662377600 run.py:483] Algo bellman_ford step 4596 current loss 0.027854, current_train_items 147104.
I0304 19:30:19.414644 22502662377600 run.py:483] Algo bellman_ford step 4597 current loss 0.093222, current_train_items 147136.
I0304 19:30:19.445532 22502662377600 run.py:483] Algo bellman_ford step 4598 current loss 0.085502, current_train_items 147168.
I0304 19:30:19.477701 22502662377600 run.py:483] Algo bellman_ford step 4599 current loss 0.099691, current_train_items 147200.
I0304 19:30:19.497641 22502662377600 run.py:483] Algo bellman_ford step 4600 current loss 0.009284, current_train_items 147232.
I0304 19:30:19.505408 22502662377600 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0304 19:30:19.505522 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:30:19.522651 22502662377600 run.py:483] Algo bellman_ford step 4601 current loss 0.046369, current_train_items 147264.
I0304 19:30:19.548508 22502662377600 run.py:483] Algo bellman_ford step 4602 current loss 0.075546, current_train_items 147296.
I0304 19:30:19.580098 22502662377600 run.py:483] Algo bellman_ford step 4603 current loss 0.081555, current_train_items 147328.
I0304 19:30:19.614741 22502662377600 run.py:483] Algo bellman_ford step 4604 current loss 0.120447, current_train_items 147360.
I0304 19:30:19.634322 22502662377600 run.py:483] Algo bellman_ford step 4605 current loss 0.005965, current_train_items 147392.
I0304 19:30:19.650410 22502662377600 run.py:483] Algo bellman_ford step 4606 current loss 0.047164, current_train_items 147424.
I0304 19:30:19.674129 22502662377600 run.py:483] Algo bellman_ford step 4607 current loss 0.080754, current_train_items 147456.
I0304 19:30:19.706171 22502662377600 run.py:483] Algo bellman_ford step 4608 current loss 0.035262, current_train_items 147488.
I0304 19:30:19.738359 22502662377600 run.py:483] Algo bellman_ford step 4609 current loss 0.086669, current_train_items 147520.
I0304 19:30:19.757909 22502662377600 run.py:483] Algo bellman_ford step 4610 current loss 0.019090, current_train_items 147552.
I0304 19:30:19.773938 22502662377600 run.py:483] Algo bellman_ford step 4611 current loss 0.049051, current_train_items 147584.
I0304 19:30:19.798094 22502662377600 run.py:483] Algo bellman_ford step 4612 current loss 0.061883, current_train_items 147616.
I0304 19:30:19.829900 22502662377600 run.py:483] Algo bellman_ford step 4613 current loss 0.095639, current_train_items 147648.
I0304 19:30:19.864631 22502662377600 run.py:483] Algo bellman_ford step 4614 current loss 0.071276, current_train_items 147680.
I0304 19:30:19.884139 22502662377600 run.py:483] Algo bellman_ford step 4615 current loss 0.018704, current_train_items 147712.
I0304 19:30:19.900400 22502662377600 run.py:483] Algo bellman_ford step 4616 current loss 0.048360, current_train_items 147744.
I0304 19:30:19.924561 22502662377600 run.py:483] Algo bellman_ford step 4617 current loss 0.061490, current_train_items 147776.
I0304 19:30:19.955560 22502662377600 run.py:483] Algo bellman_ford step 4618 current loss 0.053732, current_train_items 147808.
I0304 19:30:19.989117 22502662377600 run.py:483] Algo bellman_ford step 4619 current loss 0.092401, current_train_items 147840.
I0304 19:30:20.008356 22502662377600 run.py:483] Algo bellman_ford step 4620 current loss 0.018480, current_train_items 147872.
I0304 19:30:20.024286 22502662377600 run.py:483] Algo bellman_ford step 4621 current loss 0.024671, current_train_items 147904.
I0304 19:30:20.047858 22502662377600 run.py:483] Algo bellman_ford step 4622 current loss 0.048963, current_train_items 147936.
I0304 19:30:20.079146 22502662377600 run.py:483] Algo bellman_ford step 4623 current loss 0.086194, current_train_items 147968.
I0304 19:30:20.112142 22502662377600 run.py:483] Algo bellman_ford step 4624 current loss 0.202169, current_train_items 148000.
I0304 19:30:20.131807 22502662377600 run.py:483] Algo bellman_ford step 4625 current loss 0.011718, current_train_items 148032.
I0304 19:30:20.147977 22502662377600 run.py:483] Algo bellman_ford step 4626 current loss 0.018669, current_train_items 148064.
I0304 19:30:20.172226 22502662377600 run.py:483] Algo bellman_ford step 4627 current loss 0.095660, current_train_items 148096.
I0304 19:30:20.204347 22502662377600 run.py:483] Algo bellman_ford step 4628 current loss 0.095422, current_train_items 148128.
I0304 19:30:20.239638 22502662377600 run.py:483] Algo bellman_ford step 4629 current loss 0.172517, current_train_items 148160.
I0304 19:30:20.258898 22502662377600 run.py:483] Algo bellman_ford step 4630 current loss 0.014080, current_train_items 148192.
I0304 19:30:20.275174 22502662377600 run.py:483] Algo bellman_ford step 4631 current loss 0.051960, current_train_items 148224.
I0304 19:30:20.299846 22502662377600 run.py:483] Algo bellman_ford step 4632 current loss 0.085165, current_train_items 148256.
I0304 19:30:20.329769 22502662377600 run.py:483] Algo bellman_ford step 4633 current loss 0.036227, current_train_items 148288.
I0304 19:30:20.364017 22502662377600 run.py:483] Algo bellman_ford step 4634 current loss 0.073272, current_train_items 148320.
I0304 19:30:20.383761 22502662377600 run.py:483] Algo bellman_ford step 4635 current loss 0.009295, current_train_items 148352.
I0304 19:30:20.400438 22502662377600 run.py:483] Algo bellman_ford step 4636 current loss 0.036512, current_train_items 148384.
I0304 19:30:20.424569 22502662377600 run.py:483] Algo bellman_ford step 4637 current loss 0.159023, current_train_items 148416.
I0304 19:30:20.457265 22502662377600 run.py:483] Algo bellman_ford step 4638 current loss 0.129729, current_train_items 148448.
I0304 19:30:20.491886 22502662377600 run.py:483] Algo bellman_ford step 4639 current loss 0.102768, current_train_items 148480.
I0304 19:30:20.511536 22502662377600 run.py:483] Algo bellman_ford step 4640 current loss 0.022309, current_train_items 148512.
I0304 19:30:20.527683 22502662377600 run.py:483] Algo bellman_ford step 4641 current loss 0.034806, current_train_items 148544.
I0304 19:30:20.551733 22502662377600 run.py:483] Algo bellman_ford step 4642 current loss 0.077449, current_train_items 148576.
I0304 19:30:20.583421 22502662377600 run.py:483] Algo bellman_ford step 4643 current loss 0.167646, current_train_items 148608.
I0304 19:30:20.618022 22502662377600 run.py:483] Algo bellman_ford step 4644 current loss 0.112040, current_train_items 148640.
I0304 19:30:20.637812 22502662377600 run.py:483] Algo bellman_ford step 4645 current loss 0.009525, current_train_items 148672.
I0304 19:30:20.654047 22502662377600 run.py:483] Algo bellman_ford step 4646 current loss 0.033734, current_train_items 148704.
I0304 19:30:20.676729 22502662377600 run.py:483] Algo bellman_ford step 4647 current loss 0.057898, current_train_items 148736.
I0304 19:30:20.706479 22502662377600 run.py:483] Algo bellman_ford step 4648 current loss 0.090826, current_train_items 148768.
I0304 19:30:20.738550 22502662377600 run.py:483] Algo bellman_ford step 4649 current loss 0.100020, current_train_items 148800.
I0304 19:30:20.757991 22502662377600 run.py:483] Algo bellman_ford step 4650 current loss 0.012176, current_train_items 148832.
I0304 19:30:20.766148 22502662377600 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0304 19:30:20.766254 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:30:20.783370 22502662377600 run.py:483] Algo bellman_ford step 4651 current loss 0.082686, current_train_items 148864.
I0304 19:30:20.808140 22502662377600 run.py:483] Algo bellman_ford step 4652 current loss 0.072103, current_train_items 148896.
I0304 19:30:20.840793 22502662377600 run.py:483] Algo bellman_ford step 4653 current loss 0.134704, current_train_items 148928.
I0304 19:30:20.875362 22502662377600 run.py:483] Algo bellman_ford step 4654 current loss 0.135759, current_train_items 148960.
I0304 19:30:20.895532 22502662377600 run.py:483] Algo bellman_ford step 4655 current loss 0.019643, current_train_items 148992.
I0304 19:30:20.911554 22502662377600 run.py:483] Algo bellman_ford step 4656 current loss 0.018402, current_train_items 149024.
I0304 19:30:20.935962 22502662377600 run.py:483] Algo bellman_ford step 4657 current loss 0.062045, current_train_items 149056.
I0304 19:30:20.967265 22502662377600 run.py:483] Algo bellman_ford step 4658 current loss 0.120330, current_train_items 149088.
I0304 19:30:21.000645 22502662377600 run.py:483] Algo bellman_ford step 4659 current loss 0.092720, current_train_items 149120.
I0304 19:30:21.020733 22502662377600 run.py:483] Algo bellman_ford step 4660 current loss 0.011904, current_train_items 149152.
I0304 19:30:21.037240 22502662377600 run.py:483] Algo bellman_ford step 4661 current loss 0.041539, current_train_items 149184.
I0304 19:30:21.062053 22502662377600 run.py:483] Algo bellman_ford step 4662 current loss 0.145980, current_train_items 149216.
I0304 19:30:21.092826 22502662377600 run.py:483] Algo bellman_ford step 4663 current loss 0.144937, current_train_items 149248.
I0304 19:30:21.130182 22502662377600 run.py:483] Algo bellman_ford step 4664 current loss 0.159675, current_train_items 149280.
I0304 19:30:21.149930 22502662377600 run.py:483] Algo bellman_ford step 4665 current loss 0.016084, current_train_items 149312.
I0304 19:30:21.165801 22502662377600 run.py:483] Algo bellman_ford step 4666 current loss 0.018675, current_train_items 149344.
I0304 19:30:21.189412 22502662377600 run.py:483] Algo bellman_ford step 4667 current loss 0.097584, current_train_items 149376.
I0304 19:30:21.221826 22502662377600 run.py:483] Algo bellman_ford step 4668 current loss 0.124602, current_train_items 149408.
I0304 19:30:21.256812 22502662377600 run.py:483] Algo bellman_ford step 4669 current loss 0.130950, current_train_items 149440.
I0304 19:30:21.277087 22502662377600 run.py:483] Algo bellman_ford step 4670 current loss 0.006279, current_train_items 149472.
I0304 19:30:21.292954 22502662377600 run.py:483] Algo bellman_ford step 4671 current loss 0.039769, current_train_items 149504.
I0304 19:30:21.316148 22502662377600 run.py:483] Algo bellman_ford step 4672 current loss 0.118761, current_train_items 149536.
I0304 19:30:21.346607 22502662377600 run.py:483] Algo bellman_ford step 4673 current loss 0.093603, current_train_items 149568.
I0304 19:30:21.380305 22502662377600 run.py:483] Algo bellman_ford step 4674 current loss 0.071358, current_train_items 149600.
I0304 19:30:21.400433 22502662377600 run.py:483] Algo bellman_ford step 4675 current loss 0.014990, current_train_items 149632.
I0304 19:30:21.417181 22502662377600 run.py:483] Algo bellman_ford step 4676 current loss 0.047524, current_train_items 149664.
I0304 19:30:21.441552 22502662377600 run.py:483] Algo bellman_ford step 4677 current loss 0.260414, current_train_items 149696.
I0304 19:30:21.474418 22502662377600 run.py:483] Algo bellman_ford step 4678 current loss 0.229692, current_train_items 149728.
I0304 19:30:21.508877 22502662377600 run.py:483] Algo bellman_ford step 4679 current loss 0.135856, current_train_items 149760.
I0304 19:30:21.528570 22502662377600 run.py:483] Algo bellman_ford step 4680 current loss 0.005100, current_train_items 149792.
I0304 19:30:21.544491 22502662377600 run.py:483] Algo bellman_ford step 4681 current loss 0.018610, current_train_items 149824.
I0304 19:30:21.568106 22502662377600 run.py:483] Algo bellman_ford step 4682 current loss 0.062881, current_train_items 149856.
I0304 19:30:21.599402 22502662377600 run.py:483] Algo bellman_ford step 4683 current loss 0.114942, current_train_items 149888.
I0304 19:30:21.635137 22502662377600 run.py:483] Algo bellman_ford step 4684 current loss 0.096054, current_train_items 149920.
I0304 19:30:21.655246 22502662377600 run.py:483] Algo bellman_ford step 4685 current loss 0.012122, current_train_items 149952.
I0304 19:30:21.671100 22502662377600 run.py:483] Algo bellman_ford step 4686 current loss 0.024730, current_train_items 149984.
I0304 19:30:21.694892 22502662377600 run.py:483] Algo bellman_ford step 4687 current loss 0.126536, current_train_items 150016.
I0304 19:30:21.726814 22502662377600 run.py:483] Algo bellman_ford step 4688 current loss 0.086902, current_train_items 150048.
I0304 19:30:21.762242 22502662377600 run.py:483] Algo bellman_ford step 4689 current loss 0.125587, current_train_items 150080.
I0304 19:30:21.782432 22502662377600 run.py:483] Algo bellman_ford step 4690 current loss 0.009770, current_train_items 150112.
I0304 19:30:21.798746 22502662377600 run.py:483] Algo bellman_ford step 4691 current loss 0.018641, current_train_items 150144.
I0304 19:30:21.822201 22502662377600 run.py:483] Algo bellman_ford step 4692 current loss 0.071874, current_train_items 150176.
I0304 19:30:21.853786 22502662377600 run.py:483] Algo bellman_ford step 4693 current loss 0.200918, current_train_items 150208.
I0304 19:30:21.886869 22502662377600 run.py:483] Algo bellman_ford step 4694 current loss 0.189631, current_train_items 150240.
I0304 19:30:21.906893 22502662377600 run.py:483] Algo bellman_ford step 4695 current loss 0.008601, current_train_items 150272.
I0304 19:30:21.923359 22502662377600 run.py:483] Algo bellman_ford step 4696 current loss 0.021047, current_train_items 150304.
I0304 19:30:21.947923 22502662377600 run.py:483] Algo bellman_ford step 4697 current loss 0.084370, current_train_items 150336.
I0304 19:30:21.979086 22502662377600 run.py:483] Algo bellman_ford step 4698 current loss 0.110918, current_train_items 150368.
I0304 19:30:22.011682 22502662377600 run.py:483] Algo bellman_ford step 4699 current loss 0.152472, current_train_items 150400.
I0304 19:30:22.031730 22502662377600 run.py:483] Algo bellman_ford step 4700 current loss 0.007418, current_train_items 150432.
I0304 19:30:22.039790 22502662377600 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0304 19:30:22.039894 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:30:22.056207 22502662377600 run.py:483] Algo bellman_ford step 4701 current loss 0.034680, current_train_items 150464.
I0304 19:30:22.080767 22502662377600 run.py:483] Algo bellman_ford step 4702 current loss 0.079843, current_train_items 150496.
I0304 19:30:22.113830 22502662377600 run.py:483] Algo bellman_ford step 4703 current loss 0.067543, current_train_items 150528.
I0304 19:30:22.149365 22502662377600 run.py:483] Algo bellman_ford step 4704 current loss 0.082398, current_train_items 150560.
I0304 19:30:22.169533 22502662377600 run.py:483] Algo bellman_ford step 4705 current loss 0.023610, current_train_items 150592.
I0304 19:30:22.185268 22502662377600 run.py:483] Algo bellman_ford step 4706 current loss 0.015694, current_train_items 150624.
I0304 19:30:22.209794 22502662377600 run.py:483] Algo bellman_ford step 4707 current loss 0.070015, current_train_items 150656.
I0304 19:30:22.242155 22502662377600 run.py:483] Algo bellman_ford step 4708 current loss 0.146172, current_train_items 150688.
I0304 19:30:22.276096 22502662377600 run.py:483] Algo bellman_ford step 4709 current loss 0.076306, current_train_items 150720.
I0304 19:30:22.295756 22502662377600 run.py:483] Algo bellman_ford step 4710 current loss 0.010705, current_train_items 150752.
I0304 19:30:22.311968 22502662377600 run.py:483] Algo bellman_ford step 4711 current loss 0.024075, current_train_items 150784.
I0304 19:30:22.336408 22502662377600 run.py:483] Algo bellman_ford step 4712 current loss 0.110922, current_train_items 150816.
I0304 19:30:22.367874 22502662377600 run.py:483] Algo bellman_ford step 4713 current loss 0.142754, current_train_items 150848.
I0304 19:30:22.400595 22502662377600 run.py:483] Algo bellman_ford step 4714 current loss 0.120159, current_train_items 150880.
I0304 19:30:22.420022 22502662377600 run.py:483] Algo bellman_ford step 4715 current loss 0.002771, current_train_items 150912.
I0304 19:30:22.436375 22502662377600 run.py:483] Algo bellman_ford step 4716 current loss 0.058257, current_train_items 150944.
I0304 19:30:22.459098 22502662377600 run.py:483] Algo bellman_ford step 4717 current loss 0.039886, current_train_items 150976.
I0304 19:30:22.490481 22502662377600 run.py:483] Algo bellman_ford step 4718 current loss 0.091685, current_train_items 151008.
I0304 19:30:22.526712 22502662377600 run.py:483] Algo bellman_ford step 4719 current loss 0.131327, current_train_items 151040.
I0304 19:30:22.546330 22502662377600 run.py:483] Algo bellman_ford step 4720 current loss 0.005619, current_train_items 151072.
I0304 19:30:22.562207 22502662377600 run.py:483] Algo bellman_ford step 4721 current loss 0.043389, current_train_items 151104.
I0304 19:30:22.586848 22502662377600 run.py:483] Algo bellman_ford step 4722 current loss 0.068283, current_train_items 151136.
I0304 19:30:22.616812 22502662377600 run.py:483] Algo bellman_ford step 4723 current loss 0.095192, current_train_items 151168.
I0304 19:30:22.651345 22502662377600 run.py:483] Algo bellman_ford step 4724 current loss 0.069861, current_train_items 151200.
I0304 19:30:22.671050 22502662377600 run.py:483] Algo bellman_ford step 4725 current loss 0.003286, current_train_items 151232.
I0304 19:30:22.687450 22502662377600 run.py:483] Algo bellman_ford step 4726 current loss 0.040260, current_train_items 151264.
I0304 19:30:22.712644 22502662377600 run.py:483] Algo bellman_ford step 4727 current loss 0.097904, current_train_items 151296.
I0304 19:30:22.742812 22502662377600 run.py:483] Algo bellman_ford step 4728 current loss 0.059238, current_train_items 151328.
I0304 19:30:22.773152 22502662377600 run.py:483] Algo bellman_ford step 4729 current loss 0.050300, current_train_items 151360.
I0304 19:30:22.792749 22502662377600 run.py:483] Algo bellman_ford step 4730 current loss 0.012218, current_train_items 151392.
I0304 19:30:22.808675 22502662377600 run.py:483] Algo bellman_ford step 4731 current loss 0.012727, current_train_items 151424.
I0304 19:30:22.832026 22502662377600 run.py:483] Algo bellman_ford step 4732 current loss 0.061465, current_train_items 151456.
I0304 19:30:22.862905 22502662377600 run.py:483] Algo bellman_ford step 4733 current loss 0.076513, current_train_items 151488.
I0304 19:30:22.897166 22502662377600 run.py:483] Algo bellman_ford step 4734 current loss 0.110698, current_train_items 151520.
I0304 19:30:22.916802 22502662377600 run.py:483] Algo bellman_ford step 4735 current loss 0.004348, current_train_items 151552.
I0304 19:30:22.933224 22502662377600 run.py:483] Algo bellman_ford step 4736 current loss 0.038685, current_train_items 151584.
I0304 19:30:22.956881 22502662377600 run.py:483] Algo bellman_ford step 4737 current loss 0.107779, current_train_items 151616.
I0304 19:30:22.987761 22502662377600 run.py:483] Algo bellman_ford step 4738 current loss 0.090256, current_train_items 151648.
I0304 19:30:23.020850 22502662377600 run.py:483] Algo bellman_ford step 4739 current loss 0.072571, current_train_items 151680.
I0304 19:30:23.040430 22502662377600 run.py:483] Algo bellman_ford step 4740 current loss 0.049039, current_train_items 151712.
I0304 19:30:23.056266 22502662377600 run.py:483] Algo bellman_ford step 4741 current loss 0.042432, current_train_items 151744.
I0304 19:30:23.079610 22502662377600 run.py:483] Algo bellman_ford step 4742 current loss 0.137677, current_train_items 151776.
I0304 19:30:23.111750 22502662377600 run.py:483] Algo bellman_ford step 4743 current loss 0.161489, current_train_items 151808.
I0304 19:30:23.144329 22502662377600 run.py:483] Algo bellman_ford step 4744 current loss 0.115833, current_train_items 151840.
I0304 19:30:23.164191 22502662377600 run.py:483] Algo bellman_ford step 4745 current loss 0.038437, current_train_items 151872.
I0304 19:30:23.180885 22502662377600 run.py:483] Algo bellman_ford step 4746 current loss 0.062648, current_train_items 151904.
I0304 19:30:23.206459 22502662377600 run.py:483] Algo bellman_ford step 4747 current loss 0.156987, current_train_items 151936.
I0304 19:30:23.238171 22502662377600 run.py:483] Algo bellman_ford step 4748 current loss 0.072370, current_train_items 151968.
I0304 19:30:23.271667 22502662377600 run.py:483] Algo bellman_ford step 4749 current loss 0.130408, current_train_items 152000.
I0304 19:30:23.291454 22502662377600 run.py:483] Algo bellman_ford step 4750 current loss 0.044189, current_train_items 152032.
I0304 19:30:23.299548 22502662377600 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.9501953125, 'score': 0.9501953125, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0304 19:30:23.299653 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.950, val scores are: bellman_ford: 0.950
I0304 19:30:23.316458 22502662377600 run.py:483] Algo bellman_ford step 4751 current loss 0.030155, current_train_items 152064.
I0304 19:30:23.341331 22502662377600 run.py:483] Algo bellman_ford step 4752 current loss 0.109707, current_train_items 152096.
I0304 19:30:23.374981 22502662377600 run.py:483] Algo bellman_ford step 4753 current loss 0.333390, current_train_items 152128.
I0304 19:30:23.408684 22502662377600 run.py:483] Algo bellman_ford step 4754 current loss 0.246618, current_train_items 152160.
I0304 19:30:23.428804 22502662377600 run.py:483] Algo bellman_ford step 4755 current loss 0.007655, current_train_items 152192.
I0304 19:30:23.444738 22502662377600 run.py:483] Algo bellman_ford step 4756 current loss 0.010655, current_train_items 152224.
I0304 19:30:23.469571 22502662377600 run.py:483] Algo bellman_ford step 4757 current loss 0.069998, current_train_items 152256.
I0304 19:30:23.501138 22502662377600 run.py:483] Algo bellman_ford step 4758 current loss 0.073531, current_train_items 152288.
I0304 19:30:23.536076 22502662377600 run.py:483] Algo bellman_ford step 4759 current loss 0.155699, current_train_items 152320.
I0304 19:30:23.556257 22502662377600 run.py:483] Algo bellman_ford step 4760 current loss 0.007944, current_train_items 152352.
I0304 19:30:23.572667 22502662377600 run.py:483] Algo bellman_ford step 4761 current loss 0.079986, current_train_items 152384.
I0304 19:30:23.596586 22502662377600 run.py:483] Algo bellman_ford step 4762 current loss 0.070716, current_train_items 152416.
I0304 19:30:23.627336 22502662377600 run.py:483] Algo bellman_ford step 4763 current loss 0.072367, current_train_items 152448.
I0304 19:30:23.661840 22502662377600 run.py:483] Algo bellman_ford step 4764 current loss 0.102414, current_train_items 152480.
I0304 19:30:23.681417 22502662377600 run.py:483] Algo bellman_ford step 4765 current loss 0.017185, current_train_items 152512.
I0304 19:30:23.697113 22502662377600 run.py:483] Algo bellman_ford step 4766 current loss 0.026294, current_train_items 152544.
I0304 19:30:23.722144 22502662377600 run.py:483] Algo bellman_ford step 4767 current loss 0.081988, current_train_items 152576.
I0304 19:30:23.752715 22502662377600 run.py:483] Algo bellman_ford step 4768 current loss 0.064641, current_train_items 152608.
I0304 19:30:23.785887 22502662377600 run.py:483] Algo bellman_ford step 4769 current loss 0.103628, current_train_items 152640.
I0304 19:30:23.805824 22502662377600 run.py:483] Algo bellman_ford step 4770 current loss 0.007599, current_train_items 152672.
I0304 19:30:23.821787 22502662377600 run.py:483] Algo bellman_ford step 4771 current loss 0.031466, current_train_items 152704.
I0304 19:30:23.844801 22502662377600 run.py:483] Algo bellman_ford step 4772 current loss 0.061417, current_train_items 152736.
I0304 19:30:23.876340 22502662377600 run.py:483] Algo bellman_ford step 4773 current loss 0.133799, current_train_items 152768.
I0304 19:30:23.910585 22502662377600 run.py:483] Algo bellman_ford step 4774 current loss 0.114475, current_train_items 152800.
I0304 19:30:23.930414 22502662377600 run.py:483] Algo bellman_ford step 4775 current loss 0.006622, current_train_items 152832.
I0304 19:30:23.946653 22502662377600 run.py:483] Algo bellman_ford step 4776 current loss 0.021381, current_train_items 152864.
I0304 19:30:23.970642 22502662377600 run.py:483] Algo bellman_ford step 4777 current loss 0.194712, current_train_items 152896.
I0304 19:30:24.002483 22502662377600 run.py:483] Algo bellman_ford step 4778 current loss 0.321757, current_train_items 152928.
I0304 19:30:24.034883 22502662377600 run.py:483] Algo bellman_ford step 4779 current loss 0.291511, current_train_items 152960.
I0304 19:30:24.054849 22502662377600 run.py:483] Algo bellman_ford step 4780 current loss 0.015013, current_train_items 152992.
I0304 19:30:24.071076 22502662377600 run.py:483] Algo bellman_ford step 4781 current loss 0.034658, current_train_items 153024.
I0304 19:30:24.095420 22502662377600 run.py:483] Algo bellman_ford step 4782 current loss 0.070390, current_train_items 153056.
I0304 19:30:24.127657 22502662377600 run.py:483] Algo bellman_ford step 4783 current loss 0.168165, current_train_items 153088.
I0304 19:30:24.162275 22502662377600 run.py:483] Algo bellman_ford step 4784 current loss 0.148276, current_train_items 153120.
I0304 19:30:24.182547 22502662377600 run.py:483] Algo bellman_ford step 4785 current loss 0.024077, current_train_items 153152.
I0304 19:30:24.199155 22502662377600 run.py:483] Algo bellman_ford step 4786 current loss 0.049800, current_train_items 153184.
I0304 19:30:24.223806 22502662377600 run.py:483] Algo bellman_ford step 4787 current loss 0.075311, current_train_items 153216.
I0304 19:30:24.254750 22502662377600 run.py:483] Algo bellman_ford step 4788 current loss 0.102700, current_train_items 153248.
I0304 19:30:24.287254 22502662377600 run.py:483] Algo bellman_ford step 4789 current loss 0.099396, current_train_items 153280.
I0304 19:30:24.306760 22502662377600 run.py:483] Algo bellman_ford step 4790 current loss 0.006015, current_train_items 153312.
I0304 19:30:24.322906 22502662377600 run.py:483] Algo bellman_ford step 4791 current loss 0.030737, current_train_items 153344.
I0304 19:30:24.346948 22502662377600 run.py:483] Algo bellman_ford step 4792 current loss 0.071148, current_train_items 153376.
I0304 19:30:24.378115 22502662377600 run.py:483] Algo bellman_ford step 4793 current loss 0.093321, current_train_items 153408.
I0304 19:30:24.413956 22502662377600 run.py:483] Algo bellman_ford step 4794 current loss 0.110481, current_train_items 153440.
I0304 19:30:24.433637 22502662377600 run.py:483] Algo bellman_ford step 4795 current loss 0.011641, current_train_items 153472.
I0304 19:30:24.449844 22502662377600 run.py:483] Algo bellman_ford step 4796 current loss 0.028587, current_train_items 153504.
I0304 19:30:24.474303 22502662377600 run.py:483] Algo bellman_ford step 4797 current loss 0.112204, current_train_items 153536.
I0304 19:30:24.505188 22502662377600 run.py:483] Algo bellman_ford step 4798 current loss 0.118338, current_train_items 153568.
I0304 19:30:24.538667 22502662377600 run.py:483] Algo bellman_ford step 4799 current loss 0.136127, current_train_items 153600.
I0304 19:30:24.558618 22502662377600 run.py:483] Algo bellman_ford step 4800 current loss 0.007715, current_train_items 153632.
I0304 19:30:24.566805 22502662377600 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.96484375, 'score': 0.96484375, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0304 19:30:24.566912 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.965, val scores are: bellman_ford: 0.965
I0304 19:30:24.583800 22502662377600 run.py:483] Algo bellman_ford step 4801 current loss 0.086217, current_train_items 153664.
I0304 19:30:24.608562 22502662377600 run.py:483] Algo bellman_ford step 4802 current loss 0.171714, current_train_items 153696.
I0304 19:30:24.639799 22502662377600 run.py:483] Algo bellman_ford step 4803 current loss 0.128148, current_train_items 153728.
I0304 19:30:24.676459 22502662377600 run.py:483] Algo bellman_ford step 4804 current loss 0.136580, current_train_items 153760.
I0304 19:30:24.696423 22502662377600 run.py:483] Algo bellman_ford step 4805 current loss 0.020445, current_train_items 153792.
I0304 19:30:24.712846 22502662377600 run.py:483] Algo bellman_ford step 4806 current loss 0.024563, current_train_items 153824.
I0304 19:30:24.737267 22502662377600 run.py:483] Algo bellman_ford step 4807 current loss 0.092423, current_train_items 153856.
I0304 19:30:24.769108 22502662377600 run.py:483] Algo bellman_ford step 4808 current loss 0.138171, current_train_items 153888.
I0304 19:30:24.803159 22502662377600 run.py:483] Algo bellman_ford step 4809 current loss 0.115595, current_train_items 153920.
I0304 19:30:24.823054 22502662377600 run.py:483] Algo bellman_ford step 4810 current loss 0.008665, current_train_items 153952.
I0304 19:30:24.839228 22502662377600 run.py:483] Algo bellman_ford step 4811 current loss 0.028278, current_train_items 153984.
I0304 19:30:24.863059 22502662377600 run.py:483] Algo bellman_ford step 4812 current loss 0.065689, current_train_items 154016.
I0304 19:30:24.893651 22502662377600 run.py:483] Algo bellman_ford step 4813 current loss 0.072787, current_train_items 154048.
I0304 19:30:24.929131 22502662377600 run.py:483] Algo bellman_ford step 4814 current loss 0.097960, current_train_items 154080.
I0304 19:30:24.949034 22502662377600 run.py:483] Algo bellman_ford step 4815 current loss 0.016388, current_train_items 154112.
I0304 19:30:24.965193 22502662377600 run.py:483] Algo bellman_ford step 4816 current loss 0.033884, current_train_items 154144.
I0304 19:30:24.990216 22502662377600 run.py:483] Algo bellman_ford step 4817 current loss 0.087066, current_train_items 154176.
I0304 19:30:25.021691 22502662377600 run.py:483] Algo bellman_ford step 4818 current loss 0.053882, current_train_items 154208.
I0304 19:30:25.051506 22502662377600 run.py:483] Algo bellman_ford step 4819 current loss 0.056685, current_train_items 154240.
I0304 19:30:25.071342 22502662377600 run.py:483] Algo bellman_ford step 4820 current loss 0.006177, current_train_items 154272.
I0304 19:30:25.087409 22502662377600 run.py:483] Algo bellman_ford step 4821 current loss 0.036774, current_train_items 154304.
I0304 19:30:25.111656 22502662377600 run.py:483] Algo bellman_ford step 4822 current loss 0.067308, current_train_items 154336.
I0304 19:30:25.142472 22502662377600 run.py:483] Algo bellman_ford step 4823 current loss 0.083122, current_train_items 154368.
I0304 19:30:25.177368 22502662377600 run.py:483] Algo bellman_ford step 4824 current loss 0.091565, current_train_items 154400.
I0304 19:30:25.197430 22502662377600 run.py:483] Algo bellman_ford step 4825 current loss 0.006913, current_train_items 154432.
I0304 19:30:25.214231 22502662377600 run.py:483] Algo bellman_ford step 4826 current loss 0.025310, current_train_items 154464.
I0304 19:30:25.237770 22502662377600 run.py:483] Algo bellman_ford step 4827 current loss 0.048596, current_train_items 154496.
I0304 19:30:25.269309 22502662377600 run.py:483] Algo bellman_ford step 4828 current loss 0.072970, current_train_items 154528.
I0304 19:30:25.304108 22502662377600 run.py:483] Algo bellman_ford step 4829 current loss 0.092949, current_train_items 154560.
I0304 19:30:25.323706 22502662377600 run.py:483] Algo bellman_ford step 4830 current loss 0.006455, current_train_items 154592.
I0304 19:30:25.339981 22502662377600 run.py:483] Algo bellman_ford step 4831 current loss 0.054379, current_train_items 154624.
I0304 19:30:25.364826 22502662377600 run.py:483] Algo bellman_ford step 4832 current loss 0.072723, current_train_items 154656.
I0304 19:30:25.396882 22502662377600 run.py:483] Algo bellman_ford step 4833 current loss 0.141216, current_train_items 154688.
I0304 19:30:25.430590 22502662377600 run.py:483] Algo bellman_ford step 4834 current loss 0.084593, current_train_items 154720.
I0304 19:30:25.450223 22502662377600 run.py:483] Algo bellman_ford step 4835 current loss 0.005374, current_train_items 154752.
I0304 19:30:25.466333 22502662377600 run.py:483] Algo bellman_ford step 4836 current loss 0.025220, current_train_items 154784.
I0304 19:30:25.489348 22502662377600 run.py:483] Algo bellman_ford step 4837 current loss 0.094283, current_train_items 154816.
I0304 19:30:25.520648 22502662377600 run.py:483] Algo bellman_ford step 4838 current loss 0.182170, current_train_items 154848.
I0304 19:30:25.552957 22502662377600 run.py:483] Algo bellman_ford step 4839 current loss 0.118880, current_train_items 154880.
I0304 19:30:25.572848 22502662377600 run.py:483] Algo bellman_ford step 4840 current loss 0.012381, current_train_items 154912.
I0304 19:30:25.588841 22502662377600 run.py:483] Algo bellman_ford step 4841 current loss 0.038239, current_train_items 154944.
I0304 19:30:25.612873 22502662377600 run.py:483] Algo bellman_ford step 4842 current loss 0.054074, current_train_items 154976.
I0304 19:30:25.643906 22502662377600 run.py:483] Algo bellman_ford step 4843 current loss 0.196709, current_train_items 155008.
I0304 19:30:25.679427 22502662377600 run.py:483] Algo bellman_ford step 4844 current loss 0.190549, current_train_items 155040.
I0304 19:30:25.699281 22502662377600 run.py:483] Algo bellman_ford step 4845 current loss 0.013286, current_train_items 155072.
I0304 19:30:25.715692 22502662377600 run.py:483] Algo bellman_ford step 4846 current loss 0.035836, current_train_items 155104.
I0304 19:30:25.739145 22502662377600 run.py:483] Algo bellman_ford step 4847 current loss 0.120588, current_train_items 155136.
I0304 19:30:25.770049 22502662377600 run.py:483] Algo bellman_ford step 4848 current loss 0.076472, current_train_items 155168.
I0304 19:30:25.803730 22502662377600 run.py:483] Algo bellman_ford step 4849 current loss 0.121004, current_train_items 155200.
I0304 19:30:25.823468 22502662377600 run.py:483] Algo bellman_ford step 4850 current loss 0.013338, current_train_items 155232.
I0304 19:30:25.831620 22502662377600 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0304 19:30:25.831726 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:25.848987 22502662377600 run.py:483] Algo bellman_ford step 4851 current loss 0.029155, current_train_items 155264.
I0304 19:30:25.872392 22502662377600 run.py:483] Algo bellman_ford step 4852 current loss 0.087083, current_train_items 155296.
I0304 19:30:25.903686 22502662377600 run.py:483] Algo bellman_ford step 4853 current loss 0.068944, current_train_items 155328.
I0304 19:30:25.937649 22502662377600 run.py:483] Algo bellman_ford step 4854 current loss 0.106699, current_train_items 155360.
I0304 19:30:25.957755 22502662377600 run.py:483] Algo bellman_ford step 4855 current loss 0.010664, current_train_items 155392.
I0304 19:30:25.974095 22502662377600 run.py:483] Algo bellman_ford step 4856 current loss 0.043303, current_train_items 155424.
I0304 19:30:25.999161 22502662377600 run.py:483] Algo bellman_ford step 4857 current loss 0.093473, current_train_items 155456.
I0304 19:30:26.031281 22502662377600 run.py:483] Algo bellman_ford step 4858 current loss 0.135260, current_train_items 155488.
I0304 19:30:26.067230 22502662377600 run.py:483] Algo bellman_ford step 4859 current loss 0.176733, current_train_items 155520.
I0304 19:30:26.087216 22502662377600 run.py:483] Algo bellman_ford step 4860 current loss 0.007042, current_train_items 155552.
I0304 19:30:26.104395 22502662377600 run.py:483] Algo bellman_ford step 4861 current loss 0.040487, current_train_items 155584.
I0304 19:30:26.128010 22502662377600 run.py:483] Algo bellman_ford step 4862 current loss 0.106844, current_train_items 155616.
I0304 19:30:26.160432 22502662377600 run.py:483] Algo bellman_ford step 4863 current loss 0.099559, current_train_items 155648.
I0304 19:30:26.193336 22502662377600 run.py:483] Algo bellman_ford step 4864 current loss 0.071446, current_train_items 155680.
I0304 19:30:26.213056 22502662377600 run.py:483] Algo bellman_ford step 4865 current loss 0.010888, current_train_items 155712.
I0304 19:30:26.228928 22502662377600 run.py:483] Algo bellman_ford step 4866 current loss 0.047550, current_train_items 155744.
I0304 19:30:26.253622 22502662377600 run.py:483] Algo bellman_ford step 4867 current loss 0.137397, current_train_items 155776.
I0304 19:30:26.286103 22502662377600 run.py:483] Algo bellman_ford step 4868 current loss 0.164206, current_train_items 155808.
I0304 19:30:26.318747 22502662377600 run.py:483] Algo bellman_ford step 4869 current loss 0.170289, current_train_items 155840.
I0304 19:30:26.339483 22502662377600 run.py:483] Algo bellman_ford step 4870 current loss 0.010877, current_train_items 155872.
I0304 19:30:26.355805 22502662377600 run.py:483] Algo bellman_ford step 4871 current loss 0.077770, current_train_items 155904.
I0304 19:30:26.379422 22502662377600 run.py:483] Algo bellman_ford step 4872 current loss 0.070955, current_train_items 155936.
I0304 19:30:26.408856 22502662377600 run.py:483] Algo bellman_ford step 4873 current loss 0.051866, current_train_items 155968.
I0304 19:30:26.441599 22502662377600 run.py:483] Algo bellman_ford step 4874 current loss 0.107154, current_train_items 156000.
I0304 19:30:26.461838 22502662377600 run.py:483] Algo bellman_ford step 4875 current loss 0.008641, current_train_items 156032.
I0304 19:30:26.477807 22502662377600 run.py:483] Algo bellman_ford step 4876 current loss 0.041936, current_train_items 156064.
I0304 19:30:26.500890 22502662377600 run.py:483] Algo bellman_ford step 4877 current loss 0.040376, current_train_items 156096.
I0304 19:30:26.531486 22502662377600 run.py:483] Algo bellman_ford step 4878 current loss 0.050740, current_train_items 156128.
I0304 19:30:26.565299 22502662377600 run.py:483] Algo bellman_ford step 4879 current loss 0.100872, current_train_items 156160.
I0304 19:30:26.585118 22502662377600 run.py:483] Algo bellman_ford step 4880 current loss 0.018298, current_train_items 156192.
I0304 19:30:26.601208 22502662377600 run.py:483] Algo bellman_ford step 4881 current loss 0.032455, current_train_items 156224.
I0304 19:30:26.625552 22502662377600 run.py:483] Algo bellman_ford step 4882 current loss 0.075821, current_train_items 156256.
I0304 19:30:26.657873 22502662377600 run.py:483] Algo bellman_ford step 4883 current loss 0.076628, current_train_items 156288.
I0304 19:30:26.692307 22502662377600 run.py:483] Algo bellman_ford step 4884 current loss 0.087572, current_train_items 156320.
I0304 19:30:26.712373 22502662377600 run.py:483] Algo bellman_ford step 4885 current loss 0.007611, current_train_items 156352.
I0304 19:30:26.728940 22502662377600 run.py:483] Algo bellman_ford step 4886 current loss 0.056686, current_train_items 156384.
I0304 19:30:26.752985 22502662377600 run.py:483] Algo bellman_ford step 4887 current loss 0.057508, current_train_items 156416.
I0304 19:30:26.784008 22502662377600 run.py:483] Algo bellman_ford step 4888 current loss 0.044529, current_train_items 156448.
I0304 19:30:26.817007 22502662377600 run.py:483] Algo bellman_ford step 4889 current loss 0.098507, current_train_items 156480.
I0304 19:30:26.836912 22502662377600 run.py:483] Algo bellman_ford step 4890 current loss 0.005460, current_train_items 156512.
I0304 19:30:26.852994 22502662377600 run.py:483] Algo bellman_ford step 4891 current loss 0.033348, current_train_items 156544.
I0304 19:30:26.876603 22502662377600 run.py:483] Algo bellman_ford step 4892 current loss 0.047476, current_train_items 156576.
I0304 19:30:26.906721 22502662377600 run.py:483] Algo bellman_ford step 4893 current loss 0.080126, current_train_items 156608.
I0304 19:30:26.939515 22502662377600 run.py:483] Algo bellman_ford step 4894 current loss 0.119953, current_train_items 156640.
I0304 19:30:26.959274 22502662377600 run.py:483] Algo bellman_ford step 4895 current loss 0.027263, current_train_items 156672.
I0304 19:30:26.975692 22502662377600 run.py:483] Algo bellman_ford step 4896 current loss 0.022204, current_train_items 156704.
I0304 19:30:26.998887 22502662377600 run.py:483] Algo bellman_ford step 4897 current loss 0.042241, current_train_items 156736.
I0304 19:30:27.029397 22502662377600 run.py:483] Algo bellman_ford step 4898 current loss 0.105434, current_train_items 156768.
I0304 19:30:27.064134 22502662377600 run.py:483] Algo bellman_ford step 4899 current loss 0.173762, current_train_items 156800.
I0304 19:30:27.084344 22502662377600 run.py:483] Algo bellman_ford step 4900 current loss 0.021682, current_train_items 156832.
I0304 19:30:27.092609 22502662377600 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0304 19:30:27.092739 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:27.109071 22502662377600 run.py:483] Algo bellman_ford step 4901 current loss 0.026637, current_train_items 156864.
I0304 19:30:27.133891 22502662377600 run.py:483] Algo bellman_ford step 4902 current loss 0.067023, current_train_items 156896.
I0304 19:30:27.167420 22502662377600 run.py:483] Algo bellman_ford step 4903 current loss 0.053675, current_train_items 156928.
I0304 19:30:27.201090 22502662377600 run.py:483] Algo bellman_ford step 4904 current loss 0.059868, current_train_items 156960.
I0304 19:30:27.221071 22502662377600 run.py:483] Algo bellman_ford step 4905 current loss 0.023477, current_train_items 156992.
I0304 19:30:27.236813 22502662377600 run.py:483] Algo bellman_ford step 4906 current loss 0.037636, current_train_items 157024.
I0304 19:30:27.261427 22502662377600 run.py:483] Algo bellman_ford step 4907 current loss 0.057438, current_train_items 157056.
I0304 19:30:27.292998 22502662377600 run.py:483] Algo bellman_ford step 4908 current loss 0.063677, current_train_items 157088.
I0304 19:30:27.326216 22502662377600 run.py:483] Algo bellman_ford step 4909 current loss 0.099613, current_train_items 157120.
I0304 19:30:27.346161 22502662377600 run.py:483] Algo bellman_ford step 4910 current loss 0.006537, current_train_items 157152.
I0304 19:30:27.362555 22502662377600 run.py:483] Algo bellman_ford step 4911 current loss 0.034262, current_train_items 157184.
I0304 19:30:27.386443 22502662377600 run.py:483] Algo bellman_ford step 4912 current loss 0.074334, current_train_items 157216.
I0304 19:30:27.419117 22502662377600 run.py:483] Algo bellman_ford step 4913 current loss 0.177164, current_train_items 157248.
I0304 19:30:27.452635 22502662377600 run.py:483] Algo bellman_ford step 4914 current loss 0.112676, current_train_items 157280.
I0304 19:30:27.472437 22502662377600 run.py:483] Algo bellman_ford step 4915 current loss 0.018628, current_train_items 157312.
I0304 19:30:27.488324 22502662377600 run.py:483] Algo bellman_ford step 4916 current loss 0.069737, current_train_items 157344.
I0304 19:30:27.512467 22502662377600 run.py:483] Algo bellman_ford step 4917 current loss 0.072396, current_train_items 157376.
I0304 19:30:27.542886 22502662377600 run.py:483] Algo bellman_ford step 4918 current loss 0.063343, current_train_items 157408.
I0304 19:30:27.576755 22502662377600 run.py:483] Algo bellman_ford step 4919 current loss 0.069616, current_train_items 157440.
I0304 19:30:27.596791 22502662377600 run.py:483] Algo bellman_ford step 4920 current loss 0.009133, current_train_items 157472.
I0304 19:30:27.612336 22502662377600 run.py:483] Algo bellman_ford step 4921 current loss 0.027557, current_train_items 157504.
I0304 19:30:27.636956 22502662377600 run.py:483] Algo bellman_ford step 4922 current loss 0.099370, current_train_items 157536.
I0304 19:30:27.668312 22502662377600 run.py:483] Algo bellman_ford step 4923 current loss 0.170001, current_train_items 157568.
I0304 19:30:27.702172 22502662377600 run.py:483] Algo bellman_ford step 4924 current loss 0.117663, current_train_items 157600.
I0304 19:30:27.722046 22502662377600 run.py:483] Algo bellman_ford step 4925 current loss 0.009954, current_train_items 157632.
I0304 19:30:27.738275 22502662377600 run.py:483] Algo bellman_ford step 4926 current loss 0.083228, current_train_items 157664.
I0304 19:30:27.762825 22502662377600 run.py:483] Algo bellman_ford step 4927 current loss 0.090968, current_train_items 157696.
I0304 19:30:27.793409 22502662377600 run.py:483] Algo bellman_ford step 4928 current loss 0.150970, current_train_items 157728.
I0304 19:30:27.827375 22502662377600 run.py:483] Algo bellman_ford step 4929 current loss 0.109663, current_train_items 157760.
I0304 19:30:27.847224 22502662377600 run.py:483] Algo bellman_ford step 4930 current loss 0.005196, current_train_items 157792.
I0304 19:30:27.863065 22502662377600 run.py:483] Algo bellman_ford step 4931 current loss 0.034757, current_train_items 157824.
I0304 19:30:27.887144 22502662377600 run.py:483] Algo bellman_ford step 4932 current loss 0.077945, current_train_items 157856.
I0304 19:30:27.918127 22502662377600 run.py:483] Algo bellman_ford step 4933 current loss 0.078467, current_train_items 157888.
I0304 19:30:27.951744 22502662377600 run.py:483] Algo bellman_ford step 4934 current loss 0.092873, current_train_items 157920.
I0304 19:30:27.971504 22502662377600 run.py:483] Algo bellman_ford step 4935 current loss 0.005724, current_train_items 157952.
I0304 19:30:27.987640 22502662377600 run.py:483] Algo bellman_ford step 4936 current loss 0.029790, current_train_items 157984.
I0304 19:30:28.012980 22502662377600 run.py:483] Algo bellman_ford step 4937 current loss 0.086626, current_train_items 158016.
I0304 19:30:28.044045 22502662377600 run.py:483] Algo bellman_ford step 4938 current loss 0.060033, current_train_items 158048.
I0304 19:30:28.079794 22502662377600 run.py:483] Algo bellman_ford step 4939 current loss 0.092350, current_train_items 158080.
I0304 19:30:28.099605 22502662377600 run.py:483] Algo bellman_ford step 4940 current loss 0.003690, current_train_items 158112.
I0304 19:30:28.115417 22502662377600 run.py:483] Algo bellman_ford step 4941 current loss 0.079028, current_train_items 158144.
I0304 19:30:28.139554 22502662377600 run.py:483] Algo bellman_ford step 4942 current loss 0.094189, current_train_items 158176.
I0304 19:30:28.172008 22502662377600 run.py:483] Algo bellman_ford step 4943 current loss 0.151247, current_train_items 158208.
I0304 19:30:28.204188 22502662377600 run.py:483] Algo bellman_ford step 4944 current loss 0.100190, current_train_items 158240.
I0304 19:30:28.223971 22502662377600 run.py:483] Algo bellman_ford step 4945 current loss 0.045366, current_train_items 158272.
I0304 19:30:28.240292 22502662377600 run.py:483] Algo bellman_ford step 4946 current loss 0.071277, current_train_items 158304.
I0304 19:30:28.263870 22502662377600 run.py:483] Algo bellman_ford step 4947 current loss 0.092704, current_train_items 158336.
I0304 19:30:28.295440 22502662377600 run.py:483] Algo bellman_ford step 4948 current loss 0.103523, current_train_items 158368.
I0304 19:30:28.330981 22502662377600 run.py:483] Algo bellman_ford step 4949 current loss 0.100968, current_train_items 158400.
I0304 19:30:28.350482 22502662377600 run.py:483] Algo bellman_ford step 4950 current loss 0.011690, current_train_items 158432.
I0304 19:30:28.358586 22502662377600 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0304 19:30:28.358688 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:30:28.375075 22502662377600 run.py:483] Algo bellman_ford step 4951 current loss 0.012900, current_train_items 158464.
I0304 19:30:28.398840 22502662377600 run.py:483] Algo bellman_ford step 4952 current loss 0.037561, current_train_items 158496.
I0304 19:30:28.430760 22502662377600 run.py:483] Algo bellman_ford step 4953 current loss 0.083987, current_train_items 158528.
I0304 19:30:28.466808 22502662377600 run.py:483] Algo bellman_ford step 4954 current loss 0.074546, current_train_items 158560.
I0304 19:30:28.486890 22502662377600 run.py:483] Algo bellman_ford step 4955 current loss 0.009541, current_train_items 158592.
I0304 19:30:28.503012 22502662377600 run.py:483] Algo bellman_ford step 4956 current loss 0.020550, current_train_items 158624.
I0304 19:30:28.528473 22502662377600 run.py:483] Algo bellman_ford step 4957 current loss 0.091355, current_train_items 158656.
I0304 19:30:28.560678 22502662377600 run.py:483] Algo bellman_ford step 4958 current loss 0.100313, current_train_items 158688.
I0304 19:30:28.592508 22502662377600 run.py:483] Algo bellman_ford step 4959 current loss 0.080170, current_train_items 158720.
I0304 19:30:28.612344 22502662377600 run.py:483] Algo bellman_ford step 4960 current loss 0.014570, current_train_items 158752.
I0304 19:30:28.628787 22502662377600 run.py:483] Algo bellman_ford step 4961 current loss 0.018860, current_train_items 158784.
I0304 19:30:28.652891 22502662377600 run.py:483] Algo bellman_ford step 4962 current loss 0.035807, current_train_items 158816.
I0304 19:30:28.683205 22502662377600 run.py:483] Algo bellman_ford step 4963 current loss 0.042502, current_train_items 158848.
I0304 19:30:28.716982 22502662377600 run.py:483] Algo bellman_ford step 4964 current loss 0.082658, current_train_items 158880.
I0304 19:30:28.736729 22502662377600 run.py:483] Algo bellman_ford step 4965 current loss 0.002789, current_train_items 158912.
I0304 19:30:28.752942 22502662377600 run.py:483] Algo bellman_ford step 4966 current loss 0.050050, current_train_items 158944.
I0304 19:30:28.778054 22502662377600 run.py:483] Algo bellman_ford step 4967 current loss 0.073922, current_train_items 158976.
I0304 19:30:28.810514 22502662377600 run.py:483] Algo bellman_ford step 4968 current loss 0.085496, current_train_items 159008.
I0304 19:30:28.842136 22502662377600 run.py:483] Algo bellman_ford step 4969 current loss 0.091037, current_train_items 159040.
I0304 19:30:28.862498 22502662377600 run.py:483] Algo bellman_ford step 4970 current loss 0.006597, current_train_items 159072.
I0304 19:30:28.878662 22502662377600 run.py:483] Algo bellman_ford step 4971 current loss 0.091879, current_train_items 159104.
I0304 19:30:28.902757 22502662377600 run.py:483] Algo bellman_ford step 4972 current loss 0.077648, current_train_items 159136.
I0304 19:30:28.934669 22502662377600 run.py:483] Algo bellman_ford step 4973 current loss 0.079617, current_train_items 159168.
I0304 19:30:28.968262 22502662377600 run.py:483] Algo bellman_ford step 4974 current loss 0.092727, current_train_items 159200.
I0304 19:30:28.988633 22502662377600 run.py:483] Algo bellman_ford step 4975 current loss 0.011192, current_train_items 159232.
I0304 19:30:29.005275 22502662377600 run.py:483] Algo bellman_ford step 4976 current loss 0.029467, current_train_items 159264.
I0304 19:30:29.030025 22502662377600 run.py:483] Algo bellman_ford step 4977 current loss 0.091162, current_train_items 159296.
I0304 19:30:29.062052 22502662377600 run.py:483] Algo bellman_ford step 4978 current loss 0.080220, current_train_items 159328.
I0304 19:30:29.096735 22502662377600 run.py:483] Algo bellman_ford step 4979 current loss 0.086873, current_train_items 159360.
I0304 19:30:29.116725 22502662377600 run.py:483] Algo bellman_ford step 4980 current loss 0.015762, current_train_items 159392.
I0304 19:30:29.133089 22502662377600 run.py:483] Algo bellman_ford step 4981 current loss 0.026140, current_train_items 159424.
I0304 19:30:29.157939 22502662377600 run.py:483] Algo bellman_ford step 4982 current loss 0.077051, current_train_items 159456.
I0304 19:30:29.187679 22502662377600 run.py:483] Algo bellman_ford step 4983 current loss 0.053616, current_train_items 159488.
I0304 19:30:29.220977 22502662377600 run.py:483] Algo bellman_ford step 4984 current loss 0.061435, current_train_items 159520.
I0304 19:30:29.241320 22502662377600 run.py:483] Algo bellman_ford step 4985 current loss 0.007989, current_train_items 159552.
I0304 19:30:29.257718 22502662377600 run.py:483] Algo bellman_ford step 4986 current loss 0.038443, current_train_items 159584.
I0304 19:30:29.280051 22502662377600 run.py:483] Algo bellman_ford step 4987 current loss 0.073525, current_train_items 159616.
I0304 19:30:29.310677 22502662377600 run.py:483] Algo bellman_ford step 4988 current loss 0.098605, current_train_items 159648.
I0304 19:30:29.347055 22502662377600 run.py:483] Algo bellman_ford step 4989 current loss 0.116861, current_train_items 159680.
I0304 19:30:29.367198 22502662377600 run.py:483] Algo bellman_ford step 4990 current loss 0.012366, current_train_items 159712.
I0304 19:30:29.383216 22502662377600 run.py:483] Algo bellman_ford step 4991 current loss 0.035873, current_train_items 159744.
I0304 19:30:29.407482 22502662377600 run.py:483] Algo bellman_ford step 4992 current loss 0.144854, current_train_items 159776.
I0304 19:30:29.438529 22502662377600 run.py:483] Algo bellman_ford step 4993 current loss 0.086459, current_train_items 159808.
I0304 19:30:29.472964 22502662377600 run.py:483] Algo bellman_ford step 4994 current loss 0.099954, current_train_items 159840.
I0304 19:30:29.492668 22502662377600 run.py:483] Algo bellman_ford step 4995 current loss 0.011301, current_train_items 159872.
I0304 19:30:29.509174 22502662377600 run.py:483] Algo bellman_ford step 4996 current loss 0.023365, current_train_items 159904.
I0304 19:30:29.533092 22502662377600 run.py:483] Algo bellman_ford step 4997 current loss 0.095403, current_train_items 159936.
I0304 19:30:29.564676 22502662377600 run.py:483] Algo bellman_ford step 4998 current loss 0.120776, current_train_items 159968.
I0304 19:30:29.599476 22502662377600 run.py:483] Algo bellman_ford step 4999 current loss 0.168511, current_train_items 160000.
I0304 19:30:29.619910 22502662377600 run.py:483] Algo bellman_ford step 5000 current loss 0.011625, current_train_items 160032.
I0304 19:30:29.628194 22502662377600 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0304 19:30:29.628297 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:30:29.645298 22502662377600 run.py:483] Algo bellman_ford step 5001 current loss 0.062452, current_train_items 160064.
I0304 19:30:29.669593 22502662377600 run.py:483] Algo bellman_ford step 5002 current loss 0.132765, current_train_items 160096.
I0304 19:30:29.700057 22502662377600 run.py:483] Algo bellman_ford step 5003 current loss 0.028335, current_train_items 160128.
I0304 19:30:29.733529 22502662377600 run.py:483] Algo bellman_ford step 5004 current loss 0.080712, current_train_items 160160.
I0304 19:30:29.753765 22502662377600 run.py:483] Algo bellman_ford step 5005 current loss 0.004979, current_train_items 160192.
I0304 19:30:29.769820 22502662377600 run.py:483] Algo bellman_ford step 5006 current loss 0.028272, current_train_items 160224.
I0304 19:30:29.794517 22502662377600 run.py:483] Algo bellman_ford step 5007 current loss 0.161090, current_train_items 160256.
I0304 19:30:29.827703 22502662377600 run.py:483] Algo bellman_ford step 5008 current loss 0.236600, current_train_items 160288.
I0304 19:30:29.859794 22502662377600 run.py:483] Algo bellman_ford step 5009 current loss 0.083484, current_train_items 160320.
I0304 19:30:29.879875 22502662377600 run.py:483] Algo bellman_ford step 5010 current loss 0.021058, current_train_items 160352.
I0304 19:30:29.895843 22502662377600 run.py:483] Algo bellman_ford step 5011 current loss 0.019727, current_train_items 160384.
I0304 19:30:29.920266 22502662377600 run.py:483] Algo bellman_ford step 5012 current loss 0.050352, current_train_items 160416.
I0304 19:30:29.951157 22502662377600 run.py:483] Algo bellman_ford step 5013 current loss 0.159773, current_train_items 160448.
I0304 19:30:29.986408 22502662377600 run.py:483] Algo bellman_ford step 5014 current loss 0.205047, current_train_items 160480.
I0304 19:30:30.006403 22502662377600 run.py:483] Algo bellman_ford step 5015 current loss 0.030973, current_train_items 160512.
I0304 19:30:30.023206 22502662377600 run.py:483] Algo bellman_ford step 5016 current loss 0.051938, current_train_items 160544.
I0304 19:30:30.046932 22502662377600 run.py:483] Algo bellman_ford step 5017 current loss 0.049552, current_train_items 160576.
I0304 19:30:30.078450 22502662377600 run.py:483] Algo bellman_ford step 5018 current loss 0.149590, current_train_items 160608.
I0304 19:30:30.114427 22502662377600 run.py:483] Algo bellman_ford step 5019 current loss 0.228202, current_train_items 160640.
I0304 19:30:30.134396 22502662377600 run.py:483] Algo bellman_ford step 5020 current loss 0.011425, current_train_items 160672.
I0304 19:30:30.150378 22502662377600 run.py:483] Algo bellman_ford step 5021 current loss 0.043870, current_train_items 160704.
I0304 19:30:30.175300 22502662377600 run.py:483] Algo bellman_ford step 5022 current loss 0.100262, current_train_items 160736.
I0304 19:30:30.206791 22502662377600 run.py:483] Algo bellman_ford step 5023 current loss 0.107401, current_train_items 160768.
I0304 19:30:30.241084 22502662377600 run.py:483] Algo bellman_ford step 5024 current loss 0.089323, current_train_items 160800.
I0304 19:30:30.260814 22502662377600 run.py:483] Algo bellman_ford step 5025 current loss 0.009379, current_train_items 160832.
I0304 19:30:30.276735 22502662377600 run.py:483] Algo bellman_ford step 5026 current loss 0.067873, current_train_items 160864.
I0304 19:30:30.300191 22502662377600 run.py:483] Algo bellman_ford step 5027 current loss 0.101585, current_train_items 160896.
I0304 19:30:30.330199 22502662377600 run.py:483] Algo bellman_ford step 5028 current loss 0.117338, current_train_items 160928.
I0304 19:30:30.363723 22502662377600 run.py:483] Algo bellman_ford step 5029 current loss 0.181888, current_train_items 160960.
I0304 19:30:30.383649 22502662377600 run.py:483] Algo bellman_ford step 5030 current loss 0.024412, current_train_items 160992.
I0304 19:30:30.399927 22502662377600 run.py:483] Algo bellman_ford step 5031 current loss 0.041547, current_train_items 161024.
I0304 19:30:30.424512 22502662377600 run.py:483] Algo bellman_ford step 5032 current loss 0.100460, current_train_items 161056.
I0304 19:30:30.454908 22502662377600 run.py:483] Algo bellman_ford step 5033 current loss 0.071334, current_train_items 161088.
I0304 19:30:30.489389 22502662377600 run.py:483] Algo bellman_ford step 5034 current loss 0.229597, current_train_items 161120.
I0304 19:30:30.509467 22502662377600 run.py:483] Algo bellman_ford step 5035 current loss 0.011749, current_train_items 161152.
I0304 19:30:30.525803 22502662377600 run.py:483] Algo bellman_ford step 5036 current loss 0.030397, current_train_items 161184.
I0304 19:30:30.550024 22502662377600 run.py:483] Algo bellman_ford step 5037 current loss 0.102222, current_train_items 161216.
I0304 19:30:30.583137 22502662377600 run.py:483] Algo bellman_ford step 5038 current loss 0.142845, current_train_items 161248.
I0304 19:30:30.616977 22502662377600 run.py:483] Algo bellman_ford step 5039 current loss 0.062052, current_train_items 161280.
I0304 19:30:30.636666 22502662377600 run.py:483] Algo bellman_ford step 5040 current loss 0.005598, current_train_items 161312.
I0304 19:30:30.652342 22502662377600 run.py:483] Algo bellman_ford step 5041 current loss 0.011884, current_train_items 161344.
I0304 19:30:30.677359 22502662377600 run.py:483] Algo bellman_ford step 5042 current loss 0.092183, current_train_items 161376.
I0304 19:30:30.707851 22502662377600 run.py:483] Algo bellman_ford step 5043 current loss 0.069142, current_train_items 161408.
I0304 19:30:30.741484 22502662377600 run.py:483] Algo bellman_ford step 5044 current loss 0.094266, current_train_items 161440.
I0304 19:30:30.761024 22502662377600 run.py:483] Algo bellman_ford step 5045 current loss 0.005229, current_train_items 161472.
I0304 19:30:30.777239 22502662377600 run.py:483] Algo bellman_ford step 5046 current loss 0.037093, current_train_items 161504.
I0304 19:30:30.800158 22502662377600 run.py:483] Algo bellman_ford step 5047 current loss 0.065706, current_train_items 161536.
I0304 19:30:30.831452 22502662377600 run.py:483] Algo bellman_ford step 5048 current loss 0.124292, current_train_items 161568.
I0304 19:30:30.865938 22502662377600 run.py:483] Algo bellman_ford step 5049 current loss 0.136576, current_train_items 161600.
I0304 19:30:30.885800 22502662377600 run.py:483] Algo bellman_ford step 5050 current loss 0.006211, current_train_items 161632.
I0304 19:30:30.893840 22502662377600 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0304 19:30:30.893945 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:30:30.910612 22502662377600 run.py:483] Algo bellman_ford step 5051 current loss 0.031242, current_train_items 161664.
I0304 19:30:30.935976 22502662377600 run.py:483] Algo bellman_ford step 5052 current loss 0.076691, current_train_items 161696.
I0304 19:30:30.967955 22502662377600 run.py:483] Algo bellman_ford step 5053 current loss 0.098883, current_train_items 161728.
I0304 19:30:31.000764 22502662377600 run.py:483] Algo bellman_ford step 5054 current loss 0.128331, current_train_items 161760.
I0304 19:30:31.020812 22502662377600 run.py:483] Algo bellman_ford step 5055 current loss 0.008516, current_train_items 161792.
I0304 19:30:31.037648 22502662377600 run.py:483] Algo bellman_ford step 5056 current loss 0.058225, current_train_items 161824.
I0304 19:30:31.062777 22502662377600 run.py:483] Algo bellman_ford step 5057 current loss 0.054894, current_train_items 161856.
I0304 19:30:31.093637 22502662377600 run.py:483] Algo bellman_ford step 5058 current loss 0.127100, current_train_items 161888.
I0304 19:30:31.127889 22502662377600 run.py:483] Algo bellman_ford step 5059 current loss 0.070760, current_train_items 161920.
I0304 19:30:31.147889 22502662377600 run.py:483] Algo bellman_ford step 5060 current loss 0.051732, current_train_items 161952.
I0304 19:30:31.164160 22502662377600 run.py:483] Algo bellman_ford step 5061 current loss 0.035970, current_train_items 161984.
I0304 19:30:31.186429 22502662377600 run.py:483] Algo bellman_ford step 5062 current loss 0.031650, current_train_items 162016.
I0304 19:30:31.218224 22502662377600 run.py:483] Algo bellman_ford step 5063 current loss 0.088315, current_train_items 162048.
I0304 19:30:31.250073 22502662377600 run.py:483] Algo bellman_ford step 5064 current loss 0.106749, current_train_items 162080.
I0304 19:30:31.269822 22502662377600 run.py:483] Algo bellman_ford step 5065 current loss 0.014075, current_train_items 162112.
I0304 19:30:31.285991 22502662377600 run.py:483] Algo bellman_ford step 5066 current loss 0.034578, current_train_items 162144.
I0304 19:30:31.310889 22502662377600 run.py:483] Algo bellman_ford step 5067 current loss 0.125026, current_train_items 162176.
I0304 19:30:31.342231 22502662377600 run.py:483] Algo bellman_ford step 5068 current loss 0.126109, current_train_items 162208.
I0304 19:30:31.377239 22502662377600 run.py:483] Algo bellman_ford step 5069 current loss 0.120047, current_train_items 162240.
I0304 19:30:31.397400 22502662377600 run.py:483] Algo bellman_ford step 5070 current loss 0.009098, current_train_items 162272.
I0304 19:30:31.414131 22502662377600 run.py:483] Algo bellman_ford step 5071 current loss 0.065614, current_train_items 162304.
I0304 19:30:31.437033 22502662377600 run.py:483] Algo bellman_ford step 5072 current loss 0.073480, current_train_items 162336.
I0304 19:30:31.469222 22502662377600 run.py:483] Algo bellman_ford step 5073 current loss 0.124064, current_train_items 162368.
I0304 19:30:31.500738 22502662377600 run.py:483] Algo bellman_ford step 5074 current loss 0.086471, current_train_items 162400.
I0304 19:30:31.520799 22502662377600 run.py:483] Algo bellman_ford step 5075 current loss 0.007667, current_train_items 162432.
I0304 19:30:31.536983 22502662377600 run.py:483] Algo bellman_ford step 5076 current loss 0.038473, current_train_items 162464.
I0304 19:30:31.560661 22502662377600 run.py:483] Algo bellman_ford step 5077 current loss 0.055950, current_train_items 162496.
I0304 19:30:31.591782 22502662377600 run.py:483] Algo bellman_ford step 5078 current loss 0.090911, current_train_items 162528.
I0304 19:30:31.624634 22502662377600 run.py:483] Algo bellman_ford step 5079 current loss 0.110298, current_train_items 162560.
I0304 19:30:31.644472 22502662377600 run.py:483] Algo bellman_ford step 5080 current loss 0.023426, current_train_items 162592.
I0304 19:30:31.660292 22502662377600 run.py:483] Algo bellman_ford step 5081 current loss 0.038105, current_train_items 162624.
I0304 19:30:31.684497 22502662377600 run.py:483] Algo bellman_ford step 5082 current loss 0.065691, current_train_items 162656.
I0304 19:30:31.716387 22502662377600 run.py:483] Algo bellman_ford step 5083 current loss 0.144950, current_train_items 162688.
I0304 19:30:31.750016 22502662377600 run.py:483] Algo bellman_ford step 5084 current loss 0.126347, current_train_items 162720.
I0304 19:30:31.770273 22502662377600 run.py:483] Algo bellman_ford step 5085 current loss 0.010477, current_train_items 162752.
I0304 19:30:31.786313 22502662377600 run.py:483] Algo bellman_ford step 5086 current loss 0.035315, current_train_items 162784.
I0304 19:30:31.810259 22502662377600 run.py:483] Algo bellman_ford step 5087 current loss 0.125640, current_train_items 162816.
I0304 19:30:31.841966 22502662377600 run.py:483] Algo bellman_ford step 5088 current loss 0.088836, current_train_items 162848.
I0304 19:30:31.876832 22502662377600 run.py:483] Algo bellman_ford step 5089 current loss 0.133648, current_train_items 162880.
I0304 19:30:31.896884 22502662377600 run.py:483] Algo bellman_ford step 5090 current loss 0.013871, current_train_items 162912.
I0304 19:30:31.913314 22502662377600 run.py:483] Algo bellman_ford step 5091 current loss 0.030578, current_train_items 162944.
I0304 19:30:31.935716 22502662377600 run.py:483] Algo bellman_ford step 5092 current loss 0.121968, current_train_items 162976.
I0304 19:30:31.966417 22502662377600 run.py:483] Algo bellman_ford step 5093 current loss 0.095439, current_train_items 163008.
I0304 19:30:31.999313 22502662377600 run.py:483] Algo bellman_ford step 5094 current loss 0.110809, current_train_items 163040.
I0304 19:30:32.019095 22502662377600 run.py:483] Algo bellman_ford step 5095 current loss 0.008616, current_train_items 163072.
I0304 19:30:32.035343 22502662377600 run.py:483] Algo bellman_ford step 5096 current loss 0.019746, current_train_items 163104.
I0304 19:30:32.060111 22502662377600 run.py:483] Algo bellman_ford step 5097 current loss 0.066475, current_train_items 163136.
I0304 19:30:32.092707 22502662377600 run.py:483] Algo bellman_ford step 5098 current loss 0.076569, current_train_items 163168.
I0304 19:30:32.127999 22502662377600 run.py:483] Algo bellman_ford step 5099 current loss 0.096953, current_train_items 163200.
I0304 19:30:32.147872 22502662377600 run.py:483] Algo bellman_ford step 5100 current loss 0.010623, current_train_items 163232.
I0304 19:30:32.155655 22502662377600 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0304 19:30:32.155761 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:30:32.172175 22502662377600 run.py:483] Algo bellman_ford step 5101 current loss 0.067832, current_train_items 163264.
I0304 19:30:32.194382 22502662377600 run.py:483] Algo bellman_ford step 5102 current loss 0.030723, current_train_items 163296.
I0304 19:30:32.226078 22502662377600 run.py:483] Algo bellman_ford step 5103 current loss 0.118680, current_train_items 163328.
I0304 19:30:32.260946 22502662377600 run.py:483] Algo bellman_ford step 5104 current loss 0.120897, current_train_items 163360.
I0304 19:30:32.280775 22502662377600 run.py:483] Algo bellman_ford step 5105 current loss 0.009622, current_train_items 163392.
I0304 19:30:32.296997 22502662377600 run.py:483] Algo bellman_ford step 5106 current loss 0.049797, current_train_items 163424.
I0304 19:30:32.320738 22502662377600 run.py:483] Algo bellman_ford step 5107 current loss 0.084407, current_train_items 163456.
I0304 19:30:32.351163 22502662377600 run.py:483] Algo bellman_ford step 5108 current loss 0.062091, current_train_items 163488.
I0304 19:30:32.385236 22502662377600 run.py:483] Algo bellman_ford step 5109 current loss 0.121286, current_train_items 163520.
I0304 19:30:32.405144 22502662377600 run.py:483] Algo bellman_ford step 5110 current loss 0.007595, current_train_items 163552.
I0304 19:30:32.421393 22502662377600 run.py:483] Algo bellman_ford step 5111 current loss 0.023477, current_train_items 163584.
I0304 19:30:32.444752 22502662377600 run.py:483] Algo bellman_ford step 5112 current loss 0.032752, current_train_items 163616.
I0304 19:30:32.475312 22502662377600 run.py:483] Algo bellman_ford step 5113 current loss 0.070861, current_train_items 163648.
I0304 19:30:32.508214 22502662377600 run.py:483] Algo bellman_ford step 5114 current loss 0.074689, current_train_items 163680.
I0304 19:30:32.527884 22502662377600 run.py:483] Algo bellman_ford step 5115 current loss 0.011416, current_train_items 163712.
I0304 19:30:32.544236 22502662377600 run.py:483] Algo bellman_ford step 5116 current loss 0.028136, current_train_items 163744.
I0304 19:30:32.568796 22502662377600 run.py:483] Algo bellman_ford step 5117 current loss 0.108903, current_train_items 163776.
I0304 19:30:32.598864 22502662377600 run.py:483] Algo bellman_ford step 5118 current loss 0.082347, current_train_items 163808.
I0304 19:30:32.633005 22502662377600 run.py:483] Algo bellman_ford step 5119 current loss 0.081141, current_train_items 163840.
I0304 19:30:32.652550 22502662377600 run.py:483] Algo bellman_ford step 5120 current loss 0.009694, current_train_items 163872.
I0304 19:30:32.668313 22502662377600 run.py:483] Algo bellman_ford step 5121 current loss 0.019623, current_train_items 163904.
I0304 19:30:32.691744 22502662377600 run.py:483] Algo bellman_ford step 5122 current loss 0.067192, current_train_items 163936.
I0304 19:30:32.721826 22502662377600 run.py:483] Algo bellman_ford step 5123 current loss 0.049321, current_train_items 163968.
I0304 19:30:32.755680 22502662377600 run.py:483] Algo bellman_ford step 5124 current loss 0.111471, current_train_items 164000.
I0304 19:30:32.775087 22502662377600 run.py:483] Algo bellman_ford step 5125 current loss 0.005301, current_train_items 164032.
I0304 19:30:32.791206 22502662377600 run.py:483] Algo bellman_ford step 5126 current loss 0.072090, current_train_items 164064.
I0304 19:30:32.815441 22502662377600 run.py:483] Algo bellman_ford step 5127 current loss 0.084624, current_train_items 164096.
I0304 19:30:32.846949 22502662377600 run.py:483] Algo bellman_ford step 5128 current loss 0.068927, current_train_items 164128.
I0304 19:30:32.881957 22502662377600 run.py:483] Algo bellman_ford step 5129 current loss 0.082789, current_train_items 164160.
I0304 19:30:32.901359 22502662377600 run.py:483] Algo bellman_ford step 5130 current loss 0.003476, current_train_items 164192.
I0304 19:30:32.917717 22502662377600 run.py:483] Algo bellman_ford step 5131 current loss 0.033353, current_train_items 164224.
I0304 19:30:32.940510 22502662377600 run.py:483] Algo bellman_ford step 5132 current loss 0.061763, current_train_items 164256.
I0304 19:30:32.972134 22502662377600 run.py:483] Algo bellman_ford step 5133 current loss 0.060709, current_train_items 164288.
I0304 19:30:33.005168 22502662377600 run.py:483] Algo bellman_ford step 5134 current loss 0.066300, current_train_items 164320.
I0304 19:30:33.024957 22502662377600 run.py:483] Algo bellman_ford step 5135 current loss 0.009157, current_train_items 164352.
I0304 19:30:33.041022 22502662377600 run.py:483] Algo bellman_ford step 5136 current loss 0.013663, current_train_items 164384.
I0304 19:30:33.064933 22502662377600 run.py:483] Algo bellman_ford step 5137 current loss 0.067258, current_train_items 164416.
I0304 19:30:33.096162 22502662377600 run.py:483] Algo bellman_ford step 5138 current loss 0.053620, current_train_items 164448.
I0304 19:30:33.130544 22502662377600 run.py:483] Algo bellman_ford step 5139 current loss 0.085536, current_train_items 164480.
I0304 19:30:33.150353 22502662377600 run.py:483] Algo bellman_ford step 5140 current loss 0.004152, current_train_items 164512.
I0304 19:30:33.166777 22502662377600 run.py:483] Algo bellman_ford step 5141 current loss 0.026977, current_train_items 164544.
I0304 19:30:33.191227 22502662377600 run.py:483] Algo bellman_ford step 5142 current loss 0.061895, current_train_items 164576.
I0304 19:30:33.222312 22502662377600 run.py:483] Algo bellman_ford step 5143 current loss 0.123689, current_train_items 164608.
I0304 19:30:33.256036 22502662377600 run.py:483] Algo bellman_ford step 5144 current loss 0.078597, current_train_items 164640.
I0304 19:30:33.275455 22502662377600 run.py:483] Algo bellman_ford step 5145 current loss 0.009771, current_train_items 164672.
I0304 19:30:33.291405 22502662377600 run.py:483] Algo bellman_ford step 5146 current loss 0.036455, current_train_items 164704.
I0304 19:30:33.313829 22502662377600 run.py:483] Algo bellman_ford step 5147 current loss 0.067765, current_train_items 164736.
I0304 19:30:33.344292 22502662377600 run.py:483] Algo bellman_ford step 5148 current loss 0.105519, current_train_items 164768.
I0304 19:30:33.376974 22502662377600 run.py:483] Algo bellman_ford step 5149 current loss 0.101536, current_train_items 164800.
I0304 19:30:33.396343 22502662377600 run.py:483] Algo bellman_ford step 5150 current loss 0.008034, current_train_items 164832.
I0304 19:30:33.404823 22502662377600 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0304 19:30:33.404928 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:30:33.421796 22502662377600 run.py:483] Algo bellman_ford step 5151 current loss 0.036576, current_train_items 164864.
I0304 19:30:33.445683 22502662377600 run.py:483] Algo bellman_ford step 5152 current loss 0.026655, current_train_items 164896.
I0304 19:30:33.476128 22502662377600 run.py:483] Algo bellman_ford step 5153 current loss 0.110816, current_train_items 164928.
I0304 19:30:33.509601 22502662377600 run.py:483] Algo bellman_ford step 5154 current loss 0.087016, current_train_items 164960.
I0304 19:30:33.529350 22502662377600 run.py:483] Algo bellman_ford step 5155 current loss 0.025411, current_train_items 164992.
I0304 19:30:33.545217 22502662377600 run.py:483] Algo bellman_ford step 5156 current loss 0.019584, current_train_items 165024.
I0304 19:30:33.570107 22502662377600 run.py:483] Algo bellman_ford step 5157 current loss 0.055587, current_train_items 165056.
I0304 19:30:33.603096 22502662377600 run.py:483] Algo bellman_ford step 5158 current loss 0.121546, current_train_items 165088.
I0304 19:30:33.636249 22502662377600 run.py:483] Algo bellman_ford step 5159 current loss 0.082139, current_train_items 165120.
I0304 19:30:33.656512 22502662377600 run.py:483] Algo bellman_ford step 5160 current loss 0.008490, current_train_items 165152.
I0304 19:30:33.672570 22502662377600 run.py:483] Algo bellman_ford step 5161 current loss 0.020061, current_train_items 165184.
I0304 19:30:33.697320 22502662377600 run.py:483] Algo bellman_ford step 5162 current loss 0.084952, current_train_items 165216.
I0304 19:30:33.729907 22502662377600 run.py:483] Algo bellman_ford step 5163 current loss 0.060903, current_train_items 165248.
I0304 19:30:33.762259 22502662377600 run.py:483] Algo bellman_ford step 5164 current loss 0.069860, current_train_items 165280.
I0304 19:30:33.781606 22502662377600 run.py:483] Algo bellman_ford step 5165 current loss 0.004606, current_train_items 165312.
I0304 19:30:33.797614 22502662377600 run.py:483] Algo bellman_ford step 5166 current loss 0.020948, current_train_items 165344.
I0304 19:30:33.820636 22502662377600 run.py:483] Algo bellman_ford step 5167 current loss 0.080440, current_train_items 165376.
I0304 19:30:33.852424 22502662377600 run.py:483] Algo bellman_ford step 5168 current loss 0.057096, current_train_items 165408.
I0304 19:30:33.887185 22502662377600 run.py:483] Algo bellman_ford step 5169 current loss 0.095552, current_train_items 165440.
I0304 19:30:33.907268 22502662377600 run.py:483] Algo bellman_ford step 5170 current loss 0.006746, current_train_items 165472.
I0304 19:30:33.923617 22502662377600 run.py:483] Algo bellman_ford step 5171 current loss 0.030779, current_train_items 165504.
I0304 19:30:33.947790 22502662377600 run.py:483] Algo bellman_ford step 5172 current loss 0.083394, current_train_items 165536.
I0304 19:30:33.980180 22502662377600 run.py:483] Algo bellman_ford step 5173 current loss 0.115075, current_train_items 165568.
I0304 19:30:34.013413 22502662377600 run.py:483] Algo bellman_ford step 5174 current loss 0.092481, current_train_items 165600.
I0304 19:30:34.033114 22502662377600 run.py:483] Algo bellman_ford step 5175 current loss 0.005273, current_train_items 165632.
I0304 19:30:34.049490 22502662377600 run.py:483] Algo bellman_ford step 5176 current loss 0.020342, current_train_items 165664.
I0304 19:30:34.073392 22502662377600 run.py:483] Algo bellman_ford step 5177 current loss 0.087325, current_train_items 165696.
I0304 19:30:34.104358 22502662377600 run.py:483] Algo bellman_ford step 5178 current loss 0.105826, current_train_items 165728.
I0304 19:30:34.137308 22502662377600 run.py:483] Algo bellman_ford step 5179 current loss 0.110195, current_train_items 165760.
I0304 19:30:34.157047 22502662377600 run.py:483] Algo bellman_ford step 5180 current loss 0.007617, current_train_items 165792.
I0304 19:30:34.173150 22502662377600 run.py:483] Algo bellman_ford step 5181 current loss 0.060873, current_train_items 165824.
I0304 19:30:34.197146 22502662377600 run.py:483] Algo bellman_ford step 5182 current loss 0.102841, current_train_items 165856.
I0304 19:30:34.228439 22502662377600 run.py:483] Algo bellman_ford step 5183 current loss 0.112006, current_train_items 165888.
I0304 19:30:34.260962 22502662377600 run.py:483] Algo bellman_ford step 5184 current loss 0.063030, current_train_items 165920.
I0304 19:30:34.280947 22502662377600 run.py:483] Algo bellman_ford step 5185 current loss 0.012418, current_train_items 165952.
I0304 19:30:34.297843 22502662377600 run.py:483] Algo bellman_ford step 5186 current loss 0.064020, current_train_items 165984.
I0304 19:30:34.320590 22502662377600 run.py:483] Algo bellman_ford step 5187 current loss 0.087095, current_train_items 166016.
I0304 19:30:34.352770 22502662377600 run.py:483] Algo bellman_ford step 5188 current loss 0.143513, current_train_items 166048.
I0304 19:30:34.385569 22502662377600 run.py:483] Algo bellman_ford step 5189 current loss 0.112526, current_train_items 166080.
I0304 19:30:34.406059 22502662377600 run.py:483] Algo bellman_ford step 5190 current loss 0.006891, current_train_items 166112.
I0304 19:30:34.422240 22502662377600 run.py:483] Algo bellman_ford step 5191 current loss 0.029358, current_train_items 166144.
I0304 19:30:34.446391 22502662377600 run.py:483] Algo bellman_ford step 5192 current loss 0.143221, current_train_items 166176.
I0304 19:30:34.476655 22502662377600 run.py:483] Algo bellman_ford step 5193 current loss 0.121965, current_train_items 166208.
I0304 19:30:34.510843 22502662377600 run.py:483] Algo bellman_ford step 5194 current loss 0.150475, current_train_items 166240.
I0304 19:30:34.530802 22502662377600 run.py:483] Algo bellman_ford step 5195 current loss 0.005166, current_train_items 166272.
I0304 19:30:34.547047 22502662377600 run.py:483] Algo bellman_ford step 5196 current loss 0.042042, current_train_items 166304.
I0304 19:30:34.571279 22502662377600 run.py:483] Algo bellman_ford step 5197 current loss 0.060596, current_train_items 166336.
I0304 19:30:34.603851 22502662377600 run.py:483] Algo bellman_ford step 5198 current loss 0.124588, current_train_items 166368.
I0304 19:30:34.636850 22502662377600 run.py:483] Algo bellman_ford step 5199 current loss 0.106155, current_train_items 166400.
I0304 19:30:34.656727 22502662377600 run.py:483] Algo bellman_ford step 5200 current loss 0.005792, current_train_items 166432.
I0304 19:30:34.664650 22502662377600 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0304 19:30:34.664755 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:30:34.681660 22502662377600 run.py:483] Algo bellman_ford step 5201 current loss 0.030068, current_train_items 166464.
I0304 19:30:34.706619 22502662377600 run.py:483] Algo bellman_ford step 5202 current loss 0.046396, current_train_items 166496.
I0304 19:30:34.739509 22502662377600 run.py:483] Algo bellman_ford step 5203 current loss 0.070639, current_train_items 166528.
I0304 19:30:34.774835 22502662377600 run.py:483] Algo bellman_ford step 5204 current loss 0.062584, current_train_items 166560.
I0304 19:30:34.794915 22502662377600 run.py:483] Algo bellman_ford step 5205 current loss 0.013662, current_train_items 166592.
I0304 19:30:34.810619 22502662377600 run.py:483] Algo bellman_ford step 5206 current loss 0.047840, current_train_items 166624.
I0304 19:30:34.834535 22502662377600 run.py:483] Algo bellman_ford step 5207 current loss 0.118414, current_train_items 166656.
I0304 19:30:34.864627 22502662377600 run.py:483] Algo bellman_ford step 5208 current loss 0.076113, current_train_items 166688.
I0304 19:30:34.900706 22502662377600 run.py:483] Algo bellman_ford step 5209 current loss 0.119134, current_train_items 166720.
I0304 19:30:34.920739 22502662377600 run.py:483] Algo bellman_ford step 5210 current loss 0.062657, current_train_items 166752.
I0304 19:30:34.936397 22502662377600 run.py:483] Algo bellman_ford step 5211 current loss 0.025332, current_train_items 166784.
I0304 19:30:34.960636 22502662377600 run.py:483] Algo bellman_ford step 5212 current loss 0.058501, current_train_items 166816.
I0304 19:30:34.993850 22502662377600 run.py:483] Algo bellman_ford step 5213 current loss 0.090087, current_train_items 166848.
I0304 19:30:35.026989 22502662377600 run.py:483] Algo bellman_ford step 5214 current loss 0.086428, current_train_items 166880.
I0304 19:30:35.046394 22502662377600 run.py:483] Algo bellman_ford step 5215 current loss 0.007257, current_train_items 166912.
I0304 19:30:35.062767 22502662377600 run.py:483] Algo bellman_ford step 5216 current loss 0.062426, current_train_items 166944.
I0304 19:30:35.086564 22502662377600 run.py:483] Algo bellman_ford step 5217 current loss 0.028918, current_train_items 166976.
I0304 19:30:35.116882 22502662377600 run.py:483] Algo bellman_ford step 5218 current loss 0.050050, current_train_items 167008.
I0304 19:30:35.150234 22502662377600 run.py:483] Algo bellman_ford step 5219 current loss 0.079809, current_train_items 167040.
I0304 19:30:35.169855 22502662377600 run.py:483] Algo bellman_ford step 5220 current loss 0.009570, current_train_items 167072.
I0304 19:30:35.185897 22502662377600 run.py:483] Algo bellman_ford step 5221 current loss 0.021479, current_train_items 167104.
I0304 19:30:35.209055 22502662377600 run.py:483] Algo bellman_ford step 5222 current loss 0.035003, current_train_items 167136.
I0304 19:30:35.241325 22502662377600 run.py:483] Algo bellman_ford step 5223 current loss 0.085216, current_train_items 167168.
I0304 19:30:35.274218 22502662377600 run.py:483] Algo bellman_ford step 5224 current loss 0.060379, current_train_items 167200.
I0304 19:30:35.293902 22502662377600 run.py:483] Algo bellman_ford step 5225 current loss 0.007962, current_train_items 167232.
I0304 19:30:35.309914 22502662377600 run.py:483] Algo bellman_ford step 5226 current loss 0.024786, current_train_items 167264.
I0304 19:30:35.334292 22502662377600 run.py:483] Algo bellman_ford step 5227 current loss 0.072147, current_train_items 167296.
I0304 19:30:35.366687 22502662377600 run.py:483] Algo bellman_ford step 5228 current loss 0.122970, current_train_items 167328.
I0304 19:30:35.401534 22502662377600 run.py:483] Algo bellman_ford step 5229 current loss 0.126119, current_train_items 167360.
I0304 19:30:35.421147 22502662377600 run.py:483] Algo bellman_ford step 5230 current loss 0.007227, current_train_items 167392.
I0304 19:30:35.437358 22502662377600 run.py:483] Algo bellman_ford step 5231 current loss 0.048378, current_train_items 167424.
I0304 19:30:35.461737 22502662377600 run.py:483] Algo bellman_ford step 5232 current loss 0.097259, current_train_items 167456.
I0304 19:30:35.492820 22502662377600 run.py:483] Algo bellman_ford step 5233 current loss 0.075962, current_train_items 167488.
I0304 19:30:35.528057 22502662377600 run.py:483] Algo bellman_ford step 5234 current loss 0.087168, current_train_items 167520.
I0304 19:30:35.547848 22502662377600 run.py:483] Algo bellman_ford step 5235 current loss 0.007080, current_train_items 167552.
I0304 19:30:35.563844 22502662377600 run.py:483] Algo bellman_ford step 5236 current loss 0.013918, current_train_items 167584.
I0304 19:30:35.587733 22502662377600 run.py:483] Algo bellman_ford step 5237 current loss 0.064055, current_train_items 167616.
I0304 19:30:35.620628 22502662377600 run.py:483] Algo bellman_ford step 5238 current loss 0.091590, current_train_items 167648.
I0304 19:30:35.655112 22502662377600 run.py:483] Algo bellman_ford step 5239 current loss 0.121157, current_train_items 167680.
I0304 19:30:35.675076 22502662377600 run.py:483] Algo bellman_ford step 5240 current loss 0.035587, current_train_items 167712.
I0304 19:30:35.691258 22502662377600 run.py:483] Algo bellman_ford step 5241 current loss 0.039809, current_train_items 167744.
I0304 19:30:35.715190 22502662377600 run.py:483] Algo bellman_ford step 5242 current loss 0.039972, current_train_items 167776.
I0304 19:30:35.746416 22502662377600 run.py:483] Algo bellman_ford step 5243 current loss 0.058609, current_train_items 167808.
I0304 19:30:35.777689 22502662377600 run.py:483] Algo bellman_ford step 5244 current loss 0.049255, current_train_items 167840.
I0304 19:30:35.796877 22502662377600 run.py:483] Algo bellman_ford step 5245 current loss 0.032430, current_train_items 167872.
I0304 19:30:35.813076 22502662377600 run.py:483] Algo bellman_ford step 5246 current loss 0.032002, current_train_items 167904.
I0304 19:30:35.837864 22502662377600 run.py:483] Algo bellman_ford step 5247 current loss 0.040369, current_train_items 167936.
I0304 19:30:35.869783 22502662377600 run.py:483] Algo bellman_ford step 5248 current loss 0.048680, current_train_items 167968.
I0304 19:30:35.904052 22502662377600 run.py:483] Algo bellman_ford step 5249 current loss 0.116711, current_train_items 168000.
I0304 19:30:35.923764 22502662377600 run.py:483] Algo bellman_ford step 5250 current loss 0.005183, current_train_items 168032.
I0304 19:30:35.931720 22502662377600 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0304 19:30:35.931823 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:30:35.948407 22502662377600 run.py:483] Algo bellman_ford step 5251 current loss 0.013770, current_train_items 168064.
I0304 19:30:35.973334 22502662377600 run.py:483] Algo bellman_ford step 5252 current loss 0.104454, current_train_items 168096.
I0304 19:30:36.005669 22502662377600 run.py:483] Algo bellman_ford step 5253 current loss 0.055354, current_train_items 168128.
I0304 19:30:36.037306 22502662377600 run.py:483] Algo bellman_ford step 5254 current loss 0.046556, current_train_items 168160.
I0304 19:30:36.057173 22502662377600 run.py:483] Algo bellman_ford step 5255 current loss 0.042228, current_train_items 168192.
I0304 19:30:36.072962 22502662377600 run.py:483] Algo bellman_ford step 5256 current loss 0.019233, current_train_items 168224.
I0304 19:30:36.096023 22502662377600 run.py:483] Algo bellman_ford step 5257 current loss 0.041487, current_train_items 168256.
I0304 19:30:36.127781 22502662377600 run.py:483] Algo bellman_ford step 5258 current loss 0.108861, current_train_items 168288.
I0304 19:30:36.159330 22502662377600 run.py:483] Algo bellman_ford step 5259 current loss 0.064095, current_train_items 168320.
I0304 19:30:36.179325 22502662377600 run.py:483] Algo bellman_ford step 5260 current loss 0.004113, current_train_items 168352.
I0304 19:30:36.195700 22502662377600 run.py:483] Algo bellman_ford step 5261 current loss 0.020942, current_train_items 168384.
I0304 19:30:36.218929 22502662377600 run.py:483] Algo bellman_ford step 5262 current loss 0.076308, current_train_items 168416.
I0304 19:30:36.251119 22502662377600 run.py:483] Algo bellman_ford step 5263 current loss 0.072523, current_train_items 168448.
I0304 19:30:36.283711 22502662377600 run.py:483] Algo bellman_ford step 5264 current loss 0.079842, current_train_items 168480.
I0304 19:30:36.303313 22502662377600 run.py:483] Algo bellman_ford step 5265 current loss 0.006072, current_train_items 168512.
I0304 19:30:36.319682 22502662377600 run.py:483] Algo bellman_ford step 5266 current loss 0.034947, current_train_items 168544.
I0304 19:30:36.344421 22502662377600 run.py:483] Algo bellman_ford step 5267 current loss 0.073089, current_train_items 168576.
I0304 19:30:36.375062 22502662377600 run.py:483] Algo bellman_ford step 5268 current loss 0.071316, current_train_items 168608.
I0304 19:30:36.407049 22502662377600 run.py:483] Algo bellman_ford step 5269 current loss 0.095830, current_train_items 168640.
I0304 19:30:36.426710 22502662377600 run.py:483] Algo bellman_ford step 5270 current loss 0.009089, current_train_items 168672.
I0304 19:30:36.442743 22502662377600 run.py:483] Algo bellman_ford step 5271 current loss 0.040450, current_train_items 168704.
I0304 19:30:36.465478 22502662377600 run.py:483] Algo bellman_ford step 5272 current loss 0.067142, current_train_items 168736.
I0304 19:30:36.496550 22502662377600 run.py:483] Algo bellman_ford step 5273 current loss 0.069112, current_train_items 168768.
I0304 19:30:36.529368 22502662377600 run.py:483] Algo bellman_ford step 5274 current loss 0.089675, current_train_items 168800.
I0304 19:30:36.549241 22502662377600 run.py:483] Algo bellman_ford step 5275 current loss 0.012011, current_train_items 168832.
I0304 19:30:36.565446 22502662377600 run.py:483] Algo bellman_ford step 5276 current loss 0.026835, current_train_items 168864.
I0304 19:30:36.588738 22502662377600 run.py:483] Algo bellman_ford step 5277 current loss 0.033452, current_train_items 168896.
I0304 19:30:36.619997 22502662377600 run.py:483] Algo bellman_ford step 5278 current loss 0.086754, current_train_items 168928.
I0304 19:30:36.654062 22502662377600 run.py:483] Algo bellman_ford step 5279 current loss 0.082137, current_train_items 168960.
I0304 19:30:36.673833 22502662377600 run.py:483] Algo bellman_ford step 5280 current loss 0.008442, current_train_items 168992.
I0304 19:30:36.690237 22502662377600 run.py:483] Algo bellman_ford step 5281 current loss 0.078354, current_train_items 169024.
I0304 19:30:36.715181 22502662377600 run.py:483] Algo bellman_ford step 5282 current loss 0.092481, current_train_items 169056.
I0304 19:30:36.746774 22502662377600 run.py:483] Algo bellman_ford step 5283 current loss 0.115517, current_train_items 169088.
I0304 19:30:36.779804 22502662377600 run.py:483] Algo bellman_ford step 5284 current loss 0.146753, current_train_items 169120.
I0304 19:30:36.799651 22502662377600 run.py:483] Algo bellman_ford step 5285 current loss 0.010919, current_train_items 169152.
I0304 19:30:36.815987 22502662377600 run.py:483] Algo bellman_ford step 5286 current loss 0.033590, current_train_items 169184.
I0304 19:30:36.840829 22502662377600 run.py:483] Algo bellman_ford step 5287 current loss 0.109045, current_train_items 169216.
I0304 19:30:36.871929 22502662377600 run.py:483] Algo bellman_ford step 5288 current loss 0.108733, current_train_items 169248.
I0304 19:30:36.906606 22502662377600 run.py:483] Algo bellman_ford step 5289 current loss 0.084057, current_train_items 169280.
I0304 19:30:36.926573 22502662377600 run.py:483] Algo bellman_ford step 5290 current loss 0.006714, current_train_items 169312.
I0304 19:30:36.943191 22502662377600 run.py:483] Algo bellman_ford step 5291 current loss 0.020967, current_train_items 169344.
I0304 19:30:36.967064 22502662377600 run.py:483] Algo bellman_ford step 5292 current loss 0.082432, current_train_items 169376.
I0304 19:30:36.998621 22502662377600 run.py:483] Algo bellman_ford step 5293 current loss 0.093256, current_train_items 169408.
I0304 19:30:37.031323 22502662377600 run.py:483] Algo bellman_ford step 5294 current loss 0.095830, current_train_items 169440.
I0304 19:30:37.050783 22502662377600 run.py:483] Algo bellman_ford step 5295 current loss 0.011636, current_train_items 169472.
I0304 19:30:37.066705 22502662377600 run.py:483] Algo bellman_ford step 5296 current loss 0.038509, current_train_items 169504.
I0304 19:30:37.090082 22502662377600 run.py:483] Algo bellman_ford step 5297 current loss 0.069584, current_train_items 169536.
I0304 19:30:37.122010 22502662377600 run.py:483] Algo bellman_ford step 5298 current loss 0.138514, current_train_items 169568.
I0304 19:30:37.158570 22502662377600 run.py:483] Algo bellman_ford step 5299 current loss 0.139437, current_train_items 169600.
I0304 19:30:37.178547 22502662377600 run.py:483] Algo bellman_ford step 5300 current loss 0.007159, current_train_items 169632.
I0304 19:30:37.186356 22502662377600 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.96484375, 'score': 0.96484375, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0304 19:30:37.186459 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.965, val scores are: bellman_ford: 0.965
I0304 19:30:37.202806 22502662377600 run.py:483] Algo bellman_ford step 5301 current loss 0.039710, current_train_items 169664.
I0304 19:30:37.227606 22502662377600 run.py:483] Algo bellman_ford step 5302 current loss 0.144826, current_train_items 169696.
I0304 19:30:37.260375 22502662377600 run.py:483] Algo bellman_ford step 5303 current loss 0.101522, current_train_items 169728.
I0304 19:30:37.295199 22502662377600 run.py:483] Algo bellman_ford step 5304 current loss 0.088857, current_train_items 169760.
I0304 19:30:37.315255 22502662377600 run.py:483] Algo bellman_ford step 5305 current loss 0.011264, current_train_items 169792.
I0304 19:30:37.331317 22502662377600 run.py:483] Algo bellman_ford step 5306 current loss 0.032891, current_train_items 169824.
I0304 19:30:37.356329 22502662377600 run.py:483] Algo bellman_ford step 5307 current loss 0.078314, current_train_items 169856.
I0304 19:30:37.389028 22502662377600 run.py:483] Algo bellman_ford step 5308 current loss 0.076956, current_train_items 169888.
I0304 19:30:37.421148 22502662377600 run.py:483] Algo bellman_ford step 5309 current loss 0.068324, current_train_items 169920.
I0304 19:30:37.440753 22502662377600 run.py:483] Algo bellman_ford step 5310 current loss 0.003476, current_train_items 169952.
I0304 19:30:37.456442 22502662377600 run.py:483] Algo bellman_ford step 5311 current loss 0.016273, current_train_items 169984.
I0304 19:30:37.480589 22502662377600 run.py:483] Algo bellman_ford step 5312 current loss 0.096452, current_train_items 170016.
I0304 19:30:37.512133 22502662377600 run.py:483] Algo bellman_ford step 5313 current loss 0.185525, current_train_items 170048.
I0304 19:30:37.547034 22502662377600 run.py:483] Algo bellman_ford step 5314 current loss 0.166100, current_train_items 170080.
I0304 19:30:37.566914 22502662377600 run.py:483] Algo bellman_ford step 5315 current loss 0.008995, current_train_items 170112.
I0304 19:30:37.582995 22502662377600 run.py:483] Algo bellman_ford step 5316 current loss 0.046958, current_train_items 170144.
I0304 19:30:37.606998 22502662377600 run.py:483] Algo bellman_ford step 5317 current loss 0.209106, current_train_items 170176.
I0304 19:30:37.637181 22502662377600 run.py:483] Algo bellman_ford step 5318 current loss 0.082012, current_train_items 170208.
I0304 19:30:37.671704 22502662377600 run.py:483] Algo bellman_ford step 5319 current loss 0.116715, current_train_items 170240.
I0304 19:30:37.691497 22502662377600 run.py:483] Algo bellman_ford step 5320 current loss 0.043840, current_train_items 170272.
I0304 19:30:37.707676 22502662377600 run.py:483] Algo bellman_ford step 5321 current loss 0.060858, current_train_items 170304.
I0304 19:30:37.732800 22502662377600 run.py:483] Algo bellman_ford step 5322 current loss 0.129975, current_train_items 170336.
I0304 19:30:37.762924 22502662377600 run.py:483] Algo bellman_ford step 5323 current loss 0.081529, current_train_items 170368.
I0304 19:30:37.797416 22502662377600 run.py:483] Algo bellman_ford step 5324 current loss 0.122351, current_train_items 170400.
I0304 19:30:37.817350 22502662377600 run.py:483] Algo bellman_ford step 5325 current loss 0.012493, current_train_items 170432.
I0304 19:30:37.833374 22502662377600 run.py:483] Algo bellman_ford step 5326 current loss 0.067824, current_train_items 170464.
I0304 19:30:37.856579 22502662377600 run.py:483] Algo bellman_ford step 5327 current loss 0.067778, current_train_items 170496.
I0304 19:30:37.889096 22502662377600 run.py:483] Algo bellman_ford step 5328 current loss 0.122364, current_train_items 170528.
I0304 19:30:37.922035 22502662377600 run.py:483] Algo bellman_ford step 5329 current loss 0.247142, current_train_items 170560.
I0304 19:30:37.941644 22502662377600 run.py:483] Algo bellman_ford step 5330 current loss 0.019114, current_train_items 170592.
I0304 19:30:37.958010 22502662377600 run.py:483] Algo bellman_ford step 5331 current loss 0.016421, current_train_items 170624.
I0304 19:30:37.982078 22502662377600 run.py:483] Algo bellman_ford step 5332 current loss 0.067995, current_train_items 170656.
I0304 19:30:38.012000 22502662377600 run.py:483] Algo bellman_ford step 5333 current loss 0.112877, current_train_items 170688.
I0304 19:30:38.044773 22502662377600 run.py:483] Algo bellman_ford step 5334 current loss 0.130890, current_train_items 170720.
I0304 19:30:38.064944 22502662377600 run.py:483] Algo bellman_ford step 5335 current loss 0.033462, current_train_items 170752.
I0304 19:30:38.081380 22502662377600 run.py:483] Algo bellman_ford step 5336 current loss 0.050104, current_train_items 170784.
I0304 19:30:38.105063 22502662377600 run.py:483] Algo bellman_ford step 5337 current loss 0.055038, current_train_items 170816.
I0304 19:30:38.135434 22502662377600 run.py:483] Algo bellman_ford step 5338 current loss 0.108776, current_train_items 170848.
I0304 19:30:38.170062 22502662377600 run.py:483] Algo bellman_ford step 5339 current loss 0.195892, current_train_items 170880.
I0304 19:30:38.189705 22502662377600 run.py:483] Algo bellman_ford step 5340 current loss 0.005290, current_train_items 170912.
I0304 19:30:38.206045 22502662377600 run.py:483] Algo bellman_ford step 5341 current loss 0.033089, current_train_items 170944.
I0304 19:30:38.231613 22502662377600 run.py:483] Algo bellman_ford step 5342 current loss 0.059838, current_train_items 170976.
I0304 19:30:38.263645 22502662377600 run.py:483] Algo bellman_ford step 5343 current loss 0.087417, current_train_items 171008.
I0304 19:30:38.297203 22502662377600 run.py:483] Algo bellman_ford step 5344 current loss 0.098345, current_train_items 171040.
I0304 19:30:38.316869 22502662377600 run.py:483] Algo bellman_ford step 5345 current loss 0.005396, current_train_items 171072.
I0304 19:30:38.332438 22502662377600 run.py:483] Algo bellman_ford step 5346 current loss 0.021416, current_train_items 171104.
I0304 19:30:38.356143 22502662377600 run.py:483] Algo bellman_ford step 5347 current loss 0.033618, current_train_items 171136.
I0304 19:30:38.388715 22502662377600 run.py:483] Algo bellman_ford step 5348 current loss 0.118854, current_train_items 171168.
I0304 19:30:38.422853 22502662377600 run.py:483] Algo bellman_ford step 5349 current loss 0.079806, current_train_items 171200.
I0304 19:30:38.442502 22502662377600 run.py:483] Algo bellman_ford step 5350 current loss 0.007734, current_train_items 171232.
I0304 19:30:38.450859 22502662377600 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0304 19:30:38.450964 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:38.467330 22502662377600 run.py:483] Algo bellman_ford step 5351 current loss 0.013804, current_train_items 171264.
I0304 19:30:38.492066 22502662377600 run.py:483] Algo bellman_ford step 5352 current loss 0.089958, current_train_items 171296.
I0304 19:30:38.522938 22502662377600 run.py:483] Algo bellman_ford step 5353 current loss 0.080593, current_train_items 171328.
I0304 19:30:38.556420 22502662377600 run.py:483] Algo bellman_ford step 5354 current loss 0.096714, current_train_items 171360.
I0304 19:30:38.576593 22502662377600 run.py:483] Algo bellman_ford step 5355 current loss 0.005578, current_train_items 171392.
I0304 19:30:38.592771 22502662377600 run.py:483] Algo bellman_ford step 5356 current loss 0.030752, current_train_items 171424.
I0304 19:30:38.616802 22502662377600 run.py:483] Algo bellman_ford step 5357 current loss 0.085355, current_train_items 171456.
I0304 19:30:38.647107 22502662377600 run.py:483] Algo bellman_ford step 5358 current loss 0.044029, current_train_items 171488.
I0304 19:30:38.680660 22502662377600 run.py:483] Algo bellman_ford step 5359 current loss 0.068504, current_train_items 171520.
I0304 19:30:38.701106 22502662377600 run.py:483] Algo bellman_ford step 5360 current loss 0.019512, current_train_items 171552.
I0304 19:30:38.717539 22502662377600 run.py:483] Algo bellman_ford step 5361 current loss 0.029702, current_train_items 171584.
I0304 19:30:38.741586 22502662377600 run.py:483] Algo bellman_ford step 5362 current loss 0.110311, current_train_items 171616.
I0304 19:30:38.773438 22502662377600 run.py:483] Algo bellman_ford step 5363 current loss 0.078787, current_train_items 171648.
I0304 19:30:38.806997 22502662377600 run.py:483] Algo bellman_ford step 5364 current loss 0.067467, current_train_items 171680.
I0304 19:30:38.827246 22502662377600 run.py:483] Algo bellman_ford step 5365 current loss 0.009163, current_train_items 171712.
I0304 19:30:38.843479 22502662377600 run.py:483] Algo bellman_ford step 5366 current loss 0.019495, current_train_items 171744.
I0304 19:30:38.867285 22502662377600 run.py:483] Algo bellman_ford step 5367 current loss 0.089087, current_train_items 171776.
I0304 19:30:38.898469 22502662377600 run.py:483] Algo bellman_ford step 5368 current loss 0.155478, current_train_items 171808.
I0304 19:30:38.931489 22502662377600 run.py:483] Algo bellman_ford step 5369 current loss 0.234381, current_train_items 171840.
I0304 19:30:38.951580 22502662377600 run.py:483] Algo bellman_ford step 5370 current loss 0.015688, current_train_items 171872.
I0304 19:30:38.967777 22502662377600 run.py:483] Algo bellman_ford step 5371 current loss 0.016685, current_train_items 171904.
I0304 19:30:38.991203 22502662377600 run.py:483] Algo bellman_ford step 5372 current loss 0.028526, current_train_items 171936.
I0304 19:30:39.022760 22502662377600 run.py:483] Algo bellman_ford step 5373 current loss 0.053975, current_train_items 171968.
I0304 19:30:39.054228 22502662377600 run.py:483] Algo bellman_ford step 5374 current loss 0.092496, current_train_items 172000.
I0304 19:30:39.074020 22502662377600 run.py:483] Algo bellman_ford step 5375 current loss 0.009817, current_train_items 172032.
I0304 19:30:39.090287 22502662377600 run.py:483] Algo bellman_ford step 5376 current loss 0.028054, current_train_items 172064.
I0304 19:30:39.113763 22502662377600 run.py:483] Algo bellman_ford step 5377 current loss 0.076020, current_train_items 172096.
I0304 19:30:39.144243 22502662377600 run.py:483] Algo bellman_ford step 5378 current loss 0.024383, current_train_items 172128.
I0304 19:30:39.175149 22502662377600 run.py:483] Algo bellman_ford step 5379 current loss 0.066273, current_train_items 172160.
I0304 19:30:39.194844 22502662377600 run.py:483] Algo bellman_ford step 5380 current loss 0.024052, current_train_items 172192.
I0304 19:30:39.211141 22502662377600 run.py:483] Algo bellman_ford step 5381 current loss 0.012366, current_train_items 172224.
I0304 19:30:39.235585 22502662377600 run.py:483] Algo bellman_ford step 5382 current loss 0.096363, current_train_items 172256.
I0304 19:30:39.267265 22502662377600 run.py:483] Algo bellman_ford step 5383 current loss 0.133208, current_train_items 172288.
I0304 19:30:39.299806 22502662377600 run.py:483] Algo bellman_ford step 5384 current loss 0.090780, current_train_items 172320.
I0304 19:30:39.320170 22502662377600 run.py:483] Algo bellman_ford step 5385 current loss 0.006422, current_train_items 172352.
I0304 19:30:39.336000 22502662377600 run.py:483] Algo bellman_ford step 5386 current loss 0.031414, current_train_items 172384.
I0304 19:30:39.359722 22502662377600 run.py:483] Algo bellman_ford step 5387 current loss 0.037282, current_train_items 172416.
I0304 19:30:39.389884 22502662377600 run.py:483] Algo bellman_ford step 5388 current loss 0.042174, current_train_items 172448.
I0304 19:30:39.423791 22502662377600 run.py:483] Algo bellman_ford step 5389 current loss 0.090628, current_train_items 172480.
I0304 19:30:39.443879 22502662377600 run.py:483] Algo bellman_ford step 5390 current loss 0.011302, current_train_items 172512.
I0304 19:30:39.460597 22502662377600 run.py:483] Algo bellman_ford step 5391 current loss 0.045196, current_train_items 172544.
I0304 19:30:39.483859 22502662377600 run.py:483] Algo bellman_ford step 5392 current loss 0.088592, current_train_items 172576.
I0304 19:30:39.516160 22502662377600 run.py:483] Algo bellman_ford step 5393 current loss 0.089096, current_train_items 172608.
I0304 19:30:39.550492 22502662377600 run.py:483] Algo bellman_ford step 5394 current loss 0.072131, current_train_items 172640.
I0304 19:30:39.570544 22502662377600 run.py:483] Algo bellman_ford step 5395 current loss 0.004936, current_train_items 172672.
I0304 19:30:39.587105 22502662377600 run.py:483] Algo bellman_ford step 5396 current loss 0.069842, current_train_items 172704.
I0304 19:30:39.611427 22502662377600 run.py:483] Algo bellman_ford step 5397 current loss 0.104540, current_train_items 172736.
I0304 19:30:39.642896 22502662377600 run.py:483] Algo bellman_ford step 5398 current loss 0.085678, current_train_items 172768.
I0304 19:30:39.677778 22502662377600 run.py:483] Algo bellman_ford step 5399 current loss 0.189172, current_train_items 172800.
I0304 19:30:39.697877 22502662377600 run.py:483] Algo bellman_ford step 5400 current loss 0.003464, current_train_items 172832.
I0304 19:30:39.706017 22502662377600 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0304 19:30:39.706125 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:30:39.723419 22502662377600 run.py:483] Algo bellman_ford step 5401 current loss 0.079799, current_train_items 172864.
I0304 19:30:39.748290 22502662377600 run.py:483] Algo bellman_ford step 5402 current loss 0.099875, current_train_items 172896.
I0304 19:30:39.781448 22502662377600 run.py:483] Algo bellman_ford step 5403 current loss 0.171724, current_train_items 172928.
I0304 19:30:39.816295 22502662377600 run.py:483] Algo bellman_ford step 5404 current loss 0.065036, current_train_items 172960.
I0304 19:30:39.836029 22502662377600 run.py:483] Algo bellman_ford step 5405 current loss 0.003768, current_train_items 172992.
I0304 19:30:39.851613 22502662377600 run.py:483] Algo bellman_ford step 5406 current loss 0.048774, current_train_items 173024.
I0304 19:30:39.875180 22502662377600 run.py:483] Algo bellman_ford step 5407 current loss 0.079838, current_train_items 173056.
I0304 19:30:39.904683 22502662377600 run.py:483] Algo bellman_ford step 5408 current loss 0.102485, current_train_items 173088.
I0304 19:30:39.937421 22502662377600 run.py:483] Algo bellman_ford step 5409 current loss 0.073592, current_train_items 173120.
I0304 19:30:39.957106 22502662377600 run.py:483] Algo bellman_ford step 5410 current loss 0.006336, current_train_items 173152.
I0304 19:30:39.973704 22502662377600 run.py:483] Algo bellman_ford step 5411 current loss 0.046138, current_train_items 173184.
I0304 19:30:39.997838 22502662377600 run.py:483] Algo bellman_ford step 5412 current loss 0.174965, current_train_items 173216.
I0304 19:30:40.029310 22502662377600 run.py:483] Algo bellman_ford step 5413 current loss 0.128298, current_train_items 173248.
I0304 19:30:40.063518 22502662377600 run.py:483] Algo bellman_ford step 5414 current loss 0.146390, current_train_items 173280.
I0304 19:30:40.083262 22502662377600 run.py:483] Algo bellman_ford step 5415 current loss 0.040707, current_train_items 173312.
I0304 19:30:40.099668 22502662377600 run.py:483] Algo bellman_ford step 5416 current loss 0.036636, current_train_items 173344.
I0304 19:30:40.124002 22502662377600 run.py:483] Algo bellman_ford step 5417 current loss 0.089775, current_train_items 173376.
I0304 19:30:40.156198 22502662377600 run.py:483] Algo bellman_ford step 5418 current loss 0.135830, current_train_items 173408.
I0304 19:30:40.191535 22502662377600 run.py:483] Algo bellman_ford step 5419 current loss 0.211985, current_train_items 173440.
I0304 19:30:40.211283 22502662377600 run.py:483] Algo bellman_ford step 5420 current loss 0.044889, current_train_items 173472.
I0304 19:30:40.227615 22502662377600 run.py:483] Algo bellman_ford step 5421 current loss 0.045772, current_train_items 173504.
I0304 19:30:40.250906 22502662377600 run.py:483] Algo bellman_ford step 5422 current loss 0.040467, current_train_items 173536.
I0304 19:30:40.281364 22502662377600 run.py:483] Algo bellman_ford step 5423 current loss 0.074157, current_train_items 173568.
I0304 19:30:40.312872 22502662377600 run.py:483] Algo bellman_ford step 5424 current loss 0.159158, current_train_items 173600.
I0304 19:30:40.332732 22502662377600 run.py:483] Algo bellman_ford step 5425 current loss 0.011532, current_train_items 173632.
I0304 19:30:40.348582 22502662377600 run.py:483] Algo bellman_ford step 5426 current loss 0.028072, current_train_items 173664.
I0304 19:30:40.372811 22502662377600 run.py:483] Algo bellman_ford step 5427 current loss 0.066192, current_train_items 173696.
I0304 19:30:40.403959 22502662377600 run.py:483] Algo bellman_ford step 5428 current loss 0.093687, current_train_items 173728.
I0304 19:30:40.438760 22502662377600 run.py:483] Algo bellman_ford step 5429 current loss 0.075907, current_train_items 173760.
I0304 19:30:40.458000 22502662377600 run.py:483] Algo bellman_ford step 5430 current loss 0.006179, current_train_items 173792.
I0304 19:30:40.474428 22502662377600 run.py:483] Algo bellman_ford step 5431 current loss 0.027390, current_train_items 173824.
I0304 19:30:40.498543 22502662377600 run.py:483] Algo bellman_ford step 5432 current loss 0.068086, current_train_items 173856.
I0304 19:30:40.531758 22502662377600 run.py:483] Algo bellman_ford step 5433 current loss 0.167945, current_train_items 173888.
I0304 19:30:40.567419 22502662377600 run.py:483] Algo bellman_ford step 5434 current loss 0.204287, current_train_items 173920.
I0304 19:30:40.586853 22502662377600 run.py:483] Algo bellman_ford step 5435 current loss 0.004214, current_train_items 173952.
I0304 19:30:40.603005 22502662377600 run.py:483] Algo bellman_ford step 5436 current loss 0.028447, current_train_items 173984.
I0304 19:30:40.626986 22502662377600 run.py:483] Algo bellman_ford step 5437 current loss 0.069434, current_train_items 174016.
I0304 19:30:40.657454 22502662377600 run.py:483] Algo bellman_ford step 5438 current loss 0.120362, current_train_items 174048.
I0304 19:30:40.695088 22502662377600 run.py:483] Algo bellman_ford step 5439 current loss 0.201064, current_train_items 174080.
I0304 19:30:40.714581 22502662377600 run.py:483] Algo bellman_ford step 5440 current loss 0.013177, current_train_items 174112.
I0304 19:30:40.730518 22502662377600 run.py:483] Algo bellman_ford step 5441 current loss 0.023127, current_train_items 174144.
I0304 19:30:40.753805 22502662377600 run.py:483] Algo bellman_ford step 5442 current loss 0.062489, current_train_items 174176.
I0304 19:30:40.784473 22502662377600 run.py:483] Algo bellman_ford step 5443 current loss 0.101958, current_train_items 174208.
I0304 19:30:40.818718 22502662377600 run.py:483] Algo bellman_ford step 5444 current loss 0.091343, current_train_items 174240.
I0304 19:30:40.838235 22502662377600 run.py:483] Algo bellman_ford step 5445 current loss 0.006067, current_train_items 174272.
I0304 19:30:40.854298 22502662377600 run.py:483] Algo bellman_ford step 5446 current loss 0.081551, current_train_items 174304.
I0304 19:30:40.878600 22502662377600 run.py:483] Algo bellman_ford step 5447 current loss 0.070381, current_train_items 174336.
I0304 19:30:40.910864 22502662377600 run.py:483] Algo bellman_ford step 5448 current loss 0.095730, current_train_items 174368.
I0304 19:30:40.944135 22502662377600 run.py:483] Algo bellman_ford step 5449 current loss 0.048211, current_train_items 174400.
I0304 19:30:40.963492 22502662377600 run.py:483] Algo bellman_ford step 5450 current loss 0.006646, current_train_items 174432.
I0304 19:30:40.971747 22502662377600 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0304 19:30:40.971851 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:30:40.988140 22502662377600 run.py:483] Algo bellman_ford step 5451 current loss 0.019794, current_train_items 174464.
I0304 19:30:41.012946 22502662377600 run.py:483] Algo bellman_ford step 5452 current loss 0.092800, current_train_items 174496.
I0304 19:30:41.043380 22502662377600 run.py:483] Algo bellman_ford step 5453 current loss 0.083480, current_train_items 174528.
I0304 19:30:41.076774 22502662377600 run.py:483] Algo bellman_ford step 5454 current loss 0.109415, current_train_items 174560.
I0304 19:30:41.096640 22502662377600 run.py:483] Algo bellman_ford step 5455 current loss 0.003512, current_train_items 174592.
I0304 19:30:41.112711 22502662377600 run.py:483] Algo bellman_ford step 5456 current loss 0.010584, current_train_items 174624.
I0304 19:30:41.136851 22502662377600 run.py:483] Algo bellman_ford step 5457 current loss 0.054882, current_train_items 174656.
I0304 19:30:41.168673 22502662377600 run.py:483] Algo bellman_ford step 5458 current loss 0.062148, current_train_items 174688.
I0304 19:30:41.199759 22502662377600 run.py:483] Algo bellman_ford step 5459 current loss 0.074618, current_train_items 174720.
I0304 19:30:41.219656 22502662377600 run.py:483] Algo bellman_ford step 5460 current loss 0.017850, current_train_items 174752.
I0304 19:30:41.236218 22502662377600 run.py:483] Algo bellman_ford step 5461 current loss 0.040748, current_train_items 174784.
I0304 19:30:41.260208 22502662377600 run.py:483] Algo bellman_ford step 5462 current loss 0.037473, current_train_items 174816.
I0304 19:30:41.290734 22502662377600 run.py:483] Algo bellman_ford step 5463 current loss 0.073395, current_train_items 174848.
I0304 19:30:41.324821 22502662377600 run.py:483] Algo bellman_ford step 5464 current loss 0.096231, current_train_items 174880.
I0304 19:30:41.344761 22502662377600 run.py:483] Algo bellman_ford step 5465 current loss 0.006997, current_train_items 174912.
I0304 19:30:41.360811 22502662377600 run.py:483] Algo bellman_ford step 5466 current loss 0.028438, current_train_items 174944.
I0304 19:30:41.384163 22502662377600 run.py:483] Algo bellman_ford step 5467 current loss 0.040727, current_train_items 174976.
I0304 19:30:41.414376 22502662377600 run.py:483] Algo bellman_ford step 5468 current loss 0.087275, current_train_items 175008.
I0304 19:30:41.448208 22502662377600 run.py:483] Algo bellman_ford step 5469 current loss 0.115274, current_train_items 175040.
I0304 19:30:41.468168 22502662377600 run.py:483] Algo bellman_ford step 5470 current loss 0.009105, current_train_items 175072.
I0304 19:30:41.484695 22502662377600 run.py:483] Algo bellman_ford step 5471 current loss 0.049499, current_train_items 175104.
I0304 19:30:41.507898 22502662377600 run.py:483] Algo bellman_ford step 5472 current loss 0.077701, current_train_items 175136.
I0304 19:30:41.538910 22502662377600 run.py:483] Algo bellman_ford step 5473 current loss 0.067305, current_train_items 175168.
I0304 19:30:41.573473 22502662377600 run.py:483] Algo bellman_ford step 5474 current loss 0.086741, current_train_items 175200.
I0304 19:30:41.593293 22502662377600 run.py:483] Algo bellman_ford step 5475 current loss 0.004520, current_train_items 175232.
I0304 19:30:41.609605 22502662377600 run.py:483] Algo bellman_ford step 5476 current loss 0.039670, current_train_items 175264.
I0304 19:30:41.632836 22502662377600 run.py:483] Algo bellman_ford step 5477 current loss 0.061826, current_train_items 175296.
I0304 19:30:41.664486 22502662377600 run.py:483] Algo bellman_ford step 5478 current loss 0.076429, current_train_items 175328.
I0304 19:30:41.698079 22502662377600 run.py:483] Algo bellman_ford step 5479 current loss 0.111823, current_train_items 175360.
I0304 19:30:41.717793 22502662377600 run.py:483] Algo bellman_ford step 5480 current loss 0.005985, current_train_items 175392.
I0304 19:30:41.734508 22502662377600 run.py:483] Algo bellman_ford step 5481 current loss 0.037661, current_train_items 175424.
I0304 19:30:41.759561 22502662377600 run.py:483] Algo bellman_ford step 5482 current loss 0.109195, current_train_items 175456.
I0304 19:30:41.791541 22502662377600 run.py:483] Algo bellman_ford step 5483 current loss 0.112106, current_train_items 175488.
I0304 19:30:41.826961 22502662377600 run.py:483] Algo bellman_ford step 5484 current loss 0.100433, current_train_items 175520.
I0304 19:30:41.846695 22502662377600 run.py:483] Algo bellman_ford step 5485 current loss 0.005122, current_train_items 175552.
I0304 19:30:41.862679 22502662377600 run.py:483] Algo bellman_ford step 5486 current loss 0.036497, current_train_items 175584.
I0304 19:30:41.885816 22502662377600 run.py:483] Algo bellman_ford step 5487 current loss 0.089747, current_train_items 175616.
I0304 19:30:41.918776 22502662377600 run.py:483] Algo bellman_ford step 5488 current loss 0.204238, current_train_items 175648.
I0304 19:30:41.951637 22502662377600 run.py:483] Algo bellman_ford step 5489 current loss 0.097801, current_train_items 175680.
I0304 19:30:41.971447 22502662377600 run.py:483] Algo bellman_ford step 5490 current loss 0.013612, current_train_items 175712.
I0304 19:30:41.987803 22502662377600 run.py:483] Algo bellman_ford step 5491 current loss 0.011002, current_train_items 175744.
I0304 19:30:42.011503 22502662377600 run.py:483] Algo bellman_ford step 5492 current loss 0.108468, current_train_items 175776.
I0304 19:30:42.042423 22502662377600 run.py:483] Algo bellman_ford step 5493 current loss 0.050664, current_train_items 175808.
I0304 19:30:42.075927 22502662377600 run.py:483] Algo bellman_ford step 5494 current loss 0.093066, current_train_items 175840.
I0304 19:30:42.095299 22502662377600 run.py:483] Algo bellman_ford step 5495 current loss 0.005470, current_train_items 175872.
I0304 19:30:42.111727 22502662377600 run.py:483] Algo bellman_ford step 5496 current loss 0.010766, current_train_items 175904.
I0304 19:30:42.136008 22502662377600 run.py:483] Algo bellman_ford step 5497 current loss 0.050128, current_train_items 175936.
I0304 19:30:42.167064 22502662377600 run.py:483] Algo bellman_ford step 5498 current loss 0.084416, current_train_items 175968.
I0304 19:30:42.201539 22502662377600 run.py:483] Algo bellman_ford step 5499 current loss 0.185620, current_train_items 176000.
I0304 19:30:42.221875 22502662377600 run.py:483] Algo bellman_ford step 5500 current loss 0.011690, current_train_items 176032.
I0304 19:30:42.229695 22502662377600 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0304 19:30:42.229801 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:30:42.247095 22502662377600 run.py:483] Algo bellman_ford step 5501 current loss 0.024988, current_train_items 176064.
I0304 19:30:42.271839 22502662377600 run.py:483] Algo bellman_ford step 5502 current loss 0.070158, current_train_items 176096.
I0304 19:30:42.303291 22502662377600 run.py:483] Algo bellman_ford step 5503 current loss 0.052844, current_train_items 176128.
I0304 19:30:42.338064 22502662377600 run.py:483] Algo bellman_ford step 5504 current loss 0.090865, current_train_items 176160.
I0304 19:30:42.358255 22502662377600 run.py:483] Algo bellman_ford step 5505 current loss 0.011195, current_train_items 176192.
I0304 19:30:42.374335 22502662377600 run.py:483] Algo bellman_ford step 5506 current loss 0.025122, current_train_items 176224.
I0304 19:30:42.397856 22502662377600 run.py:483] Algo bellman_ford step 5507 current loss 0.039310, current_train_items 176256.
I0304 19:30:42.427883 22502662377600 run.py:483] Algo bellman_ford step 5508 current loss 0.064758, current_train_items 176288.
I0304 19:30:42.461340 22502662377600 run.py:483] Algo bellman_ford step 5509 current loss 0.073465, current_train_items 176320.
I0304 19:30:42.480997 22502662377600 run.py:483] Algo bellman_ford step 5510 current loss 0.004978, current_train_items 176352.
I0304 19:30:42.497404 22502662377600 run.py:483] Algo bellman_ford step 5511 current loss 0.033000, current_train_items 176384.
I0304 19:30:42.522353 22502662377600 run.py:483] Algo bellman_ford step 5512 current loss 0.045062, current_train_items 176416.
I0304 19:30:42.552901 22502662377600 run.py:483] Algo bellman_ford step 5513 current loss 0.048646, current_train_items 176448.
I0304 19:30:42.584924 22502662377600 run.py:483] Algo bellman_ford step 5514 current loss 0.060225, current_train_items 176480.
I0304 19:30:42.604274 22502662377600 run.py:483] Algo bellman_ford step 5515 current loss 0.006782, current_train_items 176512.
I0304 19:30:42.620108 22502662377600 run.py:483] Algo bellman_ford step 5516 current loss 0.041164, current_train_items 176544.
I0304 19:30:42.644221 22502662377600 run.py:483] Algo bellman_ford step 5517 current loss 0.061120, current_train_items 176576.
I0304 19:30:42.674984 22502662377600 run.py:483] Algo bellman_ford step 5518 current loss 0.057922, current_train_items 176608.
I0304 19:30:42.710133 22502662377600 run.py:483] Algo bellman_ford step 5519 current loss 0.110553, current_train_items 176640.
I0304 19:30:42.729799 22502662377600 run.py:483] Algo bellman_ford step 5520 current loss 0.014193, current_train_items 176672.
I0304 19:30:42.746275 22502662377600 run.py:483] Algo bellman_ford step 5521 current loss 0.033354, current_train_items 176704.
I0304 19:30:42.769685 22502662377600 run.py:483] Algo bellman_ford step 5522 current loss 0.041866, current_train_items 176736.
I0304 19:30:42.801505 22502662377600 run.py:483] Algo bellman_ford step 5523 current loss 0.073763, current_train_items 176768.
I0304 19:30:42.836414 22502662377600 run.py:483] Algo bellman_ford step 5524 current loss 0.114759, current_train_items 176800.
I0304 19:30:42.855940 22502662377600 run.py:483] Algo bellman_ford step 5525 current loss 0.014931, current_train_items 176832.
I0304 19:30:42.871781 22502662377600 run.py:483] Algo bellman_ford step 5526 current loss 0.022145, current_train_items 176864.
I0304 19:30:42.896862 22502662377600 run.py:483] Algo bellman_ford step 5527 current loss 0.055791, current_train_items 176896.
I0304 19:30:42.927134 22502662377600 run.py:483] Algo bellman_ford step 5528 current loss 0.109913, current_train_items 176928.
I0304 19:30:42.959833 22502662377600 run.py:483] Algo bellman_ford step 5529 current loss 0.065268, current_train_items 176960.
I0304 19:30:42.979604 22502662377600 run.py:483] Algo bellman_ford step 5530 current loss 0.008003, current_train_items 176992.
I0304 19:30:42.995946 22502662377600 run.py:483] Algo bellman_ford step 5531 current loss 0.019040, current_train_items 177024.
I0304 19:30:43.020297 22502662377600 run.py:483] Algo bellman_ford step 5532 current loss 0.149658, current_train_items 177056.
I0304 19:30:43.050966 22502662377600 run.py:483] Algo bellman_ford step 5533 current loss 0.100641, current_train_items 177088.
I0304 19:30:43.086766 22502662377600 run.py:483] Algo bellman_ford step 5534 current loss 0.152688, current_train_items 177120.
I0304 19:30:43.106139 22502662377600 run.py:483] Algo bellman_ford step 5535 current loss 0.020670, current_train_items 177152.
I0304 19:30:43.122516 22502662377600 run.py:483] Algo bellman_ford step 5536 current loss 0.083399, current_train_items 177184.
I0304 19:30:43.145644 22502662377600 run.py:483] Algo bellman_ford step 5537 current loss 0.096578, current_train_items 177216.
I0304 19:30:43.175971 22502662377600 run.py:483] Algo bellman_ford step 5538 current loss 0.160261, current_train_items 177248.
I0304 19:30:43.211404 22502662377600 run.py:483] Algo bellman_ford step 5539 current loss 0.111596, current_train_items 177280.
I0304 19:30:43.231074 22502662377600 run.py:483] Algo bellman_ford step 5540 current loss 0.006230, current_train_items 177312.
I0304 19:30:43.247689 22502662377600 run.py:483] Algo bellman_ford step 5541 current loss 0.058448, current_train_items 177344.
I0304 19:30:43.272084 22502662377600 run.py:483] Algo bellman_ford step 5542 current loss 0.081025, current_train_items 177376.
I0304 19:30:43.303013 22502662377600 run.py:483] Algo bellman_ford step 5543 current loss 0.100430, current_train_items 177408.
I0304 19:30:43.336013 22502662377600 run.py:483] Algo bellman_ford step 5544 current loss 0.212730, current_train_items 177440.
I0304 19:30:43.355665 22502662377600 run.py:483] Algo bellman_ford step 5545 current loss 0.007233, current_train_items 177472.
I0304 19:30:43.372326 22502662377600 run.py:483] Algo bellman_ford step 5546 current loss 0.052970, current_train_items 177504.
I0304 19:30:43.396316 22502662377600 run.py:483] Algo bellman_ford step 5547 current loss 0.050215, current_train_items 177536.
I0304 19:30:43.426950 22502662377600 run.py:483] Algo bellman_ford step 5548 current loss 0.047109, current_train_items 177568.
I0304 19:30:43.459677 22502662377600 run.py:483] Algo bellman_ford step 5549 current loss 0.088015, current_train_items 177600.
I0304 19:30:43.479651 22502662377600 run.py:483] Algo bellman_ford step 5550 current loss 0.007534, current_train_items 177632.
I0304 19:30:43.487906 22502662377600 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0304 19:30:43.488008 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:30:43.504614 22502662377600 run.py:483] Algo bellman_ford step 5551 current loss 0.016600, current_train_items 177664.
I0304 19:30:43.529897 22502662377600 run.py:483] Algo bellman_ford step 5552 current loss 0.095806, current_train_items 177696.
I0304 19:30:43.560874 22502662377600 run.py:483] Algo bellman_ford step 5553 current loss 0.068132, current_train_items 177728.
I0304 19:30:43.597359 22502662377600 run.py:483] Algo bellman_ford step 5554 current loss 0.150357, current_train_items 177760.
I0304 19:30:43.617297 22502662377600 run.py:483] Algo bellman_ford step 5555 current loss 0.010057, current_train_items 177792.
I0304 19:30:43.633100 22502662377600 run.py:483] Algo bellman_ford step 5556 current loss 0.015334, current_train_items 177824.
I0304 19:30:43.657186 22502662377600 run.py:483] Algo bellman_ford step 5557 current loss 0.091637, current_train_items 177856.
I0304 19:30:43.687908 22502662377600 run.py:483] Algo bellman_ford step 5558 current loss 0.058433, current_train_items 177888.
I0304 19:30:43.722148 22502662377600 run.py:483] Algo bellman_ford step 5559 current loss 0.075488, current_train_items 177920.
I0304 19:30:43.741982 22502662377600 run.py:483] Algo bellman_ford step 5560 current loss 0.013374, current_train_items 177952.
I0304 19:30:43.758161 22502662377600 run.py:483] Algo bellman_ford step 5561 current loss 0.029313, current_train_items 177984.
I0304 19:30:43.781499 22502662377600 run.py:483] Algo bellman_ford step 5562 current loss 0.055624, current_train_items 178016.
I0304 19:30:43.813189 22502662377600 run.py:483] Algo bellman_ford step 5563 current loss 0.067392, current_train_items 178048.
I0304 19:30:43.848107 22502662377600 run.py:483] Algo bellman_ford step 5564 current loss 0.087190, current_train_items 178080.
I0304 19:30:43.867803 22502662377600 run.py:483] Algo bellman_ford step 5565 current loss 0.004487, current_train_items 178112.
I0304 19:30:43.883960 22502662377600 run.py:483] Algo bellman_ford step 5566 current loss 0.041718, current_train_items 178144.
I0304 19:30:43.907943 22502662377600 run.py:483] Algo bellman_ford step 5567 current loss 0.049605, current_train_items 178176.
I0304 19:30:43.940214 22502662377600 run.py:483] Algo bellman_ford step 5568 current loss 0.090004, current_train_items 178208.
I0304 19:30:43.974360 22502662377600 run.py:483] Algo bellman_ford step 5569 current loss 0.078675, current_train_items 178240.
I0304 19:30:43.994327 22502662377600 run.py:483] Algo bellman_ford step 5570 current loss 0.012081, current_train_items 178272.
I0304 19:30:44.010670 22502662377600 run.py:483] Algo bellman_ford step 5571 current loss 0.024546, current_train_items 178304.
I0304 19:30:44.034075 22502662377600 run.py:483] Algo bellman_ford step 5572 current loss 0.041384, current_train_items 178336.
I0304 19:30:44.065176 22502662377600 run.py:483] Algo bellman_ford step 5573 current loss 0.063643, current_train_items 178368.
I0304 19:30:44.099119 22502662377600 run.py:483] Algo bellman_ford step 5574 current loss 0.091272, current_train_items 178400.
I0304 19:30:44.119493 22502662377600 run.py:483] Algo bellman_ford step 5575 current loss 0.005007, current_train_items 178432.
I0304 19:30:44.135692 22502662377600 run.py:483] Algo bellman_ford step 5576 current loss 0.030048, current_train_items 178464.
I0304 19:30:44.158683 22502662377600 run.py:483] Algo bellman_ford step 5577 current loss 0.062781, current_train_items 178496.
I0304 19:30:44.189816 22502662377600 run.py:483] Algo bellman_ford step 5578 current loss 0.081837, current_train_items 178528.
I0304 19:30:44.225186 22502662377600 run.py:483] Algo bellman_ford step 5579 current loss 0.113436, current_train_items 178560.
I0304 19:30:44.244898 22502662377600 run.py:483] Algo bellman_ford step 5580 current loss 0.008868, current_train_items 178592.
I0304 19:30:44.261028 22502662377600 run.py:483] Algo bellman_ford step 5581 current loss 0.052925, current_train_items 178624.
I0304 19:30:44.285953 22502662377600 run.py:483] Algo bellman_ford step 5582 current loss 0.043660, current_train_items 178656.
I0304 19:30:44.316745 22502662377600 run.py:483] Algo bellman_ford step 5583 current loss 0.067234, current_train_items 178688.
I0304 19:30:44.350878 22502662377600 run.py:483] Algo bellman_ford step 5584 current loss 0.107692, current_train_items 178720.
I0304 19:30:44.370771 22502662377600 run.py:483] Algo bellman_ford step 5585 current loss 0.054814, current_train_items 178752.
I0304 19:30:44.387059 22502662377600 run.py:483] Algo bellman_ford step 5586 current loss 0.038664, current_train_items 178784.
I0304 19:30:44.411999 22502662377600 run.py:483] Algo bellman_ford step 5587 current loss 0.068751, current_train_items 178816.
I0304 19:30:44.444041 22502662377600 run.py:483] Algo bellman_ford step 5588 current loss 0.093747, current_train_items 178848.
I0304 19:30:44.475616 22502662377600 run.py:483] Algo bellman_ford step 5589 current loss 0.092636, current_train_items 178880.
I0304 19:30:44.495573 22502662377600 run.py:483] Algo bellman_ford step 5590 current loss 0.007399, current_train_items 178912.
I0304 19:30:44.511758 22502662377600 run.py:483] Algo bellman_ford step 5591 current loss 0.021907, current_train_items 178944.
I0304 19:30:44.534613 22502662377600 run.py:483] Algo bellman_ford step 5592 current loss 0.103391, current_train_items 178976.
I0304 19:30:44.565691 22502662377600 run.py:483] Algo bellman_ford step 5593 current loss 0.105782, current_train_items 179008.
I0304 19:30:44.600724 22502662377600 run.py:483] Algo bellman_ford step 5594 current loss 0.075572, current_train_items 179040.
I0304 19:30:44.620039 22502662377600 run.py:483] Algo bellman_ford step 5595 current loss 0.008356, current_train_items 179072.
I0304 19:30:44.636027 22502662377600 run.py:483] Algo bellman_ford step 5596 current loss 0.023138, current_train_items 179104.
I0304 19:30:44.659722 22502662377600 run.py:483] Algo bellman_ford step 5597 current loss 0.089170, current_train_items 179136.
I0304 19:30:44.690928 22502662377600 run.py:483] Algo bellman_ford step 5598 current loss 0.111869, current_train_items 179168.
I0304 19:30:44.720793 22502662377600 run.py:483] Algo bellman_ford step 5599 current loss 0.057412, current_train_items 179200.
I0304 19:30:44.740794 22502662377600 run.py:483] Algo bellman_ford step 5600 current loss 0.006729, current_train_items 179232.
I0304 19:30:44.748537 22502662377600 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0304 19:30:44.748642 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:30:44.765391 22502662377600 run.py:483] Algo bellman_ford step 5601 current loss 0.029811, current_train_items 179264.
I0304 19:30:44.790026 22502662377600 run.py:483] Algo bellman_ford step 5602 current loss 0.071272, current_train_items 179296.
I0304 19:30:44.821354 22502662377600 run.py:483] Algo bellman_ford step 5603 current loss 0.096769, current_train_items 179328.
I0304 19:30:44.853697 22502662377600 run.py:483] Algo bellman_ford step 5604 current loss 0.067433, current_train_items 179360.
I0304 19:30:44.873878 22502662377600 run.py:483] Algo bellman_ford step 5605 current loss 0.005449, current_train_items 179392.
I0304 19:30:44.889491 22502662377600 run.py:483] Algo bellman_ford step 5606 current loss 0.062350, current_train_items 179424.
I0304 19:30:44.913506 22502662377600 run.py:483] Algo bellman_ford step 5607 current loss 0.044980, current_train_items 179456.
I0304 19:30:44.943568 22502662377600 run.py:483] Algo bellman_ford step 5608 current loss 0.036191, current_train_items 179488.
I0304 19:30:44.978002 22502662377600 run.py:483] Algo bellman_ford step 5609 current loss 0.138752, current_train_items 179520.
I0304 19:30:44.997722 22502662377600 run.py:483] Algo bellman_ford step 5610 current loss 0.017085, current_train_items 179552.
I0304 19:30:45.014144 22502662377600 run.py:483] Algo bellman_ford step 5611 current loss 0.018872, current_train_items 179584.
I0304 19:30:45.037529 22502662377600 run.py:483] Algo bellman_ford step 5612 current loss 0.050349, current_train_items 179616.
I0304 19:30:45.069743 22502662377600 run.py:483] Algo bellman_ford step 5613 current loss 0.060842, current_train_items 179648.
I0304 19:30:45.102195 22502662377600 run.py:483] Algo bellman_ford step 5614 current loss 0.065490, current_train_items 179680.
I0304 19:30:45.121751 22502662377600 run.py:483] Algo bellman_ford step 5615 current loss 0.019371, current_train_items 179712.
I0304 19:30:45.138213 22502662377600 run.py:483] Algo bellman_ford step 5616 current loss 0.024891, current_train_items 179744.
I0304 19:30:45.162213 22502662377600 run.py:483] Algo bellman_ford step 5617 current loss 0.030685, current_train_items 179776.
I0304 19:30:45.192956 22502662377600 run.py:483] Algo bellman_ford step 5618 current loss 0.058603, current_train_items 179808.
I0304 19:30:45.227734 22502662377600 run.py:483] Algo bellman_ford step 5619 current loss 0.134069, current_train_items 179840.
I0304 19:30:45.247303 22502662377600 run.py:483] Algo bellman_ford step 5620 current loss 0.019459, current_train_items 179872.
I0304 19:30:45.263492 22502662377600 run.py:483] Algo bellman_ford step 5621 current loss 0.029413, current_train_items 179904.
I0304 19:30:45.286988 22502662377600 run.py:483] Algo bellman_ford step 5622 current loss 0.057394, current_train_items 179936.
I0304 19:30:45.317927 22502662377600 run.py:483] Algo bellman_ford step 5623 current loss 0.153087, current_train_items 179968.
I0304 19:30:45.352178 22502662377600 run.py:483] Algo bellman_ford step 5624 current loss 0.157206, current_train_items 180000.
I0304 19:30:45.371601 22502662377600 run.py:483] Algo bellman_ford step 5625 current loss 0.005105, current_train_items 180032.
I0304 19:30:45.388070 22502662377600 run.py:483] Algo bellman_ford step 5626 current loss 0.058443, current_train_items 180064.
I0304 19:30:45.412271 22502662377600 run.py:483] Algo bellman_ford step 5627 current loss 0.053787, current_train_items 180096.
I0304 19:30:45.443882 22502662377600 run.py:483] Algo bellman_ford step 5628 current loss 0.102240, current_train_items 180128.
I0304 19:30:45.478322 22502662377600 run.py:483] Algo bellman_ford step 5629 current loss 0.105899, current_train_items 180160.
I0304 19:30:45.497701 22502662377600 run.py:483] Algo bellman_ford step 5630 current loss 0.011068, current_train_items 180192.
I0304 19:30:45.513688 22502662377600 run.py:483] Algo bellman_ford step 5631 current loss 0.013070, current_train_items 180224.
I0304 19:30:45.538120 22502662377600 run.py:483] Algo bellman_ford step 5632 current loss 0.120784, current_train_items 180256.
I0304 19:30:45.569220 22502662377600 run.py:483] Algo bellman_ford step 5633 current loss 0.150814, current_train_items 180288.
I0304 19:30:45.603451 22502662377600 run.py:483] Algo bellman_ford step 5634 current loss 0.193438, current_train_items 180320.
I0304 19:30:45.622931 22502662377600 run.py:483] Algo bellman_ford step 5635 current loss 0.018583, current_train_items 180352.
I0304 19:30:45.639222 22502662377600 run.py:483] Algo bellman_ford step 5636 current loss 0.036133, current_train_items 180384.
I0304 19:30:45.662921 22502662377600 run.py:483] Algo bellman_ford step 5637 current loss 0.085094, current_train_items 180416.
I0304 19:30:45.693119 22502662377600 run.py:483] Algo bellman_ford step 5638 current loss 0.098409, current_train_items 180448.
I0304 19:30:45.725638 22502662377600 run.py:483] Algo bellman_ford step 5639 current loss 0.139707, current_train_items 180480.
I0304 19:30:45.745173 22502662377600 run.py:483] Algo bellman_ford step 5640 current loss 0.009406, current_train_items 180512.
I0304 19:30:45.761103 22502662377600 run.py:483] Algo bellman_ford step 5641 current loss 0.017087, current_train_items 180544.
I0304 19:30:45.784941 22502662377600 run.py:483] Algo bellman_ford step 5642 current loss 0.112382, current_train_items 180576.
I0304 19:30:45.816184 22502662377600 run.py:483] Algo bellman_ford step 5643 current loss 0.065889, current_train_items 180608.
I0304 19:30:45.848971 22502662377600 run.py:483] Algo bellman_ford step 5644 current loss 0.075265, current_train_items 180640.
I0304 19:30:45.868300 22502662377600 run.py:483] Algo bellman_ford step 5645 current loss 0.008359, current_train_items 180672.
I0304 19:30:45.884907 22502662377600 run.py:483] Algo bellman_ford step 5646 current loss 0.026397, current_train_items 180704.
I0304 19:30:45.908440 22502662377600 run.py:483] Algo bellman_ford step 5647 current loss 0.074549, current_train_items 180736.
I0304 19:30:45.939450 22502662377600 run.py:483] Algo bellman_ford step 5648 current loss 0.069815, current_train_items 180768.
I0304 19:30:45.974732 22502662377600 run.py:483] Algo bellman_ford step 5649 current loss 0.116093, current_train_items 180800.
I0304 19:30:45.994120 22502662377600 run.py:483] Algo bellman_ford step 5650 current loss 0.022473, current_train_items 180832.
I0304 19:30:46.002065 22502662377600 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0304 19:30:46.002172 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:46.019047 22502662377600 run.py:483] Algo bellman_ford step 5651 current loss 0.016118, current_train_items 180864.
I0304 19:30:46.042831 22502662377600 run.py:483] Algo bellman_ford step 5652 current loss 0.038263, current_train_items 180896.
I0304 19:30:46.076453 22502662377600 run.py:483] Algo bellman_ford step 5653 current loss 0.151562, current_train_items 180928.
I0304 19:30:46.111533 22502662377600 run.py:483] Algo bellman_ford step 5654 current loss 0.135259, current_train_items 180960.
I0304 19:30:46.131381 22502662377600 run.py:483] Algo bellman_ford step 5655 current loss 0.005382, current_train_items 180992.
I0304 19:30:46.147486 22502662377600 run.py:483] Algo bellman_ford step 5656 current loss 0.050470, current_train_items 181024.
I0304 19:30:46.170424 22502662377600 run.py:483] Algo bellman_ford step 5657 current loss 0.044992, current_train_items 181056.
I0304 19:30:46.202624 22502662377600 run.py:483] Algo bellman_ford step 5658 current loss 0.080591, current_train_items 181088.
I0304 19:30:46.236676 22502662377600 run.py:483] Algo bellman_ford step 5659 current loss 0.133290, current_train_items 181120.
I0304 19:30:46.256520 22502662377600 run.py:483] Algo bellman_ford step 5660 current loss 0.021865, current_train_items 181152.
I0304 19:30:46.272504 22502662377600 run.py:483] Algo bellman_ford step 5661 current loss 0.026021, current_train_items 181184.
I0304 19:30:46.295564 22502662377600 run.py:483] Algo bellman_ford step 5662 current loss 0.058739, current_train_items 181216.
I0304 19:30:46.328809 22502662377600 run.py:483] Algo bellman_ford step 5663 current loss 0.074484, current_train_items 181248.
I0304 19:30:46.362812 22502662377600 run.py:483] Algo bellman_ford step 5664 current loss 0.079409, current_train_items 181280.
I0304 19:30:46.382401 22502662377600 run.py:483] Algo bellman_ford step 5665 current loss 0.027769, current_train_items 181312.
I0304 19:30:46.398956 22502662377600 run.py:483] Algo bellman_ford step 5666 current loss 0.030939, current_train_items 181344.
I0304 19:30:46.422553 22502662377600 run.py:483] Algo bellman_ford step 5667 current loss 0.081703, current_train_items 181376.
I0304 19:30:46.454510 22502662377600 run.py:483] Algo bellman_ford step 5668 current loss 0.099881, current_train_items 181408.
I0304 19:30:46.487416 22502662377600 run.py:483] Algo bellman_ford step 5669 current loss 0.091947, current_train_items 181440.
I0304 19:30:46.507276 22502662377600 run.py:483] Algo bellman_ford step 5670 current loss 0.006594, current_train_items 181472.
I0304 19:30:46.523429 22502662377600 run.py:483] Algo bellman_ford step 5671 current loss 0.016585, current_train_items 181504.
I0304 19:30:46.546519 22502662377600 run.py:483] Algo bellman_ford step 5672 current loss 0.073225, current_train_items 181536.
I0304 19:30:46.577512 22502662377600 run.py:483] Algo bellman_ford step 5673 current loss 0.066841, current_train_items 181568.
I0304 19:30:46.610633 22502662377600 run.py:483] Algo bellman_ford step 5674 current loss 0.068189, current_train_items 181600.
I0304 19:30:46.630171 22502662377600 run.py:483] Algo bellman_ford step 5675 current loss 0.007794, current_train_items 181632.
I0304 19:30:46.646266 22502662377600 run.py:483] Algo bellman_ford step 5676 current loss 0.021025, current_train_items 181664.
I0304 19:30:46.670109 22502662377600 run.py:483] Algo bellman_ford step 5677 current loss 0.115779, current_train_items 181696.
I0304 19:30:46.702751 22502662377600 run.py:483] Algo bellman_ford step 5678 current loss 0.258365, current_train_items 181728.
I0304 19:30:46.736239 22502662377600 run.py:483] Algo bellman_ford step 5679 current loss 0.202461, current_train_items 181760.
I0304 19:30:46.756001 22502662377600 run.py:483] Algo bellman_ford step 5680 current loss 0.007766, current_train_items 181792.
I0304 19:30:46.772295 22502662377600 run.py:483] Algo bellman_ford step 5681 current loss 0.031707, current_train_items 181824.
I0304 19:30:46.796208 22502662377600 run.py:483] Algo bellman_ford step 5682 current loss 0.072925, current_train_items 181856.
I0304 19:30:46.827371 22502662377600 run.py:483] Algo bellman_ford step 5683 current loss 0.111884, current_train_items 181888.
I0304 19:30:46.860475 22502662377600 run.py:483] Algo bellman_ford step 5684 current loss 0.103135, current_train_items 181920.
I0304 19:30:46.880271 22502662377600 run.py:483] Algo bellman_ford step 5685 current loss 0.013809, current_train_items 181952.
I0304 19:30:46.896634 22502662377600 run.py:483] Algo bellman_ford step 5686 current loss 0.038558, current_train_items 181984.
I0304 19:30:46.920436 22502662377600 run.py:483] Algo bellman_ford step 5687 current loss 0.090040, current_train_items 182016.
I0304 19:30:46.951947 22502662377600 run.py:483] Algo bellman_ford step 5688 current loss 0.069576, current_train_items 182048.
I0304 19:30:46.985086 22502662377600 run.py:483] Algo bellman_ford step 5689 current loss 0.089790, current_train_items 182080.
I0304 19:30:47.004769 22502662377600 run.py:483] Algo bellman_ford step 5690 current loss 0.005673, current_train_items 182112.
I0304 19:30:47.021579 22502662377600 run.py:483] Algo bellman_ford step 5691 current loss 0.070154, current_train_items 182144.
I0304 19:30:47.045081 22502662377600 run.py:483] Algo bellman_ford step 5692 current loss 0.069415, current_train_items 182176.
I0304 19:30:47.076078 22502662377600 run.py:483] Algo bellman_ford step 5693 current loss 0.071088, current_train_items 182208.
I0304 19:30:47.108819 22502662377600 run.py:483] Algo bellman_ford step 5694 current loss 0.112806, current_train_items 182240.
I0304 19:30:47.128430 22502662377600 run.py:483] Algo bellman_ford step 5695 current loss 0.003525, current_train_items 182272.
I0304 19:30:47.144876 22502662377600 run.py:483] Algo bellman_ford step 5696 current loss 0.064360, current_train_items 182304.
I0304 19:30:47.168673 22502662377600 run.py:483] Algo bellman_ford step 5697 current loss 0.084451, current_train_items 182336.
I0304 19:30:47.200956 22502662377600 run.py:483] Algo bellman_ford step 5698 current loss 0.139313, current_train_items 182368.
I0304 19:30:47.235112 22502662377600 run.py:483] Algo bellman_ford step 5699 current loss 0.116596, current_train_items 182400.
I0304 19:30:47.254887 22502662377600 run.py:483] Algo bellman_ford step 5700 current loss 0.011022, current_train_items 182432.
I0304 19:30:47.262878 22502662377600 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.962890625, 'score': 0.962890625, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0304 19:30:47.262981 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.963, val scores are: bellman_ford: 0.963
I0304 19:30:47.279349 22502662377600 run.py:483] Algo bellman_ford step 5701 current loss 0.011521, current_train_items 182464.
I0304 19:30:47.302896 22502662377600 run.py:483] Algo bellman_ford step 5702 current loss 0.064502, current_train_items 182496.
I0304 19:30:47.334747 22502662377600 run.py:483] Algo bellman_ford step 5703 current loss 0.082140, current_train_items 182528.
I0304 19:30:47.370155 22502662377600 run.py:483] Algo bellman_ford step 5704 current loss 0.086272, current_train_items 182560.
I0304 19:30:47.390408 22502662377600 run.py:483] Algo bellman_ford step 5705 current loss 0.007812, current_train_items 182592.
I0304 19:30:47.406147 22502662377600 run.py:483] Algo bellman_ford step 5706 current loss 0.012765, current_train_items 182624.
I0304 19:30:47.430775 22502662377600 run.py:483] Algo bellman_ford step 5707 current loss 0.130203, current_train_items 182656.
I0304 19:30:47.461752 22502662377600 run.py:483] Algo bellman_ford step 5708 current loss 0.145805, current_train_items 182688.
I0304 19:30:47.495918 22502662377600 run.py:483] Algo bellman_ford step 5709 current loss 0.098376, current_train_items 182720.
I0304 19:30:47.515632 22502662377600 run.py:483] Algo bellman_ford step 5710 current loss 0.004803, current_train_items 182752.
I0304 19:30:47.531654 22502662377600 run.py:483] Algo bellman_ford step 5711 current loss 0.046717, current_train_items 182784.
I0304 19:30:47.555148 22502662377600 run.py:483] Algo bellman_ford step 5712 current loss 0.057442, current_train_items 182816.
I0304 19:30:47.585658 22502662377600 run.py:483] Algo bellman_ford step 5713 current loss 0.053336, current_train_items 182848.
I0304 19:30:47.619019 22502662377600 run.py:483] Algo bellman_ford step 5714 current loss 0.069809, current_train_items 182880.
I0304 19:30:47.638785 22502662377600 run.py:483] Algo bellman_ford step 5715 current loss 0.019871, current_train_items 182912.
I0304 19:30:47.655133 22502662377600 run.py:483] Algo bellman_ford step 5716 current loss 0.034952, current_train_items 182944.
I0304 19:30:47.679326 22502662377600 run.py:483] Algo bellman_ford step 5717 current loss 0.071126, current_train_items 182976.
I0304 19:30:47.710071 22502662377600 run.py:483] Algo bellman_ford step 5718 current loss 0.121158, current_train_items 183008.
I0304 19:30:47.744526 22502662377600 run.py:483] Algo bellman_ford step 5719 current loss 0.110774, current_train_items 183040.
I0304 19:30:47.764274 22502662377600 run.py:483] Algo bellman_ford step 5720 current loss 0.007928, current_train_items 183072.
I0304 19:30:47.780505 22502662377600 run.py:483] Algo bellman_ford step 5721 current loss 0.018510, current_train_items 183104.
I0304 19:30:47.804016 22502662377600 run.py:483] Algo bellman_ford step 5722 current loss 0.258235, current_train_items 183136.
I0304 19:30:47.835072 22502662377600 run.py:483] Algo bellman_ford step 5723 current loss 0.263251, current_train_items 183168.
I0304 19:30:47.868372 22502662377600 run.py:483] Algo bellman_ford step 5724 current loss 0.337738, current_train_items 183200.
I0304 19:30:47.888168 22502662377600 run.py:483] Algo bellman_ford step 5725 current loss 0.018982, current_train_items 183232.
I0304 19:30:47.904450 22502662377600 run.py:483] Algo bellman_ford step 5726 current loss 0.048993, current_train_items 183264.
I0304 19:30:47.929241 22502662377600 run.py:483] Algo bellman_ford step 5727 current loss 0.098520, current_train_items 183296.
I0304 19:30:47.959973 22502662377600 run.py:483] Algo bellman_ford step 5728 current loss 0.068310, current_train_items 183328.
I0304 19:30:47.993763 22502662377600 run.py:483] Algo bellman_ford step 5729 current loss 0.093984, current_train_items 183360.
I0304 19:30:48.013546 22502662377600 run.py:483] Algo bellman_ford step 5730 current loss 0.007899, current_train_items 183392.
I0304 19:30:48.030355 22502662377600 run.py:483] Algo bellman_ford step 5731 current loss 0.030076, current_train_items 183424.
I0304 19:30:48.055073 22502662377600 run.py:483] Algo bellman_ford step 5732 current loss 0.095098, current_train_items 183456.
I0304 19:30:48.085690 22502662377600 run.py:483] Algo bellman_ford step 5733 current loss 0.119769, current_train_items 183488.
I0304 19:30:48.120440 22502662377600 run.py:483] Algo bellman_ford step 5734 current loss 0.157238, current_train_items 183520.
I0304 19:30:48.140585 22502662377600 run.py:483] Algo bellman_ford step 5735 current loss 0.005283, current_train_items 183552.
I0304 19:30:48.157042 22502662377600 run.py:483] Algo bellman_ford step 5736 current loss 0.034107, current_train_items 183584.
I0304 19:30:48.181000 22502662377600 run.py:483] Algo bellman_ford step 5737 current loss 0.060799, current_train_items 183616.
I0304 19:30:48.211756 22502662377600 run.py:483] Algo bellman_ford step 5738 current loss 0.041260, current_train_items 183648.
I0304 19:30:48.246387 22502662377600 run.py:483] Algo bellman_ford step 5739 current loss 0.085705, current_train_items 183680.
I0304 19:30:48.266058 22502662377600 run.py:483] Algo bellman_ford step 5740 current loss 0.025047, current_train_items 183712.
I0304 19:30:48.282593 22502662377600 run.py:483] Algo bellman_ford step 5741 current loss 0.014546, current_train_items 183744.
I0304 19:30:48.307534 22502662377600 run.py:483] Algo bellman_ford step 5742 current loss 0.060966, current_train_items 183776.
I0304 19:30:48.339650 22502662377600 run.py:483] Algo bellman_ford step 5743 current loss 0.073051, current_train_items 183808.
I0304 19:30:48.374278 22502662377600 run.py:483] Algo bellman_ford step 5744 current loss 0.091356, current_train_items 183840.
I0304 19:30:48.394193 22502662377600 run.py:483] Algo bellman_ford step 5745 current loss 0.104124, current_train_items 183872.
I0304 19:30:48.410968 22502662377600 run.py:483] Algo bellman_ford step 5746 current loss 0.021883, current_train_items 183904.
I0304 19:30:48.435524 22502662377600 run.py:483] Algo bellman_ford step 5747 current loss 0.059650, current_train_items 183936.
I0304 19:30:48.467131 22502662377600 run.py:483] Algo bellman_ford step 5748 current loss 0.090592, current_train_items 183968.
I0304 19:30:48.500397 22502662377600 run.py:483] Algo bellman_ford step 5749 current loss 0.065925, current_train_items 184000.
I0304 19:30:48.520524 22502662377600 run.py:483] Algo bellman_ford step 5750 current loss 0.011252, current_train_items 184032.
I0304 19:30:48.528967 22502662377600 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0304 19:30:48.529071 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:30:48.546517 22502662377600 run.py:483] Algo bellman_ford step 5751 current loss 0.014915, current_train_items 184064.
I0304 19:30:48.570355 22502662377600 run.py:483] Algo bellman_ford step 5752 current loss 0.084867, current_train_items 184096.
I0304 19:30:48.601001 22502662377600 run.py:483] Algo bellman_ford step 5753 current loss 0.052002, current_train_items 184128.
I0304 19:30:48.636374 22502662377600 run.py:483] Algo bellman_ford step 5754 current loss 0.183032, current_train_items 184160.
I0304 19:30:48.656485 22502662377600 run.py:483] Algo bellman_ford step 5755 current loss 0.008995, current_train_items 184192.
I0304 19:30:48.672086 22502662377600 run.py:483] Algo bellman_ford step 5756 current loss 0.021817, current_train_items 184224.
I0304 19:30:48.696270 22502662377600 run.py:483] Algo bellman_ford step 5757 current loss 0.097584, current_train_items 184256.
I0304 19:30:48.728378 22502662377600 run.py:483] Algo bellman_ford step 5758 current loss 0.112335, current_train_items 184288.
I0304 19:30:48.762479 22502662377600 run.py:483] Algo bellman_ford step 5759 current loss 0.079322, current_train_items 184320.
I0304 19:30:48.782103 22502662377600 run.py:483] Algo bellman_ford step 5760 current loss 0.065886, current_train_items 184352.
I0304 19:30:48.798563 22502662377600 run.py:483] Algo bellman_ford step 5761 current loss 0.043090, current_train_items 184384.
I0304 19:30:48.822074 22502662377600 run.py:483] Algo bellman_ford step 5762 current loss 0.060221, current_train_items 184416.
I0304 19:30:48.852381 22502662377600 run.py:483] Algo bellman_ford step 5763 current loss 0.058003, current_train_items 184448.
I0304 19:30:48.887367 22502662377600 run.py:483] Algo bellman_ford step 5764 current loss 0.086620, current_train_items 184480.
I0304 19:30:48.907273 22502662377600 run.py:483] Algo bellman_ford step 5765 current loss 0.017987, current_train_items 184512.
I0304 19:30:48.923595 22502662377600 run.py:483] Algo bellman_ford step 5766 current loss 0.026043, current_train_items 184544.
I0304 19:30:48.947146 22502662377600 run.py:483] Algo bellman_ford step 5767 current loss 0.059001, current_train_items 184576.
I0304 19:30:48.979359 22502662377600 run.py:483] Algo bellman_ford step 5768 current loss 0.101961, current_train_items 184608.
I0304 19:30:49.012984 22502662377600 run.py:483] Algo bellman_ford step 5769 current loss 0.129657, current_train_items 184640.
I0304 19:30:49.033027 22502662377600 run.py:483] Algo bellman_ford step 5770 current loss 0.005940, current_train_items 184672.
I0304 19:30:49.049204 22502662377600 run.py:483] Algo bellman_ford step 5771 current loss 0.051227, current_train_items 184704.
I0304 19:30:49.071494 22502662377600 run.py:483] Algo bellman_ford step 5772 current loss 0.081174, current_train_items 184736.
I0304 19:30:49.102148 22502662377600 run.py:483] Algo bellman_ford step 5773 current loss 0.080976, current_train_items 184768.
I0304 19:30:49.134927 22502662377600 run.py:483] Algo bellman_ford step 5774 current loss 0.084878, current_train_items 184800.
I0304 19:30:49.154838 22502662377600 run.py:483] Algo bellman_ford step 5775 current loss 0.010942, current_train_items 184832.
I0304 19:30:49.171433 22502662377600 run.py:483] Algo bellman_ford step 5776 current loss 0.054721, current_train_items 184864.
I0304 19:30:49.194207 22502662377600 run.py:483] Algo bellman_ford step 5777 current loss 0.097339, current_train_items 184896.
I0304 19:30:49.225689 22502662377600 run.py:483] Algo bellman_ford step 5778 current loss 0.117471, current_train_items 184928.
I0304 19:30:49.259812 22502662377600 run.py:483] Algo bellman_ford step 5779 current loss 0.158574, current_train_items 184960.
I0304 19:30:49.279318 22502662377600 run.py:483] Algo bellman_ford step 5780 current loss 0.014257, current_train_items 184992.
I0304 19:30:49.295640 22502662377600 run.py:483] Algo bellman_ford step 5781 current loss 0.038193, current_train_items 185024.
I0304 19:30:49.319909 22502662377600 run.py:483] Algo bellman_ford step 5782 current loss 0.070848, current_train_items 185056.
I0304 19:30:49.350165 22502662377600 run.py:483] Algo bellman_ford step 5783 current loss 0.067430, current_train_items 185088.
I0304 19:30:49.381997 22502662377600 run.py:483] Algo bellman_ford step 5784 current loss 0.120141, current_train_items 185120.
I0304 19:30:49.402156 22502662377600 run.py:483] Algo bellman_ford step 5785 current loss 0.005464, current_train_items 185152.
I0304 19:30:49.418247 22502662377600 run.py:483] Algo bellman_ford step 5786 current loss 0.078736, current_train_items 185184.
I0304 19:30:49.440743 22502662377600 run.py:483] Algo bellman_ford step 5787 current loss 0.086965, current_train_items 185216.
I0304 19:30:49.472582 22502662377600 run.py:483] Algo bellman_ford step 5788 current loss 0.087617, current_train_items 185248.
I0304 19:30:49.505088 22502662377600 run.py:483] Algo bellman_ford step 5789 current loss 0.125051, current_train_items 185280.
I0304 19:30:49.524853 22502662377600 run.py:483] Algo bellman_ford step 5790 current loss 0.006649, current_train_items 185312.
I0304 19:30:49.541499 22502662377600 run.py:483] Algo bellman_ford step 5791 current loss 0.056068, current_train_items 185344.
I0304 19:30:49.564629 22502662377600 run.py:483] Algo bellman_ford step 5792 current loss 0.070079, current_train_items 185376.
I0304 19:30:49.595617 22502662377600 run.py:483] Algo bellman_ford step 5793 current loss 0.072897, current_train_items 185408.
I0304 19:30:49.629203 22502662377600 run.py:483] Algo bellman_ford step 5794 current loss 0.076624, current_train_items 185440.
I0304 19:30:49.648701 22502662377600 run.py:483] Algo bellman_ford step 5795 current loss 0.009822, current_train_items 185472.
I0304 19:30:49.664817 22502662377600 run.py:483] Algo bellman_ford step 5796 current loss 0.021733, current_train_items 185504.
I0304 19:30:49.689037 22502662377600 run.py:483] Algo bellman_ford step 5797 current loss 0.037921, current_train_items 185536.
I0304 19:30:49.720428 22502662377600 run.py:483] Algo bellman_ford step 5798 current loss 0.055086, current_train_items 185568.
I0304 19:30:49.754670 22502662377600 run.py:483] Algo bellman_ford step 5799 current loss 0.081361, current_train_items 185600.
I0304 19:30:49.774371 22502662377600 run.py:483] Algo bellman_ford step 5800 current loss 0.003947, current_train_items 185632.
I0304 19:30:49.782141 22502662377600 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0304 19:30:49.782243 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:30:49.799206 22502662377600 run.py:483] Algo bellman_ford step 5801 current loss 0.056192, current_train_items 185664.
I0304 19:30:49.822811 22502662377600 run.py:483] Algo bellman_ford step 5802 current loss 0.045196, current_train_items 185696.
I0304 19:30:49.854720 22502662377600 run.py:483] Algo bellman_ford step 5803 current loss 0.114282, current_train_items 185728.
I0304 19:30:49.890557 22502662377600 run.py:483] Algo bellman_ford step 5804 current loss 0.127715, current_train_items 185760.
I0304 19:30:49.910467 22502662377600 run.py:483] Algo bellman_ford step 5805 current loss 0.003645, current_train_items 185792.
I0304 19:30:49.926749 22502662377600 run.py:483] Algo bellman_ford step 5806 current loss 0.012620, current_train_items 185824.
I0304 19:30:49.951719 22502662377600 run.py:483] Algo bellman_ford step 5807 current loss 0.076499, current_train_items 185856.
I0304 19:30:49.983239 22502662377600 run.py:483] Algo bellman_ford step 5808 current loss 0.087028, current_train_items 185888.
I0304 19:30:50.016842 22502662377600 run.py:483] Algo bellman_ford step 5809 current loss 0.116408, current_train_items 185920.
I0304 19:30:50.036526 22502662377600 run.py:483] Algo bellman_ford step 5810 current loss 0.010533, current_train_items 185952.
I0304 19:30:50.052694 22502662377600 run.py:483] Algo bellman_ford step 5811 current loss 0.038793, current_train_items 185984.
I0304 19:30:50.076722 22502662377600 run.py:483] Algo bellman_ford step 5812 current loss 0.072765, current_train_items 186016.
I0304 19:30:50.108765 22502662377600 run.py:483] Algo bellman_ford step 5813 current loss 0.104906, current_train_items 186048.
I0304 19:30:50.143635 22502662377600 run.py:483] Algo bellman_ford step 5814 current loss 0.090630, current_train_items 186080.
I0304 19:30:50.163690 22502662377600 run.py:483] Algo bellman_ford step 5815 current loss 0.007948, current_train_items 186112.
I0304 19:30:50.180066 22502662377600 run.py:483] Algo bellman_ford step 5816 current loss 0.025412, current_train_items 186144.
I0304 19:30:50.204187 22502662377600 run.py:483] Algo bellman_ford step 5817 current loss 0.070676, current_train_items 186176.
I0304 19:30:50.234493 22502662377600 run.py:483] Algo bellman_ford step 5818 current loss 0.073424, current_train_items 186208.
I0304 19:30:50.268894 22502662377600 run.py:483] Algo bellman_ford step 5819 current loss 0.107391, current_train_items 186240.
I0304 19:30:50.288878 22502662377600 run.py:483] Algo bellman_ford step 5820 current loss 0.005484, current_train_items 186272.
I0304 19:30:50.304959 22502662377600 run.py:483] Algo bellman_ford step 5821 current loss 0.014599, current_train_items 186304.
I0304 19:30:50.328940 22502662377600 run.py:483] Algo bellman_ford step 5822 current loss 0.067849, current_train_items 186336.
I0304 19:30:50.360459 22502662377600 run.py:483] Algo bellman_ford step 5823 current loss 0.092599, current_train_items 186368.
I0304 19:30:50.393890 22502662377600 run.py:483] Algo bellman_ford step 5824 current loss 0.097669, current_train_items 186400.
I0304 19:30:50.413774 22502662377600 run.py:483] Algo bellman_ford step 5825 current loss 0.014681, current_train_items 186432.
I0304 19:30:50.430098 22502662377600 run.py:483] Algo bellman_ford step 5826 current loss 0.026790, current_train_items 186464.
I0304 19:30:50.454427 22502662377600 run.py:483] Algo bellman_ford step 5827 current loss 0.107350, current_train_items 186496.
I0304 19:30:50.486250 22502662377600 run.py:483] Algo bellman_ford step 5828 current loss 0.114930, current_train_items 186528.
I0304 19:30:50.521318 22502662377600 run.py:483] Algo bellman_ford step 5829 current loss 0.132930, current_train_items 186560.
I0304 19:30:50.541428 22502662377600 run.py:483] Algo bellman_ford step 5830 current loss 0.015017, current_train_items 186592.
I0304 19:30:50.557873 22502662377600 run.py:483] Algo bellman_ford step 5831 current loss 0.048412, current_train_items 186624.
I0304 19:30:50.581638 22502662377600 run.py:483] Algo bellman_ford step 5832 current loss 0.069405, current_train_items 186656.
I0304 19:30:50.612857 22502662377600 run.py:483] Algo bellman_ford step 5833 current loss 0.086611, current_train_items 186688.
I0304 19:30:50.643553 22502662377600 run.py:483] Algo bellman_ford step 5834 current loss 0.069063, current_train_items 186720.
I0304 19:30:50.663523 22502662377600 run.py:483] Algo bellman_ford step 5835 current loss 0.020239, current_train_items 186752.
I0304 19:30:50.679048 22502662377600 run.py:483] Algo bellman_ford step 5836 current loss 0.020992, current_train_items 186784.
I0304 19:30:50.704750 22502662377600 run.py:483] Algo bellman_ford step 5837 current loss 0.069076, current_train_items 186816.
I0304 19:30:50.734604 22502662377600 run.py:483] Algo bellman_ford step 5838 current loss 0.041051, current_train_items 186848.
I0304 19:30:50.769960 22502662377600 run.py:483] Algo bellman_ford step 5839 current loss 0.073474, current_train_items 186880.
I0304 19:30:50.789879 22502662377600 run.py:483] Algo bellman_ford step 5840 current loss 0.011883, current_train_items 186912.
I0304 19:30:50.806494 22502662377600 run.py:483] Algo bellman_ford step 5841 current loss 0.032736, current_train_items 186944.
I0304 19:30:50.830082 22502662377600 run.py:483] Algo bellman_ford step 5842 current loss 0.095644, current_train_items 186976.
I0304 19:30:50.860066 22502662377600 run.py:483] Algo bellman_ford step 5843 current loss 0.034752, current_train_items 187008.
I0304 19:30:50.893746 22502662377600 run.py:483] Algo bellman_ford step 5844 current loss 0.104350, current_train_items 187040.
I0304 19:30:50.913791 22502662377600 run.py:483] Algo bellman_ford step 5845 current loss 0.024354, current_train_items 187072.
I0304 19:30:50.930101 22502662377600 run.py:483] Algo bellman_ford step 5846 current loss 0.040938, current_train_items 187104.
I0304 19:30:50.955173 22502662377600 run.py:483] Algo bellman_ford step 5847 current loss 0.144190, current_train_items 187136.
I0304 19:30:50.985124 22502662377600 run.py:483] Algo bellman_ford step 5848 current loss 0.164757, current_train_items 187168.
I0304 19:30:51.019563 22502662377600 run.py:483] Algo bellman_ford step 5849 current loss 0.158278, current_train_items 187200.
I0304 19:30:51.039155 22502662377600 run.py:483] Algo bellman_ford step 5850 current loss 0.010946, current_train_items 187232.
I0304 19:30:51.047420 22502662377600 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0304 19:30:51.047536 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:30:51.064365 22502662377600 run.py:483] Algo bellman_ford step 5851 current loss 0.016377, current_train_items 187264.
I0304 19:30:51.088424 22502662377600 run.py:483] Algo bellman_ford step 5852 current loss 0.224667, current_train_items 187296.
I0304 19:30:51.119169 22502662377600 run.py:483] Algo bellman_ford step 5853 current loss 0.090850, current_train_items 187328.
I0304 19:30:51.154109 22502662377600 run.py:483] Algo bellman_ford step 5854 current loss 0.188533, current_train_items 187360.
I0304 19:30:51.174082 22502662377600 run.py:483] Algo bellman_ford step 5855 current loss 0.008855, current_train_items 187392.
I0304 19:30:51.189875 22502662377600 run.py:483] Algo bellman_ford step 5856 current loss 0.024142, current_train_items 187424.
I0304 19:30:51.214055 22502662377600 run.py:483] Algo bellman_ford step 5857 current loss 0.051252, current_train_items 187456.
I0304 19:30:51.244242 22502662377600 run.py:483] Algo bellman_ford step 5858 current loss 0.067472, current_train_items 187488.
I0304 19:30:51.278100 22502662377600 run.py:483] Algo bellman_ford step 5859 current loss 0.121633, current_train_items 187520.
I0304 19:30:51.298231 22502662377600 run.py:483] Algo bellman_ford step 5860 current loss 0.008356, current_train_items 187552.
I0304 19:30:51.314693 22502662377600 run.py:483] Algo bellman_ford step 5861 current loss 0.030614, current_train_items 187584.
I0304 19:30:51.337478 22502662377600 run.py:483] Algo bellman_ford step 5862 current loss 0.046658, current_train_items 187616.
I0304 19:30:51.369263 22502662377600 run.py:483] Algo bellman_ford step 5863 current loss 0.057831, current_train_items 187648.
I0304 19:30:51.403909 22502662377600 run.py:483] Algo bellman_ford step 5864 current loss 0.102249, current_train_items 187680.
I0304 19:30:51.423755 22502662377600 run.py:483] Algo bellman_ford step 5865 current loss 0.006228, current_train_items 187712.
I0304 19:30:51.440505 22502662377600 run.py:483] Algo bellman_ford step 5866 current loss 0.041508, current_train_items 187744.
I0304 19:30:51.464487 22502662377600 run.py:483] Algo bellman_ford step 5867 current loss 0.045528, current_train_items 187776.
I0304 19:30:51.495055 22502662377600 run.py:483] Algo bellman_ford step 5868 current loss 0.072664, current_train_items 187808.
I0304 19:30:51.530188 22502662377600 run.py:483] Algo bellman_ford step 5869 current loss 0.079813, current_train_items 187840.
I0304 19:30:51.550635 22502662377600 run.py:483] Algo bellman_ford step 5870 current loss 0.009106, current_train_items 187872.
I0304 19:30:51.566931 22502662377600 run.py:483] Algo bellman_ford step 5871 current loss 0.039227, current_train_items 187904.
I0304 19:30:51.589350 22502662377600 run.py:483] Algo bellman_ford step 5872 current loss 0.048148, current_train_items 187936.
I0304 19:30:51.620264 22502662377600 run.py:483] Algo bellman_ford step 5873 current loss 0.057224, current_train_items 187968.
I0304 19:30:51.654203 22502662377600 run.py:483] Algo bellman_ford step 5874 current loss 0.088980, current_train_items 188000.
I0304 19:30:51.674159 22502662377600 run.py:483] Algo bellman_ford step 5875 current loss 0.006498, current_train_items 188032.
I0304 19:30:51.690384 22502662377600 run.py:483] Algo bellman_ford step 5876 current loss 0.037524, current_train_items 188064.
I0304 19:30:51.713055 22502662377600 run.py:483] Algo bellman_ford step 5877 current loss 0.036934, current_train_items 188096.
I0304 19:30:51.743662 22502662377600 run.py:483] Algo bellman_ford step 5878 current loss 0.056814, current_train_items 188128.
I0304 19:30:51.778295 22502662377600 run.py:483] Algo bellman_ford step 5879 current loss 0.064602, current_train_items 188160.
I0304 19:30:51.798032 22502662377600 run.py:483] Algo bellman_ford step 5880 current loss 0.015481, current_train_items 188192.
I0304 19:30:51.813949 22502662377600 run.py:483] Algo bellman_ford step 5881 current loss 0.032016, current_train_items 188224.
I0304 19:30:51.838570 22502662377600 run.py:483] Algo bellman_ford step 5882 current loss 0.199058, current_train_items 188256.
I0304 19:30:51.869512 22502662377600 run.py:483] Algo bellman_ford step 5883 current loss 0.152223, current_train_items 188288.
I0304 19:30:51.904215 22502662377600 run.py:483] Algo bellman_ford step 5884 current loss 0.203717, current_train_items 188320.
I0304 19:30:51.924357 22502662377600 run.py:483] Algo bellman_ford step 5885 current loss 0.006522, current_train_items 188352.
I0304 19:30:51.940497 22502662377600 run.py:483] Algo bellman_ford step 5886 current loss 0.053180, current_train_items 188384.
I0304 19:30:51.963810 22502662377600 run.py:483] Algo bellman_ford step 5887 current loss 0.068434, current_train_items 188416.
I0304 19:30:51.994262 22502662377600 run.py:483] Algo bellman_ford step 5888 current loss 0.083607, current_train_items 188448.
I0304 19:30:52.026590 22502662377600 run.py:483] Algo bellman_ford step 5889 current loss 0.149074, current_train_items 188480.
I0304 19:30:52.046484 22502662377600 run.py:483] Algo bellman_ford step 5890 current loss 0.004044, current_train_items 188512.
I0304 19:30:52.062764 22502662377600 run.py:483] Algo bellman_ford step 5891 current loss 0.056209, current_train_items 188544.
I0304 19:30:52.086159 22502662377600 run.py:483] Algo bellman_ford step 5892 current loss 0.062245, current_train_items 188576.
I0304 19:30:52.116662 22502662377600 run.py:483] Algo bellman_ford step 5893 current loss 0.038230, current_train_items 188608.
I0304 19:30:52.148017 22502662377600 run.py:483] Algo bellman_ford step 5894 current loss 0.056329, current_train_items 188640.
I0304 19:30:52.167520 22502662377600 run.py:483] Algo bellman_ford step 5895 current loss 0.005786, current_train_items 188672.
I0304 19:30:52.183715 22502662377600 run.py:483] Algo bellman_ford step 5896 current loss 0.050783, current_train_items 188704.
I0304 19:30:52.207887 22502662377600 run.py:483] Algo bellman_ford step 5897 current loss 0.092915, current_train_items 188736.
I0304 19:30:52.239219 22502662377600 run.py:483] Algo bellman_ford step 5898 current loss 0.078220, current_train_items 188768.
I0304 19:30:52.272388 22502662377600 run.py:483] Algo bellman_ford step 5899 current loss 0.070645, current_train_items 188800.
I0304 19:30:52.292403 22502662377600 run.py:483] Algo bellman_ford step 5900 current loss 0.005615, current_train_items 188832.
I0304 19:30:52.300332 22502662377600 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0304 19:30:52.300440 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:52.316935 22502662377600 run.py:483] Algo bellman_ford step 5901 current loss 0.016320, current_train_items 188864.
I0304 19:30:52.341569 22502662377600 run.py:483] Algo bellman_ford step 5902 current loss 0.035752, current_train_items 188896.
I0304 19:30:52.373391 22502662377600 run.py:483] Algo bellman_ford step 5903 current loss 0.071748, current_train_items 188928.
I0304 19:30:52.405825 22502662377600 run.py:483] Algo bellman_ford step 5904 current loss 0.075544, current_train_items 188960.
I0304 19:30:52.425729 22502662377600 run.py:483] Algo bellman_ford step 5905 current loss 0.007820, current_train_items 188992.
I0304 19:30:52.441918 22502662377600 run.py:483] Algo bellman_ford step 5906 current loss 0.072699, current_train_items 189024.
I0304 19:30:52.465315 22502662377600 run.py:483] Algo bellman_ford step 5907 current loss 0.064236, current_train_items 189056.
I0304 19:30:52.496624 22502662377600 run.py:483] Algo bellman_ford step 5908 current loss 0.078840, current_train_items 189088.
I0304 19:30:52.528683 22502662377600 run.py:483] Algo bellman_ford step 5909 current loss 0.135538, current_train_items 189120.
I0304 19:30:52.548222 22502662377600 run.py:483] Algo bellman_ford step 5910 current loss 0.007672, current_train_items 189152.
I0304 19:30:52.564586 22502662377600 run.py:483] Algo bellman_ford step 5911 current loss 0.032393, current_train_items 189184.
I0304 19:30:52.588980 22502662377600 run.py:483] Algo bellman_ford step 5912 current loss 0.095472, current_train_items 189216.
I0304 19:30:52.619437 22502662377600 run.py:483] Algo bellman_ford step 5913 current loss 0.069099, current_train_items 189248.
I0304 19:30:52.652606 22502662377600 run.py:483] Algo bellman_ford step 5914 current loss 0.105987, current_train_items 189280.
I0304 19:30:52.672135 22502662377600 run.py:483] Algo bellman_ford step 5915 current loss 0.034549, current_train_items 189312.
I0304 19:30:52.688427 22502662377600 run.py:483] Algo bellman_ford step 5916 current loss 0.054363, current_train_items 189344.
I0304 19:30:52.711805 22502662377600 run.py:483] Algo bellman_ford step 5917 current loss 0.102564, current_train_items 189376.
I0304 19:30:52.741973 22502662377600 run.py:483] Algo bellman_ford step 5918 current loss 0.074728, current_train_items 189408.
I0304 19:30:52.776161 22502662377600 run.py:483] Algo bellman_ford step 5919 current loss 0.099848, current_train_items 189440.
I0304 19:30:52.795871 22502662377600 run.py:483] Algo bellman_ford step 5920 current loss 0.016790, current_train_items 189472.
I0304 19:30:52.812014 22502662377600 run.py:483] Algo bellman_ford step 5921 current loss 0.037904, current_train_items 189504.
I0304 19:30:52.835374 22502662377600 run.py:483] Algo bellman_ford step 5922 current loss 0.052045, current_train_items 189536.
I0304 19:30:52.864966 22502662377600 run.py:483] Algo bellman_ford step 5923 current loss 0.062537, current_train_items 189568.
I0304 19:30:52.898262 22502662377600 run.py:483] Algo bellman_ford step 5924 current loss 0.069191, current_train_items 189600.
I0304 19:30:52.917575 22502662377600 run.py:483] Algo bellman_ford step 5925 current loss 0.003082, current_train_items 189632.
I0304 19:30:52.933362 22502662377600 run.py:483] Algo bellman_ford step 5926 current loss 0.039074, current_train_items 189664.
I0304 19:30:52.956334 22502662377600 run.py:483] Algo bellman_ford step 5927 current loss 0.056223, current_train_items 189696.
I0304 19:30:52.987397 22502662377600 run.py:483] Algo bellman_ford step 5928 current loss 0.107329, current_train_items 189728.
I0304 19:30:53.019730 22502662377600 run.py:483] Algo bellman_ford step 5929 current loss 0.086381, current_train_items 189760.
I0304 19:30:53.039288 22502662377600 run.py:483] Algo bellman_ford step 5930 current loss 0.004600, current_train_items 189792.
I0304 19:30:53.055576 22502662377600 run.py:483] Algo bellman_ford step 5931 current loss 0.020941, current_train_items 189824.
I0304 19:30:53.078974 22502662377600 run.py:483] Algo bellman_ford step 5932 current loss 0.031829, current_train_items 189856.
I0304 19:30:53.108986 22502662377600 run.py:483] Algo bellman_ford step 5933 current loss 0.078593, current_train_items 189888.
I0304 19:30:53.143432 22502662377600 run.py:483] Algo bellman_ford step 5934 current loss 0.073245, current_train_items 189920.
I0304 19:30:53.163058 22502662377600 run.py:483] Algo bellman_ford step 5935 current loss 0.017714, current_train_items 189952.
I0304 19:30:53.178807 22502662377600 run.py:483] Algo bellman_ford step 5936 current loss 0.021554, current_train_items 189984.
I0304 19:30:53.202389 22502662377600 run.py:483] Algo bellman_ford step 5937 current loss 0.047329, current_train_items 190016.
I0304 19:30:53.233952 22502662377600 run.py:483] Algo bellman_ford step 5938 current loss 0.072423, current_train_items 190048.
I0304 19:30:53.265944 22502662377600 run.py:483] Algo bellman_ford step 5939 current loss 0.076718, current_train_items 190080.
I0304 19:30:53.285619 22502662377600 run.py:483] Algo bellman_ford step 5940 current loss 0.024110, current_train_items 190112.
I0304 19:30:53.301473 22502662377600 run.py:483] Algo bellman_ford step 5941 current loss 0.063766, current_train_items 190144.
I0304 19:30:53.325743 22502662377600 run.py:483] Algo bellman_ford step 5942 current loss 0.096469, current_train_items 190176.
I0304 19:30:53.357216 22502662377600 run.py:483] Algo bellman_ford step 5943 current loss 0.102184, current_train_items 190208.
I0304 19:30:53.390832 22502662377600 run.py:483] Algo bellman_ford step 5944 current loss 0.146938, current_train_items 190240.
I0304 19:30:53.410333 22502662377600 run.py:483] Algo bellman_ford step 5945 current loss 0.017051, current_train_items 190272.
I0304 19:30:53.426692 22502662377600 run.py:483] Algo bellman_ford step 5946 current loss 0.023866, current_train_items 190304.
I0304 19:30:53.450596 22502662377600 run.py:483] Algo bellman_ford step 5947 current loss 0.047733, current_train_items 190336.
I0304 19:30:53.480604 22502662377600 run.py:483] Algo bellman_ford step 5948 current loss 0.040131, current_train_items 190368.
I0304 19:30:53.514135 22502662377600 run.py:483] Algo bellman_ford step 5949 current loss 0.117548, current_train_items 190400.
I0304 19:30:53.533559 22502662377600 run.py:483] Algo bellman_ford step 5950 current loss 0.007988, current_train_items 190432.
I0304 19:30:53.541621 22502662377600 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0304 19:30:53.541728 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:53.558440 22502662377600 run.py:483] Algo bellman_ford step 5951 current loss 0.026439, current_train_items 190464.
I0304 19:30:53.582844 22502662377600 run.py:483] Algo bellman_ford step 5952 current loss 0.073448, current_train_items 190496.
I0304 19:30:53.614092 22502662377600 run.py:483] Algo bellman_ford step 5953 current loss 0.067488, current_train_items 190528.
I0304 19:30:53.647953 22502662377600 run.py:483] Algo bellman_ford step 5954 current loss 0.094267, current_train_items 190560.
I0304 19:30:53.668056 22502662377600 run.py:483] Algo bellman_ford step 5955 current loss 0.007148, current_train_items 190592.
I0304 19:30:53.684180 22502662377600 run.py:483] Algo bellman_ford step 5956 current loss 0.041624, current_train_items 190624.
I0304 19:30:53.707113 22502662377600 run.py:483] Algo bellman_ford step 5957 current loss 0.058224, current_train_items 190656.
I0304 19:30:53.737767 22502662377600 run.py:483] Algo bellman_ford step 5958 current loss 0.036051, current_train_items 190688.
I0304 19:30:53.771104 22502662377600 run.py:483] Algo bellman_ford step 5959 current loss 0.079245, current_train_items 190720.
I0304 19:30:53.791137 22502662377600 run.py:483] Algo bellman_ford step 5960 current loss 0.012363, current_train_items 190752.
I0304 19:30:53.807592 22502662377600 run.py:483] Algo bellman_ford step 5961 current loss 0.022040, current_train_items 190784.
I0304 19:30:53.831572 22502662377600 run.py:483] Algo bellman_ford step 5962 current loss 0.068960, current_train_items 190816.
I0304 19:30:53.860827 22502662377600 run.py:483] Algo bellman_ford step 5963 current loss 0.053725, current_train_items 190848.
I0304 19:30:53.894061 22502662377600 run.py:483] Algo bellman_ford step 5964 current loss 0.128601, current_train_items 190880.
I0304 19:30:53.913578 22502662377600 run.py:483] Algo bellman_ford step 5965 current loss 0.018502, current_train_items 190912.
I0304 19:30:53.929534 22502662377600 run.py:483] Algo bellman_ford step 5966 current loss 0.014023, current_train_items 190944.
I0304 19:30:53.953680 22502662377600 run.py:483] Algo bellman_ford step 5967 current loss 0.148068, current_train_items 190976.
I0304 19:30:53.984497 22502662377600 run.py:483] Algo bellman_ford step 5968 current loss 0.275866, current_train_items 191008.
I0304 19:30:54.018655 22502662377600 run.py:483] Algo bellman_ford step 5969 current loss 0.222950, current_train_items 191040.
I0304 19:30:54.038483 22502662377600 run.py:483] Algo bellman_ford step 5970 current loss 0.007366, current_train_items 191072.
I0304 19:30:54.054643 22502662377600 run.py:483] Algo bellman_ford step 5971 current loss 0.030726, current_train_items 191104.
I0304 19:30:54.077784 22502662377600 run.py:483] Algo bellman_ford step 5972 current loss 0.070188, current_train_items 191136.
I0304 19:30:54.109201 22502662377600 run.py:483] Algo bellman_ford step 5973 current loss 0.105602, current_train_items 191168.
I0304 19:30:54.142131 22502662377600 run.py:483] Algo bellman_ford step 5974 current loss 0.107129, current_train_items 191200.
I0304 19:30:54.161899 22502662377600 run.py:483] Algo bellman_ford step 5975 current loss 0.029502, current_train_items 191232.
I0304 19:30:54.177778 22502662377600 run.py:483] Algo bellman_ford step 5976 current loss 0.012725, current_train_items 191264.
I0304 19:30:54.200646 22502662377600 run.py:483] Algo bellman_ford step 5977 current loss 0.071225, current_train_items 191296.
I0304 19:30:54.232138 22502662377600 run.py:483] Algo bellman_ford step 5978 current loss 0.112438, current_train_items 191328.
I0304 19:30:54.264502 22502662377600 run.py:483] Algo bellman_ford step 5979 current loss 0.052291, current_train_items 191360.
I0304 19:30:54.283721 22502662377600 run.py:483] Algo bellman_ford step 5980 current loss 0.004576, current_train_items 191392.
I0304 19:30:54.300433 22502662377600 run.py:483] Algo bellman_ford step 5981 current loss 0.043813, current_train_items 191424.
I0304 19:30:54.323446 22502662377600 run.py:483] Algo bellman_ford step 5982 current loss 0.041122, current_train_items 191456.
I0304 19:30:54.354093 22502662377600 run.py:483] Algo bellman_ford step 5983 current loss 0.046843, current_train_items 191488.
I0304 19:30:54.386775 22502662377600 run.py:483] Algo bellman_ford step 5984 current loss 0.067596, current_train_items 191520.
I0304 19:30:54.406524 22502662377600 run.py:483] Algo bellman_ford step 5985 current loss 0.012539, current_train_items 191552.
I0304 19:30:54.423245 22502662377600 run.py:483] Algo bellman_ford step 5986 current loss 0.027537, current_train_items 191584.
I0304 19:30:54.446731 22502662377600 run.py:483] Algo bellman_ford step 5987 current loss 0.063896, current_train_items 191616.
I0304 19:30:54.476668 22502662377600 run.py:483] Algo bellman_ford step 5988 current loss 0.084694, current_train_items 191648.
I0304 19:30:54.509949 22502662377600 run.py:483] Algo bellman_ford step 5989 current loss 0.075569, current_train_items 191680.
I0304 19:30:54.529745 22502662377600 run.py:483] Algo bellman_ford step 5990 current loss 0.005841, current_train_items 191712.
I0304 19:30:54.545325 22502662377600 run.py:483] Algo bellman_ford step 5991 current loss 0.074277, current_train_items 191744.
I0304 19:30:54.568963 22502662377600 run.py:483] Algo bellman_ford step 5992 current loss 0.108029, current_train_items 191776.
I0304 19:30:54.601591 22502662377600 run.py:483] Algo bellman_ford step 5993 current loss 0.108056, current_train_items 191808.
I0304 19:30:54.634819 22502662377600 run.py:483] Algo bellman_ford step 5994 current loss 0.097602, current_train_items 191840.
I0304 19:30:54.654037 22502662377600 run.py:483] Algo bellman_ford step 5995 current loss 0.007797, current_train_items 191872.
I0304 19:30:54.670152 22502662377600 run.py:483] Algo bellman_ford step 5996 current loss 0.014885, current_train_items 191904.
I0304 19:30:54.694081 22502662377600 run.py:483] Algo bellman_ford step 5997 current loss 0.067869, current_train_items 191936.
I0304 19:30:54.724835 22502662377600 run.py:483] Algo bellman_ford step 5998 current loss 0.081823, current_train_items 191968.
I0304 19:30:54.757257 22502662377600 run.py:483] Algo bellman_ford step 5999 current loss 0.094828, current_train_items 192000.
I0304 19:30:54.777390 22502662377600 run.py:483] Algo bellman_ford step 6000 current loss 0.021983, current_train_items 192032.
I0304 19:30:54.785246 22502662377600 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0304 19:30:54.785349 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:54.802158 22502662377600 run.py:483] Algo bellman_ford step 6001 current loss 0.022623, current_train_items 192064.
I0304 19:30:54.826352 22502662377600 run.py:483] Algo bellman_ford step 6002 current loss 0.072629, current_train_items 192096.
I0304 19:30:54.856305 22502662377600 run.py:483] Algo bellman_ford step 6003 current loss 0.182537, current_train_items 192128.
I0304 19:30:54.889657 22502662377600 run.py:483] Algo bellman_ford step 6004 current loss 0.159000, current_train_items 192160.
I0304 19:30:54.909576 22502662377600 run.py:483] Algo bellman_ford step 6005 current loss 0.006263, current_train_items 192192.
I0304 19:30:54.925364 22502662377600 run.py:483] Algo bellman_ford step 6006 current loss 0.017794, current_train_items 192224.
I0304 19:30:54.950289 22502662377600 run.py:483] Algo bellman_ford step 6007 current loss 0.073120, current_train_items 192256.
I0304 19:30:54.983450 22502662377600 run.py:483] Algo bellman_ford step 6008 current loss 0.133239, current_train_items 192288.
I0304 19:30:55.018348 22502662377600 run.py:483] Algo bellman_ford step 6009 current loss 0.084807, current_train_items 192320.
I0304 19:30:55.037943 22502662377600 run.py:483] Algo bellman_ford step 6010 current loss 0.006650, current_train_items 192352.
I0304 19:30:55.054118 22502662377600 run.py:483] Algo bellman_ford step 6011 current loss 0.032278, current_train_items 192384.
I0304 19:30:55.078284 22502662377600 run.py:483] Algo bellman_ford step 6012 current loss 0.036393, current_train_items 192416.
I0304 19:30:55.108701 22502662377600 run.py:483] Algo bellman_ford step 6013 current loss 0.041375, current_train_items 192448.
I0304 19:30:55.144110 22502662377600 run.py:483] Algo bellman_ford step 6014 current loss 0.085393, current_train_items 192480.
I0304 19:30:55.164116 22502662377600 run.py:483] Algo bellman_ford step 6015 current loss 0.017717, current_train_items 192512.
I0304 19:30:55.180503 22502662377600 run.py:483] Algo bellman_ford step 6016 current loss 0.026702, current_train_items 192544.
I0304 19:30:55.204342 22502662377600 run.py:483] Algo bellman_ford step 6017 current loss 0.051207, current_train_items 192576.
I0304 19:30:55.235812 22502662377600 run.py:483] Algo bellman_ford step 6018 current loss 0.108628, current_train_items 192608.
I0304 19:30:55.269238 22502662377600 run.py:483] Algo bellman_ford step 6019 current loss 0.089658, current_train_items 192640.
I0304 19:30:55.289160 22502662377600 run.py:483] Algo bellman_ford step 6020 current loss 0.008568, current_train_items 192672.
I0304 19:30:55.305841 22502662377600 run.py:483] Algo bellman_ford step 6021 current loss 0.021317, current_train_items 192704.
I0304 19:30:55.330254 22502662377600 run.py:483] Algo bellman_ford step 6022 current loss 0.080536, current_train_items 192736.
I0304 19:30:55.363270 22502662377600 run.py:483] Algo bellman_ford step 6023 current loss 0.138473, current_train_items 192768.
I0304 19:30:55.397167 22502662377600 run.py:483] Algo bellman_ford step 6024 current loss 0.125293, current_train_items 192800.
I0304 19:30:55.416621 22502662377600 run.py:483] Algo bellman_ford step 6025 current loss 0.005444, current_train_items 192832.
I0304 19:30:55.433061 22502662377600 run.py:483] Algo bellman_ford step 6026 current loss 0.053842, current_train_items 192864.
I0304 19:30:55.457328 22502662377600 run.py:483] Algo bellman_ford step 6027 current loss 0.125517, current_train_items 192896.
I0304 19:30:55.488702 22502662377600 run.py:483] Algo bellman_ford step 6028 current loss 0.181693, current_train_items 192928.
I0304 19:30:55.522582 22502662377600 run.py:483] Algo bellman_ford step 6029 current loss 0.213619, current_train_items 192960.
I0304 19:30:55.542376 22502662377600 run.py:483] Algo bellman_ford step 6030 current loss 0.003936, current_train_items 192992.
I0304 19:30:55.558672 22502662377600 run.py:483] Algo bellman_ford step 6031 current loss 0.052814, current_train_items 193024.
I0304 19:30:55.582606 22502662377600 run.py:483] Algo bellman_ford step 6032 current loss 0.025829, current_train_items 193056.
I0304 19:30:55.615587 22502662377600 run.py:483] Algo bellman_ford step 6033 current loss 0.088092, current_train_items 193088.
I0304 19:30:55.648901 22502662377600 run.py:483] Algo bellman_ford step 6034 current loss 0.084444, current_train_items 193120.
I0304 19:30:55.668780 22502662377600 run.py:483] Algo bellman_ford step 6035 current loss 0.003419, current_train_items 193152.
I0304 19:30:55.684851 22502662377600 run.py:483] Algo bellman_ford step 6036 current loss 0.024348, current_train_items 193184.
I0304 19:30:55.709057 22502662377600 run.py:483] Algo bellman_ford step 6037 current loss 0.073224, current_train_items 193216.
I0304 19:30:55.741058 22502662377600 run.py:483] Algo bellman_ford step 6038 current loss 0.167905, current_train_items 193248.
I0304 19:30:55.773936 22502662377600 run.py:483] Algo bellman_ford step 6039 current loss 0.110832, current_train_items 193280.
I0304 19:30:55.793230 22502662377600 run.py:483] Algo bellman_ford step 6040 current loss 0.005364, current_train_items 193312.
I0304 19:30:55.809498 22502662377600 run.py:483] Algo bellman_ford step 6041 current loss 0.029976, current_train_items 193344.
I0304 19:30:55.834203 22502662377600 run.py:483] Algo bellman_ford step 6042 current loss 0.056820, current_train_items 193376.
I0304 19:30:55.865650 22502662377600 run.py:483] Algo bellman_ford step 6043 current loss 0.113569, current_train_items 193408.
I0304 19:30:55.902033 22502662377600 run.py:483] Algo bellman_ford step 6044 current loss 0.135546, current_train_items 193440.
I0304 19:30:55.921707 22502662377600 run.py:483] Algo bellman_ford step 6045 current loss 0.009023, current_train_items 193472.
I0304 19:30:55.938191 22502662377600 run.py:483] Algo bellman_ford step 6046 current loss 0.029565, current_train_items 193504.
I0304 19:30:55.961973 22502662377600 run.py:483] Algo bellman_ford step 6047 current loss 0.036968, current_train_items 193536.
I0304 19:30:55.994224 22502662377600 run.py:483] Algo bellman_ford step 6048 current loss 0.064489, current_train_items 193568.
I0304 19:30:56.028744 22502662377600 run.py:483] Algo bellman_ford step 6049 current loss 0.053256, current_train_items 193600.
I0304 19:30:56.048532 22502662377600 run.py:483] Algo bellman_ford step 6050 current loss 0.005762, current_train_items 193632.
I0304 19:30:56.056577 22502662377600 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0304 19:30:56.056711 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:30:56.073314 22502662377600 run.py:483] Algo bellman_ford step 6051 current loss 0.017723, current_train_items 193664.
I0304 19:30:56.098061 22502662377600 run.py:483] Algo bellman_ford step 6052 current loss 0.058294, current_train_items 193696.
I0304 19:30:56.129129 22502662377600 run.py:483] Algo bellman_ford step 6053 current loss 0.050120, current_train_items 193728.
I0304 19:30:56.163938 22502662377600 run.py:483] Algo bellman_ford step 6054 current loss 0.076156, current_train_items 193760.
I0304 19:30:56.184109 22502662377600 run.py:483] Algo bellman_ford step 6055 current loss 0.010228, current_train_items 193792.
I0304 19:30:56.199686 22502662377600 run.py:483] Algo bellman_ford step 6056 current loss 0.049733, current_train_items 193824.
I0304 19:30:56.222757 22502662377600 run.py:483] Algo bellman_ford step 6057 current loss 0.065447, current_train_items 193856.
I0304 19:30:56.253550 22502662377600 run.py:483] Algo bellman_ford step 6058 current loss 0.051684, current_train_items 193888.
I0304 19:30:56.284739 22502662377600 run.py:483] Algo bellman_ford step 6059 current loss 0.055157, current_train_items 193920.
I0304 19:30:56.304853 22502662377600 run.py:483] Algo bellman_ford step 6060 current loss 0.005395, current_train_items 193952.
I0304 19:30:56.321214 22502662377600 run.py:483] Algo bellman_ford step 6061 current loss 0.025329, current_train_items 193984.
I0304 19:30:56.343240 22502662377600 run.py:483] Algo bellman_ford step 6062 current loss 0.043483, current_train_items 194016.
I0304 19:30:56.372964 22502662377600 run.py:483] Algo bellman_ford step 6063 current loss 0.082357, current_train_items 194048.
I0304 19:30:56.406929 22502662377600 run.py:483] Algo bellman_ford step 6064 current loss 0.115773, current_train_items 194080.
I0304 19:30:56.426522 22502662377600 run.py:483] Algo bellman_ford step 6065 current loss 0.007161, current_train_items 194112.
I0304 19:30:56.442678 22502662377600 run.py:483] Algo bellman_ford step 6066 current loss 0.038070, current_train_items 194144.
I0304 19:30:56.466939 22502662377600 run.py:483] Algo bellman_ford step 6067 current loss 0.060760, current_train_items 194176.
I0304 19:30:56.498471 22502662377600 run.py:483] Algo bellman_ford step 6068 current loss 0.130006, current_train_items 194208.
I0304 19:30:56.531990 22502662377600 run.py:483] Algo bellman_ford step 6069 current loss 0.074382, current_train_items 194240.
I0304 19:30:56.551915 22502662377600 run.py:483] Algo bellman_ford step 6070 current loss 0.008841, current_train_items 194272.
I0304 19:30:56.568001 22502662377600 run.py:483] Algo bellman_ford step 6071 current loss 0.021690, current_train_items 194304.
I0304 19:30:56.591807 22502662377600 run.py:483] Algo bellman_ford step 6072 current loss 0.093661, current_train_items 194336.
I0304 19:30:56.622133 22502662377600 run.py:483] Algo bellman_ford step 6073 current loss 0.148957, current_train_items 194368.
I0304 19:30:56.656050 22502662377600 run.py:483] Algo bellman_ford step 6074 current loss 0.100392, current_train_items 194400.
I0304 19:30:56.675970 22502662377600 run.py:483] Algo bellman_ford step 6075 current loss 0.007078, current_train_items 194432.
I0304 19:30:56.692189 22502662377600 run.py:483] Algo bellman_ford step 6076 current loss 0.036129, current_train_items 194464.
I0304 19:30:56.714539 22502662377600 run.py:483] Algo bellman_ford step 6077 current loss 0.059066, current_train_items 194496.
I0304 19:30:56.744971 22502662377600 run.py:483] Algo bellman_ford step 6078 current loss 0.068858, current_train_items 194528.
I0304 19:30:56.779311 22502662377600 run.py:483] Algo bellman_ford step 6079 current loss 0.104845, current_train_items 194560.
I0304 19:30:56.798800 22502662377600 run.py:483] Algo bellman_ford step 6080 current loss 0.005253, current_train_items 194592.
I0304 19:30:56.815167 22502662377600 run.py:483] Algo bellman_ford step 6081 current loss 0.018994, current_train_items 194624.
I0304 19:30:56.838651 22502662377600 run.py:483] Algo bellman_ford step 6082 current loss 0.030323, current_train_items 194656.
I0304 19:30:56.870955 22502662377600 run.py:483] Algo bellman_ford step 6083 current loss 0.079474, current_train_items 194688.
I0304 19:30:56.903113 22502662377600 run.py:483] Algo bellman_ford step 6084 current loss 0.107602, current_train_items 194720.
I0304 19:30:56.923074 22502662377600 run.py:483] Algo bellman_ford step 6085 current loss 0.009652, current_train_items 194752.
I0304 19:30:56.939508 22502662377600 run.py:483] Algo bellman_ford step 6086 current loss 0.023995, current_train_items 194784.
I0304 19:30:56.962715 22502662377600 run.py:483] Algo bellman_ford step 6087 current loss 0.072662, current_train_items 194816.
I0304 19:30:56.993093 22502662377600 run.py:483] Algo bellman_ford step 6088 current loss 0.104274, current_train_items 194848.
I0304 19:30:57.027388 22502662377600 run.py:483] Algo bellman_ford step 6089 current loss 0.153854, current_train_items 194880.
I0304 19:30:57.047746 22502662377600 run.py:483] Algo bellman_ford step 6090 current loss 0.009517, current_train_items 194912.
I0304 19:30:57.064007 22502662377600 run.py:483] Algo bellman_ford step 6091 current loss 0.021224, current_train_items 194944.
I0304 19:30:57.087450 22502662377600 run.py:483] Algo bellman_ford step 6092 current loss 0.104244, current_train_items 194976.
I0304 19:30:57.118132 22502662377600 run.py:483] Algo bellman_ford step 6093 current loss 0.080047, current_train_items 195008.
I0304 19:30:57.150917 22502662377600 run.py:483] Algo bellman_ford step 6094 current loss 0.094401, current_train_items 195040.
I0304 19:30:57.170524 22502662377600 run.py:483] Algo bellman_ford step 6095 current loss 0.005906, current_train_items 195072.
I0304 19:30:57.186663 22502662377600 run.py:483] Algo bellman_ford step 6096 current loss 0.020961, current_train_items 195104.
I0304 19:30:57.210646 22502662377600 run.py:483] Algo bellman_ford step 6097 current loss 0.078042, current_train_items 195136.
I0304 19:30:57.241174 22502662377600 run.py:483] Algo bellman_ford step 6098 current loss 0.106678, current_train_items 195168.
I0304 19:30:57.275017 22502662377600 run.py:483] Algo bellman_ford step 6099 current loss 0.119897, current_train_items 195200.
I0304 19:30:57.294705 22502662377600 run.py:483] Algo bellman_ford step 6100 current loss 0.002650, current_train_items 195232.
I0304 19:30:57.302483 22502662377600 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0304 19:30:57.302592 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:57.318814 22502662377600 run.py:483] Algo bellman_ford step 6101 current loss 0.018826, current_train_items 195264.
I0304 19:30:57.343961 22502662377600 run.py:483] Algo bellman_ford step 6102 current loss 0.082875, current_train_items 195296.
I0304 19:30:57.376506 22502662377600 run.py:483] Algo bellman_ford step 6103 current loss 0.091882, current_train_items 195328.
I0304 19:30:57.409526 22502662377600 run.py:483] Algo bellman_ford step 6104 current loss 0.058600, current_train_items 195360.
I0304 19:30:57.429313 22502662377600 run.py:483] Algo bellman_ford step 6105 current loss 0.004229, current_train_items 195392.
I0304 19:30:57.444565 22502662377600 run.py:483] Algo bellman_ford step 6106 current loss 0.034100, current_train_items 195424.
I0304 19:30:57.468359 22502662377600 run.py:483] Algo bellman_ford step 6107 current loss 0.045377, current_train_items 195456.
I0304 19:30:57.499586 22502662377600 run.py:483] Algo bellman_ford step 6108 current loss 0.065800, current_train_items 195488.
I0304 19:30:57.532555 22502662377600 run.py:483] Algo bellman_ford step 6109 current loss 0.069400, current_train_items 195520.
I0304 19:30:57.552161 22502662377600 run.py:483] Algo bellman_ford step 6110 current loss 0.004372, current_train_items 195552.
I0304 19:30:57.568279 22502662377600 run.py:483] Algo bellman_ford step 6111 current loss 0.022321, current_train_items 195584.
I0304 19:30:57.591894 22502662377600 run.py:483] Algo bellman_ford step 6112 current loss 0.126890, current_train_items 195616.
I0304 19:30:57.622525 22502662377600 run.py:483] Algo bellman_ford step 6113 current loss 0.052078, current_train_items 195648.
I0304 19:30:57.655841 22502662377600 run.py:483] Algo bellman_ford step 6114 current loss 0.067642, current_train_items 195680.
I0304 19:30:57.675676 22502662377600 run.py:483] Algo bellman_ford step 6115 current loss 0.068500, current_train_items 195712.
I0304 19:30:57.691769 22502662377600 run.py:483] Algo bellman_ford step 6116 current loss 0.034531, current_train_items 195744.
I0304 19:30:57.715601 22502662377600 run.py:483] Algo bellman_ford step 6117 current loss 0.061644, current_train_items 195776.
I0304 19:30:57.747750 22502662377600 run.py:483] Algo bellman_ford step 6118 current loss 0.093681, current_train_items 195808.
I0304 19:30:57.780744 22502662377600 run.py:483] Algo bellman_ford step 6119 current loss 0.110223, current_train_items 195840.
I0304 19:30:57.800088 22502662377600 run.py:483] Algo bellman_ford step 6120 current loss 0.043159, current_train_items 195872.
I0304 19:30:57.816415 22502662377600 run.py:483] Algo bellman_ford step 6121 current loss 0.037667, current_train_items 195904.
I0304 19:30:57.839739 22502662377600 run.py:483] Algo bellman_ford step 6122 current loss 0.055072, current_train_items 195936.
I0304 19:30:57.871970 22502662377600 run.py:483] Algo bellman_ford step 6123 current loss 0.181791, current_train_items 195968.
I0304 19:30:57.905583 22502662377600 run.py:483] Algo bellman_ford step 6124 current loss 0.140186, current_train_items 196000.
I0304 19:30:57.925157 22502662377600 run.py:483] Algo bellman_ford step 6125 current loss 0.037373, current_train_items 196032.
I0304 19:30:57.940790 22502662377600 run.py:483] Algo bellman_ford step 6126 current loss 0.054738, current_train_items 196064.
I0304 19:30:57.966383 22502662377600 run.py:483] Algo bellman_ford step 6127 current loss 0.148776, current_train_items 196096.
I0304 19:30:57.997476 22502662377600 run.py:483] Algo bellman_ford step 6128 current loss 0.115502, current_train_items 196128.
I0304 19:30:58.030918 22502662377600 run.py:483] Algo bellman_ford step 6129 current loss 0.126285, current_train_items 196160.
I0304 19:30:58.050443 22502662377600 run.py:483] Algo bellman_ford step 6130 current loss 0.008370, current_train_items 196192.
I0304 19:30:58.066419 22502662377600 run.py:483] Algo bellman_ford step 6131 current loss 0.018613, current_train_items 196224.
I0304 19:30:58.090986 22502662377600 run.py:483] Algo bellman_ford step 6132 current loss 0.118268, current_train_items 196256.
I0304 19:30:58.121079 22502662377600 run.py:483] Algo bellman_ford step 6133 current loss 0.095870, current_train_items 196288.
I0304 19:30:58.153559 22502662377600 run.py:483] Algo bellman_ford step 6134 current loss 0.082378, current_train_items 196320.
I0304 19:30:58.173129 22502662377600 run.py:483] Algo bellman_ford step 6135 current loss 0.027263, current_train_items 196352.
I0304 19:30:58.188820 22502662377600 run.py:483] Algo bellman_ford step 6136 current loss 0.019738, current_train_items 196384.
I0304 19:30:58.212662 22502662377600 run.py:483] Algo bellman_ford step 6137 current loss 0.071204, current_train_items 196416.
I0304 19:30:58.244560 22502662377600 run.py:483] Algo bellman_ford step 6138 current loss 0.099483, current_train_items 196448.
I0304 19:30:58.277889 22502662377600 run.py:483] Algo bellman_ford step 6139 current loss 0.119902, current_train_items 196480.
I0304 19:30:58.297713 22502662377600 run.py:483] Algo bellman_ford step 6140 current loss 0.007373, current_train_items 196512.
I0304 19:30:58.313769 22502662377600 run.py:483] Algo bellman_ford step 6141 current loss 0.028843, current_train_items 196544.
I0304 19:30:58.338145 22502662377600 run.py:483] Algo bellman_ford step 6142 current loss 0.038695, current_train_items 196576.
I0304 19:30:58.369636 22502662377600 run.py:483] Algo bellman_ford step 6143 current loss 0.085309, current_train_items 196608.
I0304 19:30:58.402868 22502662377600 run.py:483] Algo bellman_ford step 6144 current loss 0.087593, current_train_items 196640.
I0304 19:30:58.422180 22502662377600 run.py:483] Algo bellman_ford step 6145 current loss 0.019872, current_train_items 196672.
I0304 19:30:58.438375 22502662377600 run.py:483] Algo bellman_ford step 6146 current loss 0.033640, current_train_items 196704.
I0304 19:30:58.461538 22502662377600 run.py:483] Algo bellman_ford step 6147 current loss 0.041365, current_train_items 196736.
I0304 19:30:58.492484 22502662377600 run.py:483] Algo bellman_ford step 6148 current loss 0.071242, current_train_items 196768.
I0304 19:30:58.527300 22502662377600 run.py:483] Algo bellman_ford step 6149 current loss 0.111046, current_train_items 196800.
I0304 19:30:58.547055 22502662377600 run.py:483] Algo bellman_ford step 6150 current loss 0.007851, current_train_items 196832.
I0304 19:30:58.555260 22502662377600 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0304 19:30:58.555364 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:30:58.572049 22502662377600 run.py:483] Algo bellman_ford step 6151 current loss 0.027641, current_train_items 196864.
I0304 19:30:58.595334 22502662377600 run.py:483] Algo bellman_ford step 6152 current loss 0.032583, current_train_items 196896.
I0304 19:30:58.625152 22502662377600 run.py:483] Algo bellman_ford step 6153 current loss 0.136193, current_train_items 196928.
I0304 19:30:58.658796 22502662377600 run.py:483] Algo bellman_ford step 6154 current loss 0.071377, current_train_items 196960.
I0304 19:30:58.678390 22502662377600 run.py:483] Algo bellman_ford step 6155 current loss 0.014414, current_train_items 196992.
I0304 19:30:58.694307 22502662377600 run.py:483] Algo bellman_ford step 6156 current loss 0.047768, current_train_items 197024.
I0304 19:30:58.717975 22502662377600 run.py:483] Algo bellman_ford step 6157 current loss 0.063373, current_train_items 197056.
I0304 19:30:58.748391 22502662377600 run.py:483] Algo bellman_ford step 6158 current loss 0.115626, current_train_items 197088.
I0304 19:30:58.781540 22502662377600 run.py:483] Algo bellman_ford step 6159 current loss 0.058254, current_train_items 197120.
I0304 19:30:58.801298 22502662377600 run.py:483] Algo bellman_ford step 6160 current loss 0.105935, current_train_items 197152.
I0304 19:30:58.817873 22502662377600 run.py:483] Algo bellman_ford step 6161 current loss 0.037570, current_train_items 197184.
I0304 19:30:58.841571 22502662377600 run.py:483] Algo bellman_ford step 6162 current loss 0.160339, current_train_items 197216.
I0304 19:30:58.873915 22502662377600 run.py:483] Algo bellman_ford step 6163 current loss 0.148204, current_train_items 197248.
I0304 19:30:58.907841 22502662377600 run.py:483] Algo bellman_ford step 6164 current loss 0.152108, current_train_items 197280.
I0304 19:30:58.927456 22502662377600 run.py:483] Algo bellman_ford step 6165 current loss 0.010224, current_train_items 197312.
I0304 19:30:58.943694 22502662377600 run.py:483] Algo bellman_ford step 6166 current loss 0.025675, current_train_items 197344.
I0304 19:30:58.967792 22502662377600 run.py:483] Algo bellman_ford step 6167 current loss 0.063573, current_train_items 197376.
I0304 19:30:58.999367 22502662377600 run.py:483] Algo bellman_ford step 6168 current loss 0.100924, current_train_items 197408.
I0304 19:30:59.033287 22502662377600 run.py:483] Algo bellman_ford step 6169 current loss 0.071604, current_train_items 197440.
I0304 19:30:59.053482 22502662377600 run.py:483] Algo bellman_ford step 6170 current loss 0.012145, current_train_items 197472.
I0304 19:30:59.069620 22502662377600 run.py:483] Algo bellman_ford step 6171 current loss 0.020748, current_train_items 197504.
I0304 19:30:59.093232 22502662377600 run.py:483] Algo bellman_ford step 6172 current loss 0.074323, current_train_items 197536.
I0304 19:30:59.125928 22502662377600 run.py:483] Algo bellman_ford step 6173 current loss 0.096339, current_train_items 197568.
I0304 19:30:59.159799 22502662377600 run.py:483] Algo bellman_ford step 6174 current loss 0.100122, current_train_items 197600.
I0304 19:30:59.179685 22502662377600 run.py:483] Algo bellman_ford step 6175 current loss 0.006498, current_train_items 197632.
I0304 19:30:59.196167 22502662377600 run.py:483] Algo bellman_ford step 6176 current loss 0.034388, current_train_items 197664.
I0304 19:30:59.219083 22502662377600 run.py:483] Algo bellman_ford step 6177 current loss 0.049503, current_train_items 197696.
I0304 19:30:59.250277 22502662377600 run.py:483] Algo bellman_ford step 6178 current loss 0.076238, current_train_items 197728.
I0304 19:30:59.284115 22502662377600 run.py:483] Algo bellman_ford step 6179 current loss 0.118534, current_train_items 197760.
I0304 19:30:59.303531 22502662377600 run.py:483] Algo bellman_ford step 6180 current loss 0.008413, current_train_items 197792.
I0304 19:30:59.319746 22502662377600 run.py:483] Algo bellman_ford step 6181 current loss 0.024201, current_train_items 197824.
I0304 19:30:59.343438 22502662377600 run.py:483] Algo bellman_ford step 6182 current loss 0.092817, current_train_items 197856.
I0304 19:30:59.374656 22502662377600 run.py:483] Algo bellman_ford step 6183 current loss 0.126002, current_train_items 197888.
I0304 19:30:59.410611 22502662377600 run.py:483] Algo bellman_ford step 6184 current loss 0.087178, current_train_items 197920.
I0304 19:30:59.430691 22502662377600 run.py:483] Algo bellman_ford step 6185 current loss 0.011698, current_train_items 197952.
I0304 19:30:59.447238 22502662377600 run.py:483] Algo bellman_ford step 6186 current loss 0.069768, current_train_items 197984.
I0304 19:30:59.469951 22502662377600 run.py:483] Algo bellman_ford step 6187 current loss 0.081211, current_train_items 198016.
I0304 19:30:59.500580 22502662377600 run.py:483] Algo bellman_ford step 6188 current loss 0.054347, current_train_items 198048.
I0304 19:30:59.534524 22502662377600 run.py:483] Algo bellman_ford step 6189 current loss 0.102250, current_train_items 198080.
I0304 19:30:59.554484 22502662377600 run.py:483] Algo bellman_ford step 6190 current loss 0.007962, current_train_items 198112.
I0304 19:30:59.571085 22502662377600 run.py:483] Algo bellman_ford step 6191 current loss 0.049669, current_train_items 198144.
I0304 19:30:59.594877 22502662377600 run.py:483] Algo bellman_ford step 6192 current loss 0.064082, current_train_items 198176.
I0304 19:30:59.626942 22502662377600 run.py:483] Algo bellman_ford step 6193 current loss 0.097831, current_train_items 198208.
I0304 19:30:59.659636 22502662377600 run.py:483] Algo bellman_ford step 6194 current loss 0.083405, current_train_items 198240.
I0304 19:30:59.679118 22502662377600 run.py:483] Algo bellman_ford step 6195 current loss 0.003317, current_train_items 198272.
I0304 19:30:59.694731 22502662377600 run.py:483] Algo bellman_ford step 6196 current loss 0.043859, current_train_items 198304.
I0304 19:30:59.719139 22502662377600 run.py:483] Algo bellman_ford step 6197 current loss 0.108596, current_train_items 198336.
I0304 19:30:59.748962 22502662377600 run.py:483] Algo bellman_ford step 6198 current loss 0.081160, current_train_items 198368.
I0304 19:30:59.783422 22502662377600 run.py:483] Algo bellman_ford step 6199 current loss 0.235116, current_train_items 198400.
I0304 19:30:59.803620 22502662377600 run.py:483] Algo bellman_ford step 6200 current loss 0.002943, current_train_items 198432.
I0304 19:30:59.811515 22502662377600 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0304 19:30:59.811618 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:59.828378 22502662377600 run.py:483] Algo bellman_ford step 6201 current loss 0.013684, current_train_items 198464.
I0304 19:30:59.853265 22502662377600 run.py:483] Algo bellman_ford step 6202 current loss 0.062866, current_train_items 198496.
I0304 19:30:59.883621 22502662377600 run.py:483] Algo bellman_ford step 6203 current loss 0.042001, current_train_items 198528.
I0304 19:30:59.916949 22502662377600 run.py:483] Algo bellman_ford step 6204 current loss 0.077665, current_train_items 198560.
I0304 19:30:59.936911 22502662377600 run.py:483] Algo bellman_ford step 6205 current loss 0.005968, current_train_items 198592.
I0304 19:30:59.952061 22502662377600 run.py:483] Algo bellman_ford step 6206 current loss 0.008516, current_train_items 198624.
I0304 19:30:59.975903 22502662377600 run.py:483] Algo bellman_ford step 6207 current loss 0.083284, current_train_items 198656.
I0304 19:31:00.007224 22502662377600 run.py:483] Algo bellman_ford step 6208 current loss 0.125247, current_train_items 198688.
I0304 19:31:00.041816 22502662377600 run.py:483] Algo bellman_ford step 6209 current loss 0.123743, current_train_items 198720.
I0304 19:31:00.061356 22502662377600 run.py:483] Algo bellman_ford step 6210 current loss 0.004145, current_train_items 198752.
I0304 19:31:00.077662 22502662377600 run.py:483] Algo bellman_ford step 6211 current loss 0.055850, current_train_items 198784.
I0304 19:31:00.100737 22502662377600 run.py:483] Algo bellman_ford step 6212 current loss 0.069591, current_train_items 198816.
I0304 19:31:00.130294 22502662377600 run.py:483] Algo bellman_ford step 6213 current loss 0.063072, current_train_items 198848.
I0304 19:31:00.165421 22502662377600 run.py:483] Algo bellman_ford step 6214 current loss 0.106338, current_train_items 198880.
I0304 19:31:00.184977 22502662377600 run.py:483] Algo bellman_ford step 6215 current loss 0.008412, current_train_items 198912.
I0304 19:31:00.201117 22502662377600 run.py:483] Algo bellman_ford step 6216 current loss 0.032173, current_train_items 198944.
I0304 19:31:00.224183 22502662377600 run.py:483] Algo bellman_ford step 6217 current loss 0.103760, current_train_items 198976.
I0304 19:31:00.254930 22502662377600 run.py:483] Algo bellman_ford step 6218 current loss 0.204557, current_train_items 199008.
I0304 19:31:00.287703 22502662377600 run.py:483] Algo bellman_ford step 6219 current loss 0.166815, current_train_items 199040.
I0304 19:31:00.307453 22502662377600 run.py:483] Algo bellman_ford step 6220 current loss 0.020028, current_train_items 199072.
I0304 19:31:00.323063 22502662377600 run.py:483] Algo bellman_ford step 6221 current loss 0.022928, current_train_items 199104.
I0304 19:31:00.347541 22502662377600 run.py:483] Algo bellman_ford step 6222 current loss 0.059746, current_train_items 199136.
I0304 19:31:00.377645 22502662377600 run.py:483] Algo bellman_ford step 6223 current loss 0.057455, current_train_items 199168.
I0304 19:31:00.412004 22502662377600 run.py:483] Algo bellman_ford step 6224 current loss 0.147846, current_train_items 199200.
I0304 19:31:00.431680 22502662377600 run.py:483] Algo bellman_ford step 6225 current loss 0.004232, current_train_items 199232.
I0304 19:31:00.447703 22502662377600 run.py:483] Algo bellman_ford step 6226 current loss 0.019438, current_train_items 199264.
I0304 19:31:00.470610 22502662377600 run.py:483] Algo bellman_ford step 6227 current loss 0.038913, current_train_items 199296.
I0304 19:31:00.500304 22502662377600 run.py:483] Algo bellman_ford step 6228 current loss 0.043745, current_train_items 199328.
I0304 19:31:00.532163 22502662377600 run.py:483] Algo bellman_ford step 6229 current loss 0.070524, current_train_items 199360.
I0304 19:31:00.551813 22502662377600 run.py:483] Algo bellman_ford step 6230 current loss 0.041380, current_train_items 199392.
I0304 19:31:00.568060 22502662377600 run.py:483] Algo bellman_ford step 6231 current loss 0.037804, current_train_items 199424.
I0304 19:31:00.592073 22502662377600 run.py:483] Algo bellman_ford step 6232 current loss 0.045128, current_train_items 199456.
I0304 19:31:00.622099 22502662377600 run.py:483] Algo bellman_ford step 6233 current loss 0.055467, current_train_items 199488.
I0304 19:31:00.655010 22502662377600 run.py:483] Algo bellman_ford step 6234 current loss 0.075847, current_train_items 199520.
I0304 19:31:00.674274 22502662377600 run.py:483] Algo bellman_ford step 6235 current loss 0.003359, current_train_items 199552.
I0304 19:31:00.690436 22502662377600 run.py:483] Algo bellman_ford step 6236 current loss 0.025482, current_train_items 199584.
I0304 19:31:00.713399 22502662377600 run.py:483] Algo bellman_ford step 6237 current loss 0.045573, current_train_items 199616.
I0304 19:31:00.745207 22502662377600 run.py:483] Algo bellman_ford step 6238 current loss 0.074908, current_train_items 199648.
I0304 19:31:00.776158 22502662377600 run.py:483] Algo bellman_ford step 6239 current loss 0.077949, current_train_items 199680.
I0304 19:31:00.795565 22502662377600 run.py:483] Algo bellman_ford step 6240 current loss 0.005483, current_train_items 199712.
I0304 19:31:00.811762 22502662377600 run.py:483] Algo bellman_ford step 6241 current loss 0.051662, current_train_items 199744.
I0304 19:31:00.835653 22502662377600 run.py:483] Algo bellman_ford step 6242 current loss 0.079607, current_train_items 199776.
I0304 19:31:00.867429 22502662377600 run.py:483] Algo bellman_ford step 6243 current loss 0.125456, current_train_items 199808.
I0304 19:31:00.899733 22502662377600 run.py:483] Algo bellman_ford step 6244 current loss 0.101442, current_train_items 199840.
I0304 19:31:00.919467 22502662377600 run.py:483] Algo bellman_ford step 6245 current loss 0.007473, current_train_items 199872.
I0304 19:31:00.935605 22502662377600 run.py:483] Algo bellman_ford step 6246 current loss 0.053215, current_train_items 199904.
I0304 19:31:00.958828 22502662377600 run.py:483] Algo bellman_ford step 6247 current loss 0.055211, current_train_items 199936.
I0304 19:31:00.990706 22502662377600 run.py:483] Algo bellman_ford step 6248 current loss 0.062196, current_train_items 199968.
I0304 19:31:01.023135 22502662377600 run.py:483] Algo bellman_ford step 6249 current loss 0.067353, current_train_items 200000.
I0304 19:31:01.042539 22502662377600 run.py:483] Algo bellman_ford step 6250 current loss 0.006270, current_train_items 200032.
I0304 19:31:01.050706 22502662377600 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0304 19:31:01.050808 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:01.067755 22502662377600 run.py:483] Algo bellman_ford step 6251 current loss 0.025453, current_train_items 200064.
I0304 19:31:01.091696 22502662377600 run.py:483] Algo bellman_ford step 6252 current loss 0.077677, current_train_items 200096.
I0304 19:31:01.124060 22502662377600 run.py:483] Algo bellman_ford step 6253 current loss 0.113514, current_train_items 200128.
I0304 19:31:01.156954 22502662377600 run.py:483] Algo bellman_ford step 6254 current loss 0.061901, current_train_items 200160.
I0304 19:31:01.176467 22502662377600 run.py:483] Algo bellman_ford step 6255 current loss 0.061966, current_train_items 200192.
I0304 19:31:01.192101 22502662377600 run.py:483] Algo bellman_ford step 6256 current loss 0.020133, current_train_items 200224.
I0304 19:31:01.216313 22502662377600 run.py:483] Algo bellman_ford step 6257 current loss 0.049193, current_train_items 200256.
I0304 19:31:01.246815 22502662377600 run.py:483] Algo bellman_ford step 6258 current loss 0.061620, current_train_items 200288.
I0304 19:31:01.279559 22502662377600 run.py:483] Algo bellman_ford step 6259 current loss 0.077980, current_train_items 200320.
I0304 19:31:01.299895 22502662377600 run.py:483] Algo bellman_ford step 6260 current loss 0.018741, current_train_items 200352.
I0304 19:31:01.316321 22502662377600 run.py:483] Algo bellman_ford step 6261 current loss 0.012079, current_train_items 200384.
I0304 19:31:01.339526 22502662377600 run.py:483] Algo bellman_ford step 6262 current loss 0.028373, current_train_items 200416.
I0304 19:31:01.370629 22502662377600 run.py:483] Algo bellman_ford step 6263 current loss 0.066301, current_train_items 200448.
I0304 19:31:01.404295 22502662377600 run.py:483] Algo bellman_ford step 6264 current loss 0.100043, current_train_items 200480.
I0304 19:31:01.424064 22502662377600 run.py:483] Algo bellman_ford step 6265 current loss 0.014716, current_train_items 200512.
I0304 19:31:01.440935 22502662377600 run.py:483] Algo bellman_ford step 6266 current loss 0.034451, current_train_items 200544.
I0304 19:31:01.465704 22502662377600 run.py:483] Algo bellman_ford step 6267 current loss 0.136980, current_train_items 200576.
I0304 19:31:01.496715 22502662377600 run.py:483] Algo bellman_ford step 6268 current loss 0.083575, current_train_items 200608.
I0304 19:31:01.527331 22502662377600 run.py:483] Algo bellman_ford step 6269 current loss 0.123313, current_train_items 200640.
I0304 19:31:01.547357 22502662377600 run.py:483] Algo bellman_ford step 6270 current loss 0.004410, current_train_items 200672.
I0304 19:31:01.563743 22502662377600 run.py:483] Algo bellman_ford step 6271 current loss 0.035779, current_train_items 200704.
I0304 19:31:01.587089 22502662377600 run.py:483] Algo bellman_ford step 6272 current loss 0.104252, current_train_items 200736.
I0304 19:31:01.617483 22502662377600 run.py:483] Algo bellman_ford step 6273 current loss 0.085518, current_train_items 200768.
I0304 19:31:01.652292 22502662377600 run.py:483] Algo bellman_ford step 6274 current loss 0.171557, current_train_items 200800.
I0304 19:31:01.672315 22502662377600 run.py:483] Algo bellman_ford step 6275 current loss 0.010016, current_train_items 200832.
I0304 19:31:01.688756 22502662377600 run.py:483] Algo bellman_ford step 6276 current loss 0.047204, current_train_items 200864.
I0304 19:31:01.712125 22502662377600 run.py:483] Algo bellman_ford step 6277 current loss 0.087015, current_train_items 200896.
I0304 19:31:01.742370 22502662377600 run.py:483] Algo bellman_ford step 6278 current loss 0.149428, current_train_items 200928.
I0304 19:31:01.777028 22502662377600 run.py:483] Algo bellman_ford step 6279 current loss 0.266718, current_train_items 200960.
I0304 19:31:01.796859 22502662377600 run.py:483] Algo bellman_ford step 6280 current loss 0.007891, current_train_items 200992.
I0304 19:31:01.812880 22502662377600 run.py:483] Algo bellman_ford step 6281 current loss 0.055495, current_train_items 201024.
I0304 19:31:01.836631 22502662377600 run.py:483] Algo bellman_ford step 6282 current loss 0.113612, current_train_items 201056.
I0304 19:31:01.867809 22502662377600 run.py:483] Algo bellman_ford step 6283 current loss 0.090172, current_train_items 201088.
I0304 19:31:01.902390 22502662377600 run.py:483] Algo bellman_ford step 6284 current loss 0.169491, current_train_items 201120.
I0304 19:31:01.922311 22502662377600 run.py:483] Algo bellman_ford step 6285 current loss 0.048587, current_train_items 201152.
I0304 19:31:01.938712 22502662377600 run.py:483] Algo bellman_ford step 6286 current loss 0.034915, current_train_items 201184.
I0304 19:31:01.962440 22502662377600 run.py:483] Algo bellman_ford step 6287 current loss 0.088798, current_train_items 201216.
I0304 19:31:01.993872 22502662377600 run.py:483] Algo bellman_ford step 6288 current loss 0.103279, current_train_items 201248.
I0304 19:31:02.027649 22502662377600 run.py:483] Algo bellman_ford step 6289 current loss 0.149736, current_train_items 201280.
I0304 19:31:02.047360 22502662377600 run.py:483] Algo bellman_ford step 6290 current loss 0.009630, current_train_items 201312.
I0304 19:31:02.063775 22502662377600 run.py:483] Algo bellman_ford step 6291 current loss 0.107970, current_train_items 201344.
I0304 19:31:02.087213 22502662377600 run.py:483] Algo bellman_ford step 6292 current loss 0.056190, current_train_items 201376.
I0304 19:31:02.119053 22502662377600 run.py:483] Algo bellman_ford step 6293 current loss 0.152512, current_train_items 201408.
I0304 19:31:02.154226 22502662377600 run.py:483] Algo bellman_ford step 6294 current loss 0.104064, current_train_items 201440.
I0304 19:31:02.174184 22502662377600 run.py:483] Algo bellman_ford step 6295 current loss 0.004884, current_train_items 201472.
I0304 19:31:02.191160 22502662377600 run.py:483] Algo bellman_ford step 6296 current loss 0.040831, current_train_items 201504.
I0304 19:31:02.214388 22502662377600 run.py:483] Algo bellman_ford step 6297 current loss 0.040510, current_train_items 201536.
I0304 19:31:02.245668 22502662377600 run.py:483] Algo bellman_ford step 6298 current loss 0.062053, current_train_items 201568.
I0304 19:31:02.278657 22502662377600 run.py:483] Algo bellman_ford step 6299 current loss 0.069049, current_train_items 201600.
I0304 19:31:02.298559 22502662377600 run.py:483] Algo bellman_ford step 6300 current loss 0.012484, current_train_items 201632.
I0304 19:31:02.306139 22502662377600 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0304 19:31:02.306246 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:31:02.323228 22502662377600 run.py:483] Algo bellman_ford step 6301 current loss 0.053004, current_train_items 201664.
I0304 19:31:02.346997 22502662377600 run.py:483] Algo bellman_ford step 6302 current loss 0.078843, current_train_items 201696.
I0304 19:31:02.378494 22502662377600 run.py:483] Algo bellman_ford step 6303 current loss 0.058109, current_train_items 201728.
I0304 19:31:02.410431 22502662377600 run.py:483] Algo bellman_ford step 6304 current loss 0.069804, current_train_items 201760.
I0304 19:31:02.430607 22502662377600 run.py:483] Algo bellman_ford step 6305 current loss 0.019073, current_train_items 201792.
I0304 19:31:02.446195 22502662377600 run.py:483] Algo bellman_ford step 6306 current loss 0.022439, current_train_items 201824.
I0304 19:31:02.470003 22502662377600 run.py:483] Algo bellman_ford step 6307 current loss 0.111065, current_train_items 201856.
I0304 19:31:02.502002 22502662377600 run.py:483] Algo bellman_ford step 6308 current loss 0.090722, current_train_items 201888.
I0304 19:31:02.535248 22502662377600 run.py:483] Algo bellman_ford step 6309 current loss 0.142358, current_train_items 201920.
I0304 19:31:02.555115 22502662377600 run.py:483] Algo bellman_ford step 6310 current loss 0.011644, current_train_items 201952.
I0304 19:31:02.571156 22502662377600 run.py:483] Algo bellman_ford step 6311 current loss 0.020219, current_train_items 201984.
I0304 19:31:02.593756 22502662377600 run.py:483] Algo bellman_ford step 6312 current loss 0.078544, current_train_items 202016.
I0304 19:31:02.625692 22502662377600 run.py:483] Algo bellman_ford step 6313 current loss 0.192755, current_train_items 202048.
I0304 19:31:02.660111 22502662377600 run.py:483] Algo bellman_ford step 6314 current loss 0.200206, current_train_items 202080.
I0304 19:31:02.680216 22502662377600 run.py:483] Algo bellman_ford step 6315 current loss 0.007944, current_train_items 202112.
I0304 19:31:02.696112 22502662377600 run.py:483] Algo bellman_ford step 6316 current loss 0.031448, current_train_items 202144.
I0304 19:31:02.719828 22502662377600 run.py:483] Algo bellman_ford step 6317 current loss 0.051787, current_train_items 202176.
I0304 19:31:02.750225 22502662377600 run.py:483] Algo bellman_ford step 6318 current loss 0.034875, current_train_items 202208.
I0304 19:31:02.783154 22502662377600 run.py:483] Algo bellman_ford step 6319 current loss 0.126497, current_train_items 202240.
I0304 19:31:02.802901 22502662377600 run.py:483] Algo bellman_ford step 6320 current loss 0.021109, current_train_items 202272.
I0304 19:31:02.819431 22502662377600 run.py:483] Algo bellman_ford step 6321 current loss 0.026622, current_train_items 202304.
I0304 19:31:02.844323 22502662377600 run.py:483] Algo bellman_ford step 6322 current loss 0.060977, current_train_items 202336.
I0304 19:31:02.876656 22502662377600 run.py:483] Algo bellman_ford step 6323 current loss 0.090086, current_train_items 202368.
I0304 19:31:02.911029 22502662377600 run.py:483] Algo bellman_ford step 6324 current loss 0.089769, current_train_items 202400.
I0304 19:31:02.930854 22502662377600 run.py:483] Algo bellman_ford step 6325 current loss 0.006699, current_train_items 202432.
I0304 19:31:02.946748 22502662377600 run.py:483] Algo bellman_ford step 6326 current loss 0.032687, current_train_items 202464.
I0304 19:31:02.971596 22502662377600 run.py:483] Algo bellman_ford step 6327 current loss 0.111557, current_train_items 202496.
I0304 19:31:03.002822 22502662377600 run.py:483] Algo bellman_ford step 6328 current loss 0.124754, current_train_items 202528.
I0304 19:31:03.037208 22502662377600 run.py:483] Algo bellman_ford step 6329 current loss 0.086191, current_train_items 202560.
I0304 19:31:03.056921 22502662377600 run.py:483] Algo bellman_ford step 6330 current loss 0.013235, current_train_items 202592.
I0304 19:31:03.073037 22502662377600 run.py:483] Algo bellman_ford step 6331 current loss 0.035856, current_train_items 202624.
I0304 19:31:03.096413 22502662377600 run.py:483] Algo bellman_ford step 6332 current loss 0.073554, current_train_items 202656.
I0304 19:31:03.127745 22502662377600 run.py:483] Algo bellman_ford step 6333 current loss 0.045740, current_train_items 202688.
I0304 19:31:03.160574 22502662377600 run.py:483] Algo bellman_ford step 6334 current loss 0.085079, current_train_items 202720.
I0304 19:31:03.180291 22502662377600 run.py:483] Algo bellman_ford step 6335 current loss 0.002189, current_train_items 202752.
I0304 19:31:03.196608 22502662377600 run.py:483] Algo bellman_ford step 6336 current loss 0.017614, current_train_items 202784.
I0304 19:31:03.220775 22502662377600 run.py:483] Algo bellman_ford step 6337 current loss 0.047246, current_train_items 202816.
I0304 19:31:03.252839 22502662377600 run.py:483] Algo bellman_ford step 6338 current loss 0.049149, current_train_items 202848.
I0304 19:31:03.286017 22502662377600 run.py:483] Algo bellman_ford step 6339 current loss 0.062497, current_train_items 202880.
I0304 19:31:03.305392 22502662377600 run.py:483] Algo bellman_ford step 6340 current loss 0.003550, current_train_items 202912.
I0304 19:31:03.321145 22502662377600 run.py:483] Algo bellman_ford step 6341 current loss 0.013960, current_train_items 202944.
I0304 19:31:03.346081 22502662377600 run.py:483] Algo bellman_ford step 6342 current loss 0.086438, current_train_items 202976.
I0304 19:31:03.376629 22502662377600 run.py:483] Algo bellman_ford step 6343 current loss 0.036408, current_train_items 203008.
I0304 19:31:03.407846 22502662377600 run.py:483] Algo bellman_ford step 6344 current loss 0.047923, current_train_items 203040.
I0304 19:31:03.427574 22502662377600 run.py:483] Algo bellman_ford step 6345 current loss 0.002813, current_train_items 203072.
I0304 19:31:03.444084 22502662377600 run.py:483] Algo bellman_ford step 6346 current loss 0.078874, current_train_items 203104.
I0304 19:31:03.467735 22502662377600 run.py:483] Algo bellman_ford step 6347 current loss 0.057560, current_train_items 203136.
I0304 19:31:03.498391 22502662377600 run.py:483] Algo bellman_ford step 6348 current loss 0.099672, current_train_items 203168.
I0304 19:31:03.533865 22502662377600 run.py:483] Algo bellman_ford step 6349 current loss 0.073328, current_train_items 203200.
I0304 19:31:03.553650 22502662377600 run.py:483] Algo bellman_ford step 6350 current loss 0.006707, current_train_items 203232.
I0304 19:31:03.561727 22502662377600 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0304 19:31:03.561830 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:31:03.578995 22502662377600 run.py:483] Algo bellman_ford step 6351 current loss 0.041652, current_train_items 203264.
I0304 19:31:03.602608 22502662377600 run.py:483] Algo bellman_ford step 6352 current loss 0.150662, current_train_items 203296.
I0304 19:31:03.634076 22502662377600 run.py:483] Algo bellman_ford step 6353 current loss 0.130377, current_train_items 203328.
I0304 19:31:03.667398 22502662377600 run.py:483] Algo bellman_ford step 6354 current loss 0.105190, current_train_items 203360.
I0304 19:31:03.687145 22502662377600 run.py:483] Algo bellman_ford step 6355 current loss 0.009371, current_train_items 203392.
I0304 19:31:03.703258 22502662377600 run.py:483] Algo bellman_ford step 6356 current loss 0.021179, current_train_items 203424.
I0304 19:31:03.727649 22502662377600 run.py:483] Algo bellman_ford step 6357 current loss 0.051857, current_train_items 203456.
I0304 19:31:03.758319 22502662377600 run.py:483] Algo bellman_ford step 6358 current loss 0.066616, current_train_items 203488.
I0304 19:31:03.792316 22502662377600 run.py:483] Algo bellman_ford step 6359 current loss 0.103571, current_train_items 203520.
I0304 19:31:03.812201 22502662377600 run.py:483] Algo bellman_ford step 6360 current loss 0.010472, current_train_items 203552.
I0304 19:31:03.828736 22502662377600 run.py:483] Algo bellman_ford step 6361 current loss 0.015876, current_train_items 203584.
I0304 19:31:03.851385 22502662377600 run.py:483] Algo bellman_ford step 6362 current loss 0.058518, current_train_items 203616.
I0304 19:31:03.882261 22502662377600 run.py:483] Algo bellman_ford step 6363 current loss 0.119049, current_train_items 203648.
I0304 19:31:03.914825 22502662377600 run.py:483] Algo bellman_ford step 6364 current loss 0.083839, current_train_items 203680.
I0304 19:31:03.934266 22502662377600 run.py:483] Algo bellman_ford step 6365 current loss 0.007454, current_train_items 203712.
I0304 19:31:03.950572 22502662377600 run.py:483] Algo bellman_ford step 6366 current loss 0.021565, current_train_items 203744.
I0304 19:31:03.973969 22502662377600 run.py:483] Algo bellman_ford step 6367 current loss 0.047924, current_train_items 203776.
I0304 19:31:04.006151 22502662377600 run.py:483] Algo bellman_ford step 6368 current loss 0.238962, current_train_items 203808.
I0304 19:31:04.037926 22502662377600 run.py:483] Algo bellman_ford step 6369 current loss 0.049229, current_train_items 203840.
I0304 19:31:04.057642 22502662377600 run.py:483] Algo bellman_ford step 6370 current loss 0.012097, current_train_items 203872.
I0304 19:31:04.074141 22502662377600 run.py:483] Algo bellman_ford step 6371 current loss 0.019983, current_train_items 203904.
I0304 19:31:04.098108 22502662377600 run.py:483] Algo bellman_ford step 6372 current loss 0.093314, current_train_items 203936.
I0304 19:31:04.128457 22502662377600 run.py:483] Algo bellman_ford step 6373 current loss 0.083083, current_train_items 203968.
I0304 19:31:04.161914 22502662377600 run.py:483] Algo bellman_ford step 6374 current loss 0.062516, current_train_items 204000.
I0304 19:31:04.181912 22502662377600 run.py:483] Algo bellman_ford step 6375 current loss 0.013625, current_train_items 204032.
I0304 19:31:04.197821 22502662377600 run.py:483] Algo bellman_ford step 6376 current loss 0.021951, current_train_items 204064.
I0304 19:31:04.221896 22502662377600 run.py:483] Algo bellman_ford step 6377 current loss 0.139091, current_train_items 204096.
I0304 19:31:04.252753 22502662377600 run.py:483] Algo bellman_ford step 6378 current loss 0.096539, current_train_items 204128.
I0304 19:31:04.288020 22502662377600 run.py:483] Algo bellman_ford step 6379 current loss 0.127195, current_train_items 204160.
I0304 19:31:04.307299 22502662377600 run.py:483] Algo bellman_ford step 6380 current loss 0.014441, current_train_items 204192.
I0304 19:31:04.323225 22502662377600 run.py:483] Algo bellman_ford step 6381 current loss 0.035650, current_train_items 204224.
I0304 19:31:04.346998 22502662377600 run.py:483] Algo bellman_ford step 6382 current loss 0.102757, current_train_items 204256.
I0304 19:31:04.377070 22502662377600 run.py:483] Algo bellman_ford step 6383 current loss 0.140354, current_train_items 204288.
I0304 19:31:04.410438 22502662377600 run.py:483] Algo bellman_ford step 6384 current loss 0.112873, current_train_items 204320.
I0304 19:31:04.430658 22502662377600 run.py:483] Algo bellman_ford step 6385 current loss 0.038174, current_train_items 204352.
I0304 19:31:04.446357 22502662377600 run.py:483] Algo bellman_ford step 6386 current loss 0.022763, current_train_items 204384.
I0304 19:31:04.470180 22502662377600 run.py:483] Algo bellman_ford step 6387 current loss 0.077072, current_train_items 204416.
I0304 19:31:04.500483 22502662377600 run.py:483] Algo bellman_ford step 6388 current loss 0.073656, current_train_items 204448.
I0304 19:31:04.533576 22502662377600 run.py:483] Algo bellman_ford step 6389 current loss 0.063177, current_train_items 204480.
I0304 19:31:04.553152 22502662377600 run.py:483] Algo bellman_ford step 6390 current loss 0.004557, current_train_items 204512.
I0304 19:31:04.569663 22502662377600 run.py:483] Algo bellman_ford step 6391 current loss 0.022351, current_train_items 204544.
I0304 19:31:04.592947 22502662377600 run.py:483] Algo bellman_ford step 6392 current loss 0.052945, current_train_items 204576.
I0304 19:31:04.622425 22502662377600 run.py:483] Algo bellman_ford step 6393 current loss 0.094346, current_train_items 204608.
I0304 19:31:04.656237 22502662377600 run.py:483] Algo bellman_ford step 6394 current loss 0.112420, current_train_items 204640.
I0304 19:31:04.675928 22502662377600 run.py:483] Algo bellman_ford step 6395 current loss 0.018763, current_train_items 204672.
I0304 19:31:04.691982 22502662377600 run.py:483] Algo bellman_ford step 6396 current loss 0.020045, current_train_items 204704.
I0304 19:31:04.715163 22502662377600 run.py:483] Algo bellman_ford step 6397 current loss 0.086908, current_train_items 204736.
I0304 19:31:04.745807 22502662377600 run.py:483] Algo bellman_ford step 6398 current loss 0.102352, current_train_items 204768.
I0304 19:31:04.777246 22502662377600 run.py:483] Algo bellman_ford step 6399 current loss 0.112660, current_train_items 204800.
I0304 19:31:04.797048 22502662377600 run.py:483] Algo bellman_ford step 6400 current loss 0.007390, current_train_items 204832.
I0304 19:31:04.804708 22502662377600 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0304 19:31:04.804839 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:31:04.821026 22502662377600 run.py:483] Algo bellman_ford step 6401 current loss 0.034709, current_train_items 204864.
I0304 19:31:04.845054 22502662377600 run.py:483] Algo bellman_ford step 6402 current loss 0.070136, current_train_items 204896.
I0304 19:31:04.876763 22502662377600 run.py:483] Algo bellman_ford step 6403 current loss 0.074195, current_train_items 204928.
I0304 19:31:04.910345 22502662377600 run.py:483] Algo bellman_ford step 6404 current loss 0.097929, current_train_items 204960.
I0304 19:31:04.930277 22502662377600 run.py:483] Algo bellman_ford step 6405 current loss 0.038274, current_train_items 204992.
I0304 19:31:04.946099 22502662377600 run.py:483] Algo bellman_ford step 6406 current loss 0.029696, current_train_items 205024.
I0304 19:31:04.969380 22502662377600 run.py:483] Algo bellman_ford step 6407 current loss 0.067861, current_train_items 205056.
I0304 19:31:05.000737 22502662377600 run.py:483] Algo bellman_ford step 6408 current loss 0.109834, current_train_items 205088.
I0304 19:31:05.035658 22502662377600 run.py:483] Algo bellman_ford step 6409 current loss 0.093119, current_train_items 205120.
I0304 19:31:05.055294 22502662377600 run.py:483] Algo bellman_ford step 6410 current loss 0.010310, current_train_items 205152.
I0304 19:31:05.071114 22502662377600 run.py:483] Algo bellman_ford step 6411 current loss 0.021484, current_train_items 205184.
I0304 19:31:05.094041 22502662377600 run.py:483] Algo bellman_ford step 6412 current loss 0.054545, current_train_items 205216.
I0304 19:31:05.125243 22502662377600 run.py:483] Algo bellman_ford step 6413 current loss 0.081687, current_train_items 205248.
I0304 19:31:05.158902 22502662377600 run.py:483] Algo bellman_ford step 6414 current loss 0.061430, current_train_items 205280.
I0304 19:31:05.178413 22502662377600 run.py:483] Algo bellman_ford step 6415 current loss 0.005545, current_train_items 205312.
I0304 19:31:05.194398 22502662377600 run.py:483] Algo bellman_ford step 6416 current loss 0.018362, current_train_items 205344.
I0304 19:31:05.218747 22502662377600 run.py:483] Algo bellman_ford step 6417 current loss 0.122437, current_train_items 205376.
I0304 19:31:05.251119 22502662377600 run.py:483] Algo bellman_ford step 6418 current loss 0.114535, current_train_items 205408.
I0304 19:31:05.285836 22502662377600 run.py:483] Algo bellman_ford step 6419 current loss 0.222162, current_train_items 205440.
I0304 19:31:05.305564 22502662377600 run.py:483] Algo bellman_ford step 6420 current loss 0.007053, current_train_items 205472.
I0304 19:31:05.321617 22502662377600 run.py:483] Algo bellman_ford step 6421 current loss 0.018976, current_train_items 205504.
I0304 19:31:05.344845 22502662377600 run.py:483] Algo bellman_ford step 6422 current loss 0.062175, current_train_items 205536.
I0304 19:31:05.375350 22502662377600 run.py:483] Algo bellman_ford step 6423 current loss 0.119968, current_train_items 205568.
I0304 19:31:05.408588 22502662377600 run.py:483] Algo bellman_ford step 6424 current loss 0.079295, current_train_items 205600.
I0304 19:31:05.427889 22502662377600 run.py:483] Algo bellman_ford step 6425 current loss 0.005273, current_train_items 205632.
I0304 19:31:05.444282 22502662377600 run.py:483] Algo bellman_ford step 6426 current loss 0.021756, current_train_items 205664.
I0304 19:31:05.468300 22502662377600 run.py:483] Algo bellman_ford step 6427 current loss 0.113824, current_train_items 205696.
I0304 19:31:05.500130 22502662377600 run.py:483] Algo bellman_ford step 6428 current loss 0.120480, current_train_items 205728.
I0304 19:31:05.535677 22502662377600 run.py:483] Algo bellman_ford step 6429 current loss 0.115242, current_train_items 205760.
I0304 19:31:05.555632 22502662377600 run.py:483] Algo bellman_ford step 6430 current loss 0.010075, current_train_items 205792.
I0304 19:31:05.571168 22502662377600 run.py:483] Algo bellman_ford step 6431 current loss 0.072881, current_train_items 205824.
I0304 19:31:05.595391 22502662377600 run.py:483] Algo bellman_ford step 6432 current loss 0.091683, current_train_items 205856.
I0304 19:31:05.626970 22502662377600 run.py:483] Algo bellman_ford step 6433 current loss 0.144418, current_train_items 205888.
I0304 19:31:05.659924 22502662377600 run.py:483] Algo bellman_ford step 6434 current loss 0.207120, current_train_items 205920.
I0304 19:31:05.679638 22502662377600 run.py:483] Algo bellman_ford step 6435 current loss 0.007833, current_train_items 205952.
I0304 19:31:05.695596 22502662377600 run.py:483] Algo bellman_ford step 6436 current loss 0.020304, current_train_items 205984.
I0304 19:31:05.719027 22502662377600 run.py:483] Algo bellman_ford step 6437 current loss 0.058747, current_train_items 206016.
I0304 19:31:05.751052 22502662377600 run.py:483] Algo bellman_ford step 6438 current loss 0.082844, current_train_items 206048.
I0304 19:31:05.785060 22502662377600 run.py:483] Algo bellman_ford step 6439 current loss 0.119759, current_train_items 206080.
I0304 19:31:05.804844 22502662377600 run.py:483] Algo bellman_ford step 6440 current loss 0.019478, current_train_items 206112.
I0304 19:31:05.821117 22502662377600 run.py:483] Algo bellman_ford step 6441 current loss 0.030158, current_train_items 206144.
I0304 19:31:05.843811 22502662377600 run.py:483] Algo bellman_ford step 6442 current loss 0.047573, current_train_items 206176.
I0304 19:31:05.873205 22502662377600 run.py:483] Algo bellman_ford step 6443 current loss 0.076775, current_train_items 206208.
I0304 19:31:05.907104 22502662377600 run.py:483] Algo bellman_ford step 6444 current loss 0.061978, current_train_items 206240.
I0304 19:31:05.926497 22502662377600 run.py:483] Algo bellman_ford step 6445 current loss 0.024010, current_train_items 206272.
I0304 19:31:05.942293 22502662377600 run.py:483] Algo bellman_ford step 6446 current loss 0.051412, current_train_items 206304.
I0304 19:31:05.966768 22502662377600 run.py:483] Algo bellman_ford step 6447 current loss 0.035032, current_train_items 206336.
I0304 19:31:05.998243 22502662377600 run.py:483] Algo bellman_ford step 6448 current loss 0.070455, current_train_items 206368.
I0304 19:31:06.031741 22502662377600 run.py:483] Algo bellman_ford step 6449 current loss 0.079151, current_train_items 206400.
I0304 19:31:06.051367 22502662377600 run.py:483] Algo bellman_ford step 6450 current loss 0.006771, current_train_items 206432.
I0304 19:31:06.059900 22502662377600 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0304 19:31:06.060005 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:06.076740 22502662377600 run.py:483] Algo bellman_ford step 6451 current loss 0.010748, current_train_items 206464.
I0304 19:31:06.101910 22502662377600 run.py:483] Algo bellman_ford step 6452 current loss 0.062629, current_train_items 206496.
I0304 19:31:06.132087 22502662377600 run.py:483] Algo bellman_ford step 6453 current loss 0.068499, current_train_items 206528.
I0304 19:31:06.164087 22502662377600 run.py:483] Algo bellman_ford step 6454 current loss 0.042081, current_train_items 206560.
I0304 19:31:06.183584 22502662377600 run.py:483] Algo bellman_ford step 6455 current loss 0.042817, current_train_items 206592.
I0304 19:31:06.199296 22502662377600 run.py:483] Algo bellman_ford step 6456 current loss 0.012718, current_train_items 206624.
I0304 19:31:06.221827 22502662377600 run.py:483] Algo bellman_ford step 6457 current loss 0.040608, current_train_items 206656.
I0304 19:31:06.252694 22502662377600 run.py:483] Algo bellman_ford step 6458 current loss 0.073004, current_train_items 206688.
I0304 19:31:06.284482 22502662377600 run.py:483] Algo bellman_ford step 6459 current loss 0.087913, current_train_items 206720.
I0304 19:31:06.304545 22502662377600 run.py:483] Algo bellman_ford step 6460 current loss 0.040687, current_train_items 206752.
I0304 19:31:06.320774 22502662377600 run.py:483] Algo bellman_ford step 6461 current loss 0.012422, current_train_items 206784.
I0304 19:31:06.343770 22502662377600 run.py:483] Algo bellman_ford step 6462 current loss 0.144511, current_train_items 206816.
I0304 19:31:06.374009 22502662377600 run.py:483] Algo bellman_ford step 6463 current loss 0.066052, current_train_items 206848.
I0304 19:31:06.409610 22502662377600 run.py:483] Algo bellman_ford step 6464 current loss 0.183245, current_train_items 206880.
I0304 19:31:06.428934 22502662377600 run.py:483] Algo bellman_ford step 6465 current loss 0.004127, current_train_items 206912.
I0304 19:31:06.445192 22502662377600 run.py:483] Algo bellman_ford step 6466 current loss 0.019351, current_train_items 206944.
I0304 19:31:06.470490 22502662377600 run.py:483] Algo bellman_ford step 6467 current loss 0.065462, current_train_items 206976.
I0304 19:31:06.502583 22502662377600 run.py:483] Algo bellman_ford step 6468 current loss 0.069900, current_train_items 207008.
I0304 19:31:06.537482 22502662377600 run.py:483] Algo bellman_ford step 6469 current loss 0.093742, current_train_items 207040.
I0304 19:31:06.557484 22502662377600 run.py:483] Algo bellman_ford step 6470 current loss 0.011451, current_train_items 207072.
I0304 19:31:06.574145 22502662377600 run.py:483] Algo bellman_ford step 6471 current loss 0.032339, current_train_items 207104.
I0304 19:31:06.597035 22502662377600 run.py:483] Algo bellman_ford step 6472 current loss 0.030094, current_train_items 207136.
I0304 19:31:06.628298 22502662377600 run.py:483] Algo bellman_ford step 6473 current loss 0.132727, current_train_items 207168.
I0304 19:31:06.660927 22502662377600 run.py:483] Algo bellman_ford step 6474 current loss 0.166430, current_train_items 207200.
I0304 19:31:06.680469 22502662377600 run.py:483] Algo bellman_ford step 6475 current loss 0.013154, current_train_items 207232.
I0304 19:31:06.696850 22502662377600 run.py:483] Algo bellman_ford step 6476 current loss 0.062105, current_train_items 207264.
I0304 19:31:06.719404 22502662377600 run.py:483] Algo bellman_ford step 6477 current loss 0.066464, current_train_items 207296.
I0304 19:31:06.750507 22502662377600 run.py:483] Algo bellman_ford step 6478 current loss 0.087528, current_train_items 207328.
I0304 19:31:06.785216 22502662377600 run.py:483] Algo bellman_ford step 6479 current loss 0.098524, current_train_items 207360.
I0304 19:31:06.804860 22502662377600 run.py:483] Algo bellman_ford step 6480 current loss 0.010399, current_train_items 207392.
I0304 19:31:06.820592 22502662377600 run.py:483] Algo bellman_ford step 6481 current loss 0.040958, current_train_items 207424.
I0304 19:31:06.844074 22502662377600 run.py:483] Algo bellman_ford step 6482 current loss 0.060408, current_train_items 207456.
I0304 19:31:06.875870 22502662377600 run.py:483] Algo bellman_ford step 6483 current loss 0.074607, current_train_items 207488.
I0304 19:31:06.910093 22502662377600 run.py:483] Algo bellman_ford step 6484 current loss 0.110425, current_train_items 207520.
I0304 19:31:06.930034 22502662377600 run.py:483] Algo bellman_ford step 6485 current loss 0.010510, current_train_items 207552.
I0304 19:31:06.946395 22502662377600 run.py:483] Algo bellman_ford step 6486 current loss 0.031309, current_train_items 207584.
I0304 19:31:06.971398 22502662377600 run.py:483] Algo bellman_ford step 6487 current loss 0.054043, current_train_items 207616.
I0304 19:31:07.001533 22502662377600 run.py:483] Algo bellman_ford step 6488 current loss 0.106481, current_train_items 207648.
I0304 19:31:07.035708 22502662377600 run.py:483] Algo bellman_ford step 6489 current loss 0.102356, current_train_items 207680.
I0304 19:31:07.055683 22502662377600 run.py:483] Algo bellman_ford step 6490 current loss 0.009371, current_train_items 207712.
I0304 19:31:07.071860 22502662377600 run.py:483] Algo bellman_ford step 6491 current loss 0.028794, current_train_items 207744.
I0304 19:31:07.094920 22502662377600 run.py:483] Algo bellman_ford step 6492 current loss 0.057776, current_train_items 207776.
I0304 19:31:07.125710 22502662377600 run.py:483] Algo bellman_ford step 6493 current loss 0.064434, current_train_items 207808.
I0304 19:31:07.158843 22502662377600 run.py:483] Algo bellman_ford step 6494 current loss 0.146527, current_train_items 207840.
I0304 19:31:07.178363 22502662377600 run.py:483] Algo bellman_ford step 6495 current loss 0.006272, current_train_items 207872.
I0304 19:31:07.194212 22502662377600 run.py:483] Algo bellman_ford step 6496 current loss 0.027520, current_train_items 207904.
I0304 19:31:07.218631 22502662377600 run.py:483] Algo bellman_ford step 6497 current loss 0.058154, current_train_items 207936.
I0304 19:31:07.248890 22502662377600 run.py:483] Algo bellman_ford step 6498 current loss 0.043960, current_train_items 207968.
I0304 19:31:07.283631 22502662377600 run.py:483] Algo bellman_ford step 6499 current loss 0.092381, current_train_items 208000.
I0304 19:31:07.303407 22502662377600 run.py:483] Algo bellman_ford step 6500 current loss 0.007391, current_train_items 208032.
I0304 19:31:07.310989 22502662377600 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0304 19:31:07.311092 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:31:07.327654 22502662377600 run.py:483] Algo bellman_ford step 6501 current loss 0.038475, current_train_items 208064.
I0304 19:31:07.352591 22502662377600 run.py:483] Algo bellman_ford step 6502 current loss 0.070247, current_train_items 208096.
I0304 19:31:07.383666 22502662377600 run.py:483] Algo bellman_ford step 6503 current loss 0.056654, current_train_items 208128.
I0304 19:31:07.418944 22502662377600 run.py:483] Algo bellman_ford step 6504 current loss 0.072054, current_train_items 208160.
I0304 19:31:07.439284 22502662377600 run.py:483] Algo bellman_ford step 6505 current loss 0.009303, current_train_items 208192.
I0304 19:31:07.454481 22502662377600 run.py:483] Algo bellman_ford step 6506 current loss 0.009497, current_train_items 208224.
I0304 19:31:07.478491 22502662377600 run.py:483] Algo bellman_ford step 6507 current loss 0.072674, current_train_items 208256.
I0304 19:31:07.510582 22502662377600 run.py:483] Algo bellman_ford step 6508 current loss 0.075220, current_train_items 208288.
I0304 19:31:07.542753 22502662377600 run.py:483] Algo bellman_ford step 6509 current loss 0.111829, current_train_items 208320.
I0304 19:31:07.562452 22502662377600 run.py:483] Algo bellman_ford step 6510 current loss 0.009652, current_train_items 208352.
I0304 19:31:07.578606 22502662377600 run.py:483] Algo bellman_ford step 6511 current loss 0.036439, current_train_items 208384.
I0304 19:31:07.603094 22502662377600 run.py:483] Algo bellman_ford step 6512 current loss 0.053688, current_train_items 208416.
I0304 19:31:07.633932 22502662377600 run.py:483] Algo bellman_ford step 6513 current loss 0.071770, current_train_items 208448.
I0304 19:31:07.668952 22502662377600 run.py:483] Algo bellman_ford step 6514 current loss 0.126320, current_train_items 208480.
I0304 19:31:07.688566 22502662377600 run.py:483] Algo bellman_ford step 6515 current loss 0.007718, current_train_items 208512.
I0304 19:31:07.704454 22502662377600 run.py:483] Algo bellman_ford step 6516 current loss 0.045465, current_train_items 208544.
I0304 19:31:07.728835 22502662377600 run.py:483] Algo bellman_ford step 6517 current loss 0.044056, current_train_items 208576.
I0304 19:31:07.759911 22502662377600 run.py:483] Algo bellman_ford step 6518 current loss 0.098546, current_train_items 208608.
I0304 19:31:07.793107 22502662377600 run.py:483] Algo bellman_ford step 6519 current loss 0.062239, current_train_items 208640.
I0304 19:31:07.812850 22502662377600 run.py:483] Algo bellman_ford step 6520 current loss 0.007584, current_train_items 208672.
I0304 19:31:07.829111 22502662377600 run.py:483] Algo bellman_ford step 6521 current loss 0.023066, current_train_items 208704.
I0304 19:31:07.853620 22502662377600 run.py:483] Algo bellman_ford step 6522 current loss 0.074065, current_train_items 208736.
I0304 19:31:07.884172 22502662377600 run.py:483] Algo bellman_ford step 6523 current loss 0.068204, current_train_items 208768.
I0304 19:31:07.915726 22502662377600 run.py:483] Algo bellman_ford step 6524 current loss 0.094574, current_train_items 208800.
I0304 19:31:07.935804 22502662377600 run.py:483] Algo bellman_ford step 6525 current loss 0.006274, current_train_items 208832.
I0304 19:31:07.951720 22502662377600 run.py:483] Algo bellman_ford step 6526 current loss 0.032875, current_train_items 208864.
I0304 19:31:07.975522 22502662377600 run.py:483] Algo bellman_ford step 6527 current loss 0.084804, current_train_items 208896.
I0304 19:31:08.005908 22502662377600 run.py:483] Algo bellman_ford step 6528 current loss 0.050677, current_train_items 208928.
I0304 19:31:08.039810 22502662377600 run.py:483] Algo bellman_ford step 6529 current loss 0.068060, current_train_items 208960.
I0304 19:31:08.059365 22502662377600 run.py:483] Algo bellman_ford step 6530 current loss 0.002914, current_train_items 208992.
I0304 19:31:08.075567 22502662377600 run.py:483] Algo bellman_ford step 6531 current loss 0.025246, current_train_items 209024.
I0304 19:31:08.100980 22502662377600 run.py:483] Algo bellman_ford step 6532 current loss 0.050998, current_train_items 209056.
I0304 19:31:08.131320 22502662377600 run.py:483] Algo bellman_ford step 6533 current loss 0.090848, current_train_items 209088.
I0304 19:31:08.164667 22502662377600 run.py:483] Algo bellman_ford step 6534 current loss 0.122882, current_train_items 209120.
I0304 19:31:08.184419 22502662377600 run.py:483] Algo bellman_ford step 6535 current loss 0.006408, current_train_items 209152.
I0304 19:31:08.200515 22502662377600 run.py:483] Algo bellman_ford step 6536 current loss 0.020623, current_train_items 209184.
I0304 19:31:08.224363 22502662377600 run.py:483] Algo bellman_ford step 6537 current loss 0.052767, current_train_items 209216.
I0304 19:31:08.254752 22502662377600 run.py:483] Algo bellman_ford step 6538 current loss 0.070477, current_train_items 209248.
I0304 19:31:08.288734 22502662377600 run.py:483] Algo bellman_ford step 6539 current loss 0.109969, current_train_items 209280.
I0304 19:31:08.308671 22502662377600 run.py:483] Algo bellman_ford step 6540 current loss 0.005580, current_train_items 209312.
I0304 19:31:08.324720 22502662377600 run.py:483] Algo bellman_ford step 6541 current loss 0.028395, current_train_items 209344.
I0304 19:31:08.348556 22502662377600 run.py:483] Algo bellman_ford step 6542 current loss 0.066184, current_train_items 209376.
I0304 19:31:08.379901 22502662377600 run.py:483] Algo bellman_ford step 6543 current loss 0.137002, current_train_items 209408.
I0304 19:31:08.415325 22502662377600 run.py:483] Algo bellman_ford step 6544 current loss 0.139764, current_train_items 209440.
I0304 19:31:08.435210 22502662377600 run.py:483] Algo bellman_ford step 6545 current loss 0.005199, current_train_items 209472.
I0304 19:31:08.451013 22502662377600 run.py:483] Algo bellman_ford step 6546 current loss 0.055140, current_train_items 209504.
I0304 19:31:08.474663 22502662377600 run.py:483] Algo bellman_ford step 6547 current loss 0.062316, current_train_items 209536.
I0304 19:31:08.506616 22502662377600 run.py:483] Algo bellman_ford step 6548 current loss 0.128778, current_train_items 209568.
I0304 19:31:08.541584 22502662377600 run.py:483] Algo bellman_ford step 6549 current loss 0.136364, current_train_items 209600.
I0304 19:31:08.561565 22502662377600 run.py:483] Algo bellman_ford step 6550 current loss 0.007825, current_train_items 209632.
I0304 19:31:08.569842 22502662377600 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0304 19:31:08.569946 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:31:08.587121 22502662377600 run.py:483] Algo bellman_ford step 6551 current loss 0.009948, current_train_items 209664.
I0304 19:31:08.611261 22502662377600 run.py:483] Algo bellman_ford step 6552 current loss 0.062428, current_train_items 209696.
I0304 19:31:08.643795 22502662377600 run.py:483] Algo bellman_ford step 6553 current loss 0.102519, current_train_items 209728.
I0304 19:31:08.679327 22502662377600 run.py:483] Algo bellman_ford step 6554 current loss 0.153282, current_train_items 209760.
I0304 19:31:08.699527 22502662377600 run.py:483] Algo bellman_ford step 6555 current loss 0.016711, current_train_items 209792.
I0304 19:31:08.715236 22502662377600 run.py:483] Algo bellman_ford step 6556 current loss 0.035468, current_train_items 209824.
I0304 19:31:08.739601 22502662377600 run.py:483] Algo bellman_ford step 6557 current loss 0.040418, current_train_items 209856.
I0304 19:31:08.771666 22502662377600 run.py:483] Algo bellman_ford step 6558 current loss 0.081226, current_train_items 209888.
I0304 19:31:08.806162 22502662377600 run.py:483] Algo bellman_ford step 6559 current loss 0.173232, current_train_items 209920.
I0304 19:31:08.826559 22502662377600 run.py:483] Algo bellman_ford step 6560 current loss 0.048887, current_train_items 209952.
I0304 19:31:08.842320 22502662377600 run.py:483] Algo bellman_ford step 6561 current loss 0.026038, current_train_items 209984.
I0304 19:31:08.865166 22502662377600 run.py:483] Algo bellman_ford step 6562 current loss 0.065147, current_train_items 210016.
I0304 19:31:08.896002 22502662377600 run.py:483] Algo bellman_ford step 6563 current loss 0.102079, current_train_items 210048.
I0304 19:31:08.927446 22502662377600 run.py:483] Algo bellman_ford step 6564 current loss 0.078000, current_train_items 210080.
I0304 19:31:08.947305 22502662377600 run.py:483] Algo bellman_ford step 6565 current loss 0.005070, current_train_items 210112.
I0304 19:31:08.963034 22502662377600 run.py:483] Algo bellman_ford step 6566 current loss 0.011828, current_train_items 210144.
I0304 19:31:08.987513 22502662377600 run.py:483] Algo bellman_ford step 6567 current loss 0.060018, current_train_items 210176.
I0304 19:31:09.017136 22502662377600 run.py:483] Algo bellman_ford step 6568 current loss 0.099427, current_train_items 210208.
I0304 19:31:09.051093 22502662377600 run.py:483] Algo bellman_ford step 6569 current loss 0.166184, current_train_items 210240.
I0304 19:31:09.071019 22502662377600 run.py:483] Algo bellman_ford step 6570 current loss 0.006187, current_train_items 210272.
I0304 19:31:09.087188 22502662377600 run.py:483] Algo bellman_ford step 6571 current loss 0.031424, current_train_items 210304.
I0304 19:31:09.110516 22502662377600 run.py:483] Algo bellman_ford step 6572 current loss 0.090494, current_train_items 210336.
I0304 19:31:09.141141 22502662377600 run.py:483] Algo bellman_ford step 6573 current loss 0.046539, current_train_items 210368.
I0304 19:31:09.174256 22502662377600 run.py:483] Algo bellman_ford step 6574 current loss 0.120269, current_train_items 210400.
I0304 19:31:09.194192 22502662377600 run.py:483] Algo bellman_ford step 6575 current loss 0.077875, current_train_items 210432.
I0304 19:31:09.210494 22502662377600 run.py:483] Algo bellman_ford step 6576 current loss 0.035888, current_train_items 210464.
I0304 19:31:09.233817 22502662377600 run.py:483] Algo bellman_ford step 6577 current loss 0.031895, current_train_items 210496.
I0304 19:31:09.265046 22502662377600 run.py:483] Algo bellman_ford step 6578 current loss 0.077310, current_train_items 210528.
I0304 19:31:09.298843 22502662377600 run.py:483] Algo bellman_ford step 6579 current loss 0.080197, current_train_items 210560.
I0304 19:31:09.318976 22502662377600 run.py:483] Algo bellman_ford step 6580 current loss 0.057043, current_train_items 210592.
I0304 19:31:09.335144 22502662377600 run.py:483] Algo bellman_ford step 6581 current loss 0.021107, current_train_items 210624.
I0304 19:31:09.359073 22502662377600 run.py:483] Algo bellman_ford step 6582 current loss 0.129592, current_train_items 210656.
I0304 19:31:09.391522 22502662377600 run.py:483] Algo bellman_ford step 6583 current loss 0.168185, current_train_items 210688.
I0304 19:31:09.424311 22502662377600 run.py:483] Algo bellman_ford step 6584 current loss 0.133761, current_train_items 210720.
I0304 19:31:09.444234 22502662377600 run.py:483] Algo bellman_ford step 6585 current loss 0.044206, current_train_items 210752.
I0304 19:31:09.459913 22502662377600 run.py:483] Algo bellman_ford step 6586 current loss 0.049387, current_train_items 210784.
I0304 19:31:09.482589 22502662377600 run.py:483] Algo bellman_ford step 6587 current loss 0.032448, current_train_items 210816.
I0304 19:31:09.512935 22502662377600 run.py:483] Algo bellman_ford step 6588 current loss 0.107678, current_train_items 210848.
I0304 19:31:09.545243 22502662377600 run.py:483] Algo bellman_ford step 6589 current loss 0.125534, current_train_items 210880.
I0304 19:31:09.565365 22502662377600 run.py:483] Algo bellman_ford step 6590 current loss 0.037994, current_train_items 210912.
I0304 19:31:09.581305 22502662377600 run.py:483] Algo bellman_ford step 6591 current loss 0.027816, current_train_items 210944.
I0304 19:31:09.605404 22502662377600 run.py:483] Algo bellman_ford step 6592 current loss 0.081504, current_train_items 210976.
I0304 19:31:09.636522 22502662377600 run.py:483] Algo bellman_ford step 6593 current loss 0.114976, current_train_items 211008.
I0304 19:31:09.669394 22502662377600 run.py:483] Algo bellman_ford step 6594 current loss 0.136227, current_train_items 211040.
I0304 19:31:09.689125 22502662377600 run.py:483] Algo bellman_ford step 6595 current loss 0.005255, current_train_items 211072.
I0304 19:31:09.705092 22502662377600 run.py:483] Algo bellman_ford step 6596 current loss 0.021588, current_train_items 211104.
I0304 19:31:09.729722 22502662377600 run.py:483] Algo bellman_ford step 6597 current loss 0.047527, current_train_items 211136.
I0304 19:31:09.761503 22502662377600 run.py:483] Algo bellman_ford step 6598 current loss 0.069808, current_train_items 211168.
I0304 19:31:09.792406 22502662377600 run.py:483] Algo bellman_ford step 6599 current loss 0.101057, current_train_items 211200.
I0304 19:31:09.812225 22502662377600 run.py:483] Algo bellman_ford step 6600 current loss 0.008655, current_train_items 211232.
I0304 19:31:09.819947 22502662377600 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.96484375, 'score': 0.96484375, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0304 19:31:09.820050 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.965, val scores are: bellman_ford: 0.965
I0304 19:31:09.836774 22502662377600 run.py:483] Algo bellman_ford step 6601 current loss 0.028176, current_train_items 211264.
I0304 19:31:09.859660 22502662377600 run.py:483] Algo bellman_ford step 6602 current loss 0.037290, current_train_items 211296.
I0304 19:31:09.891126 22502662377600 run.py:483] Algo bellman_ford step 6603 current loss 0.074098, current_train_items 211328.
I0304 19:31:09.925073 22502662377600 run.py:483] Algo bellman_ford step 6604 current loss 0.081930, current_train_items 211360.
I0304 19:31:09.944755 22502662377600 run.py:483] Algo bellman_ford step 6605 current loss 0.005003, current_train_items 211392.
I0304 19:31:09.960385 22502662377600 run.py:483] Algo bellman_ford step 6606 current loss 0.030768, current_train_items 211424.
I0304 19:31:09.984072 22502662377600 run.py:483] Algo bellman_ford step 6607 current loss 0.050487, current_train_items 211456.
I0304 19:31:10.015592 22502662377600 run.py:483] Algo bellman_ford step 6608 current loss 0.061836, current_train_items 211488.
I0304 19:31:10.047089 22502662377600 run.py:483] Algo bellman_ford step 6609 current loss 0.060770, current_train_items 211520.
I0304 19:31:10.066710 22502662377600 run.py:483] Algo bellman_ford step 6610 current loss 0.003439, current_train_items 211552.
I0304 19:31:10.082494 22502662377600 run.py:483] Algo bellman_ford step 6611 current loss 0.016618, current_train_items 211584.
I0304 19:31:10.105056 22502662377600 run.py:483] Algo bellman_ford step 6612 current loss 0.034021, current_train_items 211616.
I0304 19:31:10.136533 22502662377600 run.py:483] Algo bellman_ford step 6613 current loss 0.090575, current_train_items 211648.
I0304 19:31:10.172704 22502662377600 run.py:483] Algo bellman_ford step 6614 current loss 0.108028, current_train_items 211680.
I0304 19:31:10.192404 22502662377600 run.py:483] Algo bellman_ford step 6615 current loss 0.007149, current_train_items 211712.
I0304 19:31:10.208294 22502662377600 run.py:483] Algo bellman_ford step 6616 current loss 0.017661, current_train_items 211744.
I0304 19:31:10.232278 22502662377600 run.py:483] Algo bellman_ford step 6617 current loss 0.068698, current_train_items 211776.
I0304 19:31:10.263128 22502662377600 run.py:483] Algo bellman_ford step 6618 current loss 0.067377, current_train_items 211808.
I0304 19:31:10.297353 22502662377600 run.py:483] Algo bellman_ford step 6619 current loss 0.086855, current_train_items 211840.
I0304 19:31:10.316612 22502662377600 run.py:483] Algo bellman_ford step 6620 current loss 0.028150, current_train_items 211872.
I0304 19:31:10.332342 22502662377600 run.py:483] Algo bellman_ford step 6621 current loss 0.030178, current_train_items 211904.
I0304 19:31:10.354790 22502662377600 run.py:483] Algo bellman_ford step 6622 current loss 0.057597, current_train_items 211936.
I0304 19:31:10.386075 22502662377600 run.py:483] Algo bellman_ford step 6623 current loss 0.059615, current_train_items 211968.
I0304 19:31:10.418244 22502662377600 run.py:483] Algo bellman_ford step 6624 current loss 0.071594, current_train_items 212000.
I0304 19:31:10.437704 22502662377600 run.py:483] Algo bellman_ford step 6625 current loss 0.004116, current_train_items 212032.
I0304 19:31:10.453789 22502662377600 run.py:483] Algo bellman_ford step 6626 current loss 0.019694, current_train_items 212064.
I0304 19:31:10.477458 22502662377600 run.py:483] Algo bellman_ford step 6627 current loss 0.094347, current_train_items 212096.
I0304 19:31:10.507286 22502662377600 run.py:483] Algo bellman_ford step 6628 current loss 0.079234, current_train_items 212128.
I0304 19:31:10.539675 22502662377600 run.py:483] Algo bellman_ford step 6629 current loss 0.069395, current_train_items 212160.
I0304 19:31:10.559014 22502662377600 run.py:483] Algo bellman_ford step 6630 current loss 0.007000, current_train_items 212192.
I0304 19:31:10.575121 22502662377600 run.py:483] Algo bellman_ford step 6631 current loss 0.040124, current_train_items 212224.
I0304 19:31:10.599486 22502662377600 run.py:483] Algo bellman_ford step 6632 current loss 0.073970, current_train_items 212256.
I0304 19:31:10.631050 22502662377600 run.py:483] Algo bellman_ford step 6633 current loss 0.090932, current_train_items 212288.
I0304 19:31:10.663381 22502662377600 run.py:483] Algo bellman_ford step 6634 current loss 0.067702, current_train_items 212320.
I0304 19:31:10.682983 22502662377600 run.py:483] Algo bellman_ford step 6635 current loss 0.004189, current_train_items 212352.
I0304 19:31:10.699159 22502662377600 run.py:483] Algo bellman_ford step 6636 current loss 0.043607, current_train_items 212384.
I0304 19:31:10.722815 22502662377600 run.py:483] Algo bellman_ford step 6637 current loss 0.099370, current_train_items 212416.
I0304 19:31:10.753811 22502662377600 run.py:483] Algo bellman_ford step 6638 current loss 0.083731, current_train_items 212448.
I0304 19:31:10.787787 22502662377600 run.py:483] Algo bellman_ford step 6639 current loss 0.128308, current_train_items 212480.
I0304 19:31:10.807121 22502662377600 run.py:483] Algo bellman_ford step 6640 current loss 0.004077, current_train_items 212512.
I0304 19:31:10.823351 22502662377600 run.py:483] Algo bellman_ford step 6641 current loss 0.013008, current_train_items 212544.
I0304 19:31:10.848236 22502662377600 run.py:483] Algo bellman_ford step 6642 current loss 0.100974, current_train_items 212576.
I0304 19:31:10.878556 22502662377600 run.py:483] Algo bellman_ford step 6643 current loss 0.073126, current_train_items 212608.
I0304 19:31:10.912746 22502662377600 run.py:483] Algo bellman_ford step 6644 current loss 0.078206, current_train_items 212640.
I0304 19:31:10.932170 22502662377600 run.py:483] Algo bellman_ford step 6645 current loss 0.004788, current_train_items 212672.
I0304 19:31:10.948126 22502662377600 run.py:483] Algo bellman_ford step 6646 current loss 0.015388, current_train_items 212704.
I0304 19:31:10.971648 22502662377600 run.py:483] Algo bellman_ford step 6647 current loss 0.119320, current_train_items 212736.
I0304 19:31:11.003039 22502662377600 run.py:483] Algo bellman_ford step 6648 current loss 0.178514, current_train_items 212768.
I0304 19:31:11.037287 22502662377600 run.py:483] Algo bellman_ford step 6649 current loss 0.123441, current_train_items 212800.
I0304 19:31:11.056787 22502662377600 run.py:483] Algo bellman_ford step 6650 current loss 0.010683, current_train_items 212832.
I0304 19:31:11.065073 22502662377600 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0304 19:31:11.065180 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:31:11.082212 22502662377600 run.py:483] Algo bellman_ford step 6651 current loss 0.032633, current_train_items 212864.
I0304 19:31:11.107018 22502662377600 run.py:483] Algo bellman_ford step 6652 current loss 0.155822, current_train_items 212896.
I0304 19:31:11.138814 22502662377600 run.py:483] Algo bellman_ford step 6653 current loss 0.092845, current_train_items 212928.
I0304 19:31:11.168663 22502662377600 run.py:483] Algo bellman_ford step 6654 current loss 0.050432, current_train_items 212960.
I0304 19:31:11.188234 22502662377600 run.py:483] Algo bellman_ford step 6655 current loss 0.014368, current_train_items 212992.
I0304 19:31:11.203860 22502662377600 run.py:483] Algo bellman_ford step 6656 current loss 0.043668, current_train_items 213024.
I0304 19:31:11.227369 22502662377600 run.py:483] Algo bellman_ford step 6657 current loss 0.053330, current_train_items 213056.
I0304 19:31:11.258877 22502662377600 run.py:483] Algo bellman_ford step 6658 current loss 0.077589, current_train_items 213088.
I0304 19:31:11.293931 22502662377600 run.py:483] Algo bellman_ford step 6659 current loss 0.152610, current_train_items 213120.
I0304 19:31:11.313984 22502662377600 run.py:483] Algo bellman_ford step 6660 current loss 0.009146, current_train_items 213152.
I0304 19:31:11.330444 22502662377600 run.py:483] Algo bellman_ford step 6661 current loss 0.026941, current_train_items 213184.
I0304 19:31:11.355164 22502662377600 run.py:483] Algo bellman_ford step 6662 current loss 0.077481, current_train_items 213216.
I0304 19:31:11.385037 22502662377600 run.py:483] Algo bellman_ford step 6663 current loss 0.044213, current_train_items 213248.
I0304 19:31:11.418574 22502662377600 run.py:483] Algo bellman_ford step 6664 current loss 0.053249, current_train_items 213280.
I0304 19:31:11.438106 22502662377600 run.py:483] Algo bellman_ford step 6665 current loss 0.004835, current_train_items 213312.
I0304 19:31:11.454042 22502662377600 run.py:483] Algo bellman_ford step 6666 current loss 0.016935, current_train_items 213344.
I0304 19:31:11.479542 22502662377600 run.py:483] Algo bellman_ford step 6667 current loss 0.074619, current_train_items 213376.
I0304 19:31:11.511884 22502662377600 run.py:483] Algo bellman_ford step 6668 current loss 0.064447, current_train_items 213408.
I0304 19:31:11.547133 22502662377600 run.py:483] Algo bellman_ford step 6669 current loss 0.072876, current_train_items 213440.
I0304 19:31:11.567121 22502662377600 run.py:483] Algo bellman_ford step 6670 current loss 0.006087, current_train_items 213472.
I0304 19:31:11.583216 22502662377600 run.py:483] Algo bellman_ford step 6671 current loss 0.029124, current_train_items 213504.
I0304 19:31:11.607017 22502662377600 run.py:483] Algo bellman_ford step 6672 current loss 0.052268, current_train_items 213536.
I0304 19:31:11.638737 22502662377600 run.py:483] Algo bellman_ford step 6673 current loss 0.070787, current_train_items 213568.
I0304 19:31:11.670846 22502662377600 run.py:483] Algo bellman_ford step 6674 current loss 0.073350, current_train_items 213600.
I0304 19:31:11.690765 22502662377600 run.py:483] Algo bellman_ford step 6675 current loss 0.024524, current_train_items 213632.
I0304 19:31:11.706367 22502662377600 run.py:483] Algo bellman_ford step 6676 current loss 0.013329, current_train_items 213664.
I0304 19:31:11.730784 22502662377600 run.py:483] Algo bellman_ford step 6677 current loss 0.094031, current_train_items 213696.
I0304 19:31:11.762936 22502662377600 run.py:483] Algo bellman_ford step 6678 current loss 0.120002, current_train_items 213728.
I0304 19:31:11.798226 22502662377600 run.py:483] Algo bellman_ford step 6679 current loss 0.112347, current_train_items 213760.
I0304 19:31:11.817770 22502662377600 run.py:483] Algo bellman_ford step 6680 current loss 0.004737, current_train_items 213792.
I0304 19:31:11.833849 22502662377600 run.py:483] Algo bellman_ford step 6681 current loss 0.020763, current_train_items 213824.
I0304 19:31:11.858483 22502662377600 run.py:483] Algo bellman_ford step 6682 current loss 0.100441, current_train_items 213856.
I0304 19:31:11.889360 22502662377600 run.py:483] Algo bellman_ford step 6683 current loss 0.099146, current_train_items 213888.
I0304 19:31:11.925749 22502662377600 run.py:483] Algo bellman_ford step 6684 current loss 0.121055, current_train_items 213920.
I0304 19:31:11.945661 22502662377600 run.py:483] Algo bellman_ford step 6685 current loss 0.006402, current_train_items 213952.
I0304 19:31:11.961646 22502662377600 run.py:483] Algo bellman_ford step 6686 current loss 0.021717, current_train_items 213984.
I0304 19:31:11.985156 22502662377600 run.py:483] Algo bellman_ford step 6687 current loss 0.105365, current_train_items 214016.
I0304 19:31:12.016869 22502662377600 run.py:483] Algo bellman_ford step 6688 current loss 0.093311, current_train_items 214048.
I0304 19:31:12.049954 22502662377600 run.py:483] Algo bellman_ford step 6689 current loss 0.069655, current_train_items 214080.
I0304 19:31:12.069957 22502662377600 run.py:483] Algo bellman_ford step 6690 current loss 0.006968, current_train_items 214112.
I0304 19:31:12.085736 22502662377600 run.py:483] Algo bellman_ford step 6691 current loss 0.008811, current_train_items 214144.
I0304 19:31:12.109743 22502662377600 run.py:483] Algo bellman_ford step 6692 current loss 0.043637, current_train_items 214176.
I0304 19:31:12.140858 22502662377600 run.py:483] Algo bellman_ford step 6693 current loss 0.057287, current_train_items 214208.
I0304 19:31:12.175192 22502662377600 run.py:483] Algo bellman_ford step 6694 current loss 0.056665, current_train_items 214240.
I0304 19:31:12.194671 22502662377600 run.py:483] Algo bellman_ford step 6695 current loss 0.003483, current_train_items 214272.
I0304 19:31:12.210947 22502662377600 run.py:483] Algo bellman_ford step 6696 current loss 0.033341, current_train_items 214304.
I0304 19:31:12.234934 22502662377600 run.py:483] Algo bellman_ford step 6697 current loss 0.062570, current_train_items 214336.
I0304 19:31:12.266804 22502662377600 run.py:483] Algo bellman_ford step 6698 current loss 0.052771, current_train_items 214368.
I0304 19:31:12.301457 22502662377600 run.py:483] Algo bellman_ford step 6699 current loss 0.086965, current_train_items 214400.
I0304 19:31:12.321010 22502662377600 run.py:483] Algo bellman_ford step 6700 current loss 0.006854, current_train_items 214432.
I0304 19:31:12.328607 22502662377600 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0304 19:31:12.328711 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:12.345861 22502662377600 run.py:483] Algo bellman_ford step 6701 current loss 0.041443, current_train_items 214464.
I0304 19:31:12.370048 22502662377600 run.py:483] Algo bellman_ford step 6702 current loss 0.040408, current_train_items 214496.
I0304 19:31:12.401961 22502662377600 run.py:483] Algo bellman_ford step 6703 current loss 0.122342, current_train_items 214528.
I0304 19:31:12.435012 22502662377600 run.py:483] Algo bellman_ford step 6704 current loss 0.100806, current_train_items 214560.
I0304 19:31:12.454672 22502662377600 run.py:483] Algo bellman_ford step 6705 current loss 0.005315, current_train_items 214592.
I0304 19:31:12.470432 22502662377600 run.py:483] Algo bellman_ford step 6706 current loss 0.024332, current_train_items 214624.
I0304 19:31:12.495517 22502662377600 run.py:483] Algo bellman_ford step 6707 current loss 0.058611, current_train_items 214656.
I0304 19:31:12.527440 22502662377600 run.py:483] Algo bellman_ford step 6708 current loss 0.026810, current_train_items 214688.
I0304 19:31:12.562121 22502662377600 run.py:483] Algo bellman_ford step 6709 current loss 0.078165, current_train_items 214720.
I0304 19:31:12.582025 22502662377600 run.py:483] Algo bellman_ford step 6710 current loss 0.012348, current_train_items 214752.
I0304 19:31:12.598231 22502662377600 run.py:483] Algo bellman_ford step 6711 current loss 0.030027, current_train_items 214784.
I0304 19:31:12.621232 22502662377600 run.py:483] Algo bellman_ford step 6712 current loss 0.060755, current_train_items 214816.
I0304 19:31:12.653696 22502662377600 run.py:483] Algo bellman_ford step 6713 current loss 0.087794, current_train_items 214848.
I0304 19:31:12.686970 22502662377600 run.py:483] Algo bellman_ford step 6714 current loss 0.070327, current_train_items 214880.
I0304 19:31:12.706409 22502662377600 run.py:483] Algo bellman_ford step 6715 current loss 0.024140, current_train_items 214912.
I0304 19:31:12.722508 22502662377600 run.py:483] Algo bellman_ford step 6716 current loss 0.026928, current_train_items 214944.
I0304 19:31:12.746469 22502662377600 run.py:483] Algo bellman_ford step 6717 current loss 0.049903, current_train_items 214976.
I0304 19:31:12.777888 22502662377600 run.py:483] Algo bellman_ford step 6718 current loss 0.061844, current_train_items 215008.
I0304 19:31:12.813320 22502662377600 run.py:483] Algo bellman_ford step 6719 current loss 0.124636, current_train_items 215040.
I0304 19:31:12.832709 22502662377600 run.py:483] Algo bellman_ford step 6720 current loss 0.003544, current_train_items 215072.
I0304 19:31:12.848801 22502662377600 run.py:483] Algo bellman_ford step 6721 current loss 0.016269, current_train_items 215104.
I0304 19:31:12.871557 22502662377600 run.py:483] Algo bellman_ford step 6722 current loss 0.032627, current_train_items 215136.
I0304 19:31:12.903634 22502662377600 run.py:483] Algo bellman_ford step 6723 current loss 0.082560, current_train_items 215168.
I0304 19:31:12.936458 22502662377600 run.py:483] Algo bellman_ford step 6724 current loss 0.082721, current_train_items 215200.
I0304 19:31:12.955830 22502662377600 run.py:483] Algo bellman_ford step 6725 current loss 0.004877, current_train_items 215232.
I0304 19:31:12.971919 22502662377600 run.py:483] Algo bellman_ford step 6726 current loss 0.043936, current_train_items 215264.
I0304 19:31:12.995140 22502662377600 run.py:483] Algo bellman_ford step 6727 current loss 0.038051, current_train_items 215296.
I0304 19:31:13.025890 22502662377600 run.py:483] Algo bellman_ford step 6728 current loss 0.052453, current_train_items 215328.
I0304 19:31:13.058185 22502662377600 run.py:483] Algo bellman_ford step 6729 current loss 0.073045, current_train_items 215360.
I0304 19:31:13.077667 22502662377600 run.py:483] Algo bellman_ford step 6730 current loss 0.005035, current_train_items 215392.
I0304 19:31:13.093457 22502662377600 run.py:483] Algo bellman_ford step 6731 current loss 0.017601, current_train_items 215424.
I0304 19:31:13.118113 22502662377600 run.py:483] Algo bellman_ford step 6732 current loss 0.099713, current_train_items 215456.
I0304 19:31:13.147921 22502662377600 run.py:483] Algo bellman_ford step 6733 current loss 0.045309, current_train_items 215488.
I0304 19:31:13.179239 22502662377600 run.py:483] Algo bellman_ford step 6734 current loss 0.069340, current_train_items 215520.
I0304 19:31:13.199030 22502662377600 run.py:483] Algo bellman_ford step 6735 current loss 0.004856, current_train_items 215552.
I0304 19:31:13.215219 22502662377600 run.py:483] Algo bellman_ford step 6736 current loss 0.034880, current_train_items 215584.
I0304 19:31:13.239657 22502662377600 run.py:483] Algo bellman_ford step 6737 current loss 0.045070, current_train_items 215616.
I0304 19:31:13.269630 22502662377600 run.py:483] Algo bellman_ford step 6738 current loss 0.037415, current_train_items 215648.
I0304 19:31:13.305018 22502662377600 run.py:483] Algo bellman_ford step 6739 current loss 0.088440, current_train_items 215680.
I0304 19:31:13.324424 22502662377600 run.py:483] Algo bellman_ford step 6740 current loss 0.017449, current_train_items 215712.
I0304 19:31:13.340639 22502662377600 run.py:483] Algo bellman_ford step 6741 current loss 0.013653, current_train_items 215744.
I0304 19:31:13.365157 22502662377600 run.py:483] Algo bellman_ford step 6742 current loss 0.092081, current_train_items 215776.
I0304 19:31:13.395647 22502662377600 run.py:483] Algo bellman_ford step 6743 current loss 0.071225, current_train_items 215808.
I0304 19:31:13.430194 22502662377600 run.py:483] Algo bellman_ford step 6744 current loss 0.080282, current_train_items 215840.
I0304 19:31:13.449860 22502662377600 run.py:483] Algo bellman_ford step 6745 current loss 0.005225, current_train_items 215872.
I0304 19:31:13.466020 22502662377600 run.py:483] Algo bellman_ford step 6746 current loss 0.069355, current_train_items 215904.
I0304 19:31:13.489704 22502662377600 run.py:483] Algo bellman_ford step 6747 current loss 0.060167, current_train_items 215936.
I0304 19:31:13.520175 22502662377600 run.py:483] Algo bellman_ford step 6748 current loss 0.149022, current_train_items 215968.
I0304 19:31:13.554207 22502662377600 run.py:483] Algo bellman_ford step 6749 current loss 0.144566, current_train_items 216000.
I0304 19:31:13.573737 22502662377600 run.py:483] Algo bellman_ford step 6750 current loss 0.006251, current_train_items 216032.
I0304 19:31:13.581984 22502662377600 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0304 19:31:13.582087 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:13.598739 22502662377600 run.py:483] Algo bellman_ford step 6751 current loss 0.024323, current_train_items 216064.
I0304 19:31:13.623388 22502662377600 run.py:483] Algo bellman_ford step 6752 current loss 0.075738, current_train_items 216096.
I0304 19:31:13.656054 22502662377600 run.py:483] Algo bellman_ford step 6753 current loss 0.078773, current_train_items 216128.
I0304 19:31:13.690921 22502662377600 run.py:483] Algo bellman_ford step 6754 current loss 0.067648, current_train_items 216160.
I0304 19:31:13.711014 22502662377600 run.py:483] Algo bellman_ford step 6755 current loss 0.010748, current_train_items 216192.
I0304 19:31:13.727282 22502662377600 run.py:483] Algo bellman_ford step 6756 current loss 0.032857, current_train_items 216224.
I0304 19:31:13.750914 22502662377600 run.py:483] Algo bellman_ford step 6757 current loss 0.031768, current_train_items 216256.
I0304 19:31:13.782896 22502662377600 run.py:483] Algo bellman_ford step 6758 current loss 0.118945, current_train_items 216288.
I0304 19:31:13.818001 22502662377600 run.py:483] Algo bellman_ford step 6759 current loss 0.120538, current_train_items 216320.
I0304 19:31:13.838195 22502662377600 run.py:483] Algo bellman_ford step 6760 current loss 0.022657, current_train_items 216352.
I0304 19:31:13.854651 22502662377600 run.py:483] Algo bellman_ford step 6761 current loss 0.022317, current_train_items 216384.
I0304 19:31:13.878433 22502662377600 run.py:483] Algo bellman_ford step 6762 current loss 0.033430, current_train_items 216416.
I0304 19:31:13.910316 22502662377600 run.py:483] Algo bellman_ford step 6763 current loss 0.130954, current_train_items 216448.
I0304 19:31:13.943428 22502662377600 run.py:483] Algo bellman_ford step 6764 current loss 0.150691, current_train_items 216480.
I0304 19:31:13.963451 22502662377600 run.py:483] Algo bellman_ford step 6765 current loss 0.008801, current_train_items 216512.
I0304 19:31:13.979881 22502662377600 run.py:483] Algo bellman_ford step 6766 current loss 0.091842, current_train_items 216544.
I0304 19:31:14.003666 22502662377600 run.py:483] Algo bellman_ford step 6767 current loss 0.051832, current_train_items 216576.
I0304 19:31:14.034213 22502662377600 run.py:483] Algo bellman_ford step 6768 current loss 0.062921, current_train_items 216608.
I0304 19:31:14.067615 22502662377600 run.py:483] Algo bellman_ford step 6769 current loss 0.163752, current_train_items 216640.
I0304 19:31:14.087363 22502662377600 run.py:483] Algo bellman_ford step 6770 current loss 0.012008, current_train_items 216672.
I0304 19:31:14.103383 22502662377600 run.py:483] Algo bellman_ford step 6771 current loss 0.040864, current_train_items 216704.
I0304 19:31:14.126032 22502662377600 run.py:483] Algo bellman_ford step 6772 current loss 0.035715, current_train_items 216736.
I0304 19:31:14.157814 22502662377600 run.py:483] Algo bellman_ford step 6773 current loss 0.089717, current_train_items 216768.
I0304 19:31:14.189698 22502662377600 run.py:483] Algo bellman_ford step 6774 current loss 0.088104, current_train_items 216800.
I0304 19:31:14.209439 22502662377600 run.py:483] Algo bellman_ford step 6775 current loss 0.009163, current_train_items 216832.
I0304 19:31:14.225693 22502662377600 run.py:483] Algo bellman_ford step 6776 current loss 0.045883, current_train_items 216864.
I0304 19:31:14.248835 22502662377600 run.py:483] Algo bellman_ford step 6777 current loss 0.039837, current_train_items 216896.
I0304 19:31:14.281621 22502662377600 run.py:483] Algo bellman_ford step 6778 current loss 0.098042, current_train_items 216928.
I0304 19:31:14.315028 22502662377600 run.py:483] Algo bellman_ford step 6779 current loss 0.054041, current_train_items 216960.
I0304 19:31:14.334476 22502662377600 run.py:483] Algo bellman_ford step 6780 current loss 0.009286, current_train_items 216992.
I0304 19:31:14.350390 22502662377600 run.py:483] Algo bellman_ford step 6781 current loss 0.010873, current_train_items 217024.
I0304 19:31:14.373616 22502662377600 run.py:483] Algo bellman_ford step 6782 current loss 0.055849, current_train_items 217056.
I0304 19:31:14.406115 22502662377600 run.py:483] Algo bellman_ford step 6783 current loss 0.076137, current_train_items 217088.
I0304 19:31:14.439085 22502662377600 run.py:483] Algo bellman_ford step 6784 current loss 0.081911, current_train_items 217120.
I0304 19:31:14.458857 22502662377600 run.py:483] Algo bellman_ford step 6785 current loss 0.008777, current_train_items 217152.
I0304 19:31:14.474706 22502662377600 run.py:483] Algo bellman_ford step 6786 current loss 0.015806, current_train_items 217184.
I0304 19:31:14.498027 22502662377600 run.py:483] Algo bellman_ford step 6787 current loss 0.084076, current_train_items 217216.
I0304 19:31:14.528243 22502662377600 run.py:483] Algo bellman_ford step 6788 current loss 0.098254, current_train_items 217248.
I0304 19:31:14.562193 22502662377600 run.py:483] Algo bellman_ford step 6789 current loss 0.274852, current_train_items 217280.
I0304 19:31:14.582285 22502662377600 run.py:483] Algo bellman_ford step 6790 current loss 0.006852, current_train_items 217312.
I0304 19:31:14.598311 22502662377600 run.py:483] Algo bellman_ford step 6791 current loss 0.016351, current_train_items 217344.
I0304 19:31:14.622136 22502662377600 run.py:483] Algo bellman_ford step 6792 current loss 0.031836, current_train_items 217376.
I0304 19:31:14.652654 22502662377600 run.py:483] Algo bellman_ford step 6793 current loss 0.048392, current_train_items 217408.
I0304 19:31:14.686181 22502662377600 run.py:483] Algo bellman_ford step 6794 current loss 0.109433, current_train_items 217440.
I0304 19:31:14.705903 22502662377600 run.py:483] Algo bellman_ford step 6795 current loss 0.009520, current_train_items 217472.
I0304 19:31:14.721746 22502662377600 run.py:483] Algo bellman_ford step 6796 current loss 0.014419, current_train_items 217504.
I0304 19:31:14.746608 22502662377600 run.py:483] Algo bellman_ford step 6797 current loss 0.045015, current_train_items 217536.
I0304 19:31:14.777066 22502662377600 run.py:483] Algo bellman_ford step 6798 current loss 0.064191, current_train_items 217568.
I0304 19:31:14.810745 22502662377600 run.py:483] Algo bellman_ford step 6799 current loss 0.071084, current_train_items 217600.
I0304 19:31:14.830642 22502662377600 run.py:483] Algo bellman_ford step 6800 current loss 0.020344, current_train_items 217632.
I0304 19:31:14.838500 22502662377600 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0304 19:31:14.838606 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:14.854930 22502662377600 run.py:483] Algo bellman_ford step 6801 current loss 0.028724, current_train_items 217664.
I0304 19:31:14.879324 22502662377600 run.py:483] Algo bellman_ford step 6802 current loss 0.046581, current_train_items 217696.
I0304 19:31:14.911986 22502662377600 run.py:483] Algo bellman_ford step 6803 current loss 0.071271, current_train_items 217728.
I0304 19:31:14.945886 22502662377600 run.py:483] Algo bellman_ford step 6804 current loss 0.104871, current_train_items 217760.
I0304 19:31:14.965661 22502662377600 run.py:483] Algo bellman_ford step 6805 current loss 0.004715, current_train_items 217792.
I0304 19:31:14.982050 22502662377600 run.py:483] Algo bellman_ford step 6806 current loss 0.010996, current_train_items 217824.
I0304 19:31:15.006255 22502662377600 run.py:483] Algo bellman_ford step 6807 current loss 0.048762, current_train_items 217856.
I0304 19:31:15.036235 22502662377600 run.py:483] Algo bellman_ford step 6808 current loss 0.031129, current_train_items 217888.
I0304 19:31:15.069525 22502662377600 run.py:483] Algo bellman_ford step 6809 current loss 0.073213, current_train_items 217920.
I0304 19:31:15.089319 22502662377600 run.py:483] Algo bellman_ford step 6810 current loss 0.008654, current_train_items 217952.
I0304 19:31:15.104966 22502662377600 run.py:483] Algo bellman_ford step 6811 current loss 0.012302, current_train_items 217984.
I0304 19:31:15.129516 22502662377600 run.py:483] Algo bellman_ford step 6812 current loss 0.047119, current_train_items 218016.
I0304 19:31:15.160093 22502662377600 run.py:483] Algo bellman_ford step 6813 current loss 0.075332, current_train_items 218048.
I0304 19:31:15.191639 22502662377600 run.py:483] Algo bellman_ford step 6814 current loss 0.105587, current_train_items 218080.
I0304 19:31:15.211245 22502662377600 run.py:483] Algo bellman_ford step 6815 current loss 0.012497, current_train_items 218112.
I0304 19:31:15.227358 22502662377600 run.py:483] Algo bellman_ford step 6816 current loss 0.043005, current_train_items 218144.
I0304 19:31:15.249843 22502662377600 run.py:483] Algo bellman_ford step 6817 current loss 0.044498, current_train_items 218176.
I0304 19:31:15.281144 22502662377600 run.py:483] Algo bellman_ford step 6818 current loss 0.074625, current_train_items 218208.
I0304 19:31:15.316591 22502662377600 run.py:483] Algo bellman_ford step 6819 current loss 0.117422, current_train_items 218240.
I0304 19:31:15.336122 22502662377600 run.py:483] Algo bellman_ford step 6820 current loss 0.010531, current_train_items 218272.
I0304 19:31:15.352289 22502662377600 run.py:483] Algo bellman_ford step 6821 current loss 0.013406, current_train_items 218304.
I0304 19:31:15.375622 22502662377600 run.py:483] Algo bellman_ford step 6822 current loss 0.047328, current_train_items 218336.
I0304 19:31:15.406789 22502662377600 run.py:483] Algo bellman_ford step 6823 current loss 0.118407, current_train_items 218368.
I0304 19:31:15.441656 22502662377600 run.py:483] Algo bellman_ford step 6824 current loss 0.119118, current_train_items 218400.
I0304 19:31:15.461321 22502662377600 run.py:483] Algo bellman_ford step 6825 current loss 0.005209, current_train_items 218432.
I0304 19:31:15.477459 22502662377600 run.py:483] Algo bellman_ford step 6826 current loss 0.025520, current_train_items 218464.
I0304 19:31:15.501083 22502662377600 run.py:483] Algo bellman_ford step 6827 current loss 0.071830, current_train_items 218496.
I0304 19:31:15.531576 22502662377600 run.py:483] Algo bellman_ford step 6828 current loss 0.031806, current_train_items 218528.
I0304 19:31:15.567259 22502662377600 run.py:483] Algo bellman_ford step 6829 current loss 0.101325, current_train_items 218560.
I0304 19:31:15.586676 22502662377600 run.py:483] Algo bellman_ford step 6830 current loss 0.006225, current_train_items 218592.
I0304 19:31:15.602510 22502662377600 run.py:483] Algo bellman_ford step 6831 current loss 0.023534, current_train_items 218624.
I0304 19:31:15.627817 22502662377600 run.py:483] Algo bellman_ford step 6832 current loss 0.084472, current_train_items 218656.
I0304 19:31:15.658752 22502662377600 run.py:483] Algo bellman_ford step 6833 current loss 0.063071, current_train_items 218688.
I0304 19:31:15.692306 22502662377600 run.py:483] Algo bellman_ford step 6834 current loss 0.077079, current_train_items 218720.
I0304 19:31:15.711762 22502662377600 run.py:483] Algo bellman_ford step 6835 current loss 0.010266, current_train_items 218752.
I0304 19:31:15.728041 22502662377600 run.py:483] Algo bellman_ford step 6836 current loss 0.050548, current_train_items 218784.
I0304 19:31:15.751517 22502662377600 run.py:483] Algo bellman_ford step 6837 current loss 0.050947, current_train_items 218816.
I0304 19:31:15.783478 22502662377600 run.py:483] Algo bellman_ford step 6838 current loss 0.079445, current_train_items 218848.
I0304 19:31:15.815105 22502662377600 run.py:483] Algo bellman_ford step 6839 current loss 0.086023, current_train_items 218880.
I0304 19:31:15.834444 22502662377600 run.py:483] Algo bellman_ford step 6840 current loss 0.007392, current_train_items 218912.
I0304 19:31:15.850898 22502662377600 run.py:483] Algo bellman_ford step 6841 current loss 0.041482, current_train_items 218944.
I0304 19:31:15.875506 22502662377600 run.py:483] Algo bellman_ford step 6842 current loss 0.038486, current_train_items 218976.
I0304 19:31:15.905797 22502662377600 run.py:483] Algo bellman_ford step 6843 current loss 0.039169, current_train_items 219008.
I0304 19:31:15.938768 22502662377600 run.py:483] Algo bellman_ford step 6844 current loss 0.091306, current_train_items 219040.
I0304 19:31:15.958278 22502662377600 run.py:483] Algo bellman_ford step 6845 current loss 0.007686, current_train_items 219072.
I0304 19:31:15.974898 22502662377600 run.py:483] Algo bellman_ford step 6846 current loss 0.020854, current_train_items 219104.
I0304 19:31:15.999330 22502662377600 run.py:483] Algo bellman_ford step 6847 current loss 0.050302, current_train_items 219136.
I0304 19:31:16.031413 22502662377600 run.py:483] Algo bellman_ford step 6848 current loss 0.082927, current_train_items 219168.
I0304 19:31:16.065491 22502662377600 run.py:483] Algo bellman_ford step 6849 current loss 0.124827, current_train_items 219200.
I0304 19:31:16.085009 22502662377600 run.py:483] Algo bellman_ford step 6850 current loss 0.005304, current_train_items 219232.
I0304 19:31:16.093065 22502662377600 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0304 19:31:16.093168 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:16.109936 22502662377600 run.py:483] Algo bellman_ford step 6851 current loss 0.031942, current_train_items 219264.
I0304 19:31:16.135001 22502662377600 run.py:483] Algo bellman_ford step 6852 current loss 0.026540, current_train_items 219296.
I0304 19:31:16.166691 22502662377600 run.py:483] Algo bellman_ford step 6853 current loss 0.039774, current_train_items 219328.
I0304 19:31:16.201529 22502662377600 run.py:483] Algo bellman_ford step 6854 current loss 0.091589, current_train_items 219360.
I0304 19:31:16.221667 22502662377600 run.py:483] Algo bellman_ford step 6855 current loss 0.003163, current_train_items 219392.
I0304 19:31:16.237245 22502662377600 run.py:483] Algo bellman_ford step 6856 current loss 0.008492, current_train_items 219424.
I0304 19:31:16.261190 22502662377600 run.py:483] Algo bellman_ford step 6857 current loss 0.045478, current_train_items 219456.
I0304 19:31:16.292199 22502662377600 run.py:483] Algo bellman_ford step 6858 current loss 0.053550, current_train_items 219488.
I0304 19:31:16.326285 22502662377600 run.py:483] Algo bellman_ford step 6859 current loss 0.077031, current_train_items 219520.
I0304 19:31:16.346440 22502662377600 run.py:483] Algo bellman_ford step 6860 current loss 0.017051, current_train_items 219552.
I0304 19:31:16.362487 22502662377600 run.py:483] Algo bellman_ford step 6861 current loss 0.042193, current_train_items 219584.
I0304 19:31:16.385446 22502662377600 run.py:483] Algo bellman_ford step 6862 current loss 0.039585, current_train_items 219616.
I0304 19:31:16.416074 22502662377600 run.py:483] Algo bellman_ford step 6863 current loss 0.046636, current_train_items 219648.
I0304 19:31:16.448568 22502662377600 run.py:483] Algo bellman_ford step 6864 current loss 0.088083, current_train_items 219680.
I0304 19:31:16.468502 22502662377600 run.py:483] Algo bellman_ford step 6865 current loss 0.004656, current_train_items 219712.
I0304 19:31:16.484689 22502662377600 run.py:483] Algo bellman_ford step 6866 current loss 0.014312, current_train_items 219744.
I0304 19:31:16.509422 22502662377600 run.py:483] Algo bellman_ford step 6867 current loss 0.070426, current_train_items 219776.
I0304 19:31:16.541212 22502662377600 run.py:483] Algo bellman_ford step 6868 current loss 0.068061, current_train_items 219808.
I0304 19:31:16.577175 22502662377600 run.py:483] Algo bellman_ford step 6869 current loss 0.069455, current_train_items 219840.
I0304 19:31:16.597270 22502662377600 run.py:483] Algo bellman_ford step 6870 current loss 0.007602, current_train_items 219872.
I0304 19:31:16.613708 22502662377600 run.py:483] Algo bellman_ford step 6871 current loss 0.059556, current_train_items 219904.
I0304 19:31:16.637425 22502662377600 run.py:483] Algo bellman_ford step 6872 current loss 0.147592, current_train_items 219936.
I0304 19:31:16.667906 22502662377600 run.py:483] Algo bellman_ford step 6873 current loss 0.149342, current_train_items 219968.
I0304 19:31:16.701523 22502662377600 run.py:483] Algo bellman_ford step 6874 current loss 0.105505, current_train_items 220000.
I0304 19:31:16.721275 22502662377600 run.py:483] Algo bellman_ford step 6875 current loss 0.009734, current_train_items 220032.
I0304 19:31:16.737034 22502662377600 run.py:483] Algo bellman_ford step 6876 current loss 0.017281, current_train_items 220064.
I0304 19:31:16.760867 22502662377600 run.py:483] Algo bellman_ford step 6877 current loss 0.180518, current_train_items 220096.
I0304 19:31:16.792284 22502662377600 run.py:483] Algo bellman_ford step 6878 current loss 0.092797, current_train_items 220128.
I0304 19:31:16.826535 22502662377600 run.py:483] Algo bellman_ford step 6879 current loss 0.218240, current_train_items 220160.
I0304 19:31:16.846482 22502662377600 run.py:483] Algo bellman_ford step 6880 current loss 0.014441, current_train_items 220192.
I0304 19:31:16.862290 22502662377600 run.py:483] Algo bellman_ford step 6881 current loss 0.018146, current_train_items 220224.
I0304 19:31:16.886907 22502662377600 run.py:483] Algo bellman_ford step 6882 current loss 0.034673, current_train_items 220256.
I0304 19:31:16.917172 22502662377600 run.py:483] Algo bellman_ford step 6883 current loss 0.042463, current_train_items 220288.
I0304 19:31:16.951113 22502662377600 run.py:483] Algo bellman_ford step 6884 current loss 0.125575, current_train_items 220320.
I0304 19:31:16.971341 22502662377600 run.py:483] Algo bellman_ford step 6885 current loss 0.006746, current_train_items 220352.
I0304 19:31:16.987593 22502662377600 run.py:483] Algo bellman_ford step 6886 current loss 0.023137, current_train_items 220384.
I0304 19:31:17.011701 22502662377600 run.py:483] Algo bellman_ford step 6887 current loss 0.036565, current_train_items 220416.
I0304 19:31:17.043916 22502662377600 run.py:483] Algo bellman_ford step 6888 current loss 0.073790, current_train_items 220448.
I0304 19:31:17.077852 22502662377600 run.py:483] Algo bellman_ford step 6889 current loss 0.050014, current_train_items 220480.
I0304 19:31:17.097798 22502662377600 run.py:483] Algo bellman_ford step 6890 current loss 0.016723, current_train_items 220512.
I0304 19:31:17.114329 22502662377600 run.py:483] Algo bellman_ford step 6891 current loss 0.043497, current_train_items 220544.
I0304 19:31:17.138493 22502662377600 run.py:483] Algo bellman_ford step 6892 current loss 0.110666, current_train_items 220576.
I0304 19:31:17.170720 22502662377600 run.py:483] Algo bellman_ford step 6893 current loss 0.133638, current_train_items 220608.
I0304 19:31:17.203943 22502662377600 run.py:483] Algo bellman_ford step 6894 current loss 0.127385, current_train_items 220640.
I0304 19:31:17.223882 22502662377600 run.py:483] Algo bellman_ford step 6895 current loss 0.004036, current_train_items 220672.
I0304 19:31:17.239873 22502662377600 run.py:483] Algo bellman_ford step 6896 current loss 0.020651, current_train_items 220704.
I0304 19:31:17.264409 22502662377600 run.py:483] Algo bellman_ford step 6897 current loss 0.140352, current_train_items 220736.
I0304 19:31:17.294636 22502662377600 run.py:483] Algo bellman_ford step 6898 current loss 0.034740, current_train_items 220768.
I0304 19:31:17.329014 22502662377600 run.py:483] Algo bellman_ford step 6899 current loss 0.122453, current_train_items 220800.
I0304 19:31:17.349389 22502662377600 run.py:483] Algo bellman_ford step 6900 current loss 0.015529, current_train_items 220832.
I0304 19:31:17.357219 22502662377600 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0304 19:31:17.357323 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:31:17.373783 22502662377600 run.py:483] Algo bellman_ford step 6901 current loss 0.008709, current_train_items 220864.
I0304 19:31:17.397980 22502662377600 run.py:483] Algo bellman_ford step 6902 current loss 0.034489, current_train_items 220896.
I0304 19:31:17.430711 22502662377600 run.py:483] Algo bellman_ford step 6903 current loss 0.096395, current_train_items 220928.
I0304 19:31:17.466156 22502662377600 run.py:483] Algo bellman_ford step 6904 current loss 0.065160, current_train_items 220960.
I0304 19:31:17.486174 22502662377600 run.py:483] Algo bellman_ford step 6905 current loss 0.007762, current_train_items 220992.
I0304 19:31:17.501568 22502662377600 run.py:483] Algo bellman_ford step 6906 current loss 0.016645, current_train_items 221024.
I0304 19:31:17.525586 22502662377600 run.py:483] Algo bellman_ford step 6907 current loss 0.026746, current_train_items 221056.
I0304 19:31:17.558010 22502662377600 run.py:483] Algo bellman_ford step 6908 current loss 0.088023, current_train_items 221088.
I0304 19:31:17.593277 22502662377600 run.py:483] Algo bellman_ford step 6909 current loss 0.110849, current_train_items 221120.
I0304 19:31:17.612875 22502662377600 run.py:483] Algo bellman_ford step 6910 current loss 0.011166, current_train_items 221152.
I0304 19:31:17.629579 22502662377600 run.py:483] Algo bellman_ford step 6911 current loss 0.037810, current_train_items 221184.
I0304 19:31:17.652556 22502662377600 run.py:483] Algo bellman_ford step 6912 current loss 0.054516, current_train_items 221216.
I0304 19:31:17.684936 22502662377600 run.py:483] Algo bellman_ford step 6913 current loss 0.050225, current_train_items 221248.
I0304 19:31:17.720407 22502662377600 run.py:483] Algo bellman_ford step 6914 current loss 0.066644, current_train_items 221280.
I0304 19:31:17.739727 22502662377600 run.py:483] Algo bellman_ford step 6915 current loss 0.006360, current_train_items 221312.
I0304 19:31:17.755400 22502662377600 run.py:483] Algo bellman_ford step 6916 current loss 0.010886, current_train_items 221344.
I0304 19:31:17.780670 22502662377600 run.py:483] Algo bellman_ford step 6917 current loss 0.082572, current_train_items 221376.
I0304 19:31:17.813052 22502662377600 run.py:483] Algo bellman_ford step 6918 current loss 0.142873, current_train_items 221408.
I0304 19:31:17.848441 22502662377600 run.py:483] Algo bellman_ford step 6919 current loss 0.107272, current_train_items 221440.
I0304 19:31:17.868053 22502662377600 run.py:483] Algo bellman_ford step 6920 current loss 0.010009, current_train_items 221472.
I0304 19:31:17.884227 22502662377600 run.py:483] Algo bellman_ford step 6921 current loss 0.023857, current_train_items 221504.
I0304 19:31:17.907572 22502662377600 run.py:483] Algo bellman_ford step 6922 current loss 0.101306, current_train_items 221536.
I0304 19:31:17.939285 22502662377600 run.py:483] Algo bellman_ford step 6923 current loss 0.177734, current_train_items 221568.
I0304 19:31:17.973069 22502662377600 run.py:483] Algo bellman_ford step 6924 current loss 0.178207, current_train_items 221600.
I0304 19:31:17.993196 22502662377600 run.py:483] Algo bellman_ford step 6925 current loss 0.005573, current_train_items 221632.
I0304 19:31:18.009306 22502662377600 run.py:483] Algo bellman_ford step 6926 current loss 0.032018, current_train_items 221664.
I0304 19:31:18.033658 22502662377600 run.py:483] Algo bellman_ford step 6927 current loss 0.077773, current_train_items 221696.
I0304 19:31:18.065330 22502662377600 run.py:483] Algo bellman_ford step 6928 current loss 0.084975, current_train_items 221728.
I0304 19:31:18.098975 22502662377600 run.py:483] Algo bellman_ford step 6929 current loss 0.074988, current_train_items 221760.
I0304 19:31:18.118894 22502662377600 run.py:483] Algo bellman_ford step 6930 current loss 0.004044, current_train_items 221792.
I0304 19:31:18.135211 22502662377600 run.py:483] Algo bellman_ford step 6931 current loss 0.015635, current_train_items 221824.
I0304 19:31:18.158844 22502662377600 run.py:483] Algo bellman_ford step 6932 current loss 0.045155, current_train_items 221856.
I0304 19:31:18.190451 22502662377600 run.py:483] Algo bellman_ford step 6933 current loss 0.060101, current_train_items 221888.
I0304 19:31:18.225385 22502662377600 run.py:483] Algo bellman_ford step 6934 current loss 0.145440, current_train_items 221920.
I0304 19:31:18.244946 22502662377600 run.py:483] Algo bellman_ford step 6935 current loss 0.004372, current_train_items 221952.
I0304 19:31:18.260828 22502662377600 run.py:483] Algo bellman_ford step 6936 current loss 0.022838, current_train_items 221984.
I0304 19:31:18.284709 22502662377600 run.py:483] Algo bellman_ford step 6937 current loss 0.049706, current_train_items 222016.
I0304 19:31:18.316495 22502662377600 run.py:483] Algo bellman_ford step 6938 current loss 0.039183, current_train_items 222048.
I0304 19:31:18.349152 22502662377600 run.py:483] Algo bellman_ford step 6939 current loss 0.049370, current_train_items 222080.
I0304 19:31:18.369023 22502662377600 run.py:483] Algo bellman_ford step 6940 current loss 0.006959, current_train_items 222112.
I0304 19:31:18.385507 22502662377600 run.py:483] Algo bellman_ford step 6941 current loss 0.028398, current_train_items 222144.
I0304 19:31:18.410229 22502662377600 run.py:483] Algo bellman_ford step 6942 current loss 0.082609, current_train_items 222176.
I0304 19:31:18.441759 22502662377600 run.py:483] Algo bellman_ford step 6943 current loss 0.065586, current_train_items 222208.
I0304 19:31:18.476346 22502662377600 run.py:483] Algo bellman_ford step 6944 current loss 0.085839, current_train_items 222240.
I0304 19:31:18.496251 22502662377600 run.py:483] Algo bellman_ford step 6945 current loss 0.016360, current_train_items 222272.
I0304 19:31:18.512497 22502662377600 run.py:483] Algo bellman_ford step 6946 current loss 0.015091, current_train_items 222304.
I0304 19:31:18.536492 22502662377600 run.py:483] Algo bellman_ford step 6947 current loss 0.116291, current_train_items 222336.
I0304 19:31:18.565920 22502662377600 run.py:483] Algo bellman_ford step 6948 current loss 0.098535, current_train_items 222368.
I0304 19:31:18.600194 22502662377600 run.py:483] Algo bellman_ford step 6949 current loss 0.137178, current_train_items 222400.
I0304 19:31:18.619637 22502662377600 run.py:483] Algo bellman_ford step 6950 current loss 0.007021, current_train_items 222432.
I0304 19:31:18.627864 22502662377600 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0304 19:31:18.627966 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:31:18.644773 22502662377600 run.py:483] Algo bellman_ford step 6951 current loss 0.035430, current_train_items 222464.
I0304 19:31:18.669774 22502662377600 run.py:483] Algo bellman_ford step 6952 current loss 0.170182, current_train_items 222496.
I0304 19:31:18.700536 22502662377600 run.py:483] Algo bellman_ford step 6953 current loss 0.240194, current_train_items 222528.
I0304 19:31:18.735081 22502662377600 run.py:483] Algo bellman_ford step 6954 current loss 0.213500, current_train_items 222560.
I0304 19:31:18.754723 22502662377600 run.py:483] Algo bellman_ford step 6955 current loss 0.009811, current_train_items 222592.
I0304 19:31:18.770530 22502662377600 run.py:483] Algo bellman_ford step 6956 current loss 0.042306, current_train_items 222624.
I0304 19:31:18.795791 22502662377600 run.py:483] Algo bellman_ford step 6957 current loss 0.145781, current_train_items 222656.
I0304 19:31:18.826598 22502662377600 run.py:483] Algo bellman_ford step 6958 current loss 0.091408, current_train_items 222688.
I0304 19:31:18.858506 22502662377600 run.py:483] Algo bellman_ford step 6959 current loss 0.067941, current_train_items 222720.
I0304 19:31:18.878185 22502662377600 run.py:483] Algo bellman_ford step 6960 current loss 0.006886, current_train_items 222752.
I0304 19:31:18.894691 22502662377600 run.py:483] Algo bellman_ford step 6961 current loss 0.072841, current_train_items 222784.
I0304 19:31:18.917646 22502662377600 run.py:483] Algo bellman_ford step 6962 current loss 0.049937, current_train_items 222816.
I0304 19:31:18.948916 22502662377600 run.py:483] Algo bellman_ford step 6963 current loss 0.106364, current_train_items 222848.
I0304 19:31:18.983597 22502662377600 run.py:483] Algo bellman_ford step 6964 current loss 0.114828, current_train_items 222880.
I0304 19:31:19.003175 22502662377600 run.py:483] Algo bellman_ford step 6965 current loss 0.017986, current_train_items 222912.
I0304 19:31:19.019357 22502662377600 run.py:483] Algo bellman_ford step 6966 current loss 0.017443, current_train_items 222944.
I0304 19:31:19.043192 22502662377600 run.py:483] Algo bellman_ford step 6967 current loss 0.030717, current_train_items 222976.
I0304 19:31:19.073467 22502662377600 run.py:483] Algo bellman_ford step 6968 current loss 0.053210, current_train_items 223008.
I0304 19:31:19.107645 22502662377600 run.py:483] Algo bellman_ford step 6969 current loss 0.074600, current_train_items 223040.
I0304 19:31:19.127358 22502662377600 run.py:483] Algo bellman_ford step 6970 current loss 0.007739, current_train_items 223072.
I0304 19:31:19.143581 22502662377600 run.py:483] Algo bellman_ford step 6971 current loss 0.037909, current_train_items 223104.
I0304 19:31:19.166950 22502662377600 run.py:483] Algo bellman_ford step 6972 current loss 0.100014, current_train_items 223136.
I0304 19:31:19.197755 22502662377600 run.py:483] Algo bellman_ford step 6973 current loss 0.062903, current_train_items 223168.
I0304 19:31:19.231848 22502662377600 run.py:483] Algo bellman_ford step 6974 current loss 0.063848, current_train_items 223200.
I0304 19:31:19.251375 22502662377600 run.py:483] Algo bellman_ford step 6975 current loss 0.007333, current_train_items 223232.
I0304 19:31:19.267694 22502662377600 run.py:483] Algo bellman_ford step 6976 current loss 0.021199, current_train_items 223264.
I0304 19:31:19.290924 22502662377600 run.py:483] Algo bellman_ford step 6977 current loss 0.054042, current_train_items 223296.
I0304 19:31:19.321008 22502662377600 run.py:483] Algo bellman_ford step 6978 current loss 0.043317, current_train_items 223328.
I0304 19:31:19.355480 22502662377600 run.py:483] Algo bellman_ford step 6979 current loss 0.070326, current_train_items 223360.
I0304 19:31:19.374852 22502662377600 run.py:483] Algo bellman_ford step 6980 current loss 0.023479, current_train_items 223392.
I0304 19:31:19.391659 22502662377600 run.py:483] Algo bellman_ford step 6981 current loss 0.021402, current_train_items 223424.
I0304 19:31:19.414496 22502662377600 run.py:483] Algo bellman_ford step 6982 current loss 0.048251, current_train_items 223456.
I0304 19:31:19.446518 22502662377600 run.py:483] Algo bellman_ford step 6983 current loss 0.096660, current_train_items 223488.
I0304 19:31:19.480181 22502662377600 run.py:483] Algo bellman_ford step 6984 current loss 0.094524, current_train_items 223520.
I0304 19:31:19.500353 22502662377600 run.py:483] Algo bellman_ford step 6985 current loss 0.009595, current_train_items 223552.
I0304 19:31:19.516364 22502662377600 run.py:483] Algo bellman_ford step 6986 current loss 0.033081, current_train_items 223584.
I0304 19:31:19.539591 22502662377600 run.py:483] Algo bellman_ford step 6987 current loss 0.031833, current_train_items 223616.
I0304 19:31:19.570618 22502662377600 run.py:483] Algo bellman_ford step 6988 current loss 0.059835, current_train_items 223648.
I0304 19:31:19.606394 22502662377600 run.py:483] Algo bellman_ford step 6989 current loss 0.119680, current_train_items 223680.
I0304 19:31:19.626180 22502662377600 run.py:483] Algo bellman_ford step 6990 current loss 0.008288, current_train_items 223712.
I0304 19:31:19.642237 22502662377600 run.py:483] Algo bellman_ford step 6991 current loss 0.046572, current_train_items 223744.
I0304 19:31:19.666001 22502662377600 run.py:483] Algo bellman_ford step 6992 current loss 0.068279, current_train_items 223776.
I0304 19:31:19.697878 22502662377600 run.py:483] Algo bellman_ford step 6993 current loss 0.116555, current_train_items 223808.
I0304 19:31:19.734148 22502662377600 run.py:483] Algo bellman_ford step 6994 current loss 0.128767, current_train_items 223840.
I0304 19:31:19.753626 22502662377600 run.py:483] Algo bellman_ford step 6995 current loss 0.005112, current_train_items 223872.
I0304 19:31:19.770010 22502662377600 run.py:483] Algo bellman_ford step 6996 current loss 0.026118, current_train_items 223904.
I0304 19:31:19.792155 22502662377600 run.py:483] Algo bellman_ford step 6997 current loss 0.062871, current_train_items 223936.
I0304 19:31:19.822443 22502662377600 run.py:483] Algo bellman_ford step 6998 current loss 0.059061, current_train_items 223968.
I0304 19:31:19.856156 22502662377600 run.py:483] Algo bellman_ford step 6999 current loss 0.131096, current_train_items 224000.
I0304 19:31:19.875920 22502662377600 run.py:483] Algo bellman_ford step 7000 current loss 0.010058, current_train_items 224032.
I0304 19:31:19.883850 22502662377600 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0304 19:31:19.883952 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:31:19.900524 22502662377600 run.py:483] Algo bellman_ford step 7001 current loss 0.031286, current_train_items 224064.
I0304 19:31:19.924035 22502662377600 run.py:483] Algo bellman_ford step 7002 current loss 0.033194, current_train_items 224096.
I0304 19:31:19.954262 22502662377600 run.py:483] Algo bellman_ford step 7003 current loss 0.054549, current_train_items 224128.
I0304 19:31:19.987362 22502662377600 run.py:483] Algo bellman_ford step 7004 current loss 0.065190, current_train_items 224160.
I0304 19:31:20.007187 22502662377600 run.py:483] Algo bellman_ford step 7005 current loss 0.003877, current_train_items 224192.
I0304 19:31:20.023178 22502662377600 run.py:483] Algo bellman_ford step 7006 current loss 0.036029, current_train_items 224224.
I0304 19:31:20.048038 22502662377600 run.py:483] Algo bellman_ford step 7007 current loss 0.058618, current_train_items 224256.
I0304 19:31:20.078140 22502662377600 run.py:483] Algo bellman_ford step 7008 current loss 0.069056, current_train_items 224288.
I0304 19:31:20.114014 22502662377600 run.py:483] Algo bellman_ford step 7009 current loss 0.093689, current_train_items 224320.
I0304 19:31:20.133687 22502662377600 run.py:483] Algo bellman_ford step 7010 current loss 0.007189, current_train_items 224352.
I0304 19:31:20.150007 22502662377600 run.py:483] Algo bellman_ford step 7011 current loss 0.011443, current_train_items 224384.
I0304 19:31:20.173788 22502662377600 run.py:483] Algo bellman_ford step 7012 current loss 0.050125, current_train_items 224416.
I0304 19:31:20.204451 22502662377600 run.py:483] Algo bellman_ford step 7013 current loss 0.029780, current_train_items 224448.
I0304 19:31:20.239024 22502662377600 run.py:483] Algo bellman_ford step 7014 current loss 0.102726, current_train_items 224480.
I0304 19:31:20.258703 22502662377600 run.py:483] Algo bellman_ford step 7015 current loss 0.005849, current_train_items 224512.
I0304 19:31:20.274629 22502662377600 run.py:483] Algo bellman_ford step 7016 current loss 0.023091, current_train_items 224544.
I0304 19:31:20.298707 22502662377600 run.py:483] Algo bellman_ford step 7017 current loss 0.031392, current_train_items 224576.
I0304 19:31:20.329088 22502662377600 run.py:483] Algo bellman_ford step 7018 current loss 0.039095, current_train_items 224608.
I0304 19:31:20.362142 22502662377600 run.py:483] Algo bellman_ford step 7019 current loss 0.052999, current_train_items 224640.
I0304 19:31:20.381629 22502662377600 run.py:483] Algo bellman_ford step 7020 current loss 0.004525, current_train_items 224672.
I0304 19:31:20.397561 22502662377600 run.py:483] Algo bellman_ford step 7021 current loss 0.033085, current_train_items 224704.
I0304 19:31:20.421125 22502662377600 run.py:483] Algo bellman_ford step 7022 current loss 0.075129, current_train_items 224736.
I0304 19:31:20.451435 22502662377600 run.py:483] Algo bellman_ford step 7023 current loss 0.063454, current_train_items 224768.
I0304 19:31:20.485691 22502662377600 run.py:483] Algo bellman_ford step 7024 current loss 0.071009, current_train_items 224800.
I0304 19:31:20.505144 22502662377600 run.py:483] Algo bellman_ford step 7025 current loss 0.005358, current_train_items 224832.
I0304 19:31:20.521631 22502662377600 run.py:483] Algo bellman_ford step 7026 current loss 0.029492, current_train_items 224864.
I0304 19:31:20.547171 22502662377600 run.py:483] Algo bellman_ford step 7027 current loss 0.149677, current_train_items 224896.
I0304 19:31:20.580435 22502662377600 run.py:483] Algo bellman_ford step 7028 current loss 0.087911, current_train_items 224928.
I0304 19:31:20.612526 22502662377600 run.py:483] Algo bellman_ford step 7029 current loss 0.058954, current_train_items 224960.
I0304 19:31:20.632187 22502662377600 run.py:483] Algo bellman_ford step 7030 current loss 0.003973, current_train_items 224992.
I0304 19:31:20.648326 22502662377600 run.py:483] Algo bellman_ford step 7031 current loss 0.016684, current_train_items 225024.
I0304 19:31:20.671995 22502662377600 run.py:483] Algo bellman_ford step 7032 current loss 0.067356, current_train_items 225056.
I0304 19:31:20.702959 22502662377600 run.py:483] Algo bellman_ford step 7033 current loss 0.036072, current_train_items 225088.
I0304 19:31:20.738369 22502662377600 run.py:483] Algo bellman_ford step 7034 current loss 0.079635, current_train_items 225120.
I0304 19:31:20.758377 22502662377600 run.py:483] Algo bellman_ford step 7035 current loss 0.010019, current_train_items 225152.
I0304 19:31:20.774638 22502662377600 run.py:483] Algo bellman_ford step 7036 current loss 0.052223, current_train_items 225184.
I0304 19:31:20.798285 22502662377600 run.py:483] Algo bellman_ford step 7037 current loss 0.033783, current_train_items 225216.
I0304 19:31:20.830428 22502662377600 run.py:483] Algo bellman_ford step 7038 current loss 0.065968, current_train_items 225248.
I0304 19:31:20.863533 22502662377600 run.py:483] Algo bellman_ford step 7039 current loss 0.044691, current_train_items 225280.
I0304 19:31:20.883506 22502662377600 run.py:483] Algo bellman_ford step 7040 current loss 0.076739, current_train_items 225312.
I0304 19:31:20.899625 22502662377600 run.py:483] Algo bellman_ford step 7041 current loss 0.018446, current_train_items 225344.
I0304 19:31:20.923509 22502662377600 run.py:483] Algo bellman_ford step 7042 current loss 0.031565, current_train_items 225376.
I0304 19:31:20.955566 22502662377600 run.py:483] Algo bellman_ford step 7043 current loss 0.065013, current_train_items 225408.
I0304 19:31:20.989777 22502662377600 run.py:483] Algo bellman_ford step 7044 current loss 0.079179, current_train_items 225440.
I0304 19:31:21.009707 22502662377600 run.py:483] Algo bellman_ford step 7045 current loss 0.017986, current_train_items 225472.
I0304 19:31:21.026081 22502662377600 run.py:483] Algo bellman_ford step 7046 current loss 0.059694, current_train_items 225504.
I0304 19:31:21.049604 22502662377600 run.py:483] Algo bellman_ford step 7047 current loss 0.111012, current_train_items 225536.
I0304 19:31:21.081456 22502662377600 run.py:483] Algo bellman_ford step 7048 current loss 0.115063, current_train_items 225568.
I0304 19:31:21.114740 22502662377600 run.py:483] Algo bellman_ford step 7049 current loss 0.121181, current_train_items 225600.
I0304 19:31:21.134404 22502662377600 run.py:483] Algo bellman_ford step 7050 current loss 0.004760, current_train_items 225632.
I0304 19:31:21.142494 22502662377600 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0304 19:31:21.142598 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:31:21.159008 22502662377600 run.py:483] Algo bellman_ford step 7051 current loss 0.008339, current_train_items 225664.
I0304 19:31:21.184008 22502662377600 run.py:483] Algo bellman_ford step 7052 current loss 0.081504, current_train_items 225696.
I0304 19:31:21.217018 22502662377600 run.py:483] Algo bellman_ford step 7053 current loss 0.135769, current_train_items 225728.
I0304 19:31:21.252508 22502662377600 run.py:483] Algo bellman_ford step 7054 current loss 0.080014, current_train_items 225760.
I0304 19:31:21.272448 22502662377600 run.py:483] Algo bellman_ford step 7055 current loss 0.004839, current_train_items 225792.
I0304 19:31:21.288339 22502662377600 run.py:483] Algo bellman_ford step 7056 current loss 0.019861, current_train_items 225824.
I0304 19:31:21.313139 22502662377600 run.py:483] Algo bellman_ford step 7057 current loss 0.065274, current_train_items 225856.
I0304 19:31:21.344576 22502662377600 run.py:483] Algo bellman_ford step 7058 current loss 0.085651, current_train_items 225888.
I0304 19:31:21.380618 22502662377600 run.py:483] Algo bellman_ford step 7059 current loss 0.125866, current_train_items 225920.
I0304 19:31:21.400717 22502662377600 run.py:483] Algo bellman_ford step 7060 current loss 0.010462, current_train_items 225952.
I0304 19:31:21.417000 22502662377600 run.py:483] Algo bellman_ford step 7061 current loss 0.033016, current_train_items 225984.
I0304 19:31:21.441636 22502662377600 run.py:483] Algo bellman_ford step 7062 current loss 0.089023, current_train_items 226016.
I0304 19:31:21.470989 22502662377600 run.py:483] Algo bellman_ford step 7063 current loss 0.072788, current_train_items 226048.
I0304 19:31:21.504801 22502662377600 run.py:483] Algo bellman_ford step 7064 current loss 0.130905, current_train_items 226080.
I0304 19:31:21.524664 22502662377600 run.py:483] Algo bellman_ford step 7065 current loss 0.020023, current_train_items 226112.
I0304 19:31:21.541198 22502662377600 run.py:483] Algo bellman_ford step 7066 current loss 0.052548, current_train_items 226144.
I0304 19:31:21.564834 22502662377600 run.py:483] Algo bellman_ford step 7067 current loss 0.035152, current_train_items 226176.
I0304 19:31:21.598136 22502662377600 run.py:483] Algo bellman_ford step 7068 current loss 0.093932, current_train_items 226208.
I0304 19:31:21.633870 22502662377600 run.py:483] Algo bellman_ford step 7069 current loss 0.086136, current_train_items 226240.
I0304 19:31:21.653755 22502662377600 run.py:483] Algo bellman_ford step 7070 current loss 0.061230, current_train_items 226272.
I0304 19:31:21.669922 22502662377600 run.py:483] Algo bellman_ford step 7071 current loss 0.058091, current_train_items 226304.
I0304 19:31:21.691989 22502662377600 run.py:483] Algo bellman_ford step 7072 current loss 0.067010, current_train_items 226336.
I0304 19:31:21.722836 22502662377600 run.py:483] Algo bellman_ford step 7073 current loss 0.071066, current_train_items 226368.
I0304 19:31:21.756385 22502662377600 run.py:483] Algo bellman_ford step 7074 current loss 0.073879, current_train_items 226400.
I0304 19:31:21.776538 22502662377600 run.py:483] Algo bellman_ford step 7075 current loss 0.007667, current_train_items 226432.
I0304 19:31:21.792089 22502662377600 run.py:483] Algo bellman_ford step 7076 current loss 0.013898, current_train_items 226464.
I0304 19:31:21.815443 22502662377600 run.py:483] Algo bellman_ford step 7077 current loss 0.108594, current_train_items 226496.
I0304 19:31:21.847228 22502662377600 run.py:483] Algo bellman_ford step 7078 current loss 0.088560, current_train_items 226528.
I0304 19:31:21.881520 22502662377600 run.py:483] Algo bellman_ford step 7079 current loss 0.132364, current_train_items 226560.
I0304 19:31:21.901257 22502662377600 run.py:483] Algo bellman_ford step 7080 current loss 0.008332, current_train_items 226592.
I0304 19:31:21.917091 22502662377600 run.py:483] Algo bellman_ford step 7081 current loss 0.032315, current_train_items 226624.
I0304 19:31:21.941139 22502662377600 run.py:483] Algo bellman_ford step 7082 current loss 0.133814, current_train_items 226656.
I0304 19:31:21.972429 22502662377600 run.py:483] Algo bellman_ford step 7083 current loss 0.257894, current_train_items 226688.
I0304 19:31:22.005558 22502662377600 run.py:483] Algo bellman_ford step 7084 current loss 0.170397, current_train_items 226720.
I0304 19:31:22.025379 22502662377600 run.py:483] Algo bellman_ford step 7085 current loss 0.006402, current_train_items 226752.
I0304 19:31:22.041337 22502662377600 run.py:483] Algo bellman_ford step 7086 current loss 0.040319, current_train_items 226784.
I0304 19:31:22.064038 22502662377600 run.py:483] Algo bellman_ford step 7087 current loss 0.022315, current_train_items 226816.
I0304 19:31:22.095498 22502662377600 run.py:483] Algo bellman_ford step 7088 current loss 0.090268, current_train_items 226848.
I0304 19:31:22.129124 22502662377600 run.py:483] Algo bellman_ford step 7089 current loss 0.055034, current_train_items 226880.
I0304 19:31:22.149487 22502662377600 run.py:483] Algo bellman_ford step 7090 current loss 0.007509, current_train_items 226912.
I0304 19:31:22.165433 22502662377600 run.py:483] Algo bellman_ford step 7091 current loss 0.029195, current_train_items 226944.
I0304 19:31:22.189298 22502662377600 run.py:483] Algo bellman_ford step 7092 current loss 0.083782, current_train_items 226976.
I0304 19:31:22.221596 22502662377600 run.py:483] Algo bellman_ford step 7093 current loss 0.143593, current_train_items 227008.
I0304 19:31:22.253555 22502662377600 run.py:483] Algo bellman_ford step 7094 current loss 0.114113, current_train_items 227040.
I0304 19:31:22.273404 22502662377600 run.py:483] Algo bellman_ford step 7095 current loss 0.005701, current_train_items 227072.
I0304 19:31:22.289335 22502662377600 run.py:483] Algo bellman_ford step 7096 current loss 0.027444, current_train_items 227104.
I0304 19:31:22.312127 22502662377600 run.py:483] Algo bellman_ford step 7097 current loss 0.083743, current_train_items 227136.
I0304 19:31:22.344219 22502662377600 run.py:483] Algo bellman_ford step 7098 current loss 0.149971, current_train_items 227168.
I0304 19:31:22.377017 22502662377600 run.py:483] Algo bellman_ford step 7099 current loss 0.107930, current_train_items 227200.
I0304 19:31:22.397223 22502662377600 run.py:483] Algo bellman_ford step 7100 current loss 0.004018, current_train_items 227232.
I0304 19:31:22.405182 22502662377600 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0304 19:31:22.405286 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:31:22.421875 22502662377600 run.py:483] Algo bellman_ford step 7101 current loss 0.037473, current_train_items 227264.
I0304 19:31:22.445881 22502662377600 run.py:483] Algo bellman_ford step 7102 current loss 0.040294, current_train_items 227296.
I0304 19:31:22.477515 22502662377600 run.py:483] Algo bellman_ford step 7103 current loss 0.073842, current_train_items 227328.
I0304 19:31:22.512741 22502662377600 run.py:483] Algo bellman_ford step 7104 current loss 0.119347, current_train_items 227360.
I0304 19:31:22.532475 22502662377600 run.py:483] Algo bellman_ford step 7105 current loss 0.026855, current_train_items 227392.
I0304 19:31:22.548406 22502662377600 run.py:483] Algo bellman_ford step 7106 current loss 0.029000, current_train_items 227424.
I0304 19:31:22.572506 22502662377600 run.py:483] Algo bellman_ford step 7107 current loss 0.039960, current_train_items 227456.
I0304 19:31:22.603408 22502662377600 run.py:483] Algo bellman_ford step 7108 current loss 0.082607, current_train_items 227488.
I0304 19:31:22.638911 22502662377600 run.py:483] Algo bellman_ford step 7109 current loss 0.081414, current_train_items 227520.
I0304 19:31:22.658714 22502662377600 run.py:483] Algo bellman_ford step 7110 current loss 0.007032, current_train_items 227552.
I0304 19:31:22.675039 22502662377600 run.py:483] Algo bellman_ford step 7111 current loss 0.031710, current_train_items 227584.
I0304 19:31:22.698767 22502662377600 run.py:483] Algo bellman_ford step 7112 current loss 0.052876, current_train_items 227616.
I0304 19:31:22.730446 22502662377600 run.py:483] Algo bellman_ford step 7113 current loss 0.103275, current_train_items 227648.
I0304 19:31:22.764950 22502662377600 run.py:483] Algo bellman_ford step 7114 current loss 0.093630, current_train_items 227680.
I0304 19:31:22.784605 22502662377600 run.py:483] Algo bellman_ford step 7115 current loss 0.002952, current_train_items 227712.
I0304 19:31:22.800766 22502662377600 run.py:483] Algo bellman_ford step 7116 current loss 0.047372, current_train_items 227744.
I0304 19:31:22.824898 22502662377600 run.py:483] Algo bellman_ford step 7117 current loss 0.065715, current_train_items 227776.
I0304 19:31:22.856417 22502662377600 run.py:483] Algo bellman_ford step 7118 current loss 0.096205, current_train_items 227808.
I0304 19:31:22.890369 22502662377600 run.py:483] Algo bellman_ford step 7119 current loss 0.045656, current_train_items 227840.
I0304 19:31:22.909688 22502662377600 run.py:483] Algo bellman_ford step 7120 current loss 0.004892, current_train_items 227872.
I0304 19:31:22.925854 22502662377600 run.py:483] Algo bellman_ford step 7121 current loss 0.054336, current_train_items 227904.
I0304 19:31:22.949435 22502662377600 run.py:483] Algo bellman_ford step 7122 current loss 0.096880, current_train_items 227936.
I0304 19:31:22.980236 22502662377600 run.py:483] Algo bellman_ford step 7123 current loss 0.132854, current_train_items 227968.
I0304 19:31:23.014431 22502662377600 run.py:483] Algo bellman_ford step 7124 current loss 0.092162, current_train_items 228000.
I0304 19:31:23.033843 22502662377600 run.py:483] Algo bellman_ford step 7125 current loss 0.003271, current_train_items 228032.
I0304 19:31:23.049264 22502662377600 run.py:483] Algo bellman_ford step 7126 current loss 0.010001, current_train_items 228064.
I0304 19:31:23.072242 22502662377600 run.py:483] Algo bellman_ford step 7127 current loss 0.040677, current_train_items 228096.
I0304 19:31:23.102566 22502662377600 run.py:483] Algo bellman_ford step 7128 current loss 0.107115, current_train_items 228128.
I0304 19:31:23.137578 22502662377600 run.py:483] Algo bellman_ford step 7129 current loss 0.098418, current_train_items 228160.
I0304 19:31:23.157185 22502662377600 run.py:483] Algo bellman_ford step 7130 current loss 0.022906, current_train_items 228192.
I0304 19:31:23.173666 22502662377600 run.py:483] Algo bellman_ford step 7131 current loss 0.036267, current_train_items 228224.
I0304 19:31:23.198203 22502662377600 run.py:483] Algo bellman_ford step 7132 current loss 0.079102, current_train_items 228256.
I0304 19:31:23.229624 22502662377600 run.py:483] Algo bellman_ford step 7133 current loss 0.117094, current_train_items 228288.
I0304 19:31:23.262953 22502662377600 run.py:483] Algo bellman_ford step 7134 current loss 0.078385, current_train_items 228320.
I0304 19:31:23.282432 22502662377600 run.py:483] Algo bellman_ford step 7135 current loss 0.075167, current_train_items 228352.
I0304 19:31:23.298248 22502662377600 run.py:483] Algo bellman_ford step 7136 current loss 0.017870, current_train_items 228384.
I0304 19:31:23.321329 22502662377600 run.py:483] Algo bellman_ford step 7137 current loss 0.030932, current_train_items 228416.
I0304 19:31:23.351212 22502662377600 run.py:483] Algo bellman_ford step 7138 current loss 0.062718, current_train_items 228448.
I0304 19:31:23.387336 22502662377600 run.py:483] Algo bellman_ford step 7139 current loss 0.173652, current_train_items 228480.
I0304 19:31:23.407213 22502662377600 run.py:483] Algo bellman_ford step 7140 current loss 0.006287, current_train_items 228512.
I0304 19:31:23.423140 22502662377600 run.py:483] Algo bellman_ford step 7141 current loss 0.050151, current_train_items 228544.
I0304 19:31:23.447676 22502662377600 run.py:483] Algo bellman_ford step 7142 current loss 0.051523, current_train_items 228576.
I0304 19:31:23.477286 22502662377600 run.py:483] Algo bellman_ford step 7143 current loss 0.064327, current_train_items 228608.
I0304 19:31:23.510130 22502662377600 run.py:483] Algo bellman_ford step 7144 current loss 0.095897, current_train_items 228640.
I0304 19:31:23.529713 22502662377600 run.py:483] Algo bellman_ford step 7145 current loss 0.017483, current_train_items 228672.
I0304 19:31:23.546152 22502662377600 run.py:483] Algo bellman_ford step 7146 current loss 0.031597, current_train_items 228704.
I0304 19:31:23.570458 22502662377600 run.py:483] Algo bellman_ford step 7147 current loss 0.067254, current_train_items 228736.
I0304 19:31:23.600356 22502662377600 run.py:483] Algo bellman_ford step 7148 current loss 0.054238, current_train_items 228768.
I0304 19:31:23.631701 22502662377600 run.py:483] Algo bellman_ford step 7149 current loss 0.058581, current_train_items 228800.
I0304 19:31:23.651065 22502662377600 run.py:483] Algo bellman_ford step 7150 current loss 0.009402, current_train_items 228832.
I0304 19:31:23.659404 22502662377600 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0304 19:31:23.659517 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:31:23.676568 22502662377600 run.py:483] Algo bellman_ford step 7151 current loss 0.036685, current_train_items 228864.
I0304 19:31:23.701718 22502662377600 run.py:483] Algo bellman_ford step 7152 current loss 0.046379, current_train_items 228896.
I0304 19:31:23.733535 22502662377600 run.py:483] Algo bellman_ford step 7153 current loss 0.086431, current_train_items 228928.
I0304 19:31:23.766514 22502662377600 run.py:483] Algo bellman_ford step 7154 current loss 0.066715, current_train_items 228960.
I0304 19:31:23.786443 22502662377600 run.py:483] Algo bellman_ford step 7155 current loss 0.022091, current_train_items 228992.
I0304 19:31:23.802833 22502662377600 run.py:483] Algo bellman_ford step 7156 current loss 0.026727, current_train_items 229024.
I0304 19:31:23.826485 22502662377600 run.py:483] Algo bellman_ford step 7157 current loss 0.083312, current_train_items 229056.
I0304 19:31:23.857105 22502662377600 run.py:483] Algo bellman_ford step 7158 current loss 0.036668, current_train_items 229088.
I0304 19:31:23.890952 22502662377600 run.py:483] Algo bellman_ford step 7159 current loss 0.091666, current_train_items 229120.
I0304 19:31:23.910790 22502662377600 run.py:483] Algo bellman_ford step 7160 current loss 0.003262, current_train_items 229152.
I0304 19:31:23.927309 22502662377600 run.py:483] Algo bellman_ford step 7161 current loss 0.044282, current_train_items 229184.
W0304 19:31:23.942196 22502662377600 samplers.py:155] Increasing hint lengh from 10 to 11
I0304 19:31:30.934680 22502662377600 run.py:483] Algo bellman_ford step 7162 current loss 0.054691, current_train_items 229216.
I0304 19:31:30.967215 22502662377600 run.py:483] Algo bellman_ford step 7163 current loss 0.054341, current_train_items 229248.
I0304 19:31:31.002340 22502662377600 run.py:483] Algo bellman_ford step 7164 current loss 0.105214, current_train_items 229280.
I0304 19:31:31.022236 22502662377600 run.py:483] Algo bellman_ford step 7165 current loss 0.002784, current_train_items 229312.
I0304 19:31:31.038662 22502662377600 run.py:483] Algo bellman_ford step 7166 current loss 0.029945, current_train_items 229344.
I0304 19:31:31.062881 22502662377600 run.py:483] Algo bellman_ford step 7167 current loss 0.092015, current_train_items 229376.
I0304 19:31:31.095221 22502662377600 run.py:483] Algo bellman_ford step 7168 current loss 0.063483, current_train_items 229408.
I0304 19:31:31.127046 22502662377600 run.py:483] Algo bellman_ford step 7169 current loss 0.093167, current_train_items 229440.
I0304 19:31:31.147459 22502662377600 run.py:483] Algo bellman_ford step 7170 current loss 0.002782, current_train_items 229472.
I0304 19:31:31.163995 22502662377600 run.py:483] Algo bellman_ford step 7171 current loss 0.018984, current_train_items 229504.
I0304 19:31:31.187552 22502662377600 run.py:483] Algo bellman_ford step 7172 current loss 0.103927, current_train_items 229536.
I0304 19:31:31.219332 22502662377600 run.py:483] Algo bellman_ford step 7173 current loss 0.050737, current_train_items 229568.
I0304 19:31:31.254291 22502662377600 run.py:483] Algo bellman_ford step 7174 current loss 0.124810, current_train_items 229600.
I0304 19:31:31.274757 22502662377600 run.py:483] Algo bellman_ford step 7175 current loss 0.003712, current_train_items 229632.
I0304 19:31:31.290587 22502662377600 run.py:483] Algo bellman_ford step 7176 current loss 0.032036, current_train_items 229664.
I0304 19:31:31.314617 22502662377600 run.py:483] Algo bellman_ford step 7177 current loss 0.030420, current_train_items 229696.
I0304 19:31:31.348476 22502662377600 run.py:483] Algo bellman_ford step 7178 current loss 0.079869, current_train_items 229728.
I0304 19:31:31.383980 22502662377600 run.py:483] Algo bellman_ford step 7179 current loss 0.074505, current_train_items 229760.
I0304 19:31:31.403857 22502662377600 run.py:483] Algo bellman_ford step 7180 current loss 0.003878, current_train_items 229792.
I0304 19:31:31.419991 22502662377600 run.py:483] Algo bellman_ford step 7181 current loss 0.023100, current_train_items 229824.
I0304 19:31:31.444598 22502662377600 run.py:483] Algo bellman_ford step 7182 current loss 0.050373, current_train_items 229856.
I0304 19:31:31.476092 22502662377600 run.py:483] Algo bellman_ford step 7183 current loss 0.065162, current_train_items 229888.
I0304 19:31:31.513105 22502662377600 run.py:483] Algo bellman_ford step 7184 current loss 0.200715, current_train_items 229920.
I0304 19:31:31.533105 22502662377600 run.py:483] Algo bellman_ford step 7185 current loss 0.007314, current_train_items 229952.
I0304 19:31:31.549270 22502662377600 run.py:483] Algo bellman_ford step 7186 current loss 0.033966, current_train_items 229984.
I0304 19:31:31.573985 22502662377600 run.py:483] Algo bellman_ford step 7187 current loss 0.071566, current_train_items 230016.
I0304 19:31:31.606154 22502662377600 run.py:483] Algo bellman_ford step 7188 current loss 0.078394, current_train_items 230048.
I0304 19:31:31.641854 22502662377600 run.py:483] Algo bellman_ford step 7189 current loss 0.074526, current_train_items 230080.
I0304 19:31:31.661699 22502662377600 run.py:483] Algo bellman_ford step 7190 current loss 0.005020, current_train_items 230112.
I0304 19:31:31.678169 22502662377600 run.py:483] Algo bellman_ford step 7191 current loss 0.049410, current_train_items 230144.
I0304 19:31:31.702092 22502662377600 run.py:483] Algo bellman_ford step 7192 current loss 0.056428, current_train_items 230176.
I0304 19:31:31.734498 22502662377600 run.py:483] Algo bellman_ford step 7193 current loss 0.055333, current_train_items 230208.
I0304 19:31:31.768190 22502662377600 run.py:483] Algo bellman_ford step 7194 current loss 0.111317, current_train_items 230240.
I0304 19:31:31.788272 22502662377600 run.py:483] Algo bellman_ford step 7195 current loss 0.004106, current_train_items 230272.
I0304 19:31:31.804235 22502662377600 run.py:483] Algo bellman_ford step 7196 current loss 0.015438, current_train_items 230304.
I0304 19:31:31.828279 22502662377600 run.py:483] Algo bellman_ford step 7197 current loss 0.034610, current_train_items 230336.
I0304 19:31:31.860738 22502662377600 run.py:483] Algo bellman_ford step 7198 current loss 0.065676, current_train_items 230368.
I0304 19:31:31.895130 22502662377600 run.py:483] Algo bellman_ford step 7199 current loss 0.079772, current_train_items 230400.
I0304 19:31:31.915186 22502662377600 run.py:483] Algo bellman_ford step 7200 current loss 0.005454, current_train_items 230432.
I0304 19:31:31.924977 22502662377600 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0304 19:31:31.925083 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:31.942016 22502662377600 run.py:483] Algo bellman_ford step 7201 current loss 0.053501, current_train_items 230464.
I0304 19:31:31.966269 22502662377600 run.py:483] Algo bellman_ford step 7202 current loss 0.024525, current_train_items 230496.
I0304 19:31:31.998960 22502662377600 run.py:483] Algo bellman_ford step 7203 current loss 0.078868, current_train_items 230528.
I0304 19:31:32.032863 22502662377600 run.py:483] Algo bellman_ford step 7204 current loss 0.077300, current_train_items 230560.
I0304 19:31:32.052997 22502662377600 run.py:483] Algo bellman_ford step 7205 current loss 0.022310, current_train_items 230592.
I0304 19:31:32.068643 22502662377600 run.py:483] Algo bellman_ford step 7206 current loss 0.030626, current_train_items 230624.
I0304 19:31:32.093295 22502662377600 run.py:483] Algo bellman_ford step 7207 current loss 0.115307, current_train_items 230656.
I0304 19:31:32.124730 22502662377600 run.py:483] Algo bellman_ford step 7208 current loss 0.080979, current_train_items 230688.
I0304 19:31:32.159718 22502662377600 run.py:483] Algo bellman_ford step 7209 current loss 0.091459, current_train_items 230720.
I0304 19:31:32.179064 22502662377600 run.py:483] Algo bellman_ford step 7210 current loss 0.004358, current_train_items 230752.
I0304 19:31:32.195022 22502662377600 run.py:483] Algo bellman_ford step 7211 current loss 0.010108, current_train_items 230784.
I0304 19:31:32.218729 22502662377600 run.py:483] Algo bellman_ford step 7212 current loss 0.144000, current_train_items 230816.
I0304 19:31:32.250181 22502662377600 run.py:483] Algo bellman_ford step 7213 current loss 0.170798, current_train_items 230848.
I0304 19:31:32.282579 22502662377600 run.py:483] Algo bellman_ford step 7214 current loss 0.168429, current_train_items 230880.
I0304 19:31:32.302248 22502662377600 run.py:483] Algo bellman_ford step 7215 current loss 0.038150, current_train_items 230912.
I0304 19:31:32.318366 22502662377600 run.py:483] Algo bellman_ford step 7216 current loss 0.012552, current_train_items 230944.
I0304 19:31:32.342703 22502662377600 run.py:483] Algo bellman_ford step 7217 current loss 0.069311, current_train_items 230976.
I0304 19:31:32.374862 22502662377600 run.py:483] Algo bellman_ford step 7218 current loss 0.084828, current_train_items 231008.
I0304 19:31:32.408169 22502662377600 run.py:483] Algo bellman_ford step 7219 current loss 0.073374, current_train_items 231040.
I0304 19:31:32.427713 22502662377600 run.py:483] Algo bellman_ford step 7220 current loss 0.031575, current_train_items 231072.
I0304 19:31:32.443848 22502662377600 run.py:483] Algo bellman_ford step 7221 current loss 0.019373, current_train_items 231104.
I0304 19:31:32.468797 22502662377600 run.py:483] Algo bellman_ford step 7222 current loss 0.038366, current_train_items 231136.
I0304 19:31:32.500578 22502662377600 run.py:483] Algo bellman_ford step 7223 current loss 0.101897, current_train_items 231168.
I0304 19:31:32.535000 22502662377600 run.py:483] Algo bellman_ford step 7224 current loss 0.153349, current_train_items 231200.
I0304 19:31:32.555074 22502662377600 run.py:483] Algo bellman_ford step 7225 current loss 0.005042, current_train_items 231232.
I0304 19:31:32.571303 22502662377600 run.py:483] Algo bellman_ford step 7226 current loss 0.044960, current_train_items 231264.
I0304 19:31:32.596877 22502662377600 run.py:483] Algo bellman_ford step 7227 current loss 0.057400, current_train_items 231296.
I0304 19:31:32.628851 22502662377600 run.py:483] Algo bellman_ford step 7228 current loss 0.033485, current_train_items 231328.
I0304 19:31:32.660653 22502662377600 run.py:483] Algo bellman_ford step 7229 current loss 0.081173, current_train_items 231360.
I0304 19:31:32.680246 22502662377600 run.py:483] Algo bellman_ford step 7230 current loss 0.004206, current_train_items 231392.
I0304 19:31:32.696525 22502662377600 run.py:483] Algo bellman_ford step 7231 current loss 0.030330, current_train_items 231424.
I0304 19:31:32.718883 22502662377600 run.py:483] Algo bellman_ford step 7232 current loss 0.035507, current_train_items 231456.
I0304 19:31:32.751556 22502662377600 run.py:483] Algo bellman_ford step 7233 current loss 0.058525, current_train_items 231488.
I0304 19:31:32.784323 22502662377600 run.py:483] Algo bellman_ford step 7234 current loss 0.062146, current_train_items 231520.
I0304 19:31:32.804402 22502662377600 run.py:483] Algo bellman_ford step 7235 current loss 0.005248, current_train_items 231552.
I0304 19:31:32.820863 22502662377600 run.py:483] Algo bellman_ford step 7236 current loss 0.025827, current_train_items 231584.
I0304 19:31:32.845952 22502662377600 run.py:483] Algo bellman_ford step 7237 current loss 0.133265, current_train_items 231616.
I0304 19:31:32.879362 22502662377600 run.py:483] Algo bellman_ford step 7238 current loss 0.255207, current_train_items 231648.
I0304 19:31:32.913608 22502662377600 run.py:483] Algo bellman_ford step 7239 current loss 0.112957, current_train_items 231680.
I0304 19:31:32.933498 22502662377600 run.py:483] Algo bellman_ford step 7240 current loss 0.012990, current_train_items 231712.
I0304 19:31:32.949517 22502662377600 run.py:483] Algo bellman_ford step 7241 current loss 0.018425, current_train_items 231744.
I0304 19:31:32.974883 22502662377600 run.py:483] Algo bellman_ford step 7242 current loss 0.105262, current_train_items 231776.
I0304 19:31:33.006984 22502662377600 run.py:483] Algo bellman_ford step 7243 current loss 0.117424, current_train_items 231808.
I0304 19:31:33.042638 22502662377600 run.py:483] Algo bellman_ford step 7244 current loss 0.107673, current_train_items 231840.
I0304 19:31:33.062484 22502662377600 run.py:483] Algo bellman_ford step 7245 current loss 0.006926, current_train_items 231872.
I0304 19:31:33.078656 22502662377600 run.py:483] Algo bellman_ford step 7246 current loss 0.018886, current_train_items 231904.
I0304 19:31:33.103374 22502662377600 run.py:483] Algo bellman_ford step 7247 current loss 0.044577, current_train_items 231936.
I0304 19:31:33.136525 22502662377600 run.py:483] Algo bellman_ford step 7248 current loss 0.108526, current_train_items 231968.
I0304 19:31:33.171276 22502662377600 run.py:483] Algo bellman_ford step 7249 current loss 0.130708, current_train_items 232000.
I0304 19:31:33.191245 22502662377600 run.py:483] Algo bellman_ford step 7250 current loss 0.031895, current_train_items 232032.
I0304 19:31:33.199547 22502662377600 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0304 19:31:33.199654 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:31:33.216539 22502662377600 run.py:483] Algo bellman_ford step 7251 current loss 0.046016, current_train_items 232064.
I0304 19:31:33.240799 22502662377600 run.py:483] Algo bellman_ford step 7252 current loss 0.052586, current_train_items 232096.
I0304 19:31:33.271812 22502662377600 run.py:483] Algo bellman_ford step 7253 current loss 0.079517, current_train_items 232128.
I0304 19:31:33.305933 22502662377600 run.py:483] Algo bellman_ford step 7254 current loss 0.092484, current_train_items 232160.
I0304 19:31:33.325639 22502662377600 run.py:483] Algo bellman_ford step 7255 current loss 0.005840, current_train_items 232192.
I0304 19:31:33.341113 22502662377600 run.py:483] Algo bellman_ford step 7256 current loss 0.032572, current_train_items 232224.
I0304 19:31:33.364816 22502662377600 run.py:483] Algo bellman_ford step 7257 current loss 0.051662, current_train_items 232256.
I0304 19:31:33.398793 22502662377600 run.py:483] Algo bellman_ford step 7258 current loss 0.081740, current_train_items 232288.
I0304 19:31:33.432967 22502662377600 run.py:483] Algo bellman_ford step 7259 current loss 0.089512, current_train_items 232320.
I0304 19:31:33.452797 22502662377600 run.py:483] Algo bellman_ford step 7260 current loss 0.007969, current_train_items 232352.
I0304 19:31:33.469004 22502662377600 run.py:483] Algo bellman_ford step 7261 current loss 0.020382, current_train_items 232384.
I0304 19:31:33.492820 22502662377600 run.py:483] Algo bellman_ford step 7262 current loss 0.033473, current_train_items 232416.
I0304 19:31:33.524839 22502662377600 run.py:483] Algo bellman_ford step 7263 current loss 0.056534, current_train_items 232448.
I0304 19:31:33.558473 22502662377600 run.py:483] Algo bellman_ford step 7264 current loss 0.122634, current_train_items 232480.
I0304 19:31:33.577767 22502662377600 run.py:483] Algo bellman_ford step 7265 current loss 0.006733, current_train_items 232512.
I0304 19:31:33.593877 22502662377600 run.py:483] Algo bellman_ford step 7266 current loss 0.035592, current_train_items 232544.
I0304 19:31:33.617487 22502662377600 run.py:483] Algo bellman_ford step 7267 current loss 0.053038, current_train_items 232576.
I0304 19:31:33.649147 22502662377600 run.py:483] Algo bellman_ford step 7268 current loss 0.059966, current_train_items 232608.
I0304 19:31:33.681795 22502662377600 run.py:483] Algo bellman_ford step 7269 current loss 0.077568, current_train_items 232640.
I0304 19:31:33.701636 22502662377600 run.py:483] Algo bellman_ford step 7270 current loss 0.016924, current_train_items 232672.
I0304 19:31:33.718043 22502662377600 run.py:483] Algo bellman_ford step 7271 current loss 0.031070, current_train_items 232704.
I0304 19:31:33.741833 22502662377600 run.py:483] Algo bellman_ford step 7272 current loss 0.021752, current_train_items 232736.
I0304 19:31:33.773344 22502662377600 run.py:483] Algo bellman_ford step 7273 current loss 0.057047, current_train_items 232768.
I0304 19:31:33.806712 22502662377600 run.py:483] Algo bellman_ford step 7274 current loss 0.122626, current_train_items 232800.
I0304 19:31:33.826379 22502662377600 run.py:483] Algo bellman_ford step 7275 current loss 0.005149, current_train_items 232832.
I0304 19:31:33.842519 22502662377600 run.py:483] Algo bellman_ford step 7276 current loss 0.020809, current_train_items 232864.
I0304 19:31:33.866244 22502662377600 run.py:483] Algo bellman_ford step 7277 current loss 0.056894, current_train_items 232896.
I0304 19:31:33.898651 22502662377600 run.py:483] Algo bellman_ford step 7278 current loss 0.082264, current_train_items 232928.
I0304 19:31:33.932408 22502662377600 run.py:483] Algo bellman_ford step 7279 current loss 0.096175, current_train_items 232960.
I0304 19:31:33.951911 22502662377600 run.py:483] Algo bellman_ford step 7280 current loss 0.022709, current_train_items 232992.
I0304 19:31:33.968318 22502662377600 run.py:483] Algo bellman_ford step 7281 current loss 0.034124, current_train_items 233024.
I0304 19:31:33.992033 22502662377600 run.py:483] Algo bellman_ford step 7282 current loss 0.083709, current_train_items 233056.
I0304 19:31:34.023707 22502662377600 run.py:483] Algo bellman_ford step 7283 current loss 0.153395, current_train_items 233088.
I0304 19:31:34.056688 22502662377600 run.py:483] Algo bellman_ford step 7284 current loss 0.119255, current_train_items 233120.
I0304 19:31:34.076451 22502662377600 run.py:483] Algo bellman_ford step 7285 current loss 0.003821, current_train_items 233152.
I0304 19:31:34.093535 22502662377600 run.py:483] Algo bellman_ford step 7286 current loss 0.056405, current_train_items 233184.
I0304 19:31:34.117174 22502662377600 run.py:483] Algo bellman_ford step 7287 current loss 0.041054, current_train_items 233216.
I0304 19:31:34.148619 22502662377600 run.py:483] Algo bellman_ford step 7288 current loss 0.077279, current_train_items 233248.
I0304 19:31:34.181351 22502662377600 run.py:483] Algo bellman_ford step 7289 current loss 0.072235, current_train_items 233280.
I0304 19:31:34.201212 22502662377600 run.py:483] Algo bellman_ford step 7290 current loss 0.015878, current_train_items 233312.
I0304 19:31:34.217588 22502662377600 run.py:483] Algo bellman_ford step 7291 current loss 0.033375, current_train_items 233344.
I0304 19:31:34.240315 22502662377600 run.py:483] Algo bellman_ford step 7292 current loss 0.014421, current_train_items 233376.
I0304 19:31:34.272471 22502662377600 run.py:483] Algo bellman_ford step 7293 current loss 0.048685, current_train_items 233408.
I0304 19:31:34.304554 22502662377600 run.py:483] Algo bellman_ford step 7294 current loss 0.055612, current_train_items 233440.
I0304 19:31:34.323858 22502662377600 run.py:483] Algo bellman_ford step 7295 current loss 0.004197, current_train_items 233472.
I0304 19:31:34.340112 22502662377600 run.py:483] Algo bellman_ford step 7296 current loss 0.010206, current_train_items 233504.
I0304 19:31:34.363989 22502662377600 run.py:483] Algo bellman_ford step 7297 current loss 0.099529, current_train_items 233536.
I0304 19:31:34.396007 22502662377600 run.py:483] Algo bellman_ford step 7298 current loss 0.068815, current_train_items 233568.
I0304 19:31:34.429672 22502662377600 run.py:483] Algo bellman_ford step 7299 current loss 0.146207, current_train_items 233600.
I0304 19:31:34.449536 22502662377600 run.py:483] Algo bellman_ford step 7300 current loss 0.009722, current_train_items 233632.
I0304 19:31:34.457288 22502662377600 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0304 19:31:34.457394 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0304 19:31:34.474887 22502662377600 run.py:483] Algo bellman_ford step 7301 current loss 0.018817, current_train_items 233664.
I0304 19:31:34.499430 22502662377600 run.py:483] Algo bellman_ford step 7302 current loss 0.074421, current_train_items 233696.
I0304 19:31:34.531366 22502662377600 run.py:483] Algo bellman_ford step 7303 current loss 0.071625, current_train_items 233728.
I0304 19:31:34.566166 22502662377600 run.py:483] Algo bellman_ford step 7304 current loss 0.082288, current_train_items 233760.
I0304 19:31:34.585975 22502662377600 run.py:483] Algo bellman_ford step 7305 current loss 0.005100, current_train_items 233792.
I0304 19:31:34.601649 22502662377600 run.py:483] Algo bellman_ford step 7306 current loss 0.015573, current_train_items 233824.
I0304 19:31:34.625444 22502662377600 run.py:483] Algo bellman_ford step 7307 current loss 0.102024, current_train_items 233856.
I0304 19:31:34.656038 22502662377600 run.py:483] Algo bellman_ford step 7308 current loss 0.059508, current_train_items 233888.
I0304 19:31:34.691100 22502662377600 run.py:483] Algo bellman_ford step 7309 current loss 0.100641, current_train_items 233920.
I0304 19:31:34.710990 22502662377600 run.py:483] Algo bellman_ford step 7310 current loss 0.003521, current_train_items 233952.
I0304 19:31:34.726926 22502662377600 run.py:483] Algo bellman_ford step 7311 current loss 0.011974, current_train_items 233984.
I0304 19:31:34.752685 22502662377600 run.py:483] Algo bellman_ford step 7312 current loss 0.096623, current_train_items 234016.
I0304 19:31:34.784619 22502662377600 run.py:483] Algo bellman_ford step 7313 current loss 0.153246, current_train_items 234048.
I0304 19:31:34.819405 22502662377600 run.py:483] Algo bellman_ford step 7314 current loss 0.093077, current_train_items 234080.
I0304 19:31:34.839446 22502662377600 run.py:483] Algo bellman_ford step 7315 current loss 0.010987, current_train_items 234112.
I0304 19:31:34.855398 22502662377600 run.py:483] Algo bellman_ford step 7316 current loss 0.023522, current_train_items 234144.
I0304 19:31:34.880354 22502662377600 run.py:483] Algo bellman_ford step 7317 current loss 0.109612, current_train_items 234176.
I0304 19:31:34.912317 22502662377600 run.py:483] Algo bellman_ford step 7318 current loss 0.117685, current_train_items 234208.
I0304 19:31:34.947162 22502662377600 run.py:483] Algo bellman_ford step 7319 current loss 0.105101, current_train_items 234240.
I0304 19:31:34.967204 22502662377600 run.py:483] Algo bellman_ford step 7320 current loss 0.003874, current_train_items 234272.
I0304 19:31:34.983842 22502662377600 run.py:483] Algo bellman_ford step 7321 current loss 0.023258, current_train_items 234304.
I0304 19:31:35.009613 22502662377600 run.py:483] Algo bellman_ford step 7322 current loss 0.098483, current_train_items 234336.
I0304 19:31:35.041194 22502662377600 run.py:483] Algo bellman_ford step 7323 current loss 0.067305, current_train_items 234368.
I0304 19:31:35.076251 22502662377600 run.py:483] Algo bellman_ford step 7324 current loss 0.164618, current_train_items 234400.
I0304 19:31:35.095693 22502662377600 run.py:483] Algo bellman_ford step 7325 current loss 0.002727, current_train_items 234432.
I0304 19:31:35.112197 22502662377600 run.py:483] Algo bellman_ford step 7326 current loss 0.020140, current_train_items 234464.
I0304 19:31:35.136811 22502662377600 run.py:483] Algo bellman_ford step 7327 current loss 0.038061, current_train_items 234496.
I0304 19:31:35.168334 22502662377600 run.py:483] Algo bellman_ford step 7328 current loss 0.050871, current_train_items 234528.
I0304 19:31:35.202340 22502662377600 run.py:483] Algo bellman_ford step 7329 current loss 0.074327, current_train_items 234560.
I0304 19:31:35.221778 22502662377600 run.py:483] Algo bellman_ford step 7330 current loss 0.004557, current_train_items 234592.
I0304 19:31:35.238190 22502662377600 run.py:483] Algo bellman_ford step 7331 current loss 0.038773, current_train_items 234624.
I0304 19:31:35.263345 22502662377600 run.py:483] Algo bellman_ford step 7332 current loss 0.058118, current_train_items 234656.
I0304 19:31:35.295924 22502662377600 run.py:483] Algo bellman_ford step 7333 current loss 0.063070, current_train_items 234688.
I0304 19:31:35.328501 22502662377600 run.py:483] Algo bellman_ford step 7334 current loss 0.053348, current_train_items 234720.
I0304 19:31:35.348127 22502662377600 run.py:483] Algo bellman_ford step 7335 current loss 0.001971, current_train_items 234752.
I0304 19:31:35.364005 22502662377600 run.py:483] Algo bellman_ford step 7336 current loss 0.020025, current_train_items 234784.
I0304 19:31:35.388167 22502662377600 run.py:483] Algo bellman_ford step 7337 current loss 0.047003, current_train_items 234816.
I0304 19:31:35.420371 22502662377600 run.py:483] Algo bellman_ford step 7338 current loss 0.057415, current_train_items 234848.
I0304 19:31:35.456114 22502662377600 run.py:483] Algo bellman_ford step 7339 current loss 0.064151, current_train_items 234880.
I0304 19:31:35.476116 22502662377600 run.py:483] Algo bellman_ford step 7340 current loss 0.005865, current_train_items 234912.
I0304 19:31:35.492109 22502662377600 run.py:483] Algo bellman_ford step 7341 current loss 0.020017, current_train_items 234944.
I0304 19:31:35.517271 22502662377600 run.py:483] Algo bellman_ford step 7342 current loss 0.063491, current_train_items 234976.
I0304 19:31:35.550243 22502662377600 run.py:483] Algo bellman_ford step 7343 current loss 0.050480, current_train_items 235008.
I0304 19:31:35.583476 22502662377600 run.py:483] Algo bellman_ford step 7344 current loss 0.146400, current_train_items 235040.
I0304 19:31:35.603446 22502662377600 run.py:483] Algo bellman_ford step 7345 current loss 0.023729, current_train_items 235072.
I0304 19:31:35.619271 22502662377600 run.py:483] Algo bellman_ford step 7346 current loss 0.019314, current_train_items 235104.
I0304 19:31:35.644255 22502662377600 run.py:483] Algo bellman_ford step 7347 current loss 0.056517, current_train_items 235136.
I0304 19:31:35.677070 22502662377600 run.py:483] Algo bellman_ford step 7348 current loss 0.071340, current_train_items 235168.
I0304 19:31:35.707844 22502662377600 run.py:483] Algo bellman_ford step 7349 current loss 0.059662, current_train_items 235200.
I0304 19:31:35.727663 22502662377600 run.py:483] Algo bellman_ford step 7350 current loss 0.005979, current_train_items 235232.
I0304 19:31:35.735793 22502662377600 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0304 19:31:35.735930 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:31:35.753418 22502662377600 run.py:483] Algo bellman_ford step 7351 current loss 0.035653, current_train_items 235264.
I0304 19:31:35.779044 22502662377600 run.py:483] Algo bellman_ford step 7352 current loss 0.059891, current_train_items 235296.
I0304 19:31:35.812306 22502662377600 run.py:483] Algo bellman_ford step 7353 current loss 0.067943, current_train_items 235328.
I0304 19:31:35.845877 22502662377600 run.py:483] Algo bellman_ford step 7354 current loss 0.069176, current_train_items 235360.
I0304 19:31:35.866049 22502662377600 run.py:483] Algo bellman_ford step 7355 current loss 0.013612, current_train_items 235392.
I0304 19:31:35.881572 22502662377600 run.py:483] Algo bellman_ford step 7356 current loss 0.013560, current_train_items 235424.
I0304 19:31:35.907274 22502662377600 run.py:483] Algo bellman_ford step 7357 current loss 0.042234, current_train_items 235456.
I0304 19:31:35.939094 22502662377600 run.py:483] Algo bellman_ford step 7358 current loss 0.054578, current_train_items 235488.
I0304 19:31:35.972735 22502662377600 run.py:483] Algo bellman_ford step 7359 current loss 0.059636, current_train_items 235520.
I0304 19:31:35.992787 22502662377600 run.py:483] Algo bellman_ford step 7360 current loss 0.005560, current_train_items 235552.
I0304 19:31:36.009477 22502662377600 run.py:483] Algo bellman_ford step 7361 current loss 0.061731, current_train_items 235584.
I0304 19:31:36.032523 22502662377600 run.py:483] Algo bellman_ford step 7362 current loss 0.086466, current_train_items 235616.
I0304 19:31:36.066736 22502662377600 run.py:483] Algo bellman_ford step 7363 current loss 0.149920, current_train_items 235648.
I0304 19:31:36.098867 22502662377600 run.py:483] Algo bellman_ford step 7364 current loss 0.082652, current_train_items 235680.
I0304 19:31:36.118740 22502662377600 run.py:483] Algo bellman_ford step 7365 current loss 0.027818, current_train_items 235712.
I0304 19:31:36.134898 22502662377600 run.py:483] Algo bellman_ford step 7366 current loss 0.033870, current_train_items 235744.
I0304 19:31:36.159648 22502662377600 run.py:483] Algo bellman_ford step 7367 current loss 0.045866, current_train_items 235776.
I0304 19:31:36.193694 22502662377600 run.py:483] Algo bellman_ford step 7368 current loss 0.053436, current_train_items 235808.
I0304 19:31:36.226543 22502662377600 run.py:483] Algo bellman_ford step 7369 current loss 0.039441, current_train_items 235840.
I0304 19:31:36.246178 22502662377600 run.py:483] Algo bellman_ford step 7370 current loss 0.005806, current_train_items 235872.
I0304 19:31:36.262441 22502662377600 run.py:483] Algo bellman_ford step 7371 current loss 0.044831, current_train_items 235904.
I0304 19:31:36.285527 22502662377600 run.py:483] Algo bellman_ford step 7372 current loss 0.044806, current_train_items 235936.
I0304 19:31:36.317178 22502662377600 run.py:483] Algo bellman_ford step 7373 current loss 0.054399, current_train_items 235968.
I0304 19:31:36.350486 22502662377600 run.py:483] Algo bellman_ford step 7374 current loss 0.080428, current_train_items 236000.
I0304 19:31:36.370593 22502662377600 run.py:483] Algo bellman_ford step 7375 current loss 0.004668, current_train_items 236032.
I0304 19:31:36.387678 22502662377600 run.py:483] Algo bellman_ford step 7376 current loss 0.049331, current_train_items 236064.
I0304 19:31:36.411944 22502662377600 run.py:483] Algo bellman_ford step 7377 current loss 0.058375, current_train_items 236096.
I0304 19:31:36.444140 22502662377600 run.py:483] Algo bellman_ford step 7378 current loss 0.095271, current_train_items 236128.
I0304 19:31:36.476629 22502662377600 run.py:483] Algo bellman_ford step 7379 current loss 0.129321, current_train_items 236160.
I0304 19:31:36.496197 22502662377600 run.py:483] Algo bellman_ford step 7380 current loss 0.006317, current_train_items 236192.
I0304 19:31:36.511967 22502662377600 run.py:483] Algo bellman_ford step 7381 current loss 0.047495, current_train_items 236224.
I0304 19:31:36.537368 22502662377600 run.py:483] Algo bellman_ford step 7382 current loss 0.108507, current_train_items 236256.
I0304 19:31:36.568778 22502662377600 run.py:483] Algo bellman_ford step 7383 current loss 0.085591, current_train_items 236288.
I0304 19:31:36.601399 22502662377600 run.py:483] Algo bellman_ford step 7384 current loss 0.102545, current_train_items 236320.
I0304 19:31:36.621496 22502662377600 run.py:483] Algo bellman_ford step 7385 current loss 0.005515, current_train_items 236352.
I0304 19:31:36.637790 22502662377600 run.py:483] Algo bellman_ford step 7386 current loss 0.029196, current_train_items 236384.
I0304 19:31:36.662251 22502662377600 run.py:483] Algo bellman_ford step 7387 current loss 0.070563, current_train_items 236416.
I0304 19:31:36.694785 22502662377600 run.py:483] Algo bellman_ford step 7388 current loss 0.096018, current_train_items 236448.
I0304 19:31:36.727939 22502662377600 run.py:483] Algo bellman_ford step 7389 current loss 0.152677, current_train_items 236480.
I0304 19:31:36.748094 22502662377600 run.py:483] Algo bellman_ford step 7390 current loss 0.006749, current_train_items 236512.
I0304 19:31:36.764767 22502662377600 run.py:483] Algo bellman_ford step 7391 current loss 0.029339, current_train_items 236544.
I0304 19:31:36.788910 22502662377600 run.py:483] Algo bellman_ford step 7392 current loss 0.078875, current_train_items 236576.
I0304 19:31:36.820198 22502662377600 run.py:483] Algo bellman_ford step 7393 current loss 0.117636, current_train_items 236608.
I0304 19:31:36.854737 22502662377600 run.py:483] Algo bellman_ford step 7394 current loss 0.216638, current_train_items 236640.
I0304 19:31:36.874042 22502662377600 run.py:483] Algo bellman_ford step 7395 current loss 0.005043, current_train_items 236672.
I0304 19:31:36.890482 22502662377600 run.py:483] Algo bellman_ford step 7396 current loss 0.025791, current_train_items 236704.
I0304 19:31:36.915656 22502662377600 run.py:483] Algo bellman_ford step 7397 current loss 0.062333, current_train_items 236736.
I0304 19:31:36.946948 22502662377600 run.py:483] Algo bellman_ford step 7398 current loss 0.111773, current_train_items 236768.
I0304 19:31:36.979719 22502662377600 run.py:483] Algo bellman_ford step 7399 current loss 0.094257, current_train_items 236800.
I0304 19:31:36.999435 22502662377600 run.py:483] Algo bellman_ford step 7400 current loss 0.004361, current_train_items 236832.
I0304 19:31:37.007434 22502662377600 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0304 19:31:37.007548 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:31:37.024708 22502662377600 run.py:483] Algo bellman_ford step 7401 current loss 0.020401, current_train_items 236864.
I0304 19:31:37.050329 22502662377600 run.py:483] Algo bellman_ford step 7402 current loss 0.068019, current_train_items 236896.
I0304 19:31:37.083200 22502662377600 run.py:483] Algo bellman_ford step 7403 current loss 0.075460, current_train_items 236928.
I0304 19:31:37.117994 22502662377600 run.py:483] Algo bellman_ford step 7404 current loss 0.077573, current_train_items 236960.
I0304 19:31:37.137845 22502662377600 run.py:483] Algo bellman_ford step 7405 current loss 0.009929, current_train_items 236992.
I0304 19:31:37.153308 22502662377600 run.py:483] Algo bellman_ford step 7406 current loss 0.018938, current_train_items 237024.
I0304 19:31:37.178108 22502662377600 run.py:483] Algo bellman_ford step 7407 current loss 0.093172, current_train_items 237056.
I0304 19:31:37.210684 22502662377600 run.py:483] Algo bellman_ford step 7408 current loss 0.165637, current_train_items 237088.
I0304 19:31:37.242993 22502662377600 run.py:483] Algo bellman_ford step 7409 current loss 0.131892, current_train_items 237120.
I0304 19:31:37.262804 22502662377600 run.py:483] Algo bellman_ford step 7410 current loss 0.007531, current_train_items 237152.
I0304 19:31:37.279435 22502662377600 run.py:483] Algo bellman_ford step 7411 current loss 0.016269, current_train_items 237184.
I0304 19:31:37.304960 22502662377600 run.py:483] Algo bellman_ford step 7412 current loss 0.058788, current_train_items 237216.
I0304 19:31:37.337748 22502662377600 run.py:483] Algo bellman_ford step 7413 current loss 0.147537, current_train_items 237248.
I0304 19:31:37.369933 22502662377600 run.py:483] Algo bellman_ford step 7414 current loss 0.072440, current_train_items 237280.
I0304 19:31:37.389936 22502662377600 run.py:483] Algo bellman_ford step 7415 current loss 0.007170, current_train_items 237312.
I0304 19:31:37.405814 22502662377600 run.py:483] Algo bellman_ford step 7416 current loss 0.014995, current_train_items 237344.
I0304 19:31:37.430296 22502662377600 run.py:483] Algo bellman_ford step 7417 current loss 0.078814, current_train_items 237376.
I0304 19:31:37.462182 22502662377600 run.py:483] Algo bellman_ford step 7418 current loss 0.090975, current_train_items 237408.
I0304 19:31:37.495327 22502662377600 run.py:483] Algo bellman_ford step 7419 current loss 0.083544, current_train_items 237440.
I0304 19:31:37.515149 22502662377600 run.py:483] Algo bellman_ford step 7420 current loss 0.008268, current_train_items 237472.
I0304 19:31:37.531138 22502662377600 run.py:483] Algo bellman_ford step 7421 current loss 0.019729, current_train_items 237504.
I0304 19:31:37.555426 22502662377600 run.py:483] Algo bellman_ford step 7422 current loss 0.115048, current_train_items 237536.
I0304 19:31:37.586788 22502662377600 run.py:483] Algo bellman_ford step 7423 current loss 0.098878, current_train_items 237568.
I0304 19:31:37.620299 22502662377600 run.py:483] Algo bellman_ford step 7424 current loss 0.066576, current_train_items 237600.
I0304 19:31:37.640351 22502662377600 run.py:483] Algo bellman_ford step 7425 current loss 0.006948, current_train_items 237632.
I0304 19:31:37.656363 22502662377600 run.py:483] Algo bellman_ford step 7426 current loss 0.014042, current_train_items 237664.
I0304 19:31:37.681408 22502662377600 run.py:483] Algo bellman_ford step 7427 current loss 0.089674, current_train_items 237696.
I0304 19:31:37.713290 22502662377600 run.py:483] Algo bellman_ford step 7428 current loss 0.088189, current_train_items 237728.
I0304 19:31:37.747506 22502662377600 run.py:483] Algo bellman_ford step 7429 current loss 0.147908, current_train_items 237760.
I0304 19:31:37.767551 22502662377600 run.py:483] Algo bellman_ford step 7430 current loss 0.005371, current_train_items 237792.
I0304 19:31:37.783751 22502662377600 run.py:483] Algo bellman_ford step 7431 current loss 0.011713, current_train_items 237824.
I0304 19:31:37.808619 22502662377600 run.py:483] Algo bellman_ford step 7432 current loss 0.074243, current_train_items 237856.
I0304 19:31:37.841138 22502662377600 run.py:483] Algo bellman_ford step 7433 current loss 0.097028, current_train_items 237888.
I0304 19:31:37.872632 22502662377600 run.py:483] Algo bellman_ford step 7434 current loss 0.082419, current_train_items 237920.
I0304 19:31:37.892799 22502662377600 run.py:483] Algo bellman_ford step 7435 current loss 0.008456, current_train_items 237952.
I0304 19:31:37.908898 22502662377600 run.py:483] Algo bellman_ford step 7436 current loss 0.014919, current_train_items 237984.
I0304 19:31:37.933978 22502662377600 run.py:483] Algo bellman_ford step 7437 current loss 0.105252, current_train_items 238016.
I0304 19:31:37.965610 22502662377600 run.py:483] Algo bellman_ford step 7438 current loss 0.070483, current_train_items 238048.
I0304 19:31:38.000284 22502662377600 run.py:483] Algo bellman_ford step 7439 current loss 0.095980, current_train_items 238080.
I0304 19:31:38.019965 22502662377600 run.py:483] Algo bellman_ford step 7440 current loss 0.007198, current_train_items 238112.
I0304 19:31:38.036537 22502662377600 run.py:483] Algo bellman_ford step 7441 current loss 0.028168, current_train_items 238144.
I0304 19:31:38.060268 22502662377600 run.py:483] Algo bellman_ford step 7442 current loss 0.035381, current_train_items 238176.
I0304 19:31:38.093232 22502662377600 run.py:483] Algo bellman_ford step 7443 current loss 0.083865, current_train_items 238208.
I0304 19:31:38.128157 22502662377600 run.py:483] Algo bellman_ford step 7444 current loss 0.085971, current_train_items 238240.
I0304 19:31:38.147733 22502662377600 run.py:483] Algo bellman_ford step 7445 current loss 0.005554, current_train_items 238272.
I0304 19:31:38.163952 22502662377600 run.py:483] Algo bellman_ford step 7446 current loss 0.053347, current_train_items 238304.
I0304 19:31:38.188249 22502662377600 run.py:483] Algo bellman_ford step 7447 current loss 0.047358, current_train_items 238336.
I0304 19:31:38.220273 22502662377600 run.py:483] Algo bellman_ford step 7448 current loss 0.107886, current_train_items 238368.
I0304 19:31:38.255424 22502662377600 run.py:483] Algo bellman_ford step 7449 current loss 0.195234, current_train_items 238400.
I0304 19:31:38.275485 22502662377600 run.py:483] Algo bellman_ford step 7450 current loss 0.002738, current_train_items 238432.
I0304 19:31:38.283905 22502662377600 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0304 19:31:38.284008 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:31:38.300786 22502662377600 run.py:483] Algo bellman_ford step 7451 current loss 0.018237, current_train_items 238464.
I0304 19:31:38.326545 22502662377600 run.py:483] Algo bellman_ford step 7452 current loss 0.062854, current_train_items 238496.
I0304 19:31:38.357945 22502662377600 run.py:483] Algo bellman_ford step 7453 current loss 0.053148, current_train_items 238528.
I0304 19:31:38.390827 22502662377600 run.py:483] Algo bellman_ford step 7454 current loss 0.083583, current_train_items 238560.
I0304 19:31:38.410564 22502662377600 run.py:483] Algo bellman_ford step 7455 current loss 0.006825, current_train_items 238592.
I0304 19:31:38.426868 22502662377600 run.py:483] Algo bellman_ford step 7456 current loss 0.025291, current_train_items 238624.
I0304 19:31:38.451018 22502662377600 run.py:483] Algo bellman_ford step 7457 current loss 0.041849, current_train_items 238656.
I0304 19:31:38.483421 22502662377600 run.py:483] Algo bellman_ford step 7458 current loss 0.106266, current_train_items 238688.
I0304 19:31:38.516530 22502662377600 run.py:483] Algo bellman_ford step 7459 current loss 0.103579, current_train_items 238720.
I0304 19:31:38.536694 22502662377600 run.py:483] Algo bellman_ford step 7460 current loss 0.004698, current_train_items 238752.
I0304 19:31:38.553681 22502662377600 run.py:483] Algo bellman_ford step 7461 current loss 0.016398, current_train_items 238784.
I0304 19:31:38.576761 22502662377600 run.py:483] Algo bellman_ford step 7462 current loss 0.045994, current_train_items 238816.
I0304 19:31:38.607926 22502662377600 run.py:483] Algo bellman_ford step 7463 current loss 0.033644, current_train_items 238848.
I0304 19:31:38.641516 22502662377600 run.py:483] Algo bellman_ford step 7464 current loss 0.072381, current_train_items 238880.
I0304 19:31:38.661057 22502662377600 run.py:483] Algo bellman_ford step 7465 current loss 0.002592, current_train_items 238912.
I0304 19:31:38.677156 22502662377600 run.py:483] Algo bellman_ford step 7466 current loss 0.009181, current_train_items 238944.
I0304 19:31:38.700917 22502662377600 run.py:483] Algo bellman_ford step 7467 current loss 0.061002, current_train_items 238976.
I0304 19:31:38.733688 22502662377600 run.py:483] Algo bellman_ford step 7468 current loss 0.094428, current_train_items 239008.
I0304 19:31:38.765840 22502662377600 run.py:483] Algo bellman_ford step 7469 current loss 0.065385, current_train_items 239040.
I0304 19:31:38.785963 22502662377600 run.py:483] Algo bellman_ford step 7470 current loss 0.009706, current_train_items 239072.
I0304 19:31:38.802300 22502662377600 run.py:483] Algo bellman_ford step 7471 current loss 0.031825, current_train_items 239104.
I0304 19:31:38.825446 22502662377600 run.py:483] Algo bellman_ford step 7472 current loss 0.082752, current_train_items 239136.
I0304 19:31:38.857721 22502662377600 run.py:483] Algo bellman_ford step 7473 current loss 0.112565, current_train_items 239168.
I0304 19:31:38.893094 22502662377600 run.py:483] Algo bellman_ford step 7474 current loss 0.043039, current_train_items 239200.
I0304 19:31:38.913109 22502662377600 run.py:483] Algo bellman_ford step 7475 current loss 0.003443, current_train_items 239232.
I0304 19:31:38.929610 22502662377600 run.py:483] Algo bellman_ford step 7476 current loss 0.011429, current_train_items 239264.
I0304 19:31:38.952582 22502662377600 run.py:483] Algo bellman_ford step 7477 current loss 0.067848, current_train_items 239296.
I0304 19:31:38.984918 22502662377600 run.py:483] Algo bellman_ford step 7478 current loss 0.061758, current_train_items 239328.
I0304 19:31:39.021501 22502662377600 run.py:483] Algo bellman_ford step 7479 current loss 0.109903, current_train_items 239360.
I0304 19:31:39.041038 22502662377600 run.py:483] Algo bellman_ford step 7480 current loss 0.005055, current_train_items 239392.
I0304 19:31:39.057503 22502662377600 run.py:483] Algo bellman_ford step 7481 current loss 0.031250, current_train_items 239424.
I0304 19:31:39.082738 22502662377600 run.py:483] Algo bellman_ford step 7482 current loss 0.102519, current_train_items 239456.
I0304 19:31:39.115593 22502662377600 run.py:483] Algo bellman_ford step 7483 current loss 0.089557, current_train_items 239488.
I0304 19:31:39.150104 22502662377600 run.py:483] Algo bellman_ford step 7484 current loss 0.110856, current_train_items 239520.
I0304 19:31:39.169794 22502662377600 run.py:483] Algo bellman_ford step 7485 current loss 0.002569, current_train_items 239552.
I0304 19:31:39.185983 22502662377600 run.py:483] Algo bellman_ford step 7486 current loss 0.018009, current_train_items 239584.
I0304 19:31:39.209083 22502662377600 run.py:483] Algo bellman_ford step 7487 current loss 0.062877, current_train_items 239616.
I0304 19:31:39.241429 22502662377600 run.py:483] Algo bellman_ford step 7488 current loss 0.070728, current_train_items 239648.
I0304 19:31:39.276904 22502662377600 run.py:483] Algo bellman_ford step 7489 current loss 0.069892, current_train_items 239680.
I0304 19:31:39.296639 22502662377600 run.py:483] Algo bellman_ford step 7490 current loss 0.003311, current_train_items 239712.
I0304 19:31:39.312625 22502662377600 run.py:483] Algo bellman_ford step 7491 current loss 0.014686, current_train_items 239744.
I0304 19:31:39.336020 22502662377600 run.py:483] Algo bellman_ford step 7492 current loss 0.020442, current_train_items 239776.
I0304 19:31:39.367460 22502662377600 run.py:483] Algo bellman_ford step 7493 current loss 0.046443, current_train_items 239808.
I0304 19:31:39.400293 22502662377600 run.py:483] Algo bellman_ford step 7494 current loss 0.085283, current_train_items 239840.
I0304 19:31:39.420011 22502662377600 run.py:483] Algo bellman_ford step 7495 current loss 0.003857, current_train_items 239872.
I0304 19:31:39.436378 22502662377600 run.py:483] Algo bellman_ford step 7496 current loss 0.022225, current_train_items 239904.
I0304 19:31:39.460598 22502662377600 run.py:483] Algo bellman_ford step 7497 current loss 0.081921, current_train_items 239936.
I0304 19:31:39.491727 22502662377600 run.py:483] Algo bellman_ford step 7498 current loss 0.078793, current_train_items 239968.
I0304 19:31:39.524746 22502662377600 run.py:483] Algo bellman_ford step 7499 current loss 0.096378, current_train_items 240000.
I0304 19:31:39.544488 22502662377600 run.py:483] Algo bellman_ford step 7500 current loss 0.009296, current_train_items 240032.
I0304 19:31:39.552551 22502662377600 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0304 19:31:39.552653 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:31:39.569347 22502662377600 run.py:483] Algo bellman_ford step 7501 current loss 0.017749, current_train_items 240064.
I0304 19:31:39.594254 22502662377600 run.py:483] Algo bellman_ford step 7502 current loss 0.112499, current_train_items 240096.
I0304 19:31:39.626765 22502662377600 run.py:483] Algo bellman_ford step 7503 current loss 0.079161, current_train_items 240128.
I0304 19:31:39.660432 22502662377600 run.py:483] Algo bellman_ford step 7504 current loss 0.061558, current_train_items 240160.
I0304 19:31:39.680910 22502662377600 run.py:483] Algo bellman_ford step 7505 current loss 0.021776, current_train_items 240192.
I0304 19:31:39.696916 22502662377600 run.py:483] Algo bellman_ford step 7506 current loss 0.014623, current_train_items 240224.
I0304 19:31:39.720445 22502662377600 run.py:483] Algo bellman_ford step 7507 current loss 0.042214, current_train_items 240256.
I0304 19:31:39.751846 22502662377600 run.py:483] Algo bellman_ford step 7508 current loss 0.027731, current_train_items 240288.
I0304 19:31:39.784565 22502662377600 run.py:483] Algo bellman_ford step 7509 current loss 0.048406, current_train_items 240320.
I0304 19:31:39.804229 22502662377600 run.py:483] Algo bellman_ford step 7510 current loss 0.025032, current_train_items 240352.
I0304 19:31:39.820723 22502662377600 run.py:483] Algo bellman_ford step 7511 current loss 0.027910, current_train_items 240384.
I0304 19:31:39.843702 22502662377600 run.py:483] Algo bellman_ford step 7512 current loss 0.041856, current_train_items 240416.
I0304 19:31:39.875540 22502662377600 run.py:483] Algo bellman_ford step 7513 current loss 0.054964, current_train_items 240448.
I0304 19:31:39.907740 22502662377600 run.py:483] Algo bellman_ford step 7514 current loss 0.107362, current_train_items 240480.
I0304 19:31:39.927683 22502662377600 run.py:483] Algo bellman_ford step 7515 current loss 0.008650, current_train_items 240512.
I0304 19:31:39.943133 22502662377600 run.py:483] Algo bellman_ford step 7516 current loss 0.041236, current_train_items 240544.
I0304 19:31:39.967582 22502662377600 run.py:483] Algo bellman_ford step 7517 current loss 0.074707, current_train_items 240576.
I0304 19:31:39.999384 22502662377600 run.py:483] Algo bellman_ford step 7518 current loss 0.096346, current_train_items 240608.
I0304 19:31:40.033406 22502662377600 run.py:483] Algo bellman_ford step 7519 current loss 0.125474, current_train_items 240640.
I0304 19:31:40.053082 22502662377600 run.py:483] Algo bellman_ford step 7520 current loss 0.003765, current_train_items 240672.
I0304 19:31:40.069150 22502662377600 run.py:483] Algo bellman_ford step 7521 current loss 0.052127, current_train_items 240704.
I0304 19:31:40.092909 22502662377600 run.py:483] Algo bellman_ford step 7522 current loss 0.038983, current_train_items 240736.
I0304 19:31:40.124993 22502662377600 run.py:483] Algo bellman_ford step 7523 current loss 0.081699, current_train_items 240768.
I0304 19:31:40.156054 22502662377600 run.py:483] Algo bellman_ford step 7524 current loss 0.055363, current_train_items 240800.
I0304 19:31:40.175470 22502662377600 run.py:483] Algo bellman_ford step 7525 current loss 0.048445, current_train_items 240832.
I0304 19:31:40.191440 22502662377600 run.py:483] Algo bellman_ford step 7526 current loss 0.032521, current_train_items 240864.
I0304 19:31:40.216177 22502662377600 run.py:483] Algo bellman_ford step 7527 current loss 0.083069, current_train_items 240896.
I0304 19:31:40.247411 22502662377600 run.py:483] Algo bellman_ford step 7528 current loss 0.044183, current_train_items 240928.
I0304 19:31:40.280933 22502662377600 run.py:483] Algo bellman_ford step 7529 current loss 0.064717, current_train_items 240960.
I0304 19:31:40.300405 22502662377600 run.py:483] Algo bellman_ford step 7530 current loss 0.021099, current_train_items 240992.
I0304 19:31:40.316437 22502662377600 run.py:483] Algo bellman_ford step 7531 current loss 0.008421, current_train_items 241024.
I0304 19:31:40.339612 22502662377600 run.py:483] Algo bellman_ford step 7532 current loss 0.060342, current_train_items 241056.
I0304 19:31:40.372700 22502662377600 run.py:483] Algo bellman_ford step 7533 current loss 0.143569, current_train_items 241088.
I0304 19:31:40.407090 22502662377600 run.py:483] Algo bellman_ford step 7534 current loss 0.106468, current_train_items 241120.
I0304 19:31:40.426628 22502662377600 run.py:483] Algo bellman_ford step 7535 current loss 0.006299, current_train_items 241152.
I0304 19:31:40.442726 22502662377600 run.py:483] Algo bellman_ford step 7536 current loss 0.015439, current_train_items 241184.
I0304 19:31:40.466594 22502662377600 run.py:483] Algo bellman_ford step 7537 current loss 0.086865, current_train_items 241216.
I0304 19:31:40.499136 22502662377600 run.py:483] Algo bellman_ford step 7538 current loss 0.056190, current_train_items 241248.
I0304 19:31:40.532271 22502662377600 run.py:483] Algo bellman_ford step 7539 current loss 0.045074, current_train_items 241280.
I0304 19:31:40.551980 22502662377600 run.py:483] Algo bellman_ford step 7540 current loss 0.003929, current_train_items 241312.
I0304 19:31:40.568506 22502662377600 run.py:483] Algo bellman_ford step 7541 current loss 0.032983, current_train_items 241344.
I0304 19:31:40.594299 22502662377600 run.py:483] Algo bellman_ford step 7542 current loss 0.082123, current_train_items 241376.
I0304 19:31:40.627312 22502662377600 run.py:483] Algo bellman_ford step 7543 current loss 0.066269, current_train_items 241408.
I0304 19:31:40.661260 22502662377600 run.py:483] Algo bellman_ford step 7544 current loss 0.065732, current_train_items 241440.
I0304 19:31:40.680778 22502662377600 run.py:483] Algo bellman_ford step 7545 current loss 0.004990, current_train_items 241472.
I0304 19:31:40.697046 22502662377600 run.py:483] Algo bellman_ford step 7546 current loss 0.022427, current_train_items 241504.
I0304 19:31:40.721844 22502662377600 run.py:483] Algo bellman_ford step 7547 current loss 0.065205, current_train_items 241536.
I0304 19:31:40.754351 22502662377600 run.py:483] Algo bellman_ford step 7548 current loss 0.054321, current_train_items 241568.
I0304 19:31:40.788091 22502662377600 run.py:483] Algo bellman_ford step 7549 current loss 0.050190, current_train_items 241600.
I0304 19:31:40.807711 22502662377600 run.py:483] Algo bellman_ford step 7550 current loss 0.003362, current_train_items 241632.
I0304 19:31:40.815843 22502662377600 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0304 19:31:40.815948 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:40.833040 22502662377600 run.py:483] Algo bellman_ford step 7551 current loss 0.059385, current_train_items 241664.
I0304 19:31:40.857510 22502662377600 run.py:483] Algo bellman_ford step 7552 current loss 0.019927, current_train_items 241696.
I0304 19:31:40.889905 22502662377600 run.py:483] Algo bellman_ford step 7553 current loss 0.093085, current_train_items 241728.
I0304 19:31:40.923727 22502662377600 run.py:483] Algo bellman_ford step 7554 current loss 0.105179, current_train_items 241760.
I0304 19:31:40.943412 22502662377600 run.py:483] Algo bellman_ford step 7555 current loss 0.005084, current_train_items 241792.
I0304 19:31:40.959414 22502662377600 run.py:483] Algo bellman_ford step 7556 current loss 0.060210, current_train_items 241824.
I0304 19:31:40.983662 22502662377600 run.py:483] Algo bellman_ford step 7557 current loss 0.061254, current_train_items 241856.
I0304 19:31:41.016808 22502662377600 run.py:483] Algo bellman_ford step 7558 current loss 0.078816, current_train_items 241888.
I0304 19:31:41.049279 22502662377600 run.py:483] Algo bellman_ford step 7559 current loss 0.047681, current_train_items 241920.
I0304 19:31:41.069141 22502662377600 run.py:483] Algo bellman_ford step 7560 current loss 0.005969, current_train_items 241952.
I0304 19:31:41.086178 22502662377600 run.py:483] Algo bellman_ford step 7561 current loss 0.042380, current_train_items 241984.
I0304 19:31:41.109577 22502662377600 run.py:483] Algo bellman_ford step 7562 current loss 0.077324, current_train_items 242016.
I0304 19:31:41.141730 22502662377600 run.py:483] Algo bellman_ford step 7563 current loss 0.068183, current_train_items 242048.
I0304 19:31:41.175467 22502662377600 run.py:483] Algo bellman_ford step 7564 current loss 0.065861, current_train_items 242080.
I0304 19:31:41.194851 22502662377600 run.py:483] Algo bellman_ford step 7565 current loss 0.005448, current_train_items 242112.
I0304 19:31:41.210688 22502662377600 run.py:483] Algo bellman_ford step 7566 current loss 0.108833, current_train_items 242144.
I0304 19:31:41.234792 22502662377600 run.py:483] Algo bellman_ford step 7567 current loss 0.036832, current_train_items 242176.
I0304 19:31:41.266585 22502662377600 run.py:483] Algo bellman_ford step 7568 current loss 0.064149, current_train_items 242208.
I0304 19:31:41.299183 22502662377600 run.py:483] Algo bellman_ford step 7569 current loss 0.140646, current_train_items 242240.
I0304 19:31:41.318962 22502662377600 run.py:483] Algo bellman_ford step 7570 current loss 0.005554, current_train_items 242272.
I0304 19:31:41.335406 22502662377600 run.py:483] Algo bellman_ford step 7571 current loss 0.020614, current_train_items 242304.
I0304 19:31:41.358801 22502662377600 run.py:483] Algo bellman_ford step 7572 current loss 0.030990, current_train_items 242336.
I0304 19:31:41.389441 22502662377600 run.py:483] Algo bellman_ford step 7573 current loss 0.038912, current_train_items 242368.
I0304 19:31:41.420782 22502662377600 run.py:483] Algo bellman_ford step 7574 current loss 0.032044, current_train_items 242400.
I0304 19:31:41.440524 22502662377600 run.py:483] Algo bellman_ford step 7575 current loss 0.002147, current_train_items 242432.
I0304 19:31:41.457338 22502662377600 run.py:483] Algo bellman_ford step 7576 current loss 0.017417, current_train_items 242464.
I0304 19:31:41.480184 22502662377600 run.py:483] Algo bellman_ford step 7577 current loss 0.038969, current_train_items 242496.
I0304 19:31:41.512191 22502662377600 run.py:483] Algo bellman_ford step 7578 current loss 0.055397, current_train_items 242528.
I0304 19:31:41.547268 22502662377600 run.py:483] Algo bellman_ford step 7579 current loss 0.099749, current_train_items 242560.
I0304 19:31:41.566772 22502662377600 run.py:483] Algo bellman_ford step 7580 current loss 0.003463, current_train_items 242592.
I0304 19:31:41.583013 22502662377600 run.py:483] Algo bellman_ford step 7581 current loss 0.031956, current_train_items 242624.
I0304 19:31:41.607190 22502662377600 run.py:483] Algo bellman_ford step 7582 current loss 0.074038, current_train_items 242656.
I0304 19:31:41.638773 22502662377600 run.py:483] Algo bellman_ford step 7583 current loss 0.044300, current_train_items 242688.
I0304 19:31:41.671266 22502662377600 run.py:483] Algo bellman_ford step 7584 current loss 0.061066, current_train_items 242720.
I0304 19:31:41.691370 22502662377600 run.py:483] Algo bellman_ford step 7585 current loss 0.004015, current_train_items 242752.
I0304 19:31:41.707211 22502662377600 run.py:483] Algo bellman_ford step 7586 current loss 0.032947, current_train_items 242784.
I0304 19:31:41.730827 22502662377600 run.py:483] Algo bellman_ford step 7587 current loss 0.101658, current_train_items 242816.
I0304 19:31:41.760390 22502662377600 run.py:483] Algo bellman_ford step 7588 current loss 0.061369, current_train_items 242848.
I0304 19:31:41.794448 22502662377600 run.py:483] Algo bellman_ford step 7589 current loss 0.085526, current_train_items 242880.
I0304 19:31:41.814132 22502662377600 run.py:483] Algo bellman_ford step 7590 current loss 0.022607, current_train_items 242912.
I0304 19:31:41.830004 22502662377600 run.py:483] Algo bellman_ford step 7591 current loss 0.018947, current_train_items 242944.
I0304 19:31:41.854617 22502662377600 run.py:483] Algo bellman_ford step 7592 current loss 0.172003, current_train_items 242976.
I0304 19:31:41.884909 22502662377600 run.py:483] Algo bellman_ford step 7593 current loss 0.089739, current_train_items 243008.
I0304 19:31:41.918570 22502662377600 run.py:483] Algo bellman_ford step 7594 current loss 0.131274, current_train_items 243040.
I0304 19:31:41.937993 22502662377600 run.py:483] Algo bellman_ford step 7595 current loss 0.006883, current_train_items 243072.
I0304 19:31:41.953968 22502662377600 run.py:483] Algo bellman_ford step 7596 current loss 0.025295, current_train_items 243104.
I0304 19:31:41.978101 22502662377600 run.py:483] Algo bellman_ford step 7597 current loss 0.037232, current_train_items 243136.
I0304 19:31:42.010547 22502662377600 run.py:483] Algo bellman_ford step 7598 current loss 0.079020, current_train_items 243168.
I0304 19:31:42.041613 22502662377600 run.py:483] Algo bellman_ford step 7599 current loss 0.066824, current_train_items 243200.
I0304 19:31:42.061798 22502662377600 run.py:483] Algo bellman_ford step 7600 current loss 0.007028, current_train_items 243232.
I0304 19:31:42.069690 22502662377600 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0304 19:31:42.069794 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:31:42.086621 22502662377600 run.py:483] Algo bellman_ford step 7601 current loss 0.019261, current_train_items 243264.
I0304 19:31:42.112651 22502662377600 run.py:483] Algo bellman_ford step 7602 current loss 0.086076, current_train_items 243296.
I0304 19:31:42.144414 22502662377600 run.py:483] Algo bellman_ford step 7603 current loss 0.044164, current_train_items 243328.
I0304 19:31:42.179405 22502662377600 run.py:483] Algo bellman_ford step 7604 current loss 0.121088, current_train_items 243360.
I0304 19:31:42.199279 22502662377600 run.py:483] Algo bellman_ford step 7605 current loss 0.033454, current_train_items 243392.
I0304 19:31:42.214473 22502662377600 run.py:483] Algo bellman_ford step 7606 current loss 0.010793, current_train_items 243424.
I0304 19:31:42.238041 22502662377600 run.py:483] Algo bellman_ford step 7607 current loss 0.048202, current_train_items 243456.
I0304 19:31:42.270759 22502662377600 run.py:483] Algo bellman_ford step 7608 current loss 0.055184, current_train_items 243488.
I0304 19:31:42.305108 22502662377600 run.py:483] Algo bellman_ford step 7609 current loss 0.075787, current_train_items 243520.
I0304 19:31:42.324600 22502662377600 run.py:483] Algo bellman_ford step 7610 current loss 0.003829, current_train_items 243552.
I0304 19:31:42.341088 22502662377600 run.py:483] Algo bellman_ford step 7611 current loss 0.023144, current_train_items 243584.
I0304 19:31:42.365167 22502662377600 run.py:483] Algo bellman_ford step 7612 current loss 0.044950, current_train_items 243616.
I0304 19:31:42.394812 22502662377600 run.py:483] Algo bellman_ford step 7613 current loss 0.031971, current_train_items 243648.
I0304 19:31:42.427823 22502662377600 run.py:483] Algo bellman_ford step 7614 current loss 0.086909, current_train_items 243680.
I0304 19:31:42.447486 22502662377600 run.py:483] Algo bellman_ford step 7615 current loss 0.002921, current_train_items 243712.
I0304 19:31:42.463766 22502662377600 run.py:483] Algo bellman_ford step 7616 current loss 0.018746, current_train_items 243744.
I0304 19:31:42.487510 22502662377600 run.py:483] Algo bellman_ford step 7617 current loss 0.043040, current_train_items 243776.
I0304 19:31:42.519145 22502662377600 run.py:483] Algo bellman_ford step 7618 current loss 0.051964, current_train_items 243808.
I0304 19:31:42.552803 22502662377600 run.py:483] Algo bellman_ford step 7619 current loss 0.113752, current_train_items 243840.
I0304 19:31:42.572276 22502662377600 run.py:483] Algo bellman_ford step 7620 current loss 0.006700, current_train_items 243872.
I0304 19:31:42.588348 22502662377600 run.py:483] Algo bellman_ford step 7621 current loss 0.020029, current_train_items 243904.
I0304 19:31:42.612538 22502662377600 run.py:483] Algo bellman_ford step 7622 current loss 0.061304, current_train_items 243936.
I0304 19:31:42.645458 22502662377600 run.py:483] Algo bellman_ford step 7623 current loss 0.057538, current_train_items 243968.
I0304 19:31:42.678237 22502662377600 run.py:483] Algo bellman_ford step 7624 current loss 0.052743, current_train_items 244000.
I0304 19:31:42.697881 22502662377600 run.py:483] Algo bellman_ford step 7625 current loss 0.005827, current_train_items 244032.
I0304 19:31:42.713218 22502662377600 run.py:483] Algo bellman_ford step 7626 current loss 0.012399, current_train_items 244064.
I0304 19:31:42.738253 22502662377600 run.py:483] Algo bellman_ford step 7627 current loss 0.054881, current_train_items 244096.
I0304 19:31:42.770990 22502662377600 run.py:483] Algo bellman_ford step 7628 current loss 0.042927, current_train_items 244128.
I0304 19:31:42.803635 22502662377600 run.py:483] Algo bellman_ford step 7629 current loss 0.101773, current_train_items 244160.
I0304 19:31:42.823016 22502662377600 run.py:483] Algo bellman_ford step 7630 current loss 0.022336, current_train_items 244192.
I0304 19:31:42.839495 22502662377600 run.py:483] Algo bellman_ford step 7631 current loss 0.027433, current_train_items 244224.
I0304 19:31:42.863162 22502662377600 run.py:483] Algo bellman_ford step 7632 current loss 0.041029, current_train_items 244256.
I0304 19:31:42.895663 22502662377600 run.py:483] Algo bellman_ford step 7633 current loss 0.037838, current_train_items 244288.
I0304 19:31:42.929656 22502662377600 run.py:483] Algo bellman_ford step 7634 current loss 0.065333, current_train_items 244320.
I0304 19:31:42.949204 22502662377600 run.py:483] Algo bellman_ford step 7635 current loss 0.004238, current_train_items 244352.
I0304 19:31:42.965157 22502662377600 run.py:483] Algo bellman_ford step 7636 current loss 0.016855, current_train_items 244384.
I0304 19:31:42.989207 22502662377600 run.py:483] Algo bellman_ford step 7637 current loss 0.031391, current_train_items 244416.
I0304 19:31:43.019862 22502662377600 run.py:483] Algo bellman_ford step 7638 current loss 0.037710, current_train_items 244448.
I0304 19:31:43.055722 22502662377600 run.py:483] Algo bellman_ford step 7639 current loss 0.073981, current_train_items 244480.
I0304 19:31:43.075401 22502662377600 run.py:483] Algo bellman_ford step 7640 current loss 0.002743, current_train_items 244512.
I0304 19:31:43.091253 22502662377600 run.py:483] Algo bellman_ford step 7641 current loss 0.031578, current_train_items 244544.
I0304 19:31:43.114323 22502662377600 run.py:483] Algo bellman_ford step 7642 current loss 0.041700, current_train_items 244576.
I0304 19:31:43.148416 22502662377600 run.py:483] Algo bellman_ford step 7643 current loss 0.076567, current_train_items 244608.
I0304 19:31:43.183660 22502662377600 run.py:483] Algo bellman_ford step 7644 current loss 0.058980, current_train_items 244640.
I0304 19:31:43.203221 22502662377600 run.py:483] Algo bellman_ford step 7645 current loss 0.007630, current_train_items 244672.
I0304 19:31:43.219416 22502662377600 run.py:483] Algo bellman_ford step 7646 current loss 0.024293, current_train_items 244704.
I0304 19:31:43.244655 22502662377600 run.py:483] Algo bellman_ford step 7647 current loss 0.060116, current_train_items 244736.
I0304 19:31:43.277971 22502662377600 run.py:483] Algo bellman_ford step 7648 current loss 0.071939, current_train_items 244768.
I0304 19:31:43.313193 22502662377600 run.py:483] Algo bellman_ford step 7649 current loss 0.084966, current_train_items 244800.
I0304 19:31:43.332811 22502662377600 run.py:483] Algo bellman_ford step 7650 current loss 0.036034, current_train_items 244832.
I0304 19:31:43.340984 22502662377600 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0304 19:31:43.341092 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:31:43.357600 22502662377600 run.py:483] Algo bellman_ford step 7651 current loss 0.008260, current_train_items 244864.
I0304 19:31:43.382770 22502662377600 run.py:483] Algo bellman_ford step 7652 current loss 0.119729, current_train_items 244896.
I0304 19:31:43.415249 22502662377600 run.py:483] Algo bellman_ford step 7653 current loss 0.073986, current_train_items 244928.
I0304 19:31:43.449284 22502662377600 run.py:483] Algo bellman_ford step 7654 current loss 0.134640, current_train_items 244960.
I0304 19:31:43.469396 22502662377600 run.py:483] Algo bellman_ford step 7655 current loss 0.032791, current_train_items 244992.
I0304 19:31:43.485294 22502662377600 run.py:483] Algo bellman_ford step 7656 current loss 0.020983, current_train_items 245024.
I0304 19:31:43.509992 22502662377600 run.py:483] Algo bellman_ford step 7657 current loss 0.091181, current_train_items 245056.
I0304 19:31:43.542718 22502662377600 run.py:483] Algo bellman_ford step 7658 current loss 0.136575, current_train_items 245088.
I0304 19:31:43.578021 22502662377600 run.py:483] Algo bellman_ford step 7659 current loss 0.218153, current_train_items 245120.
I0304 19:31:43.598300 22502662377600 run.py:483] Algo bellman_ford step 7660 current loss 0.006575, current_train_items 245152.
I0304 19:31:43.614677 22502662377600 run.py:483] Algo bellman_ford step 7661 current loss 0.011880, current_train_items 245184.
I0304 19:31:43.639025 22502662377600 run.py:483] Algo bellman_ford step 7662 current loss 0.049374, current_train_items 245216.
I0304 19:31:43.670546 22502662377600 run.py:483] Algo bellman_ford step 7663 current loss 0.092340, current_train_items 245248.
I0304 19:31:43.704349 22502662377600 run.py:483] Algo bellman_ford step 7664 current loss 0.123673, current_train_items 245280.
I0304 19:31:43.723998 22502662377600 run.py:483] Algo bellman_ford step 7665 current loss 0.030790, current_train_items 245312.
I0304 19:31:43.740631 22502662377600 run.py:483] Algo bellman_ford step 7666 current loss 0.032693, current_train_items 245344.
I0304 19:31:43.764941 22502662377600 run.py:483] Algo bellman_ford step 7667 current loss 0.105482, current_train_items 245376.
I0304 19:31:43.799620 22502662377600 run.py:483] Algo bellman_ford step 7668 current loss 0.142829, current_train_items 245408.
I0304 19:31:43.834252 22502662377600 run.py:483] Algo bellman_ford step 7669 current loss 0.125087, current_train_items 245440.
I0304 19:31:43.854243 22502662377600 run.py:483] Algo bellman_ford step 7670 current loss 0.012072, current_train_items 245472.
I0304 19:31:43.870862 22502662377600 run.py:483] Algo bellman_ford step 7671 current loss 0.022495, current_train_items 245504.
I0304 19:31:43.894530 22502662377600 run.py:483] Algo bellman_ford step 7672 current loss 0.087403, current_train_items 245536.
I0304 19:31:43.927001 22502662377600 run.py:483] Algo bellman_ford step 7673 current loss 0.076175, current_train_items 245568.
I0304 19:31:43.961410 22502662377600 run.py:483] Algo bellman_ford step 7674 current loss 0.138233, current_train_items 245600.
I0304 19:31:43.981469 22502662377600 run.py:483] Algo bellman_ford step 7675 current loss 0.009478, current_train_items 245632.
I0304 19:31:43.998208 22502662377600 run.py:483] Algo bellman_ford step 7676 current loss 0.068934, current_train_items 245664.
I0304 19:31:44.022650 22502662377600 run.py:483] Algo bellman_ford step 7677 current loss 0.092528, current_train_items 245696.
I0304 19:31:44.054425 22502662377600 run.py:483] Algo bellman_ford step 7678 current loss 0.070069, current_train_items 245728.
I0304 19:31:44.089205 22502662377600 run.py:483] Algo bellman_ford step 7679 current loss 0.077180, current_train_items 245760.
I0304 19:31:44.108876 22502662377600 run.py:483] Algo bellman_ford step 7680 current loss 0.004141, current_train_items 245792.
I0304 19:31:44.125139 22502662377600 run.py:483] Algo bellman_ford step 7681 current loss 0.043010, current_train_items 245824.
I0304 19:31:44.149251 22502662377600 run.py:483] Algo bellman_ford step 7682 current loss 0.074091, current_train_items 245856.
I0304 19:31:44.181351 22502662377600 run.py:483] Algo bellman_ford step 7683 current loss 0.079162, current_train_items 245888.
I0304 19:31:44.215988 22502662377600 run.py:483] Algo bellman_ford step 7684 current loss 0.159476, current_train_items 245920.
I0304 19:31:44.236231 22502662377600 run.py:483] Algo bellman_ford step 7685 current loss 0.015787, current_train_items 245952.
I0304 19:31:44.252599 22502662377600 run.py:483] Algo bellman_ford step 7686 current loss 0.052170, current_train_items 245984.
I0304 19:31:44.275646 22502662377600 run.py:483] Algo bellman_ford step 7687 current loss 0.167573, current_train_items 246016.
I0304 19:31:44.308409 22502662377600 run.py:483] Algo bellman_ford step 7688 current loss 0.050201, current_train_items 246048.
I0304 19:31:44.342811 22502662377600 run.py:483] Algo bellman_ford step 7689 current loss 0.145036, current_train_items 246080.
I0304 19:31:44.362526 22502662377600 run.py:483] Algo bellman_ford step 7690 current loss 0.013801, current_train_items 246112.
I0304 19:31:44.378803 22502662377600 run.py:483] Algo bellman_ford step 7691 current loss 0.023789, current_train_items 246144.
I0304 19:31:44.402245 22502662377600 run.py:483] Algo bellman_ford step 7692 current loss 0.040392, current_train_items 246176.
I0304 19:31:44.436017 22502662377600 run.py:483] Algo bellman_ford step 7693 current loss 0.136979, current_train_items 246208.
I0304 19:31:44.469346 22502662377600 run.py:483] Algo bellman_ford step 7694 current loss 0.138503, current_train_items 246240.
I0304 19:31:44.489211 22502662377600 run.py:483] Algo bellman_ford step 7695 current loss 0.008425, current_train_items 246272.
I0304 19:31:44.505551 22502662377600 run.py:483] Algo bellman_ford step 7696 current loss 0.035110, current_train_items 246304.
I0304 19:31:44.530261 22502662377600 run.py:483] Algo bellman_ford step 7697 current loss 0.033530, current_train_items 246336.
I0304 19:31:44.563145 22502662377600 run.py:483] Algo bellman_ford step 7698 current loss 0.072587, current_train_items 246368.
I0304 19:31:44.598857 22502662377600 run.py:483] Algo bellman_ford step 7699 current loss 0.106586, current_train_items 246400.
I0304 19:31:44.619252 22502662377600 run.py:483] Algo bellman_ford step 7700 current loss 0.017099, current_train_items 246432.
I0304 19:31:44.627227 22502662377600 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0304 19:31:44.627330 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:44.644721 22502662377600 run.py:483] Algo bellman_ford step 7701 current loss 0.030584, current_train_items 246464.
I0304 19:31:44.669449 22502662377600 run.py:483] Algo bellman_ford step 7702 current loss 0.046555, current_train_items 246496.
I0304 19:31:44.700858 22502662377600 run.py:483] Algo bellman_ford step 7703 current loss 0.055756, current_train_items 246528.
I0304 19:31:44.734218 22502662377600 run.py:483] Algo bellman_ford step 7704 current loss 0.084901, current_train_items 246560.
I0304 19:31:44.754268 22502662377600 run.py:483] Algo bellman_ford step 7705 current loss 0.006875, current_train_items 246592.
I0304 19:31:44.770130 22502662377600 run.py:483] Algo bellman_ford step 7706 current loss 0.011239, current_train_items 246624.
I0304 19:31:44.795542 22502662377600 run.py:483] Algo bellman_ford step 7707 current loss 0.038239, current_train_items 246656.
I0304 19:31:44.826673 22502662377600 run.py:483] Algo bellman_ford step 7708 current loss 0.051549, current_train_items 246688.
I0304 19:31:44.859551 22502662377600 run.py:483] Algo bellman_ford step 7709 current loss 0.085183, current_train_items 246720.
I0304 19:31:44.879723 22502662377600 run.py:483] Algo bellman_ford step 7710 current loss 0.008960, current_train_items 246752.
I0304 19:31:44.896135 22502662377600 run.py:483] Algo bellman_ford step 7711 current loss 0.008839, current_train_items 246784.
I0304 19:31:44.920197 22502662377600 run.py:483] Algo bellman_ford step 7712 current loss 0.041510, current_train_items 246816.
I0304 19:31:44.951268 22502662377600 run.py:483] Algo bellman_ford step 7713 current loss 0.044169, current_train_items 246848.
I0304 19:31:44.985009 22502662377600 run.py:483] Algo bellman_ford step 7714 current loss 0.045254, current_train_items 246880.
I0304 19:31:45.004701 22502662377600 run.py:483] Algo bellman_ford step 7715 current loss 0.018952, current_train_items 246912.
I0304 19:31:45.020771 22502662377600 run.py:483] Algo bellman_ford step 7716 current loss 0.005817, current_train_items 246944.
I0304 19:31:45.045048 22502662377600 run.py:483] Algo bellman_ford step 7717 current loss 0.038957, current_train_items 246976.
I0304 19:31:45.077016 22502662377600 run.py:483] Algo bellman_ford step 7718 current loss 0.071032, current_train_items 247008.
I0304 19:31:45.109627 22502662377600 run.py:483] Algo bellman_ford step 7719 current loss 0.062401, current_train_items 247040.
I0304 19:31:45.129669 22502662377600 run.py:483] Algo bellman_ford step 7720 current loss 0.006285, current_train_items 247072.
I0304 19:31:45.145643 22502662377600 run.py:483] Algo bellman_ford step 7721 current loss 0.015735, current_train_items 247104.
I0304 19:31:45.170394 22502662377600 run.py:483] Algo bellman_ford step 7722 current loss 0.031316, current_train_items 247136.
I0304 19:31:45.203242 22502662377600 run.py:483] Algo bellman_ford step 7723 current loss 0.058672, current_train_items 247168.
I0304 19:31:45.237565 22502662377600 run.py:483] Algo bellman_ford step 7724 current loss 0.072029, current_train_items 247200.
I0304 19:31:45.257072 22502662377600 run.py:483] Algo bellman_ford step 7725 current loss 0.005208, current_train_items 247232.
I0304 19:31:45.273160 22502662377600 run.py:483] Algo bellman_ford step 7726 current loss 0.043721, current_train_items 247264.
I0304 19:31:45.296027 22502662377600 run.py:483] Algo bellman_ford step 7727 current loss 0.064580, current_train_items 247296.
I0304 19:31:45.327847 22502662377600 run.py:483] Algo bellman_ford step 7728 current loss 0.078166, current_train_items 247328.
I0304 19:31:45.360560 22502662377600 run.py:483] Algo bellman_ford step 7729 current loss 0.109350, current_train_items 247360.
I0304 19:31:45.380060 22502662377600 run.py:483] Algo bellman_ford step 7730 current loss 0.025043, current_train_items 247392.
I0304 19:31:45.395884 22502662377600 run.py:483] Algo bellman_ford step 7731 current loss 0.015813, current_train_items 247424.
I0304 19:31:45.419440 22502662377600 run.py:483] Algo bellman_ford step 7732 current loss 0.066680, current_train_items 247456.
I0304 19:31:45.451178 22502662377600 run.py:483] Algo bellman_ford step 7733 current loss 0.184074, current_train_items 247488.
I0304 19:31:45.483292 22502662377600 run.py:483] Algo bellman_ford step 7734 current loss 0.208982, current_train_items 247520.
I0304 19:31:45.502845 22502662377600 run.py:483] Algo bellman_ford step 7735 current loss 0.005893, current_train_items 247552.
I0304 19:31:45.519010 22502662377600 run.py:483] Algo bellman_ford step 7736 current loss 0.046098, current_train_items 247584.
I0304 19:31:45.543112 22502662377600 run.py:483] Algo bellman_ford step 7737 current loss 0.025949, current_train_items 247616.
I0304 19:31:45.574344 22502662377600 run.py:483] Algo bellman_ford step 7738 current loss 0.090161, current_train_items 247648.
I0304 19:31:45.608393 22502662377600 run.py:483] Algo bellman_ford step 7739 current loss 0.109524, current_train_items 247680.
I0304 19:31:45.628401 22502662377600 run.py:483] Algo bellman_ford step 7740 current loss 0.029921, current_train_items 247712.
I0304 19:31:45.644551 22502662377600 run.py:483] Algo bellman_ford step 7741 current loss 0.028371, current_train_items 247744.
I0304 19:31:45.669780 22502662377600 run.py:483] Algo bellman_ford step 7742 current loss 0.077877, current_train_items 247776.
I0304 19:31:45.702831 22502662377600 run.py:483] Algo bellman_ford step 7743 current loss 0.119395, current_train_items 247808.
I0304 19:31:45.737050 22502662377600 run.py:483] Algo bellman_ford step 7744 current loss 0.128009, current_train_items 247840.
I0304 19:31:45.756562 22502662377600 run.py:483] Algo bellman_ford step 7745 current loss 0.061117, current_train_items 247872.
I0304 19:31:45.772439 22502662377600 run.py:483] Algo bellman_ford step 7746 current loss 0.016163, current_train_items 247904.
I0304 19:31:45.796569 22502662377600 run.py:483] Algo bellman_ford step 7747 current loss 0.056855, current_train_items 247936.
I0304 19:31:45.829808 22502662377600 run.py:483] Algo bellman_ford step 7748 current loss 0.127677, current_train_items 247968.
I0304 19:31:45.863837 22502662377600 run.py:483] Algo bellman_ford step 7749 current loss 0.080459, current_train_items 248000.
I0304 19:31:45.883596 22502662377600 run.py:483] Algo bellman_ford step 7750 current loss 0.009217, current_train_items 248032.
I0304 19:31:45.892127 22502662377600 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0304 19:31:45.892232 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:45.909357 22502662377600 run.py:483] Algo bellman_ford step 7751 current loss 0.036098, current_train_items 248064.
I0304 19:31:45.933952 22502662377600 run.py:483] Algo bellman_ford step 7752 current loss 0.072531, current_train_items 248096.
I0304 19:31:45.965956 22502662377600 run.py:483] Algo bellman_ford step 7753 current loss 0.049175, current_train_items 248128.
I0304 19:31:46.002184 22502662377600 run.py:483] Algo bellman_ford step 7754 current loss 0.111142, current_train_items 248160.
I0304 19:31:46.022655 22502662377600 run.py:483] Algo bellman_ford step 7755 current loss 0.005489, current_train_items 248192.
I0304 19:31:46.038166 22502662377600 run.py:483] Algo bellman_ford step 7756 current loss 0.020052, current_train_items 248224.
I0304 19:31:46.062615 22502662377600 run.py:483] Algo bellman_ford step 7757 current loss 0.026461, current_train_items 248256.
I0304 19:31:46.093375 22502662377600 run.py:483] Algo bellman_ford step 7758 current loss 0.061994, current_train_items 248288.
I0304 19:31:46.127383 22502662377600 run.py:483] Algo bellman_ford step 7759 current loss 0.090943, current_train_items 248320.
I0304 19:31:46.147746 22502662377600 run.py:483] Algo bellman_ford step 7760 current loss 0.004377, current_train_items 248352.
I0304 19:31:46.163992 22502662377600 run.py:483] Algo bellman_ford step 7761 current loss 0.045969, current_train_items 248384.
I0304 19:31:46.188368 22502662377600 run.py:483] Algo bellman_ford step 7762 current loss 0.050451, current_train_items 248416.
I0304 19:31:46.220049 22502662377600 run.py:483] Algo bellman_ford step 7763 current loss 0.069014, current_train_items 248448.
I0304 19:31:46.252394 22502662377600 run.py:483] Algo bellman_ford step 7764 current loss 0.050648, current_train_items 248480.
I0304 19:31:46.272150 22502662377600 run.py:483] Algo bellman_ford step 7765 current loss 0.006719, current_train_items 248512.
I0304 19:31:46.288329 22502662377600 run.py:483] Algo bellman_ford step 7766 current loss 0.024684, current_train_items 248544.
I0304 19:31:46.312360 22502662377600 run.py:483] Algo bellman_ford step 7767 current loss 0.049577, current_train_items 248576.
I0304 19:31:46.344076 22502662377600 run.py:483] Algo bellman_ford step 7768 current loss 0.052435, current_train_items 248608.
I0304 19:31:46.377803 22502662377600 run.py:483] Algo bellman_ford step 7769 current loss 0.083421, current_train_items 248640.
I0304 19:31:46.397498 22502662377600 run.py:483] Algo bellman_ford step 7770 current loss 0.004227, current_train_items 248672.
I0304 19:31:46.413794 22502662377600 run.py:483] Algo bellman_ford step 7771 current loss 0.040358, current_train_items 248704.
I0304 19:31:46.437110 22502662377600 run.py:483] Algo bellman_ford step 7772 current loss 0.032492, current_train_items 248736.
I0304 19:31:46.469501 22502662377600 run.py:483] Algo bellman_ford step 7773 current loss 0.046651, current_train_items 248768.
I0304 19:31:46.502691 22502662377600 run.py:483] Algo bellman_ford step 7774 current loss 0.071897, current_train_items 248800.
I0304 19:31:46.522555 22502662377600 run.py:483] Algo bellman_ford step 7775 current loss 0.006176, current_train_items 248832.
I0304 19:31:46.538805 22502662377600 run.py:483] Algo bellman_ford step 7776 current loss 0.079498, current_train_items 248864.
I0304 19:31:46.563593 22502662377600 run.py:483] Algo bellman_ford step 7777 current loss 0.043177, current_train_items 248896.
I0304 19:31:46.595934 22502662377600 run.py:483] Algo bellman_ford step 7778 current loss 0.098655, current_train_items 248928.
I0304 19:31:46.631229 22502662377600 run.py:483] Algo bellman_ford step 7779 current loss 0.114116, current_train_items 248960.
I0304 19:31:46.650476 22502662377600 run.py:483] Algo bellman_ford step 7780 current loss 0.002077, current_train_items 248992.
I0304 19:31:46.666824 22502662377600 run.py:483] Algo bellman_ford step 7781 current loss 0.033246, current_train_items 249024.
I0304 19:31:46.690683 22502662377600 run.py:483] Algo bellman_ford step 7782 current loss 0.060382, current_train_items 249056.
I0304 19:31:46.723314 22502662377600 run.py:483] Algo bellman_ford step 7783 current loss 0.066295, current_train_items 249088.
I0304 19:31:46.757471 22502662377600 run.py:483] Algo bellman_ford step 7784 current loss 0.060996, current_train_items 249120.
I0304 19:31:46.777949 22502662377600 run.py:483] Algo bellman_ford step 7785 current loss 0.008119, current_train_items 249152.
I0304 19:31:46.794120 22502662377600 run.py:483] Algo bellman_ford step 7786 current loss 0.037497, current_train_items 249184.
I0304 19:31:46.817432 22502662377600 run.py:483] Algo bellman_ford step 7787 current loss 0.049686, current_train_items 249216.
I0304 19:31:46.849105 22502662377600 run.py:483] Algo bellman_ford step 7788 current loss 0.037674, current_train_items 249248.
I0304 19:31:46.881134 22502662377600 run.py:483] Algo bellman_ford step 7789 current loss 0.075237, current_train_items 249280.
I0304 19:31:46.901064 22502662377600 run.py:483] Algo bellman_ford step 7790 current loss 0.005106, current_train_items 249312.
I0304 19:31:46.917169 22502662377600 run.py:483] Algo bellman_ford step 7791 current loss 0.017269, current_train_items 249344.
I0304 19:31:46.941002 22502662377600 run.py:483] Algo bellman_ford step 7792 current loss 0.063102, current_train_items 249376.
I0304 19:31:46.972248 22502662377600 run.py:483] Algo bellman_ford step 7793 current loss 0.088559, current_train_items 249408.
I0304 19:31:47.006025 22502662377600 run.py:483] Algo bellman_ford step 7794 current loss 0.064151, current_train_items 249440.
I0304 19:31:47.025653 22502662377600 run.py:483] Algo bellman_ford step 7795 current loss 0.017291, current_train_items 249472.
I0304 19:31:47.041554 22502662377600 run.py:483] Algo bellman_ford step 7796 current loss 0.024932, current_train_items 249504.
I0304 19:31:47.064626 22502662377600 run.py:483] Algo bellman_ford step 7797 current loss 0.065767, current_train_items 249536.
I0304 19:31:47.095800 22502662377600 run.py:483] Algo bellman_ford step 7798 current loss 0.103861, current_train_items 249568.
I0304 19:31:47.130923 22502662377600 run.py:483] Algo bellman_ford step 7799 current loss 0.137764, current_train_items 249600.
I0304 19:31:47.150895 22502662377600 run.py:483] Algo bellman_ford step 7800 current loss 0.003553, current_train_items 249632.
I0304 19:31:47.158766 22502662377600 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0304 19:31:47.158869 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:31:47.175533 22502662377600 run.py:483] Algo bellman_ford step 7801 current loss 0.034448, current_train_items 249664.
I0304 19:31:47.200160 22502662377600 run.py:483] Algo bellman_ford step 7802 current loss 0.040417, current_train_items 249696.
I0304 19:31:47.233693 22502662377600 run.py:483] Algo bellman_ford step 7803 current loss 0.097411, current_train_items 249728.
I0304 19:31:47.270037 22502662377600 run.py:483] Algo bellman_ford step 7804 current loss 0.109575, current_train_items 249760.
I0304 19:31:47.289965 22502662377600 run.py:483] Algo bellman_ford step 7805 current loss 0.008603, current_train_items 249792.
I0304 19:31:47.306085 22502662377600 run.py:483] Algo bellman_ford step 7806 current loss 0.014484, current_train_items 249824.
I0304 19:31:47.329345 22502662377600 run.py:483] Algo bellman_ford step 7807 current loss 0.024680, current_train_items 249856.
I0304 19:31:47.360057 22502662377600 run.py:483] Algo bellman_ford step 7808 current loss 0.042449, current_train_items 249888.
I0304 19:31:47.394096 22502662377600 run.py:483] Algo bellman_ford step 7809 current loss 0.181830, current_train_items 249920.
I0304 19:31:47.414169 22502662377600 run.py:483] Algo bellman_ford step 7810 current loss 0.005261, current_train_items 249952.
I0304 19:31:47.430660 22502662377600 run.py:483] Algo bellman_ford step 7811 current loss 0.017340, current_train_items 249984.
I0304 19:31:47.455222 22502662377600 run.py:483] Algo bellman_ford step 7812 current loss 0.079779, current_train_items 250016.
I0304 19:31:47.488205 22502662377600 run.py:483] Algo bellman_ford step 7813 current loss 0.071152, current_train_items 250048.
I0304 19:31:47.522408 22502662377600 run.py:483] Algo bellman_ford step 7814 current loss 0.067853, current_train_items 250080.
I0304 19:31:47.542058 22502662377600 run.py:483] Algo bellman_ford step 7815 current loss 0.005166, current_train_items 250112.
I0304 19:31:47.557992 22502662377600 run.py:483] Algo bellman_ford step 7816 current loss 0.026857, current_train_items 250144.
I0304 19:31:47.582583 22502662377600 run.py:483] Algo bellman_ford step 7817 current loss 0.034948, current_train_items 250176.
I0304 19:31:47.613242 22502662377600 run.py:483] Algo bellman_ford step 7818 current loss 0.062851, current_train_items 250208.
I0304 19:31:47.646306 22502662377600 run.py:483] Algo bellman_ford step 7819 current loss 0.057891, current_train_items 250240.
I0304 19:31:47.666259 22502662377600 run.py:483] Algo bellman_ford step 7820 current loss 0.006004, current_train_items 250272.
I0304 19:31:47.682736 22502662377600 run.py:483] Algo bellman_ford step 7821 current loss 0.035148, current_train_items 250304.
I0304 19:31:47.708166 22502662377600 run.py:483] Algo bellman_ford step 7822 current loss 0.040925, current_train_items 250336.
I0304 19:31:47.741522 22502662377600 run.py:483] Algo bellman_ford step 7823 current loss 0.115529, current_train_items 250368.
I0304 19:31:47.774882 22502662377600 run.py:483] Algo bellman_ford step 7824 current loss 0.090137, current_train_items 250400.
I0304 19:31:47.794620 22502662377600 run.py:483] Algo bellman_ford step 7825 current loss 0.040043, current_train_items 250432.
I0304 19:31:47.811089 22502662377600 run.py:483] Algo bellman_ford step 7826 current loss 0.046532, current_train_items 250464.
I0304 19:31:47.835571 22502662377600 run.py:483] Algo bellman_ford step 7827 current loss 0.031784, current_train_items 250496.
I0304 19:31:47.868041 22502662377600 run.py:483] Algo bellman_ford step 7828 current loss 0.097783, current_train_items 250528.
I0304 19:31:47.902171 22502662377600 run.py:483] Algo bellman_ford step 7829 current loss 0.087678, current_train_items 250560.
I0304 19:31:47.921846 22502662377600 run.py:483] Algo bellman_ford step 7830 current loss 0.018190, current_train_items 250592.
I0304 19:31:47.937806 22502662377600 run.py:483] Algo bellman_ford step 7831 current loss 0.017643, current_train_items 250624.
I0304 19:31:47.962094 22502662377600 run.py:483] Algo bellman_ford step 7832 current loss 0.050577, current_train_items 250656.
I0304 19:31:47.995170 22502662377600 run.py:483] Algo bellman_ford step 7833 current loss 0.083447, current_train_items 250688.
I0304 19:31:48.027492 22502662377600 run.py:483] Algo bellman_ford step 7834 current loss 0.093554, current_train_items 250720.
I0304 19:31:48.047282 22502662377600 run.py:483] Algo bellman_ford step 7835 current loss 0.029377, current_train_items 250752.
I0304 19:31:48.063690 22502662377600 run.py:483] Algo bellman_ford step 7836 current loss 0.012811, current_train_items 250784.
I0304 19:31:48.088177 22502662377600 run.py:483] Algo bellman_ford step 7837 current loss 0.035384, current_train_items 250816.
I0304 19:31:48.121838 22502662377600 run.py:483] Algo bellman_ford step 7838 current loss 0.096877, current_train_items 250848.
I0304 19:31:48.156997 22502662377600 run.py:483] Algo bellman_ford step 7839 current loss 0.191687, current_train_items 250880.
I0304 19:31:48.176617 22502662377600 run.py:483] Algo bellman_ford step 7840 current loss 0.010522, current_train_items 250912.
I0304 19:31:48.192876 22502662377600 run.py:483] Algo bellman_ford step 7841 current loss 0.026726, current_train_items 250944.
I0304 19:31:48.217489 22502662377600 run.py:483] Algo bellman_ford step 7842 current loss 0.026406, current_train_items 250976.
I0304 19:31:48.251876 22502662377600 run.py:483] Algo bellman_ford step 7843 current loss 0.067814, current_train_items 251008.
I0304 19:31:48.287056 22502662377600 run.py:483] Algo bellman_ford step 7844 current loss 0.085739, current_train_items 251040.
I0304 19:31:48.306618 22502662377600 run.py:483] Algo bellman_ford step 7845 current loss 0.006787, current_train_items 251072.
I0304 19:31:48.322703 22502662377600 run.py:483] Algo bellman_ford step 7846 current loss 0.015907, current_train_items 251104.
I0304 19:31:48.347339 22502662377600 run.py:483] Algo bellman_ford step 7847 current loss 0.053446, current_train_items 251136.
I0304 19:31:48.379992 22502662377600 run.py:483] Algo bellman_ford step 7848 current loss 0.073413, current_train_items 251168.
I0304 19:31:48.414382 22502662377600 run.py:483] Algo bellman_ford step 7849 current loss 0.101324, current_train_items 251200.
I0304 19:31:48.433900 22502662377600 run.py:483] Algo bellman_ford step 7850 current loss 0.007053, current_train_items 251232.
I0304 19:31:48.441916 22502662377600 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0304 19:31:48.442017 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:48.458849 22502662377600 run.py:483] Algo bellman_ford step 7851 current loss 0.060418, current_train_items 251264.
I0304 19:31:48.483419 22502662377600 run.py:483] Algo bellman_ford step 7852 current loss 0.030468, current_train_items 251296.
I0304 19:31:48.516832 22502662377600 run.py:483] Algo bellman_ford step 7853 current loss 0.091339, current_train_items 251328.
I0304 19:31:48.553319 22502662377600 run.py:483] Algo bellman_ford step 7854 current loss 0.109947, current_train_items 251360.
I0304 19:31:48.573079 22502662377600 run.py:483] Algo bellman_ford step 7855 current loss 0.005476, current_train_items 251392.
I0304 19:31:48.589806 22502662377600 run.py:483] Algo bellman_ford step 7856 current loss 0.042307, current_train_items 251424.
I0304 19:31:48.614595 22502662377600 run.py:483] Algo bellman_ford step 7857 current loss 0.151397, current_train_items 251456.
I0304 19:31:48.646663 22502662377600 run.py:483] Algo bellman_ford step 7858 current loss 0.074502, current_train_items 251488.
I0304 19:31:48.681808 22502662377600 run.py:483] Algo bellman_ford step 7859 current loss 0.181243, current_train_items 251520.
I0304 19:31:48.702001 22502662377600 run.py:483] Algo bellman_ford step 7860 current loss 0.016872, current_train_items 251552.
I0304 19:31:48.717984 22502662377600 run.py:483] Algo bellman_ford step 7861 current loss 0.026190, current_train_items 251584.
I0304 19:31:48.741937 22502662377600 run.py:483] Algo bellman_ford step 7862 current loss 0.109678, current_train_items 251616.
I0304 19:31:48.773782 22502662377600 run.py:483] Algo bellman_ford step 7863 current loss 0.222440, current_train_items 251648.
I0304 19:31:48.806359 22502662377600 run.py:483] Algo bellman_ford step 7864 current loss 0.081721, current_train_items 251680.
I0304 19:31:48.826268 22502662377600 run.py:483] Algo bellman_ford step 7865 current loss 0.005578, current_train_items 251712.
I0304 19:31:48.842696 22502662377600 run.py:483] Algo bellman_ford step 7866 current loss 0.026350, current_train_items 251744.
I0304 19:31:48.867563 22502662377600 run.py:483] Algo bellman_ford step 7867 current loss 0.070879, current_train_items 251776.
I0304 19:31:48.899378 22502662377600 run.py:483] Algo bellman_ford step 7868 current loss 0.084194, current_train_items 251808.
I0304 19:31:48.932351 22502662377600 run.py:483] Algo bellman_ford step 7869 current loss 0.076126, current_train_items 251840.
I0304 19:31:48.952253 22502662377600 run.py:483] Algo bellman_ford step 7870 current loss 0.006083, current_train_items 251872.
I0304 19:31:48.968449 22502662377600 run.py:483] Algo bellman_ford step 7871 current loss 0.023068, current_train_items 251904.
I0304 19:31:48.992571 22502662377600 run.py:483] Algo bellman_ford step 7872 current loss 0.058320, current_train_items 251936.
I0304 19:31:49.023960 22502662377600 run.py:483] Algo bellman_ford step 7873 current loss 0.085472, current_train_items 251968.
I0304 19:31:49.057638 22502662377600 run.py:483] Algo bellman_ford step 7874 current loss 0.086097, current_train_items 252000.
I0304 19:31:49.077620 22502662377600 run.py:483] Algo bellman_ford step 7875 current loss 0.031003, current_train_items 252032.
I0304 19:31:49.093485 22502662377600 run.py:483] Algo bellman_ford step 7876 current loss 0.065422, current_train_items 252064.
I0304 19:31:49.117211 22502662377600 run.py:483] Algo bellman_ford step 7877 current loss 0.183536, current_train_items 252096.
I0304 19:31:49.149676 22502662377600 run.py:483] Algo bellman_ford step 7878 current loss 0.150543, current_train_items 252128.
I0304 19:31:49.183642 22502662377600 run.py:483] Algo bellman_ford step 7879 current loss 0.203063, current_train_items 252160.
I0304 19:31:49.203256 22502662377600 run.py:483] Algo bellman_ford step 7880 current loss 0.003786, current_train_items 252192.
I0304 19:31:49.219066 22502662377600 run.py:483] Algo bellman_ford step 7881 current loss 0.016168, current_train_items 252224.
I0304 19:31:49.243122 22502662377600 run.py:483] Algo bellman_ford step 7882 current loss 0.058512, current_train_items 252256.
I0304 19:31:49.275303 22502662377600 run.py:483] Algo bellman_ford step 7883 current loss 0.066700, current_train_items 252288.
I0304 19:31:49.309556 22502662377600 run.py:483] Algo bellman_ford step 7884 current loss 0.167244, current_train_items 252320.
I0304 19:31:49.329991 22502662377600 run.py:483] Algo bellman_ford step 7885 current loss 0.006596, current_train_items 252352.
I0304 19:31:49.346230 22502662377600 run.py:483] Algo bellman_ford step 7886 current loss 0.076158, current_train_items 252384.
I0304 19:31:49.369184 22502662377600 run.py:483] Algo bellman_ford step 7887 current loss 0.025657, current_train_items 252416.
I0304 19:31:49.399282 22502662377600 run.py:483] Algo bellman_ford step 7888 current loss 0.027860, current_train_items 252448.
I0304 19:31:49.432995 22502662377600 run.py:483] Algo bellman_ford step 7889 current loss 0.046267, current_train_items 252480.
I0304 19:31:49.453214 22502662377600 run.py:483] Algo bellman_ford step 7890 current loss 0.007647, current_train_items 252512.
I0304 19:31:49.469353 22502662377600 run.py:483] Algo bellman_ford step 7891 current loss 0.013765, current_train_items 252544.
I0304 19:31:49.493469 22502662377600 run.py:483] Algo bellman_ford step 7892 current loss 0.084542, current_train_items 252576.
I0304 19:31:49.526000 22502662377600 run.py:483] Algo bellman_ford step 7893 current loss 0.103861, current_train_items 252608.
I0304 19:31:49.559379 22502662377600 run.py:483] Algo bellman_ford step 7894 current loss 0.187047, current_train_items 252640.
I0304 19:31:49.579041 22502662377600 run.py:483] Algo bellman_ford step 7895 current loss 0.004467, current_train_items 252672.
I0304 19:31:49.594876 22502662377600 run.py:483] Algo bellman_ford step 7896 current loss 0.026939, current_train_items 252704.
I0304 19:31:49.619754 22502662377600 run.py:483] Algo bellman_ford step 7897 current loss 0.098105, current_train_items 252736.
I0304 19:31:49.652614 22502662377600 run.py:483] Algo bellman_ford step 7898 current loss 0.046428, current_train_items 252768.
I0304 19:31:49.686942 22502662377600 run.py:483] Algo bellman_ford step 7899 current loss 0.091499, current_train_items 252800.
I0304 19:31:49.706829 22502662377600 run.py:483] Algo bellman_ford step 7900 current loss 0.006827, current_train_items 252832.
I0304 19:31:49.714766 22502662377600 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0304 19:31:49.714869 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:31:49.731534 22502662377600 run.py:483] Algo bellman_ford step 7901 current loss 0.031470, current_train_items 252864.
I0304 19:31:49.756701 22502662377600 run.py:483] Algo bellman_ford step 7902 current loss 0.063184, current_train_items 252896.
I0304 19:31:49.789637 22502662377600 run.py:483] Algo bellman_ford step 7903 current loss 0.049870, current_train_items 252928.
I0304 19:31:49.826390 22502662377600 run.py:483] Algo bellman_ford step 7904 current loss 0.122000, current_train_items 252960.
I0304 19:31:49.846384 22502662377600 run.py:483] Algo bellman_ford step 7905 current loss 0.008997, current_train_items 252992.
I0304 19:31:49.862779 22502662377600 run.py:483] Algo bellman_ford step 7906 current loss 0.017037, current_train_items 253024.
I0304 19:31:49.887541 22502662377600 run.py:483] Algo bellman_ford step 7907 current loss 0.058463, current_train_items 253056.
I0304 19:31:49.919486 22502662377600 run.py:483] Algo bellman_ford step 7908 current loss 0.057977, current_train_items 253088.
I0304 19:31:49.950945 22502662377600 run.py:483] Algo bellman_ford step 7909 current loss 0.058699, current_train_items 253120.
I0304 19:31:49.970779 22502662377600 run.py:483] Algo bellman_ford step 7910 current loss 0.020564, current_train_items 253152.
I0304 19:31:49.987497 22502662377600 run.py:483] Algo bellman_ford step 7911 current loss 0.011253, current_train_items 253184.
I0304 19:31:50.012609 22502662377600 run.py:483] Algo bellman_ford step 7912 current loss 0.037635, current_train_items 253216.
I0304 19:31:50.044918 22502662377600 run.py:483] Algo bellman_ford step 7913 current loss 0.067378, current_train_items 253248.
I0304 19:31:50.079415 22502662377600 run.py:483] Algo bellman_ford step 7914 current loss 0.089421, current_train_items 253280.
I0304 19:31:50.099194 22502662377600 run.py:483] Algo bellman_ford step 7915 current loss 0.015366, current_train_items 253312.
I0304 19:31:50.115553 22502662377600 run.py:483] Algo bellman_ford step 7916 current loss 0.038963, current_train_items 253344.
I0304 19:31:50.140293 22502662377600 run.py:483] Algo bellman_ford step 7917 current loss 0.059698, current_train_items 253376.
I0304 19:31:50.172476 22502662377600 run.py:483] Algo bellman_ford step 7918 current loss 0.055881, current_train_items 253408.
I0304 19:31:50.208236 22502662377600 run.py:483] Algo bellman_ford step 7919 current loss 0.081495, current_train_items 253440.
I0304 19:31:50.228183 22502662377600 run.py:483] Algo bellman_ford step 7920 current loss 0.004831, current_train_items 253472.
I0304 19:31:50.244723 22502662377600 run.py:483] Algo bellman_ford step 7921 current loss 0.046987, current_train_items 253504.
I0304 19:31:50.269060 22502662377600 run.py:483] Algo bellman_ford step 7922 current loss 0.171830, current_train_items 253536.
I0304 19:31:50.300430 22502662377600 run.py:483] Algo bellman_ford step 7923 current loss 0.049477, current_train_items 253568.
I0304 19:31:50.334894 22502662377600 run.py:483] Algo bellman_ford step 7924 current loss 0.093660, current_train_items 253600.
I0304 19:31:50.354470 22502662377600 run.py:483] Algo bellman_ford step 7925 current loss 0.013924, current_train_items 253632.
I0304 19:31:50.370711 22502662377600 run.py:483] Algo bellman_ford step 7926 current loss 0.025917, current_train_items 253664.
I0304 19:31:50.394942 22502662377600 run.py:483] Algo bellman_ford step 7927 current loss 0.097780, current_train_items 253696.
I0304 19:31:50.428146 22502662377600 run.py:483] Algo bellman_ford step 7928 current loss 0.111067, current_train_items 253728.
I0304 19:31:50.461481 22502662377600 run.py:483] Algo bellman_ford step 7929 current loss 0.066618, current_train_items 253760.
I0304 19:31:50.481414 22502662377600 run.py:483] Algo bellman_ford step 7930 current loss 0.005369, current_train_items 253792.
I0304 19:31:50.497565 22502662377600 run.py:483] Algo bellman_ford step 7931 current loss 0.036230, current_train_items 253824.
I0304 19:31:50.521161 22502662377600 run.py:483] Algo bellman_ford step 7932 current loss 0.027640, current_train_items 253856.
I0304 19:31:50.553832 22502662377600 run.py:483] Algo bellman_ford step 7933 current loss 0.095207, current_train_items 253888.
I0304 19:31:50.588607 22502662377600 run.py:483] Algo bellman_ford step 7934 current loss 0.081294, current_train_items 253920.
I0304 19:31:50.608103 22502662377600 run.py:483] Algo bellman_ford step 7935 current loss 0.003537, current_train_items 253952.
I0304 19:31:50.623989 22502662377600 run.py:483] Algo bellman_ford step 7936 current loss 0.027947, current_train_items 253984.
I0304 19:31:50.648875 22502662377600 run.py:483] Algo bellman_ford step 7937 current loss 0.048557, current_train_items 254016.
I0304 19:31:50.680644 22502662377600 run.py:483] Algo bellman_ford step 7938 current loss 0.066904, current_train_items 254048.
I0304 19:31:50.715446 22502662377600 run.py:483] Algo bellman_ford step 7939 current loss 0.079000, current_train_items 254080.
I0304 19:31:50.735392 22502662377600 run.py:483] Algo bellman_ford step 7940 current loss 0.004939, current_train_items 254112.
I0304 19:31:50.751044 22502662377600 run.py:483] Algo bellman_ford step 7941 current loss 0.005195, current_train_items 254144.
I0304 19:31:50.776090 22502662377600 run.py:483] Algo bellman_ford step 7942 current loss 0.048216, current_train_items 254176.
I0304 19:31:50.809843 22502662377600 run.py:483] Algo bellman_ford step 7943 current loss 0.082666, current_train_items 254208.
I0304 19:31:50.843341 22502662377600 run.py:483] Algo bellman_ford step 7944 current loss 0.117673, current_train_items 254240.
I0304 19:31:50.863282 22502662377600 run.py:483] Algo bellman_ford step 7945 current loss 0.004647, current_train_items 254272.
I0304 19:31:50.878953 22502662377600 run.py:483] Algo bellman_ford step 7946 current loss 0.018162, current_train_items 254304.
I0304 19:31:50.902201 22502662377600 run.py:483] Algo bellman_ford step 7947 current loss 0.042980, current_train_items 254336.
I0304 19:31:50.932761 22502662377600 run.py:483] Algo bellman_ford step 7948 current loss 0.024369, current_train_items 254368.
I0304 19:31:50.967319 22502662377600 run.py:483] Algo bellman_ford step 7949 current loss 0.086759, current_train_items 254400.
I0304 19:31:50.986909 22502662377600 run.py:483] Algo bellman_ford step 7950 current loss 0.005808, current_train_items 254432.
I0304 19:31:50.995446 22502662377600 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0304 19:31:50.995558 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:51.012228 22502662377600 run.py:483] Algo bellman_ford step 7951 current loss 0.009973, current_train_items 254464.
I0304 19:31:51.037085 22502662377600 run.py:483] Algo bellman_ford step 7952 current loss 0.058263, current_train_items 254496.
I0304 19:31:51.070263 22502662377600 run.py:483] Algo bellman_ford step 7953 current loss 0.058375, current_train_items 254528.
I0304 19:31:51.104336 22502662377600 run.py:483] Algo bellman_ford step 7954 current loss 0.074717, current_train_items 254560.
I0304 19:31:51.124267 22502662377600 run.py:483] Algo bellman_ford step 7955 current loss 0.028818, current_train_items 254592.
I0304 19:31:51.140366 22502662377600 run.py:483] Algo bellman_ford step 7956 current loss 0.049751, current_train_items 254624.
I0304 19:31:51.164767 22502662377600 run.py:483] Algo bellman_ford step 7957 current loss 0.042064, current_train_items 254656.
I0304 19:31:51.196892 22502662377600 run.py:483] Algo bellman_ford step 7958 current loss 0.094711, current_train_items 254688.
I0304 19:31:51.230418 22502662377600 run.py:483] Algo bellman_ford step 7959 current loss 0.082006, current_train_items 254720.
I0304 19:31:51.250613 22502662377600 run.py:483] Algo bellman_ford step 7960 current loss 0.009708, current_train_items 254752.
I0304 19:31:51.267189 22502662377600 run.py:483] Algo bellman_ford step 7961 current loss 0.055409, current_train_items 254784.
I0304 19:31:51.291201 22502662377600 run.py:483] Algo bellman_ford step 7962 current loss 0.061253, current_train_items 254816.
I0304 19:31:51.322782 22502662377600 run.py:483] Algo bellman_ford step 7963 current loss 0.050061, current_train_items 254848.
I0304 19:31:51.357293 22502662377600 run.py:483] Algo bellman_ford step 7964 current loss 0.078201, current_train_items 254880.
I0304 19:31:51.377136 22502662377600 run.py:483] Algo bellman_ford step 7965 current loss 0.008245, current_train_items 254912.
I0304 19:31:51.393583 22502662377600 run.py:483] Algo bellman_ford step 7966 current loss 0.015997, current_train_items 254944.
I0304 19:31:51.418923 22502662377600 run.py:483] Algo bellman_ford step 7967 current loss 0.090129, current_train_items 254976.
I0304 19:31:51.450679 22502662377600 run.py:483] Algo bellman_ford step 7968 current loss 0.046697, current_train_items 255008.
I0304 19:31:51.483345 22502662377600 run.py:483] Algo bellman_ford step 7969 current loss 0.055877, current_train_items 255040.
I0304 19:31:51.503214 22502662377600 run.py:483] Algo bellman_ford step 7970 current loss 0.008735, current_train_items 255072.
I0304 19:31:51.519533 22502662377600 run.py:483] Algo bellman_ford step 7971 current loss 0.023012, current_train_items 255104.
I0304 19:31:51.544234 22502662377600 run.py:483] Algo bellman_ford step 7972 current loss 0.049226, current_train_items 255136.
I0304 19:31:51.575746 22502662377600 run.py:483] Algo bellman_ford step 7973 current loss 0.057930, current_train_items 255168.
I0304 19:31:51.611272 22502662377600 run.py:483] Algo bellman_ford step 7974 current loss 0.080234, current_train_items 255200.
I0304 19:31:51.631425 22502662377600 run.py:483] Algo bellman_ford step 7975 current loss 0.011473, current_train_items 255232.
I0304 19:31:51.647918 22502662377600 run.py:483] Algo bellman_ford step 7976 current loss 0.051881, current_train_items 255264.
I0304 19:31:51.672679 22502662377600 run.py:483] Algo bellman_ford step 7977 current loss 0.044078, current_train_items 255296.
I0304 19:31:51.704549 22502662377600 run.py:483] Algo bellman_ford step 7978 current loss 0.070709, current_train_items 255328.
I0304 19:31:51.736729 22502662377600 run.py:483] Algo bellman_ford step 7979 current loss 0.063644, current_train_items 255360.
I0304 19:31:51.756402 22502662377600 run.py:483] Algo bellman_ford step 7980 current loss 0.004689, current_train_items 255392.
I0304 19:31:51.772405 22502662377600 run.py:483] Algo bellman_ford step 7981 current loss 0.035829, current_train_items 255424.
I0304 19:31:51.795886 22502662377600 run.py:483] Algo bellman_ford step 7982 current loss 0.022049, current_train_items 255456.
I0304 19:31:51.827379 22502662377600 run.py:483] Algo bellman_ford step 7983 current loss 0.039056, current_train_items 255488.
I0304 19:31:51.862030 22502662377600 run.py:483] Algo bellman_ford step 7984 current loss 0.079465, current_train_items 255520.
I0304 19:31:51.881971 22502662377600 run.py:483] Algo bellman_ford step 7985 current loss 0.005845, current_train_items 255552.
I0304 19:31:51.898524 22502662377600 run.py:483] Algo bellman_ford step 7986 current loss 0.041942, current_train_items 255584.
I0304 19:31:51.921897 22502662377600 run.py:483] Algo bellman_ford step 7987 current loss 0.127853, current_train_items 255616.
I0304 19:31:51.954511 22502662377600 run.py:483] Algo bellman_ford step 7988 current loss 0.079412, current_train_items 255648.
I0304 19:31:51.987972 22502662377600 run.py:483] Algo bellman_ford step 7989 current loss 0.081261, current_train_items 255680.
I0304 19:31:52.008188 22502662377600 run.py:483] Algo bellman_ford step 7990 current loss 0.002695, current_train_items 255712.
I0304 19:31:52.024625 22502662377600 run.py:483] Algo bellman_ford step 7991 current loss 0.055043, current_train_items 255744.
I0304 19:31:52.049392 22502662377600 run.py:483] Algo bellman_ford step 7992 current loss 0.070367, current_train_items 255776.
I0304 19:31:52.083082 22502662377600 run.py:483] Algo bellman_ford step 7993 current loss 0.097751, current_train_items 255808.
I0304 19:31:52.118195 22502662377600 run.py:483] Algo bellman_ford step 7994 current loss 0.102475, current_train_items 255840.
I0304 19:31:52.138101 22502662377600 run.py:483] Algo bellman_ford step 7995 current loss 0.006521, current_train_items 255872.
I0304 19:31:52.153797 22502662377600 run.py:483] Algo bellman_ford step 7996 current loss 0.011285, current_train_items 255904.
I0304 19:31:52.177959 22502662377600 run.py:483] Algo bellman_ford step 7997 current loss 0.063474, current_train_items 255936.
I0304 19:31:52.210107 22502662377600 run.py:483] Algo bellman_ford step 7998 current loss 0.037518, current_train_items 255968.
I0304 19:31:52.242236 22502662377600 run.py:483] Algo bellman_ford step 7999 current loss 0.097052, current_train_items 256000.
I0304 19:31:52.262317 22502662377600 run.py:483] Algo bellman_ford step 8000 current loss 0.007842, current_train_items 256032.
I0304 19:31:52.270195 22502662377600 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0304 19:31:52.270301 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:52.287421 22502662377600 run.py:483] Algo bellman_ford step 8001 current loss 0.044463, current_train_items 256064.
I0304 19:31:52.311725 22502662377600 run.py:483] Algo bellman_ford step 8002 current loss 0.075459, current_train_items 256096.
I0304 19:31:52.345133 22502662377600 run.py:483] Algo bellman_ford step 8003 current loss 0.129739, current_train_items 256128.
I0304 19:31:52.378095 22502662377600 run.py:483] Algo bellman_ford step 8004 current loss 0.071719, current_train_items 256160.
I0304 19:31:52.398481 22502662377600 run.py:483] Algo bellman_ford step 8005 current loss 0.011033, current_train_items 256192.
I0304 19:31:52.414429 22502662377600 run.py:483] Algo bellman_ford step 8006 current loss 0.009344, current_train_items 256224.
I0304 19:31:52.438853 22502662377600 run.py:483] Algo bellman_ford step 8007 current loss 0.058956, current_train_items 256256.
I0304 19:31:52.472210 22502662377600 run.py:483] Algo bellman_ford step 8008 current loss 0.096323, current_train_items 256288.
I0304 19:31:52.506519 22502662377600 run.py:483] Algo bellman_ford step 8009 current loss 0.090070, current_train_items 256320.
I0304 19:31:52.526520 22502662377600 run.py:483] Algo bellman_ford step 8010 current loss 0.006829, current_train_items 256352.
I0304 19:31:52.542527 22502662377600 run.py:483] Algo bellman_ford step 8011 current loss 0.077913, current_train_items 256384.
I0304 19:31:52.567592 22502662377600 run.py:483] Algo bellman_ford step 8012 current loss 0.059546, current_train_items 256416.
I0304 19:31:52.599166 22502662377600 run.py:483] Algo bellman_ford step 8013 current loss 0.076217, current_train_items 256448.
I0304 19:31:52.631227 22502662377600 run.py:483] Algo bellman_ford step 8014 current loss 0.171218, current_train_items 256480.
I0304 19:31:52.650897 22502662377600 run.py:483] Algo bellman_ford step 8015 current loss 0.028786, current_train_items 256512.
I0304 19:31:52.667584 22502662377600 run.py:483] Algo bellman_ford step 8016 current loss 0.047814, current_train_items 256544.
I0304 19:31:52.691377 22502662377600 run.py:483] Algo bellman_ford step 8017 current loss 0.027043, current_train_items 256576.
I0304 19:31:52.723149 22502662377600 run.py:483] Algo bellman_ford step 8018 current loss 0.123459, current_train_items 256608.
I0304 19:31:52.755852 22502662377600 run.py:483] Algo bellman_ford step 8019 current loss 0.061800, current_train_items 256640.
I0304 19:31:52.775142 22502662377600 run.py:483] Algo bellman_ford step 8020 current loss 0.005683, current_train_items 256672.
I0304 19:31:52.791510 22502662377600 run.py:483] Algo bellman_ford step 8021 current loss 0.014557, current_train_items 256704.
I0304 19:31:52.815041 22502662377600 run.py:483] Algo bellman_ford step 8022 current loss 0.083268, current_train_items 256736.
I0304 19:31:52.846292 22502662377600 run.py:483] Algo bellman_ford step 8023 current loss 0.097044, current_train_items 256768.
I0304 19:31:52.881892 22502662377600 run.py:483] Algo bellman_ford step 8024 current loss 0.083571, current_train_items 256800.
I0304 19:31:52.901379 22502662377600 run.py:483] Algo bellman_ford step 8025 current loss 0.043565, current_train_items 256832.
I0304 19:31:52.917914 22502662377600 run.py:483] Algo bellman_ford step 8026 current loss 0.017071, current_train_items 256864.
I0304 19:31:52.942413 22502662377600 run.py:483] Algo bellman_ford step 8027 current loss 0.048322, current_train_items 256896.
I0304 19:31:52.973715 22502662377600 run.py:483] Algo bellman_ford step 8028 current loss 0.073975, current_train_items 256928.
I0304 19:31:53.007791 22502662377600 run.py:483] Algo bellman_ford step 8029 current loss 0.116959, current_train_items 256960.
I0304 19:31:53.027436 22502662377600 run.py:483] Algo bellman_ford step 8030 current loss 0.014688, current_train_items 256992.
I0304 19:31:53.043454 22502662377600 run.py:483] Algo bellman_ford step 8031 current loss 0.040494, current_train_items 257024.
I0304 19:31:53.068216 22502662377600 run.py:483] Algo bellman_ford step 8032 current loss 0.066150, current_train_items 257056.
I0304 19:31:53.100598 22502662377600 run.py:483] Algo bellman_ford step 8033 current loss 0.093020, current_train_items 257088.
I0304 19:31:53.135220 22502662377600 run.py:483] Algo bellman_ford step 8034 current loss 0.078741, current_train_items 257120.
I0304 19:31:53.155007 22502662377600 run.py:483] Algo bellman_ford step 8035 current loss 0.008475, current_train_items 257152.
I0304 19:31:53.171492 22502662377600 run.py:483] Algo bellman_ford step 8036 current loss 0.052023, current_train_items 257184.
I0304 19:31:53.195919 22502662377600 run.py:483] Algo bellman_ford step 8037 current loss 0.053947, current_train_items 257216.
I0304 19:31:53.227745 22502662377600 run.py:483] Algo bellman_ford step 8038 current loss 0.070879, current_train_items 257248.
I0304 19:31:53.262966 22502662377600 run.py:483] Algo bellman_ford step 8039 current loss 0.109114, current_train_items 257280.
I0304 19:31:53.282895 22502662377600 run.py:483] Algo bellman_ford step 8040 current loss 0.004561, current_train_items 257312.
I0304 19:31:53.299291 22502662377600 run.py:483] Algo bellman_ford step 8041 current loss 0.029472, current_train_items 257344.
I0304 19:31:53.323595 22502662377600 run.py:483] Algo bellman_ford step 8042 current loss 0.109330, current_train_items 257376.
I0304 19:31:53.355856 22502662377600 run.py:483] Algo bellman_ford step 8043 current loss 0.066348, current_train_items 257408.
I0304 19:31:53.390141 22502662377600 run.py:483] Algo bellman_ford step 8044 current loss 0.083461, current_train_items 257440.
I0304 19:31:53.410170 22502662377600 run.py:483] Algo bellman_ford step 8045 current loss 0.012340, current_train_items 257472.
I0304 19:31:53.426801 22502662377600 run.py:483] Algo bellman_ford step 8046 current loss 0.063436, current_train_items 257504.
I0304 19:31:53.450843 22502662377600 run.py:483] Algo bellman_ford step 8047 current loss 0.065799, current_train_items 257536.
I0304 19:31:53.483073 22502662377600 run.py:483] Algo bellman_ford step 8048 current loss 0.125064, current_train_items 257568.
I0304 19:31:53.515611 22502662377600 run.py:483] Algo bellman_ford step 8049 current loss 0.076205, current_train_items 257600.
I0304 19:31:53.535365 22502662377600 run.py:483] Algo bellman_ford step 8050 current loss 0.004204, current_train_items 257632.
I0304 19:31:53.543632 22502662377600 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0304 19:31:53.543735 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:53.560682 22502662377600 run.py:483] Algo bellman_ford step 8051 current loss 0.030023, current_train_items 257664.
I0304 19:31:53.585025 22502662377600 run.py:483] Algo bellman_ford step 8052 current loss 0.042038, current_train_items 257696.
I0304 19:31:53.616927 22502662377600 run.py:483] Algo bellman_ford step 8053 current loss 0.068505, current_train_items 257728.
I0304 19:31:53.652284 22502662377600 run.py:483] Algo bellman_ford step 8054 current loss 0.169073, current_train_items 257760.
I0304 19:31:53.672165 22502662377600 run.py:483] Algo bellman_ford step 8055 current loss 0.015720, current_train_items 257792.
I0304 19:31:53.687305 22502662377600 run.py:483] Algo bellman_ford step 8056 current loss 0.005564, current_train_items 257824.
I0304 19:31:53.710456 22502662377600 run.py:483] Algo bellman_ford step 8057 current loss 0.052722, current_train_items 257856.
I0304 19:31:53.743632 22502662377600 run.py:483] Algo bellman_ford step 8058 current loss 0.064539, current_train_items 257888.
I0304 19:31:53.779320 22502662377600 run.py:483] Algo bellman_ford step 8059 current loss 0.075826, current_train_items 257920.
I0304 19:31:53.799199 22502662377600 run.py:483] Algo bellman_ford step 8060 current loss 0.005156, current_train_items 257952.
I0304 19:31:53.815361 22502662377600 run.py:483] Algo bellman_ford step 8061 current loss 0.021210, current_train_items 257984.
I0304 19:31:53.840362 22502662377600 run.py:483] Algo bellman_ford step 8062 current loss 0.048960, current_train_items 258016.
I0304 19:31:53.872404 22502662377600 run.py:483] Algo bellman_ford step 8063 current loss 0.071804, current_train_items 258048.
I0304 19:31:53.905321 22502662377600 run.py:483] Algo bellman_ford step 8064 current loss 0.086117, current_train_items 258080.
I0304 19:31:53.924815 22502662377600 run.py:483] Algo bellman_ford step 8065 current loss 0.004432, current_train_items 258112.
I0304 19:31:53.940803 22502662377600 run.py:483] Algo bellman_ford step 8066 current loss 0.029675, current_train_items 258144.
I0304 19:31:53.964883 22502662377600 run.py:483] Algo bellman_ford step 8067 current loss 0.031425, current_train_items 258176.
I0304 19:31:53.996605 22502662377600 run.py:483] Algo bellman_ford step 8068 current loss 0.043546, current_train_items 258208.
I0304 19:31:54.029094 22502662377600 run.py:483] Algo bellman_ford step 8069 current loss 0.080043, current_train_items 258240.
I0304 19:31:54.048920 22502662377600 run.py:483] Algo bellman_ford step 8070 current loss 0.032394, current_train_items 258272.
I0304 19:31:54.065855 22502662377600 run.py:483] Algo bellman_ford step 8071 current loss 0.050645, current_train_items 258304.
I0304 19:31:54.089614 22502662377600 run.py:483] Algo bellman_ford step 8072 current loss 0.043825, current_train_items 258336.
I0304 19:31:54.121827 22502662377600 run.py:483] Algo bellman_ford step 8073 current loss 0.052106, current_train_items 258368.
I0304 19:31:54.156836 22502662377600 run.py:483] Algo bellman_ford step 8074 current loss 0.075657, current_train_items 258400.
I0304 19:31:54.176759 22502662377600 run.py:483] Algo bellman_ford step 8075 current loss 0.008153, current_train_items 258432.
I0304 19:31:54.193002 22502662377600 run.py:483] Algo bellman_ford step 8076 current loss 0.040681, current_train_items 258464.
I0304 19:31:54.216549 22502662377600 run.py:483] Algo bellman_ford step 8077 current loss 0.029483, current_train_items 258496.
I0304 19:31:54.247964 22502662377600 run.py:483] Algo bellman_ford step 8078 current loss 0.126726, current_train_items 258528.
I0304 19:31:54.279999 22502662377600 run.py:483] Algo bellman_ford step 8079 current loss 0.064160, current_train_items 258560.
I0304 19:31:54.299581 22502662377600 run.py:483] Algo bellman_ford step 8080 current loss 0.013075, current_train_items 258592.
I0304 19:31:54.315792 22502662377600 run.py:483] Algo bellman_ford step 8081 current loss 0.026571, current_train_items 258624.
I0304 19:31:54.339652 22502662377600 run.py:483] Algo bellman_ford step 8082 current loss 0.131658, current_train_items 258656.
I0304 19:31:54.371657 22502662377600 run.py:483] Algo bellman_ford step 8083 current loss 0.080948, current_train_items 258688.
I0304 19:31:54.408336 22502662377600 run.py:483] Algo bellman_ford step 8084 current loss 0.139129, current_train_items 258720.
I0304 19:31:54.428360 22502662377600 run.py:483] Algo bellman_ford step 8085 current loss 0.003483, current_train_items 258752.
I0304 19:31:54.445010 22502662377600 run.py:483] Algo bellman_ford step 8086 current loss 0.031958, current_train_items 258784.
I0304 19:31:54.468238 22502662377600 run.py:483] Algo bellman_ford step 8087 current loss 0.090164, current_train_items 258816.
I0304 19:31:54.501680 22502662377600 run.py:483] Algo bellman_ford step 8088 current loss 0.277214, current_train_items 258848.
I0304 19:31:54.534366 22502662377600 run.py:483] Algo bellman_ford step 8089 current loss 0.184512, current_train_items 258880.
I0304 19:31:54.554247 22502662377600 run.py:483] Algo bellman_ford step 8090 current loss 0.008326, current_train_items 258912.
I0304 19:31:54.570077 22502662377600 run.py:483] Algo bellman_ford step 8091 current loss 0.020147, current_train_items 258944.
I0304 19:31:54.594283 22502662377600 run.py:483] Algo bellman_ford step 8092 current loss 0.020193, current_train_items 258976.
I0304 19:31:54.625430 22502662377600 run.py:483] Algo bellman_ford step 8093 current loss 0.049612, current_train_items 259008.
I0304 19:31:54.659615 22502662377600 run.py:483] Algo bellman_ford step 8094 current loss 0.088421, current_train_items 259040.
I0304 19:31:54.679228 22502662377600 run.py:483] Algo bellman_ford step 8095 current loss 0.018611, current_train_items 259072.
I0304 19:31:54.695134 22502662377600 run.py:483] Algo bellman_ford step 8096 current loss 0.030395, current_train_items 259104.
I0304 19:31:54.720146 22502662377600 run.py:483] Algo bellman_ford step 8097 current loss 0.042144, current_train_items 259136.
I0304 19:31:54.752946 22502662377600 run.py:483] Algo bellman_ford step 8098 current loss 0.056443, current_train_items 259168.
I0304 19:31:54.786608 22502662377600 run.py:483] Algo bellman_ford step 8099 current loss 0.065876, current_train_items 259200.
I0304 19:31:54.806209 22502662377600 run.py:483] Algo bellman_ford step 8100 current loss 0.004004, current_train_items 259232.
I0304 19:31:54.814400 22502662377600 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.9580078125, 'score': 0.9580078125, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0304 19:31:54.814515 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.958, val scores are: bellman_ford: 0.958
I0304 19:31:54.831376 22502662377600 run.py:483] Algo bellman_ford step 8101 current loss 0.020268, current_train_items 259264.
I0304 19:31:54.857235 22502662377600 run.py:483] Algo bellman_ford step 8102 current loss 0.130654, current_train_items 259296.
I0304 19:31:54.889424 22502662377600 run.py:483] Algo bellman_ford step 8103 current loss 0.057991, current_train_items 259328.
I0304 19:31:54.922999 22502662377600 run.py:483] Algo bellman_ford step 8104 current loss 0.076317, current_train_items 259360.
I0304 19:31:54.942818 22502662377600 run.py:483] Algo bellman_ford step 8105 current loss 0.005870, current_train_items 259392.
I0304 19:31:54.959066 22502662377600 run.py:483] Algo bellman_ford step 8106 current loss 0.046860, current_train_items 259424.
I0304 19:31:54.982916 22502662377600 run.py:483] Algo bellman_ford step 8107 current loss 0.152224, current_train_items 259456.
I0304 19:31:55.014311 22502662377600 run.py:483] Algo bellman_ford step 8108 current loss 0.176075, current_train_items 259488.
I0304 19:31:55.049029 22502662377600 run.py:483] Algo bellman_ford step 8109 current loss 0.095787, current_train_items 259520.
I0304 19:31:55.068645 22502662377600 run.py:483] Algo bellman_ford step 8110 current loss 0.013262, current_train_items 259552.
I0304 19:31:55.084254 22502662377600 run.py:483] Algo bellman_ford step 8111 current loss 0.034915, current_train_items 259584.
I0304 19:31:55.107789 22502662377600 run.py:483] Algo bellman_ford step 8112 current loss 0.119427, current_train_items 259616.
I0304 19:31:55.138063 22502662377600 run.py:483] Algo bellman_ford step 8113 current loss 0.096008, current_train_items 259648.
I0304 19:31:55.173636 22502662377600 run.py:483] Algo bellman_ford step 8114 current loss 0.152242, current_train_items 259680.
I0304 19:31:55.193290 22502662377600 run.py:483] Algo bellman_ford step 8115 current loss 0.008231, current_train_items 259712.
I0304 19:31:55.209424 22502662377600 run.py:483] Algo bellman_ford step 8116 current loss 0.013547, current_train_items 259744.
I0304 19:31:55.233849 22502662377600 run.py:483] Algo bellman_ford step 8117 current loss 0.039365, current_train_items 259776.
I0304 19:31:55.266063 22502662377600 run.py:483] Algo bellman_ford step 8118 current loss 0.040728, current_train_items 259808.
I0304 19:31:55.299294 22502662377600 run.py:483] Algo bellman_ford step 8119 current loss 0.074440, current_train_items 259840.
I0304 19:31:55.318656 22502662377600 run.py:483] Algo bellman_ford step 8120 current loss 0.006082, current_train_items 259872.
I0304 19:31:55.334916 22502662377600 run.py:483] Algo bellman_ford step 8121 current loss 0.011174, current_train_items 259904.
I0304 19:31:55.359080 22502662377600 run.py:483] Algo bellman_ford step 8122 current loss 0.060236, current_train_items 259936.
I0304 19:31:55.392399 22502662377600 run.py:483] Algo bellman_ford step 8123 current loss 0.069656, current_train_items 259968.
I0304 19:31:55.424239 22502662377600 run.py:483] Algo bellman_ford step 8124 current loss 0.050903, current_train_items 260000.
I0304 19:31:55.443585 22502662377600 run.py:483] Algo bellman_ford step 8125 current loss 0.003273, current_train_items 260032.
I0304 19:31:55.460203 22502662377600 run.py:483] Algo bellman_ford step 8126 current loss 0.036554, current_train_items 260064.
I0304 19:31:55.484845 22502662377600 run.py:483] Algo bellman_ford step 8127 current loss 0.043974, current_train_items 260096.
I0304 19:31:55.517294 22502662377600 run.py:483] Algo bellman_ford step 8128 current loss 0.100928, current_train_items 260128.
I0304 19:31:55.553497 22502662377600 run.py:483] Algo bellman_ford step 8129 current loss 0.133486, current_train_items 260160.
I0304 19:31:55.573187 22502662377600 run.py:483] Algo bellman_ford step 8130 current loss 0.016862, current_train_items 260192.
I0304 19:31:55.588890 22502662377600 run.py:483] Algo bellman_ford step 8131 current loss 0.011580, current_train_items 260224.
I0304 19:31:55.613082 22502662377600 run.py:483] Algo bellman_ford step 8132 current loss 0.052683, current_train_items 260256.
I0304 19:31:55.644819 22502662377600 run.py:483] Algo bellman_ford step 8133 current loss 0.064543, current_train_items 260288.
I0304 19:31:55.679238 22502662377600 run.py:483] Algo bellman_ford step 8134 current loss 0.073549, current_train_items 260320.
I0304 19:31:55.698723 22502662377600 run.py:483] Algo bellman_ford step 8135 current loss 0.002794, current_train_items 260352.
I0304 19:31:55.714575 22502662377600 run.py:483] Algo bellman_ford step 8136 current loss 0.018633, current_train_items 260384.
I0304 19:31:55.738222 22502662377600 run.py:483] Algo bellman_ford step 8137 current loss 0.057228, current_train_items 260416.
I0304 19:31:55.770712 22502662377600 run.py:483] Algo bellman_ford step 8138 current loss 0.042330, current_train_items 260448.
I0304 19:31:55.806518 22502662377600 run.py:483] Algo bellman_ford step 8139 current loss 0.086574, current_train_items 260480.
I0304 19:31:55.825899 22502662377600 run.py:483] Algo bellman_ford step 8140 current loss 0.003845, current_train_items 260512.
I0304 19:31:55.841833 22502662377600 run.py:483] Algo bellman_ford step 8141 current loss 0.023094, current_train_items 260544.
I0304 19:31:55.866942 22502662377600 run.py:483] Algo bellman_ford step 8142 current loss 0.087597, current_train_items 260576.
I0304 19:31:55.899652 22502662377600 run.py:483] Algo bellman_ford step 8143 current loss 0.085721, current_train_items 260608.
I0304 19:31:55.934203 22502662377600 run.py:483] Algo bellman_ford step 8144 current loss 0.082844, current_train_items 260640.
I0304 19:31:55.953591 22502662377600 run.py:483] Algo bellman_ford step 8145 current loss 0.065387, current_train_items 260672.
I0304 19:31:55.969904 22502662377600 run.py:483] Algo bellman_ford step 8146 current loss 0.020983, current_train_items 260704.
I0304 19:31:55.993762 22502662377600 run.py:483] Algo bellman_ford step 8147 current loss 0.149211, current_train_items 260736.
I0304 19:31:56.025699 22502662377600 run.py:483] Algo bellman_ford step 8148 current loss 0.164812, current_train_items 260768.
I0304 19:31:56.058554 22502662377600 run.py:483] Algo bellman_ford step 8149 current loss 0.098070, current_train_items 260800.
I0304 19:31:56.078182 22502662377600 run.py:483] Algo bellman_ford step 8150 current loss 0.006058, current_train_items 260832.
I0304 19:31:56.086283 22502662377600 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0304 19:31:56.086385 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:31:56.102921 22502662377600 run.py:483] Algo bellman_ford step 8151 current loss 0.018789, current_train_items 260864.
I0304 19:31:56.127553 22502662377600 run.py:483] Algo bellman_ford step 8152 current loss 0.097369, current_train_items 260896.
I0304 19:31:56.158206 22502662377600 run.py:483] Algo bellman_ford step 8153 current loss 0.038638, current_train_items 260928.
I0304 19:31:56.193654 22502662377600 run.py:483] Algo bellman_ford step 8154 current loss 0.159849, current_train_items 260960.
I0304 19:31:56.213532 22502662377600 run.py:483] Algo bellman_ford step 8155 current loss 0.006549, current_train_items 260992.
I0304 19:31:56.228897 22502662377600 run.py:483] Algo bellman_ford step 8156 current loss 0.032695, current_train_items 261024.
I0304 19:31:56.253094 22502662377600 run.py:483] Algo bellman_ford step 8157 current loss 0.030872, current_train_items 261056.
I0304 19:31:56.284652 22502662377600 run.py:483] Algo bellman_ford step 8158 current loss 0.072971, current_train_items 261088.
I0304 19:31:56.317704 22502662377600 run.py:483] Algo bellman_ford step 8159 current loss 0.118059, current_train_items 261120.
I0304 19:31:56.337420 22502662377600 run.py:483] Algo bellman_ford step 8160 current loss 0.004817, current_train_items 261152.
I0304 19:31:56.353721 22502662377600 run.py:483] Algo bellman_ford step 8161 current loss 0.020523, current_train_items 261184.
I0304 19:31:56.377411 22502662377600 run.py:483] Algo bellman_ford step 8162 current loss 0.038662, current_train_items 261216.
I0304 19:31:56.409658 22502662377600 run.py:483] Algo bellman_ford step 8163 current loss 0.046711, current_train_items 261248.
I0304 19:31:56.440139 22502662377600 run.py:483] Algo bellman_ford step 8164 current loss 0.065371, current_train_items 261280.
I0304 19:31:56.459502 22502662377600 run.py:483] Algo bellman_ford step 8165 current loss 0.005064, current_train_items 261312.
I0304 19:31:56.475644 22502662377600 run.py:483] Algo bellman_ford step 8166 current loss 0.072649, current_train_items 261344.
I0304 19:31:56.499642 22502662377600 run.py:483] Algo bellman_ford step 8167 current loss 0.070091, current_train_items 261376.
I0304 19:31:56.531176 22502662377600 run.py:483] Algo bellman_ford step 8168 current loss 0.036167, current_train_items 261408.
I0304 19:31:56.565837 22502662377600 run.py:483] Algo bellman_ford step 8169 current loss 0.077934, current_train_items 261440.
I0304 19:31:56.585435 22502662377600 run.py:483] Algo bellman_ford step 8170 current loss 0.005381, current_train_items 261472.
I0304 19:31:56.601605 22502662377600 run.py:483] Algo bellman_ford step 8171 current loss 0.013971, current_train_items 261504.
I0304 19:31:56.625595 22502662377600 run.py:483] Algo bellman_ford step 8172 current loss 0.080978, current_train_items 261536.
I0304 19:31:56.657763 22502662377600 run.py:483] Algo bellman_ford step 8173 current loss 0.046626, current_train_items 261568.
I0304 19:31:56.690579 22502662377600 run.py:483] Algo bellman_ford step 8174 current loss 0.069410, current_train_items 261600.
I0304 19:31:56.710370 22502662377600 run.py:483] Algo bellman_ford step 8175 current loss 0.007674, current_train_items 261632.
I0304 19:31:56.726940 22502662377600 run.py:483] Algo bellman_ford step 8176 current loss 0.042145, current_train_items 261664.
I0304 19:31:56.750451 22502662377600 run.py:483] Algo bellman_ford step 8177 current loss 0.050133, current_train_items 261696.
I0304 19:31:56.783149 22502662377600 run.py:483] Algo bellman_ford step 8178 current loss 0.076649, current_train_items 261728.
I0304 19:31:56.814328 22502662377600 run.py:483] Algo bellman_ford step 8179 current loss 0.059070, current_train_items 261760.
I0304 19:31:56.833695 22502662377600 run.py:483] Algo bellman_ford step 8180 current loss 0.004310, current_train_items 261792.
I0304 19:31:56.849730 22502662377600 run.py:483] Algo bellman_ford step 8181 current loss 0.020857, current_train_items 261824.
I0304 19:31:56.873416 22502662377600 run.py:483] Algo bellman_ford step 8182 current loss 0.041440, current_train_items 261856.
I0304 19:31:56.905632 22502662377600 run.py:483] Algo bellman_ford step 8183 current loss 0.075372, current_train_items 261888.
I0304 19:31:56.941613 22502662377600 run.py:483] Algo bellman_ford step 8184 current loss 0.089781, current_train_items 261920.
I0304 19:31:56.961233 22502662377600 run.py:483] Algo bellman_ford step 8185 current loss 0.003867, current_train_items 261952.
I0304 19:31:56.977335 22502662377600 run.py:483] Algo bellman_ford step 8186 current loss 0.012589, current_train_items 261984.
I0304 19:31:57.001174 22502662377600 run.py:483] Algo bellman_ford step 8187 current loss 0.123916, current_train_items 262016.
I0304 19:31:57.033075 22502662377600 run.py:483] Algo bellman_ford step 8188 current loss 0.097831, current_train_items 262048.
I0304 19:31:57.066660 22502662377600 run.py:483] Algo bellman_ford step 8189 current loss 0.047320, current_train_items 262080.
I0304 19:31:57.086563 22502662377600 run.py:483] Algo bellman_ford step 8190 current loss 0.013056, current_train_items 262112.
I0304 19:31:57.102854 22502662377600 run.py:483] Algo bellman_ford step 8191 current loss 0.010340, current_train_items 262144.
I0304 19:31:57.126606 22502662377600 run.py:483] Algo bellman_ford step 8192 current loss 0.052311, current_train_items 262176.
I0304 19:31:57.158207 22502662377600 run.py:483] Algo bellman_ford step 8193 current loss 0.073991, current_train_items 262208.
I0304 19:31:57.191639 22502662377600 run.py:483] Algo bellman_ford step 8194 current loss 0.090246, current_train_items 262240.
I0304 19:31:57.210971 22502662377600 run.py:483] Algo bellman_ford step 8195 current loss 0.003258, current_train_items 262272.
I0304 19:31:57.227051 22502662377600 run.py:483] Algo bellman_ford step 8196 current loss 0.012122, current_train_items 262304.
I0304 19:31:57.250319 22502662377600 run.py:483] Algo bellman_ford step 8197 current loss 0.043067, current_train_items 262336.
I0304 19:31:57.282167 22502662377600 run.py:483] Algo bellman_ford step 8198 current loss 0.052872, current_train_items 262368.
I0304 19:31:57.315419 22502662377600 run.py:483] Algo bellman_ford step 8199 current loss 0.077182, current_train_items 262400.
I0304 19:31:57.335308 22502662377600 run.py:483] Algo bellman_ford step 8200 current loss 0.005019, current_train_items 262432.
I0304 19:31:57.343289 22502662377600 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0304 19:31:57.343390 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:57.359778 22502662377600 run.py:483] Algo bellman_ford step 8201 current loss 0.006293, current_train_items 262464.
I0304 19:31:57.384592 22502662377600 run.py:483] Algo bellman_ford step 8202 current loss 0.033978, current_train_items 262496.
I0304 19:31:57.416636 22502662377600 run.py:483] Algo bellman_ford step 8203 current loss 0.048886, current_train_items 262528.
I0304 19:31:57.452081 22502662377600 run.py:483] Algo bellman_ford step 8204 current loss 0.055407, current_train_items 262560.
I0304 19:31:57.472035 22502662377600 run.py:483] Algo bellman_ford step 8205 current loss 0.003627, current_train_items 262592.
I0304 19:31:57.488505 22502662377600 run.py:483] Algo bellman_ford step 8206 current loss 0.019022, current_train_items 262624.
I0304 19:31:57.513643 22502662377600 run.py:483] Algo bellman_ford step 8207 current loss 0.045863, current_train_items 262656.
I0304 19:31:57.545631 22502662377600 run.py:483] Algo bellman_ford step 8208 current loss 0.045500, current_train_items 262688.
I0304 19:31:57.576945 22502662377600 run.py:483] Algo bellman_ford step 8209 current loss 0.034938, current_train_items 262720.
I0304 19:31:57.596665 22502662377600 run.py:483] Algo bellman_ford step 8210 current loss 0.004795, current_train_items 262752.
I0304 19:31:57.612792 22502662377600 run.py:483] Algo bellman_ford step 8211 current loss 0.030322, current_train_items 262784.
I0304 19:31:57.636755 22502662377600 run.py:483] Algo bellman_ford step 8212 current loss 0.088567, current_train_items 262816.
I0304 19:31:57.668671 22502662377600 run.py:483] Algo bellman_ford step 8213 current loss 0.081789, current_train_items 262848.
I0304 19:31:57.702516 22502662377600 run.py:483] Algo bellman_ford step 8214 current loss 0.112709, current_train_items 262880.
I0304 19:31:57.722397 22502662377600 run.py:483] Algo bellman_ford step 8215 current loss 0.004729, current_train_items 262912.
I0304 19:31:57.738060 22502662377600 run.py:483] Algo bellman_ford step 8216 current loss 0.018880, current_train_items 262944.
I0304 19:31:57.761380 22502662377600 run.py:483] Algo bellman_ford step 8217 current loss 0.043838, current_train_items 262976.
I0304 19:31:57.794732 22502662377600 run.py:483] Algo bellman_ford step 8218 current loss 0.083154, current_train_items 263008.
I0304 19:31:57.829245 22502662377600 run.py:483] Algo bellman_ford step 8219 current loss 0.053506, current_train_items 263040.
I0304 19:31:57.848598 22502662377600 run.py:483] Algo bellman_ford step 8220 current loss 0.003245, current_train_items 263072.
I0304 19:31:57.864426 22502662377600 run.py:483] Algo bellman_ford step 8221 current loss 0.040016, current_train_items 263104.
I0304 19:31:57.889430 22502662377600 run.py:483] Algo bellman_ford step 8222 current loss 0.099342, current_train_items 263136.
I0304 19:31:57.921779 22502662377600 run.py:483] Algo bellman_ford step 8223 current loss 0.061852, current_train_items 263168.
I0304 19:31:57.953475 22502662377600 run.py:483] Algo bellman_ford step 8224 current loss 0.048531, current_train_items 263200.
I0304 19:31:57.973277 22502662377600 run.py:483] Algo bellman_ford step 8225 current loss 0.007197, current_train_items 263232.
I0304 19:31:57.989601 22502662377600 run.py:483] Algo bellman_ford step 8226 current loss 0.017100, current_train_items 263264.
I0304 19:31:58.014637 22502662377600 run.py:483] Algo bellman_ford step 8227 current loss 0.054891, current_train_items 263296.
I0304 19:31:58.045440 22502662377600 run.py:483] Algo bellman_ford step 8228 current loss 0.083292, current_train_items 263328.
I0304 19:31:58.079509 22502662377600 run.py:483] Algo bellman_ford step 8229 current loss 0.066902, current_train_items 263360.
I0304 19:31:58.099204 22502662377600 run.py:483] Algo bellman_ford step 8230 current loss 0.002311, current_train_items 263392.
I0304 19:31:58.115585 22502662377600 run.py:483] Algo bellman_ford step 8231 current loss 0.021746, current_train_items 263424.
I0304 19:31:58.140247 22502662377600 run.py:483] Algo bellman_ford step 8232 current loss 0.033022, current_train_items 263456.
I0304 19:31:58.172063 22502662377600 run.py:483] Algo bellman_ford step 8233 current loss 0.033811, current_train_items 263488.
I0304 19:31:58.205282 22502662377600 run.py:483] Algo bellman_ford step 8234 current loss 0.089307, current_train_items 263520.
I0304 19:31:58.225150 22502662377600 run.py:483] Algo bellman_ford step 8235 current loss 0.005499, current_train_items 263552.
I0304 19:31:58.241710 22502662377600 run.py:483] Algo bellman_ford step 8236 current loss 0.023732, current_train_items 263584.
I0304 19:31:58.266412 22502662377600 run.py:483] Algo bellman_ford step 8237 current loss 0.049553, current_train_items 263616.
I0304 19:31:58.299078 22502662377600 run.py:483] Algo bellman_ford step 8238 current loss 0.075403, current_train_items 263648.
I0304 19:31:58.332323 22502662377600 run.py:483] Algo bellman_ford step 8239 current loss 0.054267, current_train_items 263680.
I0304 19:31:58.351798 22502662377600 run.py:483] Algo bellman_ford step 8240 current loss 0.002450, current_train_items 263712.
I0304 19:31:58.367860 22502662377600 run.py:483] Algo bellman_ford step 8241 current loss 0.012363, current_train_items 263744.
I0304 19:31:58.391445 22502662377600 run.py:483] Algo bellman_ford step 8242 current loss 0.036500, current_train_items 263776.
I0304 19:31:58.423072 22502662377600 run.py:483] Algo bellman_ford step 8243 current loss 0.059183, current_train_items 263808.
I0304 19:31:58.459132 22502662377600 run.py:483] Algo bellman_ford step 8244 current loss 0.056875, current_train_items 263840.
I0304 19:31:58.478900 22502662377600 run.py:483] Algo bellman_ford step 8245 current loss 0.013906, current_train_items 263872.
I0304 19:31:58.495049 22502662377600 run.py:483] Algo bellman_ford step 8246 current loss 0.027984, current_train_items 263904.
I0304 19:31:58.518180 22502662377600 run.py:483] Algo bellman_ford step 8247 current loss 0.099919, current_train_items 263936.
I0304 19:31:58.549828 22502662377600 run.py:483] Algo bellman_ford step 8248 current loss 0.058013, current_train_items 263968.
I0304 19:31:58.583714 22502662377600 run.py:483] Algo bellman_ford step 8249 current loss 0.152111, current_train_items 264000.
I0304 19:31:58.603425 22502662377600 run.py:483] Algo bellman_ford step 8250 current loss 0.002765, current_train_items 264032.
I0304 19:31:58.611596 22502662377600 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0304 19:31:58.611697 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:31:58.628288 22502662377600 run.py:483] Algo bellman_ford step 8251 current loss 0.024853, current_train_items 264064.
I0304 19:31:58.652818 22502662377600 run.py:483] Algo bellman_ford step 8252 current loss 0.079458, current_train_items 264096.
I0304 19:31:58.686139 22502662377600 run.py:483] Algo bellman_ford step 8253 current loss 0.067919, current_train_items 264128.
I0304 19:31:58.719340 22502662377600 run.py:483] Algo bellman_ford step 8254 current loss 0.046081, current_train_items 264160.
I0304 19:31:58.739087 22502662377600 run.py:483] Algo bellman_ford step 8255 current loss 0.026843, current_train_items 264192.
I0304 19:31:58.755608 22502662377600 run.py:483] Algo bellman_ford step 8256 current loss 0.051153, current_train_items 264224.
I0304 19:31:58.779973 22502662377600 run.py:483] Algo bellman_ford step 8257 current loss 0.054985, current_train_items 264256.
I0304 19:31:58.812420 22502662377600 run.py:483] Algo bellman_ford step 8258 current loss 0.110160, current_train_items 264288.
I0304 19:31:58.847378 22502662377600 run.py:483] Algo bellman_ford step 8259 current loss 0.093964, current_train_items 264320.
I0304 19:31:58.867288 22502662377600 run.py:483] Algo bellman_ford step 8260 current loss 0.015092, current_train_items 264352.
I0304 19:31:58.883656 22502662377600 run.py:483] Algo bellman_ford step 8261 current loss 0.012847, current_train_items 264384.
I0304 19:31:58.908639 22502662377600 run.py:483] Algo bellman_ford step 8262 current loss 0.059351, current_train_items 264416.
I0304 19:31:58.941152 22502662377600 run.py:483] Algo bellman_ford step 8263 current loss 0.127856, current_train_items 264448.
I0304 19:31:58.976238 22502662377600 run.py:483] Algo bellman_ford step 8264 current loss 0.102106, current_train_items 264480.
I0304 19:31:58.995608 22502662377600 run.py:483] Algo bellman_ford step 8265 current loss 0.058423, current_train_items 264512.
I0304 19:31:59.011403 22502662377600 run.py:483] Algo bellman_ford step 8266 current loss 0.037273, current_train_items 264544.
I0304 19:31:59.035537 22502662377600 run.py:483] Algo bellman_ford step 8267 current loss 0.122235, current_train_items 264576.
I0304 19:31:59.067968 22502662377600 run.py:483] Algo bellman_ford step 8268 current loss 0.086125, current_train_items 264608.
I0304 19:31:59.100384 22502662377600 run.py:483] Algo bellman_ford step 8269 current loss 0.110186, current_train_items 264640.
I0304 19:31:59.120460 22502662377600 run.py:483] Algo bellman_ford step 8270 current loss 0.015905, current_train_items 264672.
I0304 19:31:59.136134 22502662377600 run.py:483] Algo bellman_ford step 8271 current loss 0.011841, current_train_items 264704.
I0304 19:31:59.159837 22502662377600 run.py:483] Algo bellman_ford step 8272 current loss 0.079442, current_train_items 264736.
I0304 19:31:59.190704 22502662377600 run.py:483] Algo bellman_ford step 8273 current loss 0.041667, current_train_items 264768.
I0304 19:31:59.225487 22502662377600 run.py:483] Algo bellman_ford step 8274 current loss 0.098383, current_train_items 264800.
I0304 19:31:59.245413 22502662377600 run.py:483] Algo bellman_ford step 8275 current loss 0.005409, current_train_items 264832.
I0304 19:31:59.262144 22502662377600 run.py:483] Algo bellman_ford step 8276 current loss 0.038961, current_train_items 264864.
I0304 19:31:59.286383 22502662377600 run.py:483] Algo bellman_ford step 8277 current loss 0.051195, current_train_items 264896.
I0304 19:31:59.318450 22502662377600 run.py:483] Algo bellman_ford step 8278 current loss 0.069757, current_train_items 264928.
I0304 19:31:59.351549 22502662377600 run.py:483] Algo bellman_ford step 8279 current loss 0.080877, current_train_items 264960.
I0304 19:31:59.370897 22502662377600 run.py:483] Algo bellman_ford step 8280 current loss 0.007019, current_train_items 264992.
I0304 19:31:59.387504 22502662377600 run.py:483] Algo bellman_ford step 8281 current loss 0.024738, current_train_items 265024.
I0304 19:31:59.411458 22502662377600 run.py:483] Algo bellman_ford step 8282 current loss 0.055519, current_train_items 265056.
I0304 19:31:59.442797 22502662377600 run.py:483] Algo bellman_ford step 8283 current loss 0.071424, current_train_items 265088.
I0304 19:31:59.475577 22502662377600 run.py:483] Algo bellman_ford step 8284 current loss 0.082205, current_train_items 265120.
I0304 19:31:59.495251 22502662377600 run.py:483] Algo bellman_ford step 8285 current loss 0.004180, current_train_items 265152.
I0304 19:31:59.511756 22502662377600 run.py:483] Algo bellman_ford step 8286 current loss 0.040851, current_train_items 265184.
I0304 19:31:59.535528 22502662377600 run.py:483] Algo bellman_ford step 8287 current loss 0.059214, current_train_items 265216.
I0304 19:31:59.568646 22502662377600 run.py:483] Algo bellman_ford step 8288 current loss 0.109703, current_train_items 265248.
I0304 19:31:59.602546 22502662377600 run.py:483] Algo bellman_ford step 8289 current loss 0.061454, current_train_items 265280.
I0304 19:31:59.622341 22502662377600 run.py:483] Algo bellman_ford step 8290 current loss 0.006368, current_train_items 265312.
I0304 19:31:59.638550 22502662377600 run.py:483] Algo bellman_ford step 8291 current loss 0.017866, current_train_items 265344.
I0304 19:31:59.662977 22502662377600 run.py:483] Algo bellman_ford step 8292 current loss 0.055490, current_train_items 265376.
I0304 19:31:59.694027 22502662377600 run.py:483] Algo bellman_ford step 8293 current loss 0.056164, current_train_items 265408.
I0304 19:31:59.729299 22502662377600 run.py:483] Algo bellman_ford step 8294 current loss 0.104365, current_train_items 265440.
I0304 19:31:59.748912 22502662377600 run.py:483] Algo bellman_ford step 8295 current loss 0.020903, current_train_items 265472.
I0304 19:31:59.765153 22502662377600 run.py:483] Algo bellman_ford step 8296 current loss 0.018398, current_train_items 265504.
I0304 19:31:59.790215 22502662377600 run.py:483] Algo bellman_ford step 8297 current loss 0.094029, current_train_items 265536.
I0304 19:31:59.822238 22502662377600 run.py:483] Algo bellman_ford step 8298 current loss 0.109122, current_train_items 265568.
I0304 19:31:59.855173 22502662377600 run.py:483] Algo bellman_ford step 8299 current loss 0.100302, current_train_items 265600.
I0304 19:31:59.874931 22502662377600 run.py:483] Algo bellman_ford step 8300 current loss 0.051282, current_train_items 265632.
I0304 19:31:59.882875 22502662377600 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0304 19:31:59.882979 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:31:59.899394 22502662377600 run.py:483] Algo bellman_ford step 8301 current loss 0.014328, current_train_items 265664.
I0304 19:31:59.925206 22502662377600 run.py:483] Algo bellman_ford step 8302 current loss 0.113853, current_train_items 265696.
I0304 19:31:59.958074 22502662377600 run.py:483] Algo bellman_ford step 8303 current loss 0.076111, current_train_items 265728.
I0304 19:31:59.991768 22502662377600 run.py:483] Algo bellman_ford step 8304 current loss 0.069859, current_train_items 265760.
I0304 19:32:00.011950 22502662377600 run.py:483] Algo bellman_ford step 8305 current loss 0.005640, current_train_items 265792.
I0304 19:32:00.027492 22502662377600 run.py:483] Algo bellman_ford step 8306 current loss 0.034147, current_train_items 265824.
I0304 19:32:00.052004 22502662377600 run.py:483] Algo bellman_ford step 8307 current loss 0.100635, current_train_items 265856.
I0304 19:32:00.082176 22502662377600 run.py:483] Algo bellman_ford step 8308 current loss 0.050803, current_train_items 265888.
I0304 19:32:00.118117 22502662377600 run.py:483] Algo bellman_ford step 8309 current loss 0.151817, current_train_items 265920.
I0304 19:32:00.138020 22502662377600 run.py:483] Algo bellman_ford step 8310 current loss 0.003413, current_train_items 265952.
I0304 19:32:00.153987 22502662377600 run.py:483] Algo bellman_ford step 8311 current loss 0.016049, current_train_items 265984.
I0304 19:32:00.178342 22502662377600 run.py:483] Algo bellman_ford step 8312 current loss 0.059597, current_train_items 266016.
I0304 19:32:00.209804 22502662377600 run.py:483] Algo bellman_ford step 8313 current loss 0.053571, current_train_items 266048.
I0304 19:32:00.241800 22502662377600 run.py:483] Algo bellman_ford step 8314 current loss 0.070517, current_train_items 266080.
I0304 19:32:00.261347 22502662377600 run.py:483] Algo bellman_ford step 8315 current loss 0.005427, current_train_items 266112.
I0304 19:32:00.277765 22502662377600 run.py:483] Algo bellman_ford step 8316 current loss 0.017039, current_train_items 266144.
I0304 19:32:00.302084 22502662377600 run.py:483] Algo bellman_ford step 8317 current loss 0.053652, current_train_items 266176.
I0304 19:32:00.332280 22502662377600 run.py:483] Algo bellman_ford step 8318 current loss 0.025684, current_train_items 266208.
I0304 19:32:00.365713 22502662377600 run.py:483] Algo bellman_ford step 8319 current loss 0.069304, current_train_items 266240.
I0304 19:32:00.385479 22502662377600 run.py:483] Algo bellman_ford step 8320 current loss 0.006135, current_train_items 266272.
I0304 19:32:00.401489 22502662377600 run.py:483] Algo bellman_ford step 8321 current loss 0.017529, current_train_items 266304.
I0304 19:32:00.426442 22502662377600 run.py:483] Algo bellman_ford step 8322 current loss 0.064627, current_train_items 266336.
I0304 19:32:00.458257 22502662377600 run.py:483] Algo bellman_ford step 8323 current loss 0.043238, current_train_items 266368.
I0304 19:32:00.493600 22502662377600 run.py:483] Algo bellman_ford step 8324 current loss 0.073277, current_train_items 266400.
I0304 19:32:00.513435 22502662377600 run.py:483] Algo bellman_ford step 8325 current loss 0.002589, current_train_items 266432.
I0304 19:32:00.529131 22502662377600 run.py:483] Algo bellman_ford step 8326 current loss 0.012357, current_train_items 266464.
I0304 19:32:00.553273 22502662377600 run.py:483] Algo bellman_ford step 8327 current loss 0.030538, current_train_items 266496.
I0304 19:32:00.584251 22502662377600 run.py:483] Algo bellman_ford step 8328 current loss 0.105969, current_train_items 266528.
I0304 19:32:00.618872 22502662377600 run.py:483] Algo bellman_ford step 8329 current loss 0.084915, current_train_items 266560.
I0304 19:32:00.638761 22502662377600 run.py:483] Algo bellman_ford step 8330 current loss 0.003325, current_train_items 266592.
I0304 19:32:00.654697 22502662377600 run.py:483] Algo bellman_ford step 8331 current loss 0.034849, current_train_items 266624.
I0304 19:32:00.678591 22502662377600 run.py:483] Algo bellman_ford step 8332 current loss 0.030201, current_train_items 266656.
I0304 19:32:00.710902 22502662377600 run.py:483] Algo bellman_ford step 8333 current loss 0.105365, current_train_items 266688.
I0304 19:32:00.743869 22502662377600 run.py:483] Algo bellman_ford step 8334 current loss 0.074636, current_train_items 266720.
I0304 19:32:00.763432 22502662377600 run.py:483] Algo bellman_ford step 8335 current loss 0.005726, current_train_items 266752.
I0304 19:32:00.779628 22502662377600 run.py:483] Algo bellman_ford step 8336 current loss 0.025177, current_train_items 266784.
I0304 19:32:00.804678 22502662377600 run.py:483] Algo bellman_ford step 8337 current loss 0.032287, current_train_items 266816.
I0304 19:32:00.836192 22502662377600 run.py:483] Algo bellman_ford step 8338 current loss 0.054548, current_train_items 266848.
I0304 19:32:00.869088 22502662377600 run.py:483] Algo bellman_ford step 8339 current loss 0.064831, current_train_items 266880.
I0304 19:32:00.888633 22502662377600 run.py:483] Algo bellman_ford step 8340 current loss 0.020374, current_train_items 266912.
I0304 19:32:00.904542 22502662377600 run.py:483] Algo bellman_ford step 8341 current loss 0.028394, current_train_items 266944.
I0304 19:32:00.928009 22502662377600 run.py:483] Algo bellman_ford step 8342 current loss 0.090679, current_train_items 266976.
I0304 19:32:00.959008 22502662377600 run.py:483] Algo bellman_ford step 8343 current loss 0.059441, current_train_items 267008.
I0304 19:32:00.994008 22502662377600 run.py:483] Algo bellman_ford step 8344 current loss 0.117638, current_train_items 267040.
I0304 19:32:01.013766 22502662377600 run.py:483] Algo bellman_ford step 8345 current loss 0.010722, current_train_items 267072.
I0304 19:32:01.029661 22502662377600 run.py:483] Algo bellman_ford step 8346 current loss 0.022233, current_train_items 267104.
I0304 19:32:01.054590 22502662377600 run.py:483] Algo bellman_ford step 8347 current loss 0.112293, current_train_items 267136.
I0304 19:32:01.087369 22502662377600 run.py:483] Algo bellman_ford step 8348 current loss 0.077476, current_train_items 267168.
I0304 19:32:01.122092 22502662377600 run.py:483] Algo bellman_ford step 8349 current loss 0.085263, current_train_items 267200.
I0304 19:32:01.141901 22502662377600 run.py:483] Algo bellman_ford step 8350 current loss 0.004907, current_train_items 267232.
I0304 19:32:01.149893 22502662377600 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0304 19:32:01.149995 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:01.166858 22502662377600 run.py:483] Algo bellman_ford step 8351 current loss 0.032647, current_train_items 267264.
I0304 19:32:01.191655 22502662377600 run.py:483] Algo bellman_ford step 8352 current loss 0.066078, current_train_items 267296.
I0304 19:32:01.224010 22502662377600 run.py:483] Algo bellman_ford step 8353 current loss 0.047975, current_train_items 267328.
I0304 19:32:01.259979 22502662377600 run.py:483] Algo bellman_ford step 8354 current loss 0.097753, current_train_items 267360.
I0304 19:32:01.279724 22502662377600 run.py:483] Algo bellman_ford step 8355 current loss 0.011340, current_train_items 267392.
I0304 19:32:01.295196 22502662377600 run.py:483] Algo bellman_ford step 8356 current loss 0.028614, current_train_items 267424.
I0304 19:32:01.319196 22502662377600 run.py:483] Algo bellman_ford step 8357 current loss 0.070903, current_train_items 267456.
I0304 19:32:01.351320 22502662377600 run.py:483] Algo bellman_ford step 8358 current loss 0.079242, current_train_items 267488.
I0304 19:32:01.384161 22502662377600 run.py:483] Algo bellman_ford step 8359 current loss 0.132267, current_train_items 267520.
I0304 19:32:01.403954 22502662377600 run.py:483] Algo bellman_ford step 8360 current loss 0.008327, current_train_items 267552.
I0304 19:32:01.419943 22502662377600 run.py:483] Algo bellman_ford step 8361 current loss 0.018755, current_train_items 267584.
I0304 19:32:01.444050 22502662377600 run.py:483] Algo bellman_ford step 8362 current loss 0.057934, current_train_items 267616.
I0304 19:32:01.475311 22502662377600 run.py:483] Algo bellman_ford step 8363 current loss 0.062314, current_train_items 267648.
I0304 19:32:01.509491 22502662377600 run.py:483] Algo bellman_ford step 8364 current loss 0.077053, current_train_items 267680.
I0304 19:32:01.529080 22502662377600 run.py:483] Algo bellman_ford step 8365 current loss 0.004187, current_train_items 267712.
I0304 19:32:01.545543 22502662377600 run.py:483] Algo bellman_ford step 8366 current loss 0.008851, current_train_items 267744.
I0304 19:32:01.569077 22502662377600 run.py:483] Algo bellman_ford step 8367 current loss 0.046354, current_train_items 267776.
I0304 19:32:01.600828 22502662377600 run.py:483] Algo bellman_ford step 8368 current loss 0.074228, current_train_items 267808.
I0304 19:32:01.636697 22502662377600 run.py:483] Algo bellman_ford step 8369 current loss 0.133877, current_train_items 267840.
I0304 19:32:01.656706 22502662377600 run.py:483] Algo bellman_ford step 8370 current loss 0.003315, current_train_items 267872.
I0304 19:32:01.673129 22502662377600 run.py:483] Algo bellman_ford step 8371 current loss 0.024434, current_train_items 267904.
I0304 19:32:01.696264 22502662377600 run.py:483] Algo bellman_ford step 8372 current loss 0.088017, current_train_items 267936.
I0304 19:32:01.727955 22502662377600 run.py:483] Algo bellman_ford step 8373 current loss 0.042521, current_train_items 267968.
I0304 19:32:01.762294 22502662377600 run.py:483] Algo bellman_ford step 8374 current loss 0.049124, current_train_items 268000.
I0304 19:32:01.782326 22502662377600 run.py:483] Algo bellman_ford step 8375 current loss 0.005675, current_train_items 268032.
I0304 19:32:01.798116 22502662377600 run.py:483] Algo bellman_ford step 8376 current loss 0.011410, current_train_items 268064.
I0304 19:32:01.821580 22502662377600 run.py:483] Algo bellman_ford step 8377 current loss 0.055304, current_train_items 268096.
I0304 19:32:01.853479 22502662377600 run.py:483] Algo bellman_ford step 8378 current loss 0.068920, current_train_items 268128.
I0304 19:32:01.887545 22502662377600 run.py:483] Algo bellman_ford step 8379 current loss 0.085064, current_train_items 268160.
I0304 19:32:01.907223 22502662377600 run.py:483] Algo bellman_ford step 8380 current loss 0.020175, current_train_items 268192.
I0304 19:32:01.923381 22502662377600 run.py:483] Algo bellman_ford step 8381 current loss 0.012008, current_train_items 268224.
I0304 19:32:01.946426 22502662377600 run.py:483] Algo bellman_ford step 8382 current loss 0.061397, current_train_items 268256.
I0304 19:32:01.979228 22502662377600 run.py:483] Algo bellman_ford step 8383 current loss 0.071702, current_train_items 268288.
I0304 19:32:02.011657 22502662377600 run.py:483] Algo bellman_ford step 8384 current loss 0.064481, current_train_items 268320.
I0304 19:32:02.031686 22502662377600 run.py:483] Algo bellman_ford step 8385 current loss 0.018055, current_train_items 268352.
I0304 19:32:02.047397 22502662377600 run.py:483] Algo bellman_ford step 8386 current loss 0.010055, current_train_items 268384.
I0304 19:32:02.070696 22502662377600 run.py:483] Algo bellman_ford step 8387 current loss 0.035273, current_train_items 268416.
I0304 19:32:02.102617 22502662377600 run.py:483] Algo bellman_ford step 8388 current loss 0.042036, current_train_items 268448.
I0304 19:32:02.136300 22502662377600 run.py:483] Algo bellman_ford step 8389 current loss 0.097540, current_train_items 268480.
I0304 19:32:02.156186 22502662377600 run.py:483] Algo bellman_ford step 8390 current loss 0.004487, current_train_items 268512.
I0304 19:32:02.172544 22502662377600 run.py:483] Algo bellman_ford step 8391 current loss 0.008452, current_train_items 268544.
I0304 19:32:02.195423 22502662377600 run.py:483] Algo bellman_ford step 8392 current loss 0.047996, current_train_items 268576.
I0304 19:32:02.228736 22502662377600 run.py:483] Algo bellman_ford step 8393 current loss 0.072841, current_train_items 268608.
I0304 19:32:02.261253 22502662377600 run.py:483] Algo bellman_ford step 8394 current loss 0.052581, current_train_items 268640.
I0304 19:32:02.280914 22502662377600 run.py:483] Algo bellman_ford step 8395 current loss 0.011174, current_train_items 268672.
I0304 19:32:02.297083 22502662377600 run.py:483] Algo bellman_ford step 8396 current loss 0.028562, current_train_items 268704.
I0304 19:32:02.320915 22502662377600 run.py:483] Algo bellman_ford step 8397 current loss 0.037303, current_train_items 268736.
I0304 19:32:02.353301 22502662377600 run.py:483] Algo bellman_ford step 8398 current loss 0.065294, current_train_items 268768.
I0304 19:32:02.387736 22502662377600 run.py:483] Algo bellman_ford step 8399 current loss 0.072443, current_train_items 268800.
I0304 19:32:02.407922 22502662377600 run.py:483] Algo bellman_ford step 8400 current loss 0.005126, current_train_items 268832.
I0304 19:32:02.416106 22502662377600 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0304 19:32:02.416212 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:02.433114 22502662377600 run.py:483] Algo bellman_ford step 8401 current loss 0.023273, current_train_items 268864.
I0304 19:32:02.458402 22502662377600 run.py:483] Algo bellman_ford step 8402 current loss 0.085093, current_train_items 268896.
I0304 19:32:02.491260 22502662377600 run.py:483] Algo bellman_ford step 8403 current loss 0.055362, current_train_items 268928.
I0304 19:32:02.528359 22502662377600 run.py:483] Algo bellman_ford step 8404 current loss 0.117180, current_train_items 268960.
I0304 19:32:02.548366 22502662377600 run.py:483] Algo bellman_ford step 8405 current loss 0.004460, current_train_items 268992.
I0304 19:32:02.564220 22502662377600 run.py:483] Algo bellman_ford step 8406 current loss 0.021268, current_train_items 269024.
I0304 19:32:02.587999 22502662377600 run.py:483] Algo bellman_ford step 8407 current loss 0.073958, current_train_items 269056.
I0304 19:32:02.619375 22502662377600 run.py:483] Algo bellman_ford step 8408 current loss 0.059114, current_train_items 269088.
I0304 19:32:02.654272 22502662377600 run.py:483] Algo bellman_ford step 8409 current loss 0.082861, current_train_items 269120.
I0304 19:32:02.674046 22502662377600 run.py:483] Algo bellman_ford step 8410 current loss 0.004989, current_train_items 269152.
I0304 19:32:02.690232 22502662377600 run.py:483] Algo bellman_ford step 8411 current loss 0.022224, current_train_items 269184.
I0304 19:32:02.714784 22502662377600 run.py:483] Algo bellman_ford step 8412 current loss 0.080167, current_train_items 269216.
I0304 19:32:02.748283 22502662377600 run.py:483] Algo bellman_ford step 8413 current loss 0.057099, current_train_items 269248.
I0304 19:32:02.782977 22502662377600 run.py:483] Algo bellman_ford step 8414 current loss 0.074703, current_train_items 269280.
I0304 19:32:02.803252 22502662377600 run.py:483] Algo bellman_ford step 8415 current loss 0.005470, current_train_items 269312.
I0304 19:32:02.819543 22502662377600 run.py:483] Algo bellman_ford step 8416 current loss 0.007456, current_train_items 269344.
I0304 19:32:02.843688 22502662377600 run.py:483] Algo bellman_ford step 8417 current loss 0.084471, current_train_items 269376.
I0304 19:32:02.876582 22502662377600 run.py:483] Algo bellman_ford step 8418 current loss 0.048701, current_train_items 269408.
I0304 19:32:02.914037 22502662377600 run.py:483] Algo bellman_ford step 8419 current loss 0.100124, current_train_items 269440.
I0304 19:32:02.933661 22502662377600 run.py:483] Algo bellman_ford step 8420 current loss 0.010125, current_train_items 269472.
I0304 19:32:02.949939 22502662377600 run.py:483] Algo bellman_ford step 8421 current loss 0.052919, current_train_items 269504.
I0304 19:32:02.975508 22502662377600 run.py:483] Algo bellman_ford step 8422 current loss 0.099057, current_train_items 269536.
I0304 19:32:03.008048 22502662377600 run.py:483] Algo bellman_ford step 8423 current loss 0.040726, current_train_items 269568.
I0304 19:32:03.043142 22502662377600 run.py:483] Algo bellman_ford step 8424 current loss 0.058569, current_train_items 269600.
I0304 19:32:03.062695 22502662377600 run.py:483] Algo bellman_ford step 8425 current loss 0.010124, current_train_items 269632.
I0304 19:32:03.078390 22502662377600 run.py:483] Algo bellman_ford step 8426 current loss 0.113694, current_train_items 269664.
I0304 19:32:03.102756 22502662377600 run.py:483] Algo bellman_ford step 8427 current loss 0.095247, current_train_items 269696.
I0304 19:32:03.134690 22502662377600 run.py:483] Algo bellman_ford step 8428 current loss 0.096881, current_train_items 269728.
I0304 19:32:03.168666 22502662377600 run.py:483] Algo bellman_ford step 8429 current loss 0.072601, current_train_items 269760.
I0304 19:32:03.188730 22502662377600 run.py:483] Algo bellman_ford step 8430 current loss 0.016078, current_train_items 269792.
I0304 19:32:03.205144 22502662377600 run.py:483] Algo bellman_ford step 8431 current loss 0.066693, current_train_items 269824.
I0304 19:32:03.229020 22502662377600 run.py:483] Algo bellman_ford step 8432 current loss 0.101835, current_train_items 269856.
I0304 19:32:03.262522 22502662377600 run.py:483] Algo bellman_ford step 8433 current loss 0.148302, current_train_items 269888.
I0304 19:32:03.295068 22502662377600 run.py:483] Algo bellman_ford step 8434 current loss 0.056213, current_train_items 269920.
I0304 19:32:03.314699 22502662377600 run.py:483] Algo bellman_ford step 8435 current loss 0.004309, current_train_items 269952.
I0304 19:32:03.330527 22502662377600 run.py:483] Algo bellman_ford step 8436 current loss 0.025149, current_train_items 269984.
I0304 19:32:03.354794 22502662377600 run.py:483] Algo bellman_ford step 8437 current loss 0.065919, current_train_items 270016.
I0304 19:32:03.386729 22502662377600 run.py:483] Algo bellman_ford step 8438 current loss 0.078752, current_train_items 270048.
I0304 19:32:03.419287 22502662377600 run.py:483] Algo bellman_ford step 8439 current loss 0.074168, current_train_items 270080.
I0304 19:32:03.439325 22502662377600 run.py:483] Algo bellman_ford step 8440 current loss 0.008864, current_train_items 270112.
I0304 19:32:03.455545 22502662377600 run.py:483] Algo bellman_ford step 8441 current loss 0.035538, current_train_items 270144.
I0304 19:32:03.480000 22502662377600 run.py:483] Algo bellman_ford step 8442 current loss 0.033692, current_train_items 270176.
I0304 19:32:03.511931 22502662377600 run.py:483] Algo bellman_ford step 8443 current loss 0.068812, current_train_items 270208.
I0304 19:32:03.546595 22502662377600 run.py:483] Algo bellman_ford step 8444 current loss 0.065373, current_train_items 270240.
I0304 19:32:03.566320 22502662377600 run.py:483] Algo bellman_ford step 8445 current loss 0.007196, current_train_items 270272.
I0304 19:32:03.582260 22502662377600 run.py:483] Algo bellman_ford step 8446 current loss 0.019115, current_train_items 270304.
I0304 19:32:03.606635 22502662377600 run.py:483] Algo bellman_ford step 8447 current loss 0.103327, current_train_items 270336.
I0304 19:32:03.638570 22502662377600 run.py:483] Algo bellman_ford step 8448 current loss 0.118725, current_train_items 270368.
I0304 19:32:03.672477 22502662377600 run.py:483] Algo bellman_ford step 8449 current loss 0.090368, current_train_items 270400.
I0304 19:32:03.692028 22502662377600 run.py:483] Algo bellman_ford step 8450 current loss 0.004961, current_train_items 270432.
I0304 19:32:03.700095 22502662377600 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0304 19:32:03.700201 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:32:03.716714 22502662377600 run.py:483] Algo bellman_ford step 8451 current loss 0.013908, current_train_items 270464.
I0304 19:32:03.741574 22502662377600 run.py:483] Algo bellman_ford step 8452 current loss 0.072180, current_train_items 270496.
I0304 19:32:03.774665 22502662377600 run.py:483] Algo bellman_ford step 8453 current loss 0.150346, current_train_items 270528.
I0304 19:32:03.809419 22502662377600 run.py:483] Algo bellman_ford step 8454 current loss 0.146409, current_train_items 270560.
I0304 19:32:03.829199 22502662377600 run.py:483] Algo bellman_ford step 8455 current loss 0.012926, current_train_items 270592.
I0304 19:32:03.845324 22502662377600 run.py:483] Algo bellman_ford step 8456 current loss 0.015321, current_train_items 270624.
I0304 19:32:03.869868 22502662377600 run.py:483] Algo bellman_ford step 8457 current loss 0.073362, current_train_items 270656.
I0304 19:32:03.901039 22502662377600 run.py:483] Algo bellman_ford step 8458 current loss 0.082245, current_train_items 270688.
I0304 19:32:03.932471 22502662377600 run.py:483] Algo bellman_ford step 8459 current loss 0.091808, current_train_items 270720.
I0304 19:32:03.952360 22502662377600 run.py:483] Algo bellman_ford step 8460 current loss 0.017001, current_train_items 270752.
I0304 19:32:03.968329 22502662377600 run.py:483] Algo bellman_ford step 8461 current loss 0.008217, current_train_items 270784.
I0304 19:32:03.990987 22502662377600 run.py:483] Algo bellman_ford step 8462 current loss 0.029837, current_train_items 270816.
I0304 19:32:04.022085 22502662377600 run.py:483] Algo bellman_ford step 8463 current loss 0.032729, current_train_items 270848.
I0304 19:32:04.056339 22502662377600 run.py:483] Algo bellman_ford step 8464 current loss 0.087911, current_train_items 270880.
I0304 19:32:04.075780 22502662377600 run.py:483] Algo bellman_ford step 8465 current loss 0.043445, current_train_items 270912.
I0304 19:32:04.092278 22502662377600 run.py:483] Algo bellman_ford step 8466 current loss 0.045481, current_train_items 270944.
I0304 19:32:04.115896 22502662377600 run.py:483] Algo bellman_ford step 8467 current loss 0.056898, current_train_items 270976.
I0304 19:32:04.147392 22502662377600 run.py:483] Algo bellman_ford step 8468 current loss 0.049019, current_train_items 271008.
I0304 19:32:04.180321 22502662377600 run.py:483] Algo bellman_ford step 8469 current loss 0.056002, current_train_items 271040.
I0304 19:32:04.200136 22502662377600 run.py:483] Algo bellman_ford step 8470 current loss 0.007866, current_train_items 271072.
I0304 19:32:04.217085 22502662377600 run.py:483] Algo bellman_ford step 8471 current loss 0.043888, current_train_items 271104.
I0304 19:32:04.240582 22502662377600 run.py:483] Algo bellman_ford step 8472 current loss 0.020613, current_train_items 271136.
I0304 19:32:04.270220 22502662377600 run.py:483] Algo bellman_ford step 8473 current loss 0.032607, current_train_items 271168.
I0304 19:32:04.304647 22502662377600 run.py:483] Algo bellman_ford step 8474 current loss 0.076009, current_train_items 271200.
I0304 19:32:04.324532 22502662377600 run.py:483] Algo bellman_ford step 8475 current loss 0.010732, current_train_items 271232.
I0304 19:32:04.340942 22502662377600 run.py:483] Algo bellman_ford step 8476 current loss 0.035710, current_train_items 271264.
I0304 19:32:04.366169 22502662377600 run.py:483] Algo bellman_ford step 8477 current loss 0.125345, current_train_items 271296.
I0304 19:32:04.398136 22502662377600 run.py:483] Algo bellman_ford step 8478 current loss 0.101475, current_train_items 271328.
I0304 19:32:04.432943 22502662377600 run.py:483] Algo bellman_ford step 8479 current loss 0.072553, current_train_items 271360.
I0304 19:32:04.452191 22502662377600 run.py:483] Algo bellman_ford step 8480 current loss 0.002701, current_train_items 271392.
I0304 19:32:04.468432 22502662377600 run.py:483] Algo bellman_ford step 8481 current loss 0.024614, current_train_items 271424.
I0304 19:32:04.492856 22502662377600 run.py:483] Algo bellman_ford step 8482 current loss 0.071400, current_train_items 271456.
I0304 19:32:04.524767 22502662377600 run.py:483] Algo bellman_ford step 8483 current loss 0.077990, current_train_items 271488.
I0304 19:32:04.557301 22502662377600 run.py:483] Algo bellman_ford step 8484 current loss 0.089740, current_train_items 271520.
I0304 19:32:04.577277 22502662377600 run.py:483] Algo bellman_ford step 8485 current loss 0.003775, current_train_items 271552.
I0304 19:32:04.593162 22502662377600 run.py:483] Algo bellman_ford step 8486 current loss 0.022338, current_train_items 271584.
I0304 19:32:04.616915 22502662377600 run.py:483] Algo bellman_ford step 8487 current loss 0.038295, current_train_items 271616.
I0304 19:32:04.649309 22502662377600 run.py:483] Algo bellman_ford step 8488 current loss 0.105860, current_train_items 271648.
I0304 19:32:04.683433 22502662377600 run.py:483] Algo bellman_ford step 8489 current loss 0.078470, current_train_items 271680.
I0304 19:32:04.703576 22502662377600 run.py:483] Algo bellman_ford step 8490 current loss 0.011654, current_train_items 271712.
I0304 19:32:04.719558 22502662377600 run.py:483] Algo bellman_ford step 8491 current loss 0.027999, current_train_items 271744.
I0304 19:32:04.743948 22502662377600 run.py:483] Algo bellman_ford step 8492 current loss 0.060954, current_train_items 271776.
I0304 19:32:04.775275 22502662377600 run.py:483] Algo bellman_ford step 8493 current loss 0.063259, current_train_items 271808.
I0304 19:32:04.808055 22502662377600 run.py:483] Algo bellman_ford step 8494 current loss 0.188564, current_train_items 271840.
I0304 19:32:04.827729 22502662377600 run.py:483] Algo bellman_ford step 8495 current loss 0.010602, current_train_items 271872.
I0304 19:32:04.844364 22502662377600 run.py:483] Algo bellman_ford step 8496 current loss 0.042632, current_train_items 271904.
I0304 19:32:04.868530 22502662377600 run.py:483] Algo bellman_ford step 8497 current loss 0.048038, current_train_items 271936.
I0304 19:32:04.900953 22502662377600 run.py:483] Algo bellman_ford step 8498 current loss 0.090037, current_train_items 271968.
I0304 19:32:04.933139 22502662377600 run.py:483] Algo bellman_ford step 8499 current loss 0.072758, current_train_items 272000.
I0304 19:32:04.953431 22502662377600 run.py:483] Algo bellman_ford step 8500 current loss 0.004128, current_train_items 272032.
I0304 19:32:04.961673 22502662377600 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0304 19:32:04.961803 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:32:04.979026 22502662377600 run.py:483] Algo bellman_ford step 8501 current loss 0.011561, current_train_items 272064.
I0304 19:32:05.004611 22502662377600 run.py:483] Algo bellman_ford step 8502 current loss 0.049470, current_train_items 272096.
I0304 19:32:05.036169 22502662377600 run.py:483] Algo bellman_ford step 8503 current loss 0.023326, current_train_items 272128.
I0304 19:32:05.069597 22502662377600 run.py:483] Algo bellman_ford step 8504 current loss 0.082472, current_train_items 272160.
I0304 19:32:05.089601 22502662377600 run.py:483] Algo bellman_ford step 8505 current loss 0.005440, current_train_items 272192.
I0304 19:32:05.104975 22502662377600 run.py:483] Algo bellman_ford step 8506 current loss 0.031177, current_train_items 272224.
I0304 19:32:05.128197 22502662377600 run.py:483] Algo bellman_ford step 8507 current loss 0.044229, current_train_items 272256.
I0304 19:32:05.160410 22502662377600 run.py:483] Algo bellman_ford step 8508 current loss 0.074012, current_train_items 272288.
I0304 19:32:05.195502 22502662377600 run.py:483] Algo bellman_ford step 8509 current loss 0.096421, current_train_items 272320.
I0304 19:32:05.215368 22502662377600 run.py:483] Algo bellman_ford step 8510 current loss 0.002774, current_train_items 272352.
I0304 19:32:05.231059 22502662377600 run.py:483] Algo bellman_ford step 8511 current loss 0.031531, current_train_items 272384.
I0304 19:32:05.256114 22502662377600 run.py:483] Algo bellman_ford step 8512 current loss 0.043969, current_train_items 272416.
I0304 19:32:05.287644 22502662377600 run.py:483] Algo bellman_ford step 8513 current loss 0.092045, current_train_items 272448.
I0304 19:32:05.322055 22502662377600 run.py:483] Algo bellman_ford step 8514 current loss 0.069381, current_train_items 272480.
I0304 19:32:05.341816 22502662377600 run.py:483] Algo bellman_ford step 8515 current loss 0.003636, current_train_items 272512.
I0304 19:32:05.357775 22502662377600 run.py:483] Algo bellman_ford step 8516 current loss 0.017273, current_train_items 272544.
I0304 19:32:05.382076 22502662377600 run.py:483] Algo bellman_ford step 8517 current loss 0.095427, current_train_items 272576.
I0304 19:32:05.415652 22502662377600 run.py:483] Algo bellman_ford step 8518 current loss 0.094636, current_train_items 272608.
I0304 19:32:05.448574 22502662377600 run.py:483] Algo bellman_ford step 8519 current loss 0.082894, current_train_items 272640.
I0304 19:32:05.468300 22502662377600 run.py:483] Algo bellman_ford step 8520 current loss 0.002810, current_train_items 272672.
I0304 19:32:05.484255 22502662377600 run.py:483] Algo bellman_ford step 8521 current loss 0.012946, current_train_items 272704.
I0304 19:32:05.508239 22502662377600 run.py:483] Algo bellman_ford step 8522 current loss 0.096777, current_train_items 272736.
I0304 19:32:05.540415 22502662377600 run.py:483] Algo bellman_ford step 8523 current loss 0.129025, current_train_items 272768.
I0304 19:32:05.575259 22502662377600 run.py:483] Algo bellman_ford step 8524 current loss 0.141350, current_train_items 272800.
I0304 19:32:05.595363 22502662377600 run.py:483] Algo bellman_ford step 8525 current loss 0.007158, current_train_items 272832.
I0304 19:32:05.611521 22502662377600 run.py:483] Algo bellman_ford step 8526 current loss 0.018991, current_train_items 272864.
I0304 19:32:05.635769 22502662377600 run.py:483] Algo bellman_ford step 8527 current loss 0.062383, current_train_items 272896.
I0304 19:32:05.667335 22502662377600 run.py:483] Algo bellman_ford step 8528 current loss 0.041655, current_train_items 272928.
I0304 19:32:05.701599 22502662377600 run.py:483] Algo bellman_ford step 8529 current loss 0.082467, current_train_items 272960.
I0304 19:32:05.721626 22502662377600 run.py:483] Algo bellman_ford step 8530 current loss 0.003847, current_train_items 272992.
I0304 19:32:05.737690 22502662377600 run.py:483] Algo bellman_ford step 8531 current loss 0.021607, current_train_items 273024.
I0304 19:32:05.760750 22502662377600 run.py:483] Algo bellman_ford step 8532 current loss 0.043568, current_train_items 273056.
I0304 19:32:05.792330 22502662377600 run.py:483] Algo bellman_ford step 8533 current loss 0.032678, current_train_items 273088.
I0304 19:32:05.825338 22502662377600 run.py:483] Algo bellman_ford step 8534 current loss 0.087411, current_train_items 273120.
I0304 19:32:05.844927 22502662377600 run.py:483] Algo bellman_ford step 8535 current loss 0.003066, current_train_items 273152.
I0304 19:32:05.860906 22502662377600 run.py:483] Algo bellman_ford step 8536 current loss 0.032864, current_train_items 273184.
I0304 19:32:05.884996 22502662377600 run.py:483] Algo bellman_ford step 8537 current loss 0.095585, current_train_items 273216.
I0304 19:32:05.917446 22502662377600 run.py:483] Algo bellman_ford step 8538 current loss 0.067535, current_train_items 273248.
I0304 19:32:05.953242 22502662377600 run.py:483] Algo bellman_ford step 8539 current loss 0.082821, current_train_items 273280.
I0304 19:32:05.973182 22502662377600 run.py:483] Algo bellman_ford step 8540 current loss 0.018894, current_train_items 273312.
I0304 19:32:05.989343 22502662377600 run.py:483] Algo bellman_ford step 8541 current loss 0.115049, current_train_items 273344.
I0304 19:32:06.013778 22502662377600 run.py:483] Algo bellman_ford step 8542 current loss 0.087907, current_train_items 273376.
I0304 19:32:06.046294 22502662377600 run.py:483] Algo bellman_ford step 8543 current loss 0.116831, current_train_items 273408.
I0304 19:32:06.080200 22502662377600 run.py:483] Algo bellman_ford step 8544 current loss 0.106768, current_train_items 273440.
I0304 19:32:06.100027 22502662377600 run.py:483] Algo bellman_ford step 8545 current loss 0.004355, current_train_items 273472.
I0304 19:32:06.115665 22502662377600 run.py:483] Algo bellman_ford step 8546 current loss 0.040076, current_train_items 273504.
I0304 19:32:06.140119 22502662377600 run.py:483] Algo bellman_ford step 8547 current loss 0.110491, current_train_items 273536.
I0304 19:32:06.172054 22502662377600 run.py:483] Algo bellman_ford step 8548 current loss 0.069143, current_train_items 273568.
I0304 19:32:06.206436 22502662377600 run.py:483] Algo bellman_ford step 8549 current loss 0.113663, current_train_items 273600.
I0304 19:32:06.226329 22502662377600 run.py:483] Algo bellman_ford step 8550 current loss 0.003287, current_train_items 273632.
I0304 19:32:06.234042 22502662377600 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0304 19:32:06.234143 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0304 19:32:06.251165 22502662377600 run.py:483] Algo bellman_ford step 8551 current loss 0.056663, current_train_items 273664.
I0304 19:32:06.276414 22502662377600 run.py:483] Algo bellman_ford step 8552 current loss 0.057405, current_train_items 273696.
I0304 19:32:06.308806 22502662377600 run.py:483] Algo bellman_ford step 8553 current loss 0.049697, current_train_items 273728.
I0304 19:32:06.340470 22502662377600 run.py:483] Algo bellman_ford step 8554 current loss 0.036292, current_train_items 273760.
I0304 19:32:06.360263 22502662377600 run.py:483] Algo bellman_ford step 8555 current loss 0.008237, current_train_items 273792.
I0304 19:32:06.375918 22502662377600 run.py:483] Algo bellman_ford step 8556 current loss 0.022844, current_train_items 273824.
I0304 19:32:06.399629 22502662377600 run.py:483] Algo bellman_ford step 8557 current loss 0.036646, current_train_items 273856.
I0304 19:32:06.431476 22502662377600 run.py:483] Algo bellman_ford step 8558 current loss 0.101996, current_train_items 273888.
I0304 19:32:06.466087 22502662377600 run.py:483] Algo bellman_ford step 8559 current loss 0.107260, current_train_items 273920.
I0304 19:32:06.485883 22502662377600 run.py:483] Algo bellman_ford step 8560 current loss 0.002654, current_train_items 273952.
I0304 19:32:06.502550 22502662377600 run.py:483] Algo bellman_ford step 8561 current loss 0.029381, current_train_items 273984.
I0304 19:32:06.526322 22502662377600 run.py:483] Algo bellman_ford step 8562 current loss 0.048773, current_train_items 274016.
I0304 19:32:06.559188 22502662377600 run.py:483] Algo bellman_ford step 8563 current loss 0.081536, current_train_items 274048.
I0304 19:32:06.592354 22502662377600 run.py:483] Algo bellman_ford step 8564 current loss 0.070939, current_train_items 274080.
I0304 19:32:06.611797 22502662377600 run.py:483] Algo bellman_ford step 8565 current loss 0.003894, current_train_items 274112.
I0304 19:32:06.628549 22502662377600 run.py:483] Algo bellman_ford step 8566 current loss 0.048755, current_train_items 274144.
I0304 19:32:06.654059 22502662377600 run.py:483] Algo bellman_ford step 8567 current loss 0.072977, current_train_items 274176.
I0304 19:32:06.686097 22502662377600 run.py:483] Algo bellman_ford step 8568 current loss 0.091293, current_train_items 274208.
I0304 19:32:06.720273 22502662377600 run.py:483] Algo bellman_ford step 8569 current loss 0.089712, current_train_items 274240.
I0304 19:32:06.740125 22502662377600 run.py:483] Algo bellman_ford step 8570 current loss 0.008778, current_train_items 274272.
I0304 19:32:06.756484 22502662377600 run.py:483] Algo bellman_ford step 8571 current loss 0.043270, current_train_items 274304.
I0304 19:32:06.779870 22502662377600 run.py:483] Algo bellman_ford step 8572 current loss 0.060360, current_train_items 274336.
I0304 19:32:06.811605 22502662377600 run.py:483] Algo bellman_ford step 8573 current loss 0.073830, current_train_items 274368.
I0304 19:32:06.843917 22502662377600 run.py:483] Algo bellman_ford step 8574 current loss 0.083245, current_train_items 274400.
I0304 19:32:06.863728 22502662377600 run.py:483] Algo bellman_ford step 8575 current loss 0.003471, current_train_items 274432.
I0304 19:32:06.879885 22502662377600 run.py:483] Algo bellman_ford step 8576 current loss 0.021877, current_train_items 274464.
I0304 19:32:06.903975 22502662377600 run.py:483] Algo bellman_ford step 8577 current loss 0.043668, current_train_items 274496.
I0304 19:32:06.936258 22502662377600 run.py:483] Algo bellman_ford step 8578 current loss 0.055471, current_train_items 274528.
I0304 19:32:06.970575 22502662377600 run.py:483] Algo bellman_ford step 8579 current loss 0.088062, current_train_items 274560.
I0304 19:32:06.990099 22502662377600 run.py:483] Algo bellman_ford step 8580 current loss 0.008594, current_train_items 274592.
I0304 19:32:07.006168 22502662377600 run.py:483] Algo bellman_ford step 8581 current loss 0.006117, current_train_items 274624.
I0304 19:32:07.029214 22502662377600 run.py:483] Algo bellman_ford step 8582 current loss 0.034129, current_train_items 274656.
I0304 19:32:07.061125 22502662377600 run.py:483] Algo bellman_ford step 8583 current loss 0.050957, current_train_items 274688.
I0304 19:32:07.096156 22502662377600 run.py:483] Algo bellman_ford step 8584 current loss 0.080664, current_train_items 274720.
I0304 19:32:07.115897 22502662377600 run.py:483] Algo bellman_ford step 8585 current loss 0.002338, current_train_items 274752.
I0304 19:32:07.132190 22502662377600 run.py:483] Algo bellman_ford step 8586 current loss 0.013603, current_train_items 274784.
I0304 19:32:07.157152 22502662377600 run.py:483] Algo bellman_ford step 8587 current loss 0.065084, current_train_items 274816.
I0304 19:32:07.189283 22502662377600 run.py:483] Algo bellman_ford step 8588 current loss 0.033203, current_train_items 274848.
I0304 19:32:07.222127 22502662377600 run.py:483] Algo bellman_ford step 8589 current loss 0.044805, current_train_items 274880.
I0304 19:32:07.241788 22502662377600 run.py:483] Algo bellman_ford step 8590 current loss 0.083481, current_train_items 274912.
I0304 19:32:07.257172 22502662377600 run.py:483] Algo bellman_ford step 8591 current loss 0.030326, current_train_items 274944.
I0304 19:32:07.281247 22502662377600 run.py:483] Algo bellman_ford step 8592 current loss 0.057272, current_train_items 274976.
I0304 19:32:07.313414 22502662377600 run.py:483] Algo bellman_ford step 8593 current loss 0.080790, current_train_items 275008.
I0304 19:32:07.347060 22502662377600 run.py:483] Algo bellman_ford step 8594 current loss 0.149741, current_train_items 275040.
I0304 19:32:07.366308 22502662377600 run.py:483] Algo bellman_ford step 8595 current loss 0.020809, current_train_items 275072.
I0304 19:32:07.382349 22502662377600 run.py:483] Algo bellman_ford step 8596 current loss 0.024493, current_train_items 275104.
I0304 19:32:07.406378 22502662377600 run.py:483] Algo bellman_ford step 8597 current loss 0.099517, current_train_items 275136.
I0304 19:32:07.436953 22502662377600 run.py:483] Algo bellman_ford step 8598 current loss 0.027090, current_train_items 275168.
I0304 19:32:07.471479 22502662377600 run.py:483] Algo bellman_ford step 8599 current loss 0.167378, current_train_items 275200.
I0304 19:32:07.491280 22502662377600 run.py:483] Algo bellman_ford step 8600 current loss 0.020267, current_train_items 275232.
I0304 19:32:07.499165 22502662377600 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0304 19:32:07.499269 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:07.516231 22502662377600 run.py:483] Algo bellman_ford step 8601 current loss 0.036130, current_train_items 275264.
I0304 19:32:07.541740 22502662377600 run.py:483] Algo bellman_ford step 8602 current loss 0.064698, current_train_items 275296.
I0304 19:32:07.575501 22502662377600 run.py:483] Algo bellman_ford step 8603 current loss 0.052819, current_train_items 275328.
I0304 19:32:07.609883 22502662377600 run.py:483] Algo bellman_ford step 8604 current loss 0.081871, current_train_items 275360.
I0304 19:32:07.630147 22502662377600 run.py:483] Algo bellman_ford step 8605 current loss 0.008755, current_train_items 275392.
I0304 19:32:07.645403 22502662377600 run.py:483] Algo bellman_ford step 8606 current loss 0.023104, current_train_items 275424.
I0304 19:32:07.669467 22502662377600 run.py:483] Algo bellman_ford step 8607 current loss 0.064724, current_train_items 275456.
I0304 19:32:07.701647 22502662377600 run.py:483] Algo bellman_ford step 8608 current loss 0.059587, current_train_items 275488.
I0304 19:32:07.737309 22502662377600 run.py:483] Algo bellman_ford step 8609 current loss 0.086119, current_train_items 275520.
I0304 19:32:07.757518 22502662377600 run.py:483] Algo bellman_ford step 8610 current loss 0.040526, current_train_items 275552.
I0304 19:32:07.773753 22502662377600 run.py:483] Algo bellman_ford step 8611 current loss 0.014791, current_train_items 275584.
I0304 19:32:07.798349 22502662377600 run.py:483] Algo bellman_ford step 8612 current loss 0.063522, current_train_items 275616.
I0304 19:32:07.830734 22502662377600 run.py:483] Algo bellman_ford step 8613 current loss 0.086547, current_train_items 275648.
I0304 19:32:07.862962 22502662377600 run.py:483] Algo bellman_ford step 8614 current loss 0.046858, current_train_items 275680.
I0304 19:32:07.882667 22502662377600 run.py:483] Algo bellman_ford step 8615 current loss 0.005185, current_train_items 275712.
I0304 19:32:07.898695 22502662377600 run.py:483] Algo bellman_ford step 8616 current loss 0.022935, current_train_items 275744.
I0304 19:32:07.922882 22502662377600 run.py:483] Algo bellman_ford step 8617 current loss 0.052311, current_train_items 275776.
I0304 19:32:07.955746 22502662377600 run.py:483] Algo bellman_ford step 8618 current loss 0.049019, current_train_items 275808.
I0304 19:32:07.989518 22502662377600 run.py:483] Algo bellman_ford step 8619 current loss 0.084647, current_train_items 275840.
I0304 19:32:08.008980 22502662377600 run.py:483] Algo bellman_ford step 8620 current loss 0.002166, current_train_items 275872.
I0304 19:32:08.025095 22502662377600 run.py:483] Algo bellman_ford step 8621 current loss 0.019627, current_train_items 275904.
I0304 19:32:08.049850 22502662377600 run.py:483] Algo bellman_ford step 8622 current loss 0.051266, current_train_items 275936.
I0304 19:32:08.082865 22502662377600 run.py:483] Algo bellman_ford step 8623 current loss 0.033713, current_train_items 275968.
I0304 19:32:08.116499 22502662377600 run.py:483] Algo bellman_ford step 8624 current loss 0.044259, current_train_items 276000.
I0304 19:32:08.136295 22502662377600 run.py:483] Algo bellman_ford step 8625 current loss 0.008111, current_train_items 276032.
I0304 19:32:08.152431 22502662377600 run.py:483] Algo bellman_ford step 8626 current loss 0.019404, current_train_items 276064.
I0304 19:32:08.176812 22502662377600 run.py:483] Algo bellman_ford step 8627 current loss 0.044257, current_train_items 276096.
I0304 19:32:08.209952 22502662377600 run.py:483] Algo bellman_ford step 8628 current loss 0.062959, current_train_items 276128.
I0304 19:32:08.243224 22502662377600 run.py:483] Algo bellman_ford step 8629 current loss 0.087456, current_train_items 276160.
I0304 19:32:08.263315 22502662377600 run.py:483] Algo bellman_ford step 8630 current loss 0.004186, current_train_items 276192.
I0304 19:32:08.279606 22502662377600 run.py:483] Algo bellman_ford step 8631 current loss 0.030763, current_train_items 276224.
I0304 19:32:08.303620 22502662377600 run.py:483] Algo bellman_ford step 8632 current loss 0.049587, current_train_items 276256.
I0304 19:32:08.335576 22502662377600 run.py:483] Algo bellman_ford step 8633 current loss 0.042504, current_train_items 276288.
I0304 19:32:08.369810 22502662377600 run.py:483] Algo bellman_ford step 8634 current loss 0.089556, current_train_items 276320.
I0304 19:32:08.389873 22502662377600 run.py:483] Algo bellman_ford step 8635 current loss 0.028312, current_train_items 276352.
I0304 19:32:08.405950 22502662377600 run.py:483] Algo bellman_ford step 8636 current loss 0.009046, current_train_items 276384.
I0304 19:32:08.430783 22502662377600 run.py:483] Algo bellman_ford step 8637 current loss 0.055158, current_train_items 276416.
I0304 19:32:08.462285 22502662377600 run.py:483] Algo bellman_ford step 8638 current loss 0.044702, current_train_items 276448.
I0304 19:32:08.497069 22502662377600 run.py:483] Algo bellman_ford step 8639 current loss 0.069680, current_train_items 276480.
I0304 19:32:08.517003 22502662377600 run.py:483] Algo bellman_ford step 8640 current loss 0.024036, current_train_items 276512.
I0304 19:32:08.533477 22502662377600 run.py:483] Algo bellman_ford step 8641 current loss 0.071103, current_train_items 276544.
I0304 19:32:08.558210 22502662377600 run.py:483] Algo bellman_ford step 8642 current loss 0.097958, current_train_items 276576.
I0304 19:32:08.590962 22502662377600 run.py:483] Algo bellman_ford step 8643 current loss 0.145816, current_train_items 276608.
I0304 19:32:08.625544 22502662377600 run.py:483] Algo bellman_ford step 8644 current loss 0.113284, current_train_items 276640.
I0304 19:32:08.645123 22502662377600 run.py:483] Algo bellman_ford step 8645 current loss 0.005059, current_train_items 276672.
I0304 19:32:08.661217 22502662377600 run.py:483] Algo bellman_ford step 8646 current loss 0.038292, current_train_items 276704.
I0304 19:32:08.685539 22502662377600 run.py:483] Algo bellman_ford step 8647 current loss 0.049304, current_train_items 276736.
I0304 19:32:08.718770 22502662377600 run.py:483] Algo bellman_ford step 8648 current loss 0.085114, current_train_items 276768.
I0304 19:32:08.752131 22502662377600 run.py:483] Algo bellman_ford step 8649 current loss 0.078448, current_train_items 276800.
I0304 19:32:08.771509 22502662377600 run.py:483] Algo bellman_ford step 8650 current loss 0.002211, current_train_items 276832.
I0304 19:32:08.779962 22502662377600 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0304 19:32:08.780067 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:32:08.796999 22502662377600 run.py:483] Algo bellman_ford step 8651 current loss 0.013004, current_train_items 276864.
I0304 19:32:08.821811 22502662377600 run.py:483] Algo bellman_ford step 8652 current loss 0.063804, current_train_items 276896.
I0304 19:32:08.854490 22502662377600 run.py:483] Algo bellman_ford step 8653 current loss 0.052567, current_train_items 276928.
I0304 19:32:08.889854 22502662377600 run.py:483] Algo bellman_ford step 8654 current loss 0.070738, current_train_items 276960.
I0304 19:32:08.909945 22502662377600 run.py:483] Algo bellman_ford step 8655 current loss 0.002070, current_train_items 276992.
I0304 19:32:08.925709 22502662377600 run.py:483] Algo bellman_ford step 8656 current loss 0.009877, current_train_items 277024.
I0304 19:32:08.951371 22502662377600 run.py:483] Algo bellman_ford step 8657 current loss 0.057947, current_train_items 277056.
I0304 19:32:08.984058 22502662377600 run.py:483] Algo bellman_ford step 8658 current loss 0.080300, current_train_items 277088.
I0304 19:32:09.018029 22502662377600 run.py:483] Algo bellman_ford step 8659 current loss 0.078389, current_train_items 277120.
I0304 19:32:09.038131 22502662377600 run.py:483] Algo bellman_ford step 8660 current loss 0.002254, current_train_items 277152.
I0304 19:32:09.054117 22502662377600 run.py:483] Algo bellman_ford step 8661 current loss 0.013659, current_train_items 277184.
I0304 19:32:09.077943 22502662377600 run.py:483] Algo bellman_ford step 8662 current loss 0.042734, current_train_items 277216.
I0304 19:32:09.109825 22502662377600 run.py:483] Algo bellman_ford step 8663 current loss 0.188336, current_train_items 277248.
I0304 19:32:09.144925 22502662377600 run.py:483] Algo bellman_ford step 8664 current loss 0.155647, current_train_items 277280.
I0304 19:32:09.164454 22502662377600 run.py:483] Algo bellman_ford step 8665 current loss 0.003191, current_train_items 277312.
I0304 19:32:09.180165 22502662377600 run.py:483] Algo bellman_ford step 8666 current loss 0.022191, current_train_items 277344.
I0304 19:32:09.204378 22502662377600 run.py:483] Algo bellman_ford step 8667 current loss 0.026786, current_train_items 277376.
I0304 19:32:09.235893 22502662377600 run.py:483] Algo bellman_ford step 8668 current loss 0.034542, current_train_items 277408.
I0304 19:32:09.269079 22502662377600 run.py:483] Algo bellman_ford step 8669 current loss 0.036535, current_train_items 277440.
I0304 19:32:09.288863 22502662377600 run.py:483] Algo bellman_ford step 8670 current loss 0.004377, current_train_items 277472.
I0304 19:32:09.305294 22502662377600 run.py:483] Algo bellman_ford step 8671 current loss 0.020568, current_train_items 277504.
I0304 19:32:09.329401 22502662377600 run.py:483] Algo bellman_ford step 8672 current loss 0.033658, current_train_items 277536.
I0304 19:32:09.361471 22502662377600 run.py:483] Algo bellman_ford step 8673 current loss 0.065304, current_train_items 277568.
I0304 19:32:09.393319 22502662377600 run.py:483] Algo bellman_ford step 8674 current loss 0.085114, current_train_items 277600.
I0304 19:32:09.413229 22502662377600 run.py:483] Algo bellman_ford step 8675 current loss 0.004399, current_train_items 277632.
I0304 19:32:09.429316 22502662377600 run.py:483] Algo bellman_ford step 8676 current loss 0.019877, current_train_items 277664.
I0304 19:32:09.453386 22502662377600 run.py:483] Algo bellman_ford step 8677 current loss 0.075312, current_train_items 277696.
I0304 19:32:09.485508 22502662377600 run.py:483] Algo bellman_ford step 8678 current loss 0.069687, current_train_items 277728.
I0304 19:32:09.518699 22502662377600 run.py:483] Algo bellman_ford step 8679 current loss 0.053561, current_train_items 277760.
I0304 19:32:09.538376 22502662377600 run.py:483] Algo bellman_ford step 8680 current loss 0.004476, current_train_items 277792.
I0304 19:32:09.554669 22502662377600 run.py:483] Algo bellman_ford step 8681 current loss 0.017165, current_train_items 277824.
I0304 19:32:09.579457 22502662377600 run.py:483] Algo bellman_ford step 8682 current loss 0.062627, current_train_items 277856.
I0304 19:32:09.611328 22502662377600 run.py:483] Algo bellman_ford step 8683 current loss 0.078631, current_train_items 277888.
I0304 19:32:09.644652 22502662377600 run.py:483] Algo bellman_ford step 8684 current loss 0.098266, current_train_items 277920.
I0304 19:32:09.664665 22502662377600 run.py:483] Algo bellman_ford step 8685 current loss 0.002373, current_train_items 277952.
I0304 19:32:09.680616 22502662377600 run.py:483] Algo bellman_ford step 8686 current loss 0.040707, current_train_items 277984.
I0304 19:32:09.705212 22502662377600 run.py:483] Algo bellman_ford step 8687 current loss 0.101957, current_train_items 278016.
I0304 19:32:09.738014 22502662377600 run.py:483] Algo bellman_ford step 8688 current loss 0.094666, current_train_items 278048.
I0304 19:32:09.770992 22502662377600 run.py:483] Algo bellman_ford step 8689 current loss 0.083857, current_train_items 278080.
I0304 19:32:09.790551 22502662377600 run.py:483] Algo bellman_ford step 8690 current loss 0.002522, current_train_items 278112.
I0304 19:32:09.806810 22502662377600 run.py:483] Algo bellman_ford step 8691 current loss 0.053729, current_train_items 278144.
I0304 19:32:09.830521 22502662377600 run.py:483] Algo bellman_ford step 8692 current loss 0.053910, current_train_items 278176.
I0304 19:32:09.862591 22502662377600 run.py:483] Algo bellman_ford step 8693 current loss 0.085087, current_train_items 278208.
I0304 19:32:09.896353 22502662377600 run.py:483] Algo bellman_ford step 8694 current loss 0.127202, current_train_items 278240.
I0304 19:32:09.915873 22502662377600 run.py:483] Algo bellman_ford step 8695 current loss 0.003220, current_train_items 278272.
I0304 19:32:09.931944 22502662377600 run.py:483] Algo bellman_ford step 8696 current loss 0.020836, current_train_items 278304.
I0304 19:32:09.956255 22502662377600 run.py:483] Algo bellman_ford step 8697 current loss 0.075750, current_train_items 278336.
I0304 19:32:09.988666 22502662377600 run.py:483] Algo bellman_ford step 8698 current loss 0.043596, current_train_items 278368.
I0304 19:32:10.024426 22502662377600 run.py:483] Algo bellman_ford step 8699 current loss 0.138642, current_train_items 278400.
I0304 19:32:10.043988 22502662377600 run.py:483] Algo bellman_ford step 8700 current loss 0.003546, current_train_items 278432.
I0304 19:32:10.051717 22502662377600 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0304 19:32:10.051820 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:10.068593 22502662377600 run.py:483] Algo bellman_ford step 8701 current loss 0.019289, current_train_items 278464.
I0304 19:32:10.092881 22502662377600 run.py:483] Algo bellman_ford step 8702 current loss 0.029509, current_train_items 278496.
I0304 19:32:10.125885 22502662377600 run.py:483] Algo bellman_ford step 8703 current loss 0.106949, current_train_items 278528.
I0304 19:32:10.161810 22502662377600 run.py:483] Algo bellman_ford step 8704 current loss 0.102135, current_train_items 278560.
I0304 19:32:10.181732 22502662377600 run.py:483] Algo bellman_ford step 8705 current loss 0.012782, current_train_items 278592.
I0304 19:32:10.197293 22502662377600 run.py:483] Algo bellman_ford step 8706 current loss 0.051126, current_train_items 278624.
I0304 19:32:10.221333 22502662377600 run.py:483] Algo bellman_ford step 8707 current loss 0.069171, current_train_items 278656.
I0304 19:32:10.253990 22502662377600 run.py:483] Algo bellman_ford step 8708 current loss 0.157480, current_train_items 278688.
I0304 19:32:10.287639 22502662377600 run.py:483] Algo bellman_ford step 8709 current loss 0.167156, current_train_items 278720.
I0304 19:32:10.307067 22502662377600 run.py:483] Algo bellman_ford step 8710 current loss 0.022464, current_train_items 278752.
I0304 19:32:10.323269 22502662377600 run.py:483] Algo bellman_ford step 8711 current loss 0.023307, current_train_items 278784.
I0304 19:32:10.347340 22502662377600 run.py:483] Algo bellman_ford step 8712 current loss 0.124373, current_train_items 278816.
I0304 19:32:10.381398 22502662377600 run.py:483] Algo bellman_ford step 8713 current loss 0.095024, current_train_items 278848.
I0304 19:32:10.413980 22502662377600 run.py:483] Algo bellman_ford step 8714 current loss 0.099493, current_train_items 278880.
I0304 19:32:10.433333 22502662377600 run.py:483] Algo bellman_ford step 8715 current loss 0.010407, current_train_items 278912.
I0304 19:32:10.449610 22502662377600 run.py:483] Algo bellman_ford step 8716 current loss 0.044186, current_train_items 278944.
I0304 19:32:10.473234 22502662377600 run.py:483] Algo bellman_ford step 8717 current loss 0.027444, current_train_items 278976.
I0304 19:32:10.504612 22502662377600 run.py:483] Algo bellman_ford step 8718 current loss 0.044707, current_train_items 279008.
I0304 19:32:10.537280 22502662377600 run.py:483] Algo bellman_ford step 8719 current loss 0.082145, current_train_items 279040.
I0304 19:32:10.556737 22502662377600 run.py:483] Algo bellman_ford step 8720 current loss 0.005910, current_train_items 279072.
I0304 19:32:10.573232 22502662377600 run.py:483] Algo bellman_ford step 8721 current loss 0.023562, current_train_items 279104.
I0304 19:32:10.598047 22502662377600 run.py:483] Algo bellman_ford step 8722 current loss 0.055277, current_train_items 279136.
I0304 19:32:10.629343 22502662377600 run.py:483] Algo bellman_ford step 8723 current loss 0.061663, current_train_items 279168.
I0304 19:32:10.662397 22502662377600 run.py:483] Algo bellman_ford step 8724 current loss 0.081532, current_train_items 279200.
I0304 19:32:10.681894 22502662377600 run.py:483] Algo bellman_ford step 8725 current loss 0.009799, current_train_items 279232.
I0304 19:32:10.698099 22502662377600 run.py:483] Algo bellman_ford step 8726 current loss 0.034180, current_train_items 279264.
I0304 19:32:10.722326 22502662377600 run.py:483] Algo bellman_ford step 8727 current loss 0.052328, current_train_items 279296.
I0304 19:32:10.754055 22502662377600 run.py:483] Algo bellman_ford step 8728 current loss 0.063784, current_train_items 279328.
I0304 19:32:10.787271 22502662377600 run.py:483] Algo bellman_ford step 8729 current loss 0.068583, current_train_items 279360.
I0304 19:32:10.807144 22502662377600 run.py:483] Algo bellman_ford step 8730 current loss 0.007311, current_train_items 279392.
I0304 19:32:10.822818 22502662377600 run.py:483] Algo bellman_ford step 8731 current loss 0.007227, current_train_items 279424.
I0304 19:32:10.847077 22502662377600 run.py:483] Algo bellman_ford step 8732 current loss 0.080362, current_train_items 279456.
I0304 19:32:10.879197 22502662377600 run.py:483] Algo bellman_ford step 8733 current loss 0.064342, current_train_items 279488.
I0304 19:32:10.914218 22502662377600 run.py:483] Algo bellman_ford step 8734 current loss 0.066612, current_train_items 279520.
I0304 19:32:10.933345 22502662377600 run.py:483] Algo bellman_ford step 8735 current loss 0.014064, current_train_items 279552.
I0304 19:32:10.949285 22502662377600 run.py:483] Algo bellman_ford step 8736 current loss 0.015564, current_train_items 279584.
I0304 19:32:10.974375 22502662377600 run.py:483] Algo bellman_ford step 8737 current loss 0.130186, current_train_items 279616.
I0304 19:32:11.006250 22502662377600 run.py:483] Algo bellman_ford step 8738 current loss 0.110099, current_train_items 279648.
I0304 19:32:11.040693 22502662377600 run.py:483] Algo bellman_ford step 8739 current loss 0.119170, current_train_items 279680.
I0304 19:32:11.060230 22502662377600 run.py:483] Algo bellman_ford step 8740 current loss 0.012230, current_train_items 279712.
I0304 19:32:11.076131 22502662377600 run.py:483] Algo bellman_ford step 8741 current loss 0.014770, current_train_items 279744.
I0304 19:32:11.100580 22502662377600 run.py:483] Algo bellman_ford step 8742 current loss 0.084673, current_train_items 279776.
I0304 19:32:11.133099 22502662377600 run.py:483] Algo bellman_ford step 8743 current loss 0.107276, current_train_items 279808.
I0304 19:32:11.166882 22502662377600 run.py:483] Algo bellman_ford step 8744 current loss 0.084731, current_train_items 279840.
I0304 19:32:11.186478 22502662377600 run.py:483] Algo bellman_ford step 8745 current loss 0.048117, current_train_items 279872.
I0304 19:32:11.202821 22502662377600 run.py:483] Algo bellman_ford step 8746 current loss 0.049874, current_train_items 279904.
I0304 19:32:11.227048 22502662377600 run.py:483] Algo bellman_ford step 8747 current loss 0.043538, current_train_items 279936.
I0304 19:32:11.260246 22502662377600 run.py:483] Algo bellman_ford step 8748 current loss 0.054670, current_train_items 279968.
I0304 19:32:11.295161 22502662377600 run.py:483] Algo bellman_ford step 8749 current loss 0.060869, current_train_items 280000.
I0304 19:32:11.314527 22502662377600 run.py:483] Algo bellman_ford step 8750 current loss 0.003347, current_train_items 280032.
I0304 19:32:11.322653 22502662377600 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0304 19:32:11.322758 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:32:11.339539 22502662377600 run.py:483] Algo bellman_ford step 8751 current loss 0.019921, current_train_items 280064.
I0304 19:32:11.363629 22502662377600 run.py:483] Algo bellman_ford step 8752 current loss 0.048590, current_train_items 280096.
I0304 19:32:11.396112 22502662377600 run.py:483] Algo bellman_ford step 8753 current loss 0.045616, current_train_items 280128.
I0304 19:32:11.430731 22502662377600 run.py:483] Algo bellman_ford step 8754 current loss 0.053555, current_train_items 280160.
I0304 19:32:11.450932 22502662377600 run.py:483] Algo bellman_ford step 8755 current loss 0.009208, current_train_items 280192.
I0304 19:32:11.466714 22502662377600 run.py:483] Algo bellman_ford step 8756 current loss 0.019867, current_train_items 280224.
I0304 19:32:11.491866 22502662377600 run.py:483] Algo bellman_ford step 8757 current loss 0.167806, current_train_items 280256.
I0304 19:32:11.523366 22502662377600 run.py:483] Algo bellman_ford step 8758 current loss 0.088743, current_train_items 280288.
I0304 19:32:11.557315 22502662377600 run.py:483] Algo bellman_ford step 8759 current loss 0.131559, current_train_items 280320.
I0304 19:32:11.577127 22502662377600 run.py:483] Algo bellman_ford step 8760 current loss 0.007434, current_train_items 280352.
I0304 19:32:11.593354 22502662377600 run.py:483] Algo bellman_ford step 8761 current loss 0.024606, current_train_items 280384.
I0304 19:32:11.617537 22502662377600 run.py:483] Algo bellman_ford step 8762 current loss 0.038036, current_train_items 280416.
I0304 19:32:11.648704 22502662377600 run.py:483] Algo bellman_ford step 8763 current loss 0.048574, current_train_items 280448.
I0304 19:32:11.684071 22502662377600 run.py:483] Algo bellman_ford step 8764 current loss 0.116400, current_train_items 280480.
I0304 19:32:11.703430 22502662377600 run.py:483] Algo bellman_ford step 8765 current loss 0.040982, current_train_items 280512.
I0304 19:32:11.719941 22502662377600 run.py:483] Algo bellman_ford step 8766 current loss 0.016539, current_train_items 280544.
I0304 19:32:11.742795 22502662377600 run.py:483] Algo bellman_ford step 8767 current loss 0.022153, current_train_items 280576.
I0304 19:32:11.774763 22502662377600 run.py:483] Algo bellman_ford step 8768 current loss 0.076068, current_train_items 280608.
I0304 19:32:11.806696 22502662377600 run.py:483] Algo bellman_ford step 8769 current loss 0.066405, current_train_items 280640.
I0304 19:32:11.826516 22502662377600 run.py:483] Algo bellman_ford step 8770 current loss 0.007495, current_train_items 280672.
I0304 19:32:11.842990 22502662377600 run.py:483] Algo bellman_ford step 8771 current loss 0.022830, current_train_items 280704.
I0304 19:32:11.867300 22502662377600 run.py:483] Algo bellman_ford step 8772 current loss 0.048045, current_train_items 280736.
I0304 19:32:11.899572 22502662377600 run.py:483] Algo bellman_ford step 8773 current loss 0.072186, current_train_items 280768.
I0304 19:32:11.932829 22502662377600 run.py:483] Algo bellman_ford step 8774 current loss 0.069051, current_train_items 280800.
I0304 19:32:11.952496 22502662377600 run.py:483] Algo bellman_ford step 8775 current loss 0.011906, current_train_items 280832.
I0304 19:32:11.968824 22502662377600 run.py:483] Algo bellman_ford step 8776 current loss 0.027161, current_train_items 280864.
I0304 19:32:11.992009 22502662377600 run.py:483] Algo bellman_ford step 8777 current loss 0.089711, current_train_items 280896.
I0304 19:32:12.023897 22502662377600 run.py:483] Algo bellman_ford step 8778 current loss 0.065907, current_train_items 280928.
I0304 19:32:12.058493 22502662377600 run.py:483] Algo bellman_ford step 8779 current loss 0.051501, current_train_items 280960.
I0304 19:32:12.077821 22502662377600 run.py:483] Algo bellman_ford step 8780 current loss 0.005535, current_train_items 280992.
I0304 19:32:12.093794 22502662377600 run.py:483] Algo bellman_ford step 8781 current loss 0.073092, current_train_items 281024.
I0304 19:32:12.117766 22502662377600 run.py:483] Algo bellman_ford step 8782 current loss 0.073799, current_train_items 281056.
I0304 19:32:12.150788 22502662377600 run.py:483] Algo bellman_ford step 8783 current loss 0.077469, current_train_items 281088.
I0304 19:32:12.185193 22502662377600 run.py:483] Algo bellman_ford step 8784 current loss 0.092058, current_train_items 281120.
I0304 19:32:12.204862 22502662377600 run.py:483] Algo bellman_ford step 8785 current loss 0.005268, current_train_items 281152.
I0304 19:32:12.221050 22502662377600 run.py:483] Algo bellman_ford step 8786 current loss 0.044216, current_train_items 281184.
I0304 19:32:12.245348 22502662377600 run.py:483] Algo bellman_ford step 8787 current loss 0.059794, current_train_items 281216.
I0304 19:32:12.277219 22502662377600 run.py:483] Algo bellman_ford step 8788 current loss 0.059175, current_train_items 281248.
I0304 19:32:12.311324 22502662377600 run.py:483] Algo bellman_ford step 8789 current loss 0.050529, current_train_items 281280.
I0304 19:32:12.331115 22502662377600 run.py:483] Algo bellman_ford step 8790 current loss 0.026875, current_train_items 281312.
I0304 19:32:12.347655 22502662377600 run.py:483] Algo bellman_ford step 8791 current loss 0.017293, current_train_items 281344.
I0304 19:32:12.371240 22502662377600 run.py:483] Algo bellman_ford step 8792 current loss 0.047230, current_train_items 281376.
I0304 19:32:12.403647 22502662377600 run.py:483] Algo bellman_ford step 8793 current loss 0.059793, current_train_items 281408.
I0304 19:32:12.438352 22502662377600 run.py:483] Algo bellman_ford step 8794 current loss 0.131693, current_train_items 281440.
I0304 19:32:12.458219 22502662377600 run.py:483] Algo bellman_ford step 8795 current loss 0.007649, current_train_items 281472.
I0304 19:32:12.474342 22502662377600 run.py:483] Algo bellman_ford step 8796 current loss 0.026127, current_train_items 281504.
I0304 19:32:12.498065 22502662377600 run.py:483] Algo bellman_ford step 8797 current loss 0.072022, current_train_items 281536.
I0304 19:32:12.530379 22502662377600 run.py:483] Algo bellman_ford step 8798 current loss 0.069004, current_train_items 281568.
I0304 19:32:12.565651 22502662377600 run.py:483] Algo bellman_ford step 8799 current loss 0.123893, current_train_items 281600.
I0304 19:32:12.585342 22502662377600 run.py:483] Algo bellman_ford step 8800 current loss 0.005781, current_train_items 281632.
I0304 19:32:12.593478 22502662377600 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0304 19:32:12.593582 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0304 19:32:12.610193 22502662377600 run.py:483] Algo bellman_ford step 8801 current loss 0.020024, current_train_items 281664.
I0304 19:32:12.635130 22502662377600 run.py:483] Algo bellman_ford step 8802 current loss 0.060845, current_train_items 281696.
I0304 19:32:12.668681 22502662377600 run.py:483] Algo bellman_ford step 8803 current loss 0.082248, current_train_items 281728.
I0304 19:32:12.702897 22502662377600 run.py:483] Algo bellman_ford step 8804 current loss 0.041399, current_train_items 281760.
I0304 19:32:12.722845 22502662377600 run.py:483] Algo bellman_ford step 8805 current loss 0.004116, current_train_items 281792.
I0304 19:32:12.738546 22502662377600 run.py:483] Algo bellman_ford step 8806 current loss 0.020285, current_train_items 281824.
I0304 19:32:12.763226 22502662377600 run.py:483] Algo bellman_ford step 8807 current loss 0.062934, current_train_items 281856.
I0304 19:32:12.795141 22502662377600 run.py:483] Algo bellman_ford step 8808 current loss 0.040693, current_train_items 281888.
I0304 19:32:12.831373 22502662377600 run.py:483] Algo bellman_ford step 8809 current loss 0.078691, current_train_items 281920.
I0304 19:32:12.851108 22502662377600 run.py:483] Algo bellman_ford step 8810 current loss 0.002650, current_train_items 281952.
I0304 19:32:12.867459 22502662377600 run.py:483] Algo bellman_ford step 8811 current loss 0.012683, current_train_items 281984.
I0304 19:32:12.891483 22502662377600 run.py:483] Algo bellman_ford step 8812 current loss 0.049666, current_train_items 282016.
I0304 19:32:12.923915 22502662377600 run.py:483] Algo bellman_ford step 8813 current loss 0.053044, current_train_items 282048.
I0304 19:32:12.958002 22502662377600 run.py:483] Algo bellman_ford step 8814 current loss 0.086431, current_train_items 282080.
I0304 19:32:12.977945 22502662377600 run.py:483] Algo bellman_ford step 8815 current loss 0.003277, current_train_items 282112.
I0304 19:32:12.994181 22502662377600 run.py:483] Algo bellman_ford step 8816 current loss 0.018059, current_train_items 282144.
I0304 19:32:13.018331 22502662377600 run.py:483] Algo bellman_ford step 8817 current loss 0.028499, current_train_items 282176.
I0304 19:32:13.049831 22502662377600 run.py:483] Algo bellman_ford step 8818 current loss 0.049600, current_train_items 282208.
I0304 19:32:13.087437 22502662377600 run.py:483] Algo bellman_ford step 8819 current loss 0.075461, current_train_items 282240.
I0304 19:32:13.107400 22502662377600 run.py:483] Algo bellman_ford step 8820 current loss 0.004578, current_train_items 282272.
I0304 19:32:13.123512 22502662377600 run.py:483] Algo bellman_ford step 8821 current loss 0.016702, current_train_items 282304.
I0304 19:32:13.147788 22502662377600 run.py:483] Algo bellman_ford step 8822 current loss 0.088393, current_train_items 282336.
I0304 19:32:13.180841 22502662377600 run.py:483] Algo bellman_ford step 8823 current loss 0.116740, current_train_items 282368.
I0304 19:32:13.214195 22502662377600 run.py:483] Algo bellman_ford step 8824 current loss 0.144448, current_train_items 282400.
I0304 19:32:13.233876 22502662377600 run.py:483] Algo bellman_ford step 8825 current loss 0.003636, current_train_items 282432.
I0304 19:32:13.250157 22502662377600 run.py:483] Algo bellman_ford step 8826 current loss 0.026865, current_train_items 282464.
I0304 19:32:13.274586 22502662377600 run.py:483] Algo bellman_ford step 8827 current loss 0.074609, current_train_items 282496.
I0304 19:32:13.306648 22502662377600 run.py:483] Algo bellman_ford step 8828 current loss 0.085360, current_train_items 282528.
I0304 19:32:13.340320 22502662377600 run.py:483] Algo bellman_ford step 8829 current loss 0.051130, current_train_items 282560.
I0304 19:32:13.359987 22502662377600 run.py:483] Algo bellman_ford step 8830 current loss 0.003667, current_train_items 282592.
I0304 19:32:13.376115 22502662377600 run.py:483] Algo bellman_ford step 8831 current loss 0.024589, current_train_items 282624.
I0304 19:32:13.401038 22502662377600 run.py:483] Algo bellman_ford step 8832 current loss 0.084796, current_train_items 282656.
I0304 19:32:13.431772 22502662377600 run.py:483] Algo bellman_ford step 8833 current loss 0.061231, current_train_items 282688.
I0304 19:32:13.464593 22502662377600 run.py:483] Algo bellman_ford step 8834 current loss 0.102727, current_train_items 282720.
I0304 19:32:13.484592 22502662377600 run.py:483] Algo bellman_ford step 8835 current loss 0.014430, current_train_items 282752.
I0304 19:32:13.500530 22502662377600 run.py:483] Algo bellman_ford step 8836 current loss 0.043303, current_train_items 282784.
I0304 19:32:13.525748 22502662377600 run.py:483] Algo bellman_ford step 8837 current loss 0.088006, current_train_items 282816.
I0304 19:32:13.558135 22502662377600 run.py:483] Algo bellman_ford step 8838 current loss 0.159516, current_train_items 282848.
I0304 19:32:13.592105 22502662377600 run.py:483] Algo bellman_ford step 8839 current loss 0.163501, current_train_items 282880.
I0304 19:32:13.611832 22502662377600 run.py:483] Algo bellman_ford step 8840 current loss 0.008899, current_train_items 282912.
I0304 19:32:13.628216 22502662377600 run.py:483] Algo bellman_ford step 8841 current loss 0.019373, current_train_items 282944.
I0304 19:32:13.652986 22502662377600 run.py:483] Algo bellman_ford step 8842 current loss 0.081149, current_train_items 282976.
I0304 19:32:13.685073 22502662377600 run.py:483] Algo bellman_ford step 8843 current loss 0.101175, current_train_items 283008.
I0304 19:32:13.720746 22502662377600 run.py:483] Algo bellman_ford step 8844 current loss 0.076769, current_train_items 283040.
I0304 19:32:13.740661 22502662377600 run.py:483] Algo bellman_ford step 8845 current loss 0.045815, current_train_items 283072.
I0304 19:32:13.756668 22502662377600 run.py:483] Algo bellman_ford step 8846 current loss 0.076351, current_train_items 283104.
I0304 19:32:13.780994 22502662377600 run.py:483] Algo bellman_ford step 8847 current loss 0.110720, current_train_items 283136.
I0304 19:32:13.811904 22502662377600 run.py:483] Algo bellman_ford step 8848 current loss 0.109740, current_train_items 283168.
I0304 19:32:13.846413 22502662377600 run.py:483] Algo bellman_ford step 8849 current loss 0.145558, current_train_items 283200.
I0304 19:32:13.865947 22502662377600 run.py:483] Algo bellman_ford step 8850 current loss 0.005130, current_train_items 283232.
I0304 19:32:13.874019 22502662377600 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0304 19:32:13.874124 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:32:13.890835 22502662377600 run.py:483] Algo bellman_ford step 8851 current loss 0.030852, current_train_items 283264.
I0304 19:32:13.915902 22502662377600 run.py:483] Algo bellman_ford step 8852 current loss 0.092107, current_train_items 283296.
I0304 19:32:13.948771 22502662377600 run.py:483] Algo bellman_ford step 8853 current loss 0.122411, current_train_items 283328.
I0304 19:32:13.983119 22502662377600 run.py:483] Algo bellman_ford step 8854 current loss 0.099558, current_train_items 283360.
I0304 19:32:14.003207 22502662377600 run.py:483] Algo bellman_ford step 8855 current loss 0.006372, current_train_items 283392.
I0304 19:32:14.019180 22502662377600 run.py:483] Algo bellman_ford step 8856 current loss 0.018699, current_train_items 283424.
I0304 19:32:14.043200 22502662377600 run.py:483] Algo bellman_ford step 8857 current loss 0.041856, current_train_items 283456.
I0304 19:32:14.076291 22502662377600 run.py:483] Algo bellman_ford step 8858 current loss 0.067572, current_train_items 283488.
I0304 19:32:14.108367 22502662377600 run.py:483] Algo bellman_ford step 8859 current loss 0.067326, current_train_items 283520.
I0304 19:32:14.128257 22502662377600 run.py:483] Algo bellman_ford step 8860 current loss 0.006661, current_train_items 283552.
I0304 19:32:14.144789 22502662377600 run.py:483] Algo bellman_ford step 8861 current loss 0.017957, current_train_items 283584.
I0304 19:32:14.169281 22502662377600 run.py:483] Algo bellman_ford step 8862 current loss 0.053262, current_train_items 283616.
I0304 19:32:14.200401 22502662377600 run.py:483] Algo bellman_ford step 8863 current loss 0.055514, current_train_items 283648.
I0304 19:32:14.233440 22502662377600 run.py:483] Algo bellman_ford step 8864 current loss 0.115893, current_train_items 283680.
I0304 19:32:14.253290 22502662377600 run.py:483] Algo bellman_ford step 8865 current loss 0.011160, current_train_items 283712.
I0304 19:32:14.269382 22502662377600 run.py:483] Algo bellman_ford step 8866 current loss 0.035231, current_train_items 283744.
I0304 19:32:14.293028 22502662377600 run.py:483] Algo bellman_ford step 8867 current loss 0.065871, current_train_items 283776.
I0304 19:32:14.326067 22502662377600 run.py:483] Algo bellman_ford step 8868 current loss 0.181248, current_train_items 283808.
I0304 19:32:14.359537 22502662377600 run.py:483] Algo bellman_ford step 8869 current loss 0.081382, current_train_items 283840.
I0304 19:32:14.379737 22502662377600 run.py:483] Algo bellman_ford step 8870 current loss 0.018511, current_train_items 283872.
I0304 19:32:14.395929 22502662377600 run.py:483] Algo bellman_ford step 8871 current loss 0.012284, current_train_items 283904.
I0304 19:32:14.420643 22502662377600 run.py:483] Algo bellman_ford step 8872 current loss 0.038266, current_train_items 283936.
I0304 19:32:14.452286 22502662377600 run.py:483] Algo bellman_ford step 8873 current loss 0.058198, current_train_items 283968.
I0304 19:32:14.487552 22502662377600 run.py:483] Algo bellman_ford step 8874 current loss 0.161723, current_train_items 284000.
I0304 19:32:14.507775 22502662377600 run.py:483] Algo bellman_ford step 8875 current loss 0.005140, current_train_items 284032.
I0304 19:32:14.524049 22502662377600 run.py:483] Algo bellman_ford step 8876 current loss 0.019389, current_train_items 284064.
I0304 19:32:14.547272 22502662377600 run.py:483] Algo bellman_ford step 8877 current loss 0.048629, current_train_items 284096.
I0304 19:32:14.579086 22502662377600 run.py:483] Algo bellman_ford step 8878 current loss 0.042163, current_train_items 284128.
I0304 19:32:14.613608 22502662377600 run.py:483] Algo bellman_ford step 8879 current loss 0.110160, current_train_items 284160.
I0304 19:32:14.633421 22502662377600 run.py:483] Algo bellman_ford step 8880 current loss 0.006778, current_train_items 284192.
I0304 19:32:14.649459 22502662377600 run.py:483] Algo bellman_ford step 8881 current loss 0.047619, current_train_items 284224.
I0304 19:32:14.673031 22502662377600 run.py:483] Algo bellman_ford step 8882 current loss 0.069621, current_train_items 284256.
I0304 19:32:14.706091 22502662377600 run.py:483] Algo bellman_ford step 8883 current loss 0.059696, current_train_items 284288.
I0304 19:32:14.740207 22502662377600 run.py:483] Algo bellman_ford step 8884 current loss 0.080635, current_train_items 284320.
I0304 19:32:14.760213 22502662377600 run.py:483] Algo bellman_ford step 8885 current loss 0.006551, current_train_items 284352.
I0304 19:32:14.776846 22502662377600 run.py:483] Algo bellman_ford step 8886 current loss 0.012228, current_train_items 284384.
I0304 19:32:14.801313 22502662377600 run.py:483] Algo bellman_ford step 8887 current loss 0.102716, current_train_items 284416.
I0304 19:32:14.832563 22502662377600 run.py:483] Algo bellman_ford step 8888 current loss 0.092317, current_train_items 284448.
I0304 19:32:14.867203 22502662377600 run.py:483] Algo bellman_ford step 8889 current loss 0.143943, current_train_items 284480.
I0304 19:32:14.887134 22502662377600 run.py:483] Algo bellman_ford step 8890 current loss 0.004180, current_train_items 284512.
I0304 19:32:14.902884 22502662377600 run.py:483] Algo bellman_ford step 8891 current loss 0.028230, current_train_items 284544.
I0304 19:32:14.927181 22502662377600 run.py:483] Algo bellman_ford step 8892 current loss 0.042235, current_train_items 284576.
I0304 19:32:14.960877 22502662377600 run.py:483] Algo bellman_ford step 8893 current loss 0.112716, current_train_items 284608.
I0304 19:32:14.994510 22502662377600 run.py:483] Algo bellman_ford step 8894 current loss 0.086425, current_train_items 284640.
I0304 19:32:15.014296 22502662377600 run.py:483] Algo bellman_ford step 8895 current loss 0.005844, current_train_items 284672.
I0304 19:32:15.030279 22502662377600 run.py:483] Algo bellman_ford step 8896 current loss 0.031316, current_train_items 284704.
I0304 19:32:15.053567 22502662377600 run.py:483] Algo bellman_ford step 8897 current loss 0.040819, current_train_items 284736.
I0304 19:32:15.086638 22502662377600 run.py:483] Algo bellman_ford step 8898 current loss 0.041725, current_train_items 284768.
I0304 19:32:15.122591 22502662377600 run.py:483] Algo bellman_ford step 8899 current loss 0.068624, current_train_items 284800.
I0304 19:32:15.142901 22502662377600 run.py:483] Algo bellman_ford step 8900 current loss 0.002742, current_train_items 284832.
I0304 19:32:15.150812 22502662377600 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0304 19:32:15.150917 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:15.167474 22502662377600 run.py:483] Algo bellman_ford step 8901 current loss 0.015719, current_train_items 284864.
I0304 19:32:15.192199 22502662377600 run.py:483] Algo bellman_ford step 8902 current loss 0.062476, current_train_items 284896.
I0304 19:32:15.224068 22502662377600 run.py:483] Algo bellman_ford step 8903 current loss 0.065288, current_train_items 284928.
I0304 19:32:15.259694 22502662377600 run.py:483] Algo bellman_ford step 8904 current loss 0.078756, current_train_items 284960.
I0304 19:32:15.279745 22502662377600 run.py:483] Algo bellman_ford step 8905 current loss 0.004234, current_train_items 284992.
I0304 19:32:15.295650 22502662377600 run.py:483] Algo bellman_ford step 8906 current loss 0.028422, current_train_items 285024.
I0304 19:32:15.320097 22502662377600 run.py:483] Algo bellman_ford step 8907 current loss 0.136480, current_train_items 285056.
I0304 19:32:15.351711 22502662377600 run.py:483] Algo bellman_ford step 8908 current loss 0.073291, current_train_items 285088.
I0304 19:32:15.383993 22502662377600 run.py:483] Algo bellman_ford step 8909 current loss 0.048145, current_train_items 285120.
I0304 19:32:15.404029 22502662377600 run.py:483] Algo bellman_ford step 8910 current loss 0.005364, current_train_items 285152.
I0304 19:32:15.419990 22502662377600 run.py:483] Algo bellman_ford step 8911 current loss 0.036018, current_train_items 285184.
I0304 19:32:15.444668 22502662377600 run.py:483] Algo bellman_ford step 8912 current loss 0.070132, current_train_items 285216.
I0304 19:32:15.475706 22502662377600 run.py:483] Algo bellman_ford step 8913 current loss 0.100066, current_train_items 285248.
I0304 19:32:15.510668 22502662377600 run.py:483] Algo bellman_ford step 8914 current loss 0.079871, current_train_items 285280.
I0304 19:32:15.530454 22502662377600 run.py:483] Algo bellman_ford step 8915 current loss 0.004604, current_train_items 285312.
I0304 19:32:15.546244 22502662377600 run.py:483] Algo bellman_ford step 8916 current loss 0.010486, current_train_items 285344.
I0304 19:32:15.570850 22502662377600 run.py:483] Algo bellman_ford step 8917 current loss 0.041019, current_train_items 285376.
I0304 19:32:15.602717 22502662377600 run.py:483] Algo bellman_ford step 8918 current loss 0.047935, current_train_items 285408.
I0304 19:32:15.636364 22502662377600 run.py:483] Algo bellman_ford step 8919 current loss 0.100134, current_train_items 285440.
I0304 19:32:15.656271 22502662377600 run.py:483] Algo bellman_ford step 8920 current loss 0.007055, current_train_items 285472.
I0304 19:32:15.672472 22502662377600 run.py:483] Algo bellman_ford step 8921 current loss 0.045533, current_train_items 285504.
I0304 19:32:15.697376 22502662377600 run.py:483] Algo bellman_ford step 8922 current loss 0.037344, current_train_items 285536.
I0304 19:32:15.729242 22502662377600 run.py:483] Algo bellman_ford step 8923 current loss 0.034540, current_train_items 285568.
I0304 19:32:15.763668 22502662377600 run.py:483] Algo bellman_ford step 8924 current loss 0.129943, current_train_items 285600.
I0304 19:32:15.783562 22502662377600 run.py:483] Algo bellman_ford step 8925 current loss 0.005473, current_train_items 285632.
I0304 19:32:15.799200 22502662377600 run.py:483] Algo bellman_ford step 8926 current loss 0.048273, current_train_items 285664.
I0304 19:32:15.824104 22502662377600 run.py:483] Algo bellman_ford step 8927 current loss 0.046725, current_train_items 285696.
I0304 19:32:15.854973 22502662377600 run.py:483] Algo bellman_ford step 8928 current loss 0.028422, current_train_items 285728.
I0304 19:32:15.889159 22502662377600 run.py:483] Algo bellman_ford step 8929 current loss 0.066472, current_train_items 285760.
I0304 19:32:15.908964 22502662377600 run.py:483] Algo bellman_ford step 8930 current loss 0.003025, current_train_items 285792.
I0304 19:32:15.925623 22502662377600 run.py:483] Algo bellman_ford step 8931 current loss 0.037391, current_train_items 285824.
I0304 19:32:15.949811 22502662377600 run.py:483] Algo bellman_ford step 8932 current loss 0.054333, current_train_items 285856.
I0304 19:32:15.983605 22502662377600 run.py:483] Algo bellman_ford step 8933 current loss 0.095647, current_train_items 285888.
I0304 19:32:16.017996 22502662377600 run.py:483] Algo bellman_ford step 8934 current loss 0.067061, current_train_items 285920.
I0304 19:32:16.038026 22502662377600 run.py:483] Algo bellman_ford step 8935 current loss 0.003834, current_train_items 285952.
I0304 19:32:16.053848 22502662377600 run.py:483] Algo bellman_ford step 8936 current loss 0.023464, current_train_items 285984.
I0304 19:32:16.078292 22502662377600 run.py:483] Algo bellman_ford step 8937 current loss 0.134514, current_train_items 286016.
I0304 19:32:16.109838 22502662377600 run.py:483] Algo bellman_ford step 8938 current loss 0.080862, current_train_items 286048.
I0304 19:32:16.142666 22502662377600 run.py:483] Algo bellman_ford step 8939 current loss 0.108104, current_train_items 286080.
I0304 19:32:16.162532 22502662377600 run.py:483] Algo bellman_ford step 8940 current loss 0.008021, current_train_items 286112.
I0304 19:32:16.178617 22502662377600 run.py:483] Algo bellman_ford step 8941 current loss 0.010899, current_train_items 286144.
I0304 19:32:16.203120 22502662377600 run.py:483] Algo bellman_ford step 8942 current loss 0.053894, current_train_items 286176.
I0304 19:32:16.235934 22502662377600 run.py:483] Algo bellman_ford step 8943 current loss 0.083686, current_train_items 286208.
I0304 19:32:16.270275 22502662377600 run.py:483] Algo bellman_ford step 8944 current loss 0.087519, current_train_items 286240.
I0304 19:32:16.289808 22502662377600 run.py:483] Algo bellman_ford step 8945 current loss 0.017512, current_train_items 286272.
I0304 19:32:16.306070 22502662377600 run.py:483] Algo bellman_ford step 8946 current loss 0.026647, current_train_items 286304.
I0304 19:32:16.329625 22502662377600 run.py:483] Algo bellman_ford step 8947 current loss 0.071532, current_train_items 286336.
I0304 19:32:16.362361 22502662377600 run.py:483] Algo bellman_ford step 8948 current loss 0.085199, current_train_items 286368.
I0304 19:32:16.395798 22502662377600 run.py:483] Algo bellman_ford step 8949 current loss 0.071151, current_train_items 286400.
I0304 19:32:16.415873 22502662377600 run.py:483] Algo bellman_ford step 8950 current loss 0.002927, current_train_items 286432.
I0304 19:32:16.424078 22502662377600 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0304 19:32:16.424181 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:32:16.441124 22502662377600 run.py:483] Algo bellman_ford step 8951 current loss 0.021172, current_train_items 286464.
I0304 19:32:16.465058 22502662377600 run.py:483] Algo bellman_ford step 8952 current loss 0.059458, current_train_items 286496.
I0304 19:32:16.499644 22502662377600 run.py:483] Algo bellman_ford step 8953 current loss 0.107216, current_train_items 286528.
I0304 19:32:16.534995 22502662377600 run.py:483] Algo bellman_ford step 8954 current loss 0.067765, current_train_items 286560.
I0304 19:32:16.554832 22502662377600 run.py:483] Algo bellman_ford step 8955 current loss 0.003617, current_train_items 286592.
I0304 19:32:16.571104 22502662377600 run.py:483] Algo bellman_ford step 8956 current loss 0.021106, current_train_items 286624.
I0304 19:32:16.595592 22502662377600 run.py:483] Algo bellman_ford step 8957 current loss 0.050895, current_train_items 286656.
I0304 19:32:16.627021 22502662377600 run.py:483] Algo bellman_ford step 8958 current loss 0.049944, current_train_items 286688.
I0304 19:32:16.661662 22502662377600 run.py:483] Algo bellman_ford step 8959 current loss 0.105742, current_train_items 286720.
I0304 19:32:16.681628 22502662377600 run.py:483] Algo bellman_ford step 8960 current loss 0.004335, current_train_items 286752.
I0304 19:32:16.697404 22502662377600 run.py:483] Algo bellman_ford step 8961 current loss 0.030273, current_train_items 286784.
I0304 19:32:16.722347 22502662377600 run.py:483] Algo bellman_ford step 8962 current loss 0.085642, current_train_items 286816.
I0304 19:32:16.754396 22502662377600 run.py:483] Algo bellman_ford step 8963 current loss 0.040619, current_train_items 286848.
I0304 19:32:16.786729 22502662377600 run.py:483] Algo bellman_ford step 8964 current loss 0.065867, current_train_items 286880.
I0304 19:32:16.806280 22502662377600 run.py:483] Algo bellman_ford step 8965 current loss 0.004047, current_train_items 286912.
I0304 19:32:16.822702 22502662377600 run.py:483] Algo bellman_ford step 8966 current loss 0.010074, current_train_items 286944.
I0304 19:32:16.846835 22502662377600 run.py:483] Algo bellman_ford step 8967 current loss 0.068492, current_train_items 286976.
I0304 19:32:16.878942 22502662377600 run.py:483] Algo bellman_ford step 8968 current loss 0.207361, current_train_items 287008.
I0304 19:32:16.912491 22502662377600 run.py:483] Algo bellman_ford step 8969 current loss 0.116043, current_train_items 287040.
I0304 19:32:16.932314 22502662377600 run.py:483] Algo bellman_ford step 8970 current loss 0.004066, current_train_items 287072.
I0304 19:32:16.948433 22502662377600 run.py:483] Algo bellman_ford step 8971 current loss 0.011906, current_train_items 287104.
I0304 19:32:16.972200 22502662377600 run.py:483] Algo bellman_ford step 8972 current loss 0.054531, current_train_items 287136.
I0304 19:32:17.003396 22502662377600 run.py:483] Algo bellman_ford step 8973 current loss 0.039704, current_train_items 287168.
I0304 19:32:17.036596 22502662377600 run.py:483] Algo bellman_ford step 8974 current loss 0.115175, current_train_items 287200.
I0304 19:32:17.056533 22502662377600 run.py:483] Algo bellman_ford step 8975 current loss 0.032179, current_train_items 287232.
I0304 19:32:17.072341 22502662377600 run.py:483] Algo bellman_ford step 8976 current loss 0.013064, current_train_items 287264.
I0304 19:32:17.095112 22502662377600 run.py:483] Algo bellman_ford step 8977 current loss 0.022652, current_train_items 287296.
I0304 19:32:17.127274 22502662377600 run.py:483] Algo bellman_ford step 8978 current loss 0.043114, current_train_items 287328.
I0304 19:32:17.160910 22502662377600 run.py:483] Algo bellman_ford step 8979 current loss 0.066946, current_train_items 287360.
I0304 19:32:17.180619 22502662377600 run.py:483] Algo bellman_ford step 8980 current loss 0.004387, current_train_items 287392.
I0304 19:32:17.196892 22502662377600 run.py:483] Algo bellman_ford step 8981 current loss 0.018206, current_train_items 287424.
I0304 19:32:17.221167 22502662377600 run.py:483] Algo bellman_ford step 8982 current loss 0.035862, current_train_items 287456.
I0304 19:32:17.253333 22502662377600 run.py:483] Algo bellman_ford step 8983 current loss 0.045418, current_train_items 287488.
I0304 19:32:17.286854 22502662377600 run.py:483] Algo bellman_ford step 8984 current loss 0.076656, current_train_items 287520.
I0304 19:32:17.306648 22502662377600 run.py:483] Algo bellman_ford step 8985 current loss 0.069209, current_train_items 287552.
I0304 19:32:17.322874 22502662377600 run.py:483] Algo bellman_ford step 8986 current loss 0.027303, current_train_items 287584.
I0304 19:32:17.345763 22502662377600 run.py:483] Algo bellman_ford step 8987 current loss 0.040288, current_train_items 287616.
I0304 19:32:17.376244 22502662377600 run.py:483] Algo bellman_ford step 8988 current loss 0.043635, current_train_items 287648.
I0304 19:32:17.411689 22502662377600 run.py:483] Algo bellman_ford step 8989 current loss 0.092384, current_train_items 287680.
I0304 19:32:17.431386 22502662377600 run.py:483] Algo bellman_ford step 8990 current loss 0.027408, current_train_items 287712.
I0304 19:32:17.447248 22502662377600 run.py:483] Algo bellman_ford step 8991 current loss 0.022380, current_train_items 287744.
I0304 19:32:17.470499 22502662377600 run.py:483] Algo bellman_ford step 8992 current loss 0.053414, current_train_items 287776.
I0304 19:32:17.502023 22502662377600 run.py:483] Algo bellman_ford step 8993 current loss 0.061154, current_train_items 287808.
I0304 19:32:17.535256 22502662377600 run.py:483] Algo bellman_ford step 8994 current loss 0.095904, current_train_items 287840.
I0304 19:32:17.554368 22502662377600 run.py:483] Algo bellman_ford step 8995 current loss 0.034351, current_train_items 287872.
I0304 19:32:17.570470 22502662377600 run.py:483] Algo bellman_ford step 8996 current loss 0.026414, current_train_items 287904.
I0304 19:32:17.594856 22502662377600 run.py:483] Algo bellman_ford step 8997 current loss 0.083449, current_train_items 287936.
I0304 19:32:17.628134 22502662377600 run.py:483] Algo bellman_ford step 8998 current loss 0.061705, current_train_items 287968.
I0304 19:32:17.660604 22502662377600 run.py:483] Algo bellman_ford step 8999 current loss 0.036864, current_train_items 288000.
I0304 19:32:17.680535 22502662377600 run.py:483] Algo bellman_ford step 9000 current loss 0.004279, current_train_items 288032.
I0304 19:32:17.688564 22502662377600 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0304 19:32:17.688667 22502662377600 run.py:519] Checkpointing best model, best avg val score was 0.992, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:32:17.718249 22502662377600 run.py:483] Algo bellman_ford step 9001 current loss 0.010632, current_train_items 288064.
I0304 19:32:17.741929 22502662377600 run.py:483] Algo bellman_ford step 9002 current loss 0.060183, current_train_items 288096.
I0304 19:32:17.774196 22502662377600 run.py:483] Algo bellman_ford step 9003 current loss 0.112449, current_train_items 288128.
I0304 19:32:17.809273 22502662377600 run.py:483] Algo bellman_ford step 9004 current loss 0.072110, current_train_items 288160.
I0304 19:32:17.830169 22502662377600 run.py:483] Algo bellman_ford step 9005 current loss 0.003386, current_train_items 288192.
I0304 19:32:17.846258 22502662377600 run.py:483] Algo bellman_ford step 9006 current loss 0.012792, current_train_items 288224.
I0304 19:32:17.870110 22502662377600 run.py:483] Algo bellman_ford step 9007 current loss 0.033095, current_train_items 288256.
I0304 19:32:17.903034 22502662377600 run.py:483] Algo bellman_ford step 9008 current loss 0.071273, current_train_items 288288.
I0304 19:32:17.937427 22502662377600 run.py:483] Algo bellman_ford step 9009 current loss 0.068933, current_train_items 288320.
I0304 19:32:17.957185 22502662377600 run.py:483] Algo bellman_ford step 9010 current loss 0.095202, current_train_items 288352.
I0304 19:32:17.973319 22502662377600 run.py:483] Algo bellman_ford step 9011 current loss 0.023761, current_train_items 288384.
I0304 19:32:17.997378 22502662377600 run.py:483] Algo bellman_ford step 9012 current loss 0.031710, current_train_items 288416.
I0304 19:32:18.028567 22502662377600 run.py:483] Algo bellman_ford step 9013 current loss 0.086832, current_train_items 288448.
I0304 19:32:18.064430 22502662377600 run.py:483] Algo bellman_ford step 9014 current loss 0.112039, current_train_items 288480.
I0304 19:32:18.084225 22502662377600 run.py:483] Algo bellman_ford step 9015 current loss 0.004725, current_train_items 288512.
I0304 19:32:18.100568 22502662377600 run.py:483] Algo bellman_ford step 9016 current loss 0.044889, current_train_items 288544.
I0304 19:32:18.125730 22502662377600 run.py:483] Algo bellman_ford step 9017 current loss 0.062741, current_train_items 288576.
I0304 19:32:18.158127 22502662377600 run.py:483] Algo bellman_ford step 9018 current loss 0.135673, current_train_items 288608.
I0304 19:32:18.193708 22502662377600 run.py:483] Algo bellman_ford step 9019 current loss 0.120872, current_train_items 288640.
I0304 19:32:18.213200 22502662377600 run.py:483] Algo bellman_ford step 9020 current loss 0.002506, current_train_items 288672.
I0304 19:32:18.229204 22502662377600 run.py:483] Algo bellman_ford step 9021 current loss 0.022380, current_train_items 288704.
I0304 19:32:18.255005 22502662377600 run.py:483] Algo bellman_ford step 9022 current loss 0.032625, current_train_items 288736.
I0304 19:32:18.288274 22502662377600 run.py:483] Algo bellman_ford step 9023 current loss 0.078881, current_train_items 288768.
I0304 19:32:18.322403 22502662377600 run.py:483] Algo bellman_ford step 9024 current loss 0.099262, current_train_items 288800.
I0304 19:32:18.342367 22502662377600 run.py:483] Algo bellman_ford step 9025 current loss 0.005186, current_train_items 288832.
I0304 19:32:18.358268 22502662377600 run.py:483] Algo bellman_ford step 9026 current loss 0.026608, current_train_items 288864.
I0304 19:32:18.383157 22502662377600 run.py:483] Algo bellman_ford step 9027 current loss 0.062525, current_train_items 288896.
I0304 19:32:18.415680 22502662377600 run.py:483] Algo bellman_ford step 9028 current loss 0.065578, current_train_items 288928.
I0304 19:32:18.450033 22502662377600 run.py:483] Algo bellman_ford step 9029 current loss 0.132294, current_train_items 288960.
I0304 19:32:18.470109 22502662377600 run.py:483] Algo bellman_ford step 9030 current loss 0.003959, current_train_items 288992.
I0304 19:32:18.486392 22502662377600 run.py:483] Algo bellman_ford step 9031 current loss 0.102260, current_train_items 289024.
I0304 19:32:18.511558 22502662377600 run.py:483] Algo bellman_ford step 9032 current loss 0.071512, current_train_items 289056.
I0304 19:32:18.544016 22502662377600 run.py:483] Algo bellman_ford step 9033 current loss 0.060757, current_train_items 289088.
I0304 19:32:18.580761 22502662377600 run.py:483] Algo bellman_ford step 9034 current loss 0.135023, current_train_items 289120.
I0304 19:32:18.600733 22502662377600 run.py:483] Algo bellman_ford step 9035 current loss 0.004142, current_train_items 289152.
I0304 19:32:18.616512 22502662377600 run.py:483] Algo bellman_ford step 9036 current loss 0.031248, current_train_items 289184.
I0304 19:32:18.642004 22502662377600 run.py:483] Algo bellman_ford step 9037 current loss 0.063830, current_train_items 289216.
I0304 19:32:18.674450 22502662377600 run.py:483] Algo bellman_ford step 9038 current loss 0.081784, current_train_items 289248.
I0304 19:32:18.708457 22502662377600 run.py:483] Algo bellman_ford step 9039 current loss 0.215020, current_train_items 289280.
I0304 19:32:18.728197 22502662377600 run.py:483] Algo bellman_ford step 9040 current loss 0.013742, current_train_items 289312.
I0304 19:32:18.744544 22502662377600 run.py:483] Algo bellman_ford step 9041 current loss 0.042331, current_train_items 289344.
I0304 19:32:18.768354 22502662377600 run.py:483] Algo bellman_ford step 9042 current loss 0.062574, current_train_items 289376.
I0304 19:32:18.800417 22502662377600 run.py:483] Algo bellman_ford step 9043 current loss 0.069236, current_train_items 289408.
I0304 19:32:18.835323 22502662377600 run.py:483] Algo bellman_ford step 9044 current loss 0.106896, current_train_items 289440.
I0304 19:32:18.855323 22502662377600 run.py:483] Algo bellman_ford step 9045 current loss 0.010616, current_train_items 289472.
I0304 19:32:18.871725 22502662377600 run.py:483] Algo bellman_ford step 9046 current loss 0.039427, current_train_items 289504.
I0304 19:32:18.894875 22502662377600 run.py:483] Algo bellman_ford step 9047 current loss 0.043153, current_train_items 289536.
I0304 19:32:18.927346 22502662377600 run.py:483] Algo bellman_ford step 9048 current loss 0.131346, current_train_items 289568.
I0304 19:32:18.961001 22502662377600 run.py:483] Algo bellman_ford step 9049 current loss 0.073881, current_train_items 289600.
I0304 19:32:18.980407 22502662377600 run.py:483] Algo bellman_ford step 9050 current loss 0.012114, current_train_items 289632.
I0304 19:32:18.988528 22502662377600 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0304 19:32:18.988634 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:32:19.005265 22502662377600 run.py:483] Algo bellman_ford step 9051 current loss 0.030298, current_train_items 289664.
I0304 19:32:19.030293 22502662377600 run.py:483] Algo bellman_ford step 9052 current loss 0.104341, current_train_items 289696.
I0304 19:32:19.063152 22502662377600 run.py:483] Algo bellman_ford step 9053 current loss 0.129850, current_train_items 289728.
I0304 19:32:19.096107 22502662377600 run.py:483] Algo bellman_ford step 9054 current loss 0.074814, current_train_items 289760.
I0304 19:32:19.115881 22502662377600 run.py:483] Algo bellman_ford step 9055 current loss 0.005606, current_train_items 289792.
I0304 19:32:19.131565 22502662377600 run.py:483] Algo bellman_ford step 9056 current loss 0.065180, current_train_items 289824.
I0304 19:32:19.155424 22502662377600 run.py:483] Algo bellman_ford step 9057 current loss 0.058957, current_train_items 289856.
I0304 19:32:19.186292 22502662377600 run.py:483] Algo bellman_ford step 9058 current loss 0.059410, current_train_items 289888.
I0304 19:32:19.218458 22502662377600 run.py:483] Algo bellman_ford step 9059 current loss 0.048557, current_train_items 289920.
I0304 19:32:19.238165 22502662377600 run.py:483] Algo bellman_ford step 9060 current loss 0.012135, current_train_items 289952.
I0304 19:32:19.254959 22502662377600 run.py:483] Algo bellman_ford step 9061 current loss 0.012639, current_train_items 289984.
I0304 19:32:19.280023 22502662377600 run.py:483] Algo bellman_ford step 9062 current loss 0.053245, current_train_items 290016.
I0304 19:32:19.310597 22502662377600 run.py:483] Algo bellman_ford step 9063 current loss 0.055307, current_train_items 290048.
I0304 19:32:19.345390 22502662377600 run.py:483] Algo bellman_ford step 9064 current loss 0.077849, current_train_items 290080.
I0304 19:32:19.365122 22502662377600 run.py:483] Algo bellman_ford step 9065 current loss 0.003898, current_train_items 290112.
I0304 19:32:19.381186 22502662377600 run.py:483] Algo bellman_ford step 9066 current loss 0.029906, current_train_items 290144.
I0304 19:32:19.403564 22502662377600 run.py:483] Algo bellman_ford step 9067 current loss 0.049720, current_train_items 290176.
I0304 19:32:19.435653 22502662377600 run.py:483] Algo bellman_ford step 9068 current loss 0.048273, current_train_items 290208.
I0304 19:32:19.470323 22502662377600 run.py:483] Algo bellman_ford step 9069 current loss 0.085477, current_train_items 290240.
I0304 19:32:19.490039 22502662377600 run.py:483] Algo bellman_ford step 9070 current loss 0.003645, current_train_items 290272.
I0304 19:32:19.506494 22502662377600 run.py:483] Algo bellman_ford step 9071 current loss 0.054636, current_train_items 290304.
I0304 19:32:19.530966 22502662377600 run.py:483] Algo bellman_ford step 9072 current loss 0.093917, current_train_items 290336.
I0304 19:32:19.563265 22502662377600 run.py:483] Algo bellman_ford step 9073 current loss 0.079281, current_train_items 290368.
I0304 19:32:19.596723 22502662377600 run.py:483] Algo bellman_ford step 9074 current loss 0.063329, current_train_items 290400.
I0304 19:32:19.616298 22502662377600 run.py:483] Algo bellman_ford step 9075 current loss 0.017584, current_train_items 290432.
I0304 19:32:19.632632 22502662377600 run.py:483] Algo bellman_ford step 9076 current loss 0.028211, current_train_items 290464.
I0304 19:32:19.656561 22502662377600 run.py:483] Algo bellman_ford step 9077 current loss 0.103625, current_train_items 290496.
I0304 19:32:19.688320 22502662377600 run.py:483] Algo bellman_ford step 9078 current loss 0.093382, current_train_items 290528.
I0304 19:32:19.723201 22502662377600 run.py:483] Algo bellman_ford step 9079 current loss 0.152037, current_train_items 290560.
I0304 19:32:19.742827 22502662377600 run.py:483] Algo bellman_ford step 9080 current loss 0.005286, current_train_items 290592.
I0304 19:32:19.758826 22502662377600 run.py:483] Algo bellman_ford step 9081 current loss 0.036472, current_train_items 290624.
I0304 19:32:19.782814 22502662377600 run.py:483] Algo bellman_ford step 9082 current loss 0.022847, current_train_items 290656.
I0304 19:32:19.814082 22502662377600 run.py:483] Algo bellman_ford step 9083 current loss 0.048613, current_train_items 290688.
I0304 19:32:19.846997 22502662377600 run.py:483] Algo bellman_ford step 9084 current loss 0.060728, current_train_items 290720.
I0304 19:32:19.866745 22502662377600 run.py:483] Algo bellman_ford step 9085 current loss 0.006445, current_train_items 290752.
I0304 19:32:19.883045 22502662377600 run.py:483] Algo bellman_ford step 9086 current loss 0.020938, current_train_items 290784.
I0304 19:32:19.907418 22502662377600 run.py:483] Algo bellman_ford step 9087 current loss 0.051573, current_train_items 290816.
I0304 19:32:19.938848 22502662377600 run.py:483] Algo bellman_ford step 9088 current loss 0.028654, current_train_items 290848.
I0304 19:32:19.973970 22502662377600 run.py:483] Algo bellman_ford step 9089 current loss 0.101658, current_train_items 290880.
I0304 19:32:19.993523 22502662377600 run.py:483] Algo bellman_ford step 9090 current loss 0.002159, current_train_items 290912.
I0304 19:32:20.009562 22502662377600 run.py:483] Algo bellman_ford step 9091 current loss 0.021162, current_train_items 290944.
I0304 19:32:20.033385 22502662377600 run.py:483] Algo bellman_ford step 9092 current loss 0.041803, current_train_items 290976.
I0304 19:32:20.063450 22502662377600 run.py:483] Algo bellman_ford step 9093 current loss 0.051909, current_train_items 291008.
I0304 19:32:20.099825 22502662377600 run.py:483] Algo bellman_ford step 9094 current loss 0.165973, current_train_items 291040.
I0304 19:32:20.119744 22502662377600 run.py:483] Algo bellman_ford step 9095 current loss 0.011096, current_train_items 291072.
I0304 19:32:20.135879 22502662377600 run.py:483] Algo bellman_ford step 9096 current loss 0.019917, current_train_items 291104.
I0304 19:32:20.159831 22502662377600 run.py:483] Algo bellman_ford step 9097 current loss 0.069345, current_train_items 291136.
I0304 19:32:20.192132 22502662377600 run.py:483] Algo bellman_ford step 9098 current loss 0.065516, current_train_items 291168.
I0304 19:32:20.225156 22502662377600 run.py:483] Algo bellman_ford step 9099 current loss 0.062175, current_train_items 291200.
I0304 19:32:20.244915 22502662377600 run.py:483] Algo bellman_ford step 9100 current loss 0.003311, current_train_items 291232.
I0304 19:32:20.252943 22502662377600 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0304 19:32:20.253049 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:32:20.269868 22502662377600 run.py:483] Algo bellman_ford step 9101 current loss 0.011856, current_train_items 291264.
I0304 19:32:20.295169 22502662377600 run.py:483] Algo bellman_ford step 9102 current loss 0.062174, current_train_items 291296.
I0304 19:32:20.327540 22502662377600 run.py:483] Algo bellman_ford step 9103 current loss 0.034681, current_train_items 291328.
I0304 19:32:20.363353 22502662377600 run.py:483] Algo bellman_ford step 9104 current loss 0.060517, current_train_items 291360.
I0304 19:32:20.383267 22502662377600 run.py:483] Algo bellman_ford step 9105 current loss 0.052040, current_train_items 291392.
I0304 19:32:20.399308 22502662377600 run.py:483] Algo bellman_ford step 9106 current loss 0.025203, current_train_items 291424.
I0304 19:32:20.423933 22502662377600 run.py:483] Algo bellman_ford step 9107 current loss 0.054133, current_train_items 291456.
I0304 19:32:20.456140 22502662377600 run.py:483] Algo bellman_ford step 9108 current loss 0.050806, current_train_items 291488.
I0304 19:32:20.490206 22502662377600 run.py:483] Algo bellman_ford step 9109 current loss 0.096307, current_train_items 291520.
I0304 19:32:20.509808 22502662377600 run.py:483] Algo bellman_ford step 9110 current loss 0.002868, current_train_items 291552.
I0304 19:32:20.525752 22502662377600 run.py:483] Algo bellman_ford step 9111 current loss 0.035402, current_train_items 291584.
I0304 19:32:20.549305 22502662377600 run.py:483] Algo bellman_ford step 9112 current loss 0.059162, current_train_items 291616.
I0304 19:32:20.582570 22502662377600 run.py:483] Algo bellman_ford step 9113 current loss 0.069302, current_train_items 291648.
I0304 19:32:20.617448 22502662377600 run.py:483] Algo bellman_ford step 9114 current loss 0.096026, current_train_items 291680.
I0304 19:32:20.636982 22502662377600 run.py:483] Algo bellman_ford step 9115 current loss 0.035958, current_train_items 291712.
I0304 19:32:20.653255 22502662377600 run.py:483] Algo bellman_ford step 9116 current loss 0.012064, current_train_items 291744.
I0304 19:32:20.677094 22502662377600 run.py:483] Algo bellman_ford step 9117 current loss 0.060553, current_train_items 291776.
I0304 19:32:20.708531 22502662377600 run.py:483] Algo bellman_ford step 9118 current loss 0.045973, current_train_items 291808.
I0304 19:32:20.743578 22502662377600 run.py:483] Algo bellman_ford step 9119 current loss 0.082897, current_train_items 291840.
I0304 19:32:20.762980 22502662377600 run.py:483] Algo bellman_ford step 9120 current loss 0.003190, current_train_items 291872.
I0304 19:32:20.779049 22502662377600 run.py:483] Algo bellman_ford step 9121 current loss 0.062048, current_train_items 291904.
I0304 19:32:20.802812 22502662377600 run.py:483] Algo bellman_ford step 9122 current loss 0.047603, current_train_items 291936.
I0304 19:32:20.836368 22502662377600 run.py:483] Algo bellman_ford step 9123 current loss 0.093790, current_train_items 291968.
I0304 19:32:20.872119 22502662377600 run.py:483] Algo bellman_ford step 9124 current loss 0.094278, current_train_items 292000.
I0304 19:32:20.891605 22502662377600 run.py:483] Algo bellman_ford step 9125 current loss 0.013431, current_train_items 292032.
I0304 19:32:20.907568 22502662377600 run.py:483] Algo bellman_ford step 9126 current loss 0.029860, current_train_items 292064.
I0304 19:32:20.930707 22502662377600 run.py:483] Algo bellman_ford step 9127 current loss 0.049789, current_train_items 292096.
I0304 19:32:20.963588 22502662377600 run.py:483] Algo bellman_ford step 9128 current loss 0.106778, current_train_items 292128.
I0304 19:32:20.996901 22502662377600 run.py:483] Algo bellman_ford step 9129 current loss 0.087135, current_train_items 292160.
I0304 19:32:21.016591 22502662377600 run.py:483] Algo bellman_ford step 9130 current loss 0.005393, current_train_items 292192.
I0304 19:32:21.032621 22502662377600 run.py:483] Algo bellman_ford step 9131 current loss 0.062972, current_train_items 292224.
I0304 19:32:21.057444 22502662377600 run.py:483] Algo bellman_ford step 9132 current loss 0.093405, current_train_items 292256.
I0304 19:32:21.090247 22502662377600 run.py:483] Algo bellman_ford step 9133 current loss 0.094064, current_train_items 292288.
I0304 19:32:21.122778 22502662377600 run.py:483] Algo bellman_ford step 9134 current loss 0.080681, current_train_items 292320.
I0304 19:32:21.142244 22502662377600 run.py:483] Algo bellman_ford step 9135 current loss 0.003763, current_train_items 292352.
I0304 19:32:21.157866 22502662377600 run.py:483] Algo bellman_ford step 9136 current loss 0.019277, current_train_items 292384.
I0304 19:32:21.181157 22502662377600 run.py:483] Algo bellman_ford step 9137 current loss 0.046575, current_train_items 292416.
I0304 19:32:21.212797 22502662377600 run.py:483] Algo bellman_ford step 9138 current loss 0.079548, current_train_items 292448.
I0304 19:32:21.245418 22502662377600 run.py:483] Algo bellman_ford step 9139 current loss 0.076316, current_train_items 292480.
I0304 19:32:21.264867 22502662377600 run.py:483] Algo bellman_ford step 9140 current loss 0.004945, current_train_items 292512.
I0304 19:32:21.280865 22502662377600 run.py:483] Algo bellman_ford step 9141 current loss 0.018010, current_train_items 292544.
I0304 19:32:21.304549 22502662377600 run.py:483] Algo bellman_ford step 9142 current loss 0.097093, current_train_items 292576.
I0304 19:32:21.335870 22502662377600 run.py:483] Algo bellman_ford step 9143 current loss 0.099508, current_train_items 292608.
I0304 19:32:21.370619 22502662377600 run.py:483] Algo bellman_ford step 9144 current loss 0.097848, current_train_items 292640.
I0304 19:32:21.389986 22502662377600 run.py:483] Algo bellman_ford step 9145 current loss 0.007372, current_train_items 292672.
I0304 19:32:21.406104 22502662377600 run.py:483] Algo bellman_ford step 9146 current loss 0.043493, current_train_items 292704.
I0304 19:32:21.430431 22502662377600 run.py:483] Algo bellman_ford step 9147 current loss 0.053127, current_train_items 292736.
I0304 19:32:21.462443 22502662377600 run.py:483] Algo bellman_ford step 9148 current loss 0.087112, current_train_items 292768.
I0304 19:32:21.492846 22502662377600 run.py:483] Algo bellman_ford step 9149 current loss 0.057668, current_train_items 292800.
I0304 19:32:21.512164 22502662377600 run.py:483] Algo bellman_ford step 9150 current loss 0.005511, current_train_items 292832.
I0304 19:32:21.520367 22502662377600 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0304 19:32:21.520479 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:21.537250 22502662377600 run.py:483] Algo bellman_ford step 9151 current loss 0.020527, current_train_items 292864.
I0304 19:32:21.561815 22502662377600 run.py:483] Algo bellman_ford step 9152 current loss 0.049691, current_train_items 292896.
I0304 19:32:21.594440 22502662377600 run.py:483] Algo bellman_ford step 9153 current loss 0.093576, current_train_items 292928.
I0304 19:32:21.626712 22502662377600 run.py:483] Algo bellman_ford step 9154 current loss 0.083482, current_train_items 292960.
I0304 19:32:21.646790 22502662377600 run.py:483] Algo bellman_ford step 9155 current loss 0.007369, current_train_items 292992.
I0304 19:32:21.663222 22502662377600 run.py:483] Algo bellman_ford step 9156 current loss 0.030707, current_train_items 293024.
I0304 19:32:21.688176 22502662377600 run.py:483] Algo bellman_ford step 9157 current loss 0.065174, current_train_items 293056.
I0304 19:32:21.719935 22502662377600 run.py:483] Algo bellman_ford step 9158 current loss 0.076563, current_train_items 293088.
I0304 19:32:21.755061 22502662377600 run.py:483] Algo bellman_ford step 9159 current loss 0.123638, current_train_items 293120.
I0304 19:32:21.775152 22502662377600 run.py:483] Algo bellman_ford step 9160 current loss 0.008440, current_train_items 293152.
I0304 19:32:21.791584 22502662377600 run.py:483] Algo bellman_ford step 9161 current loss 0.022384, current_train_items 293184.
I0304 19:32:21.815571 22502662377600 run.py:483] Algo bellman_ford step 9162 current loss 0.038509, current_train_items 293216.
I0304 19:32:21.847307 22502662377600 run.py:483] Algo bellman_ford step 9163 current loss 0.051281, current_train_items 293248.
I0304 19:32:21.883120 22502662377600 run.py:483] Algo bellman_ford step 9164 current loss 0.082345, current_train_items 293280.
I0304 19:32:21.903060 22502662377600 run.py:483] Algo bellman_ford step 9165 current loss 0.004428, current_train_items 293312.
I0304 19:32:21.919296 22502662377600 run.py:483] Algo bellman_ford step 9166 current loss 0.033110, current_train_items 293344.
I0304 19:32:21.943554 22502662377600 run.py:483] Algo bellman_ford step 9167 current loss 0.047136, current_train_items 293376.
I0304 19:32:21.976364 22502662377600 run.py:483] Algo bellman_ford step 9168 current loss 0.065688, current_train_items 293408.
I0304 19:32:22.010037 22502662377600 run.py:483] Algo bellman_ford step 9169 current loss 0.053826, current_train_items 293440.
I0304 19:32:22.029894 22502662377600 run.py:483] Algo bellman_ford step 9170 current loss 0.003056, current_train_items 293472.
I0304 19:32:22.045922 22502662377600 run.py:483] Algo bellman_ford step 9171 current loss 0.015847, current_train_items 293504.
I0304 19:32:22.071515 22502662377600 run.py:483] Algo bellman_ford step 9172 current loss 0.104431, current_train_items 293536.
I0304 19:32:22.104051 22502662377600 run.py:483] Algo bellman_ford step 9173 current loss 0.046591, current_train_items 293568.
I0304 19:32:22.138922 22502662377600 run.py:483] Algo bellman_ford step 9174 current loss 0.100638, current_train_items 293600.
I0304 19:32:22.159074 22502662377600 run.py:483] Algo bellman_ford step 9175 current loss 0.016591, current_train_items 293632.
I0304 19:32:22.175715 22502662377600 run.py:483] Algo bellman_ford step 9176 current loss 0.009763, current_train_items 293664.
I0304 19:32:22.199197 22502662377600 run.py:483] Algo bellman_ford step 9177 current loss 0.056876, current_train_items 293696.
I0304 19:32:22.229741 22502662377600 run.py:483] Algo bellman_ford step 9178 current loss 0.101628, current_train_items 293728.
I0304 19:32:22.264874 22502662377600 run.py:483] Algo bellman_ford step 9179 current loss 0.155761, current_train_items 293760.
I0304 19:32:22.284592 22502662377600 run.py:483] Algo bellman_ford step 9180 current loss 0.002289, current_train_items 293792.
I0304 19:32:22.300742 22502662377600 run.py:483] Algo bellman_ford step 9181 current loss 0.045351, current_train_items 293824.
I0304 19:32:22.326107 22502662377600 run.py:483] Algo bellman_ford step 9182 current loss 0.156138, current_train_items 293856.
I0304 19:32:22.357201 22502662377600 run.py:483] Algo bellman_ford step 9183 current loss 0.116522, current_train_items 293888.
I0304 19:32:22.390691 22502662377600 run.py:483] Algo bellman_ford step 9184 current loss 0.184838, current_train_items 293920.
I0304 19:32:22.410587 22502662377600 run.py:483] Algo bellman_ford step 9185 current loss 0.023646, current_train_items 293952.
I0304 19:32:22.427052 22502662377600 run.py:483] Algo bellman_ford step 9186 current loss 0.027875, current_train_items 293984.
I0304 19:32:22.451420 22502662377600 run.py:483] Algo bellman_ford step 9187 current loss 0.049528, current_train_items 294016.
I0304 19:32:22.485121 22502662377600 run.py:483] Algo bellman_ford step 9188 current loss 0.079446, current_train_items 294048.
I0304 19:32:22.521154 22502662377600 run.py:483] Algo bellman_ford step 9189 current loss 0.068940, current_train_items 294080.
I0304 19:32:22.541132 22502662377600 run.py:483] Algo bellman_ford step 9190 current loss 0.005652, current_train_items 294112.
I0304 19:32:22.557158 22502662377600 run.py:483] Algo bellman_ford step 9191 current loss 0.064366, current_train_items 294144.
I0304 19:32:22.581416 22502662377600 run.py:483] Algo bellman_ford step 9192 current loss 0.073955, current_train_items 294176.
I0304 19:32:22.614068 22502662377600 run.py:483] Algo bellman_ford step 9193 current loss 0.083431, current_train_items 294208.
I0304 19:32:22.647124 22502662377600 run.py:483] Algo bellman_ford step 9194 current loss 0.102893, current_train_items 294240.
I0304 19:32:22.666815 22502662377600 run.py:483] Algo bellman_ford step 9195 current loss 0.009561, current_train_items 294272.
I0304 19:32:22.683500 22502662377600 run.py:483] Algo bellman_ford step 9196 current loss 0.031368, current_train_items 294304.
I0304 19:32:22.708563 22502662377600 run.py:483] Algo bellman_ford step 9197 current loss 0.081152, current_train_items 294336.
I0304 19:32:22.740517 22502662377600 run.py:483] Algo bellman_ford step 9198 current loss 0.030859, current_train_items 294368.
I0304 19:32:22.773302 22502662377600 run.py:483] Algo bellman_ford step 9199 current loss 0.072386, current_train_items 294400.
I0304 19:32:22.793529 22502662377600 run.py:483] Algo bellman_ford step 9200 current loss 0.004043, current_train_items 294432.
I0304 19:32:22.801360 22502662377600 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0304 19:32:22.801474 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:32:22.818415 22502662377600 run.py:483] Algo bellman_ford step 9201 current loss 0.020341, current_train_items 294464.
I0304 19:32:22.843167 22502662377600 run.py:483] Algo bellman_ford step 9202 current loss 0.051613, current_train_items 294496.
I0304 19:32:22.876977 22502662377600 run.py:483] Algo bellman_ford step 9203 current loss 0.062981, current_train_items 294528.
I0304 19:32:22.912026 22502662377600 run.py:483] Algo bellman_ford step 9204 current loss 0.055344, current_train_items 294560.
I0304 19:32:22.932036 22502662377600 run.py:483] Algo bellman_ford step 9205 current loss 0.020592, current_train_items 294592.
I0304 19:32:22.947954 22502662377600 run.py:483] Algo bellman_ford step 9206 current loss 0.018607, current_train_items 294624.
I0304 19:32:22.972132 22502662377600 run.py:483] Algo bellman_ford step 9207 current loss 0.029230, current_train_items 294656.
I0304 19:32:23.004343 22502662377600 run.py:483] Algo bellman_ford step 9208 current loss 0.080349, current_train_items 294688.
I0304 19:32:23.036015 22502662377600 run.py:483] Algo bellman_ford step 9209 current loss 0.117330, current_train_items 294720.
I0304 19:32:23.055423 22502662377600 run.py:483] Algo bellman_ford step 9210 current loss 0.003200, current_train_items 294752.
I0304 19:32:23.071950 22502662377600 run.py:483] Algo bellman_ford step 9211 current loss 0.024422, current_train_items 294784.
I0304 19:32:23.095890 22502662377600 run.py:483] Algo bellman_ford step 9212 current loss 0.053746, current_train_items 294816.
I0304 19:32:23.127883 22502662377600 run.py:483] Algo bellman_ford step 9213 current loss 0.054367, current_train_items 294848.
I0304 19:32:23.160705 22502662377600 run.py:483] Algo bellman_ford step 9214 current loss 0.106379, current_train_items 294880.
I0304 19:32:23.180111 22502662377600 run.py:483] Algo bellman_ford step 9215 current loss 0.032205, current_train_items 294912.
I0304 19:32:23.196346 22502662377600 run.py:483] Algo bellman_ford step 9216 current loss 0.025849, current_train_items 294944.
I0304 19:32:23.220169 22502662377600 run.py:483] Algo bellman_ford step 9217 current loss 0.049425, current_train_items 294976.
I0304 19:32:23.251501 22502662377600 run.py:483] Algo bellman_ford step 9218 current loss 0.079664, current_train_items 295008.
I0304 19:32:23.285207 22502662377600 run.py:483] Algo bellman_ford step 9219 current loss 0.058733, current_train_items 295040.
I0304 19:32:23.305028 22502662377600 run.py:483] Algo bellman_ford step 9220 current loss 0.067567, current_train_items 295072.
I0304 19:32:23.320919 22502662377600 run.py:483] Algo bellman_ford step 9221 current loss 0.015313, current_train_items 295104.
I0304 19:32:23.345610 22502662377600 run.py:483] Algo bellman_ford step 9222 current loss 0.053887, current_train_items 295136.
I0304 19:32:23.376767 22502662377600 run.py:483] Algo bellman_ford step 9223 current loss 0.085250, current_train_items 295168.
I0304 19:32:23.410372 22502662377600 run.py:483] Algo bellman_ford step 9224 current loss 0.078247, current_train_items 295200.
I0304 19:32:23.429874 22502662377600 run.py:483] Algo bellman_ford step 9225 current loss 0.005030, current_train_items 295232.
I0304 19:32:23.445888 22502662377600 run.py:483] Algo bellman_ford step 9226 current loss 0.018908, current_train_items 295264.
I0304 19:32:23.470698 22502662377600 run.py:483] Algo bellman_ford step 9227 current loss 0.089368, current_train_items 295296.
I0304 19:32:23.502747 22502662377600 run.py:483] Algo bellman_ford step 9228 current loss 0.122872, current_train_items 295328.
I0304 19:32:23.533961 22502662377600 run.py:483] Algo bellman_ford step 9229 current loss 0.081072, current_train_items 295360.
I0304 19:32:23.553306 22502662377600 run.py:483] Algo bellman_ford step 9230 current loss 0.004889, current_train_items 295392.
I0304 19:32:23.569710 22502662377600 run.py:483] Algo bellman_ford step 9231 current loss 0.051721, current_train_items 295424.
I0304 19:32:23.594137 22502662377600 run.py:483] Algo bellman_ford step 9232 current loss 0.074703, current_train_items 295456.
I0304 19:32:23.628221 22502662377600 run.py:483] Algo bellman_ford step 9233 current loss 0.184226, current_train_items 295488.
I0304 19:32:23.662032 22502662377600 run.py:483] Algo bellman_ford step 9234 current loss 0.097936, current_train_items 295520.
I0304 19:32:23.681257 22502662377600 run.py:483] Algo bellman_ford step 9235 current loss 0.009233, current_train_items 295552.
I0304 19:32:23.697503 22502662377600 run.py:483] Algo bellman_ford step 9236 current loss 0.025779, current_train_items 295584.
I0304 19:32:23.721773 22502662377600 run.py:483] Algo bellman_ford step 9237 current loss 0.033122, current_train_items 295616.
I0304 19:32:23.754956 22502662377600 run.py:483] Algo bellman_ford step 9238 current loss 0.064403, current_train_items 295648.
I0304 19:32:23.789399 22502662377600 run.py:483] Algo bellman_ford step 9239 current loss 0.061988, current_train_items 295680.
I0304 19:32:23.809276 22502662377600 run.py:483] Algo bellman_ford step 9240 current loss 0.007338, current_train_items 295712.
I0304 19:32:23.825786 22502662377600 run.py:483] Algo bellman_ford step 9241 current loss 0.055637, current_train_items 295744.
I0304 19:32:23.850628 22502662377600 run.py:483] Algo bellman_ford step 9242 current loss 0.074084, current_train_items 295776.
I0304 19:32:23.882625 22502662377600 run.py:483] Algo bellman_ford step 9243 current loss 0.042766, current_train_items 295808.
I0304 19:32:23.915587 22502662377600 run.py:483] Algo bellman_ford step 9244 current loss 0.072048, current_train_items 295840.
I0304 19:32:23.935338 22502662377600 run.py:483] Algo bellman_ford step 9245 current loss 0.006580, current_train_items 295872.
I0304 19:32:23.951564 22502662377600 run.py:483] Algo bellman_ford step 9246 current loss 0.028246, current_train_items 295904.
I0304 19:32:23.974903 22502662377600 run.py:483] Algo bellman_ford step 9247 current loss 0.025251, current_train_items 295936.
I0304 19:32:24.006787 22502662377600 run.py:483] Algo bellman_ford step 9248 current loss 0.066911, current_train_items 295968.
I0304 19:32:24.041199 22502662377600 run.py:483] Algo bellman_ford step 9249 current loss 0.062358, current_train_items 296000.
I0304 19:32:24.061409 22502662377600 run.py:483] Algo bellman_ford step 9250 current loss 0.004707, current_train_items 296032.
I0304 19:32:24.069600 22502662377600 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0304 19:32:24.069703 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:32:24.086255 22502662377600 run.py:483] Algo bellman_ford step 9251 current loss 0.013251, current_train_items 296064.
I0304 19:32:24.110992 22502662377600 run.py:483] Algo bellman_ford step 9252 current loss 0.051869, current_train_items 296096.
I0304 19:32:24.142973 22502662377600 run.py:483] Algo bellman_ford step 9253 current loss 0.058267, current_train_items 296128.
I0304 19:32:24.175278 22502662377600 run.py:483] Algo bellman_ford step 9254 current loss 0.054154, current_train_items 296160.
I0304 19:32:24.195208 22502662377600 run.py:483] Algo bellman_ford step 9255 current loss 0.006678, current_train_items 296192.
I0304 19:32:24.210953 22502662377600 run.py:483] Algo bellman_ford step 9256 current loss 0.013939, current_train_items 296224.
I0304 19:32:24.235184 22502662377600 run.py:483] Algo bellman_ford step 9257 current loss 0.042188, current_train_items 296256.
I0304 19:32:24.267814 22502662377600 run.py:483] Algo bellman_ford step 9258 current loss 0.104111, current_train_items 296288.
I0304 19:32:24.301919 22502662377600 run.py:483] Algo bellman_ford step 9259 current loss 0.075673, current_train_items 296320.
I0304 19:32:24.322171 22502662377600 run.py:483] Algo bellman_ford step 9260 current loss 0.007646, current_train_items 296352.
I0304 19:32:24.339129 22502662377600 run.py:483] Algo bellman_ford step 9261 current loss 0.027660, current_train_items 296384.
I0304 19:32:24.362133 22502662377600 run.py:483] Algo bellman_ford step 9262 current loss 0.040345, current_train_items 296416.
I0304 19:32:24.394398 22502662377600 run.py:483] Algo bellman_ford step 9263 current loss 0.081284, current_train_items 296448.
I0304 19:32:24.428209 22502662377600 run.py:483] Algo bellman_ford step 9264 current loss 0.068195, current_train_items 296480.
I0304 19:32:24.447999 22502662377600 run.py:483] Algo bellman_ford step 9265 current loss 0.017235, current_train_items 296512.
I0304 19:32:24.464175 22502662377600 run.py:483] Algo bellman_ford step 9266 current loss 0.009240, current_train_items 296544.
I0304 19:32:24.487143 22502662377600 run.py:483] Algo bellman_ford step 9267 current loss 0.039241, current_train_items 296576.
I0304 19:32:24.519383 22502662377600 run.py:483] Algo bellman_ford step 9268 current loss 0.052271, current_train_items 296608.
I0304 19:32:24.553062 22502662377600 run.py:483] Algo bellman_ford step 9269 current loss 0.114927, current_train_items 296640.
I0304 19:32:24.572849 22502662377600 run.py:483] Algo bellman_ford step 9270 current loss 0.004221, current_train_items 296672.
I0304 19:32:24.588935 22502662377600 run.py:483] Algo bellman_ford step 9271 current loss 0.025478, current_train_items 296704.
I0304 19:32:24.612810 22502662377600 run.py:483] Algo bellman_ford step 9272 current loss 0.048391, current_train_items 296736.
I0304 19:32:24.646245 22502662377600 run.py:483] Algo bellman_ford step 9273 current loss 0.076525, current_train_items 296768.
I0304 19:32:24.680715 22502662377600 run.py:483] Algo bellman_ford step 9274 current loss 0.068544, current_train_items 296800.
I0304 19:32:24.701039 22502662377600 run.py:483] Algo bellman_ford step 9275 current loss 0.011728, current_train_items 296832.
I0304 19:32:24.717310 22502662377600 run.py:483] Algo bellman_ford step 9276 current loss 0.018698, current_train_items 296864.
I0304 19:32:24.740888 22502662377600 run.py:483] Algo bellman_ford step 9277 current loss 0.055201, current_train_items 296896.
I0304 19:32:24.773302 22502662377600 run.py:483] Algo bellman_ford step 9278 current loss 0.063880, current_train_items 296928.
I0304 19:32:24.807332 22502662377600 run.py:483] Algo bellman_ford step 9279 current loss 0.137454, current_train_items 296960.
I0304 19:32:24.826831 22502662377600 run.py:483] Algo bellman_ford step 9280 current loss 0.003211, current_train_items 296992.
I0304 19:32:24.843119 22502662377600 run.py:483] Algo bellman_ford step 9281 current loss 0.037585, current_train_items 297024.
I0304 19:32:24.867177 22502662377600 run.py:483] Algo bellman_ford step 9282 current loss 0.029894, current_train_items 297056.
I0304 19:32:24.897664 22502662377600 run.py:483] Algo bellman_ford step 9283 current loss 0.021661, current_train_items 297088.
I0304 19:32:24.930579 22502662377600 run.py:483] Algo bellman_ford step 9284 current loss 0.083602, current_train_items 297120.
I0304 19:32:24.950453 22502662377600 run.py:483] Algo bellman_ford step 9285 current loss 0.009310, current_train_items 297152.
I0304 19:32:24.965999 22502662377600 run.py:483] Algo bellman_ford step 9286 current loss 0.010277, current_train_items 297184.
I0304 19:32:24.991378 22502662377600 run.py:483] Algo bellman_ford step 9287 current loss 0.068069, current_train_items 297216.
I0304 19:32:25.022698 22502662377600 run.py:483] Algo bellman_ford step 9288 current loss 0.049790, current_train_items 297248.
I0304 19:32:25.058234 22502662377600 run.py:483] Algo bellman_ford step 9289 current loss 0.071233, current_train_items 297280.
I0304 19:32:25.078458 22502662377600 run.py:483] Algo bellman_ford step 9290 current loss 0.008603, current_train_items 297312.
I0304 19:32:25.094595 22502662377600 run.py:483] Algo bellman_ford step 9291 current loss 0.022716, current_train_items 297344.
I0304 19:32:25.119323 22502662377600 run.py:483] Algo bellman_ford step 9292 current loss 0.044674, current_train_items 297376.
I0304 19:32:25.150606 22502662377600 run.py:483] Algo bellman_ford step 9293 current loss 0.063976, current_train_items 297408.
I0304 19:32:25.185370 22502662377600 run.py:483] Algo bellman_ford step 9294 current loss 0.140322, current_train_items 297440.
I0304 19:32:25.205312 22502662377600 run.py:483] Algo bellman_ford step 9295 current loss 0.007417, current_train_items 297472.
I0304 19:32:25.221516 22502662377600 run.py:483] Algo bellman_ford step 9296 current loss 0.011893, current_train_items 297504.
I0304 19:32:25.245612 22502662377600 run.py:483] Algo bellman_ford step 9297 current loss 0.093859, current_train_items 297536.
I0304 19:32:25.278173 22502662377600 run.py:483] Algo bellman_ford step 9298 current loss 0.052225, current_train_items 297568.
I0304 19:32:25.312595 22502662377600 run.py:483] Algo bellman_ford step 9299 current loss 0.098543, current_train_items 297600.
I0304 19:32:25.332590 22502662377600 run.py:483] Algo bellman_ford step 9300 current loss 0.004041, current_train_items 297632.
I0304 19:32:25.340733 22502662377600 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0304 19:32:25.340838 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:25.357758 22502662377600 run.py:483] Algo bellman_ford step 9301 current loss 0.021757, current_train_items 297664.
I0304 19:32:25.382630 22502662377600 run.py:483] Algo bellman_ford step 9302 current loss 0.041173, current_train_items 297696.
I0304 19:32:25.415502 22502662377600 run.py:483] Algo bellman_ford step 9303 current loss 0.069046, current_train_items 297728.
I0304 19:32:25.450804 22502662377600 run.py:483] Algo bellman_ford step 9304 current loss 0.130014, current_train_items 297760.
I0304 19:32:25.470995 22502662377600 run.py:483] Algo bellman_ford step 9305 current loss 0.002594, current_train_items 297792.
I0304 19:32:25.486749 22502662377600 run.py:483] Algo bellman_ford step 9306 current loss 0.018019, current_train_items 297824.
I0304 19:32:25.511568 22502662377600 run.py:483] Algo bellman_ford step 9307 current loss 0.038919, current_train_items 297856.
I0304 19:32:25.543864 22502662377600 run.py:483] Algo bellman_ford step 9308 current loss 0.091393, current_train_items 297888.
I0304 19:32:25.577638 22502662377600 run.py:483] Algo bellman_ford step 9309 current loss 0.090531, current_train_items 297920.
I0304 19:32:25.597344 22502662377600 run.py:483] Algo bellman_ford step 9310 current loss 0.003936, current_train_items 297952.
I0304 19:32:25.613472 22502662377600 run.py:483] Algo bellman_ford step 9311 current loss 0.008946, current_train_items 297984.
I0304 19:32:25.637768 22502662377600 run.py:483] Algo bellman_ford step 9312 current loss 0.044855, current_train_items 298016.
I0304 19:32:25.669173 22502662377600 run.py:483] Algo bellman_ford step 9313 current loss 0.051504, current_train_items 298048.
I0304 19:32:25.703490 22502662377600 run.py:483] Algo bellman_ford step 9314 current loss 0.065616, current_train_items 298080.
I0304 19:32:25.723371 22502662377600 run.py:483] Algo bellman_ford step 9315 current loss 0.006821, current_train_items 298112.
I0304 19:32:25.739321 22502662377600 run.py:483] Algo bellman_ford step 9316 current loss 0.007375, current_train_items 298144.
I0304 19:32:25.763741 22502662377600 run.py:483] Algo bellman_ford step 9317 current loss 0.062151, current_train_items 298176.
I0304 19:32:25.796845 22502662377600 run.py:483] Algo bellman_ford step 9318 current loss 0.056502, current_train_items 298208.
I0304 19:32:25.831257 22502662377600 run.py:483] Algo bellman_ford step 9319 current loss 0.062455, current_train_items 298240.
I0304 19:32:25.850950 22502662377600 run.py:483] Algo bellman_ford step 9320 current loss 0.008443, current_train_items 298272.
I0304 19:32:25.867126 22502662377600 run.py:483] Algo bellman_ford step 9321 current loss 0.022222, current_train_items 298304.
I0304 19:32:25.889975 22502662377600 run.py:483] Algo bellman_ford step 9322 current loss 0.043135, current_train_items 298336.
I0304 19:32:25.921240 22502662377600 run.py:483] Algo bellman_ford step 9323 current loss 0.042785, current_train_items 298368.
I0304 19:32:25.958029 22502662377600 run.py:483] Algo bellman_ford step 9324 current loss 0.090496, current_train_items 298400.
I0304 19:32:25.977864 22502662377600 run.py:483] Algo bellman_ford step 9325 current loss 0.002626, current_train_items 298432.
I0304 19:32:25.993890 22502662377600 run.py:483] Algo bellman_ford step 9326 current loss 0.022896, current_train_items 298464.
I0304 19:32:26.018617 22502662377600 run.py:483] Algo bellman_ford step 9327 current loss 0.045325, current_train_items 298496.
I0304 19:32:26.051152 22502662377600 run.py:483] Algo bellman_ford step 9328 current loss 0.088282, current_train_items 298528.
I0304 19:32:26.082587 22502662377600 run.py:483] Algo bellman_ford step 9329 current loss 0.044300, current_train_items 298560.
I0304 19:32:26.102394 22502662377600 run.py:483] Algo bellman_ford step 9330 current loss 0.003591, current_train_items 298592.
I0304 19:32:26.118661 22502662377600 run.py:483] Algo bellman_ford step 9331 current loss 0.028037, current_train_items 298624.
I0304 19:32:26.142320 22502662377600 run.py:483] Algo bellman_ford step 9332 current loss 0.076053, current_train_items 298656.
I0304 19:32:26.174721 22502662377600 run.py:483] Algo bellman_ford step 9333 current loss 0.074621, current_train_items 298688.
I0304 19:32:26.207995 22502662377600 run.py:483] Algo bellman_ford step 9334 current loss 0.147691, current_train_items 298720.
I0304 19:32:26.227594 22502662377600 run.py:483] Algo bellman_ford step 9335 current loss 0.004398, current_train_items 298752.
I0304 19:32:26.243845 22502662377600 run.py:483] Algo bellman_ford step 9336 current loss 0.056680, current_train_items 298784.
I0304 19:32:26.268795 22502662377600 run.py:483] Algo bellman_ford step 9337 current loss 0.085362, current_train_items 298816.
I0304 19:32:26.301000 22502662377600 run.py:483] Algo bellman_ford step 9338 current loss 0.065612, current_train_items 298848.
I0304 19:32:26.335505 22502662377600 run.py:483] Algo bellman_ford step 9339 current loss 0.087356, current_train_items 298880.
I0304 19:32:26.355497 22502662377600 run.py:483] Algo bellman_ford step 9340 current loss 0.010429, current_train_items 298912.
I0304 19:32:26.371256 22502662377600 run.py:483] Algo bellman_ford step 9341 current loss 0.022592, current_train_items 298944.
I0304 19:32:26.396200 22502662377600 run.py:483] Algo bellman_ford step 9342 current loss 0.052398, current_train_items 298976.
I0304 19:32:26.428549 22502662377600 run.py:483] Algo bellman_ford step 9343 current loss 0.107713, current_train_items 299008.
I0304 19:32:26.463134 22502662377600 run.py:483] Algo bellman_ford step 9344 current loss 0.081856, current_train_items 299040.
I0304 19:32:26.482898 22502662377600 run.py:483] Algo bellman_ford step 9345 current loss 0.005018, current_train_items 299072.
I0304 19:32:26.499085 22502662377600 run.py:483] Algo bellman_ford step 9346 current loss 0.011990, current_train_items 299104.
I0304 19:32:26.523438 22502662377600 run.py:483] Algo bellman_ford step 9347 current loss 0.044860, current_train_items 299136.
I0304 19:32:26.555939 22502662377600 run.py:483] Algo bellman_ford step 9348 current loss 0.055742, current_train_items 299168.
I0304 19:32:26.588740 22502662377600 run.py:483] Algo bellman_ford step 9349 current loss 0.070401, current_train_items 299200.
I0304 19:32:26.608517 22502662377600 run.py:483] Algo bellman_ford step 9350 current loss 0.041311, current_train_items 299232.
I0304 19:32:26.616712 22502662377600 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0304 19:32:26.616817 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:32:26.633418 22502662377600 run.py:483] Algo bellman_ford step 9351 current loss 0.007168, current_train_items 299264.
I0304 19:32:26.659485 22502662377600 run.py:483] Algo bellman_ford step 9352 current loss 0.039010, current_train_items 299296.
I0304 19:32:26.692327 22502662377600 run.py:483] Algo bellman_ford step 9353 current loss 0.064253, current_train_items 299328.
I0304 19:32:26.728120 22502662377600 run.py:483] Algo bellman_ford step 9354 current loss 0.120909, current_train_items 299360.
I0304 19:32:26.747970 22502662377600 run.py:483] Algo bellman_ford step 9355 current loss 0.044554, current_train_items 299392.
I0304 19:32:26.763673 22502662377600 run.py:483] Algo bellman_ford step 9356 current loss 0.023209, current_train_items 299424.
I0304 19:32:26.788045 22502662377600 run.py:483] Algo bellman_ford step 9357 current loss 0.038929, current_train_items 299456.
I0304 19:32:26.820414 22502662377600 run.py:483] Algo bellman_ford step 9358 current loss 0.043046, current_train_items 299488.
I0304 19:32:26.853926 22502662377600 run.py:483] Algo bellman_ford step 9359 current loss 0.046823, current_train_items 299520.
I0304 19:32:26.874209 22502662377600 run.py:483] Algo bellman_ford step 9360 current loss 0.024436, current_train_items 299552.
I0304 19:32:26.890414 22502662377600 run.py:483] Algo bellman_ford step 9361 current loss 0.041528, current_train_items 299584.
I0304 19:32:26.913866 22502662377600 run.py:483] Algo bellman_ford step 9362 current loss 0.042205, current_train_items 299616.
I0304 19:32:26.946618 22502662377600 run.py:483] Algo bellman_ford step 9363 current loss 0.066552, current_train_items 299648.
I0304 19:32:26.981314 22502662377600 run.py:483] Algo bellman_ford step 9364 current loss 0.097121, current_train_items 299680.
I0304 19:32:27.000789 22502662377600 run.py:483] Algo bellman_ford step 9365 current loss 0.015010, current_train_items 299712.
I0304 19:32:27.017345 22502662377600 run.py:483] Algo bellman_ford step 9366 current loss 0.039156, current_train_items 299744.
I0304 19:32:27.043126 22502662377600 run.py:483] Algo bellman_ford step 9367 current loss 0.081159, current_train_items 299776.
I0304 19:32:27.075956 22502662377600 run.py:483] Algo bellman_ford step 9368 current loss 0.071255, current_train_items 299808.
I0304 19:32:27.109159 22502662377600 run.py:483] Algo bellman_ford step 9369 current loss 0.055264, current_train_items 299840.
I0304 19:32:27.129304 22502662377600 run.py:483] Algo bellman_ford step 9370 current loss 0.050344, current_train_items 299872.
I0304 19:32:27.145989 22502662377600 run.py:483] Algo bellman_ford step 9371 current loss 0.037624, current_train_items 299904.
I0304 19:32:27.170057 22502662377600 run.py:483] Algo bellman_ford step 9372 current loss 0.057712, current_train_items 299936.
I0304 19:32:27.203116 22502662377600 run.py:483] Algo bellman_ford step 9373 current loss 0.079955, current_train_items 299968.
I0304 19:32:27.237184 22502662377600 run.py:483] Algo bellman_ford step 9374 current loss 0.056740, current_train_items 300000.
I0304 19:32:27.256931 22502662377600 run.py:483] Algo bellman_ford step 9375 current loss 0.011288, current_train_items 300032.
I0304 19:32:27.272808 22502662377600 run.py:483] Algo bellman_ford step 9376 current loss 0.056890, current_train_items 300064.
I0304 19:32:27.296453 22502662377600 run.py:483] Algo bellman_ford step 9377 current loss 0.056822, current_train_items 300096.
I0304 19:32:27.327636 22502662377600 run.py:483] Algo bellman_ford step 9378 current loss 0.051963, current_train_items 300128.
I0304 19:32:27.361515 22502662377600 run.py:483] Algo bellman_ford step 9379 current loss 0.074191, current_train_items 300160.
I0304 19:32:27.380954 22502662377600 run.py:483] Algo bellman_ford step 9380 current loss 0.007329, current_train_items 300192.
I0304 19:32:27.396632 22502662377600 run.py:483] Algo bellman_ford step 9381 current loss 0.026862, current_train_items 300224.
I0304 19:32:27.421003 22502662377600 run.py:483] Algo bellman_ford step 9382 current loss 0.098312, current_train_items 300256.
I0304 19:32:27.452218 22502662377600 run.py:483] Algo bellman_ford step 9383 current loss 0.189203, current_train_items 300288.
I0304 19:32:27.486113 22502662377600 run.py:483] Algo bellman_ford step 9384 current loss 0.153349, current_train_items 300320.
I0304 19:32:27.506009 22502662377600 run.py:483] Algo bellman_ford step 9385 current loss 0.017604, current_train_items 300352.
I0304 19:32:27.522653 22502662377600 run.py:483] Algo bellman_ford step 9386 current loss 0.068179, current_train_items 300384.
I0304 19:32:27.546682 22502662377600 run.py:483] Algo bellman_ford step 9387 current loss 0.083697, current_train_items 300416.
I0304 19:32:27.578377 22502662377600 run.py:483] Algo bellman_ford step 9388 current loss 0.087216, current_train_items 300448.
I0304 19:32:27.614052 22502662377600 run.py:483] Algo bellman_ford step 9389 current loss 0.082662, current_train_items 300480.
I0304 19:32:27.634091 22502662377600 run.py:483] Algo bellman_ford step 9390 current loss 0.020555, current_train_items 300512.
I0304 19:32:27.650572 22502662377600 run.py:483] Algo bellman_ford step 9391 current loss 0.014584, current_train_items 300544.
I0304 19:32:27.674526 22502662377600 run.py:483] Algo bellman_ford step 9392 current loss 0.023685, current_train_items 300576.
I0304 19:32:27.706426 22502662377600 run.py:483] Algo bellman_ford step 9393 current loss 0.062558, current_train_items 300608.
I0304 19:32:27.740331 22502662377600 run.py:483] Algo bellman_ford step 9394 current loss 0.130270, current_train_items 300640.
I0304 19:32:27.760309 22502662377600 run.py:483] Algo bellman_ford step 9395 current loss 0.029438, current_train_items 300672.
I0304 19:32:27.776853 22502662377600 run.py:483] Algo bellman_ford step 9396 current loss 0.047340, current_train_items 300704.
I0304 19:32:27.802296 22502662377600 run.py:483] Algo bellman_ford step 9397 current loss 0.060855, current_train_items 300736.
I0304 19:32:27.834111 22502662377600 run.py:483] Algo bellman_ford step 9398 current loss 0.066500, current_train_items 300768.
I0304 19:32:27.868505 22502662377600 run.py:483] Algo bellman_ford step 9399 current loss 0.121944, current_train_items 300800.
I0304 19:32:27.888921 22502662377600 run.py:483] Algo bellman_ford step 9400 current loss 0.003195, current_train_items 300832.
I0304 19:32:27.896878 22502662377600 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0304 19:32:27.896983 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:27.913990 22502662377600 run.py:483] Algo bellman_ford step 9401 current loss 0.017392, current_train_items 300864.
I0304 19:32:27.939874 22502662377600 run.py:483] Algo bellman_ford step 9402 current loss 0.058696, current_train_items 300896.
I0304 19:32:27.971837 22502662377600 run.py:483] Algo bellman_ford step 9403 current loss 0.032452, current_train_items 300928.
I0304 19:32:28.006406 22502662377600 run.py:483] Algo bellman_ford step 9404 current loss 0.102289, current_train_items 300960.
I0304 19:32:28.025919 22502662377600 run.py:483] Algo bellman_ford step 9405 current loss 0.001910, current_train_items 300992.
I0304 19:32:28.041510 22502662377600 run.py:483] Algo bellman_ford step 9406 current loss 0.017643, current_train_items 301024.
I0304 19:32:28.067128 22502662377600 run.py:483] Algo bellman_ford step 9407 current loss 0.048486, current_train_items 301056.
I0304 19:32:28.098600 22502662377600 run.py:483] Algo bellman_ford step 9408 current loss 0.036876, current_train_items 301088.
I0304 19:32:28.133312 22502662377600 run.py:483] Algo bellman_ford step 9409 current loss 0.119118, current_train_items 301120.
I0304 19:32:28.152776 22502662377600 run.py:483] Algo bellman_ford step 9410 current loss 0.016439, current_train_items 301152.
I0304 19:32:28.168714 22502662377600 run.py:483] Algo bellman_ford step 9411 current loss 0.012396, current_train_items 301184.
I0304 19:32:28.193059 22502662377600 run.py:483] Algo bellman_ford step 9412 current loss 0.028346, current_train_items 301216.
I0304 19:32:28.224932 22502662377600 run.py:483] Algo bellman_ford step 9413 current loss 0.024847, current_train_items 301248.
I0304 19:32:28.258313 22502662377600 run.py:483] Algo bellman_ford step 9414 current loss 0.071923, current_train_items 301280.
I0304 19:32:28.278111 22502662377600 run.py:483] Algo bellman_ford step 9415 current loss 0.006877, current_train_items 301312.
I0304 19:32:28.294353 22502662377600 run.py:483] Algo bellman_ford step 9416 current loss 0.014234, current_train_items 301344.
I0304 19:32:28.319046 22502662377600 run.py:483] Algo bellman_ford step 9417 current loss 0.065685, current_train_items 301376.
I0304 19:32:28.350340 22502662377600 run.py:483] Algo bellman_ford step 9418 current loss 0.037529, current_train_items 301408.
I0304 19:32:28.382022 22502662377600 run.py:483] Algo bellman_ford step 9419 current loss 0.034698, current_train_items 301440.
I0304 19:32:28.401260 22502662377600 run.py:483] Algo bellman_ford step 9420 current loss 0.027569, current_train_items 301472.
I0304 19:32:28.417597 22502662377600 run.py:483] Algo bellman_ford step 9421 current loss 0.038034, current_train_items 301504.
I0304 19:32:28.442053 22502662377600 run.py:483] Algo bellman_ford step 9422 current loss 0.033810, current_train_items 301536.
I0304 19:32:28.473689 22502662377600 run.py:483] Algo bellman_ford step 9423 current loss 0.038025, current_train_items 301568.
I0304 19:32:28.507489 22502662377600 run.py:483] Algo bellman_ford step 9424 current loss 0.064692, current_train_items 301600.
I0304 19:32:28.527032 22502662377600 run.py:483] Algo bellman_ford step 9425 current loss 0.015393, current_train_items 301632.
I0304 19:32:28.542728 22502662377600 run.py:483] Algo bellman_ford step 9426 current loss 0.018421, current_train_items 301664.
I0304 19:32:28.566335 22502662377600 run.py:483] Algo bellman_ford step 9427 current loss 0.041474, current_train_items 301696.
I0304 19:32:28.599238 22502662377600 run.py:483] Algo bellman_ford step 9428 current loss 0.073076, current_train_items 301728.
I0304 19:32:28.630705 22502662377600 run.py:483] Algo bellman_ford step 9429 current loss 0.182074, current_train_items 301760.
I0304 19:32:28.650105 22502662377600 run.py:483] Algo bellman_ford step 9430 current loss 0.009119, current_train_items 301792.
I0304 19:32:28.666203 22502662377600 run.py:483] Algo bellman_ford step 9431 current loss 0.031300, current_train_items 301824.
I0304 19:32:28.689986 22502662377600 run.py:483] Algo bellman_ford step 9432 current loss 0.061260, current_train_items 301856.
I0304 19:32:28.721511 22502662377600 run.py:483] Algo bellman_ford step 9433 current loss 0.046112, current_train_items 301888.
I0304 19:32:28.756011 22502662377600 run.py:483] Algo bellman_ford step 9434 current loss 0.075426, current_train_items 301920.
I0304 19:32:28.775447 22502662377600 run.py:483] Algo bellman_ford step 9435 current loss 0.003322, current_train_items 301952.
I0304 19:32:28.791665 22502662377600 run.py:483] Algo bellman_ford step 9436 current loss 0.039822, current_train_items 301984.
I0304 19:32:28.815709 22502662377600 run.py:483] Algo bellman_ford step 9437 current loss 0.035802, current_train_items 302016.
I0304 19:32:28.848344 22502662377600 run.py:483] Algo bellman_ford step 9438 current loss 0.050659, current_train_items 302048.
I0304 19:32:28.881517 22502662377600 run.py:483] Algo bellman_ford step 9439 current loss 0.106106, current_train_items 302080.
I0304 19:32:28.901510 22502662377600 run.py:483] Algo bellman_ford step 9440 current loss 0.051024, current_train_items 302112.
I0304 19:32:28.917998 22502662377600 run.py:483] Algo bellman_ford step 9441 current loss 0.084864, current_train_items 302144.
I0304 19:32:28.942009 22502662377600 run.py:483] Algo bellman_ford step 9442 current loss 0.125782, current_train_items 302176.
I0304 19:32:28.974635 22502662377600 run.py:483] Algo bellman_ford step 9443 current loss 0.070900, current_train_items 302208.
I0304 19:32:29.010605 22502662377600 run.py:483] Algo bellman_ford step 9444 current loss 0.126058, current_train_items 302240.
I0304 19:32:29.030033 22502662377600 run.py:483] Algo bellman_ford step 9445 current loss 0.095800, current_train_items 302272.
I0304 19:32:29.045714 22502662377600 run.py:483] Algo bellman_ford step 9446 current loss 0.010072, current_train_items 302304.
I0304 19:32:29.069354 22502662377600 run.py:483] Algo bellman_ford step 9447 current loss 0.049415, current_train_items 302336.
I0304 19:32:29.100795 22502662377600 run.py:483] Algo bellman_ford step 9448 current loss 0.086694, current_train_items 302368.
I0304 19:32:29.132836 22502662377600 run.py:483] Algo bellman_ford step 9449 current loss 0.160596, current_train_items 302400.
I0304 19:32:29.152404 22502662377600 run.py:483] Algo bellman_ford step 9450 current loss 0.007196, current_train_items 302432.
I0304 19:32:29.160515 22502662377600 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0304 19:32:29.160619 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:29.177304 22502662377600 run.py:483] Algo bellman_ford step 9451 current loss 0.016301, current_train_items 302464.
I0304 19:32:29.201025 22502662377600 run.py:483] Algo bellman_ford step 9452 current loss 0.039193, current_train_items 302496.
I0304 19:32:29.233407 22502662377600 run.py:483] Algo bellman_ford step 9453 current loss 0.056803, current_train_items 302528.
I0304 19:32:29.267841 22502662377600 run.py:483] Algo bellman_ford step 9454 current loss 0.075368, current_train_items 302560.
I0304 19:32:29.288035 22502662377600 run.py:483] Algo bellman_ford step 9455 current loss 0.002285, current_train_items 302592.
I0304 19:32:29.303876 22502662377600 run.py:483] Algo bellman_ford step 9456 current loss 0.009110, current_train_items 302624.
I0304 19:32:29.328081 22502662377600 run.py:483] Algo bellman_ford step 9457 current loss 0.043983, current_train_items 302656.
I0304 19:32:29.359824 22502662377600 run.py:483] Algo bellman_ford step 9458 current loss 0.058685, current_train_items 302688.
I0304 19:32:29.392023 22502662377600 run.py:483] Algo bellman_ford step 9459 current loss 0.042135, current_train_items 302720.
I0304 19:32:29.412006 22502662377600 run.py:483] Algo bellman_ford step 9460 current loss 0.014482, current_train_items 302752.
I0304 19:32:29.428506 22502662377600 run.py:483] Algo bellman_ford step 9461 current loss 0.035434, current_train_items 302784.
I0304 19:32:29.451974 22502662377600 run.py:483] Algo bellman_ford step 9462 current loss 0.023789, current_train_items 302816.
I0304 19:32:29.485152 22502662377600 run.py:483] Algo bellman_ford step 9463 current loss 0.058623, current_train_items 302848.
I0304 19:32:29.518561 22502662377600 run.py:483] Algo bellman_ford step 9464 current loss 0.048441, current_train_items 302880.
I0304 19:32:29.538113 22502662377600 run.py:483] Algo bellman_ford step 9465 current loss 0.004533, current_train_items 302912.
I0304 19:32:29.554671 22502662377600 run.py:483] Algo bellman_ford step 9466 current loss 0.036371, current_train_items 302944.
I0304 19:32:29.580289 22502662377600 run.py:483] Algo bellman_ford step 9467 current loss 0.057535, current_train_items 302976.
I0304 19:32:29.612995 22502662377600 run.py:483] Algo bellman_ford step 9468 current loss 0.046876, current_train_items 303008.
I0304 19:32:29.647221 22502662377600 run.py:483] Algo bellman_ford step 9469 current loss 0.076653, current_train_items 303040.
I0304 19:32:29.667143 22502662377600 run.py:483] Algo bellman_ford step 9470 current loss 0.005722, current_train_items 303072.
I0304 19:32:29.683000 22502662377600 run.py:483] Algo bellman_ford step 9471 current loss 0.007866, current_train_items 303104.
I0304 19:32:29.706652 22502662377600 run.py:483] Algo bellman_ford step 9472 current loss 0.031703, current_train_items 303136.
I0304 19:32:29.738825 22502662377600 run.py:483] Algo bellman_ford step 9473 current loss 0.046802, current_train_items 303168.
I0304 19:32:29.771484 22502662377600 run.py:483] Algo bellman_ford step 9474 current loss 0.050308, current_train_items 303200.
I0304 19:32:29.791657 22502662377600 run.py:483] Algo bellman_ford step 9475 current loss 0.003332, current_train_items 303232.
I0304 19:32:29.808029 22502662377600 run.py:483] Algo bellman_ford step 9476 current loss 0.030324, current_train_items 303264.
I0304 19:32:29.831657 22502662377600 run.py:483] Algo bellman_ford step 9477 current loss 0.026182, current_train_items 303296.
I0304 19:32:29.862790 22502662377600 run.py:483] Algo bellman_ford step 9478 current loss 0.086309, current_train_items 303328.
I0304 19:32:29.896084 22502662377600 run.py:483] Algo bellman_ford step 9479 current loss 0.136034, current_train_items 303360.
I0304 19:32:29.915458 22502662377600 run.py:483] Algo bellman_ford step 9480 current loss 0.005564, current_train_items 303392.
I0304 19:32:29.931406 22502662377600 run.py:483] Algo bellman_ford step 9481 current loss 0.031490, current_train_items 303424.
I0304 19:32:29.956782 22502662377600 run.py:483] Algo bellman_ford step 9482 current loss 0.092470, current_train_items 303456.
I0304 19:32:29.987193 22502662377600 run.py:483] Algo bellman_ford step 9483 current loss 0.064977, current_train_items 303488.
I0304 19:32:30.019603 22502662377600 run.py:483] Algo bellman_ford step 9484 current loss 0.067085, current_train_items 303520.
I0304 19:32:30.039244 22502662377600 run.py:483] Algo bellman_ford step 9485 current loss 0.002153, current_train_items 303552.
I0304 19:32:30.055726 22502662377600 run.py:483] Algo bellman_ford step 9486 current loss 0.010028, current_train_items 303584.
I0304 19:32:30.080606 22502662377600 run.py:483] Algo bellman_ford step 9487 current loss 0.067136, current_train_items 303616.
I0304 19:32:30.113408 22502662377600 run.py:483] Algo bellman_ford step 9488 current loss 0.069589, current_train_items 303648.
I0304 19:32:30.145785 22502662377600 run.py:483] Algo bellman_ford step 9489 current loss 0.053046, current_train_items 303680.
I0304 19:32:30.165852 22502662377600 run.py:483] Algo bellman_ford step 9490 current loss 0.002843, current_train_items 303712.
I0304 19:32:30.181846 22502662377600 run.py:483] Algo bellman_ford step 9491 current loss 0.014302, current_train_items 303744.
I0304 19:32:30.206357 22502662377600 run.py:483] Algo bellman_ford step 9492 current loss 0.080602, current_train_items 303776.
I0304 19:32:30.237136 22502662377600 run.py:483] Algo bellman_ford step 9493 current loss 0.035944, current_train_items 303808.
I0304 19:32:30.271263 22502662377600 run.py:483] Algo bellman_ford step 9494 current loss 0.153285, current_train_items 303840.
I0304 19:32:30.291187 22502662377600 run.py:483] Algo bellman_ford step 9495 current loss 0.009858, current_train_items 303872.
I0304 19:32:30.307840 22502662377600 run.py:483] Algo bellman_ford step 9496 current loss 0.032540, current_train_items 303904.
I0304 19:32:30.331962 22502662377600 run.py:483] Algo bellman_ford step 9497 current loss 0.066065, current_train_items 303936.
I0304 19:32:30.362859 22502662377600 run.py:483] Algo bellman_ford step 9498 current loss 0.077769, current_train_items 303968.
I0304 19:32:30.396969 22502662377600 run.py:483] Algo bellman_ford step 9499 current loss 0.138165, current_train_items 304000.
I0304 19:32:30.416984 22502662377600 run.py:483] Algo bellman_ford step 9500 current loss 0.003831, current_train_items 304032.
I0304 19:32:30.424621 22502662377600 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0304 19:32:30.424726 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:32:30.441418 22502662377600 run.py:483] Algo bellman_ford step 9501 current loss 0.048609, current_train_items 304064.
I0304 19:32:30.466415 22502662377600 run.py:483] Algo bellman_ford step 9502 current loss 0.049183, current_train_items 304096.
I0304 19:32:30.498688 22502662377600 run.py:483] Algo bellman_ford step 9503 current loss 0.037689, current_train_items 304128.
I0304 19:32:30.531453 22502662377600 run.py:483] Algo bellman_ford step 9504 current loss 0.060848, current_train_items 304160.
I0304 19:32:30.551227 22502662377600 run.py:483] Algo bellman_ford step 9505 current loss 0.031717, current_train_items 304192.
I0304 19:32:30.567280 22502662377600 run.py:483] Algo bellman_ford step 9506 current loss 0.024330, current_train_items 304224.
I0304 19:32:30.592020 22502662377600 run.py:483] Algo bellman_ford step 9507 current loss 0.042380, current_train_items 304256.
I0304 19:32:30.622686 22502662377600 run.py:483] Algo bellman_ford step 9508 current loss 0.055672, current_train_items 304288.
I0304 19:32:30.658589 22502662377600 run.py:483] Algo bellman_ford step 9509 current loss 0.108389, current_train_items 304320.
I0304 19:32:30.678221 22502662377600 run.py:483] Algo bellman_ford step 9510 current loss 0.003248, current_train_items 304352.
I0304 19:32:30.694850 22502662377600 run.py:483] Algo bellman_ford step 9511 current loss 0.013778, current_train_items 304384.
I0304 19:32:30.719156 22502662377600 run.py:483] Algo bellman_ford step 9512 current loss 0.049163, current_train_items 304416.
I0304 19:32:30.751171 22502662377600 run.py:483] Algo bellman_ford step 9513 current loss 0.053608, current_train_items 304448.
I0304 19:32:30.783870 22502662377600 run.py:483] Algo bellman_ford step 9514 current loss 0.045338, current_train_items 304480.
I0304 19:32:30.803508 22502662377600 run.py:483] Algo bellman_ford step 9515 current loss 0.009513, current_train_items 304512.
I0304 19:32:30.819300 22502662377600 run.py:483] Algo bellman_ford step 9516 current loss 0.031426, current_train_items 304544.
I0304 19:32:30.843411 22502662377600 run.py:483] Algo bellman_ford step 9517 current loss 0.041968, current_train_items 304576.
I0304 19:32:30.875230 22502662377600 run.py:483] Algo bellman_ford step 9518 current loss 0.052937, current_train_items 304608.
I0304 19:32:30.907682 22502662377600 run.py:483] Algo bellman_ford step 9519 current loss 0.057905, current_train_items 304640.
I0304 19:32:30.927055 22502662377600 run.py:483] Algo bellman_ford step 9520 current loss 0.001373, current_train_items 304672.
I0304 19:32:30.942476 22502662377600 run.py:483] Algo bellman_ford step 9521 current loss 0.006255, current_train_items 304704.
I0304 19:32:30.966071 22502662377600 run.py:483] Algo bellman_ford step 9522 current loss 0.021816, current_train_items 304736.
I0304 19:32:30.996395 22502662377600 run.py:483] Algo bellman_ford step 9523 current loss 0.031675, current_train_items 304768.
I0304 19:32:31.029077 22502662377600 run.py:483] Algo bellman_ford step 9524 current loss 0.036302, current_train_items 304800.
I0304 19:32:31.048420 22502662377600 run.py:483] Algo bellman_ford step 9525 current loss 0.002265, current_train_items 304832.
I0304 19:32:31.064605 22502662377600 run.py:483] Algo bellman_ford step 9526 current loss 0.009499, current_train_items 304864.
I0304 19:32:31.088940 22502662377600 run.py:483] Algo bellman_ford step 9527 current loss 0.057802, current_train_items 304896.
I0304 19:32:31.121341 22502662377600 run.py:483] Algo bellman_ford step 9528 current loss 0.056049, current_train_items 304928.
I0304 19:32:31.156223 22502662377600 run.py:483] Algo bellman_ford step 9529 current loss 0.059017, current_train_items 304960.
I0304 19:32:31.175629 22502662377600 run.py:483] Algo bellman_ford step 9530 current loss 0.007017, current_train_items 304992.
I0304 19:32:31.191644 22502662377600 run.py:483] Algo bellman_ford step 9531 current loss 0.017802, current_train_items 305024.
I0304 19:32:31.215337 22502662377600 run.py:483] Algo bellman_ford step 9532 current loss 0.043358, current_train_items 305056.
I0304 19:32:31.248584 22502662377600 run.py:483] Algo bellman_ford step 9533 current loss 0.106143, current_train_items 305088.
I0304 19:32:31.281633 22502662377600 run.py:483] Algo bellman_ford step 9534 current loss 0.068813, current_train_items 305120.
I0304 19:32:31.301186 22502662377600 run.py:483] Algo bellman_ford step 9535 current loss 0.020534, current_train_items 305152.
I0304 19:32:31.317308 22502662377600 run.py:483] Algo bellman_ford step 9536 current loss 0.019892, current_train_items 305184.
I0304 19:32:31.341499 22502662377600 run.py:483] Algo bellman_ford step 9537 current loss 0.113454, current_train_items 305216.
I0304 19:32:31.374572 22502662377600 run.py:483] Algo bellman_ford step 9538 current loss 0.084420, current_train_items 305248.
I0304 19:32:31.405880 22502662377600 run.py:483] Algo bellman_ford step 9539 current loss 0.053642, current_train_items 305280.
I0304 19:32:31.425323 22502662377600 run.py:483] Algo bellman_ford step 9540 current loss 0.003504, current_train_items 305312.
I0304 19:32:31.441226 22502662377600 run.py:483] Algo bellman_ford step 9541 current loss 0.028351, current_train_items 305344.
I0304 19:32:31.464720 22502662377600 run.py:483] Algo bellman_ford step 9542 current loss 0.032720, current_train_items 305376.
I0304 19:32:31.497993 22502662377600 run.py:483] Algo bellman_ford step 9543 current loss 0.169844, current_train_items 305408.
I0304 19:32:31.531713 22502662377600 run.py:483] Algo bellman_ford step 9544 current loss 0.149990, current_train_items 305440.
I0304 19:32:31.551374 22502662377600 run.py:483] Algo bellman_ford step 9545 current loss 0.004847, current_train_items 305472.
I0304 19:32:31.566920 22502662377600 run.py:483] Algo bellman_ford step 9546 current loss 0.036488, current_train_items 305504.
I0304 19:32:31.588685 22502662377600 run.py:483] Algo bellman_ford step 9547 current loss 0.017817, current_train_items 305536.
I0304 19:32:31.620356 22502662377600 run.py:483] Algo bellman_ford step 9548 current loss 0.067665, current_train_items 305568.
I0304 19:32:31.654222 22502662377600 run.py:483] Algo bellman_ford step 9549 current loss 0.108134, current_train_items 305600.
I0304 19:32:31.673520 22502662377600 run.py:483] Algo bellman_ford step 9550 current loss 0.013724, current_train_items 305632.
I0304 19:32:31.681552 22502662377600 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0304 19:32:31.681653 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:32:31.698340 22502662377600 run.py:483] Algo bellman_ford step 9551 current loss 0.040954, current_train_items 305664.
I0304 19:32:31.723524 22502662377600 run.py:483] Algo bellman_ford step 9552 current loss 0.087610, current_train_items 305696.
I0304 19:32:31.757668 22502662377600 run.py:483] Algo bellman_ford step 9553 current loss 0.064254, current_train_items 305728.
I0304 19:32:31.791144 22502662377600 run.py:483] Algo bellman_ford step 9554 current loss 0.057654, current_train_items 305760.
I0304 19:32:31.811121 22502662377600 run.py:483] Algo bellman_ford step 9555 current loss 0.007829, current_train_items 305792.
I0304 19:32:31.826537 22502662377600 run.py:483] Algo bellman_ford step 9556 current loss 0.047895, current_train_items 305824.
I0304 19:32:31.851679 22502662377600 run.py:483] Algo bellman_ford step 9557 current loss 0.055145, current_train_items 305856.
I0304 19:32:31.883455 22502662377600 run.py:483] Algo bellman_ford step 9558 current loss 0.067203, current_train_items 305888.
I0304 19:32:31.916558 22502662377600 run.py:483] Algo bellman_ford step 9559 current loss 0.069165, current_train_items 305920.
I0304 19:32:31.936897 22502662377600 run.py:483] Algo bellman_ford step 9560 current loss 0.009446, current_train_items 305952.
I0304 19:32:31.953270 22502662377600 run.py:483] Algo bellman_ford step 9561 current loss 0.009928, current_train_items 305984.
I0304 19:32:31.977269 22502662377600 run.py:483] Algo bellman_ford step 9562 current loss 0.025415, current_train_items 306016.
I0304 19:32:32.009105 22502662377600 run.py:483] Algo bellman_ford step 9563 current loss 0.039957, current_train_items 306048.
I0304 19:32:32.042074 22502662377600 run.py:483] Algo bellman_ford step 9564 current loss 0.045735, current_train_items 306080.
I0304 19:32:32.061676 22502662377600 run.py:483] Algo bellman_ford step 9565 current loss 0.036585, current_train_items 306112.
I0304 19:32:32.078328 22502662377600 run.py:483] Algo bellman_ford step 9566 current loss 0.020255, current_train_items 306144.
I0304 19:32:32.104002 22502662377600 run.py:483] Algo bellman_ford step 9567 current loss 0.093820, current_train_items 306176.
I0304 19:32:32.136499 22502662377600 run.py:483] Algo bellman_ford step 9568 current loss 0.106959, current_train_items 306208.
I0304 19:32:32.169919 22502662377600 run.py:483] Algo bellman_ford step 9569 current loss 0.066271, current_train_items 306240.
I0304 19:32:32.190284 22502662377600 run.py:483] Algo bellman_ford step 9570 current loss 0.010319, current_train_items 306272.
I0304 19:32:32.206443 22502662377600 run.py:483] Algo bellman_ford step 9571 current loss 0.012033, current_train_items 306304.
I0304 19:32:32.231382 22502662377600 run.py:483] Algo bellman_ford step 9572 current loss 0.149922, current_train_items 306336.
I0304 19:32:32.262639 22502662377600 run.py:483] Algo bellman_ford step 9573 current loss 0.123222, current_train_items 306368.
I0304 19:32:32.296948 22502662377600 run.py:483] Algo bellman_ford step 9574 current loss 0.155956, current_train_items 306400.
I0304 19:32:32.317228 22502662377600 run.py:483] Algo bellman_ford step 9575 current loss 0.005633, current_train_items 306432.
I0304 19:32:32.334201 22502662377600 run.py:483] Algo bellman_ford step 9576 current loss 0.031199, current_train_items 306464.
I0304 19:32:32.359302 22502662377600 run.py:483] Algo bellman_ford step 9577 current loss 0.053910, current_train_items 306496.
I0304 19:32:32.391029 22502662377600 run.py:483] Algo bellman_ford step 9578 current loss 0.038630, current_train_items 306528.
I0304 19:32:32.424922 22502662377600 run.py:483] Algo bellman_ford step 9579 current loss 0.085186, current_train_items 306560.
I0304 19:32:32.444548 22502662377600 run.py:483] Algo bellman_ford step 9580 current loss 0.011773, current_train_items 306592.
I0304 19:32:32.460593 22502662377600 run.py:483] Algo bellman_ford step 9581 current loss 0.028978, current_train_items 306624.
I0304 19:32:32.485969 22502662377600 run.py:483] Algo bellman_ford step 9582 current loss 0.028050, current_train_items 306656.
I0304 19:32:32.518941 22502662377600 run.py:483] Algo bellman_ford step 9583 current loss 0.047069, current_train_items 306688.
I0304 19:32:32.553183 22502662377600 run.py:483] Algo bellman_ford step 9584 current loss 0.137905, current_train_items 306720.
I0304 19:32:32.573135 22502662377600 run.py:483] Algo bellman_ford step 9585 current loss 0.024761, current_train_items 306752.
I0304 19:32:32.589486 22502662377600 run.py:483] Algo bellman_ford step 9586 current loss 0.018785, current_train_items 306784.
I0304 19:32:32.614022 22502662377600 run.py:483] Algo bellman_ford step 9587 current loss 0.043208, current_train_items 306816.
I0304 19:32:32.646650 22502662377600 run.py:483] Algo bellman_ford step 9588 current loss 0.055031, current_train_items 306848.
I0304 19:32:32.681984 22502662377600 run.py:483] Algo bellman_ford step 9589 current loss 0.096318, current_train_items 306880.
I0304 19:32:32.701949 22502662377600 run.py:483] Algo bellman_ford step 9590 current loss 0.003202, current_train_items 306912.
I0304 19:32:32.718171 22502662377600 run.py:483] Algo bellman_ford step 9591 current loss 0.047196, current_train_items 306944.
I0304 19:32:32.742224 22502662377600 run.py:483] Algo bellman_ford step 9592 current loss 0.035532, current_train_items 306976.
I0304 19:32:32.774655 22502662377600 run.py:483] Algo bellman_ford step 9593 current loss 0.127649, current_train_items 307008.
I0304 19:32:32.807910 22502662377600 run.py:483] Algo bellman_ford step 9594 current loss 0.049298, current_train_items 307040.
I0304 19:32:32.827923 22502662377600 run.py:483] Algo bellman_ford step 9595 current loss 0.002581, current_train_items 307072.
I0304 19:32:32.844306 22502662377600 run.py:483] Algo bellman_ford step 9596 current loss 0.019885, current_train_items 307104.
I0304 19:32:32.869155 22502662377600 run.py:483] Algo bellman_ford step 9597 current loss 0.056795, current_train_items 307136.
I0304 19:32:32.900241 22502662377600 run.py:483] Algo bellman_ford step 9598 current loss 0.038723, current_train_items 307168.
I0304 19:32:32.934136 22502662377600 run.py:483] Algo bellman_ford step 9599 current loss 0.045473, current_train_items 307200.
I0304 19:32:32.954396 22502662377600 run.py:483] Algo bellman_ford step 9600 current loss 0.001795, current_train_items 307232.
I0304 19:32:32.962343 22502662377600 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0304 19:32:32.962446 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:32.979577 22502662377600 run.py:483] Algo bellman_ford step 9601 current loss 0.009281, current_train_items 307264.
I0304 19:32:33.005180 22502662377600 run.py:483] Algo bellman_ford step 9602 current loss 0.061748, current_train_items 307296.
I0304 19:32:33.037258 22502662377600 run.py:483] Algo bellman_ford step 9603 current loss 0.037456, current_train_items 307328.
I0304 19:32:33.072950 22502662377600 run.py:483] Algo bellman_ford step 9604 current loss 0.044845, current_train_items 307360.
I0304 19:32:33.092669 22502662377600 run.py:483] Algo bellman_ford step 9605 current loss 0.004276, current_train_items 307392.
I0304 19:32:33.108279 22502662377600 run.py:483] Algo bellman_ford step 9606 current loss 0.015321, current_train_items 307424.
I0304 19:32:33.132610 22502662377600 run.py:483] Algo bellman_ford step 9607 current loss 0.054214, current_train_items 307456.
I0304 19:32:33.164573 22502662377600 run.py:483] Algo bellman_ford step 9608 current loss 0.039693, current_train_items 307488.
I0304 19:32:33.198650 22502662377600 run.py:483] Algo bellman_ford step 9609 current loss 0.053534, current_train_items 307520.
I0304 19:32:33.218537 22502662377600 run.py:483] Algo bellman_ford step 9610 current loss 0.021482, current_train_items 307552.
I0304 19:32:33.234781 22502662377600 run.py:483] Algo bellman_ford step 9611 current loss 0.018088, current_train_items 307584.
I0304 19:32:33.259627 22502662377600 run.py:483] Algo bellman_ford step 9612 current loss 0.027838, current_train_items 307616.
I0304 19:32:33.292133 22502662377600 run.py:483] Algo bellman_ford step 9613 current loss 0.082952, current_train_items 307648.
I0304 19:32:33.326268 22502662377600 run.py:483] Algo bellman_ford step 9614 current loss 0.108383, current_train_items 307680.
I0304 19:32:33.346088 22502662377600 run.py:483] Algo bellman_ford step 9615 current loss 0.003922, current_train_items 307712.
I0304 19:32:33.362199 22502662377600 run.py:483] Algo bellman_ford step 9616 current loss 0.050392, current_train_items 307744.
I0304 19:32:33.386437 22502662377600 run.py:483] Algo bellman_ford step 9617 current loss 0.089764, current_train_items 307776.
I0304 19:32:33.418163 22502662377600 run.py:483] Algo bellman_ford step 9618 current loss 0.047798, current_train_items 307808.
I0304 19:32:33.452573 22502662377600 run.py:483] Algo bellman_ford step 9619 current loss 0.064765, current_train_items 307840.
I0304 19:32:33.472191 22502662377600 run.py:483] Algo bellman_ford step 9620 current loss 0.003370, current_train_items 307872.
I0304 19:32:33.488172 22502662377600 run.py:483] Algo bellman_ford step 9621 current loss 0.011663, current_train_items 307904.
I0304 19:32:33.512950 22502662377600 run.py:483] Algo bellman_ford step 9622 current loss 0.045496, current_train_items 307936.
I0304 19:32:33.544900 22502662377600 run.py:483] Algo bellman_ford step 9623 current loss 0.065412, current_train_items 307968.
I0304 19:32:33.579397 22502662377600 run.py:483] Algo bellman_ford step 9624 current loss 0.076849, current_train_items 308000.
I0304 19:32:33.599263 22502662377600 run.py:483] Algo bellman_ford step 9625 current loss 0.027479, current_train_items 308032.
I0304 19:32:33.615337 22502662377600 run.py:483] Algo bellman_ford step 9626 current loss 0.009965, current_train_items 308064.
I0304 19:32:33.639637 22502662377600 run.py:483] Algo bellman_ford step 9627 current loss 0.110283, current_train_items 308096.
I0304 19:32:33.671356 22502662377600 run.py:483] Algo bellman_ford step 9628 current loss 0.084799, current_train_items 308128.
I0304 19:32:33.706098 22502662377600 run.py:483] Algo bellman_ford step 9629 current loss 0.121181, current_train_items 308160.
I0304 19:32:33.725818 22502662377600 run.py:483] Algo bellman_ford step 9630 current loss 0.046826, current_train_items 308192.
I0304 19:32:33.742243 22502662377600 run.py:483] Algo bellman_ford step 9631 current loss 0.020594, current_train_items 308224.
I0304 19:32:33.767806 22502662377600 run.py:483] Algo bellman_ford step 9632 current loss 0.278773, current_train_items 308256.
I0304 19:32:33.799350 22502662377600 run.py:483] Algo bellman_ford step 9633 current loss 0.153845, current_train_items 308288.
I0304 19:32:33.833986 22502662377600 run.py:483] Algo bellman_ford step 9634 current loss 0.347072, current_train_items 308320.
I0304 19:32:33.853905 22502662377600 run.py:483] Algo bellman_ford step 9635 current loss 0.031022, current_train_items 308352.
I0304 19:32:33.869967 22502662377600 run.py:483] Algo bellman_ford step 9636 current loss 0.047298, current_train_items 308384.
I0304 19:32:33.894503 22502662377600 run.py:483] Algo bellman_ford step 9637 current loss 0.092596, current_train_items 308416.
I0304 19:32:33.927007 22502662377600 run.py:483] Algo bellman_ford step 9638 current loss 0.078822, current_train_items 308448.
I0304 19:32:33.961175 22502662377600 run.py:483] Algo bellman_ford step 9639 current loss 0.152691, current_train_items 308480.
I0304 19:32:33.981117 22502662377600 run.py:483] Algo bellman_ford step 9640 current loss 0.008632, current_train_items 308512.
I0304 19:32:33.996949 22502662377600 run.py:483] Algo bellman_ford step 9641 current loss 0.028953, current_train_items 308544.
I0304 19:32:34.021699 22502662377600 run.py:483] Algo bellman_ford step 9642 current loss 0.158336, current_train_items 308576.
I0304 19:32:34.055263 22502662377600 run.py:483] Algo bellman_ford step 9643 current loss 0.109097, current_train_items 308608.
I0304 19:32:34.087536 22502662377600 run.py:483] Algo bellman_ford step 9644 current loss 0.093443, current_train_items 308640.
I0304 19:32:34.107139 22502662377600 run.py:483] Algo bellman_ford step 9645 current loss 0.037205, current_train_items 308672.
I0304 19:32:34.123318 22502662377600 run.py:483] Algo bellman_ford step 9646 current loss 0.061087, current_train_items 308704.
I0304 19:32:34.147129 22502662377600 run.py:483] Algo bellman_ford step 9647 current loss 0.078315, current_train_items 308736.
I0304 19:32:34.179872 22502662377600 run.py:483] Algo bellman_ford step 9648 current loss 0.134177, current_train_items 308768.
I0304 19:32:34.214548 22502662377600 run.py:483] Algo bellman_ford step 9649 current loss 0.175584, current_train_items 308800.
I0304 19:32:34.234283 22502662377600 run.py:483] Algo bellman_ford step 9650 current loss 0.012741, current_train_items 308832.
I0304 19:32:34.242417 22502662377600 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0304 19:32:34.242563 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:32:34.259450 22502662377600 run.py:483] Algo bellman_ford step 9651 current loss 0.025829, current_train_items 308864.
I0304 19:32:34.285629 22502662377600 run.py:483] Algo bellman_ford step 9652 current loss 0.060420, current_train_items 308896.
I0304 19:32:34.318592 22502662377600 run.py:483] Algo bellman_ford step 9653 current loss 0.106005, current_train_items 308928.
I0304 19:32:34.354359 22502662377600 run.py:483] Algo bellman_ford step 9654 current loss 0.094593, current_train_items 308960.
I0304 19:32:34.374318 22502662377600 run.py:483] Algo bellman_ford step 9655 current loss 0.018501, current_train_items 308992.
I0304 19:32:34.390265 22502662377600 run.py:483] Algo bellman_ford step 9656 current loss 0.024185, current_train_items 309024.
I0304 19:32:34.414316 22502662377600 run.py:483] Algo bellman_ford step 9657 current loss 0.055354, current_train_items 309056.
I0304 19:32:34.447186 22502662377600 run.py:483] Algo bellman_ford step 9658 current loss 0.077838, current_train_items 309088.
I0304 19:32:34.479951 22502662377600 run.py:483] Algo bellman_ford step 9659 current loss 0.058528, current_train_items 309120.
I0304 19:32:34.499763 22502662377600 run.py:483] Algo bellman_ford step 9660 current loss 0.014248, current_train_items 309152.
I0304 19:32:34.516323 22502662377600 run.py:483] Algo bellman_ford step 9661 current loss 0.028670, current_train_items 309184.
I0304 19:32:34.540257 22502662377600 run.py:483] Algo bellman_ford step 9662 current loss 0.086602, current_train_items 309216.
I0304 19:32:34.572248 22502662377600 run.py:483] Algo bellman_ford step 9663 current loss 0.051025, current_train_items 309248.
I0304 19:32:34.606770 22502662377600 run.py:483] Algo bellman_ford step 9664 current loss 0.076443, current_train_items 309280.
I0304 19:32:34.626472 22502662377600 run.py:483] Algo bellman_ford step 9665 current loss 0.009644, current_train_items 309312.
I0304 19:32:34.642666 22502662377600 run.py:483] Algo bellman_ford step 9666 current loss 0.014990, current_train_items 309344.
I0304 19:32:34.665847 22502662377600 run.py:483] Algo bellman_ford step 9667 current loss 0.052389, current_train_items 309376.
I0304 19:32:34.697328 22502662377600 run.py:483] Algo bellman_ford step 9668 current loss 0.062719, current_train_items 309408.
I0304 19:32:34.730883 22502662377600 run.py:483] Algo bellman_ford step 9669 current loss 0.066608, current_train_items 309440.
I0304 19:32:34.750966 22502662377600 run.py:483] Algo bellman_ford step 9670 current loss 0.004936, current_train_items 309472.
I0304 19:32:34.767139 22502662377600 run.py:483] Algo bellman_ford step 9671 current loss 0.011636, current_train_items 309504.
I0304 19:32:34.791148 22502662377600 run.py:483] Algo bellman_ford step 9672 current loss 0.061519, current_train_items 309536.
I0304 19:32:34.822347 22502662377600 run.py:483] Algo bellman_ford step 9673 current loss 0.041024, current_train_items 309568.
I0304 19:32:34.855800 22502662377600 run.py:483] Algo bellman_ford step 9674 current loss 0.087043, current_train_items 309600.
I0304 19:32:34.875565 22502662377600 run.py:483] Algo bellman_ford step 9675 current loss 0.004829, current_train_items 309632.
I0304 19:32:34.892096 22502662377600 run.py:483] Algo bellman_ford step 9676 current loss 0.030899, current_train_items 309664.
I0304 19:32:34.916222 22502662377600 run.py:483] Algo bellman_ford step 9677 current loss 0.083224, current_train_items 309696.
I0304 19:32:34.946993 22502662377600 run.py:483] Algo bellman_ford step 9678 current loss 0.069707, current_train_items 309728.
I0304 19:32:34.981009 22502662377600 run.py:483] Algo bellman_ford step 9679 current loss 0.077658, current_train_items 309760.
I0304 19:32:35.000644 22502662377600 run.py:483] Algo bellman_ford step 9680 current loss 0.008958, current_train_items 309792.
I0304 19:32:35.016930 22502662377600 run.py:483] Algo bellman_ford step 9681 current loss 0.008003, current_train_items 309824.
I0304 19:32:35.040843 22502662377600 run.py:483] Algo bellman_ford step 9682 current loss 0.030810, current_train_items 309856.
I0304 19:32:35.072145 22502662377600 run.py:483] Algo bellman_ford step 9683 current loss 0.052051, current_train_items 309888.
I0304 19:32:35.105030 22502662377600 run.py:483] Algo bellman_ford step 9684 current loss 0.088073, current_train_items 309920.
I0304 19:32:35.124631 22502662377600 run.py:483] Algo bellman_ford step 9685 current loss 0.009436, current_train_items 309952.
I0304 19:32:35.140674 22502662377600 run.py:483] Algo bellman_ford step 9686 current loss 0.035635, current_train_items 309984.
I0304 19:32:35.164596 22502662377600 run.py:483] Algo bellman_ford step 9687 current loss 0.063713, current_train_items 310016.
I0304 19:32:35.196174 22502662377600 run.py:483] Algo bellman_ford step 9688 current loss 0.088185, current_train_items 310048.
I0304 19:32:35.230008 22502662377600 run.py:483] Algo bellman_ford step 9689 current loss 0.075273, current_train_items 310080.
I0304 19:32:35.249845 22502662377600 run.py:483] Algo bellman_ford step 9690 current loss 0.010530, current_train_items 310112.
I0304 19:32:35.265587 22502662377600 run.py:483] Algo bellman_ford step 9691 current loss 0.006804, current_train_items 310144.
I0304 19:32:35.289566 22502662377600 run.py:483] Algo bellman_ford step 9692 current loss 0.028362, current_train_items 310176.
I0304 19:32:35.320925 22502662377600 run.py:483] Algo bellman_ford step 9693 current loss 0.149667, current_train_items 310208.
I0304 19:32:35.354892 22502662377600 run.py:483] Algo bellman_ford step 9694 current loss 0.088445, current_train_items 310240.
I0304 19:32:35.374485 22502662377600 run.py:483] Algo bellman_ford step 9695 current loss 0.023861, current_train_items 310272.
I0304 19:32:35.390596 22502662377600 run.py:483] Algo bellman_ford step 9696 current loss 0.037090, current_train_items 310304.
I0304 19:32:35.414107 22502662377600 run.py:483] Algo bellman_ford step 9697 current loss 0.029973, current_train_items 310336.
I0304 19:32:35.448352 22502662377600 run.py:483] Algo bellman_ford step 9698 current loss 0.100000, current_train_items 310368.
I0304 19:32:35.483517 22502662377600 run.py:483] Algo bellman_ford step 9699 current loss 0.081094, current_train_items 310400.
I0304 19:32:35.503227 22502662377600 run.py:483] Algo bellman_ford step 9700 current loss 0.003934, current_train_items 310432.
I0304 19:32:35.511050 22502662377600 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0304 19:32:35.511151 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:35.527793 22502662377600 run.py:483] Algo bellman_ford step 9701 current loss 0.023773, current_train_items 310464.
I0304 19:32:35.553214 22502662377600 run.py:483] Algo bellman_ford step 9702 current loss 0.037417, current_train_items 310496.
I0304 19:32:35.586242 22502662377600 run.py:483] Algo bellman_ford step 9703 current loss 0.118229, current_train_items 310528.
I0304 19:32:35.620095 22502662377600 run.py:483] Algo bellman_ford step 9704 current loss 0.105204, current_train_items 310560.
I0304 19:32:35.640349 22502662377600 run.py:483] Algo bellman_ford step 9705 current loss 0.003466, current_train_items 310592.
I0304 19:32:35.656199 22502662377600 run.py:483] Algo bellman_ford step 9706 current loss 0.026261, current_train_items 310624.
I0304 19:32:35.681627 22502662377600 run.py:483] Algo bellman_ford step 9707 current loss 0.133521, current_train_items 310656.
I0304 19:32:35.714539 22502662377600 run.py:483] Algo bellman_ford step 9708 current loss 0.163556, current_train_items 310688.
I0304 19:32:35.746654 22502662377600 run.py:483] Algo bellman_ford step 9709 current loss 0.142751, current_train_items 310720.
I0304 19:32:35.766583 22502662377600 run.py:483] Algo bellman_ford step 9710 current loss 0.002780, current_train_items 310752.
I0304 19:32:35.782523 22502662377600 run.py:483] Algo bellman_ford step 9711 current loss 0.015086, current_train_items 310784.
I0304 19:32:35.807010 22502662377600 run.py:483] Algo bellman_ford step 9712 current loss 0.048911, current_train_items 310816.
I0304 19:32:35.839581 22502662377600 run.py:483] Algo bellman_ford step 9713 current loss 0.050102, current_train_items 310848.
I0304 19:32:35.873882 22502662377600 run.py:483] Algo bellman_ford step 9714 current loss 0.042439, current_train_items 310880.
I0304 19:32:35.893645 22502662377600 run.py:483] Algo bellman_ford step 9715 current loss 0.004319, current_train_items 310912.
I0304 19:32:35.910154 22502662377600 run.py:483] Algo bellman_ford step 9716 current loss 0.029215, current_train_items 310944.
I0304 19:32:35.933138 22502662377600 run.py:483] Algo bellman_ford step 9717 current loss 0.043336, current_train_items 310976.
I0304 19:32:35.965586 22502662377600 run.py:483] Algo bellman_ford step 9718 current loss 0.061880, current_train_items 311008.
I0304 19:32:35.996995 22502662377600 run.py:483] Algo bellman_ford step 9719 current loss 0.033190, current_train_items 311040.
I0304 19:32:36.016824 22502662377600 run.py:483] Algo bellman_ford step 9720 current loss 0.008755, current_train_items 311072.
I0304 19:32:36.032427 22502662377600 run.py:483] Algo bellman_ford step 9721 current loss 0.003216, current_train_items 311104.
I0304 19:32:36.056194 22502662377600 run.py:483] Algo bellman_ford step 9722 current loss 0.035117, current_train_items 311136.
I0304 19:32:36.088749 22502662377600 run.py:483] Algo bellman_ford step 9723 current loss 0.054285, current_train_items 311168.
I0304 19:32:36.122611 22502662377600 run.py:483] Algo bellman_ford step 9724 current loss 0.037876, current_train_items 311200.
I0304 19:32:36.142444 22502662377600 run.py:483] Algo bellman_ford step 9725 current loss 0.001479, current_train_items 311232.
I0304 19:32:36.158359 22502662377600 run.py:483] Algo bellman_ford step 9726 current loss 0.014189, current_train_items 311264.
I0304 19:32:36.181847 22502662377600 run.py:483] Algo bellman_ford step 9727 current loss 0.014516, current_train_items 311296.
I0304 19:32:36.214270 22502662377600 run.py:483] Algo bellman_ford step 9728 current loss 0.036656, current_train_items 311328.
I0304 19:32:36.248450 22502662377600 run.py:483] Algo bellman_ford step 9729 current loss 0.050365, current_train_items 311360.
I0304 19:32:36.268479 22502662377600 run.py:483] Algo bellman_ford step 9730 current loss 0.004292, current_train_items 311392.
I0304 19:32:36.284775 22502662377600 run.py:483] Algo bellman_ford step 9731 current loss 0.014683, current_train_items 311424.
I0304 19:32:36.308862 22502662377600 run.py:483] Algo bellman_ford step 9732 current loss 0.040054, current_train_items 311456.
I0304 19:32:36.340634 22502662377600 run.py:483] Algo bellman_ford step 9733 current loss 0.136938, current_train_items 311488.
I0304 19:32:36.373701 22502662377600 run.py:483] Algo bellman_ford step 9734 current loss 0.068779, current_train_items 311520.
I0304 19:32:36.393527 22502662377600 run.py:483] Algo bellman_ford step 9735 current loss 0.006439, current_train_items 311552.
I0304 19:32:36.409624 22502662377600 run.py:483] Algo bellman_ford step 9736 current loss 0.026051, current_train_items 311584.
I0304 19:32:36.433804 22502662377600 run.py:483] Algo bellman_ford step 9737 current loss 0.025178, current_train_items 311616.
I0304 19:32:36.467621 22502662377600 run.py:483] Algo bellman_ford step 9738 current loss 0.066640, current_train_items 311648.
I0304 19:32:36.502130 22502662377600 run.py:483] Algo bellman_ford step 9739 current loss 0.065854, current_train_items 311680.
I0304 19:32:36.521900 22502662377600 run.py:483] Algo bellman_ford step 9740 current loss 0.001793, current_train_items 311712.
I0304 19:32:36.538728 22502662377600 run.py:483] Algo bellman_ford step 9741 current loss 0.025927, current_train_items 311744.
I0304 19:32:36.564661 22502662377600 run.py:483] Algo bellman_ford step 9742 current loss 0.061121, current_train_items 311776.
I0304 19:32:36.597303 22502662377600 run.py:483] Algo bellman_ford step 9743 current loss 0.042307, current_train_items 311808.
I0304 19:32:36.631479 22502662377600 run.py:483] Algo bellman_ford step 9744 current loss 0.065685, current_train_items 311840.
I0304 19:32:36.651351 22502662377600 run.py:483] Algo bellman_ford step 9745 current loss 0.038646, current_train_items 311872.
I0304 19:32:36.668073 22502662377600 run.py:483] Algo bellman_ford step 9746 current loss 0.018412, current_train_items 311904.
I0304 19:32:36.692218 22502662377600 run.py:483] Algo bellman_ford step 9747 current loss 0.029125, current_train_items 311936.
I0304 19:32:36.722599 22502662377600 run.py:483] Algo bellman_ford step 9748 current loss 0.022075, current_train_items 311968.
I0304 19:32:36.755352 22502662377600 run.py:483] Algo bellman_ford step 9749 current loss 0.053219, current_train_items 312000.
I0304 19:32:36.775125 22502662377600 run.py:483] Algo bellman_ford step 9750 current loss 0.001342, current_train_items 312032.
I0304 19:32:36.783304 22502662377600 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0304 19:32:36.783406 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:32:36.799951 22502662377600 run.py:483] Algo bellman_ford step 9751 current loss 0.015570, current_train_items 312064.
I0304 19:32:36.826168 22502662377600 run.py:483] Algo bellman_ford step 9752 current loss 0.030841, current_train_items 312096.
I0304 19:32:36.858178 22502662377600 run.py:483] Algo bellman_ford step 9753 current loss 0.053925, current_train_items 312128.
I0304 19:32:36.891420 22502662377600 run.py:483] Algo bellman_ford step 9754 current loss 0.074290, current_train_items 312160.
I0304 19:32:36.911376 22502662377600 run.py:483] Algo bellman_ford step 9755 current loss 0.041096, current_train_items 312192.
I0304 19:32:36.927763 22502662377600 run.py:483] Algo bellman_ford step 9756 current loss 0.027028, current_train_items 312224.
I0304 19:32:36.950930 22502662377600 run.py:483] Algo bellman_ford step 9757 current loss 0.033112, current_train_items 312256.
I0304 19:32:36.983806 22502662377600 run.py:483] Algo bellman_ford step 9758 current loss 0.071302, current_train_items 312288.
I0304 19:32:37.018023 22502662377600 run.py:483] Algo bellman_ford step 9759 current loss 0.062403, current_train_items 312320.
I0304 19:32:37.038436 22502662377600 run.py:483] Algo bellman_ford step 9760 current loss 0.030364, current_train_items 312352.
I0304 19:32:37.055158 22502662377600 run.py:483] Algo bellman_ford step 9761 current loss 0.025837, current_train_items 312384.
I0304 19:32:37.079739 22502662377600 run.py:483] Algo bellman_ford step 9762 current loss 0.072530, current_train_items 312416.
I0304 19:32:37.112630 22502662377600 run.py:483] Algo bellman_ford step 9763 current loss 0.162281, current_train_items 312448.
I0304 19:32:37.143826 22502662377600 run.py:483] Algo bellman_ford step 9764 current loss 0.102912, current_train_items 312480.
I0304 19:32:37.163539 22502662377600 run.py:483] Algo bellman_ford step 9765 current loss 0.031947, current_train_items 312512.
I0304 19:32:37.179428 22502662377600 run.py:483] Algo bellman_ford step 9766 current loss 0.029290, current_train_items 312544.
I0304 19:32:37.204306 22502662377600 run.py:483] Algo bellman_ford step 9767 current loss 0.094103, current_train_items 312576.
I0304 19:32:37.236378 22502662377600 run.py:483] Algo bellman_ford step 9768 current loss 0.080765, current_train_items 312608.
I0304 19:32:37.271045 22502662377600 run.py:483] Algo bellman_ford step 9769 current loss 0.144568, current_train_items 312640.
I0304 19:32:37.291224 22502662377600 run.py:483] Algo bellman_ford step 9770 current loss 0.009583, current_train_items 312672.
I0304 19:32:37.307239 22502662377600 run.py:483] Algo bellman_ford step 9771 current loss 0.103899, current_train_items 312704.
I0304 19:32:37.330582 22502662377600 run.py:483] Algo bellman_ford step 9772 current loss 0.083588, current_train_items 312736.
I0304 19:32:37.363960 22502662377600 run.py:483] Algo bellman_ford step 9773 current loss 0.072081, current_train_items 312768.
I0304 19:32:37.398154 22502662377600 run.py:483] Algo bellman_ford step 9774 current loss 0.064739, current_train_items 312800.
I0304 19:32:37.418144 22502662377600 run.py:483] Algo bellman_ford step 9775 current loss 0.003752, current_train_items 312832.
I0304 19:32:37.434218 22502662377600 run.py:483] Algo bellman_ford step 9776 current loss 0.012416, current_train_items 312864.
I0304 19:32:37.459003 22502662377600 run.py:483] Algo bellman_ford step 9777 current loss 0.065716, current_train_items 312896.
I0304 19:32:37.491858 22502662377600 run.py:483] Algo bellman_ford step 9778 current loss 0.075264, current_train_items 312928.
I0304 19:32:37.526020 22502662377600 run.py:483] Algo bellman_ford step 9779 current loss 0.112002, current_train_items 312960.
I0304 19:32:37.545853 22502662377600 run.py:483] Algo bellman_ford step 9780 current loss 0.008289, current_train_items 312992.
I0304 19:32:37.562263 22502662377600 run.py:483] Algo bellman_ford step 9781 current loss 0.017760, current_train_items 313024.
I0304 19:32:37.585870 22502662377600 run.py:483] Algo bellman_ford step 9782 current loss 0.060139, current_train_items 313056.
I0304 19:32:37.619942 22502662377600 run.py:483] Algo bellman_ford step 9783 current loss 0.134514, current_train_items 313088.
I0304 19:32:37.652443 22502662377600 run.py:483] Algo bellman_ford step 9784 current loss 0.051883, current_train_items 313120.
I0304 19:32:37.672649 22502662377600 run.py:483] Algo bellman_ford step 9785 current loss 0.006348, current_train_items 313152.
I0304 19:32:37.689445 22502662377600 run.py:483] Algo bellman_ford step 9786 current loss 0.034687, current_train_items 313184.
I0304 19:32:37.713412 22502662377600 run.py:483] Algo bellman_ford step 9787 current loss 0.042323, current_train_items 313216.
I0304 19:32:37.745022 22502662377600 run.py:483] Algo bellman_ford step 9788 current loss 0.038131, current_train_items 313248.
I0304 19:32:37.779435 22502662377600 run.py:483] Algo bellman_ford step 9789 current loss 0.061827, current_train_items 313280.
I0304 19:32:37.799426 22502662377600 run.py:483] Algo bellman_ford step 9790 current loss 0.028097, current_train_items 313312.
I0304 19:32:37.815427 22502662377600 run.py:483] Algo bellman_ford step 9791 current loss 0.012052, current_train_items 313344.
I0304 19:32:37.839147 22502662377600 run.py:483] Algo bellman_ford step 9792 current loss 0.080825, current_train_items 313376.
I0304 19:32:37.870924 22502662377600 run.py:483] Algo bellman_ford step 9793 current loss 0.087360, current_train_items 313408.
I0304 19:32:37.902823 22502662377600 run.py:483] Algo bellman_ford step 9794 current loss 0.074892, current_train_items 313440.
I0304 19:32:37.922719 22502662377600 run.py:483] Algo bellman_ford step 9795 current loss 0.003373, current_train_items 313472.
I0304 19:32:37.939144 22502662377600 run.py:483] Algo bellman_ford step 9796 current loss 0.013539, current_train_items 313504.
I0304 19:32:37.963593 22502662377600 run.py:483] Algo bellman_ford step 9797 current loss 0.029259, current_train_items 313536.
I0304 19:32:37.995629 22502662377600 run.py:483] Algo bellman_ford step 9798 current loss 0.042909, current_train_items 313568.
I0304 19:32:38.028455 22502662377600 run.py:483] Algo bellman_ford step 9799 current loss 0.080506, current_train_items 313600.
I0304 19:32:38.048352 22502662377600 run.py:483] Algo bellman_ford step 9800 current loss 0.019575, current_train_items 313632.
I0304 19:32:38.056144 22502662377600 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0304 19:32:38.056248 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:38.072746 22502662377600 run.py:483] Algo bellman_ford step 9801 current loss 0.020004, current_train_items 313664.
I0304 19:32:38.097600 22502662377600 run.py:483] Algo bellman_ford step 9802 current loss 0.045279, current_train_items 313696.
I0304 19:32:38.130655 22502662377600 run.py:483] Algo bellman_ford step 9803 current loss 0.117536, current_train_items 313728.
I0304 19:32:38.163333 22502662377600 run.py:483] Algo bellman_ford step 9804 current loss 0.077061, current_train_items 313760.
I0304 19:32:38.183254 22502662377600 run.py:483] Algo bellman_ford step 9805 current loss 0.004377, current_train_items 313792.
I0304 19:32:38.199189 22502662377600 run.py:483] Algo bellman_ford step 9806 current loss 0.013660, current_train_items 313824.
I0304 19:32:38.222523 22502662377600 run.py:483] Algo bellman_ford step 9807 current loss 0.036153, current_train_items 313856.
I0304 19:32:38.254733 22502662377600 run.py:483] Algo bellman_ford step 9808 current loss 0.067328, current_train_items 313888.
I0304 19:32:38.290255 22502662377600 run.py:483] Algo bellman_ford step 9809 current loss 0.093397, current_train_items 313920.
I0304 19:32:38.309892 22502662377600 run.py:483] Algo bellman_ford step 9810 current loss 0.009198, current_train_items 313952.
I0304 19:32:38.325625 22502662377600 run.py:483] Algo bellman_ford step 9811 current loss 0.025025, current_train_items 313984.
I0304 19:32:38.350769 22502662377600 run.py:483] Algo bellman_ford step 9812 current loss 0.062946, current_train_items 314016.
I0304 19:32:38.380616 22502662377600 run.py:483] Algo bellman_ford step 9813 current loss 0.030085, current_train_items 314048.
I0304 19:32:38.415158 22502662377600 run.py:483] Algo bellman_ford step 9814 current loss 0.060163, current_train_items 314080.
I0304 19:32:38.434518 22502662377600 run.py:483] Algo bellman_ford step 9815 current loss 0.003829, current_train_items 314112.
I0304 19:32:38.451011 22502662377600 run.py:483] Algo bellman_ford step 9816 current loss 0.033413, current_train_items 314144.
I0304 19:32:38.474437 22502662377600 run.py:483] Algo bellman_ford step 9817 current loss 0.018100, current_train_items 314176.
I0304 19:32:38.507442 22502662377600 run.py:483] Algo bellman_ford step 9818 current loss 0.060767, current_train_items 314208.
I0304 19:32:38.540582 22502662377600 run.py:483] Algo bellman_ford step 9819 current loss 0.059260, current_train_items 314240.
I0304 19:32:38.559905 22502662377600 run.py:483] Algo bellman_ford step 9820 current loss 0.003716, current_train_items 314272.
I0304 19:32:38.575846 22502662377600 run.py:483] Algo bellman_ford step 9821 current loss 0.027751, current_train_items 314304.
I0304 19:32:38.600437 22502662377600 run.py:483] Algo bellman_ford step 9822 current loss 0.091697, current_train_items 314336.
I0304 19:32:38.631446 22502662377600 run.py:483] Algo bellman_ford step 9823 current loss 0.052808, current_train_items 314368.
I0304 19:32:38.664649 22502662377600 run.py:483] Algo bellman_ford step 9824 current loss 0.063335, current_train_items 314400.
I0304 19:32:38.684018 22502662377600 run.py:483] Algo bellman_ford step 9825 current loss 0.002573, current_train_items 314432.
I0304 19:32:38.699916 22502662377600 run.py:483] Algo bellman_ford step 9826 current loss 0.012364, current_train_items 314464.
I0304 19:32:38.723783 22502662377600 run.py:483] Algo bellman_ford step 9827 current loss 0.053614, current_train_items 314496.
I0304 19:32:38.754681 22502662377600 run.py:483] Algo bellman_ford step 9828 current loss 0.049129, current_train_items 314528.
I0304 19:32:38.789457 22502662377600 run.py:483] Algo bellman_ford step 9829 current loss 0.068095, current_train_items 314560.
I0304 19:32:38.808716 22502662377600 run.py:483] Algo bellman_ford step 9830 current loss 0.002889, current_train_items 314592.
I0304 19:32:38.825279 22502662377600 run.py:483] Algo bellman_ford step 9831 current loss 0.016036, current_train_items 314624.
I0304 19:32:38.849270 22502662377600 run.py:483] Algo bellman_ford step 9832 current loss 0.044642, current_train_items 314656.
I0304 19:32:38.881618 22502662377600 run.py:483] Algo bellman_ford step 9833 current loss 0.066158, current_train_items 314688.
I0304 19:32:38.913624 22502662377600 run.py:483] Algo bellman_ford step 9834 current loss 0.044558, current_train_items 314720.
I0304 19:32:38.933315 22502662377600 run.py:483] Algo bellman_ford step 9835 current loss 0.016977, current_train_items 314752.
I0304 19:32:38.949378 22502662377600 run.py:483] Algo bellman_ford step 9836 current loss 0.011885, current_train_items 314784.
I0304 19:32:38.973186 22502662377600 run.py:483] Algo bellman_ford step 9837 current loss 0.027197, current_train_items 314816.
I0304 19:32:39.006283 22502662377600 run.py:483] Algo bellman_ford step 9838 current loss 0.048312, current_train_items 314848.
I0304 19:32:39.041065 22502662377600 run.py:483] Algo bellman_ford step 9839 current loss 0.049558, current_train_items 314880.
I0304 19:32:39.060398 22502662377600 run.py:483] Algo bellman_ford step 9840 current loss 0.011988, current_train_items 314912.
I0304 19:32:39.076498 22502662377600 run.py:483] Algo bellman_ford step 9841 current loss 0.017946, current_train_items 314944.
I0304 19:32:39.100913 22502662377600 run.py:483] Algo bellman_ford step 9842 current loss 0.033897, current_train_items 314976.
I0304 19:32:39.132671 22502662377600 run.py:483] Algo bellman_ford step 9843 current loss 0.075776, current_train_items 315008.
I0304 19:32:39.163791 22502662377600 run.py:483] Algo bellman_ford step 9844 current loss 0.060599, current_train_items 315040.
I0304 19:32:39.183390 22502662377600 run.py:483] Algo bellman_ford step 9845 current loss 0.002463, current_train_items 315072.
I0304 19:32:39.199697 22502662377600 run.py:483] Algo bellman_ford step 9846 current loss 0.014322, current_train_items 315104.
I0304 19:32:39.224408 22502662377600 run.py:483] Algo bellman_ford step 9847 current loss 0.053476, current_train_items 315136.
I0304 19:32:39.256229 22502662377600 run.py:483] Algo bellman_ford step 9848 current loss 0.096697, current_train_items 315168.
I0304 19:32:39.290015 22502662377600 run.py:483] Algo bellman_ford step 9849 current loss 0.066070, current_train_items 315200.
I0304 19:32:39.309720 22502662377600 run.py:483] Algo bellman_ford step 9850 current loss 0.002944, current_train_items 315232.
I0304 19:32:39.318233 22502662377600 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0304 19:32:39.318338 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:32:39.335818 22502662377600 run.py:483] Algo bellman_ford step 9851 current loss 0.019193, current_train_items 315264.
I0304 19:32:39.361975 22502662377600 run.py:483] Algo bellman_ford step 9852 current loss 0.053764, current_train_items 315296.
I0304 19:32:39.394046 22502662377600 run.py:483] Algo bellman_ford step 9853 current loss 0.071312, current_train_items 315328.
I0304 19:32:39.429011 22502662377600 run.py:483] Algo bellman_ford step 9854 current loss 0.095622, current_train_items 315360.
I0304 19:32:39.448769 22502662377600 run.py:483] Algo bellman_ford step 9855 current loss 0.003746, current_train_items 315392.
I0304 19:32:39.464655 22502662377600 run.py:483] Algo bellman_ford step 9856 current loss 0.014022, current_train_items 315424.
I0304 19:32:39.489403 22502662377600 run.py:483] Algo bellman_ford step 9857 current loss 0.050763, current_train_items 315456.
I0304 19:32:39.521332 22502662377600 run.py:483] Algo bellman_ford step 9858 current loss 0.077400, current_train_items 315488.
I0304 19:32:39.555064 22502662377600 run.py:483] Algo bellman_ford step 9859 current loss 0.106137, current_train_items 315520.
I0304 19:32:39.575601 22502662377600 run.py:483] Algo bellman_ford step 9860 current loss 0.004310, current_train_items 315552.
I0304 19:32:39.591860 22502662377600 run.py:483] Algo bellman_ford step 9861 current loss 0.036741, current_train_items 315584.
I0304 19:32:39.616653 22502662377600 run.py:483] Algo bellman_ford step 9862 current loss 0.028489, current_train_items 315616.
I0304 19:32:39.649136 22502662377600 run.py:483] Algo bellman_ford step 9863 current loss 0.050794, current_train_items 315648.
I0304 19:32:39.683134 22502662377600 run.py:483] Algo bellman_ford step 9864 current loss 0.041227, current_train_items 315680.
I0304 19:32:39.703075 22502662377600 run.py:483] Algo bellman_ford step 9865 current loss 0.003005, current_train_items 315712.
I0304 19:32:39.719147 22502662377600 run.py:483] Algo bellman_ford step 9866 current loss 0.009926, current_train_items 315744.
I0304 19:32:39.743447 22502662377600 run.py:483] Algo bellman_ford step 9867 current loss 0.082712, current_train_items 315776.
I0304 19:32:39.776359 22502662377600 run.py:483] Algo bellman_ford step 9868 current loss 0.044195, current_train_items 315808.
I0304 19:32:39.810581 22502662377600 run.py:483] Algo bellman_ford step 9869 current loss 0.074883, current_train_items 315840.
I0304 19:32:39.830864 22502662377600 run.py:483] Algo bellman_ford step 9870 current loss 0.006526, current_train_items 315872.
I0304 19:32:39.847784 22502662377600 run.py:483] Algo bellman_ford step 9871 current loss 0.025215, current_train_items 315904.
I0304 19:32:39.872329 22502662377600 run.py:483] Algo bellman_ford step 9872 current loss 0.048336, current_train_items 315936.
I0304 19:32:39.905020 22502662377600 run.py:483] Algo bellman_ford step 9873 current loss 0.063082, current_train_items 315968.
I0304 19:32:39.937879 22502662377600 run.py:483] Algo bellman_ford step 9874 current loss 0.086785, current_train_items 316000.
I0304 19:32:39.957916 22502662377600 run.py:483] Algo bellman_ford step 9875 current loss 0.003724, current_train_items 316032.
I0304 19:32:39.974150 22502662377600 run.py:483] Algo bellman_ford step 9876 current loss 0.042870, current_train_items 316064.
I0304 19:32:39.996725 22502662377600 run.py:483] Algo bellman_ford step 9877 current loss 0.053654, current_train_items 316096.
I0304 19:32:40.028730 22502662377600 run.py:483] Algo bellman_ford step 9878 current loss 0.064039, current_train_items 316128.
I0304 19:32:40.062222 22502662377600 run.py:483] Algo bellman_ford step 9879 current loss 0.066644, current_train_items 316160.
I0304 19:32:40.081957 22502662377600 run.py:483] Algo bellman_ford step 9880 current loss 0.008982, current_train_items 316192.
I0304 19:32:40.098277 22502662377600 run.py:483] Algo bellman_ford step 9881 current loss 0.027264, current_train_items 316224.
I0304 19:32:40.122983 22502662377600 run.py:483] Algo bellman_ford step 9882 current loss 0.048969, current_train_items 316256.
I0304 19:32:40.155483 22502662377600 run.py:483] Algo bellman_ford step 9883 current loss 0.128399, current_train_items 316288.
I0304 19:32:40.189306 22502662377600 run.py:483] Algo bellman_ford step 9884 current loss 0.107890, current_train_items 316320.
I0304 19:32:40.209557 22502662377600 run.py:483] Algo bellman_ford step 9885 current loss 0.005272, current_train_items 316352.
I0304 19:32:40.225739 22502662377600 run.py:483] Algo bellman_ford step 9886 current loss 0.041896, current_train_items 316384.
I0304 19:32:40.249646 22502662377600 run.py:483] Algo bellman_ford step 9887 current loss 0.027281, current_train_items 316416.
I0304 19:32:40.281291 22502662377600 run.py:483] Algo bellman_ford step 9888 current loss 0.058468, current_train_items 316448.
I0304 19:32:40.313997 22502662377600 run.py:483] Algo bellman_ford step 9889 current loss 0.072263, current_train_items 316480.
I0304 19:32:40.334300 22502662377600 run.py:483] Algo bellman_ford step 9890 current loss 0.007280, current_train_items 316512.
I0304 19:32:40.350344 22502662377600 run.py:483] Algo bellman_ford step 9891 current loss 0.037992, current_train_items 316544.
I0304 19:32:40.374026 22502662377600 run.py:483] Algo bellman_ford step 9892 current loss 0.060352, current_train_items 316576.
I0304 19:32:40.406624 22502662377600 run.py:483] Algo bellman_ford step 9893 current loss 0.056164, current_train_items 316608.
I0304 19:32:40.441610 22502662377600 run.py:483] Algo bellman_ford step 9894 current loss 0.040374, current_train_items 316640.
I0304 19:32:40.461774 22502662377600 run.py:483] Algo bellman_ford step 9895 current loss 0.005006, current_train_items 316672.
I0304 19:32:40.477898 22502662377600 run.py:483] Algo bellman_ford step 9896 current loss 0.017400, current_train_items 316704.
I0304 19:32:40.502901 22502662377600 run.py:483] Algo bellman_ford step 9897 current loss 0.065715, current_train_items 316736.
I0304 19:32:40.534928 22502662377600 run.py:483] Algo bellman_ford step 9898 current loss 0.093370, current_train_items 316768.
I0304 19:32:40.570817 22502662377600 run.py:483] Algo bellman_ford step 9899 current loss 0.092741, current_train_items 316800.
I0304 19:32:40.590907 22502662377600 run.py:483] Algo bellman_ford step 9900 current loss 0.003422, current_train_items 316832.
I0304 19:32:40.598712 22502662377600 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0304 19:32:40.598818 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:32:40.615606 22502662377600 run.py:483] Algo bellman_ford step 9901 current loss 0.016140, current_train_items 316864.
I0304 19:32:40.639846 22502662377600 run.py:483] Algo bellman_ford step 9902 current loss 0.067299, current_train_items 316896.
I0304 19:32:40.671471 22502662377600 run.py:483] Algo bellman_ford step 9903 current loss 0.095829, current_train_items 316928.
I0304 19:32:40.706088 22502662377600 run.py:483] Algo bellman_ford step 9904 current loss 0.105950, current_train_items 316960.
I0304 19:32:40.725949 22502662377600 run.py:483] Algo bellman_ford step 9905 current loss 0.002483, current_train_items 316992.
I0304 19:32:40.742137 22502662377600 run.py:483] Algo bellman_ford step 9906 current loss 0.031679, current_train_items 317024.
I0304 19:32:40.766777 22502662377600 run.py:483] Algo bellman_ford step 9907 current loss 0.049451, current_train_items 317056.
I0304 19:32:40.799188 22502662377600 run.py:483] Algo bellman_ford step 9908 current loss 0.039261, current_train_items 317088.
I0304 19:32:40.831300 22502662377600 run.py:483] Algo bellman_ford step 9909 current loss 0.052314, current_train_items 317120.
I0304 19:32:40.850881 22502662377600 run.py:483] Algo bellman_ford step 9910 current loss 0.005316, current_train_items 317152.
I0304 19:32:40.867261 22502662377600 run.py:483] Algo bellman_ford step 9911 current loss 0.006615, current_train_items 317184.
I0304 19:32:40.891738 22502662377600 run.py:483] Algo bellman_ford step 9912 current loss 0.064214, current_train_items 317216.
I0304 19:32:40.924057 22502662377600 run.py:483] Algo bellman_ford step 9913 current loss 0.076542, current_train_items 317248.
I0304 19:32:40.957486 22502662377600 run.py:483] Algo bellman_ford step 9914 current loss 0.071914, current_train_items 317280.
I0304 19:32:40.977168 22502662377600 run.py:483] Algo bellman_ford step 9915 current loss 0.042626, current_train_items 317312.
I0304 19:32:40.993786 22502662377600 run.py:483] Algo bellman_ford step 9916 current loss 0.012784, current_train_items 317344.
I0304 19:32:41.018205 22502662377600 run.py:483] Algo bellman_ford step 9917 current loss 0.031418, current_train_items 317376.
I0304 19:32:41.050397 22502662377600 run.py:483] Algo bellman_ford step 9918 current loss 0.099858, current_train_items 317408.
I0304 19:32:41.083035 22502662377600 run.py:483] Algo bellman_ford step 9919 current loss 0.110881, current_train_items 317440.
I0304 19:32:41.102493 22502662377600 run.py:483] Algo bellman_ford step 9920 current loss 0.005926, current_train_items 317472.
I0304 19:32:41.118970 22502662377600 run.py:483] Algo bellman_ford step 9921 current loss 0.015567, current_train_items 317504.
I0304 19:32:41.143157 22502662377600 run.py:483] Algo bellman_ford step 9922 current loss 0.080560, current_train_items 317536.
I0304 19:32:41.173628 22502662377600 run.py:483] Algo bellman_ford step 9923 current loss 0.059368, current_train_items 317568.
I0304 19:32:41.209728 22502662377600 run.py:483] Algo bellman_ford step 9924 current loss 0.133917, current_train_items 317600.
I0304 19:32:41.229051 22502662377600 run.py:483] Algo bellman_ford step 9925 current loss 0.024869, current_train_items 317632.
I0304 19:32:41.244991 22502662377600 run.py:483] Algo bellman_ford step 9926 current loss 0.055710, current_train_items 317664.
I0304 19:32:41.269812 22502662377600 run.py:483] Algo bellman_ford step 9927 current loss 0.038441, current_train_items 317696.
I0304 19:32:41.303169 22502662377600 run.py:483] Algo bellman_ford step 9928 current loss 0.073310, current_train_items 317728.
I0304 19:32:41.334943 22502662377600 run.py:483] Algo bellman_ford step 9929 current loss 0.148371, current_train_items 317760.
I0304 19:32:41.354742 22502662377600 run.py:483] Algo bellman_ford step 9930 current loss 0.003051, current_train_items 317792.
I0304 19:32:41.370194 22502662377600 run.py:483] Algo bellman_ford step 9931 current loss 0.012012, current_train_items 317824.
I0304 19:32:41.394438 22502662377600 run.py:483] Algo bellman_ford step 9932 current loss 0.040264, current_train_items 317856.
I0304 19:32:41.426161 22502662377600 run.py:483] Algo bellman_ford step 9933 current loss 0.092871, current_train_items 317888.
I0304 19:32:41.458824 22502662377600 run.py:483] Algo bellman_ford step 9934 current loss 0.096902, current_train_items 317920.
I0304 19:32:41.478166 22502662377600 run.py:483] Algo bellman_ford step 9935 current loss 0.004338, current_train_items 317952.
I0304 19:32:41.493957 22502662377600 run.py:483] Algo bellman_ford step 9936 current loss 0.011641, current_train_items 317984.
I0304 19:32:41.517998 22502662377600 run.py:483] Algo bellman_ford step 9937 current loss 0.055456, current_train_items 318016.
I0304 19:32:41.549323 22502662377600 run.py:483] Algo bellman_ford step 9938 current loss 0.042359, current_train_items 318048.
I0304 19:32:41.583414 22502662377600 run.py:483] Algo bellman_ford step 9939 current loss 0.173747, current_train_items 318080.
I0304 19:32:41.603384 22502662377600 run.py:483] Algo bellman_ford step 9940 current loss 0.005181, current_train_items 318112.
I0304 19:32:41.619156 22502662377600 run.py:483] Algo bellman_ford step 9941 current loss 0.009557, current_train_items 318144.
I0304 19:32:41.642879 22502662377600 run.py:483] Algo bellman_ford step 9942 current loss 0.041562, current_train_items 318176.
I0304 19:32:41.674986 22502662377600 run.py:483] Algo bellman_ford step 9943 current loss 0.042840, current_train_items 318208.
I0304 19:32:41.707087 22502662377600 run.py:483] Algo bellman_ford step 9944 current loss 0.069966, current_train_items 318240.
I0304 19:32:41.726683 22502662377600 run.py:483] Algo bellman_ford step 9945 current loss 0.008702, current_train_items 318272.
I0304 19:32:41.742385 22502662377600 run.py:483] Algo bellman_ford step 9946 current loss 0.035543, current_train_items 318304.
I0304 19:32:41.766691 22502662377600 run.py:483] Algo bellman_ford step 9947 current loss 0.035664, current_train_items 318336.
I0304 19:32:41.799292 22502662377600 run.py:483] Algo bellman_ford step 9948 current loss 0.036917, current_train_items 318368.
I0304 19:32:41.833822 22502662377600 run.py:483] Algo bellman_ford step 9949 current loss 0.061828, current_train_items 318400.
I0304 19:32:41.853195 22502662377600 run.py:483] Algo bellman_ford step 9950 current loss 0.002186, current_train_items 318432.
I0304 19:32:41.861110 22502662377600 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0304 19:32:41.861217 22502662377600 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:41.878228 22502662377600 run.py:483] Algo bellman_ford step 9951 current loss 0.012129, current_train_items 318464.
I0304 19:32:41.903273 22502662377600 run.py:483] Algo bellman_ford step 9952 current loss 0.049313, current_train_items 318496.
I0304 19:32:41.936337 22502662377600 run.py:483] Algo bellman_ford step 9953 current loss 0.072577, current_train_items 318528.
I0304 19:32:41.971998 22502662377600 run.py:483] Algo bellman_ford step 9954 current loss 0.091220, current_train_items 318560.
I0304 19:32:41.991673 22502662377600 run.py:483] Algo bellman_ford step 9955 current loss 0.004888, current_train_items 318592.
I0304 19:32:42.007942 22502662377600 run.py:483] Algo bellman_ford step 9956 current loss 0.020959, current_train_items 318624.
I0304 19:32:42.032307 22502662377600 run.py:483] Algo bellman_ford step 9957 current loss 0.039287, current_train_items 318656.
I0304 19:32:42.065054 22502662377600 run.py:483] Algo bellman_ford step 9958 current loss 0.069215, current_train_items 318688.
I0304 19:32:42.099947 22502662377600 run.py:483] Algo bellman_ford step 9959 current loss 0.081842, current_train_items 318720.
I0304 19:32:42.120087 22502662377600 run.py:483] Algo bellman_ford step 9960 current loss 0.003378, current_train_items 318752.
I0304 19:32:42.135774 22502662377600 run.py:483] Algo bellman_ford step 9961 current loss 0.026606, current_train_items 318784.
I0304 19:32:42.160668 22502662377600 run.py:483] Algo bellman_ford step 9962 current loss 0.045375, current_train_items 318816.
I0304 19:32:42.192801 22502662377600 run.py:483] Algo bellman_ford step 9963 current loss 0.066480, current_train_items 318848.
I0304 19:32:42.226003 22502662377600 run.py:483] Algo bellman_ford step 9964 current loss 0.103993, current_train_items 318880.
I0304 19:32:42.245843 22502662377600 run.py:483] Algo bellman_ford step 9965 current loss 0.003471, current_train_items 318912.
I0304 19:32:42.262208 22502662377600 run.py:483] Algo bellman_ford step 9966 current loss 0.017693, current_train_items 318944.
I0304 19:32:42.286369 22502662377600 run.py:483] Algo bellman_ford step 9967 current loss 0.066310, current_train_items 318976.
I0304 19:32:42.318133 22502662377600 run.py:483] Algo bellman_ford step 9968 current loss 0.095645, current_train_items 319008.
I0304 19:32:42.352102 22502662377600 run.py:483] Algo bellman_ford step 9969 current loss 0.075490, current_train_items 319040.
I0304 19:32:42.371982 22502662377600 run.py:483] Algo bellman_ford step 9970 current loss 0.003315, current_train_items 319072.
I0304 19:32:42.388088 22502662377600 run.py:483] Algo bellman_ford step 9971 current loss 0.016943, current_train_items 319104.
I0304 19:32:42.412087 22502662377600 run.py:483] Algo bellman_ford step 9972 current loss 0.049942, current_train_items 319136.
I0304 19:32:42.443150 22502662377600 run.py:483] Algo bellman_ford step 9973 current loss 0.086648, current_train_items 319168.
I0304 19:32:42.476890 22502662377600 run.py:483] Algo bellman_ford step 9974 current loss 0.077606, current_train_items 319200.
I0304 19:32:42.496539 22502662377600 run.py:483] Algo bellman_ford step 9975 current loss 0.001912, current_train_items 319232.
I0304 19:32:42.513053 22502662377600 run.py:483] Algo bellman_ford step 9976 current loss 0.017920, current_train_items 319264.
I0304 19:32:42.536773 22502662377600 run.py:483] Algo bellman_ford step 9977 current loss 0.072129, current_train_items 319296.
I0304 19:32:42.568515 22502662377600 run.py:483] Algo bellman_ford step 9978 current loss 0.034622, current_train_items 319328.
I0304 19:32:42.600950 22502662377600 run.py:483] Algo bellman_ford step 9979 current loss 0.037148, current_train_items 319360.
I0304 19:32:42.620375 22502662377600 run.py:483] Algo bellman_ford step 9980 current loss 0.002163, current_train_items 319392.
I0304 19:32:42.636545 22502662377600 run.py:483] Algo bellman_ford step 9981 current loss 0.015560, current_train_items 319424.
I0304 19:32:42.661389 22502662377600 run.py:483] Algo bellman_ford step 9982 current loss 0.049781, current_train_items 319456.
I0304 19:32:42.694627 22502662377600 run.py:483] Algo bellman_ford step 9983 current loss 0.049166, current_train_items 319488.
I0304 19:32:42.729157 22502662377600 run.py:483] Algo bellman_ford step 9984 current loss 0.062925, current_train_items 319520.
I0304 19:32:42.748770 22502662377600 run.py:483] Algo bellman_ford step 9985 current loss 0.025563, current_train_items 319552.
I0304 19:32:42.764588 22502662377600 run.py:483] Algo bellman_ford step 9986 current loss 0.005677, current_train_items 319584.
I0304 19:32:42.788557 22502662377600 run.py:483] Algo bellman_ford step 9987 current loss 0.029420, current_train_items 319616.
I0304 19:32:42.820479 22502662377600 run.py:483] Algo bellman_ford step 9988 current loss 0.048818, current_train_items 319648.
I0304 19:32:42.853592 22502662377600 run.py:483] Algo bellman_ford step 9989 current loss 0.059352, current_train_items 319680.
I0304 19:32:42.873260 22502662377600 run.py:483] Algo bellman_ford step 9990 current loss 0.133159, current_train_items 319712.
I0304 19:32:42.889762 22502662377600 run.py:483] Algo bellman_ford step 9991 current loss 0.042723, current_train_items 319744.
I0304 19:32:42.913343 22502662377600 run.py:483] Algo bellman_ford step 9992 current loss 0.062926, current_train_items 319776.
I0304 19:32:42.944453 22502662377600 run.py:483] Algo bellman_ford step 9993 current loss 0.038636, current_train_items 319808.
I0304 19:32:42.977330 22502662377600 run.py:483] Algo bellman_ford step 9994 current loss 0.054044, current_train_items 319840.
I0304 19:32:42.997313 22502662377600 run.py:483] Algo bellman_ford step 9995 current loss 0.011064, current_train_items 319872.
I0304 19:32:43.013617 22502662377600 run.py:483] Algo bellman_ford step 9996 current loss 0.043994, current_train_items 319904.
I0304 19:32:43.037689 22502662377600 run.py:483] Algo bellman_ford step 9997 current loss 0.023792, current_train_items 319936.
I0304 19:32:43.069521 22502662377600 run.py:483] Algo bellman_ford step 9998 current loss 0.104225, current_train_items 319968.
I0304 19:32:43.101023 22502662377600 run.py:483] Algo bellman_ford step 9999 current loss 0.068157, current_train_items 320000.
I0304 19:32:43.107133 22502662377600 run.py:527] Restoring best model from checkpoint...
I0304 19:32:45.624029 22502662377600 run.py:542] (test) algo bellman_ford : {'pi': 0.9580078125, 'score': 0.9580078125, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0304 19:32:45.624309 22502662377600 run.py:544] Done!

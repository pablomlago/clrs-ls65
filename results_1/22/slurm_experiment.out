Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-02 18:56:31.952499: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-02 18:56:31.952811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-02 18:56:31.979721: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-02 18:56:36.425403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0302 18:56:47.198314 22626471084160 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0302 18:56:47.208435 22626471084160 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0302 18:56:47.597913 22626471084160 run.py:307] Creating samplers for algo bellman_ford
W0302 18:56:47.598343 22626471084160 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:47.598631 22626471084160 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:47.805412 22626471084160 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:47.805661 22626471084160 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.050824 22626471084160 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.051076 22626471084160 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.392056 22626471084160 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.392307 22626471084160 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.811918 22626471084160 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.812178 22626471084160 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:49.346540 22626471084160 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0302 18:56:49.346796 22626471084160 samplers.py:112] Creating a dataset with 64 samples.
I0302 18:56:49.384958 22626471084160 run.py:166] Dataset not found in ./datasets_1/22/CLRS30_v1.0.0. Downloading...
I0302 18:57:05.264423 22626471084160 dataset_info.py:482] Load dataset info from ./datasets_1/22/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:05.267039 22626471084160 dataset_info.py:482] Load dataset info from ./datasets_1/22/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:05.267855 22626471084160 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/22/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0302 18:57:05.267937 22626471084160 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/22/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:21.828550 22626471084160 run.py:483] Algo bellman_ford step 0 current loss 4.642128, current_train_items 32.
I0302 18:57:24.851895 22626471084160 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.361328125, 'score': 0.361328125, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0302 18:57:24.852086 22626471084160 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.361, val scores are: bellman_ford: 0.361
I0302 18:57:34.739228 22626471084160 run.py:483] Algo bellman_ford step 1 current loss 482.308594, current_train_items 64.
I0302 18:57:45.944941 22626471084160 run.py:483] Algo bellman_ford step 2 current loss 103646560.000000, current_train_items 96.
I0302 18:57:57.142704 22626471084160 run.py:483] Algo bellman_ford step 3 current loss 13418505216.000000, current_train_items 128.
I0302 18:58:07.217902 22626471084160 run.py:483] Algo bellman_ford step 4 current loss 1314322176.000000, current_train_items 160.
I0302 18:58:07.236761 22626471084160 run.py:483] Algo bellman_ford step 5 current loss 2.301556, current_train_items 192.
I0302 18:58:07.253788 22626471084160 run.py:483] Algo bellman_ford step 6 current loss 39.418732, current_train_items 224.
I0302 18:58:07.276398 22626471084160 run.py:483] Algo bellman_ford step 7 current loss 732448.187500, current_train_items 256.
I0302 18:58:07.305797 22626471084160 run.py:483] Algo bellman_ford step 8 current loss 44706480.000000, current_train_items 288.
I0302 18:58:07.337970 22626471084160 run.py:483] Algo bellman_ford step 9 current loss 40424948.000000, current_train_items 320.
I0302 18:58:07.355690 22626471084160 run.py:483] Algo bellman_ford step 10 current loss 3.631053, current_train_items 352.
I0302 18:58:07.372385 22626471084160 run.py:483] Algo bellman_ford step 11 current loss 12.470867, current_train_items 384.
I0302 18:58:07.394188 22626471084160 run.py:483] Algo bellman_ford step 12 current loss 125320.843750, current_train_items 416.
I0302 18:58:07.425124 22626471084160 run.py:483] Algo bellman_ford step 13 current loss 4976362.000000, current_train_items 448.
I0302 18:58:07.452903 22626471084160 run.py:483] Algo bellman_ford step 14 current loss 3719881.250000, current_train_items 480.
I0302 18:58:07.470881 22626471084160 run.py:483] Algo bellman_ford step 15 current loss 1.602274, current_train_items 512.
I0302 18:58:07.487017 22626471084160 run.py:483] Algo bellman_ford step 16 current loss 2.996243, current_train_items 544.
I0302 18:58:07.511585 22626471084160 run.py:483] Algo bellman_ford step 17 current loss 5843.788086, current_train_items 576.
I0302 18:58:07.540298 22626471084160 run.py:483] Algo bellman_ford step 18 current loss 287766.843750, current_train_items 608.
I0302 18:58:07.572843 22626471084160 run.py:483] Algo bellman_ford step 19 current loss 91887.406250, current_train_items 640.
I0302 18:58:07.590508 22626471084160 run.py:483] Algo bellman_ford step 20 current loss 1.383899, current_train_items 672.
I0302 18:58:07.606282 22626471084160 run.py:483] Algo bellman_ford step 21 current loss 1.903620, current_train_items 704.
I0302 18:58:07.629746 22626471084160 run.py:483] Algo bellman_ford step 22 current loss 20.150166, current_train_items 736.
I0302 18:58:07.658865 22626471084160 run.py:483] Algo bellman_ford step 23 current loss 478.094360, current_train_items 768.
I0302 18:58:07.689916 22626471084160 run.py:483] Algo bellman_ford step 24 current loss 1047.306274, current_train_items 800.
I0302 18:58:07.707901 22626471084160 run.py:483] Algo bellman_ford step 25 current loss 1.345888, current_train_items 832.
I0302 18:58:07.724103 22626471084160 run.py:483] Algo bellman_ford step 26 current loss 1.961939, current_train_items 864.
I0302 18:58:07.747494 22626471084160 run.py:483] Algo bellman_ford step 27 current loss 4.777292, current_train_items 896.
I0302 18:58:07.777181 22626471084160 run.py:483] Algo bellman_ford step 28 current loss 16.193426, current_train_items 928.
I0302 18:58:07.809879 22626471084160 run.py:483] Algo bellman_ford step 29 current loss 61.991211, current_train_items 960.
I0302 18:58:07.827903 22626471084160 run.py:483] Algo bellman_ford step 30 current loss 1.034004, current_train_items 992.
I0302 18:58:07.843243 22626471084160 run.py:483] Algo bellman_ford step 31 current loss 1.492520, current_train_items 1024.
I0302 18:58:07.865800 22626471084160 run.py:483] Algo bellman_ford step 32 current loss 2.331564, current_train_items 1056.
I0302 18:58:07.895854 22626471084160 run.py:483] Algo bellman_ford step 33 current loss 2.806656, current_train_items 1088.
I0302 18:58:07.927229 22626471084160 run.py:483] Algo bellman_ford step 34 current loss 4.153987, current_train_items 1120.
I0302 18:58:07.944666 22626471084160 run.py:483] Algo bellman_ford step 35 current loss 1.146060, current_train_items 1152.
I0302 18:58:07.960387 22626471084160 run.py:483] Algo bellman_ford step 36 current loss 1.479436, current_train_items 1184.
I0302 18:58:07.983321 22626471084160 run.py:483] Algo bellman_ford step 37 current loss 2.100186, current_train_items 1216.
I0302 18:58:08.012948 22626471084160 run.py:483] Algo bellman_ford step 38 current loss 2.383512, current_train_items 1248.
W0302 18:58:08.035749 22626471084160 samplers.py:155] Increasing hint lengh from 9 to 11
I0302 18:58:14.697262 22626471084160 run.py:483] Algo bellman_ford step 39 current loss 9.154985, current_train_items 1280.
I0302 18:58:14.716958 22626471084160 run.py:483] Algo bellman_ford step 40 current loss 1.176103, current_train_items 1312.
I0302 18:58:14.733536 22626471084160 run.py:483] Algo bellman_ford step 41 current loss 1.634679, current_train_items 1344.
I0302 18:58:14.756407 22626471084160 run.py:483] Algo bellman_ford step 42 current loss 2.017605, current_train_items 1376.
I0302 18:58:14.787074 22626471084160 run.py:483] Algo bellman_ford step 43 current loss 3.453332, current_train_items 1408.
I0302 18:58:14.819282 22626471084160 run.py:483] Algo bellman_ford step 44 current loss 24.085093, current_train_items 1440.
I0302 18:58:14.838604 22626471084160 run.py:483] Algo bellman_ford step 45 current loss 0.967371, current_train_items 1472.
I0302 18:58:14.855177 22626471084160 run.py:483] Algo bellman_ford step 46 current loss 1.531439, current_train_items 1504.
I0302 18:58:14.877617 22626471084160 run.py:483] Algo bellman_ford step 47 current loss 1.956014, current_train_items 1536.
I0302 18:58:14.904849 22626471084160 run.py:483] Algo bellman_ford step 48 current loss 1.821984, current_train_items 1568.
I0302 18:58:14.934270 22626471084160 run.py:483] Algo bellman_ford step 49 current loss 2.166027, current_train_items 1600.
I0302 18:58:14.952806 22626471084160 run.py:483] Algo bellman_ford step 50 current loss 0.838845, current_train_items 1632.
I0302 18:58:14.962345 22626471084160 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.66015625, 'score': 0.66015625, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0302 18:58:14.962458 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.361, current avg val score is 0.660, val scores are: bellman_ford: 0.660
I0302 18:58:14.991279 22626471084160 run.py:483] Algo bellman_ford step 51 current loss 1.385566, current_train_items 1664.
I0302 18:58:15.014533 22626471084160 run.py:483] Algo bellman_ford step 52 current loss 1.863980, current_train_items 1696.
I0302 18:58:15.043614 22626471084160 run.py:483] Algo bellman_ford step 53 current loss 2.843511, current_train_items 1728.
I0302 18:58:15.075977 22626471084160 run.py:483] Algo bellman_ford step 54 current loss 1192.244141, current_train_items 1760.
I0302 18:58:15.095165 22626471084160 run.py:483] Algo bellman_ford step 55 current loss 0.853115, current_train_items 1792.
I0302 18:58:15.111311 22626471084160 run.py:483] Algo bellman_ford step 56 current loss 1.284982, current_train_items 1824.
I0302 18:58:15.133795 22626471084160 run.py:483] Algo bellman_ford step 57 current loss 2.035473, current_train_items 1856.
I0302 18:58:15.161997 22626471084160 run.py:483] Algo bellman_ford step 58 current loss 1.871039, current_train_items 1888.
I0302 18:58:15.195001 22626471084160 run.py:483] Algo bellman_ford step 59 current loss 4.507406, current_train_items 1920.
I0302 18:58:15.213574 22626471084160 run.py:483] Algo bellman_ford step 60 current loss 0.746356, current_train_items 1952.
W0302 18:58:15.222860 22626471084160 samplers.py:155] Increasing hint lengh from 6 to 7
I0302 18:58:21.440059 22626471084160 run.py:483] Algo bellman_ford step 61 current loss 1.086146, current_train_items 1984.
I0302 18:58:21.464335 22626471084160 run.py:483] Algo bellman_ford step 62 current loss 1.669134, current_train_items 2016.
I0302 18:58:21.493921 22626471084160 run.py:483] Algo bellman_ford step 63 current loss 2.005651, current_train_items 2048.
I0302 18:58:21.528317 22626471084160 run.py:483] Algo bellman_ford step 64 current loss 2.583184, current_train_items 2080.
I0302 18:58:21.547956 22626471084160 run.py:483] Algo bellman_ford step 65 current loss 0.737862, current_train_items 2112.
I0302 18:58:21.564045 22626471084160 run.py:483] Algo bellman_ford step 66 current loss 1.120968, current_train_items 2144.
I0302 18:58:21.588783 22626471084160 run.py:483] Algo bellman_ford step 67 current loss 1.908792, current_train_items 2176.
I0302 18:58:21.617036 22626471084160 run.py:483] Algo bellman_ford step 68 current loss 1.831656, current_train_items 2208.
I0302 18:58:21.649184 22626471084160 run.py:483] Algo bellman_ford step 69 current loss 2.169566, current_train_items 2240.
I0302 18:58:21.667757 22626471084160 run.py:483] Algo bellman_ford step 70 current loss 0.673865, current_train_items 2272.
I0302 18:58:21.684196 22626471084160 run.py:483] Algo bellman_ford step 71 current loss 1.186922, current_train_items 2304.
I0302 18:58:21.707630 22626471084160 run.py:483] Algo bellman_ford step 72 current loss 1.560850, current_train_items 2336.
I0302 18:58:21.737496 22626471084160 run.py:483] Algo bellman_ford step 73 current loss 1.812166, current_train_items 2368.
I0302 18:58:21.770241 22626471084160 run.py:483] Algo bellman_ford step 74 current loss 2.061261, current_train_items 2400.
I0302 18:58:21.788820 22626471084160 run.py:483] Algo bellman_ford step 75 current loss 0.557212, current_train_items 2432.
I0302 18:58:21.805582 22626471084160 run.py:483] Algo bellman_ford step 76 current loss 1.225836, current_train_items 2464.
I0302 18:58:21.828695 22626471084160 run.py:483] Algo bellman_ford step 77 current loss 1.723077, current_train_items 2496.
I0302 18:58:21.857425 22626471084160 run.py:483] Algo bellman_ford step 78 current loss 1.799763, current_train_items 2528.
I0302 18:58:21.887020 22626471084160 run.py:483] Algo bellman_ford step 79 current loss 466.588165, current_train_items 2560.
I0302 18:58:21.906059 22626471084160 run.py:483] Algo bellman_ford step 80 current loss 0.644494, current_train_items 2592.
I0302 18:58:21.922223 22626471084160 run.py:483] Algo bellman_ford step 81 current loss 1.125291, current_train_items 2624.
I0302 18:58:21.945415 22626471084160 run.py:483] Algo bellman_ford step 82 current loss 1.676591, current_train_items 2656.
I0302 18:58:21.974124 22626471084160 run.py:483] Algo bellman_ford step 83 current loss 1.717581, current_train_items 2688.
I0302 18:58:22.004835 22626471084160 run.py:483] Algo bellman_ford step 84 current loss 1.831728, current_train_items 2720.
I0302 18:58:22.023190 22626471084160 run.py:483] Algo bellman_ford step 85 current loss 0.718557, current_train_items 2752.
I0302 18:58:22.039595 22626471084160 run.py:483] Algo bellman_ford step 86 current loss 1.228471, current_train_items 2784.
I0302 18:58:22.063968 22626471084160 run.py:483] Algo bellman_ford step 87 current loss 1.874586, current_train_items 2816.
I0302 18:58:22.093909 22626471084160 run.py:483] Algo bellman_ford step 88 current loss 1.854595, current_train_items 2848.
I0302 18:58:22.125776 22626471084160 run.py:483] Algo bellman_ford step 89 current loss 2.044672, current_train_items 2880.
I0302 18:58:22.144586 22626471084160 run.py:483] Algo bellman_ford step 90 current loss 0.629318, current_train_items 2912.
I0302 18:58:22.160969 22626471084160 run.py:483] Algo bellman_ford step 91 current loss 1.141823, current_train_items 2944.
I0302 18:58:22.183698 22626471084160 run.py:483] Algo bellman_ford step 92 current loss 1.612481, current_train_items 2976.
I0302 18:58:22.214230 22626471084160 run.py:483] Algo bellman_ford step 93 current loss 1.821157, current_train_items 3008.
I0302 18:58:22.245515 22626471084160 run.py:483] Algo bellman_ford step 94 current loss 2.052076, current_train_items 3040.
I0302 18:58:22.264642 22626471084160 run.py:483] Algo bellman_ford step 95 current loss 0.626141, current_train_items 3072.
I0302 18:58:22.280604 22626471084160 run.py:483] Algo bellman_ford step 96 current loss 1.025266, current_train_items 3104.
I0302 18:58:22.303889 22626471084160 run.py:483] Algo bellman_ford step 97 current loss 1.439082, current_train_items 3136.
I0302 18:58:22.333193 22626471084160 run.py:483] Algo bellman_ford step 98 current loss 1.569150, current_train_items 3168.
I0302 18:58:22.365141 22626471084160 run.py:483] Algo bellman_ford step 99 current loss 1.930393, current_train_items 3200.
I0302 18:58:22.383695 22626471084160 run.py:483] Algo bellman_ford step 100 current loss 0.598756, current_train_items 3232.
I0302 18:58:22.393682 22626471084160 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.7451171875, 'score': 0.7451171875, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0302 18:58:22.393795 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.660, current avg val score is 0.745, val scores are: bellman_ford: 0.745
I0302 18:58:22.423105 22626471084160 run.py:483] Algo bellman_ford step 101 current loss 1.087835, current_train_items 3264.
I0302 18:58:22.446042 22626471084160 run.py:483] Algo bellman_ford step 102 current loss 1.284035, current_train_items 3296.
I0302 18:58:22.475224 22626471084160 run.py:483] Algo bellman_ford step 103 current loss 1.500044, current_train_items 3328.
I0302 18:58:22.509852 22626471084160 run.py:483] Algo bellman_ford step 104 current loss 1.976945, current_train_items 3360.
I0302 18:58:22.528964 22626471084160 run.py:483] Algo bellman_ford step 105 current loss 0.541987, current_train_items 3392.
I0302 18:58:22.545269 22626471084160 run.py:483] Algo bellman_ford step 106 current loss 0.942570, current_train_items 3424.
I0302 18:58:22.568500 22626471084160 run.py:483] Algo bellman_ford step 107 current loss 1.253932, current_train_items 3456.
I0302 18:58:22.597403 22626471084160 run.py:483] Algo bellman_ford step 108 current loss 1.678827, current_train_items 3488.
I0302 18:58:22.628075 22626471084160 run.py:483] Algo bellman_ford step 109 current loss 1.672658, current_train_items 3520.
I0302 18:58:22.646836 22626471084160 run.py:483] Algo bellman_ford step 110 current loss 0.600672, current_train_items 3552.
I0302 18:58:22.662892 22626471084160 run.py:483] Algo bellman_ford step 111 current loss 0.954122, current_train_items 3584.
I0302 18:58:22.685585 22626471084160 run.py:483] Algo bellman_ford step 112 current loss 1.181414, current_train_items 3616.
I0302 18:58:22.714916 22626471084160 run.py:483] Algo bellman_ford step 113 current loss 1.606926, current_train_items 3648.
I0302 18:58:22.745950 22626471084160 run.py:483] Algo bellman_ford step 114 current loss 1.680487, current_train_items 3680.
I0302 18:58:22.764109 22626471084160 run.py:483] Algo bellman_ford step 115 current loss 0.582388, current_train_items 3712.
I0302 18:58:22.780544 22626471084160 run.py:483] Algo bellman_ford step 116 current loss 1.084034, current_train_items 3744.
I0302 18:58:22.804617 22626471084160 run.py:483] Algo bellman_ford step 117 current loss 1.428066, current_train_items 3776.
I0302 18:58:22.833302 22626471084160 run.py:483] Algo bellman_ford step 118 current loss 1.377112, current_train_items 3808.
I0302 18:58:22.862952 22626471084160 run.py:483] Algo bellman_ford step 119 current loss 1.617434, current_train_items 3840.
I0302 18:58:22.881453 22626471084160 run.py:483] Algo bellman_ford step 120 current loss 0.565615, current_train_items 3872.
I0302 18:58:22.897869 22626471084160 run.py:483] Algo bellman_ford step 121 current loss 0.961310, current_train_items 3904.
I0302 18:58:22.921282 22626471084160 run.py:483] Algo bellman_ford step 122 current loss 1.426622, current_train_items 3936.
I0302 18:58:22.951530 22626471084160 run.py:483] Algo bellman_ford step 123 current loss 1.569036, current_train_items 3968.
I0302 18:58:22.987386 22626471084160 run.py:483] Algo bellman_ford step 124 current loss 1.934496, current_train_items 4000.
I0302 18:58:23.005823 22626471084160 run.py:483] Algo bellman_ford step 125 current loss 0.686907, current_train_items 4032.
I0302 18:58:23.022312 22626471084160 run.py:483] Algo bellman_ford step 126 current loss 1.076934, current_train_items 4064.
I0302 18:58:23.045491 22626471084160 run.py:483] Algo bellman_ford step 127 current loss 1.402664, current_train_items 4096.
I0302 18:58:23.074908 22626471084160 run.py:483] Algo bellman_ford step 128 current loss 1.449162, current_train_items 4128.
I0302 18:58:23.106958 22626471084160 run.py:483] Algo bellman_ford step 129 current loss 1.499203, current_train_items 4160.
I0302 18:58:23.125600 22626471084160 run.py:483] Algo bellman_ford step 130 current loss 0.532962, current_train_items 4192.
I0302 18:58:23.141746 22626471084160 run.py:483] Algo bellman_ford step 131 current loss 0.809980, current_train_items 4224.
I0302 18:58:23.165481 22626471084160 run.py:483] Algo bellman_ford step 132 current loss 1.532907, current_train_items 4256.
I0302 18:58:23.194514 22626471084160 run.py:483] Algo bellman_ford step 133 current loss 1.605904, current_train_items 4288.
I0302 18:58:23.225739 22626471084160 run.py:483] Algo bellman_ford step 134 current loss 1.893925, current_train_items 4320.
I0302 18:58:23.243857 22626471084160 run.py:483] Algo bellman_ford step 135 current loss 0.544975, current_train_items 4352.
I0302 18:58:23.260076 22626471084160 run.py:483] Algo bellman_ford step 136 current loss 0.977806, current_train_items 4384.
I0302 18:58:23.283788 22626471084160 run.py:483] Algo bellman_ford step 137 current loss 1.424859, current_train_items 4416.
I0302 18:58:23.313098 22626471084160 run.py:483] Algo bellman_ford step 138 current loss 1.387404, current_train_items 4448.
I0302 18:58:23.345752 22626471084160 run.py:483] Algo bellman_ford step 139 current loss 1.781734, current_train_items 4480.
I0302 18:58:23.364077 22626471084160 run.py:483] Algo bellman_ford step 140 current loss 0.455229, current_train_items 4512.
I0302 18:58:23.380775 22626471084160 run.py:483] Algo bellman_ford step 141 current loss 0.990033, current_train_items 4544.
I0302 18:58:23.403629 22626471084160 run.py:483] Algo bellman_ford step 142 current loss 1.293529, current_train_items 4576.
I0302 18:58:23.433730 22626471084160 run.py:483] Algo bellman_ford step 143 current loss 1.617859, current_train_items 4608.
I0302 18:58:23.465223 22626471084160 run.py:483] Algo bellman_ford step 144 current loss 1.567525, current_train_items 4640.
I0302 18:58:23.483540 22626471084160 run.py:483] Algo bellman_ford step 145 current loss 0.528775, current_train_items 4672.
I0302 18:58:23.499664 22626471084160 run.py:483] Algo bellman_ford step 146 current loss 0.904318, current_train_items 4704.
I0302 18:58:23.522347 22626471084160 run.py:483] Algo bellman_ford step 147 current loss 1.296282, current_train_items 4736.
I0302 18:58:23.551687 22626471084160 run.py:483] Algo bellman_ford step 148 current loss 1.466606, current_train_items 4768.
I0302 18:58:23.581949 22626471084160 run.py:483] Algo bellman_ford step 149 current loss 1.470279, current_train_items 4800.
I0302 18:58:23.600626 22626471084160 run.py:483] Algo bellman_ford step 150 current loss 0.565486, current_train_items 4832.
I0302 18:58:23.608803 22626471084160 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.8544921875, 'score': 0.8544921875, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0302 18:58:23.608920 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.745, current avg val score is 0.854, val scores are: bellman_ford: 0.854
I0302 18:58:23.637295 22626471084160 run.py:483] Algo bellman_ford step 151 current loss 0.718679, current_train_items 4864.
I0302 18:58:23.660626 22626471084160 run.py:483] Algo bellman_ford step 152 current loss 1.150802, current_train_items 4896.
I0302 18:58:23.689942 22626471084160 run.py:483] Algo bellman_ford step 153 current loss 1.340304, current_train_items 4928.
I0302 18:58:23.726061 22626471084160 run.py:483] Algo bellman_ford step 154 current loss 1.782623, current_train_items 4960.
I0302 18:58:23.745395 22626471084160 run.py:483] Algo bellman_ford step 155 current loss 0.592379, current_train_items 4992.
I0302 18:58:23.761729 22626471084160 run.py:483] Algo bellman_ford step 156 current loss 0.968519, current_train_items 5024.
I0302 18:58:23.784658 22626471084160 run.py:483] Algo bellman_ford step 157 current loss 1.169663, current_train_items 5056.
I0302 18:58:23.812704 22626471084160 run.py:483] Algo bellman_ford step 158 current loss 1.294748, current_train_items 5088.
I0302 18:58:23.844647 22626471084160 run.py:483] Algo bellman_ford step 159 current loss 1.487679, current_train_items 5120.
I0302 18:58:23.863679 22626471084160 run.py:483] Algo bellman_ford step 160 current loss 0.516566, current_train_items 5152.
I0302 18:58:23.880069 22626471084160 run.py:483] Algo bellman_ford step 161 current loss 0.825824, current_train_items 5184.
I0302 18:58:23.902163 22626471084160 run.py:483] Algo bellman_ford step 162 current loss 1.193373, current_train_items 5216.
I0302 18:58:23.931219 22626471084160 run.py:483] Algo bellman_ford step 163 current loss 1.562416, current_train_items 5248.
I0302 18:58:23.962384 22626471084160 run.py:483] Algo bellman_ford step 164 current loss 1.897635, current_train_items 5280.
I0302 18:58:23.981072 22626471084160 run.py:483] Algo bellman_ford step 165 current loss 0.561178, current_train_items 5312.
I0302 18:58:23.997642 22626471084160 run.py:483] Algo bellman_ford step 166 current loss 0.995967, current_train_items 5344.
I0302 18:58:24.020364 22626471084160 run.py:483] Algo bellman_ford step 167 current loss 1.075012, current_train_items 5376.
I0302 18:58:24.050000 22626471084160 run.py:483] Algo bellman_ford step 168 current loss 1.367659, current_train_items 5408.
I0302 18:58:24.081634 22626471084160 run.py:483] Algo bellman_ford step 169 current loss 1.511220, current_train_items 5440.
I0302 18:58:24.100428 22626471084160 run.py:483] Algo bellman_ford step 170 current loss 0.663081, current_train_items 5472.
I0302 18:58:24.116695 22626471084160 run.py:483] Algo bellman_ford step 171 current loss 0.936204, current_train_items 5504.
I0302 18:58:24.138594 22626471084160 run.py:483] Algo bellman_ford step 172 current loss 1.310659, current_train_items 5536.
I0302 18:58:24.168133 22626471084160 run.py:483] Algo bellman_ford step 173 current loss 1.483691, current_train_items 5568.
I0302 18:58:24.203524 22626471084160 run.py:483] Algo bellman_ford step 174 current loss 1.793046, current_train_items 5600.
I0302 18:58:24.221829 22626471084160 run.py:483] Algo bellman_ford step 175 current loss 0.510637, current_train_items 5632.
I0302 18:58:24.237877 22626471084160 run.py:483] Algo bellman_ford step 176 current loss 0.919603, current_train_items 5664.
I0302 18:58:24.261192 22626471084160 run.py:483] Algo bellman_ford step 177 current loss 1.287301, current_train_items 5696.
I0302 18:58:24.289132 22626471084160 run.py:483] Algo bellman_ford step 178 current loss 1.172974, current_train_items 5728.
I0302 18:58:24.319485 22626471084160 run.py:483] Algo bellman_ford step 179 current loss 1.619170, current_train_items 5760.
I0302 18:58:24.338451 22626471084160 run.py:483] Algo bellman_ford step 180 current loss 0.607263, current_train_items 5792.
I0302 18:58:24.354713 22626471084160 run.py:483] Algo bellman_ford step 181 current loss 0.809244, current_train_items 5824.
I0302 18:58:24.378721 22626471084160 run.py:483] Algo bellman_ford step 182 current loss 1.286909, current_train_items 5856.
I0302 18:58:24.407710 22626471084160 run.py:483] Algo bellman_ford step 183 current loss 1.315142, current_train_items 5888.
I0302 18:58:24.437735 22626471084160 run.py:483] Algo bellman_ford step 184 current loss 1.858110, current_train_items 5920.
I0302 18:58:24.456350 22626471084160 run.py:483] Algo bellman_ford step 185 current loss 0.419984, current_train_items 5952.
I0302 18:58:24.472872 22626471084160 run.py:483] Algo bellman_ford step 186 current loss 0.903443, current_train_items 5984.
I0302 18:58:24.495191 22626471084160 run.py:483] Algo bellman_ford step 187 current loss 1.144472, current_train_items 6016.
I0302 18:58:24.523979 22626471084160 run.py:483] Algo bellman_ford step 188 current loss 1.563124, current_train_items 6048.
I0302 18:58:24.557336 22626471084160 run.py:483] Algo bellman_ford step 189 current loss 1.748075, current_train_items 6080.
I0302 18:58:24.575915 22626471084160 run.py:483] Algo bellman_ford step 190 current loss 0.434456, current_train_items 6112.
I0302 18:58:24.592523 22626471084160 run.py:483] Algo bellman_ford step 191 current loss 0.777618, current_train_items 6144.
I0302 18:58:24.616209 22626471084160 run.py:483] Algo bellman_ford step 192 current loss 1.437164, current_train_items 6176.
I0302 18:58:24.645409 22626471084160 run.py:483] Algo bellman_ford step 193 current loss 1.414348, current_train_items 6208.
I0302 18:58:24.676759 22626471084160 run.py:483] Algo bellman_ford step 194 current loss 1.555231, current_train_items 6240.
I0302 18:58:24.695858 22626471084160 run.py:483] Algo bellman_ford step 195 current loss 0.480990, current_train_items 6272.
I0302 18:58:24.712019 22626471084160 run.py:483] Algo bellman_ford step 196 current loss 0.796830, current_train_items 6304.
I0302 18:58:24.734961 22626471084160 run.py:483] Algo bellman_ford step 197 current loss 1.232867, current_train_items 6336.
I0302 18:58:24.765148 22626471084160 run.py:483] Algo bellman_ford step 198 current loss 1.565772, current_train_items 6368.
I0302 18:58:24.797342 22626471084160 run.py:483] Algo bellman_ford step 199 current loss 1.752324, current_train_items 6400.
I0302 18:58:24.815757 22626471084160 run.py:483] Algo bellman_ford step 200 current loss 0.494786, current_train_items 6432.
I0302 18:58:24.823572 22626471084160 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.830078125, 'score': 0.830078125, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0302 18:58:24.823680 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.830, val scores are: bellman_ford: 0.830
I0302 18:58:24.840403 22626471084160 run.py:483] Algo bellman_ford step 201 current loss 0.839936, current_train_items 6464.
I0302 18:58:24.864441 22626471084160 run.py:483] Algo bellman_ford step 202 current loss 1.273237, current_train_items 6496.
I0302 18:58:24.895946 22626471084160 run.py:483] Algo bellman_ford step 203 current loss 1.555954, current_train_items 6528.
I0302 18:58:24.931343 22626471084160 run.py:483] Algo bellman_ford step 204 current loss 1.724946, current_train_items 6560.
I0302 18:58:24.950498 22626471084160 run.py:483] Algo bellman_ford step 205 current loss 0.512370, current_train_items 6592.
I0302 18:58:24.965855 22626471084160 run.py:483] Algo bellman_ford step 206 current loss 0.783334, current_train_items 6624.
I0302 18:58:24.989662 22626471084160 run.py:483] Algo bellman_ford step 207 current loss 1.311166, current_train_items 6656.
I0302 18:58:25.019172 22626471084160 run.py:483] Algo bellman_ford step 208 current loss 1.326114, current_train_items 6688.
I0302 18:58:25.049864 22626471084160 run.py:483] Algo bellman_ford step 209 current loss 1.406197, current_train_items 6720.
I0302 18:58:25.068531 22626471084160 run.py:483] Algo bellman_ford step 210 current loss 0.650625, current_train_items 6752.
I0302 18:58:25.084851 22626471084160 run.py:483] Algo bellman_ford step 211 current loss 0.781301, current_train_items 6784.
I0302 18:58:25.108510 22626471084160 run.py:483] Algo bellman_ford step 212 current loss 1.452759, current_train_items 6816.
I0302 18:58:25.138514 22626471084160 run.py:483] Algo bellman_ford step 213 current loss 1.442294, current_train_items 6848.
I0302 18:58:25.166903 22626471084160 run.py:483] Algo bellman_ford step 214 current loss 1.354549, current_train_items 6880.
I0302 18:58:25.185698 22626471084160 run.py:483] Algo bellman_ford step 215 current loss 0.479698, current_train_items 6912.
I0302 18:58:25.201776 22626471084160 run.py:483] Algo bellman_ford step 216 current loss 0.768531, current_train_items 6944.
I0302 18:58:25.224704 22626471084160 run.py:483] Algo bellman_ford step 217 current loss 1.227604, current_train_items 6976.
I0302 18:58:25.253696 22626471084160 run.py:483] Algo bellman_ford step 218 current loss 1.319241, current_train_items 7008.
I0302 18:58:25.284286 22626471084160 run.py:483] Algo bellman_ford step 219 current loss 1.486982, current_train_items 7040.
I0302 18:58:25.302887 22626471084160 run.py:483] Algo bellman_ford step 220 current loss 0.443177, current_train_items 7072.
I0302 18:58:25.319184 22626471084160 run.py:483] Algo bellman_ford step 221 current loss 0.876785, current_train_items 7104.
I0302 18:58:25.342942 22626471084160 run.py:483] Algo bellman_ford step 222 current loss 1.240268, current_train_items 7136.
I0302 18:58:25.371828 22626471084160 run.py:483] Algo bellman_ford step 223 current loss 1.386457, current_train_items 7168.
I0302 18:58:25.403539 22626471084160 run.py:483] Algo bellman_ford step 224 current loss 1.614935, current_train_items 7200.
I0302 18:58:25.422195 22626471084160 run.py:483] Algo bellman_ford step 225 current loss 0.495590, current_train_items 7232.
I0302 18:58:25.438426 22626471084160 run.py:483] Algo bellman_ford step 226 current loss 0.933838, current_train_items 7264.
I0302 18:58:25.461889 22626471084160 run.py:483] Algo bellman_ford step 227 current loss 1.269256, current_train_items 7296.
I0302 18:58:25.491497 22626471084160 run.py:483] Algo bellman_ford step 228 current loss 1.325079, current_train_items 7328.
I0302 18:58:25.524105 22626471084160 run.py:483] Algo bellman_ford step 229 current loss 1.763592, current_train_items 7360.
I0302 18:58:25.542259 22626471084160 run.py:483] Algo bellman_ford step 230 current loss 0.521175, current_train_items 7392.
I0302 18:58:25.558443 22626471084160 run.py:483] Algo bellman_ford step 231 current loss 0.832066, current_train_items 7424.
I0302 18:58:25.582074 22626471084160 run.py:483] Algo bellman_ford step 232 current loss 1.237190, current_train_items 7456.
I0302 18:58:25.611873 22626471084160 run.py:483] Algo bellman_ford step 233 current loss 1.342588, current_train_items 7488.
I0302 18:58:25.644845 22626471084160 run.py:483] Algo bellman_ford step 234 current loss 1.587972, current_train_items 7520.
I0302 18:58:25.663604 22626471084160 run.py:483] Algo bellman_ford step 235 current loss 0.552001, current_train_items 7552.
I0302 18:58:25.679563 22626471084160 run.py:483] Algo bellman_ford step 236 current loss 0.804442, current_train_items 7584.
I0302 18:58:25.702498 22626471084160 run.py:483] Algo bellman_ford step 237 current loss 1.201136, current_train_items 7616.
I0302 18:58:25.731904 22626471084160 run.py:483] Algo bellman_ford step 238 current loss 1.220710, current_train_items 7648.
I0302 18:58:25.763631 22626471084160 run.py:483] Algo bellman_ford step 239 current loss 1.404373, current_train_items 7680.
I0302 18:58:25.782035 22626471084160 run.py:483] Algo bellman_ford step 240 current loss 0.481639, current_train_items 7712.
I0302 18:58:25.798274 22626471084160 run.py:483] Algo bellman_ford step 241 current loss 0.966912, current_train_items 7744.
I0302 18:58:25.821500 22626471084160 run.py:483] Algo bellman_ford step 242 current loss 1.112218, current_train_items 7776.
I0302 18:58:25.850160 22626471084160 run.py:483] Algo bellman_ford step 243 current loss 1.344043, current_train_items 7808.
I0302 18:58:25.879335 22626471084160 run.py:483] Algo bellman_ford step 244 current loss 1.333591, current_train_items 7840.
I0302 18:58:25.897958 22626471084160 run.py:483] Algo bellman_ford step 245 current loss 0.457964, current_train_items 7872.
I0302 18:58:25.914087 22626471084160 run.py:483] Algo bellman_ford step 246 current loss 0.792011, current_train_items 7904.
I0302 18:58:25.936651 22626471084160 run.py:483] Algo bellman_ford step 247 current loss 1.210941, current_train_items 7936.
I0302 18:58:25.966638 22626471084160 run.py:483] Algo bellman_ford step 248 current loss 1.400805, current_train_items 7968.
I0302 18:58:25.998562 22626471084160 run.py:483] Algo bellman_ford step 249 current loss 1.299996, current_train_items 8000.
I0302 18:58:26.017358 22626471084160 run.py:483] Algo bellman_ford step 250 current loss 0.543184, current_train_items 8032.
I0302 18:58:26.025560 22626471084160 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.8212890625, 'score': 0.8212890625, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0302 18:58:26.025667 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.821, val scores are: bellman_ford: 0.821
I0302 18:58:26.042857 22626471084160 run.py:483] Algo bellman_ford step 251 current loss 0.926380, current_train_items 8064.
I0302 18:58:26.066978 22626471084160 run.py:483] Algo bellman_ford step 252 current loss 1.230208, current_train_items 8096.
I0302 18:58:26.097383 22626471084160 run.py:483] Algo bellman_ford step 253 current loss 1.239426, current_train_items 8128.
I0302 18:58:26.130773 22626471084160 run.py:483] Algo bellman_ford step 254 current loss 1.509900, current_train_items 8160.
I0302 18:58:26.149874 22626471084160 run.py:483] Algo bellman_ford step 255 current loss 0.564323, current_train_items 8192.
I0302 18:58:26.165543 22626471084160 run.py:483] Algo bellman_ford step 256 current loss 0.812565, current_train_items 8224.
I0302 18:58:26.188012 22626471084160 run.py:483] Algo bellman_ford step 257 current loss 1.024938, current_train_items 8256.
I0302 18:58:26.217130 22626471084160 run.py:483] Algo bellman_ford step 258 current loss 1.206952, current_train_items 8288.
I0302 18:58:26.247424 22626471084160 run.py:483] Algo bellman_ford step 259 current loss 1.395191, current_train_items 8320.
I0302 18:58:26.266300 22626471084160 run.py:483] Algo bellman_ford step 260 current loss 0.498053, current_train_items 8352.
I0302 18:58:26.282371 22626471084160 run.py:483] Algo bellman_ford step 261 current loss 0.804985, current_train_items 8384.
I0302 18:58:26.304860 22626471084160 run.py:483] Algo bellman_ford step 262 current loss 1.140384, current_train_items 8416.
I0302 18:58:26.333548 22626471084160 run.py:483] Algo bellman_ford step 263 current loss 1.177403, current_train_items 8448.
I0302 18:58:26.363960 22626471084160 run.py:483] Algo bellman_ford step 264 current loss 1.307791, current_train_items 8480.
I0302 18:58:26.382658 22626471084160 run.py:483] Algo bellman_ford step 265 current loss 0.414006, current_train_items 8512.
I0302 18:58:26.399000 22626471084160 run.py:483] Algo bellman_ford step 266 current loss 0.793830, current_train_items 8544.
I0302 18:58:26.422983 22626471084160 run.py:483] Algo bellman_ford step 267 current loss 1.186214, current_train_items 8576.
I0302 18:58:26.453357 22626471084160 run.py:483] Algo bellman_ford step 268 current loss 1.332198, current_train_items 8608.
I0302 18:58:26.483677 22626471084160 run.py:483] Algo bellman_ford step 269 current loss 1.366917, current_train_items 8640.
I0302 18:58:26.502731 22626471084160 run.py:483] Algo bellman_ford step 270 current loss 0.462609, current_train_items 8672.
I0302 18:58:26.518486 22626471084160 run.py:483] Algo bellman_ford step 271 current loss 0.653598, current_train_items 8704.
I0302 18:58:26.542329 22626471084160 run.py:483] Algo bellman_ford step 272 current loss 0.994430, current_train_items 8736.
I0302 18:58:26.571416 22626471084160 run.py:483] Algo bellman_ford step 273 current loss 1.269106, current_train_items 8768.
I0302 18:58:26.601965 22626471084160 run.py:483] Algo bellman_ford step 274 current loss 1.392063, current_train_items 8800.
I0302 18:58:26.620919 22626471084160 run.py:483] Algo bellman_ford step 275 current loss 0.497071, current_train_items 8832.
I0302 18:58:26.637205 22626471084160 run.py:483] Algo bellman_ford step 276 current loss 0.673213, current_train_items 8864.
I0302 18:58:26.661083 22626471084160 run.py:483] Algo bellman_ford step 277 current loss 1.290597, current_train_items 8896.
I0302 18:58:26.691119 22626471084160 run.py:483] Algo bellman_ford step 278 current loss 1.522052, current_train_items 8928.
I0302 18:58:26.722378 22626471084160 run.py:483] Algo bellman_ford step 279 current loss 1.451218, current_train_items 8960.
I0302 18:58:26.740763 22626471084160 run.py:483] Algo bellman_ford step 280 current loss 0.421819, current_train_items 8992.
I0302 18:58:26.757182 22626471084160 run.py:483] Algo bellman_ford step 281 current loss 0.862708, current_train_items 9024.
I0302 18:58:26.780683 22626471084160 run.py:483] Algo bellman_ford step 282 current loss 1.052926, current_train_items 9056.
I0302 18:58:26.809740 22626471084160 run.py:483] Algo bellman_ford step 283 current loss 1.254493, current_train_items 9088.
I0302 18:58:26.842796 22626471084160 run.py:483] Algo bellman_ford step 284 current loss 1.773434, current_train_items 9120.
I0302 18:58:26.861291 22626471084160 run.py:483] Algo bellman_ford step 285 current loss 0.471302, current_train_items 9152.
I0302 18:58:26.877659 22626471084160 run.py:483] Algo bellman_ford step 286 current loss 0.822843, current_train_items 9184.
I0302 18:58:26.900190 22626471084160 run.py:483] Algo bellman_ford step 287 current loss 1.129511, current_train_items 9216.
I0302 18:58:26.929996 22626471084160 run.py:483] Algo bellman_ford step 288 current loss 1.410376, current_train_items 9248.
I0302 18:58:26.962566 22626471084160 run.py:483] Algo bellman_ford step 289 current loss 1.551187, current_train_items 9280.
I0302 18:58:26.981393 22626471084160 run.py:483] Algo bellman_ford step 290 current loss 0.415138, current_train_items 9312.
I0302 18:58:26.997545 22626471084160 run.py:483] Algo bellman_ford step 291 current loss 0.828539, current_train_items 9344.
I0302 18:58:27.020380 22626471084160 run.py:483] Algo bellman_ford step 292 current loss 1.003876, current_train_items 9376.
I0302 18:58:27.049227 22626471084160 run.py:483] Algo bellman_ford step 293 current loss 1.221422, current_train_items 9408.
I0302 18:58:27.080952 22626471084160 run.py:483] Algo bellman_ford step 294 current loss 1.341946, current_train_items 9440.
I0302 18:58:27.099273 22626471084160 run.py:483] Algo bellman_ford step 295 current loss 0.462299, current_train_items 9472.
I0302 18:58:27.115122 22626471084160 run.py:483] Algo bellman_ford step 296 current loss 0.741872, current_train_items 9504.
I0302 18:58:27.139042 22626471084160 run.py:483] Algo bellman_ford step 297 current loss 1.045605, current_train_items 9536.
I0302 18:58:27.169171 22626471084160 run.py:483] Algo bellman_ford step 298 current loss 1.200901, current_train_items 9568.
I0302 18:58:27.200069 22626471084160 run.py:483] Algo bellman_ford step 299 current loss 1.491507, current_train_items 9600.
I0302 18:58:27.218958 22626471084160 run.py:483] Algo bellman_ford step 300 current loss 0.456634, current_train_items 9632.
I0302 18:58:27.226791 22626471084160 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.841796875, 'score': 0.841796875, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0302 18:58:27.226900 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.842, val scores are: bellman_ford: 0.842
I0302 18:58:27.243634 22626471084160 run.py:483] Algo bellman_ford step 301 current loss 0.741706, current_train_items 9664.
I0302 18:58:27.266225 22626471084160 run.py:483] Algo bellman_ford step 302 current loss 1.030988, current_train_items 9696.
I0302 18:58:27.295043 22626471084160 run.py:483] Algo bellman_ford step 303 current loss 1.139779, current_train_items 9728.
I0302 18:58:27.327570 22626471084160 run.py:483] Algo bellman_ford step 304 current loss 1.608724, current_train_items 9760.
I0302 18:58:27.346782 22626471084160 run.py:483] Algo bellman_ford step 305 current loss 0.484057, current_train_items 9792.
I0302 18:58:27.363327 22626471084160 run.py:483] Algo bellman_ford step 306 current loss 0.965299, current_train_items 9824.
I0302 18:58:27.387210 22626471084160 run.py:483] Algo bellman_ford step 307 current loss 1.155033, current_train_items 9856.
I0302 18:58:27.416623 22626471084160 run.py:483] Algo bellman_ford step 308 current loss 1.256076, current_train_items 9888.
I0302 18:58:27.447824 22626471084160 run.py:483] Algo bellman_ford step 309 current loss 1.410864, current_train_items 9920.
I0302 18:58:27.466290 22626471084160 run.py:483] Algo bellman_ford step 310 current loss 0.510261, current_train_items 9952.
I0302 18:58:27.482334 22626471084160 run.py:483] Algo bellman_ford step 311 current loss 0.727022, current_train_items 9984.
I0302 18:58:27.505169 22626471084160 run.py:483] Algo bellman_ford step 312 current loss 1.110527, current_train_items 10016.
I0302 18:58:27.535171 22626471084160 run.py:483] Algo bellman_ford step 313 current loss 1.338704, current_train_items 10048.
I0302 18:58:27.566597 22626471084160 run.py:483] Algo bellman_ford step 314 current loss 1.407478, current_train_items 10080.
I0302 18:58:27.584819 22626471084160 run.py:483] Algo bellman_ford step 315 current loss 0.533237, current_train_items 10112.
I0302 18:58:27.601088 22626471084160 run.py:483] Algo bellman_ford step 316 current loss 0.764756, current_train_items 10144.
I0302 18:58:27.623527 22626471084160 run.py:483] Algo bellman_ford step 317 current loss 0.921463, current_train_items 10176.
I0302 18:58:27.651756 22626471084160 run.py:483] Algo bellman_ford step 318 current loss 1.112837, current_train_items 10208.
I0302 18:58:27.682364 22626471084160 run.py:483] Algo bellman_ford step 319 current loss 1.535438, current_train_items 10240.
I0302 18:58:27.700434 22626471084160 run.py:483] Algo bellman_ford step 320 current loss 0.437495, current_train_items 10272.
I0302 18:58:27.716866 22626471084160 run.py:483] Algo bellman_ford step 321 current loss 0.926877, current_train_items 10304.
I0302 18:58:27.739684 22626471084160 run.py:483] Algo bellman_ford step 322 current loss 1.097624, current_train_items 10336.
I0302 18:58:27.768005 22626471084160 run.py:483] Algo bellman_ford step 323 current loss 1.030841, current_train_items 10368.
I0302 18:58:27.799530 22626471084160 run.py:483] Algo bellman_ford step 324 current loss 1.457497, current_train_items 10400.
I0302 18:58:27.817982 22626471084160 run.py:483] Algo bellman_ford step 325 current loss 0.525854, current_train_items 10432.
I0302 18:58:27.833750 22626471084160 run.py:483] Algo bellman_ford step 326 current loss 0.720443, current_train_items 10464.
I0302 18:58:27.856754 22626471084160 run.py:483] Algo bellman_ford step 327 current loss 1.134923, current_train_items 10496.
I0302 18:58:27.886585 22626471084160 run.py:483] Algo bellman_ford step 328 current loss 1.511364, current_train_items 10528.
I0302 18:58:27.918605 22626471084160 run.py:483] Algo bellman_ford step 329 current loss 1.490268, current_train_items 10560.
I0302 18:58:27.936877 22626471084160 run.py:483] Algo bellman_ford step 330 current loss 0.463129, current_train_items 10592.
I0302 18:58:27.953049 22626471084160 run.py:483] Algo bellman_ford step 331 current loss 0.817720, current_train_items 10624.
I0302 18:58:27.975960 22626471084160 run.py:483] Algo bellman_ford step 332 current loss 1.248877, current_train_items 10656.
I0302 18:58:28.005937 22626471084160 run.py:483] Algo bellman_ford step 333 current loss 1.375527, current_train_items 10688.
I0302 18:58:28.038862 22626471084160 run.py:483] Algo bellman_ford step 334 current loss 1.821783, current_train_items 10720.
I0302 18:58:28.057392 22626471084160 run.py:483] Algo bellman_ford step 335 current loss 0.537808, current_train_items 10752.
I0302 18:58:28.073994 22626471084160 run.py:483] Algo bellman_ford step 336 current loss 1.010403, current_train_items 10784.
I0302 18:58:28.098147 22626471084160 run.py:483] Algo bellman_ford step 337 current loss 1.273057, current_train_items 10816.
I0302 18:58:28.127432 22626471084160 run.py:483] Algo bellman_ford step 338 current loss 1.223592, current_train_items 10848.
I0302 18:58:28.160036 22626471084160 run.py:483] Algo bellman_ford step 339 current loss 1.344651, current_train_items 10880.
I0302 18:58:28.178345 22626471084160 run.py:483] Algo bellman_ford step 340 current loss 0.467780, current_train_items 10912.
I0302 18:58:28.194307 22626471084160 run.py:483] Algo bellman_ford step 341 current loss 0.898626, current_train_items 10944.
I0302 18:58:28.217260 22626471084160 run.py:483] Algo bellman_ford step 342 current loss 1.254677, current_train_items 10976.
I0302 18:58:28.245692 22626471084160 run.py:483] Algo bellman_ford step 343 current loss 1.232636, current_train_items 11008.
I0302 18:58:28.277896 22626471084160 run.py:483] Algo bellman_ford step 344 current loss 1.440796, current_train_items 11040.
I0302 18:58:28.296406 22626471084160 run.py:483] Algo bellman_ford step 345 current loss 0.528619, current_train_items 11072.
I0302 18:58:28.312670 22626471084160 run.py:483] Algo bellman_ford step 346 current loss 0.879948, current_train_items 11104.
I0302 18:58:28.336580 22626471084160 run.py:483] Algo bellman_ford step 347 current loss 1.437485, current_train_items 11136.
I0302 18:58:28.365567 22626471084160 run.py:483] Algo bellman_ford step 348 current loss 1.477898, current_train_items 11168.
I0302 18:58:28.397933 22626471084160 run.py:483] Algo bellman_ford step 349 current loss 2.064971, current_train_items 11200.
I0302 18:58:28.416552 22626471084160 run.py:483] Algo bellman_ford step 350 current loss 0.553262, current_train_items 11232.
I0302 18:58:28.424453 22626471084160 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.7861328125, 'score': 0.7861328125, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0302 18:58:28.424558 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.786, val scores are: bellman_ford: 0.786
I0302 18:58:28.441130 22626471084160 run.py:483] Algo bellman_ford step 351 current loss 0.822621, current_train_items 11264.
I0302 18:58:28.464941 22626471084160 run.py:483] Algo bellman_ford step 352 current loss 1.175663, current_train_items 11296.
I0302 18:58:28.494600 22626471084160 run.py:483] Algo bellman_ford step 353 current loss 1.472828, current_train_items 11328.
I0302 18:58:28.528746 22626471084160 run.py:483] Algo bellman_ford step 354 current loss 1.655347, current_train_items 11360.
I0302 18:58:28.547993 22626471084160 run.py:483] Algo bellman_ford step 355 current loss 0.460227, current_train_items 11392.
I0302 18:58:28.563759 22626471084160 run.py:483] Algo bellman_ford step 356 current loss 0.764131, current_train_items 11424.
I0302 18:58:28.586412 22626471084160 run.py:483] Algo bellman_ford step 357 current loss 0.979132, current_train_items 11456.
I0302 18:58:28.617176 22626471084160 run.py:483] Algo bellman_ford step 358 current loss 1.359551, current_train_items 11488.
I0302 18:58:28.650535 22626471084160 run.py:483] Algo bellman_ford step 359 current loss 1.622767, current_train_items 11520.
I0302 18:58:28.669543 22626471084160 run.py:483] Algo bellman_ford step 360 current loss 0.434841, current_train_items 11552.
I0302 18:58:28.686063 22626471084160 run.py:483] Algo bellman_ford step 361 current loss 0.722664, current_train_items 11584.
I0302 18:58:28.708034 22626471084160 run.py:483] Algo bellman_ford step 362 current loss 0.977137, current_train_items 11616.
I0302 18:58:28.737519 22626471084160 run.py:483] Algo bellman_ford step 363 current loss 1.246343, current_train_items 11648.
I0302 18:58:28.767963 22626471084160 run.py:483] Algo bellman_ford step 364 current loss 1.198747, current_train_items 11680.
I0302 18:58:28.786614 22626471084160 run.py:483] Algo bellman_ford step 365 current loss 0.464599, current_train_items 11712.
I0302 18:58:28.802719 22626471084160 run.py:483] Algo bellman_ford step 366 current loss 0.894315, current_train_items 11744.
I0302 18:58:28.825217 22626471084160 run.py:483] Algo bellman_ford step 367 current loss 0.923668, current_train_items 11776.
I0302 18:58:28.855160 22626471084160 run.py:483] Algo bellman_ford step 368 current loss 1.270748, current_train_items 11808.
I0302 18:58:28.885067 22626471084160 run.py:483] Algo bellman_ford step 369 current loss 1.291153, current_train_items 11840.
I0302 18:58:28.903782 22626471084160 run.py:483] Algo bellman_ford step 370 current loss 0.436022, current_train_items 11872.
I0302 18:58:28.919738 22626471084160 run.py:483] Algo bellman_ford step 371 current loss 0.651751, current_train_items 11904.
I0302 18:58:28.942053 22626471084160 run.py:483] Algo bellman_ford step 372 current loss 1.035814, current_train_items 11936.
I0302 18:58:28.972335 22626471084160 run.py:483] Algo bellman_ford step 373 current loss 1.182126, current_train_items 11968.
I0302 18:58:29.005757 22626471084160 run.py:483] Algo bellman_ford step 374 current loss 1.648348, current_train_items 12000.
I0302 18:58:29.024460 22626471084160 run.py:483] Algo bellman_ford step 375 current loss 0.440113, current_train_items 12032.
I0302 18:58:29.040337 22626471084160 run.py:483] Algo bellman_ford step 376 current loss 0.709379, current_train_items 12064.
I0302 18:58:29.064088 22626471084160 run.py:483] Algo bellman_ford step 377 current loss 1.092407, current_train_items 12096.
I0302 18:58:29.093364 22626471084160 run.py:483] Algo bellman_ford step 378 current loss 1.070587, current_train_items 12128.
I0302 18:58:29.126218 22626471084160 run.py:483] Algo bellman_ford step 379 current loss 1.502229, current_train_items 12160.
I0302 18:58:29.144993 22626471084160 run.py:483] Algo bellman_ford step 380 current loss 0.422777, current_train_items 12192.
I0302 18:58:29.161427 22626471084160 run.py:483] Algo bellman_ford step 381 current loss 0.816507, current_train_items 12224.
I0302 18:58:29.184722 22626471084160 run.py:483] Algo bellman_ford step 382 current loss 1.101624, current_train_items 12256.
I0302 18:58:29.214258 22626471084160 run.py:483] Algo bellman_ford step 383 current loss 1.383240, current_train_items 12288.
I0302 18:58:29.247252 22626471084160 run.py:483] Algo bellman_ford step 384 current loss 1.596316, current_train_items 12320.
I0302 18:58:29.266131 22626471084160 run.py:483] Algo bellman_ford step 385 current loss 0.498174, current_train_items 12352.
I0302 18:58:29.282535 22626471084160 run.py:483] Algo bellman_ford step 386 current loss 0.717807, current_train_items 12384.
I0302 18:58:29.305260 22626471084160 run.py:483] Algo bellman_ford step 387 current loss 1.159947, current_train_items 12416.
I0302 18:58:29.334522 22626471084160 run.py:483] Algo bellman_ford step 388 current loss 1.173670, current_train_items 12448.
I0302 18:58:29.367486 22626471084160 run.py:483] Algo bellman_ford step 389 current loss 1.417970, current_train_items 12480.
I0302 18:58:29.386663 22626471084160 run.py:483] Algo bellman_ford step 390 current loss 0.428386, current_train_items 12512.
I0302 18:58:29.402649 22626471084160 run.py:483] Algo bellman_ford step 391 current loss 0.809087, current_train_items 12544.
I0302 18:58:29.425749 22626471084160 run.py:483] Algo bellman_ford step 392 current loss 1.039770, current_train_items 12576.
I0302 18:58:29.456846 22626471084160 run.py:483] Algo bellman_ford step 393 current loss 1.353951, current_train_items 12608.
I0302 18:58:29.488536 22626471084160 run.py:483] Algo bellman_ford step 394 current loss 1.311839, current_train_items 12640.
I0302 18:58:29.506733 22626471084160 run.py:483] Algo bellman_ford step 395 current loss 0.410481, current_train_items 12672.
I0302 18:58:29.523009 22626471084160 run.py:483] Algo bellman_ford step 396 current loss 0.781632, current_train_items 12704.
I0302 18:58:29.546711 22626471084160 run.py:483] Algo bellman_ford step 397 current loss 1.233194, current_train_items 12736.
I0302 18:58:29.577201 22626471084160 run.py:483] Algo bellman_ford step 398 current loss 1.147039, current_train_items 12768.
I0302 18:58:29.607335 22626471084160 run.py:483] Algo bellman_ford step 399 current loss 1.360975, current_train_items 12800.
I0302 18:58:29.626240 22626471084160 run.py:483] Algo bellman_ford step 400 current loss 0.376320, current_train_items 12832.
I0302 18:58:29.634016 22626471084160 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.8408203125, 'score': 0.8408203125, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0302 18:58:29.634121 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.841, val scores are: bellman_ford: 0.841
I0302 18:58:29.651067 22626471084160 run.py:483] Algo bellman_ford step 401 current loss 0.698934, current_train_items 12864.
I0302 18:58:29.674970 22626471084160 run.py:483] Algo bellman_ford step 402 current loss 0.966829, current_train_items 12896.
I0302 18:58:29.705189 22626471084160 run.py:483] Algo bellman_ford step 403 current loss 1.329124, current_train_items 12928.
I0302 18:58:29.736171 22626471084160 run.py:483] Algo bellman_ford step 404 current loss 1.586284, current_train_items 12960.
I0302 18:58:29.755312 22626471084160 run.py:483] Algo bellman_ford step 405 current loss 0.516821, current_train_items 12992.
I0302 18:58:29.770641 22626471084160 run.py:483] Algo bellman_ford step 406 current loss 0.692308, current_train_items 13024.
I0302 18:58:29.793834 22626471084160 run.py:483] Algo bellman_ford step 407 current loss 1.036426, current_train_items 13056.
I0302 18:58:29.824254 22626471084160 run.py:483] Algo bellman_ford step 408 current loss 1.141632, current_train_items 13088.
I0302 18:58:29.856102 22626471084160 run.py:483] Algo bellman_ford step 409 current loss 1.359112, current_train_items 13120.
I0302 18:58:29.874586 22626471084160 run.py:483] Algo bellman_ford step 410 current loss 0.438442, current_train_items 13152.
I0302 18:58:29.890478 22626471084160 run.py:483] Algo bellman_ford step 411 current loss 0.673018, current_train_items 13184.
I0302 18:58:29.913886 22626471084160 run.py:483] Algo bellman_ford step 412 current loss 1.059150, current_train_items 13216.
I0302 18:58:29.943675 22626471084160 run.py:483] Algo bellman_ford step 413 current loss 1.143058, current_train_items 13248.
I0302 18:58:29.976246 22626471084160 run.py:483] Algo bellman_ford step 414 current loss 1.362221, current_train_items 13280.
I0302 18:58:29.994948 22626471084160 run.py:483] Algo bellman_ford step 415 current loss 0.481257, current_train_items 13312.
I0302 18:58:30.010857 22626471084160 run.py:483] Algo bellman_ford step 416 current loss 0.636810, current_train_items 13344.
I0302 18:58:30.034618 22626471084160 run.py:483] Algo bellman_ford step 417 current loss 1.075969, current_train_items 13376.
I0302 18:58:30.066323 22626471084160 run.py:483] Algo bellman_ford step 418 current loss 1.345805, current_train_items 13408.
I0302 18:58:30.098277 22626471084160 run.py:483] Algo bellman_ford step 419 current loss 1.395594, current_train_items 13440.
I0302 18:58:30.116836 22626471084160 run.py:483] Algo bellman_ford step 420 current loss 0.491485, current_train_items 13472.
I0302 18:58:30.132766 22626471084160 run.py:483] Algo bellman_ford step 421 current loss 0.720329, current_train_items 13504.
I0302 18:58:30.156281 22626471084160 run.py:483] Algo bellman_ford step 422 current loss 1.065451, current_train_items 13536.
I0302 18:58:30.185008 22626471084160 run.py:483] Algo bellman_ford step 423 current loss 1.077989, current_train_items 13568.
I0302 18:58:30.216596 22626471084160 run.py:483] Algo bellman_ford step 424 current loss 1.320716, current_train_items 13600.
I0302 18:58:30.235159 22626471084160 run.py:483] Algo bellman_ford step 425 current loss 0.468539, current_train_items 13632.
I0302 18:58:30.250939 22626471084160 run.py:483] Algo bellman_ford step 426 current loss 0.518431, current_train_items 13664.
I0302 18:58:30.273336 22626471084160 run.py:483] Algo bellman_ford step 427 current loss 1.057074, current_train_items 13696.
I0302 18:58:30.302536 22626471084160 run.py:483] Algo bellman_ford step 428 current loss 1.078944, current_train_items 13728.
I0302 18:58:30.334605 22626471084160 run.py:483] Algo bellman_ford step 429 current loss 1.312485, current_train_items 13760.
I0302 18:58:30.353259 22626471084160 run.py:483] Algo bellman_ford step 430 current loss 0.503469, current_train_items 13792.
I0302 18:58:30.369402 22626471084160 run.py:483] Algo bellman_ford step 431 current loss 0.826803, current_train_items 13824.
I0302 18:58:30.393242 22626471084160 run.py:483] Algo bellman_ford step 432 current loss 1.247929, current_train_items 13856.
I0302 18:58:30.421389 22626471084160 run.py:483] Algo bellman_ford step 433 current loss 1.268869, current_train_items 13888.
I0302 18:58:30.452878 22626471084160 run.py:483] Algo bellman_ford step 434 current loss 1.609958, current_train_items 13920.
I0302 18:58:30.471291 22626471084160 run.py:483] Algo bellman_ford step 435 current loss 0.423775, current_train_items 13952.
I0302 18:58:30.487783 22626471084160 run.py:483] Algo bellman_ford step 436 current loss 0.677683, current_train_items 13984.
I0302 18:58:30.510555 22626471084160 run.py:483] Algo bellman_ford step 437 current loss 1.074190, current_train_items 14016.
I0302 18:58:30.539483 22626471084160 run.py:483] Algo bellman_ford step 438 current loss 1.027107, current_train_items 14048.
I0302 18:58:30.571372 22626471084160 run.py:483] Algo bellman_ford step 439 current loss 1.189588, current_train_items 14080.
I0302 18:58:30.589745 22626471084160 run.py:483] Algo bellman_ford step 440 current loss 0.432609, current_train_items 14112.
I0302 18:58:30.605824 22626471084160 run.py:483] Algo bellman_ford step 441 current loss 0.741278, current_train_items 14144.
I0302 18:58:30.628566 22626471084160 run.py:483] Algo bellman_ford step 442 current loss 1.318857, current_train_items 14176.
I0302 18:58:30.657615 22626471084160 run.py:483] Algo bellman_ford step 443 current loss 1.274474, current_train_items 14208.
I0302 18:58:30.688590 22626471084160 run.py:483] Algo bellman_ford step 444 current loss 1.394325, current_train_items 14240.
I0302 18:58:30.707103 22626471084160 run.py:483] Algo bellman_ford step 445 current loss 0.414920, current_train_items 14272.
I0302 18:58:30.724058 22626471084160 run.py:483] Algo bellman_ford step 446 current loss 1.125494, current_train_items 14304.
I0302 18:58:30.747746 22626471084160 run.py:483] Algo bellman_ford step 447 current loss 1.117721, current_train_items 14336.
I0302 18:58:30.775401 22626471084160 run.py:483] Algo bellman_ford step 448 current loss 1.286704, current_train_items 14368.
I0302 18:58:30.805543 22626471084160 run.py:483] Algo bellman_ford step 449 current loss 1.938922, current_train_items 14400.
I0302 18:58:30.824355 22626471084160 run.py:483] Algo bellman_ford step 450 current loss 0.591052, current_train_items 14432.
I0302 18:58:30.832399 22626471084160 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.8203125, 'score': 0.8203125, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0302 18:58:30.832503 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.820, val scores are: bellman_ford: 0.820
I0302 18:58:30.849559 22626471084160 run.py:483] Algo bellman_ford step 451 current loss 0.799604, current_train_items 14464.
I0302 18:58:30.872797 22626471084160 run.py:483] Algo bellman_ford step 452 current loss 0.978784, current_train_items 14496.
I0302 18:58:30.903847 22626471084160 run.py:483] Algo bellman_ford step 453 current loss 1.299220, current_train_items 14528.
I0302 18:58:30.935918 22626471084160 run.py:483] Algo bellman_ford step 454 current loss 1.362134, current_train_items 14560.
I0302 18:58:30.954784 22626471084160 run.py:483] Algo bellman_ford step 455 current loss 0.535155, current_train_items 14592.
I0302 18:58:30.970952 22626471084160 run.py:483] Algo bellman_ford step 456 current loss 0.704879, current_train_items 14624.
I0302 18:58:30.993866 22626471084160 run.py:483] Algo bellman_ford step 457 current loss 1.087292, current_train_items 14656.
I0302 18:58:31.022997 22626471084160 run.py:483] Algo bellman_ford step 458 current loss 1.134002, current_train_items 14688.
I0302 18:58:31.053785 22626471084160 run.py:483] Algo bellman_ford step 459 current loss 1.614043, current_train_items 14720.
I0302 18:58:31.072352 22626471084160 run.py:483] Algo bellman_ford step 460 current loss 0.406169, current_train_items 14752.
I0302 18:58:31.088892 22626471084160 run.py:483] Algo bellman_ford step 461 current loss 0.819730, current_train_items 14784.
I0302 18:58:31.111871 22626471084160 run.py:483] Algo bellman_ford step 462 current loss 0.975975, current_train_items 14816.
I0302 18:58:31.141642 22626471084160 run.py:483] Algo bellman_ford step 463 current loss 1.271883, current_train_items 14848.
I0302 18:58:31.174606 22626471084160 run.py:483] Algo bellman_ford step 464 current loss 1.448139, current_train_items 14880.
I0302 18:58:31.193127 22626471084160 run.py:483] Algo bellman_ford step 465 current loss 0.452841, current_train_items 14912.
I0302 18:58:31.208838 22626471084160 run.py:483] Algo bellman_ford step 466 current loss 0.644280, current_train_items 14944.
I0302 18:58:31.232344 22626471084160 run.py:483] Algo bellman_ford step 467 current loss 1.082870, current_train_items 14976.
I0302 18:58:31.260623 22626471084160 run.py:483] Algo bellman_ford step 468 current loss 1.329583, current_train_items 15008.
I0302 18:58:31.293766 22626471084160 run.py:483] Algo bellman_ford step 469 current loss 1.323036, current_train_items 15040.
I0302 18:58:31.312558 22626471084160 run.py:483] Algo bellman_ford step 470 current loss 0.406111, current_train_items 15072.
I0302 18:58:31.328763 22626471084160 run.py:483] Algo bellman_ford step 471 current loss 0.693184, current_train_items 15104.
I0302 18:58:31.352236 22626471084160 run.py:483] Algo bellman_ford step 472 current loss 1.076937, current_train_items 15136.
I0302 18:58:31.382278 22626471084160 run.py:483] Algo bellman_ford step 473 current loss 1.273945, current_train_items 15168.
I0302 18:58:31.414889 22626471084160 run.py:483] Algo bellman_ford step 474 current loss 1.735279, current_train_items 15200.
I0302 18:58:31.433948 22626471084160 run.py:483] Algo bellman_ford step 475 current loss 0.455001, current_train_items 15232.
I0302 18:58:31.449786 22626471084160 run.py:483] Algo bellman_ford step 476 current loss 0.744148, current_train_items 15264.
I0302 18:58:31.473335 22626471084160 run.py:483] Algo bellman_ford step 477 current loss 0.965270, current_train_items 15296.
I0302 18:58:31.501322 22626471084160 run.py:483] Algo bellman_ford step 478 current loss 1.112690, current_train_items 15328.
I0302 18:58:31.533944 22626471084160 run.py:483] Algo bellman_ford step 479 current loss 1.545897, current_train_items 15360.
I0302 18:58:31.552542 22626471084160 run.py:483] Algo bellman_ford step 480 current loss 0.478090, current_train_items 15392.
I0302 18:58:31.568748 22626471084160 run.py:483] Algo bellman_ford step 481 current loss 0.807200, current_train_items 15424.
I0302 18:58:31.591518 22626471084160 run.py:483] Algo bellman_ford step 482 current loss 0.951250, current_train_items 15456.
I0302 18:58:31.620895 22626471084160 run.py:483] Algo bellman_ford step 483 current loss 1.114901, current_train_items 15488.
I0302 18:58:31.653393 22626471084160 run.py:483] Algo bellman_ford step 484 current loss 1.416476, current_train_items 15520.
I0302 18:58:31.672210 22626471084160 run.py:483] Algo bellman_ford step 485 current loss 0.473658, current_train_items 15552.
I0302 18:58:31.688438 22626471084160 run.py:483] Algo bellman_ford step 486 current loss 0.833676, current_train_items 15584.
I0302 18:58:31.711810 22626471084160 run.py:483] Algo bellman_ford step 487 current loss 1.044718, current_train_items 15616.
I0302 18:58:31.741011 22626471084160 run.py:483] Algo bellman_ford step 488 current loss 1.168541, current_train_items 15648.
I0302 18:58:31.771864 22626471084160 run.py:483] Algo bellman_ford step 489 current loss 1.165627, current_train_items 15680.
I0302 18:58:31.790603 22626471084160 run.py:483] Algo bellman_ford step 490 current loss 0.487927, current_train_items 15712.
I0302 18:58:31.806937 22626471084160 run.py:483] Algo bellman_ford step 491 current loss 0.720833, current_train_items 15744.
I0302 18:58:31.829643 22626471084160 run.py:483] Algo bellman_ford step 492 current loss 1.016741, current_train_items 15776.
I0302 18:58:31.858669 22626471084160 run.py:483] Algo bellman_ford step 493 current loss 1.314889, current_train_items 15808.
I0302 18:58:31.891948 22626471084160 run.py:483] Algo bellman_ford step 494 current loss 1.609259, current_train_items 15840.
I0302 18:58:31.910351 22626471084160 run.py:483] Algo bellman_ford step 495 current loss 0.443616, current_train_items 15872.
I0302 18:58:31.926234 22626471084160 run.py:483] Algo bellman_ford step 496 current loss 0.706613, current_train_items 15904.
I0302 18:58:31.950375 22626471084160 run.py:483] Algo bellman_ford step 497 current loss 1.148219, current_train_items 15936.
I0302 18:58:31.980642 22626471084160 run.py:483] Algo bellman_ford step 498 current loss 1.204608, current_train_items 15968.
I0302 18:58:32.012591 22626471084160 run.py:483] Algo bellman_ford step 499 current loss 1.382900, current_train_items 16000.
I0302 18:58:32.031304 22626471084160 run.py:483] Algo bellman_ford step 500 current loss 0.460164, current_train_items 16032.
I0302 18:58:32.038998 22626471084160 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.8544921875, 'score': 0.8544921875, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0302 18:58:32.039106 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.854, val scores are: bellman_ford: 0.854
I0302 18:58:32.055183 22626471084160 run.py:483] Algo bellman_ford step 501 current loss 0.606375, current_train_items 16064.
I0302 18:58:32.079210 22626471084160 run.py:483] Algo bellman_ford step 502 current loss 1.064041, current_train_items 16096.
I0302 18:58:32.110350 22626471084160 run.py:483] Algo bellman_ford step 503 current loss 1.251444, current_train_items 16128.
I0302 18:58:32.145398 22626471084160 run.py:483] Algo bellman_ford step 504 current loss 1.576972, current_train_items 16160.
I0302 18:58:32.164165 22626471084160 run.py:483] Algo bellman_ford step 505 current loss 0.428083, current_train_items 16192.
I0302 18:58:32.179422 22626471084160 run.py:483] Algo bellman_ford step 506 current loss 0.621637, current_train_items 16224.
I0302 18:58:32.201721 22626471084160 run.py:483] Algo bellman_ford step 507 current loss 0.906488, current_train_items 16256.
I0302 18:58:32.232096 22626471084160 run.py:483] Algo bellman_ford step 508 current loss 1.205816, current_train_items 16288.
I0302 18:58:32.264964 22626471084160 run.py:483] Algo bellman_ford step 509 current loss 1.359414, current_train_items 16320.
I0302 18:58:32.283573 22626471084160 run.py:483] Algo bellman_ford step 510 current loss 0.487614, current_train_items 16352.
I0302 18:58:32.299812 22626471084160 run.py:483] Algo bellman_ford step 511 current loss 0.787415, current_train_items 16384.
I0302 18:58:32.322921 22626471084160 run.py:483] Algo bellman_ford step 512 current loss 0.987924, current_train_items 16416.
I0302 18:58:32.353165 22626471084160 run.py:483] Algo bellman_ford step 513 current loss 1.205332, current_train_items 16448.
I0302 18:58:32.385255 22626471084160 run.py:483] Algo bellman_ford step 514 current loss 1.398150, current_train_items 16480.
I0302 18:58:32.403465 22626471084160 run.py:483] Algo bellman_ford step 515 current loss 0.423604, current_train_items 16512.
I0302 18:58:32.419426 22626471084160 run.py:483] Algo bellman_ford step 516 current loss 0.659293, current_train_items 16544.
I0302 18:58:32.443530 22626471084160 run.py:483] Algo bellman_ford step 517 current loss 0.975091, current_train_items 16576.
I0302 18:58:32.474349 22626471084160 run.py:483] Algo bellman_ford step 518 current loss 1.161703, current_train_items 16608.
I0302 18:58:32.506184 22626471084160 run.py:483] Algo bellman_ford step 519 current loss 1.503272, current_train_items 16640.
I0302 18:58:32.524552 22626471084160 run.py:483] Algo bellman_ford step 520 current loss 0.430033, current_train_items 16672.
I0302 18:58:32.541132 22626471084160 run.py:483] Algo bellman_ford step 521 current loss 0.703158, current_train_items 16704.
I0302 18:58:32.564044 22626471084160 run.py:483] Algo bellman_ford step 522 current loss 0.937081, current_train_items 16736.
I0302 18:58:32.594222 22626471084160 run.py:483] Algo bellman_ford step 523 current loss 1.184247, current_train_items 16768.
I0302 18:58:32.625484 22626471084160 run.py:483] Algo bellman_ford step 524 current loss 1.441227, current_train_items 16800.
I0302 18:58:32.643839 22626471084160 run.py:483] Algo bellman_ford step 525 current loss 0.397895, current_train_items 16832.
I0302 18:58:32.660280 22626471084160 run.py:483] Algo bellman_ford step 526 current loss 0.803274, current_train_items 16864.
I0302 18:58:32.683500 22626471084160 run.py:483] Algo bellman_ford step 527 current loss 1.081710, current_train_items 16896.
I0302 18:58:32.712047 22626471084160 run.py:483] Algo bellman_ford step 528 current loss 1.005311, current_train_items 16928.
I0302 18:58:32.744215 22626471084160 run.py:483] Algo bellman_ford step 529 current loss 1.338205, current_train_items 16960.
I0302 18:58:32.763077 22626471084160 run.py:483] Algo bellman_ford step 530 current loss 0.436544, current_train_items 16992.
I0302 18:58:32.779179 22626471084160 run.py:483] Algo bellman_ford step 531 current loss 0.827733, current_train_items 17024.
I0302 18:58:32.801801 22626471084160 run.py:483] Algo bellman_ford step 532 current loss 0.895354, current_train_items 17056.
I0302 18:58:32.831435 22626471084160 run.py:483] Algo bellman_ford step 533 current loss 1.415525, current_train_items 17088.
I0302 18:58:32.863722 22626471084160 run.py:483] Algo bellman_ford step 534 current loss 1.382851, current_train_items 17120.
I0302 18:58:32.882303 22626471084160 run.py:483] Algo bellman_ford step 535 current loss 0.447063, current_train_items 17152.
I0302 18:58:32.898103 22626471084160 run.py:483] Algo bellman_ford step 536 current loss 0.617535, current_train_items 17184.
I0302 18:58:32.920747 22626471084160 run.py:483] Algo bellman_ford step 537 current loss 0.981450, current_train_items 17216.
I0302 18:58:32.949602 22626471084160 run.py:483] Algo bellman_ford step 538 current loss 1.065250, current_train_items 17248.
I0302 18:58:32.980783 22626471084160 run.py:483] Algo bellman_ford step 539 current loss 1.230630, current_train_items 17280.
I0302 18:58:32.999476 22626471084160 run.py:483] Algo bellman_ford step 540 current loss 0.372376, current_train_items 17312.
I0302 18:58:33.015079 22626471084160 run.py:483] Algo bellman_ford step 541 current loss 0.614272, current_train_items 17344.
I0302 18:58:33.038452 22626471084160 run.py:483] Algo bellman_ford step 542 current loss 1.154183, current_train_items 17376.
I0302 18:58:33.068535 22626471084160 run.py:483] Algo bellman_ford step 543 current loss 1.107571, current_train_items 17408.
I0302 18:58:33.101411 22626471084160 run.py:483] Algo bellman_ford step 544 current loss 1.382340, current_train_items 17440.
I0302 18:58:33.119871 22626471084160 run.py:483] Algo bellman_ford step 545 current loss 0.485541, current_train_items 17472.
I0302 18:58:33.136061 22626471084160 run.py:483] Algo bellman_ford step 546 current loss 0.775919, current_train_items 17504.
I0302 18:58:33.159069 22626471084160 run.py:483] Algo bellman_ford step 547 current loss 1.049390, current_train_items 17536.
I0302 18:58:33.189191 22626471084160 run.py:483] Algo bellman_ford step 548 current loss 1.411681, current_train_items 17568.
I0302 18:58:33.218840 22626471084160 run.py:483] Algo bellman_ford step 549 current loss 1.271982, current_train_items 17600.
I0302 18:58:33.237263 22626471084160 run.py:483] Algo bellman_ford step 550 current loss 0.464014, current_train_items 17632.
I0302 18:58:33.245309 22626471084160 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.8466796875, 'score': 0.8466796875, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0302 18:58:33.245415 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.847, val scores are: bellman_ford: 0.847
I0302 18:58:33.262163 22626471084160 run.py:483] Algo bellman_ford step 551 current loss 0.644350, current_train_items 17664.
I0302 18:58:33.285012 22626471084160 run.py:483] Algo bellman_ford step 552 current loss 0.879342, current_train_items 17696.
I0302 18:58:33.316122 22626471084160 run.py:483] Algo bellman_ford step 553 current loss 1.170887, current_train_items 17728.
I0302 18:58:33.347856 22626471084160 run.py:483] Algo bellman_ford step 554 current loss 1.280439, current_train_items 17760.
I0302 18:58:33.366741 22626471084160 run.py:483] Algo bellman_ford step 555 current loss 0.447352, current_train_items 17792.
I0302 18:58:33.382535 22626471084160 run.py:483] Algo bellman_ford step 556 current loss 0.642089, current_train_items 17824.
I0302 18:58:33.406368 22626471084160 run.py:483] Algo bellman_ford step 557 current loss 1.124711, current_train_items 17856.
I0302 18:58:33.435189 22626471084160 run.py:483] Algo bellman_ford step 558 current loss 1.347404, current_train_items 17888.
I0302 18:58:33.468468 22626471084160 run.py:483] Algo bellman_ford step 559 current loss 1.520971, current_train_items 17920.
I0302 18:58:33.487385 22626471084160 run.py:483] Algo bellman_ford step 560 current loss 0.545840, current_train_items 17952.
I0302 18:58:33.503620 22626471084160 run.py:483] Algo bellman_ford step 561 current loss 0.609822, current_train_items 17984.
I0302 18:58:33.526791 22626471084160 run.py:483] Algo bellman_ford step 562 current loss 1.023210, current_train_items 18016.
I0302 18:58:33.555521 22626471084160 run.py:483] Algo bellman_ford step 563 current loss 1.157722, current_train_items 18048.
I0302 18:58:33.585824 22626471084160 run.py:483] Algo bellman_ford step 564 current loss 1.344543, current_train_items 18080.
I0302 18:58:33.604086 22626471084160 run.py:483] Algo bellman_ford step 565 current loss 0.454632, current_train_items 18112.
I0302 18:58:33.620392 22626471084160 run.py:483] Algo bellman_ford step 566 current loss 0.809359, current_train_items 18144.
I0302 18:58:33.644307 22626471084160 run.py:483] Algo bellman_ford step 567 current loss 1.200078, current_train_items 18176.
I0302 18:58:33.673431 22626471084160 run.py:483] Algo bellman_ford step 568 current loss 1.054678, current_train_items 18208.
I0302 18:58:33.704320 22626471084160 run.py:483] Algo bellman_ford step 569 current loss 1.309447, current_train_items 18240.
I0302 18:58:33.723339 22626471084160 run.py:483] Algo bellman_ford step 570 current loss 0.469163, current_train_items 18272.
I0302 18:58:33.739278 22626471084160 run.py:483] Algo bellman_ford step 571 current loss 0.737613, current_train_items 18304.
I0302 18:58:33.762657 22626471084160 run.py:483] Algo bellman_ford step 572 current loss 1.144827, current_train_items 18336.
I0302 18:58:33.793718 22626471084160 run.py:483] Algo bellman_ford step 573 current loss 1.242050, current_train_items 18368.
I0302 18:58:33.824518 22626471084160 run.py:483] Algo bellman_ford step 574 current loss 1.316022, current_train_items 18400.
I0302 18:58:33.843411 22626471084160 run.py:483] Algo bellman_ford step 575 current loss 0.501738, current_train_items 18432.
I0302 18:58:33.859327 22626471084160 run.py:483] Algo bellman_ford step 576 current loss 0.805425, current_train_items 18464.
I0302 18:58:33.882333 22626471084160 run.py:483] Algo bellman_ford step 577 current loss 1.114019, current_train_items 18496.
I0302 18:58:33.912180 22626471084160 run.py:483] Algo bellman_ford step 578 current loss 1.175264, current_train_items 18528.
I0302 18:58:33.944995 22626471084160 run.py:483] Algo bellman_ford step 579 current loss 1.451551, current_train_items 18560.
I0302 18:58:33.963232 22626471084160 run.py:483] Algo bellman_ford step 580 current loss 0.455006, current_train_items 18592.
I0302 18:58:33.979126 22626471084160 run.py:483] Algo bellman_ford step 581 current loss 0.701612, current_train_items 18624.
I0302 18:58:34.000786 22626471084160 run.py:483] Algo bellman_ford step 582 current loss 0.887071, current_train_items 18656.
I0302 18:58:34.028820 22626471084160 run.py:483] Algo bellman_ford step 583 current loss 0.897530, current_train_items 18688.
I0302 18:58:34.060348 22626471084160 run.py:483] Algo bellman_ford step 584 current loss 1.287447, current_train_items 18720.
I0302 18:58:34.078754 22626471084160 run.py:483] Algo bellman_ford step 585 current loss 0.423999, current_train_items 18752.
I0302 18:58:34.094964 22626471084160 run.py:483] Algo bellman_ford step 586 current loss 0.691989, current_train_items 18784.
I0302 18:58:34.117041 22626471084160 run.py:483] Algo bellman_ford step 587 current loss 0.758091, current_train_items 18816.
I0302 18:58:34.145838 22626471084160 run.py:483] Algo bellman_ford step 588 current loss 1.066821, current_train_items 18848.
I0302 18:58:34.178794 22626471084160 run.py:483] Algo bellman_ford step 589 current loss 1.454586, current_train_items 18880.
I0302 18:58:34.197767 22626471084160 run.py:483] Algo bellman_ford step 590 current loss 0.471486, current_train_items 18912.
I0302 18:58:34.214052 22626471084160 run.py:483] Algo bellman_ford step 591 current loss 0.751835, current_train_items 18944.
I0302 18:58:34.237430 22626471084160 run.py:483] Algo bellman_ford step 592 current loss 0.962633, current_train_items 18976.
I0302 18:58:34.266492 22626471084160 run.py:483] Algo bellman_ford step 593 current loss 1.140954, current_train_items 19008.
I0302 18:58:34.297958 22626471084160 run.py:483] Algo bellman_ford step 594 current loss 1.260378, current_train_items 19040.
I0302 18:58:34.316186 22626471084160 run.py:483] Algo bellman_ford step 595 current loss 0.387896, current_train_items 19072.
I0302 18:58:34.332358 22626471084160 run.py:483] Algo bellman_ford step 596 current loss 0.710614, current_train_items 19104.
I0302 18:58:34.356303 22626471084160 run.py:483] Algo bellman_ford step 597 current loss 0.963785, current_train_items 19136.
I0302 18:58:34.385446 22626471084160 run.py:483] Algo bellman_ford step 598 current loss 1.127905, current_train_items 19168.
I0302 18:58:34.417210 22626471084160 run.py:483] Algo bellman_ford step 599 current loss 1.205341, current_train_items 19200.
I0302 18:58:34.435856 22626471084160 run.py:483] Algo bellman_ford step 600 current loss 0.444456, current_train_items 19232.
I0302 18:58:34.443728 22626471084160 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.8427734375, 'score': 0.8427734375, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0302 18:58:34.443835 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.843, val scores are: bellman_ford: 0.843
I0302 18:58:34.460326 22626471084160 run.py:483] Algo bellman_ford step 601 current loss 0.570877, current_train_items 19264.
I0302 18:58:34.483815 22626471084160 run.py:483] Algo bellman_ford step 602 current loss 1.018157, current_train_items 19296.
I0302 18:58:34.513251 22626471084160 run.py:483] Algo bellman_ford step 603 current loss 1.223186, current_train_items 19328.
I0302 18:58:34.544667 22626471084160 run.py:483] Algo bellman_ford step 604 current loss 1.483246, current_train_items 19360.
I0302 18:58:34.563713 22626471084160 run.py:483] Algo bellman_ford step 605 current loss 0.462372, current_train_items 19392.
I0302 18:58:34.579308 22626471084160 run.py:483] Algo bellman_ford step 606 current loss 0.647166, current_train_items 19424.
I0302 18:58:34.603518 22626471084160 run.py:483] Algo bellman_ford step 607 current loss 1.049440, current_train_items 19456.
I0302 18:58:34.632282 22626471084160 run.py:483] Algo bellman_ford step 608 current loss 1.047997, current_train_items 19488.
I0302 18:58:34.664192 22626471084160 run.py:483] Algo bellman_ford step 609 current loss 1.289066, current_train_items 19520.
I0302 18:58:34.682748 22626471084160 run.py:483] Algo bellman_ford step 610 current loss 0.426532, current_train_items 19552.
I0302 18:58:34.699144 22626471084160 run.py:483] Algo bellman_ford step 611 current loss 0.748715, current_train_items 19584.
I0302 18:58:34.722499 22626471084160 run.py:483] Algo bellman_ford step 612 current loss 0.909226, current_train_items 19616.
I0302 18:58:34.750474 22626471084160 run.py:483] Algo bellman_ford step 613 current loss 1.088706, current_train_items 19648.
I0302 18:58:34.781949 22626471084160 run.py:483] Algo bellman_ford step 614 current loss 1.582994, current_train_items 19680.
I0302 18:58:34.800462 22626471084160 run.py:483] Algo bellman_ford step 615 current loss 0.368302, current_train_items 19712.
I0302 18:58:34.817078 22626471084160 run.py:483] Algo bellman_ford step 616 current loss 0.704368, current_train_items 19744.
I0302 18:58:34.840124 22626471084160 run.py:483] Algo bellman_ford step 617 current loss 0.992886, current_train_items 19776.
I0302 18:58:34.868887 22626471084160 run.py:483] Algo bellman_ford step 618 current loss 1.019821, current_train_items 19808.
I0302 18:58:34.900535 22626471084160 run.py:483] Algo bellman_ford step 619 current loss 1.231385, current_train_items 19840.
I0302 18:58:34.919400 22626471084160 run.py:483] Algo bellman_ford step 620 current loss 0.447349, current_train_items 19872.
I0302 18:58:34.936060 22626471084160 run.py:483] Algo bellman_ford step 621 current loss 0.855268, current_train_items 19904.
I0302 18:58:34.959812 22626471084160 run.py:483] Algo bellman_ford step 622 current loss 1.062130, current_train_items 19936.
I0302 18:58:34.988604 22626471084160 run.py:483] Algo bellman_ford step 623 current loss 1.239439, current_train_items 19968.
I0302 18:58:35.019998 22626471084160 run.py:483] Algo bellman_ford step 624 current loss 1.360200, current_train_items 20000.
I0302 18:58:35.038476 22626471084160 run.py:483] Algo bellman_ford step 625 current loss 0.507156, current_train_items 20032.
I0302 18:58:35.054424 22626471084160 run.py:483] Algo bellman_ford step 626 current loss 0.639038, current_train_items 20064.
I0302 18:58:35.078426 22626471084160 run.py:483] Algo bellman_ford step 627 current loss 1.012403, current_train_items 20096.
I0302 18:58:35.108773 22626471084160 run.py:483] Algo bellman_ford step 628 current loss 1.138126, current_train_items 20128.
I0302 18:58:35.141018 22626471084160 run.py:483] Algo bellman_ford step 629 current loss 1.209012, current_train_items 20160.
I0302 18:58:35.159589 22626471084160 run.py:483] Algo bellman_ford step 630 current loss 0.402670, current_train_items 20192.
I0302 18:58:35.175518 22626471084160 run.py:483] Algo bellman_ford step 631 current loss 0.669981, current_train_items 20224.
I0302 18:58:35.198748 22626471084160 run.py:483] Algo bellman_ford step 632 current loss 1.087641, current_train_items 20256.
I0302 18:58:35.227936 22626471084160 run.py:483] Algo bellman_ford step 633 current loss 1.136081, current_train_items 20288.
I0302 18:58:35.259627 22626471084160 run.py:483] Algo bellman_ford step 634 current loss 1.342275, current_train_items 20320.
I0302 18:58:35.278072 22626471084160 run.py:483] Algo bellman_ford step 635 current loss 0.473776, current_train_items 20352.
I0302 18:58:35.294140 22626471084160 run.py:483] Algo bellman_ford step 636 current loss 0.602904, current_train_items 20384.
I0302 18:58:35.317322 22626471084160 run.py:483] Algo bellman_ford step 637 current loss 0.950499, current_train_items 20416.
I0302 18:58:35.347663 22626471084160 run.py:483] Algo bellman_ford step 638 current loss 1.102764, current_train_items 20448.
I0302 18:58:35.376506 22626471084160 run.py:483] Algo bellman_ford step 639 current loss 1.287921, current_train_items 20480.
I0302 18:58:35.394997 22626471084160 run.py:483] Algo bellman_ford step 640 current loss 0.491989, current_train_items 20512.
I0302 18:58:35.410912 22626471084160 run.py:483] Algo bellman_ford step 641 current loss 0.725897, current_train_items 20544.
I0302 18:58:35.433332 22626471084160 run.py:483] Algo bellman_ford step 642 current loss 1.088704, current_train_items 20576.
I0302 18:58:35.462270 22626471084160 run.py:483] Algo bellman_ford step 643 current loss 1.107090, current_train_items 20608.
I0302 18:58:35.494448 22626471084160 run.py:483] Algo bellman_ford step 644 current loss 1.469364, current_train_items 20640.
I0302 18:58:35.513025 22626471084160 run.py:483] Algo bellman_ford step 645 current loss 0.396187, current_train_items 20672.
I0302 18:58:35.530112 22626471084160 run.py:483] Algo bellman_ford step 646 current loss 0.795761, current_train_items 20704.
I0302 18:58:35.552989 22626471084160 run.py:483] Algo bellman_ford step 647 current loss 0.896457, current_train_items 20736.
I0302 18:58:35.582998 22626471084160 run.py:483] Algo bellman_ford step 648 current loss 1.359003, current_train_items 20768.
I0302 18:58:35.615487 22626471084160 run.py:483] Algo bellman_ford step 649 current loss 1.376445, current_train_items 20800.
I0302 18:58:35.633881 22626471084160 run.py:483] Algo bellman_ford step 650 current loss 0.495988, current_train_items 20832.
I0302 18:58:35.641944 22626471084160 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.849609375, 'score': 0.849609375, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0302 18:58:35.642051 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.850, val scores are: bellman_ford: 0.850
I0302 18:58:35.659203 22626471084160 run.py:483] Algo bellman_ford step 651 current loss 0.741662, current_train_items 20864.
I0302 18:58:35.682271 22626471084160 run.py:483] Algo bellman_ford step 652 current loss 0.926420, current_train_items 20896.
I0302 18:58:35.711001 22626471084160 run.py:483] Algo bellman_ford step 653 current loss 1.000003, current_train_items 20928.
I0302 18:58:35.742539 22626471084160 run.py:483] Algo bellman_ford step 654 current loss 1.369100, current_train_items 20960.
I0302 18:58:35.761645 22626471084160 run.py:483] Algo bellman_ford step 655 current loss 0.428826, current_train_items 20992.
I0302 18:58:35.777237 22626471084160 run.py:483] Algo bellman_ford step 656 current loss 0.889170, current_train_items 21024.
I0302 18:58:35.800769 22626471084160 run.py:483] Algo bellman_ford step 657 current loss 1.055024, current_train_items 21056.
I0302 18:58:35.830723 22626471084160 run.py:483] Algo bellman_ford step 658 current loss 1.203879, current_train_items 21088.
I0302 18:58:35.861570 22626471084160 run.py:483] Algo bellman_ford step 659 current loss 1.208487, current_train_items 21120.
I0302 18:58:35.880393 22626471084160 run.py:483] Algo bellman_ford step 660 current loss 0.437472, current_train_items 21152.
I0302 18:58:35.896491 22626471084160 run.py:483] Algo bellman_ford step 661 current loss 0.641780, current_train_items 21184.
I0302 18:58:35.919067 22626471084160 run.py:483] Algo bellman_ford step 662 current loss 0.917426, current_train_items 21216.
I0302 18:58:35.948398 22626471084160 run.py:483] Algo bellman_ford step 663 current loss 1.181718, current_train_items 21248.
I0302 18:58:35.982684 22626471084160 run.py:483] Algo bellman_ford step 664 current loss 1.326630, current_train_items 21280.
I0302 18:58:36.001544 22626471084160 run.py:483] Algo bellman_ford step 665 current loss 0.443170, current_train_items 21312.
I0302 18:58:36.017649 22626471084160 run.py:483] Algo bellman_ford step 666 current loss 0.656814, current_train_items 21344.
I0302 18:58:36.041399 22626471084160 run.py:483] Algo bellman_ford step 667 current loss 1.029245, current_train_items 21376.
I0302 18:58:36.070365 22626471084160 run.py:483] Algo bellman_ford step 668 current loss 1.202345, current_train_items 21408.
I0302 18:58:36.101685 22626471084160 run.py:483] Algo bellman_ford step 669 current loss 1.309093, current_train_items 21440.
I0302 18:58:36.120588 22626471084160 run.py:483] Algo bellman_ford step 670 current loss 0.334936, current_train_items 21472.
I0302 18:58:36.136457 22626471084160 run.py:483] Algo bellman_ford step 671 current loss 0.625616, current_train_items 21504.
I0302 18:58:36.159901 22626471084160 run.py:483] Algo bellman_ford step 672 current loss 0.923328, current_train_items 21536.
I0302 18:58:36.188817 22626471084160 run.py:483] Algo bellman_ford step 673 current loss 1.330106, current_train_items 21568.
I0302 18:58:36.221564 22626471084160 run.py:483] Algo bellman_ford step 674 current loss 1.645683, current_train_items 21600.
I0302 18:58:36.240464 22626471084160 run.py:483] Algo bellman_ford step 675 current loss 0.386451, current_train_items 21632.
I0302 18:58:36.256038 22626471084160 run.py:483] Algo bellman_ford step 676 current loss 0.635060, current_train_items 21664.
I0302 18:58:36.278809 22626471084160 run.py:483] Algo bellman_ford step 677 current loss 1.198286, current_train_items 21696.
I0302 18:58:36.309030 22626471084160 run.py:483] Algo bellman_ford step 678 current loss 1.188930, current_train_items 21728.
I0302 18:58:36.341930 22626471084160 run.py:483] Algo bellman_ford step 679 current loss 1.310501, current_train_items 21760.
I0302 18:58:36.360714 22626471084160 run.py:483] Algo bellman_ford step 680 current loss 0.478689, current_train_items 21792.
I0302 18:58:36.376862 22626471084160 run.py:483] Algo bellman_ford step 681 current loss 0.886621, current_train_items 21824.
I0302 18:58:36.400287 22626471084160 run.py:483] Algo bellman_ford step 682 current loss 1.132679, current_train_items 21856.
I0302 18:58:36.428944 22626471084160 run.py:483] Algo bellman_ford step 683 current loss 1.077204, current_train_items 21888.
I0302 18:58:36.462055 22626471084160 run.py:483] Algo bellman_ford step 684 current loss 1.517048, current_train_items 21920.
I0302 18:58:36.480796 22626471084160 run.py:483] Algo bellman_ford step 685 current loss 0.530790, current_train_items 21952.
I0302 18:58:36.496749 22626471084160 run.py:483] Algo bellman_ford step 686 current loss 0.739782, current_train_items 21984.
I0302 18:58:36.520032 22626471084160 run.py:483] Algo bellman_ford step 687 current loss 0.930459, current_train_items 22016.
I0302 18:58:36.548389 22626471084160 run.py:483] Algo bellman_ford step 688 current loss 1.004902, current_train_items 22048.
I0302 18:58:36.582285 22626471084160 run.py:483] Algo bellman_ford step 689 current loss 1.507822, current_train_items 22080.
I0302 18:58:36.600719 22626471084160 run.py:483] Algo bellman_ford step 690 current loss 0.508588, current_train_items 22112.
I0302 18:58:36.617350 22626471084160 run.py:483] Algo bellman_ford step 691 current loss 0.842687, current_train_items 22144.
I0302 18:58:36.641116 22626471084160 run.py:483] Algo bellman_ford step 692 current loss 1.025000, current_train_items 22176.
I0302 18:58:36.669986 22626471084160 run.py:483] Algo bellman_ford step 693 current loss 1.397337, current_train_items 22208.
I0302 18:58:36.703479 22626471084160 run.py:483] Algo bellman_ford step 694 current loss 1.430985, current_train_items 22240.
I0302 18:58:36.722018 22626471084160 run.py:483] Algo bellman_ford step 695 current loss 0.481991, current_train_items 22272.
I0302 18:58:36.737940 22626471084160 run.py:483] Algo bellman_ford step 696 current loss 0.801270, current_train_items 22304.
I0302 18:58:36.759763 22626471084160 run.py:483] Algo bellman_ford step 697 current loss 1.074651, current_train_items 22336.
I0302 18:58:36.789276 22626471084160 run.py:483] Algo bellman_ford step 698 current loss 1.076561, current_train_items 22368.
I0302 18:58:36.822496 22626471084160 run.py:483] Algo bellman_ford step 699 current loss 1.359769, current_train_items 22400.
I0302 18:58:36.841254 22626471084160 run.py:483] Algo bellman_ford step 700 current loss 0.439925, current_train_items 22432.
I0302 18:58:36.849123 22626471084160 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.87109375, 'score': 0.87109375, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0302 18:58:36.849236 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.854, current avg val score is 0.871, val scores are: bellman_ford: 0.871
I0302 18:58:36.878646 22626471084160 run.py:483] Algo bellman_ford step 701 current loss 0.586277, current_train_items 22464.
I0302 18:58:36.901748 22626471084160 run.py:483] Algo bellman_ford step 702 current loss 1.064685, current_train_items 22496.
I0302 18:58:36.930281 22626471084160 run.py:483] Algo bellman_ford step 703 current loss 1.183752, current_train_items 22528.
I0302 18:58:36.961696 22626471084160 run.py:483] Algo bellman_ford step 704 current loss 1.111366, current_train_items 22560.
I0302 18:58:36.980709 22626471084160 run.py:483] Algo bellman_ford step 705 current loss 0.427583, current_train_items 22592.
I0302 18:58:36.996483 22626471084160 run.py:483] Algo bellman_ford step 706 current loss 0.725218, current_train_items 22624.
I0302 18:58:37.019727 22626471084160 run.py:483] Algo bellman_ford step 707 current loss 0.838315, current_train_items 22656.
I0302 18:58:37.048998 22626471084160 run.py:483] Algo bellman_ford step 708 current loss 0.959582, current_train_items 22688.
I0302 18:58:37.081602 22626471084160 run.py:483] Algo bellman_ford step 709 current loss 1.389671, current_train_items 22720.
I0302 18:58:37.100316 22626471084160 run.py:483] Algo bellman_ford step 710 current loss 0.396009, current_train_items 22752.
I0302 18:58:37.116473 22626471084160 run.py:483] Algo bellman_ford step 711 current loss 0.666235, current_train_items 22784.
I0302 18:58:37.140259 22626471084160 run.py:483] Algo bellman_ford step 712 current loss 1.050762, current_train_items 22816.
I0302 18:58:37.171057 22626471084160 run.py:483] Algo bellman_ford step 713 current loss 1.246611, current_train_items 22848.
I0302 18:58:37.201330 22626471084160 run.py:483] Algo bellman_ford step 714 current loss 1.381102, current_train_items 22880.
I0302 18:58:37.219734 22626471084160 run.py:483] Algo bellman_ford step 715 current loss 0.362145, current_train_items 22912.
I0302 18:58:37.236229 22626471084160 run.py:483] Algo bellman_ford step 716 current loss 0.821471, current_train_items 22944.
I0302 18:58:37.260443 22626471084160 run.py:483] Algo bellman_ford step 717 current loss 1.017810, current_train_items 22976.
I0302 18:58:37.289268 22626471084160 run.py:483] Algo bellman_ford step 718 current loss 0.966323, current_train_items 23008.
I0302 18:58:37.321124 22626471084160 run.py:483] Algo bellman_ford step 719 current loss 1.227344, current_train_items 23040.
I0302 18:58:37.339780 22626471084160 run.py:483] Algo bellman_ford step 720 current loss 0.514116, current_train_items 23072.
I0302 18:58:37.356285 22626471084160 run.py:483] Algo bellman_ford step 721 current loss 0.726541, current_train_items 23104.
I0302 18:58:37.379204 22626471084160 run.py:483] Algo bellman_ford step 722 current loss 0.883803, current_train_items 23136.
I0302 18:58:37.407884 22626471084160 run.py:483] Algo bellman_ford step 723 current loss 1.011867, current_train_items 23168.
I0302 18:58:37.439327 22626471084160 run.py:483] Algo bellman_ford step 724 current loss 1.218805, current_train_items 23200.
I0302 18:58:37.457833 22626471084160 run.py:483] Algo bellman_ford step 725 current loss 0.367721, current_train_items 23232.
I0302 18:58:37.474355 22626471084160 run.py:483] Algo bellman_ford step 726 current loss 0.690037, current_train_items 23264.
I0302 18:58:37.498017 22626471084160 run.py:483] Algo bellman_ford step 727 current loss 1.119713, current_train_items 23296.
I0302 18:58:37.527054 22626471084160 run.py:483] Algo bellman_ford step 728 current loss 1.056916, current_train_items 23328.
I0302 18:58:37.558999 22626471084160 run.py:483] Algo bellman_ford step 729 current loss 1.335853, current_train_items 23360.
I0302 18:58:37.577253 22626471084160 run.py:483] Algo bellman_ford step 730 current loss 0.361799, current_train_items 23392.
I0302 18:58:37.593435 22626471084160 run.py:483] Algo bellman_ford step 731 current loss 0.721999, current_train_items 23424.
I0302 18:58:37.616791 22626471084160 run.py:483] Algo bellman_ford step 732 current loss 0.898452, current_train_items 23456.
I0302 18:58:37.646011 22626471084160 run.py:483] Algo bellman_ford step 733 current loss 1.040869, current_train_items 23488.
I0302 18:58:37.678437 22626471084160 run.py:483] Algo bellman_ford step 734 current loss 1.333111, current_train_items 23520.
I0302 18:58:37.697290 22626471084160 run.py:483] Algo bellman_ford step 735 current loss 0.416318, current_train_items 23552.
I0302 18:58:37.713708 22626471084160 run.py:483] Algo bellman_ford step 736 current loss 0.642120, current_train_items 23584.
I0302 18:58:37.736670 22626471084160 run.py:483] Algo bellman_ford step 737 current loss 0.986882, current_train_items 23616.
I0302 18:58:37.766097 22626471084160 run.py:483] Algo bellman_ford step 738 current loss 1.175255, current_train_items 23648.
I0302 18:58:37.796118 22626471084160 run.py:483] Algo bellman_ford step 739 current loss 1.131149, current_train_items 23680.
I0302 18:58:37.814724 22626471084160 run.py:483] Algo bellman_ford step 740 current loss 0.442824, current_train_items 23712.
I0302 18:58:37.830883 22626471084160 run.py:483] Algo bellman_ford step 741 current loss 0.706001, current_train_items 23744.
I0302 18:58:37.853638 22626471084160 run.py:483] Algo bellman_ford step 742 current loss 0.869011, current_train_items 23776.
I0302 18:58:37.882178 22626471084160 run.py:483] Algo bellman_ford step 743 current loss 0.916262, current_train_items 23808.
I0302 18:58:37.915868 22626471084160 run.py:483] Algo bellman_ford step 744 current loss 1.349941, current_train_items 23840.
I0302 18:58:37.934421 22626471084160 run.py:483] Algo bellman_ford step 745 current loss 0.421672, current_train_items 23872.
I0302 18:58:37.950499 22626471084160 run.py:483] Algo bellman_ford step 746 current loss 0.719724, current_train_items 23904.
I0302 18:58:37.973305 22626471084160 run.py:483] Algo bellman_ford step 747 current loss 1.049283, current_train_items 23936.
I0302 18:58:38.003201 22626471084160 run.py:483] Algo bellman_ford step 748 current loss 1.464615, current_train_items 23968.
I0302 18:58:38.034831 22626471084160 run.py:483] Algo bellman_ford step 749 current loss 1.374896, current_train_items 24000.
I0302 18:58:38.053066 22626471084160 run.py:483] Algo bellman_ford step 750 current loss 0.456044, current_train_items 24032.
I0302 18:58:38.061360 22626471084160 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.869140625, 'score': 0.869140625, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0302 18:58:38.061473 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.871, current avg val score is 0.869, val scores are: bellman_ford: 0.869
I0302 18:58:38.078390 22626471084160 run.py:483] Algo bellman_ford step 751 current loss 0.754669, current_train_items 24064.
I0302 18:58:38.101109 22626471084160 run.py:483] Algo bellman_ford step 752 current loss 0.860459, current_train_items 24096.
I0302 18:58:38.130192 22626471084160 run.py:483] Algo bellman_ford step 753 current loss 1.015501, current_train_items 24128.
I0302 18:58:38.163295 22626471084160 run.py:483] Algo bellman_ford step 754 current loss 1.276909, current_train_items 24160.
I0302 18:58:38.182321 22626471084160 run.py:483] Algo bellman_ford step 755 current loss 0.382420, current_train_items 24192.
I0302 18:58:38.197876 22626471084160 run.py:483] Algo bellman_ford step 756 current loss 0.664460, current_train_items 24224.
I0302 18:58:38.221919 22626471084160 run.py:483] Algo bellman_ford step 757 current loss 0.913211, current_train_items 24256.
I0302 18:58:38.250935 22626471084160 run.py:483] Algo bellman_ford step 758 current loss 1.023665, current_train_items 24288.
I0302 18:58:38.282256 22626471084160 run.py:483] Algo bellman_ford step 759 current loss 1.171655, current_train_items 24320.
I0302 18:58:38.300899 22626471084160 run.py:483] Algo bellman_ford step 760 current loss 0.353676, current_train_items 24352.
I0302 18:58:38.316942 22626471084160 run.py:483] Algo bellman_ford step 761 current loss 0.791637, current_train_items 24384.
I0302 18:58:38.340113 22626471084160 run.py:483] Algo bellman_ford step 762 current loss 1.044016, current_train_items 24416.
I0302 18:58:38.368494 22626471084160 run.py:483] Algo bellman_ford step 763 current loss 0.984582, current_train_items 24448.
I0302 18:58:38.400726 22626471084160 run.py:483] Algo bellman_ford step 764 current loss 1.226683, current_train_items 24480.
I0302 18:58:38.419250 22626471084160 run.py:483] Algo bellman_ford step 765 current loss 0.425050, current_train_items 24512.
I0302 18:58:38.435861 22626471084160 run.py:483] Algo bellman_ford step 766 current loss 0.736374, current_train_items 24544.
I0302 18:58:38.459110 22626471084160 run.py:483] Algo bellman_ford step 767 current loss 0.945367, current_train_items 24576.
I0302 18:58:38.488631 22626471084160 run.py:483] Algo bellman_ford step 768 current loss 1.136809, current_train_items 24608.
I0302 18:58:38.521232 22626471084160 run.py:483] Algo bellman_ford step 769 current loss 1.297871, current_train_items 24640.
I0302 18:58:38.540224 22626471084160 run.py:483] Algo bellman_ford step 770 current loss 0.344326, current_train_items 24672.
I0302 18:58:38.556592 22626471084160 run.py:483] Algo bellman_ford step 771 current loss 0.777666, current_train_items 24704.
I0302 18:58:38.579253 22626471084160 run.py:483] Algo bellman_ford step 772 current loss 0.938285, current_train_items 24736.
I0302 18:58:38.608288 22626471084160 run.py:483] Algo bellman_ford step 773 current loss 0.950706, current_train_items 24768.
I0302 18:58:38.640967 22626471084160 run.py:483] Algo bellman_ford step 774 current loss 1.412752, current_train_items 24800.
I0302 18:58:38.659849 22626471084160 run.py:483] Algo bellman_ford step 775 current loss 0.322831, current_train_items 24832.
I0302 18:58:38.676526 22626471084160 run.py:483] Algo bellman_ford step 776 current loss 0.681816, current_train_items 24864.
I0302 18:58:38.700167 22626471084160 run.py:483] Algo bellman_ford step 777 current loss 1.010319, current_train_items 24896.
I0302 18:58:38.728677 22626471084160 run.py:483] Algo bellman_ford step 778 current loss 1.036076, current_train_items 24928.
I0302 18:58:38.760013 22626471084160 run.py:483] Algo bellman_ford step 779 current loss 1.101412, current_train_items 24960.
I0302 18:58:38.778548 22626471084160 run.py:483] Algo bellman_ford step 780 current loss 0.427179, current_train_items 24992.
I0302 18:58:38.794843 22626471084160 run.py:483] Algo bellman_ford step 781 current loss 0.770128, current_train_items 25024.
I0302 18:58:38.817475 22626471084160 run.py:483] Algo bellman_ford step 782 current loss 0.938499, current_train_items 25056.
I0302 18:58:38.846704 22626471084160 run.py:483] Algo bellman_ford step 783 current loss 1.065371, current_train_items 25088.
I0302 18:58:38.879612 22626471084160 run.py:483] Algo bellman_ford step 784 current loss 1.352314, current_train_items 25120.
I0302 18:58:38.898027 22626471084160 run.py:483] Algo bellman_ford step 785 current loss 0.410668, current_train_items 25152.
I0302 18:58:38.914462 22626471084160 run.py:483] Algo bellman_ford step 786 current loss 0.672982, current_train_items 25184.
I0302 18:58:38.937131 22626471084160 run.py:483] Algo bellman_ford step 787 current loss 0.899977, current_train_items 25216.
I0302 18:58:38.966749 22626471084160 run.py:483] Algo bellman_ford step 788 current loss 1.052402, current_train_items 25248.
I0302 18:58:38.996486 22626471084160 run.py:483] Algo bellman_ford step 789 current loss 1.136226, current_train_items 25280.
I0302 18:58:39.015331 22626471084160 run.py:483] Algo bellman_ford step 790 current loss 0.370167, current_train_items 25312.
I0302 18:58:39.031849 22626471084160 run.py:483] Algo bellman_ford step 791 current loss 0.820404, current_train_items 25344.
I0302 18:58:39.053687 22626471084160 run.py:483] Algo bellman_ford step 792 current loss 0.840885, current_train_items 25376.
I0302 18:58:39.083123 22626471084160 run.py:483] Algo bellman_ford step 793 current loss 1.137850, current_train_items 25408.
I0302 18:58:39.113851 22626471084160 run.py:483] Algo bellman_ford step 794 current loss 1.301237, current_train_items 25440.
I0302 18:58:39.132121 22626471084160 run.py:483] Algo bellman_ford step 795 current loss 0.397672, current_train_items 25472.
I0302 18:58:39.147833 22626471084160 run.py:483] Algo bellman_ford step 796 current loss 0.588561, current_train_items 25504.
I0302 18:58:39.171190 22626471084160 run.py:483] Algo bellman_ford step 797 current loss 0.950085, current_train_items 25536.
I0302 18:58:39.199627 22626471084160 run.py:483] Algo bellman_ford step 798 current loss 1.200285, current_train_items 25568.
I0302 18:58:39.230749 22626471084160 run.py:483] Algo bellman_ford step 799 current loss 1.349850, current_train_items 25600.
I0302 18:58:39.249370 22626471084160 run.py:483] Algo bellman_ford step 800 current loss 0.281217, current_train_items 25632.
I0302 18:58:39.257144 22626471084160 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.828125, 'score': 0.828125, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0302 18:58:39.257261 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.871, current avg val score is 0.828, val scores are: bellman_ford: 0.828
I0302 18:58:39.273869 22626471084160 run.py:483] Algo bellman_ford step 801 current loss 0.796714, current_train_items 25664.
I0302 18:58:39.298410 22626471084160 run.py:483] Algo bellman_ford step 802 current loss 1.273735, current_train_items 25696.
I0302 18:58:39.328101 22626471084160 run.py:483] Algo bellman_ford step 803 current loss 1.273692, current_train_items 25728.
I0302 18:58:39.359987 22626471084160 run.py:483] Algo bellman_ford step 804 current loss 1.251959, current_train_items 25760.
I0302 18:58:39.378973 22626471084160 run.py:483] Algo bellman_ford step 805 current loss 0.356549, current_train_items 25792.
I0302 18:58:39.394686 22626471084160 run.py:483] Algo bellman_ford step 806 current loss 0.719400, current_train_items 25824.
I0302 18:58:39.418720 22626471084160 run.py:483] Algo bellman_ford step 807 current loss 1.110265, current_train_items 25856.
I0302 18:58:39.448135 22626471084160 run.py:483] Algo bellman_ford step 808 current loss 1.052921, current_train_items 25888.
I0302 18:58:39.478826 22626471084160 run.py:483] Algo bellman_ford step 809 current loss 1.237108, current_train_items 25920.
I0302 18:58:39.497516 22626471084160 run.py:483] Algo bellman_ford step 810 current loss 0.471111, current_train_items 25952.
I0302 18:58:39.514202 22626471084160 run.py:483] Algo bellman_ford step 811 current loss 0.891976, current_train_items 25984.
I0302 18:58:39.536739 22626471084160 run.py:483] Algo bellman_ford step 812 current loss 0.886691, current_train_items 26016.
I0302 18:58:39.565384 22626471084160 run.py:483] Algo bellman_ford step 813 current loss 1.045396, current_train_items 26048.
I0302 18:58:39.597117 22626471084160 run.py:483] Algo bellman_ford step 814 current loss 1.385344, current_train_items 26080.
I0302 18:58:39.615885 22626471084160 run.py:483] Algo bellman_ford step 815 current loss 0.413485, current_train_items 26112.
I0302 18:58:39.632298 22626471084160 run.py:483] Algo bellman_ford step 816 current loss 0.840944, current_train_items 26144.
I0302 18:58:39.655671 22626471084160 run.py:483] Algo bellman_ford step 817 current loss 0.996225, current_train_items 26176.
I0302 18:58:39.685740 22626471084160 run.py:483] Algo bellman_ford step 818 current loss 1.267201, current_train_items 26208.
I0302 18:58:39.719191 22626471084160 run.py:483] Algo bellman_ford step 819 current loss 1.489564, current_train_items 26240.
I0302 18:58:39.737678 22626471084160 run.py:483] Algo bellman_ford step 820 current loss 0.501938, current_train_items 26272.
I0302 18:58:39.753463 22626471084160 run.py:483] Algo bellman_ford step 821 current loss 0.634588, current_train_items 26304.
I0302 18:58:39.776288 22626471084160 run.py:483] Algo bellman_ford step 822 current loss 0.892037, current_train_items 26336.
I0302 18:58:39.805821 22626471084160 run.py:483] Algo bellman_ford step 823 current loss 1.066256, current_train_items 26368.
I0302 18:58:39.836643 22626471084160 run.py:483] Algo bellman_ford step 824 current loss 1.390471, current_train_items 26400.
I0302 18:58:39.855360 22626471084160 run.py:483] Algo bellman_ford step 825 current loss 0.444337, current_train_items 26432.
I0302 18:58:39.871830 22626471084160 run.py:483] Algo bellman_ford step 826 current loss 0.838907, current_train_items 26464.
I0302 18:58:39.895581 22626471084160 run.py:483] Algo bellman_ford step 827 current loss 0.987615, current_train_items 26496.
I0302 18:58:39.925061 22626471084160 run.py:483] Algo bellman_ford step 828 current loss 1.158517, current_train_items 26528.
I0302 18:58:39.956719 22626471084160 run.py:483] Algo bellman_ford step 829 current loss 1.269553, current_train_items 26560.
I0302 18:58:39.975112 22626471084160 run.py:483] Algo bellman_ford step 830 current loss 0.377195, current_train_items 26592.
I0302 18:58:39.991110 22626471084160 run.py:483] Algo bellman_ford step 831 current loss 0.679334, current_train_items 26624.
I0302 18:58:40.014420 22626471084160 run.py:483] Algo bellman_ford step 832 current loss 0.983616, current_train_items 26656.
I0302 18:58:40.043454 22626471084160 run.py:483] Algo bellman_ford step 833 current loss 1.181389, current_train_items 26688.
I0302 18:58:40.074030 22626471084160 run.py:483] Algo bellman_ford step 834 current loss 1.452858, current_train_items 26720.
I0302 18:58:40.092737 22626471084160 run.py:483] Algo bellman_ford step 835 current loss 0.430209, current_train_items 26752.
I0302 18:58:40.108671 22626471084160 run.py:483] Algo bellman_ford step 836 current loss 0.644839, current_train_items 26784.
I0302 18:58:40.131900 22626471084160 run.py:483] Algo bellman_ford step 837 current loss 1.203013, current_train_items 26816.
I0302 18:58:40.161576 22626471084160 run.py:483] Algo bellman_ford step 838 current loss 1.024699, current_train_items 26848.
I0302 18:58:40.194115 22626471084160 run.py:483] Algo bellman_ford step 839 current loss 1.478302, current_train_items 26880.
I0302 18:58:40.212005 22626471084160 run.py:483] Algo bellman_ford step 840 current loss 0.534909, current_train_items 26912.
I0302 18:58:40.228135 22626471084160 run.py:483] Algo bellman_ford step 841 current loss 0.763436, current_train_items 26944.
I0302 18:58:40.251763 22626471084160 run.py:483] Algo bellman_ford step 842 current loss 1.178036, current_train_items 26976.
I0302 18:58:40.279635 22626471084160 run.py:483] Algo bellman_ford step 843 current loss 1.222530, current_train_items 27008.
I0302 18:58:40.310343 22626471084160 run.py:483] Algo bellman_ford step 844 current loss 1.219597, current_train_items 27040.
I0302 18:58:40.328510 22626471084160 run.py:483] Algo bellman_ford step 845 current loss 0.422716, current_train_items 27072.
I0302 18:58:40.344782 22626471084160 run.py:483] Algo bellman_ford step 846 current loss 0.726285, current_train_items 27104.
I0302 18:58:40.369093 22626471084160 run.py:483] Algo bellman_ford step 847 current loss 1.248457, current_train_items 27136.
I0302 18:58:40.397708 22626471084160 run.py:483] Algo bellman_ford step 848 current loss 1.157073, current_train_items 27168.
I0302 18:58:40.430049 22626471084160 run.py:483] Algo bellman_ford step 849 current loss 1.869504, current_train_items 27200.
I0302 18:58:40.448541 22626471084160 run.py:483] Algo bellman_ford step 850 current loss 0.417633, current_train_items 27232.
I0302 18:58:40.456630 22626471084160 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.818359375, 'score': 0.818359375, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0302 18:58:40.456734 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.871, current avg val score is 0.818, val scores are: bellman_ford: 0.818
I0302 18:58:40.472959 22626471084160 run.py:483] Algo bellman_ford step 851 current loss 0.634937, current_train_items 27264.
I0302 18:58:40.496322 22626471084160 run.py:483] Algo bellman_ford step 852 current loss 0.907134, current_train_items 27296.
I0302 18:58:40.527059 22626471084160 run.py:483] Algo bellman_ford step 853 current loss 1.218280, current_train_items 27328.
I0302 18:58:40.558504 22626471084160 run.py:483] Algo bellman_ford step 854 current loss 1.251434, current_train_items 27360.
I0302 18:58:40.577632 22626471084160 run.py:483] Algo bellman_ford step 855 current loss 0.454951, current_train_items 27392.
I0302 18:58:40.593706 22626471084160 run.py:483] Algo bellman_ford step 856 current loss 0.581513, current_train_items 27424.
I0302 18:58:40.616158 22626471084160 run.py:483] Algo bellman_ford step 857 current loss 0.852990, current_train_items 27456.
I0302 18:58:40.646667 22626471084160 run.py:483] Algo bellman_ford step 858 current loss 1.087209, current_train_items 27488.
I0302 18:58:40.677551 22626471084160 run.py:483] Algo bellman_ford step 859 current loss 1.148744, current_train_items 27520.
I0302 18:58:40.696458 22626471084160 run.py:483] Algo bellman_ford step 860 current loss 0.438983, current_train_items 27552.
I0302 18:58:40.712831 22626471084160 run.py:483] Algo bellman_ford step 861 current loss 0.809784, current_train_items 27584.
I0302 18:58:40.735831 22626471084160 run.py:483] Algo bellman_ford step 862 current loss 1.077216, current_train_items 27616.
I0302 18:58:40.764764 22626471084160 run.py:483] Algo bellman_ford step 863 current loss 1.221492, current_train_items 27648.
I0302 18:58:40.795963 22626471084160 run.py:483] Algo bellman_ford step 864 current loss 1.483741, current_train_items 27680.
I0302 18:58:40.814409 22626471084160 run.py:483] Algo bellman_ford step 865 current loss 0.413158, current_train_items 27712.
I0302 18:58:40.830172 22626471084160 run.py:483] Algo bellman_ford step 866 current loss 0.737914, current_train_items 27744.
I0302 18:58:40.854361 22626471084160 run.py:483] Algo bellman_ford step 867 current loss 1.180988, current_train_items 27776.
I0302 18:58:40.883629 22626471084160 run.py:483] Algo bellman_ford step 868 current loss 1.106632, current_train_items 27808.
I0302 18:58:40.916583 22626471084160 run.py:483] Algo bellman_ford step 869 current loss 1.302455, current_train_items 27840.
I0302 18:58:40.935644 22626471084160 run.py:483] Algo bellman_ford step 870 current loss 0.459710, current_train_items 27872.
I0302 18:58:40.951970 22626471084160 run.py:483] Algo bellman_ford step 871 current loss 0.714122, current_train_items 27904.
I0302 18:58:40.974147 22626471084160 run.py:483] Algo bellman_ford step 872 current loss 0.818307, current_train_items 27936.
I0302 18:58:41.003760 22626471084160 run.py:483] Algo bellman_ford step 873 current loss 1.201978, current_train_items 27968.
I0302 18:58:41.034875 22626471084160 run.py:483] Algo bellman_ford step 874 current loss 1.214139, current_train_items 28000.
I0302 18:58:41.053915 22626471084160 run.py:483] Algo bellman_ford step 875 current loss 0.437872, current_train_items 28032.
I0302 18:58:41.070425 22626471084160 run.py:483] Algo bellman_ford step 876 current loss 0.690538, current_train_items 28064.
I0302 18:58:41.095315 22626471084160 run.py:483] Algo bellman_ford step 877 current loss 1.085096, current_train_items 28096.
I0302 18:58:41.123568 22626471084160 run.py:483] Algo bellman_ford step 878 current loss 0.996953, current_train_items 28128.
I0302 18:58:41.156205 22626471084160 run.py:483] Algo bellman_ford step 879 current loss 1.304896, current_train_items 28160.
I0302 18:58:41.174741 22626471084160 run.py:483] Algo bellman_ford step 880 current loss 0.409371, current_train_items 28192.
I0302 18:58:41.190680 22626471084160 run.py:483] Algo bellman_ford step 881 current loss 0.628537, current_train_items 28224.
I0302 18:58:41.212905 22626471084160 run.py:483] Algo bellman_ford step 882 current loss 0.777434, current_train_items 28256.
I0302 18:58:41.242754 22626471084160 run.py:483] Algo bellman_ford step 883 current loss 1.039860, current_train_items 28288.
I0302 18:58:41.275678 22626471084160 run.py:483] Algo bellman_ford step 884 current loss 1.379458, current_train_items 28320.
I0302 18:58:41.294720 22626471084160 run.py:483] Algo bellman_ford step 885 current loss 0.378650, current_train_items 28352.
I0302 18:58:41.310810 22626471084160 run.py:483] Algo bellman_ford step 886 current loss 0.731997, current_train_items 28384.
I0302 18:58:41.333998 22626471084160 run.py:483] Algo bellman_ford step 887 current loss 0.994558, current_train_items 28416.
I0302 18:58:41.362659 22626471084160 run.py:483] Algo bellman_ford step 888 current loss 1.048535, current_train_items 28448.
I0302 18:58:41.395638 22626471084160 run.py:483] Algo bellman_ford step 889 current loss 1.274501, current_train_items 28480.
I0302 18:58:41.414752 22626471084160 run.py:483] Algo bellman_ford step 890 current loss 0.422870, current_train_items 28512.
I0302 18:58:41.430980 22626471084160 run.py:483] Algo bellman_ford step 891 current loss 0.640646, current_train_items 28544.
I0302 18:58:41.454209 22626471084160 run.py:483] Algo bellman_ford step 892 current loss 1.111340, current_train_items 28576.
I0302 18:58:41.484400 22626471084160 run.py:483] Algo bellman_ford step 893 current loss 1.155043, current_train_items 28608.
I0302 18:58:41.515725 22626471084160 run.py:483] Algo bellman_ford step 894 current loss 1.156143, current_train_items 28640.
I0302 18:58:41.534115 22626471084160 run.py:483] Algo bellman_ford step 895 current loss 0.376429, current_train_items 28672.
I0302 18:58:41.550060 22626471084160 run.py:483] Algo bellman_ford step 896 current loss 0.568774, current_train_items 28704.
I0302 18:58:41.573667 22626471084160 run.py:483] Algo bellman_ford step 897 current loss 1.075481, current_train_items 28736.
I0302 18:58:41.603594 22626471084160 run.py:483] Algo bellman_ford step 898 current loss 1.070556, current_train_items 28768.
I0302 18:58:41.637231 22626471084160 run.py:483] Algo bellman_ford step 899 current loss 1.498094, current_train_items 28800.
I0302 18:58:41.656112 22626471084160 run.py:483] Algo bellman_ford step 900 current loss 0.450465, current_train_items 28832.
I0302 18:58:41.663868 22626471084160 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.873046875, 'score': 0.873046875, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0302 18:58:41.663974 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.871, current avg val score is 0.873, val scores are: bellman_ford: 0.873
I0302 18:58:41.695045 22626471084160 run.py:483] Algo bellman_ford step 901 current loss 0.686057, current_train_items 28864.
I0302 18:58:41.719269 22626471084160 run.py:483] Algo bellman_ford step 902 current loss 1.037620, current_train_items 28896.
I0302 18:58:41.749930 22626471084160 run.py:483] Algo bellman_ford step 903 current loss 1.133291, current_train_items 28928.
I0302 18:58:41.782765 22626471084160 run.py:483] Algo bellman_ford step 904 current loss 1.289844, current_train_items 28960.
I0302 18:58:41.801890 22626471084160 run.py:483] Algo bellman_ford step 905 current loss 0.338226, current_train_items 28992.
I0302 18:58:41.818371 22626471084160 run.py:483] Algo bellman_ford step 906 current loss 0.817089, current_train_items 29024.
I0302 18:58:41.842036 22626471084160 run.py:483] Algo bellman_ford step 907 current loss 0.957440, current_train_items 29056.
I0302 18:58:41.872492 22626471084160 run.py:483] Algo bellman_ford step 908 current loss 1.156277, current_train_items 29088.
I0302 18:58:41.905018 22626471084160 run.py:483] Algo bellman_ford step 909 current loss 1.265177, current_train_items 29120.
I0302 18:58:41.924103 22626471084160 run.py:483] Algo bellman_ford step 910 current loss 0.370344, current_train_items 29152.
I0302 18:58:41.940959 22626471084160 run.py:483] Algo bellman_ford step 911 current loss 0.793237, current_train_items 29184.
I0302 18:58:41.964891 22626471084160 run.py:483] Algo bellman_ford step 912 current loss 0.970137, current_train_items 29216.
I0302 18:58:41.992510 22626471084160 run.py:483] Algo bellman_ford step 913 current loss 1.008280, current_train_items 29248.
I0302 18:58:42.025360 22626471084160 run.py:483] Algo bellman_ford step 914 current loss 1.260138, current_train_items 29280.
I0302 18:58:42.043505 22626471084160 run.py:483] Algo bellman_ford step 915 current loss 0.389054, current_train_items 29312.
I0302 18:58:42.059940 22626471084160 run.py:483] Algo bellman_ford step 916 current loss 0.607889, current_train_items 29344.
I0302 18:58:42.083552 22626471084160 run.py:483] Algo bellman_ford step 917 current loss 0.949723, current_train_items 29376.
I0302 18:58:42.113514 22626471084160 run.py:483] Algo bellman_ford step 918 current loss 1.169315, current_train_items 29408.
I0302 18:58:42.146417 22626471084160 run.py:483] Algo bellman_ford step 919 current loss 1.369200, current_train_items 29440.
I0302 18:58:42.165118 22626471084160 run.py:483] Algo bellman_ford step 920 current loss 0.372537, current_train_items 29472.
I0302 18:58:42.181679 22626471084160 run.py:483] Algo bellman_ford step 921 current loss 0.822549, current_train_items 29504.
I0302 18:58:42.205389 22626471084160 run.py:483] Algo bellman_ford step 922 current loss 0.948645, current_train_items 29536.
I0302 18:58:42.234191 22626471084160 run.py:483] Algo bellman_ford step 923 current loss 0.885256, current_train_items 29568.
I0302 18:58:42.265949 22626471084160 run.py:483] Algo bellman_ford step 924 current loss 1.177795, current_train_items 29600.
I0302 18:58:42.284226 22626471084160 run.py:483] Algo bellman_ford step 925 current loss 0.447533, current_train_items 29632.
I0302 18:58:42.300121 22626471084160 run.py:483] Algo bellman_ford step 926 current loss 0.638162, current_train_items 29664.
I0302 18:58:42.323743 22626471084160 run.py:483] Algo bellman_ford step 927 current loss 0.992929, current_train_items 29696.
I0302 18:58:42.353939 22626471084160 run.py:483] Algo bellman_ford step 928 current loss 1.108696, current_train_items 29728.
I0302 18:58:42.384382 22626471084160 run.py:483] Algo bellman_ford step 929 current loss 1.192304, current_train_items 29760.
I0302 18:58:42.403100 22626471084160 run.py:483] Algo bellman_ford step 930 current loss 0.371592, current_train_items 29792.
I0302 18:58:42.419342 22626471084160 run.py:483] Algo bellman_ford step 931 current loss 0.752174, current_train_items 29824.
I0302 18:58:42.442505 22626471084160 run.py:483] Algo bellman_ford step 932 current loss 1.025073, current_train_items 29856.
I0302 18:58:42.471252 22626471084160 run.py:483] Algo bellman_ford step 933 current loss 1.106431, current_train_items 29888.
I0302 18:58:42.503614 22626471084160 run.py:483] Algo bellman_ford step 934 current loss 1.341719, current_train_items 29920.
I0302 18:58:42.522089 22626471084160 run.py:483] Algo bellman_ford step 935 current loss 0.405209, current_train_items 29952.
I0302 18:58:42.538281 22626471084160 run.py:483] Algo bellman_ford step 936 current loss 0.701105, current_train_items 29984.
I0302 18:58:42.560664 22626471084160 run.py:483] Algo bellman_ford step 937 current loss 0.789448, current_train_items 30016.
I0302 18:58:42.589715 22626471084160 run.py:483] Algo bellman_ford step 938 current loss 1.050865, current_train_items 30048.
I0302 18:58:42.621822 22626471084160 run.py:483] Algo bellman_ford step 939 current loss 1.390253, current_train_items 30080.
I0302 18:58:42.640447 22626471084160 run.py:483] Algo bellman_ford step 940 current loss 0.393462, current_train_items 30112.
I0302 18:58:42.656730 22626471084160 run.py:483] Algo bellman_ford step 941 current loss 0.759157, current_train_items 30144.
I0302 18:58:42.680627 22626471084160 run.py:483] Algo bellman_ford step 942 current loss 0.992668, current_train_items 30176.
I0302 18:58:42.710443 22626471084160 run.py:483] Algo bellman_ford step 943 current loss 1.006810, current_train_items 30208.
I0302 18:58:42.743109 22626471084160 run.py:483] Algo bellman_ford step 944 current loss 1.483474, current_train_items 30240.
I0302 18:58:42.761693 22626471084160 run.py:483] Algo bellman_ford step 945 current loss 0.386578, current_train_items 30272.
I0302 18:58:42.777492 22626471084160 run.py:483] Algo bellman_ford step 946 current loss 0.703524, current_train_items 30304.
I0302 18:58:42.801863 22626471084160 run.py:483] Algo bellman_ford step 947 current loss 1.091434, current_train_items 30336.
I0302 18:58:42.830942 22626471084160 run.py:483] Algo bellman_ford step 948 current loss 0.965676, current_train_items 30368.
I0302 18:58:42.862370 22626471084160 run.py:483] Algo bellman_ford step 949 current loss 1.249373, current_train_items 30400.
I0302 18:58:42.881059 22626471084160 run.py:483] Algo bellman_ford step 950 current loss 0.442706, current_train_items 30432.
I0302 18:58:42.888954 22626471084160 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.8642578125, 'score': 0.8642578125, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0302 18:58:42.889060 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.873, current avg val score is 0.864, val scores are: bellman_ford: 0.864
I0302 18:58:42.905819 22626471084160 run.py:483] Algo bellman_ford step 951 current loss 0.606802, current_train_items 30464.
I0302 18:58:42.929623 22626471084160 run.py:483] Algo bellman_ford step 952 current loss 0.868405, current_train_items 30496.
I0302 18:58:42.959578 22626471084160 run.py:483] Algo bellman_ford step 953 current loss 1.156562, current_train_items 30528.
I0302 18:58:42.989118 22626471084160 run.py:483] Algo bellman_ford step 954 current loss 1.175187, current_train_items 30560.
I0302 18:58:43.007904 22626471084160 run.py:483] Algo bellman_ford step 955 current loss 0.333785, current_train_items 30592.
I0302 18:58:43.023868 22626471084160 run.py:483] Algo bellman_ford step 956 current loss 0.767647, current_train_items 30624.
I0302 18:58:43.047492 22626471084160 run.py:483] Algo bellman_ford step 957 current loss 1.104026, current_train_items 30656.
I0302 18:58:43.075647 22626471084160 run.py:483] Algo bellman_ford step 958 current loss 1.003460, current_train_items 30688.
I0302 18:58:43.107919 22626471084160 run.py:483] Algo bellman_ford step 959 current loss 1.388164, current_train_items 30720.
I0302 18:58:43.126930 22626471084160 run.py:483] Algo bellman_ford step 960 current loss 0.344809, current_train_items 30752.
I0302 18:58:43.143629 22626471084160 run.py:483] Algo bellman_ford step 961 current loss 0.802029, current_train_items 30784.
I0302 18:58:43.165216 22626471084160 run.py:483] Algo bellman_ford step 962 current loss 0.743311, current_train_items 30816.
I0302 18:58:43.192779 22626471084160 run.py:483] Algo bellman_ford step 963 current loss 0.994173, current_train_items 30848.
I0302 18:58:43.225143 22626471084160 run.py:483] Algo bellman_ford step 964 current loss 1.357688, current_train_items 30880.
I0302 18:58:43.243877 22626471084160 run.py:483] Algo bellman_ford step 965 current loss 0.473260, current_train_items 30912.
I0302 18:58:43.259846 22626471084160 run.py:483] Algo bellman_ford step 966 current loss 0.614524, current_train_items 30944.
I0302 18:58:43.283596 22626471084160 run.py:483] Algo bellman_ford step 967 current loss 0.882118, current_train_items 30976.
I0302 18:58:43.313368 22626471084160 run.py:483] Algo bellman_ford step 968 current loss 1.032096, current_train_items 31008.
I0302 18:58:43.344958 22626471084160 run.py:483] Algo bellman_ford step 969 current loss 1.242335, current_train_items 31040.
I0302 18:58:43.363448 22626471084160 run.py:483] Algo bellman_ford step 970 current loss 0.456188, current_train_items 31072.
I0302 18:58:43.379705 22626471084160 run.py:483] Algo bellman_ford step 971 current loss 0.717288, current_train_items 31104.
I0302 18:58:43.403222 22626471084160 run.py:483] Algo bellman_ford step 972 current loss 0.996987, current_train_items 31136.
I0302 18:58:43.433786 22626471084160 run.py:483] Algo bellman_ford step 973 current loss 1.100093, current_train_items 31168.
I0302 18:58:43.466907 22626471084160 run.py:483] Algo bellman_ford step 974 current loss 1.269824, current_train_items 31200.
I0302 18:58:43.485809 22626471084160 run.py:483] Algo bellman_ford step 975 current loss 0.467070, current_train_items 31232.
I0302 18:58:43.501667 22626471084160 run.py:483] Algo bellman_ford step 976 current loss 0.512003, current_train_items 31264.
I0302 18:58:43.524541 22626471084160 run.py:483] Algo bellman_ford step 977 current loss 0.898835, current_train_items 31296.
I0302 18:58:43.553795 22626471084160 run.py:483] Algo bellman_ford step 978 current loss 1.003262, current_train_items 31328.
I0302 18:58:43.585093 22626471084160 run.py:483] Algo bellman_ford step 979 current loss 1.198945, current_train_items 31360.
I0302 18:58:43.603697 22626471084160 run.py:483] Algo bellman_ford step 980 current loss 0.337731, current_train_items 31392.
I0302 18:58:43.620016 22626471084160 run.py:483] Algo bellman_ford step 981 current loss 0.668043, current_train_items 31424.
I0302 18:58:43.642871 22626471084160 run.py:483] Algo bellman_ford step 982 current loss 0.844170, current_train_items 31456.
I0302 18:58:43.671269 22626471084160 run.py:483] Algo bellman_ford step 983 current loss 1.007077, current_train_items 31488.
I0302 18:58:43.703520 22626471084160 run.py:483] Algo bellman_ford step 984 current loss 1.244724, current_train_items 31520.
I0302 18:58:43.722519 22626471084160 run.py:483] Algo bellman_ford step 985 current loss 0.383720, current_train_items 31552.
I0302 18:58:43.739067 22626471084160 run.py:483] Algo bellman_ford step 986 current loss 0.775244, current_train_items 31584.
I0302 18:58:43.761837 22626471084160 run.py:483] Algo bellman_ford step 987 current loss 0.959114, current_train_items 31616.
I0302 18:58:43.791583 22626471084160 run.py:483] Algo bellman_ford step 988 current loss 1.020052, current_train_items 31648.
I0302 18:58:43.823324 22626471084160 run.py:483] Algo bellman_ford step 989 current loss 1.033294, current_train_items 31680.
I0302 18:58:43.842035 22626471084160 run.py:483] Algo bellman_ford step 990 current loss 0.345793, current_train_items 31712.
I0302 18:58:43.858128 22626471084160 run.py:483] Algo bellman_ford step 991 current loss 0.621301, current_train_items 31744.
I0302 18:58:43.881099 22626471084160 run.py:483] Algo bellman_ford step 992 current loss 0.991782, current_train_items 31776.
I0302 18:58:43.909050 22626471084160 run.py:483] Algo bellman_ford step 993 current loss 1.008607, current_train_items 31808.
I0302 18:58:43.943045 22626471084160 run.py:483] Algo bellman_ford step 994 current loss 1.351831, current_train_items 31840.
I0302 18:58:43.961529 22626471084160 run.py:483] Algo bellman_ford step 995 current loss 0.451270, current_train_items 31872.
I0302 18:58:43.977421 22626471084160 run.py:483] Algo bellman_ford step 996 current loss 0.686096, current_train_items 31904.
I0302 18:58:44.000587 22626471084160 run.py:483] Algo bellman_ford step 997 current loss 0.823812, current_train_items 31936.
I0302 18:58:44.030568 22626471084160 run.py:483] Algo bellman_ford step 998 current loss 1.079841, current_train_items 31968.
I0302 18:58:44.062123 22626471084160 run.py:483] Algo bellman_ford step 999 current loss 1.223632, current_train_items 32000.
I0302 18:58:44.080889 22626471084160 run.py:483] Algo bellman_ford step 1000 current loss 0.320466, current_train_items 32032.
I0302 18:58:44.088710 22626471084160 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0302 18:58:44.088814 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.873, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 18:58:44.117283 22626471084160 run.py:483] Algo bellman_ford step 1001 current loss 0.709443, current_train_items 32064.
I0302 18:58:44.140504 22626471084160 run.py:483] Algo bellman_ford step 1002 current loss 0.749444, current_train_items 32096.
I0302 18:58:44.168655 22626471084160 run.py:483] Algo bellman_ford step 1003 current loss 1.153131, current_train_items 32128.
I0302 18:58:44.203137 22626471084160 run.py:483] Algo bellman_ford step 1004 current loss 1.353866, current_train_items 32160.
I0302 18:58:44.222126 22626471084160 run.py:483] Algo bellman_ford step 1005 current loss 0.406812, current_train_items 32192.
I0302 18:58:44.237764 22626471084160 run.py:483] Algo bellman_ford step 1006 current loss 0.718280, current_train_items 32224.
I0302 18:58:44.262243 22626471084160 run.py:483] Algo bellman_ford step 1007 current loss 0.988174, current_train_items 32256.
I0302 18:58:44.290801 22626471084160 run.py:483] Algo bellman_ford step 1008 current loss 1.041154, current_train_items 32288.
I0302 18:58:44.323541 22626471084160 run.py:483] Algo bellman_ford step 1009 current loss 1.198534, current_train_items 32320.
I0302 18:58:44.341884 22626471084160 run.py:483] Algo bellman_ford step 1010 current loss 0.334723, current_train_items 32352.
I0302 18:58:44.358103 22626471084160 run.py:483] Algo bellman_ford step 1011 current loss 0.700221, current_train_items 32384.
I0302 18:58:44.382555 22626471084160 run.py:483] Algo bellman_ford step 1012 current loss 1.081574, current_train_items 32416.
I0302 18:58:44.412367 22626471084160 run.py:483] Algo bellman_ford step 1013 current loss 1.366758, current_train_items 32448.
I0302 18:58:44.444787 22626471084160 run.py:483] Algo bellman_ford step 1014 current loss 1.223895, current_train_items 32480.
I0302 18:58:44.463201 22626471084160 run.py:483] Algo bellman_ford step 1015 current loss 0.435235, current_train_items 32512.
I0302 18:58:44.479574 22626471084160 run.py:483] Algo bellman_ford step 1016 current loss 0.729083, current_train_items 32544.
I0302 18:58:44.502815 22626471084160 run.py:483] Algo bellman_ford step 1017 current loss 0.889653, current_train_items 32576.
I0302 18:58:44.532202 22626471084160 run.py:483] Algo bellman_ford step 1018 current loss 0.931202, current_train_items 32608.
I0302 18:58:44.566478 22626471084160 run.py:483] Algo bellman_ford step 1019 current loss 1.446692, current_train_items 32640.
I0302 18:58:44.584577 22626471084160 run.py:483] Algo bellman_ford step 1020 current loss 0.398805, current_train_items 32672.
I0302 18:58:44.600536 22626471084160 run.py:483] Algo bellman_ford step 1021 current loss 0.607048, current_train_items 32704.
I0302 18:58:44.623388 22626471084160 run.py:483] Algo bellman_ford step 1022 current loss 0.820726, current_train_items 32736.
I0302 18:58:44.651771 22626471084160 run.py:483] Algo bellman_ford step 1023 current loss 0.950987, current_train_items 32768.
I0302 18:58:44.681886 22626471084160 run.py:483] Algo bellman_ford step 1024 current loss 1.144134, current_train_items 32800.
I0302 18:58:44.700309 22626471084160 run.py:483] Algo bellman_ford step 1025 current loss 0.394653, current_train_items 32832.
I0302 18:58:44.717022 22626471084160 run.py:483] Algo bellman_ford step 1026 current loss 0.732219, current_train_items 32864.
I0302 18:58:44.740313 22626471084160 run.py:483] Algo bellman_ford step 1027 current loss 1.124021, current_train_items 32896.
I0302 18:58:44.770128 22626471084160 run.py:483] Algo bellman_ford step 1028 current loss 1.473824, current_train_items 32928.
I0302 18:58:44.801331 22626471084160 run.py:483] Algo bellman_ford step 1029 current loss 1.549730, current_train_items 32960.
I0302 18:58:44.819738 22626471084160 run.py:483] Algo bellman_ford step 1030 current loss 0.539955, current_train_items 32992.
I0302 18:58:44.835456 22626471084160 run.py:483] Algo bellman_ford step 1031 current loss 0.572279, current_train_items 33024.
I0302 18:58:44.857758 22626471084160 run.py:483] Algo bellman_ford step 1032 current loss 0.871338, current_train_items 33056.
I0302 18:58:44.886750 22626471084160 run.py:483] Algo bellman_ford step 1033 current loss 1.088998, current_train_items 33088.
I0302 18:58:44.919113 22626471084160 run.py:483] Algo bellman_ford step 1034 current loss 1.274096, current_train_items 33120.
I0302 18:58:44.937363 22626471084160 run.py:483] Algo bellman_ford step 1035 current loss 0.405511, current_train_items 33152.
I0302 18:58:44.953480 22626471084160 run.py:483] Algo bellman_ford step 1036 current loss 0.615330, current_train_items 33184.
I0302 18:58:44.977327 22626471084160 run.py:483] Algo bellman_ford step 1037 current loss 0.829586, current_train_items 33216.
I0302 18:58:45.007772 22626471084160 run.py:483] Algo bellman_ford step 1038 current loss 1.079730, current_train_items 33248.
I0302 18:58:45.041270 22626471084160 run.py:483] Algo bellman_ford step 1039 current loss 1.464727, current_train_items 33280.
I0302 18:58:45.059300 22626471084160 run.py:483] Algo bellman_ford step 1040 current loss 0.306357, current_train_items 33312.
I0302 18:58:45.076038 22626471084160 run.py:483] Algo bellman_ford step 1041 current loss 0.857072, current_train_items 33344.
I0302 18:58:45.099377 22626471084160 run.py:483] Algo bellman_ford step 1042 current loss 0.977594, current_train_items 33376.
I0302 18:58:45.128633 22626471084160 run.py:483] Algo bellman_ford step 1043 current loss 1.036715, current_train_items 33408.
I0302 18:58:45.160186 22626471084160 run.py:483] Algo bellman_ford step 1044 current loss 1.170719, current_train_items 33440.
I0302 18:58:45.178388 22626471084160 run.py:483] Algo bellman_ford step 1045 current loss 0.275775, current_train_items 33472.
I0302 18:58:45.194070 22626471084160 run.py:483] Algo bellman_ford step 1046 current loss 0.540549, current_train_items 33504.
I0302 18:58:45.217507 22626471084160 run.py:483] Algo bellman_ford step 1047 current loss 1.016443, current_train_items 33536.
I0302 18:58:45.247630 22626471084160 run.py:483] Algo bellman_ford step 1048 current loss 1.245308, current_train_items 33568.
I0302 18:58:45.280184 22626471084160 run.py:483] Algo bellman_ford step 1049 current loss 1.203702, current_train_items 33600.
I0302 18:58:45.298496 22626471084160 run.py:483] Algo bellman_ford step 1050 current loss 0.422765, current_train_items 33632.
I0302 18:58:45.306509 22626471084160 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0302 18:58:45.306613 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.891, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 18:58:45.334749 22626471084160 run.py:483] Algo bellman_ford step 1051 current loss 0.598261, current_train_items 33664.
I0302 18:58:45.358899 22626471084160 run.py:483] Algo bellman_ford step 1052 current loss 1.012364, current_train_items 33696.
I0302 18:58:45.387756 22626471084160 run.py:483] Algo bellman_ford step 1053 current loss 0.946713, current_train_items 33728.
I0302 18:58:45.420664 22626471084160 run.py:483] Algo bellman_ford step 1054 current loss 1.253468, current_train_items 33760.
I0302 18:58:45.439893 22626471084160 run.py:483] Algo bellman_ford step 1055 current loss 0.395433, current_train_items 33792.
I0302 18:58:45.456016 22626471084160 run.py:483] Algo bellman_ford step 1056 current loss 0.732603, current_train_items 33824.
I0302 18:58:45.479966 22626471084160 run.py:483] Algo bellman_ford step 1057 current loss 1.064291, current_train_items 33856.
I0302 18:58:45.510882 22626471084160 run.py:483] Algo bellman_ford step 1058 current loss 1.133573, current_train_items 33888.
I0302 18:58:45.540684 22626471084160 run.py:483] Algo bellman_ford step 1059 current loss 1.025532, current_train_items 33920.
I0302 18:58:45.559623 22626471084160 run.py:483] Algo bellman_ford step 1060 current loss 0.343643, current_train_items 33952.
I0302 18:58:45.575765 22626471084160 run.py:483] Algo bellman_ford step 1061 current loss 0.666013, current_train_items 33984.
I0302 18:58:45.597642 22626471084160 run.py:483] Algo bellman_ford step 1062 current loss 0.758812, current_train_items 34016.
I0302 18:58:45.626509 22626471084160 run.py:483] Algo bellman_ford step 1063 current loss 1.039785, current_train_items 34048.
I0302 18:58:45.659276 22626471084160 run.py:483] Algo bellman_ford step 1064 current loss 1.275265, current_train_items 34080.
I0302 18:58:45.677587 22626471084160 run.py:483] Algo bellman_ford step 1065 current loss 0.329072, current_train_items 34112.
I0302 18:58:45.693723 22626471084160 run.py:483] Algo bellman_ford step 1066 current loss 0.650357, current_train_items 34144.
I0302 18:58:45.717466 22626471084160 run.py:483] Algo bellman_ford step 1067 current loss 0.948217, current_train_items 34176.
I0302 18:58:45.747067 22626471084160 run.py:483] Algo bellman_ford step 1068 current loss 0.961893, current_train_items 34208.
I0302 18:58:45.780611 22626471084160 run.py:483] Algo bellman_ford step 1069 current loss 1.272410, current_train_items 34240.
I0302 18:58:45.798957 22626471084160 run.py:483] Algo bellman_ford step 1070 current loss 0.336401, current_train_items 34272.
I0302 18:58:45.815507 22626471084160 run.py:483] Algo bellman_ford step 1071 current loss 0.702201, current_train_items 34304.
I0302 18:58:45.838658 22626471084160 run.py:483] Algo bellman_ford step 1072 current loss 0.884125, current_train_items 34336.
I0302 18:58:45.868170 22626471084160 run.py:483] Algo bellman_ford step 1073 current loss 0.865644, current_train_items 34368.
I0302 18:58:45.899202 22626471084160 run.py:483] Algo bellman_ford step 1074 current loss 1.046210, current_train_items 34400.
I0302 18:58:45.917901 22626471084160 run.py:483] Algo bellman_ford step 1075 current loss 0.363781, current_train_items 34432.
I0302 18:58:45.934290 22626471084160 run.py:483] Algo bellman_ford step 1076 current loss 0.801771, current_train_items 34464.
I0302 18:58:45.957830 22626471084160 run.py:483] Algo bellman_ford step 1077 current loss 0.921148, current_train_items 34496.
I0302 18:58:45.987884 22626471084160 run.py:483] Algo bellman_ford step 1078 current loss 1.026411, current_train_items 34528.
I0302 18:58:46.019218 22626471084160 run.py:483] Algo bellman_ford step 1079 current loss 1.169018, current_train_items 34560.
I0302 18:58:46.037490 22626471084160 run.py:483] Algo bellman_ford step 1080 current loss 0.369463, current_train_items 34592.
I0302 18:58:46.053964 22626471084160 run.py:483] Algo bellman_ford step 1081 current loss 0.648297, current_train_items 34624.
I0302 18:58:46.076401 22626471084160 run.py:483] Algo bellman_ford step 1082 current loss 1.056277, current_train_items 34656.
I0302 18:58:46.105479 22626471084160 run.py:483] Algo bellman_ford step 1083 current loss 1.093181, current_train_items 34688.
I0302 18:58:46.138090 22626471084160 run.py:483] Algo bellman_ford step 1084 current loss 1.499368, current_train_items 34720.
I0302 18:58:46.156658 22626471084160 run.py:483] Algo bellman_ford step 1085 current loss 0.399038, current_train_items 34752.
I0302 18:58:46.172869 22626471084160 run.py:483] Algo bellman_ford step 1086 current loss 0.683837, current_train_items 34784.
I0302 18:58:46.196085 22626471084160 run.py:483] Algo bellman_ford step 1087 current loss 0.844859, current_train_items 34816.
I0302 18:58:46.224452 22626471084160 run.py:483] Algo bellman_ford step 1088 current loss 0.818511, current_train_items 34848.
I0302 18:58:46.254853 22626471084160 run.py:483] Algo bellman_ford step 1089 current loss 1.047507, current_train_items 34880.
I0302 18:58:46.273690 22626471084160 run.py:483] Algo bellman_ford step 1090 current loss 0.423196, current_train_items 34912.
I0302 18:58:46.290203 22626471084160 run.py:483] Algo bellman_ford step 1091 current loss 0.694505, current_train_items 34944.
I0302 18:58:46.312689 22626471084160 run.py:483] Algo bellman_ford step 1092 current loss 0.953316, current_train_items 34976.
I0302 18:58:46.342875 22626471084160 run.py:483] Algo bellman_ford step 1093 current loss 1.022926, current_train_items 35008.
I0302 18:58:46.375862 22626471084160 run.py:483] Algo bellman_ford step 1094 current loss 1.087498, current_train_items 35040.
I0302 18:58:46.393968 22626471084160 run.py:483] Algo bellman_ford step 1095 current loss 0.345180, current_train_items 35072.
I0302 18:58:46.410024 22626471084160 run.py:483] Algo bellman_ford step 1096 current loss 0.706442, current_train_items 35104.
I0302 18:58:46.433307 22626471084160 run.py:483] Algo bellman_ford step 1097 current loss 0.944580, current_train_items 35136.
I0302 18:58:46.462856 22626471084160 run.py:483] Algo bellman_ford step 1098 current loss 1.151149, current_train_items 35168.
I0302 18:58:46.494102 22626471084160 run.py:483] Algo bellman_ford step 1099 current loss 1.123994, current_train_items 35200.
I0302 18:58:46.512567 22626471084160 run.py:483] Algo bellman_ford step 1100 current loss 0.421549, current_train_items 35232.
I0302 18:58:46.520386 22626471084160 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0302 18:58:46.520492 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.905, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 18:58:46.549290 22626471084160 run.py:483] Algo bellman_ford step 1101 current loss 0.711070, current_train_items 35264.
I0302 18:58:46.572338 22626471084160 run.py:483] Algo bellman_ford step 1102 current loss 0.824430, current_train_items 35296.
I0302 18:58:46.603101 22626471084160 run.py:483] Algo bellman_ford step 1103 current loss 0.952332, current_train_items 35328.
I0302 18:58:46.637384 22626471084160 run.py:483] Algo bellman_ford step 1104 current loss 1.404201, current_train_items 35360.
I0302 18:58:46.656526 22626471084160 run.py:483] Algo bellman_ford step 1105 current loss 0.385746, current_train_items 35392.
I0302 18:58:46.672430 22626471084160 run.py:483] Algo bellman_ford step 1106 current loss 0.628307, current_train_items 35424.
I0302 18:58:46.696182 22626471084160 run.py:483] Algo bellman_ford step 1107 current loss 0.907711, current_train_items 35456.
I0302 18:58:46.724809 22626471084160 run.py:483] Algo bellman_ford step 1108 current loss 1.229011, current_train_items 35488.
I0302 18:58:46.755096 22626471084160 run.py:483] Algo bellman_ford step 1109 current loss 1.100977, current_train_items 35520.
I0302 18:58:46.773852 22626471084160 run.py:483] Algo bellman_ford step 1110 current loss 0.439981, current_train_items 35552.
I0302 18:58:46.790074 22626471084160 run.py:483] Algo bellman_ford step 1111 current loss 0.719715, current_train_items 35584.
I0302 18:58:46.813404 22626471084160 run.py:483] Algo bellman_ford step 1112 current loss 0.982319, current_train_items 35616.
I0302 18:58:46.841181 22626471084160 run.py:483] Algo bellman_ford step 1113 current loss 1.066230, current_train_items 35648.
I0302 18:58:46.874504 22626471084160 run.py:483] Algo bellman_ford step 1114 current loss 1.212646, current_train_items 35680.
I0302 18:58:46.892795 22626471084160 run.py:483] Algo bellman_ford step 1115 current loss 0.377637, current_train_items 35712.
I0302 18:58:46.908602 22626471084160 run.py:483] Algo bellman_ford step 1116 current loss 0.609648, current_train_items 35744.
I0302 18:58:46.932353 22626471084160 run.py:483] Algo bellman_ford step 1117 current loss 0.926955, current_train_items 35776.
I0302 18:58:46.962126 22626471084160 run.py:483] Algo bellman_ford step 1118 current loss 1.014027, current_train_items 35808.
I0302 18:58:46.994742 22626471084160 run.py:483] Algo bellman_ford step 1119 current loss 1.431948, current_train_items 35840.
I0302 18:58:47.013437 22626471084160 run.py:483] Algo bellman_ford step 1120 current loss 0.437292, current_train_items 35872.
I0302 18:58:47.029167 22626471084160 run.py:483] Algo bellman_ford step 1121 current loss 0.529539, current_train_items 35904.
I0302 18:58:47.052893 22626471084160 run.py:483] Algo bellman_ford step 1122 current loss 0.846861, current_train_items 35936.
I0302 18:58:47.081404 22626471084160 run.py:483] Algo bellman_ford step 1123 current loss 1.006188, current_train_items 35968.
I0302 18:58:47.115012 22626471084160 run.py:483] Algo bellman_ford step 1124 current loss 1.121520, current_train_items 36000.
I0302 18:58:47.133373 22626471084160 run.py:483] Algo bellman_ford step 1125 current loss 0.364757, current_train_items 36032.
I0302 18:58:47.149235 22626471084160 run.py:483] Algo bellman_ford step 1126 current loss 0.594607, current_train_items 36064.
I0302 18:58:47.172556 22626471084160 run.py:483] Algo bellman_ford step 1127 current loss 0.941389, current_train_items 36096.
I0302 18:58:47.202013 22626471084160 run.py:483] Algo bellman_ford step 1128 current loss 1.241890, current_train_items 36128.
I0302 18:58:47.231676 22626471084160 run.py:483] Algo bellman_ford step 1129 current loss 1.351784, current_train_items 36160.
I0302 18:58:47.250439 22626471084160 run.py:483] Algo bellman_ford step 1130 current loss 0.418152, current_train_items 36192.
I0302 18:58:47.266525 22626471084160 run.py:483] Algo bellman_ford step 1131 current loss 0.591064, current_train_items 36224.
I0302 18:58:47.290029 22626471084160 run.py:483] Algo bellman_ford step 1132 current loss 0.886885, current_train_items 36256.
I0302 18:58:47.319527 22626471084160 run.py:483] Algo bellman_ford step 1133 current loss 0.866389, current_train_items 36288.
I0302 18:58:47.350339 22626471084160 run.py:483] Algo bellman_ford step 1134 current loss 1.203178, current_train_items 36320.
I0302 18:58:47.368770 22626471084160 run.py:483] Algo bellman_ford step 1135 current loss 0.567413, current_train_items 36352.
I0302 18:58:47.384913 22626471084160 run.py:483] Algo bellman_ford step 1136 current loss 0.705601, current_train_items 36384.
I0302 18:58:47.407519 22626471084160 run.py:483] Algo bellman_ford step 1137 current loss 0.774719, current_train_items 36416.
I0302 18:58:47.437989 22626471084160 run.py:483] Algo bellman_ford step 1138 current loss 1.030980, current_train_items 36448.
I0302 18:58:47.469833 22626471084160 run.py:483] Algo bellman_ford step 1139 current loss 0.967557, current_train_items 36480.
I0302 18:58:47.488207 22626471084160 run.py:483] Algo bellman_ford step 1140 current loss 0.403450, current_train_items 36512.
I0302 18:58:47.504521 22626471084160 run.py:483] Algo bellman_ford step 1141 current loss 0.625344, current_train_items 36544.
I0302 18:58:47.527003 22626471084160 run.py:483] Algo bellman_ford step 1142 current loss 0.854631, current_train_items 36576.
I0302 18:58:47.555695 22626471084160 run.py:483] Algo bellman_ford step 1143 current loss 1.010081, current_train_items 36608.
I0302 18:58:47.587315 22626471084160 run.py:483] Algo bellman_ford step 1144 current loss 1.102642, current_train_items 36640.
I0302 18:58:47.606151 22626471084160 run.py:483] Algo bellman_ford step 1145 current loss 0.451998, current_train_items 36672.
I0302 18:58:47.622455 22626471084160 run.py:483] Algo bellman_ford step 1146 current loss 0.703514, current_train_items 36704.
I0302 18:58:47.645882 22626471084160 run.py:483] Algo bellman_ford step 1147 current loss 0.926753, current_train_items 36736.
I0302 18:58:47.675247 22626471084160 run.py:483] Algo bellman_ford step 1148 current loss 1.040318, current_train_items 36768.
I0302 18:58:47.704691 22626471084160 run.py:483] Algo bellman_ford step 1149 current loss 1.149161, current_train_items 36800.
I0302 18:58:47.723401 22626471084160 run.py:483] Algo bellman_ford step 1150 current loss 0.450785, current_train_items 36832.
I0302 18:58:47.731395 22626471084160 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0302 18:58:47.731500 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.908, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 18:58:47.747992 22626471084160 run.py:483] Algo bellman_ford step 1151 current loss 0.627391, current_train_items 36864.
I0302 18:58:47.772189 22626471084160 run.py:483] Algo bellman_ford step 1152 current loss 0.941483, current_train_items 36896.
I0302 18:58:47.803241 22626471084160 run.py:483] Algo bellman_ford step 1153 current loss 1.083340, current_train_items 36928.
W0302 18:58:47.824429 22626471084160 samplers.py:155] Increasing hint lengh from 11 to 12
I0302 18:58:54.492702 22626471084160 run.py:483] Algo bellman_ford step 1154 current loss 1.318412, current_train_items 36960.
I0302 18:58:54.514182 22626471084160 run.py:483] Algo bellman_ford step 1155 current loss 0.415350, current_train_items 36992.
I0302 18:58:54.530612 22626471084160 run.py:483] Algo bellman_ford step 1156 current loss 0.608346, current_train_items 37024.
I0302 18:58:54.554015 22626471084160 run.py:483] Algo bellman_ford step 1157 current loss 0.867828, current_train_items 37056.
I0302 18:58:54.583304 22626471084160 run.py:483] Algo bellman_ford step 1158 current loss 1.158202, current_train_items 37088.
I0302 18:58:54.613762 22626471084160 run.py:483] Algo bellman_ford step 1159 current loss 1.085463, current_train_items 37120.
I0302 18:58:54.633515 22626471084160 run.py:483] Algo bellman_ford step 1160 current loss 0.446138, current_train_items 37152.
I0302 18:58:54.649954 22626471084160 run.py:483] Algo bellman_ford step 1161 current loss 0.608194, current_train_items 37184.
I0302 18:58:54.672415 22626471084160 run.py:483] Algo bellman_ford step 1162 current loss 0.800292, current_train_items 37216.
I0302 18:58:54.701816 22626471084160 run.py:483] Algo bellman_ford step 1163 current loss 0.906034, current_train_items 37248.
I0302 18:58:54.734355 22626471084160 run.py:483] Algo bellman_ford step 1164 current loss 1.204763, current_train_items 37280.
I0302 18:58:54.753451 22626471084160 run.py:483] Algo bellman_ford step 1165 current loss 0.377505, current_train_items 37312.
I0302 18:58:54.769705 22626471084160 run.py:483] Algo bellman_ford step 1166 current loss 0.699035, current_train_items 37344.
I0302 18:58:54.793302 22626471084160 run.py:483] Algo bellman_ford step 1167 current loss 1.097282, current_train_items 37376.
I0302 18:58:54.822729 22626471084160 run.py:483] Algo bellman_ford step 1168 current loss 1.100976, current_train_items 37408.
I0302 18:58:54.856463 22626471084160 run.py:483] Algo bellman_ford step 1169 current loss 1.183645, current_train_items 37440.
I0302 18:58:54.875739 22626471084160 run.py:483] Algo bellman_ford step 1170 current loss 0.374069, current_train_items 37472.
I0302 18:58:54.892410 22626471084160 run.py:483] Algo bellman_ford step 1171 current loss 0.884054, current_train_items 37504.
I0302 18:58:54.915518 22626471084160 run.py:483] Algo bellman_ford step 1172 current loss 1.067364, current_train_items 37536.
I0302 18:58:54.945745 22626471084160 run.py:483] Algo bellman_ford step 1173 current loss 1.203557, current_train_items 37568.
I0302 18:58:54.978277 22626471084160 run.py:483] Algo bellman_ford step 1174 current loss 1.279922, current_train_items 37600.
I0302 18:58:54.997583 22626471084160 run.py:483] Algo bellman_ford step 1175 current loss 0.302846, current_train_items 37632.
I0302 18:58:55.013822 22626471084160 run.py:483] Algo bellman_ford step 1176 current loss 0.740365, current_train_items 37664.
I0302 18:58:55.036336 22626471084160 run.py:483] Algo bellman_ford step 1177 current loss 0.951767, current_train_items 37696.
I0302 18:58:55.065394 22626471084160 run.py:483] Algo bellman_ford step 1178 current loss 0.963692, current_train_items 37728.
I0302 18:58:55.099465 22626471084160 run.py:483] Algo bellman_ford step 1179 current loss 1.275954, current_train_items 37760.
I0302 18:58:55.118359 22626471084160 run.py:483] Algo bellman_ford step 1180 current loss 0.347152, current_train_items 37792.
I0302 18:58:55.134998 22626471084160 run.py:483] Algo bellman_ford step 1181 current loss 0.785369, current_train_items 37824.
I0302 18:58:55.158264 22626471084160 run.py:483] Algo bellman_ford step 1182 current loss 0.761386, current_train_items 37856.
I0302 18:58:55.186857 22626471084160 run.py:483] Algo bellman_ford step 1183 current loss 0.915056, current_train_items 37888.
I0302 18:58:55.219427 22626471084160 run.py:483] Algo bellman_ford step 1184 current loss 1.149969, current_train_items 37920.
I0302 18:58:55.239068 22626471084160 run.py:483] Algo bellman_ford step 1185 current loss 0.410831, current_train_items 37952.
I0302 18:58:55.255049 22626471084160 run.py:483] Algo bellman_ford step 1186 current loss 0.612239, current_train_items 37984.
I0302 18:58:55.278178 22626471084160 run.py:483] Algo bellman_ford step 1187 current loss 0.809166, current_train_items 38016.
I0302 18:58:55.307218 22626471084160 run.py:483] Algo bellman_ford step 1188 current loss 0.932642, current_train_items 38048.
I0302 18:58:55.338211 22626471084160 run.py:483] Algo bellman_ford step 1189 current loss 1.074468, current_train_items 38080.
I0302 18:58:55.357384 22626471084160 run.py:483] Algo bellman_ford step 1190 current loss 0.475395, current_train_items 38112.
I0302 18:58:55.373743 22626471084160 run.py:483] Algo bellman_ford step 1191 current loss 0.571563, current_train_items 38144.
I0302 18:58:55.397781 22626471084160 run.py:483] Algo bellman_ford step 1192 current loss 0.958858, current_train_items 38176.
I0302 18:58:55.427700 22626471084160 run.py:483] Algo bellman_ford step 1193 current loss 1.085571, current_train_items 38208.
I0302 18:58:55.459676 22626471084160 run.py:483] Algo bellman_ford step 1194 current loss 1.073183, current_train_items 38240.
I0302 18:58:55.478655 22626471084160 run.py:483] Algo bellman_ford step 1195 current loss 0.406338, current_train_items 38272.
I0302 18:58:55.494977 22626471084160 run.py:483] Algo bellman_ford step 1196 current loss 0.724397, current_train_items 38304.
I0302 18:58:55.518059 22626471084160 run.py:483] Algo bellman_ford step 1197 current loss 0.910478, current_train_items 38336.
I0302 18:58:55.549184 22626471084160 run.py:483] Algo bellman_ford step 1198 current loss 1.153186, current_train_items 38368.
I0302 18:58:55.582301 22626471084160 run.py:483] Algo bellman_ford step 1199 current loss 1.305706, current_train_items 38400.
I0302 18:58:55.601713 22626471084160 run.py:483] Algo bellman_ford step 1200 current loss 0.497057, current_train_items 38432.
I0302 18:58:55.611236 22626471084160 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0302 18:58:55.611343 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.908, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 18:58:55.628354 22626471084160 run.py:483] Algo bellman_ford step 1201 current loss 0.660168, current_train_items 38464.
I0302 18:58:55.651996 22626471084160 run.py:483] Algo bellman_ford step 1202 current loss 0.743616, current_train_items 38496.
I0302 18:58:55.682868 22626471084160 run.py:483] Algo bellman_ford step 1203 current loss 0.945386, current_train_items 38528.
I0302 18:58:55.716277 22626471084160 run.py:483] Algo bellman_ford step 1204 current loss 1.369082, current_train_items 38560.
I0302 18:58:55.735715 22626471084160 run.py:483] Algo bellman_ford step 1205 current loss 0.330318, current_train_items 38592.
I0302 18:58:55.751717 22626471084160 run.py:483] Algo bellman_ford step 1206 current loss 0.577035, current_train_items 38624.
I0302 18:58:55.774633 22626471084160 run.py:483] Algo bellman_ford step 1207 current loss 0.898744, current_train_items 38656.
I0302 18:58:55.804723 22626471084160 run.py:483] Algo bellman_ford step 1208 current loss 1.017210, current_train_items 38688.
I0302 18:58:55.838267 22626471084160 run.py:483] Algo bellman_ford step 1209 current loss 1.510804, current_train_items 38720.
I0302 18:58:55.857079 22626471084160 run.py:483] Algo bellman_ford step 1210 current loss 0.329212, current_train_items 38752.
I0302 18:58:55.873304 22626471084160 run.py:483] Algo bellman_ford step 1211 current loss 0.718441, current_train_items 38784.
I0302 18:58:55.895762 22626471084160 run.py:483] Algo bellman_ford step 1212 current loss 0.938205, current_train_items 38816.
I0302 18:58:55.924340 22626471084160 run.py:483] Algo bellman_ford step 1213 current loss 0.934077, current_train_items 38848.
I0302 18:58:55.956645 22626471084160 run.py:483] Algo bellman_ford step 1214 current loss 1.123499, current_train_items 38880.
I0302 18:58:55.975833 22626471084160 run.py:483] Algo bellman_ford step 1215 current loss 0.346213, current_train_items 38912.
I0302 18:58:55.991624 22626471084160 run.py:483] Algo bellman_ford step 1216 current loss 0.588700, current_train_items 38944.
I0302 18:58:56.014674 22626471084160 run.py:483] Algo bellman_ford step 1217 current loss 0.945635, current_train_items 38976.
I0302 18:58:56.044934 22626471084160 run.py:483] Algo bellman_ford step 1218 current loss 1.133846, current_train_items 39008.
I0302 18:58:56.077469 22626471084160 run.py:483] Algo bellman_ford step 1219 current loss 1.111322, current_train_items 39040.
I0302 18:58:56.096538 22626471084160 run.py:483] Algo bellman_ford step 1220 current loss 0.372273, current_train_items 39072.
I0302 18:58:56.112554 22626471084160 run.py:483] Algo bellman_ford step 1221 current loss 0.633081, current_train_items 39104.
I0302 18:58:56.136620 22626471084160 run.py:483] Algo bellman_ford step 1222 current loss 0.922556, current_train_items 39136.
I0302 18:58:56.166417 22626471084160 run.py:483] Algo bellman_ford step 1223 current loss 0.985879, current_train_items 39168.
I0302 18:58:56.198472 22626471084160 run.py:483] Algo bellman_ford step 1224 current loss 1.100186, current_train_items 39200.
I0302 18:58:56.217396 22626471084160 run.py:483] Algo bellman_ford step 1225 current loss 0.335551, current_train_items 39232.
I0302 18:58:56.234137 22626471084160 run.py:483] Algo bellman_ford step 1226 current loss 0.629532, current_train_items 39264.
I0302 18:58:56.258038 22626471084160 run.py:483] Algo bellman_ford step 1227 current loss 0.867215, current_train_items 39296.
I0302 18:58:56.286933 22626471084160 run.py:483] Algo bellman_ford step 1228 current loss 0.865417, current_train_items 39328.
I0302 18:58:56.320626 22626471084160 run.py:483] Algo bellman_ford step 1229 current loss 1.447850, current_train_items 39360.
I0302 18:58:56.339412 22626471084160 run.py:483] Algo bellman_ford step 1230 current loss 0.329254, current_train_items 39392.
I0302 18:58:56.355753 22626471084160 run.py:483] Algo bellman_ford step 1231 current loss 0.567283, current_train_items 39424.
I0302 18:58:56.379448 22626471084160 run.py:483] Algo bellman_ford step 1232 current loss 0.958551, current_train_items 39456.
I0302 18:58:56.408641 22626471084160 run.py:483] Algo bellman_ford step 1233 current loss 0.897657, current_train_items 39488.
I0302 18:58:56.439025 22626471084160 run.py:483] Algo bellman_ford step 1234 current loss 1.026874, current_train_items 39520.
I0302 18:58:56.457756 22626471084160 run.py:483] Algo bellman_ford step 1235 current loss 0.323506, current_train_items 39552.
I0302 18:58:56.473469 22626471084160 run.py:483] Algo bellman_ford step 1236 current loss 0.615141, current_train_items 39584.
I0302 18:58:56.496250 22626471084160 run.py:483] Algo bellman_ford step 1237 current loss 0.857275, current_train_items 39616.
I0302 18:58:56.525615 22626471084160 run.py:483] Algo bellman_ford step 1238 current loss 1.072897, current_train_items 39648.
I0302 18:58:56.558931 22626471084160 run.py:483] Algo bellman_ford step 1239 current loss 1.391601, current_train_items 39680.
I0302 18:58:56.577822 22626471084160 run.py:483] Algo bellman_ford step 1240 current loss 0.340737, current_train_items 39712.
I0302 18:58:56.594033 22626471084160 run.py:483] Algo bellman_ford step 1241 current loss 0.787117, current_train_items 39744.
I0302 18:58:56.617531 22626471084160 run.py:483] Algo bellman_ford step 1242 current loss 0.822097, current_train_items 39776.
I0302 18:58:56.646631 22626471084160 run.py:483] Algo bellman_ford step 1243 current loss 1.117775, current_train_items 39808.
I0302 18:58:56.679032 22626471084160 run.py:483] Algo bellman_ford step 1244 current loss 1.165734, current_train_items 39840.
I0302 18:58:56.697667 22626471084160 run.py:483] Algo bellman_ford step 1245 current loss 0.346394, current_train_items 39872.
I0302 18:58:56.713672 22626471084160 run.py:483] Algo bellman_ford step 1246 current loss 0.547961, current_train_items 39904.
I0302 18:58:56.735953 22626471084160 run.py:483] Algo bellman_ford step 1247 current loss 0.966195, current_train_items 39936.
I0302 18:58:56.765620 22626471084160 run.py:483] Algo bellman_ford step 1248 current loss 1.119379, current_train_items 39968.
I0302 18:58:56.797244 22626471084160 run.py:483] Algo bellman_ford step 1249 current loss 1.402155, current_train_items 40000.
I0302 18:58:56.816148 22626471084160 run.py:483] Algo bellman_ford step 1250 current loss 0.357073, current_train_items 40032.
I0302 18:58:56.824813 22626471084160 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.8564453125, 'score': 0.8564453125, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0302 18:58:56.824921 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.908, current avg val score is 0.856, val scores are: bellman_ford: 0.856
I0302 18:58:56.841831 22626471084160 run.py:483] Algo bellman_ford step 1251 current loss 0.560122, current_train_items 40064.
I0302 18:58:56.865302 22626471084160 run.py:483] Algo bellman_ford step 1252 current loss 0.946277, current_train_items 40096.
I0302 18:58:56.893450 22626471084160 run.py:483] Algo bellman_ford step 1253 current loss 0.926156, current_train_items 40128.
I0302 18:58:56.923325 22626471084160 run.py:483] Algo bellman_ford step 1254 current loss 1.037994, current_train_items 40160.
I0302 18:58:56.942491 22626471084160 run.py:483] Algo bellman_ford step 1255 current loss 0.348148, current_train_items 40192.
I0302 18:58:56.958744 22626471084160 run.py:483] Algo bellman_ford step 1256 current loss 0.689516, current_train_items 40224.
I0302 18:58:56.981670 22626471084160 run.py:483] Algo bellman_ford step 1257 current loss 1.013300, current_train_items 40256.
I0302 18:58:57.010366 22626471084160 run.py:483] Algo bellman_ford step 1258 current loss 1.063410, current_train_items 40288.
I0302 18:58:57.041847 22626471084160 run.py:483] Algo bellman_ford step 1259 current loss 1.289051, current_train_items 40320.
I0302 18:58:57.061026 22626471084160 run.py:483] Algo bellman_ford step 1260 current loss 0.425909, current_train_items 40352.
I0302 18:58:57.077875 22626471084160 run.py:483] Algo bellman_ford step 1261 current loss 0.716867, current_train_items 40384.
I0302 18:58:57.100949 22626471084160 run.py:483] Algo bellman_ford step 1262 current loss 0.977140, current_train_items 40416.
I0302 18:58:57.127521 22626471084160 run.py:483] Algo bellman_ford step 1263 current loss 0.760231, current_train_items 40448.
I0302 18:58:57.161552 22626471084160 run.py:483] Algo bellman_ford step 1264 current loss 1.276664, current_train_items 40480.
I0302 18:58:57.180217 22626471084160 run.py:483] Algo bellman_ford step 1265 current loss 0.304167, current_train_items 40512.
I0302 18:58:57.196325 22626471084160 run.py:483] Algo bellman_ford step 1266 current loss 0.640140, current_train_items 40544.
I0302 18:58:57.218553 22626471084160 run.py:483] Algo bellman_ford step 1267 current loss 0.921336, current_train_items 40576.
I0302 18:58:57.247216 22626471084160 run.py:483] Algo bellman_ford step 1268 current loss 1.011350, current_train_items 40608.
I0302 18:58:57.279752 22626471084160 run.py:483] Algo bellman_ford step 1269 current loss 1.247960, current_train_items 40640.
I0302 18:58:57.298975 22626471084160 run.py:483] Algo bellman_ford step 1270 current loss 0.362469, current_train_items 40672.
I0302 18:58:57.315058 22626471084160 run.py:483] Algo bellman_ford step 1271 current loss 0.607513, current_train_items 40704.
I0302 18:58:57.336793 22626471084160 run.py:483] Algo bellman_ford step 1272 current loss 0.851257, current_train_items 40736.
I0302 18:58:57.364758 22626471084160 run.py:483] Algo bellman_ford step 1273 current loss 0.826866, current_train_items 40768.
I0302 18:58:57.397230 22626471084160 run.py:483] Algo bellman_ford step 1274 current loss 1.305577, current_train_items 40800.
I0302 18:58:57.416249 22626471084160 run.py:483] Algo bellman_ford step 1275 current loss 0.421064, current_train_items 40832.
I0302 18:58:57.432547 22626471084160 run.py:483] Algo bellman_ford step 1276 current loss 0.650712, current_train_items 40864.
I0302 18:58:57.454195 22626471084160 run.py:483] Algo bellman_ford step 1277 current loss 0.837350, current_train_items 40896.
I0302 18:58:57.482980 22626471084160 run.py:483] Algo bellman_ford step 1278 current loss 0.949681, current_train_items 40928.
I0302 18:58:57.514577 22626471084160 run.py:483] Algo bellman_ford step 1279 current loss 1.293183, current_train_items 40960.
I0302 18:58:57.533553 22626471084160 run.py:483] Algo bellman_ford step 1280 current loss 0.394988, current_train_items 40992.
I0302 18:58:57.549588 22626471084160 run.py:483] Algo bellman_ford step 1281 current loss 0.593639, current_train_items 41024.
I0302 18:58:57.572699 22626471084160 run.py:483] Algo bellman_ford step 1282 current loss 0.939185, current_train_items 41056.
I0302 18:58:57.602497 22626471084160 run.py:483] Algo bellman_ford step 1283 current loss 1.007186, current_train_items 41088.
I0302 18:58:57.633787 22626471084160 run.py:483] Algo bellman_ford step 1284 current loss 1.097909, current_train_items 41120.
I0302 18:58:57.653279 22626471084160 run.py:483] Algo bellman_ford step 1285 current loss 0.343778, current_train_items 41152.
I0302 18:58:57.669644 22626471084160 run.py:483] Algo bellman_ford step 1286 current loss 0.609418, current_train_items 41184.
I0302 18:58:57.696183 22626471084160 run.py:483] Algo bellman_ford step 1287 current loss 0.980727, current_train_items 41216.
I0302 18:58:57.726065 22626471084160 run.py:483] Algo bellman_ford step 1288 current loss 1.008450, current_train_items 41248.
I0302 18:58:57.758511 22626471084160 run.py:483] Algo bellman_ford step 1289 current loss 1.074489, current_train_items 41280.
I0302 18:58:57.778237 22626471084160 run.py:483] Algo bellman_ford step 1290 current loss 0.395927, current_train_items 41312.
I0302 18:58:57.795014 22626471084160 run.py:483] Algo bellman_ford step 1291 current loss 0.750483, current_train_items 41344.
I0302 18:58:57.817303 22626471084160 run.py:483] Algo bellman_ford step 1292 current loss 0.763208, current_train_items 41376.
I0302 18:58:57.846368 22626471084160 run.py:483] Algo bellman_ford step 1293 current loss 0.996712, current_train_items 41408.
I0302 18:58:57.879545 22626471084160 run.py:483] Algo bellman_ford step 1294 current loss 1.105541, current_train_items 41440.
I0302 18:58:57.898478 22626471084160 run.py:483] Algo bellman_ford step 1295 current loss 0.335726, current_train_items 41472.
I0302 18:58:57.914874 22626471084160 run.py:483] Algo bellman_ford step 1296 current loss 0.555762, current_train_items 41504.
I0302 18:58:57.938327 22626471084160 run.py:483] Algo bellman_ford step 1297 current loss 0.869176, current_train_items 41536.
I0302 18:58:57.966065 22626471084160 run.py:483] Algo bellman_ford step 1298 current loss 0.711491, current_train_items 41568.
I0302 18:58:57.996032 22626471084160 run.py:483] Algo bellman_ford step 1299 current loss 1.196185, current_train_items 41600.
I0302 18:58:58.014980 22626471084160 run.py:483] Algo bellman_ford step 1300 current loss 0.390974, current_train_items 41632.
I0302 18:58:58.022440 22626471084160 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.8720703125, 'score': 0.8720703125, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0302 18:58:58.022546 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.908, current avg val score is 0.872, val scores are: bellman_ford: 0.872
I0302 18:58:58.039395 22626471084160 run.py:483] Algo bellman_ford step 1301 current loss 0.595511, current_train_items 41664.
I0302 18:58:58.063415 22626471084160 run.py:483] Algo bellman_ford step 1302 current loss 1.033511, current_train_items 41696.
I0302 18:58:58.093591 22626471084160 run.py:483] Algo bellman_ford step 1303 current loss 1.183852, current_train_items 41728.
I0302 18:58:58.124555 22626471084160 run.py:483] Algo bellman_ford step 1304 current loss 1.247156, current_train_items 41760.
I0302 18:58:58.143681 22626471084160 run.py:483] Algo bellman_ford step 1305 current loss 0.324573, current_train_items 41792.
I0302 18:58:58.159675 22626471084160 run.py:483] Algo bellman_ford step 1306 current loss 0.658758, current_train_items 41824.
I0302 18:58:58.181963 22626471084160 run.py:483] Algo bellman_ford step 1307 current loss 0.881982, current_train_items 41856.
I0302 18:58:58.210570 22626471084160 run.py:483] Algo bellman_ford step 1308 current loss 0.919120, current_train_items 41888.
I0302 18:58:58.241849 22626471084160 run.py:483] Algo bellman_ford step 1309 current loss 1.091783, current_train_items 41920.
I0302 18:58:58.260636 22626471084160 run.py:483] Algo bellman_ford step 1310 current loss 0.291920, current_train_items 41952.
I0302 18:58:58.276730 22626471084160 run.py:483] Algo bellman_ford step 1311 current loss 0.572299, current_train_items 41984.
I0302 18:58:58.299052 22626471084160 run.py:483] Algo bellman_ford step 1312 current loss 0.811558, current_train_items 42016.
I0302 18:58:58.330528 22626471084160 run.py:483] Algo bellman_ford step 1313 current loss 0.992382, current_train_items 42048.
I0302 18:58:58.362965 22626471084160 run.py:483] Algo bellman_ford step 1314 current loss 1.105126, current_train_items 42080.
I0302 18:58:58.381970 22626471084160 run.py:483] Algo bellman_ford step 1315 current loss 0.352054, current_train_items 42112.
I0302 18:58:58.398223 22626471084160 run.py:483] Algo bellman_ford step 1316 current loss 0.601802, current_train_items 42144.
I0302 18:58:58.421434 22626471084160 run.py:483] Algo bellman_ford step 1317 current loss 0.808035, current_train_items 42176.
I0302 18:58:58.450704 22626471084160 run.py:483] Algo bellman_ford step 1318 current loss 0.849881, current_train_items 42208.
I0302 18:58:58.482637 22626471084160 run.py:483] Algo bellman_ford step 1319 current loss 1.181772, current_train_items 42240.
I0302 18:58:58.501319 22626471084160 run.py:483] Algo bellman_ford step 1320 current loss 0.336120, current_train_items 42272.
I0302 18:58:58.517539 22626471084160 run.py:483] Algo bellman_ford step 1321 current loss 0.529483, current_train_items 42304.
I0302 18:58:58.541191 22626471084160 run.py:483] Algo bellman_ford step 1322 current loss 1.118174, current_train_items 42336.
I0302 18:58:58.570867 22626471084160 run.py:483] Algo bellman_ford step 1323 current loss 0.960249, current_train_items 42368.
I0302 18:58:58.602140 22626471084160 run.py:483] Algo bellman_ford step 1324 current loss 1.096396, current_train_items 42400.
I0302 18:58:58.621099 22626471084160 run.py:483] Algo bellman_ford step 1325 current loss 0.410186, current_train_items 42432.
I0302 18:58:58.637079 22626471084160 run.py:483] Algo bellman_ford step 1326 current loss 0.548308, current_train_items 42464.
I0302 18:58:58.659268 22626471084160 run.py:483] Algo bellman_ford step 1327 current loss 0.752913, current_train_items 42496.
I0302 18:58:58.688740 22626471084160 run.py:483] Algo bellman_ford step 1328 current loss 0.944266, current_train_items 42528.
I0302 18:58:58.721284 22626471084160 run.py:483] Algo bellman_ford step 1329 current loss 1.055870, current_train_items 42560.
I0302 18:58:58.740231 22626471084160 run.py:483] Algo bellman_ford step 1330 current loss 0.355259, current_train_items 42592.
I0302 18:58:58.756201 22626471084160 run.py:483] Algo bellman_ford step 1331 current loss 0.620042, current_train_items 42624.
I0302 18:58:58.779004 22626471084160 run.py:483] Algo bellman_ford step 1332 current loss 0.844556, current_train_items 42656.
I0302 18:58:58.808038 22626471084160 run.py:483] Algo bellman_ford step 1333 current loss 0.982977, current_train_items 42688.
I0302 18:58:58.838977 22626471084160 run.py:483] Algo bellman_ford step 1334 current loss 1.109253, current_train_items 42720.
I0302 18:58:58.857823 22626471084160 run.py:483] Algo bellman_ford step 1335 current loss 0.412788, current_train_items 42752.
I0302 18:58:58.873690 22626471084160 run.py:483] Algo bellman_ford step 1336 current loss 0.573524, current_train_items 42784.
I0302 18:58:58.898050 22626471084160 run.py:483] Algo bellman_ford step 1337 current loss 0.948619, current_train_items 42816.
I0302 18:58:58.927036 22626471084160 run.py:483] Algo bellman_ford step 1338 current loss 1.339470, current_train_items 42848.
I0302 18:58:58.961420 22626471084160 run.py:483] Algo bellman_ford step 1339 current loss 1.494625, current_train_items 42880.
I0302 18:58:58.980318 22626471084160 run.py:483] Algo bellman_ford step 1340 current loss 0.361032, current_train_items 42912.
I0302 18:58:58.996596 22626471084160 run.py:483] Algo bellman_ford step 1341 current loss 0.611076, current_train_items 42944.
I0302 18:58:59.020050 22626471084160 run.py:483] Algo bellman_ford step 1342 current loss 0.823627, current_train_items 42976.
I0302 18:58:59.049099 22626471084160 run.py:483] Algo bellman_ford step 1343 current loss 0.865433, current_train_items 43008.
I0302 18:58:59.080922 22626471084160 run.py:483] Algo bellman_ford step 1344 current loss 1.032527, current_train_items 43040.
I0302 18:58:59.100026 22626471084160 run.py:483] Algo bellman_ford step 1345 current loss 0.380265, current_train_items 43072.
I0302 18:58:59.116367 22626471084160 run.py:483] Algo bellman_ford step 1346 current loss 0.650143, current_train_items 43104.
I0302 18:58:59.140655 22626471084160 run.py:483] Algo bellman_ford step 1347 current loss 0.979938, current_train_items 43136.
I0302 18:58:59.168707 22626471084160 run.py:483] Algo bellman_ford step 1348 current loss 0.949450, current_train_items 43168.
I0302 18:58:59.199572 22626471084160 run.py:483] Algo bellman_ford step 1349 current loss 1.192535, current_train_items 43200.
I0302 18:58:59.218576 22626471084160 run.py:483] Algo bellman_ford step 1350 current loss 0.375395, current_train_items 43232.
I0302 18:58:59.227027 22626471084160 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.8623046875, 'score': 0.8623046875, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0302 18:58:59.227135 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.908, current avg val score is 0.862, val scores are: bellman_ford: 0.862
I0302 18:58:59.244089 22626471084160 run.py:483] Algo bellman_ford step 1351 current loss 0.684159, current_train_items 43264.
I0302 18:58:59.268538 22626471084160 run.py:483] Algo bellman_ford step 1352 current loss 0.740537, current_train_items 43296.
I0302 18:58:59.296581 22626471084160 run.py:483] Algo bellman_ford step 1353 current loss 0.873618, current_train_items 43328.
I0302 18:58:59.331357 22626471084160 run.py:483] Algo bellman_ford step 1354 current loss 1.205739, current_train_items 43360.
I0302 18:58:59.350959 22626471084160 run.py:483] Algo bellman_ford step 1355 current loss 0.372249, current_train_items 43392.
I0302 18:58:59.366870 22626471084160 run.py:483] Algo bellman_ford step 1356 current loss 0.585701, current_train_items 43424.
I0302 18:58:59.389486 22626471084160 run.py:483] Algo bellman_ford step 1357 current loss 0.801505, current_train_items 43456.
I0302 18:58:59.418899 22626471084160 run.py:483] Algo bellman_ford step 1358 current loss 0.874035, current_train_items 43488.
I0302 18:58:59.452470 22626471084160 run.py:483] Algo bellman_ford step 1359 current loss 1.173751, current_train_items 43520.
I0302 18:58:59.472052 22626471084160 run.py:483] Algo bellman_ford step 1360 current loss 0.395489, current_train_items 43552.
I0302 18:58:59.489060 22626471084160 run.py:483] Algo bellman_ford step 1361 current loss 0.664349, current_train_items 43584.
I0302 18:58:59.512916 22626471084160 run.py:483] Algo bellman_ford step 1362 current loss 0.777594, current_train_items 43616.
I0302 18:58:59.542501 22626471084160 run.py:483] Algo bellman_ford step 1363 current loss 0.988597, current_train_items 43648.
I0302 18:58:59.575860 22626471084160 run.py:483] Algo bellman_ford step 1364 current loss 1.099095, current_train_items 43680.
I0302 18:58:59.595141 22626471084160 run.py:483] Algo bellman_ford step 1365 current loss 0.359031, current_train_items 43712.
I0302 18:58:59.611039 22626471084160 run.py:483] Algo bellman_ford step 1366 current loss 0.531044, current_train_items 43744.
I0302 18:58:59.633727 22626471084160 run.py:483] Algo bellman_ford step 1367 current loss 0.745078, current_train_items 43776.
I0302 18:58:59.661222 22626471084160 run.py:483] Algo bellman_ford step 1368 current loss 0.845648, current_train_items 43808.
I0302 18:58:59.693608 22626471084160 run.py:483] Algo bellman_ford step 1369 current loss 1.131143, current_train_items 43840.
I0302 18:58:59.712712 22626471084160 run.py:483] Algo bellman_ford step 1370 current loss 0.349567, current_train_items 43872.
I0302 18:58:59.729274 22626471084160 run.py:483] Algo bellman_ford step 1371 current loss 0.693076, current_train_items 43904.
I0302 18:58:59.752647 22626471084160 run.py:483] Algo bellman_ford step 1372 current loss 0.853826, current_train_items 43936.
I0302 18:58:59.783050 22626471084160 run.py:483] Algo bellman_ford step 1373 current loss 0.967597, current_train_items 43968.
I0302 18:58:59.815979 22626471084160 run.py:483] Algo bellman_ford step 1374 current loss 1.059610, current_train_items 44000.
I0302 18:58:59.835557 22626471084160 run.py:483] Algo bellman_ford step 1375 current loss 0.363176, current_train_items 44032.
I0302 18:58:59.851353 22626471084160 run.py:483] Algo bellman_ford step 1376 current loss 0.657694, current_train_items 44064.
I0302 18:58:59.874087 22626471084160 run.py:483] Algo bellman_ford step 1377 current loss 1.048760, current_train_items 44096.
I0302 18:58:59.904011 22626471084160 run.py:483] Algo bellman_ford step 1378 current loss 1.095558, current_train_items 44128.
I0302 18:58:59.936567 22626471084160 run.py:483] Algo bellman_ford step 1379 current loss 1.224458, current_train_items 44160.
I0302 18:58:59.955952 22626471084160 run.py:483] Algo bellman_ford step 1380 current loss 0.406971, current_train_items 44192.
I0302 18:58:59.972448 22626471084160 run.py:483] Algo bellman_ford step 1381 current loss 0.726911, current_train_items 44224.
I0302 18:58:59.995488 22626471084160 run.py:483] Algo bellman_ford step 1382 current loss 0.805102, current_train_items 44256.
I0302 18:59:00.025204 22626471084160 run.py:483] Algo bellman_ford step 1383 current loss 0.923240, current_train_items 44288.
I0302 18:59:00.055215 22626471084160 run.py:483] Algo bellman_ford step 1384 current loss 1.055501, current_train_items 44320.
I0302 18:59:00.074305 22626471084160 run.py:483] Algo bellman_ford step 1385 current loss 0.320001, current_train_items 44352.
I0302 18:59:00.090276 22626471084160 run.py:483] Algo bellman_ford step 1386 current loss 0.680800, current_train_items 44384.
I0302 18:59:00.112444 22626471084160 run.py:483] Algo bellman_ford step 1387 current loss 0.667420, current_train_items 44416.
I0302 18:59:00.142182 22626471084160 run.py:483] Algo bellman_ford step 1388 current loss 0.827803, current_train_items 44448.
I0302 18:59:00.174576 22626471084160 run.py:483] Algo bellman_ford step 1389 current loss 1.038096, current_train_items 44480.
I0302 18:59:00.194046 22626471084160 run.py:483] Algo bellman_ford step 1390 current loss 0.352526, current_train_items 44512.
I0302 18:59:00.209900 22626471084160 run.py:483] Algo bellman_ford step 1391 current loss 0.507624, current_train_items 44544.
I0302 18:59:00.233647 22626471084160 run.py:483] Algo bellman_ford step 1392 current loss 0.807627, current_train_items 44576.
I0302 18:59:00.262989 22626471084160 run.py:483] Algo bellman_ford step 1393 current loss 0.955088, current_train_items 44608.
I0302 18:59:00.295399 22626471084160 run.py:483] Algo bellman_ford step 1394 current loss 1.106849, current_train_items 44640.
I0302 18:59:00.313978 22626471084160 run.py:483] Algo bellman_ford step 1395 current loss 0.278586, current_train_items 44672.
I0302 18:59:00.330141 22626471084160 run.py:483] Algo bellman_ford step 1396 current loss 0.530696, current_train_items 44704.
I0302 18:59:00.353349 22626471084160 run.py:483] Algo bellman_ford step 1397 current loss 0.845834, current_train_items 44736.
I0302 18:59:00.383571 22626471084160 run.py:483] Algo bellman_ford step 1398 current loss 1.038558, current_train_items 44768.
I0302 18:59:00.415019 22626471084160 run.py:483] Algo bellman_ford step 1399 current loss 0.998812, current_train_items 44800.
I0302 18:59:00.434451 22626471084160 run.py:483] Algo bellman_ford step 1400 current loss 0.453970, current_train_items 44832.
I0302 18:59:00.442129 22626471084160 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.8828125, 'score': 0.8828125, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0302 18:59:00.442254 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.908, current avg val score is 0.883, val scores are: bellman_ford: 0.883
I0302 18:59:00.458921 22626471084160 run.py:483] Algo bellman_ford step 1401 current loss 0.555653, current_train_items 44864.
I0302 18:59:00.482919 22626471084160 run.py:483] Algo bellman_ford step 1402 current loss 1.004812, current_train_items 44896.
I0302 18:59:00.512945 22626471084160 run.py:483] Algo bellman_ford step 1403 current loss 1.014665, current_train_items 44928.
I0302 18:59:00.546140 22626471084160 run.py:483] Algo bellman_ford step 1404 current loss 1.174736, current_train_items 44960.
I0302 18:59:00.565334 22626471084160 run.py:483] Algo bellman_ford step 1405 current loss 0.409475, current_train_items 44992.
I0302 18:59:00.581718 22626471084160 run.py:483] Algo bellman_ford step 1406 current loss 0.685945, current_train_items 45024.
I0302 18:59:00.604715 22626471084160 run.py:483] Algo bellman_ford step 1407 current loss 0.768927, current_train_items 45056.
I0302 18:59:00.634573 22626471084160 run.py:483] Algo bellman_ford step 1408 current loss 0.976182, current_train_items 45088.
I0302 18:59:00.666476 22626471084160 run.py:483] Algo bellman_ford step 1409 current loss 1.078967, current_train_items 45120.
I0302 18:59:00.685170 22626471084160 run.py:483] Algo bellman_ford step 1410 current loss 0.393366, current_train_items 45152.
I0302 18:59:00.701648 22626471084160 run.py:483] Algo bellman_ford step 1411 current loss 0.579952, current_train_items 45184.
I0302 18:59:00.724421 22626471084160 run.py:483] Algo bellman_ford step 1412 current loss 0.869274, current_train_items 45216.
I0302 18:59:00.751765 22626471084160 run.py:483] Algo bellman_ford step 1413 current loss 0.723470, current_train_items 45248.
I0302 18:59:00.782967 22626471084160 run.py:483] Algo bellman_ford step 1414 current loss 1.024289, current_train_items 45280.
I0302 18:59:00.801873 22626471084160 run.py:483] Algo bellman_ford step 1415 current loss 0.307133, current_train_items 45312.
I0302 18:59:00.818407 22626471084160 run.py:483] Algo bellman_ford step 1416 current loss 0.676901, current_train_items 45344.
I0302 18:59:00.840850 22626471084160 run.py:483] Algo bellman_ford step 1417 current loss 0.728818, current_train_items 45376.
I0302 18:59:00.869048 22626471084160 run.py:483] Algo bellman_ford step 1418 current loss 0.977925, current_train_items 45408.
I0302 18:59:00.901685 22626471084160 run.py:483] Algo bellman_ford step 1419 current loss 0.999173, current_train_items 45440.
I0302 18:59:00.920773 22626471084160 run.py:483] Algo bellman_ford step 1420 current loss 0.433210, current_train_items 45472.
I0302 18:59:00.936916 22626471084160 run.py:483] Algo bellman_ford step 1421 current loss 0.600116, current_train_items 45504.
I0302 18:59:00.960483 22626471084160 run.py:483] Algo bellman_ford step 1422 current loss 0.757508, current_train_items 45536.
I0302 18:59:00.990693 22626471084160 run.py:483] Algo bellman_ford step 1423 current loss 0.920151, current_train_items 45568.
I0302 18:59:01.021947 22626471084160 run.py:483] Algo bellman_ford step 1424 current loss 1.082155, current_train_items 45600.
I0302 18:59:01.040763 22626471084160 run.py:483] Algo bellman_ford step 1425 current loss 0.365549, current_train_items 45632.
I0302 18:59:01.057498 22626471084160 run.py:483] Algo bellman_ford step 1426 current loss 0.611585, current_train_items 45664.
I0302 18:59:01.081398 22626471084160 run.py:483] Algo bellman_ford step 1427 current loss 0.816072, current_train_items 45696.
I0302 18:59:01.110909 22626471084160 run.py:483] Algo bellman_ford step 1428 current loss 0.926636, current_train_items 45728.
I0302 18:59:01.141835 22626471084160 run.py:483] Algo bellman_ford step 1429 current loss 0.935097, current_train_items 45760.
I0302 18:59:01.160933 22626471084160 run.py:483] Algo bellman_ford step 1430 current loss 0.353616, current_train_items 45792.
I0302 18:59:01.176884 22626471084160 run.py:483] Algo bellman_ford step 1431 current loss 0.597037, current_train_items 45824.
I0302 18:59:01.201579 22626471084160 run.py:483] Algo bellman_ford step 1432 current loss 0.860890, current_train_items 45856.
I0302 18:59:01.229465 22626471084160 run.py:483] Algo bellman_ford step 1433 current loss 0.831556, current_train_items 45888.
I0302 18:59:01.260358 22626471084160 run.py:483] Algo bellman_ford step 1434 current loss 0.923078, current_train_items 45920.
I0302 18:59:01.279075 22626471084160 run.py:483] Algo bellman_ford step 1435 current loss 0.395264, current_train_items 45952.
I0302 18:59:01.295465 22626471084160 run.py:483] Algo bellman_ford step 1436 current loss 0.708540, current_train_items 45984.
I0302 18:59:01.318222 22626471084160 run.py:483] Algo bellman_ford step 1437 current loss 0.877746, current_train_items 46016.
I0302 18:59:01.347064 22626471084160 run.py:483] Algo bellman_ford step 1438 current loss 0.827321, current_train_items 46048.
I0302 18:59:01.380263 22626471084160 run.py:483] Algo bellman_ford step 1439 current loss 0.994672, current_train_items 46080.
I0302 18:59:01.399548 22626471084160 run.py:483] Algo bellman_ford step 1440 current loss 0.420866, current_train_items 46112.
I0302 18:59:01.415744 22626471084160 run.py:483] Algo bellman_ford step 1441 current loss 0.693668, current_train_items 46144.
I0302 18:59:01.438502 22626471084160 run.py:483] Algo bellman_ford step 1442 current loss 0.760576, current_train_items 46176.
I0302 18:59:01.468011 22626471084160 run.py:483] Algo bellman_ford step 1443 current loss 0.879555, current_train_items 46208.
I0302 18:59:01.503897 22626471084160 run.py:483] Algo bellman_ford step 1444 current loss 1.174045, current_train_items 46240.
I0302 18:59:01.522935 22626471084160 run.py:483] Algo bellman_ford step 1445 current loss 0.374471, current_train_items 46272.
I0302 18:59:01.538599 22626471084160 run.py:483] Algo bellman_ford step 1446 current loss 0.560625, current_train_items 46304.
I0302 18:59:01.562246 22626471084160 run.py:483] Algo bellman_ford step 1447 current loss 0.890385, current_train_items 46336.
I0302 18:59:01.591159 22626471084160 run.py:483] Algo bellman_ford step 1448 current loss 1.023719, current_train_items 46368.
I0302 18:59:01.622461 22626471084160 run.py:483] Algo bellman_ford step 1449 current loss 1.068956, current_train_items 46400.
I0302 18:59:01.641723 22626471084160 run.py:483] Algo bellman_ford step 1450 current loss 0.341114, current_train_items 46432.
I0302 18:59:01.650077 22626471084160 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.8740234375, 'score': 0.8740234375, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0302 18:59:01.650198 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.908, current avg val score is 0.874, val scores are: bellman_ford: 0.874
I0302 18:59:01.666609 22626471084160 run.py:483] Algo bellman_ford step 1451 current loss 0.580441, current_train_items 46464.
I0302 18:59:01.689694 22626471084160 run.py:483] Algo bellman_ford step 1452 current loss 0.770031, current_train_items 46496.
I0302 18:59:01.720416 22626471084160 run.py:483] Algo bellman_ford step 1453 current loss 1.128430, current_train_items 46528.
I0302 18:59:01.754393 22626471084160 run.py:483] Algo bellman_ford step 1454 current loss 1.223475, current_train_items 46560.
I0302 18:59:01.773704 22626471084160 run.py:483] Algo bellman_ford step 1455 current loss 0.361226, current_train_items 46592.
I0302 18:59:01.789700 22626471084160 run.py:483] Algo bellman_ford step 1456 current loss 0.608361, current_train_items 46624.
I0302 18:59:01.811604 22626471084160 run.py:483] Algo bellman_ford step 1457 current loss 0.650218, current_train_items 46656.
I0302 18:59:01.841459 22626471084160 run.py:483] Algo bellman_ford step 1458 current loss 0.944951, current_train_items 46688.
I0302 18:59:01.873118 22626471084160 run.py:483] Algo bellman_ford step 1459 current loss 0.983625, current_train_items 46720.
I0302 18:59:01.892179 22626471084160 run.py:483] Algo bellman_ford step 1460 current loss 0.397161, current_train_items 46752.
I0302 18:59:01.907867 22626471084160 run.py:483] Algo bellman_ford step 1461 current loss 0.705472, current_train_items 46784.
I0302 18:59:01.930054 22626471084160 run.py:483] Algo bellman_ford step 1462 current loss 0.900722, current_train_items 46816.
I0302 18:59:01.959046 22626471084160 run.py:483] Algo bellman_ford step 1463 current loss 0.971922, current_train_items 46848.
I0302 18:59:01.989847 22626471084160 run.py:483] Algo bellman_ford step 1464 current loss 1.082921, current_train_items 46880.
I0302 18:59:02.008589 22626471084160 run.py:483] Algo bellman_ford step 1465 current loss 0.449850, current_train_items 46912.
I0302 18:59:02.025033 22626471084160 run.py:483] Algo bellman_ford step 1466 current loss 0.524953, current_train_items 46944.
I0302 18:59:02.048146 22626471084160 run.py:483] Algo bellman_ford step 1467 current loss 0.695494, current_train_items 46976.
I0302 18:59:02.075353 22626471084160 run.py:483] Algo bellman_ford step 1468 current loss 0.883623, current_train_items 47008.
I0302 18:59:02.105827 22626471084160 run.py:483] Algo bellman_ford step 1469 current loss 1.072375, current_train_items 47040.
I0302 18:59:02.125205 22626471084160 run.py:483] Algo bellman_ford step 1470 current loss 0.325424, current_train_items 47072.
I0302 18:59:02.141955 22626471084160 run.py:483] Algo bellman_ford step 1471 current loss 0.572621, current_train_items 47104.
I0302 18:59:02.164875 22626471084160 run.py:483] Algo bellman_ford step 1472 current loss 0.718310, current_train_items 47136.
I0302 18:59:02.194036 22626471084160 run.py:483] Algo bellman_ford step 1473 current loss 0.914341, current_train_items 47168.
I0302 18:59:02.226997 22626471084160 run.py:483] Algo bellman_ford step 1474 current loss 1.109064, current_train_items 47200.
I0302 18:59:02.246218 22626471084160 run.py:483] Algo bellman_ford step 1475 current loss 0.430865, current_train_items 47232.
I0302 18:59:02.262269 22626471084160 run.py:483] Algo bellman_ford step 1476 current loss 0.522961, current_train_items 47264.
I0302 18:59:02.285008 22626471084160 run.py:483] Algo bellman_ford step 1477 current loss 0.702111, current_train_items 47296.
I0302 18:59:02.314043 22626471084160 run.py:483] Algo bellman_ford step 1478 current loss 0.898933, current_train_items 47328.
I0302 18:59:02.347605 22626471084160 run.py:483] Algo bellman_ford step 1479 current loss 1.017732, current_train_items 47360.
I0302 18:59:02.366314 22626471084160 run.py:483] Algo bellman_ford step 1480 current loss 0.347047, current_train_items 47392.
I0302 18:59:02.382194 22626471084160 run.py:483] Algo bellman_ford step 1481 current loss 0.525307, current_train_items 47424.
I0302 18:59:02.404426 22626471084160 run.py:483] Algo bellman_ford step 1482 current loss 0.721007, current_train_items 47456.
I0302 18:59:02.433848 22626471084160 run.py:483] Algo bellman_ford step 1483 current loss 0.904967, current_train_items 47488.
I0302 18:59:02.464225 22626471084160 run.py:483] Algo bellman_ford step 1484 current loss 1.015684, current_train_items 47520.
I0302 18:59:02.483610 22626471084160 run.py:483] Algo bellman_ford step 1485 current loss 0.374052, current_train_items 47552.
I0302 18:59:02.499460 22626471084160 run.py:483] Algo bellman_ford step 1486 current loss 0.631455, current_train_items 47584.
I0302 18:59:02.522753 22626471084160 run.py:483] Algo bellman_ford step 1487 current loss 0.937707, current_train_items 47616.
I0302 18:59:02.550250 22626471084160 run.py:483] Algo bellman_ford step 1488 current loss 0.839026, current_train_items 47648.
I0302 18:59:02.582813 22626471084160 run.py:483] Algo bellman_ford step 1489 current loss 1.077474, current_train_items 47680.
I0302 18:59:02.602098 22626471084160 run.py:483] Algo bellman_ford step 1490 current loss 0.414477, current_train_items 47712.
I0302 18:59:02.618262 22626471084160 run.py:483] Algo bellman_ford step 1491 current loss 0.662887, current_train_items 47744.
I0302 18:59:02.641510 22626471084160 run.py:483] Algo bellman_ford step 1492 current loss 0.769300, current_train_items 47776.
I0302 18:59:02.670552 22626471084160 run.py:483] Algo bellman_ford step 1493 current loss 0.804764, current_train_items 47808.
I0302 18:59:02.703882 22626471084160 run.py:483] Algo bellman_ford step 1494 current loss 1.062221, current_train_items 47840.
I0302 18:59:02.722580 22626471084160 run.py:483] Algo bellman_ford step 1495 current loss 0.392044, current_train_items 47872.
I0302 18:59:02.739221 22626471084160 run.py:483] Algo bellman_ford step 1496 current loss 0.704361, current_train_items 47904.
I0302 18:59:02.762635 22626471084160 run.py:483] Algo bellman_ford step 1497 current loss 0.942917, current_train_items 47936.
I0302 18:59:02.792461 22626471084160 run.py:483] Algo bellman_ford step 1498 current loss 0.938625, current_train_items 47968.
I0302 18:59:02.826285 22626471084160 run.py:483] Algo bellman_ford step 1499 current loss 1.078355, current_train_items 48000.
I0302 18:59:02.845259 22626471084160 run.py:483] Algo bellman_ford step 1500 current loss 0.342705, current_train_items 48032.
I0302 18:59:02.853013 22626471084160 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0302 18:59:02.853143 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.908, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:59:02.883295 22626471084160 run.py:483] Algo bellman_ford step 1501 current loss 0.600910, current_train_items 48064.
I0302 18:59:02.907329 22626471084160 run.py:483] Algo bellman_ford step 1502 current loss 0.969516, current_train_items 48096.
I0302 18:59:02.936968 22626471084160 run.py:483] Algo bellman_ford step 1503 current loss 0.934834, current_train_items 48128.
I0302 18:59:02.969818 22626471084160 run.py:483] Algo bellman_ford step 1504 current loss 1.020463, current_train_items 48160.
I0302 18:59:02.989501 22626471084160 run.py:483] Algo bellman_ford step 1505 current loss 0.304965, current_train_items 48192.
I0302 18:59:03.005494 22626471084160 run.py:483] Algo bellman_ford step 1506 current loss 0.542380, current_train_items 48224.
I0302 18:59:03.028292 22626471084160 run.py:483] Algo bellman_ford step 1507 current loss 0.713510, current_train_items 48256.
I0302 18:59:03.057659 22626471084160 run.py:483] Algo bellman_ford step 1508 current loss 0.928428, current_train_items 48288.
I0302 18:59:03.089957 22626471084160 run.py:483] Algo bellman_ford step 1509 current loss 1.064651, current_train_items 48320.
I0302 18:59:03.108921 22626471084160 run.py:483] Algo bellman_ford step 1510 current loss 0.338936, current_train_items 48352.
I0302 18:59:03.125343 22626471084160 run.py:483] Algo bellman_ford step 1511 current loss 0.765587, current_train_items 48384.
I0302 18:59:03.148572 22626471084160 run.py:483] Algo bellman_ford step 1512 current loss 0.724484, current_train_items 48416.
I0302 18:59:03.176619 22626471084160 run.py:483] Algo bellman_ford step 1513 current loss 0.741892, current_train_items 48448.
I0302 18:59:03.209968 22626471084160 run.py:483] Algo bellman_ford step 1514 current loss 1.127927, current_train_items 48480.
I0302 18:59:03.228942 22626471084160 run.py:483] Algo bellman_ford step 1515 current loss 0.345570, current_train_items 48512.
I0302 18:59:03.244747 22626471084160 run.py:483] Algo bellman_ford step 1516 current loss 0.669678, current_train_items 48544.
I0302 18:59:03.267953 22626471084160 run.py:483] Algo bellman_ford step 1517 current loss 0.739790, current_train_items 48576.
I0302 18:59:03.298011 22626471084160 run.py:483] Algo bellman_ford step 1518 current loss 0.904695, current_train_items 48608.
I0302 18:59:03.330027 22626471084160 run.py:483] Algo bellman_ford step 1519 current loss 1.012351, current_train_items 48640.
I0302 18:59:03.348947 22626471084160 run.py:483] Algo bellman_ford step 1520 current loss 0.316782, current_train_items 48672.
I0302 18:59:03.364720 22626471084160 run.py:483] Algo bellman_ford step 1521 current loss 0.616609, current_train_items 48704.
I0302 18:59:03.388374 22626471084160 run.py:483] Algo bellman_ford step 1522 current loss 0.816228, current_train_items 48736.
I0302 18:59:03.417676 22626471084160 run.py:483] Algo bellman_ford step 1523 current loss 0.974571, current_train_items 48768.
I0302 18:59:03.449722 22626471084160 run.py:483] Algo bellman_ford step 1524 current loss 1.346152, current_train_items 48800.
I0302 18:59:03.468530 22626471084160 run.py:483] Algo bellman_ford step 1525 current loss 0.351629, current_train_items 48832.
I0302 18:59:03.484327 22626471084160 run.py:483] Algo bellman_ford step 1526 current loss 0.554626, current_train_items 48864.
I0302 18:59:03.508422 22626471084160 run.py:483] Algo bellman_ford step 1527 current loss 0.915004, current_train_items 48896.
I0302 18:59:03.537833 22626471084160 run.py:483] Algo bellman_ford step 1528 current loss 0.847274, current_train_items 48928.
I0302 18:59:03.570600 22626471084160 run.py:483] Algo bellman_ford step 1529 current loss 1.202037, current_train_items 48960.
I0302 18:59:03.589733 22626471084160 run.py:483] Algo bellman_ford step 1530 current loss 0.410686, current_train_items 48992.
I0302 18:59:03.605578 22626471084160 run.py:483] Algo bellman_ford step 1531 current loss 0.608674, current_train_items 49024.
I0302 18:59:03.628604 22626471084160 run.py:483] Algo bellman_ford step 1532 current loss 0.823188, current_train_items 49056.
I0302 18:59:03.658445 22626471084160 run.py:483] Algo bellman_ford step 1533 current loss 0.937493, current_train_items 49088.
I0302 18:59:03.691585 22626471084160 run.py:483] Algo bellman_ford step 1534 current loss 1.204448, current_train_items 49120.
I0302 18:59:03.710799 22626471084160 run.py:483] Algo bellman_ford step 1535 current loss 0.446795, current_train_items 49152.
I0302 18:59:03.727295 22626471084160 run.py:483] Algo bellman_ford step 1536 current loss 0.581819, current_train_items 49184.
I0302 18:59:03.750611 22626471084160 run.py:483] Algo bellman_ford step 1537 current loss 0.802284, current_train_items 49216.
I0302 18:59:03.780429 22626471084160 run.py:483] Algo bellman_ford step 1538 current loss 0.858952, current_train_items 49248.
I0302 18:59:03.815584 22626471084160 run.py:483] Algo bellman_ford step 1539 current loss 1.121168, current_train_items 49280.
I0302 18:59:03.834828 22626471084160 run.py:483] Algo bellman_ford step 1540 current loss 0.446077, current_train_items 49312.
I0302 18:59:03.850604 22626471084160 run.py:483] Algo bellman_ford step 1541 current loss 0.581684, current_train_items 49344.
I0302 18:59:03.873496 22626471084160 run.py:483] Algo bellman_ford step 1542 current loss 0.746432, current_train_items 49376.
I0302 18:59:03.902192 22626471084160 run.py:483] Algo bellman_ford step 1543 current loss 0.794646, current_train_items 49408.
I0302 18:59:03.934504 22626471084160 run.py:483] Algo bellman_ford step 1544 current loss 0.974009, current_train_items 49440.
I0302 18:59:03.953454 22626471084160 run.py:483] Algo bellman_ford step 1545 current loss 0.377828, current_train_items 49472.
I0302 18:59:03.969530 22626471084160 run.py:483] Algo bellman_ford step 1546 current loss 0.736948, current_train_items 49504.
I0302 18:59:03.992840 22626471084160 run.py:483] Algo bellman_ford step 1547 current loss 0.965705, current_train_items 49536.
I0302 18:59:04.023280 22626471084160 run.py:483] Algo bellman_ford step 1548 current loss 1.217361, current_train_items 49568.
I0302 18:59:04.056929 22626471084160 run.py:483] Algo bellman_ford step 1549 current loss 1.289681, current_train_items 49600.
I0302 18:59:04.075737 22626471084160 run.py:483] Algo bellman_ford step 1550 current loss 0.372824, current_train_items 49632.
I0302 18:59:04.083765 22626471084160 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0302 18:59:04.083871 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.913, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 18:59:04.113987 22626471084160 run.py:483] Algo bellman_ford step 1551 current loss 0.613453, current_train_items 49664.
I0302 18:59:04.136955 22626471084160 run.py:483] Algo bellman_ford step 1552 current loss 0.819470, current_train_items 49696.
I0302 18:59:04.166989 22626471084160 run.py:483] Algo bellman_ford step 1553 current loss 0.825508, current_train_items 49728.
I0302 18:59:04.198517 22626471084160 run.py:483] Algo bellman_ford step 1554 current loss 0.965900, current_train_items 49760.
I0302 18:59:04.218049 22626471084160 run.py:483] Algo bellman_ford step 1555 current loss 0.327728, current_train_items 49792.
I0302 18:59:04.234038 22626471084160 run.py:483] Algo bellman_ford step 1556 current loss 0.685544, current_train_items 49824.
I0302 18:59:04.257734 22626471084160 run.py:483] Algo bellman_ford step 1557 current loss 1.046032, current_train_items 49856.
I0302 18:59:04.287887 22626471084160 run.py:483] Algo bellman_ford step 1558 current loss 0.882417, current_train_items 49888.
I0302 18:59:04.321321 22626471084160 run.py:483] Algo bellman_ford step 1559 current loss 1.114694, current_train_items 49920.
I0302 18:59:04.340710 22626471084160 run.py:483] Algo bellman_ford step 1560 current loss 0.384524, current_train_items 49952.
I0302 18:59:04.357190 22626471084160 run.py:483] Algo bellman_ford step 1561 current loss 0.580076, current_train_items 49984.
I0302 18:59:04.380453 22626471084160 run.py:483] Algo bellman_ford step 1562 current loss 0.696838, current_train_items 50016.
I0302 18:59:04.410550 22626471084160 run.py:483] Algo bellman_ford step 1563 current loss 0.871168, current_train_items 50048.
I0302 18:59:04.443951 22626471084160 run.py:483] Algo bellman_ford step 1564 current loss 0.966598, current_train_items 50080.
I0302 18:59:04.463022 22626471084160 run.py:483] Algo bellman_ford step 1565 current loss 0.344754, current_train_items 50112.
I0302 18:59:04.479366 22626471084160 run.py:483] Algo bellman_ford step 1566 current loss 0.653627, current_train_items 50144.
I0302 18:59:04.502904 22626471084160 run.py:483] Algo bellman_ford step 1567 current loss 0.671810, current_train_items 50176.
I0302 18:59:04.531986 22626471084160 run.py:483] Algo bellman_ford step 1568 current loss 0.906015, current_train_items 50208.
I0302 18:59:04.565784 22626471084160 run.py:483] Algo bellman_ford step 1569 current loss 1.035742, current_train_items 50240.
I0302 18:59:04.584906 22626471084160 run.py:483] Algo bellman_ford step 1570 current loss 0.350860, current_train_items 50272.
I0302 18:59:04.601307 22626471084160 run.py:483] Algo bellman_ford step 1571 current loss 0.528182, current_train_items 50304.
I0302 18:59:04.625139 22626471084160 run.py:483] Algo bellman_ford step 1572 current loss 0.750234, current_train_items 50336.
I0302 18:59:04.654349 22626471084160 run.py:483] Algo bellman_ford step 1573 current loss 0.768698, current_train_items 50368.
I0302 18:59:04.684561 22626471084160 run.py:483] Algo bellman_ford step 1574 current loss 1.122929, current_train_items 50400.
I0302 18:59:04.704242 22626471084160 run.py:483] Algo bellman_ford step 1575 current loss 0.378529, current_train_items 50432.
I0302 18:59:04.720516 22626471084160 run.py:483] Algo bellman_ford step 1576 current loss 0.580797, current_train_items 50464.
I0302 18:59:04.742259 22626471084160 run.py:483] Algo bellman_ford step 1577 current loss 0.687971, current_train_items 50496.
I0302 18:59:04.772614 22626471084160 run.py:483] Algo bellman_ford step 1578 current loss 1.001155, current_train_items 50528.
I0302 18:59:04.804935 22626471084160 run.py:483] Algo bellman_ford step 1579 current loss 1.171224, current_train_items 50560.
I0302 18:59:04.824043 22626471084160 run.py:483] Algo bellman_ford step 1580 current loss 0.432282, current_train_items 50592.
I0302 18:59:04.839868 22626471084160 run.py:483] Algo bellman_ford step 1581 current loss 0.637943, current_train_items 50624.
I0302 18:59:04.862886 22626471084160 run.py:483] Algo bellman_ford step 1582 current loss 0.669852, current_train_items 50656.
I0302 18:59:04.892433 22626471084160 run.py:483] Algo bellman_ford step 1583 current loss 0.838894, current_train_items 50688.
I0302 18:59:04.925796 22626471084160 run.py:483] Algo bellman_ford step 1584 current loss 1.068081, current_train_items 50720.
I0302 18:59:04.945459 22626471084160 run.py:483] Algo bellman_ford step 1585 current loss 0.338478, current_train_items 50752.
I0302 18:59:04.961108 22626471084160 run.py:483] Algo bellman_ford step 1586 current loss 0.602164, current_train_items 50784.
I0302 18:59:04.984084 22626471084160 run.py:483] Algo bellman_ford step 1587 current loss 0.732698, current_train_items 50816.
I0302 18:59:05.013466 22626471084160 run.py:483] Algo bellman_ford step 1588 current loss 0.934363, current_train_items 50848.
I0302 18:59:05.045897 22626471084160 run.py:483] Algo bellman_ford step 1589 current loss 0.972356, current_train_items 50880.
I0302 18:59:05.065438 22626471084160 run.py:483] Algo bellman_ford step 1590 current loss 0.328741, current_train_items 50912.
I0302 18:59:05.081646 22626471084160 run.py:483] Algo bellman_ford step 1591 current loss 0.526341, current_train_items 50944.
I0302 18:59:05.104147 22626471084160 run.py:483] Algo bellman_ford step 1592 current loss 0.703715, current_train_items 50976.
I0302 18:59:05.134594 22626471084160 run.py:483] Algo bellman_ford step 1593 current loss 0.918341, current_train_items 51008.
I0302 18:59:05.168388 22626471084160 run.py:483] Algo bellman_ford step 1594 current loss 1.174055, current_train_items 51040.
I0302 18:59:05.187691 22626471084160 run.py:483] Algo bellman_ford step 1595 current loss 0.405935, current_train_items 51072.
I0302 18:59:05.203331 22626471084160 run.py:483] Algo bellman_ford step 1596 current loss 0.628664, current_train_items 51104.
I0302 18:59:05.227082 22626471084160 run.py:483] Algo bellman_ford step 1597 current loss 0.788948, current_train_items 51136.
I0302 18:59:05.255511 22626471084160 run.py:483] Algo bellman_ford step 1598 current loss 0.783563, current_train_items 51168.
I0302 18:59:05.287958 22626471084160 run.py:483] Algo bellman_ford step 1599 current loss 0.945374, current_train_items 51200.
I0302 18:59:05.307202 22626471084160 run.py:483] Algo bellman_ford step 1600 current loss 0.350037, current_train_items 51232.
I0302 18:59:05.314929 22626471084160 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0302 18:59:05.315034 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.932, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 18:59:05.331618 22626471084160 run.py:483] Algo bellman_ford step 1601 current loss 0.538458, current_train_items 51264.
I0302 18:59:05.355784 22626471084160 run.py:483] Algo bellman_ford step 1602 current loss 0.842657, current_train_items 51296.
I0302 18:59:05.385478 22626471084160 run.py:483] Algo bellman_ford step 1603 current loss 0.868370, current_train_items 51328.
I0302 18:59:05.418565 22626471084160 run.py:483] Algo bellman_ford step 1604 current loss 1.005070, current_train_items 51360.
I0302 18:59:05.437925 22626471084160 run.py:483] Algo bellman_ford step 1605 current loss 0.315313, current_train_items 51392.
I0302 18:59:05.453408 22626471084160 run.py:483] Algo bellman_ford step 1606 current loss 0.543290, current_train_items 51424.
I0302 18:59:05.476343 22626471084160 run.py:483] Algo bellman_ford step 1607 current loss 0.832411, current_train_items 51456.
I0302 18:59:05.506366 22626471084160 run.py:483] Algo bellman_ford step 1608 current loss 1.015479, current_train_items 51488.
I0302 18:59:05.539707 22626471084160 run.py:483] Algo bellman_ford step 1609 current loss 1.088028, current_train_items 51520.
I0302 18:59:05.558620 22626471084160 run.py:483] Algo bellman_ford step 1610 current loss 0.387724, current_train_items 51552.
I0302 18:59:05.575002 22626471084160 run.py:483] Algo bellman_ford step 1611 current loss 0.663842, current_train_items 51584.
I0302 18:59:05.597814 22626471084160 run.py:483] Algo bellman_ford step 1612 current loss 0.705945, current_train_items 51616.
I0302 18:59:05.627590 22626471084160 run.py:483] Algo bellman_ford step 1613 current loss 0.986262, current_train_items 51648.
I0302 18:59:05.659209 22626471084160 run.py:483] Algo bellman_ford step 1614 current loss 0.999964, current_train_items 51680.
I0302 18:59:05.678328 22626471084160 run.py:483] Algo bellman_ford step 1615 current loss 0.352527, current_train_items 51712.
I0302 18:59:05.694696 22626471084160 run.py:483] Algo bellman_ford step 1616 current loss 0.580362, current_train_items 51744.
I0302 18:59:05.717784 22626471084160 run.py:483] Algo bellman_ford step 1617 current loss 0.981584, current_train_items 51776.
I0302 18:59:05.746466 22626471084160 run.py:483] Algo bellman_ford step 1618 current loss 0.847838, current_train_items 51808.
I0302 18:59:05.780114 22626471084160 run.py:483] Algo bellman_ford step 1619 current loss 1.162812, current_train_items 51840.
I0302 18:59:05.798861 22626471084160 run.py:483] Algo bellman_ford step 1620 current loss 0.396638, current_train_items 51872.
I0302 18:59:05.814830 22626471084160 run.py:483] Algo bellman_ford step 1621 current loss 0.577224, current_train_items 51904.
I0302 18:59:05.838095 22626471084160 run.py:483] Algo bellman_ford step 1622 current loss 0.820143, current_train_items 51936.
I0302 18:59:05.867907 22626471084160 run.py:483] Algo bellman_ford step 1623 current loss 0.920497, current_train_items 51968.
I0302 18:59:05.900251 22626471084160 run.py:483] Algo bellman_ford step 1624 current loss 1.056167, current_train_items 52000.
I0302 18:59:05.919122 22626471084160 run.py:483] Algo bellman_ford step 1625 current loss 0.349914, current_train_items 52032.
I0302 18:59:05.935440 22626471084160 run.py:483] Algo bellman_ford step 1626 current loss 0.570529, current_train_items 52064.
I0302 18:59:05.958363 22626471084160 run.py:483] Algo bellman_ford step 1627 current loss 0.789181, current_train_items 52096.
I0302 18:59:05.989089 22626471084160 run.py:483] Algo bellman_ford step 1628 current loss 0.926105, current_train_items 52128.
I0302 18:59:06.020763 22626471084160 run.py:483] Algo bellman_ford step 1629 current loss 0.962966, current_train_items 52160.
I0302 18:59:06.039740 22626471084160 run.py:483] Algo bellman_ford step 1630 current loss 0.365264, current_train_items 52192.
I0302 18:59:06.056421 22626471084160 run.py:483] Algo bellman_ford step 1631 current loss 0.607634, current_train_items 52224.
I0302 18:59:06.079931 22626471084160 run.py:483] Algo bellman_ford step 1632 current loss 0.833521, current_train_items 52256.
I0302 18:59:06.108656 22626471084160 run.py:483] Algo bellman_ford step 1633 current loss 0.804778, current_train_items 52288.
I0302 18:59:06.141310 22626471084160 run.py:483] Algo bellman_ford step 1634 current loss 0.973979, current_train_items 52320.
I0302 18:59:06.160101 22626471084160 run.py:483] Algo bellman_ford step 1635 current loss 0.329081, current_train_items 52352.
I0302 18:59:06.176334 22626471084160 run.py:483] Algo bellman_ford step 1636 current loss 0.550198, current_train_items 52384.
I0302 18:59:06.199693 22626471084160 run.py:483] Algo bellman_ford step 1637 current loss 0.740963, current_train_items 52416.
I0302 18:59:06.229810 22626471084160 run.py:483] Algo bellman_ford step 1638 current loss 0.903716, current_train_items 52448.
I0302 18:59:06.264893 22626471084160 run.py:483] Algo bellman_ford step 1639 current loss 1.161006, current_train_items 52480.
I0302 18:59:06.283362 22626471084160 run.py:483] Algo bellman_ford step 1640 current loss 0.295439, current_train_items 52512.
I0302 18:59:06.299127 22626471084160 run.py:483] Algo bellman_ford step 1641 current loss 0.517364, current_train_items 52544.
I0302 18:59:06.322339 22626471084160 run.py:483] Algo bellman_ford step 1642 current loss 0.740804, current_train_items 52576.
I0302 18:59:06.351553 22626471084160 run.py:483] Algo bellman_ford step 1643 current loss 0.873094, current_train_items 52608.
I0302 18:59:06.382936 22626471084160 run.py:483] Algo bellman_ford step 1644 current loss 0.958355, current_train_items 52640.
I0302 18:59:06.401656 22626471084160 run.py:483] Algo bellman_ford step 1645 current loss 0.404057, current_train_items 52672.
I0302 18:59:06.417849 22626471084160 run.py:483] Algo bellman_ford step 1646 current loss 0.569956, current_train_items 52704.
I0302 18:59:06.441901 22626471084160 run.py:483] Algo bellman_ford step 1647 current loss 0.930757, current_train_items 52736.
I0302 18:59:06.469879 22626471084160 run.py:483] Algo bellman_ford step 1648 current loss 0.821980, current_train_items 52768.
I0302 18:59:06.503063 22626471084160 run.py:483] Algo bellman_ford step 1649 current loss 1.047722, current_train_items 52800.
I0302 18:59:06.522141 22626471084160 run.py:483] Algo bellman_ford step 1650 current loss 0.486334, current_train_items 52832.
I0302 18:59:06.529941 22626471084160 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0302 18:59:06.530048 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.932, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 18:59:06.547316 22626471084160 run.py:483] Algo bellman_ford step 1651 current loss 0.606231, current_train_items 52864.
I0302 18:59:06.571525 22626471084160 run.py:483] Algo bellman_ford step 1652 current loss 0.963328, current_train_items 52896.
I0302 18:59:06.601183 22626471084160 run.py:483] Algo bellman_ford step 1653 current loss 0.834375, current_train_items 52928.
I0302 18:59:06.632437 22626471084160 run.py:483] Algo bellman_ford step 1654 current loss 1.173541, current_train_items 52960.
I0302 18:59:06.651700 22626471084160 run.py:483] Algo bellman_ford step 1655 current loss 0.398047, current_train_items 52992.
I0302 18:59:06.667618 22626471084160 run.py:483] Algo bellman_ford step 1656 current loss 0.653539, current_train_items 53024.
I0302 18:59:06.690462 22626471084160 run.py:483] Algo bellman_ford step 1657 current loss 0.780568, current_train_items 53056.
I0302 18:59:06.718525 22626471084160 run.py:483] Algo bellman_ford step 1658 current loss 0.851676, current_train_items 53088.
I0302 18:59:06.749290 22626471084160 run.py:483] Algo bellman_ford step 1659 current loss 1.032006, current_train_items 53120.
I0302 18:59:06.768419 22626471084160 run.py:483] Algo bellman_ford step 1660 current loss 0.393861, current_train_items 53152.
I0302 18:59:06.784606 22626471084160 run.py:483] Algo bellman_ford step 1661 current loss 0.523259, current_train_items 53184.
I0302 18:59:06.806738 22626471084160 run.py:483] Algo bellman_ford step 1662 current loss 0.829301, current_train_items 53216.
I0302 18:59:06.837383 22626471084160 run.py:483] Algo bellman_ford step 1663 current loss 0.942285, current_train_items 53248.
I0302 18:59:06.870534 22626471084160 run.py:483] Algo bellman_ford step 1664 current loss 1.154661, current_train_items 53280.
I0302 18:59:06.889394 22626471084160 run.py:483] Algo bellman_ford step 1665 current loss 0.309335, current_train_items 53312.
I0302 18:59:06.905589 22626471084160 run.py:483] Algo bellman_ford step 1666 current loss 0.605164, current_train_items 53344.
I0302 18:59:06.928114 22626471084160 run.py:483] Algo bellman_ford step 1667 current loss 0.855795, current_train_items 53376.
I0302 18:59:06.956707 22626471084160 run.py:483] Algo bellman_ford step 1668 current loss 1.031710, current_train_items 53408.
I0302 18:59:06.987470 22626471084160 run.py:483] Algo bellman_ford step 1669 current loss 1.191790, current_train_items 53440.
I0302 18:59:07.006516 22626471084160 run.py:483] Algo bellman_ford step 1670 current loss 0.378199, current_train_items 53472.
I0302 18:59:07.022462 22626471084160 run.py:483] Algo bellman_ford step 1671 current loss 0.580784, current_train_items 53504.
I0302 18:59:07.044869 22626471084160 run.py:483] Algo bellman_ford step 1672 current loss 0.775549, current_train_items 53536.
I0302 18:59:07.075458 22626471084160 run.py:483] Algo bellman_ford step 1673 current loss 0.846574, current_train_items 53568.
I0302 18:59:07.108091 22626471084160 run.py:483] Algo bellman_ford step 1674 current loss 0.949672, current_train_items 53600.
I0302 18:59:07.127044 22626471084160 run.py:483] Algo bellman_ford step 1675 current loss 0.266444, current_train_items 53632.
I0302 18:59:07.143342 22626471084160 run.py:483] Algo bellman_ford step 1676 current loss 0.567980, current_train_items 53664.
I0302 18:59:07.165738 22626471084160 run.py:483] Algo bellman_ford step 1677 current loss 0.610116, current_train_items 53696.
I0302 18:59:07.194805 22626471084160 run.py:483] Algo bellman_ford step 1678 current loss 1.028548, current_train_items 53728.
I0302 18:59:07.228117 22626471084160 run.py:483] Algo bellman_ford step 1679 current loss 1.162683, current_train_items 53760.
I0302 18:59:07.247457 22626471084160 run.py:483] Algo bellman_ford step 1680 current loss 0.328917, current_train_items 53792.
I0302 18:59:07.263381 22626471084160 run.py:483] Algo bellman_ford step 1681 current loss 0.537968, current_train_items 53824.
I0302 18:59:07.285640 22626471084160 run.py:483] Algo bellman_ford step 1682 current loss 0.872463, current_train_items 53856.
I0302 18:59:07.313636 22626471084160 run.py:483] Algo bellman_ford step 1683 current loss 0.819213, current_train_items 53888.
I0302 18:59:07.346146 22626471084160 run.py:483] Algo bellman_ford step 1684 current loss 0.933890, current_train_items 53920.
I0302 18:59:07.365463 22626471084160 run.py:483] Algo bellman_ford step 1685 current loss 0.334396, current_train_items 53952.
I0302 18:59:07.381604 22626471084160 run.py:483] Algo bellman_ford step 1686 current loss 0.561011, current_train_items 53984.
I0302 18:59:07.404382 22626471084160 run.py:483] Algo bellman_ford step 1687 current loss 0.731056, current_train_items 54016.
I0302 18:59:07.433429 22626471084160 run.py:483] Algo bellman_ford step 1688 current loss 0.891855, current_train_items 54048.
I0302 18:59:07.467829 22626471084160 run.py:483] Algo bellman_ford step 1689 current loss 1.019108, current_train_items 54080.
I0302 18:59:07.486804 22626471084160 run.py:483] Algo bellman_ford step 1690 current loss 0.309497, current_train_items 54112.
I0302 18:59:07.503399 22626471084160 run.py:483] Algo bellman_ford step 1691 current loss 0.593430, current_train_items 54144.
I0302 18:59:07.526454 22626471084160 run.py:483] Algo bellman_ford step 1692 current loss 0.893481, current_train_items 54176.
I0302 18:59:07.556907 22626471084160 run.py:483] Algo bellman_ford step 1693 current loss 0.916969, current_train_items 54208.
I0302 18:59:07.589575 22626471084160 run.py:483] Algo bellman_ford step 1694 current loss 1.087219, current_train_items 54240.
I0302 18:59:07.608477 22626471084160 run.py:483] Algo bellman_ford step 1695 current loss 0.386663, current_train_items 54272.
I0302 18:59:07.624970 22626471084160 run.py:483] Algo bellman_ford step 1696 current loss 0.521057, current_train_items 54304.
I0302 18:59:07.646446 22626471084160 run.py:483] Algo bellman_ford step 1697 current loss 0.819099, current_train_items 54336.
I0302 18:59:07.674520 22626471084160 run.py:483] Algo bellman_ford step 1698 current loss 0.891222, current_train_items 54368.
I0302 18:59:07.707605 22626471084160 run.py:483] Algo bellman_ford step 1699 current loss 1.132975, current_train_items 54400.
I0302 18:59:07.726981 22626471084160 run.py:483] Algo bellman_ford step 1700 current loss 0.463104, current_train_items 54432.
I0302 18:59:07.734848 22626471084160 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.8798828125, 'score': 0.8798828125, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0302 18:59:07.734954 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.932, current avg val score is 0.880, val scores are: bellman_ford: 0.880
I0302 18:59:07.751634 22626471084160 run.py:483] Algo bellman_ford step 1701 current loss 0.600680, current_train_items 54464.
I0302 18:59:07.774542 22626471084160 run.py:483] Algo bellman_ford step 1702 current loss 0.718245, current_train_items 54496.
I0302 18:59:07.804195 22626471084160 run.py:483] Algo bellman_ford step 1703 current loss 0.863327, current_train_items 54528.
I0302 18:59:07.837013 22626471084160 run.py:483] Algo bellman_ford step 1704 current loss 1.054059, current_train_items 54560.
I0302 18:59:07.856825 22626471084160 run.py:483] Algo bellman_ford step 1705 current loss 0.358153, current_train_items 54592.
I0302 18:59:07.872493 22626471084160 run.py:483] Algo bellman_ford step 1706 current loss 0.512619, current_train_items 54624.
I0302 18:59:07.895718 22626471084160 run.py:483] Algo bellman_ford step 1707 current loss 0.730987, current_train_items 54656.
I0302 18:59:07.925428 22626471084160 run.py:483] Algo bellman_ford step 1708 current loss 0.889407, current_train_items 54688.
I0302 18:59:07.959217 22626471084160 run.py:483] Algo bellman_ford step 1709 current loss 1.007951, current_train_items 54720.
I0302 18:59:07.978087 22626471084160 run.py:483] Algo bellman_ford step 1710 current loss 0.371511, current_train_items 54752.
I0302 18:59:07.994423 22626471084160 run.py:483] Algo bellman_ford step 1711 current loss 0.587881, current_train_items 54784.
I0302 18:59:08.017693 22626471084160 run.py:483] Algo bellman_ford step 1712 current loss 0.872368, current_train_items 54816.
I0302 18:59:08.046571 22626471084160 run.py:483] Algo bellman_ford step 1713 current loss 0.878717, current_train_items 54848.
I0302 18:59:08.078623 22626471084160 run.py:483] Algo bellman_ford step 1714 current loss 0.906899, current_train_items 54880.
I0302 18:59:08.097612 22626471084160 run.py:483] Algo bellman_ford step 1715 current loss 0.357488, current_train_items 54912.
I0302 18:59:08.113381 22626471084160 run.py:483] Algo bellman_ford step 1716 current loss 0.554063, current_train_items 54944.
I0302 18:59:08.136978 22626471084160 run.py:483] Algo bellman_ford step 1717 current loss 0.838915, current_train_items 54976.
I0302 18:59:08.165170 22626471084160 run.py:483] Algo bellman_ford step 1718 current loss 0.769710, current_train_items 55008.
I0302 18:59:08.197425 22626471084160 run.py:483] Algo bellman_ford step 1719 current loss 1.001800, current_train_items 55040.
I0302 18:59:08.216474 22626471084160 run.py:483] Algo bellman_ford step 1720 current loss 0.376630, current_train_items 55072.
I0302 18:59:08.232403 22626471084160 run.py:483] Algo bellman_ford step 1721 current loss 0.706080, current_train_items 55104.
I0302 18:59:08.255407 22626471084160 run.py:483] Algo bellman_ford step 1722 current loss 0.736776, current_train_items 55136.
I0302 18:59:08.284677 22626471084160 run.py:483] Algo bellman_ford step 1723 current loss 0.891394, current_train_items 55168.
I0302 18:59:08.316842 22626471084160 run.py:483] Algo bellman_ford step 1724 current loss 0.905385, current_train_items 55200.
I0302 18:59:08.335561 22626471084160 run.py:483] Algo bellman_ford step 1725 current loss 0.345115, current_train_items 55232.
I0302 18:59:08.351877 22626471084160 run.py:483] Algo bellman_ford step 1726 current loss 0.819916, current_train_items 55264.
I0302 18:59:08.376490 22626471084160 run.py:483] Algo bellman_ford step 1727 current loss 0.939845, current_train_items 55296.
I0302 18:59:08.405948 22626471084160 run.py:483] Algo bellman_ford step 1728 current loss 0.959864, current_train_items 55328.
I0302 18:59:08.438515 22626471084160 run.py:483] Algo bellman_ford step 1729 current loss 1.125724, current_train_items 55360.
I0302 18:59:08.457681 22626471084160 run.py:483] Algo bellman_ford step 1730 current loss 0.301719, current_train_items 55392.
I0302 18:59:08.473846 22626471084160 run.py:483] Algo bellman_ford step 1731 current loss 0.587366, current_train_items 55424.
I0302 18:59:08.497085 22626471084160 run.py:483] Algo bellman_ford step 1732 current loss 0.822023, current_train_items 55456.
I0302 18:59:08.526720 22626471084160 run.py:483] Algo bellman_ford step 1733 current loss 0.908709, current_train_items 55488.
I0302 18:59:08.556850 22626471084160 run.py:483] Algo bellman_ford step 1734 current loss 1.269623, current_train_items 55520.
I0302 18:59:08.576088 22626471084160 run.py:483] Algo bellman_ford step 1735 current loss 0.370479, current_train_items 55552.
I0302 18:59:08.592479 22626471084160 run.py:483] Algo bellman_ford step 1736 current loss 0.567662, current_train_items 55584.
I0302 18:59:08.615798 22626471084160 run.py:483] Algo bellman_ford step 1737 current loss 0.803380, current_train_items 55616.
I0302 18:59:08.645187 22626471084160 run.py:483] Algo bellman_ford step 1738 current loss 0.937136, current_train_items 55648.
I0302 18:59:08.678364 22626471084160 run.py:483] Algo bellman_ford step 1739 current loss 1.012790, current_train_items 55680.
I0302 18:59:08.697238 22626471084160 run.py:483] Algo bellman_ford step 1740 current loss 0.340050, current_train_items 55712.
I0302 18:59:08.713477 22626471084160 run.py:483] Algo bellman_ford step 1741 current loss 0.602495, current_train_items 55744.
I0302 18:59:08.736646 22626471084160 run.py:483] Algo bellman_ford step 1742 current loss 0.680258, current_train_items 55776.
I0302 18:59:08.766589 22626471084160 run.py:483] Algo bellman_ford step 1743 current loss 0.775874, current_train_items 55808.
I0302 18:59:08.797973 22626471084160 run.py:483] Algo bellman_ford step 1744 current loss 0.949962, current_train_items 55840.
I0302 18:59:08.817020 22626471084160 run.py:483] Algo bellman_ford step 1745 current loss 0.305201, current_train_items 55872.
I0302 18:59:08.833407 22626471084160 run.py:483] Algo bellman_ford step 1746 current loss 0.662569, current_train_items 55904.
I0302 18:59:08.856536 22626471084160 run.py:483] Algo bellman_ford step 1747 current loss 0.793703, current_train_items 55936.
I0302 18:59:08.886125 22626471084160 run.py:483] Algo bellman_ford step 1748 current loss 0.800268, current_train_items 55968.
I0302 18:59:08.916963 22626471084160 run.py:483] Algo bellman_ford step 1749 current loss 0.911655, current_train_items 56000.
I0302 18:59:08.935974 22626471084160 run.py:483] Algo bellman_ford step 1750 current loss 0.287529, current_train_items 56032.
I0302 18:59:08.944010 22626471084160 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0302 18:59:08.944114 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.932, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 18:59:08.975997 22626471084160 run.py:483] Algo bellman_ford step 1751 current loss 0.525626, current_train_items 56064.
I0302 18:59:08.999465 22626471084160 run.py:483] Algo bellman_ford step 1752 current loss 0.721050, current_train_items 56096.
I0302 18:59:09.031207 22626471084160 run.py:483] Algo bellman_ford step 1753 current loss 0.864872, current_train_items 56128.
I0302 18:59:09.065831 22626471084160 run.py:483] Algo bellman_ford step 1754 current loss 0.964696, current_train_items 56160.
I0302 18:59:09.085752 22626471084160 run.py:483] Algo bellman_ford step 1755 current loss 0.318778, current_train_items 56192.
I0302 18:59:09.102053 22626471084160 run.py:483] Algo bellman_ford step 1756 current loss 0.584885, current_train_items 56224.
I0302 18:59:09.126127 22626471084160 run.py:483] Algo bellman_ford step 1757 current loss 0.885587, current_train_items 56256.
I0302 18:59:09.156526 22626471084160 run.py:483] Algo bellman_ford step 1758 current loss 1.110902, current_train_items 56288.
I0302 18:59:09.187043 22626471084160 run.py:483] Algo bellman_ford step 1759 current loss 1.014933, current_train_items 56320.
I0302 18:59:09.206665 22626471084160 run.py:483] Algo bellman_ford step 1760 current loss 0.358506, current_train_items 56352.
I0302 18:59:09.222929 22626471084160 run.py:483] Algo bellman_ford step 1761 current loss 0.545850, current_train_items 56384.
I0302 18:59:09.246748 22626471084160 run.py:483] Algo bellman_ford step 1762 current loss 0.834635, current_train_items 56416.
I0302 18:59:09.278085 22626471084160 run.py:483] Algo bellman_ford step 1763 current loss 0.824500, current_train_items 56448.
I0302 18:59:09.312442 22626471084160 run.py:483] Algo bellman_ford step 1764 current loss 1.185914, current_train_items 56480.
I0302 18:59:09.331686 22626471084160 run.py:483] Algo bellman_ford step 1765 current loss 0.351161, current_train_items 56512.
I0302 18:59:09.347629 22626471084160 run.py:483] Algo bellman_ford step 1766 current loss 0.571300, current_train_items 56544.
I0302 18:59:09.370517 22626471084160 run.py:483] Algo bellman_ford step 1767 current loss 0.708907, current_train_items 56576.
I0302 18:59:09.400094 22626471084160 run.py:483] Algo bellman_ford step 1768 current loss 0.946255, current_train_items 56608.
I0302 18:59:09.431539 22626471084160 run.py:483] Algo bellman_ford step 1769 current loss 1.070924, current_train_items 56640.
I0302 18:59:09.451272 22626471084160 run.py:483] Algo bellman_ford step 1770 current loss 0.354966, current_train_items 56672.
I0302 18:59:09.467520 22626471084160 run.py:483] Algo bellman_ford step 1771 current loss 0.615450, current_train_items 56704.
I0302 18:59:09.491173 22626471084160 run.py:483] Algo bellman_ford step 1772 current loss 0.958448, current_train_items 56736.
I0302 18:59:09.521281 22626471084160 run.py:483] Algo bellman_ford step 1773 current loss 0.889330, current_train_items 56768.
I0302 18:59:09.552757 22626471084160 run.py:483] Algo bellman_ford step 1774 current loss 0.992812, current_train_items 56800.
I0302 18:59:09.572062 22626471084160 run.py:483] Algo bellman_ford step 1775 current loss 0.351291, current_train_items 56832.
I0302 18:59:09.588382 22626471084160 run.py:483] Algo bellman_ford step 1776 current loss 0.628234, current_train_items 56864.
I0302 18:59:09.611768 22626471084160 run.py:483] Algo bellman_ford step 1777 current loss 0.831989, current_train_items 56896.
I0302 18:59:09.641479 22626471084160 run.py:483] Algo bellman_ford step 1778 current loss 0.912712, current_train_items 56928.
I0302 18:59:09.671993 22626471084160 run.py:483] Algo bellman_ford step 1779 current loss 0.928155, current_train_items 56960.
I0302 18:59:09.691075 22626471084160 run.py:483] Algo bellman_ford step 1780 current loss 0.338415, current_train_items 56992.
I0302 18:59:09.707167 22626471084160 run.py:483] Algo bellman_ford step 1781 current loss 0.591386, current_train_items 57024.
I0302 18:59:09.730275 22626471084160 run.py:483] Algo bellman_ford step 1782 current loss 0.681707, current_train_items 57056.
I0302 18:59:09.760132 22626471084160 run.py:483] Algo bellman_ford step 1783 current loss 1.017867, current_train_items 57088.
I0302 18:59:09.791618 22626471084160 run.py:483] Algo bellman_ford step 1784 current loss 1.087867, current_train_items 57120.
I0302 18:59:09.810857 22626471084160 run.py:483] Algo bellman_ford step 1785 current loss 0.412107, current_train_items 57152.
I0302 18:59:09.827220 22626471084160 run.py:483] Algo bellman_ford step 1786 current loss 0.750682, current_train_items 57184.
I0302 18:59:09.849502 22626471084160 run.py:483] Algo bellman_ford step 1787 current loss 0.870866, current_train_items 57216.
I0302 18:59:09.878130 22626471084160 run.py:483] Algo bellman_ford step 1788 current loss 0.905088, current_train_items 57248.
I0302 18:59:09.910546 22626471084160 run.py:483] Algo bellman_ford step 1789 current loss 1.012326, current_train_items 57280.
I0302 18:59:09.929870 22626471084160 run.py:483] Algo bellman_ford step 1790 current loss 0.292596, current_train_items 57312.
I0302 18:59:09.946644 22626471084160 run.py:483] Algo bellman_ford step 1791 current loss 0.702439, current_train_items 57344.
I0302 18:59:09.969771 22626471084160 run.py:483] Algo bellman_ford step 1792 current loss 0.725896, current_train_items 57376.
I0302 18:59:09.998216 22626471084160 run.py:483] Algo bellman_ford step 1793 current loss 0.867151, current_train_items 57408.
I0302 18:59:10.030248 22626471084160 run.py:483] Algo bellman_ford step 1794 current loss 1.070654, current_train_items 57440.
I0302 18:59:10.049104 22626471084160 run.py:483] Algo bellman_ford step 1795 current loss 0.379393, current_train_items 57472.
I0302 18:59:10.065323 22626471084160 run.py:483] Algo bellman_ford step 1796 current loss 0.628370, current_train_items 57504.
I0302 18:59:10.089042 22626471084160 run.py:483] Algo bellman_ford step 1797 current loss 0.829973, current_train_items 57536.
I0302 18:59:10.119066 22626471084160 run.py:483] Algo bellman_ford step 1798 current loss 0.984271, current_train_items 57568.
I0302 18:59:10.152392 22626471084160 run.py:483] Algo bellman_ford step 1799 current loss 1.062864, current_train_items 57600.
I0302 18:59:10.171662 22626471084160 run.py:483] Algo bellman_ford step 1800 current loss 0.372405, current_train_items 57632.
I0302 18:59:10.179452 22626471084160 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0302 18:59:10.179559 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 18:59:10.196068 22626471084160 run.py:483] Algo bellman_ford step 1801 current loss 0.459142, current_train_items 57664.
I0302 18:59:10.220141 22626471084160 run.py:483] Algo bellman_ford step 1802 current loss 0.694444, current_train_items 57696.
I0302 18:59:10.249164 22626471084160 run.py:483] Algo bellman_ford step 1803 current loss 0.747373, current_train_items 57728.
I0302 18:59:10.280822 22626471084160 run.py:483] Algo bellman_ford step 1804 current loss 1.135867, current_train_items 57760.
I0302 18:59:10.300347 22626471084160 run.py:483] Algo bellman_ford step 1805 current loss 0.375258, current_train_items 57792.
I0302 18:59:10.316038 22626471084160 run.py:483] Algo bellman_ford step 1806 current loss 0.533292, current_train_items 57824.
I0302 18:59:10.338598 22626471084160 run.py:483] Algo bellman_ford step 1807 current loss 0.732655, current_train_items 57856.
I0302 18:59:10.366839 22626471084160 run.py:483] Algo bellman_ford step 1808 current loss 0.827993, current_train_items 57888.
I0302 18:59:10.400085 22626471084160 run.py:483] Algo bellman_ford step 1809 current loss 1.136220, current_train_items 57920.
I0302 18:59:10.419180 22626471084160 run.py:483] Algo bellman_ford step 1810 current loss 0.415810, current_train_items 57952.
I0302 18:59:10.435251 22626471084160 run.py:483] Algo bellman_ford step 1811 current loss 0.590146, current_train_items 57984.
I0302 18:59:10.458401 22626471084160 run.py:483] Algo bellman_ford step 1812 current loss 0.789624, current_train_items 58016.
I0302 18:59:10.485928 22626471084160 run.py:483] Algo bellman_ford step 1813 current loss 0.783965, current_train_items 58048.
I0302 18:59:10.517872 22626471084160 run.py:483] Algo bellman_ford step 1814 current loss 1.039609, current_train_items 58080.
I0302 18:59:10.536784 22626471084160 run.py:483] Algo bellman_ford step 1815 current loss 0.354705, current_train_items 58112.
I0302 18:59:10.553384 22626471084160 run.py:483] Algo bellman_ford step 1816 current loss 0.736277, current_train_items 58144.
I0302 18:59:10.576055 22626471084160 run.py:483] Algo bellman_ford step 1817 current loss 0.709082, current_train_items 58176.
I0302 18:59:10.605359 22626471084160 run.py:483] Algo bellman_ford step 1818 current loss 0.973759, current_train_items 58208.
I0302 18:59:10.637234 22626471084160 run.py:483] Algo bellman_ford step 1819 current loss 1.073976, current_train_items 58240.
I0302 18:59:10.656075 22626471084160 run.py:483] Algo bellman_ford step 1820 current loss 0.427842, current_train_items 58272.
I0302 18:59:10.672678 22626471084160 run.py:483] Algo bellman_ford step 1821 current loss 0.594392, current_train_items 58304.
I0302 18:59:10.697623 22626471084160 run.py:483] Algo bellman_ford step 1822 current loss 0.781481, current_train_items 58336.
I0302 18:59:10.727217 22626471084160 run.py:483] Algo bellman_ford step 1823 current loss 0.877076, current_train_items 58368.
I0302 18:59:10.758523 22626471084160 run.py:483] Algo bellman_ford step 1824 current loss 0.934081, current_train_items 58400.
I0302 18:59:10.777222 22626471084160 run.py:483] Algo bellman_ford step 1825 current loss 0.361200, current_train_items 58432.
I0302 18:59:10.793534 22626471084160 run.py:483] Algo bellman_ford step 1826 current loss 0.676265, current_train_items 58464.
I0302 18:59:10.816364 22626471084160 run.py:483] Algo bellman_ford step 1827 current loss 0.742839, current_train_items 58496.
I0302 18:59:10.847023 22626471084160 run.py:483] Algo bellman_ford step 1828 current loss 1.034894, current_train_items 58528.
I0302 18:59:10.881780 22626471084160 run.py:483] Algo bellman_ford step 1829 current loss 1.089305, current_train_items 58560.
I0302 18:59:10.900616 22626471084160 run.py:483] Algo bellman_ford step 1830 current loss 0.353369, current_train_items 58592.
I0302 18:59:10.916963 22626471084160 run.py:483] Algo bellman_ford step 1831 current loss 0.631303, current_train_items 58624.
I0302 18:59:10.941796 22626471084160 run.py:483] Algo bellman_ford step 1832 current loss 0.815098, current_train_items 58656.
I0302 18:59:10.971293 22626471084160 run.py:483] Algo bellman_ford step 1833 current loss 0.976896, current_train_items 58688.
I0302 18:59:11.002974 22626471084160 run.py:483] Algo bellman_ford step 1834 current loss 0.886948, current_train_items 58720.
I0302 18:59:11.021726 22626471084160 run.py:483] Algo bellman_ford step 1835 current loss 0.287399, current_train_items 58752.
I0302 18:59:11.038145 22626471084160 run.py:483] Algo bellman_ford step 1836 current loss 0.542295, current_train_items 58784.
I0302 18:59:11.062365 22626471084160 run.py:483] Algo bellman_ford step 1837 current loss 0.924793, current_train_items 58816.
I0302 18:59:11.091122 22626471084160 run.py:483] Algo bellman_ford step 1838 current loss 0.857027, current_train_items 58848.
I0302 18:59:11.123886 22626471084160 run.py:483] Algo bellman_ford step 1839 current loss 0.936978, current_train_items 58880.
I0302 18:59:11.142822 22626471084160 run.py:483] Algo bellman_ford step 1840 current loss 0.319417, current_train_items 58912.
I0302 18:59:11.159183 22626471084160 run.py:483] Algo bellman_ford step 1841 current loss 0.552423, current_train_items 58944.
I0302 18:59:11.182983 22626471084160 run.py:483] Algo bellman_ford step 1842 current loss 0.707785, current_train_items 58976.
I0302 18:59:11.213087 22626471084160 run.py:483] Algo bellman_ford step 1843 current loss 0.787367, current_train_items 59008.
I0302 18:59:11.245534 22626471084160 run.py:483] Algo bellman_ford step 1844 current loss 1.081366, current_train_items 59040.
I0302 18:59:11.264353 22626471084160 run.py:483] Algo bellman_ford step 1845 current loss 0.367629, current_train_items 59072.
I0302 18:59:11.280410 22626471084160 run.py:483] Algo bellman_ford step 1846 current loss 0.531225, current_train_items 59104.
I0302 18:59:11.303942 22626471084160 run.py:483] Algo bellman_ford step 1847 current loss 0.767472, current_train_items 59136.
I0302 18:59:11.333801 22626471084160 run.py:483] Algo bellman_ford step 1848 current loss 0.795866, current_train_items 59168.
I0302 18:59:11.367608 22626471084160 run.py:483] Algo bellman_ford step 1849 current loss 1.068561, current_train_items 59200.
I0302 18:59:11.386572 22626471084160 run.py:483] Algo bellman_ford step 1850 current loss 0.373612, current_train_items 59232.
I0302 18:59:11.394711 22626471084160 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0302 18:59:11.394815 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 18:59:11.411556 22626471084160 run.py:483] Algo bellman_ford step 1851 current loss 0.555507, current_train_items 59264.
I0302 18:59:11.435326 22626471084160 run.py:483] Algo bellman_ford step 1852 current loss 0.880648, current_train_items 59296.
I0302 18:59:11.465339 22626471084160 run.py:483] Algo bellman_ford step 1853 current loss 1.008378, current_train_items 59328.
I0302 18:59:11.498433 22626471084160 run.py:483] Algo bellman_ford step 1854 current loss 1.074325, current_train_items 59360.
I0302 18:59:11.517850 22626471084160 run.py:483] Algo bellman_ford step 1855 current loss 0.302261, current_train_items 59392.
I0302 18:59:11.533286 22626471084160 run.py:483] Algo bellman_ford step 1856 current loss 0.554931, current_train_items 59424.
I0302 18:59:11.556780 22626471084160 run.py:483] Algo bellman_ford step 1857 current loss 0.820863, current_train_items 59456.
I0302 18:59:11.585397 22626471084160 run.py:483] Algo bellman_ford step 1858 current loss 0.847791, current_train_items 59488.
I0302 18:59:11.617671 22626471084160 run.py:483] Algo bellman_ford step 1859 current loss 1.107971, current_train_items 59520.
I0302 18:59:11.637138 22626471084160 run.py:483] Algo bellman_ford step 1860 current loss 0.320396, current_train_items 59552.
I0302 18:59:11.653716 22626471084160 run.py:483] Algo bellman_ford step 1861 current loss 0.558754, current_train_items 59584.
I0302 18:59:11.676381 22626471084160 run.py:483] Algo bellman_ford step 1862 current loss 0.839520, current_train_items 59616.
I0302 18:59:11.705613 22626471084160 run.py:483] Algo bellman_ford step 1863 current loss 0.966462, current_train_items 59648.
I0302 18:59:11.738602 22626471084160 run.py:483] Algo bellman_ford step 1864 current loss 1.194787, current_train_items 59680.
I0302 18:59:11.757943 22626471084160 run.py:483] Algo bellman_ford step 1865 current loss 0.355653, current_train_items 59712.
I0302 18:59:11.774146 22626471084160 run.py:483] Algo bellman_ford step 1866 current loss 0.615933, current_train_items 59744.
I0302 18:59:11.798444 22626471084160 run.py:483] Algo bellman_ford step 1867 current loss 0.971548, current_train_items 59776.
I0302 18:59:11.828205 22626471084160 run.py:483] Algo bellman_ford step 1868 current loss 0.945901, current_train_items 59808.
I0302 18:59:11.860518 22626471084160 run.py:483] Algo bellman_ford step 1869 current loss 0.883320, current_train_items 59840.
I0302 18:59:11.879698 22626471084160 run.py:483] Algo bellman_ford step 1870 current loss 0.357356, current_train_items 59872.
I0302 18:59:11.895757 22626471084160 run.py:483] Algo bellman_ford step 1871 current loss 0.509476, current_train_items 59904.
I0302 18:59:11.918469 22626471084160 run.py:483] Algo bellman_ford step 1872 current loss 0.736222, current_train_items 59936.
I0302 18:59:11.948256 22626471084160 run.py:483] Algo bellman_ford step 1873 current loss 0.860691, current_train_items 59968.
I0302 18:59:11.980209 22626471084160 run.py:483] Algo bellman_ford step 1874 current loss 0.984087, current_train_items 60000.
I0302 18:59:11.999654 22626471084160 run.py:483] Algo bellman_ford step 1875 current loss 0.416716, current_train_items 60032.
I0302 18:59:12.015679 22626471084160 run.py:483] Algo bellman_ford step 1876 current loss 0.532122, current_train_items 60064.
I0302 18:59:12.039042 22626471084160 run.py:483] Algo bellman_ford step 1877 current loss 0.887598, current_train_items 60096.
I0302 18:59:12.067380 22626471084160 run.py:483] Algo bellman_ford step 1878 current loss 0.778072, current_train_items 60128.
I0302 18:59:12.102288 22626471084160 run.py:483] Algo bellman_ford step 1879 current loss 1.048805, current_train_items 60160.
I0302 18:59:12.121498 22626471084160 run.py:483] Algo bellman_ford step 1880 current loss 0.327320, current_train_items 60192.
I0302 18:59:12.137992 22626471084160 run.py:483] Algo bellman_ford step 1881 current loss 0.635729, current_train_items 60224.
I0302 18:59:12.160771 22626471084160 run.py:483] Algo bellman_ford step 1882 current loss 0.755241, current_train_items 60256.
I0302 18:59:12.190441 22626471084160 run.py:483] Algo bellman_ford step 1883 current loss 0.960005, current_train_items 60288.
I0302 18:59:12.222914 22626471084160 run.py:483] Algo bellman_ford step 1884 current loss 1.084305, current_train_items 60320.
I0302 18:59:12.242177 22626471084160 run.py:483] Algo bellman_ford step 1885 current loss 0.267942, current_train_items 60352.
I0302 18:59:12.258603 22626471084160 run.py:483] Algo bellman_ford step 1886 current loss 0.647117, current_train_items 60384.
I0302 18:59:12.281698 22626471084160 run.py:483] Algo bellman_ford step 1887 current loss 0.850847, current_train_items 60416.
I0302 18:59:12.309803 22626471084160 run.py:483] Algo bellman_ford step 1888 current loss 0.790629, current_train_items 60448.
I0302 18:59:12.344007 22626471084160 run.py:483] Algo bellman_ford step 1889 current loss 1.081213, current_train_items 60480.
I0302 18:59:12.363253 22626471084160 run.py:483] Algo bellman_ford step 1890 current loss 0.323467, current_train_items 60512.
I0302 18:59:12.379515 22626471084160 run.py:483] Algo bellman_ford step 1891 current loss 0.574178, current_train_items 60544.
I0302 18:59:12.402190 22626471084160 run.py:483] Algo bellman_ford step 1892 current loss 0.701881, current_train_items 60576.
I0302 18:59:12.432533 22626471084160 run.py:483] Algo bellman_ford step 1893 current loss 0.955867, current_train_items 60608.
I0302 18:59:12.467889 22626471084160 run.py:483] Algo bellman_ford step 1894 current loss 1.178883, current_train_items 60640.
I0302 18:59:12.486645 22626471084160 run.py:483] Algo bellman_ford step 1895 current loss 0.406322, current_train_items 60672.
I0302 18:59:12.502781 22626471084160 run.py:483] Algo bellman_ford step 1896 current loss 0.612587, current_train_items 60704.
I0302 18:59:12.526114 22626471084160 run.py:483] Algo bellman_ford step 1897 current loss 0.823025, current_train_items 60736.
I0302 18:59:12.555778 22626471084160 run.py:483] Algo bellman_ford step 1898 current loss 0.857030, current_train_items 60768.
I0302 18:59:12.588725 22626471084160 run.py:483] Algo bellman_ford step 1899 current loss 1.081661, current_train_items 60800.
I0302 18:59:12.608196 22626471084160 run.py:483] Algo bellman_ford step 1900 current loss 0.428447, current_train_items 60832.
I0302 18:59:12.616013 22626471084160 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0302 18:59:12.616121 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 18:59:12.632806 22626471084160 run.py:483] Algo bellman_ford step 1901 current loss 0.600951, current_train_items 60864.
I0302 18:59:12.655976 22626471084160 run.py:483] Algo bellman_ford step 1902 current loss 0.588955, current_train_items 60896.
I0302 18:59:12.684027 22626471084160 run.py:483] Algo bellman_ford step 1903 current loss 0.773351, current_train_items 60928.
I0302 18:59:12.716420 22626471084160 run.py:483] Algo bellman_ford step 1904 current loss 0.965725, current_train_items 60960.
I0302 18:59:12.735976 22626471084160 run.py:483] Algo bellman_ford step 1905 current loss 0.420884, current_train_items 60992.
I0302 18:59:12.751739 22626471084160 run.py:483] Algo bellman_ford step 1906 current loss 0.566104, current_train_items 61024.
I0302 18:59:12.773982 22626471084160 run.py:483] Algo bellman_ford step 1907 current loss 0.673730, current_train_items 61056.
I0302 18:59:12.803096 22626471084160 run.py:483] Algo bellman_ford step 1908 current loss 0.847489, current_train_items 61088.
I0302 18:59:12.835143 22626471084160 run.py:483] Algo bellman_ford step 1909 current loss 0.937076, current_train_items 61120.
I0302 18:59:12.854203 22626471084160 run.py:483] Algo bellman_ford step 1910 current loss 0.321915, current_train_items 61152.
I0302 18:59:12.870581 22626471084160 run.py:483] Algo bellman_ford step 1911 current loss 0.535362, current_train_items 61184.
I0302 18:59:12.894364 22626471084160 run.py:483] Algo bellman_ford step 1912 current loss 0.747666, current_train_items 61216.
I0302 18:59:12.923243 22626471084160 run.py:483] Algo bellman_ford step 1913 current loss 0.808979, current_train_items 61248.
I0302 18:59:12.956172 22626471084160 run.py:483] Algo bellman_ford step 1914 current loss 1.002473, current_train_items 61280.
I0302 18:59:12.975193 22626471084160 run.py:483] Algo bellman_ford step 1915 current loss 0.259384, current_train_items 61312.
I0302 18:59:12.991553 22626471084160 run.py:483] Algo bellman_ford step 1916 current loss 0.531935, current_train_items 61344.
I0302 18:59:13.015148 22626471084160 run.py:483] Algo bellman_ford step 1917 current loss 0.725092, current_train_items 61376.
I0302 18:59:13.045318 22626471084160 run.py:483] Algo bellman_ford step 1918 current loss 0.847744, current_train_items 61408.
I0302 18:59:13.079110 22626471084160 run.py:483] Algo bellman_ford step 1919 current loss 1.087018, current_train_items 61440.
I0302 18:59:13.098059 22626471084160 run.py:483] Algo bellman_ford step 1920 current loss 0.395782, current_train_items 61472.
I0302 18:59:13.114372 22626471084160 run.py:483] Algo bellman_ford step 1921 current loss 0.621882, current_train_items 61504.
I0302 18:59:13.137045 22626471084160 run.py:483] Algo bellman_ford step 1922 current loss 0.715241, current_train_items 61536.
I0302 18:59:13.165643 22626471084160 run.py:483] Algo bellman_ford step 1923 current loss 0.726450, current_train_items 61568.
I0302 18:59:13.197983 22626471084160 run.py:483] Algo bellman_ford step 1924 current loss 1.026089, current_train_items 61600.
I0302 18:59:13.216920 22626471084160 run.py:483] Algo bellman_ford step 1925 current loss 0.332233, current_train_items 61632.
I0302 18:59:13.233286 22626471084160 run.py:483] Algo bellman_ford step 1926 current loss 0.690320, current_train_items 61664.
I0302 18:59:13.255372 22626471084160 run.py:483] Algo bellman_ford step 1927 current loss 0.591681, current_train_items 61696.
I0302 18:59:13.283855 22626471084160 run.py:483] Algo bellman_ford step 1928 current loss 0.748566, current_train_items 61728.
I0302 18:59:13.314660 22626471084160 run.py:483] Algo bellman_ford step 1929 current loss 0.941528, current_train_items 61760.
I0302 18:59:13.333884 22626471084160 run.py:483] Algo bellman_ford step 1930 current loss 0.392194, current_train_items 61792.
I0302 18:59:13.349581 22626471084160 run.py:483] Algo bellman_ford step 1931 current loss 0.419886, current_train_items 61824.
I0302 18:59:13.373875 22626471084160 run.py:483] Algo bellman_ford step 1932 current loss 0.850917, current_train_items 61856.
I0302 18:59:13.403695 22626471084160 run.py:483] Algo bellman_ford step 1933 current loss 0.971372, current_train_items 61888.
I0302 18:59:13.434780 22626471084160 run.py:483] Algo bellman_ford step 1934 current loss 0.931162, current_train_items 61920.
I0302 18:59:13.453710 22626471084160 run.py:483] Algo bellman_ford step 1935 current loss 0.348538, current_train_items 61952.
I0302 18:59:13.469902 22626471084160 run.py:483] Algo bellman_ford step 1936 current loss 0.501042, current_train_items 61984.
I0302 18:59:13.493720 22626471084160 run.py:483] Algo bellman_ford step 1937 current loss 0.679239, current_train_items 62016.
I0302 18:59:13.522289 22626471084160 run.py:483] Algo bellman_ford step 1938 current loss 0.796264, current_train_items 62048.
I0302 18:59:13.555545 22626471084160 run.py:483] Algo bellman_ford step 1939 current loss 1.027021, current_train_items 62080.
I0302 18:59:13.574726 22626471084160 run.py:483] Algo bellman_ford step 1940 current loss 0.347056, current_train_items 62112.
I0302 18:59:13.590960 22626471084160 run.py:483] Algo bellman_ford step 1941 current loss 0.507562, current_train_items 62144.
I0302 18:59:13.613852 22626471084160 run.py:483] Algo bellman_ford step 1942 current loss 0.763555, current_train_items 62176.
I0302 18:59:13.642096 22626471084160 run.py:483] Algo bellman_ford step 1943 current loss 0.799375, current_train_items 62208.
I0302 18:59:13.674889 22626471084160 run.py:483] Algo bellman_ford step 1944 current loss 0.924000, current_train_items 62240.
I0302 18:59:13.694355 22626471084160 run.py:483] Algo bellman_ford step 1945 current loss 0.340623, current_train_items 62272.
I0302 18:59:13.710611 22626471084160 run.py:483] Algo bellman_ford step 1946 current loss 0.542642, current_train_items 62304.
I0302 18:59:13.734455 22626471084160 run.py:483] Algo bellman_ford step 1947 current loss 0.668560, current_train_items 62336.
I0302 18:59:13.764260 22626471084160 run.py:483] Algo bellman_ford step 1948 current loss 1.029012, current_train_items 62368.
I0302 18:59:13.795565 22626471084160 run.py:483] Algo bellman_ford step 1949 current loss 0.911436, current_train_items 62400.
I0302 18:59:13.814753 22626471084160 run.py:483] Algo bellman_ford step 1950 current loss 0.323677, current_train_items 62432.
I0302 18:59:13.822987 22626471084160 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0302 18:59:13.823092 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 18:59:13.840299 22626471084160 run.py:483] Algo bellman_ford step 1951 current loss 0.732914, current_train_items 62464.
I0302 18:59:13.864057 22626471084160 run.py:483] Algo bellman_ford step 1952 current loss 0.664571, current_train_items 62496.
I0302 18:59:13.895520 22626471084160 run.py:483] Algo bellman_ford step 1953 current loss 0.809659, current_train_items 62528.
I0302 18:59:13.927740 22626471084160 run.py:483] Algo bellman_ford step 1954 current loss 0.921726, current_train_items 62560.
I0302 18:59:13.947266 22626471084160 run.py:483] Algo bellman_ford step 1955 current loss 0.367410, current_train_items 62592.
I0302 18:59:13.962563 22626471084160 run.py:483] Algo bellman_ford step 1956 current loss 0.433911, current_train_items 62624.
I0302 18:59:13.986163 22626471084160 run.py:483] Algo bellman_ford step 1957 current loss 0.758682, current_train_items 62656.
I0302 18:59:14.016565 22626471084160 run.py:483] Algo bellman_ford step 1958 current loss 0.880453, current_train_items 62688.
I0302 18:59:14.049872 22626471084160 run.py:483] Algo bellman_ford step 1959 current loss 0.859378, current_train_items 62720.
I0302 18:59:14.069241 22626471084160 run.py:483] Algo bellman_ford step 1960 current loss 0.348660, current_train_items 62752.
I0302 18:59:14.085608 22626471084160 run.py:483] Algo bellman_ford step 1961 current loss 0.557406, current_train_items 62784.
I0302 18:59:14.108363 22626471084160 run.py:483] Algo bellman_ford step 1962 current loss 0.760237, current_train_items 62816.
I0302 18:59:14.136994 22626471084160 run.py:483] Algo bellman_ford step 1963 current loss 0.758867, current_train_items 62848.
I0302 18:59:14.171857 22626471084160 run.py:483] Algo bellman_ford step 1964 current loss 1.024706, current_train_items 62880.
I0302 18:59:14.191078 22626471084160 run.py:483] Algo bellman_ford step 1965 current loss 0.336706, current_train_items 62912.
I0302 18:59:14.207162 22626471084160 run.py:483] Algo bellman_ford step 1966 current loss 0.550905, current_train_items 62944.
I0302 18:59:14.230280 22626471084160 run.py:483] Algo bellman_ford step 1967 current loss 0.783226, current_train_items 62976.
I0302 18:59:14.259025 22626471084160 run.py:483] Algo bellman_ford step 1968 current loss 0.802651, current_train_items 63008.
I0302 18:59:14.291071 22626471084160 run.py:483] Algo bellman_ford step 1969 current loss 0.846508, current_train_items 63040.
I0302 18:59:14.310253 22626471084160 run.py:483] Algo bellman_ford step 1970 current loss 0.290800, current_train_items 63072.
I0302 18:59:14.326524 22626471084160 run.py:483] Algo bellman_ford step 1971 current loss 0.520412, current_train_items 63104.
I0302 18:59:14.348992 22626471084160 run.py:483] Algo bellman_ford step 1972 current loss 0.725241, current_train_items 63136.
I0302 18:59:14.378387 22626471084160 run.py:483] Algo bellman_ford step 1973 current loss 0.873647, current_train_items 63168.
I0302 18:59:14.414108 22626471084160 run.py:483] Algo bellman_ford step 1974 current loss 1.090699, current_train_items 63200.
I0302 18:59:14.433266 22626471084160 run.py:483] Algo bellman_ford step 1975 current loss 0.317892, current_train_items 63232.
I0302 18:59:14.449147 22626471084160 run.py:483] Algo bellman_ford step 1976 current loss 0.526178, current_train_items 63264.
I0302 18:59:14.471279 22626471084160 run.py:483] Algo bellman_ford step 1977 current loss 0.697491, current_train_items 63296.
I0302 18:59:14.500087 22626471084160 run.py:483] Algo bellman_ford step 1978 current loss 0.836708, current_train_items 63328.
I0302 18:59:14.532990 22626471084160 run.py:483] Algo bellman_ford step 1979 current loss 1.025774, current_train_items 63360.
I0302 18:59:14.552118 22626471084160 run.py:483] Algo bellman_ford step 1980 current loss 0.357191, current_train_items 63392.
I0302 18:59:14.568501 22626471084160 run.py:483] Algo bellman_ford step 1981 current loss 0.587355, current_train_items 63424.
I0302 18:59:14.591854 22626471084160 run.py:483] Algo bellman_ford step 1982 current loss 0.667146, current_train_items 63456.
I0302 18:59:14.620139 22626471084160 run.py:483] Algo bellman_ford step 1983 current loss 0.744741, current_train_items 63488.
I0302 18:59:14.655518 22626471084160 run.py:483] Algo bellman_ford step 1984 current loss 1.018840, current_train_items 63520.
I0302 18:59:14.674921 22626471084160 run.py:483] Algo bellman_ford step 1985 current loss 0.266501, current_train_items 63552.
I0302 18:59:14.691126 22626471084160 run.py:483] Algo bellman_ford step 1986 current loss 0.563770, current_train_items 63584.
I0302 18:59:14.712993 22626471084160 run.py:483] Algo bellman_ford step 1987 current loss 0.731264, current_train_items 63616.
I0302 18:59:14.741362 22626471084160 run.py:483] Algo bellman_ford step 1988 current loss 0.703204, current_train_items 63648.
I0302 18:59:14.774927 22626471084160 run.py:483] Algo bellman_ford step 1989 current loss 0.904616, current_train_items 63680.
I0302 18:59:14.794055 22626471084160 run.py:483] Algo bellman_ford step 1990 current loss 0.400413, current_train_items 63712.
I0302 18:59:14.810206 22626471084160 run.py:483] Algo bellman_ford step 1991 current loss 0.575235, current_train_items 63744.
I0302 18:59:14.833397 22626471084160 run.py:483] Algo bellman_ford step 1992 current loss 0.656532, current_train_items 63776.
I0302 18:59:14.862736 22626471084160 run.py:483] Algo bellman_ford step 1993 current loss 0.765417, current_train_items 63808.
I0302 18:59:14.891058 22626471084160 run.py:483] Algo bellman_ford step 1994 current loss 0.745065, current_train_items 63840.
I0302 18:59:14.910320 22626471084160 run.py:483] Algo bellman_ford step 1995 current loss 0.382691, current_train_items 63872.
I0302 18:59:14.927194 22626471084160 run.py:483] Algo bellman_ford step 1996 current loss 0.635534, current_train_items 63904.
I0302 18:59:14.950054 22626471084160 run.py:483] Algo bellman_ford step 1997 current loss 0.720753, current_train_items 63936.
I0302 18:59:14.980899 22626471084160 run.py:483] Algo bellman_ford step 1998 current loss 0.795138, current_train_items 63968.
I0302 18:59:15.012348 22626471084160 run.py:483] Algo bellman_ford step 1999 current loss 0.901300, current_train_items 64000.
I0302 18:59:15.031512 22626471084160 run.py:483] Algo bellman_ford step 2000 current loss 0.295910, current_train_items 64032.
I0302 18:59:15.039299 22626471084160 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.87890625, 'score': 0.87890625, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0302 18:59:15.039404 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.879, val scores are: bellman_ford: 0.879
I0302 18:59:15.055874 22626471084160 run.py:483] Algo bellman_ford step 2001 current loss 0.480906, current_train_items 64064.
I0302 18:59:15.079895 22626471084160 run.py:483] Algo bellman_ford step 2002 current loss 0.900656, current_train_items 64096.
I0302 18:59:15.109656 22626471084160 run.py:483] Algo bellman_ford step 2003 current loss 0.715992, current_train_items 64128.
I0302 18:59:15.141825 22626471084160 run.py:483] Algo bellman_ford step 2004 current loss 0.852446, current_train_items 64160.
I0302 18:59:15.161001 22626471084160 run.py:483] Algo bellman_ford step 2005 current loss 0.370345, current_train_items 64192.
I0302 18:59:15.176897 22626471084160 run.py:483] Algo bellman_ford step 2006 current loss 0.530147, current_train_items 64224.
I0302 18:59:15.199693 22626471084160 run.py:483] Algo bellman_ford step 2007 current loss 0.730630, current_train_items 64256.
I0302 18:59:15.229964 22626471084160 run.py:483] Algo bellman_ford step 2008 current loss 0.838176, current_train_items 64288.
I0302 18:59:15.261899 22626471084160 run.py:483] Algo bellman_ford step 2009 current loss 0.924158, current_train_items 64320.
I0302 18:59:15.280711 22626471084160 run.py:483] Algo bellman_ford step 2010 current loss 0.365706, current_train_items 64352.
I0302 18:59:15.296979 22626471084160 run.py:483] Algo bellman_ford step 2011 current loss 0.568410, current_train_items 64384.
I0302 18:59:15.320056 22626471084160 run.py:483] Algo bellman_ford step 2012 current loss 0.955140, current_train_items 64416.
I0302 18:59:15.348546 22626471084160 run.py:483] Algo bellman_ford step 2013 current loss 0.971993, current_train_items 64448.
I0302 18:59:15.380658 22626471084160 run.py:483] Algo bellman_ford step 2014 current loss 0.987777, current_train_items 64480.
I0302 18:59:15.399305 22626471084160 run.py:483] Algo bellman_ford step 2015 current loss 0.319686, current_train_items 64512.
I0302 18:59:15.416021 22626471084160 run.py:483] Algo bellman_ford step 2016 current loss 0.562038, current_train_items 64544.
I0302 18:59:15.439391 22626471084160 run.py:483] Algo bellman_ford step 2017 current loss 0.908542, current_train_items 64576.
I0302 18:59:15.466816 22626471084160 run.py:483] Algo bellman_ford step 2018 current loss 0.783886, current_train_items 64608.
I0302 18:59:15.499914 22626471084160 run.py:483] Algo bellman_ford step 2019 current loss 1.074063, current_train_items 64640.
I0302 18:59:15.518980 22626471084160 run.py:483] Algo bellman_ford step 2020 current loss 0.341681, current_train_items 64672.
I0302 18:59:15.535077 22626471084160 run.py:483] Algo bellman_ford step 2021 current loss 0.489512, current_train_items 64704.
I0302 18:59:15.558368 22626471084160 run.py:483] Algo bellman_ford step 2022 current loss 0.814286, current_train_items 64736.
I0302 18:59:15.587322 22626471084160 run.py:483] Algo bellman_ford step 2023 current loss 0.838512, current_train_items 64768.
I0302 18:59:15.622487 22626471084160 run.py:483] Algo bellman_ford step 2024 current loss 0.994213, current_train_items 64800.
I0302 18:59:15.641660 22626471084160 run.py:483] Algo bellman_ford step 2025 current loss 0.507012, current_train_items 64832.
I0302 18:59:15.657564 22626471084160 run.py:483] Algo bellman_ford step 2026 current loss 0.512414, current_train_items 64864.
I0302 18:59:15.680748 22626471084160 run.py:483] Algo bellman_ford step 2027 current loss 0.815616, current_train_items 64896.
I0302 18:59:15.709709 22626471084160 run.py:483] Algo bellman_ford step 2028 current loss 0.922969, current_train_items 64928.
I0302 18:59:15.741372 22626471084160 run.py:483] Algo bellman_ford step 2029 current loss 0.861319, current_train_items 64960.
I0302 18:59:15.760947 22626471084160 run.py:483] Algo bellman_ford step 2030 current loss 0.396562, current_train_items 64992.
I0302 18:59:15.777359 22626471084160 run.py:483] Algo bellman_ford step 2031 current loss 0.585872, current_train_items 65024.
I0302 18:59:15.799614 22626471084160 run.py:483] Algo bellman_ford step 2032 current loss 0.664071, current_train_items 65056.
I0302 18:59:15.829542 22626471084160 run.py:483] Algo bellman_ford step 2033 current loss 0.816045, current_train_items 65088.
I0302 18:59:15.860998 22626471084160 run.py:483] Algo bellman_ford step 2034 current loss 0.875165, current_train_items 65120.
I0302 18:59:15.879748 22626471084160 run.py:483] Algo bellman_ford step 2035 current loss 0.337799, current_train_items 65152.
I0302 18:59:15.895653 22626471084160 run.py:483] Algo bellman_ford step 2036 current loss 0.548256, current_train_items 65184.
I0302 18:59:15.917826 22626471084160 run.py:483] Algo bellman_ford step 2037 current loss 0.714624, current_train_items 65216.
I0302 18:59:15.947046 22626471084160 run.py:483] Algo bellman_ford step 2038 current loss 0.843169, current_train_items 65248.
I0302 18:59:15.979727 22626471084160 run.py:483] Algo bellman_ford step 2039 current loss 0.853980, current_train_items 65280.
I0302 18:59:15.998634 22626471084160 run.py:483] Algo bellman_ford step 2040 current loss 0.342652, current_train_items 65312.
I0302 18:59:16.014497 22626471084160 run.py:483] Algo bellman_ford step 2041 current loss 0.525471, current_train_items 65344.
I0302 18:59:16.038623 22626471084160 run.py:483] Algo bellman_ford step 2042 current loss 0.765725, current_train_items 65376.
I0302 18:59:16.067985 22626471084160 run.py:483] Algo bellman_ford step 2043 current loss 0.780893, current_train_items 65408.
I0302 18:59:16.094927 22626471084160 run.py:483] Algo bellman_ford step 2044 current loss 0.838438, current_train_items 65440.
I0302 18:59:16.113804 22626471084160 run.py:483] Algo bellman_ford step 2045 current loss 0.323603, current_train_items 65472.
I0302 18:59:16.130516 22626471084160 run.py:483] Algo bellman_ford step 2046 current loss 0.704870, current_train_items 65504.
I0302 18:59:16.153699 22626471084160 run.py:483] Algo bellman_ford step 2047 current loss 0.846844, current_train_items 65536.
I0302 18:59:16.182535 22626471084160 run.py:483] Algo bellman_ford step 2048 current loss 0.728591, current_train_items 65568.
I0302 18:59:16.212498 22626471084160 run.py:483] Algo bellman_ford step 2049 current loss 0.906177, current_train_items 65600.
I0302 18:59:16.231664 22626471084160 run.py:483] Algo bellman_ford step 2050 current loss 0.420070, current_train_items 65632.
I0302 18:59:16.239660 22626471084160 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0302 18:59:16.239784 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 18:59:16.257081 22626471084160 run.py:483] Algo bellman_ford step 2051 current loss 0.656541, current_train_items 65664.
I0302 18:59:16.280388 22626471084160 run.py:483] Algo bellman_ford step 2052 current loss 0.647358, current_train_items 65696.
I0302 18:59:16.309597 22626471084160 run.py:483] Algo bellman_ford step 2053 current loss 0.937514, current_train_items 65728.
I0302 18:59:16.345255 22626471084160 run.py:483] Algo bellman_ford step 2054 current loss 1.231757, current_train_items 65760.
I0302 18:59:16.364752 22626471084160 run.py:483] Algo bellman_ford step 2055 current loss 0.356583, current_train_items 65792.
I0302 18:59:16.380530 22626471084160 run.py:483] Algo bellman_ford step 2056 current loss 0.521929, current_train_items 65824.
I0302 18:59:16.403020 22626471084160 run.py:483] Algo bellman_ford step 2057 current loss 0.791099, current_train_items 65856.
I0302 18:59:16.432210 22626471084160 run.py:483] Algo bellman_ford step 2058 current loss 0.745282, current_train_items 65888.
I0302 18:59:16.465163 22626471084160 run.py:483] Algo bellman_ford step 2059 current loss 1.011503, current_train_items 65920.
I0302 18:59:16.484503 22626471084160 run.py:483] Algo bellman_ford step 2060 current loss 0.414625, current_train_items 65952.
I0302 18:59:16.500938 22626471084160 run.py:483] Algo bellman_ford step 2061 current loss 0.589271, current_train_items 65984.
I0302 18:59:16.523907 22626471084160 run.py:483] Algo bellman_ford step 2062 current loss 0.818363, current_train_items 66016.
I0302 18:59:16.553911 22626471084160 run.py:483] Algo bellman_ford step 2063 current loss 0.812206, current_train_items 66048.
I0302 18:59:16.587933 22626471084160 run.py:483] Algo bellman_ford step 2064 current loss 0.901103, current_train_items 66080.
I0302 18:59:16.607051 22626471084160 run.py:483] Algo bellman_ford step 2065 current loss 0.355327, current_train_items 66112.
I0302 18:59:16.623210 22626471084160 run.py:483] Algo bellman_ford step 2066 current loss 0.518205, current_train_items 66144.
I0302 18:59:16.646739 22626471084160 run.py:483] Algo bellman_ford step 2067 current loss 0.751421, current_train_items 66176.
I0302 18:59:16.676769 22626471084160 run.py:483] Algo bellman_ford step 2068 current loss 0.823244, current_train_items 66208.
I0302 18:59:16.709186 22626471084160 run.py:483] Algo bellman_ford step 2069 current loss 0.932176, current_train_items 66240.
I0302 18:59:16.728619 22626471084160 run.py:483] Algo bellman_ford step 2070 current loss 0.368936, current_train_items 66272.
I0302 18:59:16.745075 22626471084160 run.py:483] Algo bellman_ford step 2071 current loss 0.584557, current_train_items 66304.
I0302 18:59:16.768280 22626471084160 run.py:483] Algo bellman_ford step 2072 current loss 0.653487, current_train_items 66336.
I0302 18:59:16.798196 22626471084160 run.py:483] Algo bellman_ford step 2073 current loss 0.856144, current_train_items 66368.
I0302 18:59:16.830768 22626471084160 run.py:483] Algo bellman_ford step 2074 current loss 0.892713, current_train_items 66400.
I0302 18:59:16.850016 22626471084160 run.py:483] Algo bellman_ford step 2075 current loss 0.387223, current_train_items 66432.
I0302 18:59:16.866657 22626471084160 run.py:483] Algo bellman_ford step 2076 current loss 0.617036, current_train_items 66464.
I0302 18:59:16.890087 22626471084160 run.py:483] Algo bellman_ford step 2077 current loss 0.714912, current_train_items 66496.
I0302 18:59:16.917554 22626471084160 run.py:483] Algo bellman_ford step 2078 current loss 0.789427, current_train_items 66528.
I0302 18:59:16.949455 22626471084160 run.py:483] Algo bellman_ford step 2079 current loss 1.047080, current_train_items 66560.
I0302 18:59:16.968436 22626471084160 run.py:483] Algo bellman_ford step 2080 current loss 0.289329, current_train_items 66592.
I0302 18:59:16.984677 22626471084160 run.py:483] Algo bellman_ford step 2081 current loss 0.517407, current_train_items 66624.
I0302 18:59:17.007645 22626471084160 run.py:483] Algo bellman_ford step 2082 current loss 0.716517, current_train_items 66656.
I0302 18:59:17.038358 22626471084160 run.py:483] Algo bellman_ford step 2083 current loss 0.944591, current_train_items 66688.
I0302 18:59:17.071601 22626471084160 run.py:483] Algo bellman_ford step 2084 current loss 1.188073, current_train_items 66720.
I0302 18:59:17.090833 22626471084160 run.py:483] Algo bellman_ford step 2085 current loss 0.347166, current_train_items 66752.
I0302 18:59:17.106979 22626471084160 run.py:483] Algo bellman_ford step 2086 current loss 0.730615, current_train_items 66784.
I0302 18:59:17.130043 22626471084160 run.py:483] Algo bellman_ford step 2087 current loss 0.902634, current_train_items 66816.
I0302 18:59:17.157733 22626471084160 run.py:483] Algo bellman_ford step 2088 current loss 0.687133, current_train_items 66848.
I0302 18:59:17.190402 22626471084160 run.py:483] Algo bellman_ford step 2089 current loss 0.945064, current_train_items 66880.
I0302 18:59:17.209748 22626471084160 run.py:483] Algo bellman_ford step 2090 current loss 0.331883, current_train_items 66912.
I0302 18:59:17.226017 22626471084160 run.py:483] Algo bellman_ford step 2091 current loss 0.630613, current_train_items 66944.
I0302 18:59:17.248719 22626471084160 run.py:483] Algo bellman_ford step 2092 current loss 0.934175, current_train_items 66976.
I0302 18:59:17.277895 22626471084160 run.py:483] Algo bellman_ford step 2093 current loss 0.927608, current_train_items 67008.
I0302 18:59:17.310251 22626471084160 run.py:483] Algo bellman_ford step 2094 current loss 1.081743, current_train_items 67040.
I0302 18:59:17.329483 22626471084160 run.py:483] Algo bellman_ford step 2095 current loss 0.302815, current_train_items 67072.
I0302 18:59:17.345795 22626471084160 run.py:483] Algo bellman_ford step 2096 current loss 0.492337, current_train_items 67104.
I0302 18:59:17.368811 22626471084160 run.py:483] Algo bellman_ford step 2097 current loss 0.725952, current_train_items 67136.
I0302 18:59:17.397460 22626471084160 run.py:483] Algo bellman_ford step 2098 current loss 0.715499, current_train_items 67168.
I0302 18:59:17.429208 22626471084160 run.py:483] Algo bellman_ford step 2099 current loss 0.959887, current_train_items 67200.
I0302 18:59:17.448534 22626471084160 run.py:483] Algo bellman_ford step 2100 current loss 0.338361, current_train_items 67232.
I0302 18:59:17.456388 22626471084160 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0302 18:59:17.456494 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:59:17.472957 22626471084160 run.py:483] Algo bellman_ford step 2101 current loss 0.507081, current_train_items 67264.
I0302 18:59:17.496179 22626471084160 run.py:483] Algo bellman_ford step 2102 current loss 0.717773, current_train_items 67296.
I0302 18:59:17.525986 22626471084160 run.py:483] Algo bellman_ford step 2103 current loss 0.876210, current_train_items 67328.
I0302 18:59:17.558220 22626471084160 run.py:483] Algo bellman_ford step 2104 current loss 1.013152, current_train_items 67360.
I0302 18:59:17.577769 22626471084160 run.py:483] Algo bellman_ford step 2105 current loss 0.381390, current_train_items 67392.
I0302 18:59:17.592880 22626471084160 run.py:483] Algo bellman_ford step 2106 current loss 0.454833, current_train_items 67424.
I0302 18:59:17.616419 22626471084160 run.py:483] Algo bellman_ford step 2107 current loss 0.768282, current_train_items 67456.
I0302 18:59:17.647219 22626471084160 run.py:483] Algo bellman_ford step 2108 current loss 0.923319, current_train_items 67488.
I0302 18:59:17.678354 22626471084160 run.py:483] Algo bellman_ford step 2109 current loss 0.809951, current_train_items 67520.
I0302 18:59:17.697519 22626471084160 run.py:483] Algo bellman_ford step 2110 current loss 0.396361, current_train_items 67552.
I0302 18:59:17.713731 22626471084160 run.py:483] Algo bellman_ford step 2111 current loss 0.581442, current_train_items 67584.
I0302 18:59:17.736293 22626471084160 run.py:483] Algo bellman_ford step 2112 current loss 0.825619, current_train_items 67616.
I0302 18:59:17.764881 22626471084160 run.py:483] Algo bellman_ford step 2113 current loss 0.829548, current_train_items 67648.
I0302 18:59:17.795915 22626471084160 run.py:483] Algo bellman_ford step 2114 current loss 0.978144, current_train_items 67680.
I0302 18:59:17.814768 22626471084160 run.py:483] Algo bellman_ford step 2115 current loss 0.405033, current_train_items 67712.
I0302 18:59:17.830884 22626471084160 run.py:483] Algo bellman_ford step 2116 current loss 0.587326, current_train_items 67744.
I0302 18:59:17.854723 22626471084160 run.py:483] Algo bellman_ford step 2117 current loss 0.822150, current_train_items 67776.
I0302 18:59:17.884737 22626471084160 run.py:483] Algo bellman_ford step 2118 current loss 0.930067, current_train_items 67808.
I0302 18:59:17.916403 22626471084160 run.py:483] Algo bellman_ford step 2119 current loss 0.847979, current_train_items 67840.
I0302 18:59:17.935593 22626471084160 run.py:483] Algo bellman_ford step 2120 current loss 0.339675, current_train_items 67872.
I0302 18:59:17.951487 22626471084160 run.py:483] Algo bellman_ford step 2121 current loss 0.594097, current_train_items 67904.
I0302 18:59:17.974344 22626471084160 run.py:483] Algo bellman_ford step 2122 current loss 0.635435, current_train_items 67936.
I0302 18:59:18.003981 22626471084160 run.py:483] Algo bellman_ford step 2123 current loss 0.772049, current_train_items 67968.
I0302 18:59:18.032799 22626471084160 run.py:483] Algo bellman_ford step 2124 current loss 0.869837, current_train_items 68000.
I0302 18:59:18.051695 22626471084160 run.py:483] Algo bellman_ford step 2125 current loss 0.267550, current_train_items 68032.
I0302 18:59:18.068198 22626471084160 run.py:483] Algo bellman_ford step 2126 current loss 0.636967, current_train_items 68064.
I0302 18:59:18.092750 22626471084160 run.py:483] Algo bellman_ford step 2127 current loss 0.852483, current_train_items 68096.
I0302 18:59:18.121470 22626471084160 run.py:483] Algo bellman_ford step 2128 current loss 0.825602, current_train_items 68128.
I0302 18:59:18.152863 22626471084160 run.py:483] Algo bellman_ford step 2129 current loss 0.800832, current_train_items 68160.
I0302 18:59:18.172073 22626471084160 run.py:483] Algo bellman_ford step 2130 current loss 0.385786, current_train_items 68192.
I0302 18:59:18.188835 22626471084160 run.py:483] Algo bellman_ford step 2131 current loss 0.569642, current_train_items 68224.
I0302 18:59:18.212398 22626471084160 run.py:483] Algo bellman_ford step 2132 current loss 0.769409, current_train_items 68256.
I0302 18:59:18.241519 22626471084160 run.py:483] Algo bellman_ford step 2133 current loss 0.926951, current_train_items 68288.
I0302 18:59:18.275278 22626471084160 run.py:483] Algo bellman_ford step 2134 current loss 0.906060, current_train_items 68320.
I0302 18:59:18.294323 22626471084160 run.py:483] Algo bellman_ford step 2135 current loss 0.330445, current_train_items 68352.
I0302 18:59:18.310657 22626471084160 run.py:483] Algo bellman_ford step 2136 current loss 0.516814, current_train_items 68384.
I0302 18:59:18.333430 22626471084160 run.py:483] Algo bellman_ford step 2137 current loss 0.707229, current_train_items 68416.
I0302 18:59:18.363196 22626471084160 run.py:483] Algo bellman_ford step 2138 current loss 0.838791, current_train_items 68448.
I0302 18:59:18.398534 22626471084160 run.py:483] Algo bellman_ford step 2139 current loss 0.987114, current_train_items 68480.
I0302 18:59:18.417494 22626471084160 run.py:483] Algo bellman_ford step 2140 current loss 0.441664, current_train_items 68512.
I0302 18:59:18.433559 22626471084160 run.py:483] Algo bellman_ford step 2141 current loss 0.447492, current_train_items 68544.
I0302 18:59:18.455289 22626471084160 run.py:483] Algo bellman_ford step 2142 current loss 0.571150, current_train_items 68576.
I0302 18:59:18.484143 22626471084160 run.py:483] Algo bellman_ford step 2143 current loss 0.690617, current_train_items 68608.
I0302 18:59:18.516574 22626471084160 run.py:483] Algo bellman_ford step 2144 current loss 1.031592, current_train_items 68640.
I0302 18:59:18.535603 22626471084160 run.py:483] Algo bellman_ford step 2145 current loss 0.315228, current_train_items 68672.
I0302 18:59:18.552148 22626471084160 run.py:483] Algo bellman_ford step 2146 current loss 0.568344, current_train_items 68704.
I0302 18:59:18.575866 22626471084160 run.py:483] Algo bellman_ford step 2147 current loss 0.880477, current_train_items 68736.
I0302 18:59:18.605804 22626471084160 run.py:483] Algo bellman_ford step 2148 current loss 0.840837, current_train_items 68768.
I0302 18:59:18.636834 22626471084160 run.py:483] Algo bellman_ford step 2149 current loss 0.882468, current_train_items 68800.
I0302 18:59:18.655946 22626471084160 run.py:483] Algo bellman_ford step 2150 current loss 0.402414, current_train_items 68832.
I0302 18:59:18.663969 22626471084160 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0302 18:59:18.664075 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 18:59:18.680807 22626471084160 run.py:483] Algo bellman_ford step 2151 current loss 0.527262, current_train_items 68864.
I0302 18:59:18.704419 22626471084160 run.py:483] Algo bellman_ford step 2152 current loss 0.746076, current_train_items 68896.
I0302 18:59:18.733628 22626471084160 run.py:483] Algo bellman_ford step 2153 current loss 0.916125, current_train_items 68928.
I0302 18:59:18.765759 22626471084160 run.py:483] Algo bellman_ford step 2154 current loss 0.903986, current_train_items 68960.
I0302 18:59:18.785268 22626471084160 run.py:483] Algo bellman_ford step 2155 current loss 0.401601, current_train_items 68992.
I0302 18:59:18.801151 22626471084160 run.py:483] Algo bellman_ford step 2156 current loss 0.482067, current_train_items 69024.
I0302 18:59:18.824497 22626471084160 run.py:483] Algo bellman_ford step 2157 current loss 0.722847, current_train_items 69056.
I0302 18:59:18.854470 22626471084160 run.py:483] Algo bellman_ford step 2158 current loss 0.833949, current_train_items 69088.
I0302 18:59:18.887212 22626471084160 run.py:483] Algo bellman_ford step 2159 current loss 0.985604, current_train_items 69120.
I0302 18:59:18.906740 22626471084160 run.py:483] Algo bellman_ford step 2160 current loss 0.339534, current_train_items 69152.
I0302 18:59:18.922884 22626471084160 run.py:483] Algo bellman_ford step 2161 current loss 0.498775, current_train_items 69184.
I0302 18:59:18.946327 22626471084160 run.py:483] Algo bellman_ford step 2162 current loss 0.764716, current_train_items 69216.
I0302 18:59:18.974359 22626471084160 run.py:483] Algo bellman_ford step 2163 current loss 0.812838, current_train_items 69248.
I0302 18:59:19.007668 22626471084160 run.py:483] Algo bellman_ford step 2164 current loss 1.040152, current_train_items 69280.
I0302 18:59:19.026714 22626471084160 run.py:483] Algo bellman_ford step 2165 current loss 0.374488, current_train_items 69312.
I0302 18:59:19.043379 22626471084160 run.py:483] Algo bellman_ford step 2166 current loss 0.634056, current_train_items 69344.
I0302 18:59:19.065883 22626471084160 run.py:483] Algo bellman_ford step 2167 current loss 0.645468, current_train_items 69376.
I0302 18:59:19.094353 22626471084160 run.py:483] Algo bellman_ford step 2168 current loss 0.834443, current_train_items 69408.
I0302 18:59:19.126897 22626471084160 run.py:483] Algo bellman_ford step 2169 current loss 1.142159, current_train_items 69440.
I0302 18:59:19.146195 22626471084160 run.py:483] Algo bellman_ford step 2170 current loss 0.336115, current_train_items 69472.
I0302 18:59:19.162271 22626471084160 run.py:483] Algo bellman_ford step 2171 current loss 0.630055, current_train_items 69504.
I0302 18:59:19.185254 22626471084160 run.py:483] Algo bellman_ford step 2172 current loss 0.753692, current_train_items 69536.
I0302 18:59:19.215632 22626471084160 run.py:483] Algo bellman_ford step 2173 current loss 0.853185, current_train_items 69568.
I0302 18:59:19.246294 22626471084160 run.py:483] Algo bellman_ford step 2174 current loss 0.940038, current_train_items 69600.
I0302 18:59:19.265565 22626471084160 run.py:483] Algo bellman_ford step 2175 current loss 0.358146, current_train_items 69632.
I0302 18:59:19.281304 22626471084160 run.py:483] Algo bellman_ford step 2176 current loss 0.508290, current_train_items 69664.
I0302 18:59:19.304775 22626471084160 run.py:483] Algo bellman_ford step 2177 current loss 0.866429, current_train_items 69696.
I0302 18:59:19.333290 22626471084160 run.py:483] Algo bellman_ford step 2178 current loss 0.846343, current_train_items 69728.
I0302 18:59:19.366228 22626471084160 run.py:483] Algo bellman_ford step 2179 current loss 0.875747, current_train_items 69760.
I0302 18:59:19.385114 22626471084160 run.py:483] Algo bellman_ford step 2180 current loss 0.294343, current_train_items 69792.
I0302 18:59:19.401410 22626471084160 run.py:483] Algo bellman_ford step 2181 current loss 0.487028, current_train_items 69824.
I0302 18:59:19.424801 22626471084160 run.py:483] Algo bellman_ford step 2182 current loss 0.852540, current_train_items 69856.
I0302 18:59:19.453593 22626471084160 run.py:483] Algo bellman_ford step 2183 current loss 0.967262, current_train_items 69888.
I0302 18:59:19.487194 22626471084160 run.py:483] Algo bellman_ford step 2184 current loss 1.121919, current_train_items 69920.
I0302 18:59:19.506455 22626471084160 run.py:483] Algo bellman_ford step 2185 current loss 0.345771, current_train_items 69952.
I0302 18:59:19.522188 22626471084160 run.py:483] Algo bellman_ford step 2186 current loss 0.445427, current_train_items 69984.
I0302 18:59:19.545126 22626471084160 run.py:483] Algo bellman_ford step 2187 current loss 0.680276, current_train_items 70016.
I0302 18:59:19.574629 22626471084160 run.py:483] Algo bellman_ford step 2188 current loss 0.885750, current_train_items 70048.
W0302 18:59:19.599301 22626471084160 samplers.py:155] Increasing hint lengh from 12 to 13
I0302 18:59:26.292227 22626471084160 run.py:483] Algo bellman_ford step 2189 current loss 1.489406, current_train_items 70080.
I0302 18:59:26.312909 22626471084160 run.py:483] Algo bellman_ford step 2190 current loss 0.342076, current_train_items 70112.
I0302 18:59:26.329607 22626471084160 run.py:483] Algo bellman_ford step 2191 current loss 0.591575, current_train_items 70144.
I0302 18:59:26.352831 22626471084160 run.py:483] Algo bellman_ford step 2192 current loss 0.760431, current_train_items 70176.
W0302 18:59:26.374126 22626471084160 samplers.py:155] Increasing hint lengh from 10 to 12
I0302 18:59:33.402271 22626471084160 run.py:483] Algo bellman_ford step 2193 current loss 1.261180, current_train_items 70208.
I0302 18:59:33.434679 22626471084160 run.py:483] Algo bellman_ford step 2194 current loss 0.932756, current_train_items 70240.
I0302 18:59:33.454856 22626471084160 run.py:483] Algo bellman_ford step 2195 current loss 0.350124, current_train_items 70272.
I0302 18:59:33.471595 22626471084160 run.py:483] Algo bellman_ford step 2196 current loss 0.636261, current_train_items 70304.
I0302 18:59:33.494723 22626471084160 run.py:483] Algo bellman_ford step 2197 current loss 0.711320, current_train_items 70336.
I0302 18:59:33.523357 22626471084160 run.py:483] Algo bellman_ford step 2198 current loss 0.879101, current_train_items 70368.
I0302 18:59:33.555968 22626471084160 run.py:483] Algo bellman_ford step 2199 current loss 0.958043, current_train_items 70400.
I0302 18:59:33.575811 22626471084160 run.py:483] Algo bellman_ford step 2200 current loss 0.377725, current_train_items 70432.
I0302 18:59:33.585397 22626471084160 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0302 18:59:33.585533 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 18:59:33.602738 22626471084160 run.py:483] Algo bellman_ford step 2201 current loss 0.575629, current_train_items 70464.
I0302 18:59:33.625879 22626471084160 run.py:483] Algo bellman_ford step 2202 current loss 0.608876, current_train_items 70496.
I0302 18:59:33.655811 22626471084160 run.py:483] Algo bellman_ford step 2203 current loss 0.754338, current_train_items 70528.
I0302 18:59:33.689655 22626471084160 run.py:483] Algo bellman_ford step 2204 current loss 0.971137, current_train_items 70560.
I0302 18:59:33.709984 22626471084160 run.py:483] Algo bellman_ford step 2205 current loss 0.291547, current_train_items 70592.
I0302 18:59:33.726213 22626471084160 run.py:483] Algo bellman_ford step 2206 current loss 0.509462, current_train_items 70624.
I0302 18:59:33.749127 22626471084160 run.py:483] Algo bellman_ford step 2207 current loss 0.750140, current_train_items 70656.
I0302 18:59:33.779143 22626471084160 run.py:483] Algo bellman_ford step 2208 current loss 0.859573, current_train_items 70688.
I0302 18:59:33.813356 22626471084160 run.py:483] Algo bellman_ford step 2209 current loss 1.006708, current_train_items 70720.
I0302 18:59:33.833213 22626471084160 run.py:483] Algo bellman_ford step 2210 current loss 0.358372, current_train_items 70752.
I0302 18:59:33.849483 22626471084160 run.py:483] Algo bellman_ford step 2211 current loss 0.558224, current_train_items 70784.
I0302 18:59:33.873216 22626471084160 run.py:483] Algo bellman_ford step 2212 current loss 0.889784, current_train_items 70816.
I0302 18:59:33.903928 22626471084160 run.py:483] Algo bellman_ford step 2213 current loss 1.019527, current_train_items 70848.
I0302 18:59:33.938085 22626471084160 run.py:483] Algo bellman_ford step 2214 current loss 1.211054, current_train_items 70880.
I0302 18:59:33.957931 22626471084160 run.py:483] Algo bellman_ford step 2215 current loss 0.378627, current_train_items 70912.
I0302 18:59:33.974107 22626471084160 run.py:483] Algo bellman_ford step 2216 current loss 0.474304, current_train_items 70944.
I0302 18:59:33.996372 22626471084160 run.py:483] Algo bellman_ford step 2217 current loss 0.763418, current_train_items 70976.
I0302 18:59:34.027633 22626471084160 run.py:483] Algo bellman_ford step 2218 current loss 0.895007, current_train_items 71008.
I0302 18:59:34.059905 22626471084160 run.py:483] Algo bellman_ford step 2219 current loss 0.928211, current_train_items 71040.
I0302 18:59:34.079532 22626471084160 run.py:483] Algo bellman_ford step 2220 current loss 0.289556, current_train_items 71072.
I0302 18:59:34.095609 22626471084160 run.py:483] Algo bellman_ford step 2221 current loss 0.566457, current_train_items 71104.
I0302 18:59:34.118513 22626471084160 run.py:483] Algo bellman_ford step 2222 current loss 0.707851, current_train_items 71136.
I0302 18:59:34.149140 22626471084160 run.py:483] Algo bellman_ford step 2223 current loss 0.884611, current_train_items 71168.
I0302 18:59:34.183322 22626471084160 run.py:483] Algo bellman_ford step 2224 current loss 1.096290, current_train_items 71200.
I0302 18:59:34.202798 22626471084160 run.py:483] Algo bellman_ford step 2225 current loss 0.273321, current_train_items 71232.
I0302 18:59:34.219147 22626471084160 run.py:483] Algo bellman_ford step 2226 current loss 0.632696, current_train_items 71264.
I0302 18:59:34.242593 22626471084160 run.py:483] Algo bellman_ford step 2227 current loss 0.789031, current_train_items 71296.
I0302 18:59:34.273144 22626471084160 run.py:483] Algo bellman_ford step 2228 current loss 0.900159, current_train_items 71328.
I0302 18:59:34.304029 22626471084160 run.py:483] Algo bellman_ford step 2229 current loss 0.840169, current_train_items 71360.
I0302 18:59:34.324025 22626471084160 run.py:483] Algo bellman_ford step 2230 current loss 0.348185, current_train_items 71392.
I0302 18:59:34.340477 22626471084160 run.py:483] Algo bellman_ford step 2231 current loss 0.642174, current_train_items 71424.
I0302 18:59:34.363538 22626471084160 run.py:483] Algo bellman_ford step 2232 current loss 0.771821, current_train_items 71456.
I0302 18:59:34.392906 22626471084160 run.py:483] Algo bellman_ford step 2233 current loss 0.829068, current_train_items 71488.
I0302 18:59:34.425238 22626471084160 run.py:483] Algo bellman_ford step 2234 current loss 1.131788, current_train_items 71520.
I0302 18:59:34.445044 22626471084160 run.py:483] Algo bellman_ford step 2235 current loss 0.312660, current_train_items 71552.
I0302 18:59:34.461501 22626471084160 run.py:483] Algo bellman_ford step 2236 current loss 0.497216, current_train_items 71584.
I0302 18:59:34.485665 22626471084160 run.py:483] Algo bellman_ford step 2237 current loss 0.729999, current_train_items 71616.
I0302 18:59:34.515927 22626471084160 run.py:483] Algo bellman_ford step 2238 current loss 0.907426, current_train_items 71648.
I0302 18:59:34.548251 22626471084160 run.py:483] Algo bellman_ford step 2239 current loss 0.752468, current_train_items 71680.
I0302 18:59:34.567745 22626471084160 run.py:483] Algo bellman_ford step 2240 current loss 0.456002, current_train_items 71712.
I0302 18:59:34.584467 22626471084160 run.py:483] Algo bellman_ford step 2241 current loss 0.653919, current_train_items 71744.
I0302 18:59:34.608828 22626471084160 run.py:483] Algo bellman_ford step 2242 current loss 0.708398, current_train_items 71776.
I0302 18:59:34.639185 22626471084160 run.py:483] Algo bellman_ford step 2243 current loss 0.988567, current_train_items 71808.
I0302 18:59:34.670538 22626471084160 run.py:483] Algo bellman_ford step 2244 current loss 0.825714, current_train_items 71840.
I0302 18:59:34.690080 22626471084160 run.py:483] Algo bellman_ford step 2245 current loss 0.305756, current_train_items 71872.
I0302 18:59:34.706578 22626471084160 run.py:483] Algo bellman_ford step 2246 current loss 0.562438, current_train_items 71904.
I0302 18:59:34.729289 22626471084160 run.py:483] Algo bellman_ford step 2247 current loss 0.673113, current_train_items 71936.
I0302 18:59:34.760301 22626471084160 run.py:483] Algo bellman_ford step 2248 current loss 0.772538, current_train_items 71968.
I0302 18:59:34.793640 22626471084160 run.py:483] Algo bellman_ford step 2249 current loss 0.924595, current_train_items 72000.
I0302 18:59:34.813138 22626471084160 run.py:483] Algo bellman_ford step 2250 current loss 0.362136, current_train_items 72032.
I0302 18:59:34.821543 22626471084160 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.896484375, 'score': 0.896484375, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0302 18:59:34.821651 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 18:59:34.838312 22626471084160 run.py:483] Algo bellman_ford step 2251 current loss 0.570845, current_train_items 72064.
I0302 18:59:34.860973 22626471084160 run.py:483] Algo bellman_ford step 2252 current loss 0.669706, current_train_items 72096.
I0302 18:59:34.892572 22626471084160 run.py:483] Algo bellman_ford step 2253 current loss 0.872628, current_train_items 72128.
I0302 18:59:34.926445 22626471084160 run.py:483] Algo bellman_ford step 2254 current loss 0.852818, current_train_items 72160.
I0302 18:59:34.946273 22626471084160 run.py:483] Algo bellman_ford step 2255 current loss 0.369307, current_train_items 72192.
I0302 18:59:34.962315 22626471084160 run.py:483] Algo bellman_ford step 2256 current loss 0.539131, current_train_items 72224.
I0302 18:59:34.985232 22626471084160 run.py:483] Algo bellman_ford step 2257 current loss 0.711914, current_train_items 72256.
I0302 18:59:35.014165 22626471084160 run.py:483] Algo bellman_ford step 2258 current loss 0.764076, current_train_items 72288.
I0302 18:59:35.048274 22626471084160 run.py:483] Algo bellman_ford step 2259 current loss 1.071565, current_train_items 72320.
I0302 18:59:35.068405 22626471084160 run.py:483] Algo bellman_ford step 2260 current loss 0.338309, current_train_items 72352.
I0302 18:59:35.085085 22626471084160 run.py:483] Algo bellman_ford step 2261 current loss 0.613167, current_train_items 72384.
I0302 18:59:35.109210 22626471084160 run.py:483] Algo bellman_ford step 2262 current loss 0.794647, current_train_items 72416.
I0302 18:59:35.138438 22626471084160 run.py:483] Algo bellman_ford step 2263 current loss 0.780162, current_train_items 72448.
I0302 18:59:35.171791 22626471084160 run.py:483] Algo bellman_ford step 2264 current loss 0.846706, current_train_items 72480.
I0302 18:59:35.191345 22626471084160 run.py:483] Algo bellman_ford step 2265 current loss 0.339887, current_train_items 72512.
I0302 18:59:35.207491 22626471084160 run.py:483] Algo bellman_ford step 2266 current loss 0.512805, current_train_items 72544.
I0302 18:59:35.231430 22626471084160 run.py:483] Algo bellman_ford step 2267 current loss 0.752161, current_train_items 72576.
I0302 18:59:35.261409 22626471084160 run.py:483] Algo bellman_ford step 2268 current loss 0.823248, current_train_items 72608.
I0302 18:59:35.293690 22626471084160 run.py:483] Algo bellman_ford step 2269 current loss 0.947938, current_train_items 72640.
I0302 18:59:35.313545 22626471084160 run.py:483] Algo bellman_ford step 2270 current loss 0.326421, current_train_items 72672.
I0302 18:59:35.329511 22626471084160 run.py:483] Algo bellman_ford step 2271 current loss 0.520503, current_train_items 72704.
I0302 18:59:35.351373 22626471084160 run.py:483] Algo bellman_ford step 2272 current loss 0.752146, current_train_items 72736.
I0302 18:59:35.381850 22626471084160 run.py:483] Algo bellman_ford step 2273 current loss 0.819132, current_train_items 72768.
I0302 18:59:35.417148 22626471084160 run.py:483] Algo bellman_ford step 2274 current loss 1.145612, current_train_items 72800.
I0302 18:59:35.437002 22626471084160 run.py:483] Algo bellman_ford step 2275 current loss 0.382664, current_train_items 72832.
I0302 18:59:35.453077 22626471084160 run.py:483] Algo bellman_ford step 2276 current loss 0.522715, current_train_items 72864.
I0302 18:59:35.474150 22626471084160 run.py:483] Algo bellman_ford step 2277 current loss 0.618539, current_train_items 72896.
I0302 18:59:35.503372 22626471084160 run.py:483] Algo bellman_ford step 2278 current loss 0.867071, current_train_items 72928.
I0302 18:59:35.537618 22626471084160 run.py:483] Algo bellman_ford step 2279 current loss 0.988081, current_train_items 72960.
I0302 18:59:35.557055 22626471084160 run.py:483] Algo bellman_ford step 2280 current loss 0.351213, current_train_items 72992.
I0302 18:59:35.573083 22626471084160 run.py:483] Algo bellman_ford step 2281 current loss 0.649027, current_train_items 73024.
I0302 18:59:35.596748 22626471084160 run.py:483] Algo bellman_ford step 2282 current loss 0.759468, current_train_items 73056.
I0302 18:59:35.629216 22626471084160 run.py:483] Algo bellman_ford step 2283 current loss 1.066296, current_train_items 73088.
I0302 18:59:35.663341 22626471084160 run.py:483] Algo bellman_ford step 2284 current loss 0.999322, current_train_items 73120.
I0302 18:59:35.683221 22626471084160 run.py:483] Algo bellman_ford step 2285 current loss 0.409705, current_train_items 73152.
I0302 18:59:35.699401 22626471084160 run.py:483] Algo bellman_ford step 2286 current loss 0.521760, current_train_items 73184.
I0302 18:59:35.721935 22626471084160 run.py:483] Algo bellman_ford step 2287 current loss 0.713805, current_train_items 73216.
I0302 18:59:35.751606 22626471084160 run.py:483] Algo bellman_ford step 2288 current loss 0.928097, current_train_items 73248.
I0302 18:59:35.787245 22626471084160 run.py:483] Algo bellman_ford step 2289 current loss 1.225996, current_train_items 73280.
I0302 18:59:35.806881 22626471084160 run.py:483] Algo bellman_ford step 2290 current loss 0.391617, current_train_items 73312.
I0302 18:59:35.822561 22626471084160 run.py:483] Algo bellman_ford step 2291 current loss 0.557310, current_train_items 73344.
I0302 18:59:35.844856 22626471084160 run.py:483] Algo bellman_ford step 2292 current loss 0.681236, current_train_items 73376.
I0302 18:59:35.875294 22626471084160 run.py:483] Algo bellman_ford step 2293 current loss 0.753564, current_train_items 73408.
I0302 18:59:35.908909 22626471084160 run.py:483] Algo bellman_ford step 2294 current loss 0.993370, current_train_items 73440.
I0302 18:59:35.928255 22626471084160 run.py:483] Algo bellman_ford step 2295 current loss 0.297903, current_train_items 73472.
I0302 18:59:35.944420 22626471084160 run.py:483] Algo bellman_ford step 2296 current loss 0.557582, current_train_items 73504.
I0302 18:59:35.966531 22626471084160 run.py:483] Algo bellman_ford step 2297 current loss 0.615711, current_train_items 73536.
I0302 18:59:35.995823 22626471084160 run.py:483] Algo bellman_ford step 2298 current loss 0.787817, current_train_items 73568.
I0302 18:59:36.029126 22626471084160 run.py:483] Algo bellman_ford step 2299 current loss 1.022487, current_train_items 73600.
I0302 18:59:36.048991 22626471084160 run.py:483] Algo bellman_ford step 2300 current loss 0.285211, current_train_items 73632.
I0302 18:59:36.056527 22626471084160 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0302 18:59:36.056634 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 18:59:36.073664 22626471084160 run.py:483] Algo bellman_ford step 2301 current loss 0.642843, current_train_items 73664.
I0302 18:59:36.097731 22626471084160 run.py:483] Algo bellman_ford step 2302 current loss 0.830929, current_train_items 73696.
I0302 18:59:36.129690 22626471084160 run.py:483] Algo bellman_ford step 2303 current loss 0.878345, current_train_items 73728.
I0302 18:59:36.163509 22626471084160 run.py:483] Algo bellman_ford step 2304 current loss 0.979404, current_train_items 73760.
I0302 18:59:36.183637 22626471084160 run.py:483] Algo bellman_ford step 2305 current loss 0.300338, current_train_items 73792.
I0302 18:59:36.200083 22626471084160 run.py:483] Algo bellman_ford step 2306 current loss 0.691819, current_train_items 73824.
I0302 18:59:36.223027 22626471084160 run.py:483] Algo bellman_ford step 2307 current loss 0.707593, current_train_items 73856.
I0302 18:59:36.252215 22626471084160 run.py:483] Algo bellman_ford step 2308 current loss 0.790279, current_train_items 73888.
I0302 18:59:36.286095 22626471084160 run.py:483] Algo bellman_ford step 2309 current loss 0.987661, current_train_items 73920.
I0302 18:59:36.305783 22626471084160 run.py:483] Algo bellman_ford step 2310 current loss 0.390195, current_train_items 73952.
I0302 18:59:36.322023 22626471084160 run.py:483] Algo bellman_ford step 2311 current loss 0.461968, current_train_items 73984.
I0302 18:59:36.345015 22626471084160 run.py:483] Algo bellman_ford step 2312 current loss 0.793577, current_train_items 74016.
I0302 18:59:36.375312 22626471084160 run.py:483] Algo bellman_ford step 2313 current loss 0.762981, current_train_items 74048.
I0302 18:59:36.409013 22626471084160 run.py:483] Algo bellman_ford step 2314 current loss 0.928262, current_train_items 74080.
I0302 18:59:36.428831 22626471084160 run.py:483] Algo bellman_ford step 2315 current loss 0.379982, current_train_items 74112.
I0302 18:59:36.444891 22626471084160 run.py:483] Algo bellman_ford step 2316 current loss 0.560690, current_train_items 74144.
I0302 18:59:36.467783 22626471084160 run.py:483] Algo bellman_ford step 2317 current loss 0.759502, current_train_items 74176.
I0302 18:59:36.498983 22626471084160 run.py:483] Algo bellman_ford step 2318 current loss 0.852112, current_train_items 74208.
I0302 18:59:36.533084 22626471084160 run.py:483] Algo bellman_ford step 2319 current loss 0.939743, current_train_items 74240.
I0302 18:59:36.553002 22626471084160 run.py:483] Algo bellman_ford step 2320 current loss 0.357298, current_train_items 74272.
I0302 18:59:36.568935 22626471084160 run.py:483] Algo bellman_ford step 2321 current loss 0.513289, current_train_items 74304.
I0302 18:59:36.592626 22626471084160 run.py:483] Algo bellman_ford step 2322 current loss 0.737869, current_train_items 74336.
I0302 18:59:36.622238 22626471084160 run.py:483] Algo bellman_ford step 2323 current loss 0.855340, current_train_items 74368.
I0302 18:59:36.656197 22626471084160 run.py:483] Algo bellman_ford step 2324 current loss 1.076865, current_train_items 74400.
I0302 18:59:36.675815 22626471084160 run.py:483] Algo bellman_ford step 2325 current loss 0.337173, current_train_items 74432.
I0302 18:59:36.691759 22626471084160 run.py:483] Algo bellman_ford step 2326 current loss 0.520788, current_train_items 74464.
I0302 18:59:36.714255 22626471084160 run.py:483] Algo bellman_ford step 2327 current loss 0.728545, current_train_items 74496.
I0302 18:59:36.743340 22626471084160 run.py:483] Algo bellman_ford step 2328 current loss 0.702712, current_train_items 74528.
I0302 18:59:36.775336 22626471084160 run.py:483] Algo bellman_ford step 2329 current loss 0.797690, current_train_items 74560.
I0302 18:59:36.794829 22626471084160 run.py:483] Algo bellman_ford step 2330 current loss 0.352226, current_train_items 74592.
I0302 18:59:36.810605 22626471084160 run.py:483] Algo bellman_ford step 2331 current loss 0.552712, current_train_items 74624.
I0302 18:59:36.834328 22626471084160 run.py:483] Algo bellman_ford step 2332 current loss 0.825885, current_train_items 74656.
I0302 18:59:36.864950 22626471084160 run.py:483] Algo bellman_ford step 2333 current loss 0.841448, current_train_items 74688.
I0302 18:59:36.899636 22626471084160 run.py:483] Algo bellman_ford step 2334 current loss 0.897652, current_train_items 74720.
I0302 18:59:36.919133 22626471084160 run.py:483] Algo bellman_ford step 2335 current loss 0.379189, current_train_items 74752.
I0302 18:59:36.935567 22626471084160 run.py:483] Algo bellman_ford step 2336 current loss 0.540253, current_train_items 74784.
I0302 18:59:36.958816 22626471084160 run.py:483] Algo bellman_ford step 2337 current loss 0.773849, current_train_items 74816.
I0302 18:59:36.988837 22626471084160 run.py:483] Algo bellman_ford step 2338 current loss 0.902291, current_train_items 74848.
I0302 18:59:37.021443 22626471084160 run.py:483] Algo bellman_ford step 2339 current loss 0.936151, current_train_items 74880.
I0302 18:59:37.041321 22626471084160 run.py:483] Algo bellman_ford step 2340 current loss 0.402473, current_train_items 74912.
I0302 18:59:37.057677 22626471084160 run.py:483] Algo bellman_ford step 2341 current loss 0.479806, current_train_items 74944.
I0302 18:59:37.079763 22626471084160 run.py:483] Algo bellman_ford step 2342 current loss 0.654688, current_train_items 74976.
I0302 18:59:37.108874 22626471084160 run.py:483] Algo bellman_ford step 2343 current loss 0.819636, current_train_items 75008.
I0302 18:59:37.140702 22626471084160 run.py:483] Algo bellman_ford step 2344 current loss 0.830797, current_train_items 75040.
I0302 18:59:37.160396 22626471084160 run.py:483] Algo bellman_ford step 2345 current loss 0.359111, current_train_items 75072.
I0302 18:59:37.176366 22626471084160 run.py:483] Algo bellman_ford step 2346 current loss 0.481899, current_train_items 75104.
I0302 18:59:37.199657 22626471084160 run.py:483] Algo bellman_ford step 2347 current loss 0.637829, current_train_items 75136.
I0302 18:59:37.228409 22626471084160 run.py:483] Algo bellman_ford step 2348 current loss 0.700329, current_train_items 75168.
I0302 18:59:37.260133 22626471084160 run.py:483] Algo bellman_ford step 2349 current loss 0.790775, current_train_items 75200.
I0302 18:59:37.279665 22626471084160 run.py:483] Algo bellman_ford step 2350 current loss 0.264939, current_train_items 75232.
I0302 18:59:37.287769 22626471084160 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.8828125, 'score': 0.8828125, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0302 18:59:37.287875 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.883, val scores are: bellman_ford: 0.883
I0302 18:59:37.304593 22626471084160 run.py:483] Algo bellman_ford step 2351 current loss 0.452117, current_train_items 75264.
I0302 18:59:37.328074 22626471084160 run.py:483] Algo bellman_ford step 2352 current loss 0.674168, current_train_items 75296.
I0302 18:59:37.357318 22626471084160 run.py:483] Algo bellman_ford step 2353 current loss 0.748337, current_train_items 75328.
I0302 18:59:37.392886 22626471084160 run.py:483] Algo bellman_ford step 2354 current loss 1.020344, current_train_items 75360.
I0302 18:59:37.413170 22626471084160 run.py:483] Algo bellman_ford step 2355 current loss 0.388970, current_train_items 75392.
I0302 18:59:37.428731 22626471084160 run.py:483] Algo bellman_ford step 2356 current loss 0.573383, current_train_items 75424.
I0302 18:59:37.452287 22626471084160 run.py:483] Algo bellman_ford step 2357 current loss 0.700977, current_train_items 75456.
I0302 18:59:37.481073 22626471084160 run.py:483] Algo bellman_ford step 2358 current loss 0.669639, current_train_items 75488.
I0302 18:59:37.516170 22626471084160 run.py:483] Algo bellman_ford step 2359 current loss 0.996808, current_train_items 75520.
I0302 18:59:37.536047 22626471084160 run.py:483] Algo bellman_ford step 2360 current loss 0.354530, current_train_items 75552.
I0302 18:59:37.552992 22626471084160 run.py:483] Algo bellman_ford step 2361 current loss 0.611481, current_train_items 75584.
I0302 18:59:37.576964 22626471084160 run.py:483] Algo bellman_ford step 2362 current loss 0.647719, current_train_items 75616.
I0302 18:59:37.606678 22626471084160 run.py:483] Algo bellman_ford step 2363 current loss 0.839618, current_train_items 75648.
I0302 18:59:37.639830 22626471084160 run.py:483] Algo bellman_ford step 2364 current loss 0.952740, current_train_items 75680.
I0302 18:59:37.659207 22626471084160 run.py:483] Algo bellman_ford step 2365 current loss 0.295271, current_train_items 75712.
I0302 18:59:37.675534 22626471084160 run.py:483] Algo bellman_ford step 2366 current loss 0.489360, current_train_items 75744.
I0302 18:59:37.698966 22626471084160 run.py:483] Algo bellman_ford step 2367 current loss 0.778328, current_train_items 75776.
I0302 18:59:37.729822 22626471084160 run.py:483] Algo bellman_ford step 2368 current loss 0.893120, current_train_items 75808.
I0302 18:59:37.763943 22626471084160 run.py:483] Algo bellman_ford step 2369 current loss 1.040210, current_train_items 75840.
I0302 18:59:37.784055 22626471084160 run.py:483] Algo bellman_ford step 2370 current loss 0.306207, current_train_items 75872.
I0302 18:59:37.800194 22626471084160 run.py:483] Algo bellman_ford step 2371 current loss 0.558513, current_train_items 75904.
I0302 18:59:37.823052 22626471084160 run.py:483] Algo bellman_ford step 2372 current loss 0.668341, current_train_items 75936.
I0302 18:59:37.852097 22626471084160 run.py:483] Algo bellman_ford step 2373 current loss 0.922143, current_train_items 75968.
I0302 18:59:37.884555 22626471084160 run.py:483] Algo bellman_ford step 2374 current loss 0.881897, current_train_items 76000.
I0302 18:59:37.904122 22626471084160 run.py:483] Algo bellman_ford step 2375 current loss 0.297801, current_train_items 76032.
I0302 18:59:37.920435 22626471084160 run.py:483] Algo bellman_ford step 2376 current loss 0.570520, current_train_items 76064.
I0302 18:59:37.943448 22626471084160 run.py:483] Algo bellman_ford step 2377 current loss 0.671908, current_train_items 76096.
I0302 18:59:37.974574 22626471084160 run.py:483] Algo bellman_ford step 2378 current loss 0.738563, current_train_items 76128.
I0302 18:59:38.009234 22626471084160 run.py:483] Algo bellman_ford step 2379 current loss 0.968624, current_train_items 76160.
I0302 18:59:38.029053 22626471084160 run.py:483] Algo bellman_ford step 2380 current loss 0.306461, current_train_items 76192.
I0302 18:59:38.045727 22626471084160 run.py:483] Algo bellman_ford step 2381 current loss 0.569134, current_train_items 76224.
I0302 18:59:38.069347 22626471084160 run.py:483] Algo bellman_ford step 2382 current loss 0.710133, current_train_items 76256.
I0302 18:59:38.098479 22626471084160 run.py:483] Algo bellman_ford step 2383 current loss 0.698936, current_train_items 76288.
I0302 18:59:38.132272 22626471084160 run.py:483] Algo bellman_ford step 2384 current loss 0.990538, current_train_items 76320.
I0302 18:59:38.152363 22626471084160 run.py:483] Algo bellman_ford step 2385 current loss 0.324028, current_train_items 76352.
I0302 18:59:38.168309 22626471084160 run.py:483] Algo bellman_ford step 2386 current loss 0.483959, current_train_items 76384.
I0302 18:59:38.190572 22626471084160 run.py:483] Algo bellman_ford step 2387 current loss 0.775878, current_train_items 76416.
I0302 18:59:38.220796 22626471084160 run.py:483] Algo bellman_ford step 2388 current loss 0.807155, current_train_items 76448.
I0302 18:59:38.255867 22626471084160 run.py:483] Algo bellman_ford step 2389 current loss 0.875082, current_train_items 76480.
I0302 18:59:38.275782 22626471084160 run.py:483] Algo bellman_ford step 2390 current loss 0.348436, current_train_items 76512.
I0302 18:59:38.291572 22626471084160 run.py:483] Algo bellman_ford step 2391 current loss 0.518738, current_train_items 76544.
I0302 18:59:38.313248 22626471084160 run.py:483] Algo bellman_ford step 2392 current loss 0.689146, current_train_items 76576.
I0302 18:59:38.343340 22626471084160 run.py:483] Algo bellman_ford step 2393 current loss 0.933417, current_train_items 76608.
I0302 18:59:38.379501 22626471084160 run.py:483] Algo bellman_ford step 2394 current loss 1.166288, current_train_items 76640.
I0302 18:59:38.399030 22626471084160 run.py:483] Algo bellman_ford step 2395 current loss 0.389212, current_train_items 76672.
I0302 18:59:38.415018 22626471084160 run.py:483] Algo bellman_ford step 2396 current loss 0.525783, current_train_items 76704.
I0302 18:59:38.436753 22626471084160 run.py:483] Algo bellman_ford step 2397 current loss 0.640974, current_train_items 76736.
I0302 18:59:38.466407 22626471084160 run.py:483] Algo bellman_ford step 2398 current loss 0.780633, current_train_items 76768.
I0302 18:59:38.500859 22626471084160 run.py:483] Algo bellman_ford step 2399 current loss 0.970114, current_train_items 76800.
I0302 18:59:38.521178 22626471084160 run.py:483] Algo bellman_ford step 2400 current loss 0.271820, current_train_items 76832.
I0302 18:59:38.528907 22626471084160 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0302 18:59:38.529011 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 18:59:38.545460 22626471084160 run.py:483] Algo bellman_ford step 2401 current loss 0.541311, current_train_items 76864.
I0302 18:59:38.569244 22626471084160 run.py:483] Algo bellman_ford step 2402 current loss 0.704598, current_train_items 76896.
I0302 18:59:38.600521 22626471084160 run.py:483] Algo bellman_ford step 2403 current loss 0.868113, current_train_items 76928.
I0302 18:59:38.637354 22626471084160 run.py:483] Algo bellman_ford step 2404 current loss 1.119190, current_train_items 76960.
I0302 18:59:38.657434 22626471084160 run.py:483] Algo bellman_ford step 2405 current loss 0.246381, current_train_items 76992.
I0302 18:59:38.673393 22626471084160 run.py:483] Algo bellman_ford step 2406 current loss 0.581316, current_train_items 77024.
I0302 18:59:38.696752 22626471084160 run.py:483] Algo bellman_ford step 2407 current loss 0.727709, current_train_items 77056.
I0302 18:59:38.725848 22626471084160 run.py:483] Algo bellman_ford step 2408 current loss 0.814523, current_train_items 77088.
I0302 18:59:38.758560 22626471084160 run.py:483] Algo bellman_ford step 2409 current loss 1.030398, current_train_items 77120.
I0302 18:59:38.778026 22626471084160 run.py:483] Algo bellman_ford step 2410 current loss 0.247841, current_train_items 77152.
I0302 18:59:38.794140 22626471084160 run.py:483] Algo bellman_ford step 2411 current loss 0.486542, current_train_items 77184.
I0302 18:59:38.818408 22626471084160 run.py:483] Algo bellman_ford step 2412 current loss 0.878451, current_train_items 77216.
I0302 18:59:38.849555 22626471084160 run.py:483] Algo bellman_ford step 2413 current loss 0.810516, current_train_items 77248.
I0302 18:59:38.884223 22626471084160 run.py:483] Algo bellman_ford step 2414 current loss 1.043795, current_train_items 77280.
I0302 18:59:38.903827 22626471084160 run.py:483] Algo bellman_ford step 2415 current loss 0.347887, current_train_items 77312.
I0302 18:59:38.919423 22626471084160 run.py:483] Algo bellman_ford step 2416 current loss 0.559931, current_train_items 77344.
I0302 18:59:38.942701 22626471084160 run.py:483] Algo bellman_ford step 2417 current loss 0.703822, current_train_items 77376.
I0302 18:59:38.974301 22626471084160 run.py:483] Algo bellman_ford step 2418 current loss 0.887079, current_train_items 77408.
I0302 18:59:39.006062 22626471084160 run.py:483] Algo bellman_ford step 2419 current loss 0.807224, current_train_items 77440.
I0302 18:59:39.025637 22626471084160 run.py:483] Algo bellman_ford step 2420 current loss 0.315883, current_train_items 77472.
I0302 18:59:39.041543 22626471084160 run.py:483] Algo bellman_ford step 2421 current loss 0.563264, current_train_items 77504.
I0302 18:59:39.065391 22626471084160 run.py:483] Algo bellman_ford step 2422 current loss 0.808188, current_train_items 77536.
I0302 18:59:39.095622 22626471084160 run.py:483] Algo bellman_ford step 2423 current loss 0.898879, current_train_items 77568.
I0302 18:59:39.130073 22626471084160 run.py:483] Algo bellman_ford step 2424 current loss 0.850055, current_train_items 77600.
I0302 18:59:39.149993 22626471084160 run.py:483] Algo bellman_ford step 2425 current loss 0.274092, current_train_items 77632.
I0302 18:59:39.165996 22626471084160 run.py:483] Algo bellman_ford step 2426 current loss 0.480768, current_train_items 77664.
I0302 18:59:39.190616 22626471084160 run.py:483] Algo bellman_ford step 2427 current loss 0.820369, current_train_items 77696.
I0302 18:59:39.219691 22626471084160 run.py:483] Algo bellman_ford step 2428 current loss 0.798249, current_train_items 77728.
I0302 18:59:39.252700 22626471084160 run.py:483] Algo bellman_ford step 2429 current loss 0.863480, current_train_items 77760.
I0302 18:59:39.272501 22626471084160 run.py:483] Algo bellman_ford step 2430 current loss 0.438046, current_train_items 77792.
I0302 18:59:39.288925 22626471084160 run.py:483] Algo bellman_ford step 2431 current loss 0.559476, current_train_items 77824.
I0302 18:59:39.312792 22626471084160 run.py:483] Algo bellman_ford step 2432 current loss 0.663094, current_train_items 77856.
I0302 18:59:39.342760 22626471084160 run.py:483] Algo bellman_ford step 2433 current loss 0.826671, current_train_items 77888.
I0302 18:59:39.376679 22626471084160 run.py:483] Algo bellman_ford step 2434 current loss 0.919064, current_train_items 77920.
I0302 18:59:39.396289 22626471084160 run.py:483] Algo bellman_ford step 2435 current loss 0.334756, current_train_items 77952.
I0302 18:59:39.412658 22626471084160 run.py:483] Algo bellman_ford step 2436 current loss 0.621040, current_train_items 77984.
I0302 18:59:39.435945 22626471084160 run.py:483] Algo bellman_ford step 2437 current loss 0.794144, current_train_items 78016.
I0302 18:59:39.464717 22626471084160 run.py:483] Algo bellman_ford step 2438 current loss 0.852896, current_train_items 78048.
I0302 18:59:39.497748 22626471084160 run.py:483] Algo bellman_ford step 2439 current loss 0.796100, current_train_items 78080.
I0302 18:59:39.517679 22626471084160 run.py:483] Algo bellman_ford step 2440 current loss 0.354333, current_train_items 78112.
I0302 18:59:39.533810 22626471084160 run.py:483] Algo bellman_ford step 2441 current loss 0.468762, current_train_items 78144.
I0302 18:59:39.558485 22626471084160 run.py:483] Algo bellman_ford step 2442 current loss 0.902401, current_train_items 78176.
I0302 18:59:39.588573 22626471084160 run.py:483] Algo bellman_ford step 2443 current loss 0.954148, current_train_items 78208.
I0302 18:59:39.623520 22626471084160 run.py:483] Algo bellman_ford step 2444 current loss 0.995143, current_train_items 78240.
I0302 18:59:39.643243 22626471084160 run.py:483] Algo bellman_ford step 2445 current loss 0.368495, current_train_items 78272.
I0302 18:59:39.659466 22626471084160 run.py:483] Algo bellman_ford step 2446 current loss 0.577029, current_train_items 78304.
I0302 18:59:39.683291 22626471084160 run.py:483] Algo bellman_ford step 2447 current loss 0.675132, current_train_items 78336.
I0302 18:59:39.712558 22626471084160 run.py:483] Algo bellman_ford step 2448 current loss 0.796198, current_train_items 78368.
I0302 18:59:39.745596 22626471084160 run.py:483] Algo bellman_ford step 2449 current loss 0.860624, current_train_items 78400.
I0302 18:59:39.765019 22626471084160 run.py:483] Algo bellman_ford step 2450 current loss 0.363637, current_train_items 78432.
I0302 18:59:39.773077 22626471084160 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0302 18:59:39.773193 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 18:59:39.789786 22626471084160 run.py:483] Algo bellman_ford step 2451 current loss 0.463531, current_train_items 78464.
I0302 18:59:39.813873 22626471084160 run.py:483] Algo bellman_ford step 2452 current loss 0.767079, current_train_items 78496.
I0302 18:59:39.844077 22626471084160 run.py:483] Algo bellman_ford step 2453 current loss 0.734044, current_train_items 78528.
I0302 18:59:39.878981 22626471084160 run.py:483] Algo bellman_ford step 2454 current loss 0.839210, current_train_items 78560.
I0302 18:59:39.898986 22626471084160 run.py:483] Algo bellman_ford step 2455 current loss 0.301744, current_train_items 78592.
I0302 18:59:39.914936 22626471084160 run.py:483] Algo bellman_ford step 2456 current loss 0.521933, current_train_items 78624.
I0302 18:59:39.937251 22626471084160 run.py:483] Algo bellman_ford step 2457 current loss 0.628241, current_train_items 78656.
I0302 18:59:39.967485 22626471084160 run.py:483] Algo bellman_ford step 2458 current loss 0.737801, current_train_items 78688.
I0302 18:59:40.002321 22626471084160 run.py:483] Algo bellman_ford step 2459 current loss 0.932206, current_train_items 78720.
I0302 18:59:40.022377 22626471084160 run.py:483] Algo bellman_ford step 2460 current loss 0.307981, current_train_items 78752.
I0302 18:59:40.039064 22626471084160 run.py:483] Algo bellman_ford step 2461 current loss 0.582748, current_train_items 78784.
I0302 18:59:40.062031 22626471084160 run.py:483] Algo bellman_ford step 2462 current loss 0.635877, current_train_items 78816.
I0302 18:59:40.093119 22626471084160 run.py:483] Algo bellman_ford step 2463 current loss 0.877084, current_train_items 78848.
I0302 18:59:40.126888 22626471084160 run.py:483] Algo bellman_ford step 2464 current loss 1.071862, current_train_items 78880.
I0302 18:59:40.146534 22626471084160 run.py:483] Algo bellman_ford step 2465 current loss 0.347778, current_train_items 78912.
I0302 18:59:40.162662 22626471084160 run.py:483] Algo bellman_ford step 2466 current loss 0.645961, current_train_items 78944.
I0302 18:59:40.186089 22626471084160 run.py:483] Algo bellman_ford step 2467 current loss 0.726439, current_train_items 78976.
I0302 18:59:40.217042 22626471084160 run.py:483] Algo bellman_ford step 2468 current loss 0.788615, current_train_items 79008.
I0302 18:59:40.252110 22626471084160 run.py:483] Algo bellman_ford step 2469 current loss 0.964792, current_train_items 79040.
I0302 18:59:40.272291 22626471084160 run.py:483] Algo bellman_ford step 2470 current loss 0.416803, current_train_items 79072.
I0302 18:59:40.288519 22626471084160 run.py:483] Algo bellman_ford step 2471 current loss 0.507013, current_train_items 79104.
I0302 18:59:40.312371 22626471084160 run.py:483] Algo bellman_ford step 2472 current loss 0.898283, current_train_items 79136.
I0302 18:59:40.343859 22626471084160 run.py:483] Algo bellman_ford step 2473 current loss 1.038061, current_train_items 79168.
I0302 18:59:40.376802 22626471084160 run.py:483] Algo bellman_ford step 2474 current loss 0.874357, current_train_items 79200.
I0302 18:59:40.396754 22626471084160 run.py:483] Algo bellman_ford step 2475 current loss 0.305593, current_train_items 79232.
I0302 18:59:40.412951 22626471084160 run.py:483] Algo bellman_ford step 2476 current loss 0.575782, current_train_items 79264.
I0302 18:59:40.434860 22626471084160 run.py:483] Algo bellman_ford step 2477 current loss 0.549960, current_train_items 79296.
I0302 18:59:40.464330 22626471084160 run.py:483] Algo bellman_ford step 2478 current loss 0.903477, current_train_items 79328.
I0302 18:59:40.499098 22626471084160 run.py:483] Algo bellman_ford step 2479 current loss 1.022048, current_train_items 79360.
I0302 18:59:40.518687 22626471084160 run.py:483] Algo bellman_ford step 2480 current loss 0.327344, current_train_items 79392.
I0302 18:59:40.534531 22626471084160 run.py:483] Algo bellman_ford step 2481 current loss 0.594622, current_train_items 79424.
I0302 18:59:40.558944 22626471084160 run.py:483] Algo bellman_ford step 2482 current loss 0.952805, current_train_items 79456.
I0302 18:59:40.589337 22626471084160 run.py:483] Algo bellman_ford step 2483 current loss 0.928000, current_train_items 79488.
I0302 18:59:40.624436 22626471084160 run.py:483] Algo bellman_ford step 2484 current loss 1.096941, current_train_items 79520.
I0302 18:59:40.644528 22626471084160 run.py:483] Algo bellman_ford step 2485 current loss 0.366055, current_train_items 79552.
I0302 18:59:40.660935 22626471084160 run.py:483] Algo bellman_ford step 2486 current loss 0.575894, current_train_items 79584.
I0302 18:59:40.684825 22626471084160 run.py:483] Algo bellman_ford step 2487 current loss 0.791465, current_train_items 79616.
I0302 18:59:40.712667 22626471084160 run.py:483] Algo bellman_ford step 2488 current loss 0.732065, current_train_items 79648.
I0302 18:59:40.746981 22626471084160 run.py:483] Algo bellman_ford step 2489 current loss 0.948276, current_train_items 79680.
I0302 18:59:40.766809 22626471084160 run.py:483] Algo bellman_ford step 2490 current loss 0.301575, current_train_items 79712.
I0302 18:59:40.782929 22626471084160 run.py:483] Algo bellman_ford step 2491 current loss 0.536813, current_train_items 79744.
I0302 18:59:40.805588 22626471084160 run.py:483] Algo bellman_ford step 2492 current loss 0.721378, current_train_items 79776.
I0302 18:59:40.835690 22626471084160 run.py:483] Algo bellman_ford step 2493 current loss 0.899726, current_train_items 79808.
I0302 18:59:40.868312 22626471084160 run.py:483] Algo bellman_ford step 2494 current loss 0.865767, current_train_items 79840.
I0302 18:59:40.888104 22626471084160 run.py:483] Algo bellman_ford step 2495 current loss 0.327310, current_train_items 79872.
I0302 18:59:40.904722 22626471084160 run.py:483] Algo bellman_ford step 2496 current loss 0.472911, current_train_items 79904.
I0302 18:59:40.927603 22626471084160 run.py:483] Algo bellman_ford step 2497 current loss 0.702891, current_train_items 79936.
I0302 18:59:40.956529 22626471084160 run.py:483] Algo bellman_ford step 2498 current loss 0.804486, current_train_items 79968.
I0302 18:59:40.990370 22626471084160 run.py:483] Algo bellman_ford step 2499 current loss 0.957367, current_train_items 80000.
I0302 18:59:41.010315 22626471084160 run.py:483] Algo bellman_ford step 2500 current loss 0.411780, current_train_items 80032.
I0302 18:59:41.018015 22626471084160 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0302 18:59:41.018122 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 18:59:41.034912 22626471084160 run.py:483] Algo bellman_ford step 2501 current loss 0.553822, current_train_items 80064.
I0302 18:59:41.058673 22626471084160 run.py:483] Algo bellman_ford step 2502 current loss 0.715809, current_train_items 80096.
I0302 18:59:41.088082 22626471084160 run.py:483] Algo bellman_ford step 2503 current loss 0.675341, current_train_items 80128.
I0302 18:59:41.122698 22626471084160 run.py:483] Algo bellman_ford step 2504 current loss 0.966320, current_train_items 80160.
I0302 18:59:41.142618 22626471084160 run.py:483] Algo bellman_ford step 2505 current loss 0.318376, current_train_items 80192.
I0302 18:59:41.158616 22626471084160 run.py:483] Algo bellman_ford step 2506 current loss 0.545917, current_train_items 80224.
I0302 18:59:41.182072 22626471084160 run.py:483] Algo bellman_ford step 2507 current loss 0.787742, current_train_items 80256.
I0302 18:59:41.211083 22626471084160 run.py:483] Algo bellman_ford step 2508 current loss 0.834089, current_train_items 80288.
I0302 18:59:41.245529 22626471084160 run.py:483] Algo bellman_ford step 2509 current loss 0.960054, current_train_items 80320.
I0302 18:59:41.264847 22626471084160 run.py:483] Algo bellman_ford step 2510 current loss 0.333466, current_train_items 80352.
I0302 18:59:41.281232 22626471084160 run.py:483] Algo bellman_ford step 2511 current loss 0.596452, current_train_items 80384.
I0302 18:59:41.304244 22626471084160 run.py:483] Algo bellman_ford step 2512 current loss 0.707670, current_train_items 80416.
I0302 18:59:41.333766 22626471084160 run.py:483] Algo bellman_ford step 2513 current loss 0.707351, current_train_items 80448.
I0302 18:59:41.368071 22626471084160 run.py:483] Algo bellman_ford step 2514 current loss 0.944072, current_train_items 80480.
I0302 18:59:41.387417 22626471084160 run.py:483] Algo bellman_ford step 2515 current loss 0.304214, current_train_items 80512.
I0302 18:59:41.403722 22626471084160 run.py:483] Algo bellman_ford step 2516 current loss 0.513337, current_train_items 80544.
I0302 18:59:41.426958 22626471084160 run.py:483] Algo bellman_ford step 2517 current loss 0.807957, current_train_items 80576.
I0302 18:59:41.456120 22626471084160 run.py:483] Algo bellman_ford step 2518 current loss 0.793859, current_train_items 80608.
I0302 18:59:41.490016 22626471084160 run.py:483] Algo bellman_ford step 2519 current loss 0.964430, current_train_items 80640.
I0302 18:59:41.509275 22626471084160 run.py:483] Algo bellman_ford step 2520 current loss 0.334713, current_train_items 80672.
I0302 18:59:41.525451 22626471084160 run.py:483] Algo bellman_ford step 2521 current loss 0.473191, current_train_items 80704.
I0302 18:59:41.547151 22626471084160 run.py:483] Algo bellman_ford step 2522 current loss 0.595771, current_train_items 80736.
I0302 18:59:41.576247 22626471084160 run.py:483] Algo bellman_ford step 2523 current loss 0.737000, current_train_items 80768.
I0302 18:59:41.609076 22626471084160 run.py:483] Algo bellman_ford step 2524 current loss 1.012781, current_train_items 80800.
I0302 18:59:41.628646 22626471084160 run.py:483] Algo bellman_ford step 2525 current loss 0.342544, current_train_items 80832.
I0302 18:59:41.644760 22626471084160 run.py:483] Algo bellman_ford step 2526 current loss 0.567817, current_train_items 80864.
I0302 18:59:41.667889 22626471084160 run.py:483] Algo bellman_ford step 2527 current loss 0.679987, current_train_items 80896.
I0302 18:59:41.697047 22626471084160 run.py:483] Algo bellman_ford step 2528 current loss 0.763130, current_train_items 80928.
I0302 18:59:41.731344 22626471084160 run.py:483] Algo bellman_ford step 2529 current loss 1.032330, current_train_items 80960.
I0302 18:59:41.750618 22626471084160 run.py:483] Algo bellman_ford step 2530 current loss 0.386076, current_train_items 80992.
I0302 18:59:41.766359 22626471084160 run.py:483] Algo bellman_ford step 2531 current loss 0.538740, current_train_items 81024.
I0302 18:59:41.789506 22626471084160 run.py:483] Algo bellman_ford step 2532 current loss 0.860168, current_train_items 81056.
I0302 18:59:41.819140 22626471084160 run.py:483] Algo bellman_ford step 2533 current loss 0.799534, current_train_items 81088.
I0302 18:59:41.851890 22626471084160 run.py:483] Algo bellman_ford step 2534 current loss 0.918739, current_train_items 81120.
I0302 18:59:41.871448 22626471084160 run.py:483] Algo bellman_ford step 2535 current loss 0.340426, current_train_items 81152.
I0302 18:59:41.887247 22626471084160 run.py:483] Algo bellman_ford step 2536 current loss 0.500739, current_train_items 81184.
W0302 18:59:41.903470 22626471084160 samplers.py:155] Increasing hint lengh from 9 to 10
I0302 18:59:48.469196 22626471084160 run.py:483] Algo bellman_ford step 2537 current loss 1.060217, current_train_items 81216.
I0302 18:59:48.501621 22626471084160 run.py:483] Algo bellman_ford step 2538 current loss 1.029368, current_train_items 81248.
I0302 18:59:48.536086 22626471084160 run.py:483] Algo bellman_ford step 2539 current loss 1.153672, current_train_items 81280.
I0302 18:59:48.556301 22626471084160 run.py:483] Algo bellman_ford step 2540 current loss 0.335050, current_train_items 81312.
I0302 18:59:48.572530 22626471084160 run.py:483] Algo bellman_ford step 2541 current loss 0.542308, current_train_items 81344.
I0302 18:59:48.596116 22626471084160 run.py:483] Algo bellman_ford step 2542 current loss 0.821685, current_train_items 81376.
I0302 18:59:48.625648 22626471084160 run.py:483] Algo bellman_ford step 2543 current loss 0.785983, current_train_items 81408.
I0302 18:59:48.659245 22626471084160 run.py:483] Algo bellman_ford step 2544 current loss 0.946666, current_train_items 81440.
I0302 18:59:48.679312 22626471084160 run.py:483] Algo bellman_ford step 2545 current loss 0.351566, current_train_items 81472.
I0302 18:59:48.695424 22626471084160 run.py:483] Algo bellman_ford step 2546 current loss 0.571528, current_train_items 81504.
I0302 18:59:48.719370 22626471084160 run.py:483] Algo bellman_ford step 2547 current loss 0.642205, current_train_items 81536.
I0302 18:59:48.751037 22626471084160 run.py:483] Algo bellman_ford step 2548 current loss 0.835703, current_train_items 81568.
I0302 18:59:48.783704 22626471084160 run.py:483] Algo bellman_ford step 2549 current loss 0.931682, current_train_items 81600.
I0302 18:59:48.803641 22626471084160 run.py:483] Algo bellman_ford step 2550 current loss 0.297209, current_train_items 81632.
I0302 18:59:48.813590 22626471084160 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0302 18:59:48.813698 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:59:48.830684 22626471084160 run.py:483] Algo bellman_ford step 2551 current loss 0.547005, current_train_items 81664.
I0302 18:59:48.854844 22626471084160 run.py:483] Algo bellman_ford step 2552 current loss 0.771449, current_train_items 81696.
I0302 18:59:48.886775 22626471084160 run.py:483] Algo bellman_ford step 2553 current loss 0.810911, current_train_items 81728.
I0302 18:59:48.921123 22626471084160 run.py:483] Algo bellman_ford step 2554 current loss 0.871019, current_train_items 81760.
I0302 18:59:48.941197 22626471084160 run.py:483] Algo bellman_ford step 2555 current loss 0.326910, current_train_items 81792.
I0302 18:59:48.957506 22626471084160 run.py:483] Algo bellman_ford step 2556 current loss 0.664918, current_train_items 81824.
I0302 18:59:48.981621 22626471084160 run.py:483] Algo bellman_ford step 2557 current loss 0.611624, current_train_items 81856.
I0302 18:59:49.012951 22626471084160 run.py:483] Algo bellman_ford step 2558 current loss 0.788328, current_train_items 81888.
I0302 18:59:49.048474 22626471084160 run.py:483] Algo bellman_ford step 2559 current loss 1.024973, current_train_items 81920.
I0302 18:59:49.068382 22626471084160 run.py:483] Algo bellman_ford step 2560 current loss 0.407768, current_train_items 81952.
I0302 18:59:49.084941 22626471084160 run.py:483] Algo bellman_ford step 2561 current loss 0.634932, current_train_items 81984.
I0302 18:59:49.108081 22626471084160 run.py:483] Algo bellman_ford step 2562 current loss 0.654941, current_train_items 82016.
I0302 18:59:49.140494 22626471084160 run.py:483] Algo bellman_ford step 2563 current loss 0.919151, current_train_items 82048.
I0302 18:59:49.174576 22626471084160 run.py:483] Algo bellman_ford step 2564 current loss 0.948531, current_train_items 82080.
I0302 18:59:49.194027 22626471084160 run.py:483] Algo bellman_ford step 2565 current loss 0.338753, current_train_items 82112.
I0302 18:59:49.210455 22626471084160 run.py:483] Algo bellman_ford step 2566 current loss 0.526567, current_train_items 82144.
I0302 18:59:49.233707 22626471084160 run.py:483] Algo bellman_ford step 2567 current loss 0.760507, current_train_items 82176.
I0302 18:59:49.265265 22626471084160 run.py:483] Algo bellman_ford step 2568 current loss 0.773222, current_train_items 82208.
I0302 18:59:49.298875 22626471084160 run.py:483] Algo bellman_ford step 2569 current loss 0.834160, current_train_items 82240.
I0302 18:59:49.318828 22626471084160 run.py:483] Algo bellman_ford step 2570 current loss 0.244973, current_train_items 82272.
I0302 18:59:49.335024 22626471084160 run.py:483] Algo bellman_ford step 2571 current loss 0.541538, current_train_items 82304.
I0302 18:59:49.357262 22626471084160 run.py:483] Algo bellman_ford step 2572 current loss 0.682506, current_train_items 82336.
I0302 18:59:49.387775 22626471084160 run.py:483] Algo bellman_ford step 2573 current loss 0.742393, current_train_items 82368.
I0302 18:59:49.423094 22626471084160 run.py:483] Algo bellman_ford step 2574 current loss 0.970651, current_train_items 82400.
I0302 18:59:49.443166 22626471084160 run.py:483] Algo bellman_ford step 2575 current loss 0.382136, current_train_items 82432.
I0302 18:59:49.459127 22626471084160 run.py:483] Algo bellman_ford step 2576 current loss 0.510095, current_train_items 82464.
I0302 18:59:49.481993 22626471084160 run.py:483] Algo bellman_ford step 2577 current loss 0.730429, current_train_items 82496.
I0302 18:59:49.513680 22626471084160 run.py:483] Algo bellman_ford step 2578 current loss 0.877251, current_train_items 82528.
I0302 18:59:49.546100 22626471084160 run.py:483] Algo bellman_ford step 2579 current loss 1.335088, current_train_items 82560.
I0302 18:59:49.566028 22626471084160 run.py:483] Algo bellman_ford step 2580 current loss 0.382369, current_train_items 82592.
I0302 18:59:49.582377 22626471084160 run.py:483] Algo bellman_ford step 2581 current loss 0.491745, current_train_items 82624.
I0302 18:59:49.605556 22626471084160 run.py:483] Algo bellman_ford step 2582 current loss 0.783127, current_train_items 82656.
I0302 18:59:49.635894 22626471084160 run.py:483] Algo bellman_ford step 2583 current loss 0.772846, current_train_items 82688.
I0302 18:59:49.670420 22626471084160 run.py:483] Algo bellman_ford step 2584 current loss 0.919889, current_train_items 82720.
I0302 18:59:49.690360 22626471084160 run.py:483] Algo bellman_ford step 2585 current loss 0.356929, current_train_items 82752.
I0302 18:59:49.706602 22626471084160 run.py:483] Algo bellman_ford step 2586 current loss 0.598423, current_train_items 82784.
I0302 18:59:49.729745 22626471084160 run.py:483] Algo bellman_ford step 2587 current loss 0.698208, current_train_items 82816.
I0302 18:59:49.760181 22626471084160 run.py:483] Algo bellman_ford step 2588 current loss 0.729152, current_train_items 82848.
I0302 18:59:49.794632 22626471084160 run.py:483] Algo bellman_ford step 2589 current loss 1.021295, current_train_items 82880.
I0302 18:59:49.814666 22626471084160 run.py:483] Algo bellman_ford step 2590 current loss 0.284145, current_train_items 82912.
I0302 18:59:49.831140 22626471084160 run.py:483] Algo bellman_ford step 2591 current loss 0.501902, current_train_items 82944.
I0302 18:59:49.854943 22626471084160 run.py:483] Algo bellman_ford step 2592 current loss 0.836867, current_train_items 82976.
I0302 18:59:49.886650 22626471084160 run.py:483] Algo bellman_ford step 2593 current loss 0.922740, current_train_items 83008.
I0302 18:59:49.918777 22626471084160 run.py:483] Algo bellman_ford step 2594 current loss 1.085919, current_train_items 83040.
I0302 18:59:49.938366 22626471084160 run.py:483] Algo bellman_ford step 2595 current loss 0.420624, current_train_items 83072.
I0302 18:59:49.954939 22626471084160 run.py:483] Algo bellman_ford step 2596 current loss 0.583344, current_train_items 83104.
I0302 18:59:49.979437 22626471084160 run.py:483] Algo bellman_ford step 2597 current loss 0.721410, current_train_items 83136.
I0302 18:59:50.010295 22626471084160 run.py:483] Algo bellman_ford step 2598 current loss 0.723983, current_train_items 83168.
I0302 18:59:50.043551 22626471084160 run.py:483] Algo bellman_ford step 2599 current loss 0.880041, current_train_items 83200.
I0302 18:59:50.063533 22626471084160 run.py:483] Algo bellman_ford step 2600 current loss 0.447825, current_train_items 83232.
I0302 18:59:50.071437 22626471084160 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0302 18:59:50.071544 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 18:59:50.088332 22626471084160 run.py:483] Algo bellman_ford step 2601 current loss 0.494282, current_train_items 83264.
I0302 18:59:50.112009 22626471084160 run.py:483] Algo bellman_ford step 2602 current loss 0.593413, current_train_items 83296.
I0302 18:59:50.141338 22626471084160 run.py:483] Algo bellman_ford step 2603 current loss 0.656908, current_train_items 83328.
I0302 18:59:50.175021 22626471084160 run.py:483] Algo bellman_ford step 2604 current loss 0.957358, current_train_items 83360.
I0302 18:59:50.195076 22626471084160 run.py:483] Algo bellman_ford step 2605 current loss 0.340973, current_train_items 83392.
I0302 18:59:50.210932 22626471084160 run.py:483] Algo bellman_ford step 2606 current loss 0.492404, current_train_items 83424.
I0302 18:59:50.234852 22626471084160 run.py:483] Algo bellman_ford step 2607 current loss 0.659774, current_train_items 83456.
I0302 18:59:50.266119 22626471084160 run.py:483] Algo bellman_ford step 2608 current loss 0.777412, current_train_items 83488.
I0302 18:59:50.300270 22626471084160 run.py:483] Algo bellman_ford step 2609 current loss 0.898588, current_train_items 83520.
I0302 18:59:50.320150 22626471084160 run.py:483] Algo bellman_ford step 2610 current loss 0.338606, current_train_items 83552.
I0302 18:59:50.336302 22626471084160 run.py:483] Algo bellman_ford step 2611 current loss 0.490227, current_train_items 83584.
I0302 18:59:50.359466 22626471084160 run.py:483] Algo bellman_ford step 2612 current loss 0.661050, current_train_items 83616.
I0302 18:59:50.390072 22626471084160 run.py:483] Algo bellman_ford step 2613 current loss 0.767677, current_train_items 83648.
I0302 18:59:50.422399 22626471084160 run.py:483] Algo bellman_ford step 2614 current loss 0.880909, current_train_items 83680.
I0302 18:59:50.441982 22626471084160 run.py:483] Algo bellman_ford step 2615 current loss 0.361485, current_train_items 83712.
I0302 18:59:50.457827 22626471084160 run.py:483] Algo bellman_ford step 2616 current loss 0.490552, current_train_items 83744.
I0302 18:59:50.482203 22626471084160 run.py:483] Algo bellman_ford step 2617 current loss 0.737891, current_train_items 83776.
I0302 18:59:50.513620 22626471084160 run.py:483] Algo bellman_ford step 2618 current loss 0.842441, current_train_items 83808.
I0302 18:59:50.545798 22626471084160 run.py:483] Algo bellman_ford step 2619 current loss 0.814493, current_train_items 83840.
I0302 18:59:50.565608 22626471084160 run.py:483] Algo bellman_ford step 2620 current loss 0.355036, current_train_items 83872.
I0302 18:59:50.581722 22626471084160 run.py:483] Algo bellman_ford step 2621 current loss 0.581452, current_train_items 83904.
I0302 18:59:50.605593 22626471084160 run.py:483] Algo bellman_ford step 2622 current loss 0.804017, current_train_items 83936.
I0302 18:59:50.635853 22626471084160 run.py:483] Algo bellman_ford step 2623 current loss 0.859867, current_train_items 83968.
I0302 18:59:50.668424 22626471084160 run.py:483] Algo bellman_ford step 2624 current loss 0.892283, current_train_items 84000.
I0302 18:59:50.688024 22626471084160 run.py:483] Algo bellman_ford step 2625 current loss 0.322294, current_train_items 84032.
I0302 18:59:50.704133 22626471084160 run.py:483] Algo bellman_ford step 2626 current loss 0.494501, current_train_items 84064.
I0302 18:59:50.727680 22626471084160 run.py:483] Algo bellman_ford step 2627 current loss 0.713668, current_train_items 84096.
I0302 18:59:50.758486 22626471084160 run.py:483] Algo bellman_ford step 2628 current loss 0.819995, current_train_items 84128.
I0302 18:59:50.791981 22626471084160 run.py:483] Algo bellman_ford step 2629 current loss 1.014178, current_train_items 84160.
I0302 18:59:50.811460 22626471084160 run.py:483] Algo bellman_ford step 2630 current loss 0.267047, current_train_items 84192.
I0302 18:59:50.827664 22626471084160 run.py:483] Algo bellman_ford step 2631 current loss 0.524990, current_train_items 84224.
I0302 18:59:50.850589 22626471084160 run.py:483] Algo bellman_ford step 2632 current loss 0.792253, current_train_items 84256.
I0302 18:59:50.880470 22626471084160 run.py:483] Algo bellman_ford step 2633 current loss 0.721181, current_train_items 84288.
I0302 18:59:50.916237 22626471084160 run.py:483] Algo bellman_ford step 2634 current loss 1.041641, current_train_items 84320.
I0302 18:59:50.935907 22626471084160 run.py:483] Algo bellman_ford step 2635 current loss 0.226848, current_train_items 84352.
I0302 18:59:50.951920 22626471084160 run.py:483] Algo bellman_ford step 2636 current loss 0.572271, current_train_items 84384.
I0302 18:59:50.975798 22626471084160 run.py:483] Algo bellman_ford step 2637 current loss 0.755486, current_train_items 84416.
I0302 18:59:51.007482 22626471084160 run.py:483] Algo bellman_ford step 2638 current loss 0.841461, current_train_items 84448.
I0302 18:59:51.041391 22626471084160 run.py:483] Algo bellman_ford step 2639 current loss 0.914258, current_train_items 84480.
I0302 18:59:51.060838 22626471084160 run.py:483] Algo bellman_ford step 2640 current loss 0.390209, current_train_items 84512.
I0302 18:59:51.076963 22626471084160 run.py:483] Algo bellman_ford step 2641 current loss 0.534178, current_train_items 84544.
I0302 18:59:51.099560 22626471084160 run.py:483] Algo bellman_ford step 2642 current loss 0.715411, current_train_items 84576.
I0302 18:59:51.129854 22626471084160 run.py:483] Algo bellman_ford step 2643 current loss 0.751155, current_train_items 84608.
I0302 18:59:51.164750 22626471084160 run.py:483] Algo bellman_ford step 2644 current loss 1.034956, current_train_items 84640.
I0302 18:59:51.184190 22626471084160 run.py:483] Algo bellman_ford step 2645 current loss 0.365046, current_train_items 84672.
I0302 18:59:51.200261 22626471084160 run.py:483] Algo bellman_ford step 2646 current loss 0.533080, current_train_items 84704.
I0302 18:59:51.223726 22626471084160 run.py:483] Algo bellman_ford step 2647 current loss 0.738278, current_train_items 84736.
I0302 18:59:51.254899 22626471084160 run.py:483] Algo bellman_ford step 2648 current loss 0.748016, current_train_items 84768.
I0302 18:59:51.290526 22626471084160 run.py:483] Algo bellman_ford step 2649 current loss 1.041343, current_train_items 84800.
I0302 18:59:51.310396 22626471084160 run.py:483] Algo bellman_ford step 2650 current loss 0.314943, current_train_items 84832.
I0302 18:59:51.318659 22626471084160 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.8916015625, 'score': 0.8916015625, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0302 18:59:51.318765 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.892, val scores are: bellman_ford: 0.892
I0302 18:59:51.335216 22626471084160 run.py:483] Algo bellman_ford step 2651 current loss 0.494762, current_train_items 84864.
I0302 18:59:51.358643 22626471084160 run.py:483] Algo bellman_ford step 2652 current loss 0.705970, current_train_items 84896.
I0302 18:59:51.390285 22626471084160 run.py:483] Algo bellman_ford step 2653 current loss 0.860014, current_train_items 84928.
I0302 18:59:51.424605 22626471084160 run.py:483] Algo bellman_ford step 2654 current loss 0.885106, current_train_items 84960.
I0302 18:59:51.444665 22626471084160 run.py:483] Algo bellman_ford step 2655 current loss 0.295817, current_train_items 84992.
I0302 18:59:51.460703 22626471084160 run.py:483] Algo bellman_ford step 2656 current loss 0.533659, current_train_items 85024.
I0302 18:59:51.484109 22626471084160 run.py:483] Algo bellman_ford step 2657 current loss 0.636331, current_train_items 85056.
I0302 18:59:51.513937 22626471084160 run.py:483] Algo bellman_ford step 2658 current loss 0.771766, current_train_items 85088.
I0302 18:59:51.549737 22626471084160 run.py:483] Algo bellman_ford step 2659 current loss 0.922744, current_train_items 85120.
I0302 18:59:51.569987 22626471084160 run.py:483] Algo bellman_ford step 2660 current loss 0.441662, current_train_items 85152.
I0302 18:59:51.586863 22626471084160 run.py:483] Algo bellman_ford step 2661 current loss 0.543550, current_train_items 85184.
I0302 18:59:51.610136 22626471084160 run.py:483] Algo bellman_ford step 2662 current loss 0.798059, current_train_items 85216.
I0302 18:59:51.641092 22626471084160 run.py:483] Algo bellman_ford step 2663 current loss 0.881084, current_train_items 85248.
I0302 18:59:51.677523 22626471084160 run.py:483] Algo bellman_ford step 2664 current loss 0.922854, current_train_items 85280.
I0302 18:59:51.696993 22626471084160 run.py:483] Algo bellman_ford step 2665 current loss 0.262715, current_train_items 85312.
I0302 18:59:51.713291 22626471084160 run.py:483] Algo bellman_ford step 2666 current loss 0.530334, current_train_items 85344.
I0302 18:59:51.737656 22626471084160 run.py:483] Algo bellman_ford step 2667 current loss 0.648139, current_train_items 85376.
I0302 18:59:51.768913 22626471084160 run.py:483] Algo bellman_ford step 2668 current loss 0.796977, current_train_items 85408.
I0302 18:59:51.801739 22626471084160 run.py:483] Algo bellman_ford step 2669 current loss 0.925945, current_train_items 85440.
I0302 18:59:51.821521 22626471084160 run.py:483] Algo bellman_ford step 2670 current loss 0.355182, current_train_items 85472.
I0302 18:59:51.837925 22626471084160 run.py:483] Algo bellman_ford step 2671 current loss 0.523537, current_train_items 85504.
I0302 18:59:51.861340 22626471084160 run.py:483] Algo bellman_ford step 2672 current loss 0.672064, current_train_items 85536.
I0302 18:59:51.893268 22626471084160 run.py:483] Algo bellman_ford step 2673 current loss 0.734375, current_train_items 85568.
I0302 18:59:51.928607 22626471084160 run.py:483] Algo bellman_ford step 2674 current loss 0.836641, current_train_items 85600.
I0302 18:59:51.948529 22626471084160 run.py:483] Algo bellman_ford step 2675 current loss 0.289921, current_train_items 85632.
I0302 18:59:51.964703 22626471084160 run.py:483] Algo bellman_ford step 2676 current loss 0.547408, current_train_items 85664.
I0302 18:59:51.989000 22626471084160 run.py:483] Algo bellman_ford step 2677 current loss 0.712064, current_train_items 85696.
I0302 18:59:52.018331 22626471084160 run.py:483] Algo bellman_ford step 2678 current loss 0.705518, current_train_items 85728.
I0302 18:59:52.050619 22626471084160 run.py:483] Algo bellman_ford step 2679 current loss 0.811092, current_train_items 85760.
I0302 18:59:52.070380 22626471084160 run.py:483] Algo bellman_ford step 2680 current loss 0.338301, current_train_items 85792.
I0302 18:59:52.086630 22626471084160 run.py:483] Algo bellman_ford step 2681 current loss 0.535071, current_train_items 85824.
I0302 18:59:52.110384 22626471084160 run.py:483] Algo bellman_ford step 2682 current loss 0.753631, current_train_items 85856.
I0302 18:59:52.142136 22626471084160 run.py:483] Algo bellman_ford step 2683 current loss 0.811978, current_train_items 85888.
I0302 18:59:52.176444 22626471084160 run.py:483] Algo bellman_ford step 2684 current loss 0.880537, current_train_items 85920.
I0302 18:59:52.196301 22626471084160 run.py:483] Algo bellman_ford step 2685 current loss 0.267837, current_train_items 85952.
I0302 18:59:52.212385 22626471084160 run.py:483] Algo bellman_ford step 2686 current loss 0.524013, current_train_items 85984.
I0302 18:59:52.236053 22626471084160 run.py:483] Algo bellman_ford step 2687 current loss 0.788081, current_train_items 86016.
I0302 18:59:52.266884 22626471084160 run.py:483] Algo bellman_ford step 2688 current loss 0.799498, current_train_items 86048.
I0302 18:59:52.299201 22626471084160 run.py:483] Algo bellman_ford step 2689 current loss 0.781245, current_train_items 86080.
I0302 18:59:52.319432 22626471084160 run.py:483] Algo bellman_ford step 2690 current loss 0.297880, current_train_items 86112.
I0302 18:59:52.335703 22626471084160 run.py:483] Algo bellman_ford step 2691 current loss 0.583763, current_train_items 86144.
I0302 18:59:52.360205 22626471084160 run.py:483] Algo bellman_ford step 2692 current loss 0.798709, current_train_items 86176.
I0302 18:59:52.392238 22626471084160 run.py:483] Algo bellman_ford step 2693 current loss 0.858784, current_train_items 86208.
I0302 18:59:52.426465 22626471084160 run.py:483] Algo bellman_ford step 2694 current loss 1.007104, current_train_items 86240.
I0302 18:59:52.446350 22626471084160 run.py:483] Algo bellman_ford step 2695 current loss 0.356732, current_train_items 86272.
I0302 18:59:52.462375 22626471084160 run.py:483] Algo bellman_ford step 2696 current loss 0.523264, current_train_items 86304.
I0302 18:59:52.486391 22626471084160 run.py:483] Algo bellman_ford step 2697 current loss 0.800561, current_train_items 86336.
I0302 18:59:52.517034 22626471084160 run.py:483] Algo bellman_ford step 2698 current loss 0.741088, current_train_items 86368.
I0302 18:59:52.549091 22626471084160 run.py:483] Algo bellman_ford step 2699 current loss 1.009094, current_train_items 86400.
I0302 18:59:52.568934 22626471084160 run.py:483] Algo bellman_ford step 2700 current loss 0.368201, current_train_items 86432.
I0302 18:59:52.576565 22626471084160 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0302 18:59:52.576672 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 18:59:52.593501 22626471084160 run.py:483] Algo bellman_ford step 2701 current loss 0.519773, current_train_items 86464.
I0302 18:59:52.617526 22626471084160 run.py:483] Algo bellman_ford step 2702 current loss 0.631251, current_train_items 86496.
I0302 18:59:52.650806 22626471084160 run.py:483] Algo bellman_ford step 2703 current loss 0.854282, current_train_items 86528.
I0302 18:59:52.687052 22626471084160 run.py:483] Algo bellman_ford step 2704 current loss 0.934200, current_train_items 86560.
I0302 18:59:52.707007 22626471084160 run.py:483] Algo bellman_ford step 2705 current loss 0.350743, current_train_items 86592.
I0302 18:59:52.722806 22626471084160 run.py:483] Algo bellman_ford step 2706 current loss 0.595501, current_train_items 86624.
I0302 18:59:52.746730 22626471084160 run.py:483] Algo bellman_ford step 2707 current loss 0.687387, current_train_items 86656.
I0302 18:59:52.776003 22626471084160 run.py:483] Algo bellman_ford step 2708 current loss 0.716221, current_train_items 86688.
I0302 18:59:52.808322 22626471084160 run.py:483] Algo bellman_ford step 2709 current loss 0.776977, current_train_items 86720.
I0302 18:59:52.828314 22626471084160 run.py:483] Algo bellman_ford step 2710 current loss 0.328268, current_train_items 86752.
I0302 18:59:52.844845 22626471084160 run.py:483] Algo bellman_ford step 2711 current loss 0.442003, current_train_items 86784.
I0302 18:59:52.868990 22626471084160 run.py:483] Algo bellman_ford step 2712 current loss 0.697107, current_train_items 86816.
I0302 18:59:52.900434 22626471084160 run.py:483] Algo bellman_ford step 2713 current loss 0.817664, current_train_items 86848.
I0302 18:59:52.936582 22626471084160 run.py:483] Algo bellman_ford step 2714 current loss 0.966861, current_train_items 86880.
I0302 18:59:52.956339 22626471084160 run.py:483] Algo bellman_ford step 2715 current loss 0.329509, current_train_items 86912.
I0302 18:59:52.972098 22626471084160 run.py:483] Algo bellman_ford step 2716 current loss 0.415194, current_train_items 86944.
I0302 18:59:52.996706 22626471084160 run.py:483] Algo bellman_ford step 2717 current loss 0.841808, current_train_items 86976.
I0302 18:59:53.026102 22626471084160 run.py:483] Algo bellman_ford step 2718 current loss 0.863241, current_train_items 87008.
I0302 18:59:53.059841 22626471084160 run.py:483] Algo bellman_ford step 2719 current loss 0.853928, current_train_items 87040.
I0302 18:59:53.079515 22626471084160 run.py:483] Algo bellman_ford step 2720 current loss 0.323139, current_train_items 87072.
I0302 18:59:53.095677 22626471084160 run.py:483] Algo bellman_ford step 2721 current loss 0.818090, current_train_items 87104.
I0302 18:59:53.119183 22626471084160 run.py:483] Algo bellman_ford step 2722 current loss 0.944093, current_train_items 87136.
I0302 18:59:53.151060 22626471084160 run.py:483] Algo bellman_ford step 2723 current loss 0.896806, current_train_items 87168.
I0302 18:59:53.184745 22626471084160 run.py:483] Algo bellman_ford step 2724 current loss 0.986713, current_train_items 87200.
I0302 18:59:53.204524 22626471084160 run.py:483] Algo bellman_ford step 2725 current loss 0.329474, current_train_items 87232.
I0302 18:59:53.220563 22626471084160 run.py:483] Algo bellman_ford step 2726 current loss 0.622430, current_train_items 87264.
I0302 18:59:53.244432 22626471084160 run.py:483] Algo bellman_ford step 2727 current loss 0.731319, current_train_items 87296.
I0302 18:59:53.275348 22626471084160 run.py:483] Algo bellman_ford step 2728 current loss 0.790199, current_train_items 87328.
I0302 18:59:53.308781 22626471084160 run.py:483] Algo bellman_ford step 2729 current loss 0.894051, current_train_items 87360.
I0302 18:59:53.328366 22626471084160 run.py:483] Algo bellman_ford step 2730 current loss 0.347316, current_train_items 87392.
I0302 18:59:53.344342 22626471084160 run.py:483] Algo bellman_ford step 2731 current loss 0.451962, current_train_items 87424.
I0302 18:59:53.366801 22626471084160 run.py:483] Algo bellman_ford step 2732 current loss 0.699960, current_train_items 87456.
I0302 18:59:53.396428 22626471084160 run.py:483] Algo bellman_ford step 2733 current loss 0.759480, current_train_items 87488.
I0302 18:59:53.430366 22626471084160 run.py:483] Algo bellman_ford step 2734 current loss 0.852882, current_train_items 87520.
I0302 18:59:53.450347 22626471084160 run.py:483] Algo bellman_ford step 2735 current loss 0.375868, current_train_items 87552.
I0302 18:59:53.466083 22626471084160 run.py:483] Algo bellman_ford step 2736 current loss 0.461563, current_train_items 87584.
I0302 18:59:53.489570 22626471084160 run.py:483] Algo bellman_ford step 2737 current loss 0.678039, current_train_items 87616.
I0302 18:59:53.519371 22626471084160 run.py:483] Algo bellman_ford step 2738 current loss 0.625439, current_train_items 87648.
I0302 18:59:53.552252 22626471084160 run.py:483] Algo bellman_ford step 2739 current loss 0.846618, current_train_items 87680.
I0302 18:59:53.571812 22626471084160 run.py:483] Algo bellman_ford step 2740 current loss 0.342224, current_train_items 87712.
I0302 18:59:53.587780 22626471084160 run.py:483] Algo bellman_ford step 2741 current loss 0.518511, current_train_items 87744.
I0302 18:59:53.611410 22626471084160 run.py:483] Algo bellman_ford step 2742 current loss 0.775546, current_train_items 87776.
I0302 18:59:53.643530 22626471084160 run.py:483] Algo bellman_ford step 2743 current loss 0.791171, current_train_items 87808.
I0302 18:59:53.676681 22626471084160 run.py:483] Algo bellman_ford step 2744 current loss 0.921241, current_train_items 87840.
I0302 18:59:53.696317 22626471084160 run.py:483] Algo bellman_ford step 2745 current loss 0.309842, current_train_items 87872.
I0302 18:59:53.712290 22626471084160 run.py:483] Algo bellman_ford step 2746 current loss 0.534545, current_train_items 87904.
I0302 18:59:53.735718 22626471084160 run.py:483] Algo bellman_ford step 2747 current loss 0.745864, current_train_items 87936.
I0302 18:59:53.767206 22626471084160 run.py:483] Algo bellman_ford step 2748 current loss 0.835918, current_train_items 87968.
I0302 18:59:53.800347 22626471084160 run.py:483] Algo bellman_ford step 2749 current loss 0.951836, current_train_items 88000.
I0302 18:59:53.819979 22626471084160 run.py:483] Algo bellman_ford step 2750 current loss 0.345225, current_train_items 88032.
I0302 18:59:53.827888 22626471084160 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0302 18:59:53.827996 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 18:59:53.844911 22626471084160 run.py:483] Algo bellman_ford step 2751 current loss 0.561432, current_train_items 88064.
I0302 18:59:53.868958 22626471084160 run.py:483] Algo bellman_ford step 2752 current loss 0.819838, current_train_items 88096.
I0302 18:59:53.900800 22626471084160 run.py:483] Algo bellman_ford step 2753 current loss 0.831223, current_train_items 88128.
I0302 18:59:53.933897 22626471084160 run.py:483] Algo bellman_ford step 2754 current loss 0.859056, current_train_items 88160.
I0302 18:59:53.954013 22626471084160 run.py:483] Algo bellman_ford step 2755 current loss 0.316986, current_train_items 88192.
I0302 18:59:53.970246 22626471084160 run.py:483] Algo bellman_ford step 2756 current loss 0.546508, current_train_items 88224.
I0302 18:59:53.993373 22626471084160 run.py:483] Algo bellman_ford step 2757 current loss 0.689674, current_train_items 88256.
I0302 18:59:54.025032 22626471084160 run.py:483] Algo bellman_ford step 2758 current loss 0.813162, current_train_items 88288.
I0302 18:59:54.060943 22626471084160 run.py:483] Algo bellman_ford step 2759 current loss 1.035039, current_train_items 88320.
I0302 18:59:54.080719 22626471084160 run.py:483] Algo bellman_ford step 2760 current loss 0.342481, current_train_items 88352.
I0302 18:59:54.097414 22626471084160 run.py:483] Algo bellman_ford step 2761 current loss 0.544763, current_train_items 88384.
I0302 18:59:54.121041 22626471084160 run.py:483] Algo bellman_ford step 2762 current loss 0.840016, current_train_items 88416.
I0302 18:59:54.152149 22626471084160 run.py:483] Algo bellman_ford step 2763 current loss 0.752639, current_train_items 88448.
I0302 18:59:54.185601 22626471084160 run.py:483] Algo bellman_ford step 2764 current loss 0.951982, current_train_items 88480.
I0302 18:59:54.205412 22626471084160 run.py:483] Algo bellman_ford step 2765 current loss 0.349927, current_train_items 88512.
I0302 18:59:54.221814 22626471084160 run.py:483] Algo bellman_ford step 2766 current loss 0.573260, current_train_items 88544.
I0302 18:59:54.245522 22626471084160 run.py:483] Algo bellman_ford step 2767 current loss 0.841450, current_train_items 88576.
I0302 18:59:54.277309 22626471084160 run.py:483] Algo bellman_ford step 2768 current loss 0.983293, current_train_items 88608.
I0302 18:59:54.310900 22626471084160 run.py:483] Algo bellman_ford step 2769 current loss 1.107027, current_train_items 88640.
I0302 18:59:54.330883 22626471084160 run.py:483] Algo bellman_ford step 2770 current loss 0.277013, current_train_items 88672.
I0302 18:59:54.347268 22626471084160 run.py:483] Algo bellman_ford step 2771 current loss 0.540295, current_train_items 88704.
I0302 18:59:54.371165 22626471084160 run.py:483] Algo bellman_ford step 2772 current loss 0.794815, current_train_items 88736.
I0302 18:59:54.401570 22626471084160 run.py:483] Algo bellman_ford step 2773 current loss 0.796675, current_train_items 88768.
I0302 18:59:54.435222 22626471084160 run.py:483] Algo bellman_ford step 2774 current loss 0.874218, current_train_items 88800.
I0302 18:59:54.455200 22626471084160 run.py:483] Algo bellman_ford step 2775 current loss 0.280085, current_train_items 88832.
I0302 18:59:54.471863 22626471084160 run.py:483] Algo bellman_ford step 2776 current loss 0.669236, current_train_items 88864.
I0302 18:59:54.495074 22626471084160 run.py:483] Algo bellman_ford step 2777 current loss 0.909951, current_train_items 88896.
I0302 18:59:54.525821 22626471084160 run.py:483] Algo bellman_ford step 2778 current loss 0.938542, current_train_items 88928.
I0302 18:59:54.560029 22626471084160 run.py:483] Algo bellman_ford step 2779 current loss 1.233522, current_train_items 88960.
I0302 18:59:54.579729 22626471084160 run.py:483] Algo bellman_ford step 2780 current loss 0.372481, current_train_items 88992.
I0302 18:59:54.595763 22626471084160 run.py:483] Algo bellman_ford step 2781 current loss 0.697295, current_train_items 89024.
I0302 18:59:54.619770 22626471084160 run.py:483] Algo bellman_ford step 2782 current loss 0.744771, current_train_items 89056.
I0302 18:59:54.650368 22626471084160 run.py:483] Algo bellman_ford step 2783 current loss 0.739861, current_train_items 89088.
I0302 18:59:54.684663 22626471084160 run.py:483] Algo bellman_ford step 2784 current loss 0.848717, current_train_items 89120.
I0302 18:59:54.704798 22626471084160 run.py:483] Algo bellman_ford step 2785 current loss 0.424183, current_train_items 89152.
I0302 18:59:54.720834 22626471084160 run.py:483] Algo bellman_ford step 2786 current loss 0.575429, current_train_items 89184.
I0302 18:59:54.743263 22626471084160 run.py:483] Algo bellman_ford step 2787 current loss 0.646875, current_train_items 89216.
I0302 18:59:54.773416 22626471084160 run.py:483] Algo bellman_ford step 2788 current loss 0.840245, current_train_items 89248.
I0302 18:59:54.804280 22626471084160 run.py:483] Algo bellman_ford step 2789 current loss 0.839978, current_train_items 89280.
I0302 18:59:54.823985 22626471084160 run.py:483] Algo bellman_ford step 2790 current loss 0.399887, current_train_items 89312.
I0302 18:59:54.840095 22626471084160 run.py:483] Algo bellman_ford step 2791 current loss 0.544482, current_train_items 89344.
I0302 18:59:54.863487 22626471084160 run.py:483] Algo bellman_ford step 2792 current loss 0.610128, current_train_items 89376.
I0302 18:59:54.893451 22626471084160 run.py:483] Algo bellman_ford step 2793 current loss 0.772320, current_train_items 89408.
I0302 18:59:54.929184 22626471084160 run.py:483] Algo bellman_ford step 2794 current loss 0.899820, current_train_items 89440.
I0302 18:59:54.949100 22626471084160 run.py:483] Algo bellman_ford step 2795 current loss 0.346417, current_train_items 89472.
I0302 18:59:54.965769 22626471084160 run.py:483] Algo bellman_ford step 2796 current loss 0.527152, current_train_items 89504.
I0302 18:59:54.988776 22626471084160 run.py:483] Algo bellman_ford step 2797 current loss 0.799203, current_train_items 89536.
I0302 18:59:55.020191 22626471084160 run.py:483] Algo bellman_ford step 2798 current loss 0.772277, current_train_items 89568.
I0302 18:59:55.053670 22626471084160 run.py:483] Algo bellman_ford step 2799 current loss 0.839747, current_train_items 89600.
I0302 18:59:55.073658 22626471084160 run.py:483] Algo bellman_ford step 2800 current loss 0.338454, current_train_items 89632.
I0302 18:59:55.081432 22626471084160 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0302 18:59:55.081537 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 18:59:55.098307 22626471084160 run.py:483] Algo bellman_ford step 2801 current loss 0.510789, current_train_items 89664.
I0302 18:59:55.122775 22626471084160 run.py:483] Algo bellman_ford step 2802 current loss 0.749757, current_train_items 89696.
I0302 18:59:55.154414 22626471084160 run.py:483] Algo bellman_ford step 2803 current loss 0.800181, current_train_items 89728.
I0302 18:59:55.188479 22626471084160 run.py:483] Algo bellman_ford step 2804 current loss 0.886831, current_train_items 89760.
I0302 18:59:55.208688 22626471084160 run.py:483] Algo bellman_ford step 2805 current loss 0.291925, current_train_items 89792.
I0302 18:59:55.224956 22626471084160 run.py:483] Algo bellman_ford step 2806 current loss 0.630106, current_train_items 89824.
I0302 18:59:55.247808 22626471084160 run.py:483] Algo bellman_ford step 2807 current loss 0.668522, current_train_items 89856.
I0302 18:59:55.279527 22626471084160 run.py:483] Algo bellman_ford step 2808 current loss 0.896549, current_train_items 89888.
I0302 18:59:55.313817 22626471084160 run.py:483] Algo bellman_ford step 2809 current loss 0.907000, current_train_items 89920.
I0302 18:59:55.333765 22626471084160 run.py:483] Algo bellman_ford step 2810 current loss 0.311472, current_train_items 89952.
I0302 18:59:55.350281 22626471084160 run.py:483] Algo bellman_ford step 2811 current loss 0.643805, current_train_items 89984.
I0302 18:59:55.372804 22626471084160 run.py:483] Algo bellman_ford step 2812 current loss 0.735942, current_train_items 90016.
I0302 18:59:55.404419 22626471084160 run.py:483] Algo bellman_ford step 2813 current loss 0.914473, current_train_items 90048.
I0302 18:59:55.437495 22626471084160 run.py:483] Algo bellman_ford step 2814 current loss 0.816960, current_train_items 90080.
I0302 18:59:55.456910 22626471084160 run.py:483] Algo bellman_ford step 2815 current loss 0.329221, current_train_items 90112.
I0302 18:59:55.473389 22626471084160 run.py:483] Algo bellman_ford step 2816 current loss 0.484636, current_train_items 90144.
I0302 18:59:55.497399 22626471084160 run.py:483] Algo bellman_ford step 2817 current loss 0.864657, current_train_items 90176.
I0302 18:59:55.525284 22626471084160 run.py:483] Algo bellman_ford step 2818 current loss 0.671542, current_train_items 90208.
I0302 18:59:55.559000 22626471084160 run.py:483] Algo bellman_ford step 2819 current loss 0.980893, current_train_items 90240.
I0302 18:59:55.578351 22626471084160 run.py:483] Algo bellman_ford step 2820 current loss 0.347775, current_train_items 90272.
I0302 18:59:55.594422 22626471084160 run.py:483] Algo bellman_ford step 2821 current loss 0.509998, current_train_items 90304.
I0302 18:59:55.617842 22626471084160 run.py:483] Algo bellman_ford step 2822 current loss 0.708606, current_train_items 90336.
I0302 18:59:55.648529 22626471084160 run.py:483] Algo bellman_ford step 2823 current loss 0.910017, current_train_items 90368.
I0302 18:59:55.684897 22626471084160 run.py:483] Algo bellman_ford step 2824 current loss 0.963944, current_train_items 90400.
I0302 18:59:55.704736 22626471084160 run.py:483] Algo bellman_ford step 2825 current loss 0.275603, current_train_items 90432.
I0302 18:59:55.720617 22626471084160 run.py:483] Algo bellman_ford step 2826 current loss 0.677012, current_train_items 90464.
I0302 18:59:55.744461 22626471084160 run.py:483] Algo bellman_ford step 2827 current loss 0.800165, current_train_items 90496.
I0302 18:59:55.775365 22626471084160 run.py:483] Algo bellman_ford step 2828 current loss 0.805126, current_train_items 90528.
I0302 18:59:55.807459 22626471084160 run.py:483] Algo bellman_ford step 2829 current loss 0.776358, current_train_items 90560.
I0302 18:59:55.826891 22626471084160 run.py:483] Algo bellman_ford step 2830 current loss 0.262491, current_train_items 90592.
I0302 18:59:55.843367 22626471084160 run.py:483] Algo bellman_ford step 2831 current loss 0.618352, current_train_items 90624.
I0302 18:59:55.866541 22626471084160 run.py:483] Algo bellman_ford step 2832 current loss 0.699630, current_train_items 90656.
I0302 18:59:55.896292 22626471084160 run.py:483] Algo bellman_ford step 2833 current loss 0.803026, current_train_items 90688.
I0302 18:59:55.929746 22626471084160 run.py:483] Algo bellman_ford step 2834 current loss 0.864311, current_train_items 90720.
I0302 18:59:55.949190 22626471084160 run.py:483] Algo bellman_ford step 2835 current loss 0.303030, current_train_items 90752.
I0302 18:59:55.964953 22626471084160 run.py:483] Algo bellman_ford step 2836 current loss 0.540385, current_train_items 90784.
I0302 18:59:55.988162 22626471084160 run.py:483] Algo bellman_ford step 2837 current loss 0.731662, current_train_items 90816.
I0302 18:59:56.019184 22626471084160 run.py:483] Algo bellman_ford step 2838 current loss 1.028016, current_train_items 90848.
I0302 18:59:56.055285 22626471084160 run.py:483] Algo bellman_ford step 2839 current loss 1.237159, current_train_items 90880.
I0302 18:59:56.075248 22626471084160 run.py:483] Algo bellman_ford step 2840 current loss 0.346773, current_train_items 90912.
I0302 18:59:56.091022 22626471084160 run.py:483] Algo bellman_ford step 2841 current loss 0.517361, current_train_items 90944.
I0302 18:59:56.114794 22626471084160 run.py:483] Algo bellman_ford step 2842 current loss 0.685638, current_train_items 90976.
I0302 18:59:56.146548 22626471084160 run.py:483] Algo bellman_ford step 2843 current loss 0.848532, current_train_items 91008.
I0302 18:59:56.180939 22626471084160 run.py:483] Algo bellman_ford step 2844 current loss 1.078261, current_train_items 91040.
I0302 18:59:56.200663 22626471084160 run.py:483] Algo bellman_ford step 2845 current loss 0.322827, current_train_items 91072.
I0302 18:59:56.217301 22626471084160 run.py:483] Algo bellman_ford step 2846 current loss 0.535629, current_train_items 91104.
I0302 18:59:56.240774 22626471084160 run.py:483] Algo bellman_ford step 2847 current loss 0.644572, current_train_items 91136.
I0302 18:59:56.271494 22626471084160 run.py:483] Algo bellman_ford step 2848 current loss 0.745208, current_train_items 91168.
I0302 18:59:56.306317 22626471084160 run.py:483] Algo bellman_ford step 2849 current loss 1.056360, current_train_items 91200.
I0302 18:59:56.325813 22626471084160 run.py:483] Algo bellman_ford step 2850 current loss 0.389411, current_train_items 91232.
I0302 18:59:56.333889 22626471084160 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0302 18:59:56.333994 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 18:59:56.350255 22626471084160 run.py:483] Algo bellman_ford step 2851 current loss 0.464218, current_train_items 91264.
I0302 18:59:56.374028 22626471084160 run.py:483] Algo bellman_ford step 2852 current loss 0.659941, current_train_items 91296.
I0302 18:59:56.405620 22626471084160 run.py:483] Algo bellman_ford step 2853 current loss 0.791719, current_train_items 91328.
I0302 18:59:56.439868 22626471084160 run.py:483] Algo bellman_ford step 2854 current loss 0.852238, current_train_items 91360.
I0302 18:59:56.459782 22626471084160 run.py:483] Algo bellman_ford step 2855 current loss 0.361571, current_train_items 91392.
I0302 18:59:56.475789 22626471084160 run.py:483] Algo bellman_ford step 2856 current loss 0.519176, current_train_items 91424.
I0302 18:59:56.499995 22626471084160 run.py:483] Algo bellman_ford step 2857 current loss 0.848976, current_train_items 91456.
I0302 18:59:56.529272 22626471084160 run.py:483] Algo bellman_ford step 2858 current loss 0.701823, current_train_items 91488.
I0302 18:59:56.561780 22626471084160 run.py:483] Algo bellman_ford step 2859 current loss 0.891297, current_train_items 91520.
I0302 18:59:56.581770 22626471084160 run.py:483] Algo bellman_ford step 2860 current loss 0.400500, current_train_items 91552.
I0302 18:59:56.598037 22626471084160 run.py:483] Algo bellman_ford step 2861 current loss 0.514016, current_train_items 91584.
I0302 18:59:56.621438 22626471084160 run.py:483] Algo bellman_ford step 2862 current loss 0.653393, current_train_items 91616.
I0302 18:59:56.653102 22626471084160 run.py:483] Algo bellman_ford step 2863 current loss 0.849631, current_train_items 91648.
I0302 18:59:56.683995 22626471084160 run.py:483] Algo bellman_ford step 2864 current loss 0.751774, current_train_items 91680.
I0302 18:59:56.703871 22626471084160 run.py:483] Algo bellman_ford step 2865 current loss 0.394510, current_train_items 91712.
I0302 18:59:56.720087 22626471084160 run.py:483] Algo bellman_ford step 2866 current loss 0.540249, current_train_items 91744.
I0302 18:59:56.743416 22626471084160 run.py:483] Algo bellman_ford step 2867 current loss 0.670684, current_train_items 91776.
I0302 18:59:56.773871 22626471084160 run.py:483] Algo bellman_ford step 2868 current loss 0.682911, current_train_items 91808.
I0302 18:59:56.808074 22626471084160 run.py:483] Algo bellman_ford step 2869 current loss 0.859627, current_train_items 91840.
I0302 18:59:56.828296 22626471084160 run.py:483] Algo bellman_ford step 2870 current loss 0.279330, current_train_items 91872.
I0302 18:59:56.844567 22626471084160 run.py:483] Algo bellman_ford step 2871 current loss 0.459545, current_train_items 91904.
I0302 18:59:56.868096 22626471084160 run.py:483] Algo bellman_ford step 2872 current loss 0.628174, current_train_items 91936.
I0302 18:59:56.899911 22626471084160 run.py:483] Algo bellman_ford step 2873 current loss 0.797200, current_train_items 91968.
I0302 18:59:56.932821 22626471084160 run.py:483] Algo bellman_ford step 2874 current loss 0.873062, current_train_items 92000.
I0302 18:59:56.953012 22626471084160 run.py:483] Algo bellman_ford step 2875 current loss 0.358993, current_train_items 92032.
I0302 18:59:56.969295 22626471084160 run.py:483] Algo bellman_ford step 2876 current loss 0.625437, current_train_items 92064.
I0302 18:59:56.992412 22626471084160 run.py:483] Algo bellman_ford step 2877 current loss 0.618109, current_train_items 92096.
I0302 18:59:57.024235 22626471084160 run.py:483] Algo bellman_ford step 2878 current loss 0.796569, current_train_items 92128.
I0302 18:59:57.057024 22626471084160 run.py:483] Algo bellman_ford step 2879 current loss 0.805382, current_train_items 92160.
I0302 18:59:57.077096 22626471084160 run.py:483] Algo bellman_ford step 2880 current loss 0.384670, current_train_items 92192.
I0302 18:59:57.093239 22626471084160 run.py:483] Algo bellman_ford step 2881 current loss 0.592257, current_train_items 92224.
I0302 18:59:57.116882 22626471084160 run.py:483] Algo bellman_ford step 2882 current loss 0.666155, current_train_items 92256.
I0302 18:59:57.147999 22626471084160 run.py:483] Algo bellman_ford step 2883 current loss 0.795977, current_train_items 92288.
I0302 18:59:57.179482 22626471084160 run.py:483] Algo bellman_ford step 2884 current loss 1.153774, current_train_items 92320.
I0302 18:59:57.199465 22626471084160 run.py:483] Algo bellman_ford step 2885 current loss 0.299106, current_train_items 92352.
I0302 18:59:57.215610 22626471084160 run.py:483] Algo bellman_ford step 2886 current loss 0.548420, current_train_items 92384.
I0302 18:59:57.239379 22626471084160 run.py:483] Algo bellman_ford step 2887 current loss 0.754468, current_train_items 92416.
I0302 18:59:57.269622 22626471084160 run.py:483] Algo bellman_ford step 2888 current loss 0.814175, current_train_items 92448.
I0302 18:59:57.304019 22626471084160 run.py:483] Algo bellman_ford step 2889 current loss 0.927033, current_train_items 92480.
I0302 18:59:57.324102 22626471084160 run.py:483] Algo bellman_ford step 2890 current loss 0.407425, current_train_items 92512.
I0302 18:59:57.340261 22626471084160 run.py:483] Algo bellman_ford step 2891 current loss 0.510707, current_train_items 92544.
I0302 18:59:57.364771 22626471084160 run.py:483] Algo bellman_ford step 2892 current loss 0.700734, current_train_items 92576.
I0302 18:59:57.394687 22626471084160 run.py:483] Algo bellman_ford step 2893 current loss 0.807676, current_train_items 92608.
I0302 18:59:57.428468 22626471084160 run.py:483] Algo bellman_ford step 2894 current loss 0.880006, current_train_items 92640.
I0302 18:59:57.447912 22626471084160 run.py:483] Algo bellman_ford step 2895 current loss 0.342275, current_train_items 92672.
I0302 18:59:57.464762 22626471084160 run.py:483] Algo bellman_ford step 2896 current loss 0.537202, current_train_items 92704.
I0302 18:59:57.487178 22626471084160 run.py:483] Algo bellman_ford step 2897 current loss 0.778935, current_train_items 92736.
I0302 18:59:57.517491 22626471084160 run.py:483] Algo bellman_ford step 2898 current loss 0.749736, current_train_items 92768.
I0302 18:59:57.548591 22626471084160 run.py:483] Algo bellman_ford step 2899 current loss 0.660150, current_train_items 92800.
I0302 18:59:57.568319 22626471084160 run.py:483] Algo bellman_ford step 2900 current loss 0.278033, current_train_items 92832.
I0302 18:59:57.575651 22626471084160 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0302 18:59:57.575758 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:59:57.592081 22626471084160 run.py:483] Algo bellman_ford step 2901 current loss 0.461046, current_train_items 92864.
I0302 18:59:57.615503 22626471084160 run.py:483] Algo bellman_ford step 2902 current loss 0.733529, current_train_items 92896.
I0302 18:59:57.646361 22626471084160 run.py:483] Algo bellman_ford step 2903 current loss 0.745400, current_train_items 92928.
I0302 18:59:57.680911 22626471084160 run.py:483] Algo bellman_ford step 2904 current loss 0.871163, current_train_items 92960.
I0302 18:59:57.701245 22626471084160 run.py:483] Algo bellman_ford step 2905 current loss 0.387809, current_train_items 92992.
I0302 18:59:57.717251 22626471084160 run.py:483] Algo bellman_ford step 2906 current loss 0.533593, current_train_items 93024.
I0302 18:59:57.741034 22626471084160 run.py:483] Algo bellman_ford step 2907 current loss 0.764532, current_train_items 93056.
I0302 18:59:57.773308 22626471084160 run.py:483] Algo bellman_ford step 2908 current loss 1.019000, current_train_items 93088.
I0302 18:59:57.807146 22626471084160 run.py:483] Algo bellman_ford step 2909 current loss 0.975677, current_train_items 93120.
I0302 18:59:57.826716 22626471084160 run.py:483] Algo bellman_ford step 2910 current loss 0.314944, current_train_items 93152.
I0302 18:59:57.843446 22626471084160 run.py:483] Algo bellman_ford step 2911 current loss 0.555128, current_train_items 93184.
I0302 18:59:57.866853 22626471084160 run.py:483] Algo bellman_ford step 2912 current loss 0.615444, current_train_items 93216.
I0302 18:59:57.897575 22626471084160 run.py:483] Algo bellman_ford step 2913 current loss 0.790944, current_train_items 93248.
I0302 18:59:57.932280 22626471084160 run.py:483] Algo bellman_ford step 2914 current loss 1.002694, current_train_items 93280.
I0302 18:59:57.951621 22626471084160 run.py:483] Algo bellman_ford step 2915 current loss 0.323845, current_train_items 93312.
I0302 18:59:57.967750 22626471084160 run.py:483] Algo bellman_ford step 2916 current loss 0.621624, current_train_items 93344.
I0302 18:59:57.990688 22626471084160 run.py:483] Algo bellman_ford step 2917 current loss 0.703048, current_train_items 93376.
I0302 18:59:58.022549 22626471084160 run.py:483] Algo bellman_ford step 2918 current loss 0.884513, current_train_items 93408.
I0302 18:59:58.057478 22626471084160 run.py:483] Algo bellman_ford step 2919 current loss 0.948296, current_train_items 93440.
I0302 18:59:58.077039 22626471084160 run.py:483] Algo bellman_ford step 2920 current loss 0.381718, current_train_items 93472.
I0302 18:59:58.093231 22626471084160 run.py:483] Algo bellman_ford step 2921 current loss 0.596817, current_train_items 93504.
I0302 18:59:58.117666 22626471084160 run.py:483] Algo bellman_ford step 2922 current loss 0.687718, current_train_items 93536.
I0302 18:59:58.147495 22626471084160 run.py:483] Algo bellman_ford step 2923 current loss 0.755995, current_train_items 93568.
I0302 18:59:58.182948 22626471084160 run.py:483] Algo bellman_ford step 2924 current loss 0.915020, current_train_items 93600.
I0302 18:59:58.202541 22626471084160 run.py:483] Algo bellman_ford step 2925 current loss 0.344278, current_train_items 93632.
I0302 18:59:58.218620 22626471084160 run.py:483] Algo bellman_ford step 2926 current loss 0.505668, current_train_items 93664.
I0302 18:59:58.242697 22626471084160 run.py:483] Algo bellman_ford step 2927 current loss 0.872399, current_train_items 93696.
I0302 18:59:58.274050 22626471084160 run.py:483] Algo bellman_ford step 2928 current loss 0.824255, current_train_items 93728.
I0302 18:59:58.308172 22626471084160 run.py:483] Algo bellman_ford step 2929 current loss 1.054955, current_train_items 93760.
I0302 18:59:58.327759 22626471084160 run.py:483] Algo bellman_ford step 2930 current loss 0.359815, current_train_items 93792.
I0302 18:59:58.343751 22626471084160 run.py:483] Algo bellman_ford step 2931 current loss 0.467823, current_train_items 93824.
I0302 18:59:58.368503 22626471084160 run.py:483] Algo bellman_ford step 2932 current loss 0.752913, current_train_items 93856.
I0302 18:59:58.398717 22626471084160 run.py:483] Algo bellman_ford step 2933 current loss 0.880706, current_train_items 93888.
I0302 18:59:58.431731 22626471084160 run.py:483] Algo bellman_ford step 2934 current loss 0.821784, current_train_items 93920.
I0302 18:59:58.451446 22626471084160 run.py:483] Algo bellman_ford step 2935 current loss 0.375883, current_train_items 93952.
I0302 18:59:58.467400 22626471084160 run.py:483] Algo bellman_ford step 2936 current loss 0.454506, current_train_items 93984.
I0302 18:59:58.490852 22626471084160 run.py:483] Algo bellman_ford step 2937 current loss 0.738731, current_train_items 94016.
I0302 18:59:58.519753 22626471084160 run.py:483] Algo bellman_ford step 2938 current loss 0.718454, current_train_items 94048.
I0302 18:59:58.553120 22626471084160 run.py:483] Algo bellman_ford step 2939 current loss 0.926388, current_train_items 94080.
I0302 18:59:58.572434 22626471084160 run.py:483] Algo bellman_ford step 2940 current loss 0.374970, current_train_items 94112.
I0302 18:59:58.588846 22626471084160 run.py:483] Algo bellman_ford step 2941 current loss 0.601511, current_train_items 94144.
I0302 18:59:58.612890 22626471084160 run.py:483] Algo bellman_ford step 2942 current loss 0.751278, current_train_items 94176.
I0302 18:59:58.644312 22626471084160 run.py:483] Algo bellman_ford step 2943 current loss 0.861727, current_train_items 94208.
I0302 18:59:58.679648 22626471084160 run.py:483] Algo bellman_ford step 2944 current loss 0.977837, current_train_items 94240.
I0302 18:59:58.698952 22626471084160 run.py:483] Algo bellman_ford step 2945 current loss 0.418774, current_train_items 94272.
I0302 18:59:58.715378 22626471084160 run.py:483] Algo bellman_ford step 2946 current loss 0.556116, current_train_items 94304.
I0302 18:59:58.738946 22626471084160 run.py:483] Algo bellman_ford step 2947 current loss 0.650300, current_train_items 94336.
I0302 18:59:58.769553 22626471084160 run.py:483] Algo bellman_ford step 2948 current loss 0.748182, current_train_items 94368.
I0302 18:59:58.803290 22626471084160 run.py:483] Algo bellman_ford step 2949 current loss 0.843370, current_train_items 94400.
I0302 18:59:58.822690 22626471084160 run.py:483] Algo bellman_ford step 2950 current loss 0.341675, current_train_items 94432.
I0302 18:59:58.830663 22626471084160 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.8515625, 'score': 0.8515625, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0302 18:59:58.830771 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.852, val scores are: bellman_ford: 0.852
I0302 18:59:58.847458 22626471084160 run.py:483] Algo bellman_ford step 2951 current loss 0.610175, current_train_items 94464.
I0302 18:59:58.869484 22626471084160 run.py:483] Algo bellman_ford step 2952 current loss 0.524366, current_train_items 94496.
I0302 18:59:58.899650 22626471084160 run.py:483] Algo bellman_ford step 2953 current loss 0.783780, current_train_items 94528.
I0302 18:59:58.934036 22626471084160 run.py:483] Algo bellman_ford step 2954 current loss 1.013490, current_train_items 94560.
I0302 18:59:58.953792 22626471084160 run.py:483] Algo bellman_ford step 2955 current loss 0.302951, current_train_items 94592.
I0302 18:59:58.969289 22626471084160 run.py:483] Algo bellman_ford step 2956 current loss 0.584928, current_train_items 94624.
I0302 18:59:58.993576 22626471084160 run.py:483] Algo bellman_ford step 2957 current loss 0.798176, current_train_items 94656.
I0302 18:59:59.025219 22626471084160 run.py:483] Algo bellman_ford step 2958 current loss 0.743144, current_train_items 94688.
I0302 18:59:59.059200 22626471084160 run.py:483] Algo bellman_ford step 2959 current loss 0.909096, current_train_items 94720.
I0302 18:59:59.078867 22626471084160 run.py:483] Algo bellman_ford step 2960 current loss 0.330732, current_train_items 94752.
I0302 18:59:59.095384 22626471084160 run.py:483] Algo bellman_ford step 2961 current loss 0.457155, current_train_items 94784.
I0302 18:59:59.118181 22626471084160 run.py:483] Algo bellman_ford step 2962 current loss 0.803392, current_train_items 94816.
I0302 18:59:59.148072 22626471084160 run.py:483] Algo bellman_ford step 2963 current loss 0.804505, current_train_items 94848.
I0302 18:59:59.182996 22626471084160 run.py:483] Algo bellman_ford step 2964 current loss 0.828933, current_train_items 94880.
I0302 18:59:59.202037 22626471084160 run.py:483] Algo bellman_ford step 2965 current loss 0.302372, current_train_items 94912.
I0302 18:59:59.218150 22626471084160 run.py:483] Algo bellman_ford step 2966 current loss 0.522266, current_train_items 94944.
I0302 18:59:59.241405 22626471084160 run.py:483] Algo bellman_ford step 2967 current loss 0.762205, current_train_items 94976.
I0302 18:59:59.271663 22626471084160 run.py:483] Algo bellman_ford step 2968 current loss 0.749244, current_train_items 95008.
I0302 18:59:59.305851 22626471084160 run.py:483] Algo bellman_ford step 2969 current loss 0.971490, current_train_items 95040.
I0302 18:59:59.325539 22626471084160 run.py:483] Algo bellman_ford step 2970 current loss 0.315469, current_train_items 95072.
I0302 18:59:59.341569 22626471084160 run.py:483] Algo bellman_ford step 2971 current loss 0.552767, current_train_items 95104.
I0302 18:59:59.364798 22626471084160 run.py:483] Algo bellman_ford step 2972 current loss 0.679465, current_train_items 95136.
I0302 18:59:59.394767 22626471084160 run.py:483] Algo bellman_ford step 2973 current loss 0.736115, current_train_items 95168.
I0302 18:59:59.431035 22626471084160 run.py:483] Algo bellman_ford step 2974 current loss 1.072161, current_train_items 95200.
I0302 18:59:59.450778 22626471084160 run.py:483] Algo bellman_ford step 2975 current loss 0.324600, current_train_items 95232.
I0302 18:59:59.467635 22626471084160 run.py:483] Algo bellman_ford step 2976 current loss 0.528378, current_train_items 95264.
I0302 18:59:59.490666 22626471084160 run.py:483] Algo bellman_ford step 2977 current loss 0.686689, current_train_items 95296.
I0302 18:59:59.521192 22626471084160 run.py:483] Algo bellman_ford step 2978 current loss 0.844739, current_train_items 95328.
I0302 18:59:59.556211 22626471084160 run.py:483] Algo bellman_ford step 2979 current loss 0.997738, current_train_items 95360.
I0302 18:59:59.575608 22626471084160 run.py:483] Algo bellman_ford step 2980 current loss 0.317625, current_train_items 95392.
I0302 18:59:59.591532 22626471084160 run.py:483] Algo bellman_ford step 2981 current loss 0.479283, current_train_items 95424.
I0302 18:59:59.615433 22626471084160 run.py:483] Algo bellman_ford step 2982 current loss 0.770870, current_train_items 95456.
I0302 18:59:59.648142 22626471084160 run.py:483] Algo bellman_ford step 2983 current loss 0.728847, current_train_items 95488.
I0302 18:59:59.681971 22626471084160 run.py:483] Algo bellman_ford step 2984 current loss 1.031452, current_train_items 95520.
I0302 18:59:59.701537 22626471084160 run.py:483] Algo bellman_ford step 2985 current loss 0.339518, current_train_items 95552.
I0302 18:59:59.718120 22626471084160 run.py:483] Algo bellman_ford step 2986 current loss 0.611801, current_train_items 95584.
I0302 18:59:59.742017 22626471084160 run.py:483] Algo bellman_ford step 2987 current loss 0.649153, current_train_items 95616.
I0302 18:59:59.772387 22626471084160 run.py:483] Algo bellman_ford step 2988 current loss 0.785289, current_train_items 95648.
I0302 18:59:59.806785 22626471084160 run.py:483] Algo bellman_ford step 2989 current loss 0.987135, current_train_items 95680.
I0302 18:59:59.826359 22626471084160 run.py:483] Algo bellman_ford step 2990 current loss 0.369260, current_train_items 95712.
I0302 18:59:59.843132 22626471084160 run.py:483] Algo bellman_ford step 2991 current loss 0.551721, current_train_items 95744.
I0302 18:59:59.866616 22626471084160 run.py:483] Algo bellman_ford step 2992 current loss 0.752005, current_train_items 95776.
I0302 18:59:59.897991 22626471084160 run.py:483] Algo bellman_ford step 2993 current loss 0.866806, current_train_items 95808.
I0302 18:59:59.932885 22626471084160 run.py:483] Algo bellman_ford step 2994 current loss 1.042353, current_train_items 95840.
I0302 18:59:59.952161 22626471084160 run.py:483] Algo bellman_ford step 2995 current loss 0.265314, current_train_items 95872.
I0302 18:59:59.968527 22626471084160 run.py:483] Algo bellman_ford step 2996 current loss 0.527774, current_train_items 95904.
I0302 18:59:59.993515 22626471084160 run.py:483] Algo bellman_ford step 2997 current loss 0.832534, current_train_items 95936.
I0302 19:00:00.024834 22626471084160 run.py:483] Algo bellman_ford step 2998 current loss 0.807874, current_train_items 95968.
I0302 19:00:00.057461 22626471084160 run.py:483] Algo bellman_ford step 2999 current loss 0.935651, current_train_items 96000.
I0302 19:00:00.077112 22626471084160 run.py:483] Algo bellman_ford step 3000 current loss 0.381727, current_train_items 96032.
I0302 19:00:00.084683 22626471084160 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0302 19:00:00.084789 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:00:00.101406 22626471084160 run.py:483] Algo bellman_ford step 3001 current loss 0.577995, current_train_items 96064.
I0302 19:00:00.124973 22626471084160 run.py:483] Algo bellman_ford step 3002 current loss 0.572728, current_train_items 96096.
I0302 19:00:00.156123 22626471084160 run.py:483] Algo bellman_ford step 3003 current loss 0.771461, current_train_items 96128.
I0302 19:00:00.189040 22626471084160 run.py:483] Algo bellman_ford step 3004 current loss 0.842404, current_train_items 96160.
I0302 19:00:00.209093 22626471084160 run.py:483] Algo bellman_ford step 3005 current loss 0.283800, current_train_items 96192.
I0302 19:00:00.224890 22626471084160 run.py:483] Algo bellman_ford step 3006 current loss 0.520531, current_train_items 96224.
I0302 19:00:00.249497 22626471084160 run.py:483] Algo bellman_ford step 3007 current loss 0.841991, current_train_items 96256.
I0302 19:00:00.281062 22626471084160 run.py:483] Algo bellman_ford step 3008 current loss 0.999835, current_train_items 96288.
I0302 19:00:00.316689 22626471084160 run.py:483] Algo bellman_ford step 3009 current loss 1.012477, current_train_items 96320.
I0302 19:00:00.336045 22626471084160 run.py:483] Algo bellman_ford step 3010 current loss 0.375476, current_train_items 96352.
I0302 19:00:00.352240 22626471084160 run.py:483] Algo bellman_ford step 3011 current loss 0.573823, current_train_items 96384.
I0302 19:00:00.375921 22626471084160 run.py:483] Algo bellman_ford step 3012 current loss 0.652363, current_train_items 96416.
I0302 19:00:00.406243 22626471084160 run.py:483] Algo bellman_ford step 3013 current loss 0.727214, current_train_items 96448.
I0302 19:00:00.440849 22626471084160 run.py:483] Algo bellman_ford step 3014 current loss 0.965478, current_train_items 96480.
I0302 19:00:00.460227 22626471084160 run.py:483] Algo bellman_ford step 3015 current loss 0.350994, current_train_items 96512.
I0302 19:00:00.476764 22626471084160 run.py:483] Algo bellman_ford step 3016 current loss 0.526805, current_train_items 96544.
I0302 19:00:00.500006 22626471084160 run.py:483] Algo bellman_ford step 3017 current loss 0.804107, current_train_items 96576.
I0302 19:00:00.530260 22626471084160 run.py:483] Algo bellman_ford step 3018 current loss 0.773677, current_train_items 96608.
I0302 19:00:00.562431 22626471084160 run.py:483] Algo bellman_ford step 3019 current loss 0.855218, current_train_items 96640.
I0302 19:00:00.581593 22626471084160 run.py:483] Algo bellman_ford step 3020 current loss 0.221830, current_train_items 96672.
I0302 19:00:00.597452 22626471084160 run.py:483] Algo bellman_ford step 3021 current loss 0.513564, current_train_items 96704.
I0302 19:00:00.621671 22626471084160 run.py:483] Algo bellman_ford step 3022 current loss 0.809878, current_train_items 96736.
I0302 19:00:00.652524 22626471084160 run.py:483] Algo bellman_ford step 3023 current loss 0.836992, current_train_items 96768.
I0302 19:00:00.685805 22626471084160 run.py:483] Algo bellman_ford step 3024 current loss 0.865793, current_train_items 96800.
I0302 19:00:00.705071 22626471084160 run.py:483] Algo bellman_ford step 3025 current loss 0.253492, current_train_items 96832.
I0302 19:00:00.721047 22626471084160 run.py:483] Algo bellman_ford step 3026 current loss 0.568571, current_train_items 96864.
I0302 19:00:00.743317 22626471084160 run.py:483] Algo bellman_ford step 3027 current loss 0.565891, current_train_items 96896.
I0302 19:00:00.773687 22626471084160 run.py:483] Algo bellman_ford step 3028 current loss 0.741316, current_train_items 96928.
I0302 19:00:00.805816 22626471084160 run.py:483] Algo bellman_ford step 3029 current loss 0.956346, current_train_items 96960.
I0302 19:00:00.825081 22626471084160 run.py:483] Algo bellman_ford step 3030 current loss 0.309263, current_train_items 96992.
I0302 19:00:00.840949 22626471084160 run.py:483] Algo bellman_ford step 3031 current loss 0.443741, current_train_items 97024.
I0302 19:00:00.864891 22626471084160 run.py:483] Algo bellman_ford step 3032 current loss 0.665410, current_train_items 97056.
I0302 19:00:00.896127 22626471084160 run.py:483] Algo bellman_ford step 3033 current loss 0.779607, current_train_items 97088.
I0302 19:00:00.930435 22626471084160 run.py:483] Algo bellman_ford step 3034 current loss 0.812087, current_train_items 97120.
I0302 19:00:00.949853 22626471084160 run.py:483] Algo bellman_ford step 3035 current loss 0.305593, current_train_items 97152.
I0302 19:00:00.966282 22626471084160 run.py:483] Algo bellman_ford step 3036 current loss 0.580357, current_train_items 97184.
I0302 19:00:00.990057 22626471084160 run.py:483] Algo bellman_ford step 3037 current loss 0.765694, current_train_items 97216.
I0302 19:00:01.020884 22626471084160 run.py:483] Algo bellman_ford step 3038 current loss 0.794636, current_train_items 97248.
I0302 19:00:01.053177 22626471084160 run.py:483] Algo bellman_ford step 3039 current loss 1.008790, current_train_items 97280.
I0302 19:00:01.072369 22626471084160 run.py:483] Algo bellman_ford step 3040 current loss 0.331290, current_train_items 97312.
I0302 19:00:01.088411 22626471084160 run.py:483] Algo bellman_ford step 3041 current loss 0.448661, current_train_items 97344.
I0302 19:00:01.111512 22626471084160 run.py:483] Algo bellman_ford step 3042 current loss 0.647544, current_train_items 97376.
I0302 19:00:01.142142 22626471084160 run.py:483] Algo bellman_ford step 3043 current loss 0.756680, current_train_items 97408.
I0302 19:00:01.174431 22626471084160 run.py:483] Algo bellman_ford step 3044 current loss 0.809254, current_train_items 97440.
I0302 19:00:01.193975 22626471084160 run.py:483] Algo bellman_ford step 3045 current loss 0.340759, current_train_items 97472.
I0302 19:00:01.209990 22626471084160 run.py:483] Algo bellman_ford step 3046 current loss 0.553477, current_train_items 97504.
I0302 19:00:01.233091 22626471084160 run.py:483] Algo bellman_ford step 3047 current loss 0.828680, current_train_items 97536.
I0302 19:00:01.264759 22626471084160 run.py:483] Algo bellman_ford step 3048 current loss 0.880390, current_train_items 97568.
I0302 19:00:01.297631 22626471084160 run.py:483] Algo bellman_ford step 3049 current loss 0.876495, current_train_items 97600.
I0302 19:00:01.316896 22626471084160 run.py:483] Algo bellman_ford step 3050 current loss 0.302244, current_train_items 97632.
I0302 19:00:01.324868 22626471084160 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.8818359375, 'score': 0.8818359375, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0302 19:00:01.324974 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.882, val scores are: bellman_ford: 0.882
I0302 19:00:01.341525 22626471084160 run.py:483] Algo bellman_ford step 3051 current loss 0.615897, current_train_items 97664.
I0302 19:00:01.364983 22626471084160 run.py:483] Algo bellman_ford step 3052 current loss 0.853278, current_train_items 97696.
I0302 19:00:01.396837 22626471084160 run.py:483] Algo bellman_ford step 3053 current loss 0.869114, current_train_items 97728.
I0302 19:00:01.428765 22626471084160 run.py:483] Algo bellman_ford step 3054 current loss 0.797557, current_train_items 97760.
I0302 19:00:01.448767 22626471084160 run.py:483] Algo bellman_ford step 3055 current loss 0.381343, current_train_items 97792.
I0302 19:00:01.464600 22626471084160 run.py:483] Algo bellman_ford step 3056 current loss 0.513640, current_train_items 97824.
I0302 19:00:01.488210 22626471084160 run.py:483] Algo bellman_ford step 3057 current loss 0.729418, current_train_items 97856.
I0302 19:00:01.519033 22626471084160 run.py:483] Algo bellman_ford step 3058 current loss 0.776589, current_train_items 97888.
I0302 19:00:01.551691 22626471084160 run.py:483] Algo bellman_ford step 3059 current loss 0.930953, current_train_items 97920.
I0302 19:00:01.571581 22626471084160 run.py:483] Algo bellman_ford step 3060 current loss 0.352745, current_train_items 97952.
I0302 19:00:01.587983 22626471084160 run.py:483] Algo bellman_ford step 3061 current loss 0.576182, current_train_items 97984.
I0302 19:00:01.610822 22626471084160 run.py:483] Algo bellman_ford step 3062 current loss 0.622709, current_train_items 98016.
I0302 19:00:01.642925 22626471084160 run.py:483] Algo bellman_ford step 3063 current loss 0.975174, current_train_items 98048.
I0302 19:00:01.677802 22626471084160 run.py:483] Algo bellman_ford step 3064 current loss 0.899844, current_train_items 98080.
I0302 19:00:01.697777 22626471084160 run.py:483] Algo bellman_ford step 3065 current loss 0.383623, current_train_items 98112.
I0302 19:00:01.713890 22626471084160 run.py:483] Algo bellman_ford step 3066 current loss 0.594773, current_train_items 98144.
I0302 19:00:01.738643 22626471084160 run.py:483] Algo bellman_ford step 3067 current loss 0.717379, current_train_items 98176.
I0302 19:00:01.768931 22626471084160 run.py:483] Algo bellman_ford step 3068 current loss 0.720315, current_train_items 98208.
I0302 19:00:01.804907 22626471084160 run.py:483] Algo bellman_ford step 3069 current loss 0.988237, current_train_items 98240.
I0302 19:00:01.824924 22626471084160 run.py:483] Algo bellman_ford step 3070 current loss 0.290940, current_train_items 98272.
I0302 19:00:01.841404 22626471084160 run.py:483] Algo bellman_ford step 3071 current loss 0.525307, current_train_items 98304.
I0302 19:00:01.864433 22626471084160 run.py:483] Algo bellman_ford step 3072 current loss 0.678329, current_train_items 98336.
I0302 19:00:01.894586 22626471084160 run.py:483] Algo bellman_ford step 3073 current loss 0.720186, current_train_items 98368.
I0302 19:00:01.930168 22626471084160 run.py:483] Algo bellman_ford step 3074 current loss 0.904654, current_train_items 98400.
I0302 19:00:01.950556 22626471084160 run.py:483] Algo bellman_ford step 3075 current loss 0.279920, current_train_items 98432.
I0302 19:00:01.967182 22626471084160 run.py:483] Algo bellman_ford step 3076 current loss 0.659630, current_train_items 98464.
I0302 19:00:01.990509 22626471084160 run.py:483] Algo bellman_ford step 3077 current loss 0.717723, current_train_items 98496.
I0302 19:00:02.021949 22626471084160 run.py:483] Algo bellman_ford step 3078 current loss 0.782026, current_train_items 98528.
I0302 19:00:02.054082 22626471084160 run.py:483] Algo bellman_ford step 3079 current loss 0.807822, current_train_items 98560.
I0302 19:00:02.073673 22626471084160 run.py:483] Algo bellman_ford step 3080 current loss 0.350181, current_train_items 98592.
I0302 19:00:02.090018 22626471084160 run.py:483] Algo bellman_ford step 3081 current loss 0.562527, current_train_items 98624.
I0302 19:00:02.113392 22626471084160 run.py:483] Algo bellman_ford step 3082 current loss 0.664718, current_train_items 98656.
I0302 19:00:02.144027 22626471084160 run.py:483] Algo bellman_ford step 3083 current loss 0.791107, current_train_items 98688.
I0302 19:00:02.177898 22626471084160 run.py:483] Algo bellman_ford step 3084 current loss 0.872787, current_train_items 98720.
I0302 19:00:02.197978 22626471084160 run.py:483] Algo bellman_ford step 3085 current loss 0.378367, current_train_items 98752.
I0302 19:00:02.214358 22626471084160 run.py:483] Algo bellman_ford step 3086 current loss 0.595912, current_train_items 98784.
I0302 19:00:02.236565 22626471084160 run.py:483] Algo bellman_ford step 3087 current loss 0.598121, current_train_items 98816.
I0302 19:00:02.267444 22626471084160 run.py:483] Algo bellman_ford step 3088 current loss 0.834840, current_train_items 98848.
I0302 19:00:02.302676 22626471084160 run.py:483] Algo bellman_ford step 3089 current loss 0.879331, current_train_items 98880.
I0302 19:00:02.322474 22626471084160 run.py:483] Algo bellman_ford step 3090 current loss 0.279933, current_train_items 98912.
I0302 19:00:02.338481 22626471084160 run.py:483] Algo bellman_ford step 3091 current loss 0.539782, current_train_items 98944.
I0302 19:00:02.360987 22626471084160 run.py:483] Algo bellman_ford step 3092 current loss 0.738289, current_train_items 98976.
I0302 19:00:02.391751 22626471084160 run.py:483] Algo bellman_ford step 3093 current loss 0.882561, current_train_items 99008.
I0302 19:00:02.425248 22626471084160 run.py:483] Algo bellman_ford step 3094 current loss 0.983793, current_train_items 99040.
I0302 19:00:02.445052 22626471084160 run.py:483] Algo bellman_ford step 3095 current loss 0.384118, current_train_items 99072.
I0302 19:00:02.461779 22626471084160 run.py:483] Algo bellman_ford step 3096 current loss 0.545133, current_train_items 99104.
I0302 19:00:02.486364 22626471084160 run.py:483] Algo bellman_ford step 3097 current loss 0.794767, current_train_items 99136.
I0302 19:00:02.517136 22626471084160 run.py:483] Algo bellman_ford step 3098 current loss 0.859596, current_train_items 99168.
I0302 19:00:02.550213 22626471084160 run.py:483] Algo bellman_ford step 3099 current loss 1.013134, current_train_items 99200.
I0302 19:00:02.570101 22626471084160 run.py:483] Algo bellman_ford step 3100 current loss 0.340464, current_train_items 99232.
I0302 19:00:02.578043 22626471084160 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0302 19:00:02.578148 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:00:02.595771 22626471084160 run.py:483] Algo bellman_ford step 3101 current loss 0.511317, current_train_items 99264.
I0302 19:00:02.619789 22626471084160 run.py:483] Algo bellman_ford step 3102 current loss 0.838999, current_train_items 99296.
I0302 19:00:02.651365 22626471084160 run.py:483] Algo bellman_ford step 3103 current loss 0.835236, current_train_items 99328.
I0302 19:00:02.686522 22626471084160 run.py:483] Algo bellman_ford step 3104 current loss 1.052912, current_train_items 99360.
I0302 19:00:02.706915 22626471084160 run.py:483] Algo bellman_ford step 3105 current loss 0.300630, current_train_items 99392.
I0302 19:00:02.722345 22626471084160 run.py:483] Algo bellman_ford step 3106 current loss 0.507141, current_train_items 99424.
I0302 19:00:02.745697 22626471084160 run.py:483] Algo bellman_ford step 3107 current loss 0.680881, current_train_items 99456.
I0302 19:00:02.777213 22626471084160 run.py:483] Algo bellman_ford step 3108 current loss 0.806678, current_train_items 99488.
I0302 19:00:02.809540 22626471084160 run.py:483] Algo bellman_ford step 3109 current loss 0.879373, current_train_items 99520.
I0302 19:00:02.829241 22626471084160 run.py:483] Algo bellman_ford step 3110 current loss 0.288608, current_train_items 99552.
I0302 19:00:02.845463 22626471084160 run.py:483] Algo bellman_ford step 3111 current loss 0.470700, current_train_items 99584.
I0302 19:00:02.868786 22626471084160 run.py:483] Algo bellman_ford step 3112 current loss 0.789426, current_train_items 99616.
I0302 19:00:02.899365 22626471084160 run.py:483] Algo bellman_ford step 3113 current loss 0.793418, current_train_items 99648.
I0302 19:00:02.933711 22626471084160 run.py:483] Algo bellman_ford step 3114 current loss 0.969248, current_train_items 99680.
I0302 19:00:02.953434 22626471084160 run.py:483] Algo bellman_ford step 3115 current loss 0.286895, current_train_items 99712.
I0302 19:00:02.969820 22626471084160 run.py:483] Algo bellman_ford step 3116 current loss 0.552982, current_train_items 99744.
I0302 19:00:02.993261 22626471084160 run.py:483] Algo bellman_ford step 3117 current loss 0.789794, current_train_items 99776.
I0302 19:00:03.024717 22626471084160 run.py:483] Algo bellman_ford step 3118 current loss 0.844930, current_train_items 99808.
I0302 19:00:03.057806 22626471084160 run.py:483] Algo bellman_ford step 3119 current loss 0.911305, current_train_items 99840.
I0302 19:00:03.077573 22626471084160 run.py:483] Algo bellman_ford step 3120 current loss 0.201957, current_train_items 99872.
I0302 19:00:03.093722 22626471084160 run.py:483] Algo bellman_ford step 3121 current loss 0.587632, current_train_items 99904.
I0302 19:00:03.117435 22626471084160 run.py:483] Algo bellman_ford step 3122 current loss 0.645787, current_train_items 99936.
I0302 19:00:03.148535 22626471084160 run.py:483] Algo bellman_ford step 3123 current loss 0.722546, current_train_items 99968.
I0302 19:00:03.181730 22626471084160 run.py:483] Algo bellman_ford step 3124 current loss 0.991864, current_train_items 100000.
I0302 19:00:03.201533 22626471084160 run.py:483] Algo bellman_ford step 3125 current loss 0.367527, current_train_items 100032.
I0302 19:00:03.217735 22626471084160 run.py:483] Algo bellman_ford step 3126 current loss 0.567700, current_train_items 100064.
I0302 19:00:03.240597 22626471084160 run.py:483] Algo bellman_ford step 3127 current loss 0.583070, current_train_items 100096.
I0302 19:00:03.272197 22626471084160 run.py:483] Algo bellman_ford step 3128 current loss 0.867166, current_train_items 100128.
I0302 19:00:03.304402 22626471084160 run.py:483] Algo bellman_ford step 3129 current loss 0.900330, current_train_items 100160.
I0302 19:00:03.324192 22626471084160 run.py:483] Algo bellman_ford step 3130 current loss 0.343852, current_train_items 100192.
I0302 19:00:03.340424 22626471084160 run.py:483] Algo bellman_ford step 3131 current loss 0.505091, current_train_items 100224.
I0302 19:00:03.364070 22626471084160 run.py:483] Algo bellman_ford step 3132 current loss 0.712191, current_train_items 100256.
I0302 19:00:03.394301 22626471084160 run.py:483] Algo bellman_ford step 3133 current loss 0.883081, current_train_items 100288.
I0302 19:00:03.429954 22626471084160 run.py:483] Algo bellman_ford step 3134 current loss 0.982799, current_train_items 100320.
I0302 19:00:03.449279 22626471084160 run.py:483] Algo bellman_ford step 3135 current loss 0.336528, current_train_items 100352.
I0302 19:00:03.465350 22626471084160 run.py:483] Algo bellman_ford step 3136 current loss 0.514929, current_train_items 100384.
I0302 19:00:03.489409 22626471084160 run.py:483] Algo bellman_ford step 3137 current loss 0.665810, current_train_items 100416.
I0302 19:00:03.519518 22626471084160 run.py:483] Algo bellman_ford step 3138 current loss 0.760312, current_train_items 100448.
I0302 19:00:03.553863 22626471084160 run.py:483] Algo bellman_ford step 3139 current loss 0.861120, current_train_items 100480.
I0302 19:00:03.573525 22626471084160 run.py:483] Algo bellman_ford step 3140 current loss 0.351275, current_train_items 100512.
I0302 19:00:03.590273 22626471084160 run.py:483] Algo bellman_ford step 3141 current loss 0.566063, current_train_items 100544.
I0302 19:00:03.612665 22626471084160 run.py:483] Algo bellman_ford step 3142 current loss 0.672325, current_train_items 100576.
I0302 19:00:03.644826 22626471084160 run.py:483] Algo bellman_ford step 3143 current loss 0.696529, current_train_items 100608.
I0302 19:00:03.678124 22626471084160 run.py:483] Algo bellman_ford step 3144 current loss 0.753213, current_train_items 100640.
I0302 19:00:03.697781 22626471084160 run.py:483] Algo bellman_ford step 3145 current loss 0.248779, current_train_items 100672.
I0302 19:00:03.713900 22626471084160 run.py:483] Algo bellman_ford step 3146 current loss 0.420150, current_train_items 100704.
I0302 19:00:03.736751 22626471084160 run.py:483] Algo bellman_ford step 3147 current loss 0.596821, current_train_items 100736.
I0302 19:00:03.765896 22626471084160 run.py:483] Algo bellman_ford step 3148 current loss 0.749364, current_train_items 100768.
I0302 19:00:03.801548 22626471084160 run.py:483] Algo bellman_ford step 3149 current loss 0.999374, current_train_items 100800.
I0302 19:00:03.820858 22626471084160 run.py:483] Algo bellman_ford step 3150 current loss 0.296074, current_train_items 100832.
I0302 19:00:03.829061 22626471084160 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0302 19:00:03.829174 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:03.845955 22626471084160 run.py:483] Algo bellman_ford step 3151 current loss 0.515374, current_train_items 100864.
I0302 19:00:03.870885 22626471084160 run.py:483] Algo bellman_ford step 3152 current loss 0.870358, current_train_items 100896.
I0302 19:00:03.902250 22626471084160 run.py:483] Algo bellman_ford step 3153 current loss 0.841750, current_train_items 100928.
I0302 19:00:03.936855 22626471084160 run.py:483] Algo bellman_ford step 3154 current loss 1.033648, current_train_items 100960.
I0302 19:00:03.957127 22626471084160 run.py:483] Algo bellman_ford step 3155 current loss 0.287459, current_train_items 100992.
I0302 19:00:03.972993 22626471084160 run.py:483] Algo bellman_ford step 3156 current loss 0.590920, current_train_items 101024.
I0302 19:00:03.997064 22626471084160 run.py:483] Algo bellman_ford step 3157 current loss 0.720445, current_train_items 101056.
I0302 19:00:04.026903 22626471084160 run.py:483] Algo bellman_ford step 3158 current loss 0.774468, current_train_items 101088.
I0302 19:00:04.059697 22626471084160 run.py:483] Algo bellman_ford step 3159 current loss 0.795675, current_train_items 101120.
I0302 19:00:04.079839 22626471084160 run.py:483] Algo bellman_ford step 3160 current loss 0.339840, current_train_items 101152.
I0302 19:00:04.095882 22626471084160 run.py:483] Algo bellman_ford step 3161 current loss 0.502688, current_train_items 101184.
I0302 19:00:04.119077 22626471084160 run.py:483] Algo bellman_ford step 3162 current loss 0.701428, current_train_items 101216.
I0302 19:00:04.148921 22626471084160 run.py:483] Algo bellman_ford step 3163 current loss 0.703333, current_train_items 101248.
I0302 19:00:04.183995 22626471084160 run.py:483] Algo bellman_ford step 3164 current loss 0.929732, current_train_items 101280.
I0302 19:00:04.203757 22626471084160 run.py:483] Algo bellman_ford step 3165 current loss 0.362232, current_train_items 101312.
I0302 19:00:04.220708 22626471084160 run.py:483] Algo bellman_ford step 3166 current loss 0.627441, current_train_items 101344.
I0302 19:00:04.244743 22626471084160 run.py:483] Algo bellman_ford step 3167 current loss 0.725356, current_train_items 101376.
I0302 19:00:04.275913 22626471084160 run.py:483] Algo bellman_ford step 3168 current loss 1.003682, current_train_items 101408.
I0302 19:00:04.309051 22626471084160 run.py:483] Algo bellman_ford step 3169 current loss 1.001765, current_train_items 101440.
I0302 19:00:04.329191 22626471084160 run.py:483] Algo bellman_ford step 3170 current loss 0.368178, current_train_items 101472.
I0302 19:00:04.345830 22626471084160 run.py:483] Algo bellman_ford step 3171 current loss 0.510189, current_train_items 101504.
I0302 19:00:04.369386 22626471084160 run.py:483] Algo bellman_ford step 3172 current loss 0.643385, current_train_items 101536.
I0302 19:00:04.400431 22626471084160 run.py:483] Algo bellman_ford step 3173 current loss 0.728132, current_train_items 101568.
I0302 19:00:04.431970 22626471084160 run.py:483] Algo bellman_ford step 3174 current loss 0.873395, current_train_items 101600.
I0302 19:00:04.451701 22626471084160 run.py:483] Algo bellman_ford step 3175 current loss 0.289195, current_train_items 101632.
I0302 19:00:04.467942 22626471084160 run.py:483] Algo bellman_ford step 3176 current loss 0.509212, current_train_items 101664.
I0302 19:00:04.490750 22626471084160 run.py:483] Algo bellman_ford step 3177 current loss 0.578246, current_train_items 101696.
I0302 19:00:04.519518 22626471084160 run.py:483] Algo bellman_ford step 3178 current loss 0.764597, current_train_items 101728.
I0302 19:00:04.554074 22626471084160 run.py:483] Algo bellman_ford step 3179 current loss 0.900809, current_train_items 101760.
I0302 19:00:04.573356 22626471084160 run.py:483] Algo bellman_ford step 3180 current loss 0.342907, current_train_items 101792.
I0302 19:00:04.589782 22626471084160 run.py:483] Algo bellman_ford step 3181 current loss 0.506649, current_train_items 101824.
I0302 19:00:04.613432 22626471084160 run.py:483] Algo bellman_ford step 3182 current loss 0.692725, current_train_items 101856.
I0302 19:00:04.644861 22626471084160 run.py:483] Algo bellman_ford step 3183 current loss 0.742734, current_train_items 101888.
I0302 19:00:04.681340 22626471084160 run.py:483] Algo bellman_ford step 3184 current loss 0.895888, current_train_items 101920.
I0302 19:00:04.701917 22626471084160 run.py:483] Algo bellman_ford step 3185 current loss 0.408649, current_train_items 101952.
I0302 19:00:04.718171 22626471084160 run.py:483] Algo bellman_ford step 3186 current loss 0.548779, current_train_items 101984.
I0302 19:00:04.742323 22626471084160 run.py:483] Algo bellman_ford step 3187 current loss 0.779155, current_train_items 102016.
I0302 19:00:04.772711 22626471084160 run.py:483] Algo bellman_ford step 3188 current loss 0.843924, current_train_items 102048.
I0302 19:00:04.806228 22626471084160 run.py:483] Algo bellman_ford step 3189 current loss 0.887810, current_train_items 102080.
I0302 19:00:04.826230 22626471084160 run.py:483] Algo bellman_ford step 3190 current loss 0.372066, current_train_items 102112.
I0302 19:00:04.842629 22626471084160 run.py:483] Algo bellman_ford step 3191 current loss 0.463777, current_train_items 102144.
I0302 19:00:04.866364 22626471084160 run.py:483] Algo bellman_ford step 3192 current loss 0.701487, current_train_items 102176.
I0302 19:00:04.896703 22626471084160 run.py:483] Algo bellman_ford step 3193 current loss 0.718651, current_train_items 102208.
I0302 19:00:04.930641 22626471084160 run.py:483] Algo bellman_ford step 3194 current loss 0.919133, current_train_items 102240.
I0302 19:00:04.950349 22626471084160 run.py:483] Algo bellman_ford step 3195 current loss 0.344830, current_train_items 102272.
I0302 19:00:04.966200 22626471084160 run.py:483] Algo bellman_ford step 3196 current loss 0.420484, current_train_items 102304.
I0302 19:00:04.989717 22626471084160 run.py:483] Algo bellman_ford step 3197 current loss 0.666932, current_train_items 102336.
I0302 19:00:05.020710 22626471084160 run.py:483] Algo bellman_ford step 3198 current loss 0.733824, current_train_items 102368.
I0302 19:00:05.054702 22626471084160 run.py:483] Algo bellman_ford step 3199 current loss 0.862830, current_train_items 102400.
I0302 19:00:05.074728 22626471084160 run.py:483] Algo bellman_ford step 3200 current loss 0.324274, current_train_items 102432.
I0302 19:00:05.082436 22626471084160 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0302 19:00:05.082544 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:00:05.098726 22626471084160 run.py:483] Algo bellman_ford step 3201 current loss 0.429912, current_train_items 102464.
I0302 19:00:05.122442 22626471084160 run.py:483] Algo bellman_ford step 3202 current loss 0.656764, current_train_items 102496.
I0302 19:00:05.155199 22626471084160 run.py:483] Algo bellman_ford step 3203 current loss 0.925264, current_train_items 102528.
I0302 19:00:05.189929 22626471084160 run.py:483] Algo bellman_ford step 3204 current loss 0.859356, current_train_items 102560.
I0302 19:00:05.209848 22626471084160 run.py:483] Algo bellman_ford step 3205 current loss 0.363952, current_train_items 102592.
I0302 19:00:05.225411 22626471084160 run.py:483] Algo bellman_ford step 3206 current loss 0.554214, current_train_items 102624.
I0302 19:00:05.247987 22626471084160 run.py:483] Algo bellman_ford step 3207 current loss 0.601481, current_train_items 102656.
I0302 19:00:05.279972 22626471084160 run.py:483] Algo bellman_ford step 3208 current loss 0.781733, current_train_items 102688.
I0302 19:00:05.314284 22626471084160 run.py:483] Algo bellman_ford step 3209 current loss 0.834010, current_train_items 102720.
I0302 19:00:05.333861 22626471084160 run.py:483] Algo bellman_ford step 3210 current loss 0.307300, current_train_items 102752.
I0302 19:00:05.349738 22626471084160 run.py:483] Algo bellman_ford step 3211 current loss 0.467002, current_train_items 102784.
I0302 19:00:05.373071 22626471084160 run.py:483] Algo bellman_ford step 3212 current loss 0.806524, current_train_items 102816.
I0302 19:00:05.404164 22626471084160 run.py:483] Algo bellman_ford step 3213 current loss 0.882010, current_train_items 102848.
I0302 19:00:05.437897 22626471084160 run.py:483] Algo bellman_ford step 3214 current loss 0.882048, current_train_items 102880.
I0302 19:00:05.457708 22626471084160 run.py:483] Algo bellman_ford step 3215 current loss 0.429615, current_train_items 102912.
I0302 19:00:05.473761 22626471084160 run.py:483] Algo bellman_ford step 3216 current loss 0.569806, current_train_items 102944.
I0302 19:00:05.497808 22626471084160 run.py:483] Algo bellman_ford step 3217 current loss 0.838486, current_train_items 102976.
I0302 19:00:05.527339 22626471084160 run.py:483] Algo bellman_ford step 3218 current loss 0.865964, current_train_items 103008.
I0302 19:00:05.560369 22626471084160 run.py:483] Algo bellman_ford step 3219 current loss 0.938859, current_train_items 103040.
I0302 19:00:05.580269 22626471084160 run.py:483] Algo bellman_ford step 3220 current loss 0.308256, current_train_items 103072.
I0302 19:00:05.596484 22626471084160 run.py:483] Algo bellman_ford step 3221 current loss 0.531669, current_train_items 103104.
I0302 19:00:05.620620 22626471084160 run.py:483] Algo bellman_ford step 3222 current loss 0.719198, current_train_items 103136.
I0302 19:00:05.651929 22626471084160 run.py:483] Algo bellman_ford step 3223 current loss 0.805920, current_train_items 103168.
I0302 19:00:05.683982 22626471084160 run.py:483] Algo bellman_ford step 3224 current loss 0.821695, current_train_items 103200.
I0302 19:00:05.703516 22626471084160 run.py:483] Algo bellman_ford step 3225 current loss 0.328322, current_train_items 103232.
I0302 19:00:05.719044 22626471084160 run.py:483] Algo bellman_ford step 3226 current loss 0.452053, current_train_items 103264.
I0302 19:00:05.742673 22626471084160 run.py:483] Algo bellman_ford step 3227 current loss 0.678657, current_train_items 103296.
I0302 19:00:05.774388 22626471084160 run.py:483] Algo bellman_ford step 3228 current loss 0.821595, current_train_items 103328.
I0302 19:00:05.811696 22626471084160 run.py:483] Algo bellman_ford step 3229 current loss 1.000674, current_train_items 103360.
I0302 19:00:05.831410 22626471084160 run.py:483] Algo bellman_ford step 3230 current loss 0.310753, current_train_items 103392.
I0302 19:00:05.847708 22626471084160 run.py:483] Algo bellman_ford step 3231 current loss 0.465221, current_train_items 103424.
I0302 19:00:05.870777 22626471084160 run.py:483] Algo bellman_ford step 3232 current loss 0.648329, current_train_items 103456.
I0302 19:00:05.902025 22626471084160 run.py:483] Algo bellman_ford step 3233 current loss 0.680847, current_train_items 103488.
I0302 19:00:05.937176 22626471084160 run.py:483] Algo bellman_ford step 3234 current loss 0.814791, current_train_items 103520.
I0302 19:00:05.956814 22626471084160 run.py:483] Algo bellman_ford step 3235 current loss 0.374409, current_train_items 103552.
I0302 19:00:05.973166 22626471084160 run.py:483] Algo bellman_ford step 3236 current loss 0.518797, current_train_items 103584.
I0302 19:00:05.996115 22626471084160 run.py:483] Algo bellman_ford step 3237 current loss 0.637964, current_train_items 103616.
I0302 19:00:06.028633 22626471084160 run.py:483] Algo bellman_ford step 3238 current loss 0.825466, current_train_items 103648.
I0302 19:00:06.062025 22626471084160 run.py:483] Algo bellman_ford step 3239 current loss 0.926443, current_train_items 103680.
I0302 19:00:06.081349 22626471084160 run.py:483] Algo bellman_ford step 3240 current loss 0.283504, current_train_items 103712.
I0302 19:00:06.097687 22626471084160 run.py:483] Algo bellman_ford step 3241 current loss 0.521544, current_train_items 103744.
I0302 19:00:06.120863 22626471084160 run.py:483] Algo bellman_ford step 3242 current loss 0.732284, current_train_items 103776.
I0302 19:00:06.150611 22626471084160 run.py:483] Algo bellman_ford step 3243 current loss 0.618448, current_train_items 103808.
I0302 19:00:06.185013 22626471084160 run.py:483] Algo bellman_ford step 3244 current loss 0.905899, current_train_items 103840.
I0302 19:00:06.204508 22626471084160 run.py:483] Algo bellman_ford step 3245 current loss 0.288274, current_train_items 103872.
I0302 19:00:06.220856 22626471084160 run.py:483] Algo bellman_ford step 3246 current loss 0.492182, current_train_items 103904.
I0302 19:00:06.244061 22626471084160 run.py:483] Algo bellman_ford step 3247 current loss 0.596521, current_train_items 103936.
I0302 19:00:06.275331 22626471084160 run.py:483] Algo bellman_ford step 3248 current loss 0.672020, current_train_items 103968.
I0302 19:00:06.307617 22626471084160 run.py:483] Algo bellman_ford step 3249 current loss 0.841678, current_train_items 104000.
I0302 19:00:06.327602 22626471084160 run.py:483] Algo bellman_ford step 3250 current loss 0.326849, current_train_items 104032.
I0302 19:00:06.335642 22626471084160 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.87890625, 'score': 0.87890625, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0302 19:00:06.335748 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.879, val scores are: bellman_ford: 0.879
I0302 19:00:06.352349 22626471084160 run.py:483] Algo bellman_ford step 3251 current loss 0.549743, current_train_items 104064.
I0302 19:00:06.374762 22626471084160 run.py:483] Algo bellman_ford step 3252 current loss 0.632340, current_train_items 104096.
I0302 19:00:06.407295 22626471084160 run.py:483] Algo bellman_ford step 3253 current loss 0.843806, current_train_items 104128.
I0302 19:00:06.441482 22626471084160 run.py:483] Algo bellman_ford step 3254 current loss 0.840272, current_train_items 104160.
I0302 19:00:06.462031 22626471084160 run.py:483] Algo bellman_ford step 3255 current loss 0.422834, current_train_items 104192.
I0302 19:00:06.477676 22626471084160 run.py:483] Algo bellman_ford step 3256 current loss 0.450481, current_train_items 104224.
I0302 19:00:06.500402 22626471084160 run.py:483] Algo bellman_ford step 3257 current loss 0.656695, current_train_items 104256.
I0302 19:00:06.532622 22626471084160 run.py:483] Algo bellman_ford step 3258 current loss 0.749722, current_train_items 104288.
I0302 19:00:06.567621 22626471084160 run.py:483] Algo bellman_ford step 3259 current loss 0.925456, current_train_items 104320.
I0302 19:00:06.587409 22626471084160 run.py:483] Algo bellman_ford step 3260 current loss 0.379693, current_train_items 104352.
I0302 19:00:06.603660 22626471084160 run.py:483] Algo bellman_ford step 3261 current loss 0.500757, current_train_items 104384.
I0302 19:00:06.626599 22626471084160 run.py:483] Algo bellman_ford step 3262 current loss 0.742804, current_train_items 104416.
I0302 19:00:06.657877 22626471084160 run.py:483] Algo bellman_ford step 3263 current loss 0.940299, current_train_items 104448.
I0302 19:00:06.692237 22626471084160 run.py:483] Algo bellman_ford step 3264 current loss 0.964921, current_train_items 104480.
I0302 19:00:06.711787 22626471084160 run.py:483] Algo bellman_ford step 3265 current loss 0.384413, current_train_items 104512.
I0302 19:00:06.728182 22626471084160 run.py:483] Algo bellman_ford step 3266 current loss 0.541493, current_train_items 104544.
I0302 19:00:06.751259 22626471084160 run.py:483] Algo bellman_ford step 3267 current loss 0.697547, current_train_items 104576.
I0302 19:00:06.780360 22626471084160 run.py:483] Algo bellman_ford step 3268 current loss 0.739454, current_train_items 104608.
I0302 19:00:06.816496 22626471084160 run.py:483] Algo bellman_ford step 3269 current loss 1.106908, current_train_items 104640.
I0302 19:00:06.836186 22626471084160 run.py:483] Algo bellman_ford step 3270 current loss 0.359960, current_train_items 104672.
I0302 19:00:06.852475 22626471084160 run.py:483] Algo bellman_ford step 3271 current loss 0.543546, current_train_items 104704.
I0302 19:00:06.875627 22626471084160 run.py:483] Algo bellman_ford step 3272 current loss 0.685197, current_train_items 104736.
I0302 19:00:06.905271 22626471084160 run.py:483] Algo bellman_ford step 3273 current loss 0.755378, current_train_items 104768.
I0302 19:00:06.937165 22626471084160 run.py:483] Algo bellman_ford step 3274 current loss 0.740204, current_train_items 104800.
I0302 19:00:06.957288 22626471084160 run.py:483] Algo bellman_ford step 3275 current loss 0.324468, current_train_items 104832.
I0302 19:00:06.973359 22626471084160 run.py:483] Algo bellman_ford step 3276 current loss 0.456839, current_train_items 104864.
I0302 19:00:06.996219 22626471084160 run.py:483] Algo bellman_ford step 3277 current loss 0.721660, current_train_items 104896.
I0302 19:00:07.027838 22626471084160 run.py:483] Algo bellman_ford step 3278 current loss 0.748513, current_train_items 104928.
I0302 19:00:07.062121 22626471084160 run.py:483] Algo bellman_ford step 3279 current loss 0.739513, current_train_items 104960.
I0302 19:00:07.081887 22626471084160 run.py:483] Algo bellman_ford step 3280 current loss 0.350840, current_train_items 104992.
I0302 19:00:07.097811 22626471084160 run.py:483] Algo bellman_ford step 3281 current loss 0.565379, current_train_items 105024.
I0302 19:00:07.120782 22626471084160 run.py:483] Algo bellman_ford step 3282 current loss 0.786558, current_train_items 105056.
I0302 19:00:07.151977 22626471084160 run.py:483] Algo bellman_ford step 3283 current loss 0.828778, current_train_items 105088.
I0302 19:00:07.186010 22626471084160 run.py:483] Algo bellman_ford step 3284 current loss 0.844161, current_train_items 105120.
I0302 19:00:07.205862 22626471084160 run.py:483] Algo bellman_ford step 3285 current loss 0.338123, current_train_items 105152.
I0302 19:00:07.222337 22626471084160 run.py:483] Algo bellman_ford step 3286 current loss 0.591411, current_train_items 105184.
I0302 19:00:07.245773 22626471084160 run.py:483] Algo bellman_ford step 3287 current loss 0.734193, current_train_items 105216.
I0302 19:00:07.275708 22626471084160 run.py:483] Algo bellman_ford step 3288 current loss 0.747233, current_train_items 105248.
I0302 19:00:07.307817 22626471084160 run.py:483] Algo bellman_ford step 3289 current loss 0.892307, current_train_items 105280.
I0302 19:00:07.327949 22626471084160 run.py:483] Algo bellman_ford step 3290 current loss 0.407296, current_train_items 105312.
I0302 19:00:07.344617 22626471084160 run.py:483] Algo bellman_ford step 3291 current loss 0.564211, current_train_items 105344.
I0302 19:00:07.367778 22626471084160 run.py:483] Algo bellman_ford step 3292 current loss 0.795660, current_train_items 105376.
I0302 19:00:07.399091 22626471084160 run.py:483] Algo bellman_ford step 3293 current loss 0.936010, current_train_items 105408.
I0302 19:00:07.432925 22626471084160 run.py:483] Algo bellman_ford step 3294 current loss 0.953053, current_train_items 105440.
I0302 19:00:07.452816 22626471084160 run.py:483] Algo bellman_ford step 3295 current loss 0.347188, current_train_items 105472.
I0302 19:00:07.468665 22626471084160 run.py:483] Algo bellman_ford step 3296 current loss 0.509296, current_train_items 105504.
I0302 19:00:07.493133 22626471084160 run.py:483] Algo bellman_ford step 3297 current loss 1.076481, current_train_items 105536.
I0302 19:00:07.525432 22626471084160 run.py:483] Algo bellman_ford step 3298 current loss 1.077744, current_train_items 105568.
I0302 19:00:07.560192 22626471084160 run.py:483] Algo bellman_ford step 3299 current loss 1.177153, current_train_items 105600.
I0302 19:00:07.580206 22626471084160 run.py:483] Algo bellman_ford step 3300 current loss 0.380647, current_train_items 105632.
I0302 19:00:07.587880 22626471084160 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0302 19:00:07.587987 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 19:00:07.604763 22626471084160 run.py:483] Algo bellman_ford step 3301 current loss 0.549464, current_train_items 105664.
I0302 19:00:07.628509 22626471084160 run.py:483] Algo bellman_ford step 3302 current loss 0.692100, current_train_items 105696.
I0302 19:00:07.658430 22626471084160 run.py:483] Algo bellman_ford step 3303 current loss 0.769062, current_train_items 105728.
I0302 19:00:07.690600 22626471084160 run.py:483] Algo bellman_ford step 3304 current loss 0.920360, current_train_items 105760.
I0302 19:00:07.710534 22626471084160 run.py:483] Algo bellman_ford step 3305 current loss 0.239413, current_train_items 105792.
I0302 19:00:07.726646 22626471084160 run.py:483] Algo bellman_ford step 3306 current loss 0.681968, current_train_items 105824.
I0302 19:00:07.750667 22626471084160 run.py:483] Algo bellman_ford step 3307 current loss 0.786765, current_train_items 105856.
I0302 19:00:07.781546 22626471084160 run.py:483] Algo bellman_ford step 3308 current loss 0.763228, current_train_items 105888.
I0302 19:00:07.817301 22626471084160 run.py:483] Algo bellman_ford step 3309 current loss 1.140887, current_train_items 105920.
I0302 19:00:07.836802 22626471084160 run.py:483] Algo bellman_ford step 3310 current loss 0.329214, current_train_items 105952.
I0302 19:00:07.853211 22626471084160 run.py:483] Algo bellman_ford step 3311 current loss 0.524306, current_train_items 105984.
I0302 19:00:07.876656 22626471084160 run.py:483] Algo bellman_ford step 3312 current loss 0.638183, current_train_items 106016.
I0302 19:00:07.908282 22626471084160 run.py:483] Algo bellman_ford step 3313 current loss 0.787897, current_train_items 106048.
I0302 19:00:07.940674 22626471084160 run.py:483] Algo bellman_ford step 3314 current loss 0.913211, current_train_items 106080.
I0302 19:00:07.960366 22626471084160 run.py:483] Algo bellman_ford step 3315 current loss 0.367940, current_train_items 106112.
I0302 19:00:07.976910 22626471084160 run.py:483] Algo bellman_ford step 3316 current loss 0.528467, current_train_items 106144.
I0302 19:00:07.999958 22626471084160 run.py:483] Algo bellman_ford step 3317 current loss 0.727163, current_train_items 106176.
I0302 19:00:08.030954 22626471084160 run.py:483] Algo bellman_ford step 3318 current loss 0.764068, current_train_items 106208.
I0302 19:00:08.065454 22626471084160 run.py:483] Algo bellman_ford step 3319 current loss 0.887187, current_train_items 106240.
I0302 19:00:08.085128 22626471084160 run.py:483] Algo bellman_ford step 3320 current loss 0.335386, current_train_items 106272.
I0302 19:00:08.101388 22626471084160 run.py:483] Algo bellman_ford step 3321 current loss 0.594847, current_train_items 106304.
I0302 19:00:08.125628 22626471084160 run.py:483] Algo bellman_ford step 3322 current loss 0.727054, current_train_items 106336.
I0302 19:00:08.156862 22626471084160 run.py:483] Algo bellman_ford step 3323 current loss 0.702275, current_train_items 106368.
I0302 19:00:08.189499 22626471084160 run.py:483] Algo bellman_ford step 3324 current loss 0.855510, current_train_items 106400.
I0302 19:00:08.209030 22626471084160 run.py:483] Algo bellman_ford step 3325 current loss 0.356088, current_train_items 106432.
I0302 19:00:08.225384 22626471084160 run.py:483] Algo bellman_ford step 3326 current loss 0.490112, current_train_items 106464.
I0302 19:00:08.248624 22626471084160 run.py:483] Algo bellman_ford step 3327 current loss 0.679185, current_train_items 106496.
I0302 19:00:08.279009 22626471084160 run.py:483] Algo bellman_ford step 3328 current loss 0.772102, current_train_items 106528.
I0302 19:00:08.312201 22626471084160 run.py:483] Algo bellman_ford step 3329 current loss 0.801096, current_train_items 106560.
I0302 19:00:08.331846 22626471084160 run.py:483] Algo bellman_ford step 3330 current loss 0.394699, current_train_items 106592.
I0302 19:00:08.347891 22626471084160 run.py:483] Algo bellman_ford step 3331 current loss 0.495741, current_train_items 106624.
I0302 19:00:08.371047 22626471084160 run.py:483] Algo bellman_ford step 3332 current loss 0.748930, current_train_items 106656.
I0302 19:00:08.401116 22626471084160 run.py:483] Algo bellman_ford step 3333 current loss 0.704437, current_train_items 106688.
I0302 19:00:08.433594 22626471084160 run.py:483] Algo bellman_ford step 3334 current loss 0.735590, current_train_items 106720.
I0302 19:00:08.452906 22626471084160 run.py:483] Algo bellman_ford step 3335 current loss 0.323654, current_train_items 106752.
I0302 19:00:08.468611 22626471084160 run.py:483] Algo bellman_ford step 3336 current loss 0.629171, current_train_items 106784.
I0302 19:00:08.492842 22626471084160 run.py:483] Algo bellman_ford step 3337 current loss 0.857165, current_train_items 106816.
I0302 19:00:08.525311 22626471084160 run.py:483] Algo bellman_ford step 3338 current loss 0.889632, current_train_items 106848.
I0302 19:00:08.560683 22626471084160 run.py:483] Algo bellman_ford step 3339 current loss 1.058117, current_train_items 106880.
I0302 19:00:08.579897 22626471084160 run.py:483] Algo bellman_ford step 3340 current loss 0.286023, current_train_items 106912.
I0302 19:00:08.596471 22626471084160 run.py:483] Algo bellman_ford step 3341 current loss 0.530968, current_train_items 106944.
I0302 19:00:08.620135 22626471084160 run.py:483] Algo bellman_ford step 3342 current loss 0.854078, current_train_items 106976.
I0302 19:00:08.652170 22626471084160 run.py:483] Algo bellman_ford step 3343 current loss 0.823258, current_train_items 107008.
I0302 19:00:08.683934 22626471084160 run.py:483] Algo bellman_ford step 3344 current loss 0.833075, current_train_items 107040.
I0302 19:00:08.703614 22626471084160 run.py:483] Algo bellman_ford step 3345 current loss 0.324354, current_train_items 107072.
I0302 19:00:08.719574 22626471084160 run.py:483] Algo bellman_ford step 3346 current loss 0.534706, current_train_items 107104.
I0302 19:00:08.742964 22626471084160 run.py:483] Algo bellman_ford step 3347 current loss 0.814570, current_train_items 107136.
I0302 19:00:08.773277 22626471084160 run.py:483] Algo bellman_ford step 3348 current loss 0.881115, current_train_items 107168.
I0302 19:00:08.804803 22626471084160 run.py:483] Algo bellman_ford step 3349 current loss 0.863893, current_train_items 107200.
I0302 19:00:08.824670 22626471084160 run.py:483] Algo bellman_ford step 3350 current loss 0.372409, current_train_items 107232.
I0302 19:00:08.832532 22626471084160 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.865234375, 'score': 0.865234375, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0302 19:00:08.832640 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.865, val scores are: bellman_ford: 0.865
I0302 19:00:08.850017 22626471084160 run.py:483] Algo bellman_ford step 3351 current loss 0.589272, current_train_items 107264.
I0302 19:00:08.874647 22626471084160 run.py:483] Algo bellman_ford step 3352 current loss 0.902224, current_train_items 107296.
I0302 19:00:08.906709 22626471084160 run.py:483] Algo bellman_ford step 3353 current loss 0.777158, current_train_items 107328.
I0302 19:00:08.941055 22626471084160 run.py:483] Algo bellman_ford step 3354 current loss 1.104132, current_train_items 107360.
I0302 19:00:08.960629 22626471084160 run.py:483] Algo bellman_ford step 3355 current loss 0.292623, current_train_items 107392.
I0302 19:00:08.976495 22626471084160 run.py:483] Algo bellman_ford step 3356 current loss 0.501196, current_train_items 107424.
I0302 19:00:09.000035 22626471084160 run.py:483] Algo bellman_ford step 3357 current loss 0.638793, current_train_items 107456.
I0302 19:00:09.030905 22626471084160 run.py:483] Algo bellman_ford step 3358 current loss 0.773116, current_train_items 107488.
I0302 19:00:09.066188 22626471084160 run.py:483] Algo bellman_ford step 3359 current loss 0.924684, current_train_items 107520.
I0302 19:00:09.086036 22626471084160 run.py:483] Algo bellman_ford step 3360 current loss 0.341777, current_train_items 107552.
I0302 19:00:09.102160 22626471084160 run.py:483] Algo bellman_ford step 3361 current loss 0.554431, current_train_items 107584.
I0302 19:00:09.126187 22626471084160 run.py:483] Algo bellman_ford step 3362 current loss 0.701436, current_train_items 107616.
I0302 19:00:09.156929 22626471084160 run.py:483] Algo bellman_ford step 3363 current loss 0.638701, current_train_items 107648.
I0302 19:00:09.189601 22626471084160 run.py:483] Algo bellman_ford step 3364 current loss 0.983421, current_train_items 107680.
I0302 19:00:09.209216 22626471084160 run.py:483] Algo bellman_ford step 3365 current loss 0.359905, current_train_items 107712.
I0302 19:00:09.224998 22626471084160 run.py:483] Algo bellman_ford step 3366 current loss 0.487924, current_train_items 107744.
I0302 19:00:09.249422 22626471084160 run.py:483] Algo bellman_ford step 3367 current loss 0.778300, current_train_items 107776.
I0302 19:00:09.279164 22626471084160 run.py:483] Algo bellman_ford step 3368 current loss 0.675971, current_train_items 107808.
I0302 19:00:09.311622 22626471084160 run.py:483] Algo bellman_ford step 3369 current loss 0.737948, current_train_items 107840.
I0302 19:00:09.331778 22626471084160 run.py:483] Algo bellman_ford step 3370 current loss 0.354854, current_train_items 107872.
I0302 19:00:09.348185 22626471084160 run.py:483] Algo bellman_ford step 3371 current loss 0.506083, current_train_items 107904.
I0302 19:00:09.371470 22626471084160 run.py:483] Algo bellman_ford step 3372 current loss 0.736582, current_train_items 107936.
I0302 19:00:09.403946 22626471084160 run.py:483] Algo bellman_ford step 3373 current loss 0.934863, current_train_items 107968.
I0302 19:00:09.437046 22626471084160 run.py:483] Algo bellman_ford step 3374 current loss 0.943226, current_train_items 108000.
I0302 19:00:09.456914 22626471084160 run.py:483] Algo bellman_ford step 3375 current loss 0.338126, current_train_items 108032.
I0302 19:00:09.472761 22626471084160 run.py:483] Algo bellman_ford step 3376 current loss 0.515076, current_train_items 108064.
I0302 19:00:09.495682 22626471084160 run.py:483] Algo bellman_ford step 3377 current loss 0.712130, current_train_items 108096.
I0302 19:00:09.525927 22626471084160 run.py:483] Algo bellman_ford step 3378 current loss 0.677338, current_train_items 108128.
I0302 19:00:09.561168 22626471084160 run.py:483] Algo bellman_ford step 3379 current loss 0.793235, current_train_items 108160.
I0302 19:00:09.580745 22626471084160 run.py:483] Algo bellman_ford step 3380 current loss 0.291517, current_train_items 108192.
I0302 19:00:09.597233 22626471084160 run.py:483] Algo bellman_ford step 3381 current loss 0.589302, current_train_items 108224.
I0302 19:00:09.621641 22626471084160 run.py:483] Algo bellman_ford step 3382 current loss 0.683930, current_train_items 108256.
I0302 19:00:09.651567 22626471084160 run.py:483] Algo bellman_ford step 3383 current loss 0.875720, current_train_items 108288.
I0302 19:00:09.686873 22626471084160 run.py:483] Algo bellman_ford step 3384 current loss 1.299116, current_train_items 108320.
I0302 19:00:09.706825 22626471084160 run.py:483] Algo bellman_ford step 3385 current loss 0.354677, current_train_items 108352.
I0302 19:00:09.723328 22626471084160 run.py:483] Algo bellman_ford step 3386 current loss 0.610439, current_train_items 108384.
I0302 19:00:09.746787 22626471084160 run.py:483] Algo bellman_ford step 3387 current loss 0.757148, current_train_items 108416.
I0302 19:00:09.777046 22626471084160 run.py:483] Algo bellman_ford step 3388 current loss 0.960638, current_train_items 108448.
I0302 19:00:09.808876 22626471084160 run.py:483] Algo bellman_ford step 3389 current loss 0.952328, current_train_items 108480.
I0302 19:00:09.828752 22626471084160 run.py:483] Algo bellman_ford step 3390 current loss 0.345042, current_train_items 108512.
I0302 19:00:09.844973 22626471084160 run.py:483] Algo bellman_ford step 3391 current loss 0.605078, current_train_items 108544.
I0302 19:00:09.868255 22626471084160 run.py:483] Algo bellman_ford step 3392 current loss 0.778719, current_train_items 108576.
I0302 19:00:09.898257 22626471084160 run.py:483] Algo bellman_ford step 3393 current loss 0.880491, current_train_items 108608.
I0302 19:00:09.929864 22626471084160 run.py:483] Algo bellman_ford step 3394 current loss 1.135190, current_train_items 108640.
I0302 19:00:09.949658 22626471084160 run.py:483] Algo bellman_ford step 3395 current loss 0.381398, current_train_items 108672.
I0302 19:00:09.965992 22626471084160 run.py:483] Algo bellman_ford step 3396 current loss 0.487489, current_train_items 108704.
I0302 19:00:09.989440 22626471084160 run.py:483] Algo bellman_ford step 3397 current loss 0.652620, current_train_items 108736.
I0302 19:00:10.020679 22626471084160 run.py:483] Algo bellman_ford step 3398 current loss 0.910238, current_train_items 108768.
I0302 19:00:10.053113 22626471084160 run.py:483] Algo bellman_ford step 3399 current loss 0.953878, current_train_items 108800.
I0302 19:00:10.073266 22626471084160 run.py:483] Algo bellman_ford step 3400 current loss 0.344289, current_train_items 108832.
I0302 19:00:10.080998 22626471084160 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0302 19:00:10.081105 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:00:10.098209 22626471084160 run.py:483] Algo bellman_ford step 3401 current loss 0.538652, current_train_items 108864.
I0302 19:00:10.121873 22626471084160 run.py:483] Algo bellman_ford step 3402 current loss 0.654442, current_train_items 108896.
I0302 19:00:10.152758 22626471084160 run.py:483] Algo bellman_ford step 3403 current loss 0.818217, current_train_items 108928.
I0302 19:00:10.187571 22626471084160 run.py:483] Algo bellman_ford step 3404 current loss 0.977177, current_train_items 108960.
I0302 19:00:10.207360 22626471084160 run.py:483] Algo bellman_ford step 3405 current loss 0.297652, current_train_items 108992.
I0302 19:00:10.222834 22626471084160 run.py:483] Algo bellman_ford step 3406 current loss 0.581144, current_train_items 109024.
I0302 19:00:10.246350 22626471084160 run.py:483] Algo bellman_ford step 3407 current loss 0.708162, current_train_items 109056.
I0302 19:00:10.278487 22626471084160 run.py:483] Algo bellman_ford step 3408 current loss 0.875047, current_train_items 109088.
I0302 19:00:10.311118 22626471084160 run.py:483] Algo bellman_ford step 3409 current loss 0.867856, current_train_items 109120.
I0302 19:00:10.330209 22626471084160 run.py:483] Algo bellman_ford step 3410 current loss 0.300658, current_train_items 109152.
I0302 19:00:10.346593 22626471084160 run.py:483] Algo bellman_ford step 3411 current loss 0.614200, current_train_items 109184.
I0302 19:00:10.370621 22626471084160 run.py:483] Algo bellman_ford step 3412 current loss 0.641798, current_train_items 109216.
I0302 19:00:10.402807 22626471084160 run.py:483] Algo bellman_ford step 3413 current loss 0.923316, current_train_items 109248.
I0302 19:00:10.438726 22626471084160 run.py:483] Algo bellman_ford step 3414 current loss 0.971468, current_train_items 109280.
I0302 19:00:10.458228 22626471084160 run.py:483] Algo bellman_ford step 3415 current loss 0.311079, current_train_items 109312.
I0302 19:00:10.474413 22626471084160 run.py:483] Algo bellman_ford step 3416 current loss 0.580917, current_train_items 109344.
I0302 19:00:10.498393 22626471084160 run.py:483] Algo bellman_ford step 3417 current loss 0.721214, current_train_items 109376.
I0302 19:00:10.529334 22626471084160 run.py:483] Algo bellman_ford step 3418 current loss 0.811618, current_train_items 109408.
I0302 19:00:10.561730 22626471084160 run.py:483] Algo bellman_ford step 3419 current loss 0.766293, current_train_items 109440.
I0302 19:00:10.581700 22626471084160 run.py:483] Algo bellman_ford step 3420 current loss 0.379291, current_train_items 109472.
I0302 19:00:10.597652 22626471084160 run.py:483] Algo bellman_ford step 3421 current loss 0.539353, current_train_items 109504.
I0302 19:00:10.621129 22626471084160 run.py:483] Algo bellman_ford step 3422 current loss 0.712089, current_train_items 109536.
I0302 19:00:10.652395 22626471084160 run.py:483] Algo bellman_ford step 3423 current loss 0.717640, current_train_items 109568.
I0302 19:00:10.685291 22626471084160 run.py:483] Algo bellman_ford step 3424 current loss 1.067436, current_train_items 109600.
I0302 19:00:10.704593 22626471084160 run.py:483] Algo bellman_ford step 3425 current loss 0.335538, current_train_items 109632.
I0302 19:00:10.721237 22626471084160 run.py:483] Algo bellman_ford step 3426 current loss 0.543505, current_train_items 109664.
I0302 19:00:10.744111 22626471084160 run.py:483] Algo bellman_ford step 3427 current loss 0.692732, current_train_items 109696.
I0302 19:00:10.775125 22626471084160 run.py:483] Algo bellman_ford step 3428 current loss 0.879971, current_train_items 109728.
I0302 19:00:10.805827 22626471084160 run.py:483] Algo bellman_ford step 3429 current loss 0.812890, current_train_items 109760.
I0302 19:00:10.825359 22626471084160 run.py:483] Algo bellman_ford step 3430 current loss 0.325569, current_train_items 109792.
I0302 19:00:10.841794 22626471084160 run.py:483] Algo bellman_ford step 3431 current loss 0.540412, current_train_items 109824.
I0302 19:00:10.864516 22626471084160 run.py:483] Algo bellman_ford step 3432 current loss 0.642074, current_train_items 109856.
I0302 19:00:10.897432 22626471084160 run.py:483] Algo bellman_ford step 3433 current loss 0.926097, current_train_items 109888.
I0302 19:00:10.930025 22626471084160 run.py:483] Algo bellman_ford step 3434 current loss 0.854443, current_train_items 109920.
I0302 19:00:10.949362 22626471084160 run.py:483] Algo bellman_ford step 3435 current loss 0.382264, current_train_items 109952.
I0302 19:00:10.965481 22626471084160 run.py:483] Algo bellman_ford step 3436 current loss 0.549159, current_train_items 109984.
I0302 19:00:10.989111 22626471084160 run.py:483] Algo bellman_ford step 3437 current loss 0.876981, current_train_items 110016.
I0302 19:00:11.018631 22626471084160 run.py:483] Algo bellman_ford step 3438 current loss 0.902593, current_train_items 110048.
I0302 19:00:11.050137 22626471084160 run.py:483] Algo bellman_ford step 3439 current loss 0.883913, current_train_items 110080.
I0302 19:00:11.069499 22626471084160 run.py:483] Algo bellman_ford step 3440 current loss 0.353009, current_train_items 110112.
I0302 19:00:11.085917 22626471084160 run.py:483] Algo bellman_ford step 3441 current loss 0.585046, current_train_items 110144.
I0302 19:00:11.108673 22626471084160 run.py:483] Algo bellman_ford step 3442 current loss 0.724078, current_train_items 110176.
I0302 19:00:11.140188 22626471084160 run.py:483] Algo bellman_ford step 3443 current loss 0.896229, current_train_items 110208.
I0302 19:00:11.174403 22626471084160 run.py:483] Algo bellman_ford step 3444 current loss 0.857782, current_train_items 110240.
I0302 19:00:11.193684 22626471084160 run.py:483] Algo bellman_ford step 3445 current loss 0.322433, current_train_items 110272.
I0302 19:00:11.210202 22626471084160 run.py:483] Algo bellman_ford step 3446 current loss 0.565392, current_train_items 110304.
I0302 19:00:11.234022 22626471084160 run.py:483] Algo bellman_ford step 3447 current loss 0.705592, current_train_items 110336.
I0302 19:00:11.264428 22626471084160 run.py:483] Algo bellman_ford step 3448 current loss 0.746149, current_train_items 110368.
I0302 19:00:11.297641 22626471084160 run.py:483] Algo bellman_ford step 3449 current loss 1.009205, current_train_items 110400.
I0302 19:00:11.317239 22626471084160 run.py:483] Algo bellman_ford step 3450 current loss 0.361610, current_train_items 110432.
I0302 19:00:11.325340 22626471084160 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0302 19:00:11.325445 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 19:00:11.342148 22626471084160 run.py:483] Algo bellman_ford step 3451 current loss 0.564764, current_train_items 110464.
I0302 19:00:11.366309 22626471084160 run.py:483] Algo bellman_ford step 3452 current loss 0.700327, current_train_items 110496.
I0302 19:00:11.396305 22626471084160 run.py:483] Algo bellman_ford step 3453 current loss 0.826032, current_train_items 110528.
I0302 19:00:11.432530 22626471084160 run.py:483] Algo bellman_ford step 3454 current loss 1.018795, current_train_items 110560.
I0302 19:00:11.452538 22626471084160 run.py:483] Algo bellman_ford step 3455 current loss 0.290769, current_train_items 110592.
I0302 19:00:11.468094 22626471084160 run.py:483] Algo bellman_ford step 3456 current loss 0.524209, current_train_items 110624.
I0302 19:00:11.491507 22626471084160 run.py:483] Algo bellman_ford step 3457 current loss 0.708123, current_train_items 110656.
I0302 19:00:11.523684 22626471084160 run.py:483] Algo bellman_ford step 3458 current loss 0.761539, current_train_items 110688.
I0302 19:00:11.556343 22626471084160 run.py:483] Algo bellman_ford step 3459 current loss 0.860817, current_train_items 110720.
I0302 19:00:11.576089 22626471084160 run.py:483] Algo bellman_ford step 3460 current loss 0.297447, current_train_items 110752.
I0302 19:00:11.592541 22626471084160 run.py:483] Algo bellman_ford step 3461 current loss 0.601024, current_train_items 110784.
I0302 19:00:11.615765 22626471084160 run.py:483] Algo bellman_ford step 3462 current loss 0.771398, current_train_items 110816.
I0302 19:00:11.646496 22626471084160 run.py:483] Algo bellman_ford step 3463 current loss 0.773399, current_train_items 110848.
I0302 19:00:11.678970 22626471084160 run.py:483] Algo bellman_ford step 3464 current loss 0.873591, current_train_items 110880.
I0302 19:00:11.698997 22626471084160 run.py:483] Algo bellman_ford step 3465 current loss 0.294254, current_train_items 110912.
I0302 19:00:11.715484 22626471084160 run.py:483] Algo bellman_ford step 3466 current loss 0.598513, current_train_items 110944.
I0302 19:00:11.739522 22626471084160 run.py:483] Algo bellman_ford step 3467 current loss 0.724220, current_train_items 110976.
I0302 19:00:11.771655 22626471084160 run.py:483] Algo bellman_ford step 3468 current loss 0.829347, current_train_items 111008.
I0302 19:00:11.807113 22626471084160 run.py:483] Algo bellman_ford step 3469 current loss 0.963615, current_train_items 111040.
I0302 19:00:11.827175 22626471084160 run.py:483] Algo bellman_ford step 3470 current loss 0.319751, current_train_items 111072.
I0302 19:00:11.844001 22626471084160 run.py:483] Algo bellman_ford step 3471 current loss 0.482511, current_train_items 111104.
I0302 19:00:11.866877 22626471084160 run.py:483] Algo bellman_ford step 3472 current loss 0.653221, current_train_items 111136.
I0302 19:00:11.898640 22626471084160 run.py:483] Algo bellman_ford step 3473 current loss 0.811656, current_train_items 111168.
I0302 19:00:11.932203 22626471084160 run.py:483] Algo bellman_ford step 3474 current loss 0.909846, current_train_items 111200.
I0302 19:00:11.952024 22626471084160 run.py:483] Algo bellman_ford step 3475 current loss 0.319679, current_train_items 111232.
I0302 19:00:11.968181 22626471084160 run.py:483] Algo bellman_ford step 3476 current loss 0.433720, current_train_items 111264.
I0302 19:00:11.990708 22626471084160 run.py:483] Algo bellman_ford step 3477 current loss 0.627322, current_train_items 111296.
I0302 19:00:12.020711 22626471084160 run.py:483] Algo bellman_ford step 3478 current loss 0.607631, current_train_items 111328.
I0302 19:00:12.055659 22626471084160 run.py:483] Algo bellman_ford step 3479 current loss 0.954925, current_train_items 111360.
I0302 19:00:12.075472 22626471084160 run.py:483] Algo bellman_ford step 3480 current loss 0.292270, current_train_items 111392.
I0302 19:00:12.091488 22626471084160 run.py:483] Algo bellman_ford step 3481 current loss 0.531763, current_train_items 111424.
I0302 19:00:12.114977 22626471084160 run.py:483] Algo bellman_ford step 3482 current loss 0.659252, current_train_items 111456.
I0302 19:00:12.144892 22626471084160 run.py:483] Algo bellman_ford step 3483 current loss 0.678705, current_train_items 111488.
I0302 19:00:12.178031 22626471084160 run.py:483] Algo bellman_ford step 3484 current loss 0.786669, current_train_items 111520.
I0302 19:00:12.197946 22626471084160 run.py:483] Algo bellman_ford step 3485 current loss 0.284218, current_train_items 111552.
I0302 19:00:12.214401 22626471084160 run.py:483] Algo bellman_ford step 3486 current loss 0.507206, current_train_items 111584.
I0302 19:00:12.238009 22626471084160 run.py:483] Algo bellman_ford step 3487 current loss 0.666135, current_train_items 111616.
I0302 19:00:12.268297 22626471084160 run.py:483] Algo bellman_ford step 3488 current loss 0.687682, current_train_items 111648.
I0302 19:00:12.300231 22626471084160 run.py:483] Algo bellman_ford step 3489 current loss 0.786056, current_train_items 111680.
I0302 19:00:12.320388 22626471084160 run.py:483] Algo bellman_ford step 3490 current loss 0.324496, current_train_items 111712.
I0302 19:00:12.336378 22626471084160 run.py:483] Algo bellman_ford step 3491 current loss 0.471070, current_train_items 111744.
I0302 19:00:12.359714 22626471084160 run.py:483] Algo bellman_ford step 3492 current loss 0.676001, current_train_items 111776.
I0302 19:00:12.391682 22626471084160 run.py:483] Algo bellman_ford step 3493 current loss 0.783831, current_train_items 111808.
I0302 19:00:12.424576 22626471084160 run.py:483] Algo bellman_ford step 3494 current loss 0.790631, current_train_items 111840.
I0302 19:00:12.444194 22626471084160 run.py:483] Algo bellman_ford step 3495 current loss 0.341977, current_train_items 111872.
I0302 19:00:12.460342 22626471084160 run.py:483] Algo bellman_ford step 3496 current loss 0.514487, current_train_items 111904.
I0302 19:00:12.483492 22626471084160 run.py:483] Algo bellman_ford step 3497 current loss 0.694603, current_train_items 111936.
I0302 19:00:12.514607 22626471084160 run.py:483] Algo bellman_ford step 3498 current loss 0.784881, current_train_items 111968.
I0302 19:00:12.548979 22626471084160 run.py:483] Algo bellman_ford step 3499 current loss 1.003810, current_train_items 112000.
I0302 19:00:12.568907 22626471084160 run.py:483] Algo bellman_ford step 3500 current loss 0.317531, current_train_items 112032.
I0302 19:00:12.576612 22626471084160 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0302 19:00:12.576719 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:00:12.592935 22626471084160 run.py:483] Algo bellman_ford step 3501 current loss 0.434472, current_train_items 112064.
I0302 19:00:12.617003 22626471084160 run.py:483] Algo bellman_ford step 3502 current loss 0.659114, current_train_items 112096.
I0302 19:00:12.648314 22626471084160 run.py:483] Algo bellman_ford step 3503 current loss 0.861585, current_train_items 112128.
I0302 19:00:12.684537 22626471084160 run.py:483] Algo bellman_ford step 3504 current loss 0.873857, current_train_items 112160.
I0302 19:00:12.705125 22626471084160 run.py:483] Algo bellman_ford step 3505 current loss 0.417551, current_train_items 112192.
I0302 19:00:12.720377 22626471084160 run.py:483] Algo bellman_ford step 3506 current loss 0.510898, current_train_items 112224.
I0302 19:00:12.744338 22626471084160 run.py:483] Algo bellman_ford step 3507 current loss 0.728229, current_train_items 112256.
I0302 19:00:12.774459 22626471084160 run.py:483] Algo bellman_ford step 3508 current loss 0.763888, current_train_items 112288.
I0302 19:00:12.805291 22626471084160 run.py:483] Algo bellman_ford step 3509 current loss 0.787926, current_train_items 112320.
I0302 19:00:12.824878 22626471084160 run.py:483] Algo bellman_ford step 3510 current loss 0.401554, current_train_items 112352.
I0302 19:00:12.840911 22626471084160 run.py:483] Algo bellman_ford step 3511 current loss 0.447641, current_train_items 112384.
I0302 19:00:12.864108 22626471084160 run.py:483] Algo bellman_ford step 3512 current loss 0.669249, current_train_items 112416.
I0302 19:00:12.894151 22626471084160 run.py:483] Algo bellman_ford step 3513 current loss 0.694651, current_train_items 112448.
I0302 19:00:12.926747 22626471084160 run.py:483] Algo bellman_ford step 3514 current loss 0.770980, current_train_items 112480.
I0302 19:00:12.946290 22626471084160 run.py:483] Algo bellman_ford step 3515 current loss 0.319521, current_train_items 112512.
I0302 19:00:12.961946 22626471084160 run.py:483] Algo bellman_ford step 3516 current loss 0.488770, current_train_items 112544.
I0302 19:00:12.985312 22626471084160 run.py:483] Algo bellman_ford step 3517 current loss 0.637301, current_train_items 112576.
I0302 19:00:13.016998 22626471084160 run.py:483] Algo bellman_ford step 3518 current loss 0.699402, current_train_items 112608.
I0302 19:00:13.052954 22626471084160 run.py:483] Algo bellman_ford step 3519 current loss 0.910353, current_train_items 112640.
I0302 19:00:13.072606 22626471084160 run.py:483] Algo bellman_ford step 3520 current loss 0.316722, current_train_items 112672.
I0302 19:00:13.088552 22626471084160 run.py:483] Algo bellman_ford step 3521 current loss 0.436043, current_train_items 112704.
I0302 19:00:13.111339 22626471084160 run.py:483] Algo bellman_ford step 3522 current loss 0.559573, current_train_items 112736.
I0302 19:00:13.142074 22626471084160 run.py:483] Algo bellman_ford step 3523 current loss 0.803057, current_train_items 112768.
I0302 19:00:13.175486 22626471084160 run.py:483] Algo bellman_ford step 3524 current loss 0.937912, current_train_items 112800.
I0302 19:00:13.194799 22626471084160 run.py:483] Algo bellman_ford step 3525 current loss 0.415727, current_train_items 112832.
I0302 19:00:13.210768 22626471084160 run.py:483] Algo bellman_ford step 3526 current loss 0.516703, current_train_items 112864.
I0302 19:00:13.234595 22626471084160 run.py:483] Algo bellman_ford step 3527 current loss 0.672637, current_train_items 112896.
I0302 19:00:13.267067 22626471084160 run.py:483] Algo bellman_ford step 3528 current loss 0.738308, current_train_items 112928.
I0302 19:00:13.299532 22626471084160 run.py:483] Algo bellman_ford step 3529 current loss 0.755019, current_train_items 112960.
I0302 19:00:13.318939 22626471084160 run.py:483] Algo bellman_ford step 3530 current loss 0.305017, current_train_items 112992.
I0302 19:00:13.335343 22626471084160 run.py:483] Algo bellman_ford step 3531 current loss 0.523968, current_train_items 113024.
I0302 19:00:13.360101 22626471084160 run.py:483] Algo bellman_ford step 3532 current loss 0.744055, current_train_items 113056.
I0302 19:00:13.394090 22626471084160 run.py:483] Algo bellman_ford step 3533 current loss 0.975776, current_train_items 113088.
I0302 19:00:13.428186 22626471084160 run.py:483] Algo bellman_ford step 3534 current loss 1.000890, current_train_items 113120.
I0302 19:00:13.448114 22626471084160 run.py:483] Algo bellman_ford step 3535 current loss 0.376432, current_train_items 113152.
I0302 19:00:13.464496 22626471084160 run.py:483] Algo bellman_ford step 3536 current loss 0.516468, current_train_items 113184.
I0302 19:00:13.487898 22626471084160 run.py:483] Algo bellman_ford step 3537 current loss 0.651211, current_train_items 113216.
I0302 19:00:13.519582 22626471084160 run.py:483] Algo bellman_ford step 3538 current loss 0.748821, current_train_items 113248.
I0302 19:00:13.555985 22626471084160 run.py:483] Algo bellman_ford step 3539 current loss 0.843638, current_train_items 113280.
I0302 19:00:13.575519 22626471084160 run.py:483] Algo bellman_ford step 3540 current loss 0.409799, current_train_items 113312.
I0302 19:00:13.592114 22626471084160 run.py:483] Algo bellman_ford step 3541 current loss 0.807291, current_train_items 113344.
I0302 19:00:13.616265 22626471084160 run.py:483] Algo bellman_ford step 3542 current loss 0.806982, current_train_items 113376.
I0302 19:00:13.646498 22626471084160 run.py:483] Algo bellman_ford step 3543 current loss 0.711595, current_train_items 113408.
I0302 19:00:13.679278 22626471084160 run.py:483] Algo bellman_ford step 3544 current loss 0.852157, current_train_items 113440.
I0302 19:00:13.698621 22626471084160 run.py:483] Algo bellman_ford step 3545 current loss 0.282814, current_train_items 113472.
I0302 19:00:13.714878 22626471084160 run.py:483] Algo bellman_ford step 3546 current loss 0.539903, current_train_items 113504.
I0302 19:00:13.738815 22626471084160 run.py:483] Algo bellman_ford step 3547 current loss 0.806373, current_train_items 113536.
I0302 19:00:13.769039 22626471084160 run.py:483] Algo bellman_ford step 3548 current loss 0.785956, current_train_items 113568.
I0302 19:00:13.802772 22626471084160 run.py:483] Algo bellman_ford step 3549 current loss 0.871756, current_train_items 113600.
I0302 19:00:13.822867 22626471084160 run.py:483] Algo bellman_ford step 3550 current loss 0.322282, current_train_items 113632.
I0302 19:00:13.831163 22626471084160 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.8935546875, 'score': 0.8935546875, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0302 19:00:13.831273 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.894, val scores are: bellman_ford: 0.894
I0302 19:00:13.849053 22626471084160 run.py:483] Algo bellman_ford step 3551 current loss 0.505847, current_train_items 113664.
I0302 19:00:13.875151 22626471084160 run.py:483] Algo bellman_ford step 3552 current loss 0.758185, current_train_items 113696.
I0302 19:00:13.906606 22626471084160 run.py:483] Algo bellman_ford step 3553 current loss 0.695451, current_train_items 113728.
I0302 19:00:13.941073 22626471084160 run.py:483] Algo bellman_ford step 3554 current loss 0.934142, current_train_items 113760.
I0302 19:00:13.961404 22626471084160 run.py:483] Algo bellman_ford step 3555 current loss 0.398023, current_train_items 113792.
I0302 19:00:13.977566 22626471084160 run.py:483] Algo bellman_ford step 3556 current loss 0.491785, current_train_items 113824.
I0302 19:00:14.001208 22626471084160 run.py:483] Algo bellman_ford step 3557 current loss 0.672008, current_train_items 113856.
I0302 19:00:14.032945 22626471084160 run.py:483] Algo bellman_ford step 3558 current loss 0.820232, current_train_items 113888.
I0302 19:00:14.066171 22626471084160 run.py:483] Algo bellman_ford step 3559 current loss 0.750991, current_train_items 113920.
I0302 19:00:14.086071 22626471084160 run.py:483] Algo bellman_ford step 3560 current loss 0.353145, current_train_items 113952.
I0302 19:00:14.102501 22626471084160 run.py:483] Algo bellman_ford step 3561 current loss 0.551452, current_train_items 113984.
I0302 19:00:14.127656 22626471084160 run.py:483] Algo bellman_ford step 3562 current loss 0.814231, current_train_items 114016.
I0302 19:00:14.157965 22626471084160 run.py:483] Algo bellman_ford step 3563 current loss 0.738987, current_train_items 114048.
I0302 19:00:14.194810 22626471084160 run.py:483] Algo bellman_ford step 3564 current loss 0.940831, current_train_items 114080.
I0302 19:00:14.214656 22626471084160 run.py:483] Algo bellman_ford step 3565 current loss 0.254904, current_train_items 114112.
I0302 19:00:14.231321 22626471084160 run.py:483] Algo bellman_ford step 3566 current loss 0.578691, current_train_items 114144.
I0302 19:00:14.254922 22626471084160 run.py:483] Algo bellman_ford step 3567 current loss 0.560646, current_train_items 114176.
I0302 19:00:14.285792 22626471084160 run.py:483] Algo bellman_ford step 3568 current loss 0.739069, current_train_items 114208.
I0302 19:00:14.320293 22626471084160 run.py:483] Algo bellman_ford step 3569 current loss 0.895692, current_train_items 114240.
I0302 19:00:14.340123 22626471084160 run.py:483] Algo bellman_ford step 3570 current loss 0.261779, current_train_items 114272.
I0302 19:00:14.356094 22626471084160 run.py:483] Algo bellman_ford step 3571 current loss 0.496696, current_train_items 114304.
I0302 19:00:14.379443 22626471084160 run.py:483] Algo bellman_ford step 3572 current loss 0.812342, current_train_items 114336.
I0302 19:00:14.409764 22626471084160 run.py:483] Algo bellman_ford step 3573 current loss 0.736055, current_train_items 114368.
I0302 19:00:14.445121 22626471084160 run.py:483] Algo bellman_ford step 3574 current loss 0.864126, current_train_items 114400.
I0302 19:00:14.465125 22626471084160 run.py:483] Algo bellman_ford step 3575 current loss 0.313255, current_train_items 114432.
I0302 19:00:14.481524 22626471084160 run.py:483] Algo bellman_ford step 3576 current loss 0.512217, current_train_items 114464.
I0302 19:00:14.505022 22626471084160 run.py:483] Algo bellman_ford step 3577 current loss 0.685207, current_train_items 114496.
I0302 19:00:14.535613 22626471084160 run.py:483] Algo bellman_ford step 3578 current loss 0.634964, current_train_items 114528.
I0302 19:00:14.568305 22626471084160 run.py:483] Algo bellman_ford step 3579 current loss 0.765893, current_train_items 114560.
I0302 19:00:14.588382 22626471084160 run.py:483] Algo bellman_ford step 3580 current loss 0.332135, current_train_items 114592.
I0302 19:00:14.604855 22626471084160 run.py:483] Algo bellman_ford step 3581 current loss 0.518152, current_train_items 114624.
I0302 19:00:14.629487 22626471084160 run.py:483] Algo bellman_ford step 3582 current loss 0.694115, current_train_items 114656.
I0302 19:00:14.661669 22626471084160 run.py:483] Algo bellman_ford step 3583 current loss 0.750261, current_train_items 114688.
I0302 19:00:14.696381 22626471084160 run.py:483] Algo bellman_ford step 3584 current loss 0.837883, current_train_items 114720.
I0302 19:00:14.716191 22626471084160 run.py:483] Algo bellman_ford step 3585 current loss 0.350525, current_train_items 114752.
I0302 19:00:14.732789 22626471084160 run.py:483] Algo bellman_ford step 3586 current loss 0.592573, current_train_items 114784.
I0302 19:00:14.756095 22626471084160 run.py:483] Algo bellman_ford step 3587 current loss 0.755489, current_train_items 114816.
I0302 19:00:14.787929 22626471084160 run.py:483] Algo bellman_ford step 3588 current loss 0.811871, current_train_items 114848.
I0302 19:00:14.823094 22626471084160 run.py:483] Algo bellman_ford step 3589 current loss 0.879786, current_train_items 114880.
I0302 19:00:14.842950 22626471084160 run.py:483] Algo bellman_ford step 3590 current loss 0.339173, current_train_items 114912.
I0302 19:00:14.858957 22626471084160 run.py:483] Algo bellman_ford step 3591 current loss 0.477207, current_train_items 114944.
I0302 19:00:14.883991 22626471084160 run.py:483] Algo bellman_ford step 3592 current loss 0.852081, current_train_items 114976.
I0302 19:00:14.916531 22626471084160 run.py:483] Algo bellman_ford step 3593 current loss 0.808079, current_train_items 115008.
I0302 19:00:14.951064 22626471084160 run.py:483] Algo bellman_ford step 3594 current loss 0.941735, current_train_items 115040.
I0302 19:00:14.970891 22626471084160 run.py:483] Algo bellman_ford step 3595 current loss 0.314441, current_train_items 115072.
I0302 19:00:14.987433 22626471084160 run.py:483] Algo bellman_ford step 3596 current loss 0.538236, current_train_items 115104.
I0302 19:00:15.010735 22626471084160 run.py:483] Algo bellman_ford step 3597 current loss 0.706719, current_train_items 115136.
I0302 19:00:15.040111 22626471084160 run.py:483] Algo bellman_ford step 3598 current loss 0.681076, current_train_items 115168.
I0302 19:00:15.074304 22626471084160 run.py:483] Algo bellman_ford step 3599 current loss 0.824893, current_train_items 115200.
I0302 19:00:15.094533 22626471084160 run.py:483] Algo bellman_ford step 3600 current loss 0.336483, current_train_items 115232.
I0302 19:00:15.102182 22626471084160 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0302 19:00:15.102289 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 19:00:15.119427 22626471084160 run.py:483] Algo bellman_ford step 3601 current loss 0.582579, current_train_items 115264.
I0302 19:00:15.143488 22626471084160 run.py:483] Algo bellman_ford step 3602 current loss 0.720497, current_train_items 115296.
I0302 19:00:15.174185 22626471084160 run.py:483] Algo bellman_ford step 3603 current loss 0.675203, current_train_items 115328.
I0302 19:00:15.210758 22626471084160 run.py:483] Algo bellman_ford step 3604 current loss 0.861111, current_train_items 115360.
I0302 19:00:15.230663 22626471084160 run.py:483] Algo bellman_ford step 3605 current loss 0.331544, current_train_items 115392.
I0302 19:00:15.246949 22626471084160 run.py:483] Algo bellman_ford step 3606 current loss 0.522688, current_train_items 115424.
I0302 19:00:15.269550 22626471084160 run.py:483] Algo bellman_ford step 3607 current loss 0.561783, current_train_items 115456.
I0302 19:00:15.299322 22626471084160 run.py:483] Algo bellman_ford step 3608 current loss 0.795922, current_train_items 115488.
I0302 19:00:15.330986 22626471084160 run.py:483] Algo bellman_ford step 3609 current loss 0.821939, current_train_items 115520.
I0302 19:00:15.350523 22626471084160 run.py:483] Algo bellman_ford step 3610 current loss 0.370643, current_train_items 115552.
I0302 19:00:15.366966 22626471084160 run.py:483] Algo bellman_ford step 3611 current loss 0.522746, current_train_items 115584.
I0302 19:00:15.391320 22626471084160 run.py:483] Algo bellman_ford step 3612 current loss 0.706487, current_train_items 115616.
I0302 19:00:15.421318 22626471084160 run.py:483] Algo bellman_ford step 3613 current loss 0.629994, current_train_items 115648.
I0302 19:00:15.453488 22626471084160 run.py:483] Algo bellman_ford step 3614 current loss 0.807000, current_train_items 115680.
I0302 19:00:15.472802 22626471084160 run.py:483] Algo bellman_ford step 3615 current loss 0.337894, current_train_items 115712.
I0302 19:00:15.489181 22626471084160 run.py:483] Algo bellman_ford step 3616 current loss 0.575498, current_train_items 115744.
I0302 19:00:15.512621 22626471084160 run.py:483] Algo bellman_ford step 3617 current loss 0.638907, current_train_items 115776.
I0302 19:00:15.543735 22626471084160 run.py:483] Algo bellman_ford step 3618 current loss 0.879358, current_train_items 115808.
I0302 19:00:15.574014 22626471084160 run.py:483] Algo bellman_ford step 3619 current loss 0.734886, current_train_items 115840.
I0302 19:00:15.593568 22626471084160 run.py:483] Algo bellman_ford step 3620 current loss 0.277520, current_train_items 115872.
I0302 19:00:15.609889 22626471084160 run.py:483] Algo bellman_ford step 3621 current loss 0.545308, current_train_items 115904.
I0302 19:00:15.632961 22626471084160 run.py:483] Algo bellman_ford step 3622 current loss 0.675557, current_train_items 115936.
I0302 19:00:15.664352 22626471084160 run.py:483] Algo bellman_ford step 3623 current loss 0.795100, current_train_items 115968.
I0302 19:00:15.697115 22626471084160 run.py:483] Algo bellman_ford step 3624 current loss 0.918705, current_train_items 116000.
I0302 19:00:15.716324 22626471084160 run.py:483] Algo bellman_ford step 3625 current loss 0.351160, current_train_items 116032.
I0302 19:00:15.732639 22626471084160 run.py:483] Algo bellman_ford step 3626 current loss 0.545402, current_train_items 116064.
I0302 19:00:15.756473 22626471084160 run.py:483] Algo bellman_ford step 3627 current loss 0.657138, current_train_items 116096.
I0302 19:00:15.787897 22626471084160 run.py:483] Algo bellman_ford step 3628 current loss 0.824128, current_train_items 116128.
I0302 19:00:15.820450 22626471084160 run.py:483] Algo bellman_ford step 3629 current loss 0.907085, current_train_items 116160.
I0302 19:00:15.839939 22626471084160 run.py:483] Algo bellman_ford step 3630 current loss 0.344674, current_train_items 116192.
I0302 19:00:15.855964 22626471084160 run.py:483] Algo bellman_ford step 3631 current loss 0.516480, current_train_items 116224.
I0302 19:00:15.879688 22626471084160 run.py:483] Algo bellman_ford step 3632 current loss 0.666503, current_train_items 116256.
I0302 19:00:15.910418 22626471084160 run.py:483] Algo bellman_ford step 3633 current loss 0.883086, current_train_items 116288.
I0302 19:00:15.943120 22626471084160 run.py:483] Algo bellman_ford step 3634 current loss 0.988517, current_train_items 116320.
I0302 19:00:15.962455 22626471084160 run.py:483] Algo bellman_ford step 3635 current loss 0.301225, current_train_items 116352.
I0302 19:00:15.978636 22626471084160 run.py:483] Algo bellman_ford step 3636 current loss 0.634415, current_train_items 116384.
I0302 19:00:16.000926 22626471084160 run.py:483] Algo bellman_ford step 3637 current loss 0.710689, current_train_items 116416.
I0302 19:00:16.031918 22626471084160 run.py:483] Algo bellman_ford step 3638 current loss 0.815514, current_train_items 116448.
I0302 19:00:16.064174 22626471084160 run.py:483] Algo bellman_ford step 3639 current loss 0.826929, current_train_items 116480.
I0302 19:00:16.083554 22626471084160 run.py:483] Algo bellman_ford step 3640 current loss 0.282904, current_train_items 116512.
I0302 19:00:16.099369 22626471084160 run.py:483] Algo bellman_ford step 3641 current loss 0.477831, current_train_items 116544.
I0302 19:00:16.123620 22626471084160 run.py:483] Algo bellman_ford step 3642 current loss 0.817552, current_train_items 116576.
I0302 19:00:16.157229 22626471084160 run.py:483] Algo bellman_ford step 3643 current loss 0.881365, current_train_items 116608.
I0302 19:00:16.191296 22626471084160 run.py:483] Algo bellman_ford step 3644 current loss 1.015308, current_train_items 116640.
I0302 19:00:16.210600 22626471084160 run.py:483] Algo bellman_ford step 3645 current loss 0.289165, current_train_items 116672.
I0302 19:00:16.226860 22626471084160 run.py:483] Algo bellman_ford step 3646 current loss 0.461098, current_train_items 116704.
I0302 19:00:16.251828 22626471084160 run.py:483] Algo bellman_ford step 3647 current loss 0.664856, current_train_items 116736.
I0302 19:00:16.282299 22626471084160 run.py:483] Algo bellman_ford step 3648 current loss 0.691771, current_train_items 116768.
I0302 19:00:16.315972 22626471084160 run.py:483] Algo bellman_ford step 3649 current loss 0.896475, current_train_items 116800.
I0302 19:00:16.335170 22626471084160 run.py:483] Algo bellman_ford step 3650 current loss 0.362449, current_train_items 116832.
I0302 19:00:16.343072 22626471084160 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0302 19:00:16.343188 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:00:16.360169 22626471084160 run.py:483] Algo bellman_ford step 3651 current loss 0.532767, current_train_items 116864.
I0302 19:00:16.382997 22626471084160 run.py:483] Algo bellman_ford step 3652 current loss 0.660826, current_train_items 116896.
I0302 19:00:16.413537 22626471084160 run.py:483] Algo bellman_ford step 3653 current loss 0.613031, current_train_items 116928.
I0302 19:00:16.446738 22626471084160 run.py:483] Algo bellman_ford step 3654 current loss 0.871508, current_train_items 116960.
I0302 19:00:16.466498 22626471084160 run.py:483] Algo bellman_ford step 3655 current loss 0.317256, current_train_items 116992.
I0302 19:00:16.481990 22626471084160 run.py:483] Algo bellman_ford step 3656 current loss 0.515381, current_train_items 117024.
I0302 19:00:16.505774 22626471084160 run.py:483] Algo bellman_ford step 3657 current loss 0.635900, current_train_items 117056.
I0302 19:00:16.536199 22626471084160 run.py:483] Algo bellman_ford step 3658 current loss 0.619911, current_train_items 117088.
I0302 19:00:16.570029 22626471084160 run.py:483] Algo bellman_ford step 3659 current loss 0.909890, current_train_items 117120.
I0302 19:00:16.589753 22626471084160 run.py:483] Algo bellman_ford step 3660 current loss 0.270530, current_train_items 117152.
I0302 19:00:16.606222 22626471084160 run.py:483] Algo bellman_ford step 3661 current loss 0.550866, current_train_items 117184.
I0302 19:00:16.628828 22626471084160 run.py:483] Algo bellman_ford step 3662 current loss 0.662017, current_train_items 117216.
I0302 19:00:16.660391 22626471084160 run.py:483] Algo bellman_ford step 3663 current loss 0.773721, current_train_items 117248.
I0302 19:00:16.694971 22626471084160 run.py:483] Algo bellman_ford step 3664 current loss 0.929034, current_train_items 117280.
I0302 19:00:16.714223 22626471084160 run.py:483] Algo bellman_ford step 3665 current loss 0.396916, current_train_items 117312.
I0302 19:00:16.730117 22626471084160 run.py:483] Algo bellman_ford step 3666 current loss 0.519822, current_train_items 117344.
I0302 19:00:16.753663 22626471084160 run.py:483] Algo bellman_ford step 3667 current loss 0.683566, current_train_items 117376.
I0302 19:00:16.784999 22626471084160 run.py:483] Algo bellman_ford step 3668 current loss 0.732428, current_train_items 117408.
I0302 19:00:16.820089 22626471084160 run.py:483] Algo bellman_ford step 3669 current loss 0.874846, current_train_items 117440.
I0302 19:00:16.839792 22626471084160 run.py:483] Algo bellman_ford step 3670 current loss 0.343257, current_train_items 117472.
I0302 19:00:16.856252 22626471084160 run.py:483] Algo bellman_ford step 3671 current loss 0.608316, current_train_items 117504.
I0302 19:00:16.878979 22626471084160 run.py:483] Algo bellman_ford step 3672 current loss 0.684202, current_train_items 117536.
I0302 19:00:16.910031 22626471084160 run.py:483] Algo bellman_ford step 3673 current loss 0.919726, current_train_items 117568.
I0302 19:00:16.944508 22626471084160 run.py:483] Algo bellman_ford step 3674 current loss 1.088490, current_train_items 117600.
I0302 19:00:16.964047 22626471084160 run.py:483] Algo bellman_ford step 3675 current loss 0.248727, current_train_items 117632.
I0302 19:00:16.979900 22626471084160 run.py:483] Algo bellman_ford step 3676 current loss 0.503739, current_train_items 117664.
I0302 19:00:17.002688 22626471084160 run.py:483] Algo bellman_ford step 3677 current loss 0.643919, current_train_items 117696.
I0302 19:00:17.033452 22626471084160 run.py:483] Algo bellman_ford step 3678 current loss 0.776603, current_train_items 117728.
I0302 19:00:17.067198 22626471084160 run.py:483] Algo bellman_ford step 3679 current loss 0.873412, current_train_items 117760.
I0302 19:00:17.086576 22626471084160 run.py:483] Algo bellman_ford step 3680 current loss 0.301858, current_train_items 117792.
I0302 19:00:17.103024 22626471084160 run.py:483] Algo bellman_ford step 3681 current loss 0.538774, current_train_items 117824.
I0302 19:00:17.126863 22626471084160 run.py:483] Algo bellman_ford step 3682 current loss 0.720554, current_train_items 117856.
I0302 19:00:17.157132 22626471084160 run.py:483] Algo bellman_ford step 3683 current loss 0.753809, current_train_items 117888.
I0302 19:00:17.190557 22626471084160 run.py:483] Algo bellman_ford step 3684 current loss 0.890418, current_train_items 117920.
I0302 19:00:17.210145 22626471084160 run.py:483] Algo bellman_ford step 3685 current loss 0.252126, current_train_items 117952.
I0302 19:00:17.226299 22626471084160 run.py:483] Algo bellman_ford step 3686 current loss 0.547964, current_train_items 117984.
I0302 19:00:17.248955 22626471084160 run.py:483] Algo bellman_ford step 3687 current loss 0.562696, current_train_items 118016.
I0302 19:00:17.280982 22626471084160 run.py:483] Algo bellman_ford step 3688 current loss 0.761690, current_train_items 118048.
I0302 19:00:17.316185 22626471084160 run.py:483] Algo bellman_ford step 3689 current loss 0.986603, current_train_items 118080.
I0302 19:00:17.335837 22626471084160 run.py:483] Algo bellman_ford step 3690 current loss 0.310875, current_train_items 118112.
I0302 19:00:17.352066 22626471084160 run.py:483] Algo bellman_ford step 3691 current loss 0.513776, current_train_items 118144.
I0302 19:00:17.373968 22626471084160 run.py:483] Algo bellman_ford step 3692 current loss 0.702297, current_train_items 118176.
I0302 19:00:17.406075 22626471084160 run.py:483] Algo bellman_ford step 3693 current loss 0.784238, current_train_items 118208.
I0302 19:00:17.438650 22626471084160 run.py:483] Algo bellman_ford step 3694 current loss 0.761383, current_train_items 118240.
I0302 19:00:17.458070 22626471084160 run.py:483] Algo bellman_ford step 3695 current loss 0.357355, current_train_items 118272.
I0302 19:00:17.474925 22626471084160 run.py:483] Algo bellman_ford step 3696 current loss 0.581525, current_train_items 118304.
I0302 19:00:17.498487 22626471084160 run.py:483] Algo bellman_ford step 3697 current loss 0.638111, current_train_items 118336.
I0302 19:00:17.530105 22626471084160 run.py:483] Algo bellman_ford step 3698 current loss 0.752477, current_train_items 118368.
I0302 19:00:17.561898 22626471084160 run.py:483] Algo bellman_ford step 3699 current loss 0.904706, current_train_items 118400.
I0302 19:00:17.581587 22626471084160 run.py:483] Algo bellman_ford step 3700 current loss 0.396633, current_train_items 118432.
I0302 19:00:17.589463 22626471084160 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0302 19:00:17.589568 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 19:00:17.606060 22626471084160 run.py:483] Algo bellman_ford step 3701 current loss 0.434072, current_train_items 118464.
I0302 19:00:17.630177 22626471084160 run.py:483] Algo bellman_ford step 3702 current loss 0.695082, current_train_items 118496.
I0302 19:00:17.663042 22626471084160 run.py:483] Algo bellman_ford step 3703 current loss 0.856155, current_train_items 118528.
I0302 19:00:17.696729 22626471084160 run.py:483] Algo bellman_ford step 3704 current loss 0.754999, current_train_items 118560.
I0302 19:00:17.716465 22626471084160 run.py:483] Algo bellman_ford step 3705 current loss 0.334524, current_train_items 118592.
I0302 19:00:17.732720 22626471084160 run.py:483] Algo bellman_ford step 3706 current loss 0.703937, current_train_items 118624.
I0302 19:00:17.756426 22626471084160 run.py:483] Algo bellman_ford step 3707 current loss 0.751203, current_train_items 118656.
I0302 19:00:17.786636 22626471084160 run.py:483] Algo bellman_ford step 3708 current loss 0.665860, current_train_items 118688.
I0302 19:00:17.820542 22626471084160 run.py:483] Algo bellman_ford step 3709 current loss 0.921593, current_train_items 118720.
I0302 19:00:17.840575 22626471084160 run.py:483] Algo bellman_ford step 3710 current loss 0.370908, current_train_items 118752.
I0302 19:00:17.856934 22626471084160 run.py:483] Algo bellman_ford step 3711 current loss 0.579969, current_train_items 118784.
I0302 19:00:17.880574 22626471084160 run.py:483] Algo bellman_ford step 3712 current loss 0.764912, current_train_items 118816.
I0302 19:00:17.913164 22626471084160 run.py:483] Algo bellman_ford step 3713 current loss 0.917749, current_train_items 118848.
I0302 19:00:17.947649 22626471084160 run.py:483] Algo bellman_ford step 3714 current loss 0.956745, current_train_items 118880.
I0302 19:00:17.966991 22626471084160 run.py:483] Algo bellman_ford step 3715 current loss 0.251168, current_train_items 118912.
I0302 19:00:17.983033 22626471084160 run.py:483] Algo bellman_ford step 3716 current loss 0.506505, current_train_items 118944.
I0302 19:00:18.006529 22626471084160 run.py:483] Algo bellman_ford step 3717 current loss 0.674567, current_train_items 118976.
I0302 19:00:18.037714 22626471084160 run.py:483] Algo bellman_ford step 3718 current loss 0.868114, current_train_items 119008.
I0302 19:00:18.069975 22626471084160 run.py:483] Algo bellman_ford step 3719 current loss 0.939798, current_train_items 119040.
I0302 19:00:18.089838 22626471084160 run.py:483] Algo bellman_ford step 3720 current loss 0.389421, current_train_items 119072.
I0302 19:00:18.106272 22626471084160 run.py:483] Algo bellman_ford step 3721 current loss 0.599626, current_train_items 119104.
I0302 19:00:18.129669 22626471084160 run.py:483] Algo bellman_ford step 3722 current loss 0.555180, current_train_items 119136.
I0302 19:00:18.160073 22626471084160 run.py:483] Algo bellman_ford step 3723 current loss 0.787229, current_train_items 119168.
I0302 19:00:18.193331 22626471084160 run.py:483] Algo bellman_ford step 3724 current loss 0.812336, current_train_items 119200.
I0302 19:00:18.213025 22626471084160 run.py:483] Algo bellman_ford step 3725 current loss 0.382738, current_train_items 119232.
I0302 19:00:18.229555 22626471084160 run.py:483] Algo bellman_ford step 3726 current loss 0.392360, current_train_items 119264.
I0302 19:00:18.253851 22626471084160 run.py:483] Algo bellman_ford step 3727 current loss 0.649883, current_train_items 119296.
I0302 19:00:18.284821 22626471084160 run.py:483] Algo bellman_ford step 3728 current loss 0.852731, current_train_items 119328.
I0302 19:00:18.318382 22626471084160 run.py:483] Algo bellman_ford step 3729 current loss 0.907842, current_train_items 119360.
I0302 19:00:18.338230 22626471084160 run.py:483] Algo bellman_ford step 3730 current loss 0.267211, current_train_items 119392.
I0302 19:00:18.354306 22626471084160 run.py:483] Algo bellman_ford step 3731 current loss 0.478981, current_train_items 119424.
I0302 19:00:18.378222 22626471084160 run.py:483] Algo bellman_ford step 3732 current loss 0.655441, current_train_items 119456.
I0302 19:00:18.407214 22626471084160 run.py:483] Algo bellman_ford step 3733 current loss 0.692996, current_train_items 119488.
I0302 19:00:18.441442 22626471084160 run.py:483] Algo bellman_ford step 3734 current loss 0.908598, current_train_items 119520.
I0302 19:00:18.460960 22626471084160 run.py:483] Algo bellman_ford step 3735 current loss 0.296484, current_train_items 119552.
I0302 19:00:18.477188 22626471084160 run.py:483] Algo bellman_ford step 3736 current loss 0.458243, current_train_items 119584.
I0302 19:00:18.499946 22626471084160 run.py:483] Algo bellman_ford step 3737 current loss 0.541651, current_train_items 119616.
I0302 19:00:18.531348 22626471084160 run.py:483] Algo bellman_ford step 3738 current loss 0.838251, current_train_items 119648.
I0302 19:00:18.564585 22626471084160 run.py:483] Algo bellman_ford step 3739 current loss 0.799047, current_train_items 119680.
I0302 19:00:18.584263 22626471084160 run.py:483] Algo bellman_ford step 3740 current loss 0.348416, current_train_items 119712.
I0302 19:00:18.600361 22626471084160 run.py:483] Algo bellman_ford step 3741 current loss 0.499709, current_train_items 119744.
I0302 19:00:18.623473 22626471084160 run.py:483] Algo bellman_ford step 3742 current loss 0.602140, current_train_items 119776.
I0302 19:00:18.654169 22626471084160 run.py:483] Algo bellman_ford step 3743 current loss 0.721036, current_train_items 119808.
I0302 19:00:18.685837 22626471084160 run.py:483] Algo bellman_ford step 3744 current loss 0.796312, current_train_items 119840.
I0302 19:00:18.705299 22626471084160 run.py:483] Algo bellman_ford step 3745 current loss 0.273006, current_train_items 119872.
I0302 19:00:18.721309 22626471084160 run.py:483] Algo bellman_ford step 3746 current loss 0.450495, current_train_items 119904.
I0302 19:00:18.745352 22626471084160 run.py:483] Algo bellman_ford step 3747 current loss 0.562087, current_train_items 119936.
I0302 19:00:18.777169 22626471084160 run.py:483] Algo bellman_ford step 3748 current loss 0.748824, current_train_items 119968.
I0302 19:00:18.812145 22626471084160 run.py:483] Algo bellman_ford step 3749 current loss 0.971797, current_train_items 120000.
I0302 19:00:18.831789 22626471084160 run.py:483] Algo bellman_ford step 3750 current loss 0.293056, current_train_items 120032.
I0302 19:00:18.839922 22626471084160 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0302 19:00:18.840046 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:00:18.856976 22626471084160 run.py:483] Algo bellman_ford step 3751 current loss 0.615598, current_train_items 120064.
I0302 19:00:18.881126 22626471084160 run.py:483] Algo bellman_ford step 3752 current loss 0.679964, current_train_items 120096.
I0302 19:00:18.912480 22626471084160 run.py:483] Algo bellman_ford step 3753 current loss 0.721449, current_train_items 120128.
I0302 19:00:18.948590 22626471084160 run.py:483] Algo bellman_ford step 3754 current loss 0.878275, current_train_items 120160.
I0302 19:00:18.968537 22626471084160 run.py:483] Algo bellman_ford step 3755 current loss 0.299483, current_train_items 120192.
I0302 19:00:18.984508 22626471084160 run.py:483] Algo bellman_ford step 3756 current loss 0.574659, current_train_items 120224.
I0302 19:00:19.008609 22626471084160 run.py:483] Algo bellman_ford step 3757 current loss 0.710396, current_train_items 120256.
I0302 19:00:19.039435 22626471084160 run.py:483] Algo bellman_ford step 3758 current loss 0.838004, current_train_items 120288.
I0302 19:00:19.071189 22626471084160 run.py:483] Algo bellman_ford step 3759 current loss 0.855185, current_train_items 120320.
I0302 19:00:19.091180 22626471084160 run.py:483] Algo bellman_ford step 3760 current loss 0.298341, current_train_items 120352.
I0302 19:00:19.107456 22626471084160 run.py:483] Algo bellman_ford step 3761 current loss 0.603291, current_train_items 120384.
I0302 19:00:19.130130 22626471084160 run.py:483] Algo bellman_ford step 3762 current loss 0.595164, current_train_items 120416.
I0302 19:00:19.160913 22626471084160 run.py:483] Algo bellman_ford step 3763 current loss 0.712746, current_train_items 120448.
I0302 19:00:19.194654 22626471084160 run.py:483] Algo bellman_ford step 3764 current loss 0.983443, current_train_items 120480.
I0302 19:00:19.214127 22626471084160 run.py:483] Algo bellman_ford step 3765 current loss 0.385720, current_train_items 120512.
I0302 19:00:19.230816 22626471084160 run.py:483] Algo bellman_ford step 3766 current loss 0.477735, current_train_items 120544.
I0302 19:00:19.254645 22626471084160 run.py:483] Algo bellman_ford step 3767 current loss 0.713687, current_train_items 120576.
I0302 19:00:19.286630 22626471084160 run.py:483] Algo bellman_ford step 3768 current loss 0.757812, current_train_items 120608.
I0302 19:00:19.319270 22626471084160 run.py:483] Algo bellman_ford step 3769 current loss 0.825997, current_train_items 120640.
I0302 19:00:19.339324 22626471084160 run.py:483] Algo bellman_ford step 3770 current loss 0.295506, current_train_items 120672.
I0302 19:00:19.355680 22626471084160 run.py:483] Algo bellman_ford step 3771 current loss 0.563430, current_train_items 120704.
I0302 19:00:19.378398 22626471084160 run.py:483] Algo bellman_ford step 3772 current loss 0.622612, current_train_items 120736.
I0302 19:00:19.408813 22626471084160 run.py:483] Algo bellman_ford step 3773 current loss 0.678528, current_train_items 120768.
I0302 19:00:19.439611 22626471084160 run.py:483] Algo bellman_ford step 3774 current loss 0.808994, current_train_items 120800.
I0302 19:00:19.459742 22626471084160 run.py:483] Algo bellman_ford step 3775 current loss 0.332451, current_train_items 120832.
I0302 19:00:19.476401 22626471084160 run.py:483] Algo bellman_ford step 3776 current loss 0.545372, current_train_items 120864.
I0302 19:00:19.499048 22626471084160 run.py:483] Algo bellman_ford step 3777 current loss 0.660889, current_train_items 120896.
I0302 19:00:19.530755 22626471084160 run.py:483] Algo bellman_ford step 3778 current loss 0.728843, current_train_items 120928.
I0302 19:00:19.564604 22626471084160 run.py:483] Algo bellman_ford step 3779 current loss 0.782800, current_train_items 120960.
I0302 19:00:19.584093 22626471084160 run.py:483] Algo bellman_ford step 3780 current loss 0.271481, current_train_items 120992.
I0302 19:00:19.600411 22626471084160 run.py:483] Algo bellman_ford step 3781 current loss 0.581896, current_train_items 121024.
I0302 19:00:19.623685 22626471084160 run.py:483] Algo bellman_ford step 3782 current loss 0.670707, current_train_items 121056.
I0302 19:00:19.652827 22626471084160 run.py:483] Algo bellman_ford step 3783 current loss 0.715288, current_train_items 121088.
I0302 19:00:19.688933 22626471084160 run.py:483] Algo bellman_ford step 3784 current loss 0.883682, current_train_items 121120.
I0302 19:00:19.708450 22626471084160 run.py:483] Algo bellman_ford step 3785 current loss 0.308458, current_train_items 121152.
I0302 19:00:19.724564 22626471084160 run.py:483] Algo bellman_ford step 3786 current loss 0.389159, current_train_items 121184.
I0302 19:00:19.748426 22626471084160 run.py:483] Algo bellman_ford step 3787 current loss 0.735584, current_train_items 121216.
I0302 19:00:19.779288 22626471084160 run.py:483] Algo bellman_ford step 3788 current loss 0.842039, current_train_items 121248.
I0302 19:00:19.813205 22626471084160 run.py:483] Algo bellman_ford step 3789 current loss 0.798304, current_train_items 121280.
I0302 19:00:19.833134 22626471084160 run.py:483] Algo bellman_ford step 3790 current loss 0.257438, current_train_items 121312.
I0302 19:00:19.849390 22626471084160 run.py:483] Algo bellman_ford step 3791 current loss 0.491448, current_train_items 121344.
I0302 19:00:19.871898 22626471084160 run.py:483] Algo bellman_ford step 3792 current loss 0.649495, current_train_items 121376.
I0302 19:00:19.903870 22626471084160 run.py:483] Algo bellman_ford step 3793 current loss 0.994339, current_train_items 121408.
I0302 19:00:19.937161 22626471084160 run.py:483] Algo bellman_ford step 3794 current loss 0.804711, current_train_items 121440.
I0302 19:00:19.956799 22626471084160 run.py:483] Algo bellman_ford step 3795 current loss 0.378521, current_train_items 121472.
I0302 19:00:19.973338 22626471084160 run.py:483] Algo bellman_ford step 3796 current loss 0.564228, current_train_items 121504.
I0302 19:00:19.997057 22626471084160 run.py:483] Algo bellman_ford step 3797 current loss 0.638530, current_train_items 121536.
I0302 19:00:20.028781 22626471084160 run.py:483] Algo bellman_ford step 3798 current loss 0.925198, current_train_items 121568.
I0302 19:00:20.061427 22626471084160 run.py:483] Algo bellman_ford step 3799 current loss 0.827458, current_train_items 121600.
I0302 19:00:20.081334 22626471084160 run.py:483] Algo bellman_ford step 3800 current loss 0.335406, current_train_items 121632.
I0302 19:00:20.089152 22626471084160 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.880859375, 'score': 0.880859375, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0302 19:00:20.089267 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.881, val scores are: bellman_ford: 0.881
I0302 19:00:20.105952 22626471084160 run.py:483] Algo bellman_ford step 3801 current loss 0.549827, current_train_items 121664.
I0302 19:00:20.130603 22626471084160 run.py:483] Algo bellman_ford step 3802 current loss 0.559388, current_train_items 121696.
I0302 19:00:20.161695 22626471084160 run.py:483] Algo bellman_ford step 3803 current loss 0.778674, current_train_items 121728.
I0302 19:00:20.193922 22626471084160 run.py:483] Algo bellman_ford step 3804 current loss 0.792820, current_train_items 121760.
I0302 19:00:20.214037 22626471084160 run.py:483] Algo bellman_ford step 3805 current loss 0.308695, current_train_items 121792.
I0302 19:00:20.230145 22626471084160 run.py:483] Algo bellman_ford step 3806 current loss 0.567688, current_train_items 121824.
I0302 19:00:20.253901 22626471084160 run.py:483] Algo bellman_ford step 3807 current loss 0.556079, current_train_items 121856.
I0302 19:00:20.284754 22626471084160 run.py:483] Algo bellman_ford step 3808 current loss 0.704434, current_train_items 121888.
I0302 19:00:20.319780 22626471084160 run.py:483] Algo bellman_ford step 3809 current loss 0.770167, current_train_items 121920.
I0302 19:00:20.339257 22626471084160 run.py:483] Algo bellman_ford step 3810 current loss 0.390738, current_train_items 121952.
I0302 19:00:20.355884 22626471084160 run.py:483] Algo bellman_ford step 3811 current loss 0.461685, current_train_items 121984.
I0302 19:00:20.379572 22626471084160 run.py:483] Algo bellman_ford step 3812 current loss 0.604164, current_train_items 122016.
I0302 19:00:20.412480 22626471084160 run.py:483] Algo bellman_ford step 3813 current loss 0.799823, current_train_items 122048.
I0302 19:00:20.448192 22626471084160 run.py:483] Algo bellman_ford step 3814 current loss 0.974128, current_train_items 122080.
I0302 19:00:20.467939 22626471084160 run.py:483] Algo bellman_ford step 3815 current loss 0.367741, current_train_items 122112.
I0302 19:00:20.484505 22626471084160 run.py:483] Algo bellman_ford step 3816 current loss 0.522983, current_train_items 122144.
I0302 19:00:20.508375 22626471084160 run.py:483] Algo bellman_ford step 3817 current loss 0.665101, current_train_items 122176.
I0302 19:00:20.539279 22626471084160 run.py:483] Algo bellman_ford step 3818 current loss 0.653343, current_train_items 122208.
I0302 19:00:20.573019 22626471084160 run.py:483] Algo bellman_ford step 3819 current loss 0.755113, current_train_items 122240.
I0302 19:00:20.592652 22626471084160 run.py:483] Algo bellman_ford step 3820 current loss 0.299152, current_train_items 122272.
I0302 19:00:20.608510 22626471084160 run.py:483] Algo bellman_ford step 3821 current loss 0.479947, current_train_items 122304.
I0302 19:00:20.631932 22626471084160 run.py:483] Algo bellman_ford step 3822 current loss 0.769066, current_train_items 122336.
I0302 19:00:20.663298 22626471084160 run.py:483] Algo bellman_ford step 3823 current loss 0.807170, current_train_items 122368.
I0302 19:00:20.697550 22626471084160 run.py:483] Algo bellman_ford step 3824 current loss 0.918099, current_train_items 122400.
I0302 19:00:20.717259 22626471084160 run.py:483] Algo bellman_ford step 3825 current loss 0.301664, current_train_items 122432.
I0302 19:00:20.733180 22626471084160 run.py:483] Algo bellman_ford step 3826 current loss 0.467397, current_train_items 122464.
I0302 19:00:20.757366 22626471084160 run.py:483] Algo bellman_ford step 3827 current loss 0.701310, current_train_items 122496.
I0302 19:00:20.788848 22626471084160 run.py:483] Algo bellman_ford step 3828 current loss 0.804367, current_train_items 122528.
I0302 19:00:20.821020 22626471084160 run.py:483] Algo bellman_ford step 3829 current loss 0.888725, current_train_items 122560.
I0302 19:00:20.840587 22626471084160 run.py:483] Algo bellman_ford step 3830 current loss 0.294658, current_train_items 122592.
I0302 19:00:20.856892 22626471084160 run.py:483] Algo bellman_ford step 3831 current loss 0.580283, current_train_items 122624.
I0302 19:00:20.879377 22626471084160 run.py:483] Algo bellman_ford step 3832 current loss 0.590760, current_train_items 122656.
I0302 19:00:20.911353 22626471084160 run.py:483] Algo bellman_ford step 3833 current loss 0.848778, current_train_items 122688.
I0302 19:00:20.944456 22626471084160 run.py:483] Algo bellman_ford step 3834 current loss 0.726383, current_train_items 122720.
I0302 19:00:20.964387 22626471084160 run.py:483] Algo bellman_ford step 3835 current loss 0.377773, current_train_items 122752.
I0302 19:00:20.980038 22626471084160 run.py:483] Algo bellman_ford step 3836 current loss 0.398291, current_train_items 122784.
I0302 19:00:21.004672 22626471084160 run.py:483] Algo bellman_ford step 3837 current loss 0.787191, current_train_items 122816.
I0302 19:00:21.036237 22626471084160 run.py:483] Algo bellman_ford step 3838 current loss 0.753115, current_train_items 122848.
I0302 19:00:21.069649 22626471084160 run.py:483] Algo bellman_ford step 3839 current loss 0.744212, current_train_items 122880.
I0302 19:00:21.089510 22626471084160 run.py:483] Algo bellman_ford step 3840 current loss 0.308086, current_train_items 122912.
I0302 19:00:21.105434 22626471084160 run.py:483] Algo bellman_ford step 3841 current loss 0.587968, current_train_items 122944.
I0302 19:00:21.128077 22626471084160 run.py:483] Algo bellman_ford step 3842 current loss 0.657300, current_train_items 122976.
I0302 19:00:21.159339 22626471084160 run.py:483] Algo bellman_ford step 3843 current loss 0.780893, current_train_items 123008.
I0302 19:00:21.194189 22626471084160 run.py:483] Algo bellman_ford step 3844 current loss 1.012824, current_train_items 123040.
I0302 19:00:21.213883 22626471084160 run.py:483] Algo bellman_ford step 3845 current loss 0.369556, current_train_items 123072.
I0302 19:00:21.230021 22626471084160 run.py:483] Algo bellman_ford step 3846 current loss 0.459230, current_train_items 123104.
I0302 19:00:21.255084 22626471084160 run.py:483] Algo bellman_ford step 3847 current loss 0.725680, current_train_items 123136.
I0302 19:00:21.285668 22626471084160 run.py:483] Algo bellman_ford step 3848 current loss 0.647893, current_train_items 123168.
I0302 19:00:21.319038 22626471084160 run.py:483] Algo bellman_ford step 3849 current loss 0.809823, current_train_items 123200.
I0302 19:00:21.338693 22626471084160 run.py:483] Algo bellman_ford step 3850 current loss 0.362647, current_train_items 123232.
I0302 19:00:21.346647 22626471084160 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0302 19:00:21.346754 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:00:21.363126 22626471084160 run.py:483] Algo bellman_ford step 3851 current loss 0.445795, current_train_items 123264.
I0302 19:00:21.386096 22626471084160 run.py:483] Algo bellman_ford step 3852 current loss 0.644366, current_train_items 123296.
I0302 19:00:21.418182 22626471084160 run.py:483] Algo bellman_ford step 3853 current loss 0.865055, current_train_items 123328.
I0302 19:00:21.453742 22626471084160 run.py:483] Algo bellman_ford step 3854 current loss 0.811501, current_train_items 123360.
I0302 19:00:21.473716 22626471084160 run.py:483] Algo bellman_ford step 3855 current loss 0.306856, current_train_items 123392.
I0302 19:00:21.489434 22626471084160 run.py:483] Algo bellman_ford step 3856 current loss 0.511256, current_train_items 123424.
I0302 19:00:21.512056 22626471084160 run.py:483] Algo bellman_ford step 3857 current loss 0.689420, current_train_items 123456.
I0302 19:00:21.543376 22626471084160 run.py:483] Algo bellman_ford step 3858 current loss 0.718767, current_train_items 123488.
I0302 19:00:21.579166 22626471084160 run.py:483] Algo bellman_ford step 3859 current loss 1.079877, current_train_items 123520.
I0302 19:00:21.598963 22626471084160 run.py:483] Algo bellman_ford step 3860 current loss 0.347527, current_train_items 123552.
I0302 19:00:21.615518 22626471084160 run.py:483] Algo bellman_ford step 3861 current loss 0.505382, current_train_items 123584.
I0302 19:00:21.638552 22626471084160 run.py:483] Algo bellman_ford step 3862 current loss 0.695416, current_train_items 123616.
I0302 19:00:21.669585 22626471084160 run.py:483] Algo bellman_ford step 3863 current loss 0.892742, current_train_items 123648.
I0302 19:00:21.705458 22626471084160 run.py:483] Algo bellman_ford step 3864 current loss 0.888259, current_train_items 123680.
I0302 19:00:21.724816 22626471084160 run.py:483] Algo bellman_ford step 3865 current loss 0.408260, current_train_items 123712.
I0302 19:00:21.740741 22626471084160 run.py:483] Algo bellman_ford step 3866 current loss 0.569745, current_train_items 123744.
I0302 19:00:21.764182 22626471084160 run.py:483] Algo bellman_ford step 3867 current loss 0.634555, current_train_items 123776.
I0302 19:00:21.794730 22626471084160 run.py:483] Algo bellman_ford step 3868 current loss 0.746218, current_train_items 123808.
I0302 19:00:21.827925 22626471084160 run.py:483] Algo bellman_ford step 3869 current loss 0.811768, current_train_items 123840.
I0302 19:00:21.847906 22626471084160 run.py:483] Algo bellman_ford step 3870 current loss 0.368183, current_train_items 123872.
I0302 19:00:21.863674 22626471084160 run.py:483] Algo bellman_ford step 3871 current loss 0.562340, current_train_items 123904.
I0302 19:00:21.887365 22626471084160 run.py:483] Algo bellman_ford step 3872 current loss 0.660945, current_train_items 123936.
I0302 19:00:21.916897 22626471084160 run.py:483] Algo bellman_ford step 3873 current loss 0.619206, current_train_items 123968.
I0302 19:00:21.949317 22626471084160 run.py:483] Algo bellman_ford step 3874 current loss 0.783745, current_train_items 124000.
I0302 19:00:21.969466 22626471084160 run.py:483] Algo bellman_ford step 3875 current loss 0.412771, current_train_items 124032.
I0302 19:00:21.985919 22626471084160 run.py:483] Algo bellman_ford step 3876 current loss 0.553206, current_train_items 124064.
I0302 19:00:22.009842 22626471084160 run.py:483] Algo bellman_ford step 3877 current loss 0.699142, current_train_items 124096.
I0302 19:00:22.041126 22626471084160 run.py:483] Algo bellman_ford step 3878 current loss 0.705929, current_train_items 124128.
I0302 19:00:22.074572 22626471084160 run.py:483] Algo bellman_ford step 3879 current loss 0.977157, current_train_items 124160.
I0302 19:00:22.094576 22626471084160 run.py:483] Algo bellman_ford step 3880 current loss 0.385753, current_train_items 124192.
I0302 19:00:22.110814 22626471084160 run.py:483] Algo bellman_ford step 3881 current loss 0.531780, current_train_items 124224.
I0302 19:00:22.134585 22626471084160 run.py:483] Algo bellman_ford step 3882 current loss 0.886976, current_train_items 124256.
I0302 19:00:22.165314 22626471084160 run.py:483] Algo bellman_ford step 3883 current loss 0.953252, current_train_items 124288.
I0302 19:00:22.197679 22626471084160 run.py:483] Algo bellman_ford step 3884 current loss 0.935796, current_train_items 124320.
I0302 19:00:22.217674 22626471084160 run.py:483] Algo bellman_ford step 3885 current loss 0.288275, current_train_items 124352.
I0302 19:00:22.234250 22626471084160 run.py:483] Algo bellman_ford step 3886 current loss 0.509742, current_train_items 124384.
I0302 19:00:22.257681 22626471084160 run.py:483] Algo bellman_ford step 3887 current loss 0.747699, current_train_items 124416.
I0302 19:00:22.288289 22626471084160 run.py:483] Algo bellman_ford step 3888 current loss 0.810371, current_train_items 124448.
I0302 19:00:22.323026 22626471084160 run.py:483] Algo bellman_ford step 3889 current loss 0.961512, current_train_items 124480.
I0302 19:00:22.343120 22626471084160 run.py:483] Algo bellman_ford step 3890 current loss 0.270503, current_train_items 124512.
I0302 19:00:22.359457 22626471084160 run.py:483] Algo bellman_ford step 3891 current loss 0.690549, current_train_items 124544.
I0302 19:00:22.383512 22626471084160 run.py:483] Algo bellman_ford step 3892 current loss 0.609220, current_train_items 124576.
I0302 19:00:22.412587 22626471084160 run.py:483] Algo bellman_ford step 3893 current loss 0.627771, current_train_items 124608.
I0302 19:00:22.446373 22626471084160 run.py:483] Algo bellman_ford step 3894 current loss 0.810817, current_train_items 124640.
I0302 19:00:22.466058 22626471084160 run.py:483] Algo bellman_ford step 3895 current loss 0.263998, current_train_items 124672.
I0302 19:00:22.482598 22626471084160 run.py:483] Algo bellman_ford step 3896 current loss 0.519688, current_train_items 124704.
I0302 19:00:22.507129 22626471084160 run.py:483] Algo bellman_ford step 3897 current loss 0.729560, current_train_items 124736.
I0302 19:00:22.538344 22626471084160 run.py:483] Algo bellman_ford step 3898 current loss 0.769003, current_train_items 124768.
I0302 19:00:22.571397 22626471084160 run.py:483] Algo bellman_ford step 3899 current loss 1.007488, current_train_items 124800.
I0302 19:00:22.591347 22626471084160 run.py:483] Algo bellman_ford step 3900 current loss 0.317327, current_train_items 124832.
I0302 19:00:22.599195 22626471084160 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0302 19:00:22.599300 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:00:22.616253 22626471084160 run.py:483] Algo bellman_ford step 3901 current loss 0.582905, current_train_items 124864.
I0302 19:00:22.639912 22626471084160 run.py:483] Algo bellman_ford step 3902 current loss 0.630084, current_train_items 124896.
I0302 19:00:22.671114 22626471084160 run.py:483] Algo bellman_ford step 3903 current loss 0.764641, current_train_items 124928.
I0302 19:00:22.704962 22626471084160 run.py:483] Algo bellman_ford step 3904 current loss 0.786134, current_train_items 124960.
I0302 19:00:22.725081 22626471084160 run.py:483] Algo bellman_ford step 3905 current loss 0.272642, current_train_items 124992.
I0302 19:00:22.740981 22626471084160 run.py:483] Algo bellman_ford step 3906 current loss 0.484434, current_train_items 125024.
I0302 19:00:22.763310 22626471084160 run.py:483] Algo bellman_ford step 3907 current loss 0.699828, current_train_items 125056.
I0302 19:00:22.794142 22626471084160 run.py:483] Algo bellman_ford step 3908 current loss 0.850323, current_train_items 125088.
I0302 19:00:22.828119 22626471084160 run.py:483] Algo bellman_ford step 3909 current loss 0.910537, current_train_items 125120.
I0302 19:00:22.847835 22626471084160 run.py:483] Algo bellman_ford step 3910 current loss 0.270702, current_train_items 125152.
I0302 19:00:22.863762 22626471084160 run.py:483] Algo bellman_ford step 3911 current loss 0.542471, current_train_items 125184.
I0302 19:00:22.887152 22626471084160 run.py:483] Algo bellman_ford step 3912 current loss 0.702899, current_train_items 125216.
I0302 19:00:22.917330 22626471084160 run.py:483] Algo bellman_ford step 3913 current loss 0.738124, current_train_items 125248.
I0302 19:00:22.947812 22626471084160 run.py:483] Algo bellman_ford step 3914 current loss 0.865411, current_train_items 125280.
I0302 19:00:22.967169 22626471084160 run.py:483] Algo bellman_ford step 3915 current loss 0.406990, current_train_items 125312.
I0302 19:00:22.983126 22626471084160 run.py:483] Algo bellman_ford step 3916 current loss 0.510412, current_train_items 125344.
I0302 19:00:23.007987 22626471084160 run.py:483] Algo bellman_ford step 3917 current loss 0.814028, current_train_items 125376.
I0302 19:00:23.037679 22626471084160 run.py:483] Algo bellman_ford step 3918 current loss 0.838696, current_train_items 125408.
I0302 19:00:23.070750 22626471084160 run.py:483] Algo bellman_ford step 3919 current loss 0.784577, current_train_items 125440.
I0302 19:00:23.090166 22626471084160 run.py:483] Algo bellman_ford step 3920 current loss 0.310515, current_train_items 125472.
I0302 19:00:23.106482 22626471084160 run.py:483] Algo bellman_ford step 3921 current loss 0.563391, current_train_items 125504.
I0302 19:00:23.130880 22626471084160 run.py:483] Algo bellman_ford step 3922 current loss 0.748245, current_train_items 125536.
I0302 19:00:23.162397 22626471084160 run.py:483] Algo bellman_ford step 3923 current loss 0.962215, current_train_items 125568.
I0302 19:00:23.198291 22626471084160 run.py:483] Algo bellman_ford step 3924 current loss 0.955616, current_train_items 125600.
I0302 19:00:23.217813 22626471084160 run.py:483] Algo bellman_ford step 3925 current loss 0.331126, current_train_items 125632.
I0302 19:00:23.234149 22626471084160 run.py:483] Algo bellman_ford step 3926 current loss 0.467492, current_train_items 125664.
I0302 19:00:23.257726 22626471084160 run.py:483] Algo bellman_ford step 3927 current loss 0.642249, current_train_items 125696.
I0302 19:00:23.288203 22626471084160 run.py:483] Algo bellman_ford step 3928 current loss 0.879601, current_train_items 125728.
I0302 19:00:23.321295 22626471084160 run.py:483] Algo bellman_ford step 3929 current loss 1.026352, current_train_items 125760.
I0302 19:00:23.340740 22626471084160 run.py:483] Algo bellman_ford step 3930 current loss 0.309116, current_train_items 125792.
I0302 19:00:23.356800 22626471084160 run.py:483] Algo bellman_ford step 3931 current loss 0.636730, current_train_items 125824.
I0302 19:00:23.380111 22626471084160 run.py:483] Algo bellman_ford step 3932 current loss 0.695966, current_train_items 125856.
I0302 19:00:23.412402 22626471084160 run.py:483] Algo bellman_ford step 3933 current loss 0.822073, current_train_items 125888.
I0302 19:00:23.445913 22626471084160 run.py:483] Algo bellman_ford step 3934 current loss 1.005907, current_train_items 125920.
I0302 19:00:23.465421 22626471084160 run.py:483] Algo bellman_ford step 3935 current loss 0.360150, current_train_items 125952.
I0302 19:00:23.481822 22626471084160 run.py:483] Algo bellman_ford step 3936 current loss 0.583126, current_train_items 125984.
I0302 19:00:23.504775 22626471084160 run.py:483] Algo bellman_ford step 3937 current loss 0.739990, current_train_items 126016.
I0302 19:00:23.535983 22626471084160 run.py:483] Algo bellman_ford step 3938 current loss 0.946850, current_train_items 126048.
I0302 19:00:23.570486 22626471084160 run.py:483] Algo bellman_ford step 3939 current loss 0.843521, current_train_items 126080.
I0302 19:00:23.590245 22626471084160 run.py:483] Algo bellman_ford step 3940 current loss 0.355601, current_train_items 126112.
I0302 19:00:23.606320 22626471084160 run.py:483] Algo bellman_ford step 3941 current loss 0.473690, current_train_items 126144.
I0302 19:00:23.629726 22626471084160 run.py:483] Algo bellman_ford step 3942 current loss 0.633910, current_train_items 126176.
I0302 19:00:23.659735 22626471084160 run.py:483] Algo bellman_ford step 3943 current loss 0.636286, current_train_items 126208.
I0302 19:00:23.694361 22626471084160 run.py:483] Algo bellman_ford step 3944 current loss 0.884689, current_train_items 126240.
I0302 19:00:23.713581 22626471084160 run.py:483] Algo bellman_ford step 3945 current loss 0.340405, current_train_items 126272.
I0302 19:00:23.729997 22626471084160 run.py:483] Algo bellman_ford step 3946 current loss 0.476720, current_train_items 126304.
I0302 19:00:23.753192 22626471084160 run.py:483] Algo bellman_ford step 3947 current loss 0.625680, current_train_items 126336.
I0302 19:00:23.782378 22626471084160 run.py:483] Algo bellman_ford step 3948 current loss 0.843947, current_train_items 126368.
I0302 19:00:23.816224 22626471084160 run.py:483] Algo bellman_ford step 3949 current loss 0.732408, current_train_items 126400.
I0302 19:00:23.835743 22626471084160 run.py:483] Algo bellman_ford step 3950 current loss 0.253080, current_train_items 126432.
I0302 19:00:23.843866 22626471084160 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0302 19:00:23.843972 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.938, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:00:23.873551 22626471084160 run.py:483] Algo bellman_ford step 3951 current loss 0.472144, current_train_items 126464.
I0302 19:00:23.897727 22626471084160 run.py:483] Algo bellman_ford step 3952 current loss 0.669194, current_train_items 126496.
I0302 19:00:23.928723 22626471084160 run.py:483] Algo bellman_ford step 3953 current loss 0.716056, current_train_items 126528.
I0302 19:00:23.960862 22626471084160 run.py:483] Algo bellman_ford step 3954 current loss 0.719601, current_train_items 126560.
I0302 19:00:23.981284 22626471084160 run.py:483] Algo bellman_ford step 3955 current loss 0.320914, current_train_items 126592.
I0302 19:00:23.997194 22626471084160 run.py:483] Algo bellman_ford step 3956 current loss 0.470931, current_train_items 126624.
I0302 19:00:24.021342 22626471084160 run.py:483] Algo bellman_ford step 3957 current loss 0.706012, current_train_items 126656.
I0302 19:00:24.053561 22626471084160 run.py:483] Algo bellman_ford step 3958 current loss 0.803479, current_train_items 126688.
I0302 19:00:24.089530 22626471084160 run.py:483] Algo bellman_ford step 3959 current loss 1.017760, current_train_items 126720.
I0302 19:00:24.109697 22626471084160 run.py:483] Algo bellman_ford step 3960 current loss 0.320056, current_train_items 126752.
I0302 19:00:24.125796 22626471084160 run.py:483] Algo bellman_ford step 3961 current loss 0.509142, current_train_items 126784.
I0302 19:00:24.148026 22626471084160 run.py:483] Algo bellman_ford step 3962 current loss 0.546996, current_train_items 126816.
I0302 19:00:24.179369 22626471084160 run.py:483] Algo bellman_ford step 3963 current loss 0.751240, current_train_items 126848.
I0302 19:00:24.213296 22626471084160 run.py:483] Algo bellman_ford step 3964 current loss 1.078354, current_train_items 126880.
I0302 19:00:24.232804 22626471084160 run.py:483] Algo bellman_ford step 3965 current loss 0.376983, current_train_items 126912.
I0302 19:00:24.248985 22626471084160 run.py:483] Algo bellman_ford step 3966 current loss 0.499638, current_train_items 126944.
I0302 19:00:24.273964 22626471084160 run.py:483] Algo bellman_ford step 3967 current loss 0.783264, current_train_items 126976.
I0302 19:00:24.305118 22626471084160 run.py:483] Algo bellman_ford step 3968 current loss 0.762008, current_train_items 127008.
I0302 19:00:24.338196 22626471084160 run.py:483] Algo bellman_ford step 3969 current loss 0.779896, current_train_items 127040.
I0302 19:00:24.358313 22626471084160 run.py:483] Algo bellman_ford step 3970 current loss 0.329285, current_train_items 127072.
I0302 19:00:24.374435 22626471084160 run.py:483] Algo bellman_ford step 3971 current loss 0.506576, current_train_items 127104.
I0302 19:00:24.398363 22626471084160 run.py:483] Algo bellman_ford step 3972 current loss 0.663181, current_train_items 127136.
I0302 19:00:24.430107 22626471084160 run.py:483] Algo bellman_ford step 3973 current loss 0.889950, current_train_items 127168.
I0302 19:00:24.462557 22626471084160 run.py:483] Algo bellman_ford step 3974 current loss 0.802249, current_train_items 127200.
I0302 19:00:24.482563 22626471084160 run.py:483] Algo bellman_ford step 3975 current loss 0.302025, current_train_items 127232.
I0302 19:00:24.499089 22626471084160 run.py:483] Algo bellman_ford step 3976 current loss 0.514279, current_train_items 127264.
I0302 19:00:24.521979 22626471084160 run.py:483] Algo bellman_ford step 3977 current loss 0.598707, current_train_items 127296.
I0302 19:00:24.553576 22626471084160 run.py:483] Algo bellman_ford step 3978 current loss 0.772504, current_train_items 127328.
I0302 19:00:24.586687 22626471084160 run.py:483] Algo bellman_ford step 3979 current loss 0.851124, current_train_items 127360.
I0302 19:00:24.605968 22626471084160 run.py:483] Algo bellman_ford step 3980 current loss 0.311445, current_train_items 127392.
I0302 19:00:24.622030 22626471084160 run.py:483] Algo bellman_ford step 3981 current loss 0.495976, current_train_items 127424.
I0302 19:00:24.644949 22626471084160 run.py:483] Algo bellman_ford step 3982 current loss 0.704327, current_train_items 127456.
I0302 19:00:24.676417 22626471084160 run.py:483] Algo bellman_ford step 3983 current loss 0.830252, current_train_items 127488.
I0302 19:00:24.708596 22626471084160 run.py:483] Algo bellman_ford step 3984 current loss 0.866741, current_train_items 127520.
I0302 19:00:24.728864 22626471084160 run.py:483] Algo bellman_ford step 3985 current loss 0.267664, current_train_items 127552.
I0302 19:00:24.745294 22626471084160 run.py:483] Algo bellman_ford step 3986 current loss 0.664583, current_train_items 127584.
I0302 19:00:24.768333 22626471084160 run.py:483] Algo bellman_ford step 3987 current loss 0.715104, current_train_items 127616.
I0302 19:00:24.799669 22626471084160 run.py:483] Algo bellman_ford step 3988 current loss 0.768965, current_train_items 127648.
I0302 19:00:24.833431 22626471084160 run.py:483] Algo bellman_ford step 3989 current loss 0.814035, current_train_items 127680.
I0302 19:00:24.853486 22626471084160 run.py:483] Algo bellman_ford step 3990 current loss 0.323494, current_train_items 127712.
I0302 19:00:24.869970 22626471084160 run.py:483] Algo bellman_ford step 3991 current loss 0.532115, current_train_items 127744.
I0302 19:00:24.892741 22626471084160 run.py:483] Algo bellman_ford step 3992 current loss 0.597546, current_train_items 127776.
I0302 19:00:24.923131 22626471084160 run.py:483] Algo bellman_ford step 3993 current loss 0.716987, current_train_items 127808.
I0302 19:00:24.957074 22626471084160 run.py:483] Algo bellman_ford step 3994 current loss 0.943701, current_train_items 127840.
I0302 19:00:24.977067 22626471084160 run.py:483] Algo bellman_ford step 3995 current loss 0.341564, current_train_items 127872.
I0302 19:00:24.993476 22626471084160 run.py:483] Algo bellman_ford step 3996 current loss 0.534635, current_train_items 127904.
I0302 19:00:25.017320 22626471084160 run.py:483] Algo bellman_ford step 3997 current loss 0.623319, current_train_items 127936.
I0302 19:00:25.047937 22626471084160 run.py:483] Algo bellman_ford step 3998 current loss 0.782263, current_train_items 127968.
I0302 19:00:25.082518 22626471084160 run.py:483] Algo bellman_ford step 3999 current loss 0.819512, current_train_items 128000.
I0302 19:00:25.102789 22626471084160 run.py:483] Algo bellman_ford step 4000 current loss 0.334860, current_train_items 128032.
I0302 19:00:25.110681 22626471084160 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0302 19:00:25.110788 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:00:25.127501 22626471084160 run.py:483] Algo bellman_ford step 4001 current loss 0.449641, current_train_items 128064.
I0302 19:00:25.151400 22626471084160 run.py:483] Algo bellman_ford step 4002 current loss 0.747732, current_train_items 128096.
I0302 19:00:25.182329 22626471084160 run.py:483] Algo bellman_ford step 4003 current loss 0.910378, current_train_items 128128.
I0302 19:00:25.215925 22626471084160 run.py:483] Algo bellman_ford step 4004 current loss 1.101304, current_train_items 128160.
I0302 19:00:25.236139 22626471084160 run.py:483] Algo bellman_ford step 4005 current loss 0.320715, current_train_items 128192.
I0302 19:00:25.252204 22626471084160 run.py:483] Algo bellman_ford step 4006 current loss 0.656822, current_train_items 128224.
I0302 19:00:25.275853 22626471084160 run.py:483] Algo bellman_ford step 4007 current loss 0.912228, current_train_items 128256.
I0302 19:00:25.307769 22626471084160 run.py:483] Algo bellman_ford step 4008 current loss 0.822886, current_train_items 128288.
I0302 19:00:25.340361 22626471084160 run.py:483] Algo bellman_ford step 4009 current loss 0.892784, current_train_items 128320.
I0302 19:00:25.360247 22626471084160 run.py:483] Algo bellman_ford step 4010 current loss 0.413356, current_train_items 128352.
I0302 19:00:25.376390 22626471084160 run.py:483] Algo bellman_ford step 4011 current loss 0.472798, current_train_items 128384.
I0302 19:00:25.398897 22626471084160 run.py:483] Algo bellman_ford step 4012 current loss 0.619580, current_train_items 128416.
I0302 19:00:25.430217 22626471084160 run.py:483] Algo bellman_ford step 4013 current loss 0.783759, current_train_items 128448.
I0302 19:00:25.464146 22626471084160 run.py:483] Algo bellman_ford step 4014 current loss 0.861134, current_train_items 128480.
I0302 19:00:25.484035 22626471084160 run.py:483] Algo bellman_ford step 4015 current loss 0.272482, current_train_items 128512.
I0302 19:00:25.500324 22626471084160 run.py:483] Algo bellman_ford step 4016 current loss 0.572295, current_train_items 128544.
I0302 19:00:25.524098 22626471084160 run.py:483] Algo bellman_ford step 4017 current loss 0.679429, current_train_items 128576.
I0302 19:00:25.555332 22626471084160 run.py:483] Algo bellman_ford step 4018 current loss 0.793166, current_train_items 128608.
I0302 19:00:25.588354 22626471084160 run.py:483] Algo bellman_ford step 4019 current loss 0.858774, current_train_items 128640.
I0302 19:00:25.608098 22626471084160 run.py:483] Algo bellman_ford step 4020 current loss 0.323546, current_train_items 128672.
I0302 19:00:25.624139 22626471084160 run.py:483] Algo bellman_ford step 4021 current loss 0.482374, current_train_items 128704.
I0302 19:00:25.648863 22626471084160 run.py:483] Algo bellman_ford step 4022 current loss 0.659699, current_train_items 128736.
I0302 19:00:25.679053 22626471084160 run.py:483] Algo bellman_ford step 4023 current loss 0.673269, current_train_items 128768.
I0302 19:00:25.715361 22626471084160 run.py:483] Algo bellman_ford step 4024 current loss 1.016950, current_train_items 128800.
I0302 19:00:25.734804 22626471084160 run.py:483] Algo bellman_ford step 4025 current loss 0.371490, current_train_items 128832.
I0302 19:00:25.751405 22626471084160 run.py:483] Algo bellman_ford step 4026 current loss 0.442364, current_train_items 128864.
I0302 19:00:25.774823 22626471084160 run.py:483] Algo bellman_ford step 4027 current loss 0.565615, current_train_items 128896.
I0302 19:00:25.804974 22626471084160 run.py:483] Algo bellman_ford step 4028 current loss 0.687271, current_train_items 128928.
I0302 19:00:25.837820 22626471084160 run.py:483] Algo bellman_ford step 4029 current loss 0.748753, current_train_items 128960.
I0302 19:00:25.857064 22626471084160 run.py:483] Algo bellman_ford step 4030 current loss 0.296979, current_train_items 128992.
I0302 19:00:25.872987 22626471084160 run.py:483] Algo bellman_ford step 4031 current loss 0.490357, current_train_items 129024.
I0302 19:00:25.896807 22626471084160 run.py:483] Algo bellman_ford step 4032 current loss 0.693882, current_train_items 129056.
I0302 19:00:25.928866 22626471084160 run.py:483] Algo bellman_ford step 4033 current loss 0.792940, current_train_items 129088.
I0302 19:00:25.962118 22626471084160 run.py:483] Algo bellman_ford step 4034 current loss 0.891513, current_train_items 129120.
I0302 19:00:25.981927 22626471084160 run.py:483] Algo bellman_ford step 4035 current loss 0.320574, current_train_items 129152.
I0302 19:00:25.997958 22626471084160 run.py:483] Algo bellman_ford step 4036 current loss 0.684417, current_train_items 129184.
I0302 19:00:26.021480 22626471084160 run.py:483] Algo bellman_ford step 4037 current loss 0.614071, current_train_items 129216.
I0302 19:00:26.053030 22626471084160 run.py:483] Algo bellman_ford step 4038 current loss 0.744135, current_train_items 129248.
I0302 19:00:26.086816 22626471084160 run.py:483] Algo bellman_ford step 4039 current loss 1.018282, current_train_items 129280.
I0302 19:00:26.106403 22626471084160 run.py:483] Algo bellman_ford step 4040 current loss 0.256675, current_train_items 129312.
I0302 19:00:26.122673 22626471084160 run.py:483] Algo bellman_ford step 4041 current loss 0.480880, current_train_items 129344.
I0302 19:00:26.145776 22626471084160 run.py:483] Algo bellman_ford step 4042 current loss 0.636659, current_train_items 129376.
I0302 19:00:26.177945 22626471084160 run.py:483] Algo bellman_ford step 4043 current loss 0.875568, current_train_items 129408.
I0302 19:00:26.210497 22626471084160 run.py:483] Algo bellman_ford step 4044 current loss 0.835296, current_train_items 129440.
I0302 19:00:26.230279 22626471084160 run.py:483] Algo bellman_ford step 4045 current loss 0.353985, current_train_items 129472.
I0302 19:00:26.246795 22626471084160 run.py:483] Algo bellman_ford step 4046 current loss 0.599662, current_train_items 129504.
I0302 19:00:26.270052 22626471084160 run.py:483] Algo bellman_ford step 4047 current loss 0.646146, current_train_items 129536.
I0302 19:00:26.300004 22626471084160 run.py:483] Algo bellman_ford step 4048 current loss 0.717000, current_train_items 129568.
I0302 19:00:26.334939 22626471084160 run.py:483] Algo bellman_ford step 4049 current loss 0.924024, current_train_items 129600.
I0302 19:00:26.354580 22626471084160 run.py:483] Algo bellman_ford step 4050 current loss 0.324552, current_train_items 129632.
I0302 19:00:26.362581 22626471084160 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0302 19:00:26.362685 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:00:26.379732 22626471084160 run.py:483] Algo bellman_ford step 4051 current loss 0.576253, current_train_items 129664.
I0302 19:00:26.402895 22626471084160 run.py:483] Algo bellman_ford step 4052 current loss 0.597830, current_train_items 129696.
I0302 19:00:26.433808 22626471084160 run.py:483] Algo bellman_ford step 4053 current loss 0.860372, current_train_items 129728.
I0302 19:00:26.469464 22626471084160 run.py:483] Algo bellman_ford step 4054 current loss 1.069469, current_train_items 129760.
I0302 19:00:26.489633 22626471084160 run.py:483] Algo bellman_ford step 4055 current loss 0.448435, current_train_items 129792.
I0302 19:00:26.505429 22626471084160 run.py:483] Algo bellman_ford step 4056 current loss 0.455278, current_train_items 129824.
I0302 19:00:26.528737 22626471084160 run.py:483] Algo bellman_ford step 4057 current loss 0.588969, current_train_items 129856.
I0302 19:00:26.558748 22626471084160 run.py:483] Algo bellman_ford step 4058 current loss 0.711702, current_train_items 129888.
I0302 19:00:26.590824 22626471084160 run.py:483] Algo bellman_ford step 4059 current loss 0.843408, current_train_items 129920.
I0302 19:00:26.610696 22626471084160 run.py:483] Algo bellman_ford step 4060 current loss 0.358912, current_train_items 129952.
I0302 19:00:26.627382 22626471084160 run.py:483] Algo bellman_ford step 4061 current loss 0.449470, current_train_items 129984.
I0302 19:00:26.650222 22626471084160 run.py:483] Algo bellman_ford step 4062 current loss 0.669959, current_train_items 130016.
I0302 19:00:26.680109 22626471084160 run.py:483] Algo bellman_ford step 4063 current loss 0.733204, current_train_items 130048.
I0302 19:00:26.715095 22626471084160 run.py:483] Algo bellman_ford step 4064 current loss 1.003126, current_train_items 130080.
I0302 19:00:26.734588 22626471084160 run.py:483] Algo bellman_ford step 4065 current loss 0.342673, current_train_items 130112.
I0302 19:00:26.750456 22626471084160 run.py:483] Algo bellman_ford step 4066 current loss 0.552623, current_train_items 130144.
I0302 19:00:26.775414 22626471084160 run.py:483] Algo bellman_ford step 4067 current loss 0.717864, current_train_items 130176.
I0302 19:00:26.805909 22626471084160 run.py:483] Algo bellman_ford step 4068 current loss 0.664623, current_train_items 130208.
I0302 19:00:26.840793 22626471084160 run.py:483] Algo bellman_ford step 4069 current loss 0.853580, current_train_items 130240.
I0302 19:00:26.860871 22626471084160 run.py:483] Algo bellman_ford step 4070 current loss 0.346267, current_train_items 130272.
I0302 19:00:26.877278 22626471084160 run.py:483] Algo bellman_ford step 4071 current loss 0.483620, current_train_items 130304.
I0302 19:00:26.900422 22626471084160 run.py:483] Algo bellman_ford step 4072 current loss 0.686597, current_train_items 130336.
I0302 19:00:26.929726 22626471084160 run.py:483] Algo bellman_ford step 4073 current loss 0.780183, current_train_items 130368.
I0302 19:00:26.962314 22626471084160 run.py:483] Algo bellman_ford step 4074 current loss 0.858686, current_train_items 130400.
I0302 19:00:26.982338 22626471084160 run.py:483] Algo bellman_ford step 4075 current loss 0.297507, current_train_items 130432.
I0302 19:00:26.999048 22626471084160 run.py:483] Algo bellman_ford step 4076 current loss 0.525400, current_train_items 130464.
I0302 19:00:27.023374 22626471084160 run.py:483] Algo bellman_ford step 4077 current loss 0.738985, current_train_items 130496.
I0302 19:00:27.053739 22626471084160 run.py:483] Algo bellman_ford step 4078 current loss 0.726795, current_train_items 130528.
I0302 19:00:27.086478 22626471084160 run.py:483] Algo bellman_ford step 4079 current loss 0.783233, current_train_items 130560.
I0302 19:00:27.106108 22626471084160 run.py:483] Algo bellman_ford step 4080 current loss 0.326290, current_train_items 130592.
I0302 19:00:27.122619 22626471084160 run.py:483] Algo bellman_ford step 4081 current loss 0.426713, current_train_items 130624.
I0302 19:00:27.146830 22626471084160 run.py:483] Algo bellman_ford step 4082 current loss 0.745750, current_train_items 130656.
I0302 19:00:27.179120 22626471084160 run.py:483] Algo bellman_ford step 4083 current loss 0.761194, current_train_items 130688.
I0302 19:00:27.210169 22626471084160 run.py:483] Algo bellman_ford step 4084 current loss 0.772212, current_train_items 130720.
I0302 19:00:27.230193 22626471084160 run.py:483] Algo bellman_ford step 4085 current loss 0.304345, current_train_items 130752.
I0302 19:00:27.246526 22626471084160 run.py:483] Algo bellman_ford step 4086 current loss 0.500029, current_train_items 130784.
I0302 19:00:27.269551 22626471084160 run.py:483] Algo bellman_ford step 4087 current loss 0.699313, current_train_items 130816.
I0302 19:00:27.300040 22626471084160 run.py:483] Algo bellman_ford step 4088 current loss 0.710181, current_train_items 130848.
I0302 19:00:27.331854 22626471084160 run.py:483] Algo bellman_ford step 4089 current loss 0.765312, current_train_items 130880.
I0302 19:00:27.351758 22626471084160 run.py:483] Algo bellman_ford step 4090 current loss 0.282038, current_train_items 130912.
I0302 19:00:27.367816 22626471084160 run.py:483] Algo bellman_ford step 4091 current loss 0.440300, current_train_items 130944.
I0302 19:00:27.391773 22626471084160 run.py:483] Algo bellman_ford step 4092 current loss 0.720792, current_train_items 130976.
I0302 19:00:27.423381 22626471084160 run.py:483] Algo bellman_ford step 4093 current loss 0.790603, current_train_items 131008.
I0302 19:00:27.457274 22626471084160 run.py:483] Algo bellman_ford step 4094 current loss 1.028190, current_train_items 131040.
I0302 19:00:27.476832 22626471084160 run.py:483] Algo bellman_ford step 4095 current loss 0.376411, current_train_items 131072.
I0302 19:00:27.493332 22626471084160 run.py:483] Algo bellman_ford step 4096 current loss 0.468068, current_train_items 131104.
I0302 19:00:27.517343 22626471084160 run.py:483] Algo bellman_ford step 4097 current loss 0.651247, current_train_items 131136.
I0302 19:00:27.548494 22626471084160 run.py:483] Algo bellman_ford step 4098 current loss 0.776765, current_train_items 131168.
I0302 19:00:27.582919 22626471084160 run.py:483] Algo bellman_ford step 4099 current loss 0.860268, current_train_items 131200.
I0302 19:00:27.602715 22626471084160 run.py:483] Algo bellman_ford step 4100 current loss 0.329489, current_train_items 131232.
I0302 19:00:27.610489 22626471084160 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0302 19:00:27.610594 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 19:00:27.626858 22626471084160 run.py:483] Algo bellman_ford step 4101 current loss 0.433445, current_train_items 131264.
I0302 19:00:27.648991 22626471084160 run.py:483] Algo bellman_ford step 4102 current loss 0.673611, current_train_items 131296.
I0302 19:00:27.681278 22626471084160 run.py:483] Algo bellman_ford step 4103 current loss 0.846810, current_train_items 131328.
I0302 19:00:27.716739 22626471084160 run.py:483] Algo bellman_ford step 4104 current loss 0.770512, current_train_items 131360.
I0302 19:00:27.736353 22626471084160 run.py:483] Algo bellman_ford step 4105 current loss 0.287348, current_train_items 131392.
I0302 19:00:27.752556 22626471084160 run.py:483] Algo bellman_ford step 4106 current loss 0.492304, current_train_items 131424.
I0302 19:00:27.776576 22626471084160 run.py:483] Algo bellman_ford step 4107 current loss 0.717546, current_train_items 131456.
I0302 19:00:27.807702 22626471084160 run.py:483] Algo bellman_ford step 4108 current loss 0.786343, current_train_items 131488.
I0302 19:00:27.843656 22626471084160 run.py:483] Algo bellman_ford step 4109 current loss 0.910545, current_train_items 131520.
I0302 19:00:27.863508 22626471084160 run.py:483] Algo bellman_ford step 4110 current loss 0.408909, current_train_items 131552.
I0302 19:00:27.880230 22626471084160 run.py:483] Algo bellman_ford step 4111 current loss 0.518202, current_train_items 131584.
I0302 19:00:27.902865 22626471084160 run.py:483] Algo bellman_ford step 4112 current loss 0.666828, current_train_items 131616.
I0302 19:00:27.934858 22626471084160 run.py:483] Algo bellman_ford step 4113 current loss 0.691558, current_train_items 131648.
I0302 19:00:27.969468 22626471084160 run.py:483] Algo bellman_ford step 4114 current loss 0.877646, current_train_items 131680.
I0302 19:00:27.988819 22626471084160 run.py:483] Algo bellman_ford step 4115 current loss 0.246110, current_train_items 131712.
I0302 19:00:28.005323 22626471084160 run.py:483] Algo bellman_ford step 4116 current loss 0.564695, current_train_items 131744.
I0302 19:00:28.030006 22626471084160 run.py:483] Algo bellman_ford step 4117 current loss 0.753124, current_train_items 131776.
I0302 19:00:28.060227 22626471084160 run.py:483] Algo bellman_ford step 4118 current loss 0.652391, current_train_items 131808.
I0302 19:00:28.094142 22626471084160 run.py:483] Algo bellman_ford step 4119 current loss 0.973026, current_train_items 131840.
I0302 19:00:28.113817 22626471084160 run.py:483] Algo bellman_ford step 4120 current loss 0.261213, current_train_items 131872.
I0302 19:00:28.130125 22626471084160 run.py:483] Algo bellman_ford step 4121 current loss 0.486648, current_train_items 131904.
I0302 19:00:28.154702 22626471084160 run.py:483] Algo bellman_ford step 4122 current loss 0.690084, current_train_items 131936.
I0302 19:00:28.186438 22626471084160 run.py:483] Algo bellman_ford step 4123 current loss 0.886372, current_train_items 131968.
I0302 19:00:28.220193 22626471084160 run.py:483] Algo bellman_ford step 4124 current loss 0.911882, current_train_items 132000.
I0302 19:00:28.239862 22626471084160 run.py:483] Algo bellman_ford step 4125 current loss 0.224329, current_train_items 132032.
I0302 19:00:28.256141 22626471084160 run.py:483] Algo bellman_ford step 4126 current loss 0.528541, current_train_items 132064.
I0302 19:00:28.280905 22626471084160 run.py:483] Algo bellman_ford step 4127 current loss 0.884511, current_train_items 132096.
I0302 19:00:28.311194 22626471084160 run.py:483] Algo bellman_ford step 4128 current loss 0.770706, current_train_items 132128.
I0302 19:00:28.343572 22626471084160 run.py:483] Algo bellman_ford step 4129 current loss 0.788284, current_train_items 132160.
I0302 19:00:28.363420 22626471084160 run.py:483] Algo bellman_ford step 4130 current loss 0.362927, current_train_items 132192.
I0302 19:00:28.379812 22626471084160 run.py:483] Algo bellman_ford step 4131 current loss 0.484879, current_train_items 132224.
I0302 19:00:28.403388 22626471084160 run.py:483] Algo bellman_ford step 4132 current loss 0.646973, current_train_items 132256.
I0302 19:00:28.434709 22626471084160 run.py:483] Algo bellman_ford step 4133 current loss 0.722490, current_train_items 132288.
I0302 19:00:28.469970 22626471084160 run.py:483] Algo bellman_ford step 4134 current loss 0.780325, current_train_items 132320.
I0302 19:00:28.489809 22626471084160 run.py:483] Algo bellman_ford step 4135 current loss 0.295420, current_train_items 132352.
I0302 19:00:28.505856 22626471084160 run.py:483] Algo bellman_ford step 4136 current loss 0.499750, current_train_items 132384.
I0302 19:00:28.529172 22626471084160 run.py:483] Algo bellman_ford step 4137 current loss 0.679885, current_train_items 132416.
I0302 19:00:28.559754 22626471084160 run.py:483] Algo bellman_ford step 4138 current loss 0.685451, current_train_items 132448.
I0302 19:00:28.594182 22626471084160 run.py:483] Algo bellman_ford step 4139 current loss 0.829891, current_train_items 132480.
I0302 19:00:28.614012 22626471084160 run.py:483] Algo bellman_ford step 4140 current loss 0.328388, current_train_items 132512.
I0302 19:00:28.629842 22626471084160 run.py:483] Algo bellman_ford step 4141 current loss 0.479260, current_train_items 132544.
I0302 19:00:28.654928 22626471084160 run.py:483] Algo bellman_ford step 4142 current loss 0.748604, current_train_items 132576.
I0302 19:00:28.685019 22626471084160 run.py:483] Algo bellman_ford step 4143 current loss 0.674651, current_train_items 132608.
I0302 19:00:28.718105 22626471084160 run.py:483] Algo bellman_ford step 4144 current loss 0.749922, current_train_items 132640.
I0302 19:00:28.737681 22626471084160 run.py:483] Algo bellman_ford step 4145 current loss 0.287349, current_train_items 132672.
I0302 19:00:28.753597 22626471084160 run.py:483] Algo bellman_ford step 4146 current loss 0.413073, current_train_items 132704.
I0302 19:00:28.776892 22626471084160 run.py:483] Algo bellman_ford step 4147 current loss 0.660892, current_train_items 132736.
I0302 19:00:28.806323 22626471084160 run.py:483] Algo bellman_ford step 4148 current loss 0.654276, current_train_items 132768.
I0302 19:00:28.839100 22626471084160 run.py:483] Algo bellman_ford step 4149 current loss 0.907785, current_train_items 132800.
I0302 19:00:28.858692 22626471084160 run.py:483] Algo bellman_ford step 4150 current loss 0.348835, current_train_items 132832.
I0302 19:00:28.866745 22626471084160 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0302 19:00:28.866856 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:00:28.883666 22626471084160 run.py:483] Algo bellman_ford step 4151 current loss 0.557162, current_train_items 132864.
I0302 19:00:28.907925 22626471084160 run.py:483] Algo bellman_ford step 4152 current loss 0.625267, current_train_items 132896.
I0302 19:00:28.938803 22626471084160 run.py:483] Algo bellman_ford step 4153 current loss 0.695573, current_train_items 132928.
I0302 19:00:28.974377 22626471084160 run.py:483] Algo bellman_ford step 4154 current loss 0.953573, current_train_items 132960.
I0302 19:00:28.994694 22626471084160 run.py:483] Algo bellman_ford step 4155 current loss 0.344952, current_train_items 132992.
I0302 19:00:29.011140 22626471084160 run.py:483] Algo bellman_ford step 4156 current loss 0.465979, current_train_items 133024.
I0302 19:00:29.033791 22626471084160 run.py:483] Algo bellman_ford step 4157 current loss 0.534583, current_train_items 133056.
I0302 19:00:29.064312 22626471084160 run.py:483] Algo bellman_ford step 4158 current loss 0.700932, current_train_items 133088.
I0302 19:00:29.098311 22626471084160 run.py:483] Algo bellman_ford step 4159 current loss 0.921660, current_train_items 133120.
I0302 19:00:29.118476 22626471084160 run.py:483] Algo bellman_ford step 4160 current loss 0.287314, current_train_items 133152.
I0302 19:00:29.134772 22626471084160 run.py:483] Algo bellman_ford step 4161 current loss 0.500291, current_train_items 133184.
I0302 19:00:29.156801 22626471084160 run.py:483] Algo bellman_ford step 4162 current loss 0.600436, current_train_items 133216.
I0302 19:00:29.187843 22626471084160 run.py:483] Algo bellman_ford step 4163 current loss 0.716076, current_train_items 133248.
I0302 19:00:29.221746 22626471084160 run.py:483] Algo bellman_ford step 4164 current loss 0.811870, current_train_items 133280.
I0302 19:00:29.241464 22626471084160 run.py:483] Algo bellman_ford step 4165 current loss 0.376735, current_train_items 133312.
I0302 19:00:29.257622 22626471084160 run.py:483] Algo bellman_ford step 4166 current loss 0.460173, current_train_items 133344.
I0302 19:00:29.282096 22626471084160 run.py:483] Algo bellman_ford step 4167 current loss 0.646220, current_train_items 133376.
I0302 19:00:29.312667 22626471084160 run.py:483] Algo bellman_ford step 4168 current loss 0.680876, current_train_items 133408.
I0302 19:00:29.345217 22626471084160 run.py:483] Algo bellman_ford step 4169 current loss 0.783317, current_train_items 133440.
I0302 19:00:29.364979 22626471084160 run.py:483] Algo bellman_ford step 4170 current loss 0.270463, current_train_items 133472.
I0302 19:00:29.380703 22626471084160 run.py:483] Algo bellman_ford step 4171 current loss 0.435933, current_train_items 133504.
I0302 19:00:29.403879 22626471084160 run.py:483] Algo bellman_ford step 4172 current loss 0.653360, current_train_items 133536.
I0302 19:00:29.434234 22626471084160 run.py:483] Algo bellman_ford step 4173 current loss 0.646602, current_train_items 133568.
I0302 19:00:29.469662 22626471084160 run.py:483] Algo bellman_ford step 4174 current loss 0.809784, current_train_items 133600.
I0302 19:00:29.489719 22626471084160 run.py:483] Algo bellman_ford step 4175 current loss 0.264702, current_train_items 133632.
I0302 19:00:29.506591 22626471084160 run.py:483] Algo bellman_ford step 4176 current loss 0.492189, current_train_items 133664.
I0302 19:00:29.529993 22626471084160 run.py:483] Algo bellman_ford step 4177 current loss 0.631511, current_train_items 133696.
I0302 19:00:29.561218 22626471084160 run.py:483] Algo bellman_ford step 4178 current loss 0.810533, current_train_items 133728.
I0302 19:00:29.595597 22626471084160 run.py:483] Algo bellman_ford step 4179 current loss 0.902813, current_train_items 133760.
I0302 19:00:29.615252 22626471084160 run.py:483] Algo bellman_ford step 4180 current loss 0.304139, current_train_items 133792.
I0302 19:00:29.631420 22626471084160 run.py:483] Algo bellman_ford step 4181 current loss 0.468043, current_train_items 133824.
I0302 19:00:29.655736 22626471084160 run.py:483] Algo bellman_ford step 4182 current loss 0.618855, current_train_items 133856.
I0302 19:00:29.685176 22626471084160 run.py:483] Algo bellman_ford step 4183 current loss 0.682677, current_train_items 133888.
I0302 19:00:29.719601 22626471084160 run.py:483] Algo bellman_ford step 4184 current loss 0.894545, current_train_items 133920.
I0302 19:00:29.739454 22626471084160 run.py:483] Algo bellman_ford step 4185 current loss 0.268217, current_train_items 133952.
I0302 19:00:29.755473 22626471084160 run.py:483] Algo bellman_ford step 4186 current loss 0.515860, current_train_items 133984.
I0302 19:00:29.778756 22626471084160 run.py:483] Algo bellman_ford step 4187 current loss 0.734750, current_train_items 134016.
I0302 19:00:29.809262 22626471084160 run.py:483] Algo bellman_ford step 4188 current loss 0.834406, current_train_items 134048.
I0302 19:00:29.842272 22626471084160 run.py:483] Algo bellman_ford step 4189 current loss 0.811498, current_train_items 134080.
I0302 19:00:29.862465 22626471084160 run.py:483] Algo bellman_ford step 4190 current loss 0.355160, current_train_items 134112.
I0302 19:00:29.879096 22626471084160 run.py:483] Algo bellman_ford step 4191 current loss 0.462875, current_train_items 134144.
I0302 19:00:29.900104 22626471084160 run.py:483] Algo bellman_ford step 4192 current loss 0.545108, current_train_items 134176.
I0302 19:00:29.931530 22626471084160 run.py:483] Algo bellman_ford step 4193 current loss 0.669027, current_train_items 134208.
I0302 19:00:29.967619 22626471084160 run.py:483] Algo bellman_ford step 4194 current loss 0.911834, current_train_items 134240.
I0302 19:00:29.987332 22626471084160 run.py:483] Algo bellman_ford step 4195 current loss 0.322323, current_train_items 134272.
I0302 19:00:30.003384 22626471084160 run.py:483] Algo bellman_ford step 4196 current loss 0.646115, current_train_items 134304.
I0302 19:00:30.027199 22626471084160 run.py:483] Algo bellman_ford step 4197 current loss 0.686143, current_train_items 134336.
I0302 19:00:30.057563 22626471084160 run.py:483] Algo bellman_ford step 4198 current loss 0.720541, current_train_items 134368.
I0302 19:00:30.090700 22626471084160 run.py:483] Algo bellman_ford step 4199 current loss 0.855385, current_train_items 134400.
I0302 19:00:30.110121 22626471084160 run.py:483] Algo bellman_ford step 4200 current loss 0.283902, current_train_items 134432.
I0302 19:00:30.117744 22626471084160 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0302 19:00:30.117851 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 19:00:30.134751 22626471084160 run.py:483] Algo bellman_ford step 4201 current loss 0.614562, current_train_items 134464.
I0302 19:00:30.158195 22626471084160 run.py:483] Algo bellman_ford step 4202 current loss 0.710852, current_train_items 134496.
I0302 19:00:30.188921 22626471084160 run.py:483] Algo bellman_ford step 4203 current loss 0.946483, current_train_items 134528.
I0302 19:00:30.222797 22626471084160 run.py:483] Algo bellman_ford step 4204 current loss 1.024048, current_train_items 134560.
I0302 19:00:30.242802 22626471084160 run.py:483] Algo bellman_ford step 4205 current loss 0.334832, current_train_items 134592.
I0302 19:00:30.258931 22626471084160 run.py:483] Algo bellman_ford step 4206 current loss 0.601407, current_train_items 134624.
I0302 19:00:30.282944 22626471084160 run.py:483] Algo bellman_ford step 4207 current loss 0.611621, current_train_items 134656.
I0302 19:00:30.314542 22626471084160 run.py:483] Algo bellman_ford step 4208 current loss 0.766866, current_train_items 134688.
I0302 19:00:30.349091 22626471084160 run.py:483] Algo bellman_ford step 4209 current loss 0.950076, current_train_items 134720.
I0302 19:00:30.368643 22626471084160 run.py:483] Algo bellman_ford step 4210 current loss 0.245986, current_train_items 134752.
I0302 19:00:30.384907 22626471084160 run.py:483] Algo bellman_ford step 4211 current loss 0.461456, current_train_items 134784.
I0302 19:00:30.408556 22626471084160 run.py:483] Algo bellman_ford step 4212 current loss 0.630865, current_train_items 134816.
I0302 19:00:30.439523 22626471084160 run.py:483] Algo bellman_ford step 4213 current loss 0.693609, current_train_items 134848.
I0302 19:00:30.473075 22626471084160 run.py:483] Algo bellman_ford step 4214 current loss 0.826049, current_train_items 134880.
I0302 19:00:30.492576 22626471084160 run.py:483] Algo bellman_ford step 4215 current loss 0.383718, current_train_items 134912.
I0302 19:00:30.508820 22626471084160 run.py:483] Algo bellman_ford step 4216 current loss 0.539475, current_train_items 134944.
I0302 19:00:30.532011 22626471084160 run.py:483] Algo bellman_ford step 4217 current loss 0.593604, current_train_items 134976.
I0302 19:00:30.562574 22626471084160 run.py:483] Algo bellman_ford step 4218 current loss 0.713269, current_train_items 135008.
I0302 19:00:30.595756 22626471084160 run.py:483] Algo bellman_ford step 4219 current loss 0.818484, current_train_items 135040.
I0302 19:00:30.615658 22626471084160 run.py:483] Algo bellman_ford step 4220 current loss 0.299166, current_train_items 135072.
I0302 19:00:30.632009 22626471084160 run.py:483] Algo bellman_ford step 4221 current loss 0.516570, current_train_items 135104.
I0302 19:00:30.655939 22626471084160 run.py:483] Algo bellman_ford step 4222 current loss 0.564020, current_train_items 135136.
I0302 19:00:30.687790 22626471084160 run.py:483] Algo bellman_ford step 4223 current loss 0.668895, current_train_items 135168.
I0302 19:00:30.723127 22626471084160 run.py:483] Algo bellman_ford step 4224 current loss 0.781809, current_train_items 135200.
I0302 19:00:30.742841 22626471084160 run.py:483] Algo bellman_ford step 4225 current loss 0.317313, current_train_items 135232.
I0302 19:00:30.758888 22626471084160 run.py:483] Algo bellman_ford step 4226 current loss 0.455000, current_train_items 135264.
I0302 19:00:30.782390 22626471084160 run.py:483] Algo bellman_ford step 4227 current loss 0.683282, current_train_items 135296.
I0302 19:00:30.812123 22626471084160 run.py:483] Algo bellman_ford step 4228 current loss 0.808664, current_train_items 135328.
I0302 19:00:30.844115 22626471084160 run.py:483] Algo bellman_ford step 4229 current loss 0.695120, current_train_items 135360.
I0302 19:00:30.863550 22626471084160 run.py:483] Algo bellman_ford step 4230 current loss 0.326576, current_train_items 135392.
I0302 19:00:30.879540 22626471084160 run.py:483] Algo bellman_ford step 4231 current loss 0.452280, current_train_items 135424.
I0302 19:00:30.903188 22626471084160 run.py:483] Algo bellman_ford step 4232 current loss 0.750598, current_train_items 135456.
I0302 19:00:30.933861 22626471084160 run.py:483] Algo bellman_ford step 4233 current loss 0.920590, current_train_items 135488.
I0302 19:00:30.966828 22626471084160 run.py:483] Algo bellman_ford step 4234 current loss 0.830027, current_train_items 135520.
I0302 19:00:30.986635 22626471084160 run.py:483] Algo bellman_ford step 4235 current loss 0.287911, current_train_items 135552.
I0302 19:00:31.003437 22626471084160 run.py:483] Algo bellman_ford step 4236 current loss 0.529709, current_train_items 135584.
I0302 19:00:31.027350 22626471084160 run.py:483] Algo bellman_ford step 4237 current loss 0.711607, current_train_items 135616.
I0302 19:00:31.058343 22626471084160 run.py:483] Algo bellman_ford step 4238 current loss 0.689971, current_train_items 135648.
I0302 19:00:31.093702 22626471084160 run.py:483] Algo bellman_ford step 4239 current loss 0.841359, current_train_items 135680.
I0302 19:00:31.113573 22626471084160 run.py:483] Algo bellman_ford step 4240 current loss 0.249926, current_train_items 135712.
I0302 19:00:31.129565 22626471084160 run.py:483] Algo bellman_ford step 4241 current loss 0.465734, current_train_items 135744.
I0302 19:00:31.152568 22626471084160 run.py:483] Algo bellman_ford step 4242 current loss 0.610650, current_train_items 135776.
I0302 19:00:31.184134 22626471084160 run.py:483] Algo bellman_ford step 4243 current loss 0.815477, current_train_items 135808.
I0302 19:00:31.217140 22626471084160 run.py:483] Algo bellman_ford step 4244 current loss 0.829590, current_train_items 135840.
I0302 19:00:31.236851 22626471084160 run.py:483] Algo bellman_ford step 4245 current loss 0.314644, current_train_items 135872.
I0302 19:00:31.252664 22626471084160 run.py:483] Algo bellman_ford step 4246 current loss 0.453443, current_train_items 135904.
I0302 19:00:31.277436 22626471084160 run.py:483] Algo bellman_ford step 4247 current loss 0.691209, current_train_items 135936.
I0302 19:00:31.308853 22626471084160 run.py:483] Algo bellman_ford step 4248 current loss 0.742727, current_train_items 135968.
I0302 19:00:31.344018 22626471084160 run.py:483] Algo bellman_ford step 4249 current loss 0.825539, current_train_items 136000.
I0302 19:00:31.363577 22626471084160 run.py:483] Algo bellman_ford step 4250 current loss 0.386891, current_train_items 136032.
I0302 19:00:31.371611 22626471084160 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0302 19:00:31.371715 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:00:31.388242 22626471084160 run.py:483] Algo bellman_ford step 4251 current loss 0.465016, current_train_items 136064.
I0302 19:00:31.412208 22626471084160 run.py:483] Algo bellman_ford step 4252 current loss 0.816401, current_train_items 136096.
I0302 19:00:31.443644 22626471084160 run.py:483] Algo bellman_ford step 4253 current loss 0.873399, current_train_items 136128.
I0302 19:00:31.477531 22626471084160 run.py:483] Algo bellman_ford step 4254 current loss 0.884260, current_train_items 136160.
I0302 19:00:31.497436 22626471084160 run.py:483] Algo bellman_ford step 4255 current loss 0.326720, current_train_items 136192.
I0302 19:00:31.513279 22626471084160 run.py:483] Algo bellman_ford step 4256 current loss 0.493631, current_train_items 136224.
I0302 19:00:31.536255 22626471084160 run.py:483] Algo bellman_ford step 4257 current loss 0.539034, current_train_items 136256.
I0302 19:00:31.567151 22626471084160 run.py:483] Algo bellman_ford step 4258 current loss 0.779002, current_train_items 136288.
I0302 19:00:31.601477 22626471084160 run.py:483] Algo bellman_ford step 4259 current loss 0.840564, current_train_items 136320.
I0302 19:00:31.621320 22626471084160 run.py:483] Algo bellman_ford step 4260 current loss 0.306047, current_train_items 136352.
I0302 19:00:31.637949 22626471084160 run.py:483] Algo bellman_ford step 4261 current loss 0.645629, current_train_items 136384.
I0302 19:00:31.661289 22626471084160 run.py:483] Algo bellman_ford step 4262 current loss 0.576079, current_train_items 136416.
I0302 19:00:31.692398 22626471084160 run.py:483] Algo bellman_ford step 4263 current loss 0.739536, current_train_items 136448.
I0302 19:00:31.726265 22626471084160 run.py:483] Algo bellman_ford step 4264 current loss 0.718970, current_train_items 136480.
I0302 19:00:31.745991 22626471084160 run.py:483] Algo bellman_ford step 4265 current loss 0.365907, current_train_items 136512.
I0302 19:00:31.762261 22626471084160 run.py:483] Algo bellman_ford step 4266 current loss 0.580483, current_train_items 136544.
I0302 19:00:31.785609 22626471084160 run.py:483] Algo bellman_ford step 4267 current loss 0.665691, current_train_items 136576.
I0302 19:00:31.815896 22626471084160 run.py:483] Algo bellman_ford step 4268 current loss 0.719149, current_train_items 136608.
I0302 19:00:31.848649 22626471084160 run.py:483] Algo bellman_ford step 4269 current loss 0.887770, current_train_items 136640.
I0302 19:00:31.868741 22626471084160 run.py:483] Algo bellman_ford step 4270 current loss 0.353100, current_train_items 136672.
I0302 19:00:31.884776 22626471084160 run.py:483] Algo bellman_ford step 4271 current loss 0.464057, current_train_items 136704.
I0302 19:00:31.906972 22626471084160 run.py:483] Algo bellman_ford step 4272 current loss 0.628439, current_train_items 136736.
I0302 19:00:31.937275 22626471084160 run.py:483] Algo bellman_ford step 4273 current loss 0.755875, current_train_items 136768.
I0302 19:00:31.970069 22626471084160 run.py:483] Algo bellman_ford step 4274 current loss 0.913620, current_train_items 136800.
I0302 19:00:31.989895 22626471084160 run.py:483] Algo bellman_ford step 4275 current loss 0.303861, current_train_items 136832.
I0302 19:00:32.006024 22626471084160 run.py:483] Algo bellman_ford step 4276 current loss 0.460038, current_train_items 136864.
I0302 19:00:32.028846 22626471084160 run.py:483] Algo bellman_ford step 4277 current loss 0.792713, current_train_items 136896.
I0302 19:00:32.061518 22626471084160 run.py:483] Algo bellman_ford step 4278 current loss 0.974876, current_train_items 136928.
I0302 19:00:32.095924 22626471084160 run.py:483] Algo bellman_ford step 4279 current loss 0.953745, current_train_items 136960.
I0302 19:00:32.115757 22626471084160 run.py:483] Algo bellman_ford step 4280 current loss 0.280434, current_train_items 136992.
I0302 19:00:32.131737 22626471084160 run.py:483] Algo bellman_ford step 4281 current loss 0.410486, current_train_items 137024.
I0302 19:00:32.155291 22626471084160 run.py:483] Algo bellman_ford step 4282 current loss 0.611353, current_train_items 137056.
I0302 19:00:32.185147 22626471084160 run.py:483] Algo bellman_ford step 4283 current loss 0.741363, current_train_items 137088.
I0302 19:00:32.220805 22626471084160 run.py:483] Algo bellman_ford step 4284 current loss 0.956900, current_train_items 137120.
I0302 19:00:32.240944 22626471084160 run.py:483] Algo bellman_ford step 4285 current loss 0.315023, current_train_items 137152.
I0302 19:00:32.256959 22626471084160 run.py:483] Algo bellman_ford step 4286 current loss 0.486285, current_train_items 137184.
I0302 19:00:32.280037 22626471084160 run.py:483] Algo bellman_ford step 4287 current loss 0.645985, current_train_items 137216.
I0302 19:00:32.312509 22626471084160 run.py:483] Algo bellman_ford step 4288 current loss 0.762589, current_train_items 137248.
I0302 19:00:32.346625 22626471084160 run.py:483] Algo bellman_ford step 4289 current loss 1.031032, current_train_items 137280.
I0302 19:00:32.366439 22626471084160 run.py:483] Algo bellman_ford step 4290 current loss 0.305455, current_train_items 137312.
I0302 19:00:32.383067 22626471084160 run.py:483] Algo bellman_ford step 4291 current loss 0.482073, current_train_items 137344.
I0302 19:00:32.407293 22626471084160 run.py:483] Algo bellman_ford step 4292 current loss 0.641442, current_train_items 137376.
I0302 19:00:32.440034 22626471084160 run.py:483] Algo bellman_ford step 4293 current loss 0.909709, current_train_items 137408.
I0302 19:00:32.475550 22626471084160 run.py:483] Algo bellman_ford step 4294 current loss 0.966333, current_train_items 137440.
I0302 19:00:32.495413 22626471084160 run.py:483] Algo bellman_ford step 4295 current loss 0.276182, current_train_items 137472.
I0302 19:00:32.511469 22626471084160 run.py:483] Algo bellman_ford step 4296 current loss 0.505384, current_train_items 137504.
I0302 19:00:32.534549 22626471084160 run.py:483] Algo bellman_ford step 4297 current loss 0.629100, current_train_items 137536.
I0302 19:00:32.565823 22626471084160 run.py:483] Algo bellman_ford step 4298 current loss 0.710672, current_train_items 137568.
I0302 19:00:32.596067 22626471084160 run.py:483] Algo bellman_ford step 4299 current loss 0.700065, current_train_items 137600.
I0302 19:00:32.615728 22626471084160 run.py:483] Algo bellman_ford step 4300 current loss 0.356569, current_train_items 137632.
I0302 19:00:32.623250 22626471084160 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0302 19:00:32.623357 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:00:32.639977 22626471084160 run.py:483] Algo bellman_ford step 4301 current loss 0.658388, current_train_items 137664.
I0302 19:00:32.663563 22626471084160 run.py:483] Algo bellman_ford step 4302 current loss 0.702929, current_train_items 137696.
I0302 19:00:32.695988 22626471084160 run.py:483] Algo bellman_ford step 4303 current loss 1.057975, current_train_items 137728.
I0302 19:00:32.731926 22626471084160 run.py:483] Algo bellman_ford step 4304 current loss 0.871659, current_train_items 137760.
I0302 19:00:32.751974 22626471084160 run.py:483] Algo bellman_ford step 4305 current loss 0.348785, current_train_items 137792.
I0302 19:00:32.767939 22626471084160 run.py:483] Algo bellman_ford step 4306 current loss 0.464810, current_train_items 137824.
I0302 19:00:32.792820 22626471084160 run.py:483] Algo bellman_ford step 4307 current loss 0.780634, current_train_items 137856.
I0302 19:00:32.824341 22626471084160 run.py:483] Algo bellman_ford step 4308 current loss 0.925066, current_train_items 137888.
I0302 19:00:32.860382 22626471084160 run.py:483] Algo bellman_ford step 4309 current loss 0.892596, current_train_items 137920.
I0302 19:00:32.879901 22626471084160 run.py:483] Algo bellman_ford step 4310 current loss 0.345088, current_train_items 137952.
I0302 19:00:32.896147 22626471084160 run.py:483] Algo bellman_ford step 4311 current loss 0.476897, current_train_items 137984.
I0302 19:00:32.918236 22626471084160 run.py:483] Algo bellman_ford step 4312 current loss 0.640727, current_train_items 138016.
I0302 19:00:32.949383 22626471084160 run.py:483] Algo bellman_ford step 4313 current loss 0.763165, current_train_items 138048.
I0302 19:00:32.983492 22626471084160 run.py:483] Algo bellman_ford step 4314 current loss 0.879192, current_train_items 138080.
I0302 19:00:33.002821 22626471084160 run.py:483] Algo bellman_ford step 4315 current loss 0.373115, current_train_items 138112.
I0302 19:00:33.018760 22626471084160 run.py:483] Algo bellman_ford step 4316 current loss 0.464388, current_train_items 138144.
I0302 19:00:33.042489 22626471084160 run.py:483] Algo bellman_ford step 4317 current loss 0.658907, current_train_items 138176.
I0302 19:00:33.072743 22626471084160 run.py:483] Algo bellman_ford step 4318 current loss 0.655493, current_train_items 138208.
I0302 19:00:33.106080 22626471084160 run.py:483] Algo bellman_ford step 4319 current loss 0.874886, current_train_items 138240.
I0302 19:00:33.125277 22626471084160 run.py:483] Algo bellman_ford step 4320 current loss 0.357418, current_train_items 138272.
I0302 19:00:33.141406 22626471084160 run.py:483] Algo bellman_ford step 4321 current loss 0.457384, current_train_items 138304.
I0302 19:00:33.164716 22626471084160 run.py:483] Algo bellman_ford step 4322 current loss 0.577325, current_train_items 138336.
I0302 19:00:33.194590 22626471084160 run.py:483] Algo bellman_ford step 4323 current loss 0.661636, current_train_items 138368.
I0302 19:00:33.228604 22626471084160 run.py:483] Algo bellman_ford step 4324 current loss 0.905524, current_train_items 138400.
I0302 19:00:33.247819 22626471084160 run.py:483] Algo bellman_ford step 4325 current loss 0.300837, current_train_items 138432.
I0302 19:00:33.264063 22626471084160 run.py:483] Algo bellman_ford step 4326 current loss 0.463175, current_train_items 138464.
I0302 19:00:33.287512 22626471084160 run.py:483] Algo bellman_ford step 4327 current loss 0.573290, current_train_items 138496.
I0302 19:00:33.319817 22626471084160 run.py:483] Algo bellman_ford step 4328 current loss 0.805470, current_train_items 138528.
I0302 19:00:33.352366 22626471084160 run.py:483] Algo bellman_ford step 4329 current loss 0.811477, current_train_items 138560.
I0302 19:00:33.371861 22626471084160 run.py:483] Algo bellman_ford step 4330 current loss 0.364864, current_train_items 138592.
I0302 19:00:33.388561 22626471084160 run.py:483] Algo bellman_ford step 4331 current loss 0.592402, current_train_items 138624.
I0302 19:00:33.412336 22626471084160 run.py:483] Algo bellman_ford step 4332 current loss 0.730448, current_train_items 138656.
I0302 19:00:33.442839 22626471084160 run.py:483] Algo bellman_ford step 4333 current loss 0.784306, current_train_items 138688.
I0302 19:00:33.474362 22626471084160 run.py:483] Algo bellman_ford step 4334 current loss 0.792082, current_train_items 138720.
I0302 19:00:33.493752 22626471084160 run.py:483] Algo bellman_ford step 4335 current loss 0.337880, current_train_items 138752.
I0302 19:00:33.509803 22626471084160 run.py:483] Algo bellman_ford step 4336 current loss 0.541089, current_train_items 138784.
I0302 19:00:33.533551 22626471084160 run.py:483] Algo bellman_ford step 4337 current loss 0.710611, current_train_items 138816.
I0302 19:00:33.565556 22626471084160 run.py:483] Algo bellman_ford step 4338 current loss 0.880664, current_train_items 138848.
I0302 19:00:33.598680 22626471084160 run.py:483] Algo bellman_ford step 4339 current loss 0.835304, current_train_items 138880.
I0302 19:00:33.618114 22626471084160 run.py:483] Algo bellman_ford step 4340 current loss 0.350996, current_train_items 138912.
I0302 19:00:33.633968 22626471084160 run.py:483] Algo bellman_ford step 4341 current loss 0.456228, current_train_items 138944.
I0302 19:00:33.657027 22626471084160 run.py:483] Algo bellman_ford step 4342 current loss 0.580607, current_train_items 138976.
I0302 19:00:33.687828 22626471084160 run.py:483] Algo bellman_ford step 4343 current loss 0.821393, current_train_items 139008.
I0302 19:00:33.722234 22626471084160 run.py:483] Algo bellman_ford step 4344 current loss 0.831074, current_train_items 139040.
I0302 19:00:33.741727 22626471084160 run.py:483] Algo bellman_ford step 4345 current loss 0.318702, current_train_items 139072.
I0302 19:00:33.758114 22626471084160 run.py:483] Algo bellman_ford step 4346 current loss 0.534280, current_train_items 139104.
I0302 19:00:33.781846 22626471084160 run.py:483] Algo bellman_ford step 4347 current loss 0.707433, current_train_items 139136.
I0302 19:00:33.812852 22626471084160 run.py:483] Algo bellman_ford step 4348 current loss 0.630697, current_train_items 139168.
I0302 19:00:33.845576 22626471084160 run.py:483] Algo bellman_ford step 4349 current loss 0.836084, current_train_items 139200.
I0302 19:00:33.864970 22626471084160 run.py:483] Algo bellman_ford step 4350 current loss 0.319502, current_train_items 139232.
I0302 19:00:33.872868 22626471084160 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0302 19:00:33.872982 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:00:33.889968 22626471084160 run.py:483] Algo bellman_ford step 4351 current loss 0.504466, current_train_items 139264.
I0302 19:00:33.914367 22626471084160 run.py:483] Algo bellman_ford step 4352 current loss 0.798834, current_train_items 139296.
I0302 19:00:33.944998 22626471084160 run.py:483] Algo bellman_ford step 4353 current loss 0.759133, current_train_items 139328.
I0302 19:00:33.979537 22626471084160 run.py:483] Algo bellman_ford step 4354 current loss 0.840315, current_train_items 139360.
I0302 19:00:33.999287 22626471084160 run.py:483] Algo bellman_ford step 4355 current loss 0.302704, current_train_items 139392.
I0302 19:00:34.015317 22626471084160 run.py:483] Algo bellman_ford step 4356 current loss 0.491544, current_train_items 139424.
I0302 19:00:34.039062 22626471084160 run.py:483] Algo bellman_ford step 4357 current loss 0.676139, current_train_items 139456.
I0302 19:00:34.068079 22626471084160 run.py:483] Algo bellman_ford step 4358 current loss 0.757222, current_train_items 139488.
I0302 19:00:34.100131 22626471084160 run.py:483] Algo bellman_ford step 4359 current loss 0.881444, current_train_items 139520.
I0302 19:00:34.119764 22626471084160 run.py:483] Algo bellman_ford step 4360 current loss 0.302965, current_train_items 139552.
I0302 19:00:34.136487 22626471084160 run.py:483] Algo bellman_ford step 4361 current loss 0.638003, current_train_items 139584.
I0302 19:00:34.158499 22626471084160 run.py:483] Algo bellman_ford step 4362 current loss 0.712184, current_train_items 139616.
I0302 19:00:34.190300 22626471084160 run.py:483] Algo bellman_ford step 4363 current loss 0.992703, current_train_items 139648.
I0302 19:00:34.225067 22626471084160 run.py:483] Algo bellman_ford step 4364 current loss 1.118424, current_train_items 139680.
I0302 19:00:34.244471 22626471084160 run.py:483] Algo bellman_ford step 4365 current loss 0.329374, current_train_items 139712.
I0302 19:00:34.260724 22626471084160 run.py:483] Algo bellman_ford step 4366 current loss 0.683610, current_train_items 139744.
I0302 19:00:34.283981 22626471084160 run.py:483] Algo bellman_ford step 4367 current loss 0.589397, current_train_items 139776.
I0302 19:00:34.313698 22626471084160 run.py:483] Algo bellman_ford step 4368 current loss 0.740032, current_train_items 139808.
I0302 19:00:34.347675 22626471084160 run.py:483] Algo bellman_ford step 4369 current loss 0.878694, current_train_items 139840.
I0302 19:00:34.367087 22626471084160 run.py:483] Algo bellman_ford step 4370 current loss 0.334768, current_train_items 139872.
I0302 19:00:34.383215 22626471084160 run.py:483] Algo bellman_ford step 4371 current loss 0.582127, current_train_items 139904.
I0302 19:00:34.405875 22626471084160 run.py:483] Algo bellman_ford step 4372 current loss 0.602825, current_train_items 139936.
I0302 19:00:34.436208 22626471084160 run.py:483] Algo bellman_ford step 4373 current loss 0.740689, current_train_items 139968.
I0302 19:00:34.469607 22626471084160 run.py:483] Algo bellman_ford step 4374 current loss 0.818769, current_train_items 140000.
I0302 19:00:34.489470 22626471084160 run.py:483] Algo bellman_ford step 4375 current loss 0.347702, current_train_items 140032.
I0302 19:00:34.505537 22626471084160 run.py:483] Algo bellman_ford step 4376 current loss 0.447170, current_train_items 140064.
I0302 19:00:34.528310 22626471084160 run.py:483] Algo bellman_ford step 4377 current loss 0.630832, current_train_items 140096.
I0302 19:00:34.558313 22626471084160 run.py:483] Algo bellman_ford step 4378 current loss 0.658413, current_train_items 140128.
I0302 19:00:34.591947 22626471084160 run.py:483] Algo bellman_ford step 4379 current loss 0.854591, current_train_items 140160.
I0302 19:00:34.611260 22626471084160 run.py:483] Algo bellman_ford step 4380 current loss 0.320964, current_train_items 140192.
I0302 19:00:34.627327 22626471084160 run.py:483] Algo bellman_ford step 4381 current loss 0.492756, current_train_items 140224.
I0302 19:00:34.650720 22626471084160 run.py:483] Algo bellman_ford step 4382 current loss 0.702989, current_train_items 140256.
I0302 19:00:34.681741 22626471084160 run.py:483] Algo bellman_ford step 4383 current loss 0.769125, current_train_items 140288.
I0302 19:00:34.715706 22626471084160 run.py:483] Algo bellman_ford step 4384 current loss 0.830487, current_train_items 140320.
I0302 19:00:34.735518 22626471084160 run.py:483] Algo bellman_ford step 4385 current loss 0.311451, current_train_items 140352.
I0302 19:00:34.751433 22626471084160 run.py:483] Algo bellman_ford step 4386 current loss 0.460920, current_train_items 140384.
I0302 19:00:34.774573 22626471084160 run.py:483] Algo bellman_ford step 4387 current loss 0.737558, current_train_items 140416.
I0302 19:00:34.805378 22626471084160 run.py:483] Algo bellman_ford step 4388 current loss 0.705464, current_train_items 140448.
I0302 19:00:34.837234 22626471084160 run.py:483] Algo bellman_ford step 4389 current loss 0.729104, current_train_items 140480.
I0302 19:00:34.856930 22626471084160 run.py:483] Algo bellman_ford step 4390 current loss 0.227619, current_train_items 140512.
I0302 19:00:34.872950 22626471084160 run.py:483] Algo bellman_ford step 4391 current loss 0.498184, current_train_items 140544.
I0302 19:00:34.895839 22626471084160 run.py:483] Algo bellman_ford step 4392 current loss 0.812848, current_train_items 140576.
I0302 19:00:34.926603 22626471084160 run.py:483] Algo bellman_ford step 4393 current loss 0.860238, current_train_items 140608.
I0302 19:00:34.960486 22626471084160 run.py:483] Algo bellman_ford step 4394 current loss 1.129620, current_train_items 140640.
I0302 19:00:34.980020 22626471084160 run.py:483] Algo bellman_ford step 4395 current loss 0.297782, current_train_items 140672.
I0302 19:00:34.996397 22626471084160 run.py:483] Algo bellman_ford step 4396 current loss 0.600549, current_train_items 140704.
I0302 19:00:35.019864 22626471084160 run.py:483] Algo bellman_ford step 4397 current loss 0.704435, current_train_items 140736.
I0302 19:00:35.050513 22626471084160 run.py:483] Algo bellman_ford step 4398 current loss 0.883299, current_train_items 140768.
I0302 19:00:35.083846 22626471084160 run.py:483] Algo bellman_ford step 4399 current loss 0.857842, current_train_items 140800.
I0302 19:00:35.103541 22626471084160 run.py:483] Algo bellman_ford step 4400 current loss 0.387526, current_train_items 140832.
I0302 19:00:35.111209 22626471084160 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0302 19:00:35.111316 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:00:35.128342 22626471084160 run.py:483] Algo bellman_ford step 4401 current loss 0.482668, current_train_items 140864.
I0302 19:00:35.152098 22626471084160 run.py:483] Algo bellman_ford step 4402 current loss 0.690118, current_train_items 140896.
I0302 19:00:35.182893 22626471084160 run.py:483] Algo bellman_ford step 4403 current loss 0.696626, current_train_items 140928.
I0302 19:00:35.221076 22626471084160 run.py:483] Algo bellman_ford step 4404 current loss 1.149307, current_train_items 140960.
I0302 19:00:35.241381 22626471084160 run.py:483] Algo bellman_ford step 4405 current loss 0.286585, current_train_items 140992.
I0302 19:00:35.257466 22626471084160 run.py:483] Algo bellman_ford step 4406 current loss 0.476187, current_train_items 141024.
I0302 19:00:35.281223 22626471084160 run.py:483] Algo bellman_ford step 4407 current loss 0.745957, current_train_items 141056.
I0302 19:00:35.312174 22626471084160 run.py:483] Algo bellman_ford step 4408 current loss 0.776812, current_train_items 141088.
I0302 19:00:35.347529 22626471084160 run.py:483] Algo bellman_ford step 4409 current loss 0.960911, current_train_items 141120.
I0302 19:00:35.367213 22626471084160 run.py:483] Algo bellman_ford step 4410 current loss 0.328666, current_train_items 141152.
I0302 19:00:35.383479 22626471084160 run.py:483] Algo bellman_ford step 4411 current loss 0.500924, current_train_items 141184.
I0302 19:00:35.407146 22626471084160 run.py:483] Algo bellman_ford step 4412 current loss 0.682959, current_train_items 141216.
I0302 19:00:35.439545 22626471084160 run.py:483] Algo bellman_ford step 4413 current loss 0.901800, current_train_items 141248.
I0302 19:00:35.472050 22626471084160 run.py:483] Algo bellman_ford step 4414 current loss 0.888709, current_train_items 141280.
I0302 19:00:35.491901 22626471084160 run.py:483] Algo bellman_ford step 4415 current loss 0.304216, current_train_items 141312.
I0302 19:00:35.508582 22626471084160 run.py:483] Algo bellman_ford step 4416 current loss 0.566764, current_train_items 141344.
I0302 19:00:35.531844 22626471084160 run.py:483] Algo bellman_ford step 4417 current loss 0.681075, current_train_items 141376.
I0302 19:00:35.562467 22626471084160 run.py:483] Algo bellman_ford step 4418 current loss 0.817364, current_train_items 141408.
I0302 19:00:35.596224 22626471084160 run.py:483] Algo bellman_ford step 4419 current loss 0.812902, current_train_items 141440.
I0302 19:00:35.616016 22626471084160 run.py:483] Algo bellman_ford step 4420 current loss 0.356570, current_train_items 141472.
I0302 19:00:35.631910 22626471084160 run.py:483] Algo bellman_ford step 4421 current loss 0.570519, current_train_items 141504.
I0302 19:00:35.655288 22626471084160 run.py:483] Algo bellman_ford step 4422 current loss 0.610947, current_train_items 141536.
I0302 19:00:35.686335 22626471084160 run.py:483] Algo bellman_ford step 4423 current loss 0.672863, current_train_items 141568.
I0302 19:00:35.720768 22626471084160 run.py:483] Algo bellman_ford step 4424 current loss 0.882254, current_train_items 141600.
I0302 19:00:35.740642 22626471084160 run.py:483] Algo bellman_ford step 4425 current loss 0.317774, current_train_items 141632.
I0302 19:00:35.756016 22626471084160 run.py:483] Algo bellman_ford step 4426 current loss 0.464916, current_train_items 141664.
I0302 19:00:35.779931 22626471084160 run.py:483] Algo bellman_ford step 4427 current loss 0.652821, current_train_items 141696.
I0302 19:00:35.811084 22626471084160 run.py:483] Algo bellman_ford step 4428 current loss 0.781074, current_train_items 141728.
I0302 19:00:35.844645 22626471084160 run.py:483] Algo bellman_ford step 4429 current loss 0.896098, current_train_items 141760.
I0302 19:00:35.864442 22626471084160 run.py:483] Algo bellman_ford step 4430 current loss 0.259691, current_train_items 141792.
I0302 19:00:35.880183 22626471084160 run.py:483] Algo bellman_ford step 4431 current loss 0.461724, current_train_items 141824.
I0302 19:00:35.903841 22626471084160 run.py:483] Algo bellman_ford step 4432 current loss 0.629602, current_train_items 141856.
I0302 19:00:35.934013 22626471084160 run.py:483] Algo bellman_ford step 4433 current loss 0.784290, current_train_items 141888.
I0302 19:00:35.967769 22626471084160 run.py:483] Algo bellman_ford step 4434 current loss 0.825384, current_train_items 141920.
I0302 19:00:35.987422 22626471084160 run.py:483] Algo bellman_ford step 4435 current loss 0.353863, current_train_items 141952.
I0302 19:00:36.003734 22626471084160 run.py:483] Algo bellman_ford step 4436 current loss 0.520155, current_train_items 141984.
I0302 19:00:36.027698 22626471084160 run.py:483] Algo bellman_ford step 4437 current loss 0.736745, current_train_items 142016.
I0302 19:00:36.057605 22626471084160 run.py:483] Algo bellman_ford step 4438 current loss 0.579076, current_train_items 142048.
I0302 19:00:36.092311 22626471084160 run.py:483] Algo bellman_ford step 4439 current loss 0.803738, current_train_items 142080.
I0302 19:00:36.111967 22626471084160 run.py:483] Algo bellman_ford step 4440 current loss 0.406535, current_train_items 142112.
I0302 19:00:36.128427 22626471084160 run.py:483] Algo bellman_ford step 4441 current loss 0.567710, current_train_items 142144.
I0302 19:00:36.151616 22626471084160 run.py:483] Algo bellman_ford step 4442 current loss 0.666962, current_train_items 142176.
I0302 19:00:36.182325 22626471084160 run.py:483] Algo bellman_ford step 4443 current loss 0.767582, current_train_items 142208.
I0302 19:00:36.217027 22626471084160 run.py:483] Algo bellman_ford step 4444 current loss 0.778787, current_train_items 142240.
I0302 19:00:36.236622 22626471084160 run.py:483] Algo bellman_ford step 4445 current loss 0.335911, current_train_items 142272.
I0302 19:00:36.252573 22626471084160 run.py:483] Algo bellman_ford step 4446 current loss 0.703253, current_train_items 142304.
I0302 19:00:36.276509 22626471084160 run.py:483] Algo bellman_ford step 4447 current loss 0.650829, current_train_items 142336.
I0302 19:00:36.308249 22626471084160 run.py:483] Algo bellman_ford step 4448 current loss 0.925072, current_train_items 142368.
I0302 19:00:36.341863 22626471084160 run.py:483] Algo bellman_ford step 4449 current loss 0.875349, current_train_items 142400.
I0302 19:00:36.361692 22626471084160 run.py:483] Algo bellman_ford step 4450 current loss 0.323695, current_train_items 142432.
I0302 19:00:36.369635 22626471084160 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0302 19:00:36.369740 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:00:36.385783 22626471084160 run.py:483] Algo bellman_ford step 4451 current loss 0.483875, current_train_items 142464.
I0302 19:00:36.409878 22626471084160 run.py:483] Algo bellman_ford step 4452 current loss 0.628342, current_train_items 142496.
I0302 19:00:36.439577 22626471084160 run.py:483] Algo bellman_ford step 4453 current loss 0.674418, current_train_items 142528.
I0302 19:00:36.472419 22626471084160 run.py:483] Algo bellman_ford step 4454 current loss 0.765432, current_train_items 142560.
I0302 19:00:36.492160 22626471084160 run.py:483] Algo bellman_ford step 4455 current loss 0.260946, current_train_items 142592.
I0302 19:00:36.508003 22626471084160 run.py:483] Algo bellman_ford step 4456 current loss 0.444650, current_train_items 142624.
I0302 19:00:36.532046 22626471084160 run.py:483] Algo bellman_ford step 4457 current loss 0.617067, current_train_items 142656.
I0302 19:00:36.561388 22626471084160 run.py:483] Algo bellman_ford step 4458 current loss 0.779709, current_train_items 142688.
I0302 19:00:36.598211 22626471084160 run.py:483] Algo bellman_ford step 4459 current loss 1.006227, current_train_items 142720.
I0302 19:00:36.618182 22626471084160 run.py:483] Algo bellman_ford step 4460 current loss 0.425331, current_train_items 142752.
I0302 19:00:36.634621 22626471084160 run.py:483] Algo bellman_ford step 4461 current loss 0.501302, current_train_items 142784.
I0302 19:00:36.658648 22626471084160 run.py:483] Algo bellman_ford step 4462 current loss 0.769354, current_train_items 142816.
I0302 19:00:36.688271 22626471084160 run.py:483] Algo bellman_ford step 4463 current loss 0.635534, current_train_items 142848.
I0302 19:00:36.721968 22626471084160 run.py:483] Algo bellman_ford step 4464 current loss 0.897632, current_train_items 142880.
I0302 19:00:36.741364 22626471084160 run.py:483] Algo bellman_ford step 4465 current loss 0.265062, current_train_items 142912.
I0302 19:00:36.757659 22626471084160 run.py:483] Algo bellman_ford step 4466 current loss 0.502776, current_train_items 142944.
I0302 19:00:36.780895 22626471084160 run.py:483] Algo bellman_ford step 4467 current loss 0.677725, current_train_items 142976.
I0302 19:00:36.813082 22626471084160 run.py:483] Algo bellman_ford step 4468 current loss 0.716496, current_train_items 143008.
I0302 19:00:36.847558 22626471084160 run.py:483] Algo bellman_ford step 4469 current loss 0.916434, current_train_items 143040.
I0302 19:00:36.867445 22626471084160 run.py:483] Algo bellman_ford step 4470 current loss 0.268378, current_train_items 143072.
I0302 19:00:36.883462 22626471084160 run.py:483] Algo bellman_ford step 4471 current loss 0.561362, current_train_items 143104.
I0302 19:00:36.906284 22626471084160 run.py:483] Algo bellman_ford step 4472 current loss 0.667438, current_train_items 143136.
I0302 19:00:36.936763 22626471084160 run.py:483] Algo bellman_ford step 4473 current loss 0.746778, current_train_items 143168.
I0302 19:00:36.972644 22626471084160 run.py:483] Algo bellman_ford step 4474 current loss 0.851119, current_train_items 143200.
I0302 19:00:36.992410 22626471084160 run.py:483] Algo bellman_ford step 4475 current loss 0.312320, current_train_items 143232.
I0302 19:00:37.008452 22626471084160 run.py:483] Algo bellman_ford step 4476 current loss 0.500600, current_train_items 143264.
I0302 19:00:37.031144 22626471084160 run.py:483] Algo bellman_ford step 4477 current loss 0.606235, current_train_items 143296.
I0302 19:00:37.061670 22626471084160 run.py:483] Algo bellman_ford step 4478 current loss 0.727901, current_train_items 143328.
I0302 19:00:37.096908 22626471084160 run.py:483] Algo bellman_ford step 4479 current loss 0.970606, current_train_items 143360.
I0302 19:00:37.116161 22626471084160 run.py:483] Algo bellman_ford step 4480 current loss 0.251751, current_train_items 143392.
I0302 19:00:37.132239 22626471084160 run.py:483] Algo bellman_ford step 4481 current loss 0.553673, current_train_items 143424.
I0302 19:00:37.155745 22626471084160 run.py:483] Algo bellman_ford step 4482 current loss 0.661686, current_train_items 143456.
I0302 19:00:37.186995 22626471084160 run.py:483] Algo bellman_ford step 4483 current loss 0.712201, current_train_items 143488.
I0302 19:00:37.217971 22626471084160 run.py:483] Algo bellman_ford step 4484 current loss 0.688099, current_train_items 143520.
I0302 19:00:37.237719 22626471084160 run.py:483] Algo bellman_ford step 4485 current loss 0.372934, current_train_items 143552.
I0302 19:00:37.254244 22626471084160 run.py:483] Algo bellman_ford step 4486 current loss 0.425637, current_train_items 143584.
I0302 19:00:37.278731 22626471084160 run.py:483] Algo bellman_ford step 4487 current loss 0.739971, current_train_items 143616.
I0302 19:00:37.309779 22626471084160 run.py:483] Algo bellman_ford step 4488 current loss 0.720514, current_train_items 143648.
I0302 19:00:37.342711 22626471084160 run.py:483] Algo bellman_ford step 4489 current loss 0.792902, current_train_items 143680.
I0302 19:00:37.362935 22626471084160 run.py:483] Algo bellman_ford step 4490 current loss 0.288942, current_train_items 143712.
I0302 19:00:37.378676 22626471084160 run.py:483] Algo bellman_ford step 4491 current loss 0.466786, current_train_items 143744.
I0302 19:00:37.402470 22626471084160 run.py:483] Algo bellman_ford step 4492 current loss 0.678043, current_train_items 143776.
I0302 19:00:37.433233 22626471084160 run.py:483] Algo bellman_ford step 4493 current loss 0.756843, current_train_items 143808.
I0302 19:00:37.467189 22626471084160 run.py:483] Algo bellman_ford step 4494 current loss 0.928014, current_train_items 143840.
I0302 19:00:37.486557 22626471084160 run.py:483] Algo bellman_ford step 4495 current loss 0.376043, current_train_items 143872.
I0302 19:00:37.503064 22626471084160 run.py:483] Algo bellman_ford step 4496 current loss 0.492127, current_train_items 143904.
I0302 19:00:37.526507 22626471084160 run.py:483] Algo bellman_ford step 4497 current loss 0.782023, current_train_items 143936.
I0302 19:00:37.555761 22626471084160 run.py:483] Algo bellman_ford step 4498 current loss 0.718737, current_train_items 143968.
I0302 19:00:37.587012 22626471084160 run.py:483] Algo bellman_ford step 4499 current loss 0.713060, current_train_items 144000.
I0302 19:00:37.606781 22626471084160 run.py:483] Algo bellman_ford step 4500 current loss 0.345055, current_train_items 144032.
I0302 19:00:37.614492 22626471084160 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0302 19:00:37.614598 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:00:37.631394 22626471084160 run.py:483] Algo bellman_ford step 4501 current loss 0.502294, current_train_items 144064.
I0302 19:00:37.655781 22626471084160 run.py:483] Algo bellman_ford step 4502 current loss 0.770768, current_train_items 144096.
I0302 19:00:37.687140 22626471084160 run.py:483] Algo bellman_ford step 4503 current loss 0.733134, current_train_items 144128.
I0302 19:00:37.721601 22626471084160 run.py:483] Algo bellman_ford step 4504 current loss 0.742572, current_train_items 144160.
I0302 19:00:37.741691 22626471084160 run.py:483] Algo bellman_ford step 4505 current loss 0.269423, current_train_items 144192.
I0302 19:00:37.757894 22626471084160 run.py:483] Algo bellman_ford step 4506 current loss 0.590086, current_train_items 144224.
I0302 19:00:37.780961 22626471084160 run.py:483] Algo bellman_ford step 4507 current loss 0.615569, current_train_items 144256.
I0302 19:00:37.811096 22626471084160 run.py:483] Algo bellman_ford step 4508 current loss 0.691629, current_train_items 144288.
I0302 19:00:37.843934 22626471084160 run.py:483] Algo bellman_ford step 4509 current loss 0.752868, current_train_items 144320.
I0302 19:00:37.863765 22626471084160 run.py:483] Algo bellman_ford step 4510 current loss 0.277956, current_train_items 144352.
I0302 19:00:37.880146 22626471084160 run.py:483] Algo bellman_ford step 4511 current loss 0.600119, current_train_items 144384.
I0302 19:00:37.903952 22626471084160 run.py:483] Algo bellman_ford step 4512 current loss 0.691505, current_train_items 144416.
I0302 19:00:37.934925 22626471084160 run.py:483] Algo bellman_ford step 4513 current loss 0.715665, current_train_items 144448.
I0302 19:00:37.967937 22626471084160 run.py:483] Algo bellman_ford step 4514 current loss 1.014086, current_train_items 144480.
I0302 19:00:37.987500 22626471084160 run.py:483] Algo bellman_ford step 4515 current loss 0.443323, current_train_items 144512.
I0302 19:00:38.003940 22626471084160 run.py:483] Algo bellman_ford step 4516 current loss 0.461718, current_train_items 144544.
I0302 19:00:38.026667 22626471084160 run.py:483] Algo bellman_ford step 4517 current loss 0.556027, current_train_items 144576.
I0302 19:00:38.058196 22626471084160 run.py:483] Algo bellman_ford step 4518 current loss 0.746811, current_train_items 144608.
I0302 19:00:38.095176 22626471084160 run.py:483] Algo bellman_ford step 4519 current loss 1.002808, current_train_items 144640.
I0302 19:00:38.114894 22626471084160 run.py:483] Algo bellman_ford step 4520 current loss 0.317859, current_train_items 144672.
I0302 19:00:38.131284 22626471084160 run.py:483] Algo bellman_ford step 4521 current loss 0.490886, current_train_items 144704.
I0302 19:00:38.154640 22626471084160 run.py:483] Algo bellman_ford step 4522 current loss 0.610192, current_train_items 144736.
I0302 19:00:38.187319 22626471084160 run.py:483] Algo bellman_ford step 4523 current loss 0.877934, current_train_items 144768.
I0302 19:00:38.220806 22626471084160 run.py:483] Algo bellman_ford step 4524 current loss 0.778870, current_train_items 144800.
I0302 19:00:38.240182 22626471084160 run.py:483] Algo bellman_ford step 4525 current loss 0.308482, current_train_items 144832.
I0302 19:00:38.256433 22626471084160 run.py:483] Algo bellman_ford step 4526 current loss 0.430522, current_train_items 144864.
I0302 19:00:38.280866 22626471084160 run.py:483] Algo bellman_ford step 4527 current loss 0.714463, current_train_items 144896.
I0302 19:00:38.311893 22626471084160 run.py:483] Algo bellman_ford step 4528 current loss 0.707711, current_train_items 144928.
I0302 19:00:38.346182 22626471084160 run.py:483] Algo bellman_ford step 4529 current loss 0.851256, current_train_items 144960.
I0302 19:00:38.365743 22626471084160 run.py:483] Algo bellman_ford step 4530 current loss 0.337958, current_train_items 144992.
I0302 19:00:38.381593 22626471084160 run.py:483] Algo bellman_ford step 4531 current loss 0.459535, current_train_items 145024.
I0302 19:00:38.404316 22626471084160 run.py:483] Algo bellman_ford step 4532 current loss 0.581874, current_train_items 145056.
I0302 19:00:38.433331 22626471084160 run.py:483] Algo bellman_ford step 4533 current loss 0.593361, current_train_items 145088.
I0302 19:00:38.468294 22626471084160 run.py:483] Algo bellman_ford step 4534 current loss 0.839469, current_train_items 145120.
I0302 19:00:38.487926 22626471084160 run.py:483] Algo bellman_ford step 4535 current loss 0.255768, current_train_items 145152.
I0302 19:00:38.504081 22626471084160 run.py:483] Algo bellman_ford step 4536 current loss 0.502067, current_train_items 145184.
I0302 19:00:38.527914 22626471084160 run.py:483] Algo bellman_ford step 4537 current loss 0.649228, current_train_items 145216.
I0302 19:00:38.558661 22626471084160 run.py:483] Algo bellman_ford step 4538 current loss 0.632978, current_train_items 145248.
I0302 19:00:38.593726 22626471084160 run.py:483] Algo bellman_ford step 4539 current loss 0.964212, current_train_items 145280.
I0302 19:00:38.613265 22626471084160 run.py:483] Algo bellman_ford step 4540 current loss 0.366981, current_train_items 145312.
I0302 19:00:38.629337 22626471084160 run.py:483] Algo bellman_ford step 4541 current loss 0.467517, current_train_items 145344.
I0302 19:00:38.652182 22626471084160 run.py:483] Algo bellman_ford step 4542 current loss 0.614479, current_train_items 145376.
I0302 19:00:38.682740 22626471084160 run.py:483] Algo bellman_ford step 4543 current loss 0.695020, current_train_items 145408.
I0302 19:00:38.717213 22626471084160 run.py:483] Algo bellman_ford step 4544 current loss 0.907959, current_train_items 145440.
I0302 19:00:38.736991 22626471084160 run.py:483] Algo bellman_ford step 4545 current loss 0.349376, current_train_items 145472.
I0302 19:00:38.753135 22626471084160 run.py:483] Algo bellman_ford step 4546 current loss 0.487041, current_train_items 145504.
I0302 19:00:38.776396 22626471084160 run.py:483] Algo bellman_ford step 4547 current loss 0.618773, current_train_items 145536.
I0302 19:00:38.806199 22626471084160 run.py:483] Algo bellman_ford step 4548 current loss 0.688546, current_train_items 145568.
I0302 19:00:38.840745 22626471084160 run.py:483] Algo bellman_ford step 4549 current loss 0.816293, current_train_items 145600.
I0302 19:00:38.860409 22626471084160 run.py:483] Algo bellman_ford step 4550 current loss 0.279750, current_train_items 145632.
I0302 19:00:38.868484 22626471084160 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0302 19:00:38.868589 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:00:38.885641 22626471084160 run.py:483] Algo bellman_ford step 4551 current loss 0.508613, current_train_items 145664.
I0302 19:00:38.909486 22626471084160 run.py:483] Algo bellman_ford step 4552 current loss 0.583550, current_train_items 145696.
I0302 19:00:38.941389 22626471084160 run.py:483] Algo bellman_ford step 4553 current loss 0.749221, current_train_items 145728.
I0302 19:00:38.975605 22626471084160 run.py:483] Algo bellman_ford step 4554 current loss 0.860829, current_train_items 145760.
I0302 19:00:38.996072 22626471084160 run.py:483] Algo bellman_ford step 4555 current loss 0.325430, current_train_items 145792.
I0302 19:00:39.011616 22626471084160 run.py:483] Algo bellman_ford step 4556 current loss 0.499685, current_train_items 145824.
I0302 19:00:39.034676 22626471084160 run.py:483] Algo bellman_ford step 4557 current loss 0.606852, current_train_items 145856.
I0302 19:00:39.066395 22626471084160 run.py:483] Algo bellman_ford step 4558 current loss 0.662003, current_train_items 145888.
I0302 19:00:39.101603 22626471084160 run.py:483] Algo bellman_ford step 4559 current loss 0.892512, current_train_items 145920.
I0302 19:00:39.121421 22626471084160 run.py:483] Algo bellman_ford step 4560 current loss 0.261986, current_train_items 145952.
I0302 19:00:39.137867 22626471084160 run.py:483] Algo bellman_ford step 4561 current loss 0.499005, current_train_items 145984.
I0302 19:00:39.160758 22626471084160 run.py:483] Algo bellman_ford step 4562 current loss 0.610450, current_train_items 146016.
I0302 19:00:39.190911 22626471084160 run.py:483] Algo bellman_ford step 4563 current loss 0.692114, current_train_items 146048.
I0302 19:00:39.225798 22626471084160 run.py:483] Algo bellman_ford step 4564 current loss 0.794772, current_train_items 146080.
I0302 19:00:39.245486 22626471084160 run.py:483] Algo bellman_ford step 4565 current loss 0.309541, current_train_items 146112.
I0302 19:00:39.261626 22626471084160 run.py:483] Algo bellman_ford step 4566 current loss 0.512126, current_train_items 146144.
I0302 19:00:39.285584 22626471084160 run.py:483] Algo bellman_ford step 4567 current loss 0.592325, current_train_items 146176.
I0302 19:00:39.317523 22626471084160 run.py:483] Algo bellman_ford step 4568 current loss 0.760968, current_train_items 146208.
I0302 19:00:39.350631 22626471084160 run.py:483] Algo bellman_ford step 4569 current loss 0.840964, current_train_items 146240.
I0302 19:00:39.370501 22626471084160 run.py:483] Algo bellman_ford step 4570 current loss 0.265540, current_train_items 146272.
I0302 19:00:39.386557 22626471084160 run.py:483] Algo bellman_ford step 4571 current loss 0.466781, current_train_items 146304.
I0302 19:00:39.410524 22626471084160 run.py:483] Algo bellman_ford step 4572 current loss 0.632900, current_train_items 146336.
I0302 19:00:39.440909 22626471084160 run.py:483] Algo bellman_ford step 4573 current loss 0.674302, current_train_items 146368.
I0302 19:00:39.473555 22626471084160 run.py:483] Algo bellman_ford step 4574 current loss 0.774403, current_train_items 146400.
I0302 19:00:39.493345 22626471084160 run.py:483] Algo bellman_ford step 4575 current loss 0.344787, current_train_items 146432.
I0302 19:00:39.509501 22626471084160 run.py:483] Algo bellman_ford step 4576 current loss 0.534697, current_train_items 146464.
I0302 19:00:39.532646 22626471084160 run.py:483] Algo bellman_ford step 4577 current loss 0.639066, current_train_items 146496.
I0302 19:00:39.563982 22626471084160 run.py:483] Algo bellman_ford step 4578 current loss 0.779415, current_train_items 146528.
I0302 19:00:39.598290 22626471084160 run.py:483] Algo bellman_ford step 4579 current loss 0.840246, current_train_items 146560.
I0302 19:00:39.617841 22626471084160 run.py:483] Algo bellman_ford step 4580 current loss 0.343624, current_train_items 146592.
I0302 19:00:39.634535 22626471084160 run.py:483] Algo bellman_ford step 4581 current loss 0.537673, current_train_items 146624.
I0302 19:00:39.657685 22626471084160 run.py:483] Algo bellman_ford step 4582 current loss 0.562466, current_train_items 146656.
I0302 19:00:39.688687 22626471084160 run.py:483] Algo bellman_ford step 4583 current loss 0.659023, current_train_items 146688.
I0302 19:00:39.722595 22626471084160 run.py:483] Algo bellman_ford step 4584 current loss 1.001696, current_train_items 146720.
I0302 19:00:39.742716 22626471084160 run.py:483] Algo bellman_ford step 4585 current loss 0.316594, current_train_items 146752.
I0302 19:00:39.759162 22626471084160 run.py:483] Algo bellman_ford step 4586 current loss 0.549725, current_train_items 146784.
I0302 19:00:39.781645 22626471084160 run.py:483] Algo bellman_ford step 4587 current loss 0.592167, current_train_items 146816.
I0302 19:00:39.812676 22626471084160 run.py:483] Algo bellman_ford step 4588 current loss 0.614830, current_train_items 146848.
I0302 19:00:39.845496 22626471084160 run.py:483] Algo bellman_ford step 4589 current loss 0.829415, current_train_items 146880.
I0302 19:00:39.865658 22626471084160 run.py:483] Algo bellman_ford step 4590 current loss 0.348998, current_train_items 146912.
I0302 19:00:39.881994 22626471084160 run.py:483] Algo bellman_ford step 4591 current loss 0.547387, current_train_items 146944.
I0302 19:00:39.904878 22626471084160 run.py:483] Algo bellman_ford step 4592 current loss 0.668111, current_train_items 146976.
I0302 19:00:39.936655 22626471084160 run.py:483] Algo bellman_ford step 4593 current loss 0.728949, current_train_items 147008.
I0302 19:00:39.971466 22626471084160 run.py:483] Algo bellman_ford step 4594 current loss 0.959259, current_train_items 147040.
I0302 19:00:39.990993 22626471084160 run.py:483] Algo bellman_ford step 4595 current loss 0.334973, current_train_items 147072.
I0302 19:00:40.007102 22626471084160 run.py:483] Algo bellman_ford step 4596 current loss 0.443488, current_train_items 147104.
I0302 19:00:40.032539 22626471084160 run.py:483] Algo bellman_ford step 4597 current loss 0.915635, current_train_items 147136.
I0302 19:00:40.063053 22626471084160 run.py:483] Algo bellman_ford step 4598 current loss 0.914501, current_train_items 147168.
I0302 19:00:40.095201 22626471084160 run.py:483] Algo bellman_ford step 4599 current loss 0.961238, current_train_items 147200.
I0302 19:00:40.115425 22626471084160 run.py:483] Algo bellman_ford step 4600 current loss 0.373994, current_train_items 147232.
I0302 19:00:40.123266 22626471084160 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0302 19:00:40.123373 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 19:00:40.140430 22626471084160 run.py:483] Algo bellman_ford step 4601 current loss 0.513212, current_train_items 147264.
I0302 19:00:40.165351 22626471084160 run.py:483] Algo bellman_ford step 4602 current loss 0.730452, current_train_items 147296.
I0302 19:00:40.196548 22626471084160 run.py:483] Algo bellman_ford step 4603 current loss 0.825874, current_train_items 147328.
I0302 19:00:40.231796 22626471084160 run.py:483] Algo bellman_ford step 4604 current loss 1.046299, current_train_items 147360.
I0302 19:00:40.251889 22626471084160 run.py:483] Algo bellman_ford step 4605 current loss 0.298345, current_train_items 147392.
I0302 19:00:40.268139 22626471084160 run.py:483] Algo bellman_ford step 4606 current loss 0.561260, current_train_items 147424.
I0302 19:00:40.291610 22626471084160 run.py:483] Algo bellman_ford step 4607 current loss 0.651924, current_train_items 147456.
I0302 19:00:40.323426 22626471084160 run.py:483] Algo bellman_ford step 4608 current loss 0.712363, current_train_items 147488.
I0302 19:00:40.355569 22626471084160 run.py:483] Algo bellman_ford step 4609 current loss 0.791422, current_train_items 147520.
I0302 19:00:40.375073 22626471084160 run.py:483] Algo bellman_ford step 4610 current loss 0.274784, current_train_items 147552.
I0302 19:00:40.391103 22626471084160 run.py:483] Algo bellman_ford step 4611 current loss 0.630028, current_train_items 147584.
I0302 19:00:40.415022 22626471084160 run.py:483] Algo bellman_ford step 4612 current loss 0.675092, current_train_items 147616.
I0302 19:00:40.446474 22626471084160 run.py:483] Algo bellman_ford step 4613 current loss 0.863192, current_train_items 147648.
I0302 19:00:40.481253 22626471084160 run.py:483] Algo bellman_ford step 4614 current loss 0.924890, current_train_items 147680.
I0302 19:00:40.500855 22626471084160 run.py:483] Algo bellman_ford step 4615 current loss 0.350549, current_train_items 147712.
I0302 19:00:40.517071 22626471084160 run.py:483] Algo bellman_ford step 4616 current loss 0.582744, current_train_items 147744.
I0302 19:00:40.540946 22626471084160 run.py:483] Algo bellman_ford step 4617 current loss 0.643366, current_train_items 147776.
I0302 19:00:40.571149 22626471084160 run.py:483] Algo bellman_ford step 4618 current loss 0.628300, current_train_items 147808.
I0302 19:00:40.604866 22626471084160 run.py:483] Algo bellman_ford step 4619 current loss 0.822739, current_train_items 147840.
I0302 19:00:40.624463 22626471084160 run.py:483] Algo bellman_ford step 4620 current loss 0.283119, current_train_items 147872.
I0302 19:00:40.640480 22626471084160 run.py:483] Algo bellman_ford step 4621 current loss 0.411313, current_train_items 147904.
I0302 19:00:40.663647 22626471084160 run.py:483] Algo bellman_ford step 4622 current loss 0.607851, current_train_items 147936.
I0302 19:00:40.694561 22626471084160 run.py:483] Algo bellman_ford step 4623 current loss 0.601419, current_train_items 147968.
I0302 19:00:40.727371 22626471084160 run.py:483] Algo bellman_ford step 4624 current loss 0.977576, current_train_items 148000.
I0302 19:00:40.746968 22626471084160 run.py:483] Algo bellman_ford step 4625 current loss 0.319903, current_train_items 148032.
I0302 19:00:40.763141 22626471084160 run.py:483] Algo bellman_ford step 4626 current loss 0.575716, current_train_items 148064.
I0302 19:00:40.787379 22626471084160 run.py:483] Algo bellman_ford step 4627 current loss 0.646332, current_train_items 148096.
I0302 19:00:40.819119 22626471084160 run.py:483] Algo bellman_ford step 4628 current loss 0.771329, current_train_items 148128.
I0302 19:00:40.854315 22626471084160 run.py:483] Algo bellman_ford step 4629 current loss 0.955726, current_train_items 148160.
I0302 19:00:40.874063 22626471084160 run.py:483] Algo bellman_ford step 4630 current loss 0.270017, current_train_items 148192.
I0302 19:00:40.890425 22626471084160 run.py:483] Algo bellman_ford step 4631 current loss 0.558108, current_train_items 148224.
I0302 19:00:40.914680 22626471084160 run.py:483] Algo bellman_ford step 4632 current loss 0.817973, current_train_items 148256.
I0302 19:00:40.944278 22626471084160 run.py:483] Algo bellman_ford step 4633 current loss 0.754481, current_train_items 148288.
I0302 19:00:40.978473 22626471084160 run.py:483] Algo bellman_ford step 4634 current loss 0.792119, current_train_items 148320.
I0302 19:00:40.998376 22626471084160 run.py:483] Algo bellman_ford step 4635 current loss 0.339888, current_train_items 148352.
I0302 19:00:41.015129 22626471084160 run.py:483] Algo bellman_ford step 4636 current loss 0.571464, current_train_items 148384.
I0302 19:00:41.039033 22626471084160 run.py:483] Algo bellman_ford step 4637 current loss 0.752174, current_train_items 148416.
I0302 19:00:41.071682 22626471084160 run.py:483] Algo bellman_ford step 4638 current loss 0.895709, current_train_items 148448.
I0302 19:00:41.106211 22626471084160 run.py:483] Algo bellman_ford step 4639 current loss 0.925237, current_train_items 148480.
I0302 19:00:41.125823 22626471084160 run.py:483] Algo bellman_ford step 4640 current loss 0.334657, current_train_items 148512.
I0302 19:00:41.142012 22626471084160 run.py:483] Algo bellman_ford step 4641 current loss 0.495580, current_train_items 148544.
I0302 19:00:41.165852 22626471084160 run.py:483] Algo bellman_ford step 4642 current loss 0.770209, current_train_items 148576.
I0302 19:00:41.197312 22626471084160 run.py:483] Algo bellman_ford step 4643 current loss 1.114175, current_train_items 148608.
I0302 19:00:41.231986 22626471084160 run.py:483] Algo bellman_ford step 4644 current loss 1.449916, current_train_items 148640.
I0302 19:00:41.251883 22626471084160 run.py:483] Algo bellman_ford step 4645 current loss 0.357894, current_train_items 148672.
I0302 19:00:41.268141 22626471084160 run.py:483] Algo bellman_ford step 4646 current loss 0.555354, current_train_items 148704.
I0302 19:00:41.290454 22626471084160 run.py:483] Algo bellman_ford step 4647 current loss 0.819198, current_train_items 148736.
I0302 19:00:41.319712 22626471084160 run.py:483] Algo bellman_ford step 4648 current loss 0.914562, current_train_items 148768.
I0302 19:00:41.351448 22626471084160 run.py:483] Algo bellman_ford step 4649 current loss 0.937513, current_train_items 148800.
I0302 19:00:41.371192 22626471084160 run.py:483] Algo bellman_ford step 4650 current loss 0.272098, current_train_items 148832.
I0302 19:00:41.379094 22626471084160 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0302 19:00:41.379208 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 19:00:41.396202 22626471084160 run.py:483] Algo bellman_ford step 4651 current loss 0.560416, current_train_items 148864.
I0302 19:00:41.420422 22626471084160 run.py:483] Algo bellman_ford step 4652 current loss 0.697395, current_train_items 148896.
I0302 19:00:41.452719 22626471084160 run.py:483] Algo bellman_ford step 4653 current loss 0.845272, current_train_items 148928.
I0302 19:00:41.486887 22626471084160 run.py:483] Algo bellman_ford step 4654 current loss 1.397394, current_train_items 148960.
I0302 19:00:41.506720 22626471084160 run.py:483] Algo bellman_ford step 4655 current loss 0.390156, current_train_items 148992.
I0302 19:00:41.522653 22626471084160 run.py:483] Algo bellman_ford step 4656 current loss 0.675964, current_train_items 149024.
I0302 19:00:41.546745 22626471084160 run.py:483] Algo bellman_ford step 4657 current loss 0.666459, current_train_items 149056.
I0302 19:00:41.577773 22626471084160 run.py:483] Algo bellman_ford step 4658 current loss 0.805093, current_train_items 149088.
I0302 19:00:41.610557 22626471084160 run.py:483] Algo bellman_ford step 4659 current loss 0.798584, current_train_items 149120.
I0302 19:00:41.630449 22626471084160 run.py:483] Algo bellman_ford step 4660 current loss 0.354421, current_train_items 149152.
I0302 19:00:41.646853 22626471084160 run.py:483] Algo bellman_ford step 4661 current loss 0.516777, current_train_items 149184.
I0302 19:00:41.670900 22626471084160 run.py:483] Algo bellman_ford step 4662 current loss 0.639273, current_train_items 149216.
I0302 19:00:41.701534 22626471084160 run.py:483] Algo bellman_ford step 4663 current loss 0.738312, current_train_items 149248.
I0302 19:00:41.738586 22626471084160 run.py:483] Algo bellman_ford step 4664 current loss 0.984859, current_train_items 149280.
I0302 19:00:41.758312 22626471084160 run.py:483] Algo bellman_ford step 4665 current loss 0.363135, current_train_items 149312.
I0302 19:00:41.774207 22626471084160 run.py:483] Algo bellman_ford step 4666 current loss 0.451239, current_train_items 149344.
I0302 19:00:41.797463 22626471084160 run.py:483] Algo bellman_ford step 4667 current loss 0.641390, current_train_items 149376.
I0302 19:00:41.829249 22626471084160 run.py:483] Algo bellman_ford step 4668 current loss 0.748654, current_train_items 149408.
I0302 19:00:41.864020 22626471084160 run.py:483] Algo bellman_ford step 4669 current loss 0.801839, current_train_items 149440.
I0302 19:00:41.883835 22626471084160 run.py:483] Algo bellman_ford step 4670 current loss 0.334338, current_train_items 149472.
I0302 19:00:41.899640 22626471084160 run.py:483] Algo bellman_ford step 4671 current loss 0.426439, current_train_items 149504.
I0302 19:00:41.922314 22626471084160 run.py:483] Algo bellman_ford step 4672 current loss 0.675744, current_train_items 149536.
I0302 19:00:41.952249 22626471084160 run.py:483] Algo bellman_ford step 4673 current loss 0.730604, current_train_items 149568.
I0302 19:00:41.985522 22626471084160 run.py:483] Algo bellman_ford step 4674 current loss 0.703081, current_train_items 149600.
I0302 19:00:42.005575 22626471084160 run.py:483] Algo bellman_ford step 4675 current loss 0.280042, current_train_items 149632.
I0302 19:00:42.022284 22626471084160 run.py:483] Algo bellman_ford step 4676 current loss 0.619581, current_train_items 149664.
I0302 19:00:42.045984 22626471084160 run.py:483] Algo bellman_ford step 4677 current loss 0.740474, current_train_items 149696.
I0302 19:00:42.078485 22626471084160 run.py:483] Algo bellman_ford step 4678 current loss 0.837009, current_train_items 149728.
I0302 19:00:42.112265 22626471084160 run.py:483] Algo bellman_ford step 4679 current loss 0.916608, current_train_items 149760.
I0302 19:00:42.131798 22626471084160 run.py:483] Algo bellman_ford step 4680 current loss 0.342876, current_train_items 149792.
I0302 19:00:42.147691 22626471084160 run.py:483] Algo bellman_ford step 4681 current loss 0.578642, current_train_items 149824.
I0302 19:00:42.171213 22626471084160 run.py:483] Algo bellman_ford step 4682 current loss 0.728511, current_train_items 149856.
I0302 19:00:42.201975 22626471084160 run.py:483] Algo bellman_ford step 4683 current loss 0.932205, current_train_items 149888.
I0302 19:00:42.237079 22626471084160 run.py:483] Algo bellman_ford step 4684 current loss 1.110812, current_train_items 149920.
I0302 19:00:42.257464 22626471084160 run.py:483] Algo bellman_ford step 4685 current loss 0.338568, current_train_items 149952.
I0302 19:00:42.273312 22626471084160 run.py:483] Algo bellman_ford step 4686 current loss 0.434196, current_train_items 149984.
I0302 19:00:42.296550 22626471084160 run.py:483] Algo bellman_ford step 4687 current loss 0.642443, current_train_items 150016.
I0302 19:00:42.327981 22626471084160 run.py:483] Algo bellman_ford step 4688 current loss 0.781997, current_train_items 150048.
I0302 19:00:42.363475 22626471084160 run.py:483] Algo bellman_ford step 4689 current loss 0.973541, current_train_items 150080.
I0302 19:00:42.383578 22626471084160 run.py:483] Algo bellman_ford step 4690 current loss 0.298672, current_train_items 150112.
I0302 19:00:42.399792 22626471084160 run.py:483] Algo bellman_ford step 4691 current loss 0.496646, current_train_items 150144.
I0302 19:00:42.422699 22626471084160 run.py:483] Algo bellman_ford step 4692 current loss 0.530783, current_train_items 150176.
I0302 19:00:42.453963 22626471084160 run.py:483] Algo bellman_ford step 4693 current loss 0.715649, current_train_items 150208.
I0302 19:00:42.486893 22626471084160 run.py:483] Algo bellman_ford step 4694 current loss 0.891760, current_train_items 150240.
I0302 19:00:42.506592 22626471084160 run.py:483] Algo bellman_ford step 4695 current loss 0.301364, current_train_items 150272.
I0302 19:00:42.522856 22626471084160 run.py:483] Algo bellman_ford step 4696 current loss 0.464815, current_train_items 150304.
I0302 19:00:42.547022 22626471084160 run.py:483] Algo bellman_ford step 4697 current loss 0.752785, current_train_items 150336.
I0302 19:00:42.577684 22626471084160 run.py:483] Algo bellman_ford step 4698 current loss 0.806273, current_train_items 150368.
I0302 19:00:42.610008 22626471084160 run.py:483] Algo bellman_ford step 4699 current loss 0.949897, current_train_items 150400.
I0302 19:00:42.630043 22626471084160 run.py:483] Algo bellman_ford step 4700 current loss 0.358659, current_train_items 150432.
I0302 19:00:42.637957 22626471084160 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0302 19:00:42.638080 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 19:00:42.654427 22626471084160 run.py:483] Algo bellman_ford step 4701 current loss 0.440614, current_train_items 150464.
I0302 19:00:42.678740 22626471084160 run.py:483] Algo bellman_ford step 4702 current loss 0.696561, current_train_items 150496.
I0302 19:00:42.711414 22626471084160 run.py:483] Algo bellman_ford step 4703 current loss 0.770225, current_train_items 150528.
I0302 19:00:42.746998 22626471084160 run.py:483] Algo bellman_ford step 4704 current loss 0.852632, current_train_items 150560.
I0302 19:00:42.766966 22626471084160 run.py:483] Algo bellman_ford step 4705 current loss 0.437435, current_train_items 150592.
I0302 19:00:42.782616 22626471084160 run.py:483] Algo bellman_ford step 4706 current loss 0.447088, current_train_items 150624.
I0302 19:00:42.806564 22626471084160 run.py:483] Algo bellman_ford step 4707 current loss 0.680268, current_train_items 150656.
I0302 19:00:42.838590 22626471084160 run.py:483] Algo bellman_ford step 4708 current loss 0.887714, current_train_items 150688.
I0302 19:00:42.872117 22626471084160 run.py:483] Algo bellman_ford step 4709 current loss 0.785127, current_train_items 150720.
I0302 19:00:42.891601 22626471084160 run.py:483] Algo bellman_ford step 4710 current loss 0.352369, current_train_items 150752.
I0302 19:00:42.907843 22626471084160 run.py:483] Algo bellman_ford step 4711 current loss 0.512066, current_train_items 150784.
I0302 19:00:42.932000 22626471084160 run.py:483] Algo bellman_ford step 4712 current loss 0.653924, current_train_items 150816.
I0302 19:00:42.963143 22626471084160 run.py:483] Algo bellman_ford step 4713 current loss 0.658582, current_train_items 150848.
I0302 19:00:42.995272 22626471084160 run.py:483] Algo bellman_ford step 4714 current loss 0.791120, current_train_items 150880.
I0302 19:00:43.014955 22626471084160 run.py:483] Algo bellman_ford step 4715 current loss 0.291104, current_train_items 150912.
I0302 19:00:43.031295 22626471084160 run.py:483] Algo bellman_ford step 4716 current loss 0.565214, current_train_items 150944.
I0302 19:00:43.053711 22626471084160 run.py:483] Algo bellman_ford step 4717 current loss 0.569197, current_train_items 150976.
I0302 19:00:43.084635 22626471084160 run.py:483] Algo bellman_ford step 4718 current loss 0.676077, current_train_items 151008.
I0302 19:00:43.120588 22626471084160 run.py:483] Algo bellman_ford step 4719 current loss 0.916021, current_train_items 151040.
I0302 19:00:43.140300 22626471084160 run.py:483] Algo bellman_ford step 4720 current loss 0.305134, current_train_items 151072.
I0302 19:00:43.156137 22626471084160 run.py:483] Algo bellman_ford step 4721 current loss 0.530844, current_train_items 151104.
I0302 19:00:43.180363 22626471084160 run.py:483] Algo bellman_ford step 4722 current loss 0.676086, current_train_items 151136.
I0302 19:00:43.209782 22626471084160 run.py:483] Algo bellman_ford step 4723 current loss 0.628266, current_train_items 151168.
I0302 19:00:43.244320 22626471084160 run.py:483] Algo bellman_ford step 4724 current loss 0.783067, current_train_items 151200.
I0302 19:00:43.263807 22626471084160 run.py:483] Algo bellman_ford step 4725 current loss 0.338244, current_train_items 151232.
I0302 19:00:43.280329 22626471084160 run.py:483] Algo bellman_ford step 4726 current loss 0.549011, current_train_items 151264.
I0302 19:00:43.304923 22626471084160 run.py:483] Algo bellman_ford step 4727 current loss 0.753118, current_train_items 151296.
I0302 19:00:43.334447 22626471084160 run.py:483] Algo bellman_ford step 4728 current loss 0.619582, current_train_items 151328.
I0302 19:00:43.364451 22626471084160 run.py:483] Algo bellman_ford step 4729 current loss 0.644516, current_train_items 151360.
I0302 19:00:43.383900 22626471084160 run.py:483] Algo bellman_ford step 4730 current loss 0.313845, current_train_items 151392.
I0302 19:00:43.399833 22626471084160 run.py:483] Algo bellman_ford step 4731 current loss 0.398254, current_train_items 151424.
I0302 19:00:43.422738 22626471084160 run.py:483] Algo bellman_ford step 4732 current loss 0.652842, current_train_items 151456.
I0302 19:00:43.453703 22626471084160 run.py:483] Algo bellman_ford step 4733 current loss 0.834861, current_train_items 151488.
I0302 19:00:43.487735 22626471084160 run.py:483] Algo bellman_ford step 4734 current loss 0.860092, current_train_items 151520.
I0302 19:00:43.507196 22626471084160 run.py:483] Algo bellman_ford step 4735 current loss 0.345106, current_train_items 151552.
I0302 19:00:43.523753 22626471084160 run.py:483] Algo bellman_ford step 4736 current loss 0.543013, current_train_items 151584.
I0302 19:00:43.547199 22626471084160 run.py:483] Algo bellman_ford step 4737 current loss 0.713931, current_train_items 151616.
I0302 19:00:43.577377 22626471084160 run.py:483] Algo bellman_ford step 4738 current loss 0.687901, current_train_items 151648.
I0302 19:00:43.610265 22626471084160 run.py:483] Algo bellman_ford step 4739 current loss 0.855468, current_train_items 151680.
I0302 19:00:43.629805 22626471084160 run.py:483] Algo bellman_ford step 4740 current loss 0.278225, current_train_items 151712.
I0302 19:00:43.645767 22626471084160 run.py:483] Algo bellman_ford step 4741 current loss 0.488198, current_train_items 151744.
I0302 19:00:43.668842 22626471084160 run.py:483] Algo bellman_ford step 4742 current loss 0.628271, current_train_items 151776.
I0302 19:00:43.700522 22626471084160 run.py:483] Algo bellman_ford step 4743 current loss 0.835742, current_train_items 151808.
I0302 19:00:43.732604 22626471084160 run.py:483] Algo bellman_ford step 4744 current loss 0.827906, current_train_items 151840.
I0302 19:00:43.751976 22626471084160 run.py:483] Algo bellman_ford step 4745 current loss 0.340859, current_train_items 151872.
I0302 19:00:43.768691 22626471084160 run.py:483] Algo bellman_ford step 4746 current loss 0.466175, current_train_items 151904.
I0302 19:00:43.793838 22626471084160 run.py:483] Algo bellman_ford step 4747 current loss 0.685651, current_train_items 151936.
I0302 19:00:43.824813 22626471084160 run.py:483] Algo bellman_ford step 4748 current loss 0.744202, current_train_items 151968.
I0302 19:00:43.858200 22626471084160 run.py:483] Algo bellman_ford step 4749 current loss 0.758135, current_train_items 152000.
I0302 19:00:43.877919 22626471084160 run.py:483] Algo bellman_ford step 4750 current loss 0.295527, current_train_items 152032.
I0302 19:00:43.885760 22626471084160 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.880859375, 'score': 0.880859375, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0302 19:00:43.885866 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.881, val scores are: bellman_ford: 0.881
I0302 19:00:43.902583 22626471084160 run.py:483] Algo bellman_ford step 4751 current loss 0.605077, current_train_items 152064.
I0302 19:00:43.927606 22626471084160 run.py:483] Algo bellman_ford step 4752 current loss 0.829036, current_train_items 152096.
I0302 19:00:43.961001 22626471084160 run.py:483] Algo bellman_ford step 4753 current loss 0.860908, current_train_items 152128.
I0302 19:00:43.995026 22626471084160 run.py:483] Algo bellman_ford step 4754 current loss 0.856190, current_train_items 152160.
I0302 19:00:44.015631 22626471084160 run.py:483] Algo bellman_ford step 4755 current loss 0.340962, current_train_items 152192.
I0302 19:00:44.031534 22626471084160 run.py:483] Algo bellman_ford step 4756 current loss 0.542666, current_train_items 152224.
I0302 19:00:44.055971 22626471084160 run.py:483] Algo bellman_ford step 4757 current loss 0.594855, current_train_items 152256.
I0302 19:00:44.087083 22626471084160 run.py:483] Algo bellman_ford step 4758 current loss 0.665449, current_train_items 152288.
I0302 19:00:44.121867 22626471084160 run.py:483] Algo bellman_ford step 4759 current loss 1.067554, current_train_items 152320.
I0302 19:00:44.141811 22626471084160 run.py:483] Algo bellman_ford step 4760 current loss 0.344449, current_train_items 152352.
I0302 19:00:44.158207 22626471084160 run.py:483] Algo bellman_ford step 4761 current loss 0.504899, current_train_items 152384.
I0302 19:00:44.181847 22626471084160 run.py:483] Algo bellman_ford step 4762 current loss 0.705421, current_train_items 152416.
I0302 19:00:44.212346 22626471084160 run.py:483] Algo bellman_ford step 4763 current loss 0.754219, current_train_items 152448.
I0302 19:00:44.246387 22626471084160 run.py:483] Algo bellman_ford step 4764 current loss 0.880812, current_train_items 152480.
I0302 19:00:44.265796 22626471084160 run.py:483] Algo bellman_ford step 4765 current loss 0.417959, current_train_items 152512.
I0302 19:00:44.281672 22626471084160 run.py:483] Algo bellman_ford step 4766 current loss 0.411131, current_train_items 152544.
I0302 19:00:44.306375 22626471084160 run.py:483] Algo bellman_ford step 4767 current loss 0.614952, current_train_items 152576.
I0302 19:00:44.336138 22626471084160 run.py:483] Algo bellman_ford step 4768 current loss 0.708866, current_train_items 152608.
I0302 19:00:44.368964 22626471084160 run.py:483] Algo bellman_ford step 4769 current loss 0.844269, current_train_items 152640.
I0302 19:00:44.389006 22626471084160 run.py:483] Algo bellman_ford step 4770 current loss 0.439443, current_train_items 152672.
I0302 19:00:44.405213 22626471084160 run.py:483] Algo bellman_ford step 4771 current loss 0.416577, current_train_items 152704.
I0302 19:00:44.427802 22626471084160 run.py:483] Algo bellman_ford step 4772 current loss 0.579431, current_train_items 152736.
I0302 19:00:44.458510 22626471084160 run.py:483] Algo bellman_ford step 4773 current loss 0.864145, current_train_items 152768.
I0302 19:00:44.492789 22626471084160 run.py:483] Algo bellman_ford step 4774 current loss 0.896645, current_train_items 152800.
I0302 19:00:44.512636 22626471084160 run.py:483] Algo bellman_ford step 4775 current loss 0.319491, current_train_items 152832.
I0302 19:00:44.528805 22626471084160 run.py:483] Algo bellman_ford step 4776 current loss 0.493774, current_train_items 152864.
I0302 19:00:44.552243 22626471084160 run.py:483] Algo bellman_ford step 4777 current loss 0.628391, current_train_items 152896.
I0302 19:00:44.583760 22626471084160 run.py:483] Algo bellman_ford step 4778 current loss 0.778455, current_train_items 152928.
I0302 19:00:44.616126 22626471084160 run.py:483] Algo bellman_ford step 4779 current loss 0.702346, current_train_items 152960.
I0302 19:00:44.635706 22626471084160 run.py:483] Algo bellman_ford step 4780 current loss 0.299213, current_train_items 152992.
I0302 19:00:44.651913 22626471084160 run.py:483] Algo bellman_ford step 4781 current loss 0.580479, current_train_items 153024.
I0302 19:00:44.675877 22626471084160 run.py:483] Algo bellman_ford step 4782 current loss 0.714548, current_train_items 153056.
I0302 19:00:44.707514 22626471084160 run.py:483] Algo bellman_ford step 4783 current loss 0.833800, current_train_items 153088.
I0302 19:00:44.741945 22626471084160 run.py:483] Algo bellman_ford step 4784 current loss 0.839593, current_train_items 153120.
I0302 19:00:44.762017 22626471084160 run.py:483] Algo bellman_ford step 4785 current loss 0.487697, current_train_items 153152.
I0302 19:00:44.778590 22626471084160 run.py:483] Algo bellman_ford step 4786 current loss 0.539872, current_train_items 153184.
I0302 19:00:44.802346 22626471084160 run.py:483] Algo bellman_ford step 4787 current loss 0.670784, current_train_items 153216.
I0302 19:00:44.832735 22626471084160 run.py:483] Algo bellman_ford step 4788 current loss 0.778474, current_train_items 153248.
I0302 19:00:44.864934 22626471084160 run.py:483] Algo bellman_ford step 4789 current loss 0.756765, current_train_items 153280.
I0302 19:00:44.884741 22626471084160 run.py:483] Algo bellman_ford step 4790 current loss 0.369313, current_train_items 153312.
I0302 19:00:44.900913 22626471084160 run.py:483] Algo bellman_ford step 4791 current loss 0.528755, current_train_items 153344.
I0302 19:00:44.924261 22626471084160 run.py:483] Algo bellman_ford step 4792 current loss 0.645531, current_train_items 153376.
I0302 19:00:44.955054 22626471084160 run.py:483] Algo bellman_ford step 4793 current loss 0.854415, current_train_items 153408.
I0302 19:00:44.991139 22626471084160 run.py:483] Algo bellman_ford step 4794 current loss 0.909542, current_train_items 153440.
I0302 19:00:45.010827 22626471084160 run.py:483] Algo bellman_ford step 4795 current loss 0.227685, current_train_items 153472.
I0302 19:00:45.027036 22626471084160 run.py:483] Algo bellman_ford step 4796 current loss 0.463235, current_train_items 153504.
I0302 19:00:45.051147 22626471084160 run.py:483] Algo bellman_ford step 4797 current loss 0.680221, current_train_items 153536.
I0302 19:00:45.082100 22626471084160 run.py:483] Algo bellman_ford step 4798 current loss 0.721323, current_train_items 153568.
I0302 19:00:45.114988 22626471084160 run.py:483] Algo bellman_ford step 4799 current loss 0.820108, current_train_items 153600.
I0302 19:00:45.134914 22626471084160 run.py:483] Algo bellman_ford step 4800 current loss 0.283394, current_train_items 153632.
I0302 19:00:45.142725 22626471084160 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0302 19:00:45.142831 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:00:45.159647 22626471084160 run.py:483] Algo bellman_ford step 4801 current loss 0.580934, current_train_items 153664.
I0302 19:00:45.183933 22626471084160 run.py:483] Algo bellman_ford step 4802 current loss 0.718070, current_train_items 153696.
I0302 19:00:45.214621 22626471084160 run.py:483] Algo bellman_ford step 4803 current loss 0.649067, current_train_items 153728.
I0302 19:00:45.251317 22626471084160 run.py:483] Algo bellman_ford step 4804 current loss 0.850010, current_train_items 153760.
I0302 19:00:45.271515 22626471084160 run.py:483] Algo bellman_ford step 4805 current loss 0.354255, current_train_items 153792.
I0302 19:00:45.288072 22626471084160 run.py:483] Algo bellman_ford step 4806 current loss 0.494356, current_train_items 153824.
I0302 19:00:45.311803 22626471084160 run.py:483] Algo bellman_ford step 4807 current loss 0.632728, current_train_items 153856.
I0302 19:00:45.343272 22626471084160 run.py:483] Algo bellman_ford step 4808 current loss 0.765631, current_train_items 153888.
I0302 19:00:45.376814 22626471084160 run.py:483] Algo bellman_ford step 4809 current loss 0.807154, current_train_items 153920.
I0302 19:00:45.396814 22626471084160 run.py:483] Algo bellman_ford step 4810 current loss 0.313751, current_train_items 153952.
I0302 19:00:45.413216 22626471084160 run.py:483] Algo bellman_ford step 4811 current loss 0.519393, current_train_items 153984.
I0302 19:00:45.436488 22626471084160 run.py:483] Algo bellman_ford step 4812 current loss 0.623543, current_train_items 154016.
I0302 19:00:45.466880 22626471084160 run.py:483] Algo bellman_ford step 4813 current loss 0.700936, current_train_items 154048.
I0302 19:00:45.501759 22626471084160 run.py:483] Algo bellman_ford step 4814 current loss 0.854693, current_train_items 154080.
I0302 19:00:45.521353 22626471084160 run.py:483] Algo bellman_ford step 4815 current loss 0.330777, current_train_items 154112.
I0302 19:00:45.537531 22626471084160 run.py:483] Algo bellman_ford step 4816 current loss 0.483384, current_train_items 154144.
I0302 19:00:45.562306 22626471084160 run.py:483] Algo bellman_ford step 4817 current loss 0.724080, current_train_items 154176.
I0302 19:00:45.593597 22626471084160 run.py:483] Algo bellman_ford step 4818 current loss 0.700661, current_train_items 154208.
I0302 19:00:45.623428 22626471084160 run.py:483] Algo bellman_ford step 4819 current loss 0.651190, current_train_items 154240.
I0302 19:00:45.643374 22626471084160 run.py:483] Algo bellman_ford step 4820 current loss 0.312401, current_train_items 154272.
I0302 19:00:45.659407 22626471084160 run.py:483] Algo bellman_ford step 4821 current loss 0.566991, current_train_items 154304.
I0302 19:00:45.683532 22626471084160 run.py:483] Algo bellman_ford step 4822 current loss 0.604247, current_train_items 154336.
I0302 19:00:45.714346 22626471084160 run.py:483] Algo bellman_ford step 4823 current loss 0.752180, current_train_items 154368.
I0302 19:00:45.748590 22626471084160 run.py:483] Algo bellman_ford step 4824 current loss 0.815693, current_train_items 154400.
I0302 19:00:45.768556 22626471084160 run.py:483] Algo bellman_ford step 4825 current loss 0.301520, current_train_items 154432.
I0302 19:00:45.785383 22626471084160 run.py:483] Algo bellman_ford step 4826 current loss 0.410223, current_train_items 154464.
I0302 19:00:45.808684 22626471084160 run.py:483] Algo bellman_ford step 4827 current loss 0.581350, current_train_items 154496.
I0302 19:00:45.840085 22626471084160 run.py:483] Algo bellman_ford step 4828 current loss 0.723109, current_train_items 154528.
I0302 19:00:45.874342 22626471084160 run.py:483] Algo bellman_ford step 4829 current loss 0.933719, current_train_items 154560.
I0302 19:00:45.893936 22626471084160 run.py:483] Algo bellman_ford step 4830 current loss 0.267933, current_train_items 154592.
I0302 19:00:45.910317 22626471084160 run.py:483] Algo bellman_ford step 4831 current loss 0.543221, current_train_items 154624.
I0302 19:00:45.934402 22626471084160 run.py:483] Algo bellman_ford step 4832 current loss 0.671572, current_train_items 154656.
I0302 19:00:45.966082 22626471084160 run.py:483] Algo bellman_ford step 4833 current loss 0.835787, current_train_items 154688.
I0302 19:00:45.999603 22626471084160 run.py:483] Algo bellman_ford step 4834 current loss 0.813656, current_train_items 154720.
I0302 19:00:46.018840 22626471084160 run.py:483] Algo bellman_ford step 4835 current loss 0.205527, current_train_items 154752.
I0302 19:00:46.034973 22626471084160 run.py:483] Algo bellman_ford step 4836 current loss 0.478220, current_train_items 154784.
I0302 19:00:46.057812 22626471084160 run.py:483] Algo bellman_ford step 4837 current loss 0.698631, current_train_items 154816.
I0302 19:00:46.088641 22626471084160 run.py:483] Algo bellman_ford step 4838 current loss 0.773665, current_train_items 154848.
I0302 19:00:46.120230 22626471084160 run.py:483] Algo bellman_ford step 4839 current loss 0.832472, current_train_items 154880.
I0302 19:00:46.140064 22626471084160 run.py:483] Algo bellman_ford step 4840 current loss 0.333980, current_train_items 154912.
I0302 19:00:46.156034 22626471084160 run.py:483] Algo bellman_ford step 4841 current loss 0.410838, current_train_items 154944.
I0302 19:00:46.179399 22626471084160 run.py:483] Algo bellman_ford step 4842 current loss 0.630954, current_train_items 154976.
I0302 19:00:46.209789 22626471084160 run.py:483] Algo bellman_ford step 4843 current loss 0.732289, current_train_items 155008.
I0302 19:00:46.244767 22626471084160 run.py:483] Algo bellman_ford step 4844 current loss 1.116081, current_train_items 155040.
I0302 19:00:46.264538 22626471084160 run.py:483] Algo bellman_ford step 4845 current loss 0.376509, current_train_items 155072.
I0302 19:00:46.280963 22626471084160 run.py:483] Algo bellman_ford step 4846 current loss 0.563806, current_train_items 155104.
I0302 19:00:46.303741 22626471084160 run.py:483] Algo bellman_ford step 4847 current loss 0.571990, current_train_items 155136.
I0302 19:00:46.334241 22626471084160 run.py:483] Algo bellman_ford step 4848 current loss 0.754977, current_train_items 155168.
I0302 19:00:46.367764 22626471084160 run.py:483] Algo bellman_ford step 4849 current loss 0.760467, current_train_items 155200.
I0302 19:00:46.387596 22626471084160 run.py:483] Algo bellman_ford step 4850 current loss 0.344568, current_train_items 155232.
I0302 19:00:46.395657 22626471084160 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0302 19:00:46.395763 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:00:46.413086 22626471084160 run.py:483] Algo bellman_ford step 4851 current loss 0.524484, current_train_items 155264.
I0302 19:00:46.435770 22626471084160 run.py:483] Algo bellman_ford step 4852 current loss 0.684985, current_train_items 155296.
I0302 19:00:46.466615 22626471084160 run.py:483] Algo bellman_ford step 4853 current loss 0.700205, current_train_items 155328.
I0302 19:00:46.500463 22626471084160 run.py:483] Algo bellman_ford step 4854 current loss 0.773653, current_train_items 155360.
I0302 19:00:46.520416 22626471084160 run.py:483] Algo bellman_ford step 4855 current loss 0.344740, current_train_items 155392.
I0302 19:00:46.537091 22626471084160 run.py:483] Algo bellman_ford step 4856 current loss 0.576823, current_train_items 155424.
I0302 19:00:46.561854 22626471084160 run.py:483] Algo bellman_ford step 4857 current loss 0.693436, current_train_items 155456.
I0302 19:00:46.593462 22626471084160 run.py:483] Algo bellman_ford step 4858 current loss 0.683577, current_train_items 155488.
I0302 19:00:46.629274 22626471084160 run.py:483] Algo bellman_ford step 4859 current loss 0.832094, current_train_items 155520.
I0302 19:00:46.649292 22626471084160 run.py:483] Algo bellman_ford step 4860 current loss 0.292378, current_train_items 155552.
I0302 19:00:46.666597 22626471084160 run.py:483] Algo bellman_ford step 4861 current loss 0.538035, current_train_items 155584.
I0302 19:00:46.689936 22626471084160 run.py:483] Algo bellman_ford step 4862 current loss 0.641617, current_train_items 155616.
I0302 19:00:46.721431 22626471084160 run.py:483] Algo bellman_ford step 4863 current loss 0.648263, current_train_items 155648.
I0302 19:00:46.754338 22626471084160 run.py:483] Algo bellman_ford step 4864 current loss 0.721449, current_train_items 155680.
I0302 19:00:46.773900 22626471084160 run.py:483] Algo bellman_ford step 4865 current loss 0.288711, current_train_items 155712.
I0302 19:00:46.789822 22626471084160 run.py:483] Algo bellman_ford step 4866 current loss 0.490523, current_train_items 155744.
I0302 19:00:46.813979 22626471084160 run.py:483] Algo bellman_ford step 4867 current loss 0.750583, current_train_items 155776.
I0302 19:00:46.846285 22626471084160 run.py:483] Algo bellman_ford step 4868 current loss 0.755162, current_train_items 155808.
I0302 19:00:46.878621 22626471084160 run.py:483] Algo bellman_ford step 4869 current loss 0.740551, current_train_items 155840.
I0302 19:00:46.899078 22626471084160 run.py:483] Algo bellman_ford step 4870 current loss 0.311144, current_train_items 155872.
I0302 19:00:46.915405 22626471084160 run.py:483] Algo bellman_ford step 4871 current loss 0.545008, current_train_items 155904.
I0302 19:00:46.938614 22626471084160 run.py:483] Algo bellman_ford step 4872 current loss 0.619800, current_train_items 155936.
I0302 19:00:46.967372 22626471084160 run.py:483] Algo bellman_ford step 4873 current loss 0.579500, current_train_items 155968.
I0302 19:00:46.999759 22626471084160 run.py:483] Algo bellman_ford step 4874 current loss 0.816969, current_train_items 156000.
I0302 19:00:47.019610 22626471084160 run.py:483] Algo bellman_ford step 4875 current loss 0.241688, current_train_items 156032.
I0302 19:00:47.035624 22626471084160 run.py:483] Algo bellman_ford step 4876 current loss 0.516352, current_train_items 156064.
I0302 19:00:47.058205 22626471084160 run.py:483] Algo bellman_ford step 4877 current loss 0.549527, current_train_items 156096.
I0302 19:00:47.088249 22626471084160 run.py:483] Algo bellman_ford step 4878 current loss 0.692960, current_train_items 156128.
I0302 19:00:47.122272 22626471084160 run.py:483] Algo bellman_ford step 4879 current loss 0.719022, current_train_items 156160.
I0302 19:00:47.142289 22626471084160 run.py:483] Algo bellman_ford step 4880 current loss 0.300071, current_train_items 156192.
I0302 19:00:47.158420 22626471084160 run.py:483] Algo bellman_ford step 4881 current loss 0.418824, current_train_items 156224.
I0302 19:00:47.182615 22626471084160 run.py:483] Algo bellman_ford step 4882 current loss 0.604036, current_train_items 156256.
I0302 19:00:47.214093 22626471084160 run.py:483] Algo bellman_ford step 4883 current loss 0.742155, current_train_items 156288.
I0302 19:00:47.248461 22626471084160 run.py:483] Algo bellman_ford step 4884 current loss 0.807268, current_train_items 156320.
I0302 19:00:47.268525 22626471084160 run.py:483] Algo bellman_ford step 4885 current loss 0.328367, current_train_items 156352.
I0302 19:00:47.285093 22626471084160 run.py:483] Algo bellman_ford step 4886 current loss 0.532231, current_train_items 156384.
I0302 19:00:47.308348 22626471084160 run.py:483] Algo bellman_ford step 4887 current loss 0.646727, current_train_items 156416.
I0302 19:00:47.338614 22626471084160 run.py:483] Algo bellman_ford step 4888 current loss 0.612499, current_train_items 156448.
I0302 19:00:47.371878 22626471084160 run.py:483] Algo bellman_ford step 4889 current loss 0.763642, current_train_items 156480.
I0302 19:00:47.392037 22626471084160 run.py:483] Algo bellman_ford step 4890 current loss 0.292219, current_train_items 156512.
I0302 19:00:47.408249 22626471084160 run.py:483] Algo bellman_ford step 4891 current loss 0.460061, current_train_items 156544.
I0302 19:00:47.431343 22626471084160 run.py:483] Algo bellman_ford step 4892 current loss 0.664898, current_train_items 156576.
I0302 19:00:47.461056 22626471084160 run.py:483] Algo bellman_ford step 4893 current loss 0.681015, current_train_items 156608.
I0302 19:00:47.493715 22626471084160 run.py:483] Algo bellman_ford step 4894 current loss 0.803741, current_train_items 156640.
I0302 19:00:47.513176 22626471084160 run.py:483] Algo bellman_ford step 4895 current loss 0.265332, current_train_items 156672.
I0302 19:00:47.529503 22626471084160 run.py:483] Algo bellman_ford step 4896 current loss 0.453805, current_train_items 156704.
I0302 19:00:47.552063 22626471084160 run.py:483] Algo bellman_ford step 4897 current loss 0.606660, current_train_items 156736.
I0302 19:00:47.582280 22626471084160 run.py:483] Algo bellman_ford step 4898 current loss 0.805595, current_train_items 156768.
I0302 19:00:47.617030 22626471084160 run.py:483] Algo bellman_ford step 4899 current loss 0.953577, current_train_items 156800.
I0302 19:00:47.637259 22626471084160 run.py:483] Algo bellman_ford step 4900 current loss 0.359742, current_train_items 156832.
I0302 19:00:47.645148 22626471084160 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0302 19:00:47.645286 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:00:47.661774 22626471084160 run.py:483] Algo bellman_ford step 4901 current loss 0.453747, current_train_items 156864.
I0302 19:00:47.686215 22626471084160 run.py:483] Algo bellman_ford step 4902 current loss 0.626611, current_train_items 156896.
I0302 19:00:47.719469 22626471084160 run.py:483] Algo bellman_ford step 4903 current loss 0.785185, current_train_items 156928.
I0302 19:00:47.753048 22626471084160 run.py:483] Algo bellman_ford step 4904 current loss 0.773796, current_train_items 156960.
I0302 19:00:47.773288 22626471084160 run.py:483] Algo bellman_ford step 4905 current loss 0.414996, current_train_items 156992.
I0302 19:00:47.789217 22626471084160 run.py:483] Algo bellman_ford step 4906 current loss 0.485594, current_train_items 157024.
I0302 19:00:47.813467 22626471084160 run.py:483] Algo bellman_ford step 4907 current loss 0.652849, current_train_items 157056.
I0302 19:00:47.844722 22626471084160 run.py:483] Algo bellman_ford step 4908 current loss 0.773446, current_train_items 157088.
I0302 19:00:47.878019 22626471084160 run.py:483] Algo bellman_ford step 4909 current loss 0.779153, current_train_items 157120.
I0302 19:00:47.897728 22626471084160 run.py:483] Algo bellman_ford step 4910 current loss 0.324260, current_train_items 157152.
I0302 19:00:47.914195 22626471084160 run.py:483] Algo bellman_ford step 4911 current loss 0.515838, current_train_items 157184.
I0302 19:00:47.937545 22626471084160 run.py:483] Algo bellman_ford step 4912 current loss 0.571323, current_train_items 157216.
I0302 19:00:47.969329 22626471084160 run.py:483] Algo bellman_ford step 4913 current loss 0.762759, current_train_items 157248.
I0302 19:00:48.002505 22626471084160 run.py:483] Algo bellman_ford step 4914 current loss 0.823890, current_train_items 157280.
I0302 19:00:48.021993 22626471084160 run.py:483] Algo bellman_ford step 4915 current loss 0.290270, current_train_items 157312.
I0302 19:00:48.037872 22626471084160 run.py:483] Algo bellman_ford step 4916 current loss 0.494091, current_train_items 157344.
I0302 19:00:48.061635 22626471084160 run.py:483] Algo bellman_ford step 4917 current loss 0.608757, current_train_items 157376.
I0302 19:00:48.091365 22626471084160 run.py:483] Algo bellman_ford step 4918 current loss 0.692231, current_train_items 157408.
I0302 19:00:48.125091 22626471084160 run.py:483] Algo bellman_ford step 4919 current loss 0.831725, current_train_items 157440.
I0302 19:00:48.144931 22626471084160 run.py:483] Algo bellman_ford step 4920 current loss 0.339384, current_train_items 157472.
I0302 19:00:48.160509 22626471084160 run.py:483] Algo bellman_ford step 4921 current loss 0.415888, current_train_items 157504.
I0302 19:00:48.184345 22626471084160 run.py:483] Algo bellman_ford step 4922 current loss 0.623195, current_train_items 157536.
I0302 19:00:48.215288 22626471084160 run.py:483] Algo bellman_ford step 4923 current loss 0.681826, current_train_items 157568.
I0302 19:00:48.248901 22626471084160 run.py:483] Algo bellman_ford step 4924 current loss 0.945747, current_train_items 157600.
I0302 19:00:48.268538 22626471084160 run.py:483] Algo bellman_ford step 4925 current loss 0.263066, current_train_items 157632.
I0302 19:00:48.284837 22626471084160 run.py:483] Algo bellman_ford step 4926 current loss 0.567362, current_train_items 157664.
I0302 19:00:48.309144 22626471084160 run.py:483] Algo bellman_ford step 4927 current loss 0.652736, current_train_items 157696.
I0302 19:00:48.338846 22626471084160 run.py:483] Algo bellman_ford step 4928 current loss 0.647397, current_train_items 157728.
I0302 19:00:48.372537 22626471084160 run.py:483] Algo bellman_ford step 4929 current loss 0.865545, current_train_items 157760.
I0302 19:00:48.392331 22626471084160 run.py:483] Algo bellman_ford step 4930 current loss 0.276499, current_train_items 157792.
I0302 19:00:48.408293 22626471084160 run.py:483] Algo bellman_ford step 4931 current loss 0.546507, current_train_items 157824.
I0302 19:00:48.431602 22626471084160 run.py:483] Algo bellman_ford step 4932 current loss 0.745364, current_train_items 157856.
I0302 19:00:48.462089 22626471084160 run.py:483] Algo bellman_ford step 4933 current loss 0.707211, current_train_items 157888.
I0302 19:00:48.495522 22626471084160 run.py:483] Algo bellman_ford step 4934 current loss 0.935635, current_train_items 157920.
I0302 19:00:48.514981 22626471084160 run.py:483] Algo bellman_ford step 4935 current loss 0.355573, current_train_items 157952.
I0302 19:00:48.531132 22626471084160 run.py:483] Algo bellman_ford step 4936 current loss 0.520935, current_train_items 157984.
I0302 19:00:48.556066 22626471084160 run.py:483] Algo bellman_ford step 4937 current loss 0.745320, current_train_items 158016.
I0302 19:00:48.586804 22626471084160 run.py:483] Algo bellman_ford step 4938 current loss 0.724845, current_train_items 158048.
I0302 19:00:48.622097 22626471084160 run.py:483] Algo bellman_ford step 4939 current loss 0.838897, current_train_items 158080.
I0302 19:00:48.641941 22626471084160 run.py:483] Algo bellman_ford step 4940 current loss 0.307535, current_train_items 158112.
I0302 19:00:48.657747 22626471084160 run.py:483] Algo bellman_ford step 4941 current loss 0.459637, current_train_items 158144.
I0302 19:00:48.681033 22626471084160 run.py:483] Algo bellman_ford step 4942 current loss 0.586067, current_train_items 158176.
I0302 19:00:48.713215 22626471084160 run.py:483] Algo bellman_ford step 4943 current loss 0.721130, current_train_items 158208.
I0302 19:00:48.745390 22626471084160 run.py:483] Algo bellman_ford step 4944 current loss 0.800238, current_train_items 158240.
I0302 19:00:48.765018 22626471084160 run.py:483] Algo bellman_ford step 4945 current loss 0.385208, current_train_items 158272.
I0302 19:00:48.781211 22626471084160 run.py:483] Algo bellman_ford step 4946 current loss 0.553954, current_train_items 158304.
I0302 19:00:48.804615 22626471084160 run.py:483] Algo bellman_ford step 4947 current loss 0.611419, current_train_items 158336.
I0302 19:00:48.835528 22626471084160 run.py:483] Algo bellman_ford step 4948 current loss 0.887839, current_train_items 158368.
I0302 19:00:48.870844 22626471084160 run.py:483] Algo bellman_ford step 4949 current loss 0.800638, current_train_items 158400.
I0302 19:00:48.890747 22626471084160 run.py:483] Algo bellman_ford step 4950 current loss 0.292052, current_train_items 158432.
I0302 19:00:48.899031 22626471084160 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0302 19:00:48.899137 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:00:48.915579 22626471084160 run.py:483] Algo bellman_ford step 4951 current loss 0.446530, current_train_items 158464.
I0302 19:00:48.938986 22626471084160 run.py:483] Algo bellman_ford step 4952 current loss 0.649007, current_train_items 158496.
I0302 19:00:48.970078 22626471084160 run.py:483] Algo bellman_ford step 4953 current loss 0.617406, current_train_items 158528.
I0302 19:00:49.005934 22626471084160 run.py:483] Algo bellman_ford step 4954 current loss 0.713235, current_train_items 158560.
I0302 19:00:49.026251 22626471084160 run.py:483] Algo bellman_ford step 4955 current loss 0.321442, current_train_items 158592.
I0302 19:00:49.042304 22626471084160 run.py:483] Algo bellman_ford step 4956 current loss 0.471658, current_train_items 158624.
I0302 19:00:49.067046 22626471084160 run.py:483] Algo bellman_ford step 4957 current loss 0.695860, current_train_items 158656.
I0302 19:00:49.098615 22626471084160 run.py:483] Algo bellman_ford step 4958 current loss 0.766838, current_train_items 158688.
I0302 19:00:49.130144 22626471084160 run.py:483] Algo bellman_ford step 4959 current loss 0.888738, current_train_items 158720.
I0302 19:00:49.150185 22626471084160 run.py:483] Algo bellman_ford step 4960 current loss 0.279480, current_train_items 158752.
I0302 19:00:49.166518 22626471084160 run.py:483] Algo bellman_ford step 4961 current loss 0.386852, current_train_items 158784.
I0302 19:00:49.189866 22626471084160 run.py:483] Algo bellman_ford step 4962 current loss 0.607957, current_train_items 158816.
I0302 19:00:49.219604 22626471084160 run.py:483] Algo bellman_ford step 4963 current loss 0.630736, current_train_items 158848.
I0302 19:00:49.253101 22626471084160 run.py:483] Algo bellman_ford step 4964 current loss 0.806019, current_train_items 158880.
I0302 19:00:49.272574 22626471084160 run.py:483] Algo bellman_ford step 4965 current loss 0.284997, current_train_items 158912.
I0302 19:00:49.288826 22626471084160 run.py:483] Algo bellman_ford step 4966 current loss 0.459074, current_train_items 158944.
I0302 19:00:49.313220 22626471084160 run.py:483] Algo bellman_ford step 4967 current loss 0.774227, current_train_items 158976.
I0302 19:00:49.345323 22626471084160 run.py:483] Algo bellman_ford step 4968 current loss 0.747543, current_train_items 159008.
I0302 19:00:49.377187 22626471084160 run.py:483] Algo bellman_ford step 4969 current loss 0.775741, current_train_items 159040.
I0302 19:00:49.397125 22626471084160 run.py:483] Algo bellman_ford step 4970 current loss 0.314254, current_train_items 159072.
I0302 19:00:49.413308 22626471084160 run.py:483] Algo bellman_ford step 4971 current loss 0.419338, current_train_items 159104.
I0302 19:00:49.437016 22626471084160 run.py:483] Algo bellman_ford step 4972 current loss 0.610211, current_train_items 159136.
I0302 19:00:49.468690 22626471084160 run.py:483] Algo bellman_ford step 4973 current loss 0.699902, current_train_items 159168.
I0302 19:00:49.502288 22626471084160 run.py:483] Algo bellman_ford step 4974 current loss 0.778746, current_train_items 159200.
I0302 19:00:49.522190 22626471084160 run.py:483] Algo bellman_ford step 4975 current loss 0.452538, current_train_items 159232.
I0302 19:00:49.538795 22626471084160 run.py:483] Algo bellman_ford step 4976 current loss 0.473498, current_train_items 159264.
I0302 19:00:49.562877 22626471084160 run.py:483] Algo bellman_ford step 4977 current loss 0.705681, current_train_items 159296.
I0302 19:00:49.594880 22626471084160 run.py:483] Algo bellman_ford step 4978 current loss 0.736067, current_train_items 159328.
I0302 19:00:49.629807 22626471084160 run.py:483] Algo bellman_ford step 4979 current loss 0.848467, current_train_items 159360.
I0302 19:00:49.649490 22626471084160 run.py:483] Algo bellman_ford step 4980 current loss 0.288673, current_train_items 159392.
I0302 19:00:49.665784 22626471084160 run.py:483] Algo bellman_ford step 4981 current loss 0.501317, current_train_items 159424.
I0302 19:00:49.690675 22626471084160 run.py:483] Algo bellman_ford step 4982 current loss 0.621808, current_train_items 159456.
I0302 19:00:49.719745 22626471084160 run.py:483] Algo bellman_ford step 4983 current loss 0.703649, current_train_items 159488.
I0302 19:00:49.753025 22626471084160 run.py:483] Algo bellman_ford step 4984 current loss 0.765033, current_train_items 159520.
I0302 19:00:49.773024 22626471084160 run.py:483] Algo bellman_ford step 4985 current loss 0.340554, current_train_items 159552.
I0302 19:00:49.789407 22626471084160 run.py:483] Algo bellman_ford step 4986 current loss 0.450833, current_train_items 159584.
I0302 19:00:49.811582 22626471084160 run.py:483] Algo bellman_ford step 4987 current loss 0.604225, current_train_items 159616.
I0302 19:00:49.841594 22626471084160 run.py:483] Algo bellman_ford step 4988 current loss 0.654554, current_train_items 159648.
I0302 19:00:49.877859 22626471084160 run.py:483] Algo bellman_ford step 4989 current loss 0.914335, current_train_items 159680.
I0302 19:00:49.897606 22626471084160 run.py:483] Algo bellman_ford step 4990 current loss 0.329642, current_train_items 159712.
I0302 19:00:49.913558 22626471084160 run.py:483] Algo bellman_ford step 4991 current loss 0.474910, current_train_items 159744.
I0302 19:00:49.937514 22626471084160 run.py:483] Algo bellman_ford step 4992 current loss 0.614500, current_train_items 159776.
I0302 19:00:49.968419 22626471084160 run.py:483] Algo bellman_ford step 4993 current loss 0.645909, current_train_items 159808.
I0302 19:00:50.002407 22626471084160 run.py:483] Algo bellman_ford step 4994 current loss 0.839199, current_train_items 159840.
I0302 19:00:50.021849 22626471084160 run.py:483] Algo bellman_ford step 4995 current loss 0.260357, current_train_items 159872.
I0302 19:00:50.038378 22626471084160 run.py:483] Algo bellman_ford step 4996 current loss 0.457150, current_train_items 159904.
I0302 19:00:50.061745 22626471084160 run.py:483] Algo bellman_ford step 4997 current loss 0.745026, current_train_items 159936.
I0302 19:00:50.093176 22626471084160 run.py:483] Algo bellman_ford step 4998 current loss 0.806352, current_train_items 159968.
I0302 19:00:50.127896 22626471084160 run.py:483] Algo bellman_ford step 4999 current loss 0.908389, current_train_items 160000.
I0302 19:00:50.148115 22626471084160 run.py:483] Algo bellman_ford step 5000 current loss 0.373592, current_train_items 160032.
I0302 19:00:50.155726 22626471084160 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0302 19:00:50.155830 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 19:00:50.172757 22626471084160 run.py:483] Algo bellman_ford step 5001 current loss 0.511303, current_train_items 160064.
I0302 19:00:50.196277 22626471084160 run.py:483] Algo bellman_ford step 5002 current loss 0.684147, current_train_items 160096.
I0302 19:00:50.226258 22626471084160 run.py:483] Algo bellman_ford step 5003 current loss 0.588050, current_train_items 160128.
I0302 19:00:50.259251 22626471084160 run.py:483] Algo bellman_ford step 5004 current loss 0.773659, current_train_items 160160.
I0302 19:00:50.279218 22626471084160 run.py:483] Algo bellman_ford step 5005 current loss 0.321916, current_train_items 160192.
I0302 19:00:50.295230 22626471084160 run.py:483] Algo bellman_ford step 5006 current loss 0.524944, current_train_items 160224.
I0302 19:00:50.319267 22626471084160 run.py:483] Algo bellman_ford step 5007 current loss 0.780406, current_train_items 160256.
I0302 19:00:50.352365 22626471084160 run.py:483] Algo bellman_ford step 5008 current loss 0.803538, current_train_items 160288.
I0302 19:00:50.384025 22626471084160 run.py:483] Algo bellman_ford step 5009 current loss 0.862884, current_train_items 160320.
I0302 19:00:50.403183 22626471084160 run.py:483] Algo bellman_ford step 5010 current loss 0.307582, current_train_items 160352.
I0302 19:00:50.419055 22626471084160 run.py:483] Algo bellman_ford step 5011 current loss 0.473866, current_train_items 160384.
I0302 19:00:50.442497 22626471084160 run.py:483] Algo bellman_ford step 5012 current loss 0.577430, current_train_items 160416.
I0302 19:00:50.473057 22626471084160 run.py:483] Algo bellman_ford step 5013 current loss 0.763535, current_train_items 160448.
I0302 19:00:50.507821 22626471084160 run.py:483] Algo bellman_ford step 5014 current loss 0.740544, current_train_items 160480.
I0302 19:00:50.527336 22626471084160 run.py:483] Algo bellman_ford step 5015 current loss 0.379019, current_train_items 160512.
I0302 19:00:50.544069 22626471084160 run.py:483] Algo bellman_ford step 5016 current loss 0.594097, current_train_items 160544.
I0302 19:00:50.567092 22626471084160 run.py:483] Algo bellman_ford step 5017 current loss 0.729229, current_train_items 160576.
I0302 19:00:50.598403 22626471084160 run.py:483] Algo bellman_ford step 5018 current loss 0.673335, current_train_items 160608.
I0302 19:00:50.633492 22626471084160 run.py:483] Algo bellman_ford step 5019 current loss 0.853415, current_train_items 160640.
I0302 19:00:50.652796 22626471084160 run.py:483] Algo bellman_ford step 5020 current loss 0.267305, current_train_items 160672.
I0302 19:00:50.668838 22626471084160 run.py:483] Algo bellman_ford step 5021 current loss 0.441124, current_train_items 160704.
I0302 19:00:50.693234 22626471084160 run.py:483] Algo bellman_ford step 5022 current loss 0.801026, current_train_items 160736.
I0302 19:00:50.724122 22626471084160 run.py:483] Algo bellman_ford step 5023 current loss 0.825778, current_train_items 160768.
I0302 19:00:50.757666 22626471084160 run.py:483] Algo bellman_ford step 5024 current loss 0.855451, current_train_items 160800.
I0302 19:00:50.776848 22626471084160 run.py:483] Algo bellman_ford step 5025 current loss 0.308126, current_train_items 160832.
I0302 19:00:50.792517 22626471084160 run.py:483] Algo bellman_ford step 5026 current loss 0.454439, current_train_items 160864.
I0302 19:00:50.814956 22626471084160 run.py:483] Algo bellman_ford step 5027 current loss 0.609307, current_train_items 160896.
I0302 19:00:50.844057 22626471084160 run.py:483] Algo bellman_ford step 5028 current loss 0.616930, current_train_items 160928.
I0302 19:00:50.876854 22626471084160 run.py:483] Algo bellman_ford step 5029 current loss 0.806647, current_train_items 160960.
I0302 19:00:50.896464 22626471084160 run.py:483] Algo bellman_ford step 5030 current loss 0.262942, current_train_items 160992.
I0302 19:00:50.912687 22626471084160 run.py:483] Algo bellman_ford step 5031 current loss 0.504744, current_train_items 161024.
I0302 19:00:50.936575 22626471084160 run.py:483] Algo bellman_ford step 5032 current loss 0.607117, current_train_items 161056.
I0302 19:00:50.966094 22626471084160 run.py:483] Algo bellman_ford step 5033 current loss 0.768977, current_train_items 161088.
I0302 19:00:51.000144 22626471084160 run.py:483] Algo bellman_ford step 5034 current loss 0.915729, current_train_items 161120.
I0302 19:00:51.019662 22626471084160 run.py:483] Algo bellman_ford step 5035 current loss 0.280520, current_train_items 161152.
I0302 19:00:51.035898 22626471084160 run.py:483] Algo bellman_ford step 5036 current loss 0.464199, current_train_items 161184.
I0302 19:00:51.059197 22626471084160 run.py:483] Algo bellman_ford step 5037 current loss 0.644767, current_train_items 161216.
I0302 19:00:51.091126 22626471084160 run.py:483] Algo bellman_ford step 5038 current loss 0.816438, current_train_items 161248.
I0302 19:00:51.124428 22626471084160 run.py:483] Algo bellman_ford step 5039 current loss 0.882694, current_train_items 161280.
I0302 19:00:51.143736 22626471084160 run.py:483] Algo bellman_ford step 5040 current loss 0.306718, current_train_items 161312.
I0302 19:00:51.159345 22626471084160 run.py:483] Algo bellman_ford step 5041 current loss 0.448459, current_train_items 161344.
I0302 19:00:51.183540 22626471084160 run.py:483] Algo bellman_ford step 5042 current loss 0.705438, current_train_items 161376.
I0302 19:00:51.213072 22626471084160 run.py:483] Algo bellman_ford step 5043 current loss 0.688375, current_train_items 161408.
I0302 19:00:51.246294 22626471084160 run.py:483] Algo bellman_ford step 5044 current loss 0.805636, current_train_items 161440.
I0302 19:00:51.265658 22626471084160 run.py:483] Algo bellman_ford step 5045 current loss 0.270122, current_train_items 161472.
I0302 19:00:51.281820 22626471084160 run.py:483] Algo bellman_ford step 5046 current loss 0.547434, current_train_items 161504.
I0302 19:00:51.304237 22626471084160 run.py:483] Algo bellman_ford step 5047 current loss 0.601158, current_train_items 161536.
I0302 19:00:51.335176 22626471084160 run.py:483] Algo bellman_ford step 5048 current loss 0.831466, current_train_items 161568.
I0302 19:00:51.368950 22626471084160 run.py:483] Algo bellman_ford step 5049 current loss 0.789815, current_train_items 161600.
I0302 19:00:51.388163 22626471084160 run.py:483] Algo bellman_ford step 5050 current loss 0.356667, current_train_items 161632.
I0302 19:00:51.396164 22626471084160 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0302 19:00:51.396267 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:00:51.412858 22626471084160 run.py:483] Algo bellman_ford step 5051 current loss 0.487469, current_train_items 161664.
I0302 19:00:51.437899 22626471084160 run.py:483] Algo bellman_ford step 5052 current loss 0.739011, current_train_items 161696.
I0302 19:00:51.469279 22626471084160 run.py:483] Algo bellman_ford step 5053 current loss 0.755241, current_train_items 161728.
I0302 19:00:51.502261 22626471084160 run.py:483] Algo bellman_ford step 5054 current loss 0.799313, current_train_items 161760.
I0302 19:00:51.522819 22626471084160 run.py:483] Algo bellman_ford step 5055 current loss 0.375967, current_train_items 161792.
I0302 19:00:51.539795 22626471084160 run.py:483] Algo bellman_ford step 5056 current loss 0.550619, current_train_items 161824.
I0302 19:00:51.564786 22626471084160 run.py:483] Algo bellman_ford step 5057 current loss 0.678320, current_train_items 161856.
I0302 19:00:51.595465 22626471084160 run.py:483] Algo bellman_ford step 5058 current loss 0.821209, current_train_items 161888.
I0302 19:00:51.629640 22626471084160 run.py:483] Algo bellman_ford step 5059 current loss 0.833528, current_train_items 161920.
I0302 19:00:51.649496 22626471084160 run.py:483] Algo bellman_ford step 5060 current loss 0.294578, current_train_items 161952.
I0302 19:00:51.665676 22626471084160 run.py:483] Algo bellman_ford step 5061 current loss 0.468729, current_train_items 161984.
I0302 19:00:51.687503 22626471084160 run.py:483] Algo bellman_ford step 5062 current loss 0.645669, current_train_items 162016.
I0302 19:00:51.718100 22626471084160 run.py:483] Algo bellman_ford step 5063 current loss 0.757438, current_train_items 162048.
I0302 19:00:51.750020 22626471084160 run.py:483] Algo bellman_ford step 5064 current loss 0.776013, current_train_items 162080.
I0302 19:00:51.769806 22626471084160 run.py:483] Algo bellman_ford step 5065 current loss 0.303160, current_train_items 162112.
I0302 19:00:51.785936 22626471084160 run.py:483] Algo bellman_ford step 5066 current loss 0.502211, current_train_items 162144.
I0302 19:00:51.810763 22626471084160 run.py:483] Algo bellman_ford step 5067 current loss 0.622089, current_train_items 162176.
I0302 19:00:51.841369 22626471084160 run.py:483] Algo bellman_ford step 5068 current loss 0.641827, current_train_items 162208.
I0302 19:00:51.876173 22626471084160 run.py:483] Algo bellman_ford step 5069 current loss 0.890203, current_train_items 162240.
I0302 19:00:51.896386 22626471084160 run.py:483] Algo bellman_ford step 5070 current loss 0.302093, current_train_items 162272.
I0302 19:00:51.913068 22626471084160 run.py:483] Algo bellman_ford step 5071 current loss 0.607718, current_train_items 162304.
I0302 19:00:51.934961 22626471084160 run.py:483] Algo bellman_ford step 5072 current loss 0.667318, current_train_items 162336.
I0302 19:00:51.966598 22626471084160 run.py:483] Algo bellman_ford step 5073 current loss 0.722664, current_train_items 162368.
I0302 19:00:51.997900 22626471084160 run.py:483] Algo bellman_ford step 5074 current loss 0.712124, current_train_items 162400.
I0302 19:00:52.017850 22626471084160 run.py:483] Algo bellman_ford step 5075 current loss 0.317983, current_train_items 162432.
I0302 19:00:52.034028 22626471084160 run.py:483] Algo bellman_ford step 5076 current loss 0.485881, current_train_items 162464.
I0302 19:00:52.057196 22626471084160 run.py:483] Algo bellman_ford step 5077 current loss 0.532946, current_train_items 162496.
I0302 19:00:52.087852 22626471084160 run.py:483] Algo bellman_ford step 5078 current loss 0.768695, current_train_items 162528.
I0302 19:00:52.121070 22626471084160 run.py:483] Algo bellman_ford step 5079 current loss 0.819385, current_train_items 162560.
I0302 19:00:52.140656 22626471084160 run.py:483] Algo bellman_ford step 5080 current loss 0.287047, current_train_items 162592.
I0302 19:00:52.156606 22626471084160 run.py:483] Algo bellman_ford step 5081 current loss 0.409117, current_train_items 162624.
I0302 19:00:52.180194 22626471084160 run.py:483] Algo bellman_ford step 5082 current loss 0.601590, current_train_items 162656.
I0302 19:00:52.211500 22626471084160 run.py:483] Algo bellman_ford step 5083 current loss 0.643481, current_train_items 162688.
I0302 19:00:52.245132 22626471084160 run.py:483] Algo bellman_ford step 5084 current loss 0.776082, current_train_items 162720.
I0302 19:00:52.265389 22626471084160 run.py:483] Algo bellman_ford step 5085 current loss 0.304187, current_train_items 162752.
I0302 19:00:52.281492 22626471084160 run.py:483] Algo bellman_ford step 5086 current loss 0.497357, current_train_items 162784.
I0302 19:00:52.305115 22626471084160 run.py:483] Algo bellman_ford step 5087 current loss 0.659424, current_train_items 162816.
I0302 19:00:52.336536 22626471084160 run.py:483] Algo bellman_ford step 5088 current loss 0.601358, current_train_items 162848.
I0302 19:00:52.371096 22626471084160 run.py:483] Algo bellman_ford step 5089 current loss 0.912760, current_train_items 162880.
I0302 19:00:52.391090 22626471084160 run.py:483] Algo bellman_ford step 5090 current loss 0.269252, current_train_items 162912.
I0302 19:00:52.407528 22626471084160 run.py:483] Algo bellman_ford step 5091 current loss 0.539411, current_train_items 162944.
I0302 19:00:52.429602 22626471084160 run.py:483] Algo bellman_ford step 5092 current loss 0.643839, current_train_items 162976.
I0302 19:00:52.459715 22626471084160 run.py:483] Algo bellman_ford step 5093 current loss 0.679545, current_train_items 163008.
I0302 19:00:52.492467 22626471084160 run.py:483] Algo bellman_ford step 5094 current loss 0.875360, current_train_items 163040.
I0302 19:00:52.512177 22626471084160 run.py:483] Algo bellman_ford step 5095 current loss 0.285023, current_train_items 163072.
I0302 19:00:52.528439 22626471084160 run.py:483] Algo bellman_ford step 5096 current loss 0.546227, current_train_items 163104.
I0302 19:00:52.552869 22626471084160 run.py:483] Algo bellman_ford step 5097 current loss 0.688521, current_train_items 163136.
I0302 19:00:52.584822 22626471084160 run.py:483] Algo bellman_ford step 5098 current loss 0.787229, current_train_items 163168.
I0302 19:00:52.619987 22626471084160 run.py:483] Algo bellman_ford step 5099 current loss 0.830916, current_train_items 163200.
I0302 19:00:52.640082 22626471084160 run.py:483] Algo bellman_ford step 5100 current loss 0.236045, current_train_items 163232.
I0302 19:00:52.647902 22626471084160 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0302 19:00:52.648006 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:00:52.664520 22626471084160 run.py:483] Algo bellman_ford step 5101 current loss 0.549169, current_train_items 163264.
I0302 19:00:52.686856 22626471084160 run.py:483] Algo bellman_ford step 5102 current loss 0.486940, current_train_items 163296.
I0302 19:00:52.718500 22626471084160 run.py:483] Algo bellman_ford step 5103 current loss 0.650007, current_train_items 163328.
I0302 19:00:52.753492 22626471084160 run.py:483] Algo bellman_ford step 5104 current loss 0.824967, current_train_items 163360.
I0302 19:00:52.773823 22626471084160 run.py:483] Algo bellman_ford step 5105 current loss 0.262652, current_train_items 163392.
I0302 19:00:52.790237 22626471084160 run.py:483] Algo bellman_ford step 5106 current loss 0.548874, current_train_items 163424.
I0302 19:00:52.813833 22626471084160 run.py:483] Algo bellman_ford step 5107 current loss 0.617965, current_train_items 163456.
I0302 19:00:52.843328 22626471084160 run.py:483] Algo bellman_ford step 5108 current loss 0.677706, current_train_items 163488.
I0302 19:00:52.877761 22626471084160 run.py:483] Algo bellman_ford step 5109 current loss 0.891582, current_train_items 163520.
I0302 19:00:52.897436 22626471084160 run.py:483] Algo bellman_ford step 5110 current loss 0.317636, current_train_items 163552.
I0302 19:00:52.913638 22626471084160 run.py:483] Algo bellman_ford step 5111 current loss 0.501813, current_train_items 163584.
I0302 19:00:52.936651 22626471084160 run.py:483] Algo bellman_ford step 5112 current loss 0.553603, current_train_items 163616.
I0302 19:00:52.966648 22626471084160 run.py:483] Algo bellman_ford step 5113 current loss 0.650762, current_train_items 163648.
I0302 19:00:52.999705 22626471084160 run.py:483] Algo bellman_ford step 5114 current loss 0.773422, current_train_items 163680.
I0302 19:00:53.019415 22626471084160 run.py:483] Algo bellman_ford step 5115 current loss 0.310660, current_train_items 163712.
I0302 19:00:53.035753 22626471084160 run.py:483] Algo bellman_ford step 5116 current loss 0.479813, current_train_items 163744.
I0302 19:00:53.059797 22626471084160 run.py:483] Algo bellman_ford step 5117 current loss 0.635343, current_train_items 163776.
I0302 19:00:53.089755 22626471084160 run.py:483] Algo bellman_ford step 5118 current loss 0.684967, current_train_items 163808.
I0302 19:00:53.124032 22626471084160 run.py:483] Algo bellman_ford step 5119 current loss 0.754547, current_train_items 163840.
I0302 19:00:53.143782 22626471084160 run.py:483] Algo bellman_ford step 5120 current loss 0.304998, current_train_items 163872.
I0302 19:00:53.159598 22626471084160 run.py:483] Algo bellman_ford step 5121 current loss 0.479731, current_train_items 163904.
I0302 19:00:53.182955 22626471084160 run.py:483] Algo bellman_ford step 5122 current loss 0.627611, current_train_items 163936.
I0302 19:00:53.212859 22626471084160 run.py:483] Algo bellman_ford step 5123 current loss 0.651251, current_train_items 163968.
I0302 19:00:53.246453 22626471084160 run.py:483] Algo bellman_ford step 5124 current loss 0.821924, current_train_items 164000.
I0302 19:00:53.265835 22626471084160 run.py:483] Algo bellman_ford step 5125 current loss 0.380793, current_train_items 164032.
I0302 19:00:53.282012 22626471084160 run.py:483] Algo bellman_ford step 5126 current loss 0.471090, current_train_items 164064.
I0302 19:00:53.305602 22626471084160 run.py:483] Algo bellman_ford step 5127 current loss 0.733671, current_train_items 164096.
I0302 19:00:53.336292 22626471084160 run.py:483] Algo bellman_ford step 5128 current loss 0.729402, current_train_items 164128.
I0302 19:00:53.371371 22626471084160 run.py:483] Algo bellman_ford step 5129 current loss 0.905993, current_train_items 164160.
I0302 19:00:53.390897 22626471084160 run.py:483] Algo bellman_ford step 5130 current loss 0.356215, current_train_items 164192.
I0302 19:00:53.407242 22626471084160 run.py:483] Algo bellman_ford step 5131 current loss 0.543022, current_train_items 164224.
I0302 19:00:53.429825 22626471084160 run.py:483] Algo bellman_ford step 5132 current loss 0.700545, current_train_items 164256.
I0302 19:00:53.461209 22626471084160 run.py:483] Algo bellman_ford step 5133 current loss 0.764769, current_train_items 164288.
I0302 19:00:53.494379 22626471084160 run.py:483] Algo bellman_ford step 5134 current loss 0.832089, current_train_items 164320.
I0302 19:00:53.514354 22626471084160 run.py:483] Algo bellman_ford step 5135 current loss 0.341339, current_train_items 164352.
I0302 19:00:53.530454 22626471084160 run.py:483] Algo bellman_ford step 5136 current loss 0.457716, current_train_items 164384.
I0302 19:00:53.554275 22626471084160 run.py:483] Algo bellman_ford step 5137 current loss 0.722031, current_train_items 164416.
I0302 19:00:53.584776 22626471084160 run.py:483] Algo bellman_ford step 5138 current loss 0.721083, current_train_items 164448.
I0302 19:00:53.618918 22626471084160 run.py:483] Algo bellman_ford step 5139 current loss 0.948637, current_train_items 164480.
I0302 19:00:53.638562 22626471084160 run.py:483] Algo bellman_ford step 5140 current loss 0.328723, current_train_items 164512.
I0302 19:00:53.654994 22626471084160 run.py:483] Algo bellman_ford step 5141 current loss 0.500940, current_train_items 164544.
I0302 19:00:53.679112 22626471084160 run.py:483] Algo bellman_ford step 5142 current loss 0.713263, current_train_items 164576.
I0302 19:00:53.709869 22626471084160 run.py:483] Algo bellman_ford step 5143 current loss 0.761115, current_train_items 164608.
I0302 19:00:53.743620 22626471084160 run.py:483] Algo bellman_ford step 5144 current loss 0.807223, current_train_items 164640.
I0302 19:00:53.763114 22626471084160 run.py:483] Algo bellman_ford step 5145 current loss 0.277374, current_train_items 164672.
I0302 19:00:53.779145 22626471084160 run.py:483] Algo bellman_ford step 5146 current loss 0.565127, current_train_items 164704.
I0302 19:00:53.801365 22626471084160 run.py:483] Algo bellman_ford step 5147 current loss 0.790545, current_train_items 164736.
I0302 19:00:53.831333 22626471084160 run.py:483] Algo bellman_ford step 5148 current loss 0.825360, current_train_items 164768.
I0302 19:00:53.864187 22626471084160 run.py:483] Algo bellman_ford step 5149 current loss 0.908242, current_train_items 164800.
I0302 19:00:53.883843 22626471084160 run.py:483] Algo bellman_ford step 5150 current loss 0.258136, current_train_items 164832.
I0302 19:00:53.891762 22626471084160 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0302 19:00:53.891866 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:00:53.908576 22626471084160 run.py:483] Algo bellman_ford step 5151 current loss 0.529598, current_train_items 164864.
I0302 19:00:53.932401 22626471084160 run.py:483] Algo bellman_ford step 5152 current loss 0.669491, current_train_items 164896.
I0302 19:00:53.962317 22626471084160 run.py:483] Algo bellman_ford step 5153 current loss 0.826660, current_train_items 164928.
I0302 19:00:53.995254 22626471084160 run.py:483] Algo bellman_ford step 5154 current loss 0.940825, current_train_items 164960.
I0302 19:00:54.015319 22626471084160 run.py:483] Algo bellman_ford step 5155 current loss 0.398327, current_train_items 164992.
I0302 19:00:54.031360 22626471084160 run.py:483] Algo bellman_ford step 5156 current loss 0.485940, current_train_items 165024.
I0302 19:00:54.055889 22626471084160 run.py:483] Algo bellman_ford step 5157 current loss 0.641863, current_train_items 165056.
I0302 19:00:54.088249 22626471084160 run.py:483] Algo bellman_ford step 5158 current loss 0.908908, current_train_items 165088.
I0302 19:00:54.120866 22626471084160 run.py:483] Algo bellman_ford step 5159 current loss 0.749992, current_train_items 165120.
I0302 19:00:54.140771 22626471084160 run.py:483] Algo bellman_ford step 5160 current loss 0.261288, current_train_items 165152.
I0302 19:00:54.156848 22626471084160 run.py:483] Algo bellman_ford step 5161 current loss 0.456404, current_train_items 165184.
I0302 19:00:54.181364 22626471084160 run.py:483] Algo bellman_ford step 5162 current loss 0.755839, current_train_items 165216.
I0302 19:00:54.213275 22626471084160 run.py:483] Algo bellman_ford step 5163 current loss 0.754388, current_train_items 165248.
I0302 19:00:54.245476 22626471084160 run.py:483] Algo bellman_ford step 5164 current loss 0.751089, current_train_items 165280.
I0302 19:00:54.264817 22626471084160 run.py:483] Algo bellman_ford step 5165 current loss 0.277224, current_train_items 165312.
I0302 19:00:54.280926 22626471084160 run.py:483] Algo bellman_ford step 5166 current loss 0.427704, current_train_items 165344.
I0302 19:00:54.303566 22626471084160 run.py:483] Algo bellman_ford step 5167 current loss 0.618918, current_train_items 165376.
I0302 19:00:54.334639 22626471084160 run.py:483] Algo bellman_ford step 5168 current loss 0.708257, current_train_items 165408.
I0302 19:00:54.369291 22626471084160 run.py:483] Algo bellman_ford step 5169 current loss 0.795796, current_train_items 165440.
I0302 19:00:54.389282 22626471084160 run.py:483] Algo bellman_ford step 5170 current loss 0.348154, current_train_items 165472.
I0302 19:00:54.405617 22626471084160 run.py:483] Algo bellman_ford step 5171 current loss 0.462118, current_train_items 165504.
I0302 19:00:54.429229 22626471084160 run.py:483] Algo bellman_ford step 5172 current loss 0.660545, current_train_items 165536.
I0302 19:00:54.461139 22626471084160 run.py:483] Algo bellman_ford step 5173 current loss 0.746829, current_train_items 165568.
I0302 19:00:54.494269 22626471084160 run.py:483] Algo bellman_ford step 5174 current loss 0.744675, current_train_items 165600.
I0302 19:00:54.513706 22626471084160 run.py:483] Algo bellman_ford step 5175 current loss 0.263612, current_train_items 165632.
I0302 19:00:54.530032 22626471084160 run.py:483] Algo bellman_ford step 5176 current loss 0.437347, current_train_items 165664.
I0302 19:00:54.553518 22626471084160 run.py:483] Algo bellman_ford step 5177 current loss 0.566354, current_train_items 165696.
I0302 19:00:54.584214 22626471084160 run.py:483] Algo bellman_ford step 5178 current loss 0.674549, current_train_items 165728.
I0302 19:00:54.616538 22626471084160 run.py:483] Algo bellman_ford step 5179 current loss 0.892031, current_train_items 165760.
I0302 19:00:54.636198 22626471084160 run.py:483] Algo bellman_ford step 5180 current loss 0.314889, current_train_items 165792.
I0302 19:00:54.652317 22626471084160 run.py:483] Algo bellman_ford step 5181 current loss 0.445318, current_train_items 165824.
I0302 19:00:54.676067 22626471084160 run.py:483] Algo bellman_ford step 5182 current loss 0.636607, current_train_items 165856.
I0302 19:00:54.706853 22626471084160 run.py:483] Algo bellman_ford step 5183 current loss 0.713033, current_train_items 165888.
I0302 19:00:54.739188 22626471084160 run.py:483] Algo bellman_ford step 5184 current loss 0.807167, current_train_items 165920.
I0302 19:00:54.759132 22626471084160 run.py:483] Algo bellman_ford step 5185 current loss 0.396424, current_train_items 165952.
I0302 19:00:54.775750 22626471084160 run.py:483] Algo bellman_ford step 5186 current loss 0.544564, current_train_items 165984.
I0302 19:00:54.798009 22626471084160 run.py:483] Algo bellman_ford step 5187 current loss 0.535938, current_train_items 166016.
I0302 19:00:54.829684 22626471084160 run.py:483] Algo bellman_ford step 5188 current loss 0.690142, current_train_items 166048.
I0302 19:00:54.861908 22626471084160 run.py:483] Algo bellman_ford step 5189 current loss 0.805897, current_train_items 166080.
I0302 19:00:54.882096 22626471084160 run.py:483] Algo bellman_ford step 5190 current loss 0.299709, current_train_items 166112.
I0302 19:00:54.898181 22626471084160 run.py:483] Algo bellman_ford step 5191 current loss 0.483960, current_train_items 166144.
I0302 19:00:54.922171 22626471084160 run.py:483] Algo bellman_ford step 5192 current loss 0.635209, current_train_items 166176.
I0302 19:00:54.952344 22626471084160 run.py:483] Algo bellman_ford step 5193 current loss 0.683642, current_train_items 166208.
I0302 19:00:54.985934 22626471084160 run.py:483] Algo bellman_ford step 5194 current loss 0.904111, current_train_items 166240.
I0302 19:00:55.005370 22626471084160 run.py:483] Algo bellman_ford step 5195 current loss 0.282439, current_train_items 166272.
I0302 19:00:55.021641 22626471084160 run.py:483] Algo bellman_ford step 5196 current loss 0.490103, current_train_items 166304.
I0302 19:00:55.045484 22626471084160 run.py:483] Algo bellman_ford step 5197 current loss 0.685675, current_train_items 166336.
I0302 19:00:55.077617 22626471084160 run.py:483] Algo bellman_ford step 5198 current loss 0.749219, current_train_items 166368.
I0302 19:00:55.110513 22626471084160 run.py:483] Algo bellman_ford step 5199 current loss 0.782409, current_train_items 166400.
I0302 19:00:55.130584 22626471084160 run.py:483] Algo bellman_ford step 5200 current loss 0.319980, current_train_items 166432.
I0302 19:00:55.138308 22626471084160 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0302 19:00:55.138412 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:00:55.155232 22626471084160 run.py:483] Algo bellman_ford step 5201 current loss 0.476321, current_train_items 166464.
I0302 19:00:55.179840 22626471084160 run.py:483] Algo bellman_ford step 5202 current loss 0.666835, current_train_items 166496.
I0302 19:00:55.212013 22626471084160 run.py:483] Algo bellman_ford step 5203 current loss 0.733968, current_train_items 166528.
I0302 19:00:55.247255 22626471084160 run.py:483] Algo bellman_ford step 5204 current loss 0.723836, current_train_items 166560.
I0302 19:00:55.267506 22626471084160 run.py:483] Algo bellman_ford step 5205 current loss 0.291388, current_train_items 166592.
I0302 19:00:55.283315 22626471084160 run.py:483] Algo bellman_ford step 5206 current loss 0.502216, current_train_items 166624.
I0302 19:00:55.306827 22626471084160 run.py:483] Algo bellman_ford step 5207 current loss 0.727982, current_train_items 166656.
I0302 19:00:55.336776 22626471084160 run.py:483] Algo bellman_ford step 5208 current loss 0.603930, current_train_items 166688.
I0302 19:00:55.373010 22626471084160 run.py:483] Algo bellman_ford step 5209 current loss 0.888060, current_train_items 166720.
I0302 19:00:55.392800 22626471084160 run.py:483] Algo bellman_ford step 5210 current loss 0.403272, current_train_items 166752.
I0302 19:00:55.408444 22626471084160 run.py:483] Algo bellman_ford step 5211 current loss 0.511475, current_train_items 166784.
I0302 19:00:55.432317 22626471084160 run.py:483] Algo bellman_ford step 5212 current loss 0.625437, current_train_items 166816.
I0302 19:00:55.464976 22626471084160 run.py:483] Algo bellman_ford step 5213 current loss 0.796805, current_train_items 166848.
I0302 19:00:55.498407 22626471084160 run.py:483] Algo bellman_ford step 5214 current loss 0.690860, current_train_items 166880.
I0302 19:00:55.517898 22626471084160 run.py:483] Algo bellman_ford step 5215 current loss 0.265377, current_train_items 166912.
I0302 19:00:55.534359 22626471084160 run.py:483] Algo bellman_ford step 5216 current loss 0.456208, current_train_items 166944.
I0302 19:00:55.557872 22626471084160 run.py:483] Algo bellman_ford step 5217 current loss 0.725356, current_train_items 166976.
I0302 19:00:55.587963 22626471084160 run.py:483] Algo bellman_ford step 5218 current loss 0.738574, current_train_items 167008.
I0302 19:00:55.621369 22626471084160 run.py:483] Algo bellman_ford step 5219 current loss 0.807935, current_train_items 167040.
I0302 19:00:55.641099 22626471084160 run.py:483] Algo bellman_ford step 5220 current loss 0.403928, current_train_items 167072.
I0302 19:00:55.657181 22626471084160 run.py:483] Algo bellman_ford step 5221 current loss 0.450415, current_train_items 167104.
I0302 19:00:55.679899 22626471084160 run.py:483] Algo bellman_ford step 5222 current loss 0.576400, current_train_items 167136.
I0302 19:00:55.711855 22626471084160 run.py:483] Algo bellman_ford step 5223 current loss 0.811570, current_train_items 167168.
I0302 19:00:55.744818 22626471084160 run.py:483] Algo bellman_ford step 5224 current loss 0.809405, current_train_items 167200.
I0302 19:00:55.764698 22626471084160 run.py:483] Algo bellman_ford step 5225 current loss 0.324755, current_train_items 167232.
I0302 19:00:55.780712 22626471084160 run.py:483] Algo bellman_ford step 5226 current loss 0.450817, current_train_items 167264.
I0302 19:00:55.804904 22626471084160 run.py:483] Algo bellman_ford step 5227 current loss 0.675487, current_train_items 167296.
I0302 19:00:55.837141 22626471084160 run.py:483] Algo bellman_ford step 5228 current loss 0.707723, current_train_items 167328.
I0302 19:00:55.872101 22626471084160 run.py:483] Algo bellman_ford step 5229 current loss 0.824806, current_train_items 167360.
I0302 19:00:55.891813 22626471084160 run.py:483] Algo bellman_ford step 5230 current loss 0.273364, current_train_items 167392.
I0302 19:00:55.908115 22626471084160 run.py:483] Algo bellman_ford step 5231 current loss 0.526850, current_train_items 167424.
I0302 19:00:55.932310 22626471084160 run.py:483] Algo bellman_ford step 5232 current loss 0.573932, current_train_items 167456.
I0302 19:00:55.962830 22626471084160 run.py:483] Algo bellman_ford step 5233 current loss 0.655167, current_train_items 167488.
I0302 19:00:55.997924 22626471084160 run.py:483] Algo bellman_ford step 5234 current loss 0.917720, current_train_items 167520.
I0302 19:00:56.017420 22626471084160 run.py:483] Algo bellman_ford step 5235 current loss 0.292862, current_train_items 167552.
I0302 19:00:56.033445 22626471084160 run.py:483] Algo bellman_ford step 5236 current loss 0.501525, current_train_items 167584.
I0302 19:00:56.057066 22626471084160 run.py:483] Algo bellman_ford step 5237 current loss 0.648897, current_train_items 167616.
I0302 19:00:56.089864 22626471084160 run.py:483] Algo bellman_ford step 5238 current loss 0.783787, current_train_items 167648.
I0302 19:00:56.124191 22626471084160 run.py:483] Algo bellman_ford step 5239 current loss 0.863357, current_train_items 167680.
I0302 19:00:56.144207 22626471084160 run.py:483] Algo bellman_ford step 5240 current loss 0.397475, current_train_items 167712.
I0302 19:00:56.160352 22626471084160 run.py:483] Algo bellman_ford step 5241 current loss 0.503007, current_train_items 167744.
I0302 19:00:56.184270 22626471084160 run.py:483] Algo bellman_ford step 5242 current loss 0.746211, current_train_items 167776.
I0302 19:00:56.215351 22626471084160 run.py:483] Algo bellman_ford step 5243 current loss 0.769701, current_train_items 167808.
I0302 19:00:56.246652 22626471084160 run.py:483] Algo bellman_ford step 5244 current loss 0.737505, current_train_items 167840.
I0302 19:00:56.266127 22626471084160 run.py:483] Algo bellman_ford step 5245 current loss 0.317646, current_train_items 167872.
I0302 19:00:56.282372 22626471084160 run.py:483] Algo bellman_ford step 5246 current loss 0.447131, current_train_items 167904.
I0302 19:00:56.306917 22626471084160 run.py:483] Algo bellman_ford step 5247 current loss 0.627741, current_train_items 167936.
I0302 19:00:56.338685 22626471084160 run.py:483] Algo bellman_ford step 5248 current loss 0.716513, current_train_items 167968.
I0302 19:00:56.373069 22626471084160 run.py:483] Algo bellman_ford step 5249 current loss 0.848557, current_train_items 168000.
I0302 19:00:56.392772 22626471084160 run.py:483] Algo bellman_ford step 5250 current loss 0.233834, current_train_items 168032.
I0302 19:00:56.400838 22626471084160 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0302 19:00:56.400940 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:56.417598 22626471084160 run.py:483] Algo bellman_ford step 5251 current loss 0.480131, current_train_items 168064.
I0302 19:00:56.442032 22626471084160 run.py:483] Algo bellman_ford step 5252 current loss 0.787739, current_train_items 168096.
I0302 19:00:56.473688 22626471084160 run.py:483] Algo bellman_ford step 5253 current loss 0.694198, current_train_items 168128.
I0302 19:00:56.505713 22626471084160 run.py:483] Algo bellman_ford step 5254 current loss 0.638802, current_train_items 168160.
I0302 19:00:56.525998 22626471084160 run.py:483] Algo bellman_ford step 5255 current loss 0.305777, current_train_items 168192.
I0302 19:00:56.541894 22626471084160 run.py:483] Algo bellman_ford step 5256 current loss 0.422387, current_train_items 168224.
I0302 19:00:56.564961 22626471084160 run.py:483] Algo bellman_ford step 5257 current loss 0.642937, current_train_items 168256.
I0302 19:00:56.596702 22626471084160 run.py:483] Algo bellman_ford step 5258 current loss 0.819017, current_train_items 168288.
I0302 19:00:56.628274 22626471084160 run.py:483] Algo bellman_ford step 5259 current loss 0.633868, current_train_items 168320.
I0302 19:00:56.648508 22626471084160 run.py:483] Algo bellman_ford step 5260 current loss 0.313528, current_train_items 168352.
I0302 19:00:56.664899 22626471084160 run.py:483] Algo bellman_ford step 5261 current loss 0.452130, current_train_items 168384.
I0302 19:00:56.687844 22626471084160 run.py:483] Algo bellman_ford step 5262 current loss 0.639288, current_train_items 168416.
I0302 19:00:56.719837 22626471084160 run.py:483] Algo bellman_ford step 5263 current loss 0.700733, current_train_items 168448.
I0302 19:00:56.752361 22626471084160 run.py:483] Algo bellman_ford step 5264 current loss 0.620547, current_train_items 168480.
I0302 19:00:56.772114 22626471084160 run.py:483] Algo bellman_ford step 5265 current loss 0.230492, current_train_items 168512.
I0302 19:00:56.788446 22626471084160 run.py:483] Algo bellman_ford step 5266 current loss 0.556167, current_train_items 168544.
I0302 19:00:56.813105 22626471084160 run.py:483] Algo bellman_ford step 5267 current loss 0.620578, current_train_items 168576.
I0302 19:00:56.843489 22626471084160 run.py:483] Algo bellman_ford step 5268 current loss 0.619994, current_train_items 168608.
I0302 19:00:56.875351 22626471084160 run.py:483] Algo bellman_ford step 5269 current loss 0.833805, current_train_items 168640.
I0302 19:00:56.895045 22626471084160 run.py:483] Algo bellman_ford step 5270 current loss 0.328730, current_train_items 168672.
I0302 19:00:56.911087 22626471084160 run.py:483] Algo bellman_ford step 5271 current loss 0.541395, current_train_items 168704.
I0302 19:00:56.933793 22626471084160 run.py:483] Algo bellman_ford step 5272 current loss 0.589494, current_train_items 168736.
I0302 19:00:56.964721 22626471084160 run.py:483] Algo bellman_ford step 5273 current loss 0.668495, current_train_items 168768.
I0302 19:00:56.997287 22626471084160 run.py:483] Algo bellman_ford step 5274 current loss 0.727980, current_train_items 168800.
I0302 19:00:57.017381 22626471084160 run.py:483] Algo bellman_ford step 5275 current loss 0.346372, current_train_items 168832.
I0302 19:00:57.033503 22626471084160 run.py:483] Algo bellman_ford step 5276 current loss 0.500535, current_train_items 168864.
I0302 19:00:57.056411 22626471084160 run.py:483] Algo bellman_ford step 5277 current loss 0.629066, current_train_items 168896.
I0302 19:00:57.087278 22626471084160 run.py:483] Algo bellman_ford step 5278 current loss 0.670145, current_train_items 168928.
I0302 19:00:57.121741 22626471084160 run.py:483] Algo bellman_ford step 5279 current loss 0.863987, current_train_items 168960.
I0302 19:00:57.141162 22626471084160 run.py:483] Algo bellman_ford step 5280 current loss 0.347745, current_train_items 168992.
I0302 19:00:57.157602 22626471084160 run.py:483] Algo bellman_ford step 5281 current loss 0.615481, current_train_items 169024.
I0302 19:00:57.182306 22626471084160 run.py:483] Algo bellman_ford step 5282 current loss 0.756152, current_train_items 169056.
I0302 19:00:57.213688 22626471084160 run.py:483] Algo bellman_ford step 5283 current loss 0.698226, current_train_items 169088.
I0302 19:00:57.246903 22626471084160 run.py:483] Algo bellman_ford step 5284 current loss 0.899308, current_train_items 169120.
I0302 19:00:57.266926 22626471084160 run.py:483] Algo bellman_ford step 5285 current loss 0.250788, current_train_items 169152.
I0302 19:00:57.283310 22626471084160 run.py:483] Algo bellman_ford step 5286 current loss 0.459733, current_train_items 169184.
I0302 19:00:57.307655 22626471084160 run.py:483] Algo bellman_ford step 5287 current loss 0.680947, current_train_items 169216.
I0302 19:00:57.338546 22626471084160 run.py:483] Algo bellman_ford step 5288 current loss 0.685515, current_train_items 169248.
I0302 19:00:57.372945 22626471084160 run.py:483] Algo bellman_ford step 5289 current loss 0.857937, current_train_items 169280.
I0302 19:00:57.392834 22626471084160 run.py:483] Algo bellman_ford step 5290 current loss 0.350056, current_train_items 169312.
I0302 19:00:57.409508 22626471084160 run.py:483] Algo bellman_ford step 5291 current loss 0.480420, current_train_items 169344.
I0302 19:00:57.432581 22626471084160 run.py:483] Algo bellman_ford step 5292 current loss 0.573310, current_train_items 169376.
I0302 19:00:57.463654 22626471084160 run.py:483] Algo bellman_ford step 5293 current loss 0.660688, current_train_items 169408.
I0302 19:00:57.496336 22626471084160 run.py:483] Algo bellman_ford step 5294 current loss 0.802425, current_train_items 169440.
I0302 19:00:57.515970 22626471084160 run.py:483] Algo bellman_ford step 5295 current loss 0.268536, current_train_items 169472.
I0302 19:00:57.531925 22626471084160 run.py:483] Algo bellman_ford step 5296 current loss 0.491495, current_train_items 169504.
I0302 19:00:57.555085 22626471084160 run.py:483] Algo bellman_ford step 5297 current loss 0.616328, current_train_items 169536.
I0302 19:00:57.587049 22626471084160 run.py:483] Algo bellman_ford step 5298 current loss 0.715112, current_train_items 169568.
I0302 19:00:57.623231 22626471084160 run.py:483] Algo bellman_ford step 5299 current loss 0.856029, current_train_items 169600.
I0302 19:00:57.643378 22626471084160 run.py:483] Algo bellman_ford step 5300 current loss 0.363316, current_train_items 169632.
I0302 19:00:57.651135 22626471084160 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0302 19:00:57.651252 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:00:57.667589 22626471084160 run.py:483] Algo bellman_ford step 5301 current loss 0.437804, current_train_items 169664.
I0302 19:00:57.692179 22626471084160 run.py:483] Algo bellman_ford step 5302 current loss 0.676646, current_train_items 169696.
I0302 19:00:57.724662 22626471084160 run.py:483] Algo bellman_ford step 5303 current loss 0.638144, current_train_items 169728.
I0302 19:00:57.759349 22626471084160 run.py:483] Algo bellman_ford step 5304 current loss 0.770343, current_train_items 169760.
I0302 19:00:57.779530 22626471084160 run.py:483] Algo bellman_ford step 5305 current loss 0.290507, current_train_items 169792.
I0302 19:00:57.795730 22626471084160 run.py:483] Algo bellman_ford step 5306 current loss 0.503884, current_train_items 169824.
I0302 19:00:57.820648 22626471084160 run.py:483] Algo bellman_ford step 5307 current loss 0.722217, current_train_items 169856.
I0302 19:00:57.852817 22626471084160 run.py:483] Algo bellman_ford step 5308 current loss 0.828917, current_train_items 169888.
I0302 19:00:57.885057 22626471084160 run.py:483] Algo bellman_ford step 5309 current loss 0.872315, current_train_items 169920.
I0302 19:00:57.904801 22626471084160 run.py:483] Algo bellman_ford step 5310 current loss 0.319259, current_train_items 169952.
I0302 19:00:57.920441 22626471084160 run.py:483] Algo bellman_ford step 5311 current loss 0.415156, current_train_items 169984.
I0302 19:00:57.943958 22626471084160 run.py:483] Algo bellman_ford step 5312 current loss 0.604046, current_train_items 170016.
I0302 19:00:57.975125 22626471084160 run.py:483] Algo bellman_ford step 5313 current loss 0.789269, current_train_items 170048.
I0302 19:00:58.009847 22626471084160 run.py:483] Algo bellman_ford step 5314 current loss 0.881224, current_train_items 170080.
I0302 19:00:58.029892 22626471084160 run.py:483] Algo bellman_ford step 5315 current loss 0.394443, current_train_items 170112.
I0302 19:00:58.046070 22626471084160 run.py:483] Algo bellman_ford step 5316 current loss 0.421226, current_train_items 170144.
I0302 19:00:58.069854 22626471084160 run.py:483] Algo bellman_ford step 5317 current loss 0.572838, current_train_items 170176.
I0302 19:00:58.099506 22626471084160 run.py:483] Algo bellman_ford step 5318 current loss 0.596428, current_train_items 170208.
I0302 19:00:58.133597 22626471084160 run.py:483] Algo bellman_ford step 5319 current loss 0.803022, current_train_items 170240.
I0302 19:00:58.153091 22626471084160 run.py:483] Algo bellman_ford step 5320 current loss 0.300331, current_train_items 170272.
I0302 19:00:58.169179 22626471084160 run.py:483] Algo bellman_ford step 5321 current loss 0.476026, current_train_items 170304.
I0302 19:00:58.193883 22626471084160 run.py:483] Algo bellman_ford step 5322 current loss 0.703803, current_train_items 170336.
I0302 19:00:58.223213 22626471084160 run.py:483] Algo bellman_ford step 5323 current loss 0.656619, current_train_items 170368.
I0302 19:00:58.258483 22626471084160 run.py:483] Algo bellman_ford step 5324 current loss 0.795900, current_train_items 170400.
I0302 19:00:58.278353 22626471084160 run.py:483] Algo bellman_ford step 5325 current loss 0.329565, current_train_items 170432.
I0302 19:00:58.294428 22626471084160 run.py:483] Algo bellman_ford step 5326 current loss 0.479808, current_train_items 170464.
I0302 19:00:58.317351 22626471084160 run.py:483] Algo bellman_ford step 5327 current loss 0.589023, current_train_items 170496.
I0302 19:00:58.349580 22626471084160 run.py:483] Algo bellman_ford step 5328 current loss 0.764019, current_train_items 170528.
I0302 19:00:58.382904 22626471084160 run.py:483] Algo bellman_ford step 5329 current loss 0.953224, current_train_items 170560.
I0302 19:00:58.402560 22626471084160 run.py:483] Algo bellman_ford step 5330 current loss 0.338492, current_train_items 170592.
I0302 19:00:58.418912 22626471084160 run.py:483] Algo bellman_ford step 5331 current loss 0.455228, current_train_items 170624.
I0302 19:00:58.442244 22626471084160 run.py:483] Algo bellman_ford step 5332 current loss 0.651930, current_train_items 170656.
I0302 19:00:58.471769 22626471084160 run.py:483] Algo bellman_ford step 5333 current loss 0.625234, current_train_items 170688.
I0302 19:00:58.504616 22626471084160 run.py:483] Algo bellman_ford step 5334 current loss 0.737351, current_train_items 170720.
I0302 19:00:58.524083 22626471084160 run.py:483] Algo bellman_ford step 5335 current loss 0.412020, current_train_items 170752.
I0302 19:00:58.540468 22626471084160 run.py:483] Algo bellman_ford step 5336 current loss 0.551289, current_train_items 170784.
I0302 19:00:58.563497 22626471084160 run.py:483] Algo bellman_ford step 5337 current loss 0.669381, current_train_items 170816.
I0302 19:00:58.593461 22626471084160 run.py:483] Algo bellman_ford step 5338 current loss 0.715717, current_train_items 170848.
I0302 19:00:58.627817 22626471084160 run.py:483] Algo bellman_ford step 5339 current loss 0.772649, current_train_items 170880.
I0302 19:00:58.647121 22626471084160 run.py:483] Algo bellman_ford step 5340 current loss 0.300013, current_train_items 170912.
I0302 19:00:58.663454 22626471084160 run.py:483] Algo bellman_ford step 5341 current loss 0.581537, current_train_items 170944.
I0302 19:00:58.688667 22626471084160 run.py:483] Algo bellman_ford step 5342 current loss 0.815257, current_train_items 170976.
I0302 19:00:58.719953 22626471084160 run.py:483] Algo bellman_ford step 5343 current loss 0.723059, current_train_items 171008.
I0302 19:00:58.753024 22626471084160 run.py:483] Algo bellman_ford step 5344 current loss 0.774389, current_train_items 171040.
I0302 19:00:58.772113 22626471084160 run.py:483] Algo bellman_ford step 5345 current loss 0.224176, current_train_items 171072.
I0302 19:00:58.787710 22626471084160 run.py:483] Algo bellman_ford step 5346 current loss 0.461372, current_train_items 171104.
I0302 19:00:58.810942 22626471084160 run.py:483] Algo bellman_ford step 5347 current loss 0.640900, current_train_items 171136.
I0302 19:00:58.842987 22626471084160 run.py:483] Algo bellman_ford step 5348 current loss 0.968103, current_train_items 171168.
I0302 19:00:58.876950 22626471084160 run.py:483] Algo bellman_ford step 5349 current loss 1.168355, current_train_items 171200.
I0302 19:00:58.896643 22626471084160 run.py:483] Algo bellman_ford step 5350 current loss 0.292303, current_train_items 171232.
I0302 19:00:58.904642 22626471084160 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.8818359375, 'score': 0.8818359375, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0302 19:00:58.904748 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.882, val scores are: bellman_ford: 0.882
I0302 19:00:58.921114 22626471084160 run.py:483] Algo bellman_ford step 5351 current loss 0.494375, current_train_items 171264.
I0302 19:00:58.945823 22626471084160 run.py:483] Algo bellman_ford step 5352 current loss 0.724304, current_train_items 171296.
I0302 19:00:58.976103 22626471084160 run.py:483] Algo bellman_ford step 5353 current loss 0.772727, current_train_items 171328.
I0302 19:00:59.009634 22626471084160 run.py:483] Algo bellman_ford step 5354 current loss 0.803710, current_train_items 171360.
I0302 19:00:59.029804 22626471084160 run.py:483] Algo bellman_ford step 5355 current loss 0.323078, current_train_items 171392.
I0302 19:00:59.045829 22626471084160 run.py:483] Algo bellman_ford step 5356 current loss 0.546990, current_train_items 171424.
I0302 19:00:59.069546 22626471084160 run.py:483] Algo bellman_ford step 5357 current loss 0.808784, current_train_items 171456.
I0302 19:00:59.099313 22626471084160 run.py:483] Algo bellman_ford step 5358 current loss 0.681959, current_train_items 171488.
I0302 19:00:59.132578 22626471084160 run.py:483] Algo bellman_ford step 5359 current loss 0.885062, current_train_items 171520.
I0302 19:00:59.152590 22626471084160 run.py:483] Algo bellman_ford step 5360 current loss 0.330710, current_train_items 171552.
I0302 19:00:59.168797 22626471084160 run.py:483] Algo bellman_ford step 5361 current loss 0.561400, current_train_items 171584.
I0302 19:00:59.192435 22626471084160 run.py:483] Algo bellman_ford step 5362 current loss 0.708011, current_train_items 171616.
I0302 19:00:59.223634 22626471084160 run.py:483] Algo bellman_ford step 5363 current loss 0.708433, current_train_items 171648.
I0302 19:00:59.256936 22626471084160 run.py:483] Algo bellman_ford step 5364 current loss 0.765815, current_train_items 171680.
I0302 19:00:59.276703 22626471084160 run.py:483] Algo bellman_ford step 5365 current loss 0.316575, current_train_items 171712.
I0302 19:00:59.292916 22626471084160 run.py:483] Algo bellman_ford step 5366 current loss 0.563937, current_train_items 171744.
I0302 19:00:59.316226 22626471084160 run.py:483] Algo bellman_ford step 5367 current loss 0.603938, current_train_items 171776.
I0302 19:00:59.347010 22626471084160 run.py:483] Algo bellman_ford step 5368 current loss 0.669506, current_train_items 171808.
I0302 19:00:59.379571 22626471084160 run.py:483] Algo bellman_ford step 5369 current loss 0.724636, current_train_items 171840.
I0302 19:00:59.399668 22626471084160 run.py:483] Algo bellman_ford step 5370 current loss 0.303751, current_train_items 171872.
I0302 19:00:59.415865 22626471084160 run.py:483] Algo bellman_ford step 5371 current loss 0.457371, current_train_items 171904.
I0302 19:00:59.438980 22626471084160 run.py:483] Algo bellman_ford step 5372 current loss 0.567495, current_train_items 171936.
I0302 19:00:59.469631 22626471084160 run.py:483] Algo bellman_ford step 5373 current loss 0.640009, current_train_items 171968.
I0302 19:00:59.500944 22626471084160 run.py:483] Algo bellman_ford step 5374 current loss 0.659890, current_train_items 172000.
I0302 19:00:59.520738 22626471084160 run.py:483] Algo bellman_ford step 5375 current loss 0.296474, current_train_items 172032.
I0302 19:00:59.537047 22626471084160 run.py:483] Algo bellman_ford step 5376 current loss 0.566884, current_train_items 172064.
I0302 19:00:59.560373 22626471084160 run.py:483] Algo bellman_ford step 5377 current loss 0.688036, current_train_items 172096.
I0302 19:00:59.590292 22626471084160 run.py:483] Algo bellman_ford step 5378 current loss 0.610752, current_train_items 172128.
I0302 19:00:59.620960 22626471084160 run.py:483] Algo bellman_ford step 5379 current loss 0.705820, current_train_items 172160.
I0302 19:00:59.640768 22626471084160 run.py:483] Algo bellman_ford step 5380 current loss 0.379247, current_train_items 172192.
I0302 19:00:59.657049 22626471084160 run.py:483] Algo bellman_ford step 5381 current loss 0.463305, current_train_items 172224.
I0302 19:00:59.681369 22626471084160 run.py:483] Algo bellman_ford step 5382 current loss 0.786936, current_train_items 172256.
I0302 19:00:59.712418 22626471084160 run.py:483] Algo bellman_ford step 5383 current loss 0.751828, current_train_items 172288.
I0302 19:00:59.744791 22626471084160 run.py:483] Algo bellman_ford step 5384 current loss 0.678612, current_train_items 172320.
I0302 19:00:59.764760 22626471084160 run.py:483] Algo bellman_ford step 5385 current loss 0.314145, current_train_items 172352.
I0302 19:00:59.780569 22626471084160 run.py:483] Algo bellman_ford step 5386 current loss 0.502957, current_train_items 172384.
I0302 19:00:59.803523 22626471084160 run.py:483] Algo bellman_ford step 5387 current loss 0.550644, current_train_items 172416.
I0302 19:00:59.833480 22626471084160 run.py:483] Algo bellman_ford step 5388 current loss 0.635128, current_train_items 172448.
I0302 19:00:59.867296 22626471084160 run.py:483] Algo bellman_ford step 5389 current loss 0.813888, current_train_items 172480.
I0302 19:00:59.887376 22626471084160 run.py:483] Algo bellman_ford step 5390 current loss 0.330580, current_train_items 172512.
I0302 19:00:59.904100 22626471084160 run.py:483] Algo bellman_ford step 5391 current loss 0.519419, current_train_items 172544.
I0302 19:00:59.926826 22626471084160 run.py:483] Algo bellman_ford step 5392 current loss 0.617643, current_train_items 172576.
I0302 19:00:59.958796 22626471084160 run.py:483] Algo bellman_ford step 5393 current loss 0.785406, current_train_items 172608.
I0302 19:00:59.993188 22626471084160 run.py:483] Algo bellman_ford step 5394 current loss 0.872158, current_train_items 172640.
I0302 19:01:00.012847 22626471084160 run.py:483] Algo bellman_ford step 5395 current loss 0.358506, current_train_items 172672.
I0302 19:01:00.029428 22626471084160 run.py:483] Algo bellman_ford step 5396 current loss 0.515988, current_train_items 172704.
I0302 19:01:00.053088 22626471084160 run.py:483] Algo bellman_ford step 5397 current loss 0.608420, current_train_items 172736.
I0302 19:01:00.083811 22626471084160 run.py:483] Algo bellman_ford step 5398 current loss 0.717598, current_train_items 172768.
I0302 19:01:00.118598 22626471084160 run.py:483] Algo bellman_ford step 5399 current loss 1.060608, current_train_items 172800.
I0302 19:01:00.138377 22626471084160 run.py:483] Algo bellman_ford step 5400 current loss 0.239112, current_train_items 172832.
I0302 19:01:00.146271 22626471084160 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0302 19:01:00.146375 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:01:00.163651 22626471084160 run.py:483] Algo bellman_ford step 5401 current loss 0.530426, current_train_items 172864.
I0302 19:01:00.188309 22626471084160 run.py:483] Algo bellman_ford step 5402 current loss 0.741387, current_train_items 172896.
I0302 19:01:00.221111 22626471084160 run.py:483] Algo bellman_ford step 5403 current loss 0.798252, current_train_items 172928.
I0302 19:01:00.256370 22626471084160 run.py:483] Algo bellman_ford step 5404 current loss 0.750360, current_train_items 172960.
I0302 19:01:00.276420 22626471084160 run.py:483] Algo bellman_ford step 5405 current loss 0.252210, current_train_items 172992.
I0302 19:01:00.292066 22626471084160 run.py:483] Algo bellman_ford step 5406 current loss 0.425313, current_train_items 173024.
I0302 19:01:00.315279 22626471084160 run.py:483] Algo bellman_ford step 5407 current loss 0.526207, current_train_items 173056.
I0302 19:01:00.344679 22626471084160 run.py:483] Algo bellman_ford step 5408 current loss 0.643988, current_train_items 173088.
I0302 19:01:00.377498 22626471084160 run.py:483] Algo bellman_ford step 5409 current loss 0.656967, current_train_items 173120.
I0302 19:01:00.397399 22626471084160 run.py:483] Algo bellman_ford step 5410 current loss 0.312980, current_train_items 173152.
I0302 19:01:00.414029 22626471084160 run.py:483] Algo bellman_ford step 5411 current loss 0.524491, current_train_items 173184.
I0302 19:01:00.437760 22626471084160 run.py:483] Algo bellman_ford step 5412 current loss 0.664637, current_train_items 173216.
I0302 19:01:00.468979 22626471084160 run.py:483] Algo bellman_ford step 5413 current loss 0.682577, current_train_items 173248.
I0302 19:01:00.503179 22626471084160 run.py:483] Algo bellman_ford step 5414 current loss 0.847232, current_train_items 173280.
I0302 19:01:00.522930 22626471084160 run.py:483] Algo bellman_ford step 5415 current loss 0.225465, current_train_items 173312.
I0302 19:01:00.539487 22626471084160 run.py:483] Algo bellman_ford step 5416 current loss 0.501578, current_train_items 173344.
I0302 19:01:00.563675 22626471084160 run.py:483] Algo bellman_ford step 5417 current loss 0.606953, current_train_items 173376.
I0302 19:01:00.595919 22626471084160 run.py:483] Algo bellman_ford step 5418 current loss 0.664541, current_train_items 173408.
I0302 19:01:00.631255 22626471084160 run.py:483] Algo bellman_ford step 5419 current loss 0.830402, current_train_items 173440.
I0302 19:01:00.650876 22626471084160 run.py:483] Algo bellman_ford step 5420 current loss 0.348072, current_train_items 173472.
I0302 19:01:00.667264 22626471084160 run.py:483] Algo bellman_ford step 5421 current loss 0.468372, current_train_items 173504.
I0302 19:01:00.690482 22626471084160 run.py:483] Algo bellman_ford step 5422 current loss 0.525709, current_train_items 173536.
I0302 19:01:00.720739 22626471084160 run.py:483] Algo bellman_ford step 5423 current loss 0.637871, current_train_items 173568.
I0302 19:01:00.752195 22626471084160 run.py:483] Algo bellman_ford step 5424 current loss 0.704428, current_train_items 173600.
I0302 19:01:00.772112 22626471084160 run.py:483] Algo bellman_ford step 5425 current loss 0.287051, current_train_items 173632.
I0302 19:01:00.788040 22626471084160 run.py:483] Algo bellman_ford step 5426 current loss 0.420271, current_train_items 173664.
I0302 19:01:00.812173 22626471084160 run.py:483] Algo bellman_ford step 5427 current loss 0.631450, current_train_items 173696.
I0302 19:01:00.843067 22626471084160 run.py:483] Algo bellman_ford step 5428 current loss 0.726702, current_train_items 173728.
I0302 19:01:00.877526 22626471084160 run.py:483] Algo bellman_ford step 5429 current loss 0.744832, current_train_items 173760.
I0302 19:01:00.897064 22626471084160 run.py:483] Algo bellman_ford step 5430 current loss 0.270738, current_train_items 173792.
I0302 19:01:00.913472 22626471084160 run.py:483] Algo bellman_ford step 5431 current loss 0.489810, current_train_items 173824.
I0302 19:01:00.937407 22626471084160 run.py:483] Algo bellman_ford step 5432 current loss 0.670104, current_train_items 173856.
I0302 19:01:00.970603 22626471084160 run.py:483] Algo bellman_ford step 5433 current loss 0.791935, current_train_items 173888.
I0302 19:01:01.006535 22626471084160 run.py:483] Algo bellman_ford step 5434 current loss 0.889822, current_train_items 173920.
I0302 19:01:01.026218 22626471084160 run.py:483] Algo bellman_ford step 5435 current loss 0.295587, current_train_items 173952.
I0302 19:01:01.042373 22626471084160 run.py:483] Algo bellman_ford step 5436 current loss 0.578705, current_train_items 173984.
I0302 19:01:01.065942 22626471084160 run.py:483] Algo bellman_ford step 5437 current loss 0.631041, current_train_items 174016.
I0302 19:01:01.096118 22626471084160 run.py:483] Algo bellman_ford step 5438 current loss 0.873180, current_train_items 174048.
I0302 19:01:01.133305 22626471084160 run.py:483] Algo bellman_ford step 5439 current loss 0.892992, current_train_items 174080.
I0302 19:01:01.152867 22626471084160 run.py:483] Algo bellman_ford step 5440 current loss 0.442122, current_train_items 174112.
I0302 19:01:01.168807 22626471084160 run.py:483] Algo bellman_ford step 5441 current loss 0.415721, current_train_items 174144.
I0302 19:01:01.192000 22626471084160 run.py:483] Algo bellman_ford step 5442 current loss 0.573076, current_train_items 174176.
I0302 19:01:01.222266 22626471084160 run.py:483] Algo bellman_ford step 5443 current loss 0.736656, current_train_items 174208.
I0302 19:01:01.256087 22626471084160 run.py:483] Algo bellman_ford step 5444 current loss 0.771445, current_train_items 174240.
I0302 19:01:01.275816 22626471084160 run.py:483] Algo bellman_ford step 5445 current loss 0.239054, current_train_items 174272.
I0302 19:01:01.291978 22626471084160 run.py:483] Algo bellman_ford step 5446 current loss 0.588201, current_train_items 174304.
I0302 19:01:01.316562 22626471084160 run.py:483] Algo bellman_ford step 5447 current loss 0.629225, current_train_items 174336.
I0302 19:01:01.348152 22626471084160 run.py:483] Algo bellman_ford step 5448 current loss 0.911249, current_train_items 174368.
I0302 19:01:01.382054 22626471084160 run.py:483] Algo bellman_ford step 5449 current loss 0.777509, current_train_items 174400.
I0302 19:01:01.401416 22626471084160 run.py:483] Algo bellman_ford step 5450 current loss 0.280977, current_train_items 174432.
I0302 19:01:01.409287 22626471084160 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0302 19:01:01.409392 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 19:01:01.426165 22626471084160 run.py:483] Algo bellman_ford step 5451 current loss 0.471209, current_train_items 174464.
I0302 19:01:01.450312 22626471084160 run.py:483] Algo bellman_ford step 5452 current loss 0.665308, current_train_items 174496.
I0302 19:01:01.480694 22626471084160 run.py:483] Algo bellman_ford step 5453 current loss 0.651900, current_train_items 174528.
I0302 19:01:01.514194 22626471084160 run.py:483] Algo bellman_ford step 5454 current loss 0.868605, current_train_items 174560.
I0302 19:01:01.534503 22626471084160 run.py:483] Algo bellman_ford step 5455 current loss 0.302873, current_train_items 174592.
I0302 19:01:01.550748 22626471084160 run.py:483] Algo bellman_ford step 5456 current loss 0.499205, current_train_items 174624.
I0302 19:01:01.574883 22626471084160 run.py:483] Algo bellman_ford step 5457 current loss 0.643425, current_train_items 174656.
I0302 19:01:01.606534 22626471084160 run.py:483] Algo bellman_ford step 5458 current loss 0.696312, current_train_items 174688.
I0302 19:01:01.637544 22626471084160 run.py:483] Algo bellman_ford step 5459 current loss 0.702790, current_train_items 174720.
I0302 19:01:01.657296 22626471084160 run.py:483] Algo bellman_ford step 5460 current loss 0.347381, current_train_items 174752.
I0302 19:01:01.673877 22626471084160 run.py:483] Algo bellman_ford step 5461 current loss 0.486812, current_train_items 174784.
I0302 19:01:01.697682 22626471084160 run.py:483] Algo bellman_ford step 5462 current loss 0.552910, current_train_items 174816.
I0302 19:01:01.727927 22626471084160 run.py:483] Algo bellman_ford step 5463 current loss 0.697969, current_train_items 174848.
I0302 19:01:01.762174 22626471084160 run.py:483] Algo bellman_ford step 5464 current loss 0.811846, current_train_items 174880.
I0302 19:01:01.782033 22626471084160 run.py:483] Algo bellman_ford step 5465 current loss 0.350825, current_train_items 174912.
I0302 19:01:01.797963 22626471084160 run.py:483] Algo bellman_ford step 5466 current loss 0.430770, current_train_items 174944.
I0302 19:01:01.821341 22626471084160 run.py:483] Algo bellman_ford step 5467 current loss 0.570598, current_train_items 174976.
I0302 19:01:01.851576 22626471084160 run.py:483] Algo bellman_ford step 5468 current loss 0.610339, current_train_items 175008.
I0302 19:01:01.885112 22626471084160 run.py:483] Algo bellman_ford step 5469 current loss 0.762494, current_train_items 175040.
I0302 19:01:01.905230 22626471084160 run.py:483] Algo bellman_ford step 5470 current loss 0.362983, current_train_items 175072.
I0302 19:01:01.921557 22626471084160 run.py:483] Algo bellman_ford step 5471 current loss 0.553944, current_train_items 175104.
I0302 19:01:01.944686 22626471084160 run.py:483] Algo bellman_ford step 5472 current loss 0.531133, current_train_items 175136.
I0302 19:01:01.975309 22626471084160 run.py:483] Algo bellman_ford step 5473 current loss 0.679246, current_train_items 175168.
I0302 19:01:02.009990 22626471084160 run.py:483] Algo bellman_ford step 5474 current loss 0.908506, current_train_items 175200.
I0302 19:01:02.029859 22626471084160 run.py:483] Algo bellman_ford step 5475 current loss 0.309496, current_train_items 175232.
I0302 19:01:02.046161 22626471084160 run.py:483] Algo bellman_ford step 5476 current loss 0.471536, current_train_items 175264.
I0302 19:01:02.069096 22626471084160 run.py:483] Algo bellman_ford step 5477 current loss 0.566345, current_train_items 175296.
I0302 19:01:02.100558 22626471084160 run.py:483] Algo bellman_ford step 5478 current loss 0.632260, current_train_items 175328.
I0302 19:01:02.133807 22626471084160 run.py:483] Algo bellman_ford step 5479 current loss 0.844698, current_train_items 175360.
I0302 19:01:02.153834 22626471084160 run.py:483] Algo bellman_ford step 5480 current loss 0.364140, current_train_items 175392.
I0302 19:01:02.170550 22626471084160 run.py:483] Algo bellman_ford step 5481 current loss 0.521309, current_train_items 175424.
I0302 19:01:02.195326 22626471084160 run.py:483] Algo bellman_ford step 5482 current loss 0.710229, current_train_items 175456.
I0302 19:01:02.227215 22626471084160 run.py:483] Algo bellman_ford step 5483 current loss 0.746402, current_train_items 175488.
I0302 19:01:02.262953 22626471084160 run.py:483] Algo bellman_ford step 5484 current loss 0.766287, current_train_items 175520.
I0302 19:01:02.282990 22626471084160 run.py:483] Algo bellman_ford step 5485 current loss 0.292002, current_train_items 175552.
I0302 19:01:02.298990 22626471084160 run.py:483] Algo bellman_ford step 5486 current loss 0.459743, current_train_items 175584.
I0302 19:01:02.321530 22626471084160 run.py:483] Algo bellman_ford step 5487 current loss 0.595962, current_train_items 175616.
I0302 19:01:02.353929 22626471084160 run.py:483] Algo bellman_ford step 5488 current loss 0.708767, current_train_items 175648.
I0302 19:01:02.386402 22626471084160 run.py:483] Algo bellman_ford step 5489 current loss 0.672216, current_train_items 175680.
I0302 19:01:02.406446 22626471084160 run.py:483] Algo bellman_ford step 5490 current loss 0.319071, current_train_items 175712.
I0302 19:01:02.422779 22626471084160 run.py:483] Algo bellman_ford step 5491 current loss 0.471179, current_train_items 175744.
I0302 19:01:02.446515 22626471084160 run.py:483] Algo bellman_ford step 5492 current loss 0.674993, current_train_items 175776.
I0302 19:01:02.477380 22626471084160 run.py:483] Algo bellman_ford step 5493 current loss 0.701216, current_train_items 175808.
I0302 19:01:02.511067 22626471084160 run.py:483] Algo bellman_ford step 5494 current loss 0.778098, current_train_items 175840.
I0302 19:01:02.530629 22626471084160 run.py:483] Algo bellman_ford step 5495 current loss 0.297326, current_train_items 175872.
I0302 19:01:02.547143 22626471084160 run.py:483] Algo bellman_ford step 5496 current loss 0.417941, current_train_items 175904.
I0302 19:01:02.570969 22626471084160 run.py:483] Algo bellman_ford step 5497 current loss 0.671035, current_train_items 175936.
I0302 19:01:02.602044 22626471084160 run.py:483] Algo bellman_ford step 5498 current loss 0.691861, current_train_items 175968.
I0302 19:01:02.636666 22626471084160 run.py:483] Algo bellman_ford step 5499 current loss 0.849162, current_train_items 176000.
I0302 19:01:02.657011 22626471084160 run.py:483] Algo bellman_ford step 5500 current loss 0.317734, current_train_items 176032.
I0302 19:01:02.664819 22626471084160 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0302 19:01:02.664925 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:01:02.682117 22626471084160 run.py:483] Algo bellman_ford step 5501 current loss 0.535302, current_train_items 176064.
I0302 19:01:02.706890 22626471084160 run.py:483] Algo bellman_ford step 5502 current loss 0.705322, current_train_items 176096.
I0302 19:01:02.738042 22626471084160 run.py:483] Algo bellman_ford step 5503 current loss 0.655297, current_train_items 176128.
I0302 19:01:02.772449 22626471084160 run.py:483] Algo bellman_ford step 5504 current loss 0.858515, current_train_items 176160.
I0302 19:01:02.792680 22626471084160 run.py:483] Algo bellman_ford step 5505 current loss 0.267600, current_train_items 176192.
I0302 19:01:02.808918 22626471084160 run.py:483] Algo bellman_ford step 5506 current loss 0.498273, current_train_items 176224.
I0302 19:01:02.832372 22626471084160 run.py:483] Algo bellman_ford step 5507 current loss 0.579516, current_train_items 176256.
I0302 19:01:02.862266 22626471084160 run.py:483] Algo bellman_ford step 5508 current loss 0.715876, current_train_items 176288.
I0302 19:01:02.895426 22626471084160 run.py:483] Algo bellman_ford step 5509 current loss 0.769297, current_train_items 176320.
I0302 19:01:02.915032 22626471084160 run.py:483] Algo bellman_ford step 5510 current loss 0.317799, current_train_items 176352.
I0302 19:01:02.931494 22626471084160 run.py:483] Algo bellman_ford step 5511 current loss 0.476998, current_train_items 176384.
I0302 19:01:02.956420 22626471084160 run.py:483] Algo bellman_ford step 5512 current loss 0.685710, current_train_items 176416.
I0302 19:01:02.986764 22626471084160 run.py:483] Algo bellman_ford step 5513 current loss 0.686187, current_train_items 176448.
I0302 19:01:03.018822 22626471084160 run.py:483] Algo bellman_ford step 5514 current loss 0.831398, current_train_items 176480.
I0302 19:01:03.038474 22626471084160 run.py:483] Algo bellman_ford step 5515 current loss 0.303933, current_train_items 176512.
I0302 19:01:03.054486 22626471084160 run.py:483] Algo bellman_ford step 5516 current loss 0.411646, current_train_items 176544.
I0302 19:01:03.078674 22626471084160 run.py:483] Algo bellman_ford step 5517 current loss 0.624879, current_train_items 176576.
I0302 19:01:03.109301 22626471084160 run.py:483] Algo bellman_ford step 5518 current loss 0.669020, current_train_items 176608.
I0302 19:01:03.144417 22626471084160 run.py:483] Algo bellman_ford step 5519 current loss 0.772080, current_train_items 176640.
I0302 19:01:03.163916 22626471084160 run.py:483] Algo bellman_ford step 5520 current loss 0.333763, current_train_items 176672.
I0302 19:01:03.180448 22626471084160 run.py:483] Algo bellman_ford step 5521 current loss 0.569171, current_train_items 176704.
I0302 19:01:03.203863 22626471084160 run.py:483] Algo bellman_ford step 5522 current loss 0.620639, current_train_items 176736.
I0302 19:01:03.235594 22626471084160 run.py:483] Algo bellman_ford step 5523 current loss 0.775405, current_train_items 176768.
I0302 19:01:03.270415 22626471084160 run.py:483] Algo bellman_ford step 5524 current loss 0.980931, current_train_items 176800.
I0302 19:01:03.289807 22626471084160 run.py:483] Algo bellman_ford step 5525 current loss 0.269394, current_train_items 176832.
I0302 19:01:03.305651 22626471084160 run.py:483] Algo bellman_ford step 5526 current loss 0.444344, current_train_items 176864.
I0302 19:01:03.330079 22626471084160 run.py:483] Algo bellman_ford step 5527 current loss 0.700289, current_train_items 176896.
I0302 19:01:03.360503 22626471084160 run.py:483] Algo bellman_ford step 5528 current loss 0.691984, current_train_items 176928.
I0302 19:01:03.392810 22626471084160 run.py:483] Algo bellman_ford step 5529 current loss 0.725866, current_train_items 176960.
I0302 19:01:03.412530 22626471084160 run.py:483] Algo bellman_ford step 5530 current loss 0.339313, current_train_items 176992.
I0302 19:01:03.428774 22626471084160 run.py:483] Algo bellman_ford step 5531 current loss 0.437337, current_train_items 177024.
I0302 19:01:03.452745 22626471084160 run.py:483] Algo bellman_ford step 5532 current loss 0.663197, current_train_items 177056.
I0302 19:01:03.483116 22626471084160 run.py:483] Algo bellman_ford step 5533 current loss 0.758321, current_train_items 177088.
I0302 19:01:03.518965 22626471084160 run.py:483] Algo bellman_ford step 5534 current loss 0.854745, current_train_items 177120.
I0302 19:01:03.538395 22626471084160 run.py:483] Algo bellman_ford step 5535 current loss 0.272320, current_train_items 177152.
I0302 19:01:03.554890 22626471084160 run.py:483] Algo bellman_ford step 5536 current loss 0.537774, current_train_items 177184.
I0302 19:01:03.577756 22626471084160 run.py:483] Algo bellman_ford step 5537 current loss 0.606511, current_train_items 177216.
I0302 19:01:03.607959 22626471084160 run.py:483] Algo bellman_ford step 5538 current loss 0.665896, current_train_items 177248.
I0302 19:01:03.643465 22626471084160 run.py:483] Algo bellman_ford step 5539 current loss 0.776648, current_train_items 177280.
I0302 19:01:03.663350 22626471084160 run.py:483] Algo bellman_ford step 5540 current loss 0.287328, current_train_items 177312.
I0302 19:01:03.680037 22626471084160 run.py:483] Algo bellman_ford step 5541 current loss 0.453878, current_train_items 177344.
I0302 19:01:03.704274 22626471084160 run.py:483] Algo bellman_ford step 5542 current loss 0.583834, current_train_items 177376.
I0302 19:01:03.734935 22626471084160 run.py:483] Algo bellman_ford step 5543 current loss 0.583345, current_train_items 177408.
I0302 19:01:03.768188 22626471084160 run.py:483] Algo bellman_ford step 5544 current loss 0.825843, current_train_items 177440.
I0302 19:01:03.787808 22626471084160 run.py:483] Algo bellman_ford step 5545 current loss 0.243629, current_train_items 177472.
I0302 19:01:03.804504 22626471084160 run.py:483] Algo bellman_ford step 5546 current loss 0.543053, current_train_items 177504.
I0302 19:01:03.828066 22626471084160 run.py:483] Algo bellman_ford step 5547 current loss 0.605760, current_train_items 177536.
I0302 19:01:03.858675 22626471084160 run.py:483] Algo bellman_ford step 5548 current loss 0.606351, current_train_items 177568.
I0302 19:01:03.890958 22626471084160 run.py:483] Algo bellman_ford step 5549 current loss 0.893603, current_train_items 177600.
I0302 19:01:03.910789 22626471084160 run.py:483] Algo bellman_ford step 5550 current loss 0.262002, current_train_items 177632.
I0302 19:01:03.918816 22626471084160 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0302 19:01:03.918921 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:01:03.935890 22626471084160 run.py:483] Algo bellman_ford step 5551 current loss 0.405416, current_train_items 177664.
I0302 19:01:03.961139 22626471084160 run.py:483] Algo bellman_ford step 5552 current loss 0.643200, current_train_items 177696.
I0302 19:01:03.991944 22626471084160 run.py:483] Algo bellman_ford step 5553 current loss 0.692061, current_train_items 177728.
I0302 19:01:04.028935 22626471084160 run.py:483] Algo bellman_ford step 5554 current loss 0.843172, current_train_items 177760.
I0302 19:01:04.049460 22626471084160 run.py:483] Algo bellman_ford step 5555 current loss 0.274606, current_train_items 177792.
I0302 19:01:04.065293 22626471084160 run.py:483] Algo bellman_ford step 5556 current loss 0.380734, current_train_items 177824.
I0302 19:01:04.089452 22626471084160 run.py:483] Algo bellman_ford step 5557 current loss 0.702488, current_train_items 177856.
I0302 19:01:04.120022 22626471084160 run.py:483] Algo bellman_ford step 5558 current loss 0.708159, current_train_items 177888.
I0302 19:01:04.154322 22626471084160 run.py:483] Algo bellman_ford step 5559 current loss 0.833322, current_train_items 177920.
I0302 19:01:04.174216 22626471084160 run.py:483] Algo bellman_ford step 5560 current loss 0.295737, current_train_items 177952.
I0302 19:01:04.190275 22626471084160 run.py:483] Algo bellman_ford step 5561 current loss 0.499817, current_train_items 177984.
I0302 19:01:04.213419 22626471084160 run.py:483] Algo bellman_ford step 5562 current loss 0.624867, current_train_items 178016.
I0302 19:01:04.244790 22626471084160 run.py:483] Algo bellman_ford step 5563 current loss 0.679090, current_train_items 178048.
I0302 19:01:04.279847 22626471084160 run.py:483] Algo bellman_ford step 5564 current loss 0.788799, current_train_items 178080.
I0302 19:01:04.299773 22626471084160 run.py:483] Algo bellman_ford step 5565 current loss 0.426288, current_train_items 178112.
I0302 19:01:04.316041 22626471084160 run.py:483] Algo bellman_ford step 5566 current loss 0.450108, current_train_items 178144.
I0302 19:01:04.340011 22626471084160 run.py:483] Algo bellman_ford step 5567 current loss 0.615118, current_train_items 178176.
I0302 19:01:04.372233 22626471084160 run.py:483] Algo bellman_ford step 5568 current loss 0.810950, current_train_items 178208.
I0302 19:01:04.406483 22626471084160 run.py:483] Algo bellman_ford step 5569 current loss 0.925129, current_train_items 178240.
I0302 19:01:04.426579 22626471084160 run.py:483] Algo bellman_ford step 5570 current loss 0.375118, current_train_items 178272.
I0302 19:01:04.442610 22626471084160 run.py:483] Algo bellman_ford step 5571 current loss 0.477173, current_train_items 178304.
I0302 19:01:04.465769 22626471084160 run.py:483] Algo bellman_ford step 5572 current loss 0.714798, current_train_items 178336.
I0302 19:01:04.496626 22626471084160 run.py:483] Algo bellman_ford step 5573 current loss 0.698270, current_train_items 178368.
I0302 19:01:04.530520 22626471084160 run.py:483] Algo bellman_ford step 5574 current loss 0.874151, current_train_items 178400.
I0302 19:01:04.550595 22626471084160 run.py:483] Algo bellman_ford step 5575 current loss 0.346350, current_train_items 178432.
I0302 19:01:04.566686 22626471084160 run.py:483] Algo bellman_ford step 5576 current loss 0.604044, current_train_items 178464.
I0302 19:01:04.589495 22626471084160 run.py:483] Algo bellman_ford step 5577 current loss 0.638547, current_train_items 178496.
I0302 19:01:04.620616 22626471084160 run.py:483] Algo bellman_ford step 5578 current loss 0.699395, current_train_items 178528.
I0302 19:01:04.656061 22626471084160 run.py:483] Algo bellman_ford step 5579 current loss 0.843298, current_train_items 178560.
I0302 19:01:04.675686 22626471084160 run.py:483] Algo bellman_ford step 5580 current loss 0.309732, current_train_items 178592.
I0302 19:01:04.691447 22626471084160 run.py:483] Algo bellman_ford step 5581 current loss 0.409002, current_train_items 178624.
I0302 19:01:04.716070 22626471084160 run.py:483] Algo bellman_ford step 5582 current loss 0.573989, current_train_items 178656.
I0302 19:01:04.746721 22626471084160 run.py:483] Algo bellman_ford step 5583 current loss 0.628613, current_train_items 178688.
I0302 19:01:04.780900 22626471084160 run.py:483] Algo bellman_ford step 5584 current loss 0.739316, current_train_items 178720.
I0302 19:01:04.801025 22626471084160 run.py:483] Algo bellman_ford step 5585 current loss 0.345377, current_train_items 178752.
I0302 19:01:04.817268 22626471084160 run.py:483] Algo bellman_ford step 5586 current loss 0.450916, current_train_items 178784.
I0302 19:01:04.842024 22626471084160 run.py:483] Algo bellman_ford step 5587 current loss 0.680521, current_train_items 178816.
I0302 19:01:04.874063 22626471084160 run.py:483] Algo bellman_ford step 5588 current loss 0.716980, current_train_items 178848.
I0302 19:01:04.905619 22626471084160 run.py:483] Algo bellman_ford step 5589 current loss 0.673618, current_train_items 178880.
I0302 19:01:04.925506 22626471084160 run.py:483] Algo bellman_ford step 5590 current loss 0.339336, current_train_items 178912.
I0302 19:01:04.941651 22626471084160 run.py:483] Algo bellman_ford step 5591 current loss 0.432522, current_train_items 178944.
I0302 19:01:04.963950 22626471084160 run.py:483] Algo bellman_ford step 5592 current loss 0.664651, current_train_items 178976.
I0302 19:01:04.994921 22626471084160 run.py:483] Algo bellman_ford step 5593 current loss 0.738356, current_train_items 179008.
I0302 19:01:05.029486 22626471084160 run.py:483] Algo bellman_ford step 5594 current loss 0.677751, current_train_items 179040.
I0302 19:01:05.049131 22626471084160 run.py:483] Algo bellman_ford step 5595 current loss 0.407565, current_train_items 179072.
I0302 19:01:05.065203 22626471084160 run.py:483] Algo bellman_ford step 5596 current loss 0.490779, current_train_items 179104.
I0302 19:01:05.088837 22626471084160 run.py:483] Algo bellman_ford step 5597 current loss 0.779149, current_train_items 179136.
I0302 19:01:05.119800 22626471084160 run.py:483] Algo bellman_ford step 5598 current loss 0.733709, current_train_items 179168.
I0302 19:01:05.149739 22626471084160 run.py:483] Algo bellman_ford step 5599 current loss 0.720011, current_train_items 179200.
I0302 19:01:05.169882 22626471084160 run.py:483] Algo bellman_ford step 5600 current loss 0.313599, current_train_items 179232.
I0302 19:01:05.177720 22626471084160 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0302 19:01:05.177826 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 19:01:05.194646 22626471084160 run.py:483] Algo bellman_ford step 5601 current loss 0.468769, current_train_items 179264.
I0302 19:01:05.218959 22626471084160 run.py:483] Algo bellman_ford step 5602 current loss 0.618655, current_train_items 179296.
I0302 19:01:05.250231 22626471084160 run.py:483] Algo bellman_ford step 5603 current loss 0.840116, current_train_items 179328.
I0302 19:01:05.282889 22626471084160 run.py:483] Algo bellman_ford step 5604 current loss 0.819457, current_train_items 179360.
I0302 19:01:05.302900 22626471084160 run.py:483] Algo bellman_ford step 5605 current loss 0.273471, current_train_items 179392.
I0302 19:01:05.318641 22626471084160 run.py:483] Algo bellman_ford step 5606 current loss 0.509644, current_train_items 179424.
I0302 19:01:05.342603 22626471084160 run.py:483] Algo bellman_ford step 5607 current loss 0.619519, current_train_items 179456.
I0302 19:01:05.372184 22626471084160 run.py:483] Algo bellman_ford step 5608 current loss 0.637060, current_train_items 179488.
I0302 19:01:05.406430 22626471084160 run.py:483] Algo bellman_ford step 5609 current loss 0.811621, current_train_items 179520.
I0302 19:01:05.426131 22626471084160 run.py:483] Algo bellman_ford step 5610 current loss 0.244365, current_train_items 179552.
I0302 19:01:05.442390 22626471084160 run.py:483] Algo bellman_ford step 5611 current loss 0.464105, current_train_items 179584.
I0302 19:01:05.465604 22626471084160 run.py:483] Algo bellman_ford step 5612 current loss 0.603830, current_train_items 179616.
I0302 19:01:05.497241 22626471084160 run.py:483] Algo bellman_ford step 5613 current loss 0.693712, current_train_items 179648.
I0302 19:01:05.529682 22626471084160 run.py:483] Algo bellman_ford step 5614 current loss 0.782637, current_train_items 179680.
I0302 19:01:05.549222 22626471084160 run.py:483] Algo bellman_ford step 5615 current loss 0.275408, current_train_items 179712.
I0302 19:01:05.565729 22626471084160 run.py:483] Algo bellman_ford step 5616 current loss 0.583952, current_train_items 179744.
I0302 19:01:05.589453 22626471084160 run.py:483] Algo bellman_ford step 5617 current loss 0.571213, current_train_items 179776.
I0302 19:01:05.619977 22626471084160 run.py:483] Algo bellman_ford step 5618 current loss 0.698266, current_train_items 179808.
I0302 19:01:05.654623 22626471084160 run.py:483] Algo bellman_ford step 5619 current loss 0.802013, current_train_items 179840.
I0302 19:01:05.674334 22626471084160 run.py:483] Algo bellman_ford step 5620 current loss 0.296755, current_train_items 179872.
I0302 19:01:05.690479 22626471084160 run.py:483] Algo bellman_ford step 5621 current loss 0.495836, current_train_items 179904.
I0302 19:01:05.713871 22626471084160 run.py:483] Algo bellman_ford step 5622 current loss 0.630810, current_train_items 179936.
I0302 19:01:05.744454 22626471084160 run.py:483] Algo bellman_ford step 5623 current loss 0.687142, current_train_items 179968.
I0302 19:01:05.778951 22626471084160 run.py:483] Algo bellman_ford step 5624 current loss 0.691486, current_train_items 180000.
I0302 19:01:05.798494 22626471084160 run.py:483] Algo bellman_ford step 5625 current loss 0.269231, current_train_items 180032.
I0302 19:01:05.815092 22626471084160 run.py:483] Algo bellman_ford step 5626 current loss 0.562387, current_train_items 180064.
I0302 19:01:05.838924 22626471084160 run.py:483] Algo bellman_ford step 5627 current loss 0.684086, current_train_items 180096.
I0302 19:01:05.870300 22626471084160 run.py:483] Algo bellman_ford step 5628 current loss 0.674016, current_train_items 180128.
I0302 19:01:05.904608 22626471084160 run.py:483] Algo bellman_ford step 5629 current loss 0.821146, current_train_items 180160.
I0302 19:01:05.923962 22626471084160 run.py:483] Algo bellman_ford step 5630 current loss 0.338283, current_train_items 180192.
I0302 19:01:05.939992 22626471084160 run.py:483] Algo bellman_ford step 5631 current loss 0.615348, current_train_items 180224.
I0302 19:01:05.964200 22626471084160 run.py:483] Algo bellman_ford step 5632 current loss 2.568054, current_train_items 180256.
I0302 19:01:05.994921 22626471084160 run.py:483] Algo bellman_ford step 5633 current loss 4.539210, current_train_items 180288.
I0302 19:01:06.029137 22626471084160 run.py:483] Algo bellman_ford step 5634 current loss 1.073548, current_train_items 180320.
I0302 19:01:06.048987 22626471084160 run.py:483] Algo bellman_ford step 5635 current loss 0.311665, current_train_items 180352.
I0302 19:01:06.065308 22626471084160 run.py:483] Algo bellman_ford step 5636 current loss 0.488444, current_train_items 180384.
I0302 19:01:06.088748 22626471084160 run.py:483] Algo bellman_ford step 5637 current loss 0.658050, current_train_items 180416.
I0302 19:01:06.118556 22626471084160 run.py:483] Algo bellman_ford step 5638 current loss 0.638262, current_train_items 180448.
I0302 19:01:06.150861 22626471084160 run.py:483] Algo bellman_ford step 5639 current loss 0.833015, current_train_items 180480.
I0302 19:01:06.170425 22626471084160 run.py:483] Algo bellman_ford step 5640 current loss 0.309293, current_train_items 180512.
I0302 19:01:06.186389 22626471084160 run.py:483] Algo bellman_ford step 5641 current loss 0.428624, current_train_items 180544.
I0302 19:01:06.209898 22626471084160 run.py:483] Algo bellman_ford step 5642 current loss 0.651176, current_train_items 180576.
I0302 19:01:06.240885 22626471084160 run.py:483] Algo bellman_ford step 5643 current loss 0.796330, current_train_items 180608.
I0302 19:01:06.273913 22626471084160 run.py:483] Algo bellman_ford step 5644 current loss 0.882685, current_train_items 180640.
I0302 19:01:06.293564 22626471084160 run.py:483] Algo bellman_ford step 5645 current loss 0.305141, current_train_items 180672.
I0302 19:01:06.310266 22626471084160 run.py:483] Algo bellman_ford step 5646 current loss 0.537040, current_train_items 180704.
I0302 19:01:06.333246 22626471084160 run.py:483] Algo bellman_ford step 5647 current loss 0.665587, current_train_items 180736.
I0302 19:01:06.364026 22626471084160 run.py:483] Algo bellman_ford step 5648 current loss 0.647840, current_train_items 180768.
I0302 19:01:06.399350 22626471084160 run.py:483] Algo bellman_ford step 5649 current loss 0.845192, current_train_items 180800.
I0302 19:01:06.418988 22626471084160 run.py:483] Algo bellman_ford step 5650 current loss 0.312941, current_train_items 180832.
I0302 19:01:06.426825 22626471084160 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0302 19:01:06.426930 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:01:06.443910 22626471084160 run.py:483] Algo bellman_ford step 5651 current loss 0.501130, current_train_items 180864.
I0302 19:01:06.467456 22626471084160 run.py:483] Algo bellman_ford step 5652 current loss 0.622915, current_train_items 180896.
I0302 19:01:06.500623 22626471084160 run.py:483] Algo bellman_ford step 5653 current loss 0.761580, current_train_items 180928.
I0302 19:01:06.536115 22626471084160 run.py:483] Algo bellman_ford step 5654 current loss 0.925230, current_train_items 180960.
I0302 19:01:06.556309 22626471084160 run.py:483] Algo bellman_ford step 5655 current loss 0.303906, current_train_items 180992.
I0302 19:01:06.572605 22626471084160 run.py:483] Algo bellman_ford step 5656 current loss 0.480025, current_train_items 181024.
I0302 19:01:06.595199 22626471084160 run.py:483] Algo bellman_ford step 5657 current loss 0.681242, current_train_items 181056.
I0302 19:01:06.627447 22626471084160 run.py:483] Algo bellman_ford step 5658 current loss 0.749630, current_train_items 181088.
I0302 19:01:06.661567 22626471084160 run.py:483] Algo bellman_ford step 5659 current loss 0.807340, current_train_items 181120.
I0302 19:01:06.681727 22626471084160 run.py:483] Algo bellman_ford step 5660 current loss 0.377214, current_train_items 181152.
I0302 19:01:06.697703 22626471084160 run.py:483] Algo bellman_ford step 5661 current loss 0.407802, current_train_items 181184.
I0302 19:01:06.720588 22626471084160 run.py:483] Algo bellman_ford step 5662 current loss 0.624550, current_train_items 181216.
I0302 19:01:06.753807 22626471084160 run.py:483] Algo bellman_ford step 5663 current loss 0.779455, current_train_items 181248.
I0302 19:01:06.787945 22626471084160 run.py:483] Algo bellman_ford step 5664 current loss 0.738041, current_train_items 181280.
I0302 19:01:06.807602 22626471084160 run.py:483] Algo bellman_ford step 5665 current loss 0.300251, current_train_items 181312.
I0302 19:01:06.824286 22626471084160 run.py:483] Algo bellman_ford step 5666 current loss 0.472416, current_train_items 181344.
I0302 19:01:06.847609 22626471084160 run.py:483] Algo bellman_ford step 5667 current loss 0.606062, current_train_items 181376.
I0302 19:01:06.879575 22626471084160 run.py:483] Algo bellman_ford step 5668 current loss 0.834066, current_train_items 181408.
I0302 19:01:06.912249 22626471084160 run.py:483] Algo bellman_ford step 5669 current loss 0.743988, current_train_items 181440.
I0302 19:01:06.932069 22626471084160 run.py:483] Algo bellman_ford step 5670 current loss 0.270794, current_train_items 181472.
I0302 19:01:06.948233 22626471084160 run.py:483] Algo bellman_ford step 5671 current loss 0.434106, current_train_items 181504.
I0302 19:01:06.971220 22626471084160 run.py:483] Algo bellman_ford step 5672 current loss 0.609777, current_train_items 181536.
I0302 19:01:07.002255 22626471084160 run.py:483] Algo bellman_ford step 5673 current loss 0.708950, current_train_items 181568.
I0302 19:01:07.035432 22626471084160 run.py:483] Algo bellman_ford step 5674 current loss 0.686972, current_train_items 181600.
I0302 19:01:07.055193 22626471084160 run.py:483] Algo bellman_ford step 5675 current loss 0.293333, current_train_items 181632.
I0302 19:01:07.071199 22626471084160 run.py:483] Algo bellman_ford step 5676 current loss 0.447848, current_train_items 181664.
I0302 19:01:07.094938 22626471084160 run.py:483] Algo bellman_ford step 5677 current loss 0.705312, current_train_items 181696.
I0302 19:01:07.127744 22626471084160 run.py:483] Algo bellman_ford step 5678 current loss 0.848141, current_train_items 181728.
I0302 19:01:07.161252 22626471084160 run.py:483] Algo bellman_ford step 5679 current loss 0.727787, current_train_items 181760.
I0302 19:01:07.181159 22626471084160 run.py:483] Algo bellman_ford step 5680 current loss 0.378954, current_train_items 181792.
I0302 19:01:07.197420 22626471084160 run.py:483] Algo bellman_ford step 5681 current loss 0.528826, current_train_items 181824.
I0302 19:01:07.221041 22626471084160 run.py:483] Algo bellman_ford step 5682 current loss 0.677844, current_train_items 181856.
I0302 19:01:07.251641 22626471084160 run.py:483] Algo bellman_ford step 5683 current loss 0.625738, current_train_items 181888.
I0302 19:01:07.284272 22626471084160 run.py:483] Algo bellman_ford step 5684 current loss 0.681120, current_train_items 181920.
I0302 19:01:07.304128 22626471084160 run.py:483] Algo bellman_ford step 5685 current loss 0.254671, current_train_items 181952.
I0302 19:01:07.320355 22626471084160 run.py:483] Algo bellman_ford step 5686 current loss 0.544436, current_train_items 181984.
I0302 19:01:07.343898 22626471084160 run.py:483] Algo bellman_ford step 5687 current loss 0.625653, current_train_items 182016.
I0302 19:01:07.375353 22626471084160 run.py:483] Algo bellman_ford step 5688 current loss 0.688044, current_train_items 182048.
I0302 19:01:07.408454 22626471084160 run.py:483] Algo bellman_ford step 5689 current loss 0.776414, current_train_items 182080.
I0302 19:01:07.428200 22626471084160 run.py:483] Algo bellman_ford step 5690 current loss 0.289038, current_train_items 182112.
I0302 19:01:07.445005 22626471084160 run.py:483] Algo bellman_ford step 5691 current loss 0.465926, current_train_items 182144.
I0302 19:01:07.468487 22626471084160 run.py:483] Algo bellman_ford step 5692 current loss 0.574511, current_train_items 182176.
I0302 19:01:07.499399 22626471084160 run.py:483] Algo bellman_ford step 5693 current loss 0.688458, current_train_items 182208.
I0302 19:01:07.532064 22626471084160 run.py:483] Algo bellman_ford step 5694 current loss 0.799000, current_train_items 182240.
I0302 19:01:07.551954 22626471084160 run.py:483] Algo bellman_ford step 5695 current loss 0.278566, current_train_items 182272.
I0302 19:01:07.568623 22626471084160 run.py:483] Algo bellman_ford step 5696 current loss 0.494800, current_train_items 182304.
I0302 19:01:07.591876 22626471084160 run.py:483] Algo bellman_ford step 5697 current loss 0.668247, current_train_items 182336.
I0302 19:01:07.623546 22626471084160 run.py:483] Algo bellman_ford step 5698 current loss 0.810160, current_train_items 182368.
I0302 19:01:07.657732 22626471084160 run.py:483] Algo bellman_ford step 5699 current loss 0.784397, current_train_items 182400.
I0302 19:01:07.677693 22626471084160 run.py:483] Algo bellman_ford step 5700 current loss 0.347660, current_train_items 182432.
I0302 19:01:07.685518 22626471084160 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0302 19:01:07.685631 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:01:07.702044 22626471084160 run.py:483] Algo bellman_ford step 5701 current loss 0.446058, current_train_items 182464.
I0302 19:01:07.724873 22626471084160 run.py:483] Algo bellman_ford step 5702 current loss 0.567666, current_train_items 182496.
I0302 19:01:07.756380 22626471084160 run.py:483] Algo bellman_ford step 5703 current loss 0.739031, current_train_items 182528.
I0302 19:01:07.791626 22626471084160 run.py:483] Algo bellman_ford step 5704 current loss 0.941838, current_train_items 182560.
I0302 19:01:07.811712 22626471084160 run.py:483] Algo bellman_ford step 5705 current loss 0.304256, current_train_items 182592.
I0302 19:01:07.827470 22626471084160 run.py:483] Algo bellman_ford step 5706 current loss 0.436444, current_train_items 182624.
I0302 19:01:07.851434 22626471084160 run.py:483] Algo bellman_ford step 5707 current loss 0.676537, current_train_items 182656.
I0302 19:01:07.881876 22626471084160 run.py:483] Algo bellman_ford step 5708 current loss 0.764927, current_train_items 182688.
I0302 19:01:07.915807 22626471084160 run.py:483] Algo bellman_ford step 5709 current loss 0.861173, current_train_items 182720.
I0302 19:01:07.935570 22626471084160 run.py:483] Algo bellman_ford step 5710 current loss 0.281716, current_train_items 182752.
I0302 19:01:07.951522 22626471084160 run.py:483] Algo bellman_ford step 5711 current loss 0.552415, current_train_items 182784.
I0302 19:01:07.974386 22626471084160 run.py:483] Algo bellman_ford step 5712 current loss 0.633527, current_train_items 182816.
I0302 19:01:08.004312 22626471084160 run.py:483] Algo bellman_ford step 5713 current loss 0.687310, current_train_items 182848.
I0302 19:01:08.037166 22626471084160 run.py:483] Algo bellman_ford step 5714 current loss 0.763901, current_train_items 182880.
I0302 19:01:08.056835 22626471084160 run.py:483] Algo bellman_ford step 5715 current loss 0.405705, current_train_items 182912.
I0302 19:01:08.073080 22626471084160 run.py:483] Algo bellman_ford step 5716 current loss 0.525855, current_train_items 182944.
I0302 19:01:08.097266 22626471084160 run.py:483] Algo bellman_ford step 5717 current loss 0.676852, current_train_items 182976.
I0302 19:01:08.127367 22626471084160 run.py:483] Algo bellman_ford step 5718 current loss 0.823973, current_train_items 183008.
I0302 19:01:08.161222 22626471084160 run.py:483] Algo bellman_ford step 5719 current loss 0.853632, current_train_items 183040.
I0302 19:01:08.180656 22626471084160 run.py:483] Algo bellman_ford step 5720 current loss 0.256562, current_train_items 183072.
I0302 19:01:08.196807 22626471084160 run.py:483] Algo bellman_ford step 5721 current loss 0.496385, current_train_items 183104.
I0302 19:01:08.219641 22626471084160 run.py:483] Algo bellman_ford step 5722 current loss 0.648955, current_train_items 183136.
I0302 19:01:08.249848 22626471084160 run.py:483] Algo bellman_ford step 5723 current loss 0.709924, current_train_items 183168.
I0302 19:01:08.282573 22626471084160 run.py:483] Algo bellman_ford step 5724 current loss 0.740363, current_train_items 183200.
I0302 19:01:08.301965 22626471084160 run.py:483] Algo bellman_ford step 5725 current loss 0.340916, current_train_items 183232.
I0302 19:01:08.318281 22626471084160 run.py:483] Algo bellman_ford step 5726 current loss 0.483645, current_train_items 183264.
I0302 19:01:08.342529 22626471084160 run.py:483] Algo bellman_ford step 5727 current loss 0.607179, current_train_items 183296.
I0302 19:01:08.372685 22626471084160 run.py:483] Algo bellman_ford step 5728 current loss 0.668380, current_train_items 183328.
I0302 19:01:08.405989 22626471084160 run.py:483] Algo bellman_ford step 5729 current loss 0.737741, current_train_items 183360.
I0302 19:01:08.425420 22626471084160 run.py:483] Algo bellman_ford step 5730 current loss 0.317604, current_train_items 183392.
I0302 19:01:08.441878 22626471084160 run.py:483] Algo bellman_ford step 5731 current loss 0.518873, current_train_items 183424.
I0302 19:01:08.465956 22626471084160 run.py:483] Algo bellman_ford step 5732 current loss 0.698453, current_train_items 183456.
I0302 19:01:08.496167 22626471084160 run.py:483] Algo bellman_ford step 5733 current loss 0.712232, current_train_items 183488.
I0302 19:01:08.530207 22626471084160 run.py:483] Algo bellman_ford step 5734 current loss 0.810833, current_train_items 183520.
I0302 19:01:08.549906 22626471084160 run.py:483] Algo bellman_ford step 5735 current loss 0.264674, current_train_items 183552.
I0302 19:01:08.566414 22626471084160 run.py:483] Algo bellman_ford step 5736 current loss 0.479993, current_train_items 183584.
I0302 19:01:08.589655 22626471084160 run.py:483] Algo bellman_ford step 5737 current loss 0.681803, current_train_items 183616.
I0302 19:01:08.619781 22626471084160 run.py:483] Algo bellman_ford step 5738 current loss 0.629118, current_train_items 183648.
I0302 19:01:08.653500 22626471084160 run.py:483] Algo bellman_ford step 5739 current loss 0.701102, current_train_items 183680.
I0302 19:01:08.672974 22626471084160 run.py:483] Algo bellman_ford step 5740 current loss 0.390322, current_train_items 183712.
I0302 19:01:08.689449 22626471084160 run.py:483] Algo bellman_ford step 5741 current loss 0.473408, current_train_items 183744.
I0302 19:01:08.713486 22626471084160 run.py:483] Algo bellman_ford step 5742 current loss 0.576473, current_train_items 183776.
I0302 19:01:08.744802 22626471084160 run.py:483] Algo bellman_ford step 5743 current loss 0.695790, current_train_items 183808.
I0302 19:01:08.778959 22626471084160 run.py:483] Algo bellman_ford step 5744 current loss 0.876187, current_train_items 183840.
I0302 19:01:08.798360 22626471084160 run.py:483] Algo bellman_ford step 5745 current loss 0.379588, current_train_items 183872.
I0302 19:01:08.814947 22626471084160 run.py:483] Algo bellman_ford step 5746 current loss 0.470975, current_train_items 183904.
I0302 19:01:08.838586 22626471084160 run.py:483] Algo bellman_ford step 5747 current loss 0.632109, current_train_items 183936.
I0302 19:01:08.869797 22626471084160 run.py:483] Algo bellman_ford step 5748 current loss 0.765392, current_train_items 183968.
I0302 19:01:08.902629 22626471084160 run.py:483] Algo bellman_ford step 5749 current loss 0.811999, current_train_items 184000.
I0302 19:01:08.922111 22626471084160 run.py:483] Algo bellman_ford step 5750 current loss 0.435328, current_train_items 184032.
I0302 19:01:08.930293 22626471084160 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0302 19:01:08.930399 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:01:08.947668 22626471084160 run.py:483] Algo bellman_ford step 5751 current loss 0.492654, current_train_items 184064.
I0302 19:01:08.971115 22626471084160 run.py:483] Algo bellman_ford step 5752 current loss 0.712967, current_train_items 184096.
I0302 19:01:09.001752 22626471084160 run.py:483] Algo bellman_ford step 5753 current loss 0.641114, current_train_items 184128.
I0302 19:01:09.037218 22626471084160 run.py:483] Algo bellman_ford step 5754 current loss 1.180173, current_train_items 184160.
I0302 19:01:09.057471 22626471084160 run.py:483] Algo bellman_ford step 5755 current loss 0.296692, current_train_items 184192.
I0302 19:01:09.073229 22626471084160 run.py:483] Algo bellman_ford step 5756 current loss 0.433733, current_train_items 184224.
I0302 19:01:09.097288 22626471084160 run.py:483] Algo bellman_ford step 5757 current loss 0.761374, current_train_items 184256.
I0302 19:01:09.129349 22626471084160 run.py:483] Algo bellman_ford step 5758 current loss 0.760093, current_train_items 184288.
I0302 19:01:09.163348 22626471084160 run.py:483] Algo bellman_ford step 5759 current loss 0.882343, current_train_items 184320.
I0302 19:01:09.183135 22626471084160 run.py:483] Algo bellman_ford step 5760 current loss 0.442557, current_train_items 184352.
I0302 19:01:09.199643 22626471084160 run.py:483] Algo bellman_ford step 5761 current loss 0.477987, current_train_items 184384.
I0302 19:01:09.223219 22626471084160 run.py:483] Algo bellman_ford step 5762 current loss 0.558161, current_train_items 184416.
I0302 19:01:09.253368 22626471084160 run.py:483] Algo bellman_ford step 5763 current loss 0.660162, current_train_items 184448.
I0302 19:01:09.287907 22626471084160 run.py:483] Algo bellman_ford step 5764 current loss 0.857005, current_train_items 184480.
I0302 19:01:09.307900 22626471084160 run.py:483] Algo bellman_ford step 5765 current loss 0.332433, current_train_items 184512.
I0302 19:01:09.324237 22626471084160 run.py:483] Algo bellman_ford step 5766 current loss 0.515023, current_train_items 184544.
I0302 19:01:09.347885 22626471084160 run.py:483] Algo bellman_ford step 5767 current loss 0.667312, current_train_items 184576.
I0302 19:01:09.379857 22626471084160 run.py:483] Algo bellman_ford step 5768 current loss 0.765305, current_train_items 184608.
I0302 19:01:09.413164 22626471084160 run.py:483] Algo bellman_ford step 5769 current loss 0.818384, current_train_items 184640.
I0302 19:01:09.433059 22626471084160 run.py:483] Algo bellman_ford step 5770 current loss 0.294530, current_train_items 184672.
I0302 19:01:09.449239 22626471084160 run.py:483] Algo bellman_ford step 5771 current loss 0.510075, current_train_items 184704.
I0302 19:01:09.471474 22626471084160 run.py:483] Algo bellman_ford step 5772 current loss 0.649758, current_train_items 184736.
I0302 19:01:09.502130 22626471084160 run.py:483] Algo bellman_ford step 5773 current loss 0.754289, current_train_items 184768.
I0302 19:01:09.534884 22626471084160 run.py:483] Algo bellman_ford step 5774 current loss 0.673028, current_train_items 184800.
I0302 19:01:09.554820 22626471084160 run.py:483] Algo bellman_ford step 5775 current loss 0.364634, current_train_items 184832.
I0302 19:01:09.571359 22626471084160 run.py:483] Algo bellman_ford step 5776 current loss 0.459599, current_train_items 184864.
I0302 19:01:09.594028 22626471084160 run.py:483] Algo bellman_ford step 5777 current loss 0.605344, current_train_items 184896.
I0302 19:01:09.625406 22626471084160 run.py:483] Algo bellman_ford step 5778 current loss 0.752614, current_train_items 184928.
I0302 19:01:09.659364 22626471084160 run.py:483] Algo bellman_ford step 5779 current loss 0.904721, current_train_items 184960.
I0302 19:01:09.678888 22626471084160 run.py:483] Algo bellman_ford step 5780 current loss 0.282835, current_train_items 184992.
I0302 19:01:09.695074 22626471084160 run.py:483] Algo bellman_ford step 5781 current loss 0.450794, current_train_items 185024.
I0302 19:01:09.719292 22626471084160 run.py:483] Algo bellman_ford step 5782 current loss 0.687314, current_train_items 185056.
I0302 19:01:09.749358 22626471084160 run.py:483] Algo bellman_ford step 5783 current loss 0.575835, current_train_items 185088.
I0302 19:01:09.781345 22626471084160 run.py:483] Algo bellman_ford step 5784 current loss 0.799662, current_train_items 185120.
I0302 19:01:09.801339 22626471084160 run.py:483] Algo bellman_ford step 5785 current loss 0.368789, current_train_items 185152.
I0302 19:01:09.817314 22626471084160 run.py:483] Algo bellman_ford step 5786 current loss 0.540209, current_train_items 185184.
I0302 19:01:09.839567 22626471084160 run.py:483] Algo bellman_ford step 5787 current loss 0.557121, current_train_items 185216.
I0302 19:01:09.871372 22626471084160 run.py:483] Algo bellman_ford step 5788 current loss 0.745048, current_train_items 185248.
I0302 19:01:09.904043 22626471084160 run.py:483] Algo bellman_ford step 5789 current loss 0.873537, current_train_items 185280.
I0302 19:01:09.923815 22626471084160 run.py:483] Algo bellman_ford step 5790 current loss 0.322182, current_train_items 185312.
I0302 19:01:09.940353 22626471084160 run.py:483] Algo bellman_ford step 5791 current loss 0.635071, current_train_items 185344.
I0302 19:01:09.962529 22626471084160 run.py:483] Algo bellman_ford step 5792 current loss 0.594511, current_train_items 185376.
I0302 19:01:09.993467 22626471084160 run.py:483] Algo bellman_ford step 5793 current loss 0.675217, current_train_items 185408.
I0302 19:01:10.026899 22626471084160 run.py:483] Algo bellman_ford step 5794 current loss 0.735408, current_train_items 185440.
I0302 19:01:10.046694 22626471084160 run.py:483] Algo bellman_ford step 5795 current loss 0.296750, current_train_items 185472.
I0302 19:01:10.062745 22626471084160 run.py:483] Algo bellman_ford step 5796 current loss 0.416149, current_train_items 185504.
I0302 19:01:10.086764 22626471084160 run.py:483] Algo bellman_ford step 5797 current loss 0.692740, current_train_items 185536.
I0302 19:01:10.117939 22626471084160 run.py:483] Algo bellman_ford step 5798 current loss 0.582521, current_train_items 185568.
I0302 19:01:10.152468 22626471084160 run.py:483] Algo bellman_ford step 5799 current loss 0.880488, current_train_items 185600.
I0302 19:01:10.172364 22626471084160 run.py:483] Algo bellman_ford step 5800 current loss 0.326545, current_train_items 185632.
I0302 19:01:10.179968 22626471084160 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0302 19:01:10.180074 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:01:10.197016 22626471084160 run.py:483] Algo bellman_ford step 5801 current loss 0.567205, current_train_items 185664.
I0302 19:01:10.219888 22626471084160 run.py:483] Algo bellman_ford step 5802 current loss 0.610286, current_train_items 185696.
I0302 19:01:10.251242 22626471084160 run.py:483] Algo bellman_ford step 5803 current loss 0.822830, current_train_items 185728.
I0302 19:01:10.286881 22626471084160 run.py:483] Algo bellman_ford step 5804 current loss 0.810767, current_train_items 185760.
I0302 19:01:10.307046 22626471084160 run.py:483] Algo bellman_ford step 5805 current loss 0.316197, current_train_items 185792.
I0302 19:01:10.323192 22626471084160 run.py:483] Algo bellman_ford step 5806 current loss 0.409417, current_train_items 185824.
I0302 19:01:10.347643 22626471084160 run.py:483] Algo bellman_ford step 5807 current loss 0.623513, current_train_items 185856.
I0302 19:01:10.378871 22626471084160 run.py:483] Algo bellman_ford step 5808 current loss 0.673683, current_train_items 185888.
I0302 19:01:10.412173 22626471084160 run.py:483] Algo bellman_ford step 5809 current loss 0.676316, current_train_items 185920.
I0302 19:01:10.431497 22626471084160 run.py:483] Algo bellman_ford step 5810 current loss 0.276389, current_train_items 185952.
I0302 19:01:10.447529 22626471084160 run.py:483] Algo bellman_ford step 5811 current loss 0.464172, current_train_items 185984.
I0302 19:01:10.471417 22626471084160 run.py:483] Algo bellman_ford step 5812 current loss 0.616746, current_train_items 186016.
I0302 19:01:10.503400 22626471084160 run.py:483] Algo bellman_ford step 5813 current loss 0.750038, current_train_items 186048.
I0302 19:01:10.537782 22626471084160 run.py:483] Algo bellman_ford step 5814 current loss 0.876934, current_train_items 186080.
I0302 19:01:10.557694 22626471084160 run.py:483] Algo bellman_ford step 5815 current loss 0.342482, current_train_items 186112.
I0302 19:01:10.574142 22626471084160 run.py:483] Algo bellman_ford step 5816 current loss 0.464064, current_train_items 186144.
I0302 19:01:10.597775 22626471084160 run.py:483] Algo bellman_ford step 5817 current loss 0.643912, current_train_items 186176.
I0302 19:01:10.627968 22626471084160 run.py:483] Algo bellman_ford step 5818 current loss 0.639685, current_train_items 186208.
I0302 19:01:10.662405 22626471084160 run.py:483] Algo bellman_ford step 5819 current loss 0.777613, current_train_items 186240.
I0302 19:01:10.681895 22626471084160 run.py:483] Algo bellman_ford step 5820 current loss 0.319737, current_train_items 186272.
I0302 19:01:10.698067 22626471084160 run.py:483] Algo bellman_ford step 5821 current loss 0.434003, current_train_items 186304.
I0302 19:01:10.721823 22626471084160 run.py:483] Algo bellman_ford step 5822 current loss 0.638484, current_train_items 186336.
I0302 19:01:10.752730 22626471084160 run.py:483] Algo bellman_ford step 5823 current loss 0.789102, current_train_items 186368.
I0302 19:01:10.785645 22626471084160 run.py:483] Algo bellman_ford step 5824 current loss 0.905245, current_train_items 186400.
I0302 19:01:10.805233 22626471084160 run.py:483] Algo bellman_ford step 5825 current loss 0.291928, current_train_items 186432.
I0302 19:01:10.821601 22626471084160 run.py:483] Algo bellman_ford step 5826 current loss 0.505169, current_train_items 186464.
I0302 19:01:10.845734 22626471084160 run.py:483] Algo bellman_ford step 5827 current loss 0.639662, current_train_items 186496.
I0302 19:01:10.877228 22626471084160 run.py:483] Algo bellman_ford step 5828 current loss 0.661849, current_train_items 186528.
I0302 19:01:10.912418 22626471084160 run.py:483] Algo bellman_ford step 5829 current loss 0.939643, current_train_items 186560.
I0302 19:01:10.932199 22626471084160 run.py:483] Algo bellman_ford step 5830 current loss 0.300863, current_train_items 186592.
I0302 19:01:10.948701 22626471084160 run.py:483] Algo bellman_ford step 5831 current loss 0.474488, current_train_items 186624.
I0302 19:01:10.971771 22626471084160 run.py:483] Algo bellman_ford step 5832 current loss 0.568389, current_train_items 186656.
I0302 19:01:11.002716 22626471084160 run.py:483] Algo bellman_ford step 5833 current loss 0.681915, current_train_items 186688.
I0302 19:01:11.033331 22626471084160 run.py:483] Algo bellman_ford step 5834 current loss 0.645589, current_train_items 186720.
I0302 19:01:11.053077 22626471084160 run.py:483] Algo bellman_ford step 5835 current loss 0.298658, current_train_items 186752.
I0302 19:01:11.068716 22626471084160 run.py:483] Algo bellman_ford step 5836 current loss 0.461583, current_train_items 186784.
I0302 19:01:11.093820 22626471084160 run.py:483] Algo bellman_ford step 5837 current loss 0.775323, current_train_items 186816.
I0302 19:01:11.123374 22626471084160 run.py:483] Algo bellman_ford step 5838 current loss 0.541610, current_train_items 186848.
I0302 19:01:11.158379 22626471084160 run.py:483] Algo bellman_ford step 5839 current loss 0.900561, current_train_items 186880.
I0302 19:01:11.178011 22626471084160 run.py:483] Algo bellman_ford step 5840 current loss 0.313387, current_train_items 186912.
I0302 19:01:11.194491 22626471084160 run.py:483] Algo bellman_ford step 5841 current loss 0.517630, current_train_items 186944.
I0302 19:01:11.217434 22626471084160 run.py:483] Algo bellman_ford step 5842 current loss 0.659421, current_train_items 186976.
I0302 19:01:11.247084 22626471084160 run.py:483] Algo bellman_ford step 5843 current loss 0.631699, current_train_items 187008.
I0302 19:01:11.280207 22626471084160 run.py:483] Algo bellman_ford step 5844 current loss 0.710312, current_train_items 187040.
I0302 19:01:11.300176 22626471084160 run.py:483] Algo bellman_ford step 5845 current loss 0.309767, current_train_items 187072.
I0302 19:01:11.316514 22626471084160 run.py:483] Algo bellman_ford step 5846 current loss 0.433111, current_train_items 187104.
I0302 19:01:11.341280 22626471084160 run.py:483] Algo bellman_ford step 5847 current loss 0.680480, current_train_items 187136.
I0302 19:01:11.371010 22626471084160 run.py:483] Algo bellman_ford step 5848 current loss 0.701930, current_train_items 187168.
I0302 19:01:11.404857 22626471084160 run.py:483] Algo bellman_ford step 5849 current loss 0.794919, current_train_items 187200.
I0302 19:01:11.424271 22626471084160 run.py:483] Algo bellman_ford step 5850 current loss 0.246085, current_train_items 187232.
I0302 19:01:11.432168 22626471084160 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0302 19:01:11.432275 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:01:11.449269 22626471084160 run.py:483] Algo bellman_ford step 5851 current loss 0.529026, current_train_items 187264.
I0302 19:01:11.472832 22626471084160 run.py:483] Algo bellman_ford step 5852 current loss 0.634421, current_train_items 187296.
I0302 19:01:11.503326 22626471084160 run.py:483] Algo bellman_ford step 5853 current loss 0.539870, current_train_items 187328.
I0302 19:01:11.537733 22626471084160 run.py:483] Algo bellman_ford step 5854 current loss 0.797979, current_train_items 187360.
I0302 19:01:11.557574 22626471084160 run.py:483] Algo bellman_ford step 5855 current loss 0.321020, current_train_items 187392.
I0302 19:01:11.573488 22626471084160 run.py:483] Algo bellman_ford step 5856 current loss 0.550616, current_train_items 187424.
I0302 19:01:11.597678 22626471084160 run.py:483] Algo bellman_ford step 5857 current loss 0.625289, current_train_items 187456.
I0302 19:01:11.627784 22626471084160 run.py:483] Algo bellman_ford step 5858 current loss 0.627384, current_train_items 187488.
I0302 19:01:11.661677 22626471084160 run.py:483] Algo bellman_ford step 5859 current loss 0.775601, current_train_items 187520.
I0302 19:01:11.681788 22626471084160 run.py:483] Algo bellman_ford step 5860 current loss 0.280672, current_train_items 187552.
I0302 19:01:11.698222 22626471084160 run.py:483] Algo bellman_ford step 5861 current loss 0.426349, current_train_items 187584.
I0302 19:01:11.720883 22626471084160 run.py:483] Algo bellman_ford step 5862 current loss 0.606435, current_train_items 187616.
I0302 19:01:11.752359 22626471084160 run.py:483] Algo bellman_ford step 5863 current loss 0.747070, current_train_items 187648.
I0302 19:01:11.786851 22626471084160 run.py:483] Algo bellman_ford step 5864 current loss 0.893081, current_train_items 187680.
I0302 19:01:11.806209 22626471084160 run.py:483] Algo bellman_ford step 5865 current loss 0.348948, current_train_items 187712.
I0302 19:01:11.823168 22626471084160 run.py:483] Algo bellman_ford step 5866 current loss 0.495358, current_train_items 187744.
I0302 19:01:11.846967 22626471084160 run.py:483] Algo bellman_ford step 5867 current loss 0.626128, current_train_items 187776.
I0302 19:01:11.877313 22626471084160 run.py:483] Algo bellman_ford step 5868 current loss 0.858758, current_train_items 187808.
I0302 19:01:11.912533 22626471084160 run.py:483] Algo bellman_ford step 5869 current loss 0.880449, current_train_items 187840.
I0302 19:01:11.932853 22626471084160 run.py:483] Algo bellman_ford step 5870 current loss 0.276773, current_train_items 187872.
I0302 19:01:11.949207 22626471084160 run.py:483] Algo bellman_ford step 5871 current loss 0.403679, current_train_items 187904.
I0302 19:01:11.971390 22626471084160 run.py:483] Algo bellman_ford step 5872 current loss 0.531401, current_train_items 187936.
I0302 19:01:12.002008 22626471084160 run.py:483] Algo bellman_ford step 5873 current loss 0.657414, current_train_items 187968.
I0302 19:01:12.035782 22626471084160 run.py:483] Algo bellman_ford step 5874 current loss 0.782253, current_train_items 188000.
I0302 19:01:12.055824 22626471084160 run.py:483] Algo bellman_ford step 5875 current loss 0.312209, current_train_items 188032.
I0302 19:01:12.072035 22626471084160 run.py:483] Algo bellman_ford step 5876 current loss 0.459323, current_train_items 188064.
I0302 19:01:12.094590 22626471084160 run.py:483] Algo bellman_ford step 5877 current loss 0.565392, current_train_items 188096.
I0302 19:01:12.125301 22626471084160 run.py:483] Algo bellman_ford step 5878 current loss 0.708030, current_train_items 188128.
I0302 19:01:12.159520 22626471084160 run.py:483] Algo bellman_ford step 5879 current loss 0.852029, current_train_items 188160.
I0302 19:01:12.179152 22626471084160 run.py:483] Algo bellman_ford step 5880 current loss 0.335123, current_train_items 188192.
I0302 19:01:12.195100 22626471084160 run.py:483] Algo bellman_ford step 5881 current loss 0.474343, current_train_items 188224.
I0302 19:01:12.219078 22626471084160 run.py:483] Algo bellman_ford step 5882 current loss 0.682356, current_train_items 188256.
I0302 19:01:12.249712 22626471084160 run.py:483] Algo bellman_ford step 5883 current loss 0.685400, current_train_items 188288.
I0302 19:01:12.284264 22626471084160 run.py:483] Algo bellman_ford step 5884 current loss 0.764221, current_train_items 188320.
I0302 19:01:12.304774 22626471084160 run.py:483] Algo bellman_ford step 5885 current loss 0.324008, current_train_items 188352.
I0302 19:01:12.320964 22626471084160 run.py:483] Algo bellman_ford step 5886 current loss 0.525313, current_train_items 188384.
I0302 19:01:12.343813 22626471084160 run.py:483] Algo bellman_ford step 5887 current loss 0.642847, current_train_items 188416.
I0302 19:01:12.373796 22626471084160 run.py:483] Algo bellman_ford step 5888 current loss 0.691075, current_train_items 188448.
I0302 19:01:12.406095 22626471084160 run.py:483] Algo bellman_ford step 5889 current loss 0.942849, current_train_items 188480.
I0302 19:01:12.426414 22626471084160 run.py:483] Algo bellman_ford step 5890 current loss 0.352342, current_train_items 188512.
I0302 19:01:12.442627 22626471084160 run.py:483] Algo bellman_ford step 5891 current loss 0.500819, current_train_items 188544.
I0302 19:01:12.465848 22626471084160 run.py:483] Algo bellman_ford step 5892 current loss 0.733903, current_train_items 188576.
I0302 19:01:12.495760 22626471084160 run.py:483] Algo bellman_ford step 5893 current loss 0.739130, current_train_items 188608.
I0302 19:01:12.527264 22626471084160 run.py:483] Algo bellman_ford step 5894 current loss 0.859461, current_train_items 188640.
I0302 19:01:12.546762 22626471084160 run.py:483] Algo bellman_ford step 5895 current loss 0.284457, current_train_items 188672.
I0302 19:01:12.562970 22626471084160 run.py:483] Algo bellman_ford step 5896 current loss 0.546099, current_train_items 188704.
I0302 19:01:12.586966 22626471084160 run.py:483] Algo bellman_ford step 5897 current loss 0.662664, current_train_items 188736.
I0302 19:01:12.618400 22626471084160 run.py:483] Algo bellman_ford step 5898 current loss 0.727559, current_train_items 188768.
I0302 19:01:12.651552 22626471084160 run.py:483] Algo bellman_ford step 5899 current loss 0.908561, current_train_items 188800.
I0302 19:01:12.671596 22626471084160 run.py:483] Algo bellman_ford step 5900 current loss 0.296582, current_train_items 188832.
I0302 19:01:12.679286 22626471084160 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0302 19:01:12.679390 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:01:12.695755 22626471084160 run.py:483] Algo bellman_ford step 5901 current loss 0.467108, current_train_items 188864.
I0302 19:01:12.720034 22626471084160 run.py:483] Algo bellman_ford step 5902 current loss 0.640127, current_train_items 188896.
I0302 19:01:12.751542 22626471084160 run.py:483] Algo bellman_ford step 5903 current loss 0.659492, current_train_items 188928.
I0302 19:01:12.784105 22626471084160 run.py:483] Algo bellman_ford step 5904 current loss 0.792072, current_train_items 188960.
I0302 19:01:12.804127 22626471084160 run.py:483] Algo bellman_ford step 5905 current loss 0.305463, current_train_items 188992.
I0302 19:01:12.820467 22626471084160 run.py:483] Algo bellman_ford step 5906 current loss 0.512974, current_train_items 189024.
I0302 19:01:12.843674 22626471084160 run.py:483] Algo bellman_ford step 5907 current loss 0.617799, current_train_items 189056.
I0302 19:01:12.874633 22626471084160 run.py:483] Algo bellman_ford step 5908 current loss 0.654929, current_train_items 189088.
I0302 19:01:12.906778 22626471084160 run.py:483] Algo bellman_ford step 5909 current loss 0.834714, current_train_items 189120.
I0302 19:01:12.926373 22626471084160 run.py:483] Algo bellman_ford step 5910 current loss 0.319849, current_train_items 189152.
I0302 19:01:12.942771 22626471084160 run.py:483] Algo bellman_ford step 5911 current loss 0.563273, current_train_items 189184.
I0302 19:01:12.966486 22626471084160 run.py:483] Algo bellman_ford step 5912 current loss 0.726498, current_train_items 189216.
I0302 19:01:12.996773 22626471084160 run.py:483] Algo bellman_ford step 5913 current loss 0.712410, current_train_items 189248.
I0302 19:01:13.029572 22626471084160 run.py:483] Algo bellman_ford step 5914 current loss 0.778818, current_train_items 189280.
I0302 19:01:13.049293 22626471084160 run.py:483] Algo bellman_ford step 5915 current loss 0.318882, current_train_items 189312.
I0302 19:01:13.065694 22626471084160 run.py:483] Algo bellman_ford step 5916 current loss 0.475760, current_train_items 189344.
I0302 19:01:13.089021 22626471084160 run.py:483] Algo bellman_ford step 5917 current loss 0.627091, current_train_items 189376.
I0302 19:01:13.119084 22626471084160 run.py:483] Algo bellman_ford step 5918 current loss 0.665285, current_train_items 189408.
I0302 19:01:13.153148 22626471084160 run.py:483] Algo bellman_ford step 5919 current loss 0.821573, current_train_items 189440.
I0302 19:01:13.172876 22626471084160 run.py:483] Algo bellman_ford step 5920 current loss 0.308899, current_train_items 189472.
I0302 19:01:13.189059 22626471084160 run.py:483] Algo bellman_ford step 5921 current loss 0.503719, current_train_items 189504.
I0302 19:01:13.211851 22626471084160 run.py:483] Algo bellman_ford step 5922 current loss 0.620574, current_train_items 189536.
I0302 19:01:13.241363 22626471084160 run.py:483] Algo bellman_ford step 5923 current loss 0.713687, current_train_items 189568.
I0302 19:01:13.274593 22626471084160 run.py:483] Algo bellman_ford step 5924 current loss 0.868329, current_train_items 189600.
I0302 19:01:13.293854 22626471084160 run.py:483] Algo bellman_ford step 5925 current loss 0.269825, current_train_items 189632.
I0302 19:01:13.309689 22626471084160 run.py:483] Algo bellman_ford step 5926 current loss 0.515858, current_train_items 189664.
I0302 19:01:13.332680 22626471084160 run.py:483] Algo bellman_ford step 5927 current loss 0.576335, current_train_items 189696.
I0302 19:01:13.363761 22626471084160 run.py:483] Algo bellman_ford step 5928 current loss 0.751325, current_train_items 189728.
I0302 19:01:13.395936 22626471084160 run.py:483] Algo bellman_ford step 5929 current loss 0.718059, current_train_items 189760.
I0302 19:01:13.415566 22626471084160 run.py:483] Algo bellman_ford step 5930 current loss 0.313119, current_train_items 189792.
I0302 19:01:13.432244 22626471084160 run.py:483] Algo bellman_ford step 5931 current loss 0.467396, current_train_items 189824.
I0302 19:01:13.455726 22626471084160 run.py:483] Algo bellman_ford step 5932 current loss 0.655160, current_train_items 189856.
I0302 19:01:13.484985 22626471084160 run.py:483] Algo bellman_ford step 5933 current loss 0.669503, current_train_items 189888.
I0302 19:01:13.519284 22626471084160 run.py:483] Algo bellman_ford step 5934 current loss 0.852937, current_train_items 189920.
I0302 19:01:13.539237 22626471084160 run.py:483] Algo bellman_ford step 5935 current loss 0.395718, current_train_items 189952.
I0302 19:01:13.555403 22626471084160 run.py:483] Algo bellman_ford step 5936 current loss 0.504050, current_train_items 189984.
I0302 19:01:13.579281 22626471084160 run.py:483] Algo bellman_ford step 5937 current loss 0.663376, current_train_items 190016.
I0302 19:01:13.610711 22626471084160 run.py:483] Algo bellman_ford step 5938 current loss 0.736009, current_train_items 190048.
I0302 19:01:13.642579 22626471084160 run.py:483] Algo bellman_ford step 5939 current loss 0.779332, current_train_items 190080.
I0302 19:01:13.662622 22626471084160 run.py:483] Algo bellman_ford step 5940 current loss 0.284021, current_train_items 190112.
I0302 19:01:13.678851 22626471084160 run.py:483] Algo bellman_ford step 5941 current loss 0.501919, current_train_items 190144.
I0302 19:01:13.703152 22626471084160 run.py:483] Algo bellman_ford step 5942 current loss 0.683147, current_train_items 190176.
I0302 19:01:13.734324 22626471084160 run.py:483] Algo bellman_ford step 5943 current loss 0.670498, current_train_items 190208.
I0302 19:01:13.768346 22626471084160 run.py:483] Algo bellman_ford step 5944 current loss 0.775742, current_train_items 190240.
I0302 19:01:13.788189 22626471084160 run.py:483] Algo bellman_ford step 5945 current loss 0.243445, current_train_items 190272.
I0302 19:01:13.804852 22626471084160 run.py:483] Algo bellman_ford step 5946 current loss 0.517098, current_train_items 190304.
I0302 19:01:13.829081 22626471084160 run.py:483] Algo bellman_ford step 5947 current loss 0.709575, current_train_items 190336.
I0302 19:01:13.859091 22626471084160 run.py:483] Algo bellman_ford step 5948 current loss 0.677563, current_train_items 190368.
I0302 19:01:13.893047 22626471084160 run.py:483] Algo bellman_ford step 5949 current loss 0.778571, current_train_items 190400.
I0302 19:01:13.912717 22626471084160 run.py:483] Algo bellman_ford step 5950 current loss 0.244488, current_train_items 190432.
I0302 19:01:13.920483 22626471084160 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0302 19:01:13.920588 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:01:13.937289 22626471084160 run.py:483] Algo bellman_ford step 5951 current loss 0.455364, current_train_items 190464.
I0302 19:01:13.961313 22626471084160 run.py:483] Algo bellman_ford step 5952 current loss 0.651682, current_train_items 190496.
I0302 19:01:13.992494 22626471084160 run.py:483] Algo bellman_ford step 5953 current loss 0.730423, current_train_items 190528.
I0302 19:01:14.026056 22626471084160 run.py:483] Algo bellman_ford step 5954 current loss 0.704656, current_train_items 190560.
I0302 19:01:14.046248 22626471084160 run.py:483] Algo bellman_ford step 5955 current loss 0.295162, current_train_items 190592.
I0302 19:01:14.062495 22626471084160 run.py:483] Algo bellman_ford step 5956 current loss 0.470405, current_train_items 190624.
I0302 19:01:14.084822 22626471084160 run.py:483] Algo bellman_ford step 5957 current loss 0.547065, current_train_items 190656.
I0302 19:01:14.115737 22626471084160 run.py:483] Algo bellman_ford step 5958 current loss 0.725946, current_train_items 190688.
I0302 19:01:14.148830 22626471084160 run.py:483] Algo bellman_ford step 5959 current loss 0.754211, current_train_items 190720.
I0302 19:01:14.168821 22626471084160 run.py:483] Algo bellman_ford step 5960 current loss 0.255606, current_train_items 190752.
I0302 19:01:14.184983 22626471084160 run.py:483] Algo bellman_ford step 5961 current loss 0.519513, current_train_items 190784.
I0302 19:01:14.208733 22626471084160 run.py:483] Algo bellman_ford step 5962 current loss 0.564962, current_train_items 190816.
I0302 19:01:14.238018 22626471084160 run.py:483] Algo bellman_ford step 5963 current loss 0.611412, current_train_items 190848.
I0302 19:01:14.270978 22626471084160 run.py:483] Algo bellman_ford step 5964 current loss 0.852823, current_train_items 190880.
I0302 19:01:14.290749 22626471084160 run.py:483] Algo bellman_ford step 5965 current loss 0.299168, current_train_items 190912.
I0302 19:01:14.306634 22626471084160 run.py:483] Algo bellman_ford step 5966 current loss 0.489131, current_train_items 190944.
I0302 19:01:14.330810 22626471084160 run.py:483] Algo bellman_ford step 5967 current loss 0.595288, current_train_items 190976.
I0302 19:01:14.361026 22626471084160 run.py:483] Algo bellman_ford step 5968 current loss 0.673639, current_train_items 191008.
I0302 19:01:14.395148 22626471084160 run.py:483] Algo bellman_ford step 5969 current loss 0.782213, current_train_items 191040.
I0302 19:01:14.414894 22626471084160 run.py:483] Algo bellman_ford step 5970 current loss 0.280249, current_train_items 191072.
I0302 19:01:14.430979 22626471084160 run.py:483] Algo bellman_ford step 5971 current loss 0.549022, current_train_items 191104.
I0302 19:01:14.453732 22626471084160 run.py:483] Algo bellman_ford step 5972 current loss 0.651750, current_train_items 191136.
I0302 19:01:14.484626 22626471084160 run.py:483] Algo bellman_ford step 5973 current loss 0.734049, current_train_items 191168.
I0302 19:01:14.517676 22626471084160 run.py:483] Algo bellman_ford step 5974 current loss 0.761481, current_train_items 191200.
I0302 19:01:14.537247 22626471084160 run.py:483] Algo bellman_ford step 5975 current loss 0.387497, current_train_items 191232.
I0302 19:01:14.553144 22626471084160 run.py:483] Algo bellman_ford step 5976 current loss 0.468704, current_train_items 191264.
I0302 19:01:14.575738 22626471084160 run.py:483] Algo bellman_ford step 5977 current loss 0.642163, current_train_items 191296.
I0302 19:01:14.606684 22626471084160 run.py:483] Algo bellman_ford step 5978 current loss 0.803171, current_train_items 191328.
I0302 19:01:14.638610 22626471084160 run.py:483] Algo bellman_ford step 5979 current loss 0.766791, current_train_items 191360.
I0302 19:01:14.658110 22626471084160 run.py:483] Algo bellman_ford step 5980 current loss 0.269707, current_train_items 191392.
I0302 19:01:14.674884 22626471084160 run.py:483] Algo bellman_ford step 5981 current loss 0.479578, current_train_items 191424.
I0302 19:01:14.697505 22626471084160 run.py:483] Algo bellman_ford step 5982 current loss 0.584994, current_train_items 191456.
I0302 19:01:14.727489 22626471084160 run.py:483] Algo bellman_ford step 5983 current loss 0.659784, current_train_items 191488.
I0302 19:01:14.759882 22626471084160 run.py:483] Algo bellman_ford step 5984 current loss 0.786495, current_train_items 191520.
I0302 19:01:14.779650 22626471084160 run.py:483] Algo bellman_ford step 5985 current loss 0.322520, current_train_items 191552.
I0302 19:01:14.796541 22626471084160 run.py:483] Algo bellman_ford step 5986 current loss 0.499695, current_train_items 191584.
I0302 19:01:14.819762 22626471084160 run.py:483] Algo bellman_ford step 5987 current loss 0.600351, current_train_items 191616.
I0302 19:01:14.849368 22626471084160 run.py:483] Algo bellman_ford step 5988 current loss 0.711242, current_train_items 191648.
I0302 19:01:14.882253 22626471084160 run.py:483] Algo bellman_ford step 5989 current loss 0.754680, current_train_items 191680.
I0302 19:01:14.902088 22626471084160 run.py:483] Algo bellman_ford step 5990 current loss 0.268973, current_train_items 191712.
I0302 19:01:14.917797 22626471084160 run.py:483] Algo bellman_ford step 5991 current loss 0.439875, current_train_items 191744.
I0302 19:01:14.940944 22626471084160 run.py:483] Algo bellman_ford step 5992 current loss 0.701980, current_train_items 191776.
I0302 19:01:14.973505 22626471084160 run.py:483] Algo bellman_ford step 5993 current loss 0.870035, current_train_items 191808.
I0302 19:01:15.006681 22626471084160 run.py:483] Algo bellman_ford step 5994 current loss 0.908505, current_train_items 191840.
I0302 19:01:15.026301 22626471084160 run.py:483] Algo bellman_ford step 5995 current loss 0.304733, current_train_items 191872.
I0302 19:01:15.042420 22626471084160 run.py:483] Algo bellman_ford step 5996 current loss 0.402777, current_train_items 191904.
I0302 19:01:15.066190 22626471084160 run.py:483] Algo bellman_ford step 5997 current loss 0.654129, current_train_items 191936.
I0302 19:01:15.096293 22626471084160 run.py:483] Algo bellman_ford step 5998 current loss 0.694711, current_train_items 191968.
I0302 19:01:15.128733 22626471084160 run.py:483] Algo bellman_ford step 5999 current loss 0.693145, current_train_items 192000.
I0302 19:01:15.148645 22626471084160 run.py:483] Algo bellman_ford step 6000 current loss 0.291389, current_train_items 192032.
I0302 19:01:15.156372 22626471084160 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0302 19:01:15.156478 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:01:15.173227 22626471084160 run.py:483] Algo bellman_ford step 6001 current loss 0.501501, current_train_items 192064.
I0302 19:01:15.197145 22626471084160 run.py:483] Algo bellman_ford step 6002 current loss 0.643081, current_train_items 192096.
I0302 19:01:15.226855 22626471084160 run.py:483] Algo bellman_ford step 6003 current loss 0.702168, current_train_items 192128.
I0302 19:01:15.259801 22626471084160 run.py:483] Algo bellman_ford step 6004 current loss 0.856401, current_train_items 192160.
I0302 19:01:15.279756 22626471084160 run.py:483] Algo bellman_ford step 6005 current loss 0.249630, current_train_items 192192.
I0302 19:01:15.295635 22626471084160 run.py:483] Algo bellman_ford step 6006 current loss 0.430437, current_train_items 192224.
I0302 19:01:15.320265 22626471084160 run.py:483] Algo bellman_ford step 6007 current loss 0.713031, current_train_items 192256.
I0302 19:01:15.353024 22626471084160 run.py:483] Algo bellman_ford step 6008 current loss 0.782182, current_train_items 192288.
I0302 19:01:15.388080 22626471084160 run.py:483] Algo bellman_ford step 6009 current loss 0.821313, current_train_items 192320.
I0302 19:01:15.407830 22626471084160 run.py:483] Algo bellman_ford step 6010 current loss 0.292332, current_train_items 192352.
I0302 19:01:15.424020 22626471084160 run.py:483] Algo bellman_ford step 6011 current loss 0.510935, current_train_items 192384.
I0302 19:01:15.447872 22626471084160 run.py:483] Algo bellman_ford step 6012 current loss 0.621038, current_train_items 192416.
I0302 19:01:15.478078 22626471084160 run.py:483] Algo bellman_ford step 6013 current loss 0.652881, current_train_items 192448.
I0302 19:01:15.513605 22626471084160 run.py:483] Algo bellman_ford step 6014 current loss 0.771298, current_train_items 192480.
I0302 19:01:15.533086 22626471084160 run.py:483] Algo bellman_ford step 6015 current loss 0.303794, current_train_items 192512.
I0302 19:01:15.549479 22626471084160 run.py:483] Algo bellman_ford step 6016 current loss 0.496976, current_train_items 192544.
I0302 19:01:15.572609 22626471084160 run.py:483] Algo bellman_ford step 6017 current loss 0.564558, current_train_items 192576.
I0302 19:01:15.603688 22626471084160 run.py:483] Algo bellman_ford step 6018 current loss 0.773954, current_train_items 192608.
I0302 19:01:15.637035 22626471084160 run.py:483] Algo bellman_ford step 6019 current loss 0.858958, current_train_items 192640.
I0302 19:01:15.656549 22626471084160 run.py:483] Algo bellman_ford step 6020 current loss 0.316030, current_train_items 192672.
I0302 19:01:15.673044 22626471084160 run.py:483] Algo bellman_ford step 6021 current loss 0.445900, current_train_items 192704.
I0302 19:01:15.696674 22626471084160 run.py:483] Algo bellman_ford step 6022 current loss 0.577728, current_train_items 192736.
I0302 19:01:15.729293 22626471084160 run.py:483] Algo bellman_ford step 6023 current loss 0.829979, current_train_items 192768.
I0302 19:01:15.763031 22626471084160 run.py:483] Algo bellman_ford step 6024 current loss 0.722559, current_train_items 192800.
I0302 19:01:15.782445 22626471084160 run.py:483] Algo bellman_ford step 6025 current loss 0.281455, current_train_items 192832.
I0302 19:01:15.798776 22626471084160 run.py:483] Algo bellman_ford step 6026 current loss 0.553442, current_train_items 192864.
I0302 19:01:15.822498 22626471084160 run.py:483] Algo bellman_ford step 6027 current loss 0.650127, current_train_items 192896.
I0302 19:01:15.853408 22626471084160 run.py:483] Algo bellman_ford step 6028 current loss 0.717256, current_train_items 192928.
I0302 19:01:15.886872 22626471084160 run.py:483] Algo bellman_ford step 6029 current loss 0.694185, current_train_items 192960.
I0302 19:01:15.906192 22626471084160 run.py:483] Algo bellman_ford step 6030 current loss 0.285638, current_train_items 192992.
I0302 19:01:15.922132 22626471084160 run.py:483] Algo bellman_ford step 6031 current loss 0.411529, current_train_items 193024.
I0302 19:01:15.945677 22626471084160 run.py:483] Algo bellman_ford step 6032 current loss 0.495565, current_train_items 193056.
I0302 19:01:15.977917 22626471084160 run.py:483] Algo bellman_ford step 6033 current loss 0.766843, current_train_items 193088.
I0302 19:01:16.011444 22626471084160 run.py:483] Algo bellman_ford step 6034 current loss 0.786957, current_train_items 193120.
I0302 19:01:16.030892 22626471084160 run.py:483] Algo bellman_ford step 6035 current loss 0.243215, current_train_items 193152.
I0302 19:01:16.046954 22626471084160 run.py:483] Algo bellman_ford step 6036 current loss 0.427995, current_train_items 193184.
I0302 19:01:16.070827 22626471084160 run.py:483] Algo bellman_ford step 6037 current loss 0.635656, current_train_items 193216.
I0302 19:01:16.102065 22626471084160 run.py:483] Algo bellman_ford step 6038 current loss 0.747562, current_train_items 193248.
I0302 19:01:16.134371 22626471084160 run.py:483] Algo bellman_ford step 6039 current loss 0.725510, current_train_items 193280.
I0302 19:01:16.153883 22626471084160 run.py:483] Algo bellman_ford step 6040 current loss 0.408916, current_train_items 193312.
I0302 19:01:16.170085 22626471084160 run.py:483] Algo bellman_ford step 6041 current loss 0.485478, current_train_items 193344.
I0302 19:01:16.194693 22626471084160 run.py:483] Algo bellman_ford step 6042 current loss 0.668311, current_train_items 193376.
I0302 19:01:16.225311 22626471084160 run.py:483] Algo bellman_ford step 6043 current loss 0.677264, current_train_items 193408.
I0302 19:01:16.261536 22626471084160 run.py:483] Algo bellman_ford step 6044 current loss 0.834772, current_train_items 193440.
I0302 19:01:16.281340 22626471084160 run.py:483] Algo bellman_ford step 6045 current loss 0.259742, current_train_items 193472.
I0302 19:01:16.297837 22626471084160 run.py:483] Algo bellman_ford step 6046 current loss 0.487890, current_train_items 193504.
I0302 19:01:16.321028 22626471084160 run.py:483] Algo bellman_ford step 6047 current loss 0.648500, current_train_items 193536.
I0302 19:01:16.353162 22626471084160 run.py:483] Algo bellman_ford step 6048 current loss 0.780899, current_train_items 193568.
I0302 19:01:16.387682 22626471084160 run.py:483] Algo bellman_ford step 6049 current loss 0.790999, current_train_items 193600.
I0302 19:01:16.407498 22626471084160 run.py:483] Algo bellman_ford step 6050 current loss 0.277333, current_train_items 193632.
I0302 19:01:16.415478 22626471084160 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.935546875, 'score': 0.935546875, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0302 19:01:16.415608 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.936, val scores are: bellman_ford: 0.936
I0302 19:01:16.432235 22626471084160 run.py:483] Algo bellman_ford step 6051 current loss 0.433271, current_train_items 193664.
I0302 19:01:16.456962 22626471084160 run.py:483] Algo bellman_ford step 6052 current loss 0.589725, current_train_items 193696.
I0302 19:01:16.487795 22626471084160 run.py:483] Algo bellman_ford step 6053 current loss 0.669121, current_train_items 193728.
I0302 19:01:16.522986 22626471084160 run.py:483] Algo bellman_ford step 6054 current loss 0.750719, current_train_items 193760.
I0302 19:01:16.543247 22626471084160 run.py:483] Algo bellman_ford step 6055 current loss 0.348678, current_train_items 193792.
I0302 19:01:16.558954 22626471084160 run.py:483] Algo bellman_ford step 6056 current loss 0.567155, current_train_items 193824.
I0302 19:01:16.582093 22626471084160 run.py:483] Algo bellman_ford step 6057 current loss 0.881994, current_train_items 193856.
I0302 19:01:16.612829 22626471084160 run.py:483] Algo bellman_ford step 6058 current loss 0.834201, current_train_items 193888.
I0302 19:01:16.643920 22626471084160 run.py:483] Algo bellman_ford step 6059 current loss 0.693026, current_train_items 193920.
I0302 19:01:16.664018 22626471084160 run.py:483] Algo bellman_ford step 6060 current loss 0.279561, current_train_items 193952.
I0302 19:01:16.680445 22626471084160 run.py:483] Algo bellman_ford step 6061 current loss 0.476959, current_train_items 193984.
I0302 19:01:16.702499 22626471084160 run.py:483] Algo bellman_ford step 6062 current loss 0.512681, current_train_items 194016.
I0302 19:01:16.732124 22626471084160 run.py:483] Algo bellman_ford step 6063 current loss 0.681120, current_train_items 194048.
I0302 19:01:16.766054 22626471084160 run.py:483] Algo bellman_ford step 6064 current loss 0.793520, current_train_items 194080.
I0302 19:01:16.785712 22626471084160 run.py:483] Algo bellman_ford step 6065 current loss 0.283280, current_train_items 194112.
I0302 19:01:16.801909 22626471084160 run.py:483] Algo bellman_ford step 6066 current loss 0.446764, current_train_items 194144.
I0302 19:01:16.825924 22626471084160 run.py:483] Algo bellman_ford step 6067 current loss 0.658728, current_train_items 194176.
I0302 19:01:16.857291 22626471084160 run.py:483] Algo bellman_ford step 6068 current loss 0.637069, current_train_items 194208.
I0302 19:01:16.890090 22626471084160 run.py:483] Algo bellman_ford step 6069 current loss 0.736944, current_train_items 194240.
I0302 19:01:16.910127 22626471084160 run.py:483] Algo bellman_ford step 6070 current loss 0.344385, current_train_items 194272.
I0302 19:01:16.926235 22626471084160 run.py:483] Algo bellman_ford step 6071 current loss 0.527366, current_train_items 194304.
I0302 19:01:16.949788 22626471084160 run.py:483] Algo bellman_ford step 6072 current loss 0.630655, current_train_items 194336.
I0302 19:01:16.980269 22626471084160 run.py:483] Algo bellman_ford step 6073 current loss 0.638635, current_train_items 194368.
I0302 19:01:17.014233 22626471084160 run.py:483] Algo bellman_ford step 6074 current loss 0.883994, current_train_items 194400.
I0302 19:01:17.034227 22626471084160 run.py:483] Algo bellman_ford step 6075 current loss 0.290853, current_train_items 194432.
I0302 19:01:17.050396 22626471084160 run.py:483] Algo bellman_ford step 6076 current loss 0.445920, current_train_items 194464.
I0302 19:01:17.072747 22626471084160 run.py:483] Algo bellman_ford step 6077 current loss 0.576525, current_train_items 194496.
I0302 19:01:17.103013 22626471084160 run.py:483] Algo bellman_ford step 6078 current loss 0.688408, current_train_items 194528.
I0302 19:01:17.137370 22626471084160 run.py:483] Algo bellman_ford step 6079 current loss 0.802274, current_train_items 194560.
I0302 19:01:17.157073 22626471084160 run.py:483] Algo bellman_ford step 6080 current loss 0.266965, current_train_items 194592.
I0302 19:01:17.173361 22626471084160 run.py:483] Algo bellman_ford step 6081 current loss 0.410811, current_train_items 194624.
I0302 19:01:17.196630 22626471084160 run.py:483] Algo bellman_ford step 6082 current loss 0.543753, current_train_items 194656.
I0302 19:01:17.228914 22626471084160 run.py:483] Algo bellman_ford step 6083 current loss 0.752052, current_train_items 194688.
I0302 19:01:17.261169 22626471084160 run.py:483] Algo bellman_ford step 6084 current loss 0.684480, current_train_items 194720.
I0302 19:01:17.280966 22626471084160 run.py:483] Algo bellman_ford step 6085 current loss 0.279447, current_train_items 194752.
I0302 19:01:17.297389 22626471084160 run.py:483] Algo bellman_ford step 6086 current loss 0.539984, current_train_items 194784.
I0302 19:01:17.320260 22626471084160 run.py:483] Algo bellman_ford step 6087 current loss 0.652090, current_train_items 194816.
I0302 19:01:17.350382 22626471084160 run.py:483] Algo bellman_ford step 6088 current loss 0.709427, current_train_items 194848.
I0302 19:01:17.384555 22626471084160 run.py:483] Algo bellman_ford step 6089 current loss 0.873347, current_train_items 194880.
I0302 19:01:17.404986 22626471084160 run.py:483] Algo bellman_ford step 6090 current loss 0.371474, current_train_items 194912.
I0302 19:01:17.421317 22626471084160 run.py:483] Algo bellman_ford step 6091 current loss 0.460189, current_train_items 194944.
I0302 19:01:17.444355 22626471084160 run.py:483] Algo bellman_ford step 6092 current loss 0.587108, current_train_items 194976.
I0302 19:01:17.474476 22626471084160 run.py:483] Algo bellman_ford step 6093 current loss 0.661560, current_train_items 195008.
I0302 19:01:17.506993 22626471084160 run.py:483] Algo bellman_ford step 6094 current loss 0.870893, current_train_items 195040.
I0302 19:01:17.526857 22626471084160 run.py:483] Algo bellman_ford step 6095 current loss 0.289052, current_train_items 195072.
I0302 19:01:17.543130 22626471084160 run.py:483] Algo bellman_ford step 6096 current loss 0.418261, current_train_items 195104.
I0302 19:01:17.566955 22626471084160 run.py:483] Algo bellman_ford step 6097 current loss 0.637432, current_train_items 195136.
I0302 19:01:17.597283 22626471084160 run.py:483] Algo bellman_ford step 6098 current loss 0.637114, current_train_items 195168.
I0302 19:01:17.631186 22626471084160 run.py:483] Algo bellman_ford step 6099 current loss 0.825460, current_train_items 195200.
I0302 19:01:17.651162 22626471084160 run.py:483] Algo bellman_ford step 6100 current loss 0.197455, current_train_items 195232.
I0302 19:01:17.658937 22626471084160 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0302 19:01:17.659042 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:01:17.675233 22626471084160 run.py:483] Algo bellman_ford step 6101 current loss 0.458326, current_train_items 195264.
I0302 19:01:17.699991 22626471084160 run.py:483] Algo bellman_ford step 6102 current loss 0.584477, current_train_items 195296.
I0302 19:01:17.732634 22626471084160 run.py:483] Algo bellman_ford step 6103 current loss 0.880157, current_train_items 195328.
I0302 19:01:17.765519 22626471084160 run.py:483] Algo bellman_ford step 6104 current loss 0.749366, current_train_items 195360.
I0302 19:01:17.785681 22626471084160 run.py:483] Algo bellman_ford step 6105 current loss 0.325600, current_train_items 195392.
I0302 19:01:17.801127 22626471084160 run.py:483] Algo bellman_ford step 6106 current loss 0.474182, current_train_items 195424.
I0302 19:01:17.824495 22626471084160 run.py:483] Algo bellman_ford step 6107 current loss 0.714315, current_train_items 195456.
I0302 19:01:17.855804 22626471084160 run.py:483] Algo bellman_ford step 6108 current loss 0.677094, current_train_items 195488.
I0302 19:01:17.888909 22626471084160 run.py:483] Algo bellman_ford step 6109 current loss 0.788515, current_train_items 195520.
I0302 19:01:17.908734 22626471084160 run.py:483] Algo bellman_ford step 6110 current loss 0.272255, current_train_items 195552.
I0302 19:01:17.924821 22626471084160 run.py:483] Algo bellman_ford step 6111 current loss 0.440473, current_train_items 195584.
I0302 19:01:17.948330 22626471084160 run.py:483] Algo bellman_ford step 6112 current loss 0.643902, current_train_items 195616.
I0302 19:01:17.978648 22626471084160 run.py:483] Algo bellman_ford step 6113 current loss 0.678433, current_train_items 195648.
I0302 19:01:18.012092 22626471084160 run.py:483] Algo bellman_ford step 6114 current loss 0.705108, current_train_items 195680.
I0302 19:01:18.032070 22626471084160 run.py:483] Algo bellman_ford step 6115 current loss 0.283237, current_train_items 195712.
I0302 19:01:18.048209 22626471084160 run.py:483] Algo bellman_ford step 6116 current loss 0.451597, current_train_items 195744.
I0302 19:01:18.071827 22626471084160 run.py:483] Algo bellman_ford step 6117 current loss 0.505360, current_train_items 195776.
I0302 19:01:18.103699 22626471084160 run.py:483] Algo bellman_ford step 6118 current loss 0.693933, current_train_items 195808.
I0302 19:01:18.136395 22626471084160 run.py:483] Algo bellman_ford step 6119 current loss 0.818684, current_train_items 195840.
I0302 19:01:18.155994 22626471084160 run.py:483] Algo bellman_ford step 6120 current loss 0.373144, current_train_items 195872.
I0302 19:01:18.172345 22626471084160 run.py:483] Algo bellman_ford step 6121 current loss 0.451283, current_train_items 195904.
I0302 19:01:18.195570 22626471084160 run.py:483] Algo bellman_ford step 6122 current loss 0.566909, current_train_items 195936.
I0302 19:01:18.227730 22626471084160 run.py:483] Algo bellman_ford step 6123 current loss 0.671054, current_train_items 195968.
I0302 19:01:18.261129 22626471084160 run.py:483] Algo bellman_ford step 6124 current loss 0.747205, current_train_items 196000.
I0302 19:01:18.280734 22626471084160 run.py:483] Algo bellman_ford step 6125 current loss 0.249544, current_train_items 196032.
I0302 19:01:18.296377 22626471084160 run.py:483] Algo bellman_ford step 6126 current loss 0.480925, current_train_items 196064.
I0302 19:01:18.321872 22626471084160 run.py:483] Algo bellman_ford step 6127 current loss 0.663347, current_train_items 196096.
I0302 19:01:18.352690 22626471084160 run.py:483] Algo bellman_ford step 6128 current loss 0.588136, current_train_items 196128.
I0302 19:01:18.385745 22626471084160 run.py:483] Algo bellman_ford step 6129 current loss 0.677685, current_train_items 196160.
I0302 19:01:18.405487 22626471084160 run.py:483] Algo bellman_ford step 6130 current loss 0.324085, current_train_items 196192.
I0302 19:01:18.421553 22626471084160 run.py:483] Algo bellman_ford step 6131 current loss 0.423546, current_train_items 196224.
I0302 19:01:18.445937 22626471084160 run.py:483] Algo bellman_ford step 6132 current loss 0.600484, current_train_items 196256.
I0302 19:01:18.476009 22626471084160 run.py:483] Algo bellman_ford step 6133 current loss 0.610086, current_train_items 196288.
I0302 19:01:18.508292 22626471084160 run.py:483] Algo bellman_ford step 6134 current loss 0.606854, current_train_items 196320.
I0302 19:01:18.527743 22626471084160 run.py:483] Algo bellman_ford step 6135 current loss 0.263262, current_train_items 196352.
I0302 19:01:18.543443 22626471084160 run.py:483] Algo bellman_ford step 6136 current loss 0.472051, current_train_items 196384.
I0302 19:01:18.567094 22626471084160 run.py:483] Algo bellman_ford step 6137 current loss 0.680545, current_train_items 196416.
I0302 19:01:18.599021 22626471084160 run.py:483] Algo bellman_ford step 6138 current loss 0.664592, current_train_items 196448.
I0302 19:01:18.632839 22626471084160 run.py:483] Algo bellman_ford step 6139 current loss 0.691273, current_train_items 196480.
I0302 19:01:18.652441 22626471084160 run.py:483] Algo bellman_ford step 6140 current loss 0.331775, current_train_items 196512.
I0302 19:01:18.668613 22626471084160 run.py:483] Algo bellman_ford step 6141 current loss 0.370077, current_train_items 196544.
I0302 19:01:18.692667 22626471084160 run.py:483] Algo bellman_ford step 6142 current loss 0.577673, current_train_items 196576.
I0302 19:01:18.723869 22626471084160 run.py:483] Algo bellman_ford step 6143 current loss 0.690217, current_train_items 196608.
I0302 19:01:18.757062 22626471084160 run.py:483] Algo bellman_ford step 6144 current loss 0.840855, current_train_items 196640.
I0302 19:01:18.776472 22626471084160 run.py:483] Algo bellman_ford step 6145 current loss 0.256146, current_train_items 196672.
I0302 19:01:18.792776 22626471084160 run.py:483] Algo bellman_ford step 6146 current loss 0.554084, current_train_items 196704.
I0302 19:01:18.815872 22626471084160 run.py:483] Algo bellman_ford step 6147 current loss 0.501737, current_train_items 196736.
I0302 19:01:18.846684 22626471084160 run.py:483] Algo bellman_ford step 6148 current loss 0.719414, current_train_items 196768.
I0302 19:01:18.881630 22626471084160 run.py:483] Algo bellman_ford step 6149 current loss 0.830258, current_train_items 196800.
I0302 19:01:18.901563 22626471084160 run.py:483] Algo bellman_ford step 6150 current loss 0.312797, current_train_items 196832.
I0302 19:01:18.909763 22626471084160 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.9541015625, 'score': 0.9541015625, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0302 19:01:18.909868 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.938, current avg val score is 0.954, val scores are: bellman_ford: 0.954
I0302 19:01:18.940502 22626471084160 run.py:483] Algo bellman_ford step 6151 current loss 0.454478, current_train_items 196864.
I0302 19:01:18.963660 22626471084160 run.py:483] Algo bellman_ford step 6152 current loss 0.534456, current_train_items 196896.
I0302 19:01:18.993728 22626471084160 run.py:483] Algo bellman_ford step 6153 current loss 0.650460, current_train_items 196928.
I0302 19:01:19.027883 22626471084160 run.py:483] Algo bellman_ford step 6154 current loss 0.692392, current_train_items 196960.
I0302 19:01:19.048121 22626471084160 run.py:483] Algo bellman_ford step 6155 current loss 0.258331, current_train_items 196992.
I0302 19:01:19.064231 22626471084160 run.py:483] Algo bellman_ford step 6156 current loss 0.463301, current_train_items 197024.
I0302 19:01:19.087642 22626471084160 run.py:483] Algo bellman_ford step 6157 current loss 0.567909, current_train_items 197056.
I0302 19:01:19.117606 22626471084160 run.py:483] Algo bellman_ford step 6158 current loss 0.664045, current_train_items 197088.
I0302 19:01:19.150831 22626471084160 run.py:483] Algo bellman_ford step 6159 current loss 0.766124, current_train_items 197120.
I0302 19:01:19.170795 22626471084160 run.py:483] Algo bellman_ford step 6160 current loss 0.439623, current_train_items 197152.
I0302 19:01:19.187423 22626471084160 run.py:483] Algo bellman_ford step 6161 current loss 0.581915, current_train_items 197184.
I0302 19:01:19.210779 22626471084160 run.py:483] Algo bellman_ford step 6162 current loss 0.671585, current_train_items 197216.
I0302 19:01:19.242820 22626471084160 run.py:483] Algo bellman_ford step 6163 current loss 0.828851, current_train_items 197248.
I0302 19:01:19.276917 22626471084160 run.py:483] Algo bellman_ford step 6164 current loss 0.848644, current_train_items 197280.
I0302 19:01:19.296534 22626471084160 run.py:483] Algo bellman_ford step 6165 current loss 0.257161, current_train_items 197312.
I0302 19:01:19.312874 22626471084160 run.py:483] Algo bellman_ford step 6166 current loss 0.462980, current_train_items 197344.
I0302 19:01:19.336654 22626471084160 run.py:483] Algo bellman_ford step 6167 current loss 0.657517, current_train_items 197376.
I0302 19:01:19.368189 22626471084160 run.py:483] Algo bellman_ford step 6168 current loss 0.709542, current_train_items 197408.
I0302 19:01:19.401913 22626471084160 run.py:483] Algo bellman_ford step 6169 current loss 0.926516, current_train_items 197440.
I0302 19:01:19.422127 22626471084160 run.py:483] Algo bellman_ford step 6170 current loss 0.305330, current_train_items 197472.
I0302 19:01:19.438188 22626471084160 run.py:483] Algo bellman_ford step 6171 current loss 0.435900, current_train_items 197504.
I0302 19:01:19.461794 22626471084160 run.py:483] Algo bellman_ford step 6172 current loss 0.571786, current_train_items 197536.
I0302 19:01:19.494019 22626471084160 run.py:483] Algo bellman_ford step 6173 current loss 0.706985, current_train_items 197568.
I0302 19:01:19.527616 22626471084160 run.py:483] Algo bellman_ford step 6174 current loss 0.821120, current_train_items 197600.
I0302 19:01:19.547689 22626471084160 run.py:483] Algo bellman_ford step 6175 current loss 0.290901, current_train_items 197632.
I0302 19:01:19.564273 22626471084160 run.py:483] Algo bellman_ford step 6176 current loss 0.631219, current_train_items 197664.
I0302 19:01:19.587029 22626471084160 run.py:483] Algo bellman_ford step 6177 current loss 0.586649, current_train_items 197696.
I0302 19:01:19.618145 22626471084160 run.py:483] Algo bellman_ford step 6178 current loss 0.665641, current_train_items 197728.
I0302 19:01:19.651988 22626471084160 run.py:483] Algo bellman_ford step 6179 current loss 0.882830, current_train_items 197760.
I0302 19:01:19.671700 22626471084160 run.py:483] Algo bellman_ford step 6180 current loss 0.276755, current_train_items 197792.
I0302 19:01:19.687950 22626471084160 run.py:483] Algo bellman_ford step 6181 current loss 0.457621, current_train_items 197824.
I0302 19:01:19.711311 22626471084160 run.py:483] Algo bellman_ford step 6182 current loss 0.648461, current_train_items 197856.
I0302 19:01:19.742301 22626471084160 run.py:483] Algo bellman_ford step 6183 current loss 0.725169, current_train_items 197888.
I0302 19:01:19.778254 22626471084160 run.py:483] Algo bellman_ford step 6184 current loss 0.847115, current_train_items 197920.
I0302 19:01:19.798405 22626471084160 run.py:483] Algo bellman_ford step 6185 current loss 0.352729, current_train_items 197952.
I0302 19:01:19.814761 22626471084160 run.py:483] Algo bellman_ford step 6186 current loss 0.481475, current_train_items 197984.
I0302 19:01:19.837335 22626471084160 run.py:483] Algo bellman_ford step 6187 current loss 0.678796, current_train_items 198016.
I0302 19:01:19.867831 22626471084160 run.py:483] Algo bellman_ford step 6188 current loss 0.614320, current_train_items 198048.
I0302 19:01:19.901639 22626471084160 run.py:483] Algo bellman_ford step 6189 current loss 0.958767, current_train_items 198080.
I0302 19:01:19.921504 22626471084160 run.py:483] Algo bellman_ford step 6190 current loss 0.338225, current_train_items 198112.
I0302 19:01:19.937960 22626471084160 run.py:483] Algo bellman_ford step 6191 current loss 0.501588, current_train_items 198144.
I0302 19:01:19.961722 22626471084160 run.py:483] Algo bellman_ford step 6192 current loss 0.666093, current_train_items 198176.
I0302 19:01:19.993674 22626471084160 run.py:483] Algo bellman_ford step 6193 current loss 0.704323, current_train_items 198208.
I0302 19:01:20.026585 22626471084160 run.py:483] Algo bellman_ford step 6194 current loss 0.750959, current_train_items 198240.
I0302 19:01:20.046422 22626471084160 run.py:483] Algo bellman_ford step 6195 current loss 0.323285, current_train_items 198272.
I0302 19:01:20.062124 22626471084160 run.py:483] Algo bellman_ford step 6196 current loss 0.429223, current_train_items 198304.
I0302 19:01:20.086404 22626471084160 run.py:483] Algo bellman_ford step 6197 current loss 0.670159, current_train_items 198336.
I0302 19:01:20.115959 22626471084160 run.py:483] Algo bellman_ford step 6198 current loss 0.638117, current_train_items 198368.
I0302 19:01:20.150561 22626471084160 run.py:483] Algo bellman_ford step 6199 current loss 0.836015, current_train_items 198400.
I0302 19:01:20.170709 22626471084160 run.py:483] Algo bellman_ford step 6200 current loss 0.284796, current_train_items 198432.
I0302 19:01:20.178580 22626471084160 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0302 19:01:20.178686 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 19:01:20.195593 22626471084160 run.py:483] Algo bellman_ford step 6201 current loss 0.411061, current_train_items 198464.
I0302 19:01:20.220369 22626471084160 run.py:483] Algo bellman_ford step 6202 current loss 0.636709, current_train_items 198496.
I0302 19:01:20.250331 22626471084160 run.py:483] Algo bellman_ford step 6203 current loss 0.641060, current_train_items 198528.
I0302 19:01:20.283339 22626471084160 run.py:483] Algo bellman_ford step 6204 current loss 0.710354, current_train_items 198560.
I0302 19:01:20.303446 22626471084160 run.py:483] Algo bellman_ford step 6205 current loss 0.314384, current_train_items 198592.
I0302 19:01:20.318701 22626471084160 run.py:483] Algo bellman_ford step 6206 current loss 0.430487, current_train_items 198624.
I0302 19:01:20.342280 22626471084160 run.py:483] Algo bellman_ford step 6207 current loss 0.598261, current_train_items 198656.
I0302 19:01:20.373440 22626471084160 run.py:483] Algo bellman_ford step 6208 current loss 0.699669, current_train_items 198688.
I0302 19:01:20.408287 22626471084160 run.py:483] Algo bellman_ford step 6209 current loss 0.823203, current_train_items 198720.
I0302 19:01:20.428142 22626471084160 run.py:483] Algo bellman_ford step 6210 current loss 0.254592, current_train_items 198752.
I0302 19:01:20.444351 22626471084160 run.py:483] Algo bellman_ford step 6211 current loss 0.478014, current_train_items 198784.
I0302 19:01:20.467529 22626471084160 run.py:483] Algo bellman_ford step 6212 current loss 0.621304, current_train_items 198816.
I0302 19:01:20.497133 22626471084160 run.py:483] Algo bellman_ford step 6213 current loss 0.616912, current_train_items 198848.
I0302 19:01:20.532174 22626471084160 run.py:483] Algo bellman_ford step 6214 current loss 0.712953, current_train_items 198880.
I0302 19:01:20.551679 22626471084160 run.py:483] Algo bellman_ford step 6215 current loss 0.260329, current_train_items 198912.
I0302 19:01:20.567892 22626471084160 run.py:483] Algo bellman_ford step 6216 current loss 0.451739, current_train_items 198944.
I0302 19:01:20.590792 22626471084160 run.py:483] Algo bellman_ford step 6217 current loss 0.525289, current_train_items 198976.
I0302 19:01:20.621475 22626471084160 run.py:483] Algo bellman_ford step 6218 current loss 0.584714, current_train_items 199008.
I0302 19:01:20.654112 22626471084160 run.py:483] Algo bellman_ford step 6219 current loss 0.815140, current_train_items 199040.
I0302 19:01:20.674006 22626471084160 run.py:483] Algo bellman_ford step 6220 current loss 0.263284, current_train_items 199072.
I0302 19:01:20.689721 22626471084160 run.py:483] Algo bellman_ford step 6221 current loss 0.453661, current_train_items 199104.
I0302 19:01:20.714118 22626471084160 run.py:483] Algo bellman_ford step 6222 current loss 0.662566, current_train_items 199136.
I0302 19:01:20.743995 22626471084160 run.py:483] Algo bellman_ford step 6223 current loss 0.625239, current_train_items 199168.
I0302 19:01:20.778665 22626471084160 run.py:483] Algo bellman_ford step 6224 current loss 0.852217, current_train_items 199200.
I0302 19:01:20.798292 22626471084160 run.py:483] Algo bellman_ford step 6225 current loss 0.316375, current_train_items 199232.
I0302 19:01:20.814438 22626471084160 run.py:483] Algo bellman_ford step 6226 current loss 0.470917, current_train_items 199264.
I0302 19:01:20.837036 22626471084160 run.py:483] Algo bellman_ford step 6227 current loss 0.567817, current_train_items 199296.
I0302 19:01:20.866657 22626471084160 run.py:483] Algo bellman_ford step 6228 current loss 0.606438, current_train_items 199328.
I0302 19:01:20.898347 22626471084160 run.py:483] Algo bellman_ford step 6229 current loss 0.729127, current_train_items 199360.
I0302 19:01:20.918284 22626471084160 run.py:483] Algo bellman_ford step 6230 current loss 0.278197, current_train_items 199392.
I0302 19:01:20.934676 22626471084160 run.py:483] Algo bellman_ford step 6231 current loss 0.487018, current_train_items 199424.
I0302 19:01:20.958455 22626471084160 run.py:483] Algo bellman_ford step 6232 current loss 0.625480, current_train_items 199456.
I0302 19:01:20.988419 22626471084160 run.py:483] Algo bellman_ford step 6233 current loss 0.719979, current_train_items 199488.
I0302 19:01:21.021688 22626471084160 run.py:483] Algo bellman_ford step 6234 current loss 0.800159, current_train_items 199520.
I0302 19:01:21.041310 22626471084160 run.py:483] Algo bellman_ford step 6235 current loss 0.200296, current_train_items 199552.
I0302 19:01:21.057538 22626471084160 run.py:483] Algo bellman_ford step 6236 current loss 0.427236, current_train_items 199584.
I0302 19:01:21.080372 22626471084160 run.py:483] Algo bellman_ford step 6237 current loss 0.602870, current_train_items 199616.
I0302 19:01:21.111715 22626471084160 run.py:483] Algo bellman_ford step 6238 current loss 0.617991, current_train_items 199648.
I0302 19:01:21.143041 22626471084160 run.py:483] Algo bellman_ford step 6239 current loss 0.816746, current_train_items 199680.
I0302 19:01:21.162619 22626471084160 run.py:483] Algo bellman_ford step 6240 current loss 0.309895, current_train_items 199712.
I0302 19:01:21.178844 22626471084160 run.py:483] Algo bellman_ford step 6241 current loss 0.577084, current_train_items 199744.
I0302 19:01:21.202751 22626471084160 run.py:483] Algo bellman_ford step 6242 current loss 0.646300, current_train_items 199776.
I0302 19:01:21.234182 22626471084160 run.py:483] Algo bellman_ford step 6243 current loss 0.754632, current_train_items 199808.
I0302 19:01:21.266687 22626471084160 run.py:483] Algo bellman_ford step 6244 current loss 0.696499, current_train_items 199840.
I0302 19:01:21.286391 22626471084160 run.py:483] Algo bellman_ford step 6245 current loss 0.359253, current_train_items 199872.
I0302 19:01:21.302740 22626471084160 run.py:483] Algo bellman_ford step 6246 current loss 0.538976, current_train_items 199904.
I0302 19:01:21.325484 22626471084160 run.py:483] Algo bellman_ford step 6247 current loss 0.532378, current_train_items 199936.
I0302 19:01:21.357134 22626471084160 run.py:483] Algo bellman_ford step 6248 current loss 0.726502, current_train_items 199968.
I0302 19:01:21.389507 22626471084160 run.py:483] Algo bellman_ford step 6249 current loss 0.673454, current_train_items 200000.
I0302 19:01:21.409123 22626471084160 run.py:483] Algo bellman_ford step 6250 current loss 0.278733, current_train_items 200032.
I0302 19:01:21.417062 22626471084160 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0302 19:01:21.417176 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 19:01:21.434231 22626471084160 run.py:483] Algo bellman_ford step 6251 current loss 0.519319, current_train_items 200064.
I0302 19:01:21.457978 22626471084160 run.py:483] Algo bellman_ford step 6252 current loss 0.590757, current_train_items 200096.
I0302 19:01:21.490146 22626471084160 run.py:483] Algo bellman_ford step 6253 current loss 0.725646, current_train_items 200128.
I0302 19:01:21.522913 22626471084160 run.py:483] Algo bellman_ford step 6254 current loss 0.713380, current_train_items 200160.
I0302 19:01:21.542728 22626471084160 run.py:483] Algo bellman_ford step 6255 current loss 0.341799, current_train_items 200192.
I0302 19:01:21.558614 22626471084160 run.py:483] Algo bellman_ford step 6256 current loss 0.401284, current_train_items 200224.
I0302 19:01:21.582814 22626471084160 run.py:483] Algo bellman_ford step 6257 current loss 0.595199, current_train_items 200256.
I0302 19:01:21.613235 22626471084160 run.py:483] Algo bellman_ford step 6258 current loss 0.692721, current_train_items 200288.
I0302 19:01:21.645772 22626471084160 run.py:483] Algo bellman_ford step 6259 current loss 0.825320, current_train_items 200320.
I0302 19:01:21.665952 22626471084160 run.py:483] Algo bellman_ford step 6260 current loss 0.415079, current_train_items 200352.
I0302 19:01:21.682490 22626471084160 run.py:483] Algo bellman_ford step 6261 current loss 0.445115, current_train_items 200384.
I0302 19:01:21.705587 22626471084160 run.py:483] Algo bellman_ford step 6262 current loss 0.580953, current_train_items 200416.
I0302 19:01:21.736109 22626471084160 run.py:483] Algo bellman_ford step 6263 current loss 0.613107, current_train_items 200448.
I0302 19:01:21.769987 22626471084160 run.py:483] Algo bellman_ford step 6264 current loss 0.744387, current_train_items 200480.
I0302 19:01:21.789847 22626471084160 run.py:483] Algo bellman_ford step 6265 current loss 0.289167, current_train_items 200512.
I0302 19:01:21.806772 22626471084160 run.py:483] Algo bellman_ford step 6266 current loss 0.526079, current_train_items 200544.
I0302 19:01:21.831345 22626471084160 run.py:483] Algo bellman_ford step 6267 current loss 0.683374, current_train_items 200576.
I0302 19:01:21.861976 22626471084160 run.py:483] Algo bellman_ford step 6268 current loss 0.565680, current_train_items 200608.
I0302 19:01:21.892328 22626471084160 run.py:483] Algo bellman_ford step 6269 current loss 0.668684, current_train_items 200640.
I0302 19:01:21.912308 22626471084160 run.py:483] Algo bellman_ford step 6270 current loss 0.263963, current_train_items 200672.
I0302 19:01:21.928701 22626471084160 run.py:483] Algo bellman_ford step 6271 current loss 0.445591, current_train_items 200704.
I0302 19:01:21.952035 22626471084160 run.py:483] Algo bellman_ford step 6272 current loss 0.600410, current_train_items 200736.
I0302 19:01:21.981593 22626471084160 run.py:483] Algo bellman_ford step 6273 current loss 0.639975, current_train_items 200768.
I0302 19:01:22.016579 22626471084160 run.py:483] Algo bellman_ford step 6274 current loss 0.814756, current_train_items 200800.
I0302 19:01:22.036851 22626471084160 run.py:483] Algo bellman_ford step 6275 current loss 0.342013, current_train_items 200832.
I0302 19:01:22.053246 22626471084160 run.py:483] Algo bellman_ford step 6276 current loss 0.460620, current_train_items 200864.
I0302 19:01:22.076427 22626471084160 run.py:483] Algo bellman_ford step 6277 current loss 0.625790, current_train_items 200896.
I0302 19:01:22.106420 22626471084160 run.py:483] Algo bellman_ford step 6278 current loss 0.646829, current_train_items 200928.
I0302 19:01:22.140834 22626471084160 run.py:483] Algo bellman_ford step 6279 current loss 0.841606, current_train_items 200960.
I0302 19:01:22.160722 22626471084160 run.py:483] Algo bellman_ford step 6280 current loss 0.281771, current_train_items 200992.
I0302 19:01:22.176778 22626471084160 run.py:483] Algo bellman_ford step 6281 current loss 0.394642, current_train_items 201024.
I0302 19:01:22.200192 22626471084160 run.py:483] Algo bellman_ford step 6282 current loss 0.682150, current_train_items 201056.
I0302 19:01:22.231323 22626471084160 run.py:483] Algo bellman_ford step 6283 current loss 0.706513, current_train_items 201088.
I0302 19:01:22.265839 22626471084160 run.py:483] Algo bellman_ford step 6284 current loss 0.737733, current_train_items 201120.
I0302 19:01:22.285949 22626471084160 run.py:483] Algo bellman_ford step 6285 current loss 0.282360, current_train_items 201152.
I0302 19:01:22.302335 22626471084160 run.py:483] Algo bellman_ford step 6286 current loss 0.472956, current_train_items 201184.
I0302 19:01:22.325780 22626471084160 run.py:483] Algo bellman_ford step 6287 current loss 0.553243, current_train_items 201216.
I0302 19:01:22.356987 22626471084160 run.py:483] Algo bellman_ford step 6288 current loss 0.763439, current_train_items 201248.
I0302 19:01:22.390713 22626471084160 run.py:483] Algo bellman_ford step 6289 current loss 0.871351, current_train_items 201280.
I0302 19:01:22.410586 22626471084160 run.py:483] Algo bellman_ford step 6290 current loss 0.298348, current_train_items 201312.
I0302 19:01:22.427070 22626471084160 run.py:483] Algo bellman_ford step 6291 current loss 0.475550, current_train_items 201344.
I0302 19:01:22.450042 22626471084160 run.py:483] Algo bellman_ford step 6292 current loss 0.569166, current_train_items 201376.
I0302 19:01:22.481745 22626471084160 run.py:483] Algo bellman_ford step 6293 current loss 0.958524, current_train_items 201408.
I0302 19:01:22.516752 22626471084160 run.py:483] Algo bellman_ford step 6294 current loss 0.928672, current_train_items 201440.
I0302 19:01:22.536406 22626471084160 run.py:483] Algo bellman_ford step 6295 current loss 0.260658, current_train_items 201472.
I0302 19:01:22.553319 22626471084160 run.py:483] Algo bellman_ford step 6296 current loss 0.492607, current_train_items 201504.
I0302 19:01:22.576529 22626471084160 run.py:483] Algo bellman_ford step 6297 current loss 0.566867, current_train_items 201536.
I0302 19:01:22.607784 22626471084160 run.py:483] Algo bellman_ford step 6298 current loss 0.743053, current_train_items 201568.
I0302 19:01:22.640770 22626471084160 run.py:483] Algo bellman_ford step 6299 current loss 0.650125, current_train_items 201600.
I0302 19:01:22.660639 22626471084160 run.py:483] Algo bellman_ford step 6300 current loss 0.328696, current_train_items 201632.
I0302 19:01:22.668321 22626471084160 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.9091796875, 'score': 0.9091796875, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0302 19:01:22.668428 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.909, val scores are: bellman_ford: 0.909
I0302 19:01:22.685451 22626471084160 run.py:483] Algo bellman_ford step 6301 current loss 0.532809, current_train_items 201664.
I0302 19:01:22.708890 22626471084160 run.py:483] Algo bellman_ford step 6302 current loss 0.663735, current_train_items 201696.
I0302 19:01:22.739594 22626471084160 run.py:483] Algo bellman_ford step 6303 current loss 0.607775, current_train_items 201728.
I0302 19:01:22.771218 22626471084160 run.py:483] Algo bellman_ford step 6304 current loss 0.653698, current_train_items 201760.
I0302 19:01:22.790940 22626471084160 run.py:483] Algo bellman_ford step 6305 current loss 0.289779, current_train_items 201792.
I0302 19:01:22.806547 22626471084160 run.py:483] Algo bellman_ford step 6306 current loss 0.414261, current_train_items 201824.
I0302 19:01:22.829945 22626471084160 run.py:483] Algo bellman_ford step 6307 current loss 0.584435, current_train_items 201856.
I0302 19:01:22.861731 22626471084160 run.py:483] Algo bellman_ford step 6308 current loss 0.675717, current_train_items 201888.
I0302 19:01:22.894507 22626471084160 run.py:483] Algo bellman_ford step 6309 current loss 0.771754, current_train_items 201920.
I0302 19:01:22.914144 22626471084160 run.py:483] Algo bellman_ford step 6310 current loss 0.321052, current_train_items 201952.
I0302 19:01:22.930170 22626471084160 run.py:483] Algo bellman_ford step 6311 current loss 0.498042, current_train_items 201984.
I0302 19:01:22.952513 22626471084160 run.py:483] Algo bellman_ford step 6312 current loss 0.605846, current_train_items 202016.
I0302 19:01:22.984331 22626471084160 run.py:483] Algo bellman_ford step 6313 current loss 0.661854, current_train_items 202048.
I0302 19:01:23.018715 22626471084160 run.py:483] Algo bellman_ford step 6314 current loss 0.748623, current_train_items 202080.
I0302 19:01:23.038479 22626471084160 run.py:483] Algo bellman_ford step 6315 current loss 0.285949, current_train_items 202112.
I0302 19:01:23.054398 22626471084160 run.py:483] Algo bellman_ford step 6316 current loss 0.464804, current_train_items 202144.
I0302 19:01:23.077714 22626471084160 run.py:483] Algo bellman_ford step 6317 current loss 0.641798, current_train_items 202176.
I0302 19:01:23.107834 22626471084160 run.py:483] Algo bellman_ford step 6318 current loss 0.595869, current_train_items 202208.
I0302 19:01:23.140837 22626471084160 run.py:483] Algo bellman_ford step 6319 current loss 0.699378, current_train_items 202240.
I0302 19:01:23.160544 22626471084160 run.py:483] Algo bellman_ford step 6320 current loss 0.312275, current_train_items 202272.
I0302 19:01:23.176925 22626471084160 run.py:483] Algo bellman_ford step 6321 current loss 0.427512, current_train_items 202304.
I0302 19:01:23.201323 22626471084160 run.py:483] Algo bellman_ford step 6322 current loss 0.611155, current_train_items 202336.
I0302 19:01:23.233040 22626471084160 run.py:483] Algo bellman_ford step 6323 current loss 0.696183, current_train_items 202368.
I0302 19:01:23.267053 22626471084160 run.py:483] Algo bellman_ford step 6324 current loss 0.781399, current_train_items 202400.
I0302 19:01:23.286822 22626471084160 run.py:483] Algo bellman_ford step 6325 current loss 0.330143, current_train_items 202432.
I0302 19:01:23.302715 22626471084160 run.py:483] Algo bellman_ford step 6326 current loss 0.470448, current_train_items 202464.
I0302 19:01:23.327390 22626471084160 run.py:483] Algo bellman_ford step 6327 current loss 0.585337, current_train_items 202496.
I0302 19:01:23.358072 22626471084160 run.py:483] Algo bellman_ford step 6328 current loss 0.715544, current_train_items 202528.
I0302 19:01:23.392333 22626471084160 run.py:483] Algo bellman_ford step 6329 current loss 0.734501, current_train_items 202560.
I0302 19:01:23.411961 22626471084160 run.py:483] Algo bellman_ford step 6330 current loss 0.287298, current_train_items 202592.
I0302 19:01:23.428051 22626471084160 run.py:483] Algo bellman_ford step 6331 current loss 0.492000, current_train_items 202624.
I0302 19:01:23.451114 22626471084160 run.py:483] Algo bellman_ford step 6332 current loss 0.609829, current_train_items 202656.
I0302 19:01:23.482344 22626471084160 run.py:483] Algo bellman_ford step 6333 current loss 0.784011, current_train_items 202688.
I0302 19:01:23.515130 22626471084160 run.py:483] Algo bellman_ford step 6334 current loss 0.811624, current_train_items 202720.
I0302 19:01:23.534778 22626471084160 run.py:483] Algo bellman_ford step 6335 current loss 0.288208, current_train_items 202752.
I0302 19:01:23.551138 22626471084160 run.py:483] Algo bellman_ford step 6336 current loss 0.406217, current_train_items 202784.
I0302 19:01:23.575328 22626471084160 run.py:483] Algo bellman_ford step 6337 current loss 0.534550, current_train_items 202816.
I0302 19:01:23.607302 22626471084160 run.py:483] Algo bellman_ford step 6338 current loss 0.725077, current_train_items 202848.
I0302 19:01:23.640134 22626471084160 run.py:483] Algo bellman_ford step 6339 current loss 0.699853, current_train_items 202880.
I0302 19:01:23.659432 22626471084160 run.py:483] Algo bellman_ford step 6340 current loss 0.365190, current_train_items 202912.
I0302 19:01:23.675244 22626471084160 run.py:483] Algo bellman_ford step 6341 current loss 0.495925, current_train_items 202944.
I0302 19:01:23.699308 22626471084160 run.py:483] Algo bellman_ford step 6342 current loss 0.626023, current_train_items 202976.
I0302 19:01:23.729496 22626471084160 run.py:483] Algo bellman_ford step 6343 current loss 0.644144, current_train_items 203008.
I0302 19:01:23.760190 22626471084160 run.py:483] Algo bellman_ford step 6344 current loss 0.667501, current_train_items 203040.
I0302 19:01:23.779773 22626471084160 run.py:483] Algo bellman_ford step 6345 current loss 0.266240, current_train_items 203072.
I0302 19:01:23.796146 22626471084160 run.py:483] Algo bellman_ford step 6346 current loss 0.561399, current_train_items 203104.
I0302 19:01:23.819466 22626471084160 run.py:483] Algo bellman_ford step 6347 current loss 0.602633, current_train_items 203136.
I0302 19:01:23.849586 22626471084160 run.py:483] Algo bellman_ford step 6348 current loss 0.710899, current_train_items 203168.
I0302 19:01:23.884913 22626471084160 run.py:483] Algo bellman_ford step 6349 current loss 0.753247, current_train_items 203200.
I0302 19:01:23.904744 22626471084160 run.py:483] Algo bellman_ford step 6350 current loss 0.368259, current_train_items 203232.
I0302 19:01:23.912654 22626471084160 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0302 19:01:23.912759 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:01:23.929896 22626471084160 run.py:483] Algo bellman_ford step 6351 current loss 0.462035, current_train_items 203264.
I0302 19:01:23.953208 22626471084160 run.py:483] Algo bellman_ford step 6352 current loss 0.621638, current_train_items 203296.
I0302 19:01:23.984503 22626471084160 run.py:483] Algo bellman_ford step 6353 current loss 0.621840, current_train_items 203328.
I0302 19:01:24.017827 22626471084160 run.py:483] Algo bellman_ford step 6354 current loss 0.772956, current_train_items 203360.
I0302 19:01:24.038184 22626471084160 run.py:483] Algo bellman_ford step 6355 current loss 0.309507, current_train_items 203392.
I0302 19:01:24.054177 22626471084160 run.py:483] Algo bellman_ford step 6356 current loss 0.527578, current_train_items 203424.
I0302 19:01:24.078571 22626471084160 run.py:483] Algo bellman_ford step 6357 current loss 0.700003, current_train_items 203456.
I0302 19:01:24.109194 22626471084160 run.py:483] Algo bellman_ford step 6358 current loss 0.779986, current_train_items 203488.
I0302 19:01:24.143054 22626471084160 run.py:483] Algo bellman_ford step 6359 current loss 0.686797, current_train_items 203520.
I0302 19:01:24.162945 22626471084160 run.py:483] Algo bellman_ford step 6360 current loss 0.223200, current_train_items 203552.
I0302 19:01:24.179377 22626471084160 run.py:483] Algo bellman_ford step 6361 current loss 0.485789, current_train_items 203584.
I0302 19:01:24.201937 22626471084160 run.py:483] Algo bellman_ford step 6362 current loss 0.630997, current_train_items 203616.
I0302 19:01:24.232662 22626471084160 run.py:483] Algo bellman_ford step 6363 current loss 0.556237, current_train_items 203648.
I0302 19:01:24.265433 22626471084160 run.py:483] Algo bellman_ford step 6364 current loss 0.666838, current_train_items 203680.
I0302 19:01:24.284838 22626471084160 run.py:483] Algo bellman_ford step 6365 current loss 0.259435, current_train_items 203712.
I0302 19:01:24.301253 22626471084160 run.py:483] Algo bellman_ford step 6366 current loss 0.402330, current_train_items 203744.
I0302 19:01:24.324362 22626471084160 run.py:483] Algo bellman_ford step 6367 current loss 0.569297, current_train_items 203776.
I0302 19:01:24.356349 22626471084160 run.py:483] Algo bellman_ford step 6368 current loss 0.778420, current_train_items 203808.
I0302 19:01:24.388326 22626471084160 run.py:483] Algo bellman_ford step 6369 current loss 0.667150, current_train_items 203840.
I0302 19:01:24.408166 22626471084160 run.py:483] Algo bellman_ford step 6370 current loss 0.307668, current_train_items 203872.
I0302 19:01:24.424643 22626471084160 run.py:483] Algo bellman_ford step 6371 current loss 0.480006, current_train_items 203904.
I0302 19:01:24.448363 22626471084160 run.py:483] Algo bellman_ford step 6372 current loss 0.611057, current_train_items 203936.
I0302 19:01:24.478478 22626471084160 run.py:483] Algo bellman_ford step 6373 current loss 0.675685, current_train_items 203968.
I0302 19:01:24.511890 22626471084160 run.py:483] Algo bellman_ford step 6374 current loss 0.723460, current_train_items 204000.
I0302 19:01:24.531728 22626471084160 run.py:483] Algo bellman_ford step 6375 current loss 0.300218, current_train_items 204032.
I0302 19:01:24.547652 22626471084160 run.py:483] Algo bellman_ford step 6376 current loss 0.463884, current_train_items 204064.
I0302 19:01:24.571737 22626471084160 run.py:483] Algo bellman_ford step 6377 current loss 0.639191, current_train_items 204096.
I0302 19:01:24.602353 22626471084160 run.py:483] Algo bellman_ford step 6378 current loss 0.531017, current_train_items 204128.
I0302 19:01:24.638075 22626471084160 run.py:483] Algo bellman_ford step 6379 current loss 0.807921, current_train_items 204160.
I0302 19:01:24.657499 22626471084160 run.py:483] Algo bellman_ford step 6380 current loss 0.285401, current_train_items 204192.
I0302 19:01:24.673647 22626471084160 run.py:483] Algo bellman_ford step 6381 current loss 0.479003, current_train_items 204224.
I0302 19:01:24.697313 22626471084160 run.py:483] Algo bellman_ford step 6382 current loss 0.560583, current_train_items 204256.
I0302 19:01:24.727210 22626471084160 run.py:483] Algo bellman_ford step 6383 current loss 0.737060, current_train_items 204288.
I0302 19:01:24.760544 22626471084160 run.py:483] Algo bellman_ford step 6384 current loss 0.754962, current_train_items 204320.
I0302 19:01:24.780522 22626471084160 run.py:483] Algo bellman_ford step 6385 current loss 0.367449, current_train_items 204352.
I0302 19:01:24.796315 22626471084160 run.py:483] Algo bellman_ford step 6386 current loss 0.509699, current_train_items 204384.
I0302 19:01:24.820203 22626471084160 run.py:483] Algo bellman_ford step 6387 current loss 0.508587, current_train_items 204416.
I0302 19:01:24.850247 22626471084160 run.py:483] Algo bellman_ford step 6388 current loss 0.569034, current_train_items 204448.
I0302 19:01:24.883390 22626471084160 run.py:483] Algo bellman_ford step 6389 current loss 0.696159, current_train_items 204480.
I0302 19:01:24.903191 22626471084160 run.py:483] Algo bellman_ford step 6390 current loss 0.308687, current_train_items 204512.
I0302 19:01:24.919737 22626471084160 run.py:483] Algo bellman_ford step 6391 current loss 0.427767, current_train_items 204544.
I0302 19:01:24.942687 22626471084160 run.py:483] Algo bellman_ford step 6392 current loss 0.568548, current_train_items 204576.
I0302 19:01:24.971832 22626471084160 run.py:483] Algo bellman_ford step 6393 current loss 0.554841, current_train_items 204608.
I0302 19:01:25.005453 22626471084160 run.py:483] Algo bellman_ford step 6394 current loss 0.752913, current_train_items 204640.
I0302 19:01:25.025309 22626471084160 run.py:483] Algo bellman_ford step 6395 current loss 0.289681, current_train_items 204672.
I0302 19:01:25.041455 22626471084160 run.py:483] Algo bellman_ford step 6396 current loss 0.458260, current_train_items 204704.
I0302 19:01:25.064407 22626471084160 run.py:483] Algo bellman_ford step 6397 current loss 0.747918, current_train_items 204736.
I0302 19:01:25.094641 22626471084160 run.py:483] Algo bellman_ford step 6398 current loss 0.691496, current_train_items 204768.
I0302 19:01:25.126276 22626471084160 run.py:483] Algo bellman_ford step 6399 current loss 0.740746, current_train_items 204800.
I0302 19:01:25.146376 22626471084160 run.py:483] Algo bellman_ford step 6400 current loss 0.276937, current_train_items 204832.
I0302 19:01:25.153934 22626471084160 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0302 19:01:25.154039 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:01:25.170101 22626471084160 run.py:483] Algo bellman_ford step 6401 current loss 0.445222, current_train_items 204864.
I0302 19:01:25.193722 22626471084160 run.py:483] Algo bellman_ford step 6402 current loss 0.633708, current_train_items 204896.
I0302 19:01:25.224850 22626471084160 run.py:483] Algo bellman_ford step 6403 current loss 0.788272, current_train_items 204928.
I0302 19:01:25.258285 22626471084160 run.py:483] Algo bellman_ford step 6404 current loss 0.916564, current_train_items 204960.
I0302 19:01:25.278433 22626471084160 run.py:483] Algo bellman_ford step 6405 current loss 0.299151, current_train_items 204992.
I0302 19:01:25.294073 22626471084160 run.py:483] Algo bellman_ford step 6406 current loss 0.482073, current_train_items 205024.
I0302 19:01:25.317035 22626471084160 run.py:483] Algo bellman_ford step 6407 current loss 0.537275, current_train_items 205056.
I0302 19:01:25.347835 22626471084160 run.py:483] Algo bellman_ford step 6408 current loss 0.722625, current_train_items 205088.
I0302 19:01:25.382848 22626471084160 run.py:483] Algo bellman_ford step 6409 current loss 0.818025, current_train_items 205120.
I0302 19:01:25.402389 22626471084160 run.py:483] Algo bellman_ford step 6410 current loss 0.271834, current_train_items 205152.
I0302 19:01:25.418229 22626471084160 run.py:483] Algo bellman_ford step 6411 current loss 0.446781, current_train_items 205184.
I0302 19:01:25.440828 22626471084160 run.py:483] Algo bellman_ford step 6412 current loss 0.627432, current_train_items 205216.
I0302 19:01:25.472057 22626471084160 run.py:483] Algo bellman_ford step 6413 current loss 0.876748, current_train_items 205248.
I0302 19:01:25.505911 22626471084160 run.py:483] Algo bellman_ford step 6414 current loss 0.691461, current_train_items 205280.
I0302 19:01:25.525528 22626471084160 run.py:483] Algo bellman_ford step 6415 current loss 0.217592, current_train_items 205312.
I0302 19:01:25.541633 22626471084160 run.py:483] Algo bellman_ford step 6416 current loss 0.418540, current_train_items 205344.
I0302 19:01:25.565868 22626471084160 run.py:483] Algo bellman_ford step 6417 current loss 0.689132, current_train_items 205376.
I0302 19:01:25.597856 22626471084160 run.py:483] Algo bellman_ford step 6418 current loss 0.693697, current_train_items 205408.
I0302 19:01:25.631955 22626471084160 run.py:483] Algo bellman_ford step 6419 current loss 0.791289, current_train_items 205440.
I0302 19:01:25.651574 22626471084160 run.py:483] Algo bellman_ford step 6420 current loss 0.312909, current_train_items 205472.
I0302 19:01:25.667751 22626471084160 run.py:483] Algo bellman_ford step 6421 current loss 0.437430, current_train_items 205504.
I0302 19:01:25.690892 22626471084160 run.py:483] Algo bellman_ford step 6422 current loss 0.611064, current_train_items 205536.
I0302 19:01:25.721417 22626471084160 run.py:483] Algo bellman_ford step 6423 current loss 0.724883, current_train_items 205568.
I0302 19:01:25.754900 22626471084160 run.py:483] Algo bellman_ford step 6424 current loss 0.722430, current_train_items 205600.
I0302 19:01:25.774423 22626471084160 run.py:483] Algo bellman_ford step 6425 current loss 0.310984, current_train_items 205632.
I0302 19:01:25.790934 22626471084160 run.py:483] Algo bellman_ford step 6426 current loss 0.531501, current_train_items 205664.
I0302 19:01:25.814526 22626471084160 run.py:483] Algo bellman_ford step 6427 current loss 0.594682, current_train_items 205696.
I0302 19:01:25.846063 22626471084160 run.py:483] Algo bellman_ford step 6428 current loss 0.694536, current_train_items 205728.
I0302 19:01:25.881803 22626471084160 run.py:483] Algo bellman_ford step 6429 current loss 0.906210, current_train_items 205760.
I0302 19:01:25.901751 22626471084160 run.py:483] Algo bellman_ford step 6430 current loss 0.313279, current_train_items 205792.
I0302 19:01:25.917412 22626471084160 run.py:483] Algo bellman_ford step 6431 current loss 0.555016, current_train_items 205824.
I0302 19:01:25.940981 22626471084160 run.py:483] Algo bellman_ford step 6432 current loss 0.768858, current_train_items 205856.
I0302 19:01:25.972306 22626471084160 run.py:483] Algo bellman_ford step 6433 current loss 0.682961, current_train_items 205888.
I0302 19:01:26.005067 22626471084160 run.py:483] Algo bellman_ford step 6434 current loss 0.765970, current_train_items 205920.
I0302 19:01:26.024836 22626471084160 run.py:483] Algo bellman_ford step 6435 current loss 0.299346, current_train_items 205952.
I0302 19:01:26.040946 22626471084160 run.py:483] Algo bellman_ford step 6436 current loss 0.554568, current_train_items 205984.
I0302 19:01:26.064277 22626471084160 run.py:483] Algo bellman_ford step 6437 current loss 0.652051, current_train_items 206016.
I0302 19:01:26.096099 22626471084160 run.py:483] Algo bellman_ford step 6438 current loss 0.750023, current_train_items 206048.
I0302 19:01:26.129995 22626471084160 run.py:483] Algo bellman_ford step 6439 current loss 0.768198, current_train_items 206080.
I0302 19:01:26.149335 22626471084160 run.py:483] Algo bellman_ford step 6440 current loss 0.259481, current_train_items 206112.
I0302 19:01:26.165778 22626471084160 run.py:483] Algo bellman_ford step 6441 current loss 0.597663, current_train_items 206144.
I0302 19:01:26.188209 22626471084160 run.py:483] Algo bellman_ford step 6442 current loss 0.666328, current_train_items 206176.
I0302 19:01:26.217709 22626471084160 run.py:483] Algo bellman_ford step 6443 current loss 0.669338, current_train_items 206208.
I0302 19:01:26.251638 22626471084160 run.py:483] Algo bellman_ford step 6444 current loss 0.756069, current_train_items 206240.
I0302 19:01:26.271524 22626471084160 run.py:483] Algo bellman_ford step 6445 current loss 0.347276, current_train_items 206272.
I0302 19:01:26.287459 22626471084160 run.py:483] Algo bellman_ford step 6446 current loss 0.527781, current_train_items 206304.
I0302 19:01:26.311953 22626471084160 run.py:483] Algo bellman_ford step 6447 current loss 0.640485, current_train_items 206336.
I0302 19:01:26.343056 22626471084160 run.py:483] Algo bellman_ford step 6448 current loss 0.721576, current_train_items 206368.
I0302 19:01:26.376554 22626471084160 run.py:483] Algo bellman_ford step 6449 current loss 0.773679, current_train_items 206400.
I0302 19:01:26.396076 22626471084160 run.py:483] Algo bellman_ford step 6450 current loss 0.294209, current_train_items 206432.
I0302 19:01:26.403982 22626471084160 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0302 19:01:26.404089 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:01:26.420805 22626471084160 run.py:483] Algo bellman_ford step 6451 current loss 0.543386, current_train_items 206464.
I0302 19:01:26.445440 22626471084160 run.py:483] Algo bellman_ford step 6452 current loss 0.872512, current_train_items 206496.
I0302 19:01:26.475558 22626471084160 run.py:483] Algo bellman_ford step 6453 current loss 0.770177, current_train_items 206528.
I0302 19:01:26.507804 22626471084160 run.py:483] Algo bellman_ford step 6454 current loss 0.747842, current_train_items 206560.
I0302 19:01:26.527833 22626471084160 run.py:483] Algo bellman_ford step 6455 current loss 0.291560, current_train_items 206592.
I0302 19:01:26.543710 22626471084160 run.py:483] Algo bellman_ford step 6456 current loss 0.446891, current_train_items 206624.
I0302 19:01:26.566017 22626471084160 run.py:483] Algo bellman_ford step 6457 current loss 0.688607, current_train_items 206656.
I0302 19:01:26.596703 22626471084160 run.py:483] Algo bellman_ford step 6458 current loss 0.646953, current_train_items 206688.
I0302 19:01:26.628457 22626471084160 run.py:483] Algo bellman_ford step 6459 current loss 0.755901, current_train_items 206720.
I0302 19:01:26.648432 22626471084160 run.py:483] Algo bellman_ford step 6460 current loss 0.325872, current_train_items 206752.
I0302 19:01:26.664669 22626471084160 run.py:483] Algo bellman_ford step 6461 current loss 0.412972, current_train_items 206784.
I0302 19:01:26.687552 22626471084160 run.py:483] Algo bellman_ford step 6462 current loss 0.694475, current_train_items 206816.
I0302 19:01:26.717899 22626471084160 run.py:483] Algo bellman_ford step 6463 current loss 0.646276, current_train_items 206848.
I0302 19:01:26.753544 22626471084160 run.py:483] Algo bellman_ford step 6464 current loss 0.895081, current_train_items 206880.
I0302 19:01:26.773171 22626471084160 run.py:483] Algo bellman_ford step 6465 current loss 0.220454, current_train_items 206912.
I0302 19:01:26.789580 22626471084160 run.py:483] Algo bellman_ford step 6466 current loss 0.426153, current_train_items 206944.
I0302 19:01:26.814579 22626471084160 run.py:483] Algo bellman_ford step 6467 current loss 0.721395, current_train_items 206976.
I0302 19:01:26.846757 22626471084160 run.py:483] Algo bellman_ford step 6468 current loss 0.703982, current_train_items 207008.
I0302 19:01:26.881433 22626471084160 run.py:483] Algo bellman_ford step 6469 current loss 0.703403, current_train_items 207040.
I0302 19:01:26.901383 22626471084160 run.py:483] Algo bellman_ford step 6470 current loss 0.354434, current_train_items 207072.
I0302 19:01:26.918188 22626471084160 run.py:483] Algo bellman_ford step 6471 current loss 0.476721, current_train_items 207104.
I0302 19:01:26.940805 22626471084160 run.py:483] Algo bellman_ford step 6472 current loss 0.513031, current_train_items 207136.
I0302 19:01:26.971879 22626471084160 run.py:483] Algo bellman_ford step 6473 current loss 0.817470, current_train_items 207168.
I0302 19:01:27.004555 22626471084160 run.py:483] Algo bellman_ford step 6474 current loss 0.747102, current_train_items 207200.
I0302 19:01:27.024146 22626471084160 run.py:483] Algo bellman_ford step 6475 current loss 0.271852, current_train_items 207232.
I0302 19:01:27.040628 22626471084160 run.py:483] Algo bellman_ford step 6476 current loss 0.589227, current_train_items 207264.
I0302 19:01:27.062933 22626471084160 run.py:483] Algo bellman_ford step 6477 current loss 0.550401, current_train_items 207296.
I0302 19:01:27.094197 22626471084160 run.py:483] Algo bellman_ford step 6478 current loss 0.802266, current_train_items 207328.
I0302 19:01:27.129014 22626471084160 run.py:483] Algo bellman_ford step 6479 current loss 0.893002, current_train_items 207360.
I0302 19:01:27.148808 22626471084160 run.py:483] Algo bellman_ford step 6480 current loss 0.364371, current_train_items 207392.
I0302 19:01:27.164839 22626471084160 run.py:483] Algo bellman_ford step 6481 current loss 0.470112, current_train_items 207424.
I0302 19:01:27.188121 22626471084160 run.py:483] Algo bellman_ford step 6482 current loss 0.663656, current_train_items 207456.
I0302 19:01:27.219643 22626471084160 run.py:483] Algo bellman_ford step 6483 current loss 0.758103, current_train_items 207488.
I0302 19:01:27.253998 22626471084160 run.py:483] Algo bellman_ford step 6484 current loss 0.859800, current_train_items 207520.
I0302 19:01:27.274009 22626471084160 run.py:483] Algo bellman_ford step 6485 current loss 0.335937, current_train_items 207552.
I0302 19:01:27.290537 22626471084160 run.py:483] Algo bellman_ford step 6486 current loss 0.460290, current_train_items 207584.
I0302 19:01:27.315544 22626471084160 run.py:483] Algo bellman_ford step 6487 current loss 0.744890, current_train_items 207616.
I0302 19:01:27.345476 22626471084160 run.py:483] Algo bellman_ford step 6488 current loss 0.674220, current_train_items 207648.
I0302 19:01:27.379289 22626471084160 run.py:483] Algo bellman_ford step 6489 current loss 0.740264, current_train_items 207680.
I0302 19:01:27.399270 22626471084160 run.py:483] Algo bellman_ford step 6490 current loss 0.341093, current_train_items 207712.
I0302 19:01:27.415335 22626471084160 run.py:483] Algo bellman_ford step 6491 current loss 0.437668, current_train_items 207744.
I0302 19:01:27.437977 22626471084160 run.py:483] Algo bellman_ford step 6492 current loss 0.694894, current_train_items 207776.
I0302 19:01:27.468623 22626471084160 run.py:483] Algo bellman_ford step 6493 current loss 0.693005, current_train_items 207808.
I0302 19:01:27.501863 22626471084160 run.py:483] Algo bellman_ford step 6494 current loss 0.955639, current_train_items 207840.
I0302 19:01:27.521580 22626471084160 run.py:483] Algo bellman_ford step 6495 current loss 0.289996, current_train_items 207872.
I0302 19:01:27.537350 22626471084160 run.py:483] Algo bellman_ford step 6496 current loss 0.451293, current_train_items 207904.
I0302 19:01:27.561430 22626471084160 run.py:483] Algo bellman_ford step 6497 current loss 0.730572, current_train_items 207936.
I0302 19:01:27.591541 22626471084160 run.py:483] Algo bellman_ford step 6498 current loss 0.670306, current_train_items 207968.
I0302 19:01:27.626758 22626471084160 run.py:483] Algo bellman_ford step 6499 current loss 0.812802, current_train_items 208000.
I0302 19:01:27.646722 22626471084160 run.py:483] Algo bellman_ford step 6500 current loss 0.414311, current_train_items 208032.
I0302 19:01:27.654461 22626471084160 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0302 19:01:27.654566 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 19:01:27.671347 22626471084160 run.py:483] Algo bellman_ford step 6501 current loss 0.437835, current_train_items 208064.
I0302 19:01:27.695952 22626471084160 run.py:483] Algo bellman_ford step 6502 current loss 0.706459, current_train_items 208096.
I0302 19:01:27.726702 22626471084160 run.py:483] Algo bellman_ford step 6503 current loss 0.757737, current_train_items 208128.
I0302 19:01:27.761432 22626471084160 run.py:483] Algo bellman_ford step 6504 current loss 0.724883, current_train_items 208160.
I0302 19:01:27.781611 22626471084160 run.py:483] Algo bellman_ford step 6505 current loss 0.293952, current_train_items 208192.
I0302 19:01:27.796866 22626471084160 run.py:483] Algo bellman_ford step 6506 current loss 0.334628, current_train_items 208224.
I0302 19:01:27.820441 22626471084160 run.py:483] Algo bellman_ford step 6507 current loss 0.657584, current_train_items 208256.
I0302 19:01:27.852402 22626471084160 run.py:483] Algo bellman_ford step 6508 current loss 0.595880, current_train_items 208288.
I0302 19:01:27.883833 22626471084160 run.py:483] Algo bellman_ford step 6509 current loss 0.687488, current_train_items 208320.
I0302 19:01:27.903591 22626471084160 run.py:483] Algo bellman_ford step 6510 current loss 0.319628, current_train_items 208352.
I0302 19:01:27.919776 22626471084160 run.py:483] Algo bellman_ford step 6511 current loss 0.460086, current_train_items 208384.
I0302 19:01:27.944024 22626471084160 run.py:483] Algo bellman_ford step 6512 current loss 0.610253, current_train_items 208416.
I0302 19:01:27.974622 22626471084160 run.py:483] Algo bellman_ford step 6513 current loss 0.550265, current_train_items 208448.
I0302 19:01:28.009259 22626471084160 run.py:483] Algo bellman_ford step 6514 current loss 0.841850, current_train_items 208480.
I0302 19:01:28.028703 22626471084160 run.py:483] Algo bellman_ford step 6515 current loss 0.296453, current_train_items 208512.
I0302 19:01:28.044676 22626471084160 run.py:483] Algo bellman_ford step 6516 current loss 0.472092, current_train_items 208544.
I0302 19:01:28.068805 22626471084160 run.py:483] Algo bellman_ford step 6517 current loss 0.568493, current_train_items 208576.
I0302 19:01:28.099465 22626471084160 run.py:483] Algo bellman_ford step 6518 current loss 0.608250, current_train_items 208608.
I0302 19:01:28.132363 22626471084160 run.py:483] Algo bellman_ford step 6519 current loss 0.711696, current_train_items 208640.
I0302 19:01:28.151971 22626471084160 run.py:483] Algo bellman_ford step 6520 current loss 0.333718, current_train_items 208672.
I0302 19:01:28.168049 22626471084160 run.py:483] Algo bellman_ford step 6521 current loss 0.489737, current_train_items 208704.
I0302 19:01:28.192400 22626471084160 run.py:483] Algo bellman_ford step 6522 current loss 0.665253, current_train_items 208736.
I0302 19:01:28.222890 22626471084160 run.py:483] Algo bellman_ford step 6523 current loss 0.637890, current_train_items 208768.
I0302 19:01:28.253959 22626471084160 run.py:483] Algo bellman_ford step 6524 current loss 0.696231, current_train_items 208800.
I0302 19:01:28.273701 22626471084160 run.py:483] Algo bellman_ford step 6525 current loss 0.247283, current_train_items 208832.
I0302 19:01:28.289567 22626471084160 run.py:483] Algo bellman_ford step 6526 current loss 0.434893, current_train_items 208864.
I0302 19:01:28.312781 22626471084160 run.py:483] Algo bellman_ford step 6527 current loss 0.569461, current_train_items 208896.
I0302 19:01:28.342722 22626471084160 run.py:483] Algo bellman_ford step 6528 current loss 0.631468, current_train_items 208928.
I0302 19:01:28.376855 22626471084160 run.py:483] Algo bellman_ford step 6529 current loss 0.802856, current_train_items 208960.
I0302 19:01:28.396490 22626471084160 run.py:483] Algo bellman_ford step 6530 current loss 0.304458, current_train_items 208992.
I0302 19:01:28.412877 22626471084160 run.py:483] Algo bellman_ford step 6531 current loss 0.478540, current_train_items 209024.
I0302 19:01:28.437942 22626471084160 run.py:483] Algo bellman_ford step 6532 current loss 0.717924, current_train_items 209056.
I0302 19:01:28.468042 22626471084160 run.py:483] Algo bellman_ford step 6533 current loss 0.777271, current_train_items 209088.
I0302 19:01:28.501509 22626471084160 run.py:483] Algo bellman_ford step 6534 current loss 0.826241, current_train_items 209120.
I0302 19:01:28.520879 22626471084160 run.py:483] Algo bellman_ford step 6535 current loss 0.245237, current_train_items 209152.
I0302 19:01:28.537109 22626471084160 run.py:483] Algo bellman_ford step 6536 current loss 0.481398, current_train_items 209184.
I0302 19:01:28.560208 22626471084160 run.py:483] Algo bellman_ford step 6537 current loss 0.711529, current_train_items 209216.
I0302 19:01:28.590332 22626471084160 run.py:483] Algo bellman_ford step 6538 current loss 0.820023, current_train_items 209248.
I0302 19:01:28.623803 22626471084160 run.py:483] Algo bellman_ford step 6539 current loss 0.919060, current_train_items 209280.
I0302 19:01:28.643275 22626471084160 run.py:483] Algo bellman_ford step 6540 current loss 0.308799, current_train_items 209312.
I0302 19:01:28.659419 22626471084160 run.py:483] Algo bellman_ford step 6541 current loss 0.500780, current_train_items 209344.
I0302 19:01:28.682774 22626471084160 run.py:483] Algo bellman_ford step 6542 current loss 0.622416, current_train_items 209376.
I0302 19:01:28.713959 22626471084160 run.py:483] Algo bellman_ford step 6543 current loss 0.749937, current_train_items 209408.
I0302 19:01:28.749706 22626471084160 run.py:483] Algo bellman_ford step 6544 current loss 0.910596, current_train_items 209440.
I0302 19:01:28.769391 22626471084160 run.py:483] Algo bellman_ford step 6545 current loss 0.266577, current_train_items 209472.
I0302 19:01:28.785304 22626471084160 run.py:483] Algo bellman_ford step 6546 current loss 0.436906, current_train_items 209504.
I0302 19:01:28.808406 22626471084160 run.py:483] Algo bellman_ford step 6547 current loss 0.510063, current_train_items 209536.
I0302 19:01:28.840372 22626471084160 run.py:483] Algo bellman_ford step 6548 current loss 0.754112, current_train_items 209568.
I0302 19:01:28.875435 22626471084160 run.py:483] Algo bellman_ford step 6549 current loss 0.750973, current_train_items 209600.
I0302 19:01:28.895239 22626471084160 run.py:483] Algo bellman_ford step 6550 current loss 0.308914, current_train_items 209632.
I0302 19:01:28.903239 22626471084160 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0302 19:01:28.903347 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:01:28.920508 22626471084160 run.py:483] Algo bellman_ford step 6551 current loss 0.450380, current_train_items 209664.
I0302 19:01:28.944067 22626471084160 run.py:483] Algo bellman_ford step 6552 current loss 0.590693, current_train_items 209696.
I0302 19:01:28.976448 22626471084160 run.py:483] Algo bellman_ford step 6553 current loss 0.651852, current_train_items 209728.
I0302 19:01:29.011927 22626471084160 run.py:483] Algo bellman_ford step 6554 current loss 0.876077, current_train_items 209760.
I0302 19:01:29.031702 22626471084160 run.py:483] Algo bellman_ford step 6555 current loss 0.300783, current_train_items 209792.
I0302 19:01:29.047531 22626471084160 run.py:483] Algo bellman_ford step 6556 current loss 0.534696, current_train_items 209824.
I0302 19:01:29.071214 22626471084160 run.py:483] Algo bellman_ford step 6557 current loss 0.704805, current_train_items 209856.
I0302 19:01:29.102526 22626471084160 run.py:483] Algo bellman_ford step 6558 current loss 0.778023, current_train_items 209888.
I0302 19:01:29.136628 22626471084160 run.py:483] Algo bellman_ford step 6559 current loss 0.842776, current_train_items 209920.
I0302 19:01:29.156612 22626471084160 run.py:483] Algo bellman_ford step 6560 current loss 0.304211, current_train_items 209952.
I0302 19:01:29.172419 22626471084160 run.py:483] Algo bellman_ford step 6561 current loss 0.469160, current_train_items 209984.
I0302 19:01:29.194806 22626471084160 run.py:483] Algo bellman_ford step 6562 current loss 0.525041, current_train_items 210016.
I0302 19:01:29.225196 22626471084160 run.py:483] Algo bellman_ford step 6563 current loss 0.724644, current_train_items 210048.
I0302 19:01:29.256102 22626471084160 run.py:483] Algo bellman_ford step 6564 current loss 0.728348, current_train_items 210080.
I0302 19:01:29.275773 22626471084160 run.py:483] Algo bellman_ford step 6565 current loss 0.346788, current_train_items 210112.
I0302 19:01:29.291554 22626471084160 run.py:483] Algo bellman_ford step 6566 current loss 0.436823, current_train_items 210144.
I0302 19:01:29.315284 22626471084160 run.py:483] Algo bellman_ford step 6567 current loss 0.626865, current_train_items 210176.
I0302 19:01:29.344379 22626471084160 run.py:483] Algo bellman_ford step 6568 current loss 0.705491, current_train_items 210208.
I0302 19:01:29.378140 22626471084160 run.py:483] Algo bellman_ford step 6569 current loss 0.693399, current_train_items 210240.
I0302 19:01:29.397875 22626471084160 run.py:483] Algo bellman_ford step 6570 current loss 0.283619, current_train_items 210272.
I0302 19:01:29.414071 22626471084160 run.py:483] Algo bellman_ford step 6571 current loss 0.484830, current_train_items 210304.
I0302 19:01:29.436734 22626471084160 run.py:483] Algo bellman_ford step 6572 current loss 0.537068, current_train_items 210336.
I0302 19:01:29.466660 22626471084160 run.py:483] Algo bellman_ford step 6573 current loss 0.540321, current_train_items 210368.
I0302 19:01:29.499646 22626471084160 run.py:483] Algo bellman_ford step 6574 current loss 0.834760, current_train_items 210400.
I0302 19:01:29.519506 22626471084160 run.py:483] Algo bellman_ford step 6575 current loss 0.358827, current_train_items 210432.
I0302 19:01:29.535726 22626471084160 run.py:483] Algo bellman_ford step 6576 current loss 0.462388, current_train_items 210464.
I0302 19:01:29.558357 22626471084160 run.py:483] Algo bellman_ford step 6577 current loss 0.547531, current_train_items 210496.
I0302 19:01:29.588827 22626471084160 run.py:483] Algo bellman_ford step 6578 current loss 0.588713, current_train_items 210528.
I0302 19:01:29.622344 22626471084160 run.py:483] Algo bellman_ford step 6579 current loss 0.758521, current_train_items 210560.
I0302 19:01:29.641917 22626471084160 run.py:483] Algo bellman_ford step 6580 current loss 0.374872, current_train_items 210592.
I0302 19:01:29.658127 22626471084160 run.py:483] Algo bellman_ford step 6581 current loss 0.444475, current_train_items 210624.
I0302 19:01:29.681538 22626471084160 run.py:483] Algo bellman_ford step 6582 current loss 0.613232, current_train_items 210656.
I0302 19:01:29.713582 22626471084160 run.py:483] Algo bellman_ford step 6583 current loss 0.756393, current_train_items 210688.
I0302 19:01:29.745988 22626471084160 run.py:483] Algo bellman_ford step 6584 current loss 0.682192, current_train_items 210720.
I0302 19:01:29.765800 22626471084160 run.py:483] Algo bellman_ford step 6585 current loss 0.302916, current_train_items 210752.
I0302 19:01:29.781498 22626471084160 run.py:483] Algo bellman_ford step 6586 current loss 0.437445, current_train_items 210784.
I0302 19:01:29.803265 22626471084160 run.py:483] Algo bellman_ford step 6587 current loss 0.593655, current_train_items 210816.
I0302 19:01:29.833128 22626471084160 run.py:483] Algo bellman_ford step 6588 current loss 0.686563, current_train_items 210848.
I0302 19:01:29.864540 22626471084160 run.py:483] Algo bellman_ford step 6589 current loss 0.712296, current_train_items 210880.
I0302 19:01:29.884449 22626471084160 run.py:483] Algo bellman_ford step 6590 current loss 0.335697, current_train_items 210912.
I0302 19:01:29.900353 22626471084160 run.py:483] Algo bellman_ford step 6591 current loss 0.441745, current_train_items 210944.
I0302 19:01:29.923683 22626471084160 run.py:483] Algo bellman_ford step 6592 current loss 0.588496, current_train_items 210976.
I0302 19:01:29.954078 22626471084160 run.py:483] Algo bellman_ford step 6593 current loss 0.523492, current_train_items 211008.
I0302 19:01:29.986429 22626471084160 run.py:483] Algo bellman_ford step 6594 current loss 0.704310, current_train_items 211040.
I0302 19:01:30.005722 22626471084160 run.py:483] Algo bellman_ford step 6595 current loss 0.309858, current_train_items 211072.
I0302 19:01:30.021786 22626471084160 run.py:483] Algo bellman_ford step 6596 current loss 0.472326, current_train_items 211104.
I0302 19:01:30.045650 22626471084160 run.py:483] Algo bellman_ford step 6597 current loss 0.561265, current_train_items 211136.
I0302 19:01:30.077240 22626471084160 run.py:483] Algo bellman_ford step 6598 current loss 0.637680, current_train_items 211168.
I0302 19:01:30.107164 22626471084160 run.py:483] Algo bellman_ford step 6599 current loss 0.722216, current_train_items 211200.
I0302 19:01:30.126846 22626471084160 run.py:483] Algo bellman_ford step 6600 current loss 0.262356, current_train_items 211232.
I0302 19:01:30.134688 22626471084160 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0302 19:01:30.134793 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 19:01:30.151458 22626471084160 run.py:483] Algo bellman_ford step 6601 current loss 0.500289, current_train_items 211264.
I0302 19:01:30.173897 22626471084160 run.py:483] Algo bellman_ford step 6602 current loss 0.628279, current_train_items 211296.
I0302 19:01:30.204899 22626471084160 run.py:483] Algo bellman_ford step 6603 current loss 0.665365, current_train_items 211328.
I0302 19:01:30.239309 22626471084160 run.py:483] Algo bellman_ford step 6604 current loss 0.762793, current_train_items 211360.
I0302 19:01:30.259217 22626471084160 run.py:483] Algo bellman_ford step 6605 current loss 0.278412, current_train_items 211392.
I0302 19:01:30.274871 22626471084160 run.py:483] Algo bellman_ford step 6606 current loss 0.464051, current_train_items 211424.
I0302 19:01:30.298055 22626471084160 run.py:483] Algo bellman_ford step 6607 current loss 0.689565, current_train_items 211456.
I0302 19:01:30.329188 22626471084160 run.py:483] Algo bellman_ford step 6608 current loss 0.627941, current_train_items 211488.
I0302 19:01:30.360581 22626471084160 run.py:483] Algo bellman_ford step 6609 current loss 0.607966, current_train_items 211520.
I0302 19:01:30.380070 22626471084160 run.py:483] Algo bellman_ford step 6610 current loss 0.304894, current_train_items 211552.
I0302 19:01:30.395924 22626471084160 run.py:483] Algo bellman_ford step 6611 current loss 0.439911, current_train_items 211584.
I0302 19:01:30.418096 22626471084160 run.py:483] Algo bellman_ford step 6612 current loss 0.552477, current_train_items 211616.
I0302 19:01:30.448919 22626471084160 run.py:483] Algo bellman_ford step 6613 current loss 0.706080, current_train_items 211648.
I0302 19:01:30.484686 22626471084160 run.py:483] Algo bellman_ford step 6614 current loss 0.949484, current_train_items 211680.
I0302 19:01:30.504198 22626471084160 run.py:483] Algo bellman_ford step 6615 current loss 0.361938, current_train_items 211712.
I0302 19:01:30.520110 22626471084160 run.py:483] Algo bellman_ford step 6616 current loss 0.392482, current_train_items 211744.
I0302 19:01:30.543523 22626471084160 run.py:483] Algo bellman_ford step 6617 current loss 0.586793, current_train_items 211776.
I0302 19:01:30.574027 22626471084160 run.py:483] Algo bellman_ford step 6618 current loss 0.643246, current_train_items 211808.
I0302 19:01:30.607955 22626471084160 run.py:483] Algo bellman_ford step 6619 current loss 0.799298, current_train_items 211840.
I0302 19:01:30.627191 22626471084160 run.py:483] Algo bellman_ford step 6620 current loss 0.310713, current_train_items 211872.
I0302 19:01:30.642927 22626471084160 run.py:483] Algo bellman_ford step 6621 current loss 0.405039, current_train_items 211904.
I0302 19:01:30.664916 22626471084160 run.py:483] Algo bellman_ford step 6622 current loss 0.585587, current_train_items 211936.
I0302 19:01:30.696440 22626471084160 run.py:483] Algo bellman_ford step 6623 current loss 0.593088, current_train_items 211968.
I0302 19:01:30.728395 22626471084160 run.py:483] Algo bellman_ford step 6624 current loss 0.694201, current_train_items 212000.
I0302 19:01:30.747779 22626471084160 run.py:483] Algo bellman_ford step 6625 current loss 0.312121, current_train_items 212032.
I0302 19:01:30.763904 22626471084160 run.py:483] Algo bellman_ford step 6626 current loss 0.419152, current_train_items 212064.
I0302 19:01:30.787251 22626471084160 run.py:483] Algo bellman_ford step 6627 current loss 0.645563, current_train_items 212096.
I0302 19:01:30.816599 22626471084160 run.py:483] Algo bellman_ford step 6628 current loss 0.674469, current_train_items 212128.
I0302 19:01:30.848106 22626471084160 run.py:483] Algo bellman_ford step 6629 current loss 0.744623, current_train_items 212160.
I0302 19:01:30.867752 22626471084160 run.py:483] Algo bellman_ford step 6630 current loss 0.351807, current_train_items 212192.
I0302 19:01:30.883873 22626471084160 run.py:483] Algo bellman_ford step 6631 current loss 0.539535, current_train_items 212224.
I0302 19:01:30.907895 22626471084160 run.py:483] Algo bellman_ford step 6632 current loss 0.626810, current_train_items 212256.
I0302 19:01:30.939352 22626471084160 run.py:483] Algo bellman_ford step 6633 current loss 0.699024, current_train_items 212288.
I0302 19:01:30.971427 22626471084160 run.py:483] Algo bellman_ford step 6634 current loss 0.752922, current_train_items 212320.
I0302 19:01:30.990884 22626471084160 run.py:483] Algo bellman_ford step 6635 current loss 0.291440, current_train_items 212352.
I0302 19:01:31.007112 22626471084160 run.py:483] Algo bellman_ford step 6636 current loss 0.474062, current_train_items 212384.
I0302 19:01:31.030183 22626471084160 run.py:483] Algo bellman_ford step 6637 current loss 0.542695, current_train_items 212416.
I0302 19:01:31.060497 22626471084160 run.py:483] Algo bellman_ford step 6638 current loss 0.665179, current_train_items 212448.
I0302 19:01:31.094388 22626471084160 run.py:483] Algo bellman_ford step 6639 current loss 0.816738, current_train_items 212480.
I0302 19:01:31.113580 22626471084160 run.py:483] Algo bellman_ford step 6640 current loss 0.310587, current_train_items 212512.
I0302 19:01:31.129889 22626471084160 run.py:483] Algo bellman_ford step 6641 current loss 0.433039, current_train_items 212544.
I0302 19:01:31.154139 22626471084160 run.py:483] Algo bellman_ford step 6642 current loss 0.648033, current_train_items 212576.
I0302 19:01:31.184433 22626471084160 run.py:483] Algo bellman_ford step 6643 current loss 0.657033, current_train_items 212608.
I0302 19:01:31.218472 22626471084160 run.py:483] Algo bellman_ford step 6644 current loss 0.714761, current_train_items 212640.
I0302 19:01:31.237693 22626471084160 run.py:483] Algo bellman_ford step 6645 current loss 0.352456, current_train_items 212672.
I0302 19:01:31.253751 22626471084160 run.py:483] Algo bellman_ford step 6646 current loss 0.443296, current_train_items 212704.
I0302 19:01:31.276947 22626471084160 run.py:483] Algo bellman_ford step 6647 current loss 0.678812, current_train_items 212736.
I0302 19:01:31.307807 22626471084160 run.py:483] Algo bellman_ford step 6648 current loss 0.680767, current_train_items 212768.
I0302 19:01:31.341757 22626471084160 run.py:483] Algo bellman_ford step 6649 current loss 0.773471, current_train_items 212800.
I0302 19:01:31.361124 22626471084160 run.py:483] Algo bellman_ford step 6650 current loss 0.328946, current_train_items 212832.
I0302 19:01:31.369292 22626471084160 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.88671875, 'score': 0.88671875, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0302 19:01:31.369396 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.887, val scores are: bellman_ford: 0.887
I0302 19:01:31.386515 22626471084160 run.py:483] Algo bellman_ford step 6651 current loss 0.460908, current_train_items 212864.
I0302 19:01:31.411085 22626471084160 run.py:483] Algo bellman_ford step 6652 current loss 0.768775, current_train_items 212896.
I0302 19:01:31.442545 22626471084160 run.py:483] Algo bellman_ford step 6653 current loss 0.806556, current_train_items 212928.
I0302 19:01:31.472531 22626471084160 run.py:483] Algo bellman_ford step 6654 current loss 0.590411, current_train_items 212960.
I0302 19:01:31.492376 22626471084160 run.py:483] Algo bellman_ford step 6655 current loss 0.311953, current_train_items 212992.
I0302 19:01:31.508068 22626471084160 run.py:483] Algo bellman_ford step 6656 current loss 0.569191, current_train_items 213024.
I0302 19:01:31.531193 22626471084160 run.py:483] Algo bellman_ford step 6657 current loss 0.624335, current_train_items 213056.
I0302 19:01:31.562349 22626471084160 run.py:483] Algo bellman_ford step 6658 current loss 0.847355, current_train_items 213088.
I0302 19:01:31.597526 22626471084160 run.py:483] Algo bellman_ford step 6659 current loss 0.899544, current_train_items 213120.
I0302 19:01:31.617697 22626471084160 run.py:483] Algo bellman_ford step 6660 current loss 0.397007, current_train_items 213152.
I0302 19:01:31.634222 22626471084160 run.py:483] Algo bellman_ford step 6661 current loss 0.490311, current_train_items 213184.
I0302 19:01:31.658797 22626471084160 run.py:483] Algo bellman_ford step 6662 current loss 0.690386, current_train_items 213216.
I0302 19:01:31.688788 22626471084160 run.py:483] Algo bellman_ford step 6663 current loss 0.679901, current_train_items 213248.
I0302 19:01:31.722392 22626471084160 run.py:483] Algo bellman_ford step 6664 current loss 0.802202, current_train_items 213280.
I0302 19:01:31.742169 22626471084160 run.py:483] Algo bellman_ford step 6665 current loss 0.272424, current_train_items 213312.
I0302 19:01:31.758291 22626471084160 run.py:483] Algo bellman_ford step 6666 current loss 0.536353, current_train_items 213344.
I0302 19:01:31.783806 22626471084160 run.py:483] Algo bellman_ford step 6667 current loss 0.711515, current_train_items 213376.
I0302 19:01:31.815702 22626471084160 run.py:483] Algo bellman_ford step 6668 current loss 0.701236, current_train_items 213408.
I0302 19:01:31.851124 22626471084160 run.py:483] Algo bellman_ford step 6669 current loss 0.808022, current_train_items 213440.
I0302 19:01:31.871416 22626471084160 run.py:483] Algo bellman_ford step 6670 current loss 0.307778, current_train_items 213472.
I0302 19:01:31.887540 22626471084160 run.py:483] Algo bellman_ford step 6671 current loss 0.464976, current_train_items 213504.
I0302 19:01:31.911231 22626471084160 run.py:483] Algo bellman_ford step 6672 current loss 0.544435, current_train_items 213536.
I0302 19:01:31.942901 22626471084160 run.py:483] Algo bellman_ford step 6673 current loss 0.713514, current_train_items 213568.
I0302 19:01:31.975162 22626471084160 run.py:483] Algo bellman_ford step 6674 current loss 0.735281, current_train_items 213600.
I0302 19:01:31.995024 22626471084160 run.py:483] Algo bellman_ford step 6675 current loss 0.373392, current_train_items 213632.
I0302 19:01:32.010706 22626471084160 run.py:483] Algo bellman_ford step 6676 current loss 0.427314, current_train_items 213664.
I0302 19:01:32.034998 22626471084160 run.py:483] Algo bellman_ford step 6677 current loss 0.604589, current_train_items 213696.
I0302 19:01:32.066925 22626471084160 run.py:483] Algo bellman_ford step 6678 current loss 0.777716, current_train_items 213728.
I0302 19:01:32.102482 22626471084160 run.py:483] Algo bellman_ford step 6679 current loss 0.872119, current_train_items 213760.
I0302 19:01:32.122240 22626471084160 run.py:483] Algo bellman_ford step 6680 current loss 0.300787, current_train_items 213792.
I0302 19:01:32.138440 22626471084160 run.py:483] Algo bellman_ford step 6681 current loss 0.500420, current_train_items 213824.
I0302 19:01:32.163073 22626471084160 run.py:483] Algo bellman_ford step 6682 current loss 0.580127, current_train_items 213856.
I0302 19:01:32.193646 22626471084160 run.py:483] Algo bellman_ford step 6683 current loss 0.791120, current_train_items 213888.
I0302 19:01:32.229815 22626471084160 run.py:483] Algo bellman_ford step 6684 current loss 1.008909, current_train_items 213920.
I0302 19:01:32.250070 22626471084160 run.py:483] Algo bellman_ford step 6685 current loss 0.286076, current_train_items 213952.
I0302 19:01:32.266170 22626471084160 run.py:483] Algo bellman_ford step 6686 current loss 0.503070, current_train_items 213984.
I0302 19:01:32.289601 22626471084160 run.py:483] Algo bellman_ford step 6687 current loss 0.600018, current_train_items 214016.
I0302 19:01:32.320973 22626471084160 run.py:483] Algo bellman_ford step 6688 current loss 0.714953, current_train_items 214048.
I0302 19:01:32.354406 22626471084160 run.py:483] Algo bellman_ford step 6689 current loss 0.825192, current_train_items 214080.
I0302 19:01:32.374469 22626471084160 run.py:483] Algo bellman_ford step 6690 current loss 0.276876, current_train_items 214112.
I0302 19:01:32.390297 22626471084160 run.py:483] Algo bellman_ford step 6691 current loss 0.445283, current_train_items 214144.
I0302 19:01:32.414425 22626471084160 run.py:483] Algo bellman_ford step 6692 current loss 0.835606, current_train_items 214176.
I0302 19:01:32.445556 22626471084160 run.py:483] Algo bellman_ford step 6693 current loss 0.771025, current_train_items 214208.
I0302 19:01:32.479747 22626471084160 run.py:483] Algo bellman_ford step 6694 current loss 0.827761, current_train_items 214240.
I0302 19:01:32.499319 22626471084160 run.py:483] Algo bellman_ford step 6695 current loss 0.230571, current_train_items 214272.
I0302 19:01:32.515664 22626471084160 run.py:483] Algo bellman_ford step 6696 current loss 0.524035, current_train_items 214304.
I0302 19:01:32.539436 22626471084160 run.py:483] Algo bellman_ford step 6697 current loss 0.708136, current_train_items 214336.
I0302 19:01:32.571312 22626471084160 run.py:483] Algo bellman_ford step 6698 current loss 0.742271, current_train_items 214368.
I0302 19:01:32.605683 22626471084160 run.py:483] Algo bellman_ford step 6699 current loss 0.805694, current_train_items 214400.
I0302 19:01:32.625513 22626471084160 run.py:483] Algo bellman_ford step 6700 current loss 0.552737, current_train_items 214432.
I0302 19:01:32.633452 22626471084160 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0302 19:01:32.633559 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:01:32.650736 22626471084160 run.py:483] Algo bellman_ford step 6701 current loss 0.558932, current_train_items 214464.
I0302 19:01:32.674639 22626471084160 run.py:483] Algo bellman_ford step 6702 current loss 0.824450, current_train_items 214496.
I0302 19:01:32.706213 22626471084160 run.py:483] Algo bellman_ford step 6703 current loss 0.898146, current_train_items 214528.
I0302 19:01:32.739340 22626471084160 run.py:483] Algo bellman_ford step 6704 current loss 0.944789, current_train_items 214560.
I0302 19:01:32.759634 22626471084160 run.py:483] Algo bellman_ford step 6705 current loss 0.241472, current_train_items 214592.
I0302 19:01:32.775658 22626471084160 run.py:483] Algo bellman_ford step 6706 current loss 0.527918, current_train_items 214624.
I0302 19:01:32.800545 22626471084160 run.py:483] Algo bellman_ford step 6707 current loss 0.636244, current_train_items 214656.
I0302 19:01:32.831703 22626471084160 run.py:483] Algo bellman_ford step 6708 current loss 0.577715, current_train_items 214688.
I0302 19:01:32.866239 22626471084160 run.py:483] Algo bellman_ford step 6709 current loss 0.777778, current_train_items 214720.
I0302 19:01:32.886218 22626471084160 run.py:483] Algo bellman_ford step 6710 current loss 0.391159, current_train_items 214752.
I0302 19:01:32.902585 22626471084160 run.py:483] Algo bellman_ford step 6711 current loss 0.487459, current_train_items 214784.
I0302 19:01:32.925280 22626471084160 run.py:483] Algo bellman_ford step 6712 current loss 0.559447, current_train_items 214816.
I0302 19:01:32.957720 22626471084160 run.py:483] Algo bellman_ford step 6713 current loss 0.701207, current_train_items 214848.
I0302 19:01:32.991093 22626471084160 run.py:483] Algo bellman_ford step 6714 current loss 0.710569, current_train_items 214880.
I0302 19:01:33.010496 22626471084160 run.py:483] Algo bellman_ford step 6715 current loss 0.288255, current_train_items 214912.
I0302 19:01:33.026670 22626471084160 run.py:483] Algo bellman_ford step 6716 current loss 0.437883, current_train_items 214944.
I0302 19:01:33.050390 22626471084160 run.py:483] Algo bellman_ford step 6717 current loss 0.551452, current_train_items 214976.
I0302 19:01:33.081819 22626471084160 run.py:483] Algo bellman_ford step 6718 current loss 0.593156, current_train_items 215008.
I0302 19:01:33.117127 22626471084160 run.py:483] Algo bellman_ford step 6719 current loss 0.724764, current_train_items 215040.
I0302 19:01:33.136536 22626471084160 run.py:483] Algo bellman_ford step 6720 current loss 0.234635, current_train_items 215072.
I0302 19:01:33.152641 22626471084160 run.py:483] Algo bellman_ford step 6721 current loss 0.456729, current_train_items 215104.
I0302 19:01:33.175453 22626471084160 run.py:483] Algo bellman_ford step 6722 current loss 0.567507, current_train_items 215136.
I0302 19:01:33.207142 22626471084160 run.py:483] Algo bellman_ford step 6723 current loss 0.752818, current_train_items 215168.
I0302 19:01:33.239585 22626471084160 run.py:483] Algo bellman_ford step 6724 current loss 0.778921, current_train_items 215200.
I0302 19:01:33.259024 22626471084160 run.py:483] Algo bellman_ford step 6725 current loss 0.232648, current_train_items 215232.
I0302 19:01:33.275152 22626471084160 run.py:483] Algo bellman_ford step 6726 current loss 0.468461, current_train_items 215264.
I0302 19:01:33.298329 22626471084160 run.py:483] Algo bellman_ford step 6727 current loss 0.521414, current_train_items 215296.
I0302 19:01:33.328959 22626471084160 run.py:483] Algo bellman_ford step 6728 current loss 0.684934, current_train_items 215328.
I0302 19:01:33.361173 22626471084160 run.py:483] Algo bellman_ford step 6729 current loss 0.681965, current_train_items 215360.
I0302 19:01:33.380907 22626471084160 run.py:483] Algo bellman_ford step 6730 current loss 0.335737, current_train_items 215392.
I0302 19:01:33.396861 22626471084160 run.py:483] Algo bellman_ford step 6731 current loss 0.502674, current_train_items 215424.
I0302 19:01:33.421071 22626471084160 run.py:483] Algo bellman_ford step 6732 current loss 0.675092, current_train_items 215456.
I0302 19:01:33.450468 22626471084160 run.py:483] Algo bellman_ford step 6733 current loss 0.629892, current_train_items 215488.
I0302 19:01:33.482048 22626471084160 run.py:483] Algo bellman_ford step 6734 current loss 0.723645, current_train_items 215520.
I0302 19:01:33.501630 22626471084160 run.py:483] Algo bellman_ford step 6735 current loss 0.263922, current_train_items 215552.
I0302 19:01:33.518017 22626471084160 run.py:483] Algo bellman_ford step 6736 current loss 0.594136, current_train_items 215584.
I0302 19:01:33.542301 22626471084160 run.py:483] Algo bellman_ford step 6737 current loss 0.625649, current_train_items 215616.
I0302 19:01:33.572399 22626471084160 run.py:483] Algo bellman_ford step 6738 current loss 0.672333, current_train_items 215648.
I0302 19:01:33.607834 22626471084160 run.py:483] Algo bellman_ford step 6739 current loss 0.954562, current_train_items 215680.
I0302 19:01:33.627284 22626471084160 run.py:483] Algo bellman_ford step 6740 current loss 0.299930, current_train_items 215712.
I0302 19:01:33.643592 22626471084160 run.py:483] Algo bellman_ford step 6741 current loss 0.442465, current_train_items 215744.
I0302 19:01:33.668022 22626471084160 run.py:483] Algo bellman_ford step 6742 current loss 0.603248, current_train_items 215776.
I0302 19:01:33.697915 22626471084160 run.py:483] Algo bellman_ford step 6743 current loss 0.589576, current_train_items 215808.
I0302 19:01:33.732351 22626471084160 run.py:483] Algo bellman_ford step 6744 current loss 0.857237, current_train_items 215840.
I0302 19:01:33.752004 22626471084160 run.py:483] Algo bellman_ford step 6745 current loss 0.291004, current_train_items 215872.
I0302 19:01:33.768241 22626471084160 run.py:483] Algo bellman_ford step 6746 current loss 0.473499, current_train_items 215904.
I0302 19:01:33.791792 22626471084160 run.py:483] Algo bellman_ford step 6747 current loss 0.553416, current_train_items 215936.
I0302 19:01:33.822287 22626471084160 run.py:483] Algo bellman_ford step 6748 current loss 0.603709, current_train_items 215968.
I0302 19:01:33.856130 22626471084160 run.py:483] Algo bellman_ford step 6749 current loss 0.883386, current_train_items 216000.
I0302 19:01:33.875769 22626471084160 run.py:483] Algo bellman_ford step 6750 current loss 0.357221, current_train_items 216032.
I0302 19:01:33.883659 22626471084160 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0302 19:01:33.883764 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 19:01:33.900646 22626471084160 run.py:483] Algo bellman_ford step 6751 current loss 0.464969, current_train_items 216064.
I0302 19:01:33.925309 22626471084160 run.py:483] Algo bellman_ford step 6752 current loss 0.593287, current_train_items 216096.
I0302 19:01:33.957294 22626471084160 run.py:483] Algo bellman_ford step 6753 current loss 0.688952, current_train_items 216128.
I0302 19:01:33.992074 22626471084160 run.py:483] Algo bellman_ford step 6754 current loss 0.723258, current_train_items 216160.
I0302 19:01:34.012219 22626471084160 run.py:483] Algo bellman_ford step 6755 current loss 0.326321, current_train_items 216192.
I0302 19:01:34.028715 22626471084160 run.py:483] Algo bellman_ford step 6756 current loss 0.546442, current_train_items 216224.
I0302 19:01:34.051953 22626471084160 run.py:483] Algo bellman_ford step 6757 current loss 0.504313, current_train_items 216256.
I0302 19:01:34.083249 22626471084160 run.py:483] Algo bellman_ford step 6758 current loss 0.776612, current_train_items 216288.
I0302 19:01:34.117953 22626471084160 run.py:483] Algo bellman_ford step 6759 current loss 0.877826, current_train_items 216320.
I0302 19:01:34.138004 22626471084160 run.py:483] Algo bellman_ford step 6760 current loss 0.288649, current_train_items 216352.
I0302 19:01:34.154503 22626471084160 run.py:483] Algo bellman_ford step 6761 current loss 0.376711, current_train_items 216384.
I0302 19:01:34.177708 22626471084160 run.py:483] Algo bellman_ford step 6762 current loss 0.522281, current_train_items 216416.
I0302 19:01:34.209176 22626471084160 run.py:483] Algo bellman_ford step 6763 current loss 0.674064, current_train_items 216448.
I0302 19:01:34.242087 22626471084160 run.py:483] Algo bellman_ford step 6764 current loss 0.774647, current_train_items 216480.
I0302 19:01:34.261810 22626471084160 run.py:483] Algo bellman_ford step 6765 current loss 0.256490, current_train_items 216512.
I0302 19:01:34.278292 22626471084160 run.py:483] Algo bellman_ford step 6766 current loss 0.481415, current_train_items 216544.
I0302 19:01:34.301721 22626471084160 run.py:483] Algo bellman_ford step 6767 current loss 0.562497, current_train_items 216576.
I0302 19:01:34.332038 22626471084160 run.py:483] Algo bellman_ford step 6768 current loss 0.627050, current_train_items 216608.
I0302 19:01:34.365330 22626471084160 run.py:483] Algo bellman_ford step 6769 current loss 0.759271, current_train_items 216640.
I0302 19:01:34.385235 22626471084160 run.py:483] Algo bellman_ford step 6770 current loss 0.253694, current_train_items 216672.
I0302 19:01:34.401343 22626471084160 run.py:483] Algo bellman_ford step 6771 current loss 0.418709, current_train_items 216704.
I0302 19:01:34.423907 22626471084160 run.py:483] Algo bellman_ford step 6772 current loss 0.553032, current_train_items 216736.
I0302 19:01:34.455231 22626471084160 run.py:483] Algo bellman_ford step 6773 current loss 0.628643, current_train_items 216768.
I0302 19:01:34.487269 22626471084160 run.py:483] Algo bellman_ford step 6774 current loss 0.740965, current_train_items 216800.
I0302 19:01:34.507250 22626471084160 run.py:483] Algo bellman_ford step 6775 current loss 0.298995, current_train_items 216832.
I0302 19:01:34.523481 22626471084160 run.py:483] Algo bellman_ford step 6776 current loss 0.442873, current_train_items 216864.
I0302 19:01:34.546494 22626471084160 run.py:483] Algo bellman_ford step 6777 current loss 0.585686, current_train_items 216896.
I0302 19:01:34.578924 22626471084160 run.py:483] Algo bellman_ford step 6778 current loss 0.917411, current_train_items 216928.
I0302 19:01:34.611894 22626471084160 run.py:483] Algo bellman_ford step 6779 current loss 0.832925, current_train_items 216960.
I0302 19:01:34.631447 22626471084160 run.py:483] Algo bellman_ford step 6780 current loss 0.304625, current_train_items 216992.
I0302 19:01:34.647485 22626471084160 run.py:483] Algo bellman_ford step 6781 current loss 0.491717, current_train_items 217024.
I0302 19:01:34.670373 22626471084160 run.py:483] Algo bellman_ford step 6782 current loss 0.622423, current_train_items 217056.
I0302 19:01:34.702619 22626471084160 run.py:483] Algo bellman_ford step 6783 current loss 0.738865, current_train_items 217088.
I0302 19:01:34.735668 22626471084160 run.py:483] Algo bellman_ford step 6784 current loss 0.830549, current_train_items 217120.
I0302 19:01:34.755599 22626471084160 run.py:483] Algo bellman_ford step 6785 current loss 0.240776, current_train_items 217152.
I0302 19:01:34.771427 22626471084160 run.py:483] Algo bellman_ford step 6786 current loss 0.459433, current_train_items 217184.
I0302 19:01:34.794232 22626471084160 run.py:483] Algo bellman_ford step 6787 current loss 0.512668, current_train_items 217216.
I0302 19:01:34.823973 22626471084160 run.py:483] Algo bellman_ford step 6788 current loss 0.716026, current_train_items 217248.
I0302 19:01:34.857869 22626471084160 run.py:483] Algo bellman_ford step 6789 current loss 0.851382, current_train_items 217280.
I0302 19:01:34.878011 22626471084160 run.py:483] Algo bellman_ford step 6790 current loss 0.384559, current_train_items 217312.
I0302 19:01:34.894132 22626471084160 run.py:483] Algo bellman_ford step 6791 current loss 0.463321, current_train_items 217344.
I0302 19:01:34.917817 22626471084160 run.py:483] Algo bellman_ford step 6792 current loss 0.517160, current_train_items 217376.
I0302 19:01:34.947837 22626471084160 run.py:483] Algo bellman_ford step 6793 current loss 0.668876, current_train_items 217408.
I0302 19:01:34.981165 22626471084160 run.py:483] Algo bellman_ford step 6794 current loss 0.969201, current_train_items 217440.
I0302 19:01:35.000951 22626471084160 run.py:483] Algo bellman_ford step 6795 current loss 0.261161, current_train_items 217472.
I0302 19:01:35.017047 22626471084160 run.py:483] Algo bellman_ford step 6796 current loss 0.404896, current_train_items 217504.
I0302 19:01:35.041629 22626471084160 run.py:483] Algo bellman_ford step 6797 current loss 0.675727, current_train_items 217536.
I0302 19:01:35.071889 22626471084160 run.py:483] Algo bellman_ford step 6798 current loss 0.761565, current_train_items 217568.
I0302 19:01:35.105107 22626471084160 run.py:483] Algo bellman_ford step 6799 current loss 0.848126, current_train_items 217600.
I0302 19:01:35.124914 22626471084160 run.py:483] Algo bellman_ford step 6800 current loss 0.349294, current_train_items 217632.
I0302 19:01:35.132745 22626471084160 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0302 19:01:35.132850 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 19:01:35.149323 22626471084160 run.py:483] Algo bellman_ford step 6801 current loss 0.458973, current_train_items 217664.
I0302 19:01:35.173276 22626471084160 run.py:483] Algo bellman_ford step 6802 current loss 0.621039, current_train_items 217696.
I0302 19:01:35.205766 22626471084160 run.py:483] Algo bellman_ford step 6803 current loss 0.686103, current_train_items 217728.
I0302 19:01:35.240079 22626471084160 run.py:483] Algo bellman_ford step 6804 current loss 0.750662, current_train_items 217760.
I0302 19:01:35.260412 22626471084160 run.py:483] Algo bellman_ford step 6805 current loss 0.253351, current_train_items 217792.
I0302 19:01:35.276548 22626471084160 run.py:483] Algo bellman_ford step 6806 current loss 0.354278, current_train_items 217824.
I0302 19:01:35.300750 22626471084160 run.py:483] Algo bellman_ford step 6807 current loss 0.646748, current_train_items 217856.
I0302 19:01:35.330669 22626471084160 run.py:483] Algo bellman_ford step 6808 current loss 0.705050, current_train_items 217888.
I0302 19:01:35.364353 22626471084160 run.py:483] Algo bellman_ford step 6809 current loss 0.736577, current_train_items 217920.
I0302 19:01:35.384058 22626471084160 run.py:483] Algo bellman_ford step 6810 current loss 0.398084, current_train_items 217952.
I0302 19:01:35.399727 22626471084160 run.py:483] Algo bellman_ford step 6811 current loss 0.397389, current_train_items 217984.
I0302 19:01:35.424447 22626471084160 run.py:483] Algo bellman_ford step 6812 current loss 0.674791, current_train_items 218016.
I0302 19:01:35.454754 22626471084160 run.py:483] Algo bellman_ford step 6813 current loss 0.731054, current_train_items 218048.
I0302 19:01:35.486145 22626471084160 run.py:483] Algo bellman_ford step 6814 current loss 0.879656, current_train_items 218080.
I0302 19:01:35.505856 22626471084160 run.py:483] Algo bellman_ford step 6815 current loss 0.318084, current_train_items 218112.
I0302 19:01:35.521956 22626471084160 run.py:483] Algo bellman_ford step 6816 current loss 0.428683, current_train_items 218144.
I0302 19:01:35.544464 22626471084160 run.py:483] Algo bellman_ford step 6817 current loss 0.600872, current_train_items 218176.
I0302 19:01:35.575754 22626471084160 run.py:483] Algo bellman_ford step 6818 current loss 0.725207, current_train_items 218208.
I0302 19:01:35.610903 22626471084160 run.py:483] Algo bellman_ford step 6819 current loss 0.833346, current_train_items 218240.
I0302 19:01:35.630494 22626471084160 run.py:483] Algo bellman_ford step 6820 current loss 0.259959, current_train_items 218272.
I0302 19:01:35.646544 22626471084160 run.py:483] Algo bellman_ford step 6821 current loss 0.425600, current_train_items 218304.
I0302 19:01:35.669785 22626471084160 run.py:483] Algo bellman_ford step 6822 current loss 0.657249, current_train_items 218336.
I0302 19:01:35.700883 22626471084160 run.py:483] Algo bellman_ford step 6823 current loss 0.778818, current_train_items 218368.
I0302 19:01:35.735692 22626471084160 run.py:483] Algo bellman_ford step 6824 current loss 0.835559, current_train_items 218400.
I0302 19:01:35.755492 22626471084160 run.py:483] Algo bellman_ford step 6825 current loss 0.278730, current_train_items 218432.
I0302 19:01:35.771807 22626471084160 run.py:483] Algo bellman_ford step 6826 current loss 0.483943, current_train_items 218464.
I0302 19:01:35.795327 22626471084160 run.py:483] Algo bellman_ford step 6827 current loss 0.560157, current_train_items 218496.
I0302 19:01:35.825595 22626471084160 run.py:483] Algo bellman_ford step 6828 current loss 0.633992, current_train_items 218528.
I0302 19:01:35.861008 22626471084160 run.py:483] Algo bellman_ford step 6829 current loss 0.821420, current_train_items 218560.
I0302 19:01:35.880686 22626471084160 run.py:483] Algo bellman_ford step 6830 current loss 0.319886, current_train_items 218592.
I0302 19:01:35.896628 22626471084160 run.py:483] Algo bellman_ford step 6831 current loss 0.413550, current_train_items 218624.
I0302 19:01:35.921869 22626471084160 run.py:483] Algo bellman_ford step 6832 current loss 0.615495, current_train_items 218656.
I0302 19:01:35.952738 22626471084160 run.py:483] Algo bellman_ford step 6833 current loss 0.594796, current_train_items 218688.
I0302 19:01:35.986309 22626471084160 run.py:483] Algo bellman_ford step 6834 current loss 0.725873, current_train_items 218720.
I0302 19:01:36.006002 22626471084160 run.py:483] Algo bellman_ford step 6835 current loss 0.302424, current_train_items 218752.
I0302 19:01:36.022319 22626471084160 run.py:483] Algo bellman_ford step 6836 current loss 0.468459, current_train_items 218784.
I0302 19:01:36.045599 22626471084160 run.py:483] Algo bellman_ford step 6837 current loss 0.606112, current_train_items 218816.
I0302 19:01:36.077488 22626471084160 run.py:483] Algo bellman_ford step 6838 current loss 0.652296, current_train_items 218848.
I0302 19:01:36.109431 22626471084160 run.py:483] Algo bellman_ford step 6839 current loss 0.755453, current_train_items 218880.
I0302 19:01:36.128911 22626471084160 run.py:483] Algo bellman_ford step 6840 current loss 0.267592, current_train_items 218912.
I0302 19:01:36.145521 22626471084160 run.py:483] Algo bellman_ford step 6841 current loss 0.574512, current_train_items 218944.
I0302 19:01:36.170008 22626471084160 run.py:483] Algo bellman_ford step 6842 current loss 0.709568, current_train_items 218976.
I0302 19:01:36.199759 22626471084160 run.py:483] Algo bellman_ford step 6843 current loss 0.687680, current_train_items 219008.
I0302 19:01:36.232961 22626471084160 run.py:483] Algo bellman_ford step 6844 current loss 0.742313, current_train_items 219040.
I0302 19:01:36.252406 22626471084160 run.py:483] Algo bellman_ford step 6845 current loss 0.314676, current_train_items 219072.
I0302 19:01:36.269074 22626471084160 run.py:483] Algo bellman_ford step 6846 current loss 0.428143, current_train_items 219104.
I0302 19:01:36.293105 22626471084160 run.py:483] Algo bellman_ford step 6847 current loss 0.644920, current_train_items 219136.
I0302 19:01:36.324674 22626471084160 run.py:483] Algo bellman_ford step 6848 current loss 0.832253, current_train_items 219168.
I0302 19:01:36.359142 22626471084160 run.py:483] Algo bellman_ford step 6849 current loss 0.945104, current_train_items 219200.
I0302 19:01:36.378799 22626471084160 run.py:483] Algo bellman_ford step 6850 current loss 0.326446, current_train_items 219232.
I0302 19:01:36.387031 22626471084160 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0302 19:01:36.387135 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 19:01:36.403935 22626471084160 run.py:483] Algo bellman_ford step 6851 current loss 0.493827, current_train_items 219264.
I0302 19:01:36.428027 22626471084160 run.py:483] Algo bellman_ford step 6852 current loss 0.704516, current_train_items 219296.
I0302 19:01:36.459096 22626471084160 run.py:483] Algo bellman_ford step 6853 current loss 0.648815, current_train_items 219328.
I0302 19:01:36.493763 22626471084160 run.py:483] Algo bellman_ford step 6854 current loss 0.806976, current_train_items 219360.
I0302 19:01:36.513827 22626471084160 run.py:483] Algo bellman_ford step 6855 current loss 0.295810, current_train_items 219392.
I0302 19:01:36.529528 22626471084160 run.py:483] Algo bellman_ford step 6856 current loss 0.455941, current_train_items 219424.
I0302 19:01:36.553136 22626471084160 run.py:483] Algo bellman_ford step 6857 current loss 0.681849, current_train_items 219456.
I0302 19:01:36.583614 22626471084160 run.py:483] Algo bellman_ford step 6858 current loss 0.657214, current_train_items 219488.
I0302 19:01:36.617277 22626471084160 run.py:483] Algo bellman_ford step 6859 current loss 0.747801, current_train_items 219520.
I0302 19:01:36.637277 22626471084160 run.py:483] Algo bellman_ford step 6860 current loss 0.339758, current_train_items 219552.
I0302 19:01:36.653361 22626471084160 run.py:483] Algo bellman_ford step 6861 current loss 0.472220, current_train_items 219584.
I0302 19:01:36.676183 22626471084160 run.py:483] Algo bellman_ford step 6862 current loss 0.557387, current_train_items 219616.
I0302 19:01:36.706739 22626471084160 run.py:483] Algo bellman_ford step 6863 current loss 0.618558, current_train_items 219648.
I0302 19:01:36.738903 22626471084160 run.py:483] Algo bellman_ford step 6864 current loss 0.777123, current_train_items 219680.
I0302 19:01:36.758641 22626471084160 run.py:483] Algo bellman_ford step 6865 current loss 0.302216, current_train_items 219712.
I0302 19:01:36.774793 22626471084160 run.py:483] Algo bellman_ford step 6866 current loss 0.406266, current_train_items 219744.
I0302 19:01:36.799220 22626471084160 run.py:483] Algo bellman_ford step 6867 current loss 0.680211, current_train_items 219776.
I0302 19:01:36.830383 22626471084160 run.py:483] Algo bellman_ford step 6868 current loss 0.706709, current_train_items 219808.
I0302 19:01:36.866126 22626471084160 run.py:483] Algo bellman_ford step 6869 current loss 0.780796, current_train_items 219840.
I0302 19:01:36.886244 22626471084160 run.py:483] Algo bellman_ford step 6870 current loss 0.322271, current_train_items 219872.
I0302 19:01:36.902695 22626471084160 run.py:483] Algo bellman_ford step 6871 current loss 0.461055, current_train_items 219904.
I0302 19:01:36.925796 22626471084160 run.py:483] Algo bellman_ford step 6872 current loss 0.605428, current_train_items 219936.
I0302 19:01:36.955917 22626471084160 run.py:483] Algo bellman_ford step 6873 current loss 0.618100, current_train_items 219968.
I0302 19:01:36.989464 22626471084160 run.py:483] Algo bellman_ford step 6874 current loss 0.736334, current_train_items 220000.
I0302 19:01:37.009337 22626471084160 run.py:483] Algo bellman_ford step 6875 current loss 0.323921, current_train_items 220032.
I0302 19:01:37.025237 22626471084160 run.py:483] Algo bellman_ford step 6876 current loss 0.424155, current_train_items 220064.
I0302 19:01:37.048572 22626471084160 run.py:483] Algo bellman_ford step 6877 current loss 0.658548, current_train_items 220096.
I0302 19:01:37.079409 22626471084160 run.py:483] Algo bellman_ford step 6878 current loss 0.693873, current_train_items 220128.
I0302 19:01:37.113417 22626471084160 run.py:483] Algo bellman_ford step 6879 current loss 0.765755, current_train_items 220160.
I0302 19:01:37.133137 22626471084160 run.py:483] Algo bellman_ford step 6880 current loss 0.354504, current_train_items 220192.
I0302 19:01:37.148921 22626471084160 run.py:483] Algo bellman_ford step 6881 current loss 0.448252, current_train_items 220224.
I0302 19:01:37.173487 22626471084160 run.py:483] Algo bellman_ford step 6882 current loss 0.572336, current_train_items 220256.
I0302 19:01:37.203243 22626471084160 run.py:483] Algo bellman_ford step 6883 current loss 0.557955, current_train_items 220288.
I0302 19:01:37.236776 22626471084160 run.py:483] Algo bellman_ford step 6884 current loss 0.707173, current_train_items 220320.
I0302 19:01:37.256758 22626471084160 run.py:483] Algo bellman_ford step 6885 current loss 0.297090, current_train_items 220352.
I0302 19:01:37.273099 22626471084160 run.py:483] Algo bellman_ford step 6886 current loss 0.465135, current_train_items 220384.
I0302 19:01:37.296692 22626471084160 run.py:483] Algo bellman_ford step 6887 current loss 0.624918, current_train_items 220416.
I0302 19:01:37.328698 22626471084160 run.py:483] Algo bellman_ford step 6888 current loss 0.674616, current_train_items 220448.
I0302 19:01:37.362707 22626471084160 run.py:483] Algo bellman_ford step 6889 current loss 0.663514, current_train_items 220480.
I0302 19:01:37.382768 22626471084160 run.py:483] Algo bellman_ford step 6890 current loss 0.310674, current_train_items 220512.
I0302 19:01:37.399246 22626471084160 run.py:483] Algo bellman_ford step 6891 current loss 0.498497, current_train_items 220544.
I0302 19:01:37.422835 22626471084160 run.py:483] Algo bellman_ford step 6892 current loss 0.585674, current_train_items 220576.
I0302 19:01:37.454543 22626471084160 run.py:483] Algo bellman_ford step 6893 current loss 0.693091, current_train_items 220608.
I0302 19:01:37.487125 22626471084160 run.py:483] Algo bellman_ford step 6894 current loss 0.799305, current_train_items 220640.
I0302 19:01:37.507067 22626471084160 run.py:483] Algo bellman_ford step 6895 current loss 0.284071, current_train_items 220672.
I0302 19:01:37.523266 22626471084160 run.py:483] Algo bellman_ford step 6896 current loss 0.464376, current_train_items 220704.
I0302 19:01:37.547399 22626471084160 run.py:483] Algo bellman_ford step 6897 current loss 0.714096, current_train_items 220736.
I0302 19:01:37.577338 22626471084160 run.py:483] Algo bellman_ford step 6898 current loss 0.513875, current_train_items 220768.
I0302 19:01:37.611709 22626471084160 run.py:483] Algo bellman_ford step 6899 current loss 0.771164, current_train_items 220800.
I0302 19:01:37.631803 22626471084160 run.py:483] Algo bellman_ford step 6900 current loss 0.328145, current_train_items 220832.
I0302 19:01:37.639650 22626471084160 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0302 19:01:37.639755 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:01:37.656308 22626471084160 run.py:483] Algo bellman_ford step 6901 current loss 0.367322, current_train_items 220864.
I0302 19:01:37.679981 22626471084160 run.py:483] Algo bellman_ford step 6902 current loss 0.598848, current_train_items 220896.
I0302 19:01:37.712376 22626471084160 run.py:483] Algo bellman_ford step 6903 current loss 0.706746, current_train_items 220928.
I0302 19:01:37.747745 22626471084160 run.py:483] Algo bellman_ford step 6904 current loss 0.722315, current_train_items 220960.
I0302 19:01:37.767496 22626471084160 run.py:483] Algo bellman_ford step 6905 current loss 0.356437, current_train_items 220992.
I0302 19:01:37.782959 22626471084160 run.py:483] Algo bellman_ford step 6906 current loss 0.403034, current_train_items 221024.
I0302 19:01:37.806625 22626471084160 run.py:483] Algo bellman_ford step 6907 current loss 0.594359, current_train_items 221056.
I0302 19:01:37.838770 22626471084160 run.py:483] Algo bellman_ford step 6908 current loss 0.730378, current_train_items 221088.
I0302 19:01:37.873703 22626471084160 run.py:483] Algo bellman_ford step 6909 current loss 0.850487, current_train_items 221120.
I0302 19:01:37.893557 22626471084160 run.py:483] Algo bellman_ford step 6910 current loss 0.323940, current_train_items 221152.
I0302 19:01:37.910332 22626471084160 run.py:483] Algo bellman_ford step 6911 current loss 0.586627, current_train_items 221184.
I0302 19:01:37.932871 22626471084160 run.py:483] Algo bellman_ford step 6912 current loss 0.609533, current_train_items 221216.
I0302 19:01:37.964993 22626471084160 run.py:483] Algo bellman_ford step 6913 current loss 0.682610, current_train_items 221248.
I0302 19:01:38.000231 22626471084160 run.py:483] Algo bellman_ford step 6914 current loss 0.859089, current_train_items 221280.
I0302 19:01:38.019564 22626471084160 run.py:483] Algo bellman_ford step 6915 current loss 0.379345, current_train_items 221312.
I0302 19:01:38.035310 22626471084160 run.py:483] Algo bellman_ford step 6916 current loss 0.446738, current_train_items 221344.
I0302 19:01:38.059911 22626471084160 run.py:483] Algo bellman_ford step 6917 current loss 0.766712, current_train_items 221376.
I0302 19:01:38.091952 22626471084160 run.py:483] Algo bellman_ford step 6918 current loss 0.754260, current_train_items 221408.
I0302 19:01:38.127294 22626471084160 run.py:483] Algo bellman_ford step 6919 current loss 0.740281, current_train_items 221440.
I0302 19:01:38.146943 22626471084160 run.py:483] Algo bellman_ford step 6920 current loss 0.307739, current_train_items 221472.
I0302 19:01:38.163104 22626471084160 run.py:483] Algo bellman_ford step 6921 current loss 0.427877, current_train_items 221504.
I0302 19:01:38.186255 22626471084160 run.py:483] Algo bellman_ford step 6922 current loss 0.618543, current_train_items 221536.
I0302 19:01:38.217946 22626471084160 run.py:483] Algo bellman_ford step 6923 current loss 0.775213, current_train_items 221568.
I0302 19:01:38.251276 22626471084160 run.py:483] Algo bellman_ford step 6924 current loss 0.696430, current_train_items 221600.
I0302 19:01:38.271165 22626471084160 run.py:483] Algo bellman_ford step 6925 current loss 0.270019, current_train_items 221632.
I0302 19:01:38.287070 22626471084160 run.py:483] Algo bellman_ford step 6926 current loss 0.377175, current_train_items 221664.
I0302 19:01:38.310991 22626471084160 run.py:483] Algo bellman_ford step 6927 current loss 0.690522, current_train_items 221696.
I0302 19:01:38.342415 22626471084160 run.py:483] Algo bellman_ford step 6928 current loss 0.718536, current_train_items 221728.
I0302 19:01:38.375857 22626471084160 run.py:483] Algo bellman_ford step 6929 current loss 0.787987, current_train_items 221760.
I0302 19:01:38.395647 22626471084160 run.py:483] Algo bellman_ford step 6930 current loss 0.295433, current_train_items 221792.
I0302 19:01:38.411951 22626471084160 run.py:483] Algo bellman_ford step 6931 current loss 0.441456, current_train_items 221824.
I0302 19:01:38.434742 22626471084160 run.py:483] Algo bellman_ford step 6932 current loss 0.550062, current_train_items 221856.
I0302 19:01:38.466025 22626471084160 run.py:483] Algo bellman_ford step 6933 current loss 0.696227, current_train_items 221888.
I0302 19:01:38.500927 22626471084160 run.py:483] Algo bellman_ford step 6934 current loss 0.828249, current_train_items 221920.
I0302 19:01:38.520675 22626471084160 run.py:483] Algo bellman_ford step 6935 current loss 0.327433, current_train_items 221952.
I0302 19:01:38.536626 22626471084160 run.py:483] Algo bellman_ford step 6936 current loss 0.503911, current_train_items 221984.
I0302 19:01:38.560268 22626471084160 run.py:483] Algo bellman_ford step 6937 current loss 0.572817, current_train_items 222016.
I0302 19:01:38.591461 22626471084160 run.py:483] Algo bellman_ford step 6938 current loss 0.639549, current_train_items 222048.
I0302 19:01:38.623935 22626471084160 run.py:483] Algo bellman_ford step 6939 current loss 0.691593, current_train_items 222080.
I0302 19:01:38.643830 22626471084160 run.py:483] Algo bellman_ford step 6940 current loss 0.289315, current_train_items 222112.
I0302 19:01:38.660397 22626471084160 run.py:483] Algo bellman_ford step 6941 current loss 0.574873, current_train_items 222144.
I0302 19:01:38.684363 22626471084160 run.py:483] Algo bellman_ford step 6942 current loss 0.698144, current_train_items 222176.
I0302 19:01:38.715704 22626471084160 run.py:483] Algo bellman_ford step 6943 current loss 0.710491, current_train_items 222208.
I0302 19:01:38.750224 22626471084160 run.py:483] Algo bellman_ford step 6944 current loss 0.866095, current_train_items 222240.
I0302 19:01:38.770151 22626471084160 run.py:483] Algo bellman_ford step 6945 current loss 0.312188, current_train_items 222272.
I0302 19:01:38.786500 22626471084160 run.py:483] Algo bellman_ford step 6946 current loss 0.600858, current_train_items 222304.
I0302 19:01:38.810324 22626471084160 run.py:483] Algo bellman_ford step 6947 current loss 0.627645, current_train_items 222336.
I0302 19:01:38.839528 22626471084160 run.py:483] Algo bellman_ford step 6948 current loss 0.616507, current_train_items 222368.
I0302 19:01:38.874076 22626471084160 run.py:483] Algo bellman_ford step 6949 current loss 0.837380, current_train_items 222400.
I0302 19:01:38.893566 22626471084160 run.py:483] Algo bellman_ford step 6950 current loss 0.284021, current_train_items 222432.
I0302 19:01:38.901667 22626471084160 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0302 19:01:38.901771 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:01:38.918706 22626471084160 run.py:483] Algo bellman_ford step 6951 current loss 0.496716, current_train_items 222464.
I0302 19:01:38.942851 22626471084160 run.py:483] Algo bellman_ford step 6952 current loss 0.676016, current_train_items 222496.
I0302 19:01:38.973387 22626471084160 run.py:483] Algo bellman_ford step 6953 current loss 0.708653, current_train_items 222528.
I0302 19:01:39.008397 22626471084160 run.py:483] Algo bellman_ford step 6954 current loss 0.899486, current_train_items 222560.
I0302 19:01:39.028682 22626471084160 run.py:483] Algo bellman_ford step 6955 current loss 0.298455, current_train_items 222592.
I0302 19:01:39.044682 22626471084160 run.py:483] Algo bellman_ford step 6956 current loss 0.468668, current_train_items 222624.
I0302 19:01:39.069770 22626471084160 run.py:483] Algo bellman_ford step 6957 current loss 0.754125, current_train_items 222656.
I0302 19:01:39.100694 22626471084160 run.py:483] Algo bellman_ford step 6958 current loss 0.730651, current_train_items 222688.
I0302 19:01:39.132505 22626471084160 run.py:483] Algo bellman_ford step 6959 current loss 0.743370, current_train_items 222720.
I0302 19:01:39.152325 22626471084160 run.py:483] Algo bellman_ford step 6960 current loss 0.302267, current_train_items 222752.
I0302 19:01:39.168943 22626471084160 run.py:483] Algo bellman_ford step 6961 current loss 0.516319, current_train_items 222784.
I0302 19:01:39.191764 22626471084160 run.py:483] Algo bellman_ford step 6962 current loss 0.602273, current_train_items 222816.
I0302 19:01:39.222708 22626471084160 run.py:483] Algo bellman_ford step 6963 current loss 0.802050, current_train_items 222848.
I0302 19:01:39.257802 22626471084160 run.py:483] Algo bellman_ford step 6964 current loss 0.712409, current_train_items 222880.
I0302 19:01:39.277541 22626471084160 run.py:483] Algo bellman_ford step 6965 current loss 0.390627, current_train_items 222912.
I0302 19:01:39.293878 22626471084160 run.py:483] Algo bellman_ford step 6966 current loss 0.475102, current_train_items 222944.
I0302 19:01:39.317578 22626471084160 run.py:483] Algo bellman_ford step 6967 current loss 0.523186, current_train_items 222976.
I0302 19:01:39.347896 22626471084160 run.py:483] Algo bellman_ford step 6968 current loss 0.619947, current_train_items 223008.
I0302 19:01:39.381789 22626471084160 run.py:483] Algo bellman_ford step 6969 current loss 0.836351, current_train_items 223040.
I0302 19:01:39.401715 22626471084160 run.py:483] Algo bellman_ford step 6970 current loss 0.242718, current_train_items 223072.
I0302 19:01:39.417918 22626471084160 run.py:483] Algo bellman_ford step 6971 current loss 0.445344, current_train_items 223104.
I0302 19:01:39.441135 22626471084160 run.py:483] Algo bellman_ford step 6972 current loss 0.700757, current_train_items 223136.
I0302 19:01:39.471784 22626471084160 run.py:483] Algo bellman_ford step 6973 current loss 0.673327, current_train_items 223168.
I0302 19:01:39.506253 22626471084160 run.py:483] Algo bellman_ford step 6974 current loss 0.731465, current_train_items 223200.
I0302 19:01:39.525855 22626471084160 run.py:483] Algo bellman_ford step 6975 current loss 0.340088, current_train_items 223232.
I0302 19:01:39.542281 22626471084160 run.py:483] Algo bellman_ford step 6976 current loss 0.484900, current_train_items 223264.
I0302 19:01:39.565276 22626471084160 run.py:483] Algo bellman_ford step 6977 current loss 0.721346, current_train_items 223296.
I0302 19:01:39.595294 22626471084160 run.py:483] Algo bellman_ford step 6978 current loss 0.875049, current_train_items 223328.
I0302 19:01:39.629619 22626471084160 run.py:483] Algo bellman_ford step 6979 current loss 0.953399, current_train_items 223360.
I0302 19:01:39.649113 22626471084160 run.py:483] Algo bellman_ford step 6980 current loss 0.351853, current_train_items 223392.
I0302 19:01:39.665829 22626471084160 run.py:483] Algo bellman_ford step 6981 current loss 0.506719, current_train_items 223424.
I0302 19:01:39.688707 22626471084160 run.py:483] Algo bellman_ford step 6982 current loss 0.519413, current_train_items 223456.
I0302 19:01:39.720666 22626471084160 run.py:483] Algo bellman_ford step 6983 current loss 0.726211, current_train_items 223488.
I0302 19:01:39.754268 22626471084160 run.py:483] Algo bellman_ford step 6984 current loss 0.786267, current_train_items 223520.
I0302 19:01:39.774364 22626471084160 run.py:483] Algo bellman_ford step 6985 current loss 0.263894, current_train_items 223552.
I0302 19:01:39.790455 22626471084160 run.py:483] Algo bellman_ford step 6986 current loss 0.500749, current_train_items 223584.
I0302 19:01:39.813702 22626471084160 run.py:483] Algo bellman_ford step 6987 current loss 0.685848, current_train_items 223616.
I0302 19:01:39.844512 22626471084160 run.py:483] Algo bellman_ford step 6988 current loss 0.697714, current_train_items 223648.
I0302 19:01:39.880410 22626471084160 run.py:483] Algo bellman_ford step 6989 current loss 0.902782, current_train_items 223680.
I0302 19:01:39.900498 22626471084160 run.py:483] Algo bellman_ford step 6990 current loss 0.309824, current_train_items 223712.
I0302 19:01:39.916507 22626471084160 run.py:483] Algo bellman_ford step 6991 current loss 0.500769, current_train_items 223744.
I0302 19:01:39.939856 22626471084160 run.py:483] Algo bellman_ford step 6992 current loss 0.653989, current_train_items 223776.
I0302 19:01:39.971597 22626471084160 run.py:483] Algo bellman_ford step 6993 current loss 0.715029, current_train_items 223808.
I0302 19:01:40.008292 22626471084160 run.py:483] Algo bellman_ford step 6994 current loss 0.722208, current_train_items 223840.
I0302 19:01:40.027811 22626471084160 run.py:483] Algo bellman_ford step 6995 current loss 0.323507, current_train_items 223872.
I0302 19:01:40.044280 22626471084160 run.py:483] Algo bellman_ford step 6996 current loss 0.470443, current_train_items 223904.
I0302 19:01:40.066249 22626471084160 run.py:483] Algo bellman_ford step 6997 current loss 0.623727, current_train_items 223936.
I0302 19:01:40.096333 22626471084160 run.py:483] Algo bellman_ford step 6998 current loss 0.642831, current_train_items 223968.
I0302 19:01:40.130021 22626471084160 run.py:483] Algo bellman_ford step 6999 current loss 0.884115, current_train_items 224000.
I0302 19:01:40.150029 22626471084160 run.py:483] Algo bellman_ford step 7000 current loss 0.302496, current_train_items 224032.
I0302 19:01:40.157636 22626471084160 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.8876953125, 'score': 0.8876953125, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0302 19:01:40.157742 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.888, val scores are: bellman_ford: 0.888
I0302 19:01:40.174247 22626471084160 run.py:483] Algo bellman_ford step 7001 current loss 0.452733, current_train_items 224064.
I0302 19:01:40.197508 22626471084160 run.py:483] Algo bellman_ford step 7002 current loss 0.630253, current_train_items 224096.
I0302 19:01:40.227381 22626471084160 run.py:483] Algo bellman_ford step 7003 current loss 0.623823, current_train_items 224128.
I0302 19:01:40.260349 22626471084160 run.py:483] Algo bellman_ford step 7004 current loss 0.871235, current_train_items 224160.
I0302 19:01:40.280194 22626471084160 run.py:483] Algo bellman_ford step 7005 current loss 0.283253, current_train_items 224192.
I0302 19:01:40.296223 22626471084160 run.py:483] Algo bellman_ford step 7006 current loss 0.524133, current_train_items 224224.
I0302 19:01:40.320638 22626471084160 run.py:483] Algo bellman_ford step 7007 current loss 0.598356, current_train_items 224256.
I0302 19:01:40.350279 22626471084160 run.py:483] Algo bellman_ford step 7008 current loss 0.619226, current_train_items 224288.
I0302 19:01:40.386205 22626471084160 run.py:483] Algo bellman_ford step 7009 current loss 0.841680, current_train_items 224320.
I0302 19:01:40.405957 22626471084160 run.py:483] Algo bellman_ford step 7010 current loss 0.342266, current_train_items 224352.
I0302 19:01:40.422317 22626471084160 run.py:483] Algo bellman_ford step 7011 current loss 0.463460, current_train_items 224384.
I0302 19:01:40.445772 22626471084160 run.py:483] Algo bellman_ford step 7012 current loss 0.586697, current_train_items 224416.
I0302 19:01:40.476112 22626471084160 run.py:483] Algo bellman_ford step 7013 current loss 0.614593, current_train_items 224448.
I0302 19:01:40.510002 22626471084160 run.py:483] Algo bellman_ford step 7014 current loss 0.696007, current_train_items 224480.
I0302 19:01:40.529572 22626471084160 run.py:483] Algo bellman_ford step 7015 current loss 0.271697, current_train_items 224512.
I0302 19:01:40.545538 22626471084160 run.py:483] Algo bellman_ford step 7016 current loss 0.419041, current_train_items 224544.
I0302 19:01:40.569358 22626471084160 run.py:483] Algo bellman_ford step 7017 current loss 0.561759, current_train_items 224576.
I0302 19:01:40.599383 22626471084160 run.py:483] Algo bellman_ford step 7018 current loss 0.613039, current_train_items 224608.
I0302 19:01:40.632073 22626471084160 run.py:483] Algo bellman_ford step 7019 current loss 0.682922, current_train_items 224640.
I0302 19:01:40.651595 22626471084160 run.py:483] Algo bellman_ford step 7020 current loss 0.298874, current_train_items 224672.
I0302 19:01:40.667632 22626471084160 run.py:483] Algo bellman_ford step 7021 current loss 0.551802, current_train_items 224704.
I0302 19:01:40.691088 22626471084160 run.py:483] Algo bellman_ford step 7022 current loss 0.607831, current_train_items 224736.
I0302 19:01:40.720990 22626471084160 run.py:483] Algo bellman_ford step 7023 current loss 0.648860, current_train_items 224768.
I0302 19:01:40.755322 22626471084160 run.py:483] Algo bellman_ford step 7024 current loss 0.733273, current_train_items 224800.
I0302 19:01:40.774960 22626471084160 run.py:483] Algo bellman_ford step 7025 current loss 0.343839, current_train_items 224832.
I0302 19:01:40.791563 22626471084160 run.py:483] Algo bellman_ford step 7026 current loss 0.445421, current_train_items 224864.
I0302 19:01:40.816416 22626471084160 run.py:483] Algo bellman_ford step 7027 current loss 0.766769, current_train_items 224896.
I0302 19:01:40.849150 22626471084160 run.py:483] Algo bellman_ford step 7028 current loss 0.854874, current_train_items 224928.
I0302 19:01:40.881088 22626471084160 run.py:483] Algo bellman_ford step 7029 current loss 0.840084, current_train_items 224960.
I0302 19:01:40.900742 22626471084160 run.py:483] Algo bellman_ford step 7030 current loss 0.369143, current_train_items 224992.
I0302 19:01:40.916963 22626471084160 run.py:483] Algo bellman_ford step 7031 current loss 0.415661, current_train_items 225024.
I0302 19:01:40.939966 22626471084160 run.py:483] Algo bellman_ford step 7032 current loss 0.567145, current_train_items 225056.
I0302 19:01:40.969971 22626471084160 run.py:483] Algo bellman_ford step 7033 current loss 0.717716, current_train_items 225088.
I0302 19:01:41.005373 22626471084160 run.py:483] Algo bellman_ford step 7034 current loss 0.869080, current_train_items 225120.
I0302 19:01:41.025079 22626471084160 run.py:483] Algo bellman_ford step 7035 current loss 0.272721, current_train_items 225152.
I0302 19:01:41.041309 22626471084160 run.py:483] Algo bellman_ford step 7036 current loss 0.482155, current_train_items 225184.
I0302 19:01:41.064703 22626471084160 run.py:483] Algo bellman_ford step 7037 current loss 0.600596, current_train_items 225216.
I0302 19:01:41.096456 22626471084160 run.py:483] Algo bellman_ford step 7038 current loss 0.639567, current_train_items 225248.
I0302 19:01:41.129309 22626471084160 run.py:483] Algo bellman_ford step 7039 current loss 0.704244, current_train_items 225280.
I0302 19:01:41.149026 22626471084160 run.py:483] Algo bellman_ford step 7040 current loss 0.280939, current_train_items 225312.
I0302 19:01:41.165188 22626471084160 run.py:483] Algo bellman_ford step 7041 current loss 0.429849, current_train_items 225344.
I0302 19:01:41.188557 22626471084160 run.py:483] Algo bellman_ford step 7042 current loss 0.595885, current_train_items 225376.
I0302 19:01:41.219938 22626471084160 run.py:483] Algo bellman_ford step 7043 current loss 0.726054, current_train_items 225408.
I0302 19:01:41.254393 22626471084160 run.py:483] Algo bellman_ford step 7044 current loss 0.706431, current_train_items 225440.
I0302 19:01:41.273983 22626471084160 run.py:483] Algo bellman_ford step 7045 current loss 0.316607, current_train_items 225472.
I0302 19:01:41.290441 22626471084160 run.py:483] Algo bellman_ford step 7046 current loss 0.487428, current_train_items 225504.
I0302 19:01:41.313532 22626471084160 run.py:483] Algo bellman_ford step 7047 current loss 0.649164, current_train_items 225536.
I0302 19:01:41.345103 22626471084160 run.py:483] Algo bellman_ford step 7048 current loss 0.616220, current_train_items 225568.
I0302 19:01:41.378357 22626471084160 run.py:483] Algo bellman_ford step 7049 current loss 0.704760, current_train_items 225600.
I0302 19:01:41.398246 22626471084160 run.py:483] Algo bellman_ford step 7050 current loss 0.275759, current_train_items 225632.
I0302 19:01:41.406285 22626471084160 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0302 19:01:41.406392 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:01:41.422774 22626471084160 run.py:483] Algo bellman_ford step 7051 current loss 0.428040, current_train_items 225664.
I0302 19:01:41.447521 22626471084160 run.py:483] Algo bellman_ford step 7052 current loss 0.648188, current_train_items 225696.
I0302 19:01:41.479949 22626471084160 run.py:483] Algo bellman_ford step 7053 current loss 0.830771, current_train_items 225728.
I0302 19:01:41.515287 22626471084160 run.py:483] Algo bellman_ford step 7054 current loss 0.912634, current_train_items 225760.
I0302 19:01:41.535434 22626471084160 run.py:483] Algo bellman_ford step 7055 current loss 0.352624, current_train_items 225792.
I0302 19:01:41.551379 22626471084160 run.py:483] Algo bellman_ford step 7056 current loss 0.485406, current_train_items 225824.
I0302 19:01:41.575929 22626471084160 run.py:483] Algo bellman_ford step 7057 current loss 0.618611, current_train_items 225856.
I0302 19:01:41.606819 22626471084160 run.py:483] Algo bellman_ford step 7058 current loss 0.693062, current_train_items 225888.
I0302 19:01:41.642756 22626471084160 run.py:483] Algo bellman_ford step 7059 current loss 0.745084, current_train_items 225920.
I0302 19:01:41.662772 22626471084160 run.py:483] Algo bellman_ford step 7060 current loss 0.301790, current_train_items 225952.
I0302 19:01:41.679075 22626471084160 run.py:483] Algo bellman_ford step 7061 current loss 0.465149, current_train_items 225984.
I0302 19:01:41.703474 22626471084160 run.py:483] Algo bellman_ford step 7062 current loss 0.579815, current_train_items 226016.
I0302 19:01:41.732098 22626471084160 run.py:483] Algo bellman_ford step 7063 current loss 0.540045, current_train_items 226048.
I0302 19:01:41.765743 22626471084160 run.py:483] Algo bellman_ford step 7064 current loss 0.756282, current_train_items 226080.
I0302 19:01:41.785173 22626471084160 run.py:483] Algo bellman_ford step 7065 current loss 0.289333, current_train_items 226112.
I0302 19:01:41.801765 22626471084160 run.py:483] Algo bellman_ford step 7066 current loss 0.424510, current_train_items 226144.
I0302 19:01:41.825314 22626471084160 run.py:483] Algo bellman_ford step 7067 current loss 0.566476, current_train_items 226176.
I0302 19:01:41.858228 22626471084160 run.py:483] Algo bellman_ford step 7068 current loss 0.772185, current_train_items 226208.
I0302 19:01:41.894058 22626471084160 run.py:483] Algo bellman_ford step 7069 current loss 0.807289, current_train_items 226240.
I0302 19:01:41.914245 22626471084160 run.py:483] Algo bellman_ford step 7070 current loss 0.316064, current_train_items 226272.
I0302 19:01:41.930308 22626471084160 run.py:483] Algo bellman_ford step 7071 current loss 0.546987, current_train_items 226304.
I0302 19:01:41.952460 22626471084160 run.py:483] Algo bellman_ford step 7072 current loss 0.632737, current_train_items 226336.
I0302 19:01:41.982752 22626471084160 run.py:483] Algo bellman_ford step 7073 current loss 0.568210, current_train_items 226368.
I0302 19:01:42.016174 22626471084160 run.py:483] Algo bellman_ford step 7074 current loss 0.685774, current_train_items 226400.
I0302 19:01:42.036010 22626471084160 run.py:483] Algo bellman_ford step 7075 current loss 0.251944, current_train_items 226432.
I0302 19:01:42.051614 22626471084160 run.py:483] Algo bellman_ford step 7076 current loss 0.434550, current_train_items 226464.
I0302 19:01:42.074736 22626471084160 run.py:483] Algo bellman_ford step 7077 current loss 0.658372, current_train_items 226496.
I0302 19:01:42.106264 22626471084160 run.py:483] Algo bellman_ford step 7078 current loss 0.587003, current_train_items 226528.
I0302 19:01:42.140468 22626471084160 run.py:483] Algo bellman_ford step 7079 current loss 0.757476, current_train_items 226560.
I0302 19:01:42.159783 22626471084160 run.py:483] Algo bellman_ford step 7080 current loss 0.225219, current_train_items 226592.
I0302 19:01:42.175755 22626471084160 run.py:483] Algo bellman_ford step 7081 current loss 0.405589, current_train_items 226624.
I0302 19:01:42.199318 22626471084160 run.py:483] Algo bellman_ford step 7082 current loss 0.610644, current_train_items 226656.
I0302 19:01:42.230401 22626471084160 run.py:483] Algo bellman_ford step 7083 current loss 0.762670, current_train_items 226688.
I0302 19:01:42.263558 22626471084160 run.py:483] Algo bellman_ford step 7084 current loss 0.731968, current_train_items 226720.
I0302 19:01:42.283381 22626471084160 run.py:483] Algo bellman_ford step 7085 current loss 0.285015, current_train_items 226752.
I0302 19:01:42.299341 22626471084160 run.py:483] Algo bellman_ford step 7086 current loss 0.438593, current_train_items 226784.
I0302 19:01:42.321738 22626471084160 run.py:483] Algo bellman_ford step 7087 current loss 0.517295, current_train_items 226816.
I0302 19:01:42.352550 22626471084160 run.py:483] Algo bellman_ford step 7088 current loss 0.662109, current_train_items 226848.
I0302 19:01:42.386002 22626471084160 run.py:483] Algo bellman_ford step 7089 current loss 0.643794, current_train_items 226880.
I0302 19:01:42.406459 22626471084160 run.py:483] Algo bellman_ford step 7090 current loss 0.262420, current_train_items 226912.
I0302 19:01:42.422411 22626471084160 run.py:483] Algo bellman_ford step 7091 current loss 0.406623, current_train_items 226944.
I0302 19:01:42.445864 22626471084160 run.py:483] Algo bellman_ford step 7092 current loss 0.638600, current_train_items 226976.
I0302 19:01:42.477783 22626471084160 run.py:483] Algo bellman_ford step 7093 current loss 0.764749, current_train_items 227008.
I0302 19:01:42.509645 22626471084160 run.py:483] Algo bellman_ford step 7094 current loss 0.689757, current_train_items 227040.
I0302 19:01:42.529480 22626471084160 run.py:483] Algo bellman_ford step 7095 current loss 0.383281, current_train_items 227072.
I0302 19:01:42.545552 22626471084160 run.py:483] Algo bellman_ford step 7096 current loss 0.464871, current_train_items 227104.
I0302 19:01:42.567844 22626471084160 run.py:483] Algo bellman_ford step 7097 current loss 0.535725, current_train_items 227136.
I0302 19:01:42.599432 22626471084160 run.py:483] Algo bellman_ford step 7098 current loss 0.777407, current_train_items 227168.
I0302 19:01:42.632052 22626471084160 run.py:483] Algo bellman_ford step 7099 current loss 0.695926, current_train_items 227200.
I0302 19:01:42.652260 22626471084160 run.py:483] Algo bellman_ford step 7100 current loss 0.292676, current_train_items 227232.
I0302 19:01:42.660105 22626471084160 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0302 19:01:42.660219 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:01:42.676899 22626471084160 run.py:483] Algo bellman_ford step 7101 current loss 0.467966, current_train_items 227264.
I0302 19:01:42.700546 22626471084160 run.py:483] Algo bellman_ford step 7102 current loss 0.611921, current_train_items 227296.
I0302 19:01:42.732196 22626471084160 run.py:483] Algo bellman_ford step 7103 current loss 0.639395, current_train_items 227328.
I0302 19:01:42.767685 22626471084160 run.py:483] Algo bellman_ford step 7104 current loss 0.786955, current_train_items 227360.
I0302 19:01:42.787845 22626471084160 run.py:483] Algo bellman_ford step 7105 current loss 0.337020, current_train_items 227392.
I0302 19:01:42.803970 22626471084160 run.py:483] Algo bellman_ford step 7106 current loss 0.483984, current_train_items 227424.
I0302 19:01:42.827948 22626471084160 run.py:483] Algo bellman_ford step 7107 current loss 0.684354, current_train_items 227456.
I0302 19:01:42.858777 22626471084160 run.py:483] Algo bellman_ford step 7108 current loss 0.672644, current_train_items 227488.
I0302 19:01:42.894336 22626471084160 run.py:483] Algo bellman_ford step 7109 current loss 0.878636, current_train_items 227520.
I0302 19:01:42.914225 22626471084160 run.py:483] Algo bellman_ford step 7110 current loss 0.326085, current_train_items 227552.
I0302 19:01:42.930597 22626471084160 run.py:483] Algo bellman_ford step 7111 current loss 0.575832, current_train_items 227584.
I0302 19:01:42.954077 22626471084160 run.py:483] Algo bellman_ford step 7112 current loss 0.611916, current_train_items 227616.
I0302 19:01:42.986078 22626471084160 run.py:483] Algo bellman_ford step 7113 current loss 0.819064, current_train_items 227648.
I0302 19:01:43.019875 22626471084160 run.py:483] Algo bellman_ford step 7114 current loss 0.979075, current_train_items 227680.
I0302 19:01:43.039441 22626471084160 run.py:483] Algo bellman_ford step 7115 current loss 0.372988, current_train_items 227712.
I0302 19:01:43.055632 22626471084160 run.py:483] Algo bellman_ford step 7116 current loss 0.568455, current_train_items 227744.
I0302 19:01:43.079475 22626471084160 run.py:483] Algo bellman_ford step 7117 current loss 0.576604, current_train_items 227776.
I0302 19:01:43.111114 22626471084160 run.py:483] Algo bellman_ford step 7118 current loss 0.713636, current_train_items 227808.
I0302 19:01:43.144918 22626471084160 run.py:483] Algo bellman_ford step 7119 current loss 0.739629, current_train_items 227840.
I0302 19:01:43.164299 22626471084160 run.py:483] Algo bellman_ford step 7120 current loss 0.320415, current_train_items 227872.
I0302 19:01:43.180560 22626471084160 run.py:483] Algo bellman_ford step 7121 current loss 0.517051, current_train_items 227904.
I0302 19:01:43.204184 22626471084160 run.py:483] Algo bellman_ford step 7122 current loss 0.587299, current_train_items 227936.
I0302 19:01:43.234702 22626471084160 run.py:483] Algo bellman_ford step 7123 current loss 0.625239, current_train_items 227968.
I0302 19:01:43.269028 22626471084160 run.py:483] Algo bellman_ford step 7124 current loss 0.869000, current_train_items 228000.
I0302 19:01:43.288650 22626471084160 run.py:483] Algo bellman_ford step 7125 current loss 0.348210, current_train_items 228032.
I0302 19:01:43.304205 22626471084160 run.py:483] Algo bellman_ford step 7126 current loss 0.438955, current_train_items 228064.
I0302 19:01:43.326837 22626471084160 run.py:483] Algo bellman_ford step 7127 current loss 0.578841, current_train_items 228096.
I0302 19:01:43.357407 22626471084160 run.py:483] Algo bellman_ford step 7128 current loss 0.614309, current_train_items 228128.
I0302 19:01:43.392281 22626471084160 run.py:483] Algo bellman_ford step 7129 current loss 0.762847, current_train_items 228160.
I0302 19:01:43.411962 22626471084160 run.py:483] Algo bellman_ford step 7130 current loss 0.334957, current_train_items 228192.
I0302 19:01:43.428334 22626471084160 run.py:483] Algo bellman_ford step 7131 current loss 0.478592, current_train_items 228224.
I0302 19:01:43.452537 22626471084160 run.py:483] Algo bellman_ford step 7132 current loss 0.606124, current_train_items 228256.
I0302 19:01:43.483372 22626471084160 run.py:483] Algo bellman_ford step 7133 current loss 0.752079, current_train_items 228288.
I0302 19:01:43.516582 22626471084160 run.py:483] Algo bellman_ford step 7134 current loss 0.665088, current_train_items 228320.
I0302 19:01:43.536182 22626471084160 run.py:483] Algo bellman_ford step 7135 current loss 0.302679, current_train_items 228352.
I0302 19:01:43.552092 22626471084160 run.py:483] Algo bellman_ford step 7136 current loss 0.407432, current_train_items 228384.
I0302 19:01:43.574929 22626471084160 run.py:483] Algo bellman_ford step 7137 current loss 0.576757, current_train_items 228416.
I0302 19:01:43.604298 22626471084160 run.py:483] Algo bellman_ford step 7138 current loss 0.604660, current_train_items 228448.
I0302 19:01:43.640097 22626471084160 run.py:483] Algo bellman_ford step 7139 current loss 0.835319, current_train_items 228480.
I0302 19:01:43.659618 22626471084160 run.py:483] Algo bellman_ford step 7140 current loss 0.295694, current_train_items 228512.
I0302 19:01:43.675427 22626471084160 run.py:483] Algo bellman_ford step 7141 current loss 0.522685, current_train_items 228544.
I0302 19:01:43.699540 22626471084160 run.py:483] Algo bellman_ford step 7142 current loss 0.607238, current_train_items 228576.
I0302 19:01:43.728935 22626471084160 run.py:483] Algo bellman_ford step 7143 current loss 0.626716, current_train_items 228608.
I0302 19:01:43.761827 22626471084160 run.py:483] Algo bellman_ford step 7144 current loss 0.909123, current_train_items 228640.
I0302 19:01:43.781521 22626471084160 run.py:483] Algo bellman_ford step 7145 current loss 0.297740, current_train_items 228672.
I0302 19:01:43.797981 22626471084160 run.py:483] Algo bellman_ford step 7146 current loss 0.482923, current_train_items 228704.
I0302 19:01:43.822008 22626471084160 run.py:483] Algo bellman_ford step 7147 current loss 0.637915, current_train_items 228736.
I0302 19:01:43.851805 22626471084160 run.py:483] Algo bellman_ford step 7148 current loss 0.597080, current_train_items 228768.
I0302 19:01:43.883458 22626471084160 run.py:483] Algo bellman_ford step 7149 current loss 0.724321, current_train_items 228800.
I0302 19:01:43.902852 22626471084160 run.py:483] Algo bellman_ford step 7150 current loss 0.301879, current_train_items 228832.
I0302 19:01:43.910746 22626471084160 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0302 19:01:43.910852 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:01:43.928009 22626471084160 run.py:483] Algo bellman_ford step 7151 current loss 0.426461, current_train_items 228864.
I0302 19:01:43.952637 22626471084160 run.py:483] Algo bellman_ford step 7152 current loss 0.666258, current_train_items 228896.
I0302 19:01:43.984412 22626471084160 run.py:483] Algo bellman_ford step 7153 current loss 0.797216, current_train_items 228928.
I0302 19:01:44.017535 22626471084160 run.py:483] Algo bellman_ford step 7154 current loss 0.703089, current_train_items 228960.
I0302 19:01:44.038183 22626471084160 run.py:483] Algo bellman_ford step 7155 current loss 0.283932, current_train_items 228992.
I0302 19:01:44.054647 22626471084160 run.py:483] Algo bellman_ford step 7156 current loss 0.523808, current_train_items 229024.
I0302 19:01:44.078342 22626471084160 run.py:483] Algo bellman_ford step 7157 current loss 0.625945, current_train_items 229056.
I0302 19:01:44.108757 22626471084160 run.py:483] Algo bellman_ford step 7158 current loss 0.591767, current_train_items 229088.
I0302 19:01:44.142910 22626471084160 run.py:483] Algo bellman_ford step 7159 current loss 0.707851, current_train_items 229120.
I0302 19:01:44.162768 22626471084160 run.py:483] Algo bellman_ford step 7160 current loss 0.237979, current_train_items 229152.
I0302 19:01:44.179615 22626471084160 run.py:483] Algo bellman_ford step 7161 current loss 0.539163, current_train_items 229184.
W0302 19:01:44.194342 22626471084160 samplers.py:155] Increasing hint lengh from 10 to 11
I0302 19:01:50.773288 22626471084160 run.py:483] Algo bellman_ford step 7162 current loss 0.682604, current_train_items 229216.
I0302 19:01:50.805504 22626471084160 run.py:483] Algo bellman_ford step 7163 current loss 0.630655, current_train_items 229248.
I0302 19:01:50.840722 22626471084160 run.py:483] Algo bellman_ford step 7164 current loss 0.756400, current_train_items 229280.
I0302 19:01:50.860608 22626471084160 run.py:483] Algo bellman_ford step 7165 current loss 0.341459, current_train_items 229312.
I0302 19:01:50.876908 22626471084160 run.py:483] Algo bellman_ford step 7166 current loss 0.462101, current_train_items 229344.
I0302 19:01:50.900570 22626471084160 run.py:483] Algo bellman_ford step 7167 current loss 0.561988, current_train_items 229376.
I0302 19:01:50.932286 22626471084160 run.py:483] Algo bellman_ford step 7168 current loss 0.549552, current_train_items 229408.
I0302 19:01:50.963760 22626471084160 run.py:483] Algo bellman_ford step 7169 current loss 0.713992, current_train_items 229440.
I0302 19:01:50.983985 22626471084160 run.py:483] Algo bellman_ford step 7170 current loss 0.328402, current_train_items 229472.
I0302 19:01:51.000677 22626471084160 run.py:483] Algo bellman_ford step 7171 current loss 0.447843, current_train_items 229504.
I0302 19:01:51.024314 22626471084160 run.py:483] Algo bellman_ford step 7172 current loss 0.625999, current_train_items 229536.
I0302 19:01:51.055634 22626471084160 run.py:483] Algo bellman_ford step 7173 current loss 0.586427, current_train_items 229568.
I0302 19:01:51.090349 22626471084160 run.py:483] Algo bellman_ford step 7174 current loss 0.828917, current_train_items 229600.
I0302 19:01:51.110375 22626471084160 run.py:483] Algo bellman_ford step 7175 current loss 0.322771, current_train_items 229632.
I0302 19:01:51.126217 22626471084160 run.py:483] Algo bellman_ford step 7176 current loss 0.515226, current_train_items 229664.
I0302 19:01:51.149835 22626471084160 run.py:483] Algo bellman_ford step 7177 current loss 0.639003, current_train_items 229696.
I0302 19:01:51.183277 22626471084160 run.py:483] Algo bellman_ford step 7178 current loss 0.662122, current_train_items 229728.
I0302 19:01:51.217947 22626471084160 run.py:483] Algo bellman_ford step 7179 current loss 0.709757, current_train_items 229760.
I0302 19:01:51.237840 22626471084160 run.py:483] Algo bellman_ford step 7180 current loss 0.247352, current_train_items 229792.
I0302 19:01:51.254135 22626471084160 run.py:483] Algo bellman_ford step 7181 current loss 0.522576, current_train_items 229824.
I0302 19:01:51.278543 22626471084160 run.py:483] Algo bellman_ford step 7182 current loss 0.708734, current_train_items 229856.
I0302 19:01:51.309456 22626471084160 run.py:483] Algo bellman_ford step 7183 current loss 0.696363, current_train_items 229888.
I0302 19:01:51.346257 22626471084160 run.py:483] Algo bellman_ford step 7184 current loss 0.901212, current_train_items 229920.
I0302 19:01:51.366313 22626471084160 run.py:483] Algo bellman_ford step 7185 current loss 0.337050, current_train_items 229952.
I0302 19:01:51.382547 22626471084160 run.py:483] Algo bellman_ford step 7186 current loss 0.567011, current_train_items 229984.
I0302 19:01:51.407082 22626471084160 run.py:483] Algo bellman_ford step 7187 current loss 0.664928, current_train_items 230016.
I0302 19:01:51.438548 22626471084160 run.py:483] Algo bellman_ford step 7188 current loss 0.703042, current_train_items 230048.
I0302 19:01:51.474089 22626471084160 run.py:483] Algo bellman_ford step 7189 current loss 0.783308, current_train_items 230080.
I0302 19:01:51.493992 22626471084160 run.py:483] Algo bellman_ford step 7190 current loss 0.285379, current_train_items 230112.
I0302 19:01:51.510507 22626471084160 run.py:483] Algo bellman_ford step 7191 current loss 0.425383, current_train_items 230144.
I0302 19:01:51.534148 22626471084160 run.py:483] Algo bellman_ford step 7192 current loss 0.611623, current_train_items 230176.
I0302 19:01:51.565987 22626471084160 run.py:483] Algo bellman_ford step 7193 current loss 0.624261, current_train_items 230208.
I0302 19:01:51.599506 22626471084160 run.py:483] Algo bellman_ford step 7194 current loss 0.712104, current_train_items 230240.
I0302 19:01:51.619527 22626471084160 run.py:483] Algo bellman_ford step 7195 current loss 0.267945, current_train_items 230272.
I0302 19:01:51.635370 22626471084160 run.py:483] Algo bellman_ford step 7196 current loss 0.456361, current_train_items 230304.
I0302 19:01:51.658953 22626471084160 run.py:483] Algo bellman_ford step 7197 current loss 0.536718, current_train_items 230336.
I0302 19:01:51.690939 22626471084160 run.py:483] Algo bellman_ford step 7198 current loss 0.637319, current_train_items 230368.
I0302 19:01:51.724945 22626471084160 run.py:483] Algo bellman_ford step 7199 current loss 0.719614, current_train_items 230400.
I0302 19:01:51.745239 22626471084160 run.py:483] Algo bellman_ford step 7200 current loss 0.323573, current_train_items 230432.
I0302 19:01:51.754620 22626471084160 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0302 19:01:51.754728 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:01:51.771463 22626471084160 run.py:483] Algo bellman_ford step 7201 current loss 0.496606, current_train_items 230464.
I0302 19:01:51.795384 22626471084160 run.py:483] Algo bellman_ford step 7202 current loss 0.591340, current_train_items 230496.
I0302 19:01:51.827217 22626471084160 run.py:483] Algo bellman_ford step 7203 current loss 0.654306, current_train_items 230528.
I0302 19:01:51.860988 22626471084160 run.py:483] Algo bellman_ford step 7204 current loss 0.660500, current_train_items 230560.
I0302 19:01:51.881250 22626471084160 run.py:483] Algo bellman_ford step 7205 current loss 0.476352, current_train_items 230592.
I0302 19:01:51.896934 22626471084160 run.py:483] Algo bellman_ford step 7206 current loss 0.471514, current_train_items 230624.
I0302 19:01:51.920938 22626471084160 run.py:483] Algo bellman_ford step 7207 current loss 0.623466, current_train_items 230656.
I0302 19:01:51.951874 22626471084160 run.py:483] Algo bellman_ford step 7208 current loss 0.697391, current_train_items 230688.
I0302 19:01:51.986687 22626471084160 run.py:483] Algo bellman_ford step 7209 current loss 0.861207, current_train_items 230720.
I0302 19:01:52.006456 22626471084160 run.py:483] Algo bellman_ford step 7210 current loss 0.289405, current_train_items 230752.
I0302 19:01:52.022410 22626471084160 run.py:483] Algo bellman_ford step 7211 current loss 0.407527, current_train_items 230784.
I0302 19:01:52.045736 22626471084160 run.py:483] Algo bellman_ford step 7212 current loss 0.607873, current_train_items 230816.
I0302 19:01:52.076988 22626471084160 run.py:483] Algo bellman_ford step 7213 current loss 0.634453, current_train_items 230848.
I0302 19:01:52.108780 22626471084160 run.py:483] Algo bellman_ford step 7214 current loss 0.771853, current_train_items 230880.
I0302 19:01:52.128449 22626471084160 run.py:483] Algo bellman_ford step 7215 current loss 0.389163, current_train_items 230912.
I0302 19:01:52.144680 22626471084160 run.py:483] Algo bellman_ford step 7216 current loss 0.444960, current_train_items 230944.
I0302 19:01:52.168793 22626471084160 run.py:483] Algo bellman_ford step 7217 current loss 0.655919, current_train_items 230976.
I0302 19:01:52.200148 22626471084160 run.py:483] Algo bellman_ford step 7218 current loss 0.647886, current_train_items 231008.
I0302 19:01:52.233203 22626471084160 run.py:483] Algo bellman_ford step 7219 current loss 0.704041, current_train_items 231040.
I0302 19:01:52.252880 22626471084160 run.py:483] Algo bellman_ford step 7220 current loss 0.370614, current_train_items 231072.
I0302 19:01:52.269144 22626471084160 run.py:483] Algo bellman_ford step 7221 current loss 0.557172, current_train_items 231104.
I0302 19:01:52.293875 22626471084160 run.py:483] Algo bellman_ford step 7222 current loss 0.605075, current_train_items 231136.
I0302 19:01:52.324932 22626471084160 run.py:483] Algo bellman_ford step 7223 current loss 0.672661, current_train_items 231168.
I0302 19:01:52.359837 22626471084160 run.py:483] Algo bellman_ford step 7224 current loss 0.842716, current_train_items 231200.
I0302 19:01:52.379587 22626471084160 run.py:483] Algo bellman_ford step 7225 current loss 0.282602, current_train_items 231232.
I0302 19:01:52.395867 22626471084160 run.py:483] Algo bellman_ford step 7226 current loss 0.455923, current_train_items 231264.
I0302 19:01:52.421226 22626471084160 run.py:483] Algo bellman_ford step 7227 current loss 0.696274, current_train_items 231296.
I0302 19:01:52.452329 22626471084160 run.py:483] Algo bellman_ford step 7228 current loss 0.624160, current_train_items 231328.
I0302 19:01:52.484375 22626471084160 run.py:483] Algo bellman_ford step 7229 current loss 0.802175, current_train_items 231360.
I0302 19:01:52.504080 22626471084160 run.py:483] Algo bellman_ford step 7230 current loss 0.283374, current_train_items 231392.
I0302 19:01:52.520439 22626471084160 run.py:483] Algo bellman_ford step 7231 current loss 0.444203, current_train_items 231424.
I0302 19:01:52.542114 22626471084160 run.py:483] Algo bellman_ford step 7232 current loss 0.505192, current_train_items 231456.
I0302 19:01:52.574164 22626471084160 run.py:483] Algo bellman_ford step 7233 current loss 0.733778, current_train_items 231488.
I0302 19:01:52.606818 22626471084160 run.py:483] Algo bellman_ford step 7234 current loss 0.759785, current_train_items 231520.
I0302 19:01:52.626577 22626471084160 run.py:483] Algo bellman_ford step 7235 current loss 0.276245, current_train_items 231552.
I0302 19:01:52.643164 22626471084160 run.py:483] Algo bellman_ford step 7236 current loss 0.548395, current_train_items 231584.
I0302 19:01:52.667625 22626471084160 run.py:483] Algo bellman_ford step 7237 current loss 0.605067, current_train_items 231616.
I0302 19:01:52.700379 22626471084160 run.py:483] Algo bellman_ford step 7238 current loss 0.728253, current_train_items 231648.
I0302 19:01:52.734188 22626471084160 run.py:483] Algo bellman_ford step 7239 current loss 0.779824, current_train_items 231680.
I0302 19:01:52.753973 22626471084160 run.py:483] Algo bellman_ford step 7240 current loss 0.333638, current_train_items 231712.
I0302 19:01:52.770095 22626471084160 run.py:483] Algo bellman_ford step 7241 current loss 0.469206, current_train_items 231744.
I0302 19:01:52.794990 22626471084160 run.py:483] Algo bellman_ford step 7242 current loss 0.826952, current_train_items 231776.
I0302 19:01:52.826487 22626471084160 run.py:483] Algo bellman_ford step 7243 current loss 0.654442, current_train_items 231808.
I0302 19:01:52.861934 22626471084160 run.py:483] Algo bellman_ford step 7244 current loss 0.773275, current_train_items 231840.
I0302 19:01:52.881685 22626471084160 run.py:483] Algo bellman_ford step 7245 current loss 0.325096, current_train_items 231872.
I0302 19:01:52.897833 22626471084160 run.py:483] Algo bellman_ford step 7246 current loss 0.461222, current_train_items 231904.
I0302 19:01:52.922305 22626471084160 run.py:483] Algo bellman_ford step 7247 current loss 0.572911, current_train_items 231936.
I0302 19:01:52.955259 22626471084160 run.py:483] Algo bellman_ford step 7248 current loss 0.711970, current_train_items 231968.
I0302 19:01:52.989584 22626471084160 run.py:483] Algo bellman_ford step 7249 current loss 0.725079, current_train_items 232000.
I0302 19:01:53.009454 22626471084160 run.py:483] Algo bellman_ford step 7250 current loss 0.374772, current_train_items 232032.
I0302 19:01:53.017892 22626471084160 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0302 19:01:53.017998 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:01:53.034847 22626471084160 run.py:483] Algo bellman_ford step 7251 current loss 0.498105, current_train_items 232064.
I0302 19:01:53.058581 22626471084160 run.py:483] Algo bellman_ford step 7252 current loss 0.609492, current_train_items 232096.
I0302 19:01:53.089007 22626471084160 run.py:483] Algo bellman_ford step 7253 current loss 0.583213, current_train_items 232128.
I0302 19:01:53.123086 22626471084160 run.py:483] Algo bellman_ford step 7254 current loss 0.764341, current_train_items 232160.
I0302 19:01:53.142932 22626471084160 run.py:483] Algo bellman_ford step 7255 current loss 0.331696, current_train_items 232192.
I0302 19:01:53.158515 22626471084160 run.py:483] Algo bellman_ford step 7256 current loss 0.476836, current_train_items 232224.
I0302 19:01:53.182055 22626471084160 run.py:483] Algo bellman_ford step 7257 current loss 0.603385, current_train_items 232256.
I0302 19:01:53.215877 22626471084160 run.py:483] Algo bellman_ford step 7258 current loss 0.764422, current_train_items 232288.
I0302 19:01:53.249934 22626471084160 run.py:483] Algo bellman_ford step 7259 current loss 0.804990, current_train_items 232320.
I0302 19:01:53.269780 22626471084160 run.py:483] Algo bellman_ford step 7260 current loss 0.304281, current_train_items 232352.
I0302 19:01:53.286013 22626471084160 run.py:483] Algo bellman_ford step 7261 current loss 0.494722, current_train_items 232384.
I0302 19:01:53.309512 22626471084160 run.py:483] Algo bellman_ford step 7262 current loss 0.609009, current_train_items 232416.
I0302 19:01:53.340931 22626471084160 run.py:483] Algo bellman_ford step 7263 current loss 0.736571, current_train_items 232448.
I0302 19:01:53.374690 22626471084160 run.py:483] Algo bellman_ford step 7264 current loss 0.715414, current_train_items 232480.
I0302 19:01:53.393958 22626471084160 run.py:483] Algo bellman_ford step 7265 current loss 0.280980, current_train_items 232512.
I0302 19:01:53.410120 22626471084160 run.py:483] Algo bellman_ford step 7266 current loss 0.480221, current_train_items 232544.
I0302 19:01:53.433440 22626471084160 run.py:483] Algo bellman_ford step 7267 current loss 0.655948, current_train_items 232576.
I0302 19:01:53.465052 22626471084160 run.py:483] Algo bellman_ford step 7268 current loss 0.581538, current_train_items 232608.
I0302 19:01:53.497798 22626471084160 run.py:483] Algo bellman_ford step 7269 current loss 0.788152, current_train_items 232640.
I0302 19:01:53.517399 22626471084160 run.py:483] Algo bellman_ford step 7270 current loss 0.313012, current_train_items 232672.
I0302 19:01:53.533865 22626471084160 run.py:483] Algo bellman_ford step 7271 current loss 0.499165, current_train_items 232704.
I0302 19:01:53.557255 22626471084160 run.py:483] Algo bellman_ford step 7272 current loss 0.514522, current_train_items 232736.
I0302 19:01:53.588128 22626471084160 run.py:483] Algo bellman_ford step 7273 current loss 0.686390, current_train_items 232768.
I0302 19:01:53.621357 22626471084160 run.py:483] Algo bellman_ford step 7274 current loss 0.771161, current_train_items 232800.
I0302 19:01:53.641176 22626471084160 run.py:483] Algo bellman_ford step 7275 current loss 0.306064, current_train_items 232832.
I0302 19:01:53.657287 22626471084160 run.py:483] Algo bellman_ford step 7276 current loss 0.429965, current_train_items 232864.
I0302 19:01:53.680915 22626471084160 run.py:483] Algo bellman_ford step 7277 current loss 0.594636, current_train_items 232896.
I0302 19:01:53.712620 22626471084160 run.py:483] Algo bellman_ford step 7278 current loss 0.762638, current_train_items 232928.
I0302 19:01:53.746409 22626471084160 run.py:483] Algo bellman_ford step 7279 current loss 0.762743, current_train_items 232960.
I0302 19:01:53.765897 22626471084160 run.py:483] Algo bellman_ford step 7280 current loss 0.307584, current_train_items 232992.
I0302 19:01:53.782338 22626471084160 run.py:483] Algo bellman_ford step 7281 current loss 0.447372, current_train_items 233024.
I0302 19:01:53.805369 22626471084160 run.py:483] Algo bellman_ford step 7282 current loss 0.609528, current_train_items 233056.
I0302 19:01:53.836747 22626471084160 run.py:483] Algo bellman_ford step 7283 current loss 0.691838, current_train_items 233088.
I0302 19:01:53.869360 22626471084160 run.py:483] Algo bellman_ford step 7284 current loss 0.730472, current_train_items 233120.
I0302 19:01:53.889106 22626471084160 run.py:483] Algo bellman_ford step 7285 current loss 0.291936, current_train_items 233152.
I0302 19:01:53.906249 22626471084160 run.py:483] Algo bellman_ford step 7286 current loss 0.668709, current_train_items 233184.
I0302 19:01:53.929287 22626471084160 run.py:483] Algo bellman_ford step 7287 current loss 0.501449, current_train_items 233216.
I0302 19:01:53.960419 22626471084160 run.py:483] Algo bellman_ford step 7288 current loss 0.656782, current_train_items 233248.
I0302 19:01:53.992944 22626471084160 run.py:483] Algo bellman_ford step 7289 current loss 0.689930, current_train_items 233280.
I0302 19:01:54.012830 22626471084160 run.py:483] Algo bellman_ford step 7290 current loss 0.320870, current_train_items 233312.
I0302 19:01:54.029227 22626471084160 run.py:483] Algo bellman_ford step 7291 current loss 0.463751, current_train_items 233344.
I0302 19:01:54.051544 22626471084160 run.py:483] Algo bellman_ford step 7292 current loss 0.471383, current_train_items 233376.
I0302 19:01:54.083626 22626471084160 run.py:483] Algo bellman_ford step 7293 current loss 0.601162, current_train_items 233408.
I0302 19:01:54.115719 22626471084160 run.py:483] Algo bellman_ford step 7294 current loss 0.701044, current_train_items 233440.
I0302 19:01:54.134950 22626471084160 run.py:483] Algo bellman_ford step 7295 current loss 0.316451, current_train_items 233472.
I0302 19:01:54.151207 22626471084160 run.py:483] Algo bellman_ford step 7296 current loss 0.427431, current_train_items 233504.
I0302 19:01:54.175204 22626471084160 run.py:483] Algo bellman_ford step 7297 current loss 0.644814, current_train_items 233536.
I0302 19:01:54.206795 22626471084160 run.py:483] Algo bellman_ford step 7298 current loss 0.644343, current_train_items 233568.
I0302 19:01:54.240040 22626471084160 run.py:483] Algo bellman_ford step 7299 current loss 0.754666, current_train_items 233600.
I0302 19:01:54.259835 22626471084160 run.py:483] Algo bellman_ford step 7300 current loss 0.327972, current_train_items 233632.
I0302 19:01:54.267635 22626471084160 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0302 19:01:54.267750 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:01:54.284941 22626471084160 run.py:483] Algo bellman_ford step 7301 current loss 0.412641, current_train_items 233664.
I0302 19:01:54.308783 22626471084160 run.py:483] Algo bellman_ford step 7302 current loss 0.566416, current_train_items 233696.
I0302 19:01:54.340377 22626471084160 run.py:483] Algo bellman_ford step 7303 current loss 0.633163, current_train_items 233728.
I0302 19:01:54.375144 22626471084160 run.py:483] Algo bellman_ford step 7304 current loss 0.698982, current_train_items 233760.
I0302 19:01:54.395163 22626471084160 run.py:483] Algo bellman_ford step 7305 current loss 0.326783, current_train_items 233792.
I0302 19:01:54.410994 22626471084160 run.py:483] Algo bellman_ford step 7306 current loss 0.401065, current_train_items 233824.
I0302 19:01:54.434519 22626471084160 run.py:483] Algo bellman_ford step 7307 current loss 0.682388, current_train_items 233856.
I0302 19:01:54.464804 22626471084160 run.py:483] Algo bellman_ford step 7308 current loss 0.546337, current_train_items 233888.
I0302 19:01:54.499370 22626471084160 run.py:483] Algo bellman_ford step 7309 current loss 0.854661, current_train_items 233920.
I0302 19:01:54.519194 22626471084160 run.py:483] Algo bellman_ford step 7310 current loss 0.255948, current_train_items 233952.
I0302 19:01:54.535234 22626471084160 run.py:483] Algo bellman_ford step 7311 current loss 0.385052, current_train_items 233984.
I0302 19:01:54.560660 22626471084160 run.py:483] Algo bellman_ford step 7312 current loss 0.641672, current_train_items 234016.
I0302 19:01:54.592356 22626471084160 run.py:483] Algo bellman_ford step 7313 current loss 0.557543, current_train_items 234048.
I0302 19:01:54.627149 22626471084160 run.py:483] Algo bellman_ford step 7314 current loss 0.650961, current_train_items 234080.
I0302 19:01:54.646714 22626471084160 run.py:483] Algo bellman_ford step 7315 current loss 0.252912, current_train_items 234112.
I0302 19:01:54.662724 22626471084160 run.py:483] Algo bellman_ford step 7316 current loss 0.405341, current_train_items 234144.
I0302 19:01:54.687409 22626471084160 run.py:483] Algo bellman_ford step 7317 current loss 0.726662, current_train_items 234176.
I0302 19:01:54.718605 22626471084160 run.py:483] Algo bellman_ford step 7318 current loss 0.704465, current_train_items 234208.
I0302 19:01:54.753826 22626471084160 run.py:483] Algo bellman_ford step 7319 current loss 0.813604, current_train_items 234240.
I0302 19:01:54.773603 22626471084160 run.py:483] Algo bellman_ford step 7320 current loss 0.312715, current_train_items 234272.
I0302 19:01:54.790379 22626471084160 run.py:483] Algo bellman_ford step 7321 current loss 0.527190, current_train_items 234304.
I0302 19:01:54.815498 22626471084160 run.py:483] Algo bellman_ford step 7322 current loss 0.617333, current_train_items 234336.
I0302 19:01:54.846242 22626471084160 run.py:483] Algo bellman_ford step 7323 current loss 0.628006, current_train_items 234368.
I0302 19:01:54.881372 22626471084160 run.py:483] Algo bellman_ford step 7324 current loss 0.736809, current_train_items 234400.
I0302 19:01:54.900816 22626471084160 run.py:483] Algo bellman_ford step 7325 current loss 0.294187, current_train_items 234432.
I0302 19:01:54.917360 22626471084160 run.py:483] Algo bellman_ford step 7326 current loss 0.483098, current_train_items 234464.
I0302 19:01:54.941298 22626471084160 run.py:483] Algo bellman_ford step 7327 current loss 0.600418, current_train_items 234496.
I0302 19:01:54.972340 22626471084160 run.py:483] Algo bellman_ford step 7328 current loss 0.685739, current_train_items 234528.
I0302 19:01:55.006782 22626471084160 run.py:483] Algo bellman_ford step 7329 current loss 0.700949, current_train_items 234560.
I0302 19:01:55.026036 22626471084160 run.py:483] Algo bellman_ford step 7330 current loss 0.299641, current_train_items 234592.
I0302 19:01:55.042560 22626471084160 run.py:483] Algo bellman_ford step 7331 current loss 0.527414, current_train_items 234624.
I0302 19:01:55.067351 22626471084160 run.py:483] Algo bellman_ford step 7332 current loss 0.676332, current_train_items 234656.
I0302 19:01:55.099777 22626471084160 run.py:483] Algo bellman_ford step 7333 current loss 0.702341, current_train_items 234688.
I0302 19:01:55.131936 22626471084160 run.py:483] Algo bellman_ford step 7334 current loss 0.636480, current_train_items 234720.
I0302 19:01:55.151511 22626471084160 run.py:483] Algo bellman_ford step 7335 current loss 0.298107, current_train_items 234752.
I0302 19:01:55.167420 22626471084160 run.py:483] Algo bellman_ford step 7336 current loss 0.491296, current_train_items 234784.
I0302 19:01:55.191281 22626471084160 run.py:483] Algo bellman_ford step 7337 current loss 0.584628, current_train_items 234816.
I0302 19:01:55.222687 22626471084160 run.py:483] Algo bellman_ford step 7338 current loss 0.581771, current_train_items 234848.
I0302 19:01:55.258268 22626471084160 run.py:483] Algo bellman_ford step 7339 current loss 0.719964, current_train_items 234880.
I0302 19:01:55.277710 22626471084160 run.py:483] Algo bellman_ford step 7340 current loss 0.269995, current_train_items 234912.
I0302 19:01:55.293803 22626471084160 run.py:483] Algo bellman_ford step 7341 current loss 0.505922, current_train_items 234944.
I0302 19:01:55.318659 22626471084160 run.py:483] Algo bellman_ford step 7342 current loss 0.686186, current_train_items 234976.
I0302 19:01:55.350481 22626471084160 run.py:483] Algo bellman_ford step 7343 current loss 0.639941, current_train_items 235008.
I0302 19:01:55.383581 22626471084160 run.py:483] Algo bellman_ford step 7344 current loss 0.733820, current_train_items 235040.
I0302 19:01:55.403224 22626471084160 run.py:483] Algo bellman_ford step 7345 current loss 0.320014, current_train_items 235072.
I0302 19:01:55.419119 22626471084160 run.py:483] Algo bellman_ford step 7346 current loss 0.404821, current_train_items 235104.
I0302 19:01:55.443661 22626471084160 run.py:483] Algo bellman_ford step 7347 current loss 0.659340, current_train_items 235136.
I0302 19:01:55.475857 22626471084160 run.py:483] Algo bellman_ford step 7348 current loss 0.570647, current_train_items 235168.
I0302 19:01:55.506268 22626471084160 run.py:483] Algo bellman_ford step 7349 current loss 0.636038, current_train_items 235200.
I0302 19:01:55.525904 22626471084160 run.py:483] Algo bellman_ford step 7350 current loss 0.300793, current_train_items 235232.
I0302 19:01:55.533628 22626471084160 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0302 19:01:55.533760 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 19:01:55.551105 22626471084160 run.py:483] Algo bellman_ford step 7351 current loss 0.529992, current_train_items 235264.
I0302 19:01:55.576406 22626471084160 run.py:483] Algo bellman_ford step 7352 current loss 0.696831, current_train_items 235296.
I0302 19:01:55.608595 22626471084160 run.py:483] Algo bellman_ford step 7353 current loss 0.694064, current_train_items 235328.
I0302 19:01:55.641963 22626471084160 run.py:483] Algo bellman_ford step 7354 current loss 0.737730, current_train_items 235360.
I0302 19:01:55.662326 22626471084160 run.py:483] Algo bellman_ford step 7355 current loss 0.325031, current_train_items 235392.
I0302 19:01:55.677981 22626471084160 run.py:483] Algo bellman_ford step 7356 current loss 0.409492, current_train_items 235424.
I0302 19:01:55.702815 22626471084160 run.py:483] Algo bellman_ford step 7357 current loss 0.619419, current_train_items 235456.
I0302 19:01:55.734090 22626471084160 run.py:483] Algo bellman_ford step 7358 current loss 0.730208, current_train_items 235488.
I0302 19:01:55.767682 22626471084160 run.py:483] Algo bellman_ford step 7359 current loss 0.767877, current_train_items 235520.
I0302 19:01:55.787812 22626471084160 run.py:483] Algo bellman_ford step 7360 current loss 0.299299, current_train_items 235552.
I0302 19:01:55.804582 22626471084160 run.py:483] Algo bellman_ford step 7361 current loss 0.495366, current_train_items 235584.
I0302 19:01:55.827067 22626471084160 run.py:483] Algo bellman_ford step 7362 current loss 0.626193, current_train_items 235616.
I0302 19:01:55.860657 22626471084160 run.py:483] Algo bellman_ford step 7363 current loss 0.809537, current_train_items 235648.
I0302 19:01:55.892465 22626471084160 run.py:483] Algo bellman_ford step 7364 current loss 0.760718, current_train_items 235680.
I0302 19:01:55.912128 22626471084160 run.py:483] Algo bellman_ford step 7365 current loss 0.356750, current_train_items 235712.
I0302 19:01:55.928325 22626471084160 run.py:483] Algo bellman_ford step 7366 current loss 0.446094, current_train_items 235744.
I0302 19:01:55.952725 22626471084160 run.py:483] Algo bellman_ford step 7367 current loss 0.504680, current_train_items 235776.
I0302 19:01:55.986175 22626471084160 run.py:483] Algo bellman_ford step 7368 current loss 0.715171, current_train_items 235808.
I0302 19:01:56.018650 22626471084160 run.py:483] Algo bellman_ford step 7369 current loss 0.609813, current_train_items 235840.
I0302 19:01:56.038464 22626471084160 run.py:483] Algo bellman_ford step 7370 current loss 0.228071, current_train_items 235872.
I0302 19:01:56.054827 22626471084160 run.py:483] Algo bellman_ford step 7371 current loss 0.642019, current_train_items 235904.
I0302 19:01:56.077471 22626471084160 run.py:483] Algo bellman_ford step 7372 current loss 0.656476, current_train_items 235936.
I0302 19:01:56.108839 22626471084160 run.py:483] Algo bellman_ford step 7373 current loss 0.612060, current_train_items 235968.
I0302 19:01:56.141880 22626471084160 run.py:483] Algo bellman_ford step 7374 current loss 0.819956, current_train_items 236000.
I0302 19:01:56.161901 22626471084160 run.py:483] Algo bellman_ford step 7375 current loss 0.240996, current_train_items 236032.
I0302 19:01:56.178722 22626471084160 run.py:483] Algo bellman_ford step 7376 current loss 0.419533, current_train_items 236064.
I0302 19:01:56.202986 22626471084160 run.py:483] Algo bellman_ford step 7377 current loss 0.627196, current_train_items 236096.
I0302 19:01:56.234448 22626471084160 run.py:483] Algo bellman_ford step 7378 current loss 0.634616, current_train_items 236128.
I0302 19:01:56.266945 22626471084160 run.py:483] Algo bellman_ford step 7379 current loss 0.743689, current_train_items 236160.
I0302 19:01:56.286364 22626471084160 run.py:483] Algo bellman_ford step 7380 current loss 0.352102, current_train_items 236192.
I0302 19:01:56.302226 22626471084160 run.py:483] Algo bellman_ford step 7381 current loss 0.417337, current_train_items 236224.
I0302 19:01:56.327151 22626471084160 run.py:483] Algo bellman_ford step 7382 current loss 0.597962, current_train_items 236256.
I0302 19:01:56.357855 22626471084160 run.py:483] Algo bellman_ford step 7383 current loss 0.689469, current_train_items 236288.
I0302 19:01:56.390020 22626471084160 run.py:483] Algo bellman_ford step 7384 current loss 0.693327, current_train_items 236320.
I0302 19:01:56.409977 22626471084160 run.py:483] Algo bellman_ford step 7385 current loss 0.246819, current_train_items 236352.
I0302 19:01:56.426332 22626471084160 run.py:483] Algo bellman_ford step 7386 current loss 0.416606, current_train_items 236384.
I0302 19:01:56.450352 22626471084160 run.py:483] Algo bellman_ford step 7387 current loss 0.590745, current_train_items 236416.
I0302 19:01:56.482601 22626471084160 run.py:483] Algo bellman_ford step 7388 current loss 0.629549, current_train_items 236448.
I0302 19:01:56.515034 22626471084160 run.py:483] Algo bellman_ford step 7389 current loss 0.620680, current_train_items 236480.
I0302 19:01:56.535289 22626471084160 run.py:483] Algo bellman_ford step 7390 current loss 0.282485, current_train_items 236512.
I0302 19:01:56.551856 22626471084160 run.py:483] Algo bellman_ford step 7391 current loss 0.497588, current_train_items 236544.
I0302 19:01:56.575275 22626471084160 run.py:483] Algo bellman_ford step 7392 current loss 0.590586, current_train_items 236576.
I0302 19:01:56.606069 22626471084160 run.py:483] Algo bellman_ford step 7393 current loss 0.571253, current_train_items 236608.
I0302 19:01:56.640083 22626471084160 run.py:483] Algo bellman_ford step 7394 current loss 0.745411, current_train_items 236640.
I0302 19:01:56.659486 22626471084160 run.py:483] Algo bellman_ford step 7395 current loss 0.312697, current_train_items 236672.
I0302 19:01:56.676005 22626471084160 run.py:483] Algo bellman_ford step 7396 current loss 0.429692, current_train_items 236704.
I0302 19:01:56.700850 22626471084160 run.py:483] Algo bellman_ford step 7397 current loss 0.692182, current_train_items 236736.
I0302 19:01:56.731710 22626471084160 run.py:483] Algo bellman_ford step 7398 current loss 0.582347, current_train_items 236768.
I0302 19:01:56.763725 22626471084160 run.py:483] Algo bellman_ford step 7399 current loss 0.735205, current_train_items 236800.
I0302 19:01:56.783481 22626471084160 run.py:483] Algo bellman_ford step 7400 current loss 0.300640, current_train_items 236832.
I0302 19:01:56.791086 22626471084160 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.943359375, 'score': 0.943359375, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0302 19:01:56.791207 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.943, val scores are: bellman_ford: 0.943
I0302 19:01:56.808229 22626471084160 run.py:483] Algo bellman_ford step 7401 current loss 0.443830, current_train_items 236864.
I0302 19:01:56.833532 22626471084160 run.py:483] Algo bellman_ford step 7402 current loss 0.566346, current_train_items 236896.
I0302 19:01:56.865909 22626471084160 run.py:483] Algo bellman_ford step 7403 current loss 0.734070, current_train_items 236928.
I0302 19:01:56.900380 22626471084160 run.py:483] Algo bellman_ford step 7404 current loss 0.810047, current_train_items 236960.
I0302 19:01:56.920400 22626471084160 run.py:483] Algo bellman_ford step 7405 current loss 0.345654, current_train_items 236992.
I0302 19:01:56.935954 22626471084160 run.py:483] Algo bellman_ford step 7406 current loss 0.393656, current_train_items 237024.
I0302 19:01:56.959945 22626471084160 run.py:483] Algo bellman_ford step 7407 current loss 0.547544, current_train_items 237056.
I0302 19:01:56.992017 22626471084160 run.py:483] Algo bellman_ford step 7408 current loss 0.664001, current_train_items 237088.
I0302 19:01:57.024049 22626471084160 run.py:483] Algo bellman_ford step 7409 current loss 0.749650, current_train_items 237120.
I0302 19:01:57.043520 22626471084160 run.py:483] Algo bellman_ford step 7410 current loss 0.302133, current_train_items 237152.
I0302 19:01:57.060302 22626471084160 run.py:483] Algo bellman_ford step 7411 current loss 0.497794, current_train_items 237184.
I0302 19:01:57.085195 22626471084160 run.py:483] Algo bellman_ford step 7412 current loss 0.607607, current_train_items 237216.
I0302 19:01:57.117502 22626471084160 run.py:483] Algo bellman_ford step 7413 current loss 0.689224, current_train_items 237248.
I0302 19:01:57.149567 22626471084160 run.py:483] Algo bellman_ford step 7414 current loss 0.758523, current_train_items 237280.
I0302 19:01:57.168909 22626471084160 run.py:483] Algo bellman_ford step 7415 current loss 0.300000, current_train_items 237312.
I0302 19:01:57.184829 22626471084160 run.py:483] Algo bellman_ford step 7416 current loss 0.475269, current_train_items 237344.
I0302 19:01:57.208712 22626471084160 run.py:483] Algo bellman_ford step 7417 current loss 0.726342, current_train_items 237376.
I0302 19:01:57.239853 22626471084160 run.py:483] Algo bellman_ford step 7418 current loss 0.797448, current_train_items 237408.
I0302 19:01:57.272556 22626471084160 run.py:483] Algo bellman_ford step 7419 current loss 0.801889, current_train_items 237440.
I0302 19:01:57.292346 22626471084160 run.py:483] Algo bellman_ford step 7420 current loss 0.337396, current_train_items 237472.
I0302 19:01:57.308325 22626471084160 run.py:483] Algo bellman_ford step 7421 current loss 0.517464, current_train_items 237504.
I0302 19:01:57.332104 22626471084160 run.py:483] Algo bellman_ford step 7422 current loss 0.588051, current_train_items 237536.
I0302 19:01:57.363026 22626471084160 run.py:483] Algo bellman_ford step 7423 current loss 0.756454, current_train_items 237568.
I0302 19:01:57.395798 22626471084160 run.py:483] Algo bellman_ford step 7424 current loss 0.721290, current_train_items 237600.
I0302 19:01:57.415280 22626471084160 run.py:483] Algo bellman_ford step 7425 current loss 0.270467, current_train_items 237632.
I0302 19:01:57.431283 22626471084160 run.py:483] Algo bellman_ford step 7426 current loss 0.379722, current_train_items 237664.
I0302 19:01:57.455669 22626471084160 run.py:483] Algo bellman_ford step 7427 current loss 0.570870, current_train_items 237696.
I0302 19:01:57.486382 22626471084160 run.py:483] Algo bellman_ford step 7428 current loss 0.650918, current_train_items 237728.
I0302 19:01:57.520384 22626471084160 run.py:483] Algo bellman_ford step 7429 current loss 0.950685, current_train_items 237760.
I0302 19:01:57.539793 22626471084160 run.py:483] Algo bellman_ford step 7430 current loss 0.267751, current_train_items 237792.
I0302 19:01:57.555603 22626471084160 run.py:483] Algo bellman_ford step 7431 current loss 0.420546, current_train_items 237824.
I0302 19:01:57.579804 22626471084160 run.py:483] Algo bellman_ford step 7432 current loss 0.646632, current_train_items 237856.
I0302 19:01:57.611556 22626471084160 run.py:483] Algo bellman_ford step 7433 current loss 0.779215, current_train_items 237888.
I0302 19:01:57.642479 22626471084160 run.py:483] Algo bellman_ford step 7434 current loss 0.773676, current_train_items 237920.
I0302 19:01:57.661890 22626471084160 run.py:483] Algo bellman_ford step 7435 current loss 0.276163, current_train_items 237952.
I0302 19:01:57.678105 22626471084160 run.py:483] Algo bellman_ford step 7436 current loss 0.503651, current_train_items 237984.
I0302 19:01:57.702730 22626471084160 run.py:483] Algo bellman_ford step 7437 current loss 0.550780, current_train_items 238016.
I0302 19:01:57.733439 22626471084160 run.py:483] Algo bellman_ford step 7438 current loss 0.646367, current_train_items 238048.
I0302 19:01:57.767625 22626471084160 run.py:483] Algo bellman_ford step 7439 current loss 0.718473, current_train_items 238080.
I0302 19:01:57.787199 22626471084160 run.py:483] Algo bellman_ford step 7440 current loss 0.364794, current_train_items 238112.
I0302 19:01:57.803804 22626471084160 run.py:483] Algo bellman_ford step 7441 current loss 0.558991, current_train_items 238144.
I0302 19:01:57.826924 22626471084160 run.py:483] Algo bellman_ford step 7442 current loss 0.536204, current_train_items 238176.
I0302 19:01:57.858924 22626471084160 run.py:483] Algo bellman_ford step 7443 current loss 0.681809, current_train_items 238208.
I0302 19:01:57.893137 22626471084160 run.py:483] Algo bellman_ford step 7444 current loss 0.662301, current_train_items 238240.
I0302 19:01:57.912582 22626471084160 run.py:483] Algo bellman_ford step 7445 current loss 0.288801, current_train_items 238272.
I0302 19:01:57.928687 22626471084160 run.py:483] Algo bellman_ford step 7446 current loss 0.472772, current_train_items 238304.
I0302 19:01:57.952299 22626471084160 run.py:483] Algo bellman_ford step 7447 current loss 0.554274, current_train_items 238336.
I0302 19:01:57.983138 22626471084160 run.py:483] Algo bellman_ford step 7448 current loss 0.628967, current_train_items 238368.
I0302 19:01:58.017963 22626471084160 run.py:483] Algo bellman_ford step 7449 current loss 0.742892, current_train_items 238400.
I0302 19:01:58.037469 22626471084160 run.py:483] Algo bellman_ford step 7450 current loss 0.317945, current_train_items 238432.
I0302 19:01:58.045469 22626471084160 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0302 19:01:58.045573 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:01:58.062501 22626471084160 run.py:483] Algo bellman_ford step 7451 current loss 0.430205, current_train_items 238464.
I0302 19:01:58.088132 22626471084160 run.py:483] Algo bellman_ford step 7452 current loss 0.605421, current_train_items 238496.
I0302 19:01:58.119292 22626471084160 run.py:483] Algo bellman_ford step 7453 current loss 0.637560, current_train_items 238528.
I0302 19:01:58.152146 22626471084160 run.py:483] Algo bellman_ford step 7454 current loss 0.862966, current_train_items 238560.
I0302 19:01:58.172251 22626471084160 run.py:483] Algo bellman_ford step 7455 current loss 0.292742, current_train_items 238592.
I0302 19:01:58.188695 22626471084160 run.py:483] Algo bellman_ford step 7456 current loss 0.419970, current_train_items 238624.
I0302 19:01:58.212302 22626471084160 run.py:483] Algo bellman_ford step 7457 current loss 0.523393, current_train_items 238656.
I0302 19:01:58.244642 22626471084160 run.py:483] Algo bellman_ford step 7458 current loss 0.661178, current_train_items 238688.
I0302 19:01:58.277841 22626471084160 run.py:483] Algo bellman_ford step 7459 current loss 0.802203, current_train_items 238720.
I0302 19:01:58.297878 22626471084160 run.py:483] Algo bellman_ford step 7460 current loss 0.206858, current_train_items 238752.
I0302 19:01:58.314698 22626471084160 run.py:483] Algo bellman_ford step 7461 current loss 0.444974, current_train_items 238784.
I0302 19:01:58.337888 22626471084160 run.py:483] Algo bellman_ford step 7462 current loss 0.596095, current_train_items 238816.
I0302 19:01:58.369064 22626471084160 run.py:483] Algo bellman_ford step 7463 current loss 0.609928, current_train_items 238848.
I0302 19:01:58.402416 22626471084160 run.py:483] Algo bellman_ford step 7464 current loss 0.858862, current_train_items 238880.
I0302 19:01:58.422114 22626471084160 run.py:483] Algo bellman_ford step 7465 current loss 0.283836, current_train_items 238912.
I0302 19:01:58.438200 22626471084160 run.py:483] Algo bellman_ford step 7466 current loss 0.432556, current_train_items 238944.
I0302 19:01:58.461625 22626471084160 run.py:483] Algo bellman_ford step 7467 current loss 0.602202, current_train_items 238976.
I0302 19:01:58.493895 22626471084160 run.py:483] Algo bellman_ford step 7468 current loss 0.597897, current_train_items 239008.
I0302 19:01:58.526038 22626471084160 run.py:483] Algo bellman_ford step 7469 current loss 0.713577, current_train_items 239040.
I0302 19:01:58.546292 22626471084160 run.py:483] Algo bellman_ford step 7470 current loss 0.254945, current_train_items 239072.
I0302 19:01:58.562553 22626471084160 run.py:483] Algo bellman_ford step 7471 current loss 0.443948, current_train_items 239104.
I0302 19:01:58.585527 22626471084160 run.py:483] Algo bellman_ford step 7472 current loss 0.650884, current_train_items 239136.
I0302 19:01:58.617311 22626471084160 run.py:483] Algo bellman_ford step 7473 current loss 0.718187, current_train_items 239168.
I0302 19:01:58.652496 22626471084160 run.py:483] Algo bellman_ford step 7474 current loss 0.727656, current_train_items 239200.
I0302 19:01:58.672592 22626471084160 run.py:483] Algo bellman_ford step 7475 current loss 0.250822, current_train_items 239232.
I0302 19:01:58.688941 22626471084160 run.py:483] Algo bellman_ford step 7476 current loss 0.492929, current_train_items 239264.
I0302 19:01:58.711697 22626471084160 run.py:483] Algo bellman_ford step 7477 current loss 0.552013, current_train_items 239296.
I0302 19:01:58.743685 22626471084160 run.py:483] Algo bellman_ford step 7478 current loss 0.581493, current_train_items 239328.
I0302 19:01:58.780169 22626471084160 run.py:483] Algo bellman_ford step 7479 current loss 0.857744, current_train_items 239360.
I0302 19:01:58.800019 22626471084160 run.py:483] Algo bellman_ford step 7480 current loss 0.283304, current_train_items 239392.
I0302 19:01:58.816408 22626471084160 run.py:483] Algo bellman_ford step 7481 current loss 0.454029, current_train_items 239424.
I0302 19:01:58.841491 22626471084160 run.py:483] Algo bellman_ford step 7482 current loss 0.638038, current_train_items 239456.
I0302 19:01:58.874223 22626471084160 run.py:483] Algo bellman_ford step 7483 current loss 0.706252, current_train_items 239488.
I0302 19:01:58.908662 22626471084160 run.py:483] Algo bellman_ford step 7484 current loss 0.760144, current_train_items 239520.
I0302 19:01:58.928840 22626471084160 run.py:483] Algo bellman_ford step 7485 current loss 0.275580, current_train_items 239552.
I0302 19:01:58.945134 22626471084160 run.py:483] Algo bellman_ford step 7486 current loss 0.493475, current_train_items 239584.
I0302 19:01:58.968040 22626471084160 run.py:483] Algo bellman_ford step 7487 current loss 0.473882, current_train_items 239616.
I0302 19:01:59.000061 22626471084160 run.py:483] Algo bellman_ford step 7488 current loss 0.634852, current_train_items 239648.
I0302 19:01:59.035551 22626471084160 run.py:483] Algo bellman_ford step 7489 current loss 0.751915, current_train_items 239680.
I0302 19:01:59.055247 22626471084160 run.py:483] Algo bellman_ford step 7490 current loss 0.343865, current_train_items 239712.
I0302 19:01:59.071099 22626471084160 run.py:483] Algo bellman_ford step 7491 current loss 0.477117, current_train_items 239744.
I0302 19:01:59.094086 22626471084160 run.py:483] Algo bellman_ford step 7492 current loss 0.491375, current_train_items 239776.
I0302 19:01:59.125595 22626471084160 run.py:483] Algo bellman_ford step 7493 current loss 0.606298, current_train_items 239808.
I0302 19:01:59.158521 22626471084160 run.py:483] Algo bellman_ford step 7494 current loss 0.732588, current_train_items 239840.
I0302 19:01:59.178178 22626471084160 run.py:483] Algo bellman_ford step 7495 current loss 0.401107, current_train_items 239872.
I0302 19:01:59.194576 22626471084160 run.py:483] Algo bellman_ford step 7496 current loss 0.436918, current_train_items 239904.
I0302 19:01:59.218389 22626471084160 run.py:483] Algo bellman_ford step 7497 current loss 0.547709, current_train_items 239936.
I0302 19:01:59.249444 22626471084160 run.py:483] Algo bellman_ford step 7498 current loss 0.682749, current_train_items 239968.
I0302 19:01:59.282366 22626471084160 run.py:483] Algo bellman_ford step 7499 current loss 0.699381, current_train_items 240000.
I0302 19:01:59.302419 22626471084160 run.py:483] Algo bellman_ford step 7500 current loss 0.332215, current_train_items 240032.
I0302 19:01:59.310098 22626471084160 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0302 19:01:59.310215 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:01:59.327043 22626471084160 run.py:483] Algo bellman_ford step 7501 current loss 0.390504, current_train_items 240064.
I0302 19:01:59.351307 22626471084160 run.py:483] Algo bellman_ford step 7502 current loss 0.569174, current_train_items 240096.
I0302 19:01:59.383164 22626471084160 run.py:483] Algo bellman_ford step 7503 current loss 0.678491, current_train_items 240128.
I0302 19:01:59.416877 22626471084160 run.py:483] Algo bellman_ford step 7504 current loss 0.738877, current_train_items 240160.
I0302 19:01:59.437012 22626471084160 run.py:483] Algo bellman_ford step 7505 current loss 0.325210, current_train_items 240192.
I0302 19:01:59.452989 22626471084160 run.py:483] Algo bellman_ford step 7506 current loss 0.534071, current_train_items 240224.
I0302 19:01:59.476292 22626471084160 run.py:483] Algo bellman_ford step 7507 current loss 0.531467, current_train_items 240256.
I0302 19:01:59.507331 22626471084160 run.py:483] Algo bellman_ford step 7508 current loss 0.607970, current_train_items 240288.
I0302 19:01:59.539878 22626471084160 run.py:483] Algo bellman_ford step 7509 current loss 0.700898, current_train_items 240320.
I0302 19:01:59.559655 22626471084160 run.py:483] Algo bellman_ford step 7510 current loss 0.231986, current_train_items 240352.
I0302 19:01:59.576185 22626471084160 run.py:483] Algo bellman_ford step 7511 current loss 0.571570, current_train_items 240384.
I0302 19:01:59.598999 22626471084160 run.py:483] Algo bellman_ford step 7512 current loss 0.630267, current_train_items 240416.
I0302 19:01:59.630304 22626471084160 run.py:483] Algo bellman_ford step 7513 current loss 0.625384, current_train_items 240448.
I0302 19:01:59.662605 22626471084160 run.py:483] Algo bellman_ford step 7514 current loss 0.670292, current_train_items 240480.
I0302 19:01:59.682253 22626471084160 run.py:483] Algo bellman_ford step 7515 current loss 0.344047, current_train_items 240512.
I0302 19:01:59.697839 22626471084160 run.py:483] Algo bellman_ford step 7516 current loss 0.552095, current_train_items 240544.
I0302 19:01:59.722386 22626471084160 run.py:483] Algo bellman_ford step 7517 current loss 0.634920, current_train_items 240576.
I0302 19:01:59.754196 22626471084160 run.py:483] Algo bellman_ford step 7518 current loss 0.669154, current_train_items 240608.
I0302 19:01:59.788275 22626471084160 run.py:483] Algo bellman_ford step 7519 current loss 0.812373, current_train_items 240640.
I0302 19:01:59.808070 22626471084160 run.py:483] Algo bellman_ford step 7520 current loss 0.248733, current_train_items 240672.
I0302 19:01:59.824329 22626471084160 run.py:483] Algo bellman_ford step 7521 current loss 0.405499, current_train_items 240704.
I0302 19:01:59.848163 22626471084160 run.py:483] Algo bellman_ford step 7522 current loss 0.630995, current_train_items 240736.
I0302 19:01:59.879593 22626471084160 run.py:483] Algo bellman_ford step 7523 current loss 0.807790, current_train_items 240768.
I0302 19:01:59.910844 22626471084160 run.py:483] Algo bellman_ford step 7524 current loss 0.682583, current_train_items 240800.
I0302 19:01:59.930367 22626471084160 run.py:483] Algo bellman_ford step 7525 current loss 0.201051, current_train_items 240832.
I0302 19:01:59.946427 22626471084160 run.py:483] Algo bellman_ford step 7526 current loss 0.449804, current_train_items 240864.
I0302 19:01:59.971180 22626471084160 run.py:483] Algo bellman_ford step 7527 current loss 0.644671, current_train_items 240896.
I0302 19:02:00.001860 22626471084160 run.py:483] Algo bellman_ford step 7528 current loss 0.616929, current_train_items 240928.
I0302 19:02:00.035271 22626471084160 run.py:483] Algo bellman_ford step 7529 current loss 0.639844, current_train_items 240960.
I0302 19:02:00.054822 22626471084160 run.py:483] Algo bellman_ford step 7530 current loss 0.275494, current_train_items 240992.
I0302 19:02:00.070862 22626471084160 run.py:483] Algo bellman_ford step 7531 current loss 0.412743, current_train_items 241024.
I0302 19:02:00.094109 22626471084160 run.py:483] Algo bellman_ford step 7532 current loss 0.571354, current_train_items 241056.
I0302 19:02:00.126933 22626471084160 run.py:483] Algo bellman_ford step 7533 current loss 0.623944, current_train_items 241088.
I0302 19:02:00.161463 22626471084160 run.py:483] Algo bellman_ford step 7534 current loss 0.720357, current_train_items 241120.
I0302 19:02:00.181088 22626471084160 run.py:483] Algo bellman_ford step 7535 current loss 0.274291, current_train_items 241152.
I0302 19:02:00.197307 22626471084160 run.py:483] Algo bellman_ford step 7536 current loss 0.444197, current_train_items 241184.
I0302 19:02:00.220978 22626471084160 run.py:483] Algo bellman_ford step 7537 current loss 0.619731, current_train_items 241216.
I0302 19:02:00.252863 22626471084160 run.py:483] Algo bellman_ford step 7538 current loss 0.680847, current_train_items 241248.
I0302 19:02:00.285660 22626471084160 run.py:483] Algo bellman_ford step 7539 current loss 0.679547, current_train_items 241280.
I0302 19:02:00.305538 22626471084160 run.py:483] Algo bellman_ford step 7540 current loss 0.249739, current_train_items 241312.
I0302 19:02:00.322195 22626471084160 run.py:483] Algo bellman_ford step 7541 current loss 0.492853, current_train_items 241344.
I0302 19:02:00.347213 22626471084160 run.py:483] Algo bellman_ford step 7542 current loss 0.558863, current_train_items 241376.
I0302 19:02:00.379387 22626471084160 run.py:483] Algo bellman_ford step 7543 current loss 0.654902, current_train_items 241408.
I0302 19:02:00.413235 22626471084160 run.py:483] Algo bellman_ford step 7544 current loss 0.716434, current_train_items 241440.
I0302 19:02:00.432533 22626471084160 run.py:483] Algo bellman_ford step 7545 current loss 0.218407, current_train_items 241472.
I0302 19:02:00.448728 22626471084160 run.py:483] Algo bellman_ford step 7546 current loss 0.561801, current_train_items 241504.
I0302 19:02:00.473435 22626471084160 run.py:483] Algo bellman_ford step 7547 current loss 0.718650, current_train_items 241536.
I0302 19:02:00.505468 22626471084160 run.py:483] Algo bellman_ford step 7548 current loss 0.834974, current_train_items 241568.
I0302 19:02:00.539334 22626471084160 run.py:483] Algo bellman_ford step 7549 current loss 0.713820, current_train_items 241600.
I0302 19:02:00.558947 22626471084160 run.py:483] Algo bellman_ford step 7550 current loss 0.329366, current_train_items 241632.
I0302 19:02:00.566972 22626471084160 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.951171875, 'score': 0.951171875, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0302 19:02:00.567079 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.951, val scores are: bellman_ford: 0.951
I0302 19:02:00.584224 22626471084160 run.py:483] Algo bellman_ford step 7551 current loss 0.477449, current_train_items 241664.
I0302 19:02:00.608724 22626471084160 run.py:483] Algo bellman_ford step 7552 current loss 0.587543, current_train_items 241696.
I0302 19:02:00.641069 22626471084160 run.py:483] Algo bellman_ford step 7553 current loss 0.700549, current_train_items 241728.
I0302 19:02:00.674838 22626471084160 run.py:483] Algo bellman_ford step 7554 current loss 0.806045, current_train_items 241760.
I0302 19:02:00.695044 22626471084160 run.py:483] Algo bellman_ford step 7555 current loss 0.241306, current_train_items 241792.
I0302 19:02:00.711178 22626471084160 run.py:483] Algo bellman_ford step 7556 current loss 0.581925, current_train_items 241824.
I0302 19:02:00.735383 22626471084160 run.py:483] Algo bellman_ford step 7557 current loss 0.816168, current_train_items 241856.
I0302 19:02:00.768462 22626471084160 run.py:483] Algo bellman_ford step 7558 current loss 0.849663, current_train_items 241888.
I0302 19:02:00.800900 22626471084160 run.py:483] Algo bellman_ford step 7559 current loss 0.815307, current_train_items 241920.
I0302 19:02:00.820712 22626471084160 run.py:483] Algo bellman_ford step 7560 current loss 0.279326, current_train_items 241952.
I0302 19:02:00.837773 22626471084160 run.py:483] Algo bellman_ford step 7561 current loss 0.468180, current_train_items 241984.
I0302 19:02:00.860930 22626471084160 run.py:483] Algo bellman_ford step 7562 current loss 0.624370, current_train_items 242016.
I0302 19:02:00.892802 22626471084160 run.py:483] Algo bellman_ford step 7563 current loss 0.684880, current_train_items 242048.
I0302 19:02:00.926436 22626471084160 run.py:483] Algo bellman_ford step 7564 current loss 0.748310, current_train_items 242080.
I0302 19:02:00.946183 22626471084160 run.py:483] Algo bellman_ford step 7565 current loss 0.337568, current_train_items 242112.
I0302 19:02:00.962132 22626471084160 run.py:483] Algo bellman_ford step 7566 current loss 0.470453, current_train_items 242144.
I0302 19:02:00.986492 22626471084160 run.py:483] Algo bellman_ford step 7567 current loss 0.590241, current_train_items 242176.
I0302 19:02:01.017680 22626471084160 run.py:483] Algo bellman_ford step 7568 current loss 0.678720, current_train_items 242208.
I0302 19:02:01.050611 22626471084160 run.py:483] Algo bellman_ford step 7569 current loss 0.763610, current_train_items 242240.
I0302 19:02:01.070881 22626471084160 run.py:483] Algo bellman_ford step 7570 current loss 0.342719, current_train_items 242272.
I0302 19:02:01.087336 22626471084160 run.py:483] Algo bellman_ford step 7571 current loss 0.491610, current_train_items 242304.
I0302 19:02:01.110590 22626471084160 run.py:483] Algo bellman_ford step 7572 current loss 0.653615, current_train_items 242336.
I0302 19:02:01.140923 22626471084160 run.py:483] Algo bellman_ford step 7573 current loss 0.605294, current_train_items 242368.
I0302 19:02:01.172233 22626471084160 run.py:483] Algo bellman_ford step 7574 current loss 0.622581, current_train_items 242400.
I0302 19:02:01.192169 22626471084160 run.py:483] Algo bellman_ford step 7575 current loss 0.236925, current_train_items 242432.
I0302 19:02:01.209028 22626471084160 run.py:483] Algo bellman_ford step 7576 current loss 0.429454, current_train_items 242464.
I0302 19:02:01.231989 22626471084160 run.py:483] Algo bellman_ford step 7577 current loss 0.525817, current_train_items 242496.
I0302 19:02:01.263665 22626471084160 run.py:483] Algo bellman_ford step 7578 current loss 0.620213, current_train_items 242528.
I0302 19:02:01.298587 22626471084160 run.py:483] Algo bellman_ford step 7579 current loss 0.824540, current_train_items 242560.
I0302 19:02:01.318276 22626471084160 run.py:483] Algo bellman_ford step 7580 current loss 0.291521, current_train_items 242592.
I0302 19:02:01.334677 22626471084160 run.py:483] Algo bellman_ford step 7581 current loss 0.507701, current_train_items 242624.
I0302 19:02:01.358688 22626471084160 run.py:483] Algo bellman_ford step 7582 current loss 0.636808, current_train_items 242656.
I0302 19:02:01.389975 22626471084160 run.py:483] Algo bellman_ford step 7583 current loss 0.658467, current_train_items 242688.
I0302 19:02:01.422636 22626471084160 run.py:483] Algo bellman_ford step 7584 current loss 0.778980, current_train_items 242720.
I0302 19:02:01.442593 22626471084160 run.py:483] Algo bellman_ford step 7585 current loss 0.219634, current_train_items 242752.
I0302 19:02:01.458561 22626471084160 run.py:483] Algo bellman_ford step 7586 current loss 0.535395, current_train_items 242784.
I0302 19:02:01.482112 22626471084160 run.py:483] Algo bellman_ford step 7587 current loss 0.644090, current_train_items 242816.
I0302 19:02:01.511377 22626471084160 run.py:483] Algo bellman_ford step 7588 current loss 0.555818, current_train_items 242848.
I0302 19:02:01.545587 22626471084160 run.py:483] Algo bellman_ford step 7589 current loss 0.733202, current_train_items 242880.
I0302 19:02:01.565558 22626471084160 run.py:483] Algo bellman_ford step 7590 current loss 0.257938, current_train_items 242912.
I0302 19:02:01.581575 22626471084160 run.py:483] Algo bellman_ford step 7591 current loss 0.442108, current_train_items 242944.
I0302 19:02:01.606233 22626471084160 run.py:483] Algo bellman_ford step 7592 current loss 0.712038, current_train_items 242976.
I0302 19:02:01.636461 22626471084160 run.py:483] Algo bellman_ford step 7593 current loss 0.807638, current_train_items 243008.
I0302 19:02:01.670074 22626471084160 run.py:483] Algo bellman_ford step 7594 current loss 0.766631, current_train_items 243040.
I0302 19:02:01.689811 22626471084160 run.py:483] Algo bellman_ford step 7595 current loss 0.299190, current_train_items 243072.
I0302 19:02:01.705880 22626471084160 run.py:483] Algo bellman_ford step 7596 current loss 0.477810, current_train_items 243104.
I0302 19:02:01.730430 22626471084160 run.py:483] Algo bellman_ford step 7597 current loss 0.681350, current_train_items 243136.
I0302 19:02:01.762667 22626471084160 run.py:483] Algo bellman_ford step 7598 current loss 0.645140, current_train_items 243168.
I0302 19:02:01.793404 22626471084160 run.py:483] Algo bellman_ford step 7599 current loss 0.840917, current_train_items 243200.
I0302 19:02:01.813327 22626471084160 run.py:483] Algo bellman_ford step 7600 current loss 0.289815, current_train_items 243232.
I0302 19:02:01.820957 22626471084160 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0302 19:02:01.821063 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:02:01.838006 22626471084160 run.py:483] Algo bellman_ford step 7601 current loss 0.489885, current_train_items 243264.
I0302 19:02:01.863788 22626471084160 run.py:483] Algo bellman_ford step 7602 current loss 0.672741, current_train_items 243296.
I0302 19:02:01.895375 22626471084160 run.py:483] Algo bellman_ford step 7603 current loss 0.622395, current_train_items 243328.
I0302 19:02:01.930175 22626471084160 run.py:483] Algo bellman_ford step 7604 current loss 0.777275, current_train_items 243360.
I0302 19:02:01.950473 22626471084160 run.py:483] Algo bellman_ford step 7605 current loss 0.285592, current_train_items 243392.
I0302 19:02:01.965766 22626471084160 run.py:483] Algo bellman_ford step 7606 current loss 0.465805, current_train_items 243424.
I0302 19:02:01.989261 22626471084160 run.py:483] Algo bellman_ford step 7607 current loss 0.481476, current_train_items 243456.
I0302 19:02:02.020922 22626471084160 run.py:483] Algo bellman_ford step 7608 current loss 0.646042, current_train_items 243488.
I0302 19:02:02.055471 22626471084160 run.py:483] Algo bellman_ford step 7609 current loss 0.781090, current_train_items 243520.
I0302 19:02:02.075232 22626471084160 run.py:483] Algo bellman_ford step 7610 current loss 0.254699, current_train_items 243552.
I0302 19:02:02.091852 22626471084160 run.py:483] Algo bellman_ford step 7611 current loss 0.482896, current_train_items 243584.
I0302 19:02:02.115616 22626471084160 run.py:483] Algo bellman_ford step 7612 current loss 0.575067, current_train_items 243616.
I0302 19:02:02.144793 22626471084160 run.py:483] Algo bellman_ford step 7613 current loss 0.593636, current_train_items 243648.
I0302 19:02:02.177824 22626471084160 run.py:483] Algo bellman_ford step 7614 current loss 0.849272, current_train_items 243680.
I0302 19:02:02.197553 22626471084160 run.py:483] Algo bellman_ford step 7615 current loss 0.283296, current_train_items 243712.
I0302 19:02:02.213913 22626471084160 run.py:483] Algo bellman_ford step 7616 current loss 0.478990, current_train_items 243744.
I0302 19:02:02.237625 22626471084160 run.py:483] Algo bellman_ford step 7617 current loss 0.551022, current_train_items 243776.
I0302 19:02:02.268331 22626471084160 run.py:483] Algo bellman_ford step 7618 current loss 0.625616, current_train_items 243808.
I0302 19:02:02.302193 22626471084160 run.py:483] Algo bellman_ford step 7619 current loss 0.743658, current_train_items 243840.
I0302 19:02:02.321898 22626471084160 run.py:483] Algo bellman_ford step 7620 current loss 0.339635, current_train_items 243872.
I0302 19:02:02.338110 22626471084160 run.py:483] Algo bellman_ford step 7621 current loss 0.578764, current_train_items 243904.
I0302 19:02:02.362314 22626471084160 run.py:483] Algo bellman_ford step 7622 current loss 0.567053, current_train_items 243936.
I0302 19:02:02.395096 22626471084160 run.py:483] Algo bellman_ford step 7623 current loss 0.668477, current_train_items 243968.
I0302 19:02:02.428044 22626471084160 run.py:483] Algo bellman_ford step 7624 current loss 0.650696, current_train_items 244000.
I0302 19:02:02.447989 22626471084160 run.py:483] Algo bellman_ford step 7625 current loss 0.270907, current_train_items 244032.
I0302 19:02:02.463422 22626471084160 run.py:483] Algo bellman_ford step 7626 current loss 0.401630, current_train_items 244064.
I0302 19:02:02.488037 22626471084160 run.py:483] Algo bellman_ford step 7627 current loss 0.717606, current_train_items 244096.
I0302 19:02:02.520308 22626471084160 run.py:483] Algo bellman_ford step 7628 current loss 0.725449, current_train_items 244128.
I0302 19:02:02.553287 22626471084160 run.py:483] Algo bellman_ford step 7629 current loss 0.788544, current_train_items 244160.
I0302 19:02:02.572905 22626471084160 run.py:483] Algo bellman_ford step 7630 current loss 0.265446, current_train_items 244192.
I0302 19:02:02.589412 22626471084160 run.py:483] Algo bellman_ford step 7631 current loss 0.640386, current_train_items 244224.
I0302 19:02:02.613142 22626471084160 run.py:483] Algo bellman_ford step 7632 current loss 0.615433, current_train_items 244256.
I0302 19:02:02.645291 22626471084160 run.py:483] Algo bellman_ford step 7633 current loss 0.694118, current_train_items 244288.
I0302 19:02:02.679258 22626471084160 run.py:483] Algo bellman_ford step 7634 current loss 0.626952, current_train_items 244320.
I0302 19:02:02.698911 22626471084160 run.py:483] Algo bellman_ford step 7635 current loss 0.252295, current_train_items 244352.
I0302 19:02:02.715028 22626471084160 run.py:483] Algo bellman_ford step 7636 current loss 0.455218, current_train_items 244384.
I0302 19:02:02.738924 22626471084160 run.py:483] Algo bellman_ford step 7637 current loss 0.572891, current_train_items 244416.
I0302 19:02:02.769354 22626471084160 run.py:483] Algo bellman_ford step 7638 current loss 0.583310, current_train_items 244448.
I0302 19:02:02.805354 22626471084160 run.py:483] Algo bellman_ford step 7639 current loss 0.666323, current_train_items 244480.
I0302 19:02:02.824847 22626471084160 run.py:483] Algo bellman_ford step 7640 current loss 0.284319, current_train_items 244512.
I0302 19:02:02.840822 22626471084160 run.py:483] Algo bellman_ford step 7641 current loss 0.519392, current_train_items 244544.
I0302 19:02:02.863885 22626471084160 run.py:483] Algo bellman_ford step 7642 current loss 0.522892, current_train_items 244576.
I0302 19:02:02.897568 22626471084160 run.py:483] Algo bellman_ford step 7643 current loss 0.722604, current_train_items 244608.
I0302 19:02:02.932924 22626471084160 run.py:483] Algo bellman_ford step 7644 current loss 0.745373, current_train_items 244640.
I0302 19:02:02.952345 22626471084160 run.py:483] Algo bellman_ford step 7645 current loss 0.235099, current_train_items 244672.
I0302 19:02:02.968617 22626471084160 run.py:483] Algo bellman_ford step 7646 current loss 0.516204, current_train_items 244704.
I0302 19:02:02.993148 22626471084160 run.py:483] Algo bellman_ford step 7647 current loss 0.612840, current_train_items 244736.
I0302 19:02:03.025899 22626471084160 run.py:483] Algo bellman_ford step 7648 current loss 0.743664, current_train_items 244768.
I0302 19:02:03.061213 22626471084160 run.py:483] Algo bellman_ford step 7649 current loss 0.958651, current_train_items 244800.
I0302 19:02:03.080841 22626471084160 run.py:483] Algo bellman_ford step 7650 current loss 0.345578, current_train_items 244832.
I0302 19:02:03.088809 22626471084160 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0302 19:02:03.088916 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:02:03.105383 22626471084160 run.py:483] Algo bellman_ford step 7651 current loss 0.432724, current_train_items 244864.
I0302 19:02:03.130547 22626471084160 run.py:483] Algo bellman_ford step 7652 current loss 0.625439, current_train_items 244896.
I0302 19:02:03.162293 22626471084160 run.py:483] Algo bellman_ford step 7653 current loss 0.581683, current_train_items 244928.
I0302 19:02:03.196165 22626471084160 run.py:483] Algo bellman_ford step 7654 current loss 0.833562, current_train_items 244960.
I0302 19:02:03.216302 22626471084160 run.py:483] Algo bellman_ford step 7655 current loss 0.217607, current_train_items 244992.
I0302 19:02:03.232152 22626471084160 run.py:483] Algo bellman_ford step 7656 current loss 0.544324, current_train_items 245024.
I0302 19:02:03.256348 22626471084160 run.py:483] Algo bellman_ford step 7657 current loss 0.714941, current_train_items 245056.
I0302 19:02:03.288348 22626471084160 run.py:483] Algo bellman_ford step 7658 current loss 0.691763, current_train_items 245088.
I0302 19:02:03.323145 22626471084160 run.py:483] Algo bellman_ford step 7659 current loss 1.082659, current_train_items 245120.
I0302 19:02:03.343286 22626471084160 run.py:483] Algo bellman_ford step 7660 current loss 0.301383, current_train_items 245152.
I0302 19:02:03.359513 22626471084160 run.py:483] Algo bellman_ford step 7661 current loss 0.400264, current_train_items 245184.
I0302 19:02:03.383104 22626471084160 run.py:483] Algo bellman_ford step 7662 current loss 0.559657, current_train_items 245216.
I0302 19:02:03.414333 22626471084160 run.py:483] Algo bellman_ford step 7663 current loss 0.632574, current_train_items 245248.
I0302 19:02:03.447998 22626471084160 run.py:483] Algo bellman_ford step 7664 current loss 0.724211, current_train_items 245280.
I0302 19:02:03.467625 22626471084160 run.py:483] Algo bellman_ford step 7665 current loss 0.327884, current_train_items 245312.
I0302 19:02:03.483823 22626471084160 run.py:483] Algo bellman_ford step 7666 current loss 0.453697, current_train_items 245344.
I0302 19:02:03.507173 22626471084160 run.py:483] Algo bellman_ford step 7667 current loss 0.640260, current_train_items 245376.
I0302 19:02:03.540781 22626471084160 run.py:483] Algo bellman_ford step 7668 current loss 0.668314, current_train_items 245408.
I0302 19:02:03.575266 22626471084160 run.py:483] Algo bellman_ford step 7669 current loss 0.732349, current_train_items 245440.
I0302 19:02:03.595264 22626471084160 run.py:483] Algo bellman_ford step 7670 current loss 0.285614, current_train_items 245472.
I0302 19:02:03.611773 22626471084160 run.py:483] Algo bellman_ford step 7671 current loss 0.435442, current_train_items 245504.
I0302 19:02:03.635351 22626471084160 run.py:483] Algo bellman_ford step 7672 current loss 0.603725, current_train_items 245536.
I0302 19:02:03.667159 22626471084160 run.py:483] Algo bellman_ford step 7673 current loss 0.731512, current_train_items 245568.
I0302 19:02:03.701532 22626471084160 run.py:483] Algo bellman_ford step 7674 current loss 0.739022, current_train_items 245600.
I0302 19:02:03.721620 22626471084160 run.py:483] Algo bellman_ford step 7675 current loss 0.265462, current_train_items 245632.
I0302 19:02:03.738325 22626471084160 run.py:483] Algo bellman_ford step 7676 current loss 0.491167, current_train_items 245664.
I0302 19:02:03.762581 22626471084160 run.py:483] Algo bellman_ford step 7677 current loss 0.554574, current_train_items 245696.
I0302 19:02:03.793609 22626471084160 run.py:483] Algo bellman_ford step 7678 current loss 0.544840, current_train_items 245728.
I0302 19:02:03.828168 22626471084160 run.py:483] Algo bellman_ford step 7679 current loss 0.735309, current_train_items 245760.
I0302 19:02:03.847828 22626471084160 run.py:483] Algo bellman_ford step 7680 current loss 0.284127, current_train_items 245792.
I0302 19:02:03.864393 22626471084160 run.py:483] Algo bellman_ford step 7681 current loss 0.582397, current_train_items 245824.
I0302 19:02:03.887794 22626471084160 run.py:483] Algo bellman_ford step 7682 current loss 0.567388, current_train_items 245856.
I0302 19:02:03.919389 22626471084160 run.py:483] Algo bellman_ford step 7683 current loss 0.672789, current_train_items 245888.
I0302 19:02:03.954220 22626471084160 run.py:483] Algo bellman_ford step 7684 current loss 0.789536, current_train_items 245920.
I0302 19:02:03.974458 22626471084160 run.py:483] Algo bellman_ford step 7685 current loss 0.245844, current_train_items 245952.
I0302 19:02:03.990927 22626471084160 run.py:483] Algo bellman_ford step 7686 current loss 0.501384, current_train_items 245984.
I0302 19:02:04.013692 22626471084160 run.py:483] Algo bellman_ford step 7687 current loss 0.581001, current_train_items 246016.
I0302 19:02:04.046367 22626471084160 run.py:483] Algo bellman_ford step 7688 current loss 0.586682, current_train_items 246048.
I0302 19:02:04.080587 22626471084160 run.py:483] Algo bellman_ford step 7689 current loss 0.689338, current_train_items 246080.
I0302 19:02:04.100447 22626471084160 run.py:483] Algo bellman_ford step 7690 current loss 0.293922, current_train_items 246112.
I0302 19:02:04.116795 22626471084160 run.py:483] Algo bellman_ford step 7691 current loss 0.453180, current_train_items 246144.
I0302 19:02:04.140110 22626471084160 run.py:483] Algo bellman_ford step 7692 current loss 0.608297, current_train_items 246176.
I0302 19:02:04.173124 22626471084160 run.py:483] Algo bellman_ford step 7693 current loss 0.658562, current_train_items 246208.
I0302 19:02:04.206175 22626471084160 run.py:483] Algo bellman_ford step 7694 current loss 0.696354, current_train_items 246240.
I0302 19:02:04.225774 22626471084160 run.py:483] Algo bellman_ford step 7695 current loss 0.278707, current_train_items 246272.
I0302 19:02:04.242412 22626471084160 run.py:483] Algo bellman_ford step 7696 current loss 0.456976, current_train_items 246304.
I0302 19:02:04.267056 22626471084160 run.py:483] Algo bellman_ford step 7697 current loss 0.556855, current_train_items 246336.
I0302 19:02:04.299649 22626471084160 run.py:483] Algo bellman_ford step 7698 current loss 0.658205, current_train_items 246368.
I0302 19:02:04.335345 22626471084160 run.py:483] Algo bellman_ford step 7699 current loss 0.754659, current_train_items 246400.
I0302 19:02:04.355647 22626471084160 run.py:483] Algo bellman_ford step 7700 current loss 0.346168, current_train_items 246432.
I0302 19:02:04.363435 22626471084160 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0302 19:02:04.363541 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:02:04.380793 22626471084160 run.py:483] Algo bellman_ford step 7701 current loss 0.438630, current_train_items 246464.
I0302 19:02:04.405112 22626471084160 run.py:483] Algo bellman_ford step 7702 current loss 0.525822, current_train_items 246496.
I0302 19:02:04.436095 22626471084160 run.py:483] Algo bellman_ford step 7703 current loss 0.565886, current_train_items 246528.
I0302 19:02:04.469465 22626471084160 run.py:483] Algo bellman_ford step 7704 current loss 0.708993, current_train_items 246560.
I0302 19:02:04.489517 22626471084160 run.py:483] Algo bellman_ford step 7705 current loss 0.251934, current_train_items 246592.
I0302 19:02:04.505394 22626471084160 run.py:483] Algo bellman_ford step 7706 current loss 0.413400, current_train_items 246624.
I0302 19:02:04.530547 22626471084160 run.py:483] Algo bellman_ford step 7707 current loss 0.609593, current_train_items 246656.
I0302 19:02:04.561413 22626471084160 run.py:483] Algo bellman_ford step 7708 current loss 0.575138, current_train_items 246688.
I0302 19:02:04.594232 22626471084160 run.py:483] Algo bellman_ford step 7709 current loss 0.709562, current_train_items 246720.
I0302 19:02:04.614417 22626471084160 run.py:483] Algo bellman_ford step 7710 current loss 0.362071, current_train_items 246752.
I0302 19:02:04.630768 22626471084160 run.py:483] Algo bellman_ford step 7711 current loss 0.421049, current_train_items 246784.
I0302 19:02:04.654568 22626471084160 run.py:483] Algo bellman_ford step 7712 current loss 0.602618, current_train_items 246816.
I0302 19:02:04.685463 22626471084160 run.py:483] Algo bellman_ford step 7713 current loss 0.717069, current_train_items 246848.
I0302 19:02:04.719079 22626471084160 run.py:483] Algo bellman_ford step 7714 current loss 0.683869, current_train_items 246880.
I0302 19:02:04.738756 22626471084160 run.py:483] Algo bellman_ford step 7715 current loss 0.344206, current_train_items 246912.
I0302 19:02:04.754830 22626471084160 run.py:483] Algo bellman_ford step 7716 current loss 0.424874, current_train_items 246944.
I0302 19:02:04.779083 22626471084160 run.py:483] Algo bellman_ford step 7717 current loss 0.600906, current_train_items 246976.
I0302 19:02:04.810474 22626471084160 run.py:483] Algo bellman_ford step 7718 current loss 0.644535, current_train_items 247008.
I0302 19:02:04.842607 22626471084160 run.py:483] Algo bellman_ford step 7719 current loss 0.681553, current_train_items 247040.
I0302 19:02:04.862416 22626471084160 run.py:483] Algo bellman_ford step 7720 current loss 0.323280, current_train_items 247072.
I0302 19:02:04.878489 22626471084160 run.py:483] Algo bellman_ford step 7721 current loss 0.433366, current_train_items 247104.
I0302 19:02:04.902858 22626471084160 run.py:483] Algo bellman_ford step 7722 current loss 0.596269, current_train_items 247136.
I0302 19:02:04.935273 22626471084160 run.py:483] Algo bellman_ford step 7723 current loss 0.628927, current_train_items 247168.
I0302 19:02:04.969715 22626471084160 run.py:483] Algo bellman_ford step 7724 current loss 0.685155, current_train_items 247200.
I0302 19:02:04.989358 22626471084160 run.py:483] Algo bellman_ford step 7725 current loss 0.281116, current_train_items 247232.
I0302 19:02:05.005720 22626471084160 run.py:483] Algo bellman_ford step 7726 current loss 0.447893, current_train_items 247264.
I0302 19:02:05.028226 22626471084160 run.py:483] Algo bellman_ford step 7727 current loss 0.622859, current_train_items 247296.
I0302 19:02:05.059770 22626471084160 run.py:483] Algo bellman_ford step 7728 current loss 0.655028, current_train_items 247328.
I0302 19:02:05.092656 22626471084160 run.py:483] Algo bellman_ford step 7729 current loss 0.665214, current_train_items 247360.
I0302 19:02:05.112449 22626471084160 run.py:483] Algo bellman_ford step 7730 current loss 0.282947, current_train_items 247392.
I0302 19:02:05.128377 22626471084160 run.py:483] Algo bellman_ford step 7731 current loss 0.479772, current_train_items 247424.
I0302 19:02:05.151897 22626471084160 run.py:483] Algo bellman_ford step 7732 current loss 0.583842, current_train_items 247456.
I0302 19:02:05.183354 22626471084160 run.py:483] Algo bellman_ford step 7733 current loss 0.681919, current_train_items 247488.
I0302 19:02:05.215774 22626471084160 run.py:483] Algo bellman_ford step 7734 current loss 0.662554, current_train_items 247520.
I0302 19:02:05.235300 22626471084160 run.py:483] Algo bellman_ford step 7735 current loss 0.262216, current_train_items 247552.
I0302 19:02:05.251575 22626471084160 run.py:483] Algo bellman_ford step 7736 current loss 0.567896, current_train_items 247584.
I0302 19:02:05.275637 22626471084160 run.py:483] Algo bellman_ford step 7737 current loss 0.566355, current_train_items 247616.
I0302 19:02:05.306491 22626471084160 run.py:483] Algo bellman_ford step 7738 current loss 0.745704, current_train_items 247648.
I0302 19:02:05.340345 22626471084160 run.py:483] Algo bellman_ford step 7739 current loss 0.768558, current_train_items 247680.
I0302 19:02:05.359700 22626471084160 run.py:483] Algo bellman_ford step 7740 current loss 0.221805, current_train_items 247712.
I0302 19:02:05.375959 22626471084160 run.py:483] Algo bellman_ford step 7741 current loss 0.464427, current_train_items 247744.
I0302 19:02:05.401077 22626471084160 run.py:483] Algo bellman_ford step 7742 current loss 0.804449, current_train_items 247776.
I0302 19:02:05.433393 22626471084160 run.py:483] Algo bellman_ford step 7743 current loss 0.818784, current_train_items 247808.
I0302 19:02:05.467173 22626471084160 run.py:483] Algo bellman_ford step 7744 current loss 0.910377, current_train_items 247840.
I0302 19:02:05.487051 22626471084160 run.py:483] Algo bellman_ford step 7745 current loss 0.316985, current_train_items 247872.
I0302 19:02:05.503093 22626471084160 run.py:483] Algo bellman_ford step 7746 current loss 0.382586, current_train_items 247904.
I0302 19:02:05.526923 22626471084160 run.py:483] Algo bellman_ford step 7747 current loss 0.638928, current_train_items 247936.
I0302 19:02:05.559725 22626471084160 run.py:483] Algo bellman_ford step 7748 current loss 0.735609, current_train_items 247968.
I0302 19:02:05.593948 22626471084160 run.py:483] Algo bellman_ford step 7749 current loss 0.714047, current_train_items 248000.
I0302 19:02:05.613929 22626471084160 run.py:483] Algo bellman_ford step 7750 current loss 0.326758, current_train_items 248032.
I0302 19:02:05.621976 22626471084160 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.953125, 'score': 0.953125, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0302 19:02:05.622080 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.953, val scores are: bellman_ford: 0.953
I0302 19:02:05.639225 22626471084160 run.py:483] Algo bellman_ford step 7751 current loss 0.536944, current_train_items 248064.
I0302 19:02:05.663550 22626471084160 run.py:483] Algo bellman_ford step 7752 current loss 0.632148, current_train_items 248096.
I0302 19:02:05.695214 22626471084160 run.py:483] Algo bellman_ford step 7753 current loss 0.607951, current_train_items 248128.
I0302 19:02:05.731240 22626471084160 run.py:483] Algo bellman_ford step 7754 current loss 0.766093, current_train_items 248160.
I0302 19:02:05.751701 22626471084160 run.py:483] Algo bellman_ford step 7755 current loss 0.298263, current_train_items 248192.
I0302 19:02:05.767271 22626471084160 run.py:483] Algo bellman_ford step 7756 current loss 0.466038, current_train_items 248224.
I0302 19:02:05.791466 22626471084160 run.py:483] Algo bellman_ford step 7757 current loss 0.589651, current_train_items 248256.
I0302 19:02:05.821886 22626471084160 run.py:483] Algo bellman_ford step 7758 current loss 0.662007, current_train_items 248288.
I0302 19:02:05.855365 22626471084160 run.py:483] Algo bellman_ford step 7759 current loss 0.755780, current_train_items 248320.
I0302 19:02:05.875298 22626471084160 run.py:483] Algo bellman_ford step 7760 current loss 0.314034, current_train_items 248352.
I0302 19:02:05.891577 22626471084160 run.py:483] Algo bellman_ford step 7761 current loss 0.555950, current_train_items 248384.
I0302 19:02:05.915609 22626471084160 run.py:483] Algo bellman_ford step 7762 current loss 0.563602, current_train_items 248416.
I0302 19:02:05.946944 22626471084160 run.py:483] Algo bellman_ford step 7763 current loss 0.701956, current_train_items 248448.
I0302 19:02:05.979542 22626471084160 run.py:483] Algo bellman_ford step 7764 current loss 0.690421, current_train_items 248480.
I0302 19:02:05.999143 22626471084160 run.py:483] Algo bellman_ford step 7765 current loss 0.349171, current_train_items 248512.
I0302 19:02:06.015545 22626471084160 run.py:483] Algo bellman_ford step 7766 current loss 0.455873, current_train_items 248544.
I0302 19:02:06.038875 22626471084160 run.py:483] Algo bellman_ford step 7767 current loss 0.513920, current_train_items 248576.
I0302 19:02:06.070472 22626471084160 run.py:483] Algo bellman_ford step 7768 current loss 0.527243, current_train_items 248608.
I0302 19:02:06.104168 22626471084160 run.py:483] Algo bellman_ford step 7769 current loss 0.697833, current_train_items 248640.
I0302 19:02:06.124200 22626471084160 run.py:483] Algo bellman_ford step 7770 current loss 0.364842, current_train_items 248672.
I0302 19:02:06.140535 22626471084160 run.py:483] Algo bellman_ford step 7771 current loss 0.435873, current_train_items 248704.
I0302 19:02:06.164095 22626471084160 run.py:483] Algo bellman_ford step 7772 current loss 0.571664, current_train_items 248736.
I0302 19:02:06.196133 22626471084160 run.py:483] Algo bellman_ford step 7773 current loss 0.676852, current_train_items 248768.
I0302 19:02:06.229828 22626471084160 run.py:483] Algo bellman_ford step 7774 current loss 0.712635, current_train_items 248800.
I0302 19:02:06.249917 22626471084160 run.py:483] Algo bellman_ford step 7775 current loss 0.283955, current_train_items 248832.
I0302 19:02:06.266368 22626471084160 run.py:483] Algo bellman_ford step 7776 current loss 0.593993, current_train_items 248864.
I0302 19:02:06.290906 22626471084160 run.py:483] Algo bellman_ford step 7777 current loss 0.643444, current_train_items 248896.
I0302 19:02:06.322989 22626471084160 run.py:483] Algo bellman_ford step 7778 current loss 0.626819, current_train_items 248928.
I0302 19:02:06.358666 22626471084160 run.py:483] Algo bellman_ford step 7779 current loss 0.798077, current_train_items 248960.
I0302 19:02:06.378273 22626471084160 run.py:483] Algo bellman_ford step 7780 current loss 0.193243, current_train_items 248992.
I0302 19:02:06.394757 22626471084160 run.py:483] Algo bellman_ford step 7781 current loss 0.528162, current_train_items 249024.
I0302 19:02:06.418689 22626471084160 run.py:483] Algo bellman_ford step 7782 current loss 0.633811, current_train_items 249056.
I0302 19:02:06.450949 22626471084160 run.py:483] Algo bellman_ford step 7783 current loss 0.686941, current_train_items 249088.
I0302 19:02:06.485293 22626471084160 run.py:483] Algo bellman_ford step 7784 current loss 0.724259, current_train_items 249120.
I0302 19:02:06.505380 22626471084160 run.py:483] Algo bellman_ford step 7785 current loss 0.301678, current_train_items 249152.
I0302 19:02:06.521684 22626471084160 run.py:483] Algo bellman_ford step 7786 current loss 0.493517, current_train_items 249184.
I0302 19:02:06.544845 22626471084160 run.py:483] Algo bellman_ford step 7787 current loss 0.498145, current_train_items 249216.
I0302 19:02:06.576105 22626471084160 run.py:483] Algo bellman_ford step 7788 current loss 0.588198, current_train_items 249248.
I0302 19:02:06.607768 22626471084160 run.py:483] Algo bellman_ford step 7789 current loss 1.036088, current_train_items 249280.
I0302 19:02:06.627701 22626471084160 run.py:483] Algo bellman_ford step 7790 current loss 0.315098, current_train_items 249312.
I0302 19:02:06.643518 22626471084160 run.py:483] Algo bellman_ford step 7791 current loss 0.501045, current_train_items 249344.
I0302 19:02:06.667263 22626471084160 run.py:483] Algo bellman_ford step 7792 current loss 0.601398, current_train_items 249376.
I0302 19:02:06.697999 22626471084160 run.py:483] Algo bellman_ford step 7793 current loss 0.613993, current_train_items 249408.
I0302 19:02:06.731183 22626471084160 run.py:483] Algo bellman_ford step 7794 current loss 0.674491, current_train_items 249440.
I0302 19:02:06.750644 22626471084160 run.py:483] Algo bellman_ford step 7795 current loss 0.322501, current_train_items 249472.
I0302 19:02:06.766530 22626471084160 run.py:483] Algo bellman_ford step 7796 current loss 0.557379, current_train_items 249504.
I0302 19:02:06.789652 22626471084160 run.py:483] Algo bellman_ford step 7797 current loss 0.556551, current_train_items 249536.
I0302 19:02:06.820392 22626471084160 run.py:483] Algo bellman_ford step 7798 current loss 0.530912, current_train_items 249568.
I0302 19:02:06.855220 22626471084160 run.py:483] Algo bellman_ford step 7799 current loss 0.692056, current_train_items 249600.
I0302 19:02:06.875150 22626471084160 run.py:483] Algo bellman_ford step 7800 current loss 0.338215, current_train_items 249632.
I0302 19:02:06.883098 22626471084160 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0302 19:02:06.883216 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:02:06.900195 22626471084160 run.py:483] Algo bellman_ford step 7801 current loss 0.505698, current_train_items 249664.
I0302 19:02:06.924608 22626471084160 run.py:483] Algo bellman_ford step 7802 current loss 0.611166, current_train_items 249696.
I0302 19:02:06.957637 22626471084160 run.py:483] Algo bellman_ford step 7803 current loss 0.676035, current_train_items 249728.
I0302 19:02:06.993809 22626471084160 run.py:483] Algo bellman_ford step 7804 current loss 0.792572, current_train_items 249760.
I0302 19:02:07.014076 22626471084160 run.py:483] Algo bellman_ford step 7805 current loss 0.260017, current_train_items 249792.
I0302 19:02:07.030294 22626471084160 run.py:483] Algo bellman_ford step 7806 current loss 0.456442, current_train_items 249824.
I0302 19:02:07.053270 22626471084160 run.py:483] Algo bellman_ford step 7807 current loss 0.526716, current_train_items 249856.
I0302 19:02:07.083870 22626471084160 run.py:483] Algo bellman_ford step 7808 current loss 0.564679, current_train_items 249888.
I0302 19:02:07.117658 22626471084160 run.py:483] Algo bellman_ford step 7809 current loss 0.668287, current_train_items 249920.
I0302 19:02:07.137433 22626471084160 run.py:483] Algo bellman_ford step 7810 current loss 0.329176, current_train_items 249952.
I0302 19:02:07.153962 22626471084160 run.py:483] Algo bellman_ford step 7811 current loss 0.482443, current_train_items 249984.
I0302 19:02:07.178357 22626471084160 run.py:483] Algo bellman_ford step 7812 current loss 0.585744, current_train_items 250016.
I0302 19:02:07.211128 22626471084160 run.py:483] Algo bellman_ford step 7813 current loss 0.698604, current_train_items 250048.
I0302 19:02:07.244796 22626471084160 run.py:483] Algo bellman_ford step 7814 current loss 0.737190, current_train_items 250080.
I0302 19:02:07.264288 22626471084160 run.py:483] Algo bellman_ford step 7815 current loss 0.308443, current_train_items 250112.
I0302 19:02:07.280246 22626471084160 run.py:483] Algo bellman_ford step 7816 current loss 0.451103, current_train_items 250144.
I0302 19:02:07.304638 22626471084160 run.py:483] Algo bellman_ford step 7817 current loss 0.556612, current_train_items 250176.
I0302 19:02:07.335027 22626471084160 run.py:483] Algo bellman_ford step 7818 current loss 0.589400, current_train_items 250208.
I0302 19:02:07.367810 22626471084160 run.py:483] Algo bellman_ford step 7819 current loss 0.672992, current_train_items 250240.
I0302 19:02:07.387544 22626471084160 run.py:483] Algo bellman_ford step 7820 current loss 0.349575, current_train_items 250272.
I0302 19:02:07.404069 22626471084160 run.py:483] Algo bellman_ford step 7821 current loss 0.478940, current_train_items 250304.
I0302 19:02:07.429124 22626471084160 run.py:483] Algo bellman_ford step 7822 current loss 0.527102, current_train_items 250336.
I0302 19:02:07.462358 22626471084160 run.py:483] Algo bellman_ford step 7823 current loss 0.688618, current_train_items 250368.
I0302 19:02:07.495091 22626471084160 run.py:483] Algo bellman_ford step 7824 current loss 0.803540, current_train_items 250400.
I0302 19:02:07.514831 22626471084160 run.py:483] Algo bellman_ford step 7825 current loss 0.257754, current_train_items 250432.
I0302 19:02:07.531363 22626471084160 run.py:483] Algo bellman_ford step 7826 current loss 0.503986, current_train_items 250464.
I0302 19:02:07.555634 22626471084160 run.py:483] Algo bellman_ford step 7827 current loss 0.685809, current_train_items 250496.
I0302 19:02:07.587382 22626471084160 run.py:483] Algo bellman_ford step 7828 current loss 0.706695, current_train_items 250528.
I0302 19:02:07.621325 22626471084160 run.py:483] Algo bellman_ford step 7829 current loss 0.800177, current_train_items 250560.
I0302 19:02:07.641000 22626471084160 run.py:483] Algo bellman_ford step 7830 current loss 0.224287, current_train_items 250592.
I0302 19:02:07.656931 22626471084160 run.py:483] Algo bellman_ford step 7831 current loss 0.491323, current_train_items 250624.
I0302 19:02:07.681020 22626471084160 run.py:483] Algo bellman_ford step 7832 current loss 0.701033, current_train_items 250656.
I0302 19:02:07.713352 22626471084160 run.py:483] Algo bellman_ford step 7833 current loss 0.722895, current_train_items 250688.
I0302 19:02:07.745255 22626471084160 run.py:483] Algo bellman_ford step 7834 current loss 0.903732, current_train_items 250720.
I0302 19:02:07.765071 22626471084160 run.py:483] Algo bellman_ford step 7835 current loss 0.311717, current_train_items 250752.
I0302 19:02:07.781459 22626471084160 run.py:483] Algo bellman_ford step 7836 current loss 0.437024, current_train_items 250784.
I0302 19:02:07.805466 22626471084160 run.py:483] Algo bellman_ford step 7837 current loss 0.554956, current_train_items 250816.
I0302 19:02:07.838377 22626471084160 run.py:483] Algo bellman_ford step 7838 current loss 0.683721, current_train_items 250848.
I0302 19:02:07.873704 22626471084160 run.py:483] Algo bellman_ford step 7839 current loss 0.875848, current_train_items 250880.
I0302 19:02:07.893435 22626471084160 run.py:483] Algo bellman_ford step 7840 current loss 0.272086, current_train_items 250912.
I0302 19:02:07.909690 22626471084160 run.py:483] Algo bellman_ford step 7841 current loss 0.523977, current_train_items 250944.
I0302 19:02:07.933738 22626471084160 run.py:483] Algo bellman_ford step 7842 current loss 0.651915, current_train_items 250976.
I0302 19:02:07.966747 22626471084160 run.py:483] Algo bellman_ford step 7843 current loss 0.653874, current_train_items 251008.
I0302 19:02:08.002029 22626471084160 run.py:483] Algo bellman_ford step 7844 current loss 0.767309, current_train_items 251040.
I0302 19:02:08.021725 22626471084160 run.py:483] Algo bellman_ford step 7845 current loss 0.414963, current_train_items 251072.
I0302 19:02:08.037857 22626471084160 run.py:483] Algo bellman_ford step 7846 current loss 0.473532, current_train_items 251104.
I0302 19:02:08.062207 22626471084160 run.py:483] Algo bellman_ford step 7847 current loss 0.604554, current_train_items 251136.
I0302 19:02:08.094419 22626471084160 run.py:483] Algo bellman_ford step 7848 current loss 0.700712, current_train_items 251168.
I0302 19:02:08.128589 22626471084160 run.py:483] Algo bellman_ford step 7849 current loss 0.814698, current_train_items 251200.
I0302 19:02:08.148210 22626471084160 run.py:483] Algo bellman_ford step 7850 current loss 0.262106, current_train_items 251232.
I0302 19:02:08.156385 22626471084160 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0302 19:02:08.156489 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 19:02:08.173329 22626471084160 run.py:483] Algo bellman_ford step 7851 current loss 0.466848, current_train_items 251264.
I0302 19:02:08.197076 22626471084160 run.py:483] Algo bellman_ford step 7852 current loss 0.542510, current_train_items 251296.
I0302 19:02:08.230119 22626471084160 run.py:483] Algo bellman_ford step 7853 current loss 0.672274, current_train_items 251328.
I0302 19:02:08.266723 22626471084160 run.py:483] Algo bellman_ford step 7854 current loss 0.749270, current_train_items 251360.
I0302 19:02:08.286510 22626471084160 run.py:483] Algo bellman_ford step 7855 current loss 0.351372, current_train_items 251392.
I0302 19:02:08.302887 22626471084160 run.py:483] Algo bellman_ford step 7856 current loss 0.630491, current_train_items 251424.
I0302 19:02:08.327105 22626471084160 run.py:483] Algo bellman_ford step 7857 current loss 0.592782, current_train_items 251456.
I0302 19:02:08.358744 22626471084160 run.py:483] Algo bellman_ford step 7858 current loss 0.591727, current_train_items 251488.
I0302 19:02:08.393932 22626471084160 run.py:483] Algo bellman_ford step 7859 current loss 0.793580, current_train_items 251520.
I0302 19:02:08.413894 22626471084160 run.py:483] Algo bellman_ford step 7860 current loss 0.290311, current_train_items 251552.
I0302 19:02:08.429907 22626471084160 run.py:483] Algo bellman_ford step 7861 current loss 0.391653, current_train_items 251584.
I0302 19:02:08.453865 22626471084160 run.py:483] Algo bellman_ford step 7862 current loss 0.645975, current_train_items 251616.
I0302 19:02:08.485531 22626471084160 run.py:483] Algo bellman_ford step 7863 current loss 0.652199, current_train_items 251648.
I0302 19:02:08.517816 22626471084160 run.py:483] Algo bellman_ford step 7864 current loss 0.699078, current_train_items 251680.
I0302 19:02:08.537727 22626471084160 run.py:483] Algo bellman_ford step 7865 current loss 0.305675, current_train_items 251712.
I0302 19:02:08.554298 22626471084160 run.py:483] Algo bellman_ford step 7866 current loss 0.619751, current_train_items 251744.
I0302 19:02:08.578792 22626471084160 run.py:483] Algo bellman_ford step 7867 current loss 0.565266, current_train_items 251776.
I0302 19:02:08.609780 22626471084160 run.py:483] Algo bellman_ford step 7868 current loss 0.757159, current_train_items 251808.
I0302 19:02:08.642462 22626471084160 run.py:483] Algo bellman_ford step 7869 current loss 0.744239, current_train_items 251840.
I0302 19:02:08.662428 22626471084160 run.py:483] Algo bellman_ford step 7870 current loss 0.314080, current_train_items 251872.
I0302 19:02:08.678791 22626471084160 run.py:483] Algo bellman_ford step 7871 current loss 0.443284, current_train_items 251904.
I0302 19:02:08.702558 22626471084160 run.py:483] Algo bellman_ford step 7872 current loss 0.642580, current_train_items 251936.
I0302 19:02:08.733467 22626471084160 run.py:483] Algo bellman_ford step 7873 current loss 0.643682, current_train_items 251968.
I0302 19:02:08.767027 22626471084160 run.py:483] Algo bellman_ford step 7874 current loss 0.710890, current_train_items 252000.
I0302 19:02:08.787375 22626471084160 run.py:483] Algo bellman_ford step 7875 current loss 0.354303, current_train_items 252032.
I0302 19:02:08.803147 22626471084160 run.py:483] Algo bellman_ford step 7876 current loss 0.379797, current_train_items 252064.
I0302 19:02:08.826436 22626471084160 run.py:483] Algo bellman_ford step 7877 current loss 0.612654, current_train_items 252096.
I0302 19:02:08.858030 22626471084160 run.py:483] Algo bellman_ford step 7878 current loss 0.668110, current_train_items 252128.
I0302 19:02:08.891937 22626471084160 run.py:483] Algo bellman_ford step 7879 current loss 0.802386, current_train_items 252160.
I0302 19:02:08.911477 22626471084160 run.py:483] Algo bellman_ford step 7880 current loss 0.252735, current_train_items 252192.
I0302 19:02:08.927304 22626471084160 run.py:483] Algo bellman_ford step 7881 current loss 0.397738, current_train_items 252224.
I0302 19:02:08.951017 22626471084160 run.py:483] Algo bellman_ford step 7882 current loss 0.580464, current_train_items 252256.
I0302 19:02:08.983012 22626471084160 run.py:483] Algo bellman_ford step 7883 current loss 0.669039, current_train_items 252288.
I0302 19:02:09.017245 22626471084160 run.py:483] Algo bellman_ford step 7884 current loss 0.918604, current_train_items 252320.
I0302 19:02:09.037295 22626471084160 run.py:483] Algo bellman_ford step 7885 current loss 0.325056, current_train_items 252352.
I0302 19:02:09.053576 22626471084160 run.py:483] Algo bellman_ford step 7886 current loss 0.458410, current_train_items 252384.
I0302 19:02:09.076163 22626471084160 run.py:483] Algo bellman_ford step 7887 current loss 0.491956, current_train_items 252416.
I0302 19:02:09.105934 22626471084160 run.py:483] Algo bellman_ford step 7888 current loss 0.599669, current_train_items 252448.
I0302 19:02:09.139569 22626471084160 run.py:483] Algo bellman_ford step 7889 current loss 0.702702, current_train_items 252480.
I0302 19:02:09.159531 22626471084160 run.py:483] Algo bellman_ford step 7890 current loss 0.258906, current_train_items 252512.
I0302 19:02:09.175543 22626471084160 run.py:483] Algo bellman_ford step 7891 current loss 0.462145, current_train_items 252544.
I0302 19:02:09.199367 22626471084160 run.py:483] Algo bellman_ford step 7892 current loss 0.566425, current_train_items 252576.
I0302 19:02:09.231566 22626471084160 run.py:483] Algo bellman_ford step 7893 current loss 0.784054, current_train_items 252608.
I0302 19:02:09.264658 22626471084160 run.py:483] Algo bellman_ford step 7894 current loss 0.784016, current_train_items 252640.
I0302 19:02:09.284137 22626471084160 run.py:483] Algo bellman_ford step 7895 current loss 0.255468, current_train_items 252672.
I0302 19:02:09.299997 22626471084160 run.py:483] Algo bellman_ford step 7896 current loss 0.437684, current_train_items 252704.
I0302 19:02:09.324859 22626471084160 run.py:483] Algo bellman_ford step 7897 current loss 0.600485, current_train_items 252736.
I0302 19:02:09.357160 22626471084160 run.py:483] Algo bellman_ford step 7898 current loss 0.619791, current_train_items 252768.
I0302 19:02:09.391149 22626471084160 run.py:483] Algo bellman_ford step 7899 current loss 0.837488, current_train_items 252800.
I0302 19:02:09.410993 22626471084160 run.py:483] Algo bellman_ford step 7900 current loss 0.313409, current_train_items 252832.
I0302 19:02:09.418671 22626471084160 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0302 19:02:09.418777 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:02:09.435541 22626471084160 run.py:483] Algo bellman_ford step 7901 current loss 0.428356, current_train_items 252864.
I0302 19:02:09.460149 22626471084160 run.py:483] Algo bellman_ford step 7902 current loss 0.565875, current_train_items 252896.
I0302 19:02:09.492736 22626471084160 run.py:483] Algo bellman_ford step 7903 current loss 0.733410, current_train_items 252928.
I0302 19:02:09.529177 22626471084160 run.py:483] Algo bellman_ford step 7904 current loss 0.783532, current_train_items 252960.
I0302 19:02:09.549183 22626471084160 run.py:483] Algo bellman_ford step 7905 current loss 0.295002, current_train_items 252992.
I0302 19:02:09.565681 22626471084160 run.py:483] Algo bellman_ford step 7906 current loss 0.533910, current_train_items 253024.
I0302 19:02:09.590497 22626471084160 run.py:483] Algo bellman_ford step 7907 current loss 0.560036, current_train_items 253056.
I0302 19:02:09.621556 22626471084160 run.py:483] Algo bellman_ford step 7908 current loss 0.608151, current_train_items 253088.
I0302 19:02:09.652760 22626471084160 run.py:483] Algo bellman_ford step 7909 current loss 0.673845, current_train_items 253120.
I0302 19:02:09.672396 22626471084160 run.py:483] Algo bellman_ford step 7910 current loss 0.298074, current_train_items 253152.
I0302 19:02:09.689173 22626471084160 run.py:483] Algo bellman_ford step 7911 current loss 0.489797, current_train_items 253184.
I0302 19:02:09.713845 22626471084160 run.py:483] Algo bellman_ford step 7912 current loss 0.723108, current_train_items 253216.
I0302 19:02:09.745569 22626471084160 run.py:483] Algo bellman_ford step 7913 current loss 0.608438, current_train_items 253248.
I0302 19:02:09.779569 22626471084160 run.py:483] Algo bellman_ford step 7914 current loss 0.679990, current_train_items 253280.
I0302 19:02:09.799386 22626471084160 run.py:483] Algo bellman_ford step 7915 current loss 0.250985, current_train_items 253312.
I0302 19:02:09.815860 22626471084160 run.py:483] Algo bellman_ford step 7916 current loss 0.594315, current_train_items 253344.
I0302 19:02:09.839580 22626471084160 run.py:483] Algo bellman_ford step 7917 current loss 0.642217, current_train_items 253376.
I0302 19:02:09.871319 22626471084160 run.py:483] Algo bellman_ford step 7918 current loss 0.601711, current_train_items 253408.
I0302 19:02:09.906926 22626471084160 run.py:483] Algo bellman_ford step 7919 current loss 0.682570, current_train_items 253440.
I0302 19:02:09.926530 22626471084160 run.py:483] Algo bellman_ford step 7920 current loss 0.315214, current_train_items 253472.
I0302 19:02:09.942936 22626471084160 run.py:483] Algo bellman_ford step 7921 current loss 0.492287, current_train_items 253504.
I0302 19:02:09.966990 22626471084160 run.py:483] Algo bellman_ford step 7922 current loss 0.634225, current_train_items 253536.
I0302 19:02:09.997979 22626471084160 run.py:483] Algo bellman_ford step 7923 current loss 0.620348, current_train_items 253568.
I0302 19:02:10.032043 22626471084160 run.py:483] Algo bellman_ford step 7924 current loss 0.765071, current_train_items 253600.
I0302 19:02:10.051455 22626471084160 run.py:483] Algo bellman_ford step 7925 current loss 0.306408, current_train_items 253632.
I0302 19:02:10.067755 22626471084160 run.py:483] Algo bellman_ford step 7926 current loss 0.473075, current_train_items 253664.
I0302 19:02:10.091679 22626471084160 run.py:483] Algo bellman_ford step 7927 current loss 0.639232, current_train_items 253696.
I0302 19:02:10.124083 22626471084160 run.py:483] Algo bellman_ford step 7928 current loss 0.800628, current_train_items 253728.
I0302 19:02:10.157072 22626471084160 run.py:483] Algo bellman_ford step 7929 current loss 0.836895, current_train_items 253760.
I0302 19:02:10.176839 22626471084160 run.py:483] Algo bellman_ford step 7930 current loss 0.342054, current_train_items 253792.
I0302 19:02:10.193202 22626471084160 run.py:483] Algo bellman_ford step 7931 current loss 0.436216, current_train_items 253824.
I0302 19:02:10.216183 22626471084160 run.py:483] Algo bellman_ford step 7932 current loss 0.493646, current_train_items 253856.
I0302 19:02:10.248471 22626471084160 run.py:483] Algo bellman_ford step 7933 current loss 0.709263, current_train_items 253888.
I0302 19:02:10.282826 22626471084160 run.py:483] Algo bellman_ford step 7934 current loss 0.753574, current_train_items 253920.
I0302 19:02:10.302269 22626471084160 run.py:483] Algo bellman_ford step 7935 current loss 0.309411, current_train_items 253952.
I0302 19:02:10.318212 22626471084160 run.py:483] Algo bellman_ford step 7936 current loss 0.407273, current_train_items 253984.
I0302 19:02:10.342900 22626471084160 run.py:483] Algo bellman_ford step 7937 current loss 0.478123, current_train_items 254016.
I0302 19:02:10.373950 22626471084160 run.py:483] Algo bellman_ford step 7938 current loss 0.603609, current_train_items 254048.
I0302 19:02:10.408570 22626471084160 run.py:483] Algo bellman_ford step 7939 current loss 0.780011, current_train_items 254080.
I0302 19:02:10.428316 22626471084160 run.py:483] Algo bellman_ford step 7940 current loss 0.298477, current_train_items 254112.
I0302 19:02:10.444038 22626471084160 run.py:483] Algo bellman_ford step 7941 current loss 0.451320, current_train_items 254144.
I0302 19:02:10.468578 22626471084160 run.py:483] Algo bellman_ford step 7942 current loss 0.560563, current_train_items 254176.
I0302 19:02:10.501557 22626471084160 run.py:483] Algo bellman_ford step 7943 current loss 0.744523, current_train_items 254208.
I0302 19:02:10.535061 22626471084160 run.py:483] Algo bellman_ford step 7944 current loss 0.769729, current_train_items 254240.
I0302 19:02:10.554650 22626471084160 run.py:483] Algo bellman_ford step 7945 current loss 0.310281, current_train_items 254272.
I0302 19:02:10.570411 22626471084160 run.py:483] Algo bellman_ford step 7946 current loss 0.439443, current_train_items 254304.
I0302 19:02:10.593665 22626471084160 run.py:483] Algo bellman_ford step 7947 current loss 0.551671, current_train_items 254336.
I0302 19:02:10.623714 22626471084160 run.py:483] Algo bellman_ford step 7948 current loss 0.634747, current_train_items 254368.
I0302 19:02:10.658365 22626471084160 run.py:483] Algo bellman_ford step 7949 current loss 0.779191, current_train_items 254400.
I0302 19:02:10.677839 22626471084160 run.py:483] Algo bellman_ford step 7950 current loss 0.291709, current_train_items 254432.
I0302 19:02:10.685996 22626471084160 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0302 19:02:10.686101 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:02:10.703031 22626471084160 run.py:483] Algo bellman_ford step 7951 current loss 0.458579, current_train_items 254464.
I0302 19:02:10.727468 22626471084160 run.py:483] Algo bellman_ford step 7952 current loss 0.532336, current_train_items 254496.
I0302 19:02:10.760224 22626471084160 run.py:483] Algo bellman_ford step 7953 current loss 0.665725, current_train_items 254528.
I0302 19:02:10.793962 22626471084160 run.py:483] Algo bellman_ford step 7954 current loss 0.646734, current_train_items 254560.
I0302 19:02:10.814263 22626471084160 run.py:483] Algo bellman_ford step 7955 current loss 0.341293, current_train_items 254592.
I0302 19:02:10.830450 22626471084160 run.py:483] Algo bellman_ford step 7956 current loss 0.544100, current_train_items 254624.
I0302 19:02:10.854567 22626471084160 run.py:483] Algo bellman_ford step 7957 current loss 0.522607, current_train_items 254656.
I0302 19:02:10.886366 22626471084160 run.py:483] Algo bellman_ford step 7958 current loss 0.656509, current_train_items 254688.
I0302 19:02:10.919837 22626471084160 run.py:483] Algo bellman_ford step 7959 current loss 0.639513, current_train_items 254720.
I0302 19:02:10.939947 22626471084160 run.py:483] Algo bellman_ford step 7960 current loss 0.263046, current_train_items 254752.
I0302 19:02:10.956619 22626471084160 run.py:483] Algo bellman_ford step 7961 current loss 0.432406, current_train_items 254784.
I0302 19:02:10.980554 22626471084160 run.py:483] Algo bellman_ford step 7962 current loss 0.588292, current_train_items 254816.
I0302 19:02:11.011531 22626471084160 run.py:483] Algo bellman_ford step 7963 current loss 0.639181, current_train_items 254848.
I0302 19:02:11.046116 22626471084160 run.py:483] Algo bellman_ford step 7964 current loss 0.750657, current_train_items 254880.
I0302 19:02:11.065850 22626471084160 run.py:483] Algo bellman_ford step 7965 current loss 0.312293, current_train_items 254912.
I0302 19:02:11.082408 22626471084160 run.py:483] Algo bellman_ford step 7966 current loss 0.513256, current_train_items 254944.
I0302 19:02:11.107614 22626471084160 run.py:483] Algo bellman_ford step 7967 current loss 0.662233, current_train_items 254976.
I0302 19:02:11.138779 22626471084160 run.py:483] Algo bellman_ford step 7968 current loss 0.570112, current_train_items 255008.
I0302 19:02:11.171349 22626471084160 run.py:483] Algo bellman_ford step 7969 current loss 0.747991, current_train_items 255040.
I0302 19:02:11.191138 22626471084160 run.py:483] Algo bellman_ford step 7970 current loss 0.374784, current_train_items 255072.
I0302 19:02:11.207377 22626471084160 run.py:483] Algo bellman_ford step 7971 current loss 0.554837, current_train_items 255104.
I0302 19:02:11.231696 22626471084160 run.py:483] Algo bellman_ford step 7972 current loss 0.624948, current_train_items 255136.
I0302 19:02:11.262619 22626471084160 run.py:483] Algo bellman_ford step 7973 current loss 0.575327, current_train_items 255168.
I0302 19:02:11.297836 22626471084160 run.py:483] Algo bellman_ford step 7974 current loss 0.746235, current_train_items 255200.
I0302 19:02:11.317761 22626471084160 run.py:483] Algo bellman_ford step 7975 current loss 0.294465, current_train_items 255232.
I0302 19:02:11.334139 22626471084160 run.py:483] Algo bellman_ford step 7976 current loss 0.431289, current_train_items 255264.
I0302 19:02:11.358302 22626471084160 run.py:483] Algo bellman_ford step 7977 current loss 0.579820, current_train_items 255296.
I0302 19:02:11.390264 22626471084160 run.py:483] Algo bellman_ford step 7978 current loss 0.663609, current_train_items 255328.
I0302 19:02:11.422271 22626471084160 run.py:483] Algo bellman_ford step 7979 current loss 0.687040, current_train_items 255360.
I0302 19:02:11.442218 22626471084160 run.py:483] Algo bellman_ford step 7980 current loss 0.277333, current_train_items 255392.
I0302 19:02:11.458251 22626471084160 run.py:483] Algo bellman_ford step 7981 current loss 0.486519, current_train_items 255424.
I0302 19:02:11.481585 22626471084160 run.py:483] Algo bellman_ford step 7982 current loss 0.554366, current_train_items 255456.
I0302 19:02:11.512943 22626471084160 run.py:483] Algo bellman_ford step 7983 current loss 0.647301, current_train_items 255488.
I0302 19:02:11.547235 22626471084160 run.py:483] Algo bellman_ford step 7984 current loss 0.825845, current_train_items 255520.
I0302 19:02:11.567053 22626471084160 run.py:483] Algo bellman_ford step 7985 current loss 0.257751, current_train_items 255552.
I0302 19:02:11.583522 22626471084160 run.py:483] Algo bellman_ford step 7986 current loss 0.478878, current_train_items 255584.
I0302 19:02:11.606330 22626471084160 run.py:483] Algo bellman_ford step 7987 current loss 0.503913, current_train_items 255616.
I0302 19:02:11.638400 22626471084160 run.py:483] Algo bellman_ford step 7988 current loss 0.597182, current_train_items 255648.
I0302 19:02:11.671174 22626471084160 run.py:483] Algo bellman_ford step 7989 current loss 0.931669, current_train_items 255680.
I0302 19:02:11.691196 22626471084160 run.py:483] Algo bellman_ford step 7990 current loss 0.343675, current_train_items 255712.
I0302 19:02:11.707521 22626471084160 run.py:483] Algo bellman_ford step 7991 current loss 0.554690, current_train_items 255744.
I0302 19:02:11.731301 22626471084160 run.py:483] Algo bellman_ford step 7992 current loss 0.597828, current_train_items 255776.
I0302 19:02:11.764597 22626471084160 run.py:483] Algo bellman_ford step 7993 current loss 0.736287, current_train_items 255808.
I0302 19:02:11.799387 22626471084160 run.py:483] Algo bellman_ford step 7994 current loss 0.756911, current_train_items 255840.
I0302 19:02:11.818929 22626471084160 run.py:483] Algo bellman_ford step 7995 current loss 0.335931, current_train_items 255872.
I0302 19:02:11.834696 22626471084160 run.py:483] Algo bellman_ford step 7996 current loss 0.389835, current_train_items 255904.
I0302 19:02:11.858307 22626471084160 run.py:483] Algo bellman_ford step 7997 current loss 0.595120, current_train_items 255936.
I0302 19:02:11.889824 22626471084160 run.py:483] Algo bellman_ford step 7998 current loss 0.579847, current_train_items 255968.
I0302 19:02:11.921504 22626471084160 run.py:483] Algo bellman_ford step 7999 current loss 0.753026, current_train_items 256000.
I0302 19:02:11.941476 22626471084160 run.py:483] Algo bellman_ford step 8000 current loss 0.342062, current_train_items 256032.
I0302 19:02:11.949301 22626471084160 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0302 19:02:11.949407 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:02:11.966482 22626471084160 run.py:483] Algo bellman_ford step 8001 current loss 0.464862, current_train_items 256064.
I0302 19:02:11.990051 22626471084160 run.py:483] Algo bellman_ford step 8002 current loss 0.580901, current_train_items 256096.
I0302 19:02:12.022945 22626471084160 run.py:483] Algo bellman_ford step 8003 current loss 0.623603, current_train_items 256128.
I0302 19:02:12.056072 22626471084160 run.py:483] Algo bellman_ford step 8004 current loss 0.565411, current_train_items 256160.
I0302 19:02:12.076377 22626471084160 run.py:483] Algo bellman_ford step 8005 current loss 0.241947, current_train_items 256192.
I0302 19:02:12.092463 22626471084160 run.py:483] Algo bellman_ford step 8006 current loss 0.423648, current_train_items 256224.
I0302 19:02:12.116666 22626471084160 run.py:483] Algo bellman_ford step 8007 current loss 0.690577, current_train_items 256256.
I0302 19:02:12.149368 22626471084160 run.py:483] Algo bellman_ford step 8008 current loss 0.670617, current_train_items 256288.
I0302 19:02:12.183357 22626471084160 run.py:483] Algo bellman_ford step 8009 current loss 0.742730, current_train_items 256320.
I0302 19:02:12.202976 22626471084160 run.py:483] Algo bellman_ford step 8010 current loss 0.280327, current_train_items 256352.
I0302 19:02:12.219006 22626471084160 run.py:483] Algo bellman_ford step 8011 current loss 0.469225, current_train_items 256384.
I0302 19:02:12.243579 22626471084160 run.py:483] Algo bellman_ford step 8012 current loss 0.569588, current_train_items 256416.
I0302 19:02:12.274654 22626471084160 run.py:483] Algo bellman_ford step 8013 current loss 0.745329, current_train_items 256448.
I0302 19:02:12.306199 22626471084160 run.py:483] Algo bellman_ford step 8014 current loss 0.682706, current_train_items 256480.
I0302 19:02:12.326009 22626471084160 run.py:483] Algo bellman_ford step 8015 current loss 0.351422, current_train_items 256512.
I0302 19:02:12.342789 22626471084160 run.py:483] Algo bellman_ford step 8016 current loss 0.463714, current_train_items 256544.
I0302 19:02:12.366396 22626471084160 run.py:483] Algo bellman_ford step 8017 current loss 0.566188, current_train_items 256576.
I0302 19:02:12.397813 22626471084160 run.py:483] Algo bellman_ford step 8018 current loss 0.584833, current_train_items 256608.
I0302 19:02:12.430030 22626471084160 run.py:483] Algo bellman_ford step 8019 current loss 0.751987, current_train_items 256640.
I0302 19:02:12.449547 22626471084160 run.py:483] Algo bellman_ford step 8020 current loss 0.385714, current_train_items 256672.
I0302 19:02:12.465901 22626471084160 run.py:483] Algo bellman_ford step 8021 current loss 0.512333, current_train_items 256704.
I0302 19:02:12.489181 22626471084160 run.py:483] Algo bellman_ford step 8022 current loss 0.566169, current_train_items 256736.
I0302 19:02:12.519723 22626471084160 run.py:483] Algo bellman_ford step 8023 current loss 0.672822, current_train_items 256768.
I0302 19:02:12.554846 22626471084160 run.py:483] Algo bellman_ford step 8024 current loss 0.751820, current_train_items 256800.
I0302 19:02:12.574352 22626471084160 run.py:483] Algo bellman_ford step 8025 current loss 0.377633, current_train_items 256832.
I0302 19:02:12.590904 22626471084160 run.py:483] Algo bellman_ford step 8026 current loss 0.449130, current_train_items 256864.
I0302 19:02:12.614917 22626471084160 run.py:483] Algo bellman_ford step 8027 current loss 0.626662, current_train_items 256896.
I0302 19:02:12.645521 22626471084160 run.py:483] Algo bellman_ford step 8028 current loss 0.655608, current_train_items 256928.
I0302 19:02:12.679410 22626471084160 run.py:483] Algo bellman_ford step 8029 current loss 0.696712, current_train_items 256960.
I0302 19:02:12.698831 22626471084160 run.py:483] Algo bellman_ford step 8030 current loss 0.308283, current_train_items 256992.
I0302 19:02:12.714915 22626471084160 run.py:483] Algo bellman_ford step 8031 current loss 0.415342, current_train_items 257024.
I0302 19:02:12.739249 22626471084160 run.py:483] Algo bellman_ford step 8032 current loss 0.645170, current_train_items 257056.
I0302 19:02:12.771338 22626471084160 run.py:483] Algo bellman_ford step 8033 current loss 0.654229, current_train_items 257088.
I0302 19:02:12.805959 22626471084160 run.py:483] Algo bellman_ford step 8034 current loss 0.743773, current_train_items 257120.
I0302 19:02:12.825905 22626471084160 run.py:483] Algo bellman_ford step 8035 current loss 0.294002, current_train_items 257152.
I0302 19:02:12.842574 22626471084160 run.py:483] Algo bellman_ford step 8036 current loss 0.469699, current_train_items 257184.
I0302 19:02:12.866343 22626471084160 run.py:483] Algo bellman_ford step 8037 current loss 0.561059, current_train_items 257216.
I0302 19:02:12.897723 22626471084160 run.py:483] Algo bellman_ford step 8038 current loss 0.612622, current_train_items 257248.
I0302 19:02:12.932671 22626471084160 run.py:483] Algo bellman_ford step 8039 current loss 0.778044, current_train_items 257280.
I0302 19:02:12.952323 22626471084160 run.py:483] Algo bellman_ford step 8040 current loss 0.309573, current_train_items 257312.
I0302 19:02:12.968806 22626471084160 run.py:483] Algo bellman_ford step 8041 current loss 0.502033, current_train_items 257344.
I0302 19:02:12.992738 22626471084160 run.py:483] Algo bellman_ford step 8042 current loss 0.574262, current_train_items 257376.
I0302 19:02:13.024734 22626471084160 run.py:483] Algo bellman_ford step 8043 current loss 0.531906, current_train_items 257408.
I0302 19:02:13.058757 22626471084160 run.py:483] Algo bellman_ford step 8044 current loss 0.711859, current_train_items 257440.
I0302 19:02:13.078745 22626471084160 run.py:483] Algo bellman_ford step 8045 current loss 0.252406, current_train_items 257472.
I0302 19:02:13.095479 22626471084160 run.py:483] Algo bellman_ford step 8046 current loss 0.497991, current_train_items 257504.
I0302 19:02:13.118995 22626471084160 run.py:483] Algo bellman_ford step 8047 current loss 0.608544, current_train_items 257536.
I0302 19:02:13.151049 22626471084160 run.py:483] Algo bellman_ford step 8048 current loss 0.587931, current_train_items 257568.
I0302 19:02:13.183326 22626471084160 run.py:483] Algo bellman_ford step 8049 current loss 0.658468, current_train_items 257600.
I0302 19:02:13.202780 22626471084160 run.py:483] Algo bellman_ford step 8050 current loss 0.289941, current_train_items 257632.
I0302 19:02:13.211085 22626471084160 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0302 19:02:13.211200 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 19:02:13.228213 22626471084160 run.py:483] Algo bellman_ford step 8051 current loss 0.454756, current_train_items 257664.
I0302 19:02:13.252466 22626471084160 run.py:483] Algo bellman_ford step 8052 current loss 0.493635, current_train_items 257696.
I0302 19:02:13.284337 22626471084160 run.py:483] Algo bellman_ford step 8053 current loss 0.531344, current_train_items 257728.
I0302 19:02:13.320024 22626471084160 run.py:483] Algo bellman_ford step 8054 current loss 0.805578, current_train_items 257760.
I0302 19:02:13.340102 22626471084160 run.py:483] Algo bellman_ford step 8055 current loss 0.392220, current_train_items 257792.
I0302 19:02:13.355364 22626471084160 run.py:483] Algo bellman_ford step 8056 current loss 0.397226, current_train_items 257824.
I0302 19:02:13.377935 22626471084160 run.py:483] Algo bellman_ford step 8057 current loss 0.511627, current_train_items 257856.
I0302 19:02:13.410730 22626471084160 run.py:483] Algo bellman_ford step 8058 current loss 0.700759, current_train_items 257888.
I0302 19:02:13.446684 22626471084160 run.py:483] Algo bellman_ford step 8059 current loss 0.734288, current_train_items 257920.
I0302 19:02:13.466366 22626471084160 run.py:483] Algo bellman_ford step 8060 current loss 0.265207, current_train_items 257952.
I0302 19:02:13.482369 22626471084160 run.py:483] Algo bellman_ford step 8061 current loss 0.499964, current_train_items 257984.
I0302 19:02:13.507449 22626471084160 run.py:483] Algo bellman_ford step 8062 current loss 0.680490, current_train_items 258016.
I0302 19:02:13.539394 22626471084160 run.py:483] Algo bellman_ford step 8063 current loss 0.651761, current_train_items 258048.
I0302 19:02:13.573185 22626471084160 run.py:483] Algo bellman_ford step 8064 current loss 0.735130, current_train_items 258080.
I0302 19:02:13.593128 22626471084160 run.py:483] Algo bellman_ford step 8065 current loss 0.274416, current_train_items 258112.
I0302 19:02:13.609073 22626471084160 run.py:483] Algo bellman_ford step 8066 current loss 0.499071, current_train_items 258144.
I0302 19:02:13.632775 22626471084160 run.py:483] Algo bellman_ford step 8067 current loss 0.566387, current_train_items 258176.
I0302 19:02:13.664464 22626471084160 run.py:483] Algo bellman_ford step 8068 current loss 0.636310, current_train_items 258208.
I0302 19:02:13.697023 22626471084160 run.py:483] Algo bellman_ford step 8069 current loss 0.742269, current_train_items 258240.
I0302 19:02:13.717143 22626471084160 run.py:483] Algo bellman_ford step 8070 current loss 0.328504, current_train_items 258272.
I0302 19:02:13.734251 22626471084160 run.py:483] Algo bellman_ford step 8071 current loss 0.470126, current_train_items 258304.
I0302 19:02:13.757958 22626471084160 run.py:483] Algo bellman_ford step 8072 current loss 0.675453, current_train_items 258336.
I0302 19:02:13.789741 22626471084160 run.py:483] Algo bellman_ford step 8073 current loss 0.662836, current_train_items 258368.
I0302 19:02:13.824738 22626471084160 run.py:483] Algo bellman_ford step 8074 current loss 0.774577, current_train_items 258400.
I0302 19:02:13.844955 22626471084160 run.py:483] Algo bellman_ford step 8075 current loss 0.316560, current_train_items 258432.
I0302 19:02:13.861138 22626471084160 run.py:483] Algo bellman_ford step 8076 current loss 0.463613, current_train_items 258464.
I0302 19:02:13.884896 22626471084160 run.py:483] Algo bellman_ford step 8077 current loss 0.497870, current_train_items 258496.
I0302 19:02:13.916175 22626471084160 run.py:483] Algo bellman_ford step 8078 current loss 0.804623, current_train_items 258528.
I0302 19:02:13.948246 22626471084160 run.py:483] Algo bellman_ford step 8079 current loss 0.642527, current_train_items 258560.
I0302 19:02:13.968050 22626471084160 run.py:483] Algo bellman_ford step 8080 current loss 0.374957, current_train_items 258592.
I0302 19:02:13.984286 22626471084160 run.py:483] Algo bellman_ford step 8081 current loss 0.505402, current_train_items 258624.
I0302 19:02:14.008005 22626471084160 run.py:483] Algo bellman_ford step 8082 current loss 0.653908, current_train_items 258656.
I0302 19:02:14.039903 22626471084160 run.py:483] Algo bellman_ford step 8083 current loss 0.693762, current_train_items 258688.
I0302 19:02:14.076640 22626471084160 run.py:483] Algo bellman_ford step 8084 current loss 0.822810, current_train_items 258720.
I0302 19:02:14.096592 22626471084160 run.py:483] Algo bellman_ford step 8085 current loss 0.354093, current_train_items 258752.
I0302 19:02:14.113218 22626471084160 run.py:483] Algo bellman_ford step 8086 current loss 0.453360, current_train_items 258784.
I0302 19:02:14.136167 22626471084160 run.py:483] Algo bellman_ford step 8087 current loss 0.578683, current_train_items 258816.
I0302 19:02:14.169018 22626471084160 run.py:483] Algo bellman_ford step 8088 current loss 0.585829, current_train_items 258848.
I0302 19:02:14.201413 22626471084160 run.py:483] Algo bellman_ford step 8089 current loss 0.665544, current_train_items 258880.
I0302 19:02:14.221375 22626471084160 run.py:483] Algo bellman_ford step 8090 current loss 0.308177, current_train_items 258912.
I0302 19:02:14.237108 22626471084160 run.py:483] Algo bellman_ford step 8091 current loss 0.404061, current_train_items 258944.
I0302 19:02:14.261372 22626471084160 run.py:483] Algo bellman_ford step 8092 current loss 0.631648, current_train_items 258976.
I0302 19:02:14.292415 22626471084160 run.py:483] Algo bellman_ford step 8093 current loss 0.626739, current_train_items 259008.
I0302 19:02:14.326748 22626471084160 run.py:483] Algo bellman_ford step 8094 current loss 0.734786, current_train_items 259040.
I0302 19:02:14.346743 22626471084160 run.py:483] Algo bellman_ford step 8095 current loss 0.296976, current_train_items 259072.
I0302 19:02:14.362727 22626471084160 run.py:483] Algo bellman_ford step 8096 current loss 0.481190, current_train_items 259104.
I0302 19:02:14.388052 22626471084160 run.py:483] Algo bellman_ford step 8097 current loss 0.578416, current_train_items 259136.
I0302 19:02:14.420912 22626471084160 run.py:483] Algo bellman_ford step 8098 current loss 0.686904, current_train_items 259168.
I0302 19:02:14.454548 22626471084160 run.py:483] Algo bellman_ford step 8099 current loss 0.705920, current_train_items 259200.
I0302 19:02:14.474293 22626471084160 run.py:483] Algo bellman_ford step 8100 current loss 0.300181, current_train_items 259232.
I0302 19:02:14.482106 22626471084160 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0302 19:02:14.482220 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:02:14.499212 22626471084160 run.py:483] Algo bellman_ford step 8101 current loss 0.459936, current_train_items 259264.
I0302 19:02:14.524742 22626471084160 run.py:483] Algo bellman_ford step 8102 current loss 0.680129, current_train_items 259296.
I0302 19:02:14.556412 22626471084160 run.py:483] Algo bellman_ford step 8103 current loss 0.586108, current_train_items 259328.
I0302 19:02:14.590282 22626471084160 run.py:483] Algo bellman_ford step 8104 current loss 0.794100, current_train_items 259360.
I0302 19:02:14.610535 22626471084160 run.py:483] Algo bellman_ford step 8105 current loss 0.285810, current_train_items 259392.
I0302 19:02:14.626720 22626471084160 run.py:483] Algo bellman_ford step 8106 current loss 0.446731, current_train_items 259424.
I0302 19:02:14.650471 22626471084160 run.py:483] Algo bellman_ford step 8107 current loss 0.584985, current_train_items 259456.
I0302 19:02:14.681532 22626471084160 run.py:483] Algo bellman_ford step 8108 current loss 0.803191, current_train_items 259488.
I0302 19:02:14.716354 22626471084160 run.py:483] Algo bellman_ford step 8109 current loss 0.794924, current_train_items 259520.
I0302 19:02:14.736246 22626471084160 run.py:483] Algo bellman_ford step 8110 current loss 0.270536, current_train_items 259552.
I0302 19:02:14.752037 22626471084160 run.py:483] Algo bellman_ford step 8111 current loss 0.497836, current_train_items 259584.
I0302 19:02:14.775337 22626471084160 run.py:483] Algo bellman_ford step 8112 current loss 0.603484, current_train_items 259616.
I0302 19:02:14.805416 22626471084160 run.py:483] Algo bellman_ford step 8113 current loss 0.564745, current_train_items 259648.
I0302 19:02:14.840983 22626471084160 run.py:483] Algo bellman_ford step 8114 current loss 0.837604, current_train_items 259680.
I0302 19:02:14.860843 22626471084160 run.py:483] Algo bellman_ford step 8115 current loss 0.341119, current_train_items 259712.
I0302 19:02:14.877194 22626471084160 run.py:483] Algo bellman_ford step 8116 current loss 0.425438, current_train_items 259744.
I0302 19:02:14.901538 22626471084160 run.py:483] Algo bellman_ford step 8117 current loss 0.587514, current_train_items 259776.
I0302 19:02:14.933510 22626471084160 run.py:483] Algo bellman_ford step 8118 current loss 0.726818, current_train_items 259808.
I0302 19:02:14.966623 22626471084160 run.py:483] Algo bellman_ford step 8119 current loss 0.709914, current_train_items 259840.
I0302 19:02:14.986095 22626471084160 run.py:483] Algo bellman_ford step 8120 current loss 0.301576, current_train_items 259872.
I0302 19:02:15.002447 22626471084160 run.py:483] Algo bellman_ford step 8121 current loss 0.445398, current_train_items 259904.
I0302 19:02:15.026429 22626471084160 run.py:483] Algo bellman_ford step 8122 current loss 0.547468, current_train_items 259936.
I0302 19:02:15.059472 22626471084160 run.py:483] Algo bellman_ford step 8123 current loss 0.642468, current_train_items 259968.
I0302 19:02:15.091415 22626471084160 run.py:483] Algo bellman_ford step 8124 current loss 0.731992, current_train_items 260000.
I0302 19:02:15.111087 22626471084160 run.py:483] Algo bellman_ford step 8125 current loss 0.282747, current_train_items 260032.
I0302 19:02:15.127599 22626471084160 run.py:483] Algo bellman_ford step 8126 current loss 0.636896, current_train_items 260064.
I0302 19:02:15.152203 22626471084160 run.py:483] Algo bellman_ford step 8127 current loss 0.673474, current_train_items 260096.
I0302 19:02:15.184480 22626471084160 run.py:483] Algo bellman_ford step 8128 current loss 0.721851, current_train_items 260128.
I0302 19:02:15.220732 22626471084160 run.py:483] Algo bellman_ford step 8129 current loss 0.821839, current_train_items 260160.
I0302 19:02:15.240387 22626471084160 run.py:483] Algo bellman_ford step 8130 current loss 0.301816, current_train_items 260192.
I0302 19:02:15.256199 22626471084160 run.py:483] Algo bellman_ford step 8131 current loss 0.445745, current_train_items 260224.
I0302 19:02:15.280253 22626471084160 run.py:483] Algo bellman_ford step 8132 current loss 0.665404, current_train_items 260256.
I0302 19:02:15.311751 22626471084160 run.py:483] Algo bellman_ford step 8133 current loss 0.636777, current_train_items 260288.
I0302 19:02:15.345983 22626471084160 run.py:483] Algo bellman_ford step 8134 current loss 0.783275, current_train_items 260320.
I0302 19:02:15.365676 22626471084160 run.py:483] Algo bellman_ford step 8135 current loss 0.272221, current_train_items 260352.
I0302 19:02:15.381610 22626471084160 run.py:483] Algo bellman_ford step 8136 current loss 0.473088, current_train_items 260384.
I0302 19:02:15.405465 22626471084160 run.py:483] Algo bellman_ford step 8137 current loss 0.640836, current_train_items 260416.
I0302 19:02:15.437320 22626471084160 run.py:483] Algo bellman_ford step 8138 current loss 0.716760, current_train_items 260448.
I0302 19:02:15.472650 22626471084160 run.py:483] Algo bellman_ford step 8139 current loss 0.875326, current_train_items 260480.
I0302 19:02:15.491988 22626471084160 run.py:483] Algo bellman_ford step 8140 current loss 0.304673, current_train_items 260512.
I0302 19:02:15.507993 22626471084160 run.py:483] Algo bellman_ford step 8141 current loss 0.521599, current_train_items 260544.
I0302 19:02:15.532708 22626471084160 run.py:483] Algo bellman_ford step 8142 current loss 0.556261, current_train_items 260576.
I0302 19:02:15.564877 22626471084160 run.py:483] Algo bellman_ford step 8143 current loss 0.610367, current_train_items 260608.
I0302 19:02:15.599146 22626471084160 run.py:483] Algo bellman_ford step 8144 current loss 0.689706, current_train_items 260640.
I0302 19:02:15.618895 22626471084160 run.py:483] Algo bellman_ford step 8145 current loss 0.295857, current_train_items 260672.
I0302 19:02:15.635421 22626471084160 run.py:483] Algo bellman_ford step 8146 current loss 0.476244, current_train_items 260704.
I0302 19:02:15.659359 22626471084160 run.py:483] Algo bellman_ford step 8147 current loss 0.550273, current_train_items 260736.
I0302 19:02:15.691039 22626471084160 run.py:483] Algo bellman_ford step 8148 current loss 0.736328, current_train_items 260768.
I0302 19:02:15.723845 22626471084160 run.py:483] Algo bellman_ford step 8149 current loss 0.736583, current_train_items 260800.
I0302 19:02:15.743634 22626471084160 run.py:483] Algo bellman_ford step 8150 current loss 0.327199, current_train_items 260832.
I0302 19:02:15.751925 22626471084160 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.9462890625, 'score': 0.9462890625, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0302 19:02:15.752032 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.946, val scores are: bellman_ford: 0.946
I0302 19:02:15.768733 22626471084160 run.py:483] Algo bellman_ford step 8151 current loss 0.383481, current_train_items 260864.
I0302 19:02:15.793095 22626471084160 run.py:483] Algo bellman_ford step 8152 current loss 0.613692, current_train_items 260896.
I0302 19:02:15.823385 22626471084160 run.py:483] Algo bellman_ford step 8153 current loss 0.583624, current_train_items 260928.
I0302 19:02:15.859051 22626471084160 run.py:483] Algo bellman_ford step 8154 current loss 0.716800, current_train_items 260960.
I0302 19:02:15.879226 22626471084160 run.py:483] Algo bellman_ford step 8155 current loss 0.279448, current_train_items 260992.
I0302 19:02:15.894783 22626471084160 run.py:483] Algo bellman_ford step 8156 current loss 0.410275, current_train_items 261024.
I0302 19:02:15.919121 22626471084160 run.py:483] Algo bellman_ford step 8157 current loss 0.523221, current_train_items 261056.
I0302 19:02:15.950662 22626471084160 run.py:483] Algo bellman_ford step 8158 current loss 0.674167, current_train_items 261088.
I0302 19:02:15.983850 22626471084160 run.py:483] Algo bellman_ford step 8159 current loss 0.732434, current_train_items 261120.
I0302 19:02:16.003879 22626471084160 run.py:483] Algo bellman_ford step 8160 current loss 0.296675, current_train_items 261152.
I0302 19:02:16.020229 22626471084160 run.py:483] Algo bellman_ford step 8161 current loss 0.497856, current_train_items 261184.
I0302 19:02:16.044031 22626471084160 run.py:483] Algo bellman_ford step 8162 current loss 0.661585, current_train_items 261216.
I0302 19:02:16.076110 22626471084160 run.py:483] Algo bellman_ford step 8163 current loss 0.724052, current_train_items 261248.
I0302 19:02:16.106758 22626471084160 run.py:483] Algo bellman_ford step 8164 current loss 0.742572, current_train_items 261280.
I0302 19:02:16.126206 22626471084160 run.py:483] Algo bellman_ford step 8165 current loss 0.234111, current_train_items 261312.
I0302 19:02:16.142451 22626471084160 run.py:483] Algo bellman_ford step 8166 current loss 0.522869, current_train_items 261344.
I0302 19:02:16.166249 22626471084160 run.py:483] Algo bellman_ford step 8167 current loss 0.634424, current_train_items 261376.
I0302 19:02:16.197175 22626471084160 run.py:483] Algo bellman_ford step 8168 current loss 0.577982, current_train_items 261408.
I0302 19:02:16.231706 22626471084160 run.py:483] Algo bellman_ford step 8169 current loss 0.833637, current_train_items 261440.
I0302 19:02:16.251548 22626471084160 run.py:483] Algo bellman_ford step 8170 current loss 0.244532, current_train_items 261472.
I0302 19:02:16.267849 22626471084160 run.py:483] Algo bellman_ford step 8171 current loss 0.403525, current_train_items 261504.
I0302 19:02:16.291924 22626471084160 run.py:483] Algo bellman_ford step 8172 current loss 0.599684, current_train_items 261536.
I0302 19:02:16.323829 22626471084160 run.py:483] Algo bellman_ford step 8173 current loss 0.591221, current_train_items 261568.
I0302 19:02:16.356845 22626471084160 run.py:483] Algo bellman_ford step 8174 current loss 0.726705, current_train_items 261600.
I0302 19:02:16.376925 22626471084160 run.py:483] Algo bellman_ford step 8175 current loss 0.335289, current_train_items 261632.
I0302 19:02:16.393555 22626471084160 run.py:483] Algo bellman_ford step 8176 current loss 0.444019, current_train_items 261664.
I0302 19:02:16.417098 22626471084160 run.py:483] Algo bellman_ford step 8177 current loss 0.556503, current_train_items 261696.
I0302 19:02:16.449826 22626471084160 run.py:483] Algo bellman_ford step 8178 current loss 0.638451, current_train_items 261728.
I0302 19:02:16.481191 22626471084160 run.py:483] Algo bellman_ford step 8179 current loss 0.547523, current_train_items 261760.
I0302 19:02:16.500852 22626471084160 run.py:483] Algo bellman_ford step 8180 current loss 0.286941, current_train_items 261792.
I0302 19:02:16.517013 22626471084160 run.py:483] Algo bellman_ford step 8181 current loss 0.410327, current_train_items 261824.
I0302 19:02:16.540711 22626471084160 run.py:483] Algo bellman_ford step 8182 current loss 0.533012, current_train_items 261856.
I0302 19:02:16.572653 22626471084160 run.py:483] Algo bellman_ford step 8183 current loss 0.654143, current_train_items 261888.
I0302 19:02:16.608579 22626471084160 run.py:483] Algo bellman_ford step 8184 current loss 0.937261, current_train_items 261920.
I0302 19:02:16.628485 22626471084160 run.py:483] Algo bellman_ford step 8185 current loss 0.329413, current_train_items 261952.
I0302 19:02:16.644616 22626471084160 run.py:483] Algo bellman_ford step 8186 current loss 0.499969, current_train_items 261984.
I0302 19:02:16.668573 22626471084160 run.py:483] Algo bellman_ford step 8187 current loss 0.607096, current_train_items 262016.
I0302 19:02:16.700470 22626471084160 run.py:483] Algo bellman_ford step 8188 current loss 0.611367, current_train_items 262048.
I0302 19:02:16.733973 22626471084160 run.py:483] Algo bellman_ford step 8189 current loss 0.750231, current_train_items 262080.
I0302 19:02:16.753955 22626471084160 run.py:483] Algo bellman_ford step 8190 current loss 0.329646, current_train_items 262112.
I0302 19:02:16.770318 22626471084160 run.py:483] Algo bellman_ford step 8191 current loss 0.489442, current_train_items 262144.
I0302 19:02:16.793996 22626471084160 run.py:483] Algo bellman_ford step 8192 current loss 0.608977, current_train_items 262176.
I0302 19:02:16.825279 22626471084160 run.py:483] Algo bellman_ford step 8193 current loss 0.554988, current_train_items 262208.
I0302 19:02:16.858510 22626471084160 run.py:483] Algo bellman_ford step 8194 current loss 0.764292, current_train_items 262240.
I0302 19:02:16.878192 22626471084160 run.py:483] Algo bellman_ford step 8195 current loss 0.320355, current_train_items 262272.
I0302 19:02:16.894583 22626471084160 run.py:483] Algo bellman_ford step 8196 current loss 0.422398, current_train_items 262304.
I0302 19:02:16.917981 22626471084160 run.py:483] Algo bellman_ford step 8197 current loss 0.505231, current_train_items 262336.
I0302 19:02:16.949573 22626471084160 run.py:483] Algo bellman_ford step 8198 current loss 0.589947, current_train_items 262368.
I0302 19:02:16.982998 22626471084160 run.py:483] Algo bellman_ford step 8199 current loss 0.752418, current_train_items 262400.
I0302 19:02:17.003272 22626471084160 run.py:483] Algo bellman_ford step 8200 current loss 0.279391, current_train_items 262432.
I0302 19:02:17.011220 22626471084160 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0302 19:02:17.011325 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 19:02:17.027721 22626471084160 run.py:483] Algo bellman_ford step 8201 current loss 0.390616, current_train_items 262464.
I0302 19:02:17.052308 22626471084160 run.py:483] Algo bellman_ford step 8202 current loss 0.516841, current_train_items 262496.
I0302 19:02:17.083708 22626471084160 run.py:483] Algo bellman_ford step 8203 current loss 0.657397, current_train_items 262528.
I0302 19:02:17.118857 22626471084160 run.py:483] Algo bellman_ford step 8204 current loss 0.738145, current_train_items 262560.
I0302 19:02:17.138894 22626471084160 run.py:483] Algo bellman_ford step 8205 current loss 0.273541, current_train_items 262592.
I0302 19:02:17.155140 22626471084160 run.py:483] Algo bellman_ford step 8206 current loss 0.412129, current_train_items 262624.
I0302 19:02:17.179736 22626471084160 run.py:483] Algo bellman_ford step 8207 current loss 0.572833, current_train_items 262656.
I0302 19:02:17.211344 22626471084160 run.py:483] Algo bellman_ford step 8208 current loss 0.598747, current_train_items 262688.
I0302 19:02:17.242612 22626471084160 run.py:483] Algo bellman_ford step 8209 current loss 0.548504, current_train_items 262720.
I0302 19:02:17.262235 22626471084160 run.py:483] Algo bellman_ford step 8210 current loss 0.267357, current_train_items 262752.
I0302 19:02:17.278308 22626471084160 run.py:483] Algo bellman_ford step 8211 current loss 0.433565, current_train_items 262784.
I0302 19:02:17.301802 22626471084160 run.py:483] Algo bellman_ford step 8212 current loss 0.608960, current_train_items 262816.
I0302 19:02:17.333338 22626471084160 run.py:483] Algo bellman_ford step 8213 current loss 0.626576, current_train_items 262848.
I0302 19:02:17.366631 22626471084160 run.py:483] Algo bellman_ford step 8214 current loss 0.813594, current_train_items 262880.
I0302 19:02:17.386264 22626471084160 run.py:483] Algo bellman_ford step 8215 current loss 0.254640, current_train_items 262912.
I0302 19:02:17.401827 22626471084160 run.py:483] Algo bellman_ford step 8216 current loss 0.490829, current_train_items 262944.
I0302 19:02:17.424502 22626471084160 run.py:483] Algo bellman_ford step 8217 current loss 0.681447, current_train_items 262976.
I0302 19:02:17.457072 22626471084160 run.py:483] Algo bellman_ford step 8218 current loss 0.665651, current_train_items 263008.
I0302 19:02:17.491410 22626471084160 run.py:483] Algo bellman_ford step 8219 current loss 0.758764, current_train_items 263040.
I0302 19:02:17.510769 22626471084160 run.py:483] Algo bellman_ford step 8220 current loss 0.341208, current_train_items 263072.
I0302 19:02:17.526651 22626471084160 run.py:483] Algo bellman_ford step 8221 current loss 0.486103, current_train_items 263104.
I0302 19:02:17.551441 22626471084160 run.py:483] Algo bellman_ford step 8222 current loss 0.690003, current_train_items 263136.
I0302 19:02:17.583481 22626471084160 run.py:483] Algo bellman_ford step 8223 current loss 0.652012, current_train_items 263168.
I0302 19:02:17.614682 22626471084160 run.py:483] Algo bellman_ford step 8224 current loss 0.634702, current_train_items 263200.
I0302 19:02:17.634372 22626471084160 run.py:483] Algo bellman_ford step 8225 current loss 0.283054, current_train_items 263232.
I0302 19:02:17.650756 22626471084160 run.py:483] Algo bellman_ford step 8226 current loss 0.422342, current_train_items 263264.
I0302 19:02:17.675230 22626471084160 run.py:483] Algo bellman_ford step 8227 current loss 0.690753, current_train_items 263296.
I0302 19:02:17.705854 22626471084160 run.py:483] Algo bellman_ford step 8228 current loss 0.637557, current_train_items 263328.
I0302 19:02:17.739611 22626471084160 run.py:483] Algo bellman_ford step 8229 current loss 0.745550, current_train_items 263360.
I0302 19:02:17.759219 22626471084160 run.py:483] Algo bellman_ford step 8230 current loss 0.271529, current_train_items 263392.
I0302 19:02:17.775532 22626471084160 run.py:483] Algo bellman_ford step 8231 current loss 0.497177, current_train_items 263424.
I0302 19:02:17.800107 22626471084160 run.py:483] Algo bellman_ford step 8232 current loss 0.543544, current_train_items 263456.
I0302 19:02:17.831362 22626471084160 run.py:483] Algo bellman_ford step 8233 current loss 0.558616, current_train_items 263488.
I0302 19:02:17.864439 22626471084160 run.py:483] Algo bellman_ford step 8234 current loss 0.685706, current_train_items 263520.
I0302 19:02:17.884091 22626471084160 run.py:483] Algo bellman_ford step 8235 current loss 0.280201, current_train_items 263552.
I0302 19:02:17.900716 22626471084160 run.py:483] Algo bellman_ford step 8236 current loss 0.492850, current_train_items 263584.
I0302 19:02:17.925226 22626471084160 run.py:483] Algo bellman_ford step 8237 current loss 0.668701, current_train_items 263616.
I0302 19:02:17.957201 22626471084160 run.py:483] Algo bellman_ford step 8238 current loss 0.647504, current_train_items 263648.
I0302 19:02:17.990127 22626471084160 run.py:483] Algo bellman_ford step 8239 current loss 0.668357, current_train_items 263680.
I0302 19:02:18.009683 22626471084160 run.py:483] Algo bellman_ford step 8240 current loss 0.286268, current_train_items 263712.
I0302 19:02:18.025913 22626471084160 run.py:483] Algo bellman_ford step 8241 current loss 0.420493, current_train_items 263744.
I0302 19:02:18.049387 22626471084160 run.py:483] Algo bellman_ford step 8242 current loss 0.580122, current_train_items 263776.
I0302 19:02:18.080555 22626471084160 run.py:483] Algo bellman_ford step 8243 current loss 0.584904, current_train_items 263808.
I0302 19:02:18.116014 22626471084160 run.py:483] Algo bellman_ford step 8244 current loss 0.739458, current_train_items 263840.
I0302 19:02:18.135921 22626471084160 run.py:483] Algo bellman_ford step 8245 current loss 0.288288, current_train_items 263872.
I0302 19:02:18.152158 22626471084160 run.py:483] Algo bellman_ford step 8246 current loss 0.462320, current_train_items 263904.
I0302 19:02:18.175117 22626471084160 run.py:483] Algo bellman_ford step 8247 current loss 0.553408, current_train_items 263936.
I0302 19:02:18.205902 22626471084160 run.py:483] Algo bellman_ford step 8248 current loss 0.549130, current_train_items 263968.
I0302 19:02:18.239678 22626471084160 run.py:483] Algo bellman_ford step 8249 current loss 0.767338, current_train_items 264000.
I0302 19:02:18.259255 22626471084160 run.py:483] Algo bellman_ford step 8250 current loss 0.215261, current_train_items 264032.
I0302 19:02:18.267265 22626471084160 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0302 19:02:18.267371 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:02:18.284050 22626471084160 run.py:483] Algo bellman_ford step 8251 current loss 0.487019, current_train_items 264064.
I0302 19:02:18.308590 22626471084160 run.py:483] Algo bellman_ford step 8252 current loss 0.602425, current_train_items 264096.
I0302 19:02:18.341700 22626471084160 run.py:483] Algo bellman_ford step 8253 current loss 0.728442, current_train_items 264128.
I0302 19:02:18.374997 22626471084160 run.py:483] Algo bellman_ford step 8254 current loss 0.681031, current_train_items 264160.
I0302 19:02:18.395334 22626471084160 run.py:483] Algo bellman_ford step 8255 current loss 0.387974, current_train_items 264192.
I0302 19:02:18.411796 22626471084160 run.py:483] Algo bellman_ford step 8256 current loss 0.440190, current_train_items 264224.
I0302 19:02:18.436055 22626471084160 run.py:483] Algo bellman_ford step 8257 current loss 0.591455, current_train_items 264256.
I0302 19:02:18.468289 22626471084160 run.py:483] Algo bellman_ford step 8258 current loss 0.647122, current_train_items 264288.
I0302 19:02:18.503330 22626471084160 run.py:483] Algo bellman_ford step 8259 current loss 0.779590, current_train_items 264320.
I0302 19:02:18.523065 22626471084160 run.py:483] Algo bellman_ford step 8260 current loss 0.312032, current_train_items 264352.
I0302 19:02:18.539533 22626471084160 run.py:483] Algo bellman_ford step 8261 current loss 0.445249, current_train_items 264384.
I0302 19:02:18.563975 22626471084160 run.py:483] Algo bellman_ford step 8262 current loss 0.588061, current_train_items 264416.
I0302 19:02:18.596521 22626471084160 run.py:483] Algo bellman_ford step 8263 current loss 0.663585, current_train_items 264448.
I0302 19:02:18.631689 22626471084160 run.py:483] Algo bellman_ford step 8264 current loss 0.744782, current_train_items 264480.
I0302 19:02:18.651130 22626471084160 run.py:483] Algo bellman_ford step 8265 current loss 0.264821, current_train_items 264512.
I0302 19:02:18.666921 22626471084160 run.py:483] Algo bellman_ford step 8266 current loss 0.494820, current_train_items 264544.
I0302 19:02:18.691047 22626471084160 run.py:483] Algo bellman_ford step 8267 current loss 0.608161, current_train_items 264576.
I0302 19:02:18.723113 22626471084160 run.py:483] Algo bellman_ford step 8268 current loss 0.880912, current_train_items 264608.
I0302 19:02:18.755767 22626471084160 run.py:483] Algo bellman_ford step 8269 current loss 0.641707, current_train_items 264640.
I0302 19:02:18.775916 22626471084160 run.py:483] Algo bellman_ford step 8270 current loss 0.264974, current_train_items 264672.
I0302 19:02:18.791635 22626471084160 run.py:483] Algo bellman_ford step 8271 current loss 0.358213, current_train_items 264704.
I0302 19:02:18.815253 22626471084160 run.py:483] Algo bellman_ford step 8272 current loss 0.560993, current_train_items 264736.
I0302 19:02:18.845751 22626471084160 run.py:483] Algo bellman_ford step 8273 current loss 0.610766, current_train_items 264768.
I0302 19:02:18.880616 22626471084160 run.py:483] Algo bellman_ford step 8274 current loss 0.684685, current_train_items 264800.
I0302 19:02:18.900717 22626471084160 run.py:483] Algo bellman_ford step 8275 current loss 0.318607, current_train_items 264832.
I0302 19:02:18.917490 22626471084160 run.py:483] Algo bellman_ford step 8276 current loss 0.449326, current_train_items 264864.
I0302 19:02:18.941789 22626471084160 run.py:483] Algo bellman_ford step 8277 current loss 0.571757, current_train_items 264896.
I0302 19:02:18.973303 22626471084160 run.py:483] Algo bellman_ford step 8278 current loss 0.538070, current_train_items 264928.
I0302 19:02:19.006264 22626471084160 run.py:483] Algo bellman_ford step 8279 current loss 0.686068, current_train_items 264960.
I0302 19:02:19.025695 22626471084160 run.py:483] Algo bellman_ford step 8280 current loss 0.310224, current_train_items 264992.
I0302 19:02:19.042272 22626471084160 run.py:483] Algo bellman_ford step 8281 current loss 0.468805, current_train_items 265024.
I0302 19:02:19.065771 22626471084160 run.py:483] Algo bellman_ford step 8282 current loss 0.616730, current_train_items 265056.
I0302 19:02:19.096580 22626471084160 run.py:483] Algo bellman_ford step 8283 current loss 0.654150, current_train_items 265088.
I0302 19:02:19.129464 22626471084160 run.py:483] Algo bellman_ford step 8284 current loss 0.688137, current_train_items 265120.
I0302 19:02:19.149483 22626471084160 run.py:483] Algo bellman_ford step 8285 current loss 0.306566, current_train_items 265152.
I0302 19:02:19.166002 22626471084160 run.py:483] Algo bellman_ford step 8286 current loss 0.436755, current_train_items 265184.
I0302 19:02:19.189921 22626471084160 run.py:483] Algo bellman_ford step 8287 current loss 0.554788, current_train_items 265216.
I0302 19:02:19.222676 22626471084160 run.py:483] Algo bellman_ford step 8288 current loss 0.687060, current_train_items 265248.
I0302 19:02:19.256468 22626471084160 run.py:483] Algo bellman_ford step 8289 current loss 0.675117, current_train_items 265280.
I0302 19:02:19.276207 22626471084160 run.py:483] Algo bellman_ford step 8290 current loss 0.283121, current_train_items 265312.
I0302 19:02:19.292549 22626471084160 run.py:483] Algo bellman_ford step 8291 current loss 0.472229, current_train_items 265344.
I0302 19:02:19.316875 22626471084160 run.py:483] Algo bellman_ford step 8292 current loss 0.627614, current_train_items 265376.
I0302 19:02:19.347443 22626471084160 run.py:483] Algo bellman_ford step 8293 current loss 0.597029, current_train_items 265408.
I0302 19:02:19.382892 22626471084160 run.py:483] Algo bellman_ford step 8294 current loss 0.769750, current_train_items 265440.
I0302 19:02:19.402676 22626471084160 run.py:483] Algo bellman_ford step 8295 current loss 0.245401, current_train_items 265472.
I0302 19:02:19.419036 22626471084160 run.py:483] Algo bellman_ford step 8296 current loss 0.487945, current_train_items 265504.
I0302 19:02:19.443900 22626471084160 run.py:483] Algo bellman_ford step 8297 current loss 0.707657, current_train_items 265536.
I0302 19:02:19.475686 22626471084160 run.py:483] Algo bellman_ford step 8298 current loss 0.608798, current_train_items 265568.
I0302 19:02:19.508893 22626471084160 run.py:483] Algo bellman_ford step 8299 current loss 0.694306, current_train_items 265600.
I0302 19:02:19.528974 22626471084160 run.py:483] Algo bellman_ford step 8300 current loss 0.250646, current_train_items 265632.
I0302 19:02:19.536729 22626471084160 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.939453125, 'score': 0.939453125, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0302 19:02:19.536834 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.939, val scores are: bellman_ford: 0.939
I0302 19:02:19.553253 22626471084160 run.py:483] Algo bellman_ford step 8301 current loss 0.546867, current_train_items 265664.
I0302 19:02:19.579084 22626471084160 run.py:483] Algo bellman_ford step 8302 current loss 0.671704, current_train_items 265696.
I0302 19:02:19.611436 22626471084160 run.py:483] Algo bellman_ford step 8303 current loss 0.722204, current_train_items 265728.
I0302 19:02:19.645309 22626471084160 run.py:483] Algo bellman_ford step 8304 current loss 0.732113, current_train_items 265760.
I0302 19:02:19.665327 22626471084160 run.py:483] Algo bellman_ford step 8305 current loss 0.317646, current_train_items 265792.
I0302 19:02:19.681102 22626471084160 run.py:483] Algo bellman_ford step 8306 current loss 0.529677, current_train_items 265824.
I0302 19:02:19.705332 22626471084160 run.py:483] Algo bellman_ford step 8307 current loss 0.577372, current_train_items 265856.
I0302 19:02:19.734941 22626471084160 run.py:483] Algo bellman_ford step 8308 current loss 0.569244, current_train_items 265888.
I0302 19:02:19.770783 22626471084160 run.py:483] Algo bellman_ford step 8309 current loss 0.798387, current_train_items 265920.
I0302 19:02:19.790410 22626471084160 run.py:483] Algo bellman_ford step 8310 current loss 0.267592, current_train_items 265952.
I0302 19:02:19.806460 22626471084160 run.py:483] Algo bellman_ford step 8311 current loss 0.427238, current_train_items 265984.
I0302 19:02:19.830540 22626471084160 run.py:483] Algo bellman_ford step 8312 current loss 0.601487, current_train_items 266016.
I0302 19:02:19.861487 22626471084160 run.py:483] Algo bellman_ford step 8313 current loss 0.642555, current_train_items 266048.
I0302 19:02:19.893460 22626471084160 run.py:483] Algo bellman_ford step 8314 current loss 0.645175, current_train_items 266080.
I0302 19:02:19.913171 22626471084160 run.py:483] Algo bellman_ford step 8315 current loss 0.240377, current_train_items 266112.
I0302 19:02:19.929993 22626471084160 run.py:483] Algo bellman_ford step 8316 current loss 0.428809, current_train_items 266144.
I0302 19:02:19.953487 22626471084160 run.py:483] Algo bellman_ford step 8317 current loss 0.573654, current_train_items 266176.
I0302 19:02:19.983108 22626471084160 run.py:483] Algo bellman_ford step 8318 current loss 0.575434, current_train_items 266208.
I0302 19:02:20.016521 22626471084160 run.py:483] Algo bellman_ford step 8319 current loss 0.678851, current_train_items 266240.
I0302 19:02:20.036330 22626471084160 run.py:483] Algo bellman_ford step 8320 current loss 0.242214, current_train_items 266272.
I0302 19:02:20.052492 22626471084160 run.py:483] Algo bellman_ford step 8321 current loss 0.392794, current_train_items 266304.
I0302 19:02:20.077517 22626471084160 run.py:483] Algo bellman_ford step 8322 current loss 0.611143, current_train_items 266336.
I0302 19:02:20.108496 22626471084160 run.py:483] Algo bellman_ford step 8323 current loss 0.626087, current_train_items 266368.
I0302 19:02:20.143407 22626471084160 run.py:483] Algo bellman_ford step 8324 current loss 0.719996, current_train_items 266400.
I0302 19:02:20.162801 22626471084160 run.py:483] Algo bellman_ford step 8325 current loss 0.343973, current_train_items 266432.
I0302 19:02:20.178479 22626471084160 run.py:483] Algo bellman_ford step 8326 current loss 0.405494, current_train_items 266464.
I0302 19:02:20.202313 22626471084160 run.py:483] Algo bellman_ford step 8327 current loss 0.521037, current_train_items 266496.
I0302 19:02:20.232950 22626471084160 run.py:483] Algo bellman_ford step 8328 current loss 0.657506, current_train_items 266528.
I0302 19:02:20.267661 22626471084160 run.py:483] Algo bellman_ford step 8329 current loss 0.697967, current_train_items 266560.
I0302 19:02:20.287246 22626471084160 run.py:483] Algo bellman_ford step 8330 current loss 0.321084, current_train_items 266592.
I0302 19:02:20.303348 22626471084160 run.py:483] Algo bellman_ford step 8331 current loss 0.460421, current_train_items 266624.
I0302 19:02:20.326783 22626471084160 run.py:483] Algo bellman_ford step 8332 current loss 0.515944, current_train_items 266656.
I0302 19:02:20.358820 22626471084160 run.py:483] Algo bellman_ford step 8333 current loss 0.701827, current_train_items 266688.
I0302 19:02:20.391914 22626471084160 run.py:483] Algo bellman_ford step 8334 current loss 0.769026, current_train_items 266720.
I0302 19:02:20.411505 22626471084160 run.py:483] Algo bellman_ford step 8335 current loss 0.296887, current_train_items 266752.
I0302 19:02:20.427688 22626471084160 run.py:483] Algo bellman_ford step 8336 current loss 0.507204, current_train_items 266784.
I0302 19:02:20.452628 22626471084160 run.py:483] Algo bellman_ford step 8337 current loss 0.530918, current_train_items 266816.
I0302 19:02:20.484023 22626471084160 run.py:483] Algo bellman_ford step 8338 current loss 0.719450, current_train_items 266848.
I0302 19:02:20.516723 22626471084160 run.py:483] Algo bellman_ford step 8339 current loss 0.798913, current_train_items 266880.
I0302 19:02:20.536541 22626471084160 run.py:483] Algo bellman_ford step 8340 current loss 0.395302, current_train_items 266912.
I0302 19:02:20.552836 22626471084160 run.py:483] Algo bellman_ford step 8341 current loss 0.398480, current_train_items 266944.
I0302 19:02:20.576132 22626471084160 run.py:483] Algo bellman_ford step 8342 current loss 0.518830, current_train_items 266976.
I0302 19:02:20.606745 22626471084160 run.py:483] Algo bellman_ford step 8343 current loss 0.626849, current_train_items 267008.
I0302 19:02:20.641743 22626471084160 run.py:483] Algo bellman_ford step 8344 current loss 0.888399, current_train_items 267040.
I0302 19:02:20.661640 22626471084160 run.py:483] Algo bellman_ford step 8345 current loss 0.316038, current_train_items 267072.
I0302 19:02:20.677693 22626471084160 run.py:483] Algo bellman_ford step 8346 current loss 0.410981, current_train_items 267104.
I0302 19:02:20.702512 22626471084160 run.py:483] Algo bellman_ford step 8347 current loss 0.586919, current_train_items 267136.
I0302 19:02:20.734514 22626471084160 run.py:483] Algo bellman_ford step 8348 current loss 0.649042, current_train_items 267168.
I0302 19:02:20.768985 22626471084160 run.py:483] Algo bellman_ford step 8349 current loss 0.722507, current_train_items 267200.
I0302 19:02:20.788879 22626471084160 run.py:483] Algo bellman_ford step 8350 current loss 0.283893, current_train_items 267232.
I0302 19:02:20.796861 22626471084160 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0302 19:02:20.796965 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:02:20.813801 22626471084160 run.py:483] Algo bellman_ford step 8351 current loss 0.428353, current_train_items 267264.
I0302 19:02:20.838474 22626471084160 run.py:483] Algo bellman_ford step 8352 current loss 0.638273, current_train_items 267296.
I0302 19:02:20.870229 22626471084160 run.py:483] Algo bellman_ford step 8353 current loss 0.650854, current_train_items 267328.
I0302 19:02:20.906069 22626471084160 run.py:483] Algo bellman_ford step 8354 current loss 0.865916, current_train_items 267360.
I0302 19:02:20.926290 22626471084160 run.py:483] Algo bellman_ford step 8355 current loss 0.304010, current_train_items 267392.
I0302 19:02:20.941916 22626471084160 run.py:483] Algo bellman_ford step 8356 current loss 0.491852, current_train_items 267424.
I0302 19:02:20.965867 22626471084160 run.py:483] Algo bellman_ford step 8357 current loss 0.508799, current_train_items 267456.
I0302 19:02:20.997594 22626471084160 run.py:483] Algo bellman_ford step 8358 current loss 0.602797, current_train_items 267488.
I0302 19:02:21.030542 22626471084160 run.py:483] Algo bellman_ford step 8359 current loss 0.702523, current_train_items 267520.
I0302 19:02:21.050823 22626471084160 run.py:483] Algo bellman_ford step 8360 current loss 0.304088, current_train_items 267552.
I0302 19:02:21.066825 22626471084160 run.py:483] Algo bellman_ford step 8361 current loss 0.410807, current_train_items 267584.
I0302 19:02:21.090970 22626471084160 run.py:483] Algo bellman_ford step 8362 current loss 0.560364, current_train_items 267616.
I0302 19:02:21.122023 22626471084160 run.py:483] Algo bellman_ford step 8363 current loss 0.670468, current_train_items 267648.
I0302 19:02:21.156206 22626471084160 run.py:483] Algo bellman_ford step 8364 current loss 0.920532, current_train_items 267680.
I0302 19:02:21.175898 22626471084160 run.py:483] Algo bellman_ford step 8365 current loss 0.317579, current_train_items 267712.
I0302 19:02:21.192407 22626471084160 run.py:483] Algo bellman_ford step 8366 current loss 0.425699, current_train_items 267744.
I0302 19:02:21.215438 22626471084160 run.py:483] Algo bellman_ford step 8367 current loss 0.494945, current_train_items 267776.
I0302 19:02:21.246974 22626471084160 run.py:483] Algo bellman_ford step 8368 current loss 0.640039, current_train_items 267808.
I0302 19:02:21.283051 22626471084160 run.py:483] Algo bellman_ford step 8369 current loss 0.812724, current_train_items 267840.
I0302 19:02:21.303343 22626471084160 run.py:483] Algo bellman_ford step 8370 current loss 0.261540, current_train_items 267872.
I0302 19:02:21.319885 22626471084160 run.py:483] Algo bellman_ford step 8371 current loss 0.645528, current_train_items 267904.
I0302 19:02:21.342799 22626471084160 run.py:483] Algo bellman_ford step 8372 current loss 0.720681, current_train_items 267936.
I0302 19:02:21.374015 22626471084160 run.py:483] Algo bellman_ford step 8373 current loss 0.745140, current_train_items 267968.
I0302 19:02:21.408377 22626471084160 run.py:483] Algo bellman_ford step 8374 current loss 0.865828, current_train_items 268000.
I0302 19:02:21.428846 22626471084160 run.py:483] Algo bellman_ford step 8375 current loss 0.243994, current_train_items 268032.
I0302 19:02:21.444685 22626471084160 run.py:483] Algo bellman_ford step 8376 current loss 0.511587, current_train_items 268064.
I0302 19:02:21.467813 22626471084160 run.py:483] Algo bellman_ford step 8377 current loss 0.607368, current_train_items 268096.
I0302 19:02:21.499558 22626471084160 run.py:483] Algo bellman_ford step 8378 current loss 0.668237, current_train_items 268128.
I0302 19:02:21.533949 22626471084160 run.py:483] Algo bellman_ford step 8379 current loss 0.964976, current_train_items 268160.
I0302 19:02:21.553808 22626471084160 run.py:483] Algo bellman_ford step 8380 current loss 0.390390, current_train_items 268192.
I0302 19:02:21.570148 22626471084160 run.py:483] Algo bellman_ford step 8381 current loss 0.493985, current_train_items 268224.
I0302 19:02:21.593015 22626471084160 run.py:483] Algo bellman_ford step 8382 current loss 0.487113, current_train_items 268256.
I0302 19:02:21.625377 22626471084160 run.py:483] Algo bellman_ford step 8383 current loss 0.776794, current_train_items 268288.
I0302 19:02:21.657605 22626471084160 run.py:483] Algo bellman_ford step 8384 current loss 0.688763, current_train_items 268320.
I0302 19:02:21.677793 22626471084160 run.py:483] Algo bellman_ford step 8385 current loss 0.345775, current_train_items 268352.
I0302 19:02:21.693592 22626471084160 run.py:483] Algo bellman_ford step 8386 current loss 0.421602, current_train_items 268384.
I0302 19:02:21.716737 22626471084160 run.py:483] Algo bellman_ford step 8387 current loss 0.557316, current_train_items 268416.
I0302 19:02:21.748198 22626471084160 run.py:483] Algo bellman_ford step 8388 current loss 0.684281, current_train_items 268448.
I0302 19:02:21.781600 22626471084160 run.py:483] Algo bellman_ford step 8389 current loss 0.722744, current_train_items 268480.
I0302 19:02:21.801564 22626471084160 run.py:483] Algo bellman_ford step 8390 current loss 0.334595, current_train_items 268512.
I0302 19:02:21.818011 22626471084160 run.py:483] Algo bellman_ford step 8391 current loss 0.482072, current_train_items 268544.
I0302 19:02:21.840914 22626471084160 run.py:483] Algo bellman_ford step 8392 current loss 0.564685, current_train_items 268576.
I0302 19:02:21.873745 22626471084160 run.py:483] Algo bellman_ford step 8393 current loss 0.636606, current_train_items 268608.
I0302 19:02:21.906383 22626471084160 run.py:483] Algo bellman_ford step 8394 current loss 0.763451, current_train_items 268640.
I0302 19:02:21.926341 22626471084160 run.py:483] Algo bellman_ford step 8395 current loss 0.343657, current_train_items 268672.
I0302 19:02:21.942611 22626471084160 run.py:483] Algo bellman_ford step 8396 current loss 0.492369, current_train_items 268704.
I0302 19:02:21.966175 22626471084160 run.py:483] Algo bellman_ford step 8397 current loss 0.583365, current_train_items 268736.
I0302 19:02:21.998148 22626471084160 run.py:483] Algo bellman_ford step 8398 current loss 0.613293, current_train_items 268768.
I0302 19:02:22.032531 22626471084160 run.py:483] Algo bellman_ford step 8399 current loss 0.729091, current_train_items 268800.
I0302 19:02:22.052701 22626471084160 run.py:483] Algo bellman_ford step 8400 current loss 0.347889, current_train_items 268832.
I0302 19:02:22.060329 22626471084160 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0302 19:02:22.060435 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:22.077497 22626471084160 run.py:483] Algo bellman_ford step 8401 current loss 0.433486, current_train_items 268864.
I0302 19:02:22.102398 22626471084160 run.py:483] Algo bellman_ford step 8402 current loss 0.616404, current_train_items 268896.
I0302 19:02:22.135092 22626471084160 run.py:483] Algo bellman_ford step 8403 current loss 0.646072, current_train_items 268928.
I0302 19:02:22.171825 22626471084160 run.py:483] Algo bellman_ford step 8404 current loss 0.792526, current_train_items 268960.
I0302 19:02:22.191770 22626471084160 run.py:483] Algo bellman_ford step 8405 current loss 0.307108, current_train_items 268992.
I0302 19:02:22.207780 22626471084160 run.py:483] Algo bellman_ford step 8406 current loss 0.448835, current_train_items 269024.
I0302 19:02:22.231255 22626471084160 run.py:483] Algo bellman_ford step 8407 current loss 0.628204, current_train_items 269056.
I0302 19:02:22.262314 22626471084160 run.py:483] Algo bellman_ford step 8408 current loss 0.668593, current_train_items 269088.
I0302 19:02:22.297265 22626471084160 run.py:483] Algo bellman_ford step 8409 current loss 0.748790, current_train_items 269120.
I0302 19:02:22.316924 22626471084160 run.py:483] Algo bellman_ford step 8410 current loss 0.247181, current_train_items 269152.
I0302 19:02:22.333070 22626471084160 run.py:483] Algo bellman_ford step 8411 current loss 0.451677, current_train_items 269184.
I0302 19:02:22.357402 22626471084160 run.py:483] Algo bellman_ford step 8412 current loss 0.555052, current_train_items 269216.
I0302 19:02:22.390370 22626471084160 run.py:483] Algo bellman_ford step 8413 current loss 0.699409, current_train_items 269248.
I0302 19:02:22.425068 22626471084160 run.py:483] Algo bellman_ford step 8414 current loss 0.786882, current_train_items 269280.
I0302 19:02:22.445096 22626471084160 run.py:483] Algo bellman_ford step 8415 current loss 0.373482, current_train_items 269312.
I0302 19:02:22.461397 22626471084160 run.py:483] Algo bellman_ford step 8416 current loss 0.414772, current_train_items 269344.
I0302 19:02:22.485315 22626471084160 run.py:483] Algo bellman_ford step 8417 current loss 0.681398, current_train_items 269376.
I0302 19:02:22.517661 22626471084160 run.py:483] Algo bellman_ford step 8418 current loss 0.580092, current_train_items 269408.
I0302 19:02:22.554769 22626471084160 run.py:483] Algo bellman_ford step 8419 current loss 0.791851, current_train_items 269440.
I0302 19:02:22.574258 22626471084160 run.py:483] Algo bellman_ford step 8420 current loss 0.255791, current_train_items 269472.
I0302 19:02:22.590478 22626471084160 run.py:483] Algo bellman_ford step 8421 current loss 0.519974, current_train_items 269504.
I0302 19:02:22.615618 22626471084160 run.py:483] Algo bellman_ford step 8422 current loss 0.623584, current_train_items 269536.
I0302 19:02:22.647863 22626471084160 run.py:483] Algo bellman_ford step 8423 current loss 0.603945, current_train_items 269568.
I0302 19:02:22.682957 22626471084160 run.py:483] Algo bellman_ford step 8424 current loss 0.857174, current_train_items 269600.
I0302 19:02:22.702592 22626471084160 run.py:483] Algo bellman_ford step 8425 current loss 0.275326, current_train_items 269632.
I0302 19:02:22.718376 22626471084160 run.py:483] Algo bellman_ford step 8426 current loss 0.489553, current_train_items 269664.
I0302 19:02:22.742220 22626471084160 run.py:483] Algo bellman_ford step 8427 current loss 0.613333, current_train_items 269696.
I0302 19:02:22.773778 22626471084160 run.py:483] Algo bellman_ford step 8428 current loss 0.618293, current_train_items 269728.
I0302 19:02:22.807703 22626471084160 run.py:483] Algo bellman_ford step 8429 current loss 0.729191, current_train_items 269760.
I0302 19:02:22.827354 22626471084160 run.py:483] Algo bellman_ford step 8430 current loss 0.319153, current_train_items 269792.
I0302 19:02:22.844003 22626471084160 run.py:483] Algo bellman_ford step 8431 current loss 0.557702, current_train_items 269824.
I0302 19:02:22.867793 22626471084160 run.py:483] Algo bellman_ford step 8432 current loss 0.468653, current_train_items 269856.
I0302 19:02:22.900726 22626471084160 run.py:483] Algo bellman_ford step 8433 current loss 0.697911, current_train_items 269888.
I0302 19:02:22.932953 22626471084160 run.py:483] Algo bellman_ford step 8434 current loss 0.714610, current_train_items 269920.
I0302 19:02:22.952259 22626471084160 run.py:483] Algo bellman_ford step 8435 current loss 0.268439, current_train_items 269952.
I0302 19:02:22.968199 22626471084160 run.py:483] Algo bellman_ford step 8436 current loss 0.495796, current_train_items 269984.
I0302 19:02:22.992030 22626471084160 run.py:483] Algo bellman_ford step 8437 current loss 0.536246, current_train_items 270016.
I0302 19:02:23.023530 22626471084160 run.py:483] Algo bellman_ford step 8438 current loss 0.605093, current_train_items 270048.
I0302 19:02:23.056020 22626471084160 run.py:483] Algo bellman_ford step 8439 current loss 0.730834, current_train_items 270080.
I0302 19:02:23.075637 22626471084160 run.py:483] Algo bellman_ford step 8440 current loss 0.313027, current_train_items 270112.
I0302 19:02:23.091951 22626471084160 run.py:483] Algo bellman_ford step 8441 current loss 0.465313, current_train_items 270144.
I0302 19:02:23.115988 22626471084160 run.py:483] Algo bellman_ford step 8442 current loss 0.631394, current_train_items 270176.
I0302 19:02:23.147537 22626471084160 run.py:483] Algo bellman_ford step 8443 current loss 0.651412, current_train_items 270208.
I0302 19:02:23.182401 22626471084160 run.py:483] Algo bellman_ford step 8444 current loss 0.773478, current_train_items 270240.
I0302 19:02:23.201802 22626471084160 run.py:483] Algo bellman_ford step 8445 current loss 0.313355, current_train_items 270272.
I0302 19:02:23.217902 22626471084160 run.py:483] Algo bellman_ford step 8446 current loss 0.477167, current_train_items 270304.
I0302 19:02:23.241767 22626471084160 run.py:483] Algo bellman_ford step 8447 current loss 0.592748, current_train_items 270336.
I0302 19:02:23.272965 22626471084160 run.py:483] Algo bellman_ford step 8448 current loss 0.678950, current_train_items 270368.
I0302 19:02:23.307286 22626471084160 run.py:483] Algo bellman_ford step 8449 current loss 0.858886, current_train_items 270400.
I0302 19:02:23.326892 22626471084160 run.py:483] Algo bellman_ford step 8450 current loss 0.293765, current_train_items 270432.
I0302 19:02:23.334881 22626471084160 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0302 19:02:23.334985 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:02:23.351556 22626471084160 run.py:483] Algo bellman_ford step 8451 current loss 0.452014, current_train_items 270464.
I0302 19:02:23.376299 22626471084160 run.py:483] Algo bellman_ford step 8452 current loss 0.558945, current_train_items 270496.
I0302 19:02:23.408827 22626471084160 run.py:483] Algo bellman_ford step 8453 current loss 0.661285, current_train_items 270528.
I0302 19:02:23.443897 22626471084160 run.py:483] Algo bellman_ford step 8454 current loss 0.769909, current_train_items 270560.
I0302 19:02:23.464315 22626471084160 run.py:483] Algo bellman_ford step 8455 current loss 0.252186, current_train_items 270592.
I0302 19:02:23.480615 22626471084160 run.py:483] Algo bellman_ford step 8456 current loss 0.455754, current_train_items 270624.
I0302 19:02:23.505143 22626471084160 run.py:483] Algo bellman_ford step 8457 current loss 0.625309, current_train_items 270656.
I0302 19:02:23.535372 22626471084160 run.py:483] Algo bellman_ford step 8458 current loss 0.654430, current_train_items 270688.
I0302 19:02:23.566924 22626471084160 run.py:483] Algo bellman_ford step 8459 current loss 0.734814, current_train_items 270720.
I0302 19:02:23.586946 22626471084160 run.py:483] Algo bellman_ford step 8460 current loss 0.575032, current_train_items 270752.
I0302 19:02:23.603049 22626471084160 run.py:483] Algo bellman_ford step 8461 current loss 0.475787, current_train_items 270784.
I0302 19:02:23.625643 22626471084160 run.py:483] Algo bellman_ford step 8462 current loss 0.542868, current_train_items 270816.
I0302 19:02:23.656620 22626471084160 run.py:483] Algo bellman_ford step 8463 current loss 0.559480, current_train_items 270848.
I0302 19:02:23.691216 22626471084160 run.py:483] Algo bellman_ford step 8464 current loss 0.769133, current_train_items 270880.
I0302 19:02:23.710606 22626471084160 run.py:483] Algo bellman_ford step 8465 current loss 0.309145, current_train_items 270912.
I0302 19:02:23.727260 22626471084160 run.py:483] Algo bellman_ford step 8466 current loss 0.475515, current_train_items 270944.
I0302 19:02:23.750590 22626471084160 run.py:483] Algo bellman_ford step 8467 current loss 0.552353, current_train_items 270976.
I0302 19:02:23.781395 22626471084160 run.py:483] Algo bellman_ford step 8468 current loss 0.760473, current_train_items 271008.
I0302 19:02:23.814479 22626471084160 run.py:483] Algo bellman_ford step 8469 current loss 0.625176, current_train_items 271040.
I0302 19:02:23.834506 22626471084160 run.py:483] Algo bellman_ford step 8470 current loss 0.318780, current_train_items 271072.
I0302 19:02:23.851173 22626471084160 run.py:483] Algo bellman_ford step 8471 current loss 0.540828, current_train_items 271104.
I0302 19:02:23.874749 22626471084160 run.py:483] Algo bellman_ford step 8472 current loss 0.538804, current_train_items 271136.
I0302 19:02:23.903829 22626471084160 run.py:483] Algo bellman_ford step 8473 current loss 0.538830, current_train_items 271168.
I0302 19:02:23.938174 22626471084160 run.py:483] Algo bellman_ford step 8474 current loss 0.817311, current_train_items 271200.
I0302 19:02:23.958625 22626471084160 run.py:483] Algo bellman_ford step 8475 current loss 0.283704, current_train_items 271232.
I0302 19:02:23.975174 22626471084160 run.py:483] Algo bellman_ford step 8476 current loss 0.460102, current_train_items 271264.
I0302 19:02:24.000367 22626471084160 run.py:483] Algo bellman_ford step 8477 current loss 0.607902, current_train_items 271296.
I0302 19:02:24.032128 22626471084160 run.py:483] Algo bellman_ford step 8478 current loss 0.513859, current_train_items 271328.
I0302 19:02:24.067099 22626471084160 run.py:483] Algo bellman_ford step 8479 current loss 0.634664, current_train_items 271360.
I0302 19:02:24.086372 22626471084160 run.py:483] Algo bellman_ford step 8480 current loss 0.258481, current_train_items 271392.
I0302 19:02:24.102761 22626471084160 run.py:483] Algo bellman_ford step 8481 current loss 0.582853, current_train_items 271424.
I0302 19:02:24.126836 22626471084160 run.py:483] Algo bellman_ford step 8482 current loss 0.647920, current_train_items 271456.
I0302 19:02:24.158621 22626471084160 run.py:483] Algo bellman_ford step 8483 current loss 0.690190, current_train_items 271488.
I0302 19:02:24.191446 22626471084160 run.py:483] Algo bellman_ford step 8484 current loss 0.772007, current_train_items 271520.
I0302 19:02:24.211789 22626471084160 run.py:483] Algo bellman_ford step 8485 current loss 0.340826, current_train_items 271552.
I0302 19:02:24.227770 22626471084160 run.py:483] Algo bellman_ford step 8486 current loss 0.351010, current_train_items 271584.
I0302 19:02:24.251383 22626471084160 run.py:483] Algo bellman_ford step 8487 current loss 0.635646, current_train_items 271616.
I0302 19:02:24.283609 22626471084160 run.py:483] Algo bellman_ford step 8488 current loss 0.693287, current_train_items 271648.
I0302 19:02:24.317773 22626471084160 run.py:483] Algo bellman_ford step 8489 current loss 0.698565, current_train_items 271680.
I0302 19:02:24.338011 22626471084160 run.py:483] Algo bellman_ford step 8490 current loss 0.337470, current_train_items 271712.
I0302 19:02:24.353999 22626471084160 run.py:483] Algo bellman_ford step 8491 current loss 0.512494, current_train_items 271744.
I0302 19:02:24.378487 22626471084160 run.py:483] Algo bellman_ford step 8492 current loss 0.798307, current_train_items 271776.
I0302 19:02:24.409690 22626471084160 run.py:483] Algo bellman_ford step 8493 current loss 0.687298, current_train_items 271808.
I0302 19:02:24.442324 22626471084160 run.py:483] Algo bellman_ford step 8494 current loss 0.752192, current_train_items 271840.
I0302 19:02:24.462212 22626471084160 run.py:483] Algo bellman_ford step 8495 current loss 0.266958, current_train_items 271872.
I0302 19:02:24.478782 22626471084160 run.py:483] Algo bellman_ford step 8496 current loss 0.553533, current_train_items 271904.
I0302 19:02:24.503173 22626471084160 run.py:483] Algo bellman_ford step 8497 current loss 0.492012, current_train_items 271936.
I0302 19:02:24.534914 22626471084160 run.py:483] Algo bellman_ford step 8498 current loss 0.685593, current_train_items 271968.
I0302 19:02:24.567043 22626471084160 run.py:483] Algo bellman_ford step 8499 current loss 0.661517, current_train_items 272000.
I0302 19:02:24.586737 22626471084160 run.py:483] Algo bellman_ford step 8500 current loss 0.273793, current_train_items 272032.
I0302 19:02:24.594512 22626471084160 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0302 19:02:24.594645 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:02:24.611744 22626471084160 run.py:483] Algo bellman_ford step 8501 current loss 0.435237, current_train_items 272064.
I0302 19:02:24.636975 22626471084160 run.py:483] Algo bellman_ford step 8502 current loss 0.601013, current_train_items 272096.
I0302 19:02:24.667914 22626471084160 run.py:483] Algo bellman_ford step 8503 current loss 0.578776, current_train_items 272128.
I0302 19:02:24.701418 22626471084160 run.py:483] Algo bellman_ford step 8504 current loss 0.691827, current_train_items 272160.
I0302 19:02:24.721539 22626471084160 run.py:483] Algo bellman_ford step 8505 current loss 0.291555, current_train_items 272192.
I0302 19:02:24.736867 22626471084160 run.py:483] Algo bellman_ford step 8506 current loss 0.398323, current_train_items 272224.
I0302 19:02:24.759397 22626471084160 run.py:483] Algo bellman_ford step 8507 current loss 0.564079, current_train_items 272256.
I0302 19:02:24.791433 22626471084160 run.py:483] Algo bellman_ford step 8508 current loss 0.628457, current_train_items 272288.
I0302 19:02:24.826324 22626471084160 run.py:483] Algo bellman_ford step 8509 current loss 0.754879, current_train_items 272320.
I0302 19:02:24.845905 22626471084160 run.py:483] Algo bellman_ford step 8510 current loss 0.299586, current_train_items 272352.
I0302 19:02:24.861596 22626471084160 run.py:483] Algo bellman_ford step 8511 current loss 0.427853, current_train_items 272384.
I0302 19:02:24.886548 22626471084160 run.py:483] Algo bellman_ford step 8512 current loss 0.612268, current_train_items 272416.
I0302 19:02:24.917556 22626471084160 run.py:483] Algo bellman_ford step 8513 current loss 0.728894, current_train_items 272448.
I0302 19:02:24.952055 22626471084160 run.py:483] Algo bellman_ford step 8514 current loss 0.773761, current_train_items 272480.
I0302 19:02:24.971817 22626471084160 run.py:483] Algo bellman_ford step 8515 current loss 0.287769, current_train_items 272512.
I0302 19:02:24.987881 22626471084160 run.py:483] Algo bellman_ford step 8516 current loss 0.443563, current_train_items 272544.
I0302 19:02:25.011970 22626471084160 run.py:483] Algo bellman_ford step 8517 current loss 0.563146, current_train_items 272576.
I0302 19:02:25.045039 22626471084160 run.py:483] Algo bellman_ford step 8518 current loss 0.697286, current_train_items 272608.
I0302 19:02:25.078322 22626471084160 run.py:483] Algo bellman_ford step 8519 current loss 0.776080, current_train_items 272640.
I0302 19:02:25.098060 22626471084160 run.py:483] Algo bellman_ford step 8520 current loss 0.285953, current_train_items 272672.
I0302 19:02:25.113980 22626471084160 run.py:483] Algo bellman_ford step 8521 current loss 0.449912, current_train_items 272704.
I0302 19:02:25.137873 22626471084160 run.py:483] Algo bellman_ford step 8522 current loss 0.479666, current_train_items 272736.
I0302 19:02:25.169152 22626471084160 run.py:483] Algo bellman_ford step 8523 current loss 0.802476, current_train_items 272768.
I0302 19:02:25.203586 22626471084160 run.py:483] Algo bellman_ford step 8524 current loss 0.828030, current_train_items 272800.
I0302 19:02:25.223586 22626471084160 run.py:483] Algo bellman_ford step 8525 current loss 0.294650, current_train_items 272832.
I0302 19:02:25.239884 22626471084160 run.py:483] Algo bellman_ford step 8526 current loss 0.502655, current_train_items 272864.
I0302 19:02:25.263619 22626471084160 run.py:483] Algo bellman_ford step 8527 current loss 0.726221, current_train_items 272896.
I0302 19:02:25.294650 22626471084160 run.py:483] Algo bellman_ford step 8528 current loss 0.682103, current_train_items 272928.
I0302 19:02:25.329398 22626471084160 run.py:483] Algo bellman_ford step 8529 current loss 0.864416, current_train_items 272960.
I0302 19:02:25.349083 22626471084160 run.py:483] Algo bellman_ford step 8530 current loss 0.234731, current_train_items 272992.
I0302 19:02:25.365390 22626471084160 run.py:483] Algo bellman_ford step 8531 current loss 0.467016, current_train_items 273024.
I0302 19:02:25.388015 22626471084160 run.py:483] Algo bellman_ford step 8532 current loss 0.553735, current_train_items 273056.
I0302 19:02:25.419091 22626471084160 run.py:483] Algo bellman_ford step 8533 current loss 0.978582, current_train_items 273088.
I0302 19:02:25.451896 22626471084160 run.py:483] Algo bellman_ford step 8534 current loss 0.791827, current_train_items 273120.
I0302 19:02:25.471772 22626471084160 run.py:483] Algo bellman_ford step 8535 current loss 0.223891, current_train_items 273152.
I0302 19:02:25.488002 22626471084160 run.py:483] Algo bellman_ford step 8536 current loss 0.471979, current_train_items 273184.
I0302 19:02:25.512010 22626471084160 run.py:483] Algo bellman_ford step 8537 current loss 0.544977, current_train_items 273216.
I0302 19:02:25.543438 22626471084160 run.py:483] Algo bellman_ford step 8538 current loss 0.673064, current_train_items 273248.
I0302 19:02:25.579203 22626471084160 run.py:483] Algo bellman_ford step 8539 current loss 0.884483, current_train_items 273280.
I0302 19:02:25.599219 22626471084160 run.py:483] Algo bellman_ford step 8540 current loss 0.277058, current_train_items 273312.
I0302 19:02:25.615295 22626471084160 run.py:483] Algo bellman_ford step 8541 current loss 0.539056, current_train_items 273344.
I0302 19:02:25.639476 22626471084160 run.py:483] Algo bellman_ford step 8542 current loss 0.597262, current_train_items 273376.
I0302 19:02:25.671856 22626471084160 run.py:483] Algo bellman_ford step 8543 current loss 0.624631, current_train_items 273408.
I0302 19:02:25.705333 22626471084160 run.py:483] Algo bellman_ford step 8544 current loss 0.868814, current_train_items 273440.
I0302 19:02:25.725094 22626471084160 run.py:483] Algo bellman_ford step 8545 current loss 0.275680, current_train_items 273472.
I0302 19:02:25.740866 22626471084160 run.py:483] Algo bellman_ford step 8546 current loss 0.420718, current_train_items 273504.
I0302 19:02:25.765070 22626471084160 run.py:483] Algo bellman_ford step 8547 current loss 0.841011, current_train_items 273536.
I0302 19:02:25.796374 22626471084160 run.py:483] Algo bellman_ford step 8548 current loss 0.723963, current_train_items 273568.
I0302 19:02:25.830586 22626471084160 run.py:483] Algo bellman_ford step 8549 current loss 0.903530, current_train_items 273600.
I0302 19:02:25.850389 22626471084160 run.py:483] Algo bellman_ford step 8550 current loss 0.311665, current_train_items 273632.
I0302 19:02:25.858362 22626471084160 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0302 19:02:25.858468 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:02:25.875550 22626471084160 run.py:483] Algo bellman_ford step 8551 current loss 0.499690, current_train_items 273664.
I0302 19:02:25.900486 22626471084160 run.py:483] Algo bellman_ford step 8552 current loss 0.569040, current_train_items 273696.
I0302 19:02:25.932674 22626471084160 run.py:483] Algo bellman_ford step 8553 current loss 0.749589, current_train_items 273728.
I0302 19:02:25.964885 22626471084160 run.py:483] Algo bellman_ford step 8554 current loss 0.731978, current_train_items 273760.
I0302 19:02:25.985066 22626471084160 run.py:483] Algo bellman_ford step 8555 current loss 0.377337, current_train_items 273792.
I0302 19:02:26.000887 22626471084160 run.py:483] Algo bellman_ford step 8556 current loss 0.536272, current_train_items 273824.
I0302 19:02:26.024696 22626471084160 run.py:483] Algo bellman_ford step 8557 current loss 0.642552, current_train_items 273856.
I0302 19:02:26.056244 22626471084160 run.py:483] Algo bellman_ford step 8558 current loss 0.682073, current_train_items 273888.
I0302 19:02:26.091381 22626471084160 run.py:483] Algo bellman_ford step 8559 current loss 0.791835, current_train_items 273920.
I0302 19:02:26.111348 22626471084160 run.py:483] Algo bellman_ford step 8560 current loss 0.257941, current_train_items 273952.
I0302 19:02:26.127908 22626471084160 run.py:483] Algo bellman_ford step 8561 current loss 0.403565, current_train_items 273984.
I0302 19:02:26.151572 22626471084160 run.py:483] Algo bellman_ford step 8562 current loss 0.605510, current_train_items 274016.
I0302 19:02:26.184206 22626471084160 run.py:483] Algo bellman_ford step 8563 current loss 0.721054, current_train_items 274048.
I0302 19:02:26.216708 22626471084160 run.py:483] Algo bellman_ford step 8564 current loss 0.770014, current_train_items 274080.
I0302 19:02:26.236229 22626471084160 run.py:483] Algo bellman_ford step 8565 current loss 0.304766, current_train_items 274112.
I0302 19:02:26.253048 22626471084160 run.py:483] Algo bellman_ford step 8566 current loss 0.443439, current_train_items 274144.
I0302 19:02:26.277867 22626471084160 run.py:483] Algo bellman_ford step 8567 current loss 0.593345, current_train_items 274176.
I0302 19:02:26.309660 22626471084160 run.py:483] Algo bellman_ford step 8568 current loss 0.740946, current_train_items 274208.
I0302 19:02:26.344300 22626471084160 run.py:483] Algo bellman_ford step 8569 current loss 0.855760, current_train_items 274240.
I0302 19:02:26.364143 22626471084160 run.py:483] Algo bellman_ford step 8570 current loss 0.298214, current_train_items 274272.
I0302 19:02:26.380320 22626471084160 run.py:483] Algo bellman_ford step 8571 current loss 0.546276, current_train_items 274304.
I0302 19:02:26.403405 22626471084160 run.py:483] Algo bellman_ford step 8572 current loss 0.532222, current_train_items 274336.
I0302 19:02:26.434875 22626471084160 run.py:483] Algo bellman_ford step 8573 current loss 0.606901, current_train_items 274368.
I0302 19:02:26.467407 22626471084160 run.py:483] Algo bellman_ford step 8574 current loss 0.704150, current_train_items 274400.
I0302 19:02:26.487338 22626471084160 run.py:483] Algo bellman_ford step 8575 current loss 0.349901, current_train_items 274432.
I0302 19:02:26.503449 22626471084160 run.py:483] Algo bellman_ford step 8576 current loss 0.431133, current_train_items 274464.
I0302 19:02:26.527198 22626471084160 run.py:483] Algo bellman_ford step 8577 current loss 0.545204, current_train_items 274496.
I0302 19:02:26.559359 22626471084160 run.py:483] Algo bellman_ford step 8578 current loss 0.659479, current_train_items 274528.
I0302 19:02:26.593578 22626471084160 run.py:483] Algo bellman_ford step 8579 current loss 0.707074, current_train_items 274560.
I0302 19:02:26.613162 22626471084160 run.py:483] Algo bellman_ford step 8580 current loss 0.320311, current_train_items 274592.
I0302 19:02:26.629420 22626471084160 run.py:483] Algo bellman_ford step 8581 current loss 0.423780, current_train_items 274624.
I0302 19:02:26.652259 22626471084160 run.py:483] Algo bellman_ford step 8582 current loss 0.513909, current_train_items 274656.
I0302 19:02:26.684117 22626471084160 run.py:483] Algo bellman_ford step 8583 current loss 0.661431, current_train_items 274688.
I0302 19:02:26.718923 22626471084160 run.py:483] Algo bellman_ford step 8584 current loss 0.726054, current_train_items 274720.
I0302 19:02:26.738652 22626471084160 run.py:483] Algo bellman_ford step 8585 current loss 0.283532, current_train_items 274752.
I0302 19:02:26.754786 22626471084160 run.py:483] Algo bellman_ford step 8586 current loss 0.417494, current_train_items 274784.
I0302 19:02:26.779778 22626471084160 run.py:483] Algo bellman_ford step 8587 current loss 0.616201, current_train_items 274816.
I0302 19:02:26.811298 22626471084160 run.py:483] Algo bellman_ford step 8588 current loss 0.538848, current_train_items 274848.
I0302 19:02:26.844087 22626471084160 run.py:483] Algo bellman_ford step 8589 current loss 0.691547, current_train_items 274880.
I0302 19:02:26.864004 22626471084160 run.py:483] Algo bellman_ford step 8590 current loss 0.372072, current_train_items 274912.
I0302 19:02:26.879514 22626471084160 run.py:483] Algo bellman_ford step 8591 current loss 0.419228, current_train_items 274944.
I0302 19:02:26.903105 22626471084160 run.py:483] Algo bellman_ford step 8592 current loss 0.618757, current_train_items 274976.
I0302 19:02:26.935535 22626471084160 run.py:483] Algo bellman_ford step 8593 current loss 0.718459, current_train_items 275008.
I0302 19:02:26.969332 22626471084160 run.py:483] Algo bellman_ford step 8594 current loss 0.903668, current_train_items 275040.
I0302 19:02:26.988892 22626471084160 run.py:483] Algo bellman_ford step 8595 current loss 0.271200, current_train_items 275072.
I0302 19:02:27.004810 22626471084160 run.py:483] Algo bellman_ford step 8596 current loss 0.391914, current_train_items 275104.
I0302 19:02:27.028796 22626471084160 run.py:483] Algo bellman_ford step 8597 current loss 0.542594, current_train_items 275136.
I0302 19:02:27.059025 22626471084160 run.py:483] Algo bellman_ford step 8598 current loss 0.518971, current_train_items 275168.
I0302 19:02:27.093709 22626471084160 run.py:483] Algo bellman_ford step 8599 current loss 0.727417, current_train_items 275200.
I0302 19:02:27.113588 22626471084160 run.py:483] Algo bellman_ford step 8600 current loss 0.353062, current_train_items 275232.
I0302 19:02:27.121356 22626471084160 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0302 19:02:27.121462 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 19:02:27.138378 22626471084160 run.py:483] Algo bellman_ford step 8601 current loss 0.518517, current_train_items 275264.
I0302 19:02:27.163109 22626471084160 run.py:483] Algo bellman_ford step 8602 current loss 0.679570, current_train_items 275296.
I0302 19:02:27.196011 22626471084160 run.py:483] Algo bellman_ford step 8603 current loss 0.592641, current_train_items 275328.
I0302 19:02:27.230668 22626471084160 run.py:483] Algo bellman_ford step 8604 current loss 0.746662, current_train_items 275360.
I0302 19:02:27.250666 22626471084160 run.py:483] Algo bellman_ford step 8605 current loss 0.284082, current_train_items 275392.
I0302 19:02:27.266117 22626471084160 run.py:483] Algo bellman_ford step 8606 current loss 0.375659, current_train_items 275424.
I0302 19:02:27.290017 22626471084160 run.py:483] Algo bellman_ford step 8607 current loss 0.541004, current_train_items 275456.
I0302 19:02:27.321542 22626471084160 run.py:483] Algo bellman_ford step 8608 current loss 0.612481, current_train_items 275488.
I0302 19:02:27.357214 22626471084160 run.py:483] Algo bellman_ford step 8609 current loss 0.776060, current_train_items 275520.
I0302 19:02:27.376947 22626471084160 run.py:483] Algo bellman_ford step 8610 current loss 0.272761, current_train_items 275552.
I0302 19:02:27.393244 22626471084160 run.py:483] Algo bellman_ford step 8611 current loss 0.444611, current_train_items 275584.
I0302 19:02:27.417466 22626471084160 run.py:483] Algo bellman_ford step 8612 current loss 0.604649, current_train_items 275616.
I0302 19:02:27.449217 22626471084160 run.py:483] Algo bellman_ford step 8613 current loss 0.668327, current_train_items 275648.
I0302 19:02:27.481279 22626471084160 run.py:483] Algo bellman_ford step 8614 current loss 0.640420, current_train_items 275680.
I0302 19:02:27.500844 22626471084160 run.py:483] Algo bellman_ford step 8615 current loss 0.224392, current_train_items 275712.
I0302 19:02:27.516991 22626471084160 run.py:483] Algo bellman_ford step 8616 current loss 0.427093, current_train_items 275744.
I0302 19:02:27.540983 22626471084160 run.py:483] Algo bellman_ford step 8617 current loss 0.518502, current_train_items 275776.
I0302 19:02:27.573583 22626471084160 run.py:483] Algo bellman_ford step 8618 current loss 0.645727, current_train_items 275808.
I0302 19:02:27.607429 22626471084160 run.py:483] Algo bellman_ford step 8619 current loss 0.730081, current_train_items 275840.
I0302 19:02:27.626994 22626471084160 run.py:483] Algo bellman_ford step 8620 current loss 0.319735, current_train_items 275872.
I0302 19:02:27.643244 22626471084160 run.py:483] Algo bellman_ford step 8621 current loss 0.427362, current_train_items 275904.
I0302 19:02:27.667742 22626471084160 run.py:483] Algo bellman_ford step 8622 current loss 0.637374, current_train_items 275936.
I0302 19:02:27.700142 22626471084160 run.py:483] Algo bellman_ford step 8623 current loss 0.586395, current_train_items 275968.
I0302 19:02:27.733804 22626471084160 run.py:483] Algo bellman_ford step 8624 current loss 0.659803, current_train_items 276000.
I0302 19:02:27.753456 22626471084160 run.py:483] Algo bellman_ford step 8625 current loss 0.341011, current_train_items 276032.
I0302 19:02:27.769778 22626471084160 run.py:483] Algo bellman_ford step 8626 current loss 0.478252, current_train_items 276064.
I0302 19:02:27.793854 22626471084160 run.py:483] Algo bellman_ford step 8627 current loss 0.512814, current_train_items 276096.
I0302 19:02:27.826718 22626471084160 run.py:483] Algo bellman_ford step 8628 current loss 0.731676, current_train_items 276128.
I0302 19:02:27.860043 22626471084160 run.py:483] Algo bellman_ford step 8629 current loss 0.898177, current_train_items 276160.
I0302 19:02:27.879816 22626471084160 run.py:483] Algo bellman_ford step 8630 current loss 0.312419, current_train_items 276192.
I0302 19:02:27.896118 22626471084160 run.py:483] Algo bellman_ford step 8631 current loss 0.414626, current_train_items 276224.
I0302 19:02:27.919663 22626471084160 run.py:483] Algo bellman_ford step 8632 current loss 0.691035, current_train_items 276256.
I0302 19:02:27.950930 22626471084160 run.py:483] Algo bellman_ford step 8633 current loss 0.574461, current_train_items 276288.
I0302 19:02:27.984767 22626471084160 run.py:483] Algo bellman_ford step 8634 current loss 0.720757, current_train_items 276320.
I0302 19:02:28.004744 22626471084160 run.py:483] Algo bellman_ford step 8635 current loss 0.334693, current_train_items 276352.
I0302 19:02:28.020888 22626471084160 run.py:483] Algo bellman_ford step 8636 current loss 0.393714, current_train_items 276384.
I0302 19:02:28.045559 22626471084160 run.py:483] Algo bellman_ford step 8637 current loss 0.574749, current_train_items 276416.
I0302 19:02:28.076326 22626471084160 run.py:483] Algo bellman_ford step 8638 current loss 0.567488, current_train_items 276448.
I0302 19:02:28.110663 22626471084160 run.py:483] Algo bellman_ford step 8639 current loss 0.659351, current_train_items 276480.
I0302 19:02:28.130386 22626471084160 run.py:483] Algo bellman_ford step 8640 current loss 0.295367, current_train_items 276512.
I0302 19:02:28.146922 22626471084160 run.py:483] Algo bellman_ford step 8641 current loss 0.485202, current_train_items 276544.
I0302 19:02:28.171175 22626471084160 run.py:483] Algo bellman_ford step 8642 current loss 0.508415, current_train_items 276576.
I0302 19:02:28.203270 22626471084160 run.py:483] Algo bellman_ford step 8643 current loss 0.624610, current_train_items 276608.
I0302 19:02:28.237351 22626471084160 run.py:483] Algo bellman_ford step 8644 current loss 0.730887, current_train_items 276640.
I0302 19:02:28.257009 22626471084160 run.py:483] Algo bellman_ford step 8645 current loss 0.345444, current_train_items 276672.
I0302 19:02:28.273236 22626471084160 run.py:483] Algo bellman_ford step 8646 current loss 0.421158, current_train_items 276704.
I0302 19:02:28.297486 22626471084160 run.py:483] Algo bellman_ford step 8647 current loss 0.626274, current_train_items 276736.
I0302 19:02:28.330272 22626471084160 run.py:483] Algo bellman_ford step 8648 current loss 0.636736, current_train_items 276768.
I0302 19:02:28.363604 22626471084160 run.py:483] Algo bellman_ford step 8649 current loss 0.678500, current_train_items 276800.
I0302 19:02:28.383196 22626471084160 run.py:483] Algo bellman_ford step 8650 current loss 0.406097, current_train_items 276832.
I0302 19:02:28.391302 22626471084160 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0302 19:02:28.391407 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 19:02:28.408310 22626471084160 run.py:483] Algo bellman_ford step 8651 current loss 0.385532, current_train_items 276864.
I0302 19:02:28.432642 22626471084160 run.py:483] Algo bellman_ford step 8652 current loss 0.507276, current_train_items 276896.
I0302 19:02:28.464983 22626471084160 run.py:483] Algo bellman_ford step 8653 current loss 0.580458, current_train_items 276928.
I0302 19:02:28.500091 22626471084160 run.py:483] Algo bellman_ford step 8654 current loss 0.768642, current_train_items 276960.
I0302 19:02:28.520116 22626471084160 run.py:483] Algo bellman_ford step 8655 current loss 0.257566, current_train_items 276992.
I0302 19:02:28.536027 22626471084160 run.py:483] Algo bellman_ford step 8656 current loss 0.418164, current_train_items 277024.
I0302 19:02:28.561327 22626471084160 run.py:483] Algo bellman_ford step 8657 current loss 0.612204, current_train_items 277056.
I0302 19:02:28.593763 22626471084160 run.py:483] Algo bellman_ford step 8658 current loss 0.618179, current_train_items 277088.
I0302 19:02:28.627523 22626471084160 run.py:483] Algo bellman_ford step 8659 current loss 0.676886, current_train_items 277120.
I0302 19:02:28.647448 22626471084160 run.py:483] Algo bellman_ford step 8660 current loss 0.266892, current_train_items 277152.
I0302 19:02:28.663450 22626471084160 run.py:483] Algo bellman_ford step 8661 current loss 0.417968, current_train_items 277184.
I0302 19:02:28.687008 22626471084160 run.py:483] Algo bellman_ford step 8662 current loss 0.674720, current_train_items 277216.
I0302 19:02:28.718195 22626471084160 run.py:483] Algo bellman_ford step 8663 current loss 0.695169, current_train_items 277248.
I0302 19:02:28.753422 22626471084160 run.py:483] Algo bellman_ford step 8664 current loss 0.672215, current_train_items 277280.
I0302 19:02:28.773145 22626471084160 run.py:483] Algo bellman_ford step 8665 current loss 0.313769, current_train_items 277312.
I0302 19:02:28.788839 22626471084160 run.py:483] Algo bellman_ford step 8666 current loss 0.390953, current_train_items 277344.
I0302 19:02:28.812619 22626471084160 run.py:483] Algo bellman_ford step 8667 current loss 0.509659, current_train_items 277376.
I0302 19:02:28.843710 22626471084160 run.py:483] Algo bellman_ford step 8668 current loss 0.613866, current_train_items 277408.
I0302 19:02:28.876616 22626471084160 run.py:483] Algo bellman_ford step 8669 current loss 0.628189, current_train_items 277440.
I0302 19:02:28.896594 22626471084160 run.py:483] Algo bellman_ford step 8670 current loss 0.282919, current_train_items 277472.
I0302 19:02:28.912923 22626471084160 run.py:483] Algo bellman_ford step 8671 current loss 0.480148, current_train_items 277504.
I0302 19:02:28.936302 22626471084160 run.py:483] Algo bellman_ford step 8672 current loss 0.568533, current_train_items 277536.
I0302 19:02:28.968048 22626471084160 run.py:483] Algo bellman_ford step 8673 current loss 0.603309, current_train_items 277568.
I0302 19:02:28.999930 22626471084160 run.py:483] Algo bellman_ford step 8674 current loss 0.791424, current_train_items 277600.
I0302 19:02:29.019974 22626471084160 run.py:483] Algo bellman_ford step 8675 current loss 0.319159, current_train_items 277632.
I0302 19:02:29.036044 22626471084160 run.py:483] Algo bellman_ford step 8676 current loss 0.455962, current_train_items 277664.
I0302 19:02:29.060066 22626471084160 run.py:483] Algo bellman_ford step 8677 current loss 0.598029, current_train_items 277696.
I0302 19:02:29.091975 22626471084160 run.py:483] Algo bellman_ford step 8678 current loss 0.622314, current_train_items 277728.
I0302 19:02:29.125114 22626471084160 run.py:483] Algo bellman_ford step 8679 current loss 0.740050, current_train_items 277760.
I0302 19:02:29.144683 22626471084160 run.py:483] Algo bellman_ford step 8680 current loss 0.316582, current_train_items 277792.
I0302 19:02:29.161055 22626471084160 run.py:483] Algo bellman_ford step 8681 current loss 0.487364, current_train_items 277824.
I0302 19:02:29.185316 22626471084160 run.py:483] Algo bellman_ford step 8682 current loss 0.533532, current_train_items 277856.
I0302 19:02:29.216926 22626471084160 run.py:483] Algo bellman_ford step 8683 current loss 0.568884, current_train_items 277888.
I0302 19:02:29.249922 22626471084160 run.py:483] Algo bellman_ford step 8684 current loss 0.704499, current_train_items 277920.
I0302 19:02:29.269829 22626471084160 run.py:483] Algo bellman_ford step 8685 current loss 0.334493, current_train_items 277952.
I0302 19:02:29.285782 22626471084160 run.py:483] Algo bellman_ford step 8686 current loss 0.451077, current_train_items 277984.
I0302 19:02:29.310166 22626471084160 run.py:483] Algo bellman_ford step 8687 current loss 0.642459, current_train_items 278016.
I0302 19:02:29.342275 22626471084160 run.py:483] Algo bellman_ford step 8688 current loss 0.665798, current_train_items 278048.
I0302 19:02:29.375201 22626471084160 run.py:483] Algo bellman_ford step 8689 current loss 0.640285, current_train_items 278080.
I0302 19:02:29.394956 22626471084160 run.py:483] Algo bellman_ford step 8690 current loss 0.230591, current_train_items 278112.
I0302 19:02:29.411310 22626471084160 run.py:483] Algo bellman_ford step 8691 current loss 0.465710, current_train_items 278144.
I0302 19:02:29.434688 22626471084160 run.py:483] Algo bellman_ford step 8692 current loss 0.589842, current_train_items 278176.
I0302 19:02:29.466348 22626471084160 run.py:483] Algo bellman_ford step 8693 current loss 0.670461, current_train_items 278208.
I0302 19:02:29.499660 22626471084160 run.py:483] Algo bellman_ford step 8694 current loss 0.813350, current_train_items 278240.
I0302 19:02:29.519503 22626471084160 run.py:483] Algo bellman_ford step 8695 current loss 0.305190, current_train_items 278272.
I0302 19:02:29.535382 22626471084160 run.py:483] Algo bellman_ford step 8696 current loss 0.412935, current_train_items 278304.
I0302 19:02:29.559731 22626471084160 run.py:483] Algo bellman_ford step 8697 current loss 0.609017, current_train_items 278336.
I0302 19:02:29.591589 22626471084160 run.py:483] Algo bellman_ford step 8698 current loss 0.646417, current_train_items 278368.
I0302 19:02:29.627068 22626471084160 run.py:483] Algo bellman_ford step 8699 current loss 0.714090, current_train_items 278400.
I0302 19:02:29.647140 22626471084160 run.py:483] Algo bellman_ford step 8700 current loss 0.248361, current_train_items 278432.
I0302 19:02:29.654742 22626471084160 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0302 19:02:29.654848 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.954, current avg val score is 0.955, val scores are: bellman_ford: 0.955
I0302 19:02:29.684047 22626471084160 run.py:483] Algo bellman_ford step 8701 current loss 0.417642, current_train_items 278464.
I0302 19:02:29.708128 22626471084160 run.py:483] Algo bellman_ford step 8702 current loss 0.562467, current_train_items 278496.
I0302 19:02:29.741067 22626471084160 run.py:483] Algo bellman_ford step 8703 current loss 0.680692, current_train_items 278528.
I0302 19:02:29.777096 22626471084160 run.py:483] Algo bellman_ford step 8704 current loss 0.749772, current_train_items 278560.
I0302 19:02:29.797643 22626471084160 run.py:483] Algo bellman_ford step 8705 current loss 0.288970, current_train_items 278592.
I0302 19:02:29.813464 22626471084160 run.py:483] Algo bellman_ford step 8706 current loss 0.530803, current_train_items 278624.
I0302 19:02:29.837431 22626471084160 run.py:483] Algo bellman_ford step 8707 current loss 0.558230, current_train_items 278656.
I0302 19:02:29.869938 22626471084160 run.py:483] Algo bellman_ford step 8708 current loss 0.655130, current_train_items 278688.
I0302 19:02:29.903820 22626471084160 run.py:483] Algo bellman_ford step 8709 current loss 0.786549, current_train_items 278720.
I0302 19:02:29.923437 22626471084160 run.py:483] Algo bellman_ford step 8710 current loss 0.339366, current_train_items 278752.
I0302 19:02:29.939877 22626471084160 run.py:483] Algo bellman_ford step 8711 current loss 0.515932, current_train_items 278784.
I0302 19:02:29.963732 22626471084160 run.py:483] Algo bellman_ford step 8712 current loss 0.576058, current_train_items 278816.
I0302 19:02:29.997650 22626471084160 run.py:483] Algo bellman_ford step 8713 current loss 0.785568, current_train_items 278848.
I0302 19:02:30.030407 22626471084160 run.py:483] Algo bellman_ford step 8714 current loss 0.695514, current_train_items 278880.
I0302 19:02:30.049827 22626471084160 run.py:483] Algo bellman_ford step 8715 current loss 0.262983, current_train_items 278912.
I0302 19:02:30.066307 22626471084160 run.py:483] Algo bellman_ford step 8716 current loss 0.520157, current_train_items 278944.
I0302 19:02:30.089619 22626471084160 run.py:483] Algo bellman_ford step 8717 current loss 0.535439, current_train_items 278976.
I0302 19:02:30.120556 22626471084160 run.py:483] Algo bellman_ford step 8718 current loss 0.656539, current_train_items 279008.
I0302 19:02:30.153204 22626471084160 run.py:483] Algo bellman_ford step 8719 current loss 0.857291, current_train_items 279040.
I0302 19:02:30.172928 22626471084160 run.py:483] Algo bellman_ford step 8720 current loss 0.357376, current_train_items 279072.
I0302 19:02:30.189441 22626471084160 run.py:483] Algo bellman_ford step 8721 current loss 0.511194, current_train_items 279104.
I0302 19:02:30.214289 22626471084160 run.py:483] Algo bellman_ford step 8722 current loss 0.586565, current_train_items 279136.
I0302 19:02:30.245300 22626471084160 run.py:483] Algo bellman_ford step 8723 current loss 0.640940, current_train_items 279168.
I0302 19:02:30.278593 22626471084160 run.py:483] Algo bellman_ford step 8724 current loss 0.766081, current_train_items 279200.
I0302 19:02:30.298329 22626471084160 run.py:483] Algo bellman_ford step 8725 current loss 0.302609, current_train_items 279232.
I0302 19:02:30.314681 22626471084160 run.py:483] Algo bellman_ford step 8726 current loss 0.458248, current_train_items 279264.
I0302 19:02:30.338641 22626471084160 run.py:483] Algo bellman_ford step 8727 current loss 0.597425, current_train_items 279296.
I0302 19:02:30.370043 22626471084160 run.py:483] Algo bellman_ford step 8728 current loss 0.665426, current_train_items 279328.
I0302 19:02:30.403306 22626471084160 run.py:483] Algo bellman_ford step 8729 current loss 0.881011, current_train_items 279360.
I0302 19:02:30.423234 22626471084160 run.py:483] Algo bellman_ford step 8730 current loss 0.298263, current_train_items 279392.
I0302 19:02:30.438949 22626471084160 run.py:483] Algo bellman_ford step 8731 current loss 0.394890, current_train_items 279424.
I0302 19:02:30.462868 22626471084160 run.py:483] Algo bellman_ford step 8732 current loss 0.680366, current_train_items 279456.
I0302 19:02:30.494883 22626471084160 run.py:483] Algo bellman_ford step 8733 current loss 0.766231, current_train_items 279488.
I0302 19:02:30.530167 22626471084160 run.py:483] Algo bellman_ford step 8734 current loss 0.903297, current_train_items 279520.
I0302 19:02:30.549529 22626471084160 run.py:483] Algo bellman_ford step 8735 current loss 0.361192, current_train_items 279552.
I0302 19:02:30.565720 22626471084160 run.py:483] Algo bellman_ford step 8736 current loss 0.480758, current_train_items 279584.
I0302 19:02:30.590792 22626471084160 run.py:483] Algo bellman_ford step 8737 current loss 0.565737, current_train_items 279616.
I0302 19:02:30.622344 22626471084160 run.py:483] Algo bellman_ford step 8738 current loss 0.648439, current_train_items 279648.
I0302 19:02:30.656846 22626471084160 run.py:483] Algo bellman_ford step 8739 current loss 0.833725, current_train_items 279680.
I0302 19:02:30.676953 22626471084160 run.py:483] Algo bellman_ford step 8740 current loss 0.356020, current_train_items 279712.
I0302 19:02:30.693006 22626471084160 run.py:483] Algo bellman_ford step 8741 current loss 0.492916, current_train_items 279744.
I0302 19:02:30.717275 22626471084160 run.py:483] Algo bellman_ford step 8742 current loss 0.689878, current_train_items 279776.
I0302 19:02:30.749660 22626471084160 run.py:483] Algo bellman_ford step 8743 current loss 0.685439, current_train_items 279808.
I0302 19:02:30.783867 22626471084160 run.py:483] Algo bellman_ford step 8744 current loss 0.645606, current_train_items 279840.
I0302 19:02:30.803613 22626471084160 run.py:483] Algo bellman_ford step 8745 current loss 0.320241, current_train_items 279872.
I0302 19:02:30.820029 22626471084160 run.py:483] Algo bellman_ford step 8746 current loss 0.513592, current_train_items 279904.
I0302 19:02:30.843806 22626471084160 run.py:483] Algo bellman_ford step 8747 current loss 0.710515, current_train_items 279936.
I0302 19:02:30.876747 22626471084160 run.py:483] Algo bellman_ford step 8748 current loss 0.892042, current_train_items 279968.
I0302 19:02:30.911677 22626471084160 run.py:483] Algo bellman_ford step 8749 current loss 0.931951, current_train_items 280000.
I0302 19:02:30.931342 22626471084160 run.py:483] Algo bellman_ford step 8750 current loss 0.320194, current_train_items 280032.
I0302 19:02:30.939612 22626471084160 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0302 19:02:30.939718 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:02:30.956797 22626471084160 run.py:483] Algo bellman_ford step 8751 current loss 0.502823, current_train_items 280064.
I0302 19:02:30.980940 22626471084160 run.py:483] Algo bellman_ford step 8752 current loss 0.624294, current_train_items 280096.
I0302 19:02:31.013422 22626471084160 run.py:483] Algo bellman_ford step 8753 current loss 0.770901, current_train_items 280128.
I0302 19:02:31.047846 22626471084160 run.py:483] Algo bellman_ford step 8754 current loss 0.677634, current_train_items 280160.
I0302 19:02:31.067946 22626471084160 run.py:483] Algo bellman_ford step 8755 current loss 0.296820, current_train_items 280192.
I0302 19:02:31.083710 22626471084160 run.py:483] Algo bellman_ford step 8756 current loss 0.469391, current_train_items 280224.
I0302 19:02:31.108906 22626471084160 run.py:483] Algo bellman_ford step 8757 current loss 0.657380, current_train_items 280256.
I0302 19:02:31.140278 22626471084160 run.py:483] Algo bellman_ford step 8758 current loss 0.725145, current_train_items 280288.
I0302 19:02:31.173933 22626471084160 run.py:483] Algo bellman_ford step 8759 current loss 0.824896, current_train_items 280320.
I0302 19:02:31.194084 22626471084160 run.py:483] Algo bellman_ford step 8760 current loss 0.312790, current_train_items 280352.
I0302 19:02:31.210449 22626471084160 run.py:483] Algo bellman_ford step 8761 current loss 0.513106, current_train_items 280384.
I0302 19:02:31.234625 22626471084160 run.py:483] Algo bellman_ford step 8762 current loss 0.572437, current_train_items 280416.
I0302 19:02:31.265636 22626471084160 run.py:483] Algo bellman_ford step 8763 current loss 0.645956, current_train_items 280448.
I0302 19:02:31.301234 22626471084160 run.py:483] Algo bellman_ford step 8764 current loss 0.742002, current_train_items 280480.
I0302 19:02:31.320852 22626471084160 run.py:483] Algo bellman_ford step 8765 current loss 0.281099, current_train_items 280512.
I0302 19:02:31.337454 22626471084160 run.py:483] Algo bellman_ford step 8766 current loss 0.457812, current_train_items 280544.
I0302 19:02:31.360128 22626471084160 run.py:483] Algo bellman_ford step 8767 current loss 0.541857, current_train_items 280576.
I0302 19:02:31.391119 22626471084160 run.py:483] Algo bellman_ford step 8768 current loss 0.688455, current_train_items 280608.
I0302 19:02:31.423519 22626471084160 run.py:483] Algo bellman_ford step 8769 current loss 0.611803, current_train_items 280640.
I0302 19:02:31.443530 22626471084160 run.py:483] Algo bellman_ford step 8770 current loss 0.262566, current_train_items 280672.
I0302 19:02:31.460044 22626471084160 run.py:483] Algo bellman_ford step 8771 current loss 0.410431, current_train_items 280704.
I0302 19:02:31.484087 22626471084160 run.py:483] Algo bellman_ford step 8772 current loss 0.606918, current_train_items 280736.
I0302 19:02:31.516305 22626471084160 run.py:483] Algo bellman_ford step 8773 current loss 0.706859, current_train_items 280768.
I0302 19:02:31.549304 22626471084160 run.py:483] Algo bellman_ford step 8774 current loss 0.710328, current_train_items 280800.
I0302 19:02:31.569148 22626471084160 run.py:483] Algo bellman_ford step 8775 current loss 0.357628, current_train_items 280832.
I0302 19:02:31.585371 22626471084160 run.py:483] Algo bellman_ford step 8776 current loss 0.470085, current_train_items 280864.
I0302 19:02:31.608662 22626471084160 run.py:483] Algo bellman_ford step 8777 current loss 0.569815, current_train_items 280896.
I0302 19:02:31.639918 22626471084160 run.py:483] Algo bellman_ford step 8778 current loss 0.605853, current_train_items 280928.
I0302 19:02:31.674618 22626471084160 run.py:483] Algo bellman_ford step 8779 current loss 0.688619, current_train_items 280960.
I0302 19:02:31.694272 22626471084160 run.py:483] Algo bellman_ford step 8780 current loss 0.336125, current_train_items 280992.
I0302 19:02:31.710522 22626471084160 run.py:483] Algo bellman_ford step 8781 current loss 0.536332, current_train_items 281024.
I0302 19:02:31.734454 22626471084160 run.py:483] Algo bellman_ford step 8782 current loss 0.678141, current_train_items 281056.
I0302 19:02:31.766708 22626471084160 run.py:483] Algo bellman_ford step 8783 current loss 0.732829, current_train_items 281088.
I0302 19:02:31.800914 22626471084160 run.py:483] Algo bellman_ford step 8784 current loss 0.806058, current_train_items 281120.
I0302 19:02:31.821075 22626471084160 run.py:483] Algo bellman_ford step 8785 current loss 0.291252, current_train_items 281152.
I0302 19:02:31.837302 22626471084160 run.py:483] Algo bellman_ford step 8786 current loss 0.499567, current_train_items 281184.
I0302 19:02:31.861734 22626471084160 run.py:483] Algo bellman_ford step 8787 current loss 0.677423, current_train_items 281216.
I0302 19:02:31.893063 22626471084160 run.py:483] Algo bellman_ford step 8788 current loss 0.710918, current_train_items 281248.
I0302 19:02:31.927251 22626471084160 run.py:483] Algo bellman_ford step 8789 current loss 0.888443, current_train_items 281280.
I0302 19:02:31.947529 22626471084160 run.py:483] Algo bellman_ford step 8790 current loss 0.312245, current_train_items 281312.
I0302 19:02:31.964244 22626471084160 run.py:483] Algo bellman_ford step 8791 current loss 0.553526, current_train_items 281344.
I0302 19:02:31.987641 22626471084160 run.py:483] Algo bellman_ford step 8792 current loss 0.552656, current_train_items 281376.
I0302 19:02:32.019548 22626471084160 run.py:483] Algo bellman_ford step 8793 current loss 0.656734, current_train_items 281408.
I0302 19:02:32.054424 22626471084160 run.py:483] Algo bellman_ford step 8794 current loss 0.746282, current_train_items 281440.
I0302 19:02:32.074326 22626471084160 run.py:483] Algo bellman_ford step 8795 current loss 0.287279, current_train_items 281472.
I0302 19:02:32.090560 22626471084160 run.py:483] Algo bellman_ford step 8796 current loss 0.481711, current_train_items 281504.
I0302 19:02:32.114175 22626471084160 run.py:483] Algo bellman_ford step 8797 current loss 0.573844, current_train_items 281536.
I0302 19:02:32.146255 22626471084160 run.py:483] Algo bellman_ford step 8798 current loss 0.675446, current_train_items 281568.
I0302 19:02:32.181461 22626471084160 run.py:483] Algo bellman_ford step 8799 current loss 0.748472, current_train_items 281600.
I0302 19:02:32.201179 22626471084160 run.py:483] Algo bellman_ford step 8800 current loss 0.257189, current_train_items 281632.
I0302 19:02:32.208921 22626471084160 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0302 19:02:32.209028 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:02:32.225503 22626471084160 run.py:483] Algo bellman_ford step 8801 current loss 0.442748, current_train_items 281664.
I0302 19:02:32.249911 22626471084160 run.py:483] Algo bellman_ford step 8802 current loss 0.592912, current_train_items 281696.
I0302 19:02:32.282938 22626471084160 run.py:483] Algo bellman_ford step 8803 current loss 0.668857, current_train_items 281728.
I0302 19:02:32.317297 22626471084160 run.py:483] Algo bellman_ford step 8804 current loss 0.718375, current_train_items 281760.
I0302 19:02:32.337214 22626471084160 run.py:483] Algo bellman_ford step 8805 current loss 0.262905, current_train_items 281792.
I0302 19:02:32.353173 22626471084160 run.py:483] Algo bellman_ford step 8806 current loss 0.411510, current_train_items 281824.
I0302 19:02:32.377545 22626471084160 run.py:483] Algo bellman_ford step 8807 current loss 0.583009, current_train_items 281856.
I0302 19:02:32.409054 22626471084160 run.py:483] Algo bellman_ford step 8808 current loss 0.653352, current_train_items 281888.
I0302 19:02:32.445444 22626471084160 run.py:483] Algo bellman_ford step 8809 current loss 0.732796, current_train_items 281920.
I0302 19:02:32.465128 22626471084160 run.py:483] Algo bellman_ford step 8810 current loss 0.356408, current_train_items 281952.
I0302 19:02:32.481512 22626471084160 run.py:483] Algo bellman_ford step 8811 current loss 0.445614, current_train_items 281984.
I0302 19:02:32.505376 22626471084160 run.py:483] Algo bellman_ford step 8812 current loss 0.552328, current_train_items 282016.
I0302 19:02:32.537335 22626471084160 run.py:483] Algo bellman_ford step 8813 current loss 0.608946, current_train_items 282048.
I0302 19:02:32.571539 22626471084160 run.py:483] Algo bellman_ford step 8814 current loss 0.747605, current_train_items 282080.
I0302 19:02:32.591144 22626471084160 run.py:483] Algo bellman_ford step 8815 current loss 0.302027, current_train_items 282112.
I0302 19:02:32.607451 22626471084160 run.py:483] Algo bellman_ford step 8816 current loss 0.467741, current_train_items 282144.
I0302 19:02:32.631546 22626471084160 run.py:483] Algo bellman_ford step 8817 current loss 0.510480, current_train_items 282176.
I0302 19:02:32.662141 22626471084160 run.py:483] Algo bellman_ford step 8818 current loss 0.678021, current_train_items 282208.
I0302 19:02:32.699236 22626471084160 run.py:483] Algo bellman_ford step 8819 current loss 0.885613, current_train_items 282240.
I0302 19:02:32.719100 22626471084160 run.py:483] Algo bellman_ford step 8820 current loss 0.353482, current_train_items 282272.
I0302 19:02:32.735186 22626471084160 run.py:483] Algo bellman_ford step 8821 current loss 0.442831, current_train_items 282304.
I0302 19:02:32.758822 22626471084160 run.py:483] Algo bellman_ford step 8822 current loss 0.531228, current_train_items 282336.
I0302 19:02:32.791028 22626471084160 run.py:483] Algo bellman_ford step 8823 current loss 0.681938, current_train_items 282368.
I0302 19:02:32.824209 22626471084160 run.py:483] Algo bellman_ford step 8824 current loss 0.699378, current_train_items 282400.
I0302 19:02:32.843923 22626471084160 run.py:483] Algo bellman_ford step 8825 current loss 0.240752, current_train_items 282432.
I0302 19:02:32.860280 22626471084160 run.py:483] Algo bellman_ford step 8826 current loss 0.499910, current_train_items 282464.
I0302 19:02:32.884474 22626471084160 run.py:483] Algo bellman_ford step 8827 current loss 0.592990, current_train_items 282496.
I0302 19:02:32.916352 22626471084160 run.py:483] Algo bellman_ford step 8828 current loss 0.696762, current_train_items 282528.
I0302 19:02:32.949530 22626471084160 run.py:483] Algo bellman_ford step 8829 current loss 0.740379, current_train_items 282560.
I0302 19:02:32.969108 22626471084160 run.py:483] Algo bellman_ford step 8830 current loss 0.372905, current_train_items 282592.
I0302 19:02:32.985162 22626471084160 run.py:483] Algo bellman_ford step 8831 current loss 0.479516, current_train_items 282624.
I0302 19:02:33.009973 22626471084160 run.py:483] Algo bellman_ford step 8832 current loss 0.618733, current_train_items 282656.
I0302 19:02:33.039989 22626471084160 run.py:483] Algo bellman_ford step 8833 current loss 0.580004, current_train_items 282688.
I0302 19:02:33.072545 22626471084160 run.py:483] Algo bellman_ford step 8834 current loss 0.736570, current_train_items 282720.
I0302 19:02:33.092564 22626471084160 run.py:483] Algo bellman_ford step 8835 current loss 0.320543, current_train_items 282752.
I0302 19:02:33.108629 22626471084160 run.py:483] Algo bellman_ford step 8836 current loss 0.443984, current_train_items 282784.
I0302 19:02:33.133689 22626471084160 run.py:483] Algo bellman_ford step 8837 current loss 0.579443, current_train_items 282816.
I0302 19:02:33.165555 22626471084160 run.py:483] Algo bellman_ford step 8838 current loss 0.646256, current_train_items 282848.
I0302 19:02:33.199514 22626471084160 run.py:483] Algo bellman_ford step 8839 current loss 0.741984, current_train_items 282880.
I0302 19:02:33.219477 22626471084160 run.py:483] Algo bellman_ford step 8840 current loss 0.395406, current_train_items 282912.
I0302 19:02:33.236092 22626471084160 run.py:483] Algo bellman_ford step 8841 current loss 0.484017, current_train_items 282944.
I0302 19:02:33.260379 22626471084160 run.py:483] Algo bellman_ford step 8842 current loss 0.547047, current_train_items 282976.
I0302 19:02:33.291913 22626471084160 run.py:483] Algo bellman_ford step 8843 current loss 0.626614, current_train_items 283008.
I0302 19:02:33.327555 22626471084160 run.py:483] Algo bellman_ford step 8844 current loss 0.808897, current_train_items 283040.
I0302 19:02:33.347130 22626471084160 run.py:483] Algo bellman_ford step 8845 current loss 0.411910, current_train_items 283072.
I0302 19:02:33.363219 22626471084160 run.py:483] Algo bellman_ford step 8846 current loss 0.412048, current_train_items 283104.
I0302 19:02:33.387100 22626471084160 run.py:483] Algo bellman_ford step 8847 current loss 0.528217, current_train_items 283136.
I0302 19:02:33.417610 22626471084160 run.py:483] Algo bellman_ford step 8848 current loss 0.582754, current_train_items 283168.
I0302 19:02:33.451473 22626471084160 run.py:483] Algo bellman_ford step 8849 current loss 0.725847, current_train_items 283200.
I0302 19:02:33.470938 22626471084160 run.py:483] Algo bellman_ford step 8850 current loss 0.319627, current_train_items 283232.
I0302 19:02:33.479048 22626471084160 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.94921875, 'score': 0.94921875, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0302 19:02:33.479152 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.955, current avg val score is 0.949, val scores are: bellman_ford: 0.949
I0302 19:02:33.496007 22626471084160 run.py:483] Algo bellman_ford step 8851 current loss 0.426890, current_train_items 283264.
I0302 19:02:33.520895 22626471084160 run.py:483] Algo bellman_ford step 8852 current loss 0.536985, current_train_items 283296.
I0302 19:02:33.553320 22626471084160 run.py:483] Algo bellman_ford step 8853 current loss 0.580682, current_train_items 283328.
I0302 19:02:33.587586 22626471084160 run.py:483] Algo bellman_ford step 8854 current loss 0.749385, current_train_items 283360.
I0302 19:02:33.607772 22626471084160 run.py:483] Algo bellman_ford step 8855 current loss 0.412853, current_train_items 283392.
I0302 19:02:33.623936 22626471084160 run.py:483] Algo bellman_ford step 8856 current loss 0.452465, current_train_items 283424.
I0302 19:02:33.647475 22626471084160 run.py:483] Algo bellman_ford step 8857 current loss 0.577506, current_train_items 283456.
I0302 19:02:33.679651 22626471084160 run.py:483] Algo bellman_ford step 8858 current loss 0.604123, current_train_items 283488.
I0302 19:02:33.712077 22626471084160 run.py:483] Algo bellman_ford step 8859 current loss 0.686962, current_train_items 283520.
I0302 19:02:33.732057 22626471084160 run.py:483] Algo bellman_ford step 8860 current loss 0.289788, current_train_items 283552.
I0302 19:02:33.748517 22626471084160 run.py:483] Algo bellman_ford step 8861 current loss 0.478604, current_train_items 283584.
I0302 19:02:33.772501 22626471084160 run.py:483] Algo bellman_ford step 8862 current loss 0.563457, current_train_items 283616.
I0302 19:02:33.803109 22626471084160 run.py:483] Algo bellman_ford step 8863 current loss 0.604441, current_train_items 283648.
I0302 19:02:33.836022 22626471084160 run.py:483] Algo bellman_ford step 8864 current loss 0.697010, current_train_items 283680.
I0302 19:02:33.855942 22626471084160 run.py:483] Algo bellman_ford step 8865 current loss 0.322791, current_train_items 283712.
I0302 19:02:33.871932 22626471084160 run.py:483] Algo bellman_ford step 8866 current loss 0.429399, current_train_items 283744.
I0302 19:02:33.894991 22626471084160 run.py:483] Algo bellman_ford step 8867 current loss 0.661888, current_train_items 283776.
I0302 19:02:33.927096 22626471084160 run.py:483] Algo bellman_ford step 8868 current loss 0.679254, current_train_items 283808.
I0302 19:02:33.960327 22626471084160 run.py:483] Algo bellman_ford step 8869 current loss 0.682804, current_train_items 283840.
I0302 19:02:33.980557 22626471084160 run.py:483] Algo bellman_ford step 8870 current loss 0.328179, current_train_items 283872.
I0302 19:02:33.996735 22626471084160 run.py:483] Algo bellman_ford step 8871 current loss 0.490653, current_train_items 283904.
I0302 19:02:34.020790 22626471084160 run.py:483] Algo bellman_ford step 8872 current loss 0.728011, current_train_items 283936.
I0302 19:02:34.051921 22626471084160 run.py:483] Algo bellman_ford step 8873 current loss 0.658269, current_train_items 283968.
I0302 19:02:34.086807 22626471084160 run.py:483] Algo bellman_ford step 8874 current loss 1.041975, current_train_items 284000.
I0302 19:02:34.106810 22626471084160 run.py:483] Algo bellman_ford step 8875 current loss 0.319054, current_train_items 284032.
I0302 19:02:34.123126 22626471084160 run.py:483] Algo bellman_ford step 8876 current loss 0.425716, current_train_items 284064.
I0302 19:02:34.146200 22626471084160 run.py:483] Algo bellman_ford step 8877 current loss 0.595170, current_train_items 284096.
I0302 19:02:34.177301 22626471084160 run.py:483] Algo bellman_ford step 8878 current loss 0.662116, current_train_items 284128.
I0302 19:02:34.211433 22626471084160 run.py:483] Algo bellman_ford step 8879 current loss 0.845150, current_train_items 284160.
I0302 19:02:34.231378 22626471084160 run.py:483] Algo bellman_ford step 8880 current loss 0.339098, current_train_items 284192.
I0302 19:02:34.247502 22626471084160 run.py:483] Algo bellman_ford step 8881 current loss 0.496306, current_train_items 284224.
I0302 19:02:34.270765 22626471084160 run.py:483] Algo bellman_ford step 8882 current loss 0.568827, current_train_items 284256.
I0302 19:02:34.303698 22626471084160 run.py:483] Algo bellman_ford step 8883 current loss 0.638201, current_train_items 284288.
I0302 19:02:34.337839 22626471084160 run.py:483] Algo bellman_ford step 8884 current loss 0.725240, current_train_items 284320.
I0302 19:02:34.357936 22626471084160 run.py:483] Algo bellman_ford step 8885 current loss 0.312718, current_train_items 284352.
I0302 19:02:34.374582 22626471084160 run.py:483] Algo bellman_ford step 8886 current loss 0.460163, current_train_items 284384.
I0302 19:02:34.398437 22626471084160 run.py:483] Algo bellman_ford step 8887 current loss 0.558794, current_train_items 284416.
I0302 19:02:34.429294 22626471084160 run.py:483] Algo bellman_ford step 8888 current loss 0.710377, current_train_items 284448.
I0302 19:02:34.463960 22626471084160 run.py:483] Algo bellman_ford step 8889 current loss 0.864513, current_train_items 284480.
I0302 19:02:34.484013 22626471084160 run.py:483] Algo bellman_ford step 8890 current loss 0.289526, current_train_items 284512.
I0302 19:02:34.499899 22626471084160 run.py:483] Algo bellman_ford step 8891 current loss 0.448391, current_train_items 284544.
I0302 19:02:34.523533 22626471084160 run.py:483] Algo bellman_ford step 8892 current loss 0.514730, current_train_items 284576.
I0302 19:02:34.556692 22626471084160 run.py:483] Algo bellman_ford step 8893 current loss 0.682791, current_train_items 284608.
I0302 19:02:34.590504 22626471084160 run.py:483] Algo bellman_ford step 8894 current loss 0.840533, current_train_items 284640.
I0302 19:02:34.610369 22626471084160 run.py:483] Algo bellman_ford step 8895 current loss 0.394507, current_train_items 284672.
I0302 19:02:34.626450 22626471084160 run.py:483] Algo bellman_ford step 8896 current loss 0.499887, current_train_items 284704.
I0302 19:02:34.649226 22626471084160 run.py:483] Algo bellman_ford step 8897 current loss 0.544731, current_train_items 284736.
I0302 19:02:34.681953 22626471084160 run.py:483] Algo bellman_ford step 8898 current loss 0.625496, current_train_items 284768.
I0302 19:02:34.717241 22626471084160 run.py:483] Algo bellman_ford step 8899 current loss 0.714743, current_train_items 284800.
I0302 19:02:34.737221 22626471084160 run.py:483] Algo bellman_ford step 8900 current loss 0.307154, current_train_items 284832.
I0302 19:02:34.744958 22626471084160 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.95703125, 'score': 0.95703125, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0302 19:02:34.745063 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.955, current avg val score is 0.957, val scores are: bellman_ford: 0.957
I0302 19:02:34.774899 22626471084160 run.py:483] Algo bellman_ford step 8901 current loss 0.436779, current_train_items 284864.
I0302 19:02:34.799363 22626471084160 run.py:483] Algo bellman_ford step 8902 current loss 0.584994, current_train_items 284896.
I0302 19:02:34.830391 22626471084160 run.py:483] Algo bellman_ford step 8903 current loss 0.693045, current_train_items 284928.
I0302 19:02:34.866793 22626471084160 run.py:483] Algo bellman_ford step 8904 current loss 0.851255, current_train_items 284960.
I0302 19:02:34.887233 22626471084160 run.py:483] Algo bellman_ford step 8905 current loss 0.333034, current_train_items 284992.
I0302 19:02:34.903385 22626471084160 run.py:483] Algo bellman_ford step 8906 current loss 0.398685, current_train_items 285024.
I0302 19:02:34.927388 22626471084160 run.py:483] Algo bellman_ford step 8907 current loss 0.510590, current_train_items 285056.
I0302 19:02:34.958697 22626471084160 run.py:483] Algo bellman_ford step 8908 current loss 0.619656, current_train_items 285088.
I0302 19:02:34.990876 22626471084160 run.py:483] Algo bellman_ford step 8909 current loss 0.641214, current_train_items 285120.
I0302 19:02:35.010700 22626471084160 run.py:483] Algo bellman_ford step 8910 current loss 0.334712, current_train_items 285152.
I0302 19:02:35.026782 22626471084160 run.py:483] Algo bellman_ford step 8911 current loss 0.445968, current_train_items 285184.
I0302 19:02:35.051132 22626471084160 run.py:483] Algo bellman_ford step 8912 current loss 0.561250, current_train_items 285216.
I0302 19:02:35.081530 22626471084160 run.py:483] Algo bellman_ford step 8913 current loss 0.525828, current_train_items 285248.
I0302 19:02:35.116547 22626471084160 run.py:483] Algo bellman_ford step 8914 current loss 0.733705, current_train_items 285280.
I0302 19:02:35.136570 22626471084160 run.py:483] Algo bellman_ford step 8915 current loss 0.314673, current_train_items 285312.
I0302 19:02:35.152464 22626471084160 run.py:483] Algo bellman_ford step 8916 current loss 0.447381, current_train_items 285344.
I0302 19:02:35.176763 22626471084160 run.py:483] Algo bellman_ford step 8917 current loss 0.619599, current_train_items 285376.
I0302 19:02:35.208287 22626471084160 run.py:483] Algo bellman_ford step 8918 current loss 0.594716, current_train_items 285408.
I0302 19:02:35.241780 22626471084160 run.py:483] Algo bellman_ford step 8919 current loss 0.717154, current_train_items 285440.
I0302 19:02:35.261622 22626471084160 run.py:483] Algo bellman_ford step 8920 current loss 0.305053, current_train_items 285472.
I0302 19:02:35.278019 22626471084160 run.py:483] Algo bellman_ford step 8921 current loss 0.493854, current_train_items 285504.
I0302 19:02:35.302610 22626471084160 run.py:483] Algo bellman_ford step 8922 current loss 0.636838, current_train_items 285536.
I0302 19:02:35.333675 22626471084160 run.py:483] Algo bellman_ford step 8923 current loss 0.589507, current_train_items 285568.
I0302 19:02:35.368146 22626471084160 run.py:483] Algo bellman_ford step 8924 current loss 0.774797, current_train_items 285600.
I0302 19:02:35.387835 22626471084160 run.py:483] Algo bellman_ford step 8925 current loss 0.235249, current_train_items 285632.
I0302 19:02:35.403617 22626471084160 run.py:483] Algo bellman_ford step 8926 current loss 0.507658, current_train_items 285664.
I0302 19:02:35.428046 22626471084160 run.py:483] Algo bellman_ford step 8927 current loss 0.646295, current_train_items 285696.
I0302 19:02:35.458577 22626471084160 run.py:483] Algo bellman_ford step 8928 current loss 0.617219, current_train_items 285728.
I0302 19:02:35.492477 22626471084160 run.py:483] Algo bellman_ford step 8929 current loss 0.780184, current_train_items 285760.
I0302 19:02:35.512151 22626471084160 run.py:483] Algo bellman_ford step 8930 current loss 0.308698, current_train_items 285792.
I0302 19:02:35.528902 22626471084160 run.py:483] Algo bellman_ford step 8931 current loss 0.479318, current_train_items 285824.
I0302 19:02:35.552832 22626471084160 run.py:483] Algo bellman_ford step 8932 current loss 0.530926, current_train_items 285856.
I0302 19:02:35.585819 22626471084160 run.py:483] Algo bellman_ford step 8933 current loss 0.670808, current_train_items 285888.
I0302 19:02:35.620129 22626471084160 run.py:483] Algo bellman_ford step 8934 current loss 0.694942, current_train_items 285920.
I0302 19:02:35.640100 22626471084160 run.py:483] Algo bellman_ford step 8935 current loss 0.287991, current_train_items 285952.
I0302 19:02:35.655992 22626471084160 run.py:483] Algo bellman_ford step 8936 current loss 0.448462, current_train_items 285984.
I0302 19:02:35.679869 22626471084160 run.py:483] Algo bellman_ford step 8937 current loss 0.625046, current_train_items 286016.
I0302 19:02:35.710845 22626471084160 run.py:483] Algo bellman_ford step 8938 current loss 0.641697, current_train_items 286048.
I0302 19:02:35.743510 22626471084160 run.py:483] Algo bellman_ford step 8939 current loss 0.632880, current_train_items 286080.
I0302 19:02:35.763209 22626471084160 run.py:483] Algo bellman_ford step 8940 current loss 0.308867, current_train_items 286112.
I0302 19:02:35.779257 22626471084160 run.py:483] Algo bellman_ford step 8941 current loss 0.426163, current_train_items 286144.
I0302 19:02:35.803678 22626471084160 run.py:483] Algo bellman_ford step 8942 current loss 0.598002, current_train_items 286176.
I0302 19:02:35.836324 22626471084160 run.py:483] Algo bellman_ford step 8943 current loss 0.667193, current_train_items 286208.
I0302 19:02:35.870139 22626471084160 run.py:483] Algo bellman_ford step 8944 current loss 0.721353, current_train_items 286240.
I0302 19:02:35.889670 22626471084160 run.py:483] Algo bellman_ford step 8945 current loss 0.355551, current_train_items 286272.
I0302 19:02:35.905810 22626471084160 run.py:483] Algo bellman_ford step 8946 current loss 0.426639, current_train_items 286304.
I0302 19:02:35.929203 22626471084160 run.py:483] Algo bellman_ford step 8947 current loss 0.617986, current_train_items 286336.
I0302 19:02:35.961249 22626471084160 run.py:483] Algo bellman_ford step 8948 current loss 0.810852, current_train_items 286368.
I0302 19:02:35.994668 22626471084160 run.py:483] Algo bellman_ford step 8949 current loss 0.777492, current_train_items 286400.
I0302 19:02:36.014047 22626471084160 run.py:483] Algo bellman_ford step 8950 current loss 0.342788, current_train_items 286432.
I0302 19:02:36.022212 22626471084160 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0302 19:02:36.022315 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:02:36.039466 22626471084160 run.py:483] Algo bellman_ford step 8951 current loss 0.466183, current_train_items 286464.
I0302 19:02:36.063535 22626471084160 run.py:483] Algo bellman_ford step 8952 current loss 0.523949, current_train_items 286496.
I0302 19:02:36.098031 22626471084160 run.py:483] Algo bellman_ford step 8953 current loss 0.696640, current_train_items 286528.
I0302 19:02:36.133670 22626471084160 run.py:483] Algo bellman_ford step 8954 current loss 0.752067, current_train_items 286560.
I0302 19:02:36.153785 22626471084160 run.py:483] Algo bellman_ford step 8955 current loss 0.369457, current_train_items 286592.
I0302 19:02:36.170175 22626471084160 run.py:483] Algo bellman_ford step 8956 current loss 0.524861, current_train_items 286624.
I0302 19:02:36.194800 22626471084160 run.py:483] Algo bellman_ford step 8957 current loss 0.716367, current_train_items 286656.
I0302 19:02:36.225980 22626471084160 run.py:483] Algo bellman_ford step 8958 current loss 0.686117, current_train_items 286688.
I0302 19:02:36.260916 22626471084160 run.py:483] Algo bellman_ford step 8959 current loss 0.792619, current_train_items 286720.
I0302 19:02:36.280980 22626471084160 run.py:483] Algo bellman_ford step 8960 current loss 0.292199, current_train_items 286752.
I0302 19:02:36.296762 22626471084160 run.py:483] Algo bellman_ford step 8961 current loss 0.413198, current_train_items 286784.
I0302 19:02:36.321602 22626471084160 run.py:483] Algo bellman_ford step 8962 current loss 0.533349, current_train_items 286816.
I0302 19:02:36.353369 22626471084160 run.py:483] Algo bellman_ford step 8963 current loss 0.586421, current_train_items 286848.
I0302 19:02:36.385882 22626471084160 run.py:483] Algo bellman_ford step 8964 current loss 0.756309, current_train_items 286880.
I0302 19:02:36.405586 22626471084160 run.py:483] Algo bellman_ford step 8965 current loss 0.285819, current_train_items 286912.
I0302 19:02:36.421871 22626471084160 run.py:483] Algo bellman_ford step 8966 current loss 0.464717, current_train_items 286944.
I0302 19:02:36.445873 22626471084160 run.py:483] Algo bellman_ford step 8967 current loss 0.611046, current_train_items 286976.
I0302 19:02:36.477929 22626471084160 run.py:483] Algo bellman_ford step 8968 current loss 0.613075, current_train_items 287008.
I0302 19:02:36.511634 22626471084160 run.py:483] Algo bellman_ford step 8969 current loss 0.642039, current_train_items 287040.
I0302 19:02:36.531233 22626471084160 run.py:483] Algo bellman_ford step 8970 current loss 0.305468, current_train_items 287072.
I0302 19:02:36.547382 22626471084160 run.py:483] Algo bellman_ford step 8971 current loss 0.430930, current_train_items 287104.
I0302 19:02:36.571167 22626471084160 run.py:483] Algo bellman_ford step 8972 current loss 0.631537, current_train_items 287136.
I0302 19:02:36.602335 22626471084160 run.py:483] Algo bellman_ford step 8973 current loss 0.626392, current_train_items 287168.
I0302 19:02:36.635466 22626471084160 run.py:483] Algo bellman_ford step 8974 current loss 0.647611, current_train_items 287200.
I0302 19:02:36.655544 22626471084160 run.py:483] Algo bellman_ford step 8975 current loss 0.354081, current_train_items 287232.
I0302 19:02:36.671505 22626471084160 run.py:483] Algo bellman_ford step 8976 current loss 0.410779, current_train_items 287264.
I0302 19:02:36.694386 22626471084160 run.py:483] Algo bellman_ford step 8977 current loss 0.581742, current_train_items 287296.
I0302 19:02:36.726548 22626471084160 run.py:483] Algo bellman_ford step 8978 current loss 0.650686, current_train_items 287328.
I0302 19:02:36.759867 22626471084160 run.py:483] Algo bellman_ford step 8979 current loss 0.770190, current_train_items 287360.
I0302 19:02:36.779422 22626471084160 run.py:483] Algo bellman_ford step 8980 current loss 0.300563, current_train_items 287392.
I0302 19:02:36.795813 22626471084160 run.py:483] Algo bellman_ford step 8981 current loss 0.450422, current_train_items 287424.
I0302 19:02:36.819783 22626471084160 run.py:483] Algo bellman_ford step 8982 current loss 0.508002, current_train_items 287456.
I0302 19:02:36.851753 22626471084160 run.py:483] Algo bellman_ford step 8983 current loss 0.613229, current_train_items 287488.
I0302 19:02:36.885622 22626471084160 run.py:483] Algo bellman_ford step 8984 current loss 0.651731, current_train_items 287520.
I0302 19:02:36.905719 22626471084160 run.py:483] Algo bellman_ford step 8985 current loss 0.239728, current_train_items 287552.
I0302 19:02:36.921862 22626471084160 run.py:483] Algo bellman_ford step 8986 current loss 0.407221, current_train_items 287584.
I0302 19:02:36.944772 22626471084160 run.py:483] Algo bellman_ford step 8987 current loss 0.633200, current_train_items 287616.
I0302 19:02:36.974913 22626471084160 run.py:483] Algo bellman_ford step 8988 current loss 0.705346, current_train_items 287648.
I0302 19:02:37.010733 22626471084160 run.py:483] Algo bellman_ford step 8989 current loss 0.790411, current_train_items 287680.
I0302 19:02:37.030786 22626471084160 run.py:483] Algo bellman_ford step 8990 current loss 0.279670, current_train_items 287712.
I0302 19:02:37.046633 22626471084160 run.py:483] Algo bellman_ford step 8991 current loss 0.418883, current_train_items 287744.
I0302 19:02:37.069796 22626471084160 run.py:483] Algo bellman_ford step 8992 current loss 0.576946, current_train_items 287776.
I0302 19:02:37.101080 22626471084160 run.py:483] Algo bellman_ford step 8993 current loss 0.675750, current_train_items 287808.
I0302 19:02:37.134656 22626471084160 run.py:483] Algo bellman_ford step 8994 current loss 0.760371, current_train_items 287840.
I0302 19:02:37.153914 22626471084160 run.py:483] Algo bellman_ford step 8995 current loss 0.276380, current_train_items 287872.
I0302 19:02:37.170134 22626471084160 run.py:483] Algo bellman_ford step 8996 current loss 0.391911, current_train_items 287904.
I0302 19:02:37.194776 22626471084160 run.py:483] Algo bellman_ford step 8997 current loss 0.559634, current_train_items 287936.
I0302 19:02:37.227841 22626471084160 run.py:483] Algo bellman_ford step 8998 current loss 0.684108, current_train_items 287968.
I0302 19:02:37.260320 22626471084160 run.py:483] Algo bellman_ford step 8999 current loss 0.788371, current_train_items 288000.
I0302 19:02:37.280660 22626471084160 run.py:483] Algo bellman_ford step 9000 current loss 0.288773, current_train_items 288032.
I0302 19:02:37.288331 22626471084160 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0302 19:02:37.288436 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0302 19:02:37.305500 22626471084160 run.py:483] Algo bellman_ford step 9001 current loss 0.443413, current_train_items 288064.
I0302 19:02:37.329112 22626471084160 run.py:483] Algo bellman_ford step 9002 current loss 0.626312, current_train_items 288096.
I0302 19:02:37.360444 22626471084160 run.py:483] Algo bellman_ford step 9003 current loss 0.706551, current_train_items 288128.
I0302 19:02:37.394954 22626471084160 run.py:483] Algo bellman_ford step 9004 current loss 0.776076, current_train_items 288160.
I0302 19:02:37.415063 22626471084160 run.py:483] Algo bellman_ford step 9005 current loss 0.307836, current_train_items 288192.
I0302 19:02:37.431187 22626471084160 run.py:483] Algo bellman_ford step 9006 current loss 0.391829, current_train_items 288224.
I0302 19:02:37.454303 22626471084160 run.py:483] Algo bellman_ford step 9007 current loss 0.588423, current_train_items 288256.
I0302 19:02:37.486692 22626471084160 run.py:483] Algo bellman_ford step 9008 current loss 0.749839, current_train_items 288288.
I0302 19:02:37.520975 22626471084160 run.py:483] Algo bellman_ford step 9009 current loss 0.708667, current_train_items 288320.
I0302 19:02:37.540818 22626471084160 run.py:483] Algo bellman_ford step 9010 current loss 0.340376, current_train_items 288352.
I0302 19:02:37.556974 22626471084160 run.py:483] Algo bellman_ford step 9011 current loss 0.436399, current_train_items 288384.
I0302 19:02:37.580332 22626471084160 run.py:483] Algo bellman_ford step 9012 current loss 0.605689, current_train_items 288416.
I0302 19:02:37.611331 22626471084160 run.py:483] Algo bellman_ford step 9013 current loss 0.604517, current_train_items 288448.
I0302 19:02:37.647013 22626471084160 run.py:483] Algo bellman_ford step 9014 current loss 0.851726, current_train_items 288480.
I0302 19:02:37.666918 22626471084160 run.py:483] Algo bellman_ford step 9015 current loss 0.288346, current_train_items 288512.
I0302 19:02:37.683281 22626471084160 run.py:483] Algo bellman_ford step 9016 current loss 0.490352, current_train_items 288544.
I0302 19:02:37.707792 22626471084160 run.py:483] Algo bellman_ford step 9017 current loss 0.663364, current_train_items 288576.
I0302 19:02:37.739600 22626471084160 run.py:483] Algo bellman_ford step 9018 current loss 0.603556, current_train_items 288608.
I0302 19:02:37.774925 22626471084160 run.py:483] Algo bellman_ford step 9019 current loss 0.665358, current_train_items 288640.
I0302 19:02:37.794234 22626471084160 run.py:483] Algo bellman_ford step 9020 current loss 0.355922, current_train_items 288672.
I0302 19:02:37.810387 22626471084160 run.py:483] Algo bellman_ford step 9021 current loss 0.483581, current_train_items 288704.
I0302 19:02:37.835650 22626471084160 run.py:483] Algo bellman_ford step 9022 current loss 0.582339, current_train_items 288736.
I0302 19:02:37.868334 22626471084160 run.py:483] Algo bellman_ford step 9023 current loss 0.693998, current_train_items 288768.
I0302 19:02:37.902045 22626471084160 run.py:483] Algo bellman_ford step 9024 current loss 0.850178, current_train_items 288800.
I0302 19:02:37.921769 22626471084160 run.py:483] Algo bellman_ford step 9025 current loss 0.312651, current_train_items 288832.
I0302 19:02:37.937664 22626471084160 run.py:483] Algo bellman_ford step 9026 current loss 0.325480, current_train_items 288864.
I0302 19:02:37.962173 22626471084160 run.py:483] Algo bellman_ford step 9027 current loss 0.537432, current_train_items 288896.
I0302 19:02:37.994350 22626471084160 run.py:483] Algo bellman_ford step 9028 current loss 0.609212, current_train_items 288928.
I0302 19:02:38.028265 22626471084160 run.py:483] Algo bellman_ford step 9029 current loss 0.736763, current_train_items 288960.
I0302 19:02:38.048216 22626471084160 run.py:483] Algo bellman_ford step 9030 current loss 0.294766, current_train_items 288992.
I0302 19:02:38.064377 22626471084160 run.py:483] Algo bellman_ford step 9031 current loss 0.438089, current_train_items 289024.
I0302 19:02:38.088847 22626471084160 run.py:483] Algo bellman_ford step 9032 current loss 0.697413, current_train_items 289056.
I0302 19:02:38.120906 22626471084160 run.py:483] Algo bellman_ford step 9033 current loss 0.611109, current_train_items 289088.
I0302 19:02:38.157363 22626471084160 run.py:483] Algo bellman_ford step 9034 current loss 0.840506, current_train_items 289120.
I0302 19:02:38.176917 22626471084160 run.py:483] Algo bellman_ford step 9035 current loss 0.332948, current_train_items 289152.
I0302 19:02:38.192742 22626471084160 run.py:483] Algo bellman_ford step 9036 current loss 0.541211, current_train_items 289184.
I0302 19:02:38.217540 22626471084160 run.py:483] Algo bellman_ford step 9037 current loss 0.554383, current_train_items 289216.
I0302 19:02:38.249232 22626471084160 run.py:483] Algo bellman_ford step 9038 current loss 0.680527, current_train_items 289248.
I0302 19:02:38.283055 22626471084160 run.py:483] Algo bellman_ford step 9039 current loss 0.663795, current_train_items 289280.
I0302 19:02:38.302500 22626471084160 run.py:483] Algo bellman_ford step 9040 current loss 0.291514, current_train_items 289312.
I0302 19:02:38.318944 22626471084160 run.py:483] Algo bellman_ford step 9041 current loss 0.396838, current_train_items 289344.
I0302 19:02:38.342248 22626471084160 run.py:483] Algo bellman_ford step 9042 current loss 0.523048, current_train_items 289376.
I0302 19:02:38.373338 22626471084160 run.py:483] Algo bellman_ford step 9043 current loss 0.629655, current_train_items 289408.
I0302 19:02:38.407575 22626471084160 run.py:483] Algo bellman_ford step 9044 current loss 0.754070, current_train_items 289440.
I0302 19:02:38.427489 22626471084160 run.py:483] Algo bellman_ford step 9045 current loss 0.316908, current_train_items 289472.
I0302 19:02:38.443997 22626471084160 run.py:483] Algo bellman_ford step 9046 current loss 0.476408, current_train_items 289504.
I0302 19:02:38.466681 22626471084160 run.py:483] Algo bellman_ford step 9047 current loss 0.665339, current_train_items 289536.
I0302 19:02:38.498523 22626471084160 run.py:483] Algo bellman_ford step 9048 current loss 0.684575, current_train_items 289568.
I0302 19:02:38.531987 22626471084160 run.py:483] Algo bellman_ford step 9049 current loss 0.725844, current_train_items 289600.
I0302 19:02:38.551545 22626471084160 run.py:483] Algo bellman_ford step 9050 current loss 0.274204, current_train_items 289632.
I0302 19:02:38.559526 22626471084160 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0302 19:02:38.559631 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:02:38.576175 22626471084160 run.py:483] Algo bellman_ford step 9051 current loss 0.432715, current_train_items 289664.
I0302 19:02:38.601240 22626471084160 run.py:483] Algo bellman_ford step 9052 current loss 0.796552, current_train_items 289696.
I0302 19:02:38.633590 22626471084160 run.py:483] Algo bellman_ford step 9053 current loss 0.804124, current_train_items 289728.
I0302 19:02:38.666940 22626471084160 run.py:483] Algo bellman_ford step 9054 current loss 0.884425, current_train_items 289760.
I0302 19:02:38.687144 22626471084160 run.py:483] Algo bellman_ford step 9055 current loss 0.280522, current_train_items 289792.
I0302 19:02:38.702944 22626471084160 run.py:483] Algo bellman_ford step 9056 current loss 0.461022, current_train_items 289824.
I0302 19:02:38.726749 22626471084160 run.py:483] Algo bellman_ford step 9057 current loss 0.574291, current_train_items 289856.
I0302 19:02:38.756975 22626471084160 run.py:483] Algo bellman_ford step 9058 current loss 0.595414, current_train_items 289888.
I0302 19:02:38.788918 22626471084160 run.py:483] Algo bellman_ford step 9059 current loss 0.648758, current_train_items 289920.
I0302 19:02:38.808591 22626471084160 run.py:483] Algo bellman_ford step 9060 current loss 0.377337, current_train_items 289952.
I0302 19:02:38.825356 22626471084160 run.py:483] Algo bellman_ford step 9061 current loss 0.491556, current_train_items 289984.
I0302 19:02:38.850339 22626471084160 run.py:483] Algo bellman_ford step 9062 current loss 0.603226, current_train_items 290016.
I0302 19:02:38.880679 22626471084160 run.py:483] Algo bellman_ford step 9063 current loss 0.606627, current_train_items 290048.
I0302 19:02:38.915717 22626471084160 run.py:483] Algo bellman_ford step 9064 current loss 0.751901, current_train_items 290080.
I0302 19:02:38.935723 22626471084160 run.py:483] Algo bellman_ford step 9065 current loss 0.290548, current_train_items 290112.
I0302 19:02:38.951951 22626471084160 run.py:483] Algo bellman_ford step 9066 current loss 0.510192, current_train_items 290144.
I0302 19:02:38.973950 22626471084160 run.py:483] Algo bellman_ford step 9067 current loss 0.493361, current_train_items 290176.
I0302 19:02:39.005827 22626471084160 run.py:483] Algo bellman_ford step 9068 current loss 0.578834, current_train_items 290208.
I0302 19:02:39.040703 22626471084160 run.py:483] Algo bellman_ford step 9069 current loss 0.734946, current_train_items 290240.
I0302 19:02:39.060830 22626471084160 run.py:483] Algo bellman_ford step 9070 current loss 0.270112, current_train_items 290272.
I0302 19:02:39.077291 22626471084160 run.py:483] Algo bellman_ford step 9071 current loss 0.500405, current_train_items 290304.
I0302 19:02:39.101762 22626471084160 run.py:483] Algo bellman_ford step 9072 current loss 0.600956, current_train_items 290336.
I0302 19:02:39.133750 22626471084160 run.py:483] Algo bellman_ford step 9073 current loss 0.589556, current_train_items 290368.
I0302 19:02:39.167036 22626471084160 run.py:483] Algo bellman_ford step 9074 current loss 0.650454, current_train_items 290400.
I0302 19:02:39.186959 22626471084160 run.py:483] Algo bellman_ford step 9075 current loss 0.224131, current_train_items 290432.
I0302 19:02:39.203191 22626471084160 run.py:483] Algo bellman_ford step 9076 current loss 0.567949, current_train_items 290464.
I0302 19:02:39.227178 22626471084160 run.py:483] Algo bellman_ford step 9077 current loss 0.537423, current_train_items 290496.
I0302 19:02:39.258671 22626471084160 run.py:483] Algo bellman_ford step 9078 current loss 0.558698, current_train_items 290528.
I0302 19:02:39.293086 22626471084160 run.py:483] Algo bellman_ford step 9079 current loss 0.822577, current_train_items 290560.
I0302 19:02:39.313048 22626471084160 run.py:483] Algo bellman_ford step 9080 current loss 0.316951, current_train_items 290592.
I0302 19:02:39.329127 22626471084160 run.py:483] Algo bellman_ford step 9081 current loss 0.449210, current_train_items 290624.
I0302 19:02:39.352614 22626471084160 run.py:483] Algo bellman_ford step 9082 current loss 0.539905, current_train_items 290656.
I0302 19:02:39.383841 22626471084160 run.py:483] Algo bellman_ford step 9083 current loss 0.620560, current_train_items 290688.
I0302 19:02:39.416800 22626471084160 run.py:483] Algo bellman_ford step 9084 current loss 0.702067, current_train_items 290720.
I0302 19:02:39.436802 22626471084160 run.py:483] Algo bellman_ford step 9085 current loss 0.278825, current_train_items 290752.
I0302 19:02:39.453134 22626471084160 run.py:483] Algo bellman_ford step 9086 current loss 0.395539, current_train_items 290784.
I0302 19:02:39.477340 22626471084160 run.py:483] Algo bellman_ford step 9087 current loss 0.556197, current_train_items 290816.
I0302 19:02:39.508291 22626471084160 run.py:483] Algo bellman_ford step 9088 current loss 0.517703, current_train_items 290848.
I0302 19:02:39.543506 22626471084160 run.py:483] Algo bellman_ford step 9089 current loss 0.843741, current_train_items 290880.
I0302 19:02:39.563273 22626471084160 run.py:483] Algo bellman_ford step 9090 current loss 0.212160, current_train_items 290912.
I0302 19:02:39.579425 22626471084160 run.py:483] Algo bellman_ford step 9091 current loss 0.469518, current_train_items 290944.
I0302 19:02:39.602639 22626471084160 run.py:483] Algo bellman_ford step 9092 current loss 0.527575, current_train_items 290976.
I0302 19:02:39.632218 22626471084160 run.py:483] Algo bellman_ford step 9093 current loss 0.574953, current_train_items 291008.
I0302 19:02:39.668430 22626471084160 run.py:483] Algo bellman_ford step 9094 current loss 0.838076, current_train_items 291040.
I0302 19:02:39.688037 22626471084160 run.py:483] Algo bellman_ford step 9095 current loss 0.437453, current_train_items 291072.
I0302 19:02:39.704211 22626471084160 run.py:483] Algo bellman_ford step 9096 current loss 0.480601, current_train_items 291104.
I0302 19:02:39.727783 22626471084160 run.py:483] Algo bellman_ford step 9097 current loss 0.599108, current_train_items 291136.
I0302 19:02:39.759635 22626471084160 run.py:483] Algo bellman_ford step 9098 current loss 0.687980, current_train_items 291168.
I0302 19:02:39.792682 22626471084160 run.py:483] Algo bellman_ford step 9099 current loss 0.700125, current_train_items 291200.
I0302 19:02:39.812516 22626471084160 run.py:483] Algo bellman_ford step 9100 current loss 0.232854, current_train_items 291232.
I0302 19:02:39.820345 22626471084160 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0302 19:02:39.820450 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:02:39.837163 22626471084160 run.py:483] Algo bellman_ford step 9101 current loss 0.399478, current_train_items 291264.
I0302 19:02:39.862293 22626471084160 run.py:483] Algo bellman_ford step 9102 current loss 0.658758, current_train_items 291296.
I0302 19:02:39.894447 22626471084160 run.py:483] Algo bellman_ford step 9103 current loss 0.605730, current_train_items 291328.
I0302 19:02:39.930193 22626471084160 run.py:483] Algo bellman_ford step 9104 current loss 0.792968, current_train_items 291360.
I0302 19:02:39.950438 22626471084160 run.py:483] Algo bellman_ford step 9105 current loss 0.284735, current_train_items 291392.
I0302 19:02:39.966605 22626471084160 run.py:483] Algo bellman_ford step 9106 current loss 0.511075, current_train_items 291424.
I0302 19:02:39.990913 22626471084160 run.py:483] Algo bellman_ford step 9107 current loss 0.608695, current_train_items 291456.
I0302 19:02:40.023058 22626471084160 run.py:483] Algo bellman_ford step 9108 current loss 0.673511, current_train_items 291488.
I0302 19:02:40.057267 22626471084160 run.py:483] Algo bellman_ford step 9109 current loss 0.655698, current_train_items 291520.
I0302 19:02:40.076873 22626471084160 run.py:483] Algo bellman_ford step 9110 current loss 0.304238, current_train_items 291552.
I0302 19:02:40.092920 22626471084160 run.py:483] Algo bellman_ford step 9111 current loss 0.413768, current_train_items 291584.
I0302 19:02:40.116406 22626471084160 run.py:483] Algo bellman_ford step 9112 current loss 0.749660, current_train_items 291616.
I0302 19:02:40.149519 22626471084160 run.py:483] Algo bellman_ford step 9113 current loss 0.611205, current_train_items 291648.
I0302 19:02:40.184367 22626471084160 run.py:483] Algo bellman_ford step 9114 current loss 0.658300, current_train_items 291680.
I0302 19:02:40.203988 22626471084160 run.py:483] Algo bellman_ford step 9115 current loss 0.335177, current_train_items 291712.
I0302 19:02:40.220421 22626471084160 run.py:483] Algo bellman_ford step 9116 current loss 0.422832, current_train_items 291744.
I0302 19:02:40.244056 22626471084160 run.py:483] Algo bellman_ford step 9117 current loss 0.651711, current_train_items 291776.
I0302 19:02:40.275289 22626471084160 run.py:483] Algo bellman_ford step 9118 current loss 0.651890, current_train_items 291808.
I0302 19:02:40.310622 22626471084160 run.py:483] Algo bellman_ford step 9119 current loss 0.794800, current_train_items 291840.
I0302 19:02:40.330186 22626471084160 run.py:483] Algo bellman_ford step 9120 current loss 0.245836, current_train_items 291872.
I0302 19:02:40.346303 22626471084160 run.py:483] Algo bellman_ford step 9121 current loss 0.454467, current_train_items 291904.
I0302 19:02:40.369983 22626471084160 run.py:483] Algo bellman_ford step 9122 current loss 0.617007, current_train_items 291936.
I0302 19:02:40.403103 22626471084160 run.py:483] Algo bellman_ford step 9123 current loss 0.798138, current_train_items 291968.
I0302 19:02:40.438781 22626471084160 run.py:483] Algo bellman_ford step 9124 current loss 0.734095, current_train_items 292000.
I0302 19:02:40.458465 22626471084160 run.py:483] Algo bellman_ford step 9125 current loss 0.271350, current_train_items 292032.
I0302 19:02:40.474337 22626471084160 run.py:483] Algo bellman_ford step 9126 current loss 0.440344, current_train_items 292064.
I0302 19:02:40.497464 22626471084160 run.py:483] Algo bellman_ford step 9127 current loss 0.577350, current_train_items 292096.
I0302 19:02:40.529964 22626471084160 run.py:483] Algo bellman_ford step 9128 current loss 0.680026, current_train_items 292128.
I0302 19:02:40.563321 22626471084160 run.py:483] Algo bellman_ford step 9129 current loss 0.758704, current_train_items 292160.
I0302 19:02:40.582910 22626471084160 run.py:483] Algo bellman_ford step 9130 current loss 0.268278, current_train_items 292192.
I0302 19:02:40.599025 22626471084160 run.py:483] Algo bellman_ford step 9131 current loss 0.481958, current_train_items 292224.
I0302 19:02:40.623565 22626471084160 run.py:483] Algo bellman_ford step 9132 current loss 0.604386, current_train_items 292256.
I0302 19:02:40.655740 22626471084160 run.py:483] Algo bellman_ford step 9133 current loss 0.731107, current_train_items 292288.
I0302 19:02:40.688662 22626471084160 run.py:483] Algo bellman_ford step 9134 current loss 0.672724, current_train_items 292320.
I0302 19:02:40.708259 22626471084160 run.py:483] Algo bellman_ford step 9135 current loss 0.289914, current_train_items 292352.
I0302 19:02:40.724013 22626471084160 run.py:483] Algo bellman_ford step 9136 current loss 0.450950, current_train_items 292384.
I0302 19:02:40.747140 22626471084160 run.py:483] Algo bellman_ford step 9137 current loss 0.519514, current_train_items 292416.
I0302 19:02:40.778681 22626471084160 run.py:483] Algo bellman_ford step 9138 current loss 0.652971, current_train_items 292448.
I0302 19:02:40.811378 22626471084160 run.py:483] Algo bellman_ford step 9139 current loss 0.723028, current_train_items 292480.
I0302 19:02:40.831014 22626471084160 run.py:483] Algo bellman_ford step 9140 current loss 0.298202, current_train_items 292512.
I0302 19:02:40.847107 22626471084160 run.py:483] Algo bellman_ford step 9141 current loss 0.491208, current_train_items 292544.
I0302 19:02:40.870769 22626471084160 run.py:483] Algo bellman_ford step 9142 current loss 0.626155, current_train_items 292576.
I0302 19:02:40.901969 22626471084160 run.py:483] Algo bellman_ford step 9143 current loss 0.654431, current_train_items 292608.
I0302 19:02:40.936516 22626471084160 run.py:483] Algo bellman_ford step 9144 current loss 0.706003, current_train_items 292640.
I0302 19:02:40.956097 22626471084160 run.py:483] Algo bellman_ford step 9145 current loss 0.278715, current_train_items 292672.
I0302 19:02:40.972386 22626471084160 run.py:483] Algo bellman_ford step 9146 current loss 0.425473, current_train_items 292704.
I0302 19:02:40.996554 22626471084160 run.py:483] Algo bellman_ford step 9147 current loss 0.580615, current_train_items 292736.
I0302 19:02:41.028537 22626471084160 run.py:483] Algo bellman_ford step 9148 current loss 0.705633, current_train_items 292768.
I0302 19:02:41.059074 22626471084160 run.py:483] Algo bellman_ford step 9149 current loss 0.714511, current_train_items 292800.
I0302 19:02:41.078512 22626471084160 run.py:483] Algo bellman_ford step 9150 current loss 0.292658, current_train_items 292832.
I0302 19:02:41.086598 22626471084160 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.9599609375, 'score': 0.9599609375, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0302 19:02:41.086705 22626471084160 run.py:519] Checkpointing best model, best avg val score was 0.957, current avg val score is 0.960, val scores are: bellman_ford: 0.960
I0302 19:02:41.116859 22626471084160 run.py:483] Algo bellman_ford step 9151 current loss 0.448126, current_train_items 292864.
I0302 19:02:41.140967 22626471084160 run.py:483] Algo bellman_ford step 9152 current loss 0.486071, current_train_items 292896.
I0302 19:02:41.173595 22626471084160 run.py:483] Algo bellman_ford step 9153 current loss 0.608797, current_train_items 292928.
I0302 19:02:41.206259 22626471084160 run.py:483] Algo bellman_ford step 9154 current loss 0.700306, current_train_items 292960.
I0302 19:02:41.226262 22626471084160 run.py:483] Algo bellman_ford step 9155 current loss 0.269875, current_train_items 292992.
I0302 19:02:41.242792 22626471084160 run.py:483] Algo bellman_ford step 9156 current loss 0.513712, current_train_items 293024.
I0302 19:02:41.267688 22626471084160 run.py:483] Algo bellman_ford step 9157 current loss 0.623634, current_train_items 293056.
I0302 19:02:41.298869 22626471084160 run.py:483] Algo bellman_ford step 9158 current loss 0.640865, current_train_items 293088.
I0302 19:02:41.333588 22626471084160 run.py:483] Algo bellman_ford step 9159 current loss 0.714971, current_train_items 293120.
I0302 19:02:41.353606 22626471084160 run.py:483] Algo bellman_ford step 9160 current loss 0.230097, current_train_items 293152.
I0302 19:02:41.370010 22626471084160 run.py:483] Algo bellman_ford step 9161 current loss 0.399252, current_train_items 293184.
I0302 19:02:41.393550 22626471084160 run.py:483] Algo bellman_ford step 9162 current loss 0.584931, current_train_items 293216.
I0302 19:02:41.424940 22626471084160 run.py:483] Algo bellman_ford step 9163 current loss 0.597567, current_train_items 293248.
I0302 19:02:41.460316 22626471084160 run.py:483] Algo bellman_ford step 9164 current loss 0.726995, current_train_items 293280.
I0302 19:02:41.480357 22626471084160 run.py:483] Algo bellman_ford step 9165 current loss 0.344909, current_train_items 293312.
I0302 19:02:41.496615 22626471084160 run.py:483] Algo bellman_ford step 9166 current loss 0.444051, current_train_items 293344.
I0302 19:02:41.520394 22626471084160 run.py:483] Algo bellman_ford step 9167 current loss 0.579510, current_train_items 293376.
I0302 19:02:41.552108 22626471084160 run.py:483] Algo bellman_ford step 9168 current loss 0.613278, current_train_items 293408.
I0302 19:02:41.585213 22626471084160 run.py:483] Algo bellman_ford step 9169 current loss 0.666382, current_train_items 293440.
I0302 19:02:41.605049 22626471084160 run.py:483] Algo bellman_ford step 9170 current loss 0.257619, current_train_items 293472.
I0302 19:02:41.621109 22626471084160 run.py:483] Algo bellman_ford step 9171 current loss 0.423689, current_train_items 293504.
I0302 19:02:41.646383 22626471084160 run.py:483] Algo bellman_ford step 9172 current loss 0.688657, current_train_items 293536.
I0302 19:02:41.678007 22626471084160 run.py:483] Algo bellman_ford step 9173 current loss 0.616502, current_train_items 293568.
I0302 19:02:41.712677 22626471084160 run.py:483] Algo bellman_ford step 9174 current loss 0.763069, current_train_items 293600.
I0302 19:02:41.732794 22626471084160 run.py:483] Algo bellman_ford step 9175 current loss 0.288214, current_train_items 293632.
I0302 19:02:41.749262 22626471084160 run.py:483] Algo bellman_ford step 9176 current loss 0.337665, current_train_items 293664.
I0302 19:02:41.772105 22626471084160 run.py:483] Algo bellman_ford step 9177 current loss 0.534405, current_train_items 293696.
I0302 19:02:41.802336 22626471084160 run.py:483] Algo bellman_ford step 9178 current loss 0.586234, current_train_items 293728.
I0302 19:02:41.837641 22626471084160 run.py:483] Algo bellman_ford step 9179 current loss 0.784203, current_train_items 293760.
I0302 19:02:41.856972 22626471084160 run.py:483] Algo bellman_ford step 9180 current loss 0.328438, current_train_items 293792.
I0302 19:02:41.873184 22626471084160 run.py:483] Algo bellman_ford step 9181 current loss 0.505524, current_train_items 293824.
I0302 19:02:41.897968 22626471084160 run.py:483] Algo bellman_ford step 9182 current loss 0.602530, current_train_items 293856.
I0302 19:02:41.928561 22626471084160 run.py:483] Algo bellman_ford step 9183 current loss 0.577292, current_train_items 293888.
I0302 19:02:41.961718 22626471084160 run.py:483] Algo bellman_ford step 9184 current loss 0.804111, current_train_items 293920.
I0302 19:02:41.981577 22626471084160 run.py:483] Algo bellman_ford step 9185 current loss 0.335113, current_train_items 293952.
I0302 19:02:41.998076 22626471084160 run.py:483] Algo bellman_ford step 9186 current loss 0.479921, current_train_items 293984.
I0302 19:02:42.022341 22626471084160 run.py:483] Algo bellman_ford step 9187 current loss 0.557231, current_train_items 294016.
I0302 19:02:42.055122 22626471084160 run.py:483] Algo bellman_ford step 9188 current loss 0.621973, current_train_items 294048.
I0302 19:02:42.090911 22626471084160 run.py:483] Algo bellman_ford step 9189 current loss 0.832579, current_train_items 294080.
I0302 19:02:42.110826 22626471084160 run.py:483] Algo bellman_ford step 9190 current loss 0.381363, current_train_items 294112.
I0302 19:02:42.126822 22626471084160 run.py:483] Algo bellman_ford step 9191 current loss 0.541225, current_train_items 294144.
I0302 19:02:42.151088 22626471084160 run.py:483] Algo bellman_ford step 9192 current loss 0.546334, current_train_items 294176.
I0302 19:02:42.182883 22626471084160 run.py:483] Algo bellman_ford step 9193 current loss 0.657046, current_train_items 294208.
I0302 19:02:42.216024 22626471084160 run.py:483] Algo bellman_ford step 9194 current loss 0.677857, current_train_items 294240.
I0302 19:02:42.235834 22626471084160 run.py:483] Algo bellman_ford step 9195 current loss 0.322865, current_train_items 294272.
I0302 19:02:42.252509 22626471084160 run.py:483] Algo bellman_ford step 9196 current loss 0.512358, current_train_items 294304.
I0302 19:02:42.276683 22626471084160 run.py:483] Algo bellman_ford step 9197 current loss 0.711973, current_train_items 294336.
I0302 19:02:42.307509 22626471084160 run.py:483] Algo bellman_ford step 9198 current loss 0.649828, current_train_items 294368.
I0302 19:02:42.340069 22626471084160 run.py:483] Algo bellman_ford step 9199 current loss 0.711270, current_train_items 294400.
I0302 19:02:42.360317 22626471084160 run.py:483] Algo bellman_ford step 9200 current loss 0.326525, current_train_items 294432.
I0302 19:02:42.367829 22626471084160 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0302 19:02:42.367934 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:02:42.384748 22626471084160 run.py:483] Algo bellman_ford step 9201 current loss 0.408938, current_train_items 294464.
I0302 19:02:42.409198 22626471084160 run.py:483] Algo bellman_ford step 9202 current loss 0.529623, current_train_items 294496.
I0302 19:02:42.442416 22626471084160 run.py:483] Algo bellman_ford step 9203 current loss 0.722020, current_train_items 294528.
I0302 19:02:42.477860 22626471084160 run.py:483] Algo bellman_ford step 9204 current loss 0.705603, current_train_items 294560.
I0302 19:02:42.498326 22626471084160 run.py:483] Algo bellman_ford step 9205 current loss 0.274128, current_train_items 294592.
I0302 19:02:42.514540 22626471084160 run.py:483] Algo bellman_ford step 9206 current loss 0.413929, current_train_items 294624.
I0302 19:02:42.538717 22626471084160 run.py:483] Algo bellman_ford step 9207 current loss 0.510570, current_train_items 294656.
I0302 19:02:42.570706 22626471084160 run.py:483] Algo bellman_ford step 9208 current loss 0.585010, current_train_items 294688.
I0302 19:02:42.602397 22626471084160 run.py:483] Algo bellman_ford step 9209 current loss 0.706021, current_train_items 294720.
I0302 19:02:42.622288 22626471084160 run.py:483] Algo bellman_ford step 9210 current loss 0.308589, current_train_items 294752.
I0302 19:02:42.638962 22626471084160 run.py:483] Algo bellman_ford step 9211 current loss 0.458136, current_train_items 294784.
I0302 19:02:42.662542 22626471084160 run.py:483] Algo bellman_ford step 9212 current loss 0.594149, current_train_items 294816.
I0302 19:02:42.693917 22626471084160 run.py:483] Algo bellman_ford step 9213 current loss 0.664678, current_train_items 294848.
I0302 19:02:42.726824 22626471084160 run.py:483] Algo bellman_ford step 9214 current loss 0.727598, current_train_items 294880.
I0302 19:02:42.746402 22626471084160 run.py:483] Algo bellman_ford step 9215 current loss 0.329543, current_train_items 294912.
I0302 19:02:42.762899 22626471084160 run.py:483] Algo bellman_ford step 9216 current loss 0.539006, current_train_items 294944.
I0302 19:02:42.786619 22626471084160 run.py:483] Algo bellman_ford step 9217 current loss 0.732017, current_train_items 294976.
I0302 19:02:42.817688 22626471084160 run.py:483] Algo bellman_ford step 9218 current loss 0.772550, current_train_items 295008.
I0302 19:02:42.850920 22626471084160 run.py:483] Algo bellman_ford step 9219 current loss 0.795359, current_train_items 295040.
I0302 19:02:42.870804 22626471084160 run.py:483] Algo bellman_ford step 9220 current loss 0.318454, current_train_items 295072.
I0302 19:02:42.886838 22626471084160 run.py:483] Algo bellman_ford step 9221 current loss 0.378464, current_train_items 295104.
I0302 19:02:42.911461 22626471084160 run.py:483] Algo bellman_ford step 9222 current loss 0.626832, current_train_items 295136.
I0302 19:02:42.942531 22626471084160 run.py:483] Algo bellman_ford step 9223 current loss 0.625327, current_train_items 295168.
I0302 19:02:42.976142 22626471084160 run.py:483] Algo bellman_ford step 9224 current loss 0.675754, current_train_items 295200.
I0302 19:02:42.996038 22626471084160 run.py:483] Algo bellman_ford step 9225 current loss 0.302428, current_train_items 295232.
I0302 19:02:43.012136 22626471084160 run.py:483] Algo bellman_ford step 9226 current loss 0.371886, current_train_items 295264.
I0302 19:02:43.036601 22626471084160 run.py:483] Algo bellman_ford step 9227 current loss 0.592865, current_train_items 295296.
I0302 19:02:43.068481 22626471084160 run.py:483] Algo bellman_ford step 9228 current loss 0.637342, current_train_items 295328.
I0302 19:02:43.100003 22626471084160 run.py:483] Algo bellman_ford step 9229 current loss 0.717228, current_train_items 295360.
I0302 19:02:43.119533 22626471084160 run.py:483] Algo bellman_ford step 9230 current loss 0.327187, current_train_items 295392.
I0302 19:02:43.136124 22626471084160 run.py:483] Algo bellman_ford step 9231 current loss 0.460629, current_train_items 295424.
I0302 19:02:43.160316 22626471084160 run.py:483] Algo bellman_ford step 9232 current loss 0.702723, current_train_items 295456.
I0302 19:02:43.194082 22626471084160 run.py:483] Algo bellman_ford step 9233 current loss 0.699915, current_train_items 295488.
I0302 19:02:43.228150 22626471084160 run.py:483] Algo bellman_ford step 9234 current loss 0.703406, current_train_items 295520.
I0302 19:02:43.247525 22626471084160 run.py:483] Algo bellman_ford step 9235 current loss 0.302253, current_train_items 295552.
I0302 19:02:43.263846 22626471084160 run.py:483] Algo bellman_ford step 9236 current loss 0.455098, current_train_items 295584.
I0302 19:02:43.287738 22626471084160 run.py:483] Algo bellman_ford step 9237 current loss 0.549823, current_train_items 295616.
I0302 19:02:43.320584 22626471084160 run.py:483] Algo bellman_ford step 9238 current loss 0.707144, current_train_items 295648.
I0302 19:02:43.354658 22626471084160 run.py:483] Algo bellman_ford step 9239 current loss 0.690482, current_train_items 295680.
I0302 19:02:43.374219 22626471084160 run.py:483] Algo bellman_ford step 9240 current loss 0.250621, current_train_items 295712.
I0302 19:02:43.391017 22626471084160 run.py:483] Algo bellman_ford step 9241 current loss 0.456467, current_train_items 295744.
I0302 19:02:43.415525 22626471084160 run.py:483] Algo bellman_ford step 9242 current loss 0.611207, current_train_items 295776.
I0302 19:02:43.448029 22626471084160 run.py:483] Algo bellman_ford step 9243 current loss 0.588549, current_train_items 295808.
I0302 19:02:43.480604 22626471084160 run.py:483] Algo bellman_ford step 9244 current loss 0.618914, current_train_items 295840.
I0302 19:02:43.500412 22626471084160 run.py:483] Algo bellman_ford step 9245 current loss 0.275077, current_train_items 295872.
I0302 19:02:43.516734 22626471084160 run.py:483] Algo bellman_ford step 9246 current loss 0.433289, current_train_items 295904.
I0302 19:02:43.539948 22626471084160 run.py:483] Algo bellman_ford step 9247 current loss 0.548857, current_train_items 295936.
I0302 19:02:43.571751 22626471084160 run.py:483] Algo bellman_ford step 9248 current loss 0.681009, current_train_items 295968.
I0302 19:02:43.606258 22626471084160 run.py:483] Algo bellman_ford step 9249 current loss 0.788270, current_train_items 296000.
I0302 19:02:43.626325 22626471084160 run.py:483] Algo bellman_ford step 9250 current loss 0.291505, current_train_items 296032.
I0302 19:02:43.634346 22626471084160 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0302 19:02:43.634452 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:02:43.651064 22626471084160 run.py:483] Algo bellman_ford step 9251 current loss 0.484189, current_train_items 296064.
I0302 19:02:43.675769 22626471084160 run.py:483] Algo bellman_ford step 9252 current loss 0.559905, current_train_items 296096.
I0302 19:02:43.707129 22626471084160 run.py:483] Algo bellman_ford step 9253 current loss 0.585769, current_train_items 296128.
I0302 19:02:43.740319 22626471084160 run.py:483] Algo bellman_ford step 9254 current loss 0.684718, current_train_items 296160.
I0302 19:02:43.760808 22626471084160 run.py:483] Algo bellman_ford step 9255 current loss 0.283411, current_train_items 296192.
I0302 19:02:43.776884 22626471084160 run.py:483] Algo bellman_ford step 9256 current loss 0.404030, current_train_items 296224.
I0302 19:02:43.801028 22626471084160 run.py:483] Algo bellman_ford step 9257 current loss 0.762431, current_train_items 296256.
I0302 19:02:43.833137 22626471084160 run.py:483] Algo bellman_ford step 9258 current loss 0.716746, current_train_items 296288.
I0302 19:02:43.867224 22626471084160 run.py:483] Algo bellman_ford step 9259 current loss 0.689247, current_train_items 296320.
I0302 19:02:43.887320 22626471084160 run.py:483] Algo bellman_ford step 9260 current loss 0.311339, current_train_items 296352.
I0302 19:02:43.904348 22626471084160 run.py:483] Algo bellman_ford step 9261 current loss 0.508185, current_train_items 296384.
I0302 19:02:43.927114 22626471084160 run.py:483] Algo bellman_ford step 9262 current loss 0.601249, current_train_items 296416.
I0302 19:02:43.959376 22626471084160 run.py:483] Algo bellman_ford step 9263 current loss 0.790660, current_train_items 296448.
I0302 19:02:43.993216 22626471084160 run.py:483] Algo bellman_ford step 9264 current loss 0.774891, current_train_items 296480.
I0302 19:02:44.013058 22626471084160 run.py:483] Algo bellman_ford step 9265 current loss 0.304894, current_train_items 296512.
I0302 19:02:44.029348 22626471084160 run.py:483] Algo bellman_ford step 9266 current loss 0.428109, current_train_items 296544.
I0302 19:02:44.052007 22626471084160 run.py:483] Algo bellman_ford step 9267 current loss 0.561706, current_train_items 296576.
I0302 19:02:44.083379 22626471084160 run.py:483] Algo bellman_ford step 9268 current loss 0.636611, current_train_items 296608.
I0302 19:02:44.116961 22626471084160 run.py:483] Algo bellman_ford step 9269 current loss 0.728776, current_train_items 296640.
I0302 19:02:44.136828 22626471084160 run.py:483] Algo bellman_ford step 9270 current loss 0.313456, current_train_items 296672.
I0302 19:02:44.152951 22626471084160 run.py:483] Algo bellman_ford step 9271 current loss 0.442711, current_train_items 296704.
I0302 19:02:44.176358 22626471084160 run.py:483] Algo bellman_ford step 9272 current loss 0.564272, current_train_items 296736.
I0302 19:02:44.209447 22626471084160 run.py:483] Algo bellman_ford step 9273 current loss 0.693278, current_train_items 296768.
I0302 19:02:44.244196 22626471084160 run.py:483] Algo bellman_ford step 9274 current loss 0.863926, current_train_items 296800.
I0302 19:02:44.264345 22626471084160 run.py:483] Algo bellman_ford step 9275 current loss 0.333006, current_train_items 296832.
I0302 19:02:44.280660 22626471084160 run.py:483] Algo bellman_ford step 9276 current loss 0.452362, current_train_items 296864.
I0302 19:02:44.304380 22626471084160 run.py:483] Algo bellman_ford step 9277 current loss 0.610020, current_train_items 296896.
I0302 19:02:44.336381 22626471084160 run.py:483] Algo bellman_ford step 9278 current loss 0.625871, current_train_items 296928.
I0302 19:02:44.370317 22626471084160 run.py:483] Algo bellman_ford step 9279 current loss 0.866943, current_train_items 296960.
I0302 19:02:44.389574 22626471084160 run.py:483] Algo bellman_ford step 9280 current loss 0.327306, current_train_items 296992.
I0302 19:02:44.406038 22626471084160 run.py:483] Algo bellman_ford step 9281 current loss 0.472373, current_train_items 297024.
I0302 19:02:44.429891 22626471084160 run.py:483] Algo bellman_ford step 9282 current loss 0.560862, current_train_items 297056.
I0302 19:02:44.459800 22626471084160 run.py:483] Algo bellman_ford step 9283 current loss 0.489133, current_train_items 297088.
I0302 19:02:44.492564 22626471084160 run.py:483] Algo bellman_ford step 9284 current loss 0.718127, current_train_items 297120.
I0302 19:02:44.512530 22626471084160 run.py:483] Algo bellman_ford step 9285 current loss 0.233068, current_train_items 297152.
I0302 19:02:44.528252 22626471084160 run.py:483] Algo bellman_ford step 9286 current loss 0.399637, current_train_items 297184.
I0302 19:02:44.553471 22626471084160 run.py:483] Algo bellman_ford step 9287 current loss 0.624071, current_train_items 297216.
I0302 19:02:44.584467 22626471084160 run.py:483] Algo bellman_ford step 9288 current loss 0.572022, current_train_items 297248.
I0302 19:02:44.620016 22626471084160 run.py:483] Algo bellman_ford step 9289 current loss 0.736817, current_train_items 297280.
I0302 19:02:44.639761 22626471084160 run.py:483] Algo bellman_ford step 9290 current loss 0.333170, current_train_items 297312.
I0302 19:02:44.655934 22626471084160 run.py:483] Algo bellman_ford step 9291 current loss 0.467163, current_train_items 297344.
I0302 19:02:44.680208 22626471084160 run.py:483] Algo bellman_ford step 9292 current loss 0.586760, current_train_items 297376.
I0302 19:02:44.710801 22626471084160 run.py:483] Algo bellman_ford step 9293 current loss 0.674639, current_train_items 297408.
I0302 19:02:44.745355 22626471084160 run.py:483] Algo bellman_ford step 9294 current loss 0.831693, current_train_items 297440.
I0302 19:02:44.765187 22626471084160 run.py:483] Algo bellman_ford step 9295 current loss 0.246624, current_train_items 297472.
I0302 19:02:44.781462 22626471084160 run.py:483] Algo bellman_ford step 9296 current loss 0.386024, current_train_items 297504.
I0302 19:02:44.805638 22626471084160 run.py:483] Algo bellman_ford step 9297 current loss 0.577091, current_train_items 297536.
I0302 19:02:44.837485 22626471084160 run.py:483] Algo bellman_ford step 9298 current loss 0.594316, current_train_items 297568.
I0302 19:02:44.871594 22626471084160 run.py:483] Algo bellman_ford step 9299 current loss 0.677871, current_train_items 297600.
I0302 19:02:44.891579 22626471084160 run.py:483] Algo bellman_ford step 9300 current loss 0.326039, current_train_items 297632.
I0302 19:02:44.899483 22626471084160 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.939453125, 'score': 0.939453125, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0302 19:02:44.899589 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.939, val scores are: bellman_ford: 0.939
I0302 19:02:44.916512 22626471084160 run.py:483] Algo bellman_ford step 9301 current loss 0.417204, current_train_items 297664.
I0302 19:02:44.941131 22626471084160 run.py:483] Algo bellman_ford step 9302 current loss 0.503709, current_train_items 297696.
I0302 19:02:44.973347 22626471084160 run.py:483] Algo bellman_ford step 9303 current loss 0.597950, current_train_items 297728.
I0302 19:02:45.008728 22626471084160 run.py:483] Algo bellman_ford step 9304 current loss 0.739335, current_train_items 297760.
I0302 19:02:45.028952 22626471084160 run.py:483] Algo bellman_ford step 9305 current loss 0.289659, current_train_items 297792.
I0302 19:02:45.044759 22626471084160 run.py:483] Algo bellman_ford step 9306 current loss 0.448721, current_train_items 297824.
I0302 19:02:45.068904 22626471084160 run.py:483] Algo bellman_ford step 9307 current loss 0.641159, current_train_items 297856.
I0302 19:02:45.100533 22626471084160 run.py:483] Algo bellman_ford step 9308 current loss 0.706841, current_train_items 297888.
I0302 19:02:45.134229 22626471084160 run.py:483] Algo bellman_ford step 9309 current loss 0.742430, current_train_items 297920.
I0302 19:02:45.154012 22626471084160 run.py:483] Algo bellman_ford step 9310 current loss 0.310033, current_train_items 297952.
I0302 19:02:45.170132 22626471084160 run.py:483] Algo bellman_ford step 9311 current loss 0.373877, current_train_items 297984.
I0302 19:02:45.194105 22626471084160 run.py:483] Algo bellman_ford step 9312 current loss 0.574055, current_train_items 298016.
I0302 19:02:45.224681 22626471084160 run.py:483] Algo bellman_ford step 9313 current loss 0.659313, current_train_items 298048.
I0302 19:02:45.258914 22626471084160 run.py:483] Algo bellman_ford step 9314 current loss 0.797961, current_train_items 298080.
I0302 19:02:45.278498 22626471084160 run.py:483] Algo bellman_ford step 9315 current loss 0.295959, current_train_items 298112.
I0302 19:02:45.294394 22626471084160 run.py:483] Algo bellman_ford step 9316 current loss 0.375582, current_train_items 298144.
I0302 19:02:45.318536 22626471084160 run.py:483] Algo bellman_ford step 9317 current loss 0.545490, current_train_items 298176.
I0302 19:02:45.350879 22626471084160 run.py:483] Algo bellman_ford step 9318 current loss 0.669644, current_train_items 298208.
I0302 19:02:45.384685 22626471084160 run.py:483] Algo bellman_ford step 9319 current loss 0.600726, current_train_items 298240.
I0302 19:02:45.404531 22626471084160 run.py:483] Algo bellman_ford step 9320 current loss 0.343745, current_train_items 298272.
I0302 19:02:45.420847 22626471084160 run.py:483] Algo bellman_ford step 9321 current loss 0.435433, current_train_items 298304.
I0302 19:02:45.443690 22626471084160 run.py:483] Algo bellman_ford step 9322 current loss 0.540422, current_train_items 298336.
I0302 19:02:45.474292 22626471084160 run.py:483] Algo bellman_ford step 9323 current loss 0.653425, current_train_items 298368.
I0302 19:02:45.510731 22626471084160 run.py:483] Algo bellman_ford step 9324 current loss 0.941069, current_train_items 298400.
I0302 19:02:45.530265 22626471084160 run.py:483] Algo bellman_ford step 9325 current loss 0.285615, current_train_items 298432.
I0302 19:02:45.546324 22626471084160 run.py:483] Algo bellman_ford step 9326 current loss 0.522397, current_train_items 298464.
I0302 19:02:45.570716 22626471084160 run.py:483] Algo bellman_ford step 9327 current loss 0.715231, current_train_items 298496.
I0302 19:02:45.603040 22626471084160 run.py:483] Algo bellman_ford step 9328 current loss 0.718607, current_train_items 298528.
I0302 19:02:45.633972 22626471084160 run.py:483] Algo bellman_ford step 9329 current loss 0.742443, current_train_items 298560.
I0302 19:02:45.653513 22626471084160 run.py:483] Algo bellman_ford step 9330 current loss 0.297788, current_train_items 298592.
I0302 19:02:45.669814 22626471084160 run.py:483] Algo bellman_ford step 9331 current loss 0.534106, current_train_items 298624.
I0302 19:02:45.693313 22626471084160 run.py:483] Algo bellman_ford step 9332 current loss 0.514104, current_train_items 298656.
I0302 19:02:45.724610 22626471084160 run.py:483] Algo bellman_ford step 9333 current loss 0.673850, current_train_items 298688.
I0302 19:02:45.757555 22626471084160 run.py:483] Algo bellman_ford step 9334 current loss 0.715836, current_train_items 298720.
I0302 19:02:45.777461 22626471084160 run.py:483] Algo bellman_ford step 9335 current loss 0.291243, current_train_items 298752.
I0302 19:02:45.793868 22626471084160 run.py:483] Algo bellman_ford step 9336 current loss 0.488098, current_train_items 298784.
I0302 19:02:45.818756 22626471084160 run.py:483] Algo bellman_ford step 9337 current loss 0.665792, current_train_items 298816.
I0302 19:02:45.850498 22626471084160 run.py:483] Algo bellman_ford step 9338 current loss 0.628390, current_train_items 298848.
I0302 19:02:45.884701 22626471084160 run.py:483] Algo bellman_ford step 9339 current loss 0.726355, current_train_items 298880.
I0302 19:02:45.904610 22626471084160 run.py:483] Algo bellman_ford step 9340 current loss 0.316660, current_train_items 298912.
I0302 19:02:45.920439 22626471084160 run.py:483] Algo bellman_ford step 9341 current loss 0.417573, current_train_items 298944.
I0302 19:02:45.944498 22626471084160 run.py:483] Algo bellman_ford step 9342 current loss 0.581339, current_train_items 298976.
I0302 19:02:45.976305 22626471084160 run.py:483] Algo bellman_ford step 9343 current loss 0.663015, current_train_items 299008.
I0302 19:02:46.010550 22626471084160 run.py:483] Algo bellman_ford step 9344 current loss 0.703400, current_train_items 299040.
I0302 19:02:46.030117 22626471084160 run.py:483] Algo bellman_ford step 9345 current loss 0.315978, current_train_items 299072.
I0302 19:02:46.046268 22626471084160 run.py:483] Algo bellman_ford step 9346 current loss 0.474544, current_train_items 299104.
I0302 19:02:46.070463 22626471084160 run.py:483] Algo bellman_ford step 9347 current loss 0.587011, current_train_items 299136.
I0302 19:02:46.102615 22626471084160 run.py:483] Algo bellman_ford step 9348 current loss 0.704966, current_train_items 299168.
I0302 19:02:46.135353 22626471084160 run.py:483] Algo bellman_ford step 9349 current loss 0.699624, current_train_items 299200.
I0302 19:02:46.155129 22626471084160 run.py:483] Algo bellman_ford step 9350 current loss 0.359104, current_train_items 299232.
I0302 19:02:46.163167 22626471084160 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0302 19:02:46.163272 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:02:46.179801 22626471084160 run.py:483] Algo bellman_ford step 9351 current loss 0.442909, current_train_items 299264.
I0302 19:02:46.205093 22626471084160 run.py:483] Algo bellman_ford step 9352 current loss 0.640851, current_train_items 299296.
I0302 19:02:46.237779 22626471084160 run.py:483] Algo bellman_ford step 9353 current loss 0.670264, current_train_items 299328.
I0302 19:02:46.273651 22626471084160 run.py:483] Algo bellman_ford step 9354 current loss 0.766040, current_train_items 299360.
I0302 19:02:46.293726 22626471084160 run.py:483] Algo bellman_ford step 9355 current loss 0.314253, current_train_items 299392.
I0302 19:02:46.309248 22626471084160 run.py:483] Algo bellman_ford step 9356 current loss 0.387097, current_train_items 299424.
I0302 19:02:46.333676 22626471084160 run.py:483] Algo bellman_ford step 9357 current loss 0.629610, current_train_items 299456.
I0302 19:02:46.365735 22626471084160 run.py:483] Algo bellman_ford step 9358 current loss 0.643677, current_train_items 299488.
I0302 19:02:46.399363 22626471084160 run.py:483] Algo bellman_ford step 9359 current loss 0.689895, current_train_items 299520.
I0302 19:02:46.419535 22626471084160 run.py:483] Algo bellman_ford step 9360 current loss 0.335214, current_train_items 299552.
I0302 19:02:46.435728 22626471084160 run.py:483] Algo bellman_ford step 9361 current loss 0.479621, current_train_items 299584.
I0302 19:02:46.459053 22626471084160 run.py:483] Algo bellman_ford step 9362 current loss 0.510178, current_train_items 299616.
I0302 19:02:46.491464 22626471084160 run.py:483] Algo bellman_ford step 9363 current loss 0.587475, current_train_items 299648.
I0302 19:02:46.525987 22626471084160 run.py:483] Algo bellman_ford step 9364 current loss 0.750266, current_train_items 299680.
I0302 19:02:46.545745 22626471084160 run.py:483] Algo bellman_ford step 9365 current loss 0.350992, current_train_items 299712.
I0302 19:02:46.562553 22626471084160 run.py:483] Algo bellman_ford step 9366 current loss 0.484624, current_train_items 299744.
I0302 19:02:46.587894 22626471084160 run.py:483] Algo bellman_ford step 9367 current loss 0.717072, current_train_items 299776.
I0302 19:02:46.620344 22626471084160 run.py:483] Algo bellman_ford step 9368 current loss 0.657841, current_train_items 299808.
I0302 19:02:46.653767 22626471084160 run.py:483] Algo bellman_ford step 9369 current loss 0.652248, current_train_items 299840.
I0302 19:02:46.673873 22626471084160 run.py:483] Algo bellman_ford step 9370 current loss 0.330593, current_train_items 299872.
I0302 19:02:46.690348 22626471084160 run.py:483] Algo bellman_ford step 9371 current loss 0.489020, current_train_items 299904.
I0302 19:02:46.714471 22626471084160 run.py:483] Algo bellman_ford step 9372 current loss 0.635458, current_train_items 299936.
I0302 19:02:46.746980 22626471084160 run.py:483] Algo bellman_ford step 9373 current loss 0.665125, current_train_items 299968.
I0302 19:02:46.781031 22626471084160 run.py:483] Algo bellman_ford step 9374 current loss 0.681970, current_train_items 300000.
I0302 19:02:46.800877 22626471084160 run.py:483] Algo bellman_ford step 9375 current loss 0.269494, current_train_items 300032.
I0302 19:02:46.816705 22626471084160 run.py:483] Algo bellman_ford step 9376 current loss 0.509462, current_train_items 300064.
I0302 19:02:46.840432 22626471084160 run.py:483] Algo bellman_ford step 9377 current loss 0.552761, current_train_items 300096.
I0302 19:02:46.871108 22626471084160 run.py:483] Algo bellman_ford step 9378 current loss 0.643730, current_train_items 300128.
I0302 19:02:46.904520 22626471084160 run.py:483] Algo bellman_ford step 9379 current loss 0.728888, current_train_items 300160.
I0302 19:02:46.924273 22626471084160 run.py:483] Algo bellman_ford step 9380 current loss 0.266928, current_train_items 300192.
I0302 19:02:46.940143 22626471084160 run.py:483] Algo bellman_ford step 9381 current loss 0.402742, current_train_items 300224.
I0302 19:02:46.964047 22626471084160 run.py:483] Algo bellman_ford step 9382 current loss 0.587168, current_train_items 300256.
I0302 19:02:46.995090 22626471084160 run.py:483] Algo bellman_ford step 9383 current loss 0.540053, current_train_items 300288.
I0302 19:02:47.029037 22626471084160 run.py:483] Algo bellman_ford step 9384 current loss 0.662653, current_train_items 300320.
I0302 19:02:47.049287 22626471084160 run.py:483] Algo bellman_ford step 9385 current loss 0.347391, current_train_items 300352.
I0302 19:02:47.065900 22626471084160 run.py:483] Algo bellman_ford step 9386 current loss 0.528571, current_train_items 300384.
I0302 19:02:47.089315 22626471084160 run.py:483] Algo bellman_ford step 9387 current loss 0.667132, current_train_items 300416.
I0302 19:02:47.121088 22626471084160 run.py:483] Algo bellman_ford step 9388 current loss 0.655059, current_train_items 300448.
I0302 19:02:47.157178 22626471084160 run.py:483] Algo bellman_ford step 9389 current loss 0.647740, current_train_items 300480.
I0302 19:02:47.177463 22626471084160 run.py:483] Algo bellman_ford step 9390 current loss 0.345037, current_train_items 300512.
I0302 19:02:47.194020 22626471084160 run.py:483] Algo bellman_ford step 9391 current loss 0.414244, current_train_items 300544.
I0302 19:02:47.217426 22626471084160 run.py:483] Algo bellman_ford step 9392 current loss 0.532446, current_train_items 300576.
I0302 19:02:47.248964 22626471084160 run.py:483] Algo bellman_ford step 9393 current loss 0.594927, current_train_items 300608.
I0302 19:02:47.282199 22626471084160 run.py:483] Algo bellman_ford step 9394 current loss 0.699304, current_train_items 300640.
I0302 19:02:47.302279 22626471084160 run.py:483] Algo bellman_ford step 9395 current loss 0.362809, current_train_items 300672.
I0302 19:02:47.318580 22626471084160 run.py:483] Algo bellman_ford step 9396 current loss 0.552495, current_train_items 300704.
I0302 19:02:47.343453 22626471084160 run.py:483] Algo bellman_ford step 9397 current loss 0.614563, current_train_items 300736.
I0302 19:02:47.374689 22626471084160 run.py:483] Algo bellman_ford step 9398 current loss 0.555929, current_train_items 300768.
I0302 19:02:47.408854 22626471084160 run.py:483] Algo bellman_ford step 9399 current loss 0.683951, current_train_items 300800.
I0302 19:02:47.428934 22626471084160 run.py:483] Algo bellman_ford step 9400 current loss 0.288541, current_train_items 300832.
I0302 19:02:47.436393 22626471084160 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0302 19:02:47.436501 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:02:47.453453 22626471084160 run.py:483] Algo bellman_ford step 9401 current loss 0.495493, current_train_items 300864.
I0302 19:02:47.478844 22626471084160 run.py:483] Algo bellman_ford step 9402 current loss 0.638930, current_train_items 300896.
I0302 19:02:47.510601 22626471084160 run.py:483] Algo bellman_ford step 9403 current loss 0.586714, current_train_items 300928.
I0302 19:02:47.545635 22626471084160 run.py:483] Algo bellman_ford step 9404 current loss 0.669653, current_train_items 300960.
I0302 19:02:47.565879 22626471084160 run.py:483] Algo bellman_ford step 9405 current loss 0.285139, current_train_items 300992.
I0302 19:02:47.581859 22626471084160 run.py:483] Algo bellman_ford step 9406 current loss 0.476046, current_train_items 301024.
I0302 19:02:47.607292 22626471084160 run.py:483] Algo bellman_ford step 9407 current loss 0.603312, current_train_items 301056.
I0302 19:02:47.638625 22626471084160 run.py:483] Algo bellman_ford step 9408 current loss 0.577275, current_train_items 301088.
I0302 19:02:47.673527 22626471084160 run.py:483] Algo bellman_ford step 9409 current loss 0.709547, current_train_items 301120.
I0302 19:02:47.693200 22626471084160 run.py:483] Algo bellman_ford step 9410 current loss 0.309563, current_train_items 301152.
I0302 19:02:47.709584 22626471084160 run.py:483] Algo bellman_ford step 9411 current loss 0.511826, current_train_items 301184.
I0302 19:02:47.733719 22626471084160 run.py:483] Algo bellman_ford step 9412 current loss 0.592230, current_train_items 301216.
I0302 19:02:47.765300 22626471084160 run.py:483] Algo bellman_ford step 9413 current loss 0.697282, current_train_items 301248.
I0302 19:02:47.798458 22626471084160 run.py:483] Algo bellman_ford step 9414 current loss 0.776218, current_train_items 301280.
I0302 19:02:47.818338 22626471084160 run.py:483] Algo bellman_ford step 9415 current loss 0.367226, current_train_items 301312.
I0302 19:02:47.834756 22626471084160 run.py:483] Algo bellman_ford step 9416 current loss 0.373209, current_train_items 301344.
I0302 19:02:47.859198 22626471084160 run.py:483] Algo bellman_ford step 9417 current loss 0.571967, current_train_items 301376.
I0302 19:02:47.890124 22626471084160 run.py:483] Algo bellman_ford step 9418 current loss 0.636535, current_train_items 301408.
I0302 19:02:47.921715 22626471084160 run.py:483] Algo bellman_ford step 9419 current loss 0.656523, current_train_items 301440.
I0302 19:02:47.941295 22626471084160 run.py:483] Algo bellman_ford step 9420 current loss 0.328921, current_train_items 301472.
I0302 19:02:47.957738 22626471084160 run.py:483] Algo bellman_ford step 9421 current loss 0.430051, current_train_items 301504.
I0302 19:02:47.982160 22626471084160 run.py:483] Algo bellman_ford step 9422 current loss 0.644893, current_train_items 301536.
I0302 19:02:48.013683 22626471084160 run.py:483] Algo bellman_ford step 9423 current loss 0.650677, current_train_items 301568.
I0302 19:02:48.047434 22626471084160 run.py:483] Algo bellman_ford step 9424 current loss 0.776907, current_train_items 301600.
I0302 19:02:48.067047 22626471084160 run.py:483] Algo bellman_ford step 9425 current loss 0.346933, current_train_items 301632.
I0302 19:02:48.082797 22626471084160 run.py:483] Algo bellman_ford step 9426 current loss 0.383997, current_train_items 301664.
I0302 19:02:48.106529 22626471084160 run.py:483] Algo bellman_ford step 9427 current loss 0.536807, current_train_items 301696.
I0302 19:02:48.138690 22626471084160 run.py:483] Algo bellman_ford step 9428 current loss 0.717766, current_train_items 301728.
I0302 19:02:48.169635 22626471084160 run.py:483] Algo bellman_ford step 9429 current loss 0.907158, current_train_items 301760.
I0302 19:02:48.189081 22626471084160 run.py:483] Algo bellman_ford step 9430 current loss 0.303000, current_train_items 301792.
I0302 19:02:48.205330 22626471084160 run.py:483] Algo bellman_ford step 9431 current loss 0.575155, current_train_items 301824.
I0302 19:02:48.229061 22626471084160 run.py:483] Algo bellman_ford step 9432 current loss 0.624791, current_train_items 301856.
I0302 19:02:48.259728 22626471084160 run.py:483] Algo bellman_ford step 9433 current loss 0.654205, current_train_items 301888.
I0302 19:02:48.294262 22626471084160 run.py:483] Algo bellman_ford step 9434 current loss 0.786517, current_train_items 301920.
I0302 19:02:48.314039 22626471084160 run.py:483] Algo bellman_ford step 9435 current loss 0.297490, current_train_items 301952.
I0302 19:02:48.330405 22626471084160 run.py:483] Algo bellman_ford step 9436 current loss 0.428361, current_train_items 301984.
I0302 19:02:48.354369 22626471084160 run.py:483] Algo bellman_ford step 9437 current loss 0.542656, current_train_items 302016.
I0302 19:02:48.386254 22626471084160 run.py:483] Algo bellman_ford step 9438 current loss 0.633473, current_train_items 302048.
I0302 19:02:48.418892 22626471084160 run.py:483] Algo bellman_ford step 9439 current loss 0.780946, current_train_items 302080.
I0302 19:02:48.438602 22626471084160 run.py:483] Algo bellman_ford step 9440 current loss 0.292458, current_train_items 302112.
I0302 19:02:48.455121 22626471084160 run.py:483] Algo bellman_ford step 9441 current loss 0.430906, current_train_items 302144.
I0302 19:02:48.478845 22626471084160 run.py:483] Algo bellman_ford step 9442 current loss 0.599565, current_train_items 302176.
I0302 19:02:48.511395 22626471084160 run.py:483] Algo bellman_ford step 9443 current loss 0.618522, current_train_items 302208.
I0302 19:02:48.547591 22626471084160 run.py:483] Algo bellman_ford step 9444 current loss 0.838748, current_train_items 302240.
I0302 19:02:48.567124 22626471084160 run.py:483] Algo bellman_ford step 9445 current loss 0.368572, current_train_items 302272.
I0302 19:02:48.582907 22626471084160 run.py:483] Algo bellman_ford step 9446 current loss 0.381149, current_train_items 302304.
I0302 19:02:48.606511 22626471084160 run.py:483] Algo bellman_ford step 9447 current loss 0.551368, current_train_items 302336.
I0302 19:02:48.637705 22626471084160 run.py:483] Algo bellman_ford step 9448 current loss 0.674457, current_train_items 302368.
I0302 19:02:48.669985 22626471084160 run.py:483] Algo bellman_ford step 9449 current loss 0.652285, current_train_items 302400.
I0302 19:02:48.689853 22626471084160 run.py:483] Algo bellman_ford step 9450 current loss 0.324511, current_train_items 302432.
I0302 19:02:48.697924 22626471084160 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0302 19:02:48.698032 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 19:02:48.714881 22626471084160 run.py:483] Algo bellman_ford step 9451 current loss 0.406396, current_train_items 302464.
I0302 19:02:48.738165 22626471084160 run.py:483] Algo bellman_ford step 9452 current loss 0.551529, current_train_items 302496.
I0302 19:02:48.770485 22626471084160 run.py:483] Algo bellman_ford step 9453 current loss 0.614900, current_train_items 302528.
I0302 19:02:48.804937 22626471084160 run.py:483] Algo bellman_ford step 9454 current loss 0.762736, current_train_items 302560.
I0302 19:02:48.825320 22626471084160 run.py:483] Algo bellman_ford step 9455 current loss 0.333387, current_train_items 302592.
I0302 19:02:48.841258 22626471084160 run.py:483] Algo bellman_ford step 9456 current loss 0.421376, current_train_items 302624.
I0302 19:02:48.865393 22626471084160 run.py:483] Algo bellman_ford step 9457 current loss 0.517351, current_train_items 302656.
I0302 19:02:48.897124 22626471084160 run.py:483] Algo bellman_ford step 9458 current loss 0.601900, current_train_items 302688.
I0302 19:02:48.929286 22626471084160 run.py:483] Algo bellman_ford step 9459 current loss 0.701516, current_train_items 302720.
I0302 19:02:48.949288 22626471084160 run.py:483] Algo bellman_ford step 9460 current loss 0.268386, current_train_items 302752.
I0302 19:02:48.965812 22626471084160 run.py:483] Algo bellman_ford step 9461 current loss 0.428953, current_train_items 302784.
I0302 19:02:48.989125 22626471084160 run.py:483] Algo bellman_ford step 9462 current loss 0.474470, current_train_items 302816.
I0302 19:02:49.021556 22626471084160 run.py:483] Algo bellman_ford step 9463 current loss 0.580991, current_train_items 302848.
I0302 19:02:49.054990 22626471084160 run.py:483] Algo bellman_ford step 9464 current loss 0.705092, current_train_items 302880.
I0302 19:02:49.074870 22626471084160 run.py:483] Algo bellman_ford step 9465 current loss 0.382553, current_train_items 302912.
I0302 19:02:49.091449 22626471084160 run.py:483] Algo bellman_ford step 9466 current loss 0.518786, current_train_items 302944.
I0302 19:02:49.117171 22626471084160 run.py:483] Algo bellman_ford step 9467 current loss 0.581113, current_train_items 302976.
I0302 19:02:49.149659 22626471084160 run.py:483] Algo bellman_ford step 9468 current loss 0.604143, current_train_items 303008.
I0302 19:02:49.184008 22626471084160 run.py:483] Algo bellman_ford step 9469 current loss 0.724369, current_train_items 303040.
I0302 19:02:49.204270 22626471084160 run.py:483] Algo bellman_ford step 9470 current loss 0.305235, current_train_items 303072.
I0302 19:02:49.220335 22626471084160 run.py:483] Algo bellman_ford step 9471 current loss 0.488875, current_train_items 303104.
I0302 19:02:49.244028 22626471084160 run.py:483] Algo bellman_ford step 9472 current loss 0.551495, current_train_items 303136.
I0302 19:02:49.275962 22626471084160 run.py:483] Algo bellman_ford step 9473 current loss 0.754447, current_train_items 303168.
I0302 19:02:49.308630 22626471084160 run.py:483] Algo bellman_ford step 9474 current loss 0.732144, current_train_items 303200.
I0302 19:02:49.328424 22626471084160 run.py:483] Algo bellman_ford step 9475 current loss 0.313684, current_train_items 303232.
I0302 19:02:49.344741 22626471084160 run.py:483] Algo bellman_ford step 9476 current loss 0.459313, current_train_items 303264.
I0302 19:02:49.368213 22626471084160 run.py:483] Algo bellman_ford step 9477 current loss 0.514563, current_train_items 303296.
I0302 19:02:49.399054 22626471084160 run.py:483] Algo bellman_ford step 9478 current loss 0.625702, current_train_items 303328.
I0302 19:02:49.432165 22626471084160 run.py:483] Algo bellman_ford step 9479 current loss 0.692310, current_train_items 303360.
I0302 19:02:49.451732 22626471084160 run.py:483] Algo bellman_ford step 9480 current loss 0.346723, current_train_items 303392.
I0302 19:02:49.467749 22626471084160 run.py:483] Algo bellman_ford step 9481 current loss 0.466199, current_train_items 303424.
I0302 19:02:49.493059 22626471084160 run.py:483] Algo bellman_ford step 9482 current loss 0.733611, current_train_items 303456.
I0302 19:02:49.523458 22626471084160 run.py:483] Algo bellman_ford step 9483 current loss 0.626271, current_train_items 303488.
I0302 19:02:49.555994 22626471084160 run.py:483] Algo bellman_ford step 9484 current loss 0.750728, current_train_items 303520.
I0302 19:02:49.575779 22626471084160 run.py:483] Algo bellman_ford step 9485 current loss 0.327438, current_train_items 303552.
I0302 19:02:49.592300 22626471084160 run.py:483] Algo bellman_ford step 9486 current loss 0.432694, current_train_items 303584.
I0302 19:02:49.616975 22626471084160 run.py:483] Algo bellman_ford step 9487 current loss 0.582034, current_train_items 303616.
I0302 19:02:49.649782 22626471084160 run.py:483] Algo bellman_ford step 9488 current loss 0.596684, current_train_items 303648.
I0302 19:02:49.682611 22626471084160 run.py:483] Algo bellman_ford step 9489 current loss 0.662043, current_train_items 303680.
I0302 19:02:49.702476 22626471084160 run.py:483] Algo bellman_ford step 9490 current loss 0.283891, current_train_items 303712.
I0302 19:02:49.718517 22626471084160 run.py:483] Algo bellman_ford step 9491 current loss 0.458504, current_train_items 303744.
I0302 19:02:49.742846 22626471084160 run.py:483] Algo bellman_ford step 9492 current loss 0.564357, current_train_items 303776.
I0302 19:02:49.773367 22626471084160 run.py:483] Algo bellman_ford step 9493 current loss 0.561347, current_train_items 303808.
I0302 19:02:49.807127 22626471084160 run.py:483] Algo bellman_ford step 9494 current loss 0.850462, current_train_items 303840.
I0302 19:02:49.827062 22626471084160 run.py:483] Algo bellman_ford step 9495 current loss 0.298792, current_train_items 303872.
I0302 19:02:49.843726 22626471084160 run.py:483] Algo bellman_ford step 9496 current loss 0.463864, current_train_items 303904.
I0302 19:02:49.867567 22626471084160 run.py:483] Algo bellman_ford step 9497 current loss 0.584049, current_train_items 303936.
I0302 19:02:49.898278 22626471084160 run.py:483] Algo bellman_ford step 9498 current loss 0.583775, current_train_items 303968.
I0302 19:02:49.932593 22626471084160 run.py:483] Algo bellman_ford step 9499 current loss 0.731048, current_train_items 304000.
I0302 19:02:49.952881 22626471084160 run.py:483] Algo bellman_ford step 9500 current loss 0.241708, current_train_items 304032.
I0302 19:02:49.960637 22626471084160 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0302 19:02:49.960743 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:02:49.977279 22626471084160 run.py:483] Algo bellman_ford step 9501 current loss 0.418967, current_train_items 304064.
I0302 19:02:50.002032 22626471084160 run.py:483] Algo bellman_ford step 9502 current loss 0.608027, current_train_items 304096.
I0302 19:02:50.033746 22626471084160 run.py:483] Algo bellman_ford step 9503 current loss 0.671606, current_train_items 304128.
I0302 19:02:50.066737 22626471084160 run.py:483] Algo bellman_ford step 9504 current loss 0.676480, current_train_items 304160.
I0302 19:02:50.086916 22626471084160 run.py:483] Algo bellman_ford step 9505 current loss 0.305671, current_train_items 304192.
I0302 19:02:50.103151 22626471084160 run.py:483] Algo bellman_ford step 9506 current loss 0.426572, current_train_items 304224.
I0302 19:02:50.127722 22626471084160 run.py:483] Algo bellman_ford step 9507 current loss 0.543755, current_train_items 304256.
I0302 19:02:50.157880 22626471084160 run.py:483] Algo bellman_ford step 9508 current loss 0.602452, current_train_items 304288.
I0302 19:02:50.193805 22626471084160 run.py:483] Algo bellman_ford step 9509 current loss 0.782417, current_train_items 304320.
I0302 19:02:50.213875 22626471084160 run.py:483] Algo bellman_ford step 9510 current loss 0.333241, current_train_items 304352.
I0302 19:02:50.230566 22626471084160 run.py:483] Algo bellman_ford step 9511 current loss 0.455682, current_train_items 304384.
I0302 19:02:50.254740 22626471084160 run.py:483] Algo bellman_ford step 9512 current loss 0.613317, current_train_items 304416.
I0302 19:02:50.286371 22626471084160 run.py:483] Algo bellman_ford step 9513 current loss 0.697473, current_train_items 304448.
I0302 19:02:50.319403 22626471084160 run.py:483] Algo bellman_ford step 9514 current loss 0.697932, current_train_items 304480.
I0302 19:02:50.339117 22626471084160 run.py:483] Algo bellman_ford step 9515 current loss 0.253448, current_train_items 304512.
I0302 19:02:50.354930 22626471084160 run.py:483] Algo bellman_ford step 9516 current loss 0.403523, current_train_items 304544.
I0302 19:02:50.379331 22626471084160 run.py:483] Algo bellman_ford step 9517 current loss 0.577883, current_train_items 304576.
I0302 19:02:50.411194 22626471084160 run.py:483] Algo bellman_ford step 9518 current loss 0.639603, current_train_items 304608.
I0302 19:02:50.443671 22626471084160 run.py:483] Algo bellman_ford step 9519 current loss 0.645525, current_train_items 304640.
I0302 19:02:50.463267 22626471084160 run.py:483] Algo bellman_ford step 9520 current loss 0.314175, current_train_items 304672.
I0302 19:02:50.478829 22626471084160 run.py:483] Algo bellman_ford step 9521 current loss 0.397938, current_train_items 304704.
I0302 19:02:50.502222 22626471084160 run.py:483] Algo bellman_ford step 9522 current loss 0.521401, current_train_items 304736.
I0302 19:02:50.532224 22626471084160 run.py:483] Algo bellman_ford step 9523 current loss 0.575104, current_train_items 304768.
I0302 19:02:50.564960 22626471084160 run.py:483] Algo bellman_ford step 9524 current loss 0.638983, current_train_items 304800.
I0302 19:02:50.584374 22626471084160 run.py:483] Algo bellman_ford step 9525 current loss 0.214395, current_train_items 304832.
I0302 19:02:50.600607 22626471084160 run.py:483] Algo bellman_ford step 9526 current loss 0.426842, current_train_items 304864.
I0302 19:02:50.625082 22626471084160 run.py:483] Algo bellman_ford step 9527 current loss 0.683104, current_train_items 304896.
I0302 19:02:50.657550 22626471084160 run.py:483] Algo bellman_ford step 9528 current loss 0.717632, current_train_items 304928.
I0302 19:02:50.692308 22626471084160 run.py:483] Algo bellman_ford step 9529 current loss 0.766005, current_train_items 304960.
I0302 19:02:50.711983 22626471084160 run.py:483] Algo bellman_ford step 9530 current loss 0.261961, current_train_items 304992.
I0302 19:02:50.728166 22626471084160 run.py:483] Algo bellman_ford step 9531 current loss 0.447123, current_train_items 305024.
I0302 19:02:50.751837 22626471084160 run.py:483] Algo bellman_ford step 9532 current loss 0.573502, current_train_items 305056.
I0302 19:02:50.784963 22626471084160 run.py:483] Algo bellman_ford step 9533 current loss 0.725350, current_train_items 305088.
I0302 19:02:50.818652 22626471084160 run.py:483] Algo bellman_ford step 9534 current loss 0.685254, current_train_items 305120.
I0302 19:02:50.838547 22626471084160 run.py:483] Algo bellman_ford step 9535 current loss 0.302874, current_train_items 305152.
I0302 19:02:50.854724 22626471084160 run.py:483] Algo bellman_ford step 9536 current loss 0.482481, current_train_items 305184.
I0302 19:02:50.878681 22626471084160 run.py:483] Algo bellman_ford step 9537 current loss 0.594362, current_train_items 305216.
I0302 19:02:50.911450 22626471084160 run.py:483] Algo bellman_ford step 9538 current loss 0.630125, current_train_items 305248.
I0302 19:02:50.943042 22626471084160 run.py:483] Algo bellman_ford step 9539 current loss 0.770059, current_train_items 305280.
I0302 19:02:50.962836 22626471084160 run.py:483] Algo bellman_ford step 9540 current loss 0.338242, current_train_items 305312.
I0302 19:02:50.978816 22626471084160 run.py:483] Algo bellman_ford step 9541 current loss 0.446792, current_train_items 305344.
I0302 19:02:51.002185 22626471084160 run.py:483] Algo bellman_ford step 9542 current loss 0.571400, current_train_items 305376.
I0302 19:02:51.035241 22626471084160 run.py:483] Algo bellman_ford step 9543 current loss 0.691485, current_train_items 305408.
I0302 19:02:51.068924 22626471084160 run.py:483] Algo bellman_ford step 9544 current loss 0.748152, current_train_items 305440.
I0302 19:02:51.088789 22626471084160 run.py:483] Algo bellman_ford step 9545 current loss 0.309984, current_train_items 305472.
I0302 19:02:51.104438 22626471084160 run.py:483] Algo bellman_ford step 9546 current loss 0.446705, current_train_items 305504.
I0302 19:02:51.126376 22626471084160 run.py:483] Algo bellman_ford step 9547 current loss 0.498691, current_train_items 305536.
I0302 19:02:51.157661 22626471084160 run.py:483] Algo bellman_ford step 9548 current loss 0.689773, current_train_items 305568.
I0302 19:02:51.191957 22626471084160 run.py:483] Algo bellman_ford step 9549 current loss 0.721510, current_train_items 305600.
I0302 19:02:51.211730 22626471084160 run.py:483] Algo bellman_ford step 9550 current loss 0.227190, current_train_items 305632.
I0302 19:02:51.219726 22626471084160 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.947265625, 'score': 0.947265625, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0302 19:02:51.219831 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.947, val scores are: bellman_ford: 0.947
I0302 19:02:51.236614 22626471084160 run.py:483] Algo bellman_ford step 9551 current loss 0.500931, current_train_items 305664.
I0302 19:02:51.261295 22626471084160 run.py:483] Algo bellman_ford step 9552 current loss 0.615855, current_train_items 305696.
I0302 19:02:51.294775 22626471084160 run.py:483] Algo bellman_ford step 9553 current loss 0.652794, current_train_items 305728.
I0302 19:02:51.328304 22626471084160 run.py:483] Algo bellman_ford step 9554 current loss 0.615006, current_train_items 305760.
I0302 19:02:51.348311 22626471084160 run.py:483] Algo bellman_ford step 9555 current loss 0.344561, current_train_items 305792.
I0302 19:02:51.363759 22626471084160 run.py:483] Algo bellman_ford step 9556 current loss 0.460476, current_train_items 305824.
I0302 19:02:51.387939 22626471084160 run.py:483] Algo bellman_ford step 9557 current loss 0.580651, current_train_items 305856.
I0302 19:02:51.419182 22626471084160 run.py:483] Algo bellman_ford step 9558 current loss 0.762238, current_train_items 305888.
I0302 19:02:51.452304 22626471084160 run.py:483] Algo bellman_ford step 9559 current loss 0.804552, current_train_items 305920.
I0302 19:02:51.472684 22626471084160 run.py:483] Algo bellman_ford step 9560 current loss 0.323001, current_train_items 305952.
I0302 19:02:51.488883 22626471084160 run.py:483] Algo bellman_ford step 9561 current loss 0.460409, current_train_items 305984.
I0302 19:02:51.512278 22626471084160 run.py:483] Algo bellman_ford step 9562 current loss 0.552592, current_train_items 306016.
I0302 19:02:51.543346 22626471084160 run.py:483] Algo bellman_ford step 9563 current loss 0.616365, current_train_items 306048.
I0302 19:02:51.576688 22626471084160 run.py:483] Algo bellman_ford step 9564 current loss 0.660461, current_train_items 306080.
I0302 19:02:51.596479 22626471084160 run.py:483] Algo bellman_ford step 9565 current loss 0.367597, current_train_items 306112.
I0302 19:02:51.613177 22626471084160 run.py:483] Algo bellman_ford step 9566 current loss 0.492686, current_train_items 306144.
I0302 19:02:51.637914 22626471084160 run.py:483] Algo bellman_ford step 9567 current loss 0.722544, current_train_items 306176.
I0302 19:02:51.670002 22626471084160 run.py:483] Algo bellman_ford step 9568 current loss 0.710886, current_train_items 306208.
I0302 19:02:51.703470 22626471084160 run.py:483] Algo bellman_ford step 9569 current loss 0.674852, current_train_items 306240.
I0302 19:02:51.723370 22626471084160 run.py:483] Algo bellman_ford step 9570 current loss 0.329805, current_train_items 306272.
I0302 19:02:51.739548 22626471084160 run.py:483] Algo bellman_ford step 9571 current loss 0.476029, current_train_items 306304.
I0302 19:02:51.764235 22626471084160 run.py:483] Algo bellman_ford step 9572 current loss 0.688744, current_train_items 306336.
I0302 19:02:51.795366 22626471084160 run.py:483] Algo bellman_ford step 9573 current loss 0.590176, current_train_items 306368.
I0302 19:02:51.829709 22626471084160 run.py:483] Algo bellman_ford step 9574 current loss 0.857637, current_train_items 306400.
I0302 19:02:51.849649 22626471084160 run.py:483] Algo bellman_ford step 9575 current loss 0.297836, current_train_items 306432.
I0302 19:02:51.866541 22626471084160 run.py:483] Algo bellman_ford step 9576 current loss 0.442514, current_train_items 306464.
I0302 19:02:51.891237 22626471084160 run.py:483] Algo bellman_ford step 9577 current loss 0.699789, current_train_items 306496.
I0302 19:02:51.922595 22626471084160 run.py:483] Algo bellman_ford step 9578 current loss 0.538335, current_train_items 306528.
I0302 19:02:51.956237 22626471084160 run.py:483] Algo bellman_ford step 9579 current loss 0.704846, current_train_items 306560.
I0302 19:02:51.976092 22626471084160 run.py:483] Algo bellman_ford step 9580 current loss 0.306234, current_train_items 306592.
I0302 19:02:51.992091 22626471084160 run.py:483] Algo bellman_ford step 9581 current loss 0.471653, current_train_items 306624.
I0302 19:02:52.017531 22626471084160 run.py:483] Algo bellman_ford step 9582 current loss 0.578600, current_train_items 306656.
I0302 19:02:52.050511 22626471084160 run.py:483] Algo bellman_ford step 9583 current loss 0.669792, current_train_items 306688.
I0302 19:02:52.085206 22626471084160 run.py:483] Algo bellman_ford step 9584 current loss 0.746734, current_train_items 306720.
I0302 19:02:52.105048 22626471084160 run.py:483] Algo bellman_ford step 9585 current loss 0.342195, current_train_items 306752.
I0302 19:02:52.121387 22626471084160 run.py:483] Algo bellman_ford step 9586 current loss 0.402253, current_train_items 306784.
I0302 19:02:52.145361 22626471084160 run.py:483] Algo bellman_ford step 9587 current loss 0.575418, current_train_items 306816.
I0302 19:02:52.177501 22626471084160 run.py:483] Algo bellman_ford step 9588 current loss 0.677209, current_train_items 306848.
I0302 19:02:52.212531 22626471084160 run.py:483] Algo bellman_ford step 9589 current loss 0.823709, current_train_items 306880.
I0302 19:02:52.232403 22626471084160 run.py:483] Algo bellman_ford step 9590 current loss 0.258604, current_train_items 306912.
I0302 19:02:52.248647 22626471084160 run.py:483] Algo bellman_ford step 9591 current loss 0.440817, current_train_items 306944.
I0302 19:02:52.272359 22626471084160 run.py:483] Algo bellman_ford step 9592 current loss 0.657781, current_train_items 306976.
I0302 19:02:52.304301 22626471084160 run.py:483] Algo bellman_ford step 9593 current loss 0.590202, current_train_items 307008.
I0302 19:02:52.337346 22626471084160 run.py:483] Algo bellman_ford step 9594 current loss 0.681337, current_train_items 307040.
I0302 19:02:52.357231 22626471084160 run.py:483] Algo bellman_ford step 9595 current loss 0.295569, current_train_items 307072.
I0302 19:02:52.373720 22626471084160 run.py:483] Algo bellman_ford step 9596 current loss 0.450439, current_train_items 307104.
I0302 19:02:52.398682 22626471084160 run.py:483] Algo bellman_ford step 9597 current loss 0.557898, current_train_items 307136.
I0302 19:02:52.429273 22626471084160 run.py:483] Algo bellman_ford step 9598 current loss 0.515117, current_train_items 307168.
I0302 19:02:52.462998 22626471084160 run.py:483] Algo bellman_ford step 9599 current loss 0.732287, current_train_items 307200.
I0302 19:02:52.482871 22626471084160 run.py:483] Algo bellman_ford step 9600 current loss 0.301779, current_train_items 307232.
I0302 19:02:52.490585 22626471084160 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.9462890625, 'score': 0.9462890625, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0302 19:02:52.490690 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.946, val scores are: bellman_ford: 0.946
I0302 19:02:52.507955 22626471084160 run.py:483] Algo bellman_ford step 9601 current loss 0.504630, current_train_items 307264.
I0302 19:02:52.532993 22626471084160 run.py:483] Algo bellman_ford step 9602 current loss 0.521591, current_train_items 307296.
I0302 19:02:52.564765 22626471084160 run.py:483] Algo bellman_ford step 9603 current loss 0.548283, current_train_items 307328.
I0302 19:02:52.599973 22626471084160 run.py:483] Algo bellman_ford step 9604 current loss 0.757370, current_train_items 307360.
I0302 19:02:52.620300 22626471084160 run.py:483] Algo bellman_ford step 9605 current loss 0.304191, current_train_items 307392.
I0302 19:02:52.636009 22626471084160 run.py:483] Algo bellman_ford step 9606 current loss 0.451201, current_train_items 307424.
I0302 19:02:52.659868 22626471084160 run.py:483] Algo bellman_ford step 9607 current loss 0.523057, current_train_items 307456.
I0302 19:02:52.691203 22626471084160 run.py:483] Algo bellman_ford step 9608 current loss 0.617446, current_train_items 307488.
I0302 19:02:52.724686 22626471084160 run.py:483] Algo bellman_ford step 9609 current loss 0.720221, current_train_items 307520.
I0302 19:02:52.744591 22626471084160 run.py:483] Algo bellman_ford step 9610 current loss 0.326206, current_train_items 307552.
I0302 19:02:52.760903 22626471084160 run.py:483] Algo bellman_ford step 9611 current loss 0.463446, current_train_items 307584.
I0302 19:02:52.785547 22626471084160 run.py:483] Algo bellman_ford step 9612 current loss 0.650533, current_train_items 307616.
I0302 19:02:52.817101 22626471084160 run.py:483] Algo bellman_ford step 9613 current loss 0.704058, current_train_items 307648.
I0302 19:02:52.851018 22626471084160 run.py:483] Algo bellman_ford step 9614 current loss 0.759841, current_train_items 307680.
I0302 19:02:52.870973 22626471084160 run.py:483] Algo bellman_ford step 9615 current loss 0.339413, current_train_items 307712.
I0302 19:02:52.887281 22626471084160 run.py:483] Algo bellman_ford step 9616 current loss 0.530030, current_train_items 307744.
I0302 19:02:52.911293 22626471084160 run.py:483] Algo bellman_ford step 9617 current loss 0.701743, current_train_items 307776.
I0302 19:02:52.942812 22626471084160 run.py:483] Algo bellman_ford step 9618 current loss 0.609779, current_train_items 307808.
I0302 19:02:52.976705 22626471084160 run.py:483] Algo bellman_ford step 9619 current loss 0.818549, current_train_items 307840.
I0302 19:02:52.996316 22626471084160 run.py:483] Algo bellman_ford step 9620 current loss 0.312979, current_train_items 307872.
I0302 19:02:53.012385 22626471084160 run.py:483] Algo bellman_ford step 9621 current loss 0.427627, current_train_items 307904.
I0302 19:02:53.036911 22626471084160 run.py:483] Algo bellman_ford step 9622 current loss 0.670360, current_train_items 307936.
I0302 19:02:53.068497 22626471084160 run.py:483] Algo bellman_ford step 9623 current loss 0.608754, current_train_items 307968.
I0302 19:02:53.102850 22626471084160 run.py:483] Algo bellman_ford step 9624 current loss 0.853252, current_train_items 308000.
I0302 19:02:53.122530 22626471084160 run.py:483] Algo bellman_ford step 9625 current loss 0.326368, current_train_items 308032.
I0302 19:02:53.138715 22626471084160 run.py:483] Algo bellman_ford step 9626 current loss 0.450596, current_train_items 308064.
I0302 19:02:53.162689 22626471084160 run.py:483] Algo bellman_ford step 9627 current loss 0.541837, current_train_items 308096.
I0302 19:02:53.194143 22626471084160 run.py:483] Algo bellman_ford step 9628 current loss 0.584300, current_train_items 308128.
I0302 19:02:53.228773 22626471084160 run.py:483] Algo bellman_ford step 9629 current loss 0.805981, current_train_items 308160.
I0302 19:02:53.248620 22626471084160 run.py:483] Algo bellman_ford step 9630 current loss 0.278879, current_train_items 308192.
I0302 19:02:53.265240 22626471084160 run.py:483] Algo bellman_ford step 9631 current loss 0.375721, current_train_items 308224.
I0302 19:02:53.290207 22626471084160 run.py:483] Algo bellman_ford step 9632 current loss 0.663413, current_train_items 308256.
I0302 19:02:53.321229 22626471084160 run.py:483] Algo bellman_ford step 9633 current loss 0.603944, current_train_items 308288.
I0302 19:02:53.355759 22626471084160 run.py:483] Algo bellman_ford step 9634 current loss 0.795315, current_train_items 308320.
I0302 19:02:53.375627 22626471084160 run.py:483] Algo bellman_ford step 9635 current loss 0.295979, current_train_items 308352.
I0302 19:02:53.391818 22626471084160 run.py:483] Algo bellman_ford step 9636 current loss 0.440009, current_train_items 308384.
I0302 19:02:53.416320 22626471084160 run.py:483] Algo bellman_ford step 9637 current loss 0.604379, current_train_items 308416.
I0302 19:02:53.448599 22626471084160 run.py:483] Algo bellman_ford step 9638 current loss 0.701135, current_train_items 308448.
I0302 19:02:53.482192 22626471084160 run.py:483] Algo bellman_ford step 9639 current loss 0.693523, current_train_items 308480.
I0302 19:02:53.502162 22626471084160 run.py:483] Algo bellman_ford step 9640 current loss 0.216744, current_train_items 308512.
I0302 19:02:53.518021 22626471084160 run.py:483] Algo bellman_ford step 9641 current loss 0.363343, current_train_items 308544.
I0302 19:02:53.542599 22626471084160 run.py:483] Algo bellman_ford step 9642 current loss 0.607845, current_train_items 308576.
I0302 19:02:53.575904 22626471084160 run.py:483] Algo bellman_ford step 9643 current loss 0.663890, current_train_items 308608.
I0302 19:02:53.607491 22626471084160 run.py:483] Algo bellman_ford step 9644 current loss 0.796868, current_train_items 308640.
I0302 19:02:53.627133 22626471084160 run.py:483] Algo bellman_ford step 9645 current loss 0.258532, current_train_items 308672.
I0302 19:02:53.643461 22626471084160 run.py:483] Algo bellman_ford step 9646 current loss 0.531171, current_train_items 308704.
I0302 19:02:53.667000 22626471084160 run.py:483] Algo bellman_ford step 9647 current loss 0.554477, current_train_items 308736.
I0302 19:02:53.699458 22626471084160 run.py:483] Algo bellman_ford step 9648 current loss 0.604694, current_train_items 308768.
I0302 19:02:53.733980 22626471084160 run.py:483] Algo bellman_ford step 9649 current loss 0.709054, current_train_items 308800.
I0302 19:02:53.753527 22626471084160 run.py:483] Algo bellman_ford step 9650 current loss 0.359466, current_train_items 308832.
I0302 19:02:53.761562 22626471084160 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0302 19:02:53.761696 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 19:02:53.778631 22626471084160 run.py:483] Algo bellman_ford step 9651 current loss 0.467797, current_train_items 308864.
I0302 19:02:53.804446 22626471084160 run.py:483] Algo bellman_ford step 9652 current loss 0.694616, current_train_items 308896.
I0302 19:02:53.836956 22626471084160 run.py:483] Algo bellman_ford step 9653 current loss 0.739221, current_train_items 308928.
I0302 19:02:53.872799 22626471084160 run.py:483] Algo bellman_ford step 9654 current loss 0.778980, current_train_items 308960.
I0302 19:02:53.893121 22626471084160 run.py:483] Algo bellman_ford step 9655 current loss 0.332427, current_train_items 308992.
I0302 19:02:53.909224 22626471084160 run.py:483] Algo bellman_ford step 9656 current loss 0.482485, current_train_items 309024.
I0302 19:02:53.933393 22626471084160 run.py:483] Algo bellman_ford step 9657 current loss 0.510893, current_train_items 309056.
I0302 19:02:53.966058 22626471084160 run.py:483] Algo bellman_ford step 9658 current loss 0.593954, current_train_items 309088.
I0302 19:02:53.998770 22626471084160 run.py:483] Algo bellman_ford step 9659 current loss 0.693517, current_train_items 309120.
I0302 19:02:54.018523 22626471084160 run.py:483] Algo bellman_ford step 9660 current loss 0.288173, current_train_items 309152.
I0302 19:02:54.035059 22626471084160 run.py:483] Algo bellman_ford step 9661 current loss 0.435779, current_train_items 309184.
I0302 19:02:54.058749 22626471084160 run.py:483] Algo bellman_ford step 9662 current loss 0.605078, current_train_items 309216.
I0302 19:02:54.090216 22626471084160 run.py:483] Algo bellman_ford step 9663 current loss 0.571134, current_train_items 309248.
I0302 19:02:54.124526 22626471084160 run.py:483] Algo bellman_ford step 9664 current loss 0.740581, current_train_items 309280.
I0302 19:02:54.144379 22626471084160 run.py:483] Algo bellman_ford step 9665 current loss 0.337999, current_train_items 309312.
I0302 19:02:54.160801 22626471084160 run.py:483] Algo bellman_ford step 9666 current loss 0.424399, current_train_items 309344.
I0302 19:02:54.183887 22626471084160 run.py:483] Algo bellman_ford step 9667 current loss 0.606126, current_train_items 309376.
I0302 19:02:54.215306 22626471084160 run.py:483] Algo bellman_ford step 9668 current loss 0.569070, current_train_items 309408.
I0302 19:02:54.248742 22626471084160 run.py:483] Algo bellman_ford step 9669 current loss 0.682849, current_train_items 309440.
I0302 19:02:54.269068 22626471084160 run.py:483] Algo bellman_ford step 9670 current loss 0.285559, current_train_items 309472.
I0302 19:02:54.285324 22626471084160 run.py:483] Algo bellman_ford step 9671 current loss 0.426332, current_train_items 309504.
I0302 19:02:54.309285 22626471084160 run.py:483] Algo bellman_ford step 9672 current loss 0.575225, current_train_items 309536.
I0302 19:02:54.340304 22626471084160 run.py:483] Algo bellman_ford step 9673 current loss 0.514802, current_train_items 309568.
I0302 19:02:54.373730 22626471084160 run.py:483] Algo bellman_ford step 9674 current loss 0.696111, current_train_items 309600.
I0302 19:02:54.393861 22626471084160 run.py:483] Algo bellman_ford step 9675 current loss 0.254801, current_train_items 309632.
I0302 19:02:54.410503 22626471084160 run.py:483] Algo bellman_ford step 9676 current loss 0.453275, current_train_items 309664.
I0302 19:02:54.434627 22626471084160 run.py:483] Algo bellman_ford step 9677 current loss 0.517163, current_train_items 309696.
I0302 19:02:54.464885 22626471084160 run.py:483] Algo bellman_ford step 9678 current loss 0.607343, current_train_items 309728.
I0302 19:02:54.499311 22626471084160 run.py:483] Algo bellman_ford step 9679 current loss 0.686797, current_train_items 309760.
I0302 19:02:54.519109 22626471084160 run.py:483] Algo bellman_ford step 9680 current loss 0.272960, current_train_items 309792.
I0302 19:02:54.535598 22626471084160 run.py:483] Algo bellman_ford step 9681 current loss 0.415200, current_train_items 309824.
I0302 19:02:54.559277 22626471084160 run.py:483] Algo bellman_ford step 9682 current loss 0.494664, current_train_items 309856.
I0302 19:02:54.590701 22626471084160 run.py:483] Algo bellman_ford step 9683 current loss 0.685441, current_train_items 309888.
I0302 19:02:54.623378 22626471084160 run.py:483] Algo bellman_ford step 9684 current loss 0.609688, current_train_items 309920.
I0302 19:02:54.643455 22626471084160 run.py:483] Algo bellman_ford step 9685 current loss 0.294912, current_train_items 309952.
I0302 19:02:54.659618 22626471084160 run.py:483] Algo bellman_ford step 9686 current loss 0.471253, current_train_items 309984.
I0302 19:02:54.683695 22626471084160 run.py:483] Algo bellman_ford step 9687 current loss 0.615916, current_train_items 310016.
I0302 19:02:54.715075 22626471084160 run.py:483] Algo bellman_ford step 9688 current loss 0.702860, current_train_items 310048.
I0302 19:02:54.748977 22626471084160 run.py:483] Algo bellman_ford step 9689 current loss 0.698478, current_train_items 310080.
I0302 19:02:54.769096 22626471084160 run.py:483] Algo bellman_ford step 9690 current loss 0.284715, current_train_items 310112.
I0302 19:02:54.784911 22626471084160 run.py:483] Algo bellman_ford step 9691 current loss 0.443327, current_train_items 310144.
I0302 19:02:54.808708 22626471084160 run.py:483] Algo bellman_ford step 9692 current loss 0.475074, current_train_items 310176.
I0302 19:02:54.839819 22626471084160 run.py:483] Algo bellman_ford step 9693 current loss 0.680464, current_train_items 310208.
I0302 19:02:54.873711 22626471084160 run.py:483] Algo bellman_ford step 9694 current loss 0.686221, current_train_items 310240.
I0302 19:02:54.893460 22626471084160 run.py:483] Algo bellman_ford step 9695 current loss 0.314697, current_train_items 310272.
I0302 19:02:54.909579 22626471084160 run.py:483] Algo bellman_ford step 9696 current loss 0.500190, current_train_items 310304.
I0302 19:02:54.933063 22626471084160 run.py:483] Algo bellman_ford step 9697 current loss 0.493746, current_train_items 310336.
I0302 19:02:54.967375 22626471084160 run.py:483] Algo bellman_ford step 9698 current loss 0.857016, current_train_items 310368.
I0302 19:02:55.002803 22626471084160 run.py:483] Algo bellman_ford step 9699 current loss 0.919770, current_train_items 310400.
I0302 19:02:55.022675 22626471084160 run.py:483] Algo bellman_ford step 9700 current loss 0.403751, current_train_items 310432.
I0302 19:02:55.030259 22626471084160 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0302 19:02:55.030368 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:02:55.047143 22626471084160 run.py:483] Algo bellman_ford step 9701 current loss 0.523432, current_train_items 310464.
I0302 19:02:55.072240 22626471084160 run.py:483] Algo bellman_ford step 9702 current loss 0.603244, current_train_items 310496.
I0302 19:02:55.104959 22626471084160 run.py:483] Algo bellman_ford step 9703 current loss 0.672602, current_train_items 310528.
I0302 19:02:55.138852 22626471084160 run.py:483] Algo bellman_ford step 9704 current loss 0.698075, current_train_items 310560.
I0302 19:02:55.159437 22626471084160 run.py:483] Algo bellman_ford step 9705 current loss 0.336763, current_train_items 310592.
I0302 19:02:55.175403 22626471084160 run.py:483] Algo bellman_ford step 9706 current loss 0.483792, current_train_items 310624.
I0302 19:02:55.200652 22626471084160 run.py:483] Algo bellman_ford step 9707 current loss 0.735244, current_train_items 310656.
I0302 19:02:55.233252 22626471084160 run.py:483] Algo bellman_ford step 9708 current loss 0.660641, current_train_items 310688.
I0302 19:02:55.264854 22626471084160 run.py:483] Algo bellman_ford step 9709 current loss 0.695311, current_train_items 310720.
I0302 19:02:55.284693 22626471084160 run.py:483] Algo bellman_ford step 9710 current loss 0.292093, current_train_items 310752.
I0302 19:02:55.300760 22626471084160 run.py:483] Algo bellman_ford step 9711 current loss 0.477301, current_train_items 310784.
I0302 19:02:55.324969 22626471084160 run.py:483] Algo bellman_ford step 9712 current loss 0.691925, current_train_items 310816.
I0302 19:02:55.357375 22626471084160 run.py:483] Algo bellman_ford step 9713 current loss 0.585716, current_train_items 310848.
I0302 19:02:55.391731 22626471084160 run.py:483] Algo bellman_ford step 9714 current loss 0.751140, current_train_items 310880.
I0302 19:02:55.411575 22626471084160 run.py:483] Algo bellman_ford step 9715 current loss 0.296328, current_train_items 310912.
I0302 19:02:55.428188 22626471084160 run.py:483] Algo bellman_ford step 9716 current loss 0.464744, current_train_items 310944.
I0302 19:02:55.450869 22626471084160 run.py:483] Algo bellman_ford step 9717 current loss 0.608866, current_train_items 310976.
I0302 19:02:55.482633 22626471084160 run.py:483] Algo bellman_ford step 9718 current loss 0.695215, current_train_items 311008.
I0302 19:02:55.513546 22626471084160 run.py:483] Algo bellman_ford step 9719 current loss 0.509982, current_train_items 311040.
I0302 19:02:55.533044 22626471084160 run.py:483] Algo bellman_ford step 9720 current loss 0.300111, current_train_items 311072.
I0302 19:02:55.548821 22626471084160 run.py:483] Algo bellman_ford step 9721 current loss 0.395752, current_train_items 311104.
I0302 19:02:55.572553 22626471084160 run.py:483] Algo bellman_ford step 9722 current loss 0.565417, current_train_items 311136.
I0302 19:02:55.604602 22626471084160 run.py:483] Algo bellman_ford step 9723 current loss 0.674051, current_train_items 311168.
I0302 19:02:55.638341 22626471084160 run.py:483] Algo bellman_ford step 9724 current loss 0.673657, current_train_items 311200.
I0302 19:02:55.658242 22626471084160 run.py:483] Algo bellman_ford step 9725 current loss 0.280518, current_train_items 311232.
I0302 19:02:55.674250 22626471084160 run.py:483] Algo bellman_ford step 9726 current loss 0.474138, current_train_items 311264.
I0302 19:02:55.697562 22626471084160 run.py:483] Algo bellman_ford step 9727 current loss 0.484902, current_train_items 311296.
I0302 19:02:55.729905 22626471084160 run.py:483] Algo bellman_ford step 9728 current loss 0.592215, current_train_items 311328.
I0302 19:02:55.764325 22626471084160 run.py:483] Algo bellman_ford step 9729 current loss 0.695907, current_train_items 311360.
I0302 19:02:55.784361 22626471084160 run.py:483] Algo bellman_ford step 9730 current loss 0.290074, current_train_items 311392.
I0302 19:02:55.800734 22626471084160 run.py:483] Algo bellman_ford step 9731 current loss 0.441122, current_train_items 311424.
I0302 19:02:55.824369 22626471084160 run.py:483] Algo bellman_ford step 9732 current loss 0.641527, current_train_items 311456.
I0302 19:02:55.855483 22626471084160 run.py:483] Algo bellman_ford step 9733 current loss 0.659975, current_train_items 311488.
I0302 19:02:55.888520 22626471084160 run.py:483] Algo bellman_ford step 9734 current loss 0.631830, current_train_items 311520.
I0302 19:02:55.908292 22626471084160 run.py:483] Algo bellman_ford step 9735 current loss 0.283096, current_train_items 311552.
I0302 19:02:55.924456 22626471084160 run.py:483] Algo bellman_ford step 9736 current loss 0.506311, current_train_items 311584.
I0302 19:02:55.948604 22626471084160 run.py:483] Algo bellman_ford step 9737 current loss 0.620093, current_train_items 311616.
I0302 19:02:55.981679 22626471084160 run.py:483] Algo bellman_ford step 9738 current loss 0.650239, current_train_items 311648.
I0302 19:02:56.016080 22626471084160 run.py:483] Algo bellman_ford step 9739 current loss 0.716181, current_train_items 311680.
I0302 19:02:56.035809 22626471084160 run.py:483] Algo bellman_ford step 9740 current loss 0.296980, current_train_items 311712.
I0302 19:02:56.052733 22626471084160 run.py:483] Algo bellman_ford step 9741 current loss 0.447570, current_train_items 311744.
I0302 19:02:56.078457 22626471084160 run.py:483] Algo bellman_ford step 9742 current loss 0.658887, current_train_items 311776.
I0302 19:02:56.110722 22626471084160 run.py:483] Algo bellman_ford step 9743 current loss 0.659674, current_train_items 311808.
I0302 19:02:56.144510 22626471084160 run.py:483] Algo bellman_ford step 9744 current loss 0.678517, current_train_items 311840.
I0302 19:02:56.164306 22626471084160 run.py:483] Algo bellman_ford step 9745 current loss 0.245527, current_train_items 311872.
I0302 19:02:56.181147 22626471084160 run.py:483] Algo bellman_ford step 9746 current loss 0.430231, current_train_items 311904.
I0302 19:02:56.204941 22626471084160 run.py:483] Algo bellman_ford step 9747 current loss 0.617098, current_train_items 311936.
I0302 19:02:56.234622 22626471084160 run.py:483] Algo bellman_ford step 9748 current loss 0.547631, current_train_items 311968.
I0302 19:02:56.266918 22626471084160 run.py:483] Algo bellman_ford step 9749 current loss 0.739513, current_train_items 312000.
I0302 19:02:56.286411 22626471084160 run.py:483] Algo bellman_ford step 9750 current loss 0.296407, current_train_items 312032.
I0302 19:02:56.294555 22626471084160 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0302 19:02:56.294661 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0302 19:02:56.311167 22626471084160 run.py:483] Algo bellman_ford step 9751 current loss 0.494576, current_train_items 312064.
I0302 19:02:56.336975 22626471084160 run.py:483] Algo bellman_ford step 9752 current loss 0.615930, current_train_items 312096.
I0302 19:02:56.368000 22626471084160 run.py:483] Algo bellman_ford step 9753 current loss 0.711251, current_train_items 312128.
I0302 19:02:56.401307 22626471084160 run.py:483] Algo bellman_ford step 9754 current loss 0.666185, current_train_items 312160.
I0302 19:02:56.421546 22626471084160 run.py:483] Algo bellman_ford step 9755 current loss 0.356217, current_train_items 312192.
I0302 19:02:56.438178 22626471084160 run.py:483] Algo bellman_ford step 9756 current loss 0.498009, current_train_items 312224.
I0302 19:02:56.460976 22626471084160 run.py:483] Algo bellman_ford step 9757 current loss 0.474281, current_train_items 312256.
I0302 19:02:56.493482 22626471084160 run.py:483] Algo bellman_ford step 9758 current loss 0.643931, current_train_items 312288.
I0302 19:02:56.526808 22626471084160 run.py:483] Algo bellman_ford step 9759 current loss 0.664945, current_train_items 312320.
I0302 19:02:56.547204 22626471084160 run.py:483] Algo bellman_ford step 9760 current loss 0.298099, current_train_items 312352.
I0302 19:02:56.563831 22626471084160 run.py:483] Algo bellman_ford step 9761 current loss 0.466649, current_train_items 312384.
I0302 19:02:56.587904 22626471084160 run.py:483] Algo bellman_ford step 9762 current loss 0.612996, current_train_items 312416.
I0302 19:02:56.620083 22626471084160 run.py:483] Algo bellman_ford step 9763 current loss 0.645274, current_train_items 312448.
I0302 19:02:56.651283 22626471084160 run.py:483] Algo bellman_ford step 9764 current loss 0.629096, current_train_items 312480.
I0302 19:02:56.670941 22626471084160 run.py:483] Algo bellman_ford step 9765 current loss 0.261678, current_train_items 312512.
I0302 19:02:56.686788 22626471084160 run.py:483] Algo bellman_ford step 9766 current loss 0.435764, current_train_items 312544.
I0302 19:02:56.710994 22626471084160 run.py:483] Algo bellman_ford step 9767 current loss 0.678807, current_train_items 312576.
I0302 19:02:56.742151 22626471084160 run.py:483] Algo bellman_ford step 9768 current loss 0.616916, current_train_items 312608.
I0302 19:02:56.776309 22626471084160 run.py:483] Algo bellman_ford step 9769 current loss 0.723465, current_train_items 312640.
I0302 19:02:56.796597 22626471084160 run.py:483] Algo bellman_ford step 9770 current loss 0.275717, current_train_items 312672.
I0302 19:02:56.812529 22626471084160 run.py:483] Algo bellman_ford step 9771 current loss 0.478711, current_train_items 312704.
I0302 19:02:56.835810 22626471084160 run.py:483] Algo bellman_ford step 9772 current loss 0.550932, current_train_items 312736.
I0302 19:02:56.868652 22626471084160 run.py:483] Algo bellman_ford step 9773 current loss 0.714239, current_train_items 312768.
I0302 19:02:56.902802 22626471084160 run.py:483] Algo bellman_ford step 9774 current loss 0.795662, current_train_items 312800.
I0302 19:02:56.922800 22626471084160 run.py:483] Algo bellman_ford step 9775 current loss 0.327026, current_train_items 312832.
I0302 19:02:56.938758 22626471084160 run.py:483] Algo bellman_ford step 9776 current loss 0.413000, current_train_items 312864.
I0302 19:02:56.963423 22626471084160 run.py:483] Algo bellman_ford step 9777 current loss 0.578950, current_train_items 312896.
I0302 19:02:56.995775 22626471084160 run.py:483] Algo bellman_ford step 9778 current loss 0.675101, current_train_items 312928.
I0302 19:02:57.029850 22626471084160 run.py:483] Algo bellman_ford step 9779 current loss 0.694913, current_train_items 312960.
I0302 19:02:57.049764 22626471084160 run.py:483] Algo bellman_ford step 9780 current loss 0.337465, current_train_items 312992.
I0302 19:02:57.066214 22626471084160 run.py:483] Algo bellman_ford step 9781 current loss 0.426870, current_train_items 313024.
I0302 19:02:57.089877 22626471084160 run.py:483] Algo bellman_ford step 9782 current loss 0.529395, current_train_items 313056.
I0302 19:02:57.123290 22626471084160 run.py:483] Algo bellman_ford step 9783 current loss 0.619166, current_train_items 313088.
I0302 19:02:57.155596 22626471084160 run.py:483] Algo bellman_ford step 9784 current loss 0.649848, current_train_items 313120.
I0302 19:02:57.175718 22626471084160 run.py:483] Algo bellman_ford step 9785 current loss 0.257518, current_train_items 313152.
I0302 19:02:57.192324 22626471084160 run.py:483] Algo bellman_ford step 9786 current loss 0.501323, current_train_items 313184.
I0302 19:02:57.216053 22626471084160 run.py:483] Algo bellman_ford step 9787 current loss 0.591709, current_train_items 313216.
I0302 19:02:57.246965 22626471084160 run.py:483] Algo bellman_ford step 9788 current loss 0.643578, current_train_items 313248.
I0302 19:02:57.280557 22626471084160 run.py:483] Algo bellman_ford step 9789 current loss 0.785430, current_train_items 313280.
I0302 19:02:57.300494 22626471084160 run.py:483] Algo bellman_ford step 9790 current loss 0.340464, current_train_items 313312.
I0302 19:02:57.316583 22626471084160 run.py:483] Algo bellman_ford step 9791 current loss 0.493240, current_train_items 313344.
I0302 19:02:57.339959 22626471084160 run.py:483] Algo bellman_ford step 9792 current loss 0.713504, current_train_items 313376.
I0302 19:02:57.371578 22626471084160 run.py:483] Algo bellman_ford step 9793 current loss 0.757062, current_train_items 313408.
I0302 19:02:57.403573 22626471084160 run.py:483] Algo bellman_ford step 9794 current loss 0.794180, current_train_items 313440.
I0302 19:02:57.423502 22626471084160 run.py:483] Algo bellman_ford step 9795 current loss 0.332658, current_train_items 313472.
I0302 19:02:57.440005 22626471084160 run.py:483] Algo bellman_ford step 9796 current loss 0.483390, current_train_items 313504.
I0302 19:02:57.463775 22626471084160 run.py:483] Algo bellman_ford step 9797 current loss 0.555847, current_train_items 313536.
I0302 19:02:57.495054 22626471084160 run.py:483] Algo bellman_ford step 9798 current loss 0.627507, current_train_items 313568.
I0302 19:02:57.527917 22626471084160 run.py:483] Algo bellman_ford step 9799 current loss 0.681549, current_train_items 313600.
I0302 19:02:57.548043 22626471084160 run.py:483] Algo bellman_ford step 9800 current loss 0.360706, current_train_items 313632.
I0302 19:02:57.555747 22626471084160 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0302 19:02:57.555851 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:02:57.572331 22626471084160 run.py:483] Algo bellman_ford step 9801 current loss 0.455752, current_train_items 313664.
I0302 19:02:57.597144 22626471084160 run.py:483] Algo bellman_ford step 9802 current loss 0.668752, current_train_items 313696.
I0302 19:02:57.629868 22626471084160 run.py:483] Algo bellman_ford step 9803 current loss 0.834212, current_train_items 313728.
I0302 19:02:57.662945 22626471084160 run.py:483] Algo bellman_ford step 9804 current loss 0.686866, current_train_items 313760.
I0302 19:02:57.683159 22626471084160 run.py:483] Algo bellman_ford step 9805 current loss 0.368887, current_train_items 313792.
I0302 19:02:57.699256 22626471084160 run.py:483] Algo bellman_ford step 9806 current loss 0.534600, current_train_items 313824.
I0302 19:02:57.722523 22626471084160 run.py:483] Algo bellman_ford step 9807 current loss 0.611626, current_train_items 313856.
I0302 19:02:57.754182 22626471084160 run.py:483] Algo bellman_ford step 9808 current loss 0.744387, current_train_items 313888.
I0302 19:02:57.789875 22626471084160 run.py:483] Algo bellman_ford step 9809 current loss 0.946932, current_train_items 313920.
I0302 19:02:57.809614 22626471084160 run.py:483] Algo bellman_ford step 9810 current loss 0.257739, current_train_items 313952.
I0302 19:02:57.825452 22626471084160 run.py:483] Algo bellman_ford step 9811 current loss 0.440221, current_train_items 313984.
I0302 19:02:57.850539 22626471084160 run.py:483] Algo bellman_ford step 9812 current loss 0.661356, current_train_items 314016.
I0302 19:02:57.880269 22626471084160 run.py:483] Algo bellman_ford step 9813 current loss 0.598288, current_train_items 314048.
I0302 19:02:57.914124 22626471084160 run.py:483] Algo bellman_ford step 9814 current loss 0.706783, current_train_items 314080.
I0302 19:02:57.933582 22626471084160 run.py:483] Algo bellman_ford step 9815 current loss 0.289526, current_train_items 314112.
I0302 19:02:57.950018 22626471084160 run.py:483] Algo bellman_ford step 9816 current loss 0.460207, current_train_items 314144.
I0302 19:02:57.973386 22626471084160 run.py:483] Algo bellman_ford step 9817 current loss 0.595976, current_train_items 314176.
I0302 19:02:58.006144 22626471084160 run.py:483] Algo bellman_ford step 9818 current loss 0.659698, current_train_items 314208.
I0302 19:02:58.039323 22626471084160 run.py:483] Algo bellman_ford step 9819 current loss 0.763898, current_train_items 314240.
I0302 19:02:58.058861 22626471084160 run.py:483] Algo bellman_ford step 9820 current loss 0.328262, current_train_items 314272.
I0302 19:02:58.074905 22626471084160 run.py:483] Algo bellman_ford step 9821 current loss 0.513261, current_train_items 314304.
I0302 19:02:58.099415 22626471084160 run.py:483] Algo bellman_ford step 9822 current loss 0.602126, current_train_items 314336.
I0302 19:02:58.130473 22626471084160 run.py:483] Algo bellman_ford step 9823 current loss 0.527628, current_train_items 314368.
I0302 19:02:58.163590 22626471084160 run.py:483] Algo bellman_ford step 9824 current loss 0.754940, current_train_items 314400.
I0302 19:02:58.183176 22626471084160 run.py:483] Algo bellman_ford step 9825 current loss 0.285095, current_train_items 314432.
I0302 19:02:58.199071 22626471084160 run.py:483] Algo bellman_ford step 9826 current loss 0.436694, current_train_items 314464.
I0302 19:02:58.222864 22626471084160 run.py:483] Algo bellman_ford step 9827 current loss 0.595241, current_train_items 314496.
I0302 19:02:58.253501 22626471084160 run.py:483] Algo bellman_ford step 9828 current loss 0.687646, current_train_items 314528.
I0302 19:02:58.288184 22626471084160 run.py:483] Algo bellman_ford step 9829 current loss 0.718336, current_train_items 314560.
I0302 19:02:58.307821 22626471084160 run.py:483] Algo bellman_ford step 9830 current loss 0.252936, current_train_items 314592.
I0302 19:02:58.324575 22626471084160 run.py:483] Algo bellman_ford step 9831 current loss 0.476979, current_train_items 314624.
I0302 19:02:58.348524 22626471084160 run.py:483] Algo bellman_ford step 9832 current loss 0.594147, current_train_items 314656.
I0302 19:02:58.380708 22626471084160 run.py:483] Algo bellman_ford step 9833 current loss 0.657548, current_train_items 314688.
I0302 19:02:58.412867 22626471084160 run.py:483] Algo bellman_ford step 9834 current loss 0.668468, current_train_items 314720.
I0302 19:02:58.432251 22626471084160 run.py:483] Algo bellman_ford step 9835 current loss 0.252925, current_train_items 314752.
I0302 19:02:58.448395 22626471084160 run.py:483] Algo bellman_ford step 9836 current loss 0.428641, current_train_items 314784.
I0302 19:02:58.472309 22626471084160 run.py:483] Algo bellman_ford step 9837 current loss 0.668576, current_train_items 314816.
I0302 19:02:58.504873 22626471084160 run.py:483] Algo bellman_ford step 9838 current loss 0.752987, current_train_items 314848.
I0302 19:02:58.540141 22626471084160 run.py:483] Algo bellman_ford step 9839 current loss 0.785095, current_train_items 314880.
I0302 19:02:58.559672 22626471084160 run.py:483] Algo bellman_ford step 9840 current loss 0.275608, current_train_items 314912.
I0302 19:02:58.575864 22626471084160 run.py:483] Algo bellman_ford step 9841 current loss 0.427668, current_train_items 314944.
I0302 19:02:58.600171 22626471084160 run.py:483] Algo bellman_ford step 9842 current loss 0.539448, current_train_items 314976.
I0302 19:02:58.631454 22626471084160 run.py:483] Algo bellman_ford step 9843 current loss 0.593148, current_train_items 315008.
I0302 19:02:58.662749 22626471084160 run.py:483] Algo bellman_ford step 9844 current loss 0.658186, current_train_items 315040.
I0302 19:02:58.682378 22626471084160 run.py:483] Algo bellman_ford step 9845 current loss 0.294979, current_train_items 315072.
I0302 19:02:58.698779 22626471084160 run.py:483] Algo bellman_ford step 9846 current loss 0.460648, current_train_items 315104.
I0302 19:02:58.723487 22626471084160 run.py:483] Algo bellman_ford step 9847 current loss 0.663161, current_train_items 315136.
I0302 19:02:58.754962 22626471084160 run.py:483] Algo bellman_ford step 9848 current loss 0.805172, current_train_items 315168.
I0302 19:02:58.788470 22626471084160 run.py:483] Algo bellman_ford step 9849 current loss 0.858280, current_train_items 315200.
I0302 19:02:58.808254 22626471084160 run.py:483] Algo bellman_ford step 9850 current loss 0.276702, current_train_items 315232.
I0302 19:02:58.816336 22626471084160 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0302 19:02:58.816440 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:02:58.833891 22626471084160 run.py:483] Algo bellman_ford step 9851 current loss 0.515818, current_train_items 315264.
I0302 19:02:58.859672 22626471084160 run.py:483] Algo bellman_ford step 9852 current loss 0.710402, current_train_items 315296.
I0302 19:02:58.890872 22626471084160 run.py:483] Algo bellman_ford step 9853 current loss 0.599299, current_train_items 315328.
I0302 19:02:58.925502 22626471084160 run.py:483] Algo bellman_ford step 9854 current loss 0.772818, current_train_items 315360.
I0302 19:02:58.945385 22626471084160 run.py:483] Algo bellman_ford step 9855 current loss 0.327952, current_train_items 315392.
I0302 19:02:58.961474 22626471084160 run.py:483] Algo bellman_ford step 9856 current loss 0.519965, current_train_items 315424.
I0302 19:02:58.986281 22626471084160 run.py:483] Algo bellman_ford step 9857 current loss 0.578772, current_train_items 315456.
I0302 19:02:59.017372 22626471084160 run.py:483] Algo bellman_ford step 9858 current loss 0.606215, current_train_items 315488.
I0302 19:02:59.050907 22626471084160 run.py:483] Algo bellman_ford step 9859 current loss 0.726136, current_train_items 315520.
I0302 19:02:59.071247 22626471084160 run.py:483] Algo bellman_ford step 9860 current loss 0.298839, current_train_items 315552.
I0302 19:02:59.087723 22626471084160 run.py:483] Algo bellman_ford step 9861 current loss 0.425507, current_train_items 315584.
I0302 19:02:59.111832 22626471084160 run.py:483] Algo bellman_ford step 9862 current loss 0.678425, current_train_items 315616.
I0302 19:02:59.143840 22626471084160 run.py:483] Algo bellman_ford step 9863 current loss 0.584870, current_train_items 315648.
I0302 19:02:59.177358 22626471084160 run.py:483] Algo bellman_ford step 9864 current loss 0.692028, current_train_items 315680.
I0302 19:02:59.197100 22626471084160 run.py:483] Algo bellman_ford step 9865 current loss 0.270776, current_train_items 315712.
I0302 19:02:59.213208 22626471084160 run.py:483] Algo bellman_ford step 9866 current loss 0.487495, current_train_items 315744.
I0302 19:02:59.237081 22626471084160 run.py:483] Algo bellman_ford step 9867 current loss 0.647604, current_train_items 315776.
I0302 19:02:59.269646 22626471084160 run.py:483] Algo bellman_ford step 9868 current loss 0.748401, current_train_items 315808.
I0302 19:02:59.303464 22626471084160 run.py:483] Algo bellman_ford step 9869 current loss 0.755545, current_train_items 315840.
I0302 19:02:59.323647 22626471084160 run.py:483] Algo bellman_ford step 9870 current loss 0.337822, current_train_items 315872.
I0302 19:02:59.340502 22626471084160 run.py:483] Algo bellman_ford step 9871 current loss 0.461679, current_train_items 315904.
I0302 19:02:59.364883 22626471084160 run.py:483] Algo bellman_ford step 9872 current loss 0.539409, current_train_items 315936.
I0302 19:02:59.396896 22626471084160 run.py:483] Algo bellman_ford step 9873 current loss 0.595743, current_train_items 315968.
I0302 19:02:59.429457 22626471084160 run.py:483] Algo bellman_ford step 9874 current loss 0.678309, current_train_items 316000.
I0302 19:02:59.449595 22626471084160 run.py:483] Algo bellman_ford step 9875 current loss 0.253782, current_train_items 316032.
I0302 19:02:59.465847 22626471084160 run.py:483] Algo bellman_ford step 9876 current loss 0.386688, current_train_items 316064.
I0302 19:02:59.488122 22626471084160 run.py:483] Algo bellman_ford step 9877 current loss 0.537986, current_train_items 316096.
I0302 19:02:59.519378 22626471084160 run.py:483] Algo bellman_ford step 9878 current loss 0.601657, current_train_items 316128.
I0302 19:02:59.552412 22626471084160 run.py:483] Algo bellman_ford step 9879 current loss 0.630810, current_train_items 316160.
I0302 19:02:59.571798 22626471084160 run.py:483] Algo bellman_ford step 9880 current loss 0.296119, current_train_items 316192.
I0302 19:02:59.588109 22626471084160 run.py:483] Algo bellman_ford step 9881 current loss 0.479076, current_train_items 316224.
I0302 19:02:59.612126 22626471084160 run.py:483] Algo bellman_ford step 9882 current loss 0.502004, current_train_items 316256.
I0302 19:02:59.644080 22626471084160 run.py:483] Algo bellman_ford step 9883 current loss 0.741107, current_train_items 316288.
I0302 19:02:59.677800 22626471084160 run.py:483] Algo bellman_ford step 9884 current loss 0.618053, current_train_items 316320.
I0302 19:02:59.698340 22626471084160 run.py:483] Algo bellman_ford step 9885 current loss 0.279360, current_train_items 316352.
I0302 19:02:59.714437 22626471084160 run.py:483] Algo bellman_ford step 9886 current loss 0.461995, current_train_items 316384.
I0302 19:02:59.738331 22626471084160 run.py:483] Algo bellman_ford step 9887 current loss 0.512012, current_train_items 316416.
I0302 19:02:59.769721 22626471084160 run.py:483] Algo bellman_ford step 9888 current loss 0.625304, current_train_items 316448.
I0302 19:02:59.802284 22626471084160 run.py:483] Algo bellman_ford step 9889 current loss 0.622078, current_train_items 316480.
I0302 19:02:59.822396 22626471084160 run.py:483] Algo bellman_ford step 9890 current loss 0.281818, current_train_items 316512.
I0302 19:02:59.838523 22626471084160 run.py:483] Algo bellman_ford step 9891 current loss 0.453229, current_train_items 316544.
I0302 19:02:59.861621 22626471084160 run.py:483] Algo bellman_ford step 9892 current loss 0.565608, current_train_items 316576.
I0302 19:02:59.893996 22626471084160 run.py:483] Algo bellman_ford step 9893 current loss 0.645882, current_train_items 316608.
I0302 19:02:59.928660 22626471084160 run.py:483] Algo bellman_ford step 9894 current loss 0.673172, current_train_items 316640.
I0302 19:02:59.948754 22626471084160 run.py:483] Algo bellman_ford step 9895 current loss 0.260789, current_train_items 316672.
I0302 19:02:59.964875 22626471084160 run.py:483] Algo bellman_ford step 9896 current loss 0.404859, current_train_items 316704.
I0302 19:02:59.989279 22626471084160 run.py:483] Algo bellman_ford step 9897 current loss 0.577015, current_train_items 316736.
I0302 19:03:00.020947 22626471084160 run.py:483] Algo bellman_ford step 9898 current loss 0.602382, current_train_items 316768.
I0302 19:03:00.056457 22626471084160 run.py:483] Algo bellman_ford step 9899 current loss 0.811694, current_train_items 316800.
I0302 19:03:00.076589 22626471084160 run.py:483] Algo bellman_ford step 9900 current loss 0.281870, current_train_items 316832.
I0302 19:03:00.084266 22626471084160 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0302 19:03:00.084373 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:03:00.101220 22626471084160 run.py:483] Algo bellman_ford step 9901 current loss 0.483655, current_train_items 316864.
I0302 19:03:00.125408 22626471084160 run.py:483] Algo bellman_ford step 9902 current loss 0.634240, current_train_items 316896.
I0302 19:03:00.156980 22626471084160 run.py:483] Algo bellman_ford step 9903 current loss 0.644202, current_train_items 316928.
I0302 19:03:00.191470 22626471084160 run.py:483] Algo bellman_ford step 9904 current loss 0.583149, current_train_items 316960.
I0302 19:03:00.211351 22626471084160 run.py:483] Algo bellman_ford step 9905 current loss 0.261891, current_train_items 316992.
I0302 19:03:00.227662 22626471084160 run.py:483] Algo bellman_ford step 9906 current loss 0.401911, current_train_items 317024.
I0302 19:03:00.252369 22626471084160 run.py:483] Algo bellman_ford step 9907 current loss 0.687884, current_train_items 317056.
I0302 19:03:00.283832 22626471084160 run.py:483] Algo bellman_ford step 9908 current loss 0.597154, current_train_items 317088.
I0302 19:03:00.315394 22626471084160 run.py:483] Algo bellman_ford step 9909 current loss 0.700115, current_train_items 317120.
I0302 19:03:00.335124 22626471084160 run.py:483] Algo bellman_ford step 9910 current loss 0.275150, current_train_items 317152.
I0302 19:03:00.351590 22626471084160 run.py:483] Algo bellman_ford step 9911 current loss 0.452667, current_train_items 317184.
I0302 19:03:00.375394 22626471084160 run.py:483] Algo bellman_ford step 9912 current loss 0.644261, current_train_items 317216.
I0302 19:03:00.407540 22626471084160 run.py:483] Algo bellman_ford step 9913 current loss 0.688555, current_train_items 317248.
I0302 19:03:00.440696 22626471084160 run.py:483] Algo bellman_ford step 9914 current loss 0.772093, current_train_items 317280.
I0302 19:03:00.460539 22626471084160 run.py:483] Algo bellman_ford step 9915 current loss 0.301458, current_train_items 317312.
I0302 19:03:00.476948 22626471084160 run.py:483] Algo bellman_ford step 9916 current loss 0.448962, current_train_items 317344.
I0302 19:03:00.501001 22626471084160 run.py:483] Algo bellman_ford step 9917 current loss 0.570948, current_train_items 317376.
I0302 19:03:00.532581 22626471084160 run.py:483] Algo bellman_ford step 9918 current loss 0.611014, current_train_items 317408.
I0302 19:03:00.565378 22626471084160 run.py:483] Algo bellman_ford step 9919 current loss 0.666730, current_train_items 317440.
I0302 19:03:00.585371 22626471084160 run.py:483] Algo bellman_ford step 9920 current loss 0.288325, current_train_items 317472.
I0302 19:03:00.601875 22626471084160 run.py:483] Algo bellman_ford step 9921 current loss 0.512012, current_train_items 317504.
I0302 19:03:00.625618 22626471084160 run.py:483] Algo bellman_ford step 9922 current loss 0.576227, current_train_items 317536.
I0302 19:03:00.655495 22626471084160 run.py:483] Algo bellman_ford step 9923 current loss 0.568212, current_train_items 317568.
I0302 19:03:00.691287 22626471084160 run.py:483] Algo bellman_ford step 9924 current loss 0.712797, current_train_items 317600.
I0302 19:03:00.710513 22626471084160 run.py:483] Algo bellman_ford step 9925 current loss 0.282450, current_train_items 317632.
I0302 19:03:00.726567 22626471084160 run.py:483] Algo bellman_ford step 9926 current loss 0.479561, current_train_items 317664.
I0302 19:03:00.751072 22626471084160 run.py:483] Algo bellman_ford step 9927 current loss 0.578120, current_train_items 317696.
I0302 19:03:00.783732 22626471084160 run.py:483] Algo bellman_ford step 9928 current loss 0.673463, current_train_items 317728.
I0302 19:03:00.814931 22626471084160 run.py:483] Algo bellman_ford step 9929 current loss 0.667904, current_train_items 317760.
I0302 19:03:00.834319 22626471084160 run.py:483] Algo bellman_ford step 9930 current loss 0.277616, current_train_items 317792.
I0302 19:03:00.849893 22626471084160 run.py:483] Algo bellman_ford step 9931 current loss 0.396369, current_train_items 317824.
I0302 19:03:00.874280 22626471084160 run.py:483] Algo bellman_ford step 9932 current loss 0.565452, current_train_items 317856.
I0302 19:03:00.905499 22626471084160 run.py:483] Algo bellman_ford step 9933 current loss 0.655056, current_train_items 317888.
I0302 19:03:00.937498 22626471084160 run.py:483] Algo bellman_ford step 9934 current loss 0.699112, current_train_items 317920.
I0302 19:03:00.957120 22626471084160 run.py:483] Algo bellman_ford step 9935 current loss 0.256791, current_train_items 317952.
I0302 19:03:00.973070 22626471084160 run.py:483] Algo bellman_ford step 9936 current loss 0.473642, current_train_items 317984.
I0302 19:03:00.996678 22626471084160 run.py:483] Algo bellman_ford step 9937 current loss 0.528195, current_train_items 318016.
I0302 19:03:01.027741 22626471084160 run.py:483] Algo bellman_ford step 9938 current loss 0.638815, current_train_items 318048.
I0302 19:03:01.062047 22626471084160 run.py:483] Algo bellman_ford step 9939 current loss 0.854170, current_train_items 318080.
I0302 19:03:01.081475 22626471084160 run.py:483] Algo bellman_ford step 9940 current loss 0.338519, current_train_items 318112.
I0302 19:03:01.097289 22626471084160 run.py:483] Algo bellman_ford step 9941 current loss 0.356405, current_train_items 318144.
I0302 19:03:01.120687 22626471084160 run.py:483] Algo bellman_ford step 9942 current loss 0.558153, current_train_items 318176.
I0302 19:03:01.152313 22626471084160 run.py:483] Algo bellman_ford step 9943 current loss 0.544813, current_train_items 318208.
I0302 19:03:01.184286 22626471084160 run.py:483] Algo bellman_ford step 9944 current loss 0.710284, current_train_items 318240.
I0302 19:03:01.203464 22626471084160 run.py:483] Algo bellman_ford step 9945 current loss 0.365182, current_train_items 318272.
I0302 19:03:01.219226 22626471084160 run.py:483] Algo bellman_ford step 9946 current loss 0.586735, current_train_items 318304.
I0302 19:03:01.243179 22626471084160 run.py:483] Algo bellman_ford step 9947 current loss 0.679286, current_train_items 318336.
I0302 19:03:01.275015 22626471084160 run.py:483] Algo bellman_ford step 9948 current loss 0.642042, current_train_items 318368.
I0302 19:03:01.309848 22626471084160 run.py:483] Algo bellman_ford step 9949 current loss 0.770211, current_train_items 318400.
I0302 19:03:01.329237 22626471084160 run.py:483] Algo bellman_ford step 9950 current loss 0.257449, current_train_items 318432.
I0302 19:03:01.337173 22626471084160 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0302 19:03:01.337282 22626471084160 run.py:522] Not saving new best model, best avg val score was 0.960, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:03:01.354430 22626471084160 run.py:483] Algo bellman_ford step 9951 current loss 0.423703, current_train_items 318464.
I0302 19:03:01.379027 22626471084160 run.py:483] Algo bellman_ford step 9952 current loss 0.533719, current_train_items 318496.
I0302 19:03:01.411835 22626471084160 run.py:483] Algo bellman_ford step 9953 current loss 0.748287, current_train_items 318528.
I0302 19:03:01.447446 22626471084160 run.py:483] Algo bellman_ford step 9954 current loss 0.755719, current_train_items 318560.
I0302 19:03:01.467323 22626471084160 run.py:483] Algo bellman_ford step 9955 current loss 0.316580, current_train_items 318592.
I0302 19:03:01.483728 22626471084160 run.py:483] Algo bellman_ford step 9956 current loss 0.492406, current_train_items 318624.
I0302 19:03:01.507900 22626471084160 run.py:483] Algo bellman_ford step 9957 current loss 0.582161, current_train_items 318656.
I0302 19:03:01.540119 22626471084160 run.py:483] Algo bellman_ford step 9958 current loss 0.659520, current_train_items 318688.
I0302 19:03:01.574797 22626471084160 run.py:483] Algo bellman_ford step 9959 current loss 0.715877, current_train_items 318720.
I0302 19:03:01.594900 22626471084160 run.py:483] Algo bellman_ford step 9960 current loss 0.338537, current_train_items 318752.
I0302 19:03:01.610712 22626471084160 run.py:483] Algo bellman_ford step 9961 current loss 0.453284, current_train_items 318784.
I0302 19:03:01.635518 22626471084160 run.py:483] Algo bellman_ford step 9962 current loss 0.651169, current_train_items 318816.
I0302 19:03:01.667203 22626471084160 run.py:483] Algo bellman_ford step 9963 current loss 0.632543, current_train_items 318848.
I0302 19:03:01.700433 22626471084160 run.py:483] Algo bellman_ford step 9964 current loss 0.733503, current_train_items 318880.
I0302 19:03:01.719816 22626471084160 run.py:483] Algo bellman_ford step 9965 current loss 0.321944, current_train_items 318912.
I0302 19:03:01.736314 22626471084160 run.py:483] Algo bellman_ford step 9966 current loss 0.488869, current_train_items 318944.
I0302 19:03:01.760369 22626471084160 run.py:483] Algo bellman_ford step 9967 current loss 0.577093, current_train_items 318976.
I0302 19:03:01.792045 22626471084160 run.py:483] Algo bellman_ford step 9968 current loss 0.649181, current_train_items 319008.
I0302 19:03:01.826025 22626471084160 run.py:483] Algo bellman_ford step 9969 current loss 0.654847, current_train_items 319040.
I0302 19:03:01.846169 22626471084160 run.py:483] Algo bellman_ford step 9970 current loss 0.272575, current_train_items 319072.
I0302 19:03:01.862188 22626471084160 run.py:483] Algo bellman_ford step 9971 current loss 0.489629, current_train_items 319104.
I0302 19:03:01.885973 22626471084160 run.py:483] Algo bellman_ford step 9972 current loss 0.631080, current_train_items 319136.
I0302 19:03:01.916468 22626471084160 run.py:483] Algo bellman_ford step 9973 current loss 0.703857, current_train_items 319168.
I0302 19:03:01.950012 22626471084160 run.py:483] Algo bellman_ford step 9974 current loss 0.668021, current_train_items 319200.
I0302 19:03:01.970150 22626471084160 run.py:483] Algo bellman_ford step 9975 current loss 0.310853, current_train_items 319232.
I0302 19:03:01.986606 22626471084160 run.py:483] Algo bellman_ford step 9976 current loss 0.435270, current_train_items 319264.
I0302 19:03:02.010216 22626471084160 run.py:483] Algo bellman_ford step 9977 current loss 0.529620, current_train_items 319296.
I0302 19:03:02.041545 22626471084160 run.py:483] Algo bellman_ford step 9978 current loss 0.599974, current_train_items 319328.
I0302 19:03:02.074013 22626471084160 run.py:483] Algo bellman_ford step 9979 current loss 0.634929, current_train_items 319360.
I0302 19:03:02.093666 22626471084160 run.py:483] Algo bellman_ford step 9980 current loss 0.303475, current_train_items 319392.
I0302 19:03:02.109802 22626471084160 run.py:483] Algo bellman_ford step 9981 current loss 0.484092, current_train_items 319424.
I0302 19:03:02.134562 22626471084160 run.py:483] Algo bellman_ford step 9982 current loss 0.583884, current_train_items 319456.
I0302 19:03:02.167958 22626471084160 run.py:483] Algo bellman_ford step 9983 current loss 0.683403, current_train_items 319488.
I0302 19:03:02.202309 22626471084160 run.py:483] Algo bellman_ford step 9984 current loss 0.769610, current_train_items 319520.
I0302 19:03:02.221962 22626471084160 run.py:483] Algo bellman_ford step 9985 current loss 0.318237, current_train_items 319552.
I0302 19:03:02.237887 22626471084160 run.py:483] Algo bellman_ford step 9986 current loss 0.361063, current_train_items 319584.
I0302 19:03:02.261659 22626471084160 run.py:483] Algo bellman_ford step 9987 current loss 0.596108, current_train_items 319616.
I0302 19:03:02.293315 22626471084160 run.py:483] Algo bellman_ford step 9988 current loss 0.650782, current_train_items 319648.
I0302 19:03:02.326439 22626471084160 run.py:483] Algo bellman_ford step 9989 current loss 0.657280, current_train_items 319680.
I0302 19:03:02.346295 22626471084160 run.py:483] Algo bellman_ford step 9990 current loss 0.239014, current_train_items 319712.
I0302 19:03:02.362687 22626471084160 run.py:483] Algo bellman_ford step 9991 current loss 0.382442, current_train_items 319744.
I0302 19:03:02.386353 22626471084160 run.py:483] Algo bellman_ford step 9992 current loss 0.653808, current_train_items 319776.
I0302 19:03:02.417267 22626471084160 run.py:483] Algo bellman_ford step 9993 current loss 0.657473, current_train_items 319808.
I0302 19:03:02.449629 22626471084160 run.py:483] Algo bellman_ford step 9994 current loss 0.610931, current_train_items 319840.
I0302 19:03:02.469505 22626471084160 run.py:483] Algo bellman_ford step 9995 current loss 0.360469, current_train_items 319872.
I0302 19:03:02.486010 22626471084160 run.py:483] Algo bellman_ford step 9996 current loss 0.530011, current_train_items 319904.
I0302 19:03:02.510293 22626471084160 run.py:483] Algo bellman_ford step 9997 current loss 0.471935, current_train_items 319936.
I0302 19:03:02.542122 22626471084160 run.py:483] Algo bellman_ford step 9998 current loss 0.548177, current_train_items 319968.
I0302 19:03:02.573495 22626471084160 run.py:483] Algo bellman_ford step 9999 current loss 0.781093, current_train_items 320000.
I0302 19:03:02.579428 22626471084160 run.py:527] Restoring best model from checkpoint...
I0302 19:03:05.376140 22626471084160 run.py:542] (test) algo bellman_ford : {'pi': 0.095703125, 'score': 0.095703125, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0302 19:03:05.376341 22626471084160 run.py:544] Done!
